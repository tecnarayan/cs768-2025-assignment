

@article{random_rank,
title = "The rank of a random matrix",
journal = "Applied Mathematics and Computation",
volume = "185",
number = "1",
pages = "689 - 694",
year = "2007",
issn = "0096-3003",
doi = "https://doi.org/10.1016/j.amc.2006.07.076",
url = "http://www.sciencedirect.com/science/article/pii/S0096300306009040",
author = "Xinlong Feng and Zhinan Zhang",
keywords = "Random matrix, Defective rank, Numerical rank, Condition number",
abstract = "This work is concerned with the numerical rank of matrix in the matrix computations. We conclude that a real random matrix has full rank with probability 1 and a rational random matrix has full rank with probability 1 too. Finally, the applications of the numerical matrix are given."
}

@article{recovery_guarantee,
  author    = {Kai Zhong and
               Zhao Song and
               Prateek Jain and
               Peter L. Bartlett and
               Inderjit S. Dhillon},
  title     = {Recovery Guarantees for One-hidden-layer Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1706.03175},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03175},
  archivePrefix = {arXiv},
  eprint    = {1706.03175},
  timestamp = {Mon, 13 Aug 2018 16:48:34 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ZhongS0BD17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{computer_vision,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@ARTICLE{nlp, 
author={G. Hinton and L. Deng and D. Yu and G. E. Dahl and A. Mohamed and N. Jaitly and A. Senior and V. Vanhoucke and P. Nguyen and T. N. Sainath and B. Kingsbury}, 
journal={IEEE Signal Processing Magazine}, 
title={Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups}, 
year={2012}, 
volume={29}, 
number={6}, 
pages={82-97}, 
keywords={feedforward neural nets;Gaussian processes;hidden Markov models;speech recognition;deep neural networks;acoustic modeling;speech recognition;hidden Markov models;temporal variability;Gaussian mixture models;feed-forward neural network;posterior probabilities;HMM states;Automatic speech recognition;Speech recognition;Hidden Markov models;Training;Gaussian processes;Acoustics;Neural networks;Data models}, 
doi={10.1109/MSP.2012.2205597}, 
ISSN={1053-5888}, 
month={Nov},}

@article{zhang_onelayer_relu,
  title={Learning One-hidden-layer ReLU Networks via Gradient Descent},
  author={Zhang, Xiao and Yu, Yaodong and Wang, Lingxiao and Gu, Quanquan},
  journal={arXiv preprint arXiv:1806.07808},
  year={2018}
}

@article{brutzkus_onelayer_cnn_gaussian,
  title={Globally optimal gradient descent for a convnet with gaussian inputs},
  author={Brutzkus, Alon and Globerson, Amir},
  journal={arXiv preprint arXiv:1702.07966},
  year={2017}
}

@article{du_onelayer_cnn,
  title={When is a Convolutional Filter Easy to Learn?},
  author={Du, Simon S and Lee, Jason D and Tian, Yuandong},
  journal={arXiv preprint arXiv:1709.06129},
  year={2017}
}

@InProceedings{du_onelayer_cnn2,
  title = 	 {Gradient Descent Learns One-hidden-layer {CNN}: Don’t be Afraid of Spurious Local Minima},
  author = 	 {Du, Simon and Lee, Jason and Tian, Yuandong and Singh, Aarti and Poczos, Barnabas},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1339--1348},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/du18b/du18b.pdf},
  url = 	 {http://proceedings.mlr.press/v80/du18b.html},
  abstract = 	 {We consider the problem of learning an one-hidden-layer neural network with non-overlapping convolutional layer and ReLU activation function, i.e., $f(Z; w, a) = \sum_j a_j\sigma(w^\top Z_j)$, in which both the convolutional weights $w$ and the output weights $a$ are parameters to be learned. We prove that with Gaussian input $\mathbf{Z}$ there is a spurious local minimizer. Surprisingly, in the presence of the spurious local minimizer, starting from randomly initialized weights, gradient descent with weight normalization can still be proven to recover the true parameters with constant probability (which can be boosted to probability $1$ with multiple restarts). We also show that with constant probability, the same procedure could also converge to the spurious local minimum, showing that the local minimum plays a non-trivial role in the dynamics of gradient descent. Furthermore, a quantitative analysis shows that the gradient descent dynamics has two phases: it starts off slow, but converges much faster after several iterations.}
}


@article{tian_onelayer_relu_gaussian,
  title={An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis},
  author={Tian, Yuandong},
  journal={arXiv preprint arXiv:1703.00560},
  year={2017}
}

@article{zhong_onelayer_smooth,
  title={Recovery guarantees for one-hidden-layer neural networks},
  author={Zhong, Kai and Song, Zhao and Jain, Prateek and Bartlett, Peter L and Dhillon, Inderjit S},
  journal={arXiv preprint arXiv:1706.03175},
  year={2017}
}

@article{soltanolkotabi_onelayer_smooth,
  title={Theoretical insights into the optimization landscape of over-parameterized shallow neural networks},
  author={Soltanolkotabi, Mahdi and Javanmard, Adel and Lee, Jason D},
  journal={IEEE Transactions on Information Theory},
  year={2018},
  publisher={IEEE}
}


@article{fu_onelayer_smooth,
  title={Local Geometry of One-Hidden-Layer Neural Networks for Logistic Regression},
  author={Fu, Haoyu and Chi, Yuejie and Liang, Yingbin},
  journal={arXiv preprint arXiv:1802.06463},
  year={2018}
}


@article{shallow_power,
title = "Approximation capabilities of multilayer feedforward networks",
journal = "Neural Networks",
volume = "4",
number = "2",
pages = "251 - 257",
year = "1991",
issn = "0893-6080",
doi = "https://doi.org/10.1016/0893-6080(91)90009-T",
url = "http://www.sciencedirect.com/science/article/pii/089360809190009T",
author = "Kurt Hornik",
keywords = "Multilayer feedforward networks, Activation function, Universal approximation capabilities, Input environment measure, () approximation, Uniform approximation, Sobolev spaces, Smooth approximation",
abstract = "We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives."
}

@article{du_overparameterized,
  author    = {Simon S. Du and
               Xiyu Zhai and
               Barnab{\'{a}}s P{\'{o}}czos and
               Aarti Singh},
  title     = {Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1810.02054},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.02054},
  archivePrefix = {arXiv},
  eprint    = {1810.02054},
  timestamp = {Tue, 30 Oct 2018 10:49:09 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-02054},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{li_overparameterized,
  author    = {Yuanzhi Li and
               Yingyu Liang},
  title     = {Learning Overparameterized Neural Networks via Stochastic Gradient
               Descent on Structured Data},
  journal   = {CoRR},
  volume    = {abs/1808.01204},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.01204},
  archivePrefix = {arXiv},
  eprint    = {1808.01204},
  timestamp = {Sun, 02 Sep 2018 15:01:55 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1808-01204},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{brutzkus_overparameterized_linear,
  author    = {Alon Brutzkus and
               Amir Globerson and
               Eran Malach and
               Shai Shalev{-}Shwartz},
  title     = {{SGD} Learns Over-parameterized Networks that Provably Generalize
               on Linearly Separable Data},
  journal   = {CoRR},
  volume    = {abs/1710.10174},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.10174},
  archivePrefix = {arXiv},
  eprint    = {1710.10174},
  timestamp = {Mon, 13 Aug 2018 16:47:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1710-10174},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{zhu_rnn,
  author    = {Zeyuan Allen{-}Zhu and
               Yuanzhi Li and
               Zhao Song},
  title     = {On the Convergence Rate of Training Recurrent Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1810.12065},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.12065},
  archivePrefix = {arXiv},
  eprint    = {1810.12065},
  timestamp = {Thu, 01 Nov 2018 18:03:07 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-12065},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{elman_rnn,
title = "Finding structure in time",
journal = "Cognitive Science",
volume = "14",
number = "2",
pages = "179 - 211",
year = "1990",
issn = "0364-0213",
doi = "https://doi.org/10.1016/0364-0213(90)90002-E",
url = "http://www.sciencedirect.com/science/article/pii/036402139090002E",
author = "Jeffrey L. Elman",
abstract = "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
}

@article{cho_rnn,
  author    = {KyungHyun Cho and
               Bart van Merrienboer and
               Dzmitry Bahdanau and
               Yoshua Bengio},
  title     = {On the Properties of Neural Machine Translation: Encoder-Decoder Approaches},
  journal   = {CoRR},
  volume    = {abs/1409.1259},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.1259},
  archivePrefix = {arXiv},
  eprint    = {1409.1259},
  timestamp = {Mon, 13 Aug 2018 16:47:23 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ChoMBB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{chung_rnn,
  author    = {Junyoung Chung and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               KyungHyun Cho and
               Yoshua Bengio},
  title     = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
               Modeling},
  journal   = {CoRR},
  volume    = {abs/1412.3555},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.3555},
  archivePrefix = {arXiv},
  eprint    = {1412.3555},
  timestamp = {Mon, 13 Aug 2018 16:47:38 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ChungGCB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{khrulkov_rnn,
  author    = {Valentin Khrulkov and
               Alexander Novikov and
               Ivan V. Oseledets},
  title     = {Expressive power of recurrent neural networks},
  journal   = {CoRR},
  volume    = {abs/1711.00811},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.00811},
  archivePrefix = {arXiv},
  eprint    = {1711.00811},
  timestamp = {Mon, 13 Aug 2018 16:45:56 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1711-00811},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{infinite_width,
  author    = {Pedro Savarese and
               Itay Evron and
               Daniel Soudry and
               Nathan Srebro},
  title     = {How do infinite width bounded norm networks look in function space?},
  journal   = {CoRR},
  volume    = {abs/1902.05040},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.05040},
  archivePrefix = {arXiv},
  eprint    = {1902.05040},
  timestamp = {Sat, 02 Mar 2019 16:35:43 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1902-05040},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{neyshabur_reg,
  title={In search of the real inductive bias: On the role of implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  journal={arXiv preprint arXiv:1412.6614},
  year={2014}
}

@book{boyd_convex,
  title={Convex optimization},
  author={Boyd, Stephen and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@ARTICLE{robust_candes, 
author={E. J. {Candes} and J. {Romberg} and T. {Tao}}, 
journal={IEEE Transactions on Information Theory}, 
title={Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information}, 
year={2006}, 
volume={52}, 
number={2}, 
pages={489-509}, 
keywords={image reconstruction;linear programming;convex programming;minimisation;Fourier analysis;image sampling;piecewise constant techniques;probability;sparse matrices;indeterminancy;signal reconstruction;signal sampling;robust uncertainty principle;signal reconstruction;incomplete frequency information;discrete-time signal;Fourier coefficient;minimization problem;convex optimization;image reconstruction;linear programming;sparse random matrix;trigonometric expansion;nonlinear sampling theorem;piecewise constant object;probability value;Robustness;Uncertainty;Signal reconstruction;Frequency;Image reconstruction;Mathematics;Biomedical imaging;Sampling methods;Linear programming;Signal processing;Convex optimization;duality in optimization;free probability;image reconstruction;linear programming;random matrices;sparsity;total-variation minimization;trigonometric expansions;uncertainty principle}, 
doi={10.1109/TIT.2005.862083}, 
ISSN={0018-9448}, 
month={Feb},}

@Article{sparsity_zhao,
author="Zhao, Yun-Bin",
title="Equivalence and Strong Equivalence Between the Sparsest and Least {$\ell_1$}-Norm Nonnegative Solutions of Linear Systems and Their Applications",
journal="Journal of the Operations Research Society of China",
year="2014",
month="Jun",
day="01",
volume="2",
number="2",
pages="171--193",
issn="2194-6698",
doi="10.1007/s40305-014-0043-1",
url="https://doi.org/10.1007/s40305-014-0043-1"
}

@article{quantized_hartmut,
  title={Gradient descent quantizes ReLU network features},
  author={Maennel, Hartmut and Bousquet, Olivier and Gelly, Sylvain},
  journal={arXiv preprint arXiv:1803.08367},
  year={2018}
}

@book{semiinfinite_goberna,
author = {Goberna, Miguel Angel and L\'{o}pez-Cerd\'{a}, Marco},
year = {1998},
month = {01},
pages = {},
title = {Linear semi-infinite optimization},
doi = {10.1007/978-1-4899-8044-1_3}
}

@book{TalagrandBook,
  title={Probability in Banach Spaces: isoperimetry and processes},
  author={Ledoux, Michel and Talagrand, Michel},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@inproceedings{earlystopping_caruna,
  title={Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping},
  author={Caruana, Rich and Lawrence, Steve and Giles, C Lee},
  booktitle={Advances in neural information processing systems},
  pages={402--408},
  year={2001}
}

@inproceedings{weightdecay_krogh,
  title={A simple weight decay can improve generalization},
  author={Krogh, Anders and Hertz, John A},
  booktitle={Advances in neural information processing systems},
  pages={950--957},
  year={1992}
}


@misc{cifar10,
  title={The {CIFAR}-10 dataset},
  author={Krizhevsky, Alex and Nair, Vinod and Hinton, Geoffrey},
  howpublished="\url{http://www. cs. toronto. edu/kriz/cifar. html}",
  year={2014}
}

@misc{news20,
  title={20 Newsgroups},
  howpublished="\url{http://qwone.com/~jason/20Newsgroups/}"
}

@misc{rcv,
  title={Text datasets in matlab format},
  howpublished="\url{http://www.cad.zju.edu.cn/home/dengcai/Data/TextData.htm}"
}

@misc{mnist,
  title={The {MNIST} database of handwritten digits},
  author={LeCun, Yann},
  howpublished ="\url{http://yann. lecun. com/exdb/mnist/}"
}

@misc{regression,
author = "L. Torgo",
institution = "University of Porto, Department of Computer Science",
title = "Regression Data Sets",
howpublished = "\url{http://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html}"
}

@incollection{kmenas_andrewng,
  title={Learning feature representations with k-means},
  author={Coates, Adam and Ng, Andrew Y},
  booktitle={Neural networks: Tricks of the trade},
  pages={561--580},
  year={2012},
  publisher={Springer}
}

@article{mitchell1997mcgraw,
  title={Mcgraw-hill science},
  author={Mitchell, Tom M and Learning, Machine},
  journal={Engineering/Math},
  volume={1},
  pages={27},
  year={1997}
}

@article{zhao2006model,
  title={On model selection consistency of Lasso},
  author={Zhao, Peng and Yu, Bin},
  journal={Journal of Machine learning research},
  volume={7},
  number={Nov},
  pages={2541--2563},
  year={2006}
}

@article{fung2011equivalence,
  title={Equivalence of minimal  $\ell_0$-and $\ell_p$-norm solutions of linear equalities, inequalities and linear programs for sufficiently small p},
  author={Fung, GM and Mangasarian, OL},
  journal={Journal of optimization theory and applications},
  volume={151},
  number={1},
  pages={1--10},
  year={2011},
  publisher={Springer}
}


@article{frank_wolfe,
author = {Frank, Marguerite and Wolfe, Philip},
title = {An algorithm for quadratic programming},
journal = {Naval Research Logistics Quarterly},
volume = {3},
number = {1‐2},
pages = {95-110},
doi = {10.1002/nav.3800030109},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800030109},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.3800030109},
year = {1956}
}

@article{sion_minimax,
author = "Sion, Maurice",
fjournal = "Pacific Journal of Mathematics",
journal = "Pacific J. Math.",
number = "1",
pages = "171--176",
publisher = "Pacific Journal of Mathematics, A Non-profit Corporation",
title = "On general minimax theorems.",
url = "https://projecteuclid.org:443/euclid.pjm/1103040253",
volume = "8",
year = "1958"
}

@misc{cvx,
  author       = {Michael Grant and Stephen Boyd},
  title        = {{CVX}: Matlab Software for Disciplined Convex Programming, version 2.1},
  howpublished = {\url{http://cvxr.com/cvx}},
  month        = mar,
  year         = 2014
}

@article{implicit_reg_blanc,
  author    = {Guy Blanc and
               Neha Gupta and
               Gregory Valiant and
               Paul Valiant},
  title     = {Implicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck
               like process},
  journal   = {CoRR},
  volume    = {abs/1904.09080},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.09080},
  archivePrefix = {arXiv},
  eprint    = {1904.09080},
  timestamp = {Fri, 26 Apr 2019 13:18:53 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1904-09080},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{margin_theory_tengyu,
  title={On the margin theory of feedforward neural networks},
  author={Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
  journal={arXiv preprint arXiv:1810.05369},
  year={2018}
}

@article{lazy_training_bach,
  title={A note on lazy training in supervised differentiable programming},
  author={Chizat, Lenaic and Bach, Francis},
  journal={arXiv preprint arXiv:1812.07956},
  year={2018}
}

@inproceedings{rudelson2010non,
  title={Non-asymptotic theory of random matrices: extreme singular values},
  author={Rudelson, Mark and Vershynin, Roman},
  booktitle={Proceedings of the International Congress of Mathematicians 2010 (ICM 2010) (In 4 Volumes) Vol. I: Plenary Lectures and Ceremonies Vols. II--IV: Invited Lectures},
  pages={1576--1602},
  year={2010},
  organization={World Scientific}
}

@article{understanding_zhang,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}

@article{baksalary2007particular,
  title={Particular formulae for the Moore--Penrose inverse of a columnwise partitioned matrix},
  author={Baksalary, Jerzy K and Baksalary, Oskar Maria},
  journal={Linear algebra and its applications},
  volume={421},
  number={1},
  pages={16--23},
  year={2007},
  publisher={Elsevier}
}

@inproceedings{bengio2006convex,
  title={Convex neural networks},
  author={Bengio, Yoshua and Roux, Nicolas L and Vincent, Pascal and Delalleau, Olivier and Marcotte, Patrice},
  booktitle={Advances in neural information processing systems},
  pages={123--130},
  year={2006}
}

@article{bach2017breaking,
  title={Breaking the curse of dimensionality with convex neural networks},
  author={Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={629--681},
  year={2017},
  publisher={JMLR. org}
}

@article{guruswami2009hardness,
  title={Hardness of learning halfspaces with noise},
  author={Guruswami, Venkatesan and Raghavendra, Prasad},
  journal={SIAM Journal on Computing},
  volume={39},
  number={2},
  pages={742--765},
  year={2009},
  publisher={SIAM}
}

@article{ha1980minimax,
  title={Minimax and fixed point theorems},
  author={Ha, Chung-Wei},
  journal={Mathematische Annalen},
  volume={248},
  number={1},
  pages={73--77},
  year={1980},
  publisher={Springer}
}

@misc{spgl1,
  author = {E. van den Berg and M. P. Friedlander},
  title = {{SPGL1}: A solver for large-scale sparse reconstruction},
  note = {http://www.cs.ubc.ca/labs/scl/spgl1},
  month = {June},
  year ={2007}
}


@article{superscs,
  title={SuperMann: a superlinearly convergent algorithm for finding fixed points of nonexpansive operators},
  author={Themelis, Andreas and Patrinos, Panagiotis},
  journal={IEEE Transactions on Automatic Control},
  year={2019},
  publisher={IEEE}
}

@inproceedings{ntk_jacot,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}

@article{ntk_1d,
  title={Gradient Dynamics of Shallow Univariate ReLU Networks},
  author={Williams, Francis and Trager, Matthew and Silva, Claudio and Panozzo, Daniele and Zorin, Denis and Bruna, Joan},
  journal={arXiv preprint arXiv:1906.07842},
  year={2019}
}

@incollection{neal_priorinfinite,
  title={Priors for infinite networks},
  author={Neal, Radford M},
  booktitle={Bayesian Learning for Neural Networks},
  pages={29--53},
  year={1996},
  publisher={Springer}
}

@article{gaussian_infinite,
  title={Gaussian process behaviour in wide deep neural networks},
  author={Matthews, Alexander G de G and Rowland, Mark and Hron, Jiri and Turner, Richard E and Ghahramani, Zoubin},
  journal={arXiv preprint arXiv:1804.11271},
  year={2018}
}

@article{gaussian_infinite2,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.00165},
  year={2017}
}

@article{ntk_relu_1d,
  title={On the inductive bias of neural tangent kernels},
  author={Bietti, Alberto and Mairal, Julien},
  journal={arXiv preprint arXiv:1905.12173},
  year={2019}
}




@inproceedings{gunasekar_implicit_mf,
  title={Implicit regularization in matrix factorization},
  author={Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6151--6159},
  year={2017}
}

@article{arora_implicit_deepmf,
  title={Implicit Regularization in Deep Matrix Factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  journal={arXiv preprint arXiv:1905.13655},
  year={2019}
}

@inproceedings{telgrasky_deeplinear,
title={Gradient descent aligns the layers of deep linear networks},
author={Ziwei Ji and Matus Telgarsky},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HJflg30qKX},
}

@inproceedings{du_deeplinear,
  title={Width Provably Matters in Optimization for Deep Linear Neural Networks},
  author={Du, Simon and Hu, Wei},
  booktitle={International Conference on Machine Learning},
  pages={1655--1664},
  year={2019}
}

@article{saxe_deeplinear,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}

@article{shamir_convergence_deeplinear,
  title={Exponential convergence time of gradient descent for one-dimensional deep linear neural networks},
  author={Shamir, Ohad},
  journal={arXiv preprint arXiv:1809.08587},
  year={2018}
}

@article{allenzhu_backward,
    title={Backward Feature Correction: How Deep Learning Performs Deep Learning},
    author={Zeyuan Allen-Zhu and Yuanzhi Li},
    year={2020},
    eprint={2001.04413},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{recht_deeplinear,
  title={Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global},
  author={Laurent, Thomas and Brecht, James},
  booktitle={International Conference on Machine Learning},
  pages={2902--2907},
  year={2018}
}

@article{arora_deeplinearconv,
  author    = {Sanjeev Arora and
               Nadav Cohen and
               Noah Golowich and
               Wei Hu},
  title     = {A Convergence Analysis of Gradient Descent for Deep Linear Neural
               Networks},
  journal   = {CoRR},
  volume    = {abs/1810.02281},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.02281},
  archivePrefix = {arXiv},
  eprint    = {1810.02281},
  timestamp = {Tue, 30 Oct 2018 10:49:09 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-02281},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{parhi_minimum,
  title={Minimum ``Norm'' Neural Networks are Splines},
  author={Parhi, Rahul and Nowak, Robert D},
  journal={arXiv preprint arXiv:1910.02333},
  year={2019}
}

@inproceedings{srebro_lowrankrecovery,
  title={Global optimality of local search for low rank matrix recovery},
  author={Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3873--3881},
  year={2016}
}

@inproceedings{gunasekar_cnn,
  title={Implicit bias of gradient descent on linear convolutional networks},
  author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9461--9471},
  year={2018}
}

@inproceedings{du_relu_alignment,
  title={Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced},
  author={Du, Simon S and Hu, Wei and Lee, Jason D},
  booktitle={Advances in Neural Information Processing Systems},
  pages={384--395},
  year={2018}
}

@article{arora_linear_alignment,
  title={On the optimization of deep networks: Implicit acceleration by overparameterization},
  author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  journal={arXiv preprint arXiv:1802.06509},
  year={2018}
}

@inproceedings{rosset2007,
  title={L1 regularization in infinite dimensional feature spaces},
  author={Rosset, Saharon and Swirszcz, Grzegorz and Srebro, Nathan and Zhu, Ji},
  booktitle={International Conference on Computational Learning Theory},
  pages={544--558},
  year={2007},
  organization={Springer}
}





@misc{vulnerability,
    title={Analysis of adversarial attacks against CNN-based image forgery detectors},
    author={Diego Gragnaniello and Francesco Marra and Giovanni Poggi and Luisa Verdoliva},
    year={2018},
    eprint={1808.08426},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{mangal2020covidaid,
    title={CovidAID: COVID-19 Detection Using Chest X-Ray},
    author={Arpan Mangal and Surya Kalia and Harish Rajgopal and Krithika Rangarajan and Vinay Namboodiri and Subhashis Banerjee and Chetan Arora},
    year={2020},
    eprint={2004.09803},
    archivePrefix={arXiv},
    primaryClass={eess.IV}
}

@inproceedings{zhang2019multibranch,
  title={Deep neural networks with multi-branch architectures are intrinsically less non-convex},
  author={Zhang, Hongyang and Shao, Junru and Salakhutdinov, Ruslan},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1099--1109},
  year={2019}
}




@article{higdim1,
  title={Training neural networks on high-dimensional data using random projection},
  author={W{\'o}jcik, Piotr Iwo and Kurdziel, Marcin},
  journal={Pattern Analysis and Applications},
  volume={22},
  number={3},
  pages={1221--1231},
  year={2019},
  publisher={Springer}
}


@article{highdim_bio,
  title={FsNet: Feature Selection Network on High-dimensional Biological Data},
  author={Singh, Dinesh and Yamada, Makoto},
  journal={arXiv preprint arXiv:2001.08322},
  year={2020}
}

@article{highdim_medical,
  title={Framelet pooling aided deep learning network: the method to process high dimensional medical data},
  author={Hyun, Chang Min and Kim, Kang Cheol and Cho, Hyun Cheol and Choi, Jae Kyu and Seo, Jin Keun},
  journal={Machine Learning: Science and Technology},
  volume={1},
  number={1},
  pages={015009},
  year={2020},
  publisher={IOP Publishing}
}


@inproceedings{whitening,
  title={Decorrelated batch normalization},
  author={Huang, Lei and Yang, Dawei and Lang, Bo and Deng, Jia},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={791--800},
  year={2018}
}

@inproceedings{fewshot,
  title={A Closer Look at Few-shot Classification},
  author={Chen, Wei-Yu and Liu, Yen-Cheng and Kira, Zsolt and Wang, Yu-Chiang Frank and Huang, Jia-Bin},
  booktitle={International Conference on Learning Representations},
  year={2018}
}





@InProceedings{batch_norm,
  title = 	 {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author = 	 {Sergey Ioffe and Christian Szegedy},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {448--456},
  year = 	 {2015},
  editor = 	 {Francis Bach and David Blei},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/ioffe15.pdf},
  url = 	 {http://proceedings.mlr.press/v37/ioffe15.html},
  abstract = 	 {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.}
}


@article{papyan2020neuralcollapse,
  title={Prevalence of neural collapse during the terminal phase of deep learning training},
  author={Papyan, Vardan and Han, XY and Donoho, David L},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={40},
  pages={24652--24663},
  year={2020},
  publisher={National Acad Sciences}
}

@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}


@article{ergen2020journal,
  title={Convex Geometry and Duality of Over-parameterized Neural Networks},
  author={Ergen, Tolga and Pilanci, Mert},
  journal={arXiv preprint arXiv:2002.11219},
  year={2020}
}

@InProceedings{ergen2020aistats,
  title = 	 {Convex Geometry of Two-Layer ReLU Networks: Implicit Autoencoding and Interpretable Models},
  author = 	 {Ergen, Tolga and Pilanci, Mert},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {4024--4033},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Online},
  month = 	 {26--28 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/ergen20a/ergen20a.pdf},
  url = 	 {http://proceedings.mlr.press/v108/ergen20a.html}
}




@INPROCEEDINGS{ergen2019shallow,
  author={T. {Ergen} and M. {Pilanci}},
  booktitle={2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, 
  title={Convex Optimization for Shallow Neural Networks}, 
  year={2019},
  volume={},
  number={},
  pages={79-83},}
  


@inproceedings{ergen2020cnn,
title={Implicit Convex Regularizers of CNN Architectures: Convex Optimization of Two- and Three-Layer Networks in Polynomial Time},
author={Tolga Ergen and Mert Pilanci},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=0N8jUH4JMv6}
}

@inproceedings{sahiner2021vectoroutput,
title={Vector-output ReLU Neural Network Problems are Copositive Programs: Convex Analysis of Two Layer Networks and Polynomial-time Algorithms},
author={Arda Sahiner and Tolga Ergen and John M. Pauly and Mert Pilanci},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=fGF8qAqpXXG}
}

@article{ergen2021bn,
  author    = {Tolga Ergen and
               Arda Sahiner and
               Batu Ozturkler and
               John M. Pauly and
               Morteza Mardani and
               Mert Pilanci},
  title     = {Demystifying Batch Normalization in ReLU Networks: Equivalent Convex
               Optimization Models and Implicit Regularization},
  journal   = {CoRR},
  volume    = {abs/2103.01499},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.01499},
  archivePrefix = {arXiv},
  eprint    = {2103.01499},
  timestamp = {Thu, 04 Mar 2021 17:00:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-01499.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{vikul2021generative,
  author={Gupta, Vikul and Bartan, Burak and Ergen, Tolga and Pilanci, Mert},
  booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Convex Neural Autoregressive Models: Towards Tractable, Expressive, and Theoretically-Backed Models for Sequential Forecasting and Generation}, 
  year={2021},
  volume={},
  number={},
  pages={3890-3894},
  doi={10.1109/ICASSP39728.2021.9413662}}
  
  
  @InProceedings{pilanci2020convex, 
  title = {Neural Networks are Convex Regularizers: Exact Polynomial-time Convex Optimization Formulations for Two-layer Networks}, 
  author = {Pilanci, Mert and Ergen, Tolga}, 
  booktitle = {Proceedings of the 37th International Conference on Machine Learning}, 
  pages = {7695--7705}, 
  year = {2020}, 
  editor = {Hal Daumé III and Aarti Singh}, 
  volume = {119}, 
  series = {Proceedings of Machine Learning Research}, month = {13--18 Jul}, 
  publisher = {PMLR}, 
  pdf = {http://proceedings.mlr.press/v119/pilanci20a/pilanci20a.pdf}, 
  url = { http://proceedings.mlr.press/v119/pilanci20a.html } }
  
  
  @inproceedings{ergen2019cutting,
  title={Convex duality and cutting plane methods for over-parameterized neural networks},
  author={Ergen, Tolga and Pilanci, Mert},
  booktitle={OPT-ML workshop},
  year={2019}
}


@inproceedings{ergen2020workshop,
  title={Convex Programs for Global Optimization of Convolutional Neural Networks in Polynomial-Time},
  author={Ergen, Tolga and Pilanci, Mert},
  booktitle={OPT-ML workshop},
  year={2020}
}