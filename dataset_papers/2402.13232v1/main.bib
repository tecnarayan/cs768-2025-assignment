@article{wu2023nextgpt,
  title={NExT-GPT: Any-to-Any Multimodal LLM},
  author={Shengqiong Wu and Hao Fei and Leigang Qu and Wei Ji and Tat-Seng Chua},
  journal = {CoRR},
  volume = {abs/2309.05519},
  year={2023}
}

@inproceedings{driess2023palme,
    title={PaLM-E: An Embodied Multimodal Language Model},
    author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
    booktitle={arXiv preprint arXiv:2303.03378},
    year={2023}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI and : and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mo Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{gao2023llamaadapterv2,
  title = {LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model},
  author={Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and Li, Hongsheng and Qiao, Yu},
  journal={arXiv preprint arXiv:2304.15010},
  year={2023}
}

@article{zhang2023llamaadapter,
  title = {LLaMA-Adapter: Efficient Finetuning of Language Models with Zero-init Attention},
  author={Zhang, Renrui and Han, Jiaming and Liu, Chris and Gao, Peng and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Qiao, Yu},
  journal={arXiv preprint arXiv:2303.16199},
  year={2023}
}

@inproceedings{
yang2022touch,
  title={Touch and Go: Learning from Human-Collected Vision and Touch},
  author={Fengyu Yang and Chenyang Ma and Jiacheng Zhang and Jing Zhu and Wenzhen Yuan and Andrew Owens},
  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2022}
}

@misc{kerr2023selfsupervised,
      title={Self-Supervised Visuo-Tactile Pretraining to Locate and Follow Garment Features}, 
      author={Justin Kerr and Huang Huang and Albert Wilcox and Ryan Hoque and Jeffrey Ichnowski and Roberto Calandra and Ken Goldberg},
      year={2023},
      eprint={2209.13042},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@misc{yuan2017connecting,
      title={Connecting Look and Feel: Associating the visual and tactile properties of physical materials}, 
      author={Wenzhen Yuan and Shaoxiong Wang and Siyuan Dong and Edward Adelson},
      year={2017},
      eprint={1704.03822},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{li2019connecting,
      title={Connecting Touch and Vision via Cross-Modal Prediction}, 
      author={Yunzhu Li and Jun-Yan Zhu and Russ Tedrake and Antonio Torralba},
      year={2019},
      eprint={1906.06322},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{liu2023improvedllava,
      author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
      title={Improved Baselines with Visual Instruction Tuning}, 
      publisher={arXiv:2310.03744},
      year={2023},
}

@inproceedings{liu2023llava,
author      = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
title       = {Visual Instruction Tuning},
booktitle   = {NeurIPS},
year        = {2023}
}

@misc{burns2023weaktostrong,
      title={Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision}, 
      author={Collin Burns and Pavel Izmailov and Jan Hendrik Kirchner and Bowen Baker and Leo Gao and Leopold Aschenbrenner and Yining Chen and Adrien Ecoffet and Manas Joglekar and Jan Leike and Ilya Sutskever and Jeff Wu},
      year={2023},
      eprint={2312.09390},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
    zhong2022touching,
    title={Touching a Ne{RF}: Leveraging Neural Radiance Fields for Tactile Sensory Data Generation},
    author={Shaohong Zhong and Alessandro Albini and Oiwi Parker Jones and Perla Maiolino and Ingmar Posner},
    booktitle={6th Annual Conference on Robot Learning},
    year={2022},
    url={https://openreview.net/forum?id=No3mbanRlZJ}
}

@inproceedings{
suresh2022midastouch,
title={MidasTouch: Monte-Carlo inference over distributions across sliding touch},
author={Sudharshan Suresh and Zilin Si and Stuart Anderson and Michael Kaess and Mustafa Mukadam},
booktitle={6th Annual Conference on Robot Learning},
year={2022},
url={https://openreview.net/forum?id=JWROnOf4w-K}
}

@article{higuera2023learning,
      title={Learning to Read Braille: Bridging the Tactile Reality Gap with Diffusion Models}, 
      author={Carolina Higuera and Byron Boots and Mustafa Mukadam},
      year={2023},
      eprint={2304.01182},
      archivePrefix={arXiv},
}

@misc{chen2023sharegpt4v,
      title={ShareGPT4V: Improving Large Multi-Modal Models with Better Captions}, 
      author={Lin Chen and Jinsong Li and Xiaoyi Dong and Pan Zhang and Conghui He and Jiaqi Wang and Feng Zhao and Dahua Lin},
      year={2023},
      eprint={2311.12793},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{girdhar2023imagebind,
  title={ImageBind: One Embedding Space To Bind Them All},
  author={Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang
and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
  booktitle={CVPR},
  year={2023}
}

@misc{guzhov2021audioclip,
      title={AudioCLIP: Extending CLIP to Image, Text and Audio}, 
      author={Andrey Guzhov and Federico Raue and Jörn Hees and Andreas Dengel},
      year={2021},
      eprint={2106.13043},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@misc{reed2022generalist,
      title={A Generalist Agent}, 
      author={Scott Reed and Konrad Zolna and Emilio Parisotto and Sergio Gomez Colmenarejo and Alexander Novikov and Gabriel Barth-Maron and Mai Gimenez and Yury Sulsky and Jackie Kay and Jost Tobias Springenberg and Tom Eccles and Jake Bruce and Ali Razavi and Ashley Edwards and Nicolas Heess and Yutian Chen and Raia Hadsell and Oriol Vinyals and Mahyar Bordbar and Nando de Freitas},
      year={2022},
      eprint={2205.06175},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@inproceedings{rt22023arxiv,
    title={RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control},
    author={Anthony Brohan and Noah Brown and Justice Carbajal and Yevgen Chebotar and Xi Chen and Krzysztof Choromanski and Tianli Ding and Danny Driess and Avinava Dubey and Chelsea Finn and Pete Florence and Chuyuan Fu and Montse Gonzalez Arenas and Keerthana Gopalakrishnan and Kehang Han and Karol Hausman and Alex Herzog and Jasmine Hsu and Brian Ichter and Alex Irpan and Nikhil Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Isabel Leal  and Lisa Lee and Tsang-Wei Edward Lee and Sergey Levine and Yao Lu and Henryk Michalewski and Igor Mordatch and Karl Pertsch and Kanishka Rao and Krista Reymann and Michael Ryoo and Grecia Salazar and Pannag Sanketi and Pierre Sermanet and Jaspiar Singh and Anikait Singh and Radu Soricut and Huong Tran and Vincent Vanhoucke and Quan Vuong and Ayzaan Wahid and Stefan Welker and Paul Wohlhart and  Jialin Wu and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Tianhe Yu and Brianna Zitkovich},
    booktitle={arXiv preprint arXiv:2307.15818},
    year={2023}
}

@inproceedings{li2022blip,
      title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}, 
      author={Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi},
      year={2022},
      booktitle={ICML},
}

@misc{li2023blip2,
      title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models}, 
      author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
      year={2023},
      eprint={2301.12597},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{fu2023safe,
      title={Safe Self-Supervised Learning in Real of Visuo-Tactile Feedback Policies for Industrial Insertion}, 
      author={Letian Fu and Huang Huang and Lars Berscheid and Hui Li and Ken Goldberg and Sachin Chitta},
      year={2023},
      eprint={2210.01340},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@ARTICLE{10185075,
  author={Zhang, Fan and Demiris, Yiannis},
  journal={IEEE Robotics and Automation Letters}, 
  title={Visual-Tactile Learning of Garment Unfolding for Robot-Assisted Dressing}, 
  year={2023},
  volume={8},
  number={9},
  pages={5512-5519},
  keywords={Clothing;Robot sensing systems;Predictive models;Robots;Grasping;Visualization;Grippers;Force and tactile sensing;physical human-robot interaction;model learning for control},
  doi={10.1109/LRA.2023.3296371}}

@misc{chen2022visuotactile,
      title={Visuo-Tactile Transformers for Manipulation}, 
      author={Yizhou Chen and Andrea Sipos and Mark Van der Merwe and Nima Fazeli},
      year={2022},
      eprint={2210.00121},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@misc{zhang2021pointclip,
      title={PointCLIP: Point Cloud Understanding by CLIP}, 
      author={Renrui Zhang and Ziyu Guo and Wei Zhang and Kunchang Li and Xupeng Miao and Bin Cui and Yu Qiao and Peng Gao and Hongsheng Li},
      year={2021},
      eprint={2112.02413},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{guo2023pointbind,
      title={Point-Bind and Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following}, 
      author={Ziyu Guo and Renrui Zhang and Xiangyang Zhu and Yiwen Tang and Xianzheng Ma and Jiaming Han and Kexin Chen and Peng Gao and Xianzhi Li and Hongsheng Li and Pheng-Ann Heng},
      year={2023},
      eprint={2309.00615},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@ARTICLE{digit,
  author={Lambeta, Mike and Chou, Po-Wei and Tian, Stephen and Yang, Brian and Maloon, Benjamin and Most, Victoria Rose and Stroud, Dave and Santos, Raymond and Byagowi, Ahmad and Kammerer, Gregg and Jayaraman, Dinesh and Calandra, Roberto},
  journal={IEEE Robotics and Automation Letters}, 
  title={DIGIT: A Novel Design for a Low-Cost Compact High-Resolution Tactile Sensor With Application to In-Hand Manipulation}, 
  year={2020},
  volume={5},
  number={3},
  pages={3838-3845},
  keywords={Tactile sensors;Cameras;Task analysis;Reliability;Perception for grasping and manipulation;force and tactile sensing;deep learning in robotics and automation;learning and adaptive systems},
  doi={10.1109/LRA.2020.2977257}}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{rombach2022highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2022},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{brooks2023instructpix2pix,
      title={InstructPix2Pix: Learning to Follow Image Editing Instructions}, 
      author={Tim Brooks and Aleksander Holynski and Alexei A. Efros},
      year={2023},
      eprint={2211.09800},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{wang2023cut,
      title={Cut and Learn for Unsupervised Object Detection and Instance Segmentation}, 
      author={Xudong Wang and Rohit Girdhar and Stella X. Yu and Ishan Misra},
      year={2023},
      eprint={2301.11320},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{caron2021emerging,
      title={Emerging Properties in Self-Supervised Vision Transformers}, 
      author={Mathilde Caron and Hugo Touvron and Ishan Misra and Hervé Jégou and Julien Mairal and Piotr Bojanowski and Armand Joulin},
      year={2021},
      eprint={2104.14294},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{johansson2009coding,
  title={Coding and use of tactile signals from the fingertips in object manipulation tasks},
  author={Johansson, Roland S and Flanagan, J Randall},
  journal={Nature Reviews Neuroscience},
  volume={10},
  number={5},
  pages={345--359},
  year={2009},
  publisher={Nature Publishing Group UK London}
}

@article{dave2024multimodal,
  title={Multimodal Visual-Tactile Representation Learning through Self-Supervised Contrastive Pre-Training},
  author={Dave, Vedant and Lygerakis, Fotios and Rueckert, Elmar},
  journal={arXiv preprint arXiv:2401.12024},
  year={2024}
}

@misc{han2023imagebindllm,
      title={ImageBind-LLM: Multi-modality Instruction Tuning}, 
      author={Jiaming Han and Renrui Zhang and Wenqi Shao and Peng Gao and Peng Xu and Han Xiao and Kaipeng Zhang and Chris Liu and Song Wen and Ziyu Guo and Xudong Lu and Shuai Ren and Yafei Wen and Xiaoxin Chen and Xiangyu Yue and Hongsheng Li and Yu Qiao},
      year={2023},
      eprint={2309.03905},
      archivePrefix={arXiv},
      primaryClass={cs.MM}
}

@misc{lu2023unifiedio,
      title={Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action}, 
      author={Jiasen Lu and Christopher Clark and Sangho Lee and Zichen Zhang and Savya Khosla and Ryan Marten and Derek Hoiem and Aniruddha Kembhavi},
      year={2023},
      eprint={2312.17172},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{zhu2023minigpt,
  title={MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@misc{moon2023anymal,
      title={AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model}, 
      author={Seungwhan Moon and Andrea Madotto and Zhaojiang Lin and Tushar Nagarajan and Matt Smith and Shashank Jain and Chun-Fu Yeh and Prakash Murugesan and Peyman Heidari and Yue Liu and Kavya Srinet and Babak Damavandi and Anuj Kumar},
      year={2023},
      eprint={2309.16058},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{tang2023codi2,
      title={CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation}, 
      author={Zineng Tang and Ziyi Yang and Mahmoud Khademi and Yang Liu and Chenguang Zhu and Mohit Bansal},
      year={2023},
      eprint={2311.18775},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{sun2023generative,
      title={Generative Multimodal Models are In-Context Learners}, 
      author={Quan Sun and Yufeng Cui and Xiaosong Zhang and Fan Zhang and Qiying Yu and Zhengxiong Luo and Yueze Wang and Yongming Rao and Jingjing Liu and Tiejun Huang and Xinlong Wang},
      year={2023},
      eprint={2312.13286},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{lin2023sphinx,
      title={SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models}, 
      author={Ziyi Lin and Chris Liu and Renrui Zhang and Peng Gao and Longtian Qiu and Han Xiao and Han Qiu and Chen Lin and Wenqi Shao and Keqin Chen and Jiaming Han and Siyuan Huang and Yichi Zhang and Xuming He and Hongsheng Li and Yu Qiao},
      year={2023},
      eprint={2311.07575},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{dai2023instructblip,
      title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning}, 
      author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
      year={2023},
      eprint={2305.06500},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{chen2023shikra,
      title={Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic}, 
      author={Keqin Chen and Zhao Zhang and Weili Zeng and Richong Zhang and Feng Zhu and Rui Zhao},
      year={2023},
      eprint={2306.15195},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{cai2023making,
      title={Making Large Multimodal Models Understand Arbitrary Visual Prompts}, 
      author={Mu Cai and Haotian Liu and Siva Karthik Mustikovela and Gregory P. Meyer and Yuning Chai and Dennis Park and Yong Jae Lee},
      year={2023},
      eprint={2312.00784},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@misc{bai2023qwenvl,
      title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond}, 
      author={Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou},
      year={2023},
      eprint={2308.12966},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

 @inproceedings{qi2023general,
   author={Qi, Haozhi and Yi, Brent and Ma, Yi and Suresh, Sudharshan and Lambeta, Mike and Calandra, Roberto and Malik, Jitendra},
   title={{General In-Hand Object Rotation with Vision and Touch}},
   booktitle={Conference on Robot Learning (CoRL)},
   year={2023}
  }

@misc{su2023pandagpt,
      title={PandaGPT: One Model To Instruction-Follow Them All}, 
      author={Yixuan Su and Tian Lan and Huayang Li and Jialu Xu and Yan Wang and Deng Cai},
      year={2023},
      eprint={2305.16355},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@software{ilharco_gabriel_2021_5143773,
  author       = {Ilharco, Gabriel and
                  Wortsman, Mitchell and
                  Wightman, Ross and
                  Gordon, Cade and
                  Carlini, Nicholas and
                  Taori, Rohan and
                  Dave, Achal and
                  Shankar, Vaishaal and
                  Namkoong, Hongseok and
                  Miller, John and
                  Hajishirzi, Hannaneh and
                  Farhadi, Ali and
                  Schmidt, Ludwig},
  title        = {OpenCLIP},
  month        = jul,
  year         = 2021,
  note         = {If you use this software, please cite it as below.},
  publisher    = {Zenodo},
  version      = {0.1},
  doi          = {10.5281/zenodo.5143773},
  url          = {https://doi.org/10.5281/zenodo.5143773}
}

@inproceedings{
hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@inproceedings{Dosovitskiy2020,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle=ICLR,
  year={2020}
}

@article{bertelson2004psychology,
  title={The psychology of multimodal perception},
  author={Bertelson, Paul and De Gelder, Beatrice},
  journal={Crossmodal space and crossmodal attention},
  pages={141--177},
  year={2004}
}

@article{turk2014multimodal,
  title={Multimodal interaction: A review},
  author={Turk, Matthew},
  journal={Pattern recognition letters},
  volume={36},
  pages={189--195},
  year={2014},
  publisher={Elsevier}
}

@article{gucclu2014unsupervised,
  title={Unsupervised feature learning improves prediction of human brain activity in response to natural images},
  author={G{\"u}{\c{c}}l{\"u}, Umut and van Gerven, Marcel AJ},
  journal={PLoS computational biology},
  volume={10},
  number={8},
  pages={e1003724},
  year={2014},
  publisher={Public Library of Science San Francisco, USA}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@inproceedings{jia2021scaling,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  booktitle={International conference on machine learning},
  pages={4904--4916},
  year={2021},
  organization={PMLR}
}

@inproceedings{li2023scaling,
  title={Scaling language-image pre-training via masking},
  author={Li, Yanghao and Fan, Haoqi and Hu, Ronghang and Feichtenhofer, Christoph and He, Kaiming},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={23390--23400},
  year={2023}
}

@inproceedings{mahajan2018exploring,
  title={Exploring the limits of weakly supervised pretraining},
  author={Mahajan, Dhruv and Girshick, Ross and Ramanathan, Vignesh and He, Kaiming and Paluri, Manohar and Li, Yixuan and Bharambe, Ashwin and Van Der Maaten, Laurens},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={181--196},
  year={2018}
}

@article{jiang2023motiongpt,
    title={MotionGPT: Human Motion as a Foreign Language},
    author={Jiang, Biao and Chen, Xin and Liu, Wen and Yu, Jingyi and Yu, Gang and Chen, Tao},
    journal={arXiv preprint arXiv:2306.14795},
    year={2023}
}

@article{dahiya2009tactile,
  title={Tactile sensing—from humans to humanoids},
  author={Dahiya, Ravinder S and Metta, Giorgio and Valle, Maurizio and Sandini, Giulio},
  journal={IEEE transactions on robotics},
  volume={26},
  number={1},
  pages={1--20},
  year={2009},
  publisher={IEEE}
}

@article{klatzky2003skin,
  title={The Skin and Its Receptors 148 Pathways to Cortex and Major Cortical Areas},
  author={Klatzky, Roberta L and Lederman, Susan J},
  journal={Handbook of psychology, experimental psychology},
  volume={4},
  pages={147},
  year={2003},
  publisher={John Wiley \& Sons}
}

@misc{radford2021learning,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{calandra2018more,
  title={More than a feeling: Learning to grasp and regrasp using vision and touch},
  author={Calandra, Roberto and Owens, Andrew and Jayaraman, Dinesh and Lin, Justin and Yuan, Wenzhen and Malik, Jitendra and Adelson, Edward H and Levine, Sergey},
  journal={IEEE Robotics and Automation Letters},
  volume={3},
  number={4},
  pages={3300--3307},
  year={2018},
  publisher={IEEE}
}

@article{yuan2017gelsight,
  title={Gelsight: High-resolution robot tactile sensors for estimating geometry and force},
  author={Yuan, Wenzhen and Dong, Siyuan and Adelson, Edward H},
  journal={Sensors},
  volume={17},
  number={12},
  pages={2762},
  year={2017},
  publisher={MDPI}
}


@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@misc{selfinstruct,
  title={Self-Instruct: Aligning Language Model with Self Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@article{touvron2023llama1,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}


@InProceedings{he2021,
    author    = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll\'ar, Piotr and Girshick, Ross},
    title     = {Masked Autoencoders Are Scalable Vision Learners},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {16000-16009}
}

@article{Bachmann2022,
  title={MultiMAE: Multi-modal Multi-task Masked Autoencoders},
  author={Bachmann, Roman and Mizrahi, David and Atanov, Andrei and Zamir, Amir},
  journal={arXiv:2204.01678},
  year={2022}
}

@article{geng2022multimodal,
  title={Multimodal Masked Autoencoders Learn Transferable Representations},
  author={Geng, Xinyang and Liu, Hao and Lee, Lisa and Schuurams, Dale and Levine, Sergey and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2205.14204},
  year={2022}
}

@inproceedings{wang2022debiased,
  title={Debiased learning from naturally imbalanced pseudo-labels},
  author={Wang, Xudong and Wu, Zhirong and Lian, Long and Yu, Stella X},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14647--14657},
  year={2022}
}

@article{sohn2020fixmatch,
    title={FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence},
    author={Kihyuk Sohn and David Berthelot and Chun-Liang Li and Zizhao Zhang and Nicholas Carlini and Ekin D. Cubuk and Alex Kurakin and Han Zhang and Colin Raffel},
    journal={arXiv preprint arXiv:2001.07685},
    year={2020},
}

@inproceedings{lee2013pseudo,
  title={Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks},
  author={Lee, Dong-Hyun and others},
  booktitle={Workshop on challenges in representation learning, ICML},
  volume={3},
  number={2},
  pages={896},
  year={2013},
  organization={Atlanta}
}

@article{mclachlan1975iterative,
  title={Iterative reclassification procedure for constructing an asymptotically optimal rule of allocation in discriminant analysis},
  author={McLachlan, Geoffrey J},
  journal={Journal of the American Statistical Association},
  volume={70},
  number={350},
  pages={365--369},
  year={1975},
  publisher={Taylor \& Francis}
}

@article{rosenberg2005semi,
  title={Semi-supervised self-training of object detection models},
  author={Rosenberg, Chuck and Hebert, Martial and Schneiderman, Henry},
  year={2005},
  publisher={Carnegie Mellon University}
}

@article{berthelot2019mixmatch,
  title={Mixmatch: A holistic approach to semi-supervised learning},
  author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin A},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{instructnerf2023,
author = {Haque, Ayaan and Tancik, Matthew and Efros, Alexei and Holynski, Aleksander and Kanazawa, Angjoo},
title = {Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
year = {2023},
} 

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}


@inproceedings{cai2023vipllava,
author      = {Cai, Mu and Liu, Haotian and Mustikovela,  Siva Karthik and Meyer, Gregory P. and Chai, Yuning and Park, Dennis and Lee, Yong Jae},
title       = {Making Large Multimodal Models Understand Arbitrary Visual Prompts},
booktitle   = {arXiv:2312.00784},
year        = {2023}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{loshchilov2016sgdr,
  title={Sgdr: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle=ICLR,
  year={2017}
}

@inproceedings{Chen2020generative,
  title={Generative pretraining from pixels},
  author={Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  booktitle=ICML,
  year={2020},
}

@article{Goyal2017b,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv:1706.02677},
  year={2017}
}

@article{bruck2022cross,
  title={Cross-modal perception of identity by sound and taste in bottlenose dolphins},
  author={Bruck, Jason N and Walmsley, Sam F and Janik, Vincent M},
  journal={Science Advances},
  volume={8},
  number={20},
  pages={eabm7684},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@article{stone2015contributions,
  title={The contributions of vision and haptics to reaching and grasping},
  author={Stone, Kayla D and Gonzalez, Claudia LR},
  journal={Frontiers in psychology},
  volume={6},
  pages={1403},
  year={2015},
  publisher={Frontiers Media SA}
}

@article{bresciani2006vision,
  title={Vision and touch are automatically integrated for the perception of sequences of events},
  author={Bresciani, Jean-Pierre and Dammeier, Franziska and Ernst, Marc O},
  journal={Journal of vision},
  volume={6},
  number={5},
  pages={2--2},
  year={2006},
  publisher={The Association for Research in Vision and Ophthalmology}
}

@article{ittyerah2007memory,
  title={Memory for curvature of objects: Haptic touch vs. vision},
  author={Ittyerah, Miriam and Marks, Lawrence E},
  journal={British Journal of Psychology},
  volume={98},
  number={4},
  pages={589--610},
  year={2007},
  publisher={Wiley Online Library}
}

@article{jones2005comparison,
  title={A comparison of learning with haptic and visual modalities.},
  author={Jones, M Gail and Bokinsky, Alexandra and Tretter, Thomas and Negishi, Atsuko},
  year={2005}
}

@article{camponogara2021integration,
  title={Integration of haptics and vision in human multisensory grasping},
  author={Camponogara, Ivan and Volcic, Robert},
  journal={Cortex},
  volume={135},
  pages={173--185},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{chorley2009development,
  title={Development of a tactile sensor based on biologically inspired edge encoding},
  author={Chorley, Craig and Melhuish, Chris and Pipe, Tony and Rossiter, Jonathan},
  booktitle={2009 International Conference on Advanced Robotics},
  pages={1--6},
  year={2009},
  organization={IEEE}
}

@INPROCEEDINGS{fingervision,
  author={Yamaguchi, Akihiko and Atkeson, Christopher G.},
  booktitle={2016 IEEE-RAS 16th International Conference on Humanoid Robots (Humanoids)}, 
  title={Combining finger vision and optical tactile sensing: Reducing and handling errors while cutting vegetables}, 
  year={2016},
  volume={},
  number={},
  pages={1045-1051},
  keywords={Skin;Cameras;Force;Robot vision systems},
  doi={10.1109/HUMANOIDS.2016.7803400}}


@article{sferrazza2019design,
  title={Design, motivation and evaluation of a full-resolution optical tactile sensor},
  author={Sferrazza, Carmelo and D’Andrea, Raffaello},
  journal={Sensors},
  volume={19},
  number={4},
  pages={928},
  year={2019},
  publisher={MDPI}
}

@article{shimonomura2019tactile,
  title={Tactile image sensors employing camera: A review},
  author={Shimonomura, Kazuhiro},
  journal={Sensors},
  volume={19},
  number={18},
  pages={3933},
  year={2019},
  publisher={MDPI}
}

@misc{ajbarnett_2023, 
    author={ajbarnett}, 
    title={400 Words to Describe Texture},
    publisher={Owlcation}, 
    year={2023},
} 

@inproceedings{yuan2018active,
  title={Active clothing material perception using tactile sensing and deep learning},
  author={Yuan, Wenzhen and Mo, Yuchen and Wang, Shaoxiong and Adelson, Edward H},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={4842--4849},
  year={2018},
  organization={IEEE}
}

@article{goldberg1984active,
  title={Active touch and robot perception},
  author={Goldberg, Kenneth Y and Bajcsy, Ruzena},
  journal={Cognition and Brain Theory},
  volume={7},
  number={2},
  pages={199--214},
  year={1984}
}

@ARTICLE{pacchierotti2017,
  author={Pacchierotti, Claudio and Sinclair, Stephen and Solazzi, Massimiliano and Frisoli, Antonio and Hayward, Vincent and Prattichizzo, Domenico},
  journal={IEEE Transactions on Haptics}, 
  title={Wearable Haptic Systems for the Fingertip and the Hand: Taxonomy, Review, and Perspectives}, 
  year={2017},
  volume={10},
  number={4},
  pages={580-600},
  keywords={Haptic interfaces;Robots;Exoskeletons;Wearable computing;Vibrations;Force feedback;Taxonomy;Haptic interfaces;Wearable haptics;fingertip haptics;hand exoskeletons;wearable devices;wearable interfaces;cutaneous force feedback;tactile force feedback;taxonomy;review},
  doi={10.1109/TOH.2017.2689006}}

@inproceedings{kampouris2016multi,
  title={Multi-sensorial and explorative recognition of garments and their material properties in unconstrained environment},
  author={Kampouris, Christos and Mariolis, Ioannis and Peleka, Georgia and Skartados, Evangelos and Kargakos, Andreas and Triantafyllou, Dimitra and Malassiotis, Sotiris},
  booktitle={2016 IEEE international conference on robotics and automation (ICRA)},
  pages={1656--1663},
  year={2016},
  organization={IEEE}
}

@inproceedings{li2013sensing,
  title={Sensing and recognizing surface textures using a gelsight sensor},
  author={Li, Rui and Adelson, Edward H},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1241--1247},
  year={2013}
}

@article{ojala2002multiresolution,
  title={Multiresolution gray-scale and rotation invariant texture classification with local binary patterns},
  author={Ojala, Timo and Pietikainen, Matti and Maenpaa, Topi},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={24},
  number={7},
  pages={971--987},
  year={2002},
  publisher={IEEE}
}

@InProceedings{burnel2023lesslabels,
author="Burnel, Jean-Christophe
and Courtrai, Luc
and Lef{\`e}vre, S{\'e}bastien",
editor="Rousseau, Jean-Jacques
and Kapralos, Bill",
title="Less Labels, More Modalities: A Self-Training Framework to Reuse Pretrained Networks",
booktitle="Pattern Recognition, Computer Vision, and Image Processing. ICPR 2022 International Workshops and Challenges",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="287--302",
abstract="Remote sensing largely benefits from recent advances in deep learning. Beyond traditional color imagery, remote sensing data often features some extra bands (e.g. multi or hyperspectral imagery) or multiple sources, leading to the so-called multimodal scenario. While multimodal data can lead to better performances, it also requires to design specific deep networks, to collect specifically-annotated datasets, and to perform full retraining of the models. However, a major drawback of deep learning is the large number of annotations that is required to ensure such a training phase. Besides, for some given task and modality combination, annotated data might not be available, thus requiring a tedious labeling phase. In this paper, we show how to benefit from additional modalities without requiring additional labels. We propose a self-training framework that allows us to add a modality to a pretrained model in order to improve its performance. The main features of our framework are the generation of pseudo-labels that act as annotations on the new modality, but also the generation of a pseudo-modality corresponding to the labeled monomodal dataset. Experiments on the ISPRS Potsdam dataset, where we complement color orthophotography with a digital surface model, shows the relevance of our approach, especially for land cover classes that can take advantage of the two modalities.",
isbn="978-3-031-37731-0"
}

@inproceedings{gao2021objectfolder,
  title={ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations},
  author={Gao, Ruohan and Chang, Yen-Yu and Mall, Shivani and Fei-Fei, Li and Wu, Jiajun},
  booktitle={Conference on Robot Learning},
  year={2021}
}

@InProceedings{Gao_2022_CVPR,
    author    = {Gao, Ruohan and Si, Zilin and Chang, Yen-Yu and Clarke, Samuel and Bohg, Jeannette and Fei-Fei, Li and Yuan, Wenzhen and Wu, Jiajun},
    title     = {ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {10598-10608}
}

@article{radosavovic2023robot,
  title={Robot Learning with Sensorimotor Pre-training},
  author={Radosavovic, Ilija and Shi, Baifeng and Fu, Letian and Goldberg, Ken and Darrell, Trevor and Malik, Jitendra},
  journal={arXiv preprint arXiv:2306.10007},
  year={2023}
}

@article{schmidt2019neuronal,
  title={Neuronal correlates of label facilitated tactile perception},
  author={Schmidt, Timo Torsten and Miller, Tally McCormick and Blankenburg, Felix and Pulverm{\"u}ller, Friedemann},
  journal={Scientific Reports},
  volume={9},
  number={1},
  pages={1606},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@article{speed2021crossmodal,
  title={Crossmodal associations with olfactory, auditory, and tactile stimuli in children and adults},
  author={Speed, Laura J and Croijmans, Ilja and Dolscheid, Sarah and Majid, Asifa},
  journal={i-Perception},
  volume={12},
  number={6},
  pages={20416695211048513},
  year={2021},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{miller2018verbal,
  title={Verbal labels facilitate tactile perception},
  author={Miller, Tally McCormick and Schmidt, Timo Torsten and Blankenburg, Felix and Pulverm{\"u}ller, Friedemann},
  journal={Cognition},
  volume={171},
  pages={172--179},
  year={2018},
  publisher={Elsevier}
}

@article{wang2020long,
  title={Long-tailed recognition by routing diverse distribution-aware experts},
  author={Wang, Xudong and Lian, Long and Miao, Zhongqi and Liu, Ziwei and Yu, Stella X},
  journal={arXiv preprint arXiv:2010.01809},
  year={2020}
}

@article{yang2024binding,
  title={Binding Touch to Everything: Learning Unified Multimodal Tactile Representations},
  author={Yang, Fengyu and Feng, Chao and Chen, Ziyang and Park, Hyoungseob and Wang, Daniel and Dou, Yiming and Zeng, Ziyao and Chen, Xien and Gangopadhyay, Rit and Owens, Andrew and others},
  journal={arXiv preprint arXiv:2401.18084},
  year={2024}
}