@article{nesterov2017random,
  title={Random gradient-free minimization of convex functions},
  author={Nesterov, Yurii and Spokoiny, Vladimir},
  journal={Foundations of Computational Mathematics},
  volume={17},
  number={2},
  pages={527--566},
  year={2017}
}

@book{rechenberg1973,
  title={{Evolutionsstrategie: Optimierung technischer Systeme nach Prinzipien der biologischen Evolution}},
  author={Rechenberg, Ingo},
  year={1973},
  publisher={Stuttgart: Frommann-Holzboog}
}

@book{mcbook,
   author = {Art B. Owen},
   year = 2013,
   title = {Monte Carlo Theory, Methods and Examples}
}

@inproceedings{vicol2021unbiased,
  title={Unbiased gradient estimation in unrolled computation graphs with persistent evolution strategies},
  author={Vicol, Paul and Metz, Luke and Sohl-Dickstein, Jascha},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={10553--10563},
  year={2021}
}

@phdthesis{vicol2023bilevel,
  title={On Bilevel Optimization without Full Unrolls: Methods and Applications},
  author={Vicol, Paul Adrian},
  year={2023},
  school={University of Toronto (Canada)}
}

@article{werbos1990backpropagation,
  title={{Backpropagation through time: What it does and how to do it}},
  author={Werbos, Paul J},
  journal={Proceedings of the IEEE},
  volume={78},
  number={10},
  pages={1550--1560},
  year={1990}
}


@techreport{rumelhart1985learning,
  title={Learning Internal Representations by Error Propagation},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  year={1985},
  institution={California University San Diego, La Jolla Institute for Cognitive Science}
}

@article{williams1990efficient,
  title={An efficient gradient-based algorithm for on-line training of recurrent network trajectories},
  author={Williams, Ronald J and Peng, Jing},
  journal={Neural Computation},
  volume={2},
  number={4},
  pages={490--501},
  year={1990}
}

@article{tallec2017unbiased,
  title={Unbiased online recurrent optimization},
  author={Tallec, Corentin and Ollivier, Yann},
  journal={arXiv preprint arXiv:1702.05043},
  year={2017}
}

@article{tallec2017unbiasing,
  title={Unbiasing truncated backpropagation through time},
  author={Tallec, Corentin and Ollivier, Yann},
  journal={arXiv preprint arXiv:1705.08209},
  year={2017}
}

@inproceedings{mujika2018approximating,
  title={{Approximating real-time recurrent learning with random Kronecker factors}},
  author={Mujika, Asier and Meier, Florian and Steger, Angelika},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={6594--6603},
  year={2018}
}

@article{williams1989learning,
  title={A learning algorithm for continually running fully recurrent neural networks},
  author={Williams, Ronald J and Zipser, David},
  journal={Neural Computation},
  volume={1},
  number={2},
  pages={270--280},
  year={1989}
}

@inproceedings{
menick2021practical,
title={Practical Real Time Recurrent Learning with a Sparse Approximation},
author={Jacob Menick and Erich Elsen and Utku Evci and Simon Osindero and Karen Simonyan and Alex Graves},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=q3KSThy2GwB}
}

% %%%%%%%%%%%%%%%%%
% Chaos
% %%%%%%%%%%%%%%%%%
@article{parmas2019unified,
  title={A unified view of likelihood ratio and reparameterization gradients and an optimal importance sampling scheme},
  author={Parmas, Paavo and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:1910.06419},
  year={2019}
}

@inproceedings{parmas2018pipps,
  title={{PIPPS: Flexible model-based policy search robust to the curse of chaos}},
  author={Parmas, Paavo and Rasmussen, Carl Edward and Peters, Jan and Doya, Kenji},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={4062--4071},
  year={2018}
}

@article{metz2021gradients,
  title={Gradients are not all you need},
  author={Metz, Luke and Freeman, C Daniel and Schoenholz, Samuel S and Kachman, Tal},
  journal={arXiv preprint arXiv:2111.05803},
  year={2021}
}
% %%%%%%%%%%%%%%%%%


% %%%%%%%%%%%%%%%%%%
% Learned optimizers
% %%%%%%%%%%%%%%%%%%
@inproceedings{metz2019understanding,
  title={Understanding and correcting pathologies in the training of learned optimizers},
  author={Metz, Luke and Maheswaranathan, Niru and Nixon, Jeremy and Freeman, Daniel and Sohl-Dickstein, Jascha},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={4556--4565},
  year={2019}
}

@article{metz2020tasks,
    title={{Tasks, stability, architecture, and compute: Training more effective learned optimizers, and using them to train themselves}},
    author={Luke Metz and Niru Maheswaranathan and C. Daniel Freeman and Ben Poole and Jascha Sohl-Dickstein},
    journal={arXiv preprint arXiv:2009.11243},
    year={2020}
}

@article{metz2020using,
  title={Using a thousand optimization tasks to learn hyperparameter search strategies},
  author={Metz, Luke and Maheswaranathan, Niru and Sun, Ruoxi and Freeman, C Daniel and Poole, Ben and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:2002.11887},
  year={2020}
}

@article{metz2018meta,
  title={Meta-learning update rules for unsupervised representation learning},
  author={Metz, Luke and Maheswaranathan, Niru and Cheung, Brian and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1804.00222},
  year={2018}
}

@article{wichrowska2017learned,
  title={Learned optimizers that scale and generalize},
  author={Wichrowska, Olga and Maheswaranathan, Niru and Hoffman, Matthew W and Colmenarejo, Sergio Gomez and Denil, Misha and de Freitas, Nando and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1703.04813},
  year={2017}
}

@inproceedings{andrychowicz2016learning,
  title={Learning to learn by gradient descent by gradient descent},
  author={Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and De Freitas, Nando},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={3981--3989},
  year={2016}
}

@article{li2016learning,
  title={Learning to optimize},
  author={Li, Ke and Malik, Jitendra},
  journal={arXiv preprint arXiv:1606.01885},
  year={2016}
}

@article{li2017learning,
  title={Learning to optimize neural nets},
  author={Li, Ke and Malik, Jitendra},
  journal={arXiv preprint arXiv:1703.00441},
  year={2017}
}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Hyperparameter Optimization
% %%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{shaban2019truncated,
  title={Truncated back-propagation for bilevel optimization},
  author={Shaban, Amirreza and Cheng, Ching-An and Hatch, Nathan and Boots, Byron},
  booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages={1723--1732},
  year={2019}
}

@inproceedings{maclaurin2015gradient,
  title={Gradient-based hyperparameter optimization through reversible learning},
  author={Maclaurin, Dougal and Duvenaud, David and Adams, Ryan},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={2113--2122},
  year={2015}
}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%

% %%%%%%%%%%%%%%%%%%%%%%
% RNN
% %%%%%%%%%%%%%%%%%%%%%%
@inproceedings{merity2017regularizing,
  title={{Regularizing and optimizing LSTM language models}},
  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}
% %%%%%%%%%%%%%%%%%%%%%%

% %%%%%%%%%%%%%%%%%%%%%%
% Reinforcement Learning
% %%%%%%%%%%%%%%%%%%%%%%
@article{salimans2017evolution,
  title={Evolution strategies as a scalable alternative to reinforcement learning},
  author={Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1703.03864},
  year={2017}
}

@article{mania2018simple,
  title={Simple random search provides a competitive approach to reinforcement learning},
  author={Mania, Horia and Guy, Aurelia and Recht, Benjamin},
  journal={arXiv preprint arXiv:1803.07055},
  year={2018}
}
% %%%%%%%%%%%%%%%%%%%%%%


% %%%%%%%%%%%%%%%%%%%%%%
% Meta-Learning
% %%%%%%%%%%%%%%%%%%%%%%
@article{bertinetto2018meta,
  title={Meta-learning with differentiable closed-form solvers},
  author={Bertinetto, Luca and Henriques, Joao F and Torr, Philip HS and Vedaldi, Andrea},
  journal={arXiv preprint arXiv:1805.08136},
  year={2018}
}

@book{finn2018learning,
  title={Learning to learn with gradients},
  author={Finn, Chelsea B},
  year={2018},
  publisher={University of California, Berkeley}
}

@article{finn2018probabilistic,
  title={Probabilistic model-agnostic meta-learning},
  author={Finn, Chelsea and Xu, Kelvin and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={31},
  year={2018}
}
% %%%%%%%%%%%%%%%%%%%%%%

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Schulman Stochastic Computation Graphs
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{schulman2015gradient,
  title={Gradient estimation using stochastic computation graphs},
  author={Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
  journal={Advances in Neural Information Processing Systems},
  year={2015}
}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{marcus1993building,
  title={{Building a large annotated corpus of English: The Penn Treebank}},
  author={Marcus, Mitchell P and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
  journal={Computational Linguistics},
  volume={19},
  number={2},
  pages={313--330},
  year={1993},
  publisher={MIT Press}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Reparameterization gradient estimators
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{ruiz2016generalized,
  title={The generalized reparameterization gradient},
  author={Ruiz, Francisco R and AUEB, Titsias RC and Blei, David and others},
  journal={Advances in Neural Information Processing Systems},
  year={2016}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@inproceedings{maheswaranathan2019guided,
  title={Guided evolutionary strategies: Augmenting random search with surrogate gradients},
  author={Maheswaranathan, Niru and Metz, Luke and Tucker, George and Choi, Dami and Sohl-Dickstein, Jascha},
  booktitle={International Conference on Machine Learning},
  pages={4264--4273},
  year={2019},
  organization={PMLR}
}

@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@article{baydin2017online,
  title={Online learning rate adaptation with hypergradient descent},
  author={Baydin, Atilim Gunes and Cornish, Robert and Rubio, David Martinez and Schmidt, Mark and Wood, Frank},
  journal={arXiv preprint arXiv:1703.04782},
  year={2017}
}

@article{chandra2022gradient,
  title={Gradient descent: The ultimate optimizer},
  author={Chandra, Kartik and Xie, Audrey and Ragan-Kelley, Jonathan and Meijer, Erik},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={8214--8225},
  year={2022}
}

@article{wu2018understanding,
  title={Understanding short-horizon bias in stochastic meta-optimization},
  author={Wu, Yuhuai and Ren, Mengye and Liao, Renjie and Grosse, Roger},
  journal={arXiv preprint arXiv:1803.02021},
  year={2018}
}

@inproceedings{benzing2019optimal,
  title={{Optimal Kronecker-sum approximation of real time recurrent learning}},
  author={Benzing, Frederik and Gauy, Marcelo Matheus and Mujika, Asier and Martinsson, Anders and Steger, Angelika},
  booktitle={International Conference on Machine Learning},
  pages={604--613},
  year={2019},
  organization={PMLR}
}

@inproceedings{silver2021learning,
  title={Learning by Directional Gradient Descent},
  author={Silver, David and Goyal, Anirudh and Danihelka, Ivo and Hessel, Matteo and van Hasselt, Hado},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{ni2017sensitivity,
  title={{Sensitivity analysis on chaotic dynamical systems by Non-Intrusive Least Squares Shadowing (NILSS)}},
  author={Ni, Angxiu and Wang, Qiqi},
  journal={Journal of Computational Physics},
  volume={347},
  pages={56--77},
  year={2017},
  publisher={Elsevier}
}

@article{kochkov2021machine,
  title={Machine learning--accelerated computational fluid dynamics},
  author={Kochkov, Dmitrii and Smith, Jamie A and Alieva, Ayya and Wang, Qing and Brenner, Michael P and Hoyer, Stephan},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={21},
  pages={e2101784118},
  year={2021},
  publisher={National Acad Sciences}
}

@article{lea2000sensitivity,
  title={Sensitivity analysis of the climate of a chaotic system},
  author={Lea, Daniel J and Allen, Myles R and Haine, Thomas WN},
  journal={Tellus A: Dynamic Meteorology and Oceanography},
  volume={52},
  number={5},
  pages={523--532},
  year={2000},
  publisher={Taylor \& Francis}
}

@article{kohl2002adjoint,
  title={An adjoint method for the assimilation of statistical characteristics into eddy-resolving ocean models},
  author={K{\"o}hl, Armin and Willebrand, J{\"u}rgen},
  journal={Tellus A: Dynamic Meteorology and Oceanography},
  volume={54},
  number={4},
  pages={406--425},
  year={2002},
  publisher={Taylor \& Francis}
}

@article{bischof1996sensitivity,
  title={{Sensitivity analysis of the MM5 weather model using automatic differentiation}},
  author={Bischof, Christian H and Pusch, Gordon D and Knoesel, Ralf},
  journal={Computers in Physics},
  volume={10},
  number={6},
  pages={605--612},
  year={1996},
  publisher={American Institute of Physics}
}

@article{mcgreivy2021optimized,
  title={Optimized finite-build stellarator coils using automatic differentiation},
  author={McGreivy, Nick and Hudson, Stuart R and Zhu, Caoxiang},
  journal={Nuclear Fusion},
  volume={61},
  number={2},
  pages={026020},
  year={2021},
  publisher={IOP Publishing}
}

@article{kingma2013auto,
  title={{Auto-encoding variational Bayes}},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@book{schwefel1977evolutionsstrategien,
  title={{Evolutionsstrategien f{\"u}r die numerische Optimierung}},
  author={Schwefel, Hans-Paul and Schwefel, Hans-Paul},
  year={1977},
  publisher={Springer}
}

@article{wierstra2014natural,
  title={Natural evolution strategies},
  author={Wierstra, Daan and Schaul, Tom and Glasmachers, Tobias and Sun, Yi and Peters, Jan and Schmidhuber, J{\"u}rgen},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={949--980},
  year={2014},
  publisher={JMLR. org}
}

@article{staines2012variational,
  title={Variational optimization},
  author={Staines, Joe and Barber, David},
  journal={arXiv preprint arXiv:1212.4507},
  year={2012}
}

@article{anonymous2023,
    title={Noise Reuse in Online Evolution Strategies},
    author={Anonymous Authors},
    journal={Submitted to ICML},
    year={2023}
}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Hyperparameter optimization
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{bergstra2011algorithms,
  title={Algorithms for hyper-parameter optimization},
  author={Bergstra, James S and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={2546--2554},
  year={2011}
}

@inproceedings{snoek2012practical,
  title={{Practical Bayesian optimization of machine learning algorithms}},
  author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={2951--2959},
  year={2012}
}

@article{li2017hyperband,
  title={{Hyperband: A novel bandit-based approach to hyperparameter optimization}},
  author={Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={6765--6816},
  year={2017}
}

@inproceedings{snoek2015scalable,
  title={{Scalable Bayesian optimization using deep neural networks}},
  author={Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Mostofa and Prabhat, M and Adams, Ryan},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={2171--2180},
  year={2015}
}

@inproceedings{jamieson2016non,
  title={Non-stochastic best arm identification and hyperparameter optimization},
  author={Jamieson, Kevin and Talwalkar, Ameet},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2016}
}

@article{swersky2014freeze,
  title={{Freeze-thaw Bayesian optimization}},
  author={Swersky, Kevin and Snoek, Jasper and Adams, Ryan Prescott},
  journal={arXiv preprint arXiv:1406.3896},
  year={2014}
}

@inproceedings{pedregosa2016hyperparameter,
  title={Hyperparameter optimization with approximate gradient},
  author={Pedregosa, Fabian},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={737--746},
  year={2016}
}

@article{franceschi2017forward,
  title={Forward and reverse gradient-based hyperparameter optimization},
  author={Franceschi, Luca and Donini, Michele and Frasconi, Paolo and Pontil, Massimiliano},
  journal={arXiv preprint arXiv:1703.01785},
  year={2017}
}

@inproceedings{domke2012generic,
  title={Generic methods for optimization-based modeling},
  author={Domke, Justin},
  booktitle={Proceedings of Machine Learning Research},
  pages={318--326},
  year={2012}
}

@article{feng2017gradient,
  title={Gradient-based Regularization Parameter Selection for Problems with Non-smooth Penalty Functions},
  author={Feng, Jean and Simon, Noah},
  journal={Journal of Computational and Graphical Statistics},
  year={2017},
  publisher={Taylor \& Francis}
}

@inproceedings{luketina2016scalable,
  title={Scalable gradient-based tuning of continuous regularization hyperparameters},
  author={Luketina, Jelena and Berglund, Mathias and Greff, Klaus and Raiko, Tapani},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={2952--2960},
  year={2016}
}

@article{jaderberg2017population,
  title={Population-based training of neural networks},
  author={Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and others},
  journal={arXiv preprint arXiv:1711.09846},
  year={2017}
}

@inproceedings{vicol2022implicit,
  title={{On Implicit Bias in Overparameterized Bilevel Optimization}},
  author={Vicol, Paul and Lorraine, Jonathan and Pedregosa, Fabian and Duvenaud, David and Grosse, Roger},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2022}
}

@inproceedings{foo2008efficient,
  title={Efficient multiple hyperparameter learning for log-linear models},
  author={Foo, Chuan-Sheng and Do, Chuong B and Ng, Andrew Y},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={377--384},
  year={2008}
}

@inproceedings{larsen1996design,
  title={{Design and regularization of neural networks: The optimal use of a validation set}},
  author={Larsen, Jan and Hansen, Lars Kai and Svarer, Claus and Ohlsson, M},
  booktitle={IEEE Signal Processing Society Workshop},
  pages={62--71},
  year={1996}
}

@article{bengio2000gradient,
  title={Gradient-based optimization of hyperparameters},
  author={Bengio, Yoshua},
  journal={Neural Computation},
  volume={12},
  number={8},
  pages={1889--1900},
  year={2000}
}

@inproceedings{lorraine2020optimizing,
  title={Optimizing millions of hyperparameters by implicit differentiation},
  author={Lorraine, Jonathan and Vicol, Paul and Duvenaud, David},
  booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages={1540--1552},
  year={2020}
}

@article{blondel2021efficient,
  title={Efficient and Modular Implicit Differentiation},
  author={Blondel, Mathieu and Berthet, Quentin and Cuturi, Marco and Frostig, Roy and Hoyer, Stephan and Llinares-L{\'o}pez, Felipe and Pedregosa, Fabian and Vert, Jean-Philippe},
  journal={arXiv preprint arXiv:2105.15183},
  year={2021}
}

@article{micaelli2020non,
  title={Non-greedy gradient-based hyperparameter optimization over long horizons},
  author={Micaelli, Paul and Storkey, Amos},
  journal={arXiv preprint arXiv:2007.07869},
  year={2020}
}

@article{lorraine2018stochastic,
  title={Stochastic Hyperparameter Optimization through Hypernetworks},
  author={Lorraine, Jonathan and Duvenaud, David},
  journal={arXiv preprint arXiv:1802.09419},
  year={2018}
}

@inproceedings{mackay2019self,
  title={{Self-Tuning Networks: Bilevel optimization of hyperparameters using structured best-response functions}},
  author={MacKay, Matthew and Vicol, Paul and Lorraine, Jon and Duvenaud, David and Grosse, Roger},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019}
}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DROPOUT
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{srivastava2014dropout,
  title={{Dropout: A simple way to prevent neural networks from overfitting}},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014}
}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% %%%%%%%%%%%%%%%%%%%
% UCI Yacht Data
% %%%%%%%%%%%%%%%%%%%
@misc{asuncion2007uci,
  title={{UCI Machine Learning Repository}},
  author={Asuncion, Arthur and Newman, David},
  year={2007},
  publisher={Irvine, CA, USA}
}

 @inproceedings{li2023noise,
    title={Noise Reuse in Online Evolution Strategies},
    author={Oscar Li and James Harrison and Jascha Sohl-Dickstein and Luke Metz},
    booktitle={International Conference on Machine Learning},
    note={Under review.},
    year={2023}
}
