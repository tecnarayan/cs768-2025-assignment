\begin{thebibliography}{10}

\bibitem{nanda2023progress}
Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt.
\newblock Progress measures for grokking via mechanistic interpretability.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{olah2020zoom}
Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and
  Shan Carter.
\newblock Zoom in: An introduction to circuits.
\newblock {\em Distill}, 2020.
\newblock https://distill.pub/2020/circuits/zoom-in.

\bibitem{olsson2022context}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma,
  Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly,
  Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott
  Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario
  Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock In-context learning and induction heads.
\newblock {\em Transformer Circuits Thread}, 2022.
\newblock
  https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.

\bibitem{wang2023interpretability}
Kevin~Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob
  Steinhardt.
\newblock Interpretability in the wild: a circuit for indirect object
  identification in {GPT}-2 small.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{michaud2023quantization}
Eric~J Michaud, Ziming Liu, Uzay Girit, and Max Tegmark.
\newblock The quantization model of neural scaling.
\newblock {\em arXiv preprint arXiv:2303.13506}, 2023.

\bibitem{akyurek2022learning}
Ekin Aky{\"u}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
\newblock What learning algorithm is in-context learning? investigations with
  linear models.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem{liu2022towards}
Ziming Liu, Ouail Kitouni, Niklas~S Nolte, Eric Michaud, Max Tegmark, and Mike
  Williams.
\newblock Towards understanding grokking: An effective theory of representation
  learning.
\newblock {\em Advances in Neural Information Processing Systems},
  35:34651--34663, 2022.

\bibitem{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{elhage2021mathematical}
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben
  Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn
  Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson
  Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark,
  Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock A mathematical framework for transformer circuits.
\newblock {\em Transformer Circuits Thread}, 2021.
\newblock https://transformer-circuits.pub/2021/framework/index.html.

\bibitem{elhage2022superposition}
Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan,
  Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen,
  Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg,
  and Christopher Olah.
\newblock Toy models of superposition.
\newblock {\em Transformer Circuits Thread}, 2022.

\bibitem{chughtai2023toy}
Bilal Chughtai, Lawrence Chan, and Neel Nanda.
\newblock A toy model of universality: Reverse engineering how networks learn
  group operations.
\newblock In {\em The Fortieth International Conference on Machine Learning},
  2023.

\bibitem{liu2023omnigrok}
Ziming Liu, Eric~J Michaud, and Max Tegmark.
\newblock Omnigrok: Grokking beyond algorithmic data.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{conmy2023towards}
Arthur Conmy, Augustine~N Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and
  Adri{\`a} Garriga-Alonso.
\newblock Towards automated circuit discovery for mechanistic interpretability.
\newblock {\em arXiv preprint arXiv:2304.14997}, 2023.

\bibitem{gurnee2023finding}
Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and
  Dimitris Bertsimas.
\newblock Finding neurons in a haystack: Case studies with sparse probing.
\newblock {\em arXiv preprint arXiv:2305.01610}, 2023.

\bibitem{hoshen2016visual}
Yedid Hoshen and Shmuel Peleg.
\newblock Visual learning of arithmetic operation.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2016.

\bibitem{kim2020integration}
Samuel Kim, Peter~Y Lu, Srijon Mukherjee, Michael Gilbert, Li~Jing, Vladimir
  {\v{C}}eperi{\'c}, and Marin Solja{\v{c}}i{\'c}.
\newblock Integration of neural network-based symbolic regression in deep
  learning for scientific discovery.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  32(9):4166--4177, 2020.

\bibitem{power2022grokking}
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.
\newblock Grokking: Generalization beyond overfitting on small algorithmic
  datasets.
\newblock {\em arXiv preprint arXiv:2201.02177}, 2022.

\bibitem{barak2022hidden}
Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril
  Zhang.
\newblock Hidden progress in deep learning: S{G}{D} learns parities near the
  computational limit.
\newblock {\em Advances in Neural Information Processing Systems},
  35:21750--21764, 2022.

\bibitem{he2021machine}
Yang-Hui He.
\newblock Machine-learning mathematical structures.
\newblock {\em International Journal of Data Science in the Mathematical
  Sciences}, 1(01):23--47, 2023.

\bibitem{gukov2021learning}
Sergei Gukov, James Halverson, Fabian Ruehle, and Piotr Su{\l}kowski.
\newblock Learning to unknot.
\newblock {\em Machine Learning: Science and Technology}, 2(2):025035, 2021.

\bibitem{davies2021advancing}
Alex Davies, Petar Veli{\v{c}}kovi{\'c}, Lars Buesing, Sam Blackwell, Daniel
  Zheng, Nenad Toma{\v{s}}ev, Richard Tanburn, Peter Battaglia, Charles
  Blundell, Andr{\'a}s Juh{\'a}sz, et~al.
\newblock Advancing mathematics by guiding human intuition with ai.
\newblock {\em Nature}, 600(7887):70--74, 2021.

\bibitem{hartmann2006phase}
Alexander~K Hartmann and Martin Weigt.
\newblock {\em Phase transitions in combinatorial optimization problems:
  basics, algorithms and statistical mechanics}.
\newblock John Wiley \& Sons, 2006.

\bibitem{saitta2011phase}
Lorenza Saitta, Attilio Giordana, and Antoine Cornuejols.
\newblock {\em Phase transitions in machine learning}.
\newblock Cambridge University Press, 2011.

\bibitem{lubana2022mechanistic}
Ekdeep~Singh Lubana, Eric~J Bigelow, Robert~P Dick, David Krueger, and Hidenori
  Tanaka.
\newblock Mechanistic mode connectivity.
\newblock In {\em International Conference on Machine Learning}, pages
  22965--23004. PMLR, 2023.

\bibitem{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
  Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed~H.
  Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
  Fedus.
\newblock Emergent abilities of large language models.
\newblock {\em Transactions on Machine Learning Research}, 2022.
\newblock Survey Certification.

\bibitem{schaeffer2023emergent}
Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo.
\newblock Are emergent abilities of large language models a mirage?
\newblock {\em arXiv preprint arXiv:2304.15004}, 2023.

\bibitem{hassid-etal-2022-much}
Michael Hassid, Hao Peng, Daniel Rotem, Jungo Kasai, Ivan Montero, Noah~A.
  Smith, and Roy Schwartz.
\newblock How much does attention actually attend? questioning the importance
  of attention in pretrained transformers.
\newblock In {\em Findings of the Association for Computational Linguistics:
  EMNLP 2022}, pages 1403--1416, Abu Dhabi, United Arab Emirates, December
  2022. Association for Computational Linguistics.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em International Conference on Learning Representations}, 2019.

\end{thebibliography}
