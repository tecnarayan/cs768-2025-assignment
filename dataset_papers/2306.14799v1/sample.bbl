\begin{thebibliography}{10}

\bibitem{achdou2019mean}
Yves Achdou and Jean-Michel Lasry.
\newblock Mean field games for modeling crowd motion.
\newblock {\em Contributions to partial differential equations and
  applications}, pages 17--42, 2019.

\bibitem{anahtarci2023q}
Berkay Anahtarci, Can~Deha Kariksiz, and Naci Saldi.
\newblock Q-learning in regularized mean-field games.
\newblock {\em Dynamic Games and Applications}, 13(1):89--117, 2023.

\bibitem{angiuli2022unified}
Andrea Angiuli, Jean-Pierre Fouque, and Mathieu Lauri{\`e}re.
\newblock Unified reinforcement {Q}-learning for mean field game and control
  problems.
\newblock {\em Mathematics of Control, Signals, and Systems}, 34(2):217--271,
  2022.

\bibitem{cardaliaguet2015mean}
Pierre Cardaliaguet and P~Jameson Graber.
\newblock Mean field games systems of first order.
\newblock {\em ESAIM: Control, Optimisation and Calculus of Variations},
  21(3):690--722, 2015.

\bibitem{carmona2020applications}
Rene Carmona.
\newblock Applications of mean field games in financial engineering and
  economic theory.
\newblock {\em arXiv preprint arXiv:2012.05237}, 2020.

\bibitem{carmona2021deep}
Ren{\'e} Carmona and Mathieu Lauri{\`e}re.
\newblock Deep learning for mean field games and mean field control with
  applications to finance.
\newblock {\em arXiv preprint arXiv:2107.04568}, 2021.

\bibitem{carmona2019model}
Ren{\'e} Carmona, Mathieu Lauri{\`e}re, and Zongjun Tan.
\newblock Model-free mean-field reinforcement learning: mean-field {MDP} and
  mean-field {Q}-learning.
\newblock {\em arXiv preprint arXiv:1910.12802}, 2019.

\bibitem{chen2022individual}
Yang Chen, Libo Zhang, Jiamou Liu, and Shuyue Hu.
\newblock Individual-level inverse reinforcement learning for mean field games.
\newblock In {\em Proceedings of the 21st International Conference on
  Autonomous Agents and Multiagent Systems}, pages 253--262, 2022.

\bibitem{chen2021adversarial}
Yang Chen, Libo Zhang, Jiamou Liu, and Michael Witbrock.
\newblock Adversarial inverse reinforcement learning for mean field games.
\newblock {\em arXiv preprint arXiv:2104.14654}, 2021.

\bibitem{cui2021approximately}
Kai Cui and Heinz Koeppl.
\newblock Approximately solving mean field games via entropy-regularized deep
  reinforcement learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1909--1917. PMLR, 2021.

\bibitem{dogbe2010modeling}
Christian Dogb{\'e}.
\newblock Modeling crowd dynamics by the mean-field limit approach.
\newblock {\em Mathematical and Computer Modelling}, 52(9-10):1506--1520, 2010.

\bibitem{elhenawy2015intersection}
Mohammed Elhenawy, Ahmed~A Elbery, Abdallah~A Hassan, and Hesham~A Rakha.
\newblock An intersection game-theory-based traffic control algorithm in a
  connected vehicle environment.
\newblock In {\em 2015 IEEE 18th international conference on intelligent
  transportation systems}, pages 343--347. IEEE, 2015.

\bibitem{garg2021iq}
Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, and Stefano
  Ermon.
\newblock Iq-learn: Inverse soft-q learning for imitation.
\newblock {\em Advances in Neural Information Processing Systems},
  34:4028--4039, 2021.

\bibitem{geist2022concave}
Matthieu Geist, Julien P{\'e}rolat, Mathieu Lauri{\`e}re, Romuald Elie, Sarah
  Perrin, Oliver Bachem, R{\'e}mi Munos, and Olivier Pietquin.
\newblock Concave utility reinforcement learning: The mean-field game
  viewpoint.
\newblock In {\em Proceedings of the 21st International Conference on
  Autonomous Agents and Multiagent Systems}, pages 489--497, 2022.

\bibitem{ghasemipour2020divergence}
Seyed Kamyar~Seyed Ghasemipour, Richard Zemel, and Shixiang Gu.
\newblock A divergence minimization perspective on imitation learning methods.
\newblock In {\em Conference on Robot Learning}, pages 1259--1277. PMLR, 2020.

\bibitem{gomes2010discrete}
Diogo~A Gomes, Joana Mohr, and Rafael~Rigao Souza.
\newblock Discrete time, finite state space mean field games.
\newblock {\em Journal de math{\'e}matiques pures et appliqu{\'e}es},
  93(3):308--328, 2010.

\bibitem{gu2021mean}
Haotian Gu, Xin Guo, Xiaoli Wei, and Renyuan Xu.
\newblock Mean-field controls with {Q}-learning for cooperative {MARL}:
  convergence and complexity analysis.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 3(4):1168--1196,
  2021.

\bibitem{ho2016generative}
Jonathan Ho and Stefano Ermon.
\newblock Generative adversarial imitation learning.
\newblock In {\em Advances in neural information processing systems (NeurIPS)},
  2016.

\bibitem{ke2021imitation}
Liyiming Ke, Sanjiban Choudhury, Matt Barnes, Wen Sun, Gilwoo Lee, and
  Siddhartha Srinivasa.
\newblock Imitation learning as f-divergence minimization.
\newblock In {\em Algorithmic Foundations of Robotics XIV: Proceedings of the
  Fourteenth Workshop on the Algorithmic Foundations of Robotics 14}, pages
  313--329. Springer, 2021.

\bibitem{osa2018algorithmic}
Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J~Andrew Bagnell, Pieter Abbeel,
  Jan Peters, et~al.
\newblock An algorithmic perspective on imitation learning.
\newblock {\em Foundations and Trends{\textregistered} in Robotics},
  7(1-2):1--179, 2018.

\bibitem{pasztor2021efficient}
Barna Pasztor, Ilija Bogunovic, and Andreas Krause.
\newblock Efficient model-based multi-agent mean-field reinforcement learning.
\newblock {\em arXiv preprint arXiv:2107.04050}, 2021.

\bibitem{perolat2022scaling}
Julien P{\'e}rolat, Sarah Perrin, Romuald Elie, Mathieu Lauri{\`e}re, Georgios
  Piliouras, Matthieu Geist, Karl Tuyls, and Olivier Pietquin.
\newblock Scaling mean field games by online mirror descent.
\newblock In {\em Proceedings of the 21st International Conference on
  Autonomous Agents and Multiagent Systems}, pages 1028--1037, 2022.

\bibitem{perrin2020fictitious}
Sarah Perrin, Julien P{\'e}rolat, Mathieu Lauri{\`e}re, Matthieu Geist, Romuald
  Elie, and Olivier Pietquin.
\newblock Fictitious play for mean field games: Continuous time analysis and
  applications.
\newblock {\em Advances in Neural Information Processing Systems},
  33:13199--13213, 2020.

\bibitem{pomerleau1991efficient}
Dean~A Pomerleau.
\newblock Efficient training of artificial neural networks for autonomous
  navigation.
\newblock {\em Neural computation}, 3(1):88--97, 1991.

\bibitem{ratliff2006maximum}
Nathan~D Ratliff, J~Andrew Bagnell, and Martin~A Zinkevich.
\newblock Maximum margin planning.
\newblock In {\em Proceedings of the 23rd international conference on Machine
  learning}, pages 729--736, 2006.

\bibitem{ross2010efficient}
St{\'e}phane Ross and Drew Bagnell.
\newblock Efficient reductions for imitation learning.
\newblock In {\em Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 661--668. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem{ross2011reduction}
St{\'e}phane Ross, Geoffrey Gordon, and Drew Bagnell.
\newblock A reduction of imitation learning and structured prediction to
  no-regret online learning.
\newblock In {\em Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pages 627--635. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem{subramanian2019reinforcement}
Jayakumar Subramanian and Aditya Mahajan.
\newblock Reinforcement learning in stationary mean-field games.
\newblock In {\em Proceedings of the 18th International Conference on
  Autonomous Agents and MultiAgent Systems}, pages 251--259, 2019.

\bibitem{swamy2021}
Gokul Swamy, Sanjiban Choudhury, J~Andrew Bagnell, and Steven Wu.
\newblock Of moments and matching: A game-theoretic framework for closing the
  imitation gap.
\newblock In {\em International Conference on Machine Learning}, pages
  10022--10032. PMLR, 2021.

\bibitem{tanaka2018linearly}
Takashi Tanaka, Ehsan Nekouei, and Karl~Henrik Johansson.
\newblock Linearly solvable mean-field road traffic games.
\newblock In {\em 2018 56th Annual Allerton Conference on Communication,
  Control, and Computing (Allerton)}, pages 283--289. IEEE, 2018.

\bibitem{wang2020research}
Huisheng Wang, Yuejiang Li, and H~Vicky Zhao.
\newblock Research on intelligent traffic control methods at intersections
  based on game theory.
\newblock {\em arXiv preprint arXiv:2009.05216}, 2020.

\bibitem{xu2020error}
Tian Xu, Ziniu Li, and Yang Yu.
\newblock Error bounds of imitating policies and environments.
\newblock {\em Advances in Neural Information Processing Systems},
  33:15737--15749, 2020.

\bibitem{yang2017learning}
Jiachen Yang, Xiaojing Ye, Rakshit Trivedi, Huan Xu, and Hongyuan Zha.
\newblock Learning deep mean field games for modeling large population
  behavior.
\newblock {\em arXiv preprint arXiv:1711.03156}, 2017.

\bibitem{yardim2022policy}
Batuhan Yardim, Semih Cayci, Matthieu Geist, and Niao He.
\newblock Policy mirror ascent for efficient and independent learning in mean
  field games.
\newblock {\em arXiv preprint arXiv:2212.14449}, 2022.

\bibitem{zhao2023imitation}
Zhiyu Zhao, Renyuan Xu, Haifeng Zhang, Jun Wang, Mingyuan Zhang, and Yaodong
  Yang.
\newblock Imitation learning for mean field games with correlated equilibria.
\newblock {\em openreview preprint: id=VUdMeSbExWg}, 2023.

\bibitem{ziebart2008maximum}
Brian~D Ziebart, Andrew Maas, J~Andrew Bagnell, and Anind~K Dey.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In {\em Proceedings of the 23rd national conference on Artificial
  intelligence-Volume 3}, pages 1433--1438, 2008.

\end{thebibliography}
