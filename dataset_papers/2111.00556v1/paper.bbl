\begin{thebibliography}{10}

\bibitem{zhu2019deep}
Ligeng Zhu, Zhijian Liu, and Song Han.
\newblock Deep leakage from gradients.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  14774--14784, 2019.

\bibitem{mcmahan2017federated}
Brendan McMahan and Daniel Ramage.
\newblock Federated learning: Collaborative machine learning without
  centralized training data.
\newblock {\em Google Research Blog}, 3, 2017.

\bibitem{melis2019exploiting}
Luca Melis, Congzheng Song, Emiliano De~Cristofaro, and Vitaly Shmatikov.
\newblock Exploiting unintended feature leakage in collaborative learning.
\newblock In {\em 2019 IEEE Symposium on Security and Privacy (SP)}, pages
  691--706. IEEE, 2019.

\bibitem{shokri2017membership}
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.
\newblock Membership inference attacks against machine learning models.
\newblock In {\em 2017 IEEE Symposium on Security and Privacy (SP)}, pages
  3--18. IEEE, 2017.

\bibitem{geiping2020inverting}
Jonas Geiping, Hartmut Bauermeister, Hannah Dr{\"o}ge, and Michael Moeller.
\newblock Inverting gradients--how easy is it to break privacy in federated
  learning?
\newblock {\em arXiv preprint arXiv:2003.14053}, 2020.

\bibitem{wei2020framework}
Wenqi Wei, Ling Liu, Margaret Loper, Ka-Ho Chow, Mehmet~Emre Gursoy, Stacey
  Truex, and Yanzhao Wu.
\newblock A framework for evaluating gradient leakage attacks in federated
  learning.
\newblock {\em arXiv preprint arXiv:2004.10397}, 2020.

\bibitem{yin2021see}
Hongxu Yin, Arun Mallya, Arash Vahdat, Jose~M Alvarez, Jan Kautz, and Pavlo
  Molchanov.
\newblock See through gradients: Image batch recovery via gradinversion.
\newblock {\em arXiv preprint arXiv:2104.07586}, 2021.

\bibitem{dang2021method}
Trung Dang, Om~Thakkar, Swaroop Ramaswamy, Rajiv Mathews, Peter Chin, and
  Fran{\c{c}}oise Beaufays.
\newblock A method to reveal speaker identity in distributed asr training, and
  how to counter it.
\newblock {\em arXiv preprint arXiv:2104.07815}, 2021.

\bibitem{zhao2020idlg}
Bo~Zhao, Konda~Reddy Mopuri, and Hakan Bilen.
\newblock idlg: Improved deep leakage from gradients.
\newblock {\em arXiv preprint arXiv:2001.02610}, 2020.

\bibitem{wainakh2021label}
Aidmar Wainakh, Till M{\"u}{\ss}ig, Tim Grube, and Max M{\"u}hlh{\"a}user.
\newblock Label leakage from gradients in distributed machine learning.
\newblock In {\em 2021 IEEE 18th Annual Consumer Communications \& Networking
  Conference (CCNC)}, pages 1--4. IEEE, 2021.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{tan2019efficientnet}
Mingxing Tan and Quoc Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In {\em International Conference on Machine Learning}, pages
  6105--6114. PMLR, 2019.

\bibitem{gupta2018distributed}
Otkrist Gupta and Ramesh Raskar.
\newblock Distributed learning of deep neural network over multiple agents.
\newblock {\em Journal of Network and Computer Applications}, 116:1--8, 2018.

\bibitem{vepakomma2020nopeek}
Praneeth Vepakomma, Abhishek Singh, Otkrist Gupta, and Ramesh Raskar.
\newblock Nopeek: Information leakage reduction to share activations in
  distributed deep learning.
\newblock In {\em 2020 International Conference on Data Mining Workshops
  (ICDMW)}, pages 933--942. IEEE, 2020.

\bibitem{chan2015listen}
W.~{Chan}, N.~{Jaitly}, Q.~{Le}, and O.~{Vinyals}.
\newblock Listen, attend and spell: A neural network for large vocabulary
  conversational speech recognition.
\newblock In {\em 2016 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 4960--4964, 2016.

\bibitem{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In {\em Artificial intelligence and statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem{sim2019investigation}
Khe~Chai Sim, Petr Zadrazil, and Fran{\c{c}}oise Beaufays.
\newblock An investigation into on-device personalization of end-to-end
  automatic speech recognition models.
\newblock {\em arXiv preprint arXiv:1909.06678}, 2019.

\bibitem{maas2013rectifier}
Andrew~L Maas, Awni~Y Hannun, and Andrew~Y Ng.
\newblock Rectifier nonlinearities improve neural network acoustic models.
\newblock In {\em Proc. icml}, volume~30, page~3. Citeseer, 2013.

\bibitem{clevert2015fast}
Djork-Arn{\'e} Clevert, Thomas Unterthiner, and Sepp Hochreiter.
\newblock Fast and accurate deep network learning by exponential linear units
  (elus).
\newblock {\em arXiv preprint arXiv:1511.07289}, 2015.

\bibitem{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock {\em arXiv preprint arXiv:1409.0473}, 2014.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{ramachandran2017searching}
Prajit Ramachandran, Barret Zoph, and Quoc~V Le.
\newblock Searching for activation functions.
\newblock {\em arXiv preprint arXiv:1710.05941}, 2017.

\bibitem{elfwing2018sigmoid}
Stefan Elfwing, Eiji Uchibe, and Kenji Doya.
\newblock Sigmoid-weighted linear units for neural network function
  approximation in reinforcement learning.
\newblock {\em Neural Networks}, 107:3--11, 2018.

\bibitem{abadi2016tensorflow}
Mart{\'\i}n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
  Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et~al.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In {\em 12th $\{$USENIX$\}$ symposium on operating systems design and
  implementation ($\{$OSDI$\}$ 16)}, pages 265--283, 2016.

\bibitem{chorowski2015attention}
Jan~K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua
  Bengio.
\newblock Attention-based models for speech recognition.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~28, pages 577--585, 2015.

\bibitem{shen2019lingvo}
Jonathan Shen, Patrick Nguyen, Yonghui Wu, Zhifeng Chen, Mia~X Chen, Ye~Jia,
  Anjuli Kannan, Tara Sainath, Yuan Cao, Chung-Cheng Chiu, et~al.
\newblock Lingvo: a modular and scalable framework for sequence-to-sequence
  modeling.
\newblock {\em arXiv preprint arXiv:1902.08295}, 2019.

\bibitem{panayotov2015librispeech}
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur.
\newblock Librispeech: an asr corpus based on public domain audio books.
\newblock In {\em 2015 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 5206--5210. IEEE, 2015.

\bibitem{rosenblatt1958perceptron}
Frank Rosenblatt.
\newblock The perceptron: a probabilistic model for information storage and
  organization in the brain.
\newblock {\em Psychological review}, 65(6):386, 1958.

\bibitem{riedmiller1992rprop}
Martin Riedmiller and Heinrich Braun.
\newblock Rprop-a fast adaptive learning algorithm.
\newblock In {\em Proc. of ISCIS VII), Universitat}. Citeseer, 1992.

\bibitem{hinton2012coursera}
Geoffrey Hinton, N~Srivastava, K~Swersky, T~Tieleman, and A~Mohamed.
\newblock Coursera: Neural networks for machine learning.
\newblock {\em Lecture 9c: Using noise as a regularizer}, 2012.

\bibitem{seide20141}
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech dnns.
\newblock In {\em Fifteenth Annual Conference of the International Speech
  Communication Association}, 2014.

\bibitem{bernstein2018signsgd}
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree
  Anandkumar.
\newblock signsgd: Compressed optimisation for non-convex problems.
\newblock In {\em International Conference on Machine Learning}, pages
  560--569. PMLR, 2018.

\bibitem{karimireddy2019error}
Sai~Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi.
\newblock Error feedback fixes signsgd and other gradient compression schemes.
\newblock In {\em International Conference on Machine Learning}, pages
  3252--3261. PMLR, 2019.

\bibitem{balles2020geometry}
Lukas Balles, Fabian Pedregosa, and Nicolas~Le Roux.
\newblock The geometry of sign gradient descent.
\newblock {\em arXiv preprint arXiv:2002.08056}, 2020.

\bibitem{jin2020stochastic}
Richeng Jin, Yufan Huang, Xiaofan He, Tianfu Wu, and Huaiyu Dai.
\newblock Stochastic-sign sgd for federated learning with theoretical
  guarantees.
\newblock {\em arXiv preprint arXiv:2002.10940}, 2020.

\bibitem{aji2017sparse}
Alham~Fikri Aji and Kenneth Heafield.
\newblock Sparse communication for distributed gradient descent.
\newblock {\em arXiv preprint arXiv:1704.05021}, 2017.

\bibitem{chen2018adacomp}
Chia-Yu Chen, Jungwook Choi, Daniel Brand, Ankur Agrawal, Wei Zhang, and
  Kailash Gopalakrishnan.
\newblock Adacomp: Adaptive residual gradient compression for data-parallel
  distributed training.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\end{thebibliography}
