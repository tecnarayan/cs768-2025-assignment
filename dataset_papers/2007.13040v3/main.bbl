\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
  Ghemawat, S., Irving, G., Isard, M., et~al.
\newblock Tensorflow: a system for large-scale machine learning.
\newblock In \emph{OSDI}, volume~16, pp.\  265--283, 2016.

\bibitem[Achille \& Soatto(2018)Achille and Soatto]{achille2018information}
Achille, A. and Soatto, S.
\newblock Information dropout: Learning optimal representations through noisy
  computation.
\newblock \emph{T-PAMI}, 40\penalty0 (12):\penalty0 2897--2905, 2018.

\bibitem[Alemi et~al.(2017)Alemi, Fischer, Dillon, and Murphy]{alemi2017deep}
Alemi, A.~A., Fischer, I., Dillon, J.~V., and Murphy, K.
\newblock Deep variational information bottleneck.
\newblock In \emph{ICLR}, 2017.

\bibitem[Antoniou et~al.(2019)Antoniou, Edwards, and
  Storkey]{antoniou2019train}
Antoniou, A., Edwards, H., and Storkey, A.
\newblock How to train your maml.
\newblock In \emph{ICLR}, 2019.

\bibitem[Bartlett \& Mendelson(2002)Bartlett and
  Mendelson]{bartlett2002rademacher}
Bartlett, P.~L. and Mendelson, S.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Nov):\penalty0 463--482, 2002.

\bibitem[Cubuk et~al.(2019)Cubuk, Zoph, Mane, Vasudevan, and
  Le]{cubuk2019autoaugment}
Cubuk, E.~D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q.~V.
\newblock Autoaugment: Learning augmentation strategies from data.
\newblock In \emph{CVPR}, pp.\  113--123, 2019.

\bibitem[Dou et~al.(2019)Dou, Yu, and Anastasopoulos]{dou2019investigating}
Dou, Z.-Y., Yu, K., and Anastasopoulos, A.
\newblock Investigating meta-learning algorithms for low-resource natural
  language understanding tasks.
\newblock In \emph{EMNLP}, 2019.

\bibitem[Finn \& Levine(2018)Finn and Levine]{finn2017meta}
Finn, C. and Levine, S.
\newblock Meta-learning and universality: Deep representations and gradient
  descent can approximate any learning algorithm.
\newblock \emph{ICLR}, 2018.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{ICML}, pp.\  1126--1135, 2017.

\bibitem[Finn et~al.(2018)Finn, Xu, and Levine]{finn2018probabilistic}
Finn, C., Xu, K., and Levine, S.
\newblock Probabilistic model-agnostic meta-learning.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Flennerhag et~al.(2020)Flennerhag, Rusu, Pascanu, Yin, and
  Hadsell]{flennerhag2020meta}
Flennerhag, S., Rusu, A.~A., Pascanu, R., Yin, H., and Hadsell, R.
\newblock Meta-learning with warped gradient descent.
\newblock In \emph{ICLR}, 2020.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{gal2016dropout}
Gal, Y. and Ghahramani, Z.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{ICML}, pp.\  1050--1059, 2016.

\bibitem[Grant et~al.(2018)Grant, Finn, Levine, Darrell, and
  Griffiths]{grant2018recasting}
Grant, E., Finn, C., Levine, S., Darrell, T., and Griffiths, T.
\newblock Recasting gradient-based meta-learning as hierarchical bayes.
\newblock In \emph{ICLR}, 2018.

\bibitem[Gu et~al.(2018)Gu, Wang, Chen, Cho, and Li]{gu2018meta}
Gu, J., Wang, Y., Chen, Y., Cho, K., and Li, V.~O.
\newblock Meta-learning for low-resource neural machine translation.
\newblock In \emph{EMNLP}, 2018.

\bibitem[Jamal \& Qi(2019)Jamal and Qi]{jamal2019task}
Jamal, M.~A. and Qi, G.-J.
\newblock Task agnostic meta-learning for few-shot learning.
\newblock In \emph{CVPR}, pp.\  11719--11727, 2019.

\bibitem[Kang et~al.(2019)Kang, Liu, Wang, Yu, Feng, and Darrell]{kang2019few}
Kang, B., Liu, Z., Wang, X., Yu, F., Feng, J., and Darrell, T.
\newblock Few-shot object detection via feature reweighting.
\newblock In \emph{ICCV}, pp.\  8420--8429, 2019.

\bibitem[Krogh \& Hertz(1992)Krogh and Hertz]{krogh1992simple}
Krogh, A. and Hertz, J.~A.
\newblock A simple weight decay can improve generalization.
\newblock In \emph{NeurIPS}, pp.\  950--957, 1992.

\bibitem[Lake et~al.(2011)Lake, Salakhutdinov, Gross, and
  Tenenbaum]{lake2011one}
Lake, B., Salakhutdinov, R., Gross, J., and Tenenbaum, J.
\newblock One shot learning of simple visual concepts.
\newblock In \emph{Proceedings of the annual meeting of the cognitive science
  society}, volume~33, 2011.

\bibitem[Lee et~al.(2020)Lee, Nam, Yang, and Hwang]{lee2020meta}
Lee, H.~B., Nam, T., Yang, E., and Hwang, S.~J.
\newblock Meta dropout: Learning to perturb latent features for generalization.
\newblock In \emph{ICLR}, 2020.

\bibitem[Lee \& Choi(2018)Lee and Choi]{lee2018gradient}
Lee, Y. and Choi, S.
\newblock Gradient-based meta-learning with learned layerwise metric and
  subspace.
\newblock In \emph{ICML}, pp.\  2933--2942, 2018.

\bibitem[Li et~al.(2017)Li, Zhou, Chen, and Li]{li2017meta}
Li, Z., Zhou, F., Chen, F., and Li, H.
\newblock Meta-sgd: Learning to learn quickly for few shot learning.
\newblock \emph{arXiv preprint arXiv:1707.09835}, 2017.

\bibitem[Liu et~al.(2019)Liu, Huang, Mallya, Karras, Aila, Lehtinen, and
  Kautz]{liu2019few}
Liu, M.-Y., Huang, X., Mallya, A., Karras, T., Aila, T., Lehtinen, J., and
  Kautz, J.
\newblock Few-shot unsupervised image-to-image translation.
\newblock In \emph{ICCV}, pp.\  10551--10560, 2019.

\bibitem[Madotto et~al.(2019)Madotto, Lin, Wu, and
  Fung]{madotto2019personalizing}
Madotto, A., Lin, Z., Wu, C.-S., and Fung, P.
\newblock Personalizing dialogue agents via meta-learning.
\newblock In \emph{ACL}, 2019.

\bibitem[Martin et~al.(2019)Martin, Polyakov, Zhu, Tian, Mukherjee, and
  Liu]{martin2019all}
Martin, E.~J., Polyakov, V.~R., Zhu, X.-W., Tian, L., Mukherjee, P., and Liu,
  X.
\newblock All-assay-max2 pqsar: Activity predictions as accurate as
  four-concentration ic50s for 8558 novartis assays.
\newblock \emph{Journal of chemical information and modeling}, 59\penalty0
  (10):\penalty0 4450--4459, 2019.

\bibitem[Mishra et~al.(2018)Mishra, Rohaninejad, Chen, and
  Abbeel]{mishra2018simple}
Mishra, N., Rohaninejad, M., Chen, X., and Abbeel, P.
\newblock A simple neural attentive meta-learner.
\newblock \emph{ICLR}, 2018.

\bibitem[Oh et~al.(2021)Oh, Yoo, Kim, and Yun]{oh2021boil}
Oh, J., Yoo, H., Kim, C., and Yun, S.-Y.
\newblock Boil: Towards representation change for few-shot learning.
\newblock In \emph{ICLR}, 2021.

\bibitem[Oreshkin et~al.(2018)Oreshkin, L{\'o}pez, and
  Lacoste]{oreshkin2018tadam}
Oreshkin, B., L{\'o}pez, P.~R., and Lacoste, A.
\newblock Tadam: Task dependent adaptive metric for improved few-shot learning.
\newblock In \emph{NeurIPS}, pp.\  721--731, 2018.

\bibitem[Park \& Oliva(2019)Park and Oliva]{park2019meta}
Park, E. and Oliva, J.~B.
\newblock Meta-curvature.
\newblock In \emph{NeurIPS}, pp.\  3309--3319, 2019.

\bibitem[Raghu et~al.(2020)Raghu, Raghu, Bengio, and Vinyals]{raghu2020rapid}
Raghu, A., Raghu, M., Bengio, S., and Vinyals, O.
\newblock Rapid learning or feature reuse? towards understanding the
  effectiveness of maml.
\newblock In \emph{ICLR}, 2020.

\bibitem[Rajendran et~al.(2020)Rajendran, Irpan, and Jang]{rajendran2020meta}
Rajendran, J., Irpan, A., and Jang, E.
\newblock Meta-learning requires meta-augmentation.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{snell2017prototypical}
Snell, J., Swersky, K., and Zemel, R.
\newblock Prototypical networks for few-shot learning.
\newblock In \emph{NIPS}, pp.\  4077--4087, 2017.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{JMLR}, 15\penalty0 (1):\penalty0 1929--1958, 2014.

\bibitem[Sung et~al.(2018)Sung, Yang, Zhang, Xiang, Torr, and
  Hospedales]{sung2018learning}
Sung, F., Yang, Y., Zhang, L., Xiang, T., Torr, P.~H., and Hospedales, T.~M.
\newblock Learning to compare: Relation network for few-shot learning.
\newblock In \emph{CVPR}, pp.\  1199--1208, 2018.

\bibitem[Thrun \& Pratt(1998)Thrun and Pratt]{thrun2012learning}
Thrun, S. and Pratt, L.
\newblock \emph{Learning to learn}.
\newblock Springer Science \& Business Media, 1998.

\bibitem[Tishby \& Zaslavsky(2015)Tishby and Zaslavsky]{tishby2015deep}
Tishby, N. and Zaslavsky, N.
\newblock Deep learning and the information bottleneck principle.
\newblock In \emph{2015 IEEE Information Theory Workshop (ITW)}, pp.\  1--5.
  IEEE, 2015.

\bibitem[Verma et~al.(2019)Verma, Lamb, Beckham, Najafi, Mitliagkas, Courville,
  Lopez-Paz, and Bengio]{verma2019manifold}
Verma, V., Lamb, A., Beckham, C., Najafi, A., Mitliagkas, I., Courville, A.,
  Lopez-Paz, D., and Bengio, Y.
\newblock Manifold mixup: Better representations by interpolating hidden
  states.
\newblock \emph{ICML}, 2019.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, Wierstra,
  et~al.]{vinyals2016matching}
Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et~al.
\newblock Matching networks for one shot learning.
\newblock In \emph{NIPS}, pp.\  3630--3638, 2016.

\bibitem[Vuorio et~al.(2019)Vuorio, Sun, Hu, and Lim]{vuorio2019multimodal}
Vuorio, R., Sun, S.-H., Hu, H., and Lim, J.~J.
\newblock Multimodal model-agnostic meta-learning via task-aware modulation.
\newblock In \emph{NeurIPS}, pp.\  1--12, 2019.

\bibitem[Xiang et~al.(2014)Xiang, Mottaghi, and Savarese]{xiang2014beyond}
Xiang, Y., Mottaghi, R., and Savarese, S.
\newblock Beyond pascal: A benchmark for 3d object detection in the wild.
\newblock In \emph{WACV}, pp.\  75--82. IEEE, 2014.

\bibitem[Xie et~al.(2018)Xie, Singh, Levine, and Finn]{xie2018few}
Xie, A., Singh, A., Levine, S., and Finn, C.
\newblock Few-shot goal inference for visuomotor learning and planning.
\newblock In \emph{CoRL}, pp.\  40--52, 2018.

\bibitem[Yao et~al.(2019)Yao, Wei, Huang, and Li]{yao2019hierarchically}
Yao, H., Wei, Y., Huang, J., and Li, Z.
\newblock Hierarchically structured meta-learning.
\newblock In \emph{ICML}, 2019.

\bibitem[Yao et~al.(2020)Yao, Wu, Tao, Li, Ding, Li, and Li]{yao2020automated}
Yao, H., Wu, X., Tao, Z., Li, Y., Ding, B., Li, R., and Li, Z.
\newblock Automated relational meta-learning.
\newblock In \emph{ICLR}, 2020.

\bibitem[Yin et~al.(2020)Yin, Tucker, Zhou, Levine, and Finn]{yin2020meta}
Yin, M., Tucker, G., Zhou, M., Levine, S., and Finn, C.
\newblock Meta-learning without memorization.
\newblock \emph{ICLR}, 2020.

\bibitem[Yu et~al.(2018)Yu, Finn, Xie, Dasari, Zhang, Abbeel, and
  Levine]{yu2018one}
Yu, T., Finn, C., Xie, A., Dasari, S., Zhang, T., Abbeel, P., and Levine, S.
\newblock One-shot imitation from observing humans via domain-adaptive
  meta-learning.
\newblock In \emph{RSS}, 2018.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2018mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{ICLR}, 2018.

\bibitem[Zhang et~al.(2021)Zhang, Deng, Kawaguchi, Ghorbani, and
  Zou]{zhang2020does}
Zhang, L., Deng, Z., Kawaguchi, K., Ghorbani, A., and Zou, J.
\newblock How does mixup help with robustness and generalization?
\newblock In \emph{ICLR}, 2021.

\bibitem[Zhong et~al.(2020)Zhong, Zheng, Kang, Li, and Yang]{zhong2020random}
Zhong, Z., Zheng, L., Kang, G., Li, S., and Yang, Y.
\newblock Random erasing data augmentation.
\newblock In \emph{AAAI}, 2020.

\bibitem[Zintgraf et~al.(2019)Zintgraf, Shiarlis, Kurin, Hofmann, and
  Whiteson]{zintgraf2019fast}
Zintgraf, L.~M., Shiarlis, K., Kurin, V., Hofmann, K., and Whiteson, S.
\newblock Fast context adaptation via meta-learning.
\newblock In \emph{ICML}, 2019.

\end{thebibliography}
