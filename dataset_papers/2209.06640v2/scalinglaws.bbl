\begin{thebibliography}{10}

\bibitem{abnar2021exploring}
Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi.
\newblock Exploring the limits of large scale pre-training.
\newblock {\em arXiv preprint arXiv:2110.02095}, 2021.

\bibitem{bahri2021explaining}
Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma.
\newblock Explaining neural scaling laws.
\newblock {\em arXiv preprint arXiv:2102.06701}, 2021.

\bibitem{bansal2022data}
Yamini Bansal, Behrooz Ghorbani, Ankush Garg, Biao Zhang, Maxim Krikun, Colin
  Cherry, Behnam Neyshabur, and Orhan Firat.
\newblock Data scaling laws in {NMT}: The effect of noise and architecture.
\newblock {\em arXiv preprint arXiv:2202.01994}, 2022.

\bibitem{beleites2013sample}
Claudia Beleites, Ute Neugebauer, Thomas Bocklitz, Christoph Krafft, and
  J{\"u}rgen Popp.
\newblock Sample size planning for classification models.
\newblock {\em Analytica chimica acta}, 760:25--33, 2013.

\bibitem{beyer2021knowledge}
Lucas Beyer, Xiaohua Zhai, Am{\'e}lie Royer, Larisa Markeeva, Rohan Anil, and
  Alexander Kolesnikov.
\newblock Knowledge distillation: A good teacher is patient and consistent.
\newblock {\em arXiv preprint arXiv:2106.05237}, 2021.

\bibitem{bigbench}
{BIG-bench collaboration}.
\newblock Beyond the imitation game: Measuring and extrapolating the
  capabilities of language models.
\newblock {\em In preparation}, 2021.

\bibitem{bousquet2021theory}
Olivier Bousquet, Steve Hanneke, Shay Moran, Ramon Van~Handel, and Amir
  Yehudayoff.
\newblock A theory of universal learning.
\newblock In {\em Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory
  of Computing}, pages 532--541, 2021.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em {NeurIPS}}, 33:1877--1901, 2020.

\bibitem{cho2015much}
Junghwan Cho, Kyewook Lee, Ellie Shin, Garry Choy, and Synho Do.
\newblock How much data is needed to train a medical image deep learning system
  to achieve necessary high accuracy?
\newblock {\em arXiv preprint arXiv:1511.06348}, 2015.

\bibitem{dai2021coatnet}
Zihang Dai, Hanxiao Liu, Quoc~V Le, and Mingxing Tan.
\newblock Coatnet: Marrying convolution and attention for all data sizes.
\newblock {\em {NeurIPS}}, 34:3965--3977, 2021.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em {CVPR}}, 2009.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{domhan2015speeding}
Tobias Domhan, Jost~Tobias Springenberg, and Frank Hutter.
\newblock Speeding up automatic hyperparameter optimization of deep neural
  networks by extrapolation of learning curves.
\newblock In {\em {IJCAI}}, 2015.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em {ICLR}}, 2020.

\bibitem{elsken2019neural}
Thomas Elsken, Jan~Hendrik Metzen, and Frank Hutter.
\newblock Neural architecture search: A survey.
\newblock {\em {JMLR}}, 20(1):1997--2017, 2019.

\bibitem{FeiFei2004LearningGV}
Li~Fei-Fei, Rob Fergus, and Pietro Perona.
\newblock Learning generative visual models from few training examples: An
  incremental bayesian approach tested on 101 object categories.
\newblock {\em Computer Vision and Pattern Recognition Workshop}, 2004.

\bibitem{figueroa2012predicting}
Rosa~L Figueroa, Qing Zeng-Treitler, Sasikiran Kandula, and Long~H Ngo.
\newblock Predicting sample size required for classification performance.
\newblock {\em BMC medical informatics and decision making}, 12(1):1--10, 2012.

\bibitem{ghorbani2021scaling}
Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun,
  Xavier Garcia, Ciprian Chelba, and Colin Cherry.
\newblock Scaling laws for neural machine translation.
\newblock {\em arXiv preprint arXiv:2109.07740}, 2021.

\bibitem{gordon2021data}
Mitchell~A Gordon, Kevin Duh, and Jared Kaplan.
\newblock Data and parameter scaling laws for neural machine translation.
\newblock In {\em {EMNLP}}, pages 5915--5922, 2021.

\bibitem{hestness2017deep}
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun,
  Hassan Kianinejad, Md~Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou.
\newblock Deep learning scaling is predictable, empirically.
\newblock {\em arXiv preprint arXiv:1712.00409}, 2017.

\bibitem{hutter2021learning}
Marcus Hutter.
\newblock Learning curve theory.
\newblock {\em arXiv preprint arXiv:2102.04074}, 2021.

\bibitem{johnson-etal-2018-predicting}
Mark Johnson, Peter Anderson, Mark Dras, and Mark Steedman.
\newblock Predicting accuracy on large datasets from smaller pilot data.
\newblock In {\em Proceedings of the 56th Annual Meeting of the Association for
  Computational Linguistics}, pages 450--455, Melbourne, Australia, 2018.
  Association for Computational Linguistics.

\bibitem{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock {\em arXiv preprint arXiv:2001.08361}, 2020.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{klein2016learning}
Aaron Klein, Stefan Falkner, Jost~Tobias Springenberg, and Frank Hutter.
\newblock Learning curve prediction with bayesian neural networks.
\newblock In {\em {ICLR}}, 2017.

\bibitem{kolesnikov2020big}
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung,
  Sylvain Gelly, and Neil Houlsby.
\newblock Big transfer ({B}i{T}): General visual representation learning.
\newblock In {\em {ECCV}}, pages 491--507, 2020.

\bibitem{Krizhevsky09learningmultiple}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem{mukherjee2003estimating}
Sayan Mukherjee, Pablo Tamayo, Simon Rogers, Ryan Rifkin, Anna Engle, Colin
  Campbell, Todd~R Golub, and Jill~P Mesirov.
\newblock Estimating dataset size requirements for classifying dna microarray
  data.
\newblock {\em Journal of computational biology}, 10(2):119--142, 2003.

\bibitem{nakkiran2020deep}
Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi.
\newblock The deep bootstrap framework: Good online learners are good offline
  generalizers.
\newblock {\em {ICLR}}, 2020.

\bibitem{rosenfeld2021scaling}
Jonathan~S Rosenfeld.
\newblock Scaling laws for deep learning.
\newblock {\em arXiv preprint arXiv:2108.07686}, 2021.

\bibitem{rosenfeld2019constructive}
Jonathan~S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit.
\newblock A constructive prediction of the generalization error across scales.
\newblock {\em arXiv preprint arXiv:1909.12673}, 2019.

\bibitem{sharma2022scaling}
Utkarsh Sharma and Jared Kaplan.
\newblock Scaling laws from the data manifold dimension.
\newblock {\em {JMLR}}, 23(9):1--34, 2022.

\bibitem{shazeer2018adafactor}
Noam Shazeer and Mitchell Stern.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock In {\em {ICML}}, 2018.

\bibitem{sun2017revisiting}
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta.
\newblock Revisiting unreasonable effectiveness of data in deep learning era.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 843--852, 2017.

\bibitem{tan2019efficientnet}
Mingxing Tan and Quoc Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In {\em {ICML}}, pages 6105--6114, 2019.

\bibitem{thoppilan2022lamda}
Romal Thoppilan, Daniel De~Freitas, Jamie Hall, Noam Shazeer, Apoorv
  Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du,
  et~al.
\newblock Lamda: Language models for dialog applications.
\newblock {\em arXiv preprint arXiv:2201.08239}, 2022.

\bibitem{tolstikhin2021mlp}
Ilya~O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua
  Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers,
  Jakob Uszkoreit, et~al.
\newblock {MLP}-mixer: An all-{MLP} architecture for vision.
\newblock {\em {NeurIPS}}, 34, 2021.

\bibitem{WelinderEtal2010}
P.~Welinder, S.~Branson, T.~Mita, C.~Wah, F.~Schroff, S.~Belongie, and
  P.~Perona.
\newblock {Caltech-UCSD Birds 200}.
\newblock Technical Report CNS-TR-2010-001, California Institute of Technology,
  2010.

\bibitem{wightman2021resnet}
Ross Wightman, Hugo Touvron, and Herv{\'e} J{\'e}gou.
\newblock Resnet strikes back: An improved training procedure in timm.
\newblock {\em arXiv preprint arXiv:2110.00476}, 2021.

\bibitem{zhai2106scaling}
X~Zhai, A~Kolesnikov, N~Houlsby, and L~Beyer.
\newblock Scaling vision transformers.
\newblock {\em arXiv preprint arXiv:2106.04560}, 2021.

\end{thebibliography}
