\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alain et~al.(2015)Alain, Lamb, Sankar, Courville, and
  Bengio]{alain2015variance}
Alain, G., Lamb, A., Sankar, C., Courville, A., and Bengio, Y.
\newblock Variance reduction in sgd by distributed importance sampling.
\newblock \emph{arXiv preprint arXiv:1511.06481}, 2015.

\bibitem[Asi \& Duchi(2019)Asi and Duchi]{asi2019importance}
Asi, H. and Duchi, J.~C.
\newblock The importance of better models in stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1903.08619}, 2019.

\bibitem[Bertsekas(1979)]{bertsekas1979convexification}
Bertsekas, D.~P.
\newblock Convexification procedures and decomposition methods for nonconvex
  optimization problems.
\newblock \emph{Journal of Optimization Theory and Applications}, 29\penalty0
  (2):\penalty0 169--197, 1979.

\bibitem[Birodkar et~al.(2019)Birodkar, Mobahi, and
  Bengio]{birodkar2019semantic}
Birodkar, V., Mobahi, H., and Bengio, S.
\newblock Semantic redundancies in image-classification datasets: The 10\% you
  don't need.
\newblock \emph{arXiv preprint arXiv:1901.11409}, 2019.

\bibitem[Bollapragada et~al.(2019)Bollapragada, Byrd, and
  Nocedal]{bollapragada2019exact}
Bollapragada, R., Byrd, R.~H., and Nocedal, J.
\newblock Exact and inexact subsampled newton methods for optimization.
\newblock \emph{IMA Journal of Numerical Analysis}, 39\penalty0 (2):\penalty0
  545--578, 2019.

\bibitem[Bowman et~al.(2015)Bowman, Angeli, Potts, and
  Manning]{bowman-etal-2015-large}
Bowman, S.~R., Angeli, G., Potts, C., and Manning, C.~D.
\newblock A large annotated corpus for learning natural language inference.
\newblock In \emph{Proceedings of the 2015 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  632--642, Lisbon, Portugal, September
  2015. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D15-1075}.
\newblock URL \url{https://aclanthology.org/D15-1075}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Carmon et~al.(2018)Carmon, Duchi, Hinder, and
  Sidford]{carmon2018accelerated}
Carmon, Y., Duchi, J.~C., Hinder, O., and Sidford, A.
\newblock Accelerated methods for nonconvex optimization.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (2):\penalty0
  1751--1772, 2018.

\bibitem[Coleman et~al.(2020)Coleman, Yeh, Mussmann, Mirzasoleiman, Bailis,
  Liang, Leskovec, and Zaharia]{coleman2020selection}
Coleman, C., Yeh, C., Mussmann, S., Mirzasoleiman, B., Bailis, P., Liang, P.,
  Leskovec, J., and Zaharia, M.
\newblock Selection via proxy: Efficient data selection for deep learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Defazio \& Bottou(2019)Defazio and Bottou]{defazio2019ineffectiveness}
Defazio, A. and Bottou, L.
\newblock On the ineffectiveness of variance reduced optimization for deep
  learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 1755--1765, 2019.

\bibitem[Dembo et~al.(1982)Dembo, Eisenstat, and Steihaug]{dembo1982inexact}
Dembo, R.~S., Eisenstat, S.~C., and Steihaug, T.
\newblock Inexact newton methods.
\newblock \emph{SIAM Journal on Numerical analysis}, 19\penalty0 (2):\penalty0
  400--408, 1982.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013stochastic}
Ghadimi, S. and Lan, G.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Hutchinson(1989)]{hutchinson1989stochastic}
Hutchinson, M.~F.
\newblock A stochastic estimator of the trace of the influence matrix for
  laplacian smoothing splines.
\newblock \emph{Communications in Statistics-Simulation and Computation},
  18\penalty0 (3):\penalty0 1059--1076, 1989.

\bibitem[Jin et~al.(2021)Jin, Netrapalli, Ge, Kakade, and
  Jordan]{jin2021nonconvex}
Jin, C., Netrapalli, P., Ge, R., Kakade, S.~M., and Jordan, M.~I.
\newblock On nonconvex optimization for machine learning: Gradients,
  stochasticity, and saddle points.
\newblock \emph{Journal of the ACM (JACM)}, 68\penalty0 (2):\penalty0 1--29,
  2021.

\bibitem[Katharopoulos \& Fleuret(2018)Katharopoulos and
  Fleuret]{katharopoulos2018not}
Katharopoulos, A. and Fleuret, F.
\newblock Not all samples are created equal: Deep learning with importance
  sampling.
\newblock In \emph{International conference on machine learning}, pp.\
  2525--2534. PMLR, 2018.

\bibitem[Killamsetty et~al.(2021{\natexlab{a}})Killamsetty, Durga,
  Ramakrishnan, De, and Iyer]{killamsetty2021grad}
Killamsetty, K., Durga, S., Ramakrishnan, G., De, A., and Iyer, R.
\newblock Grad-match: Gradient matching based data subset selection for
  efficient deep model training.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5464--5474. PMLR, 2021{\natexlab{a}}.

\bibitem[Killamsetty et~al.(2021{\natexlab{b}})Killamsetty, Sivasubramanian,
  Ramakrishnan, and Iyer]{killamsetty2021glister}
Killamsetty, K., Sivasubramanian, D., Ramakrishnan, G., and Iyer, R.
\newblock Glister: Generalization based data subset selection for efficient and
  robust learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pp.\  8110--8118, 2021{\natexlab{b}}.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Nair, and Hinton]{cifar10}
Krizhevsky, A., Nair, V., and Hinton, G.
\newblock Cifar-10 (canadian institute for advanced research).
\newblock 2009.
\newblock URL \url{http://www.cs.toronto.edu/~kriz/cifar.html}.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L., and Stoyanov, V.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Loshchilov \& Hutter(2015)Loshchilov and Hutter]{loshchilov2015online}
Loshchilov, I. and Hutter, F.
\newblock Online batch selection for faster training of neural networks.
\newblock \emph{arXiv preprint arXiv:1511.06343}, 2015.

\bibitem[Marquardt(1963)]{marquardt1963algorithm}
Marquardt, D.~W.
\newblock An algorithm for least-squares estimation of nonlinear parameters.
\newblock \emph{Journal of the society for Industrial and Applied Mathematics},
  11\penalty0 (2):\penalty0 431--441, 1963.

\bibitem[Martens \& Grosse(2015)Martens and Grosse]{martens2015optimizing}
Martens, J. and Grosse, R.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{International conference on machine learning}, pp.\
  2408--2417. PMLR, 2015.

\bibitem[Mindermann et~al.(2022)Mindermann, Brauner, Razzak, Sharma, Kirsch,
  Xu, H{\"o}ltgen, Gomez, Morisot, Farquhar, et~al.]{mindermann2022prioritized}
Mindermann, S., Brauner, J.~M., Razzak, M.~T., Sharma, M., Kirsch, A., Xu, W.,
  H{\"o}ltgen, B., Gomez, A.~N., Morisot, A., Farquhar, S., et~al.
\newblock Prioritized training on points that are learnable, worth learning,
  and not yet learnt.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  15630--15649. PMLR, 2022.

\bibitem[Mirzasoleiman et~al.(2013)Mirzasoleiman, Karbasi, Sarkar, and
  Krause]{mirzasoleiman2013distributed}
Mirzasoleiman, B., Karbasi, A., Sarkar, R., and Krause, A.
\newblock Distributed submodular maximization: Identifying representative
  elements in massive data.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2049--2057, 2013.

\bibitem[Mirzasoleiman et~al.(2020)Mirzasoleiman, Bilmes, and
  Leskovec]{mirzasoleiman2020coresets}
Mirzasoleiman, B., Bilmes, J., and Leskovec, J.
\newblock Coresets for data-efficient training of machine learning models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6950--6960. PMLR, 2020.

\bibitem[Paul et~al.(2021)Paul, Ganguli, and Dziugaite]{paul2021deep}
Paul, M., Ganguli, S., and Dziugaite, G.~K.
\newblock Deep learning on a data diet: Finding important examples early in
  training.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 20596--20607, 2021.

\bibitem[Pooladzandi et~al.(2022)Pooladzandi, Davini, and
  Mirzasoleiman]{pooladzandi2022adaptive}
Pooladzandi, O., Davini, D., and Mirzasoleiman, B.
\newblock Adaptive second order coresets for data-efficient machine learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  17848--17869. PMLR, 2022.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{imagenet15russakovsky}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., Berg, A.~C., and Fei-Fei, L.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 115\penalty0
  (3):\penalty0 211--252, 2015.
\newblock \doi{10.1007/s11263-015-0816-y}.

\bibitem[Schaul et~al.(2015)Schaul, Quan, Antonoglou, and
  Silver]{schaul2015prioritized}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock \emph{arXiv preprint arXiv:1511.05952}, 2015.

\bibitem[Schwartz et~al.(2019)Schwartz, Dodge, Smith, and
  Etzioni]{schwartz2019green}
Schwartz, R., Dodge, J., Smith, N.~A., and Etzioni, O.
\newblock Green ai.
\newblock \emph{arXiv preprint arXiv:1907.10597}, 2019.

\bibitem[Strubell et~al.(2019)Strubell, Ganesh, and
  McCallum]{strubell2019energy}
Strubell, E., Ganesh, A., and McCallum, A.
\newblock Energy and policy considerations for deep learning in nlp.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  3645--3650, 2019.

\bibitem[Toneva et~al.(2018)Toneva, Sordoni, des Combes, Trischler, Bengio, and
  Gordon]{toneva2018empirical}
Toneva, M., Sordoni, A., des Combes, R.~T., Trischler, A., Bengio, Y., and
  Gordon, G.~J.
\newblock An empirical study of example forgetting during deep neural network
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Wang \& Srebro(2019)Wang and Srebro]{wang2019stochastic}
Wang, W. and Srebro, N.
\newblock Stochastic nonconvex optimization with large minibatches.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  857--882. PMLR, 2019.

\bibitem[Xiao et~al.(2015)Xiao, Xia, Yang, Huang, and Wang]{xiao2015learning}
Xiao, T., Xia, T., Yang, Y., Huang, C., and Wang, X.
\newblock Learning from massive noisy labeled data for image classification.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  2691--2699, 2015.

\bibitem[Xu et~al.(2020)Xu, Roosta, and Mahoney]{xu2020newton}
Xu, P., Roosta, F., and Mahoney, M.~W.
\newblock Newton-type methods for non-convex optimization under inexact hessian
  information.
\newblock \emph{Mathematical Programming}, 184\penalty0 (1):\penalty0 35--70,
  2020.

\bibitem[Yao et~al.(2018)Yao, Xu, Roosta-Khorasani, and
  Mahoney]{yao2018inexact}
Yao, Z., Xu, P., Roosta-Khorasani, F., and Mahoney, M.~W.
\newblock Inexact non-convex newton-type methods.
\newblock \emph{arXiv preprint arXiv:1802.06925}, 2018.

\bibitem[Yao et~al.(2020)Yao, Gholami, Shen, Keutzer, and
  Mahoney]{yao2020adahessian}
Yao, Z., Gholami, A., Shen, S., Keutzer, K., and Mahoney, M.~W.
\newblock Adahessian: An adaptive second order optimizer for machine learning.
\newblock \emph{arXiv preprint arXiv:2006.00719}, 2020.

\bibitem[Zhai et~al.(2022)Zhai, Kolesnikov, Houlsby, and
  Beyer]{zhai2022scaling}
Zhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L.
\newblock Scaling vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  12104--12113, 2022.

\end{thebibliography}
