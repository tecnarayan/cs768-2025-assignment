\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abernethy et~al.(2008)Abernethy, Hazan, and
  Rakhlin]{abernethy2008competing}
Jacob Abernethy, Elad Hazan, and Alexander Rakhlin.
\newblock Competing in the dark: An efficient algorithm for bandit linear
  optimization.
\newblock In \emph{Proc. of the 21st Annual Conference on Learning Theory
  (COLT)}, 2008.

\bibitem[Alon et~al.(1997)Alon, Ben-David, Cesa-Bianchi, and
  Haussler]{alon1997scale}
Noga Alon, Shai Ben-David, Nicolo Cesa-Bianchi, and David Haussler.
\newblock Scale-sensitive dimensions, uniform convergence, and learnability.
\newblock \emph{Journal of the ACM}, 44:\penalty0 615--631, 1997.

\bibitem[Audibert and Bubeck(2009)]{audibert2009minimax}
Jean-Yves Audibert and S{\'e}bastien Bubeck.
\newblock Minimax policies for adversarial and stochastic bandits.
\newblock In \emph{COLT}, volume~7, pages 1--122, 2009.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, Freund, and
  Schapire]{auer2002non}
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert~E. Schapire.
\newblock The nonstochastic multiarmed bandit problem.
\newblock \emph{SIAM Journal on Computing}, 32\penalty0 (1):\penalty0 48--77,
  2002.

\bibitem[Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang, and
  Yang]{ayoub2020model}
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \emph{International Conference on Machine Learning}, pages
  463--474. PMLR, 2020.

\bibitem[Bubeck et~al.(2011)Bubeck, Munos, Stoltz, and
  Szepesv{\'a}ri]{bubeck2011x}
S{\'e}bastien Bubeck, R{\'e}mi Munos, Gilles Stoltz, and Csaba Szepesv{\'a}ri.
\newblock {X}-armed bandits.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0 (5), 2011.

\bibitem[Bubeck et~al.(2012)Bubeck, Cesa-Bianchi, and
  Kakade]{bubeck2012towards}
S{\'e}bastien Bubeck, Nicolo Cesa-Bianchi, and Sham~M Kakade.
\newblock Towards minimax policies for online linear optimization with bandit
  feedback.
\newblock In \emph{Conference on Learning Theory}, pages 41--1. JMLR Workshop
  and Conference Proceedings, 2012.

\bibitem[Bubeck et~al.(2017)Bubeck, Lee, and Eldan]{bubeck2017kernel}
S{\'e}bastien Bubeck, Yin~Tat Lee, and Ronen Eldan.
\newblock Kernel-based methods for bandit convex optimization.
\newblock In \emph{Proceedings of the 49th Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 72--85, 2017.

\bibitem[Cesa-Bianchi and Lugosi(2006)]{PLG}
Nicol{\`o} Cesa-Bianchi and G{\'a}bor Lugosi.
\newblock \emph{Prediction, Learning, and Games}.
\newblock Cambridge University Press, New York, NY, USA, 2006.
\newblock ISBN 0521841089.

\bibitem[Dani et~al.(2007)Dani, Hayes, and Kakade]{dani2007price}
Varsha Dani, Thomas~P Hayes, and Sham Kakade.
\newblock The price of bandit information for online optimization.
\newblock 2007.

\bibitem[Daniely et~al.(2011)Daniely, Sabato, Ben-David, and
  Shalev-Shwartz]{daniely2011multiclass}
Amit Daniely, Sivan Sabato, Shai Ben-David, and Shai Shalev-Shwartz.
\newblock Multiclass learnability and the {ERM} principle.
\newblock In \emph{Proceedings of the 24th Annual Conference on Learning
  Theory}, pages 207--232, 2011.

\bibitem[Dean et~al.(2020)Dean, Mania, Matni, Recht, and Tu]{dean2020sample}
Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu.
\newblock On the sample complexity of the linear quadratic regulator.
\newblock \emph{Foundations of Computational Mathematics}, 20\penalty0
  (4):\penalty0 633--679, 2020.

\bibitem[Dong et~al.(2019)Dong, Van~Roy, and Zhou]{dong2019provably}
Shi Dong, Benjamin Van~Roy, and Zhengyuan Zhou.
\newblock Provably efficient reinforcement learning with aggregated states.
\newblock \emph{arXiv preprint arXiv:1912.06366}, 2019.

\bibitem[Du et~al.(2019)Du, Krishnamurthy, Jiang, Agarwal, Dudik, and
  Langford]{du2019latent}
Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and
  John Langford.
\newblock Provably efficient {RL} with rich observations via latent state
  decoding.
\newblock In \emph{International Conference on Machine Learning}, pages
  1665--1674. PMLR, 2019.

\bibitem[Du et~al.(2021)Du, Kakade, Lee, Lovett, Mahajan, Sun, and
  Wang]{du2021bilinear}
Simon~S Du, Sham~M Kakade, Jason~D Lee, Shachar Lovett, Gaurav Mahajan, Wen
  Sun, and Ruosong Wang.
\newblock Bilinear classes: A structural framework for provable generalization
  in {RL}.
\newblock \emph{International Conference on Machine Learning}, 2021.

\bibitem[Flaxman et~al.(2005)Flaxman, Kalai, and McMahan]{flaxman2005online}
Abraham~D Flaxman, Adam~Tauman Kalai, and H~Brendan McMahan.
\newblock Online convex optimization in the bandit setting: gradient descent
  without a gradient.
\newblock In \emph{Proceedings of the sixteenth annual ACM-SIAM symposium on
  Discrete algorithms}, pages 385--394, 2005.

\bibitem[Foster et~al.(2021)Foster, Kakade, Qian, and
  Rakhlin]{foster2021statistical}
Dylan~J Foster, Sham~M Kakade, Jian Qian, and Alexander Rakhlin.
\newblock The statistical complexity of interactive decision making.
\newblock \emph{arXiv preprint arXiv:2112.13487}, 2021.

\bibitem[Hazan and Kale(2011)]{hazan2011better}
Elad Hazan and Satyen Kale.
\newblock Better algorithms for benign bandits.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0 (4), 2011.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang2017contextual}
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E
  Schapire.
\newblock Contextual decision processes with low {Bellman} rank are
  {PAC}-learnable.
\newblock In \emph{International Conference on Machine Learning}, pages
  1704--1713, 2017.

\bibitem[Jin et~al.(2020)Jin, Yang, Wang, and Jordan]{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pages 2137--2143, 2020.

\bibitem[Jin et~al.(2021)Jin, Liu, and Miryoosefi]{jin2021bellman}
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi.
\newblock Bellman eluder dimension: New rich classes of {RL} problems, and
  sample-efficient algorithms.
\newblock \emph{Neural Information Processing Systems}, 2021.

\bibitem[Jin and Luo(2020)]{jin2020simultaneously}
Tiancheng Jin and Haipeng Luo.
\newblock Simultaneously learning stochastic and adversarial episodic {MDPs}
  with known transition.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 16557--16566, 2020.

\bibitem[Kirschner(2021)]{kirschner2021information}
Johannes Kirschner.
\newblock \emph{Information-Directed Sampling-Frequentist Analysis and
  Applications}.
\newblock PhD thesis, ETH Zurich, 2021.

\bibitem[Kirschner et~al.(2020)Kirschner, Lattimore, and
  Krause]{kirschner2020information}
Johannes Kirschner, Tor Lattimore, and Andreas Krause.
\newblock Information directed sampling for linear partial monitoring.
\newblock In \emph{Conference on Learning Theory}, pages 2328--2369. PMLR,
  2020.

\bibitem[Kirschner et~al.(2021)Kirschner, Lattimore, Vernade, and
  Szepesv{\'a}ri]{kirschner2021asymptotically}
Johannes Kirschner, Tor Lattimore, Claire Vernade, and Csaba Szepesv{\'a}ri.
\newblock Asymptotically optimal information-directed sampling.
\newblock In \emph{Conference on Learning Theory}, pages 2777--2821. PMLR,
  2021.

\bibitem[Kleinberg(2004)]{kleinberg2004nearly}
Robert Kleinberg.
\newblock Nearly tight bounds for the continuum-armed bandit problem.
\newblock \emph{Advances in Neural Information Processing Systems},
  17:\penalty0 697--704, 2004.

\bibitem[Krishnamurthy et~al.(2016)Krishnamurthy, Agarwal, and
  Langford]{krishnamurthy2016pac}
Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock {PAC} reinforcement learning with rich observations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1840--1848, 2016.

\bibitem[Kwon et~al.(2021)Kwon, Efroni, Caramanis, and Mannor]{kwon2021rl}
Jeongyeol Kwon, Yonathan Efroni, Constantine Caramanis, and Shie Mannor.
\newblock {RL} for latent {MDPs}: Regret guarantees and a lower bound.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Lattimore(2020)]{lattimore2020improved}
Tor Lattimore.
\newblock Improved regret for zeroth-order adversarial bandit convex
  optimisation.
\newblock \emph{Mathematical Statistics and Learning}, 2\penalty0 (3):\penalty0
  311--334, 2020.

\bibitem[Lattimore(2021)]{lattimore2021minimax}
Tor Lattimore.
\newblock Minimax regret for bandit convex optimisation of ridge functions.
\newblock \emph{arXiv preprint arXiv:2106.00444}, 2021.

\bibitem[Lattimore(2022)]{lattimore2022minimax}
Tor Lattimore.
\newblock Minimax regret for partial monitoring: Infinite outcomes and
  rustichini's regret.
\newblock \emph{arXiv preprint arXiv:2202.10997}, 2022.

\bibitem[Lattimore and Gy\"{o}rgy(2021)]{lattimore2021mirror}
Tor Lattimore and Andras Gy\"{o}rgy.
\newblock Mirror descent and the information ratio.
\newblock In \emph{Conference on Learning Theory}, pages 2965--2992. PMLR,
  2021.

\bibitem[Lattimore and Szepesv{\'a}ri(2019)]{lattimore2019information}
Tor Lattimore and Csaba Szepesv{\'a}ri.
\newblock An information-theoretic approach to minimax regret in partial
  monitoring.
\newblock In \emph{Conference on Learning Theory}, pages 2111--2139. PMLR,
  2019.

\bibitem[Lattimore and Szepesv{\'a}ri(2020{\natexlab{a}})]{lattimore2020bandit}
Tor Lattimore and Csaba Szepesv{\'a}ri.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020{\natexlab{a}}.

\bibitem[Lattimore and
  Szepesv{\'a}ri(2020{\natexlab{b}})]{lattimore2020exploration}
Tor Lattimore and Csaba Szepesv{\'a}ri.
\newblock Exploration by optimisation in partial monitoring.
\newblock In \emph{Conference on Learning Theory}, pages 2488--2515. PMLR,
  2020{\natexlab{b}}.

\bibitem[Li(2009)]{li2009unifying}
Lihong Li.
\newblock \emph{A unifying framework for computational reinforcement learning
  theory}.
\newblock Rutgers, The State University of New Jersey---New Brunswick, 2009.

\bibitem[Liu et~al.(2022)Liu, Wang, and Jin]{liu2022learning}
Qinghua Liu, Yuanhao Wang, and Chi Jin.
\newblock Learning markov games with adversarial opponents: Efficient
  algorithms and fundamental limits.
\newblock \emph{arXiv preprint arXiv:2203.06803}, 2022.

\bibitem[Magureanu et~al.(2014)Magureanu, Combes, and
  Proutiere]{magureanu2014lipschitz}
Stefan Magureanu, Richard Combes, and Alexandre Proutiere.
\newblock Lipschitz bandits: Regret lower bound and optimal algorithms.
\newblock In \emph{Conference on Learning Theory}, pages 975--999. PMLR, 2014.

\bibitem[Modi et~al.(2020)Modi, Jiang, Tewari, and Singh]{modi2020sample}
Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh.
\newblock Sample complexity of reinforcement learning using linearly combined
  model ensembles.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2010--2020. PMLR, 2020.

\bibitem[Neu et~al.(2010)Neu, Gy{\"o}rgy, Szepesv{\'a}ri,
  et~al.]{neu2010online}
Gergely Neu, Andr{\'a}s Gy{\"o}rgy, Csaba Szepesv{\'a}ri, et~al.
\newblock The online loop-free stochastic shortest-path problem.
\newblock In \emph{COLT}, volume 2010, pages 231--243. Citeseer, 2010.

\bibitem[Neu et~al.(2014)Neu, Gy{\"o}rgy, Szepesv{\'a}ri, and
  Antos]{neu2014online}
Gergely Neu, Andr{\'a}s Gy{\"o}rgy, Csaba Szepesv{\'a}ri, and Andr{\'a}s Antos.
\newblock Online {Markov} decision processes under bandit feedback.
\newblock \emph{IEEE Transactions on Automatic Control}, 59:\penalty0 676--691,
  2014.

\bibitem[Polyanskiy(2020)]{polyanskiy2020lecture}
Yury Polyanskiy.
\newblock Information theoretic methods in statistics and computer science.
\newblock 2020.
\newblock URL \url{https://people.lids.mit.edu/yp/homepage/sdpi_course.html}.

\bibitem[Polyanskiy and Wu(2014)]{polyanskiy2014lecture}
Yury Polyanskiy and Yihong Wu.
\newblock Lecture notes on information theory.
\newblock 2014.

\bibitem[Rakhlin et~al.(2010)Rakhlin, Sridharan, and Tewari]{rakhlin2010online}
Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari.
\newblock Online learning: Random averages, combinatorial parameters, and
  learnability.
\newblock \emph{Advances in Neural Information Processing Systems 23}, pages
  1984--1992, 2010.

\bibitem[Russo and Van~Roy(2013)]{russo2013eluder}
Daniel Russo and Benjamin Van~Roy.
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2256--2264, 2013.

\bibitem[Russo and Van~Roy(2014)]{russo2014learning}
Daniel Russo and Benjamin Van~Roy.
\newblock Learning to optimize via posterior sampling.
\newblock \emph{Mathematics of Operations Research}, 39\penalty0 (4):\penalty0
  1221--1243, 2014.

\bibitem[Russo and Van~Roy(2018)]{russo2018learning}
Daniel Russo and Benjamin Van~Roy.
\newblock Learning to optimize via information-directed sampling.
\newblock \emph{Operations Research}, 66\penalty0 (1):\penalty0 230--252, 2018.

\bibitem[Sekhari et~al.(2021)Sekhari, Dann, Mohri, Mansour, and
  Sridharan]{sekhari2021agnostic}
Ayush Sekhari, Christoph Dann, Mehryar Mohri, Yishay Mansour, and Karthik
  Sridharan.
\newblock Agnostic reinforcement learning with low-rank {MDPs} and rich
  observations.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Shalev-Shwartz et~al.(2010)Shalev-Shwartz, Shamir, Srebro, and
  Sridharan]{shalev2010learnability}
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan.
\newblock Learnability, stability, and uniform convergence.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 2010.

\bibitem[Sion(1958)]{sion1958minimax}
Maurice Sion.
\newblock On general minimax theorems.
\newblock \emph{Pacific J. Math.}, 8:\penalty0 171--176, 1958.

\bibitem[Sun et~al.(2019)Sun, Jiang, Krishnamurthy, Agarwal, and
  Langford]{sun2019model}
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock Model-based {RL} in contextual decision processes: {PAC} bounds and
  exponential improvements over model-free approaches.
\newblock In \emph{Conference on learning theory}, pages 2898--2933. PMLR,
  2019.

\bibitem[Vapnik(1995)]{vapnik1995nature}
Vladimir~N Vapnik.
\newblock The nature of statistical learning theory.
\newblock 1995.

\bibitem[Wang et~al.(2020)Wang, Salakhutdinov, and Yang]{wang2020provably}
Ruosong Wang, Russ~R Salakhutdinov, and Lin Yang.
\newblock Reinforcement learning with general value function approximation:
  Provably efficient approach via bounded eluder dimension.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Yang and Wang(2019)]{yang2019sample}
Lin Yang and Mengdi Wang.
\newblock Sample-optimal parametric {Q}-learning using linearly additive
  features.
\newblock In \emph{International Conference on Machine Learning}, pages
  6995--7004. PMLR, 2019.

\bibitem[Zhang(2006)]{zhang2006from}
Tong Zhang.
\newblock From $\epsilon$-entropy to {KL}-entropy: Analysis of minimum
  information complexity density estimation.
\newblock \emph{The Annals of Statistics}, 34\penalty0 (5):\penalty0
  2180--2210, 2006.

\bibitem[Zhou et~al.(2021)Zhou, Gu, and Szepesvari]{zhou2021nearly}
Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari.
\newblock Nearly minimax optimal reinforcement learning for linear mixture
  markov decision processes.
\newblock In \emph{Conference on Learning Theory}, pages 4532--4576. PMLR,
  2021.

\bibitem[Zimin and Neu(2013)]{zimin2013online}
Alexander Zimin and Gergely Neu.
\newblock Online learning in episodic markovian decision processes by relative
  entropy policy search.
\newblock \emph{Advances in neural information processing systems}, 26, 2013.

\end{thebibliography}
