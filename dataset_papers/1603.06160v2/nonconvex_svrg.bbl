\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal \& Bottou(2014)Agarwal and Bottou]{agarwal2014}
Agarwal, Alekh and Bottou, Leon.
\newblock A lower bound for the optimization of finite sums.
\newblock \emph{arXiv:1410.0723}, 2014.

\bibitem[Bertsekas(2011)]{bertsekas11.survey}
Bertsekas, Dimitri~P.
\newblock Incremental gradient, subgradient, and proximal methods for convex
  optimization: A survey.
\newblock In S.~Sra, S.~Nowozin, S.~Wright (ed.), \emph{Optimization for
  Machine Learning}. MIT Press, 2011.

\bibitem[Bottou(1991)]{bot91}
Bottou, L{\'e}on.
\newblock Stochastic gradient learning in neural networks.
\newblock \emph{Proceedings of Neuro-N{\i}mes}, 91\penalty0 (8), 1991.

\bibitem[Defazio et~al.(2014{\natexlab{a}})Defazio, Bach, and
  Lacoste-Julien]{Defazio14}
Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon.
\newblock {SAGA}: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{NIPS 27}, pp.\  1646--1654, 2014{\natexlab{a}}.

\bibitem[Defazio et~al.(2014{\natexlab{b}})Defazio, Caetano, and
  Domke]{defazio2014finito}
Defazio, Aaron~J, Caetano, Tib{\'e}rio~S, and Domke, Justin.
\newblock Finito: A faster, permutable incremental gradient method for big data
  problems.
\newblock \emph{arXiv:1407.2710}, 2014{\natexlab{b}}.

\bibitem[Dekel et~al.(2012)Dekel, Gilad-Bachrach, Shamir, and Xiao]{Dekel2012}
Dekel, Ofer, Gilad-Bachrach, Ran, Shamir, Ohad, and Xiao, Lin.
\newblock Optimal distributed online prediction using mini-batches.
\newblock \emph{The Journal of Machine Learning Research}, 13\penalty0
  (1):\penalty0 165--202, January 2012.
\newblock ISSN 1532-4435.

\bibitem[Ge et~al.(2015)Ge, Huang, Jin, and Yuan]{Ge15}
Ge, Rong, Huang, Furong, Jin, Chi, and Yuan, Yang.
\newblock Escaping from saddle points - online stochastic gradient for tensor
  decomposition.
\newblock In \emph{Proceedings of The 28th Conference on Learning Theory,
  {COLT} 2015}, pp.\  797--842, 2015.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{Ghadimi13}
Ghadimi, Saeed and Lan, Guanghui.
\newblock Stochastic first- and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{{SIAM} Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.
\newblock \doi{10.1137/120880811}.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{Glorot10}
Glorot, Xavier and Bengio, Yoshua.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{In Proceedings of the International Conference on Artificial
  Intelligence and Statistics (AISTATSâ€™10)}, 2010.

\bibitem[Hazan et~al.(2015)Hazan, Levy, and Shalev-Shwartz]{hazan2015}
Hazan, Elad, Levy, Kfir, and Shalev-Shwartz, Shai.
\newblock Beyond convexity: Stochastic quasi-convex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1585--1593, 2015.

\bibitem[Hong(2014)]{hong2014}
Hong, Mingyi.
\newblock A distributed, asynchronous and incremental algorithm for nonconvex
  optimization: An admm based approach.
\newblock \emph{arXiv preprint arXiv:1412.6058}, 2014.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{Johnson13}
Johnson, Rie and Zhang, Tong.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{NIPS 26}, pp.\  315--323, 2013.

\bibitem[Kone{\v c}n{\'y} \& Richt{\'a}rik(2013)Kone{\v c}n{\'y} and
  Richt{\'a}rik]{Konecny2013}
Kone{\v c}n{\'y}, Jakub and Richt{\'a}rik, Peter.
\newblock {S}emi-{S}tochastic {G}radient {D}escent {M}ethods.
\newblock \emph{arXiv:1312.1666}, 2013.

\bibitem[Kone{\v c}n{\'y} et~al.(2015)Kone{\v c}n{\'y}, Liu, Richt{\'a}rik, and
  Tak{\'a}{\v c}]{Konecny15}
Kone{\v c}n{\'y}, Jakub, Liu, Jie, Richt{\'a}rik, Peter, and Tak{\'a}{\v c},
  Martin.
\newblock {M}ini-{B}atch {S}emi-{S}tochastic {G}radient {D}escent in the
  {P}roximal {S}etting.
\newblock \emph{arXiv:1504.04407}, 2015.

\bibitem[Kushner \& Clark(2012)Kushner and Clark]{kushner2012}
Kushner, Harold~Joseph and Clark, Dean~S.
\newblock \emph{Stochastic approximation methods for constrained and
  unconstrained systems}, volume~26.
\newblock Springer Science \& Business Media, 2012.

\bibitem[{Lan} \& {Zhou}(2015){Lan} and {Zhou}]{lan2015}
{Lan}, Guanghui and {Zhou}, Yi.
\newblock An optimal randomized incremental gradient method.
\newblock \emph{arXiv:1507.02000}, 2015.

\bibitem[Li et~al.(2014)Li, Zhang, Chen, and Smola]{Likdd2014}
Li, Mu, Zhang, Tong, Chen, Yuqiang, and Smola, Alexander~J.
\newblock Efficient mini-batch training for stochastic optimization.
\newblock In \emph{Proceedings of the 20th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, KDD '14, pp.\  661--670. ACM, 2014.

\bibitem[Lian et~al.(2015)Lian, Huang, Li, and Liu]{Lian2015}
Lian, Xiangru, Huang, Yijun, Li, Yuncheng, and Liu, Ji.
\newblock {Asynchronous Parallel Stochastic Gradient for Nonconvex
  Optimization}.
\newblock In \emph{NIPS}, 2015.

\bibitem[Ljung(1977)]{ljung1977}
Ljung, Lennart.
\newblock Analysis of recursive stochastic algorithms.
\newblock \emph{Automatic Control, IEEE Transactions on}, 22\penalty0
  (4):\penalty0 551--575, 1977.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirov09}
Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on Optimization}, 19\penalty0 (4):\penalty0
  1574--1609, 2009.

\bibitem[Nemirovski \& Yudin(1983)Nemirovski and Yudin]{nemYud83}
Nemirovski, Arkadi and Yudin, D.
\newblock \emph{{Problem Complexity and Method Efficiency in Optimization}}.
\newblock John Wiley and Sons, 1983.

\bibitem[Nesterov(2003)]{nesterov03}
Nesterov, Yurii.
\newblock \emph{{Introductory Lectures On Convex Optimization: A Basic
  Course}}.
\newblock Springer, 2003.

\bibitem[Nesterov \& Polyak(2006)Nesterov and Polyak]{nesterov2006}
Nesterov, Yurii and Polyak, Boris~T.
\newblock Cubic regularization of newton method and its global performance.
\newblock \emph{Mathematical Programming}, 108\penalty0 (1):\penalty0 177--205,
  2006.

\bibitem[Poljak \& Tsypkin(1973)Poljak and Tsypkin]{poljak1973}
Poljak, BT and Tsypkin, Ya~Z.
\newblock Pseudogradient adaptation and training algorithms.
\newblock \emph{Automation and Remote Control}, 34:\penalty0 45--67, 1973.

\bibitem[Polyak(1963)]{Polyak1963}
Polyak, B.T.
\newblock Gradient methods for the minimisation of functionals.
\newblock \emph{{USSR} Computational Mathematics and Mathematical Physics},
  3\penalty0 (4):\penalty0 864--878, January 1963.

\bibitem[Reddi et~al.(2015)Reddi, Hefny, Sra, Poczos, and Smola]{Reddi2015}
Reddi, Sashank, Hefny, Ahmed, Sra, Suvrit, Poczos, Barnabas, and Smola, Alex~J.
\newblock On variance reduction in stochastic gradient descent and its
  asynchronous variants.
\newblock In \emph{NIPS 28}, pp.\  2629--2637, 2015.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{RobMon51}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{Annals of Mathematical Statistics}, 22:\penalty0 400--407,
  1951.

\bibitem[Schmidt et~al.(2013)Schmidt, Roux, and Bach]{Schmidt13}
Schmidt, Mark~W., Roux, Nicolas~Le, and Bach, Francis~R.
\newblock {Minimizing Finite Sums with the Stochastic Average Gradient}.
\newblock \emph{arXiv:1309.2388}, 2013.

\bibitem[Shalev{-}Shwartz(2015)]{Shwartz15}
Shalev{-}Shwartz, Shai.
\newblock {SDCA} without duality.
\newblock \emph{CoRR}, abs/1502.06177, 2015.

\bibitem[Shalev-Shwartz \& Zhang(2013)Shalev-Shwartz and Zhang]{sdca}
Shalev-Shwartz, Shai and Zhang, Tong.
\newblock Stochastic dual coordinate ascent methods for regularized loss.
\newblock \emph{The Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 567--599, 2013.

\bibitem[Shamir(2014)]{shamir2014stochastic}
Shamir, Ohad.
\newblock A stochastic {PCA} and {SVD} algorithm with an exponential
  convergence rate.
\newblock \emph{arXiv:1409.2848}, 2014.

\bibitem[Shamir(2015)]{shamir2015fast}
Shamir, Ohad.
\newblock Fast stochastic algorithms for {SVD} and {PCA}: {C}onvergence
  properties and convexity.
\newblock \emph{arXiv:1507.08788}, 2015.

\bibitem[Sra(2012)]{Sra2012}
Sra, Suvrit.
\newblock Scalable nonconvex inexact proximal splitting.
\newblock In \emph{NIPS}, pp.\  530--538, 2012.

\bibitem[Xiao \& Zhang(2014)Xiao and Zhang]{Xiao14}
Xiao, Lin and Zhang, Tong.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock \emph{{SIAM} Journal on Optimization}, 24\penalty0 (4):\penalty0
  2057--2075, 2014.

\bibitem[Zhu \& Yuan(2015)Zhu and Yuan]{Zhu15}
Zhu, Zeyuan~Allen and Yuan, Yang.
\newblock Univr: {A} universal variance reduction framework for proximal
  stochastic gradient method.
\newblock \emph{CoRR}, abs/1506.01972, 2015.

\end{thebibliography}
