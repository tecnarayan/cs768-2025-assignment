\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Efroni et~al.(2020)Efroni, Mannor, and Pirotta]{efroni2020exploration}
Yonathan Efroni, Shie Mannor, and Matteo Pirotta.
\newblock Exploration-exploitation in constrained mdps.
\newblock \emph{arXiv preprint arXiv:2003.02189}, 2020.

\bibitem[Singh et~al.(2020)Singh, Gupta, and Shroff]{singh2020learning}
Rahul Singh, Abhishek Gupta, and Ness~B Shroff.
\newblock Learning in markov decision processes under constraints.
\newblock \emph{arXiv preprint arXiv:2002.12435}, 2020.

\bibitem[Brantley et~al.(2020)Brantley, Dudik, Lykouris, Miryoosefi,
  Simchowitz, Slivkins, and Sun]{brantley2020constrained}
Kiant{\'e} Brantley, Miroslav Dudik, Thodoris Lykouris, Sobhan Miryoosefi, Max
  Simchowitz, Aleksandrs Slivkins, and Wen Sun.
\newblock Constrained episodic reinforcement learning in concave-convex and
  knapsack settings.
\newblock \emph{arXiv preprint arXiv:2006.05051}, 2020.

\bibitem[Zheng and Ratliff(2020)]{zheng2020constrained}
Liyuan Zheng and Lillian Ratliff.
\newblock Constrained upper confidence reinforcement learning.
\newblock In \emph{Learning for Dynamics and Control}, pages 620--629. PMLR,
  2020.

\bibitem[Kalagarla et~al.(2020)Kalagarla, Jain, and Nuzzo]{kalagarla2020sample}
Krishna~C Kalagarla, Rahul Jain, and Pierluigi Nuzzo.
\newblock A sample-efficient algorithm for episodic finite-horizon mdp with
  constraints.
\newblock \emph{arXiv preprint arXiv:2009.11348}, 2020.

\bibitem[Liu et~al.(2021)Liu, Zhou, Kalathil, Kumar, and Tian]{liu2021learning}
Tao Liu, Ruida Zhou, Dileep Kalathil, PR~Kumar, and Chao Tian.
\newblock Learning policies with zero or bounded constraint violation for
  constrained mdps.
\newblock \emph{arXiv preprint arXiv:2106.02684}, 2021.

\bibitem[Ding et~al.(2021)Ding, Wei, Yang, Wang, and
  Jovanovic]{ding2021provably}
Dongsheng Ding, Xiaohan Wei, Zhuoran Yang, Zhaoran Wang, and Mihailo Jovanovic.
\newblock Provably efficient safe exploration via primal-dual policy
  optimization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3304--3312. PMLR, 2021.

\bibitem[Xu et~al.(2021)Xu, Liang, and Lan]{xu2021crpo}
Tengyu Xu, Yingbin Liang, and Guanghui Lan.
\newblock Crpo: A new approach for safe reinforcement learning with convergence
  guarantee.
\newblock In \emph{International Conference on Machine Learning}, pages
  11480--11491. PMLR, 2021.

\bibitem[Ding et~al.(2020)Ding, Zhang, Basar, and Jovanovic]{ding2020natural}
Dongsheng Ding, Kaiqing Zhang, Tamer Basar, and Mihailo~R Jovanovic.
\newblock Natural policy gradient primal-dual method for constrained markov
  decision processes.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Bai et~al.(2021)Bai, Bedi, Agarwal, Koppel, and
  Aggarwal]{bai2021achieving}
Qinbo Bai, Amrit~Singh Bedi, Mridul Agarwal, Alec Koppel, and Vaneet Aggarwal.
\newblock Achieving zero constraint violation for constrained reinforcement
  learning via primal-dual approach.
\newblock \emph{arXiv preprint arXiv:2109.06332}, 2021.

\bibitem[Koenig and Simmons(1993)]{koenig1993complexity}
Sven Koenig and Reid~G Simmons.
\newblock Complexity analysis of real-time reinforcement learning.
\newblock In \emph{AAAI}, pages 99--107, 1993.

\bibitem[Azar et~al.(2012)Azar, Munos, and Kappen]{azar2012sample}
Mohammad~Gheshlaghi Azar, R{\'e}mi Munos, and Bert Kappen.
\newblock On the sample complexity of reinforcement learning with a generative
  model.
\newblock \emph{arXiv preprint arXiv:1206.6461}, 2012.

\bibitem[Wei et~al.(2021)Wei, Liu, and Ying]{wei2021provably}
Honghao Wei, Xin Liu, and Lei Ying.
\newblock A provably-efficient model-free algorithm for constrained markov
  decision processes.
\newblock \emph{arXiv preprint arXiv:2106.01577}, 2021.

\bibitem[Jin et~al.(2020)Jin, Yang, Wang, and Jordan]{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pages 2137--2143. PMLR,
  2020.

\bibitem[Amani et~al.(2021)Amani, Thrampoulidis, and Yang]{amani2021safe}
Sanae Amani, Christos Thrampoulidis, and Lin~F Yang.
\newblock Safe reinforcement learning with linear function approximation.
\newblock \emph{arXiv preprint arXiv:2106.06239}, 2021.

\bibitem[Yang and Wang(2019)]{yang2019sample}
Lin Yang and Mengdi Wang.
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock In \emph{International Conference on Machine Learning}, pages
  6995--7004. PMLR, 2019.

\bibitem[Zhou et~al.(2021)Zhou, He, and Gu]{zhou2021provably}
Dongruo Zhou, Jiafan He, and Quanquan Gu.
\newblock Provably efficient reinforcement learning for discounted mdps with
  feature mapping.
\newblock In \emph{International Conference on Machine Learning}, pages
  12793--12802. PMLR, 2021.

\bibitem[Agarwal et~al.(2021)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2021theory}
Alekh Agarwal, Sham~M Kakade, Jason~D Lee, and Gaurav Mahajan.
\newblock On the theory of policy gradient methods: Optimality, approximation,
  and distribution shift.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (98):\penalty0 1--76, 2021.

\bibitem[Paternain et~al.(2019)Paternain, Calvo-Fullana, Chamon, and
  Ribeiro]{paternain2019safe}
Santiago Paternain, Miguel Calvo-Fullana, Luiz~FO Chamon, and Alejandro
  Ribeiro.
\newblock Safe policies for reinforcement learning via primal-dual methods.
\newblock \emph{arXiv preprint arXiv:1911.09101}, 2019.

\bibitem[Qiu et~al.(2020)Qiu, Wei, Yang, Ye, and Wang]{qiu2020upper}
Shuang Qiu, Xiaohan Wei, Zhuoran Yang, Jieping Ye, and Zhaoran Wang.
\newblock Upper confidence primal-dual optimization: Stochastically constrained
  markov decision processes with adversarial losses and unknown transitions.
\newblock \emph{arXiv preprint arXiv:2003.00660}, 2020.

\bibitem[Besson and Kaufmann(2018)]{besson2018doubling}
Lilian Besson and Emilie Kaufmann.
\newblock What doubling tricks can and can't do for multi-armed bandits.
\newblock \emph{arXiv preprint arXiv:1803.06971}, 2018.

\bibitem[Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang, and
  Yang]{ayoub2020model}
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \emph{International Conference on Machine Learning}, pages
  463--474. PMLR, 2020.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is q-learning provably efficient?
\newblock \emph{arXiv preprint arXiv:1807.03765}, 2018.

\bibitem[Miryoosefi and Jin(2022)]{miryoosefi2022simple}
Sobhan Miryoosefi and Chi Jin.
\newblock A simple reward-free approach to constrained reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  15666--15698. PMLR, 2022.

\bibitem[Modi et~al.(2021)Modi, Chen, Krishnamurthy, Jiang, and
  Agarwal]{modi2021model}
Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal.
\newblock Model-free representation learning and exploration in low-rank mdps.
\newblock \emph{arXiv preprint arXiv:2102.07035}, 2021.

\bibitem[Zhang et~al.(2022)Zhang, Song, Uehara, Wang, Agarwal, and
  Sun]{zhang2022efficient}
Xuezhou Zhang, Yuda Song, Masatoshi Uehara, Mengdi Wang, Alekh Agarwal, and Wen
  Sun.
\newblock Efficient reinforcement learning in block mdps: A model-free
  representation learning approach.
\newblock In \emph{International Conference on Machine Learning}, pages
  26517--26547. PMLR, 2022.

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Krishnamurthy, and
  Sun]{agarwal2020flambe}
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun.
\newblock Flambe: Structural complexity and representation learning of low rank
  mdps.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 20095--20107, 2020.

\bibitem[Pan et~al.(2019)Pan, Cai, Meng, Chen, Huang, and
  Liu]{pan2019reinforcement}
Ling Pan, Qingpeng Cai, Qi~Meng, Wei Chen, Longbo Huang, and Tie-Yan Liu.
\newblock Reinforcement learning with dynamic boltzmann softmax updates.
\newblock \emph{arXiv preprint arXiv:1903.05926}, 2019.

\bibitem[Epasto et~al.(2020)Epasto, Mahdian, Mirrokni, and
  Zampetakis]{epasto2020optimal}
Alessandro Epasto, Mohammad Mahdian, Vahab Mirrokni, and Manolis Zampetakis.
\newblock Optimal approximation--smoothness tradeoffs for soft-max functions.
\newblock \emph{arXiv preprint arXiv:2010.11450}, 2020.

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and
  Szepesv{\'a}ri]{abbasi2011improved}
Yasin Abbasi-Yadkori, D{\'a}vid P{\'a}l, and Csaba Szepesv{\'a}ri.
\newblock Improved algorithms for linear stochastic bandits.
\newblock \emph{Advances in neural information processing systems},
  24:\penalty0 2312--2320, 2011.

\bibitem[Vershynin(2010)]{vershynin2010introduction}
Roman Vershynin.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock \emph{arXiv preprint arXiv:1011.3027}, 2010.

\bibitem[Vaswani et~al.(2022)Vaswani, Yang, and
  Szepesv{\'a}ri]{vaswani2022near}
Sharan Vaswani, Lin~F Yang, and Csaba Szepesv{\'a}ri.
\newblock Near-optimal sample complexity bounds for constrained mdps.
\newblock \emph{arXiv preprint arXiv:2206.06270}, 2022.

\bibitem[Hajek(1982)]{hajek1982hitting}
Bruce Hajek.
\newblock Hitting-time and occupation-time bounds implied by drift analysis
  with applications.
\newblock \emph{Advances in Applied probability}, 14\penalty0 (3):\penalty0
  502--525, 1982.

\bibitem[Xie et~al.(2020)Xie, Chen, Wang, and Yang]{xie2020learning}
Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang.
\newblock Learning zero-sum simultaneous-move markov games using function
  approximation and correlated equilibrium.
\newblock \emph{arXiv preprint arXiv:2002.07066}, 2020.

\bibitem[Ding and Lavaei(2022)]{ding2022provably}
Yuhao Ding and Javad Lavaei.
\newblock Provably efficient primal-dual reinforcement learning for cmdps with
  non-stationary objectives and constraints.
\newblock \emph{arXiv preprint arXiv:2201.11965}, 2022.

\end{thebibliography}
