\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, et~al.]{flamingo}
J.-B. Alayrac, J.~Donahue, P.~Luc, A.~Miech, I.~Barr, Y.~Hasson, K.~Lenc, A.~Mensch, K.~Millican, M.~Reynolds, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{arXiv preprint arXiv:2204.14198}, 2022.

\bibitem[Baker et~al.(2022)Baker, Akkaya, Zhokhov, Huizinga, Tang, Ecoffet, Houghton, Sampedro, and Clune]{vpt}
B.~Baker, I.~Akkaya, P.~Zhokhov, J.~Huizinga, J.~Tang, A.~Ecoffet, B.~Houghton, R.~Sampedro, and J.~Clune.
\newblock Video pretraining (vpt): Learning to act by watching unlabeled online videos.
\newblock \emph{arXiv preprint arXiv:2206.11795}, 2022.

\bibitem[Bavishi et~al.(2023)Bavishi, Elsen, Hawthorne, Nye, Odena, Somani, and Ta\c{s}\i{}rlar]{fuyu-8b}
R.~Bavishi, E.~Elsen, C.~Hawthorne, M.~Nye, A.~Odena, A.~Somani, and S.~Ta\c{s}\i{}rlar.
\newblock Introducing our multimodal models, 2023.
\newblock URL \url{https://www.adept.ai/blog/fuyu-8b}.

\bibitem[Brohan et~al.(2022{\natexlab{a}})Brohan, Brown, Carbajal, Chebotar, Dabis, Finn, Gopalakrishnan, Hausman, Herzog, Hsu, et~al.]{rt1}
A.~Brohan, N.~Brown, J.~Carbajal, Y.~Chebotar, J.~Dabis, C.~Finn, K.~Gopalakrishnan, K.~Hausman, A.~Herzog, J.~Hsu, et~al.
\newblock Rt-1: Robotics transformer for real-world control at scale.
\newblock \emph{arXiv preprint arXiv:2212.06817}, 2022{\natexlab{a}}.

\bibitem[Brohan et~al.(2022{\natexlab{b}})Brohan, Chebotar, Finn, Hausman, Herzog, Ho, Ibarz, Irpan, Jang, Julian, et~al.]{saycan}
A.~Brohan, Y.~Chebotar, C.~Finn, K.~Hausman, A.~Herzog, D.~Ho, J.~Ibarz, A.~Irpan, E.~Jang, R.~Julian, et~al.
\newblock Do as i can, not as i say: Grounding language in robotic affordances.
\newblock In \emph{6th Annual Conference on Robot Learning}, 2022{\natexlab{b}}.

\bibitem[Brohan et~al.(2023)Brohan, Brown, Carbajal, Chebotar, Chen, Choromanski, Ding, Driess, Dubey, Finn, et~al.]{rt2}
A.~Brohan, N.~Brown, J.~Carbajal, Y.~Chebotar, X.~Chen, K.~Choromanski, T.~Ding, D.~Driess, A.~Dubey, C.~Finn, et~al.
\newblock Rt-2: Vision-language-action models transfer web knowledge to robotic control.
\newblock \emph{arXiv preprint arXiv:2307.15818}, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{gpt3}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal, A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Burda et~al.(2018)Burda, Edwards, Storkey, and Klimov]{burda2018exploration}
Y.~Burda, H.~Edwards, A.~Storkey, and O.~Klimov.
\newblock Exploration by random network distillation.
\newblock \emph{arXiv preprint arXiv:1810.12894}, 2018.

\bibitem[Cai et~al.(2023{\natexlab{a}})Cai, Wang, Ma, Liu, and Liang]{shaofei}
S.~Cai, Z.~Wang, X.~Ma, A.~Liu, and Y.~Liang.
\newblock Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction.
\newblock \emph{arXiv preprint arXiv:2301.10034}, 2023{\natexlab{a}}.

\bibitem[Cai et~al.(2023{\natexlab{b}})Cai, Zhang, Wang, Ma, Liu, and Liang]{groot}
S.~Cai, B.~Zhang, Z.~Wang, X.~Ma, A.~Liu, and Y.~Liang.
\newblock Groot: Learning to follow instructions by watching gameplay videos.
\newblock \emph{arXiv preprint arXiv:2310.08235}, 2023{\natexlab{b}}.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel, Srinivas, and Mordatch]{chen2021decision}
L.~Chen, K.~Lu, A.~Rajeswaran, K.~Lee, A.~Grover, M.~Laskin, P.~Abbeel, A.~Srinivas, and I.~Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 15084--15097, 2021.

\bibitem[Chen et~al.(2023)Chen, Li, Dong, Zhang, He, Wang, Zhao, and Lin]{sharegpt4v}
L.~Chen, J.~Li, X.~Dong, P.~Zhang, C.~He, J.~Wang, F.~Zhao, and D.~Lin.
\newblock Sharegpt4v: Improving large multi-modal models with better captions.
\newblock \emph{arXiv preprint arXiv:2311.12793}, 2023.

\bibitem[Cheng et~al.(2024)Cheng, Zhang, Zhang, Meng, Hong, Li, Wang, Wang, Yin, Zhao, and He]{cheng2024exploring}
Y.~Cheng, C.~Zhang, Z.~Zhang, X.~Meng, S.~Hong, W.~Li, Z.~Wang, Z.~Wang, F.~Yin, J.~Zhao, and X.~He.
\newblock Exploring large language model based intelligent agents: Definitions, methods, and prospects.
\newblock \emph{arXiv preprint arXiv: 2401.03428}, 2024.

\bibitem[Driess et~al.(2023)Driess, Xia, Sajjadi, Lynch, Chowdhery, Ichter, Wahid, Tompson, Vuong, Yu, et~al.]{palme}
D.~Driess, F.~Xia, M.~S. Sajjadi, C.~Lynch, A.~Chowdhery, B.~Ichter, A.~Wahid, J.~Tompson, Q.~Vuong, T.~Yu, et~al.
\newblock Palm-e: An embodied multimodal language model.
\newblock \emph{arXiv preprint arXiv:2303.03378}, 2023.

\bibitem[Durante et~al.(2024)Durante, Sarkar, Gong, Taori, Noda, Tang, Adeli, Lakshmikanth, Schulman, Milstein, Terzopoulos, Famoti, Kuno, Llorens, Vo, Ikeuchi, Fei-Fei, Gao, Wake, and Huang]{durante2024interactive}
Z.~Durante, B.~Sarkar, R.~Gong, R.~Taori, Y.~Noda, P.~Tang, E.~Adeli, S.~K. Lakshmikanth, K.~Schulman, A.~Milstein, D.~Terzopoulos, A.~Famoti, N.~Kuno, A.~Llorens, H.~Vo, K.~Ikeuchi, L.~Fei-Fei, J.~Gao, N.~Wake, and Q.~Huang.
\newblock An interactive agent foundation model.
\newblock \emph{arXiv preprint arXiv: 2402.05929}, 2024.

\bibitem[Fan et~al.(2022)Fan, Wang, Jiang, Mandlekar, Yang, Zhu, Tang, Huang, Zhu, and Anandkumar]{minedojo}
L.~Fan, G.~Wang, Y.~Jiang, A.~Mandlekar, Y.~Yang, H.~Zhu, A.~Tang, D.-A. Huang, Y.~Zhu, and A.~Anandkumar.
\newblock Minedojo: Building open-ended embodied agents with internet-scale knowledge.
\newblock \emph{Advances in Neural Information Processing Systems Datasets and Benchmarks}, 2022.

\bibitem[Gong et~al.(2023)Gong, Huang, Ma, Vo, Durante, Noda, Zheng, Zhu, Terzopoulos, Fei-Fei, et~al.]{jxma_llm1}
R.~Gong, Q.~Huang, X.~Ma, H.~Vo, Z.~Durante, Y.~Noda, Z.~Zheng, S.-C. Zhu, D.~Terzopoulos, L.~Fei-Fei, et~al.
\newblock Mindagent: Emergent gaming interaction.
\newblock \emph{arXiv preprint arXiv:2309.09971}, 2023.

\bibitem[Guss et~al.(2019)Guss, Houghton, Topin, Wang, Codel, Veloso, and Salakhutdinov]{minerl}
W.~H. Guss, B.~Houghton, N.~Topin, P.~Wang, C.~Codel, M.~Veloso, and R.~Salakhutdinov.
\newblock Minerl: A large-scale dataset of minecraft demonstrations.
\newblock \emph{arXiv preprint arXiv:1907.13440}, 2019.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and Hochreiter]{FID}
M.~Heusel, H.~Ramsauer, T.~Unterthiner, B.~Nessler, and S.~Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash equilibrium.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus, S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Hinck et~al.(2024)Hinck, Olson, Cobbley, Tseng, and Lal]{llava-gemma}
M.~Hinck, M.~L. Olson, D.~Cobbley, S.-Y. Tseng, and V.~Lal.
\newblock Llava-gemma: Accelerating multimodal foundation models with a compact language model.
\newblock \emph{arXiv preprint arXiv:2404.01331}, 2024.

\bibitem[Hu and Clune(2024)]{thoughtcloning}
S.~Hu and J.~Clune.
\newblock Thought cloning: Learning to think while acting by imitating human thinking.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Huang et~al.(2023)Huang, Ma, Yong, Linghu, et~al.]{leo}
J.~Huang, X.~Ma, S.~Yong, X.~Linghu, et~al.
\newblock An embodied generalist agent in 3d world.
\newblock \emph{arXiv preprint arXiv:2311.12871}, 2023.

\bibitem[Huang et~al.(2022{\natexlab{a}})Huang, Abbeel, Pathak, and Mordatch]{huang2022language}
W.~Huang, P.~Abbeel, D.~Pathak, and I.~Mordatch.
\newblock Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.
\newblock \emph{ICML}, 2022{\natexlab{a}}.

\bibitem[Huang et~al.(2022{\natexlab{b}})Huang, Xia, Xiao, Chan, Liang, Florence, Zeng, Tompson, Mordatch, Chebotar, et~al.]{innermonologue}
W.~Huang, F.~Xia, T.~Xiao, H.~Chan, J.~Liang, P.~Florence, A.~Zeng, J.~Tompson, I.~Mordatch, Y.~Chebotar, et~al.
\newblock Inner monologue: Embodied reasoning through planning with language models.
\newblock \emph{arXiv preprint arXiv:2207.05608}, 2022{\natexlab{b}}.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
J.~Kaplan, S.~McCandlish, T.~Henighan, T.~B. Brown, B.~Chess, R.~Child, S.~Gray, A.~Radford, J.~Wu, and D.~Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, K{\"u}ttler, Lewis, Yih, Rockt{\"a}schel, et~al.]{rag}
P.~Lewis, E.~Perez, A.~Piktus, F.~Petroni, V.~Karpukhin, N.~Goyal, H.~K{\"u}ttler, M.~Lewis, W.-t. Yih, T.~Rockt{\"a}schel, et~al.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 9459--9474, 2020.

\bibitem[Liang et~al.(2022)Liang, Huang, Xia, Xu, Hausman, Ichter, Florence, and Zeng]{codeaspolicies}
J.~Liang, W.~Huang, F.~Xia, P.~Xu, K.~Hausman, B.~Ichter, P.~Florence, and A.~Zeng.
\newblock Code as policies: Language model programs for embodied control.
\newblock \emph{arXiv preprint arXiv:2209.07753}, 2022.

\bibitem[Lifshitz et~al.(2023)Lifshitz, Paster, Chan, Ba, and McIlraith]{steve1}
S.~Lifshitz, K.~Paster, H.~Chan, J.~Ba, and S.~McIlraith.
\newblock Steve-1: A generative model for text-to-behavior in minecraft.
\newblock \emph{arXiv preprint arXiv:2306.00937}, 2023.

\bibitem[Lin et~al.(2023)Lin, Wang, Ma, and Liang]{mcu}
H.~Lin, Z.~Wang, J.~Ma, and Y.~Liang.
\newblock Mcu: A task-centric framework for open-ended agent evaluation in minecraft.
\newblock \emph{arXiv preprint arXiv:2310.08367}, 2023.

\bibitem[Lin et~al.(2024)Lin, Huang, Ye, Chen, Wang, Li, Ma, Wan, Zou, and Liang]{lin2024selecting}
H.~Lin, B.~Huang, H.~Ye, Q.~Chen, Z.~Wang, S.~Li, J.~Ma, X.~Wan, J.~Zou, and Y.~Liang.
\newblock Selecting large language model to fine-tune via rectified scaling law.
\newblock \emph{arXiv preprint arXiv:2402.02314}, 2024.

\bibitem[Liu et~al.(2024)Liu, Li, Wu, and Lee]{llava}
H.~Liu, C.~Li, Q.~Wu, and Y.~J. Lee.
\newblock Visual instruction tuning.
\newblock \emph{Advances in neural information processing systems}, 36, 2024.

\bibitem[Ma et~al.(2022)Ma, Yong, Zheng, Li, Liang, Zhu, and Huang]{jxma_eai1}
X.~Ma, S.~Yong, Z.~Zheng, Q.~Li, Y.~Liang, S.-C. Zhu, and S.~Huang.
\newblock Sqa3d: Situated question answering in 3d scenes.
\newblock \emph{arXiv preprint arXiv:2210.07474}, 2022.

\bibitem[Man et~al.(2024)Man, Gui, and Wang]{man2024situational}
Y.~Man, L.-Y. Gui, and Y.-X. Wang.
\newblock Situational awareness matters in 3d vision language reasoning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 13678--13688, 2024.

\bibitem[Mentzer et~al.(2023)Mentzer, Minnen, Agustsson, and Tschannen]{mentzer2023finite}
F.~Mentzer, D.~Minnen, E.~Agustsson, and M.~Tschannen.
\newblock Finite scalar quantization: Vq-vae made simple.
\newblock \emph{arXiv preprint arXiv:2309.15505}, 2023.

\bibitem[OpenAI(2023)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Qin et~al.(2023)Qin, Zhou, Liu, Yin, Sheng, Zhang, Qiao, and Shao]{mp5}
Y.~Qin, E.~Zhou, Q.~Liu, Z.~Yin, L.~Sheng, R.~Zhang, Y.~Qiao, and J.~Shao.
\newblock Mp5: A multi-modal open-ended embodied system in minecraft via active perception.
\newblock \emph{arXiv preprint arXiv:2312.07472}, 2023.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{t5}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou, W.~Li, and P.~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of machine learning research}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Reed et~al.(2022)Reed, Zolna, Parisotto, Colmenarejo, Novikov, Barth-Maron, Gimenez, Sulsky, Kay, Springenberg, et~al.]{gato}
S.~Reed, K.~Zolna, E.~Parisotto, S.~G. Colmenarejo, A.~Novikov, G.~Barth-Maron, M.~Gimenez, Y.~Sulsky, J.~Kay, J.~T. Springenberg, et~al.
\newblock A generalist agent.
\newblock \emph{arXiv preprint arXiv:2205.06175}, 2022.

\bibitem[Shinn et~al.(2023)Shinn, Labash, and Gopinath]{reflexion}
N.~Shinn, B.~Labash, and A.~Gopinath.
\newblock Reflexion: an autonomous agent with dynamic memory and self-reflection.
\newblock \emph{arXiv preprint arXiv:2303.11366}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{llama2}
H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei, N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Unterthiner et~al.(2018)Unterthiner, Van~Steenkiste, Kurach, Marinier, Michalski, and Gelly]{FVD}
T.~Unterthiner, S.~Van~Steenkiste, K.~Kurach, R.~Marinier, M.~Michalski, and S.~Gelly.
\newblock Towards accurate generative models of video: A new metric \& challenges.
\newblock \emph{arXiv preprint arXiv:1812.01717}, 2018.

\bibitem[Van Den~Oord et~al.(2017)Van Den~Oord, Vinyals, et~al.]{vq}
A.~Van Den~Oord, O.~Vinyals, et~al.
\newblock Neural discrete representation learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Vinyals et~al.(2015)Vinyals, Toshev, Bengio, and Erhan]{vinyals2015show}
O.~Vinyals, A.~Toshev, S.~Bengio, and D.~Erhan.
\newblock Show and tell: A neural image caption generator.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 3156--3164, 2015.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Xie, Jiang, Mandlekar, Xiao, Zhu, Fan, and Anandkumar]{voyager}
G.~Wang, Y.~Xie, Y.~Jiang, A.~Mandlekar, C.~Xiao, Y.~Zhu, L.~Fan, and A.~Anandkumar.
\newblock Voyager: An open-ended embodied agent with large language models.
\newblock \emph{arXiv preprint arXiv:2305.16291}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2022)Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi]{selfinstruct}
Y.~Wang, Y.~Kordi, S.~Mishra, A.~Liu, N.~A. Smith, D.~Khashabi, and H.~Hajishirzi.
\newblock Self-instruct: Aligning language models with self-generated instructions, 2022.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Cai, Chen, Liu, Ma, Liang, and CraftJarvis]{deps}
Z.~Wang, S.~Cai, G.~Chen, A.~Liu, X.~Ma, Y.~Liang, and T.~CraftJarvis.
\newblock Describe, explain, plan and select: interactive planning with large language models enables open-world multi-task agents.
\newblock In \emph{Proceedings of the 37th International Conference on Neural Information Processing Systems}, pages 34153--34189, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2023{\natexlab{c}})Wang, Cai, Liu, Jin, Hou, Zhang, Lin, He, Zheng, Yang, Ma, and Liang]{jarvis1}
Z.~Wang, S.~Cai, A.~Liu, Y.~Jin, J.~Hou, B.~Zhang, H.~Lin, Z.~He, Z.~Zheng, Y.~Yang, X.~Ma, and Y.~Liang.
\newblock Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models.
\newblock \emph{arXiv preprint arXiv: 2311.05997}, 2023{\natexlab{c}}.

\bibitem[Wang et~al.(2024)Wang, Liu, Lin, Li, Ma, and Liang]{rat}
Z.~Wang, A.~Liu, H.~Lin, J.~Li, X.~Ma, and Y.~Liang.
\newblock Rat: Retrieval augmented thoughts elicit context-aware reasoning in long-horizon generation.
\newblock \emph{arXiv preprint arXiv:2403.05313}, 2024.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Chi, Le, and Zhou]{chainofthought}
J.~Wei, X.~Wang, D.~Schuurmans, M.~Bosma, E.~Chi, Q.~Le, and D.~Zhou.
\newblock Chain of thought prompting elicits reasoning in large language models.
\newblock \emph{36th Conference on Neural Information Processing Systems (NeurIPS 2022)}, 2022.

\bibitem[Yao et~al.(2022)Yao, Zhao, Yu, Du, Shafran, Narasimhan, and Cao]{react}
S.~Yao, J.~Zhao, D.~Yu, N.~Du, I.~Shafran, K.~Narasimhan, and Y.~Cao.
\newblock React: Synergizing reasoning and acting in language models.
\newblock \emph{arXiv preprint arXiv:2210.03629}, 2022.

\bibitem[Yuan et~al.(2023)Yuan, Zhang, Wang, Xie, Cai, Dong, and Lu]{plan4mc}
H.~Yuan, C.~Zhang, H.~Wang, F.~Xie, P.~Cai, H.~Dong, and Z.~Lu.
\newblock Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks.
\newblock \emph{arXiv preprint arXiv:2303.16563}, 2023.

\bibitem[Zhang et~al.(2023)Zhang, Yang, Hu, Wang, Li, Sun, Zhang, Zhang, Liu, Zhu, et~al.]{proagent}
C.~Zhang, K.~Yang, S.~Hu, Z.~Wang, G.~Li, Y.~Sun, C.~Zhang, Z.~Zhang, A.~Liu, S.-C. Zhu, et~al.
\newblock Proagent: Building proactive cooperative ai with large language models.
\newblock \emph{arXiv preprint arXiv:2308.11339}, 2023.

\bibitem[Zhao et~al.(2023)Zhao, Cai, Si, Ma, An, Chen, Liu, Wang, Han, and Chang]{jxma_vl1}
H.~Zhao, Z.~Cai, S.~Si, X.~Ma, K.~An, L.~Chen, Z.~Liu, S.~Wang, W.~Han, and B.~Chang.
\newblock Mmicl: Empowering vision-language model with multi-modal in-context learning.
\newblock \emph{arXiv preprint arXiv:2309.07915}, 2023.

\bibitem[Zheng et~al.(2023)Zheng, Feng, Lu, et~al.]{steveeye}
S.~Zheng, Y.~Feng, Z.~Lu, et~al.
\newblock Steve-eye: Equipping llm-based embodied agents with visual perception in open worlds.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[Zhu et~al.(2023)Zhu, Chen, Tian, Tao, Su, Yang, Huang, Li, Lu, Wang, et~al.]{gitm}
X.~Zhu, Y.~Chen, H.~Tian, C.~Tao, W.~Su, C.~Yang, G.~Huang, B.~Li, L.~Lu, X.~Wang, et~al.
\newblock Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory.
\newblock \emph{arXiv preprint arXiv:2305.17144}, 2023.

\end{thebibliography}
