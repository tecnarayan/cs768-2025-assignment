\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Antos et~al.(2008)Antos, Szepesv{\'a}ri, and Munos]{antos2008fitted}
Antos, A., Szepesv{\'a}ri, C., and Munos, R.
\newblock Fitted {Q}-iteration in continuous action-space {MDP}s.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, pp.\  9--16, 2008.

\bibitem[Auer et~al.(2009)Auer, Jaksch, and Ortner]{auer2009near}
Auer, P., Jaksch, T., and Ortner, R.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, pp.\  89--96, 2009.

\bibitem[Bai \& Jin(2020)Bai and Jin]{bai2020provable}
Bai, Y. and Jin, C.
\newblock Provable self-play algorithms for competitive reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  551--560, 2020.

\bibitem[Ba{\c{s}}ar \& Bernhard(2008)Ba{\c{s}}ar and Bernhard]{bacsar2008h}
Ba{\c{s}}ar, T. and Bernhard, P.
\newblock \emph{H-infinity optimal control and related minimax design problems:
  a dynamic game approach}.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Bemporad et~al.(2003)Bemporad, Borrelli, and Morari]{bemporad2003min}
Bemporad, A., Borrelli, F., and Morari, M.
\newblock Min-max control of constrained uncertain discrete-time linear
  systems.
\newblock \emph{IEEE Transactions on automatic control}, 48\penalty0
  (9):\penalty0 1600--1606, 2003.

\bibitem[Bogunovic et~al.(2018)Bogunovic, Scarlett, Jegelka, and
  Cevher]{bogunovic2018adversarially}
Bogunovic, I., Scarlett, J., Jegelka, S., and Cevher, V.
\newblock Adversarially robust optimization with {G}aussian processes.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, pp.\  5760--5770, 2018.

\bibitem[Camacho \& Alba(2013)Camacho and Alba]{camacho2013model}
Camacho, E.~F. and Alba, C.~B.
\newblock \emph{Model predictive control}.
\newblock Springer science \& business media, 2013.

\bibitem[Chowdhury \& Gopalan(2017)Chowdhury and
  Gopalan]{chowdhury2017kernelized}
Chowdhury, S.~R. and Gopalan, A.
\newblock On kernelized multi-armed bandits.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  844--853, 2017.

\bibitem[Chowdhury \& Gopalan(2019)Chowdhury and Gopalan]{chowdhury2019online}
Chowdhury, S.~R. and Gopalan, A.
\newblock Online learning in kernelized {M}arkov decision processes.
\newblock In \emph{Conference on Artificial Intelligence and Statistics
  (AISTATS)}, pp.\  3197--3205, 2019.

\bibitem[Chua et~al.(2018)Chua, Calandra, McAllister, and Levine]{chua2018deep}
Chua, K., Calandra, R., McAllister, R., and Levine, S.
\newblock Deep reinforcement learning in a handful of trials using
  probabilistic dynamics models.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, pp.\  4754--4765, 2018.

\bibitem[Cover \& Thomas(1991)Cover and Thomas]{cover1991entropy}
Cover, T.~M. and Thomas, J.~A.
\newblock Entropy, relative entropy and mutual information.
\newblock \emph{Elements of information theory}, 2\penalty0 (1):\penalty0
  12--13, 1991.

\bibitem[Curi et~al.(2020{\natexlab{a}})Curi, Berkenkamp, and
  Krause]{curi2020efficient}
Curi, S., Berkenkamp, F., and Krause, A.
\newblock Efficient model-based reinforcement learning through optimistic
  policy search and planning.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)},
  33, 2020{\natexlab{a}}.

\bibitem[Curi et~al.(2020{\natexlab{b}})Curi, Melchior, Berkenkamp, and
  Krause]{curi2020structured}
Curi, S., Melchior, S., Berkenkamp, F., and Krause, A.
\newblock Structured variational inference in partially observable unstable
  gaussian process state space models.
\newblock In \emph{Learning for Dynamics and Control}, pp.\  147--157,
  2020{\natexlab{b}}.

\bibitem[Deisenroth \& Rasmussen(2011)Deisenroth and
  Rasmussen]{deisenroth2011pilco}
Deisenroth, M. and Rasmussen, C.~E.
\newblock Pilco: A model-based and data-efficient approach to policy search.
\newblock In \emph{International Conference on machine learning (ICML)}, pp.\
  465--472, 2011.

\bibitem[Der~Kiureghian \& Ditlevsen(2009)Der~Kiureghian and
  Ditlevsen]{der2009aleatory}
Der~Kiureghian, A. and Ditlevsen, O.
\newblock Aleatory or epistemic? does it matter?
\newblock \emph{Structural safety}, 31\penalty0 (2):\penalty0 105--112, 2009.

\bibitem[Desautels et~al.(2014)Desautels, Krause, and
  Burdick]{desautels2014parallelizing}
Desautels, T., Krause, A., and Burdick, J.~W.
\newblock Parallelizing exploration-exploitation tradeoffs in {G}aussian
  process bandit optimization.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 15:\penalty0
  3873--3923, 2014.

\bibitem[Dulac-Arnold et~al.(2019)Dulac-Arnold, Mankowitz, and
  Hester]{dulac2019challenges}
Dulac-Arnold, G., Mankowitz, D., and Hester, T.
\newblock Challenges of real-world reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1904.12901}, 2019.

\bibitem[Durand et~al.(2018)Durand, Maillard, and Pineau]{durand2018streaming}
Durand, A., Maillard, O.-A., and Pineau, J.
\newblock Streaming kernel regression with provably adaptive mean, variance,
  and regularization.
\newblock \emph{The Journal of Machine Learning Research (JMLR)}, 19\penalty0
  (1):\penalty0 650--683, 2018.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  1861--1870, 2018.

\bibitem[Iyengar(2005)]{iyengar2005robust}
Iyengar, G.~N.
\newblock Robust dynamic programming.
\newblock \emph{Mathematics of Operations Research}, 30\penalty0 (2):\penalty0
  257--280, 2005.

\bibitem[Kamalaruban et~al.(2020)Kamalaruban, Huang, Hsieh, Rolland, Shi, and
  Cevher]{kamalaruban2020robust}
Kamalaruban, P., Huang, Y.-T., Hsieh, Y.-P., Rolland, P., Shi, C., and Cevher,
  V.
\newblock Robust reinforcement learning via adversarial training with
  {L}angevin dynamics.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)},
  33, 2020.

\bibitem[Kamthe \& Deisenroth(2018)Kamthe and Deisenroth]{kamthe2018data}
Kamthe, S. and Deisenroth, M.
\newblock Data-efficient reinforcement learning with probabilistic model
  predictive control.
\newblock In \emph{Conference on Artificial Intelligence and Statistics
  (AISTATS)}, pp.\  1701--1710, 2018.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kirschner et~al.(2020)Kirschner, Bogunovic, Jegelka, and
  Krause]{kirschner2020distributionally}
Kirschner, J., Bogunovic, I., Jegelka, S., and Krause, A.
\newblock Distributionally robust {B}ayesian optimization.
\newblock In \emph{Conference on Artificial Intelligence and Statistics
  (AISTATS)}, pp.\  2174--2184, 2020.

\bibitem[Krause \& Ong(2011)Krause and Ong]{krause2011contextual}
Krause, A. and Ong, C.~S.
\newblock Contextual {G}aussian process bandit optimization.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, pp.\  2447--2455, 2011.

\bibitem[Lagoudakis \& Parr(2002)Lagoudakis and Parr]{lagoudakis2002value}
Lagoudakis, M.~G. and Parr, R.
\newblock Value function approximation in zero-sum {M}arkov games.
\newblock In \emph{Conference on Uncertainty in artificial intelligence (UAI)},
  pp.\  283--292, 2002.

\bibitem[Littman(1994)]{littman1994markov}
Littman, M.~L.
\newblock Markov games as a framework for multi-agent reinforcement learning.
\newblock In \emph{Machine learning proceedings}, pp.\  157--163. Elsevier,
  1994.

\bibitem[Littman \& Szepesv{\'a}ri(1996)Littman and
  Szepesv{\'a}ri]{littman1996generalized}
Littman, M.~L. and Szepesv{\'a}ri, C.
\newblock A generalized reinforcement-learning model: Convergence and
  applications.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  310--318, 1996.

\bibitem[Malik et~al.(2019)Malik, Kuleshov, Song, Nemer, Seymour, and
  Ermon]{malik2019calibrated}
Malik, A., Kuleshov, V., Song, J., Nemer, D., Seymour, H., and Ermon, S.
\newblock Calibrated model-based deep reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  4314--4323, 2019.

\bibitem[Mohamed et~al.(2019)Mohamed, Rosca, Figurnov, and
  Mnih]{mohamed2019monte}
Mohamed, S., Rosca, M., Figurnov, M., and Mnih, A.
\newblock Monte {C}arlo gradient estimation in machine learning.
\newblock \emph{arXiv preprint arXiv:1906.10652}, 2019.

\bibitem[Nilim \& El~Ghaoui(2005)Nilim and El~Ghaoui]{nilim2005robust}
Nilim, A. and El~Ghaoui, L.
\newblock Robust control of {M}arkov decision processes with uncertain
  transition matrices.
\newblock \emph{Operations Research}, 53\penalty0 (5):\penalty0 780--798, 2005.

\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and Roy]{osband2016deep}
Osband, I., Blundell, C., Pritzel, A., and Roy, B.~V.
\newblock Deep exploration via bootstrapped {DQN}.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)},
  pp.\  4033--4041, 2016.

\bibitem[Peng et~al.(2018)Peng, Andrychowicz, Zaremba, and Abbeel]{peng2018sim}
Peng, X.~B., Andrychowicz, M., Zaremba, W., and Abbeel, P.
\newblock Sim-to-real transfer of robotic control with dynamics randomization.
\newblock In \emph{IEEE international conference on robotics and automation
  (ICRA)}, pp.\  1--8. IEEE, 2018.

\bibitem[Perolat et~al.(2015)Perolat, Scherrer, Piot, and
  Pietquin]{perolat2015approximate}
Perolat, J., Scherrer, B., Piot, B., and Pietquin, O.
\newblock Approximate dynamic programming for two-player zero-sum {M}arkov
  games.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  1321--1329, 2015.

\bibitem[Pinto et~al.(2017)Pinto, Davidson, Sukthankar, and
  Gupta]{pinto2017robust}
Pinto, L., Davidson, J., Sukthankar, R., and Gupta, A.
\newblock Robust adversarial reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  2817--2826, 2017.

\bibitem[Rajeswaran et~al.(2017)Rajeswaran, Ghotra, Ravindran, and
  Levine]{rajeswaran2016epopt}
Rajeswaran, A., Ghotra, S., Ravindran, B., and Levine, S.
\newblock Epopt: Learning robust neural network policies using model ensembles.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1889--1897. PMLR, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{silver2014deterministic}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  387--395, 2014.

\bibitem[Srinivas et~al.(2010)Srinivas, Krause, Kakade, and
  Seeger]{srinivas2010gaussian}
Srinivas, N., Krause, A., Kakade, S., and Seeger, M.
\newblock Gaussian process optimization in the bandit setting: no regret and
  experimental design.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  1015--1022, 2010.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Tamar et~al.(2014)Tamar, Mannor, and Xu]{tamar2014scaling}
Tamar, A., Mannor, S., and Xu, H.
\newblock Scaling up robust {MDP}s using function approximation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  181--189, 2014.

\bibitem[Tessler et~al.(2019)Tessler, Efroni, and Mannor]{tessler2019action}
Tessler, C., Efroni, Y., and Mannor, S.
\newblock Action robust reinforcement learning and applications in continuous
  control.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  6215--6224, 2019.

\bibitem[Tobin et~al.(2017)Tobin, Fong, Ray, Schneider, Zaremba, and
  Abbeel]{tobin2017domain}
Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel, P.
\newblock Domain randomization for transferring deep neural networks from
  simulation to the real world.
\newblock In \emph{International Conference on Intelligent Robots and Systems
  (IROS)}, pp.\  23--30. IEEE, 2017.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{Conference on Intelligent Robots and Systems}, pp.\
  5026--5033. IEEE, 2012.

\bibitem[Valko et~al.(2013)Valko, Korda, Munos, Flaounas, and
  Cristianini]{valko2013finite}
Valko, M., Korda, N., Munos, R., Flaounas, I., and Cristianini, N.
\newblock Finite-time analysis of kernelised contextual bandits.
\newblock In \emph{Uncertainty in Artificial Intelligence (UAI)}, pp.\  654,
  2013.

\bibitem[van Hasselt et~al.(2016)van Hasselt, Guez, Hessel, Mnih, and
  Silver]{van2016learning}
van Hasselt, H., Guez, A., Hessel, M., Mnih, V., and Silver, D.
\newblock Learning values across many orders of magnitude.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)},
  pp.\  4294--4302, 2016.

\bibitem[Vinitsky et~al.(2020)Vinitsky, Du, Parvate, Jang, Abbeel, and
  Bayen]{vinitsky2020robust}
Vinitsky, E., Du, Y., Parvate, K., Jang, K., Abbeel, P., and Bayen, A.
\newblock Robust reinforcement learning using adversarial populations.
\newblock \emph{arXiv preprint arXiv:2008.01825}, 2020.

\bibitem[Wiesemann et~al.(2013)Wiesemann, Kuhn, and
  Rustem]{wiesemann2013robust}
Wiesemann, W., Kuhn, D., and Rustem, B.
\newblock Robust {M}arkov decision processes.
\newblock \emph{Mathematics of Operations Research}, 38\penalty0 (1):\penalty0
  153--183, 2013.

\bibitem[Zhang et~al.(2020)Zhang, Kakade, Basar, and Yang]{zhang2020model}
Zhang, K., Kakade, S., Basar, T., and Yang, L.
\newblock Model-based multi-agent rl in zero-sum markov games with near-optimal
  sample complexity.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)},
  33, 2020.

\end{thebibliography}
