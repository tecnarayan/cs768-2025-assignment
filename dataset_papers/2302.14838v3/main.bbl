\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahmad et~al.(2021)Ahmad, Chakraborty, Ray, and
  Chang]{Ahmad2021UnifiedPF}
Ahmad, W.~U., Chakraborty, S., Ray, B., and Chang, K.-W.
\newblock Unified pre-training for program understanding and generation.
\newblock \emph{ArXiv}, abs/2103.06333, 2021.

\bibitem[Bender et~al.(2020)Bender, Liu, Chen, Chu, Cheng, Kindermans, and
  Le]{Bender2020CanWS}
Bender, G., Liu, H., Chen, B., Chu, G., Cheng, S., Kindermans, P.-J., and Le,
  Q.~V.
\newblock Can weight sharing outperform random architecture search? an
  investigation with tunas.
\newblock \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  14311--14320, 2020.

\bibitem[{BigScience Workshop} et~al.(2022){BigScience Workshop}, {:}, Scao,
  Fan, Akiki, Pavlick, Ilić, Hesslow, Castagné, Luccioni, Yvon, Gallé, Tow,
  Rush, Biderman, Webson, Ammanamanchi, Wang, Sagot, Muennighoff, del Moral,
  Ruwase, Bawden, Bekman, McMillan-Major, Beltagy, Nguyen, Saulnier, Tan,
  Suarez, Sanh, Laurençon, Jernite, Launay, Mitchell, Raffel, Gokaslan, Simhi,
  Soroa, Aji, Alfassy, Rogers, Nitzav, Xu, Mou, Emezue, Klamm, Leong, van
  Strien, Adelani, Radev, Ponferrada, Levkovizh, Kim, Natan, De~Toni, Dupont,
  Kruszewski, Pistilli, Elsahar, Benyamina, Tran, Yu, Abdulmumin, Johnson,
  Gonzalez-Dios, de~la Rosa, Chim, Dodge, Zhu, Chang, Frohberg, Tobing,
  Bhattacharjee, Almubarak, Chen, Lo, Von~Werra, Weber, Phan, allal, Tanguy,
  Dey, Muñoz, Masoud, Grandury, Šaško, Huang, Coavoux, Singh, Jiang, Vu,
  Jauhar, Ghaleb, Subramani, Kassner, Khamis, Nguyen, Espejel, de~Gibert,
  Villegas, Henderson, Colombo, Amuok, Lhoest, Harliman, Bommasani, López,
  Ribeiro, Osei, Pyysalo, Nagel, Bose, Muhammad, Sharma, Longpre, Nikpoor,
  Silberberg, Pai, Zink, Torrent, Schick, Thrush, Danchev, Nikoulina, Laippala,
  Lepercq, Prabhu, Alyafeai, Talat, Raja, Heinzerling, Si, Taşar, Salesky,
  Mielke, Lee, Sharma, Santilli, Chaffin, Stiegler, Datta, Szczechla,
  Chhablani, Wang, Pandey, Strobelt, Fries, Rozen, Gao, Sutawika, Bari,
  Al-shaibani, Manica, Nayak, Teehan, Albanie, Shen, Ben-David, Bach, Kim,
  Bers, Fevry, Neeraj, Thakker, Raunak, Tang, Yong, Sun, Brody, Uri, Tojarieh,
  Roberts, Chung, Tae, Phang, Press, Li, Narayanan, Bourfoune, Casper, Rasley,
  Ryabinin, Mishra, Zhang, Shoeybi, Peyrounette, Patry, Tazi, Sanseviero, von
  Platen, Cornette, Lavallée, Lacroix, Rajbhandari, Gandhi, Smith, Requena,
  Patil, Dettmers, Baruwa, Singh, Cheveleva, Ligozat, Subramonian, Névéol,
  Lovering, Garrette, Tunuguntla, Reiter, Taktasheva, Voloshina, Bogdanov,
  Winata, Schoelkopf, Kalo, Novikova, Forde, Clive, Kasai, Kawamura, Hazan,
  Carpuat, Clinciu, Kim, Cheng, Serikov, Antverg, van~der Wal, Zhang, Zhang,
  Gehrmann, Mirkin, Pais, Shavrina, Scialom, Yun, Limisiewicz, Rieser,
  Protasov, Mikhailov, Pruksachatkun, Belinkov, Bamberger, Kasner, Rueda,
  Pestana, Feizpour, Khan, Faranak, Santos, Hevia, Unldreaj, Aghagol,
  Abdollahi, Tammour, HajiHosseini, Behroozi, Ajibade, Saxena, Ferrandis,
  Contractor, Lansky, David, Kiela, Nguyen, Tan, Baylor, Ozoani, Mirza,
  Ononiwu, Rezanejad, Jones, Bhattacharya, Solaiman, Sedenko, Nejadgholi,
  Passmore, Seltzer, Sanz, Dutra, Samagaio, Elbadri, Mieskes, Gerchick,
  Akinlolu, McKenna, Qiu, Ghauri, Burynok, Abrar, Rajani, Elkott, Fahmy,
  Samuel, An, Kromann, Hao, Alizadeh, Shubber, Wang, Roy, Viguier, Le, Oyebade,
  Le, Yang, Nguyen, Kashyap, Palasciano, Callahan, Shukla, Miranda-Escalada,
  Singh, Beilharz, Wang, Brito, Zhou, Jain, Xu, Fourrier, Periñán, Molano,
  Yu, Manjavacas, Barth, Fuhrimann, Altay, Bayrak, Burns, Vrabec, Bello, Dash,
  Kang, Giorgi, Golde, Posada, Sivaraman, Bulchandani, Liu, Shinzato,
  de~Bykhovetz, Takeuchi, Pàmies, Castillo, Nezhurina, Sänger, Samwald,
  Cullan, Weinberg, De~Wolf, Mihaljcic, Liu, Freidank, Kang, Seelam, Dahlberg,
  Broad, Muellner, Fung, Haller, Chandrasekhar, Eisenberg, Martin, Canalli, Su,
  Su, Cahyawijaya, Garda, Deshmukh, Mishra, Kiblawi, Ott, Sang-aroonsiri,
  Kumar, Schweter, Bharati, Laud, Gigant, Kainuma, Kusa, Labrak, Bajaj,
  Venkatraman, Xu, Xu, Xu, Tan, Xie, Ye, Bras, Belkada, and Wolf]{workshop22}
{BigScience Workshop}, {:}, Scao, T.~L., Fan, A., Akiki, C., Pavlick, E.,
  Ilić, S., Hesslow, D., Castagné, R., Luccioni, A.~S., Yvon, F., Gallé, M.,
  Tow, J., Rush, A.~M., Biderman, S., Webson, A., Ammanamanchi, P.~S., Wang,
  T., Sagot, B., Muennighoff, N., del Moral, A.~V., Ruwase, O., Bawden, R.,
  Bekman, S., McMillan-Major, A., Beltagy, I., Nguyen, H., Saulnier, L., Tan,
  S., Suarez, P.~O., Sanh, V., Laurençon, H., Jernite, Y., Launay, J.,
  Mitchell, M., Raffel, C., Gokaslan, A., Simhi, A., Soroa, A., Aji, A.~F.,
  Alfassy, A., Rogers, A., Nitzav, A.~K., Xu, C., Mou, C., Emezue, C., Klamm,
  C., Leong, C., van Strien, D., Adelani, D.~I., Radev, D., Ponferrada, E.~G.,
  Levkovizh, E., Kim, E., Natan, E.~B., De~Toni, F., Dupont, G., Kruszewski,
  G., Pistilli, G., Elsahar, H., Benyamina, H., Tran, H., Yu, I., Abdulmumin,
  I., Johnson, I., Gonzalez-Dios, I., de~la Rosa, J., Chim, J., Dodge, J., Zhu,
  J., Chang, J., Frohberg, J., Tobing, J., Bhattacharjee, J., Almubarak, K.,
  Chen, K., Lo, K., Von~Werra, L., Weber, L., Phan, L., allal, L.~B., Tanguy,
  L., Dey, M., Muñoz, M.~R., Masoud, M., Grandury, M., Šaško, M., Huang, M.,
  Coavoux, M., Singh, M., Jiang, M. T.-J., Vu, M.~C., Jauhar, M.~A., Ghaleb,
  M., Subramani, N., Kassner, N., Khamis, N., Nguyen, O., Espejel, O.,
  de~Gibert, O., Villegas, P., Henderson, P., Colombo, P., Amuok, P., Lhoest,
  Q., Harliman, R., Bommasani, R., López, R.~L., Ribeiro, R., Osei, S.,
  Pyysalo, S., Nagel, S., Bose, S., Muhammad, S.~H., Sharma, S., Longpre, S.,
  Nikpoor, S., Silberberg, S., Pai, S., Zink, S., Torrent, T.~T., Schick, T.,
  Thrush, T., Danchev, V., Nikoulina, V., Laippala, V., Lepercq, V., Prabhu,
  V., Alyafeai, Z., Talat, Z., Raja, A., Heinzerling, B., Si, C., Taşar,
  D.~E., Salesky, E., Mielke, S.~J., Lee, W.~Y., Sharma, A., Santilli, A.,
  Chaffin, A., Stiegler, A., Datta, D., Szczechla, E., Chhablani, G., Wang, H.,
  Pandey, H., Strobelt, H., Fries, J.~A., Rozen, J., Gao, L., Sutawika, L.,
  Bari, M.~S., Al-shaibani, M.~S., Manica, M., Nayak, N., Teehan, R., Albanie,
  S., Shen, S., Ben-David, S., Bach, S.~H., Kim, T., Bers, T., Fevry, T.,
  Neeraj, T., Thakker, U., Raunak, V., Tang, X., Yong, Z.-X., Sun, Z., Brody,
  S., Uri, Y., Tojarieh, H., Roberts, A., Chung, H.~W., Tae, J., Phang, J.,
  Press, O., Li, C., Narayanan, D., Bourfoune, H., Casper, J., Rasley, J.,
  Ryabinin, M., Mishra, M., Zhang, M., Shoeybi, M., Peyrounette, M., Patry, N.,
  Tazi, N., Sanseviero, O., von Platen, P., Cornette, P., Lavallée, P.~F.,
  Lacroix, R., Rajbhandari, S., Gandhi, S., Smith, S., Requena, S., Patil, S.,
  Dettmers, T., Baruwa, A., Singh, A., Cheveleva, A., Ligozat, A.-L.,
  Subramonian, A., Névéol, A., Lovering, C., Garrette, D., Tunuguntla, D.,
  Reiter, E., Taktasheva, E., Voloshina, E., Bogdanov, E., Winata, G.~I.,
  Schoelkopf, H., Kalo, J.-C., Novikova, J., Forde, J.~Z., Clive, J., Kasai,
  J., Kawamura, K., Hazan, L., Carpuat, M., Clinciu, M., Kim, N., Cheng, N.,
  Serikov, O., Antverg, O., van~der Wal, O., Zhang, R., Zhang, R., Gehrmann,
  S., Mirkin, S., Pais, S., Shavrina, T., Scialom, T., Yun, T., Limisiewicz,
  T., Rieser, V., Protasov, V., Mikhailov, V., Pruksachatkun, Y., Belinkov, Y.,
  Bamberger, Z., Kasner, Z., Rueda, A., Pestana, A., Feizpour, A., Khan, A.,
  Faranak, A., Santos, A., Hevia, A., Unldreaj, A., Aghagol, A., Abdollahi, A.,
  Tammour, A., HajiHosseini, A., Behroozi, B., Ajibade, B., Saxena, B.,
  Ferrandis, C.~M., Contractor, D., Lansky, D., David, D., Kiela, D., Nguyen,
  D.~A., Tan, E., Baylor, E., Ozoani, E., Mirza, F., Ononiwu, F., Rezanejad,
  H., Jones, H., Bhattacharya, I., Solaiman, I., Sedenko, I., Nejadgholi, I.,
  Passmore, J., Seltzer, J., Sanz, J.~B., Dutra, L., Samagaio, M., Elbadri, M.,
  Mieskes, M., Gerchick, M., Akinlolu, M., McKenna, M., Qiu, M., Ghauri, M.,
  Burynok, M., Abrar, N., Rajani, N., Elkott, N., Fahmy, N., Samuel, O., An,
  R., Kromann, R., Hao, R., Alizadeh, S., Shubber, S., Wang, S., Roy, S.,
  Viguier, S., Le, T., Oyebade, T., Le, T., Yang, Y., Nguyen, Z., Kashyap,
  A.~R., Palasciano, A., Callahan, A., Shukla, A., Miranda-Escalada, A., Singh,
  A., Beilharz, B., Wang, B., Brito, C., Zhou, C., Jain, C., Xu, C., Fourrier,
  C., Periñán, D.~L., Molano, D., Yu, D., Manjavacas, E., Barth, F.,
  Fuhrimann, F., Altay, G., Bayrak, G., Burns, G., Vrabec, H.~U., Bello, I.,
  Dash, I., Kang, J., Giorgi, J., Golde, J., Posada, J.~D., Sivaraman, K.~R.,
  Bulchandani, L., Liu, L., Shinzato, L., de~Bykhovetz, M.~H., Takeuchi, M.,
  Pàmies, M., Castillo, M.~A., Nezhurina, M., Sänger, M., Samwald, M.,
  Cullan, M., Weinberg, M., De~Wolf, M., Mihaljcic, M., Liu, M., Freidank, M.,
  Kang, M., Seelam, N., Dahlberg, N., Broad, N.~M., Muellner, N., Fung, P.,
  Haller, P., Chandrasekhar, R., Eisenberg, R., Martin, R., Canalli, R., Su,
  R., Su, R., Cahyawijaya, S., Garda, S., Deshmukh, S.~S., Mishra, S., Kiblawi,
  S., Ott, S., Sang-aroonsiri, S., Kumar, S., Schweter, S., Bharati, S., Laud,
  T., Gigant, T., Kainuma, T., Kusa, W., Labrak, Y., Bajaj, Y.~S., Venkatraman,
  Y., Xu, Y., Xu, Y., Xu, Y., Tan, Z., Xie, Z., Ye, Z., Bras, M., Belkada, Y.,
  and Wolf, T.
\newblock Bloom: A 176b-parameter open-access multilingual language model,
  2022.
\newblock URL \url{https://arxiv.org/abs/2211.05100}.

\bibitem[Brooks et~al.(2022)Brooks, Walls, Lewis, and
  Singh]{context_policy_iteration}
Brooks, E., Walls, L., Lewis, R.~L., and Singh, S.
\newblock In-context policy iteration, 2022.
\newblock URL \url{https://arxiv.org/abs/2210.03821}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown20}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
  D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
  S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
  I., and Amodei, D.
\newblock Language models are few-shot learners, 2020.
\newblock URL \url{https://arxiv.org/abs/2005.14165}.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Ponde, Kaplan, Edwards,
  Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin,
  Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such,
  Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol, Babuschkin,
  Balaji, Jain, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight,
  Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and
  Zaremba]{Chen2021EvaluatingLL}
Chen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan, J., Edwards, H.,
  Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov,
  M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N.,
  Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P.,
  Such, F.~P., Cummings, D.~W., Plappert, M., Chantzis, F., Barnes, E.,
  Herbert-Voss, A., Guss, W.~H., Nichol, A., Babuschkin, I., Balaji, S.~A.,
  Jain, S., Carr, A., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford,
  A., Knight, M.~M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew,
  B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W.
\newblock Evaluating large language models trained on code.
\newblock \emph{ArXiv}, abs/2107.03374, 2021.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez,
  Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury,
  Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski,
  Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov,
  Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira,
  Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei,
  Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{chowdhery22}
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,
  Barham, P., Chung, H.~W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K.,
  Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N.,
  Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J.,
  Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A.,
  Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K.,
  Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov,
  A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A.~M., Pillai,
  T.~S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee,
  K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J.,
  Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N.
\newblock Palm: Scaling language modeling with pathways, 2022.
\newblock URL \url{https://arxiv.org/abs/2204.02311}.

\bibitem[Cormen et~al.(2009)Cormen, Leiserson, Rivest, and
  Stein]{cormen2009clrs}
Cormen, T.~H., Leiserson, C.~E., Rivest, R.~L., and Stein, C.
\newblock \emph{Introduction to Algorithms, Third Edition}.
\newblock The MIT Press, 3rd edition, 2009.
\newblock ISBN 0262033844.

\bibitem[Dakhel et~al.(2022)Dakhel, Majdinasab, Nikanjam, Khomh, Desmarais, and
  Jiang]{Dakhel2022GitHubCA}
Dakhel, A.~M., Majdinasab, V., Nikanjam, A., Khomh, F., Desmarais, M.~C., and
  Jiang, Z.~M.
\newblock Github copilot ai pair programmer: Asset or liability?
\newblock \emph{ArXiv}, abs/2206.15331, 2022.

\bibitem[Dohan et~al.(2022)Dohan, Xu, Lewkowycz, Austin, Bieber, Lopes, Wu,
  Michalewski, Saurous, Sohl-dickstein, Murphy, and Sutton]{cascades}
Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R.~G., Wu, Y.,
  Michalewski, H., Saurous, R.~A., Sohl-dickstein, J., Murphy, K., and Sutton,
  C.
\newblock Language model cascades, 2022.
\newblock URL \url{https://arxiv.org/abs/2207.10342}.

\bibitem[Dong et~al.(2021)Dong, Liu, Musial, and Gabrys]{dong2021nats}
Dong, X., Liu, L., Musial, K., and Gabrys, B.
\newblock {NATS-Bench}: Benchmarking nas algorithms for architecture topology
  and size.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI)}, 2021.
\newblock \doi{10.1109/TPAMI.2021.3054824}.
\newblock \mbox{doi}:\url{10.1109/TPAMI.2021.3054824}.

\bibitem[Du et~al.(2021)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu,
  Firat, Zoph, Fedus, Bosma, Zhou, Wang, Wang, Webster, Pellat, Robinson,
  Meier-Hellstern, Duke, Dixon, Zhang, Le, Wu, Chen, and Cui]{du21}
Du, N., Huang, Y., Dai, A.~M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M.,
  Zhou, Y., Yu, A.~W., Firat, O., Zoph, B., Fedus, L., Bosma, M., Zhou, Z.,
  Wang, T., Wang, Y.~E., Webster, K., Pellat, M., Robinson, K.,
  Meier-Hellstern, K., Duke, T., Dixon, L., Zhang, K., Le, Q.~V., Wu, Y., Chen,
  Z., and Cui, C.
\newblock Glam: Efficient scaling of language models with mixture-of-experts,
  2021.
\newblock URL \url{https://arxiv.org/abs/2112.06905}.

\bibitem[Elsken et~al.(2018)Elsken, Metzen, and Hutter]{Elsken2018EfficientMN}
Elsken, T., Metzen, J.~H., and Hutter, F.
\newblock Efficient multi-objective neural architecture search via lamarckian
  evolution.
\newblock \emph{arXiv: Machine Learning}, 2018.

\bibitem[Feng et~al.(2020)Feng, Guo, Tang, Duan, Feng, Gong, Shou, Qin, Liu,
  Jiang, and Zhou]{Feng2020CodeBERTAP}
Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B.,
  Liu, T., Jiang, D., and Zhou, M.
\newblock Codebert: A pre-trained model for programming and natural languages.
\newblock \emph{ArXiv}, abs/2002.08155, 2020.

\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and
  Dahl]{gilmer2017message_passing}
Gilmer, J., Schoenholz, S.~S., Riley, P.~F., Vinyals, O., and Dahl, G.~E.
\newblock Neural message passing for quantum chemistry.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning - Volume 70}, ICML'17, pp.\  1263–1272. JMLR.org, 2017.

\bibitem[Greydanus(2020)]{greydanus2020mnist1d}
Greydanus, S.
\newblock Scaling *down* deep learning.
\newblock \emph{CoRR}, abs/2011.14439, 2020.
\newblock URL \url{https://arxiv.org/abs/2011.14439}.

\bibitem[Heek et~al.(2020)Heek, Levskaya, Oliver, Ritter, Rondepierre, Steiner,
  and van {Z}ee]{flax2020github}
Heek, J., Levskaya, A., Oliver, A., Ritter, M., Rondepierre, B., Steiner, A.,
  and van {Z}ee, M.
\newblock {F}lax: A neural network library and ecosystem for {JAX}, 2020.
\newblock URL \url{http://github.com/google/flax}.

\bibitem[Hennigan et~al.(2020)Hennigan, Cai, Norman, and
  Babuschkin]{haiku2020github}
Hennigan, T., Cai, T., Norman, T., and Babuschkin, I.
\newblock {H}aiku: {S}onnet for {JAX}, 2020.
\newblock URL \url{http://github.com/deepmind/dm-haiku}.

\bibitem[Ibarz et~al.(2022)Ibarz, Kurin, Papamakarios, Nikiforou, Bennani,
  Csord{\'a}s, Dudzik, Bovsnjak, Vitvitskyi, Rubanova, Deac, Bevilacqua, Ganin,
  Blundell, and Veli\v{c}kovi\'{c}]{clrs_generalist}
Ibarz, B., Kurin, V., Papamakarios, G., Nikiforou, K., Bennani, M.~A.,
  Csord{\'a}s, R., Dudzik, A., Bovsnjak, M., Vitvitskyi, A., Rubanova, Y.,
  Deac, A., Bevilacqua, B., Ganin, Y., Blundell, C., and Veli\v{c}kovi\'{c}, P.
\newblock A generalist neural algorithmic learner.
\newblock \emph{ArXiv}, abs/2209.11142, 2022.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and
  Iwasawa]{Kojima2022LargeLM}
Kojima, T., Gu, S.~S., Reid, M., Matsuo, Y., and Iwasawa, Y.
\newblock Large language models are zero-shot reasoners.
\newblock \emph{ArXiv}, abs/2205.11916, 2022.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{LeCun1998GradientbasedLA}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proc. IEEE}, 86:\penalty0 2278--2324, 1998.

\bibitem[Lehman et~al.(2022)Lehman, Gordon, Jain, Ndousse, Yeh, and
  Stanley]{Lehman2022EvolutionTL}
Lehman, J., Gordon, J., Jain, S., Ndousse, K., Yeh, C., and Stanley, K.~O.
\newblock Evolution through large models.
\newblock \emph{ArXiv}, abs/2206.08896, 2022.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{prompt_tune}
Lester, B., Al-Rfou, R., and Constant, N.
\newblock The power of scale for parameter-efficient prompt tuning, 2021.
\newblock URL \url{https://arxiv.org/abs/2104.08691}.

\bibitem[Li \& Talwalkar(2019)Li and Talwalkar]{Li2019RandomSA}
Li, L. and Talwalkar, A.~S.
\newblock Random search and reproducibility for neural architecture search.
\newblock \emph{ArXiv}, abs/1902.07638, 2019.

\bibitem[Liu et~al.(2020)Liu, Brock, Simonyan, and Le]{Liu2020EvolvingNL}
Liu, H., Brock, A., Simonyan, K., and Le, Q.~V.
\newblock Evolving normalization-activation layers.
\newblock \emph{ArXiv}, abs/2004.02967, 2020.

\bibitem[Liu et~al.(2021)Liu, Shen, Zhang, Dolan, Carin, and
  Chen]{Liu2021WhatMG}
Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W.
\newblock What makes good in-context examples for gpt-3?
\newblock In \emph{Workshop on Knowledge Extraction and Integration for Deep
  Learning Architectures; Deep Learning Inside Out}, 2021.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{loshchilov2018adamw}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[Lu et~al.(2021)Lu, Bartolo, Moore, Riedel, and
  Stenetorp]{Lu2021FantasticallyOP}
Lu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P.
\newblock Fantastically ordered prompts and where to find them: Overcoming
  few-shot prompt order sensitivity.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics}, 2021.

\bibitem[Meyerson et~al.(2023)Meyerson, Nelson, Bradley, Moradi, Hoover, and
  Lehman]{meyerson2023lmc}
Meyerson, E., Nelson, M.~J., Bradley, H., Moradi, A., Hoover, A.~K., and
  Lehman, J.
\newblock Language model crossover: Variation through few-shot prompting, 2023.
\newblock URL \url{https://arxiv.org/abs/2302.12170}.

\bibitem[Min et~al.(2022)Min, Lyu, Holtzman, Artetxe, Lewis, Hajishirzi, and
  Zettlemoyer]{Min2022RethinkingTR}
Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and
  Zettlemoyer, L.
\newblock Rethinking the role of demonstrations: What makes in-context learning
  work?
\newblock \emph{ArXiv}, abs/2202.12837, 2022.

\bibitem[Mnih \& Hinton(2007)Mnih and Hinton]{andriy2007bilinear}
Mnih, A. and Hinton, G.
\newblock Three new graphical models for statistical language modelling.
\newblock In \emph{Proceedings of the 24th International Conference on Machine
  Learning}, ICML '07, pp.\  641–648, New York, NY, USA, 2007. Association
  for Computing Machinery.
\newblock ISBN 9781595937933.
\newblock \doi{10.1145/1273496.1273577}.
\newblock URL \url{https://doi.org/10.1145/1273496.1273577}.

\bibitem[Noorbakhsh et~al.(2021)Noorbakhsh, Sulaiman, Sharifi, Roy, and
  Jamshidi]{Noorbakhsh2021PretrainedLM}
Noorbakhsh, K., Sulaiman, M., Sharifi, M., Roy, K., and Jamshidi, P.
\newblock Pretrained language models are symbolic mathematics solvers too!
\newblock \emph{ArXiv}, abs/2110.03501, 2021.

\bibitem[Odena et~al.(2021)Odena, Sutton, Dohan, Jiang, Michalewski, Austin,
  Bosma, Nye, Terry, and Le]{odena2021program}
Odena, A., Sutton, C., Dohan, D.~M., Jiang, E., Michalewski, H., Austin, J.,
  Bosma, M.~P., Nye, M., Terry, M., and Le, Q.~V.
\newblock Program synthesis with large language models.
\newblock In \emph{n/a}, pp.\  n/a, n/a, 2021.
\newblock n/a.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell,
  Welinder, Christiano, Leike, and Lowe]{Ouyang2022TrainingLM}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.~L., Mishkin, P.,
  Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton,
  F., Miller, L.~E., Simens, M., Askell, A., Welinder, P., Christiano, P.~F.,
  Leike, J., and Lowe, R.~J.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{ArXiv}, abs/2203.02155, 2022.

\bibitem[Qian et~al.(2022)Qian, Wang, Li, LI, and Yan]{Qian2022LimitationsOL}
Qian, J., Wang, H., Li, Z., LI, S., and Yan, X.
\newblock Limitations of language models in arithmetic and symbolic induction.
\newblock \emph{ArXiv}, abs/2208.05051, 2022.

\bibitem[Real et~al.(2017)Real, Moore, Selle, Saxena, Suematsu, Tan, Le, and
  Kurakin]{Real2017LargeScaleEO}
Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.~L., Tan, J., Le,
  Q.~V., and Kurakin, A.
\newblock Large-scale evolution of image classifiers.
\newblock \emph{ArXiv}, abs/1703.01041, 2017.

\bibitem[Real et~al.(2018)Real, Aggarwal, Huang, and Le]{Real2018RegularizedEF}
Real, E., Aggarwal, A., Huang, Y., and Le, Q.~V.
\newblock Regularized evolution for image classifier architecture search.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2018.

\bibitem[Real et~al.(2020)Real, Liang, So, and Le]{Real2020AutoMLZeroEM}
Real, E., Liang, C., So, D.~R., and Le, Q.~V.
\newblock Automl-zero: Evolving machine learning algorithms from scratch.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Rubin et~al.(2021)Rubin, Herzig, and Berant]{Rubin2021LearningTR}
Rubin, O., Herzig, J., and Berant, J.
\newblock Learning to retrieve prompts for in-context learning.
\newblock \emph{ArXiv}, abs/2112.08633, 2021.

\bibitem[Sanh et~al.(2021)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai,
  Chaffin, Stiegler, Scao, Raja, Dey, Bari, Xu, Thakker, Sharma, Szczechla,
  Kim, Chhablani, Nayak, Datta, Chang, Jiang, Wang, Manica, Shen, Yong, Pandey,
  Bawden, Wang, Neeraj, Rozen, Sharma, Santilli, F{\'e}vry, Fries, Teehan,
  Biderman, Gao, Bers, Wolf, and Rush]{Sanh2021MultitaskPT}
Sanh, V., Webson, A., Raffel, C., Bach, S.~H., Sutawika, L., Alyafeai, Z.,
  Chaffin, A., Stiegler, A., Scao, T.~L., Raja, A., Dey, M., Bari, M.~S., Xu,
  C., Thakker, U., Sharma, S., Szczechla, E., Kim, T., Chhablani, G., Nayak,
  N.~V., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S.,
  Yong, Z.~X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma,
  A., Santilli, A., F{\'e}vry, T., Fries, J.~A., Teehan, R., Biderman, S.~R.,
  Gao, L., Bers, T., Wolf, T., and Rush, A.~M.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock \emph{ArXiv}, abs/2110.08207, 2021.

\bibitem[Sciuto et~al.(2019)Sciuto, Yu, Jaggi, Musat, and
  Salzmann]{Sciuto2019EvaluatingTS}
Sciuto, C., Yu, K., Jaggi, M., Musat, C.~C., and Salzmann, M.
\newblock Evaluating the search phase of neural architecture search.
\newblock \emph{ArXiv}, abs/1902.08142, 2019.

\bibitem[So et~al.(2019)So, Liang, and Le]{So2019TheET}
So, D.~R., Liang, C., and Le, Q.~V.
\newblock The evolved transformer.
\newblock \emph{ArXiv}, abs/1901.11117, 2019.

\bibitem[So et~al.(2021)So, Mańke, Liu, Dai, Shazeer, and Le]{So21}
So, D.~R., Mańke, W., Liu, H., Dai, Z., Shazeer, N., and Le, Q.~V.
\newblock Primer: Searching for efficient transformers for language modeling,
  2021.
\newblock URL \url{https://arxiv.org/abs/2109.08668}.

\bibitem[Thoppilan et~al.(2022)Thoppilan, De~Freitas, Hall, Shazeer,
  Kulshreshtha, Cheng, Jin, Bos, Baker, Du, Li, Lee, Zheng, Ghafouri, Menegali,
  Huang, Krikun, Lepikhin, Qin, Chen, Xu, Chen, Roberts, Bosma, Zhao, Zhou,
  Chang, Krivokon, Rusch, Pickett, Srinivasan, Man, Meier-Hellstern, Morris,
  Doshi, Santos, Duke, Soraker, Zevenbergen, Prabhakaran, Diaz, Hutchinson,
  Olson, Molina, Hoffman-John, Lee, Aroyo, Rajakumar, Butryna, Lamm, Kuzmina,
  Fenton, Cohen, Bernstein, Kurzweil, Aguera-Arcas, Cui, Croak, Chi, and
  Le]{thoppilan22}
Thoppilan, R., De~Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng,
  H.-T., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Lee, H., Zheng, H.~S.,
  Ghafouri, A., Menegali, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J.,
  Chen, D., Xu, Y., Chen, Z., Roberts, A., Bosma, M., Zhao, V., Zhou, Y.,
  Chang, C.-C., Krivokon, I., Rusch, W., Pickett, M., Srinivasan, P., Man, L.,
  Meier-Hellstern, K., Morris, M.~R., Doshi, T., Santos, R.~D., Duke, T.,
  Soraker, J., Zevenbergen, B., Prabhakaran, V., Diaz, M., Hutchinson, B.,
  Olson, K., Molina, A., Hoffman-John, E., Lee, J., Aroyo, L., Rajakumar, R.,
  Butryna, A., Lamm, M., Kuzmina, V., Fenton, J., Cohen, A., Bernstein, R.,
  Kurzweil, R., Aguera-Arcas, B., Cui, C., Croak, M., Chi, E., and Le, Q.
\newblock Lamda: Language models for dialog applications, 2022.
\newblock URL \url{https://arxiv.org/abs/2201.08239}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{transformers}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need, 2017.
\newblock URL \url{https://arxiv.org/abs/1706.03762}.

\bibitem[Veli\v{c}kovi\'{c} et~al.(2022)Veli\v{c}kovi\'{c}, Badia, Budden,
  Pascanu, Banino, Dashevskiy, Hadsell, and Blundell]{Velivckovic2022TheCA}
Veli\v{c}kovi\'{c}, P., Badia, A.~P., Budden, D., Pascanu, R., Banino, A.,
  Dashevskiy, M., Hadsell, R., and Blundell, C.
\newblock The clrs algorithmic reasoning benchmark.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Wang et~al.(2021)Wang, Wang, Joty, and Hoi]{Wang2021CodeT5IU}
Wang, Y., Wang, W., Joty, S.~R., and Hoi, S. C.~H.
\newblock Codet5: Identifier-aware unified pre-trained encoder-decoder models
  for code understanding and generation.
\newblock \emph{ArXiv}, abs/2109.00859, 2021.

\bibitem[Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and
  Le]{Wei2021FinetunedLM}
Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A.~W., Lester, B., Du, N., Dai,
  A.~M., and Le, Q.~V.
\newblock Finetuned language models are zero-shot learners.
\newblock \emph{ArXiv}, abs/2109.01652, 2021.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, hsin Chi, Le, and
  Zhou]{Wei2022ChainOT}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., hsin Chi, E.~H., Le, Q., and
  Zhou, D.
\newblock Chain of thought prompting elicits reasoning in large language
  models.
\newblock \emph{ArXiv}, abs/2201.11903, 2022.

\bibitem[Xu et~al.(2022)Xu, Alon, Neubig, and Hellendoorn]{Xu2022ASE}
Xu, F.~F., Alon, U., Neubig, G., and Hellendoorn, V.~J.
\newblock A systematic evaluation of large language models of code.
\newblock \emph{Proceedings of the 6th ACM SIGPLAN International Symposium on
  Machine Programming}, 2022.

\bibitem[Xu et~al.(2020)Xu, Li, Zhang, Du, ichi Kawarabayashi, and
  Jegelka]{xu2020algalignment}
Xu, K., Li, J., Zhang, M., Du, S.~S., ichi Kawarabayashi, K., and Jegelka, S.
\newblock What can neural networks reason about?
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rJxbJeHFPS}.

\bibitem[Yao(1999)]{Yao1999EvolvingAN}
Yao, X.
\newblock Evolving artificial neural networks.
\newblock \emph{Proc. IEEE}, 87:\penalty0 1423--1447, 1999.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura, Sridhar, Wang,
  and Zettlemoyer]{zhang2022}
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C.,
  Diab, M., Li, X., Lin, X.~V., Mihaylov, T., Ott, M., Shleifer, S., Shuster,
  K., Simig, D., Koura, P.~S., Sridhar, A., Wang, T., and Zettlemoyer, L.
\newblock Opt: Open pre-trained transformer language models, 2022.
\newblock URL \url{https://arxiv.org/abs/2205.01068}.

\bibitem[Zhao et~al.(2021)Zhao, Wallace, Feng, Klein, and
  Singh]{Zhao2021CalibrateBU}
Zhao, T., Wallace, E., Feng, S., Klein, D., and Singh, S.
\newblock Calibrate before use: Improving few-shot performance of language
  models.
\newblock \emph{ArXiv}, abs/2102.09690, 2021.

\bibitem[Zhou et~al.(2022)Zhou, Muresanu, Han, Paster, Pitis, Chan, and
  Ba]{Zhou2022LargeLM}
Zhou, Y., Muresanu, A.~I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J.
\newblock Large language models are human-level prompt engineers.
\newblock \emph{ArXiv}, abs/2211.01910, 2022.

\end{thebibliography}
