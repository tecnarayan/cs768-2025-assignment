\begin{thebibliography}{10}

\bibitem{burnetas1997optimal}
Apostolos Burnetas and Michael Katehakis.
\newblock Optimal adaptive policies for {M}arkov decision processes.
\newblock {\em Mathematics of Operations Research}, 22(1):222--255, 1997.

\bibitem{lai1985asymptotically}
Tze~Leung Lai and Herbert Robbins.
\newblock Asymptotically efficient adaptive allocation rules.
\newblock {\em Advances in applied mathematics}, 6(1):4--22, 1985.

\bibitem{kaelbling1996reinforcement}
Leslie~Pack Kaelbling, Michael~L Littman, and Andrew~W Moore.
\newblock Reinforcement learning: A survey.
\newblock {\em arXiv preprint cs/9605103}, 1996.

\bibitem{valiant1984theory}
Leslie~G Valiant.
\newblock A theory of the learnable.
\newblock {\em Communications of the ACM}, 27(11):1134--1142, 1984.

\bibitem{littlestone1988learning}
Nick Littlestone.
\newblock Learning quickly when irrelevant attributes abound: A new
  linear-threshold algorithm.
\newblock {\em Machine learning}, 2(4):285--318, 1988.

\bibitem{li2011knows}
Lihong Li, Michael~L Littman, Thomas~J Walsh, and Alexander~L Strehl.
\newblock Knows what it knows: a framework for self-aware learning.
\newblock {\em Machine learning}, 82(3):399--443, 2011.

\bibitem{jaksch2010near}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock {\em The Journal of Machine Learning Research}, 99:1563--1600, 2010.

\bibitem{kearns2002near}
Michael Kearns and Satinder Singh.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock {\em Machine Learning}, 49(2-3):209--232, 2002.

\bibitem{brafman2003r}
Ronen Brafman and Moshe Tennenholtz.
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock {\em The Journal of Machine Learning Research}, 3:213--231, 2003.

\bibitem{strehl2006pac}
Alexander Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael Littman.
\newblock Pac model-free reinforcement learning.
\newblock In {\em Proceedings of the 23rd international conference on Machine
  learning}, pages 881--888. ACM, 2006.

\bibitem{osband2013more}
Ian Osband, Daniel Russo, and Benjamin Van~Roy.
\newblock {(More) Efficient Reinforcement Learning via Posterior Sampling}.
\newblock {\em Advances in Neural Information Processing Systems}, 2013.

\bibitem{auer2003using}
Peter Auer.
\newblock Using confidence bounds for exploitation-exploration trade-offs.
\newblock {\em The Journal of Machine Learning Research}, 3:397--422, 2003.

\bibitem{bubeck2011xarmed}
S{\'e}bastien Bubeck, R{\'e}mi Munos, Gilles Stoltz, and Csaba Szepesv{\'a}ri.
\newblock X-armed bandits.
\newblock {\em Journal of Machine Learning Research}, 12:1587â€“1627, 2011.

\bibitem{russo2013}
Daniel Russo and Benjamin~Van Roy.
\newblock Learning to optimize via posterior sampling.
\newblock {\em CoRR}, abs/1301.2609, 2013.

\bibitem{osband2014near}
Ian Osband and Benjamin Van~Roy.
\newblock Near-optimal regret bounds for reinforcement learning in factored
  {MDP}s.
\newblock {\em arXiv preprint arXiv:1403.3741}, 2014.

\bibitem{abbasi2011improved}
Yassin Abbasi-Yadkori, D{\'a}vid P{\'a}l, and Csaba Szepesv{\'a}ri.
\newblock Improved algorithms for linear stochastic bandits.
\newblock {\em Advances in Neural Information Processing Systems}, 24, 2011.

\bibitem{ibrahimi2012efficient}
Morteza Ibrahimi, Adel Javanmard, and Benjamin Van~Roy.
\newblock Efficient reinforcement learning for high dimensional linear
  quadratic systems.
\newblock In {\em NIPS}, pages 2645--2653, 2012.

\bibitem{ortner2012online}
Ronald Ortner, Daniil Ryabko, et~al.
\newblock Online regret bounds for undiscounted continuous reinforcement
  learning.
\newblock In {\em NIPS}, pages 1772--1780, 2012.

\bibitem{russo2013eluder}
Daniel Russo and Benjamin Van~Roy.
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2256--2264, 2013.

\bibitem{thompson1933}
William Thompson.
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock {\em Biometrika}, 25(3/4):285--294, 1933.

\bibitem{strens2000bayesian}
Malcom Strens.
\newblock A {B}ayesian framework for reinforcement learning.
\newblock In {\em Proceedings of the 17th International Conference on Machine
  Learning}, pages 943--950, 2000.

\bibitem{bertsekas1995dynamic}
Dimitri Bertsekas.
\newblock {\em Dynamic programming and optimal control}, volume~1.
\newblock Athena Scientific Belmont, MA, 1995.

\bibitem{van2014generalization}
Benjamin Van~Roy and Zheng Wen.
\newblock Generalization and exploration via randomized value functions.
\newblock {\em arXiv preprint arXiv:1402.0635}, 2014.

\end{thebibliography}
