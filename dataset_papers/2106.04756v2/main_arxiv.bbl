\begin{thebibliography}{10}

\bibitem{achterberg2020presolve}
Tobias Achterberg, Robert~E Bixby, Zonghao Gu, Edward Rothberg, and Dieter
  Weninger.
\newblock Presolve reductions in mixed integer programming.
\newblock {\em INFORMS Journal on Computing}, 32(2):473--506, 2020.

\bibitem{adler2017odl}
Jonas Adler, Holger Kohr, and Ozan Öktem.
\newblock Operator discretization library {(ODL)}, January 2017.

\bibitem{alacaoglu2019convergence}
Ahmet Alacaoglu, Olivier Fercoq, and Volkan Cevher.
\newblock On the convergence of stochastic primal-dual hybrid gradient.
\newblock {\em arXiv preprint arXiv:1911.00799}, 2019.

\bibitem{ali2017semismooth}
Alnur Ali, Eric Wong, and J~Zico Kolter.
\newblock A semismooth {N}ewton method for fast, generic convex programming.
\newblock In {\em International Conference on Machine Learning}, pages 70--79.
  PMLR, 2017.

\bibitem{amos2017optnet}
Brandon Amos and J.~Zico Kolter.
\newblock {O}pt{N}et: Differentiable optimization as a layer in neural
  networks.
\newblock In Doina Precup and Yee~Whye Teh, editors, {\em Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of {\em
  Proceedings of Machine Learning Research}, pages 136--145. PMLR, 06--11 Aug
  2017.

\bibitem{applegate2021infeasibility}
David Applegate, Mateo D{\'\i}az, Haihao Lu, and Miles Lubin.
\newblock Infeasibility detection with primal-dual hybrid gradient for
  large-scale linear programming.
\newblock {\em arXiv preprint arXiv:2102.04592}, 2021.

\bibitem{applegate2021restarts}
David {Applegate}, Oliver {Hinder}, Haihao {Lu}, and Miles {Lubin}.
\newblock {Faster First-Order Primal-Dual Methods for Linear Programming using
  Restarts and Sharpness}.
\newblock {\em arXiv preprint arXiv:2105.12715}, 2021.

\bibitem{arrow58}
K.~J. Arrow, L.~Hurwicz, and H.~Uzawa.
\newblock {\em Studies in linear and non-linear programming}.
\newblock Stanford University Press, 1958.

\bibitem{barabasi1999graph}
Albert-L{\'a}szl{\'o} Barab{\'a}si and R{\'e}ka Albert.
\newblock Emergence of scaling in random networks.
\newblock {\em Science}, 286(5439):509--512, 1999.

\bibitem{pmlr-v119-basu20a}
Kinjal Basu, Amol Ghoting, Rahul Mazumder, and Yao Pan.
\newblock {ECLIPSE}: An extreme-scale linear program solver for
  web-applications.
\newblock In Hal~Daumé III and Aarti Singh, editors, {\em Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 704--714, Virtual, 13--18
  Jul 2020. PMLR.

\bibitem{bauschke2011convex}
Heinz~H Bauschke and Patrick~L Combettes.
\newblock {\em Convex analysis and monotone operator theory in Hilbert spaces},
  volume 408.
\newblock Springer, 2 edition, 2017.

\bibitem{BeckBook}
Amir Beck.
\newblock {\em First-Order Methods in Optimization}.
\newblock Society for Industrial and Applied Mathematics, Philadelphia, PA,
  2017.

\bibitem{beck2019fom}
Amir Beck and Nili Guttmann-Beck.
\newblock {FOM}--a {MATLAB} toolbox of first-order methods for solving convex
  optimization problems.
\newblock {\em Optimization Methods and Software}, 34(1):172--193, 2019.

\bibitem{becker2011templates}
Stephen~R Becker, Emmanuel~J Cand{\`e}s, and Michael~C Grant.
\newblock Templates for convex cone problems with applications to sparse signal
  recovery.
\newblock {\em Mathematical programming computation}, 3(3):165, 2011.

\bibitem{bezanson2017julia}
Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral~B. Shah.
\newblock Julia: A fresh approach to numerical computing.
\newblock {\em SIAM Review}, 59(1):65--98, 2017.

\bibitem{boyd2011distributed}
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock {\em Foundations and Trends{\textregistered} in Machine learning},
  3(1):1--122, 2011.

\bibitem{boyd2004convex}
Stephen Boyd and Lieven Vandenberghe.
\newblock {\em Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem{chambolle2018stochastic}
Antonin Chambolle, Matthias~J Ehrhardt, Peter Richt{\'a}rik, and Carola-Bibiane
  Schonlieb.
\newblock Stochastic primal-dual hybrid gradient algorithm with arbitrary
  sampling and imaging applications.
\newblock {\em SIAM Journal on Optimization}, 28(4):2783--2808, 2018.

\bibitem{chambolle2011first}
Antonin Chambolle and Thomas Pock.
\newblock A first-order primal-dual algorithm for convex problems with
  applications to imaging.
\newblock {\em Journal of mathematical imaging and vision}, 40(1):120--145,
  2011.

\bibitem{chambolle2016ergodic}
Antonin Chambolle and Thomas Pock.
\newblock On the ergodic convergence rates of a first-order primal--dual
  algorithm.
\newblock {\em Mathematical Programming}, 159(1):253--287, 2016.

\bibitem{condat2013primal}
Laurent Condat.
\newblock A primal--dual splitting method for convex optimization involving
  {L}ipschitzian, proximable and linear composite terms.
\newblock {\em Journal of Optimization Theory and Applications},
  158(2):460--479, 2013.

\bibitem{dantzig2016linear}
George Dantzig.
\newblock {\em Linear programming and extensions}.
\newblock Princeton university press, 2016.

\bibitem{dantzig1990origins}
George~B Dantzig.
\newblock Origins of the simplex method.
\newblock In {\em A history of scientific computing}, pages 141--151.
  Association for Computing Machinery, New York, NY, USA, 1990.

\bibitem{eckstein1990alternating}
Jonathan Eckstein and Dimitri~P Bertsekas.
\newblock An alternating direction method for linear programming.
\newblock Technical Report LIDS-P-1967, Laboratory for Information and Decision
  Systems, Massachusetts Institute of Technology, 1990.

\bibitem{eckstein1992douglas}
Jonathan Eckstein and Dimitri~P Bertsekas.
\newblock On the {D}ouglas--{R}achford splitting method and the proximal point
  algorithm for maximal monotone operators.
\newblock {\em Mathematical Programming}, 55(1-3):293--318, 1992.

\bibitem{EcksteinMatyasfalvi2018}
Jonathan Eckstein and Gyorgy Matyasfalvi.
\newblock Efficient distributed-memory parallel matrix-vector multiplication
  with wide or tall unstructured sparse matrices.
\newblock {\em arXiv preprint arXiv:1812.00904}, 2018.

\bibitem{esser2010general}
Ernie Esser, Xiaoqun Zhang, and Tony~F Chan.
\newblock A general framework for a class of first order primal-dual algorithms
  for convex optimization in imaging science.
\newblock {\em {SIAM} Journal on Imaging Sciences}, 3(4):1015--1046, 2010.

\bibitem{fougner2018parameter}
Christopher Fougner and Stephen Boyd.
\newblock Parameter selection and preconditioning for a graph form solver.
\newblock In {\em Emerging Applications of Control and Systems Theory}, pages
  41--61. Springer, 2018.

\bibitem{gamrath2020scip}
Gerald Gamrath, Daniel Anderson, Ksenia Bestuzheva, Wei-Kun Chen, Leon Eifler,
  Maxime Gasse, Patrick Gemander, Ambros Gleixner, Leona Gottwald, Katrin
  Halbig, et~al.
\newblock The {SCIP} optimization suite 7.0.
\newblock ZIB-Report 20-10, Zuse Institut Berlin, 2020.

\bibitem{garstka_2019}
Michael Garstka, Mark Cannon, and Paul Goulart.
\newblock {COSMO}: A conic operator splitting method for large convex problems.
\newblock In {\em European Control Conference}, 2019.

\bibitem{netlib}
David~M Gay.
\newblock Electronic mail distribution of linear programming test problems.
\newblock {\em Mathematical Programming Society COAL Newsletter}, 13:10--12,
  1985.

\bibitem{gilpin2012first}
Andrew Gilpin, Javier Pena, and Tuomas Sandholm.
\newblock First-order algorithm with $\mathcal{O}(\ln(1/\epsilon))$ convergence
  for $\epsilon$-equilibrium in two-person zero-sum games.
\newblock {\em Mathematical programming}, 133(1):279--298, 2012.

\bibitem{giselsson2016linear}
Pontus Giselsson and Stephen Boyd.
\newblock Linear convergence and metric selection for {D}ouglas-{R}achford
  splitting and {ADMM}.
\newblock {\em IEEE Transactions on Automatic Control}, 62(2):532--544, 2016.

\bibitem{gleixner2021miplib}
Ambros Gleixner, Gregor Hendel, Gerald Gamrath, Tobias Achterberg, Michael
  Bastubbe, Timo Berthold, Philipp~M. Christophel, Kati Jarck, Thorsten Koch,
  Jeff Linderoth, Marco L\"ubbecke, Hans~D. Mittelmann, Derya Ozyurt, Ted~K.
  Ralphs, Domenico Salvagnin, and Yuji Shinano.
\newblock {MIPLIB 2017: Data-Driven Compilation of the 6th Mixed-Integer
  Programming Library}.
\newblock {\em Mathematical Programming Computation}, 2021.

\bibitem{goldmantucker}
A.~J. Goldman and A.~W. Tucker.
\newblock Theory of linear programming.
\newblock {\em Linear inequalities and related systems}, 38:53--97, 1956.

\bibitem{goldstein2015adaptive}
Tom Goldstein, Min Li, and Xiaoming Yuan.
\newblock Adaptive primal-dual splitting methods for statistical learning and
  image processing.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2089--2097, 2015.

\bibitem{goldstein2013adaptive}
Tom Goldstein, Min Li, Xiaoming Yuan, Ernie Esser, and Richard Baraniuk.
\newblock Adaptive primal-dual hybrid gradient methods for saddle-point
  problems.
\newblock {\em arXiv preprint arXiv:1305.0546}, 2013.

\bibitem{he2012convergence}
Bingsheng He and Xiaoming Yuan.
\newblock Convergence analysis of primal-dual algorithms for a saddle-point
  problem: from contraction perspective.
\newblock {\em SIAM Journal on Imaging Sciences}, 5(1):119--149, 2012.

\bibitem{korpelevich1976extragradient}
Galina~M Korpelevich.
\newblock The extragradient method for finding saddle points and other
  problems.
\newblock {\em Matecon}, 12:747--756, 1976.

\bibitem{Lan2011}
Guanghui Lan, Zhaosong Lu, and Renato D.~C. Monteiro.
\newblock Primal-dual first-order methods with $\mathcal{O}(1/\epsilon)$
  iteration-complexity for cone programming.
\newblock {\em Mathematical Programming}, 126(1):1--29, Jan 2011.

\bibitem{li2020asymptotically}
Xudong Li, Defeng Sun, and Kim-Chuan Toh.
\newblock An asymptotically superlinearly convergent semismooth newton
  augmented lagrangian method for linear programming.
\newblock {\em SIAM Journal on Optimization}, 30(3):2410--2440, 2020.

\bibitem{lin2021admm}
Tianyi Lin, Shiqian Ma, Yinyu Ye, and Shuzhong Zhang.
\newblock An {ADMM}-based interior-point method for large-scale linear
  programming.
\newblock {\em Optimization Methods and Software}, 36(2-3):389--424, 2021.

\bibitem{malitsky2018linesearch}
Yura Malitsky and Thomas Pock.
\newblock A first-order primal-dual algorithm with linesearch.
\newblock {\em SIAM Journal on Optimization}, 28(1):411--432, 2018.

\bibitem{maros2002computational}
I.~Maros.
\newblock {\em Computational Techniques of the Simplex Method}.
\newblock International Series in Operations Research \& Management Science.
  Springer US, 2002.

\bibitem{mittelmannbenchmark}
H.D. Mittelmann.
\newblock Decision tree for optimization software.
\newblock \url{http://plato.asu.edu/guide.html}, 2021.

\bibitem{nair2020neuralmip}
Vinod {Nair}, Sergey {Bartunov}, Felix {Gimeno}, Ingrid {von Glehn}, Pawel
  {Lichocki}, Ivan {Lobov}, Brendan {O'Donoghue}, Nicolas {Sonnerat}, Christian
  {Tjandraatmadja}, Pengming {Wang}, Ravichandra {Addanki}, Tharindi
  {Hapuarachchi}, Thomas {Keck}, James {Keeling}, Pushmeet {Kohli}, Ira
  {Ktena}, Yujia {Li}, Oriol {Vinyals}, and Yori {Zwols}.
\newblock {Solving Mixed Integer Programs Using Neural Networks}.
\newblock {\em arXiv preprint arXiv:2012.13349}, December 2020.

\bibitem{necoara2019linearfom}
I.~Necoara, Yu. Nesterov, and F.~Glineur.
\newblock Linear convergence of first order methods for non-strongly convex
  optimization.
\newblock {\em Mathematical Programming}, 175(1):69--107, May 2019.

\bibitem{nemirovski2004prox}
Arkadi Nemirovski.
\newblock Prox-method with rate of convergence {$O(1/t)$} for variational
  inequalities with {L}ipschitz continuous monotone operators and smooth
  convex-concave saddle point problems.
\newblock {\em SIAM Journal on Optimization}, 15(1):229--251, 2004.

\bibitem{nesterov2014pagerank}
Y~Nesterov.
\newblock Subgradient methods for huge-scale optimization problems.
\newblock {\em Mathematical Programming}, 146:275--297, 2014.

\bibitem{nesterov1983method}
Yurii Nesterov.
\newblock A method of solving a convex programming problem with convergence
  rate {$O(1/k^2)$}.
\newblock {\em Soviet Mathematics Doklady}, 27(2):372--376, 1983.

\bibitem{nesterov1994interior}
Yurii Nesterov and Arkadii Nemirovskii.
\newblock {\em Interior-point polynomial algorithms in convex programming},
  volume~13.
\newblock {SIAM}, 1994.

\bibitem{ocpb:16}
B.~O'Donoghue, E.~Chu, N.~Parikh, and S.~Boyd.
\newblock Conic optimization via operator splitting and homogeneous self-dual
  embedding.
\newblock {\em Journal of Optimization Theory and Applications},
  169(3):1042--1068, June 2016.

\bibitem{scs}
B.~O'Donoghue, E.~Chu, N.~Parikh, and S.~Boyd.
\newblock {SCS}: Splitting conic solver, version 2.1.0.
\newblock \url{https://github.com/cvxgrp/scs}, November 2017.

\bibitem{o2020operator}
Brendan O'Donoghue.
\newblock Operator splitting for a homogeneous embedding of the monotone linear
  complementarity problem.
\newblock {\em arXiv preprint arXiv:2004.02177}, 2020.

\bibitem{o-h1984history}
William Orchard-Hays.
\newblock History of mathematical programming systems.
\newblock {\em IEEE Annals of the History of Computing}, 6(3):296--312, 1984.

\bibitem{o2020equivalence}
Daniel O’Connor and Lieven Vandenberghe.
\newblock On the equivalence of the primal-dual hybrid gradient method and
  {D}ouglas--{R}achford splitting.
\newblock {\em Mathematical Programming}, 179(1):85--108, 2020.

\bibitem{o2015adaptive}
Brendan O’Donoghue and Emmanuel Candes.
\newblock Adaptive restart for accelerated gradient schemes.
\newblock {\em Foundations of computational mathematics}, 15(3):715--732, 2015.

\bibitem{parikh2014proximal}
Neal Parikh and Stephen Boyd.
\newblock Proximal algorithms.
\newblock {\em Foundations and Trends{\textregistered} in Optimization},
  1(3):127--239, 2014.

\bibitem{pock2011diagonal}
Thomas Pock and Antonin Chambolle.
\newblock Diagonal preconditioning for first order primal-dual algorithms in
  convex optimization.
\newblock In {\em 2011 International Conference on Computer Vision}, pages
  1762--1769. IEEE, 2011.

\bibitem{pock2009algorithm}
Thomas Pock, Daniel Cremers, Horst Bischof, and Antonin Chambolle.
\newblock An algorithm for minimizing the mumford-shah functional.
\newblock In {\em 2009 IEEE 12th International Conference on Computer Vision},
  pages 1133--1140. IEEE, 2009.

\bibitem{Renegar2019}
James Renegar.
\newblock Accelerated first-order methods for hyperbolic programming.
\newblock {\em Mathematical Programming}, 173(1):1--35, Jan 2019.

\bibitem{rockafellar1976monotone}
R~Tyrrell Rockafellar.
\newblock Monotone operators and the proximal point algorithm.
\newblock {\em {SIAM} journal on control and optimization}, 14(5):877--898,
  1976.

\bibitem{ruiz2001scaling}
Daniel Ruiz.
\newblock A scaling algorithm to equilibrate both rows and columns norms in
  matrices.
\newblock Technical report, CM-P00040415, 2001.

\bibitem{ryu2016primer}
Ernest~K Ryu and Stephen Boyd.
\newblock Primer on monotone operator methods.
\newblock {\em Appl. Comput. Math}, 15(1):3--43, 2016.

\bibitem{schrijver1998theory}
Alexander Schrijver.
\newblock {\em Theory of linear and integer programming}.
\newblock John Wiley \& Sons, 1998.

\bibitem{souto2020exploiting}
Mario Souto, Joaquim~D Garcia, and {\'A}lvaro Veiga.
\newblock Exploiting low-rank structure in semidefinite programming by
  approximate operator splitting.
\newblock {\em Optimization}, pages 1--28, 2020.

\bibitem{stellato2018osqp}
B.~Stellato, G.~Banjac, P.~Goulart, A.~Bemporad, and S.~Boyd.
\newblock {OSQP}: an operator splitting solver for quadratic programs.
\newblock {\em Mathematical Programming Computation}, 12(4):637--672, 2020.

\bibitem{SpmvGPUBenchmark2020}
Yuhsiang~M. Tsai, Terry Cojean, and Hartwig Anzt.
\newblock Sparse linear algebra on {AMD} and {NVIDIA} {GPUs} -- the race is on.
\newblock In Ponnuswamy Sadayappan, Bradford~L. Chamberlain, Guido Juckeland,
  and Hatem Ltaief, editors, {\em High Performance Computing}, pages 309--327,
  Cham, 2020. Springer International Publishing.

\bibitem{vanderbei2015linear}
Robert~J Vanderbei et~al.
\newblock {\em Linear programming}, volume~3.
\newblock Springer, 2015.

\bibitem{wang2017admmlp}
Sinong Wang and Ness Shroff.
\newblock A new alternating direction method for linear programming.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem{yang2018rsg}
Tianbao Yang and Qihang Lin.
\newblock {RSG}: Beating subgradient method without smoothness and strong
  convexity.
\newblock {\em The Journal of Machine Learning Research}, 19(1):236--268, 2018.

\bibitem{zhu2008efficient}
Mingqiang Zhu and Tony Chan.
\newblock An efficient primal-dual hybrid gradient algorithm for total
  variation image restoration.
\newblock {\em UCLA CAM Report}, 34:8--34, 2008.

\end{thebibliography}
