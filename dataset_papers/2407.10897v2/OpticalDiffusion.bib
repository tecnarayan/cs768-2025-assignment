
@article{lin_all-optical_2018,
	title = {All-optical machine learning using diffractive deep neural networks},
	volume = {361},
	issn = {10959203},
	doi = {10.1126/science.aat8084},
	abstract = {Deep learning has been transforming our ability to execute advanced inference tasks using computers. Here we introduce a physical mechanism to perform machine learning by demonstrating an all-optical diffractive deep neural network (D2NN) architecture that can implement various functions following the deep learning–based design of passive diffractive layers that work collectively. We created 3D-printed D2NNs that implement classification of images of handwritten digits and fashion products, as well as the function of an imaging lens at a terahertz spectrum. Our all-optical deep learning framework can perform, at the speed of light, various complex functions that computer-based neural networks can execute; will find applications in all-optical image analysis, feature detection, and object classification; and will also enable new camera designs and optical components that perform distinctive tasks using D2NNs.},
	number = {6406},
	journal = {Science},
	author = {Lin, Xing and Rivenson, Yair and Yardimci, Nezih T. and Veli, Muhammed and Luo, Yi and Jarrahi, Mona and Ozcan, Aydogan},
	month = sep,
	year = {2018},
	pmid = {30049787},
	note = {523 citations (Semantic Scholar/DOI) [2022-01-23]
Publisher: American Association for the Advancement of Science
\_eprint: 1804.08711},
	pages = {1004--1008},
}

@article{wetzstein_inference_2020,
	title = {Inference in artificial intelligence with deep optics and photonics},
	volume = {588},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2973-6},
	doi = {10.1038/s41586-020-2973-6},
	abstract = {Artificial intelligence tasks across numerous applications require accelerators for fast and low-power execution. Optical computing systems may be able to meet these domain-specific needs but, despite half a century of research, general-purpose optical computing systems have yet to mature into a practical technology. Artificial intelligence inference, however, especially for visual computing applications, may offer opportunities for inference based on optical and photonic systems. In this Perspective, we review recent work on optical computing for artificial intelligence applications and discuss its promise and challenges. Recent work on optical computing for artificial intelligence applications is reviewed and the potential and challenges of all-optical and hybrid optical networks are discussed.},
	number = {7836},
	journal = {Nature 2020 588:7836},
	author = {Wetzstein, Gordon and Ozcan, Aydogan and Gigan, Sylvain and Fan, Shanhui and Englund, Dirk and Soljačić, Marin and Denz, Cornelia and Miller, David A. B. and Psaltis, Demetri},
	month = dec,
	year = {2020},
	note = {101 citations (Semantic Scholar/DOI) [2022-01-23]
Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, multidisciplinary, Science},
	pages = {39--47},
	file = {Attachment:C\:\\Users\\ilker\\Zotero\\storage\\DWPD8XEX\\Wetzstein et al. - 2020 - Inference in artificial intelligence with deep optics and photonics(2).pdf:application/pdf},
}

@article{kulce_all-optical_2021,
	title = {All-optical synthesis of an arbitrary linear transformation using diffractive surfaces},
	volume = {10},
	issn = {2047-7538},
	url = {https://www.nature.com/articles/s41377-021-00623-5},
	doi = {10.1038/s41377-021-00623-5},
	abstract = {Spatially-engineered diffractive surfaces have emerged as a powerful framework to control light-matter interactions for statistical inference and the design of task-specific optical components. Here, we report the design of diffractive surfaces to all-optically perform arbitrary complex-valued linear transformations between an input (Ni) and output (No), where Ni and No represent the number of pixels at the input and output fields-of-view (FOVs), respectively. First, we consider a single diffractive surface and use a matrix pseudoinverse-based method to determine the complex-valued transmission coefficients of the diffractive features/neurons to all-optically perform a desired/target linear transformation. In addition to this data-free design approach, we also consider a deep learning-based design method to optimize the transmission coefficients of diffractive surfaces by using examples of input/output fields corresponding to the target transformation. We compared the all-optical transformation errors and diffraction efficiencies achieved using data-free designs as well as data-driven (deep learning-based) diffractive designs to all-optically perform (i) arbitrarily-chosen complex-valued transformations including unitary, nonunitary, and noninvertible transforms, (ii) 2D discrete Fourier transformation, (iii) arbitrary 2D permutation operations, and (iv) high-pass filtered coherent imaging. Our analyses reveal that if the total number (N) of spatially-engineered diffractive features/neurons is ≥Ni × No, both design methods succeed in all-optical implementation of the target transformation, achieving negligible error. However, compared to data-free designs, deep learning-based diffractive designs are found to achieve significantly larger diffraction efficiencies for a given N and their all-optical transformations are more accurate for N {\textless} Ni × No. These conclusions are generally applicable to various optical processors that employ spatially-engineered diffractive surfaces.},
	number = {1},
	journal = {Light: Science \& Applications 2021 10:1},
	author = {Kulce, Onur and Mengu, Deniz and Rivenson, Yair and Ozcan, Aydogan},
	month = sep,
	year = {2021},
	note = {3 citations (Semantic Scholar/DOI) [2022-01-23]
Publisher: Nature Publishing Group},
	keywords = {Optical techniques, Transformation optics},
	pages = {1--21},
}

@article{zhou_large-scale_2021,
	title = {Large-scale neuromorphic optoelectronic computing with a reconfigurable diffractive processing unit},
	volume = {15},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1749-4893},
	url = {https://www.nature.com/articles/s41566-021-00796-w},
	doi = {10.1038/s41566-021-00796-w},
	abstract = {There is an ever-growing demand for artificial intelligence. Optical processors, which compute with photons instead of electrons, can fundamentally accelerate the development of artificial intelligence by offering substantially improved computing performance. There has been long-term interest in optically constructing the most widely used artificial-intelligence architecture, that is, artificial neural networks, to achieve brain-inspired information processing at the speed of light. However, owing to restrictions in design flexibility and the accumulation of system errors, existing processor architectures are not reconfigurable and have limited model complexity and experimental performance. Here, we propose the reconfigurable diffractive processing unit, an optoelectronic fused computing architecture based on the diffraction of light, which can support different neural networks and achieve a high model complexity with millions of neurons. Along with the developed adaptive training approach to circumvent system errors, we achieved excellent experimental accuracies for high-speed image and video recognition over benchmark datasets and a computing performance superior to that of cutting-edge electronic computing platforms.},
	language = {en},
	number = {5},
	urldate = {2022-03-30},
	journal = {Nature Photonics},
	author = {Zhou, Tiankuang and Lin, Xing and Wu, Jiamin and Chen, Yitong and Xie, Hao and Li, Yipeng and Fan, Jingtao and Wu, Huaqiang and Fang, Lu and Dai, Qionghai},
	month = may,
	year = {2021},
	note = {Number: 5
Publisher: Nature Publishing Group},
	keywords = {Applied optics, Imaging and sensing},
	pages = {367--373},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\YNBZMUVD\\Zhou et al. - 2021 - Large-scale neuromorphic optoelectronic computing .pdf:application/pdf;Snapshot:C\:\\Users\\ilker\\Zotero\\storage\\B2359DD7\\s41566-021-00796-w.html:text/html},
}

@article{chang_hybrid_2018,
	title = {Hybrid optical-electronic convolutional neural networks with optimized diffractive optics for image classification},
	volume = {8},
	copyright = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-30619-y},
	doi = {10.1038/s41598-018-30619-y},
	abstract = {Convolutional neural networks (CNNs) excel in a wide variety of computer vision applications, but their high performance also comes at a high computational cost. Despite efforts to increase efficiency both algorithmically and with specialized hardware, it remains difficult to deploy CNNs in embedded systems due to tight power budgets. Here we explore a complementary strategy that incorporates a layer of optical computing prior to electronic computing, improving performance on image classification tasks while adding minimal electronic computational cost or processing time. We propose a design for an optical convolutional layer based on an optimized diffractive optical element and test our design in two simulations: a learned optical correlator and an optoelectronic two-layer CNN. We demonstrate in simulation and with an optical prototype that the classification accuracies of our optical systems rival those of the analogous electronic implementations, while providing substantial savings on computational cost.},
	language = {en},
	number = {1},
	urldate = {2022-03-30},
	journal = {Scientific Reports},
	author = {Chang, Julie and Sitzmann, Vincent and Dun, Xiong and Heidrich, Wolfgang and Wetzstein, Gordon},
	month = aug,
	year = {2018},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computer science, Imaging and sensing},
	pages = {12324},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\DJ4KEWQV\\Chang et al. - 2018 - Hybrid optical-electronic convolutional neural net.pdf:application/pdf;Snapshot:C\:\\Users\\ilker\\Zotero\\storage\\YIIIWHB9\\s41598-018-30619-y.html:text/html},
}

@article{wright_deep_2022,
	title = {Deep physical neural networks trained with backpropagation},
	volume = {601},
	copyright = {2022 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-04223-6},
	doi = {10.1038/s41586-021-04223-6},
	abstract = {Deep-learning models have become pervasive tools in science and engineering. However, their energy requirements now increasingly limit their scalability1. Deep-learning accelerators2–9 aim to perform deep learning energy-efficiently, usually targeting the inference phase and often by exploiting physical substrates beyond conventional electronics. Approaches so far10–22 have been unable to apply the backpropagation algorithm to train unconventional novel hardware in situ. The advantages of backpropagation have made it the de facto training method for large-scale neural networks, so this deficiency constitutes a major impediment. Here we introduce a hybrid in situ–in silico algorithm, called physics-aware training, that applies backpropagation to train controllable physical systems. Just as deep learning realizes computations with deep neural networks made from layers of mathematical functions, our approach allows us to train deep physical neural networks made from layers of controllable physical systems, even when the physical layers lack any mathematical isomorphism to conventional artificial neural network layers. To demonstrate the universality of our approach, we train diverse physical neural networks based on optics, mechanics and electronics to experimentally perform audio and image classification tasks. Physics-aware training combines the scalability of backpropagation with the automatic mitigation of imperfections and noise achievable with in situ algorithms. Physical neural networks have the potential to perform machine learning faster and more energy-efficiently than conventional electronic processors and, more broadly, can endow physical systems with automatically designed physical functionalities, for example, for robotics23–26, materials27–29 and smart sensors30–32.},
	language = {en},
	number = {7894},
	urldate = {2022-02-05},
	journal = {Nature},
	author = {Wright, Logan G. and Onodera, Tatsuhiro and Stein, Martin M. and Wang, Tianyu and Schachter, Darren T. and Hu, Zoey and McMahon, Peter L.},
	month = jan,
	year = {2022},
	note = {Number: 7894
Publisher: Nature Publishing Group},
	keywords = {Computational science, Nonlinear optics},
	pages = {549--555},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\MUSJQ5TC\\Wright et al. - 2022 - Deep physical neural networks trained with backpro.pdf:application/pdf;Snapshot:C\:\\Users\\ilker\\Zotero\\storage\\XQ37RRQC\\s41586-021-04223-6.html:text/html},
}

@article{shen_deep_2017,
	title = {Deep learning with coherent nanophotonic circuits},
	volume = {11},
	issn = {17494893},
	doi = {10.1038/NPHOTON.2017.93},
	abstract = {Artificial neural networks are computational network models inspired by signal processing in the brain. These models have dramatically improved performance for many machine-learning tasks, including speech and image recognition. However, today's computing hardware is inefficient at implementing neural networks, in large part because much of it was designed for von Neumann computing schemes. Significant effort has been made towards developing electronic architectures tuned to implement artificial neural networks that exhibit improved computational speed and accuracy. Here, we propose a new architecture for a fully optical neural network that, in principle, could offer an enhancement in computational speed and power efficiency over state-of-the-art electronics for conventional inference tasks. We experimentally demonstrate the essential part of the concept using a programmable nanophotonic processor featuring a cascaded array of 56 programmable Mach-Zehnder interferometers in a silicon photonic integrated circuit and show its utility for vowel recognition.},
	number = {7},
	journal = {Nature Photonics},
	author = {Shen, Yichen and Harris, Nicholas C. and Skirlo, Scott and Prabhu, Mihika and Baehr-Jones, Tom and Hochberg, Michael and Sun, Xin and Zhao, Shijie and Larochelle, Hugo and Englund, Dirk and Soljacic, Marin},
	month = jun,
	year = {2017},
	note = {925 citations (Semantic Scholar/DOI) [2022-01-23]
Publisher: Nature Publishing Group
\_eprint: 1610.02365},
	keywords = {Integrated optics, Silicon photonics},
	pages = {441--446},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\RWXBFMDF\\Shen et al. - 2017 - Deep learning with coherent nanophotonic circuits.pdf:application/pdf;Snapshot:C\:\\Users\\ilker\\Zotero\\storage\\LECB69JR\\nphoton.2017.html:text/html},
}

@article{isil_super-resolution_2022,
	title = {Super-resolution image display using diffractive decoders},
	volume = {8},
	url = {https://www.science.org/doi/10.1126/sciadv.add3433},
	doi = {10.1126/sciadv.add3433},
	abstract = {High-resolution image projection over a large field of view (FOV) is hindered by the restricted space-bandwidth product (SBP) of wavefront modulators. We report a deep learning–enabled diffractive display based on a jointly trained pair of an electronic encoder and a diffractive decoder to synthesize/project super-resolved images using low-resolution wavefront modulators. The digital encoder rapidly preprocesses the high-resolution images so that their spatial information is encoded into low-resolution patterns, projected via a low SBP wavefront modulator. The diffractive decoder processes these low-resolution patterns using transmissive layers structured using deep learning to all-optically synthesize/project super-resolved images at its output FOV. This diffractive image display can achieve a super-resolution factor of {\textasciitilde}4, increasing the SBP by {\textasciitilde}16-fold. We experimentally validate its success using 3D-printed diffractive decoders that operate at the terahertz spectrum. This diffractive image decoder can be scaled to operate at visible wavelengths and used to design large SBP displays that are compact, low power, and computationally efficient.},
	number = {48},
	urldate = {2023-08-20},
	journal = {Science Advances},
	author = {Isil, Cagatay and Mengu, Deniz and Zhao, Yifan and Tabassum, Anika and Li, Jingxi and Luo, Yi and Jarrahi, Mona and Ozcan, Aydogan},
	month = dec,
	year = {2022},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eadd3433},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\T667RM2K\\Işıl et al. - 2022 - Super-resolution image display using diffractive d.pdf:application/pdf},
}

@book{goodman_introduction_2017,
	address = {New York},
	edition = {Fourth},
	title = {Introduction to {Fourier} {Optics}},
	isbn = {978-1-319-11916-4},
	abstract = {Applying Fourier analysis specifically to optics, Introduction to Fourier Optics focuses on its application to diffraction, imaging, optical information processing, holography, and optical communications enabling you to successfully comprehend complex topics.},
	language = {English},
	publisher = {W. H. Freeman},
	author = {Goodman, Joseph},
	month = may,
	year = {2017},
}

@article{hu_diffractive_2024,
	title = {Diffractive optical computing in free space},
	volume = {15},
	copyright = {2024 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-024-45982-w},
	doi = {10.1038/s41467-024-45982-w},
	abstract = {Structured optical materials create new computing paradigms using photons, with transformative impact on various fields, including machine learning, computer vision, imaging, telecommunications, and sensing. This Perspective sheds light on the potential of free-space optical systems based on engineered surfaces for advancing optical computing. Manipulating light in unprecedented ways, emerging structured surfaces enable all-optical implementation of various mathematical functions and machine learning tasks. Diffractive networks, in particular, bring deep-learning principles into the design and operation of free-space optical systems to create new functionalities. Metasurfaces consisting of deeply subwavelength units are achieving exotic optical responses that provide independent control over different properties of light and can bring major advances in computational throughput and data-transfer bandwidth of free-space optical processors. Unlike integrated photonics-based optoelectronic systems that demand preprocessed inputs, free-space optical processors have direct access to all the optical degrees of freedom that carry information about an input scene/object without needing digital recovery or preprocessing of information. To realize the full potential of free-space optical computing architectures, diffractive surfaces and metasurfaces need to advance symbiotically and co-evolve in their designs, 3D fabrication/integration, cascadability, and computing accuracy to serve the needs of next-generation machine vision, computational imaging, mathematical computing, and telecommunication technologies.},
	language = {en},
	number = {1},
	urldate = {2024-05-18},
	journal = {Nature Communications},
	author = {Hu, Jingtian and Mengu, Deniz and Tzarouchis, Dimitrios C. and Edwards, Brian and Engheta, Nader and Ozcan, Aydogan},
	month = feb,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Optics and photonics, Mathematics and computing},
	pages = {1525},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\2BHH5J5S\\Hu et al. - 2024 - Diffractive optical computing in free space.pdf:application/pdf},
}

@inproceedings{song_denoising_2020,
	title = {Denoising {Diffusion} {Implicit} {Models}},
	url = {https://openreview.net/forum?id=St1giarCHLP},
	abstract = {Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples \$10 {\textbackslash}times\$ to \$50 {\textbackslash}times\$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error.},
	language = {en},
	urldate = {2024-05-18},
	author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
	month = oct,
	year = {2020},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\YTSB8BUH\\Song et al. - 2020 - Denoising Diffusion Implicit Models.pdf:application/pdf},
}

@inproceedings{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
	urldate = {2024-05-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	year = {2020},
	pages = {6840--6851},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\CT56XK9N\\Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf},
}

@inproceedings{sohl-dickstein_deep_2015,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	url = {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	language = {en},
	urldate = {2024-05-19},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	pages = {2256--2265},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\ZGRGJ3CD\\Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf:application/pdf},
}

@inproceedings{song_score-based_2020,
	title = {Score-{Based} {Generative} {Modeling} through {Stochastic} {Differential} {Equations}},
	url = {https://openreview.net/forum?id=PxTIG12RRHS&utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter},
	abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of \$1024{\textbackslash}times 1024\$ images for the first time from a score-based generative model.},
	language = {en},
	urldate = {2024-05-19},
	author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
	month = oct,
	year = {2020},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\RQ5SMFEC\\Song et al. - 2020 - Score-Based Generative Modeling through Stochastic.pdf:application/pdf},
}

@inproceedings{nichol_improved_2021,
	title = {Improved {Denoising} {Diffusion} {Probabilistic} {Models}},
	url = {https://proceedings.mlr.press/v139/nichol21a.html},
	abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code and pre-trained models at https://github.com/openai/improved-diffusion.},
	language = {en},
	urldate = {2024-05-19},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Nichol, Alexander Quinn and Dhariwal, Prafulla},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {8162--8171},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\HYTE9568\\Nichol and Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf:application/pdf},
}

@inproceedings{saharia_photorealistic_2022,
	title = {Photorealistic {Text}-to-{Image} {Diffusion} {Models} with {Deep} {Language} {Understanding}},
	url = {https://openreview.net/forum?id=08Yk-n5l2Al},
	abstract = {We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g., T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment.},
	language = {en},
	urldate = {2024-05-19},
	author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Gontijo-Lopes, Raphael and Ayan, Burcu Karagol and Salimans, Tim and Ho, Jonathan and Fleet, David J. and Norouzi, Mohammad},
	month = oct,
	year = {2022},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\UVJHCPN5\\Saharia et al. - 2022 - Photorealistic Text-to-Image Diffusion Models with.pdf:application/pdf},
}

@inproceedings{rombach_high-resolution_2022,
	address = {New Orleans, LA, USA},
	title = {High-{Resolution} {Image} {Synthesis} with {Latent} {Diffusion} {Models}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9878449/},
	doi = {10.1109/CVPR52688.2022.01042},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and ﬂexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the ﬁrst time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual ﬁdelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and ﬂexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while signiﬁcantly reducing computational requirements compared to pixel-based DMs.},
	language = {en},
	urldate = {2024-05-19},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bjorn},
	month = jun,
	year = {2022},
	pages = {10674--10685},
	file = {Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf:C\:\\Users\\ilker\\Zotero\\storage\\DT4RI9D8\\Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf:application/pdf},
}

@inproceedings{jalal_robust_2021,
	title = {Robust {Compressed} {Sensing} {MRI} with {Deep} {Generative} {Priors}},
	url = {https://openreview.net/forum?id=wHoIjrT6MMb},
	abstract = {The CSGM framework (Bora-Jalal-Price-Dimakis'17) has shown that deep generative priors can be powerful tools for solving inverse problems. However, to date this framework has been empirically successful only on certain datasets (for example, human faces and MNIST digits), and it is known to perform poorly on out-of-distribution samples. In this paper, we present the first successful application of the CSGM framework on clinical MRI data. We train a generative prior on brain scans from the fastMRI dataset, and show that posterior sampling via Langevin dynamics achieves high quality reconstructions. Furthermore, our experiments and theory show that posterior sampling is robust to changes in the ground-truth distribution and measurement process. Our code and models are available at: {\textbackslash}url\{https://github.com/utcsilab/csgm-mri-langevin\}.},
	language = {en},
	urldate = {2024-05-19},
	author = {Jalal, Ajil and Arvinte, Marius and Daras, Giannis and Price, Eric and Dimakis, Alex and Tamir, Jonathan},
	month = nov,
	year = {2021},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\CWHRTXQH\\Jalal et al. - 2021 - Robust Compressed Sensing MRI with Deep Generative.pdf:application/pdf},
}

@article{saharia_image_2023,
	title = {Image {Super}-{Resolution} via {Iterative} {Refinement}},
	volume = {45},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/document/9887996},
	doi = {10.1109/TPAMI.2022.3204461},
	abstract = {We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models (Ho et al. 2020), (Sohl-Dickstein et al. 2015) to image-to-image translation, and performs super-resolution through a stochastic iterative denoising process. Output images are initialized with pure Gaussian noise and iteratively refined using a U-Net architecture that is trained on denoising at various noise levels, conditioned on a low-resolution input image. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8× face super-resolution task on CelebA-HQ for which SR3 achieves a fool rate close to 50\%, suggesting photo-realistic outputs, while GAN baselines do not exceed a fool rate of 34\%. We evaluate SR3 on a 4× super-resolution task on ImageNet, where SR3 outperforms baselines in human evaluation and classification accuracy of a ResNet-50 classifier trained on high-resolution images. We further show the effectiveness of SR3 in cascaded image generation, where a generative model is chained with super-resolution models to synthesize high-resolution images with competitive FID scores on the class-conditional 256×256 ImageNet generation challenge.},
	number = {4},
	urldate = {2024-05-19},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
	month = apr,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Task analysis, Data models, deep generative models, diffusion models, Diffusion processes, Faces, Image super-resolution, Iterative methods, Noise reduction, Superresolution},
	pages = {4713--4726},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\ilker\\Zotero\\storage\\WLAWVPXU\\9887996.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\3TR7UGUW\\Saharia et al. - 2023 - Image Super-Resolution via Iterative Refinement.pdf:application/pdf},
}

@inproceedings{dhariwal_diffusion_2021,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	url = {https://openreview.net/forum?id=AAWuCvzaVt},
	abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\${\textbackslash}times\$128, 4.59 on ImageNet 256\${\textbackslash}times\$256, and 7.72 on ImageNet 512\${\textbackslash}times\$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\${\textbackslash}times\$256 and 3.85 on ImageNet 512\${\textbackslash}times\$512.},
	language = {en},
	urldate = {2024-05-19},
	author = {Dhariwal, Prafulla and Nichol, Alexander Quinn},
	month = nov,
	year = {2021},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\4XQT74YU\\Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf:application/pdf},
}

@article{kufeoglu_bitcoin_2019,
	title = {Bitcoin mining: {A} global review of energy and power demand},
	volume = {58},
	issn = {2214-6296},
	shorttitle = {Bitcoin mining},
	url = {https://www.sciencedirect.com/science/article/pii/S2214629619305948},
	doi = {10.1016/j.erss.2019.101273},
	abstract = {After its introduction in 2008, increasing Bitcoin prices and a booming number of other cryptocurrencies lead to a growing discussion of how much energy is consumed during the production of these currencies. Being the most expensive and the most popular cryptocurrency, both the business world and the research community have started to question the energy intensity of Bitcoin mining. This paper only focuses on computational power demand during the proof-of-work process rather than estimating the whole energy intensity of mining. We make use of 160GB of Bitcoin blockchain data to estimate the energy consumption and power demand of Bitcoin mining. We considered the performance of 269 different hardware models (CPU, GPU, FPGA, and ASIC). For estimations, we defined two metrics, namely; minimum consumption and maximum consumption. The targeted time span for the analysis was from 3 January 2009 to 5 June 2018. We show that the historical peak of power consumption of Bitcoin mining took place during the bi-weekly period commencing on 18 December 2017 with a demand of between 1.3 and 14.8 GW. This maximum demand figure was between the installed capacities of Finland (∼16 GW) and Denmark (∼14 GW). We also show that, during June 2018, energy consumption of Bitcoin mining from difficulty recalculation was between 15.47 and 50.24 TWh per year.},
	urldate = {2024-05-19},
	journal = {Energy Research \& Social Science},
	author = {Küfeoğlu, Sinan and Özkuran, Mahmut},
	month = dec,
	year = {2019},
	keywords = {Energy, Bitcoin, Blockchain, Consumption, Mining},
	pages = {101273},
	file = {ScienceDirect Snapshot:C\:\\Users\\ilker\\Zotero\\storage\\C5WVQYZS\\S2214629619305948.html:text/html},
}

@article{shih_parallel_2023,
	title = {Parallel {Sampling} of {Diffusion} {Models}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/0d1986a61e30e5fa408c81216a616e20-Abstract-Conference.html},
	language = {en},
	urldate = {2024-05-19},
	journal = {Advances in Neural Information Processing Systems},
	author = {Shih, Andy and Belkhale, Suneel and Ermon, Stefano and Sadigh, Dorsa and Anari, Nima},
	month = dec,
	year = {2023},
	pages = {4263--4276},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\67AT68PH\\Shih et al. - 2023 - Parallel Sampling of Diffusion Models.pdf:application/pdf},
}

@article{leiserson_theres_2020,
	title = {There’s plenty of room at the {Top}: {What} will drive computer performance after {Moore}’s law?},
	volume = {368},
	shorttitle = {There’s plenty of room at the {Top}},
	url = {https://www.science.org/doi/10.1126/science.aam9744},
	doi = {10.1126/science.aam9744},
	abstract = {The miniaturization of semiconductor transistors has driven the growth in computer performance for more than 50 years. As miniaturization approaches its limits, bringing an end to Moore’s law, performance gains will need to come from software, algorithms, and hardware. We refer to these technologies as the “Top” of the computing stack to distinguish them from the traditional technologies at the “Bottom”: semiconductor physics and silicon-fabrication technology. In the post-Moore era, the Top will provide substantial performance gains, but these gains will be opportunistic, uneven, and sporadic, and they will suffer from the law of diminishing returns. Big system components offer a promising context for tackling the challenges of working at the Top.},
	number = {6495},
	urldate = {2024-05-19},
	journal = {Science},
	author = {Leiserson, Charles E. and Thompson, Neil C. and Emer, Joel S. and Kuszmaul, Bradley C. and Lampson, Butler W. and Sanchez, Daniel and Schardl, Tao B.},
	month = jun,
	year = {2020},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eaam9744},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\M8RK3HP2\\Leiserson et al. - 2020 - There’s plenty of room at the Top What will drive.pdf:application/pdf},
}

@misc{ramesh_hierarchical_2022,
	title = {Hierarchical {Text}-{Conditional} {Image} {Generation} with {CLIP} {Latents}},
	url = {https://arxiv.org/abs/2204.06125v1},
	abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
	language = {en},
	urldate = {2024-05-22},
	journal = {arXiv.org},
	author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
	month = apr,
	year = {2022},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\CUXZXY9E\\Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf:application/pdf},
}

@misc{salimans_progressive_2022,
	title = {Progressive {Distillation} for {Fast} {Sampling} of {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2202.00512},
	doi = {10.48550/arXiv.2202.00512},
	abstract = {Diffusion models have recently shown great promise for generative modeling, outperforming GANs on perceptual quality and autoregressive models at density estimation. A remaining downside is their slow sampling time: generating high quality samples takes many hundreds or thousands of model evaluations. Here we make two contributions to help eliminate this downside: First, we present new parameterizations of diffusion models that provide increased stability when using few sampling steps. Second, we present a method to distill a trained deterministic diffusion sampler, using many steps, into a new diffusion model that takes half as many sampling steps. We then keep progressively applying this distillation procedure to our model, halving the number of required sampling steps each time. On standard image generation benchmarks like CIFAR-10, ImageNet, and LSUN, we start out with state-of-the-art samplers taking as many as 8192 steps, and are able to distill down to models taking as few as 4 steps without losing much perceptual quality; achieving, for example, a FID of 3.0 on CIFAR-10 in 4 steps. Finally, we show that the full progressive distillation procedure does not take more time than it takes to train the original model, thus representing an efficient solution for generative modeling using diffusion at both train and test time.},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Salimans, Tim and Ho, Jonathan},
	month = jun,
	year = {2022},
	note = {arXiv:2202.00512 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ilker\\Zotero\\storage\\5ZQRD2VE\\Salimans and Ho - 2022 - Progressive Distillation for Fast Sampling of Diff.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ilker\\Zotero\\storage\\FKR9QS56\\2202.html:text/html},
}

@misc{kong_fast_2021,
	title = {On {Fast} {Sampling} of {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2106.00132},
	doi = {10.48550/arXiv.2106.00132},
	abstract = {In this work, we propose FastDPM, a unified framework for fast sampling in diffusion probabilistic models. FastDPM generalizes previous methods and gives rise to new algorithms with improved sample quality. We systematically investigate the fast sampling methods under this framework across different domains, on different datasets, and with different amount of conditional information provided for generation. We find the performance of a particular method depends on data domains (e.g., image or audio), the trade-off between sampling speed and sample quality, and the amount of conditional information. We further provide insights and recipes on the choice of methods for practitioners.},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Kong, Zhifeng and Ping, Wei},
	month = jun,
	year = {2021},
	note = {arXiv:2106.00132 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\ilker\\Zotero\\storage\\HPV2DTXL\\Kong and Ping - 2021 - On Fast Sampling of Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ilker\\Zotero\\storage\\W33CMLAA\\2106.html:text/html},
}

@misc{wang_diffusebot_2023,
	title = {{DiffuseBot}: {Breeding} {Soft} {Robots} {With} {Physics}-{Augmented} {Generative} {Diffusion} {Models}},
	shorttitle = {{DiffuseBot}},
	url = {http://arxiv.org/abs/2311.17053},
	doi = {10.48550/arXiv.2311.17053},
	abstract = {Nature evolves creatures with a high complexity of morphological and behavioral intelligence, meanwhile computational methods lag in approaching that diversity and efficacy. Co-optimization of artificial creatures' morphology and control in silico shows promise for applications in physical soft robotics and virtual character creation; such approaches, however, require developing new learning algorithms that can reason about function atop pure structure. In this paper, we present DiffuseBot, a physics-augmented diffusion model that generates soft robot morphologies capable of excelling in a wide spectrum of tasks. DiffuseBot bridges the gap between virtually generated content and physical utility by (i) augmenting the diffusion process with a physical dynamical simulation which provides a certificate of performance, and (ii) introducing a co-design procedure that jointly optimizes physical design and control by leveraging information about physical sensitivities from differentiable simulation. We showcase a range of simulated and fabricated robots along with their capabilities. Check our website at https://diffusebot.github.io/},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Wang, Tsun-Hsuan and Zheng, Juntian and Ma, Pingchuan and Du, Yilun and Kim, Byungchul and Spielberg, Andrew and Tenenbaum, Joshua and Gan, Chuang and Rus, Daniela},
	month = nov,
	year = {2023},
	note = {arXiv:2311.17053 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:C\:\\Users\\ilker\\Zotero\\storage\\IHI3BXUU\\Wang et al. - 2023 - DiffuseBot Breeding Soft Robots With Physics-Augm.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ilker\\Zotero\\storage\\9NYKTLYV\\2311.html:text/html},
}

@misc{anderson_optical_2023,
	title = {Optical {Transformers}},
	url = {http://arxiv.org/abs/2302.10360},
	doi = {10.48550/arXiv.2302.10360},
	abstract = {The rapidly increasing size of deep-learning models has caused renewed and growing interest in alternatives to digital computers to dramatically reduce the energy cost of running state-of-the-art neural networks. Optical matrix-vector multipliers are best suited to performing computations with very large operands, which suggests that large Transformer models could be a good target for optical computing. To test this idea, we performed small-scale optical experiments with a prototype accelerator to demonstrate that Transformer operations can run on optical hardware despite noise and errors. Using simulations, validated by our experiments, we then explored the energy efficiency of optical implementations of Transformers and identified scaling laws for model performance with respect to optical energy usage. We found that the optical energy per multiply-accumulate (MAC) scales as \${\textbackslash}frac\{1\}\{d\}\$ where \$d\$ is the Transformer width, an asymptotic advantage over digital systems. We conclude that with well-engineered, large-scale optical hardware, it may be possible to achieve a \$100 {\textbackslash}times\$ energy-efficiency advantage for running some of the largest current Transformer models, and that if both the models and the optical hardware are scaled to the quadrillion-parameter regime, optical computers could have a \${\textgreater}8,000{\textbackslash}times\$ energy-efficiency advantage over state-of-the-art digital-electronic processors that achieve 300 fJ/MAC. We analyzed how these results motivate and inform the construction of future optical accelerators along with optics-amenable deep-learning approaches. With assumptions about future improvements to electronics and Transformer quantization techniques (5\${\textbackslash}times\$ cheaper memory access, double the digital--analog conversion efficiency, and 4-bit precision), we estimated that optical computers' advantage against current 300-fJ/MAC digital processors could grow to \${\textgreater}100,000{\textbackslash}times\$.},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Anderson, Maxwell G. and Ma, Shi-Yuan and Wang, Tianyu and Wright, Logan G. and McMahon, Peter L.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.10360 [physics]},
	keywords = {Computer Science - Emerging Technologies, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Physics - Applied Physics, Physics - Optics},
	file = {arXiv Fulltext PDF:C\:\\Users\\ilker\\Zotero\\storage\\X2N3BWDV\\Anderson et al. - 2023 - Optical Transformers.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ilker\\Zotero\\storage\\288I836E\\2302.html:text/html},
}

@article{isil_all-optical_2024,
	title = {All-optical image denoising using a diffractive visual processor},
	volume = {13},
	copyright = {2024 The Author(s)},
	issn = {2047-7538},
	url = {https://www.nature.com/articles/s41377-024-01385-6},
	doi = {10.1038/s41377-024-01385-6},
	abstract = {Image denoising, one of the essential inverse problems, targets to remove noise/artifacts from input images. In general, digital image denoising algorithms, executed on computers, present latency due to several iterations implemented in, e.g., graphics processing units (GPUs). While deep learning-enabled methods can operate non-iteratively, they also introduce latency and impose a significant computational burden, leading to increased power consumption. Here, we introduce an analog diffractive image denoiser to all-optically and non-iteratively clean various forms of noise and artifacts from input images – implemented at the speed of light propagation within a thin diffractive visual processor that axially spans {\textless}250 × λ, where λ is the wavelength of light. This all-optical image denoiser comprises passive transmissive layers optimized using deep learning to physically scatter the optical modes that represent various noise features, causing them to miss the output image Field-of-View (FoV) while retaining the object features of interest. Our results show that these diffractive denoisers can efficiently remove salt and pepper noise and image rendering-related spatial artifacts from input phase or intensity images while achieving an output power efficiency of {\textasciitilde}30–40\%. We experimentally demonstrated the effectiveness of this analog denoiser architecture using a 3D-printed diffractive visual processor operating at the terahertz spectrum. Owing to their speed, power-efficiency, and minimal computational overhead, all-optical diffractive denoisers can be transformative for various image display and projection systems, including, e.g., holographic displays.},
	language = {en},
	number = {1},
	urldate = {2024-05-22},
	journal = {Light: Science \& Applications},
	author = {Isil, Cagatay and Gan, Tianyi and Ardic, Fazil Onuralp and Mentesoglu, Koray and Digani, Jagrit and Karaca, Huseyin and Chen, Hanlong and Li, Jingxi and Mengu, Deniz and Jarrahi, Mona and Akşit, Kaan and Ozcan, Aydogan},
	month = feb,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Applied optics, Imaging and sensing, Optical physics, Transformation optics},
	pages = {43},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\C3WV9X6D\\Işıl et al. - 2024 - All-optical image denoising using a diffractive vi.pdf:application/pdf},
}

@article{herron_latent_2024,
	title = {Latent {Diffusion} {Models} for {Structural} {Component} {Design}},
	volume = {171},
	issn = {0010-4485},
	url = {https://www.sciencedirect.com/science/article/pii/S0010448524000344},
	doi = {10.1016/j.cad.2024.103707},
	abstract = {Recent advances in generative modeling, namely Diffusion models, have revolutionized generative modeling, enabling high-quality image generation tailored to user needs. This paper proposes a framework for the generative design of structural components. Specifically, we employ a Latent Diffusion model to generate potential designs of a component that can satisfy a set of problem-specific loading conditions. One of the distinct advantages our approach offers over other generative approaches is the editing of existing designs. We train our model using a dataset of geometries obtained from structural topology optimization utilizing the SIMP algorithm. Consequently, our framework generates inherently near-optimal designs. Our work presents quantitative results that support the structural performance of the generated designs and the variability in potential candidate designs. Furthermore, we provide evidence of the scalability of our framework by operating over voxel domains with resolutions varying from 323 to 1283. Our framework can be used as a starting point for generating novel near-optimal designs similar to topology-optimized designs.},
	urldate = {2024-05-22},
	journal = {Computer-Aided Design},
	author = {Herron, Ethan and Rade, Jaydeep and Jignasu, Anushrut and Ganapathysubramanian, Baskar and Balu, Aditya and Sarkar, Soumik and Krishnamurthy, Adarsh},
	month = jun,
	year = {2024},
	keywords = {Diffusion models, Generative design, Topology optimization},
	pages = {103707},
	file = {Submitted Version:C\:\\Users\\ilker\\Zotero\\storage\\W837B6C9\\Herron et al. - 2024 - Latent Diffusion Models for Structural Component D.pdf:application/pdf},
}

@article{lim_maxwellnet_2022,
	title = {{MaxwellNet}: {Physics}-driven deep neural network training based on {Maxwell}’s equations},
	volume = {7},
	issn = {2378-0967},
	shorttitle = {{MaxwellNet}},
	url = {https://doi.org/10.1063/5.0071616},
	doi = {10.1063/5.0071616},
	abstract = {Maxwell’s equations govern light propagation and its interaction with matter. Therefore, the solution of Maxwell’s equations using computational electromagnetic simulations plays a critical role in understanding light–matter interaction and designing optical elements. Such simulations are often time-consuming, and recent activities have been described to replace or supplement them with trained deep neural networks (DNNs). Such DNNs typically require extensive, computationally demanding simulations using conventional electromagnetic solvers to compose the training dataset. In this paper, we present a novel scheme to train a DNN that solves Maxwell’s equations speedily and accurately without relying on other computational electromagnetic solvers. Our approach is to train a DNN using the residual of Maxwell’s equations as the physics-driven loss function for a network that finds the electric field given the spatial distribution of the material property. We demonstrate it by training a single network that simultaneously finds multiple solutions of various aspheric micro-lenses. Furthermore, we exploit the speed of this network in a novel inverse design scheme to design a micro-lens that maximizes a desired merit function. We believe that our approach opens up a novel way for light simulation and optical design of photonic devices.},
	number = {1},
	urldate = {2024-05-22},
	journal = {APL Photonics},
	author = {Lim, Joowon and Psaltis, Demetri},
	month = jan,
	year = {2022},
	pages = {011301},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\2FAUQC9X\\Lim and Psaltis - 2022 - MaxwellNet Physics-driven deep neural network tra.pdf:application/pdf;Snapshot:C\:\\Users\\ilker\\Zotero\\storage\\9SCEINSC\\MaxwellNet-Physics-driven-deep-neural-network.html:text/html},
}

@article{gu_neurolight_2022,
	title = {{NeurOLight}: {A} {Physics}-{Agnostic} {Neural} {Operator} {Enabling} {Parametric} {Photonic} {Device} {Simulation}},
	volume = {35},
	shorttitle = {{NeurOLight}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/5ddfb189c022a317ff1c72e6639079de-Abstract-Conference.html},
	language = {en},
	urldate = {2024-05-22},
	journal = {Advances in Neural Information Processing Systems},
	author = {Gu, Jiaqi and Gao, Zhengqi and Feng, Chenghao and Zhu, Hanqing and Chen, Ray and Boning, Duane and Pan, David},
	month = dec,
	year = {2022},
	pages = {14623--14636},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\UB7NEPL3\\Gu et al. - 2022 - NeurOLight A Physics-Agnostic Neural Operator Ena.pdf:application/pdf},
}

@misc{gu_neurolight_2022-1,
	title = {{NeurOLight}: {A} {Physics}-{Agnostic} {Neural} {Operator} {Enabling} {Parametric} {Photonic} {Device} {Simulation}},
	shorttitle = {{NeurOLight}},
	url = {http://arxiv.org/abs/2209.10098},
	doi = {10.48550/arXiv.2209.10098},
	abstract = {Optical computing is an emerging technology for next-generation efficient artificial intelligence (AI) due to its ultra-high speed and efficiency. Electromagnetic field simulation is critical to the design, optimization, and validation of photonic devices and circuits. However, costly numerical simulation significantly hinders the scalability and turn-around time in the photonic circuit design loop. Recently, physics-informed neural networks have been proposed to predict the optical field solution of a single instance of a partial differential equation (PDE) with predefined parameters. Their complicated PDE formulation and lack of efficient parametrization mechanisms limit their flexibility and generalization in practical simulation scenarios. In this work, for the first time, a physics-agnostic neural operator-based framework, dubbed NeurOLight, is proposed to learn a family of frequency-domain Maxwell PDEs for ultra-fast parametric photonic device simulation. We balance the efficiency and generalization of NeurOLight via several novel techniques. Specifically, we discretize different devices into a unified domain, represent parametric PDEs with a compact wave prior, and encode the incident light via masked source modeling. We design our model with parameter-efficient cross-shaped NeurOLight blocks and adopt superposition-based augmentation for data-efficient learning. With these synergistic approaches, NeurOLight generalizes to a large space of unseen simulation settings, demonstrates 2-orders-of-magnitude faster simulation speed than numerical solvers, and outperforms prior neural network models by {\textasciitilde}54\% lower prediction error with {\textasciitilde}44\% fewer parameters. Our code is available at https://github.com/JeremieMelo/NeurOLight.},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Gu, Jiaqi and Gao, Zhengqi and Feng, Chenghao and Zhu, Hanqing and Chen, Ray T. and Boning, Duane S. and Pan, David Z.},
	month = sep,
	year = {2022},
	note = {arXiv:2209.10098 [physics]},
	keywords = {Computer Science - Emerging Technologies, Computer Science - Machine Learning, Physics - Optics},
	file = {arXiv Fulltext PDF:C\:\\Users\\ilker\\Zotero\\storage\\Q45BEKVG\\Gu et al. - 2022 - NeurOLight A Physics-Agnostic Neural Operator Ena.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\ilker\\Zotero\\storage\\687PP3JJ\\2209.html:text/html},
}

@article{kamilov_learning_2015,
	title = {Learning approach to optical tomography},
	volume = {2},
	copyright = {© 2015 Optical Society of America},
	issn = {2334-2536},
	url = {https://opg.optica.org/optica/abstract.cfm?uri=optica-2-6-517},
	doi = {10.1364/OPTICA.2.000517},
	abstract = {Optical tomography has been widely investigated for biomedical imaging applications. In recent years optical tomography has been combined with digital holography and has been employed to produce high-quality images of phase objects such as cells. In this paper we describe a method for imaging 3D phase objects in a tomographic configuration implemented by training an artificial neural network to reproduce the complex amplitude of the experimentally measured scattered light. The network is designed such that the voxel values of the refractive index of the 3D object are the variables that are adapted during the training process. We demonstrate the method experimentally by forming images of the 3D refractive index distribution of Hela cells.},
	language = {EN},
	number = {6},
	urldate = {2024-05-22},
	journal = {Optica},
	author = {Kamilov, Ulugbek S. and Papadopoulos, Ioannis N. and Shoreh, Morteza H. and Goy, Alexandre and Vonesch, Cedric and Unser, Michael and Psaltis, Demetri},
	month = jun,
	year = {2015},
	note = {Publisher: Optica Publishing Group},
	keywords = {Beam propagation methods, Biomedical imaging, Imaging techniques, Phase imaging, Refractive index, Three dimensional imaging},
	pages = {517--522},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\IJSDJL27\\Kamilov et al. - 2015 - Learning approach to optical tomography.pdf:application/pdf},
}

@article{paszke_automatic_2017,
	title = {Automatic differentiation in {PyTorch}},
	url = {https://openreview.net/forum?id=BJJsrmfCZ},
	abstract = {In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd, and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
	language = {en},
	urldate = {2024-05-22},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	month = oct,
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\Q8UJZK85\\Paszke et al. - 2017 - Automatic differentiation in PyTorch.pdf:application/pdf},
}

@article{mcmahon_physics_2023,
	title = {The physics of optical computing},
	volume = {5},
	copyright = {2023 Springer Nature Limited},
	issn = {2522-5820},
	url = {https://www.nature.com/articles/s42254-023-00645-5},
	doi = {10.1038/s42254-023-00645-5},
	abstract = {There has been a resurgence of interest in optical computing since the early 2010s, both in academia and in industry, with much of the excitement centred around special-purpose optical computers for neural-network processing. Optical computing has been a topic of periodic study since the 1960s, including for neural networks in the 1980s and early 1990s, and a wide variety of optical-computing schemes and architectures have been proposed. In this Perspective article, we provide a systematic explanation of why and how optics might be able to give speed or energy-efficiency benefits over electronics for computing, enumerating 11 features of optics that can be harnessed when designing an optical computer. One often-mentioned motivation for optical computing — that the speed of light is fast — is emphatically not a key differentiating physical property of optics for computing; understanding where an advantage could come from is more subtle. We discuss how gaining an advantage over state-of-the-art electronic processors will likely only be achievable by careful design that harnesses more than 1 of the 11 features, while avoiding a number of pitfalls that we describe.},
	language = {en},
	number = {12},
	urldate = {2024-05-22},
	journal = {Nature Reviews Physics},
	author = {McMahon, Peter L.},
	month = dec,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Optoelectronic devices and components, Transformation optics},
	pages = {717--734},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\TE8U47EB\\McMahon - 2023 - The physics of optical computing.pdf:application/pdf},
}

@misc{noauthor_physics_nodate,
	title = {The physics of optical computing {\textbar} {Nature} {Reviews} {Physics}},
	url = {https://www.nature.com/articles/s42254-023-00645-5},
	urldate = {2024-05-22},
	file = {The physics of optical computing | Nature Reviews Physics:C\:\\Users\\ilker\\Zotero\\storage\\W6REMGH5\\s42254-023-00645-5.html:text/html},
}

@article{mcmahon_physics_2023-1,
	title = {The physics of optical computing},
	volume = {5},
	copyright = {2023 Springer Nature Limited},
	issn = {2522-5820},
	url = {https://www.nature.com/articles/s42254-023-00645-5},
	doi = {10.1038/s42254-023-00645-5},
	abstract = {There has been a resurgence of interest in optical computing since the early 2010s, both in academia and in industry, with much of the excitement centred around special-purpose optical computers for neural-network processing. Optical computing has been a topic of periodic study since the 1960s, including for neural networks in the 1980s and early 1990s, and a wide variety of optical-computing schemes and architectures have been proposed. In this Perspective article, we provide a systematic explanation of why and how optics might be able to give speed or energy-efficiency benefits over electronics for computing, enumerating 11 features of optics that can be harnessed when designing an optical computer. One often-mentioned motivation for optical computing — that the speed of light is fast — is emphatically not a key differentiating physical property of optics for computing; understanding where an advantage could come from is more subtle. We discuss how gaining an advantage over state-of-the-art electronic processors will likely only be achievable by careful design that harnesses more than 1 of the 11 features, while avoiding a number of pitfalls that we describe.},
	language = {en},
	number = {12},
	urldate = {2024-05-22},
	journal = {Nature Reviews Physics},
	author = {McMahon, Peter L.},
	month = dec,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Optoelectronic devices and components, Transformation optics},
	pages = {717--734},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\ARTLTQL2\\McMahon - 2023 - The physics of optical computing.pdf:application/pdf},
}

@article{tait_silicon_2019,
	title = {Silicon {Photonic} {Modulator} {Neuron}},
	volume = {11},
	url = {https://link.aps.org/doi/10.1103/PhysRevApplied.11.064043},
	doi = {10.1103/PhysRevApplied.11.064043},
	abstract = {There has been recent interest in neuromorphic photonics, a field with the promise to access pivotal and unexplored regimes of machine intelligence. Progress has been made on isolated neurons and analog interconnects; nevertheless, this renewal of interest has yet to produce a demonstration of a silicon photonic neuron capable of interacting with other like neurons. We report a modulator-class photonic neuron fabricated in a conventional silicon photonic process line. We demonstrate the behaviors of transfer-function configurability, fan-in, inhibition, time-resolved pulse processing, and, crucially, autaptic cascadability—a sufficient set of behaviors for a device to act as a neuron participating in a network of like neurons. The silicon photonic modulator neuron constitutes the final piece needed to make photonic neural networks fully integrated on currently available silicon photonic platforms.},
	number = {6},
	urldate = {2024-05-22},
	journal = {Physical Review Applied},
	author = {Tait, Alexander N. and Ferreira de Lima, Thomas and Nahmias, Mitchell A. and Miller, Heidi B. and Peng, Hsuan-Tung and Shastri, Bhavin J. and Prucnal, Paul R.},
	month = jun,
	year = {2019},
	note = {Publisher: American Physical Society},
	pages = {064043},
	file = {APS Snapshot:C\:\\Users\\ilker\\Zotero\\storage\\SG76E64K\\PhysRevApplied.11.html:text/html;Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\J3PWX7MN\\Tait et al. - 2019 - Silicon Photonic Modulator Neuron.pdf:application/pdf},
}

@article{menssen_scalable_2023,
	title = {Scalable photonic integrated circuits for high-fidelity light control},
	volume = {10},
	copyright = {© 2023 Optica Publishing Group},
	issn = {2334-2536},
	url = {https://opg.optica.org/optica/abstract.cfm?uri=optica-10-10-1366},
	doi = {10.1364/OPTICA.489504},
	abstract = {Advances in laser technology have driven discoveries in atomic, molecular, and optical (AMO) physics and emerging applications, from quantum computers with cold atoms or ions, to quantum networks with solid-state color centers. This progress is motivating the development of a new generation of optical control systems that can manipulate the light field with high fidelity at wavelengths relevant for AMO applications. These systems are characterized by criteria: (C1) operation at a design wavelength of choice in the visible (VIS) or near-infrared (IR) spectrum, (C2) a scalable platform that can support large channel counts, (C3) high-intensity modulation extinction and (C4) repeatability compatible with low gate errors, and (C5) fast switching times. Here, we provide a pathway to address these challenges by introducing an atom control architecture based on VIS-IR photonic integrated circuit (PIC) technology. Based on a complementary metal–oxide–semiconductor fabrication process, this atom-control PIC (APIC) technology can meet system requirements (C1)–(C5). As a proof of concept, we demonstrate a 16-channel silicon-nitride-based APIC with (5.8±0.4)n s response times and \&gt;30d B extinction ratio at a wavelength of 780\&\#x00A0;nm.},
	language = {EN},
	number = {10},
	urldate = {2024-05-22},
	journal = {Optica},
	author = {Menssen, Adrian J. and Hermans, Artur and Christen, Ian and Propson, Thomas and Li, Chao and Leenheer, Andrew J. and Zimmermann, Matthew and Dong, Mark and Larocque, Hugo and Raniwala, Hamza and Gilbert, Gerald and Eichenfield, Matt and Englund, Dirk R.},
	month = oct,
	year = {2023},
	note = {Publisher: Optica Publishing Group},
	keywords = {Aluminum oxide, Defect center materials, Extinction ratios, Grating coupler, Integrated photonics, Optical systems},
	pages = {1366--1372},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\ATE5TMAP\\Menssen et al. - 2023 - Scalable photonic integrated circuits for high-fid.pdf:application/pdf},
}

@misc{noauthor_full_nodate,
	title = {A full degree-of-freedom spatiotemporal light modulator {\textbar} {Nature} {Photonics}},
	url = {https://www.nature.com/articles/s41566-022-01086-9},
	urldate = {2024-05-22},
}

@article{panuski_full_2022,
	title = {A full degree-of-freedom spatiotemporal light modulator},
	volume = {16},
	copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1749-4893},
	url = {https://www.nature.com/articles/s41566-022-01086-9},
	doi = {10.1038/s41566-022-01086-9},
	abstract = {Harnessing the full complexity of optical fields requires the complete control of all degrees of freedom within a region of space and time—an open goal for present-day spatial light modulators, active metasurfaces and optical phased arrays. Here, we resolve this challenge with a programmable photonic crystal cavity array enabled by four key advances: (1) near-unity vertical coupling to high-finesse microcavities through inverse design; (2) scalable fabrication by optimized 300 mm full-wafer processing; (3) picometre-precision resonance alignment using automated, closed-loop ‘holographic trimming’; and (4) out-of-plane cavity control via a high-speed μLED array. Combining each, we demonstrate the near-complete spatiotemporal control of a 64 resonator, two-dimensional spatial light modulator with nanosecond- and femtojoule-order switching. Simultaneously operating wavelength-scale modes near the space–bandwidth and time–bandwidth limits, this work opens a new regime of programmability at the fundamental limits of multimode optical control.},
	language = {en},
	number = {12},
	urldate = {2024-05-22},
	journal = {Nature Photonics},
	author = {Panuski, Christopher L. and Christen, Ian and Minkov, Momchil and Brabec, Cole J. and Trajtenberg-Mills, Sivan and Griffiths, Alexander D. and McKendry, Jonathan J. D. and Leake, Gerald L. and Coleman, Daniel J. and Tran, Cung and St Louis, Jeffrey and Mucci, John and Horvath, Cameron and Westwood-Bachman, Jocelyn N. and Preble, Stefan F. and Dawson, Martin D. and Strain, Michael J. and Fanto, Michael L. and Englund, Dirk R.},
	month = dec,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Integrated optics, Nanophotonics and plasmonics},
	pages = {834--842},
	file = {Full Text PDF:C\:\\Users\\ilker\\Zotero\\storage\\4SJILK9C\\Panuski et al. - 2022 - A full degree-of-freedom spatiotemporal light modu.pdf:application/pdf},
}

@article{peng2020neural,
  title={Neural holography with camera-in-the-loop training},
  author={Peng, Yifan and Choi, Suyeon and Padmanaban, Nitish and Wetzstein, Gordon},
  journal={ACM Transactions on Graphics (TOG)},
  volume={39},
  number={6},
  pages={1--14},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{wei2023spatially,
  title={Spatially varying nanophotonic neural networks},
  author={Wei, Kaixuan and Li, Xiao and Froech, Johannes and Chakravarthula, Praneeth and Whitehead, James and Tseng, Ethan and Majumdar, Arka and Heide, Felix},
  journal={arXiv preprint arXiv:2308.03407},
  year={2023}
}

@article{feit1988beam,
  title={Beam nonparaxiality, filament formation, and beam breakup in the self-focusing of optical beams},
  author={Feit, MDand and Fleck, JA},
  journal={JOSA B},
  volume={5},
  number={3},
  pages={633--640},
  year={1988},
  publisher={Optica Publishing Group}
}

@article{henighan2020scaling,
  title={Scaling laws for autoregressive generative modeling},
  author={Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B and Dhariwal, Prafulla and Gray, Scott and others},
  journal={arXiv preprint arXiv:2010.14701},
  year={2020}
}

@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={Medical image computing and computer-assisted intervention--MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@inproceedings{choi2020stargan,
  title={Stargan v2: Diverse image synthesis for multiple domains},
  author={Choi, Yunjey and Uh, Youngjung and Yoo, Jaejun and Ha, Jung-Woo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8188--8197},
  year={2020}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@online{xiao2017/online,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  date         = {2017-08-28},
  year         = {2017},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  eprint       = {cs.LG/1708.07747},
}

@misc{noauthor_googlecreativelabquickdraw-dataset_2024,
	title = {googlecreativelab/quickdraw-dataset},
	url = {https://github.com/googlecreativelab/quickdraw-dataset},
	abstract = {Documentation on how to access and use the Quick, Draw! Dataset.},
	urldate = {2024-10-29},
	publisher = {Google Creative Lab},
	month = oct,
	year = {2024},
	note = {original-date: 2017-05-09T18:28:32Z},
	keywords = {dataset, quickdraw-dataset},
}
@inproceedings{10.1145/3630106.3658542,
author = {Luccioni, Sasha and Jernite, Yacine and Strubell, Emma},
title = {Power Hungry Processing: Watts Driving the Cost of AI Deployment?},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3658542},
doi = {10.1145/3630106.3658542},
abstract = {Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of “generality” comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and ‘general-purpose’ models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {85–99},
numpages = {15},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}