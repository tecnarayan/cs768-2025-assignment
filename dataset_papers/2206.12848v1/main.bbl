\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bertsekas(2005)]{bertsekas2005dynamic}
Bertsekas, D.
\newblock \emph{Dynamic programming and optimal control}.
\newblock Athena scientific Belmont, MA, 2005.

\bibitem[Bertsekas \& Tsitsiklis(1996)Bertsekas and
  Tsitsiklis]{bertsekas1996neuro}
Bertsekas, D.~P. and Tsitsiklis, J.~N.
\newblock \emph{Neuro-dynamic programming}.
\newblock Athena Scientific, 1996.

\bibitem[Bhatnagar \& Kumar(2004)Bhatnagar and
  Kumar]{bhatnagar2004simultaneous}
Bhatnagar, S. and Kumar, S.
\newblock A simultaneous perturbation stochastic approximation-based
  actor-critic algorithm for markov decision processes.
\newblock \emph{IEEE Transactions on Automatic Control}, 49\penalty0
  (4):\penalty0 592--598, 2004.

\bibitem[Bhatnagar et~al.(2008)Bhatnagar, Ghavamzadeh, Lee, and
  Sutton]{bhatnagar2008incremental}
Bhatnagar, S., Ghavamzadeh, M., Lee, M., and Sutton, R.~S.
\newblock Incremental natural actor-critic algorithms.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  105--112, 2008.

\bibitem[Bhatnagar et~al.(2009)Bhatnagar, Sutton, Ghavamzadeh, and
  Lee]{bhatnagar2009natural}
Bhatnagar, S., Sutton, R.~S., Ghavamzadeh, M., and Lee, M.
\newblock Natural actor--critic algorithms.
\newblock \emph{Automatica}, 45\penalty0 (11):\penalty0 2471--2482, 2009.

\bibitem[Borkar(2009)]{borkar2009stochastic}
Borkar, V.~S.
\newblock \emph{Stochastic approximation: a dynamical systems viewpoint},
  volume~48.
\newblock Springer, 2009.

\bibitem[Borkar \& Meyn(2000)Borkar and Meyn]{borkar2000ode}
Borkar, V.~S. and Meyn, S.~P.
\newblock The ode method for convergence of stochastic approximation and
  reinforcement learning.
\newblock \emph{SIAM Journal on Control and Optimization}, 38\penalty0
  (2):\penalty0 447--469, 2000.

\bibitem[Dalal et~al.(2018)Dalal, Sz{\"o}r{\'e}nyi, Thoppe, and
  Mannor]{dalal2018finite}
Dalal, G., Sz{\"o}r{\'e}nyi, B., Thoppe, G., and Mannor, S.
\newblock Finite sample analyses for td (0) with function approximation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2018.

\bibitem[Di~Castro \& Meir(2010)Di~Castro and Meir]{dicastro2010convergent}
Di~Castro, D. and Meir, R.
\newblock A convergent online single time scale actor critic algorithm.
\newblock \emph{The Journal of Machine Learning Research}, 11:\penalty0
  367--410, 2010.

\bibitem[Di-Castro~Shashua et~al.(2021)Di-Castro~Shashua, Di~Castro, and
  Mannor]{di2021sim}
Di-Castro~Shashua, S., Di~Castro, D., and Mannor, S.
\newblock Sim and real: Better together.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Fan et~al.(2020)Fan, Wang, Xie, and Yang]{fan2020theoretical}
Fan, J., Wang, Z., Xie, Y., and Yang, Z.
\newblock A theoretical analysis of deep q-learning.
\newblock In \emph{Learning for Dynamics and Control}, pp.\  486--489. PMLR,
  2020.

\bibitem[Fedus et~al.(2020)Fedus, Ramachandran, Agarwal, Bengio, Larochelle,
  Rowland, and Dabney]{fedus2020revisiting}
Fedus, W., Ramachandran, P., Agarwal, R., Bengio, Y., Larochelle, H., Rowland,
  M., and Dabney, W.
\newblock Revisiting fundamentals of experience replay.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3061--3071. PMLR, 2020.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and
  Meger]{fujimoto2018addressing}
Fujimoto, S., Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1587--1596. PMLR, 2018.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pp.\
  1861--1870. PMLR, 2018.

\bibitem[Horgan et~al.(2018)Horgan, Quan, Budden, Barth-Maron, Hessel,
  Van~Hasselt, and Silver]{horgan2018distributed}
Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., Van~Hasselt, H.,
  and Silver, D.
\newblock Distributed prioritized experience replay.
\newblock \emph{arXiv preprint arXiv:1803.00933}, 2018.

\bibitem[Konda \& Tsitsiklis(2000)Konda and Tsitsiklis]{konda2000actor}
Konda, V.~R. and Tsitsiklis, J.~N.
\newblock Actor-critic algorithms.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1008--1014. Citeseer, 2000.

\bibitem[Kushner \& Yin(2003)Kushner and Yin]{kushner2003stochastic}
Kushner, H. and Yin, G.~G.
\newblock \emph{Stochastic approximation and recursive algorithms and
  applications}, volume~35.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Kushner \& Clark(2012)Kushner and Clark]{kushner2012stochastic}
Kushner, H.~J. and Clark, D.~S.
\newblock \emph{Stochastic approximation methods for constrained and
  unconstrained systems}, volume~26.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Laguna \& Marklund(2013)Laguna and Marklund]{laguna2013business}
Laguna, M. and Marklund, J.
\newblock \emph{Business process modeling, simulation, and design}.
\newblock Taylor \& Francis, 2013.

\bibitem[Lahire et~al.(2021)Lahire, Geist, and Rachelson]{lahire2021large}
Lahire, T., Geist, M., and Rachelson, E.
\newblock Large batch experience replay.
\newblock \emph{arXiv preprint arXiv:2110.01528}, 2021.

\bibitem[Lazic et~al.(2021)Lazic, Yin, Abbasi-Yadkori, and
  Szepesvari]{lazic2021improved}
Lazic, N., Yin, D., Abbasi-Yadkori, Y., and Szepesvari, C.
\newblock Improved regret bound and experience replay in regularized policy
  iteration.
\newblock \emph{arXiv preprint arXiv:2102.12611}, 2021.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Lin(1993)]{lin1993reinforcement}
Lin, L.-J.
\newblock Reinforcement learning for robots using neural networks.
\newblock Technical report, Carnegie-Mellon Univ Pittsburgh PA School of
  Computer Science, 1993.

\bibitem[Liu \& Zou(2018)Liu and Zou]{liu2018effects}
Liu, R. and Zou, J.
\newblock The effects of memory replay in reinforcement learning.
\newblock In \emph{2018 56th Annual Allerton Conference on Communication,
  Control, and Computing (Allerton)}, pp.\  478--485. IEEE, 2018.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., and Riedmiller, M.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Norris(1998)]{norris1998markov}
Norris, J.~R.
\newblock \emph{Markov chains}.
\newblock Number~2 in 1. Cambridge university press, 1998.

\bibitem[Oppenheim et~al.(1997)Oppenheim, Willsky, Nawab, Hern{\'a}ndez,
  et~al.]{oppenheim1997signals}
Oppenheim, A.~V., Willsky, A.~S., Nawab, S.~H., Hern{\'a}ndez, G.~M., et~al.
\newblock \emph{Signals \& systems}.
\newblock Pearson Educaci{\'o}n, 1997.

\bibitem[Pan et~al.(2018)Pan, Zaheer, White, Patterson, and
  White]{pan2018organizing}
Pan, Y., Zaheer, M., White, A., Patterson, A., and White, M.
\newblock Organizing experience: a deeper look at replay mechanisms for
  sample-based planning in continuous state domains.
\newblock \emph{arXiv preprint arXiv:1806.04624}, 2018.

\bibitem[Porat(2008)]{porat2008digital}
Porat, B.
\newblock \emph{Digital processing of random signals: theory and methods}.
\newblock Courier Dover Publications, 2008.

\bibitem[Puterman(1994)]{puterman1994markov}
Puterman, M.~L.
\newblock \emph{Markov Decision Processes}.
\newblock Wiley and Sons, 1994.

\bibitem[Riedmiller(2005)]{riedmiller2005neural}
Riedmiller, M.
\newblock Neural fitted q iteration--first experiences with a data efficient
  neural reinforcement learning method.
\newblock In \emph{European conference on machine learning}, pp.\  317--328.
  Springer, 2005.

\bibitem[Schaul et~al.(2015)Schaul, Quan, Antonoglou, and
  Silver]{schaul2015prioritized}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock \emph{arXiv preprint arXiv:1511.05952}, 2015.

\bibitem[Wang et~al.(2016)Wang, Bapst, Heess, Mnih, Munos, Kavukcuoglu, and
  de~Freitas]{wang2016sample}
Wang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R., Kavukcuoglu, K., and
  de~Freitas, N.
\newblock Sample efficient actor-critic with experience replay.
\newblock \emph{arXiv preprint arXiv:1611.01224}, 2016.

\bibitem[Wu et~al.(2020)Wu, Zhang, Xu, and Gu]{wu2020finite}
Wu, Y., Zhang, W., Xu, P., and Gu, Q.
\newblock A finite time analysis of two time-scale actor critic methods.
\newblock \emph{arXiv preprint arXiv:2005.01350}, 2020.

\bibitem[Zha et~al.(2019)Zha, Lai, Zhou, and Hu]{zha2019experience}
Zha, D., Lai, K.-H., Zhou, K., and Hu, X.
\newblock Experience replay optimization.
\newblock \emph{arXiv preprint arXiv:1906.08387}, 2019.

\bibitem[Zhang \& Sutton(2017)Zhang and Sutton]{zhang2017deeper}
Zhang, S. and Sutton, R.~S.
\newblock A deeper look at experience replay.
\newblock \emph{arXiv preprint arXiv:1712.01275}, 2017.

\bibitem[Zou et~al.(2019)Zou, Xu, and Liang]{zou2019finite}
Zou, S., Xu, T., and Liang, Y.
\newblock Finite-sample analysis for sarsa with linear function approximation.
\newblock \emph{arXiv preprint arXiv:1902.02234}, 2019.

\end{thebibliography}
