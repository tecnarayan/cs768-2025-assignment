@article{baraniuk2008simple,
  title={A simple proof of the restricted isometry property for random matrices},
  author={Baraniuk, Richard and Davenport, Mark and DeVore, Ronald and Wakin, Michael},
  journal={Constructive Approximation},
  volume={28},
  number={3},
  pages={253--263},
  year={2008},
  publisher={Springer}
}

@article{vaskevicius2019implicit,
  title={Implicit regularization for optimal sparse recovery},
  author={Vaskevicius, Tomas and Kanade, Varun and Rebeschini, Patrick},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{muthukumar2020harmless,
  title={Harmless interpolation of noisy data in regression},
  author={Muthukumar, Vidya and Vodrahalli, Kailas and Subramanian, Vignesh and Sahai, Anant},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={1},
  number={1},
  pages={67--83},
  year={2020},
  publisher={IEEE}
}
@book{vershynin2018high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge university press}
}
@article{bellec2018slope,
  title={Slope meets lasso: improved oracle bounds and optimality},
  author={Bellec, Pierre C and Lecu{\'e}, Guillaume and Tsybakov, Alexandre B},
  journal={The Annals of Statistics},
  volume={46},
  number={6B},
  pages={3603--3642},
  year={2018},
  publisher={JSTOR}
}
@article{cai2011orthogonal,
  title={Orthogonal matching pursuit for sparse signal recovery with noise},
  author={Cai, T Tony and Wang, Lie},
  journal={IEEE Transactions on Information theory},
  volume={57},
  number={7},
  pages={4680--4688},
  year={2011},
  publisher={IEEE}
}
@article{bickel2009simultaneous,
  title={Simultaneous analysis of Lasso and Dantzig selector},
  author={Bickel, Peter J and Ritov, Ya’acov and Tsybakov, Alexandre B},
  journal={The Annals of statistics},
  volume={37},
  number={4},
  pages={1705--1732},
  year={2009},
  publisher={Institute of Mathematical Statistics}
}
@article{belloni2011square,
  title={Square-root lasso: pivotal recovery of sparse signals via conic programming},
  author={Belloni, Alexandre and Chernozhukov, Victor and Wang, Lie},
  journal={Biometrika},
  volume={98},
  number={4},
  pages={791--806},
  year={2011},
  publisher={Oxford University Press}
}
@inproceedings{woodworth2020kernel,
  title={Kernel and rich regimes in overparametrized models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={3635--3673},
  year={2020},
  organization={PMLR}
}
@article{hastie2022surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={The Annals of Statistics},
  volume={50},
  number={2},
  pages={949--986},
  year={2022},
  publisher={Institute of Mathematical Statistics}
}
@article{bartlett2020benign,
  title={Benign overfitting in linear regression},
  author={Bartlett, Peter L and Long, Philip M and Lugosi, G{\'a}bor and Tsigler, Alexander},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30063--30070},
  year={2020},
  publisher={National Acad Sciences}
}
@article{li2021implicit,
  title={Implicit sparse regularization: The impact of depth and early stopping},
  author={Li, Jiangyuan and Nguyen, Thanh and Hegde, Chinmay and Wong, Ka Wai},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={28298--28309},
  year={2021}
}
@inproceedings{
gissin2020the,
title={The Implicit Bias of Depth: How Incremental Learning Drives Generalization},
author={Daniel Gissin and Shai Shalev-Shwartz and Amit Daniely},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1lj0nNFwB}
}
@inproceedings{wang2022tight,
  title={Tight bounds for minimum {$\ell_1$}-norm interpolation of noisy data},
  author={Wang, Guillaume and Donhauser, Konstantin and Yang, Fanny},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={10572--10602},
  year={2022},
  organization={PMLR}
}
@article{chatterji2022foolish,
  title={Foolish Crowds Support Benign Overfitting},
  author={Chatterji, Niladri S and Long, Philip M},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={125},
  pages={1--12},
  year={2022}
}
@article{koehler2021uniform,
  title={Uniform convergence of interpolators: Gaussian width, norm bounds and benign overfitting},
  author={Koehler, Frederic and Zhou, Lijia and Sutherland, Danica J and Srebro, Nathan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={20657--20668},
  year={2021}
}
@article{chinot_robustness_2022,
	title = {On the robustness of minimum norm interpolators and regularized empirical risk minimizers},
	volume = {50},
	issn = {0090-5364},
	doi = {10.1214/22-AOS2190},
	number = {4},
	urldate = {2023-01-10},
	journal = {The Annals of Statistics},
	author = {Chinot, Geoffrey and Löffler, Matthias and van de Geer, Sara},
	month = aug,
	year = {2022},
}
@article{ju2020overfitting,
  title={Overfitting can be harmless for basis pursuit, but only to a degree},
  author={Ju, Peizhong and Lin, Xiaojun and Liu, Jia},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7956--7967},
  year={2020}
}
@article{zhou2020uniform,
  title={On uniform convergence and low-norm interpolation learning},
  author={Zhou, Lijia and Sutherland, Danica J and Srebro, Nati},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6867--6877},
  year={2020}
}
@article{tsigler2020benign,
  title={Benign overfitting in ridge regression},
  author={Tsigler, Alexander and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2009.14286},
  year={2020}
}
@article{li2021minimum,
  title={Minimum {$\ell_1$}-norm interpolators: Precise asymptotics and multiple descent},
  author={Li, Yue and Wei, Yuting},
  journal={arXiv preprint arXiv:2110.09502},
  year={2021}
}
@article{muthukumar2021classification,
  title={Classification vs regression in overparameterized regimes: Does the loss function matter?},
  author={Muthukumar, Vidya and Narang, Adhyyan and Subramanian, Vignesh and Belkin, Mikhail and Hsu, Daniel and Sahai, Anant},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={10104--10172},
  year={2021},
  publisher={JMLRORG}
}
@article{belkin2020two,
  title={Two models of double descent for weak features},
  author={Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={2},
  number={4},
  pages={1167--1180},
  year={2020},
  publisher={SIAM}
}
@inproceedings{razin2021implicit,
  title={Implicit regularization in tensor factorization},
  author={Razin, Noam and Maman, Asaf and Cohen, Nadav},
  booktitle={International Conference on Machine Learning},
  pages={8913--8924},
  year={2021},
  organization={PMLR}
}
@article{ge2021understanding,
  title={Understanding Deflation Process in Over-parametrized Tensor Decomposition},
  author={Ge, Rong and Ren, Yunwei and Wang, Xiang and Zhou, Mo},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={1299--1311},
  year={2021}
}
@article{razin2020implicit,
  title={Implicit regularization in deep learning may not be explainable by norms},
  author={Razin, Noam and Cohen, Nadav},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21174--21187},
  year={2020}
}
@article{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{arora2022understanding,
  title={Understanding Gradient Descent on Edge of Stability in Deep Learning},
  author={Arora, Sanjeev and Li, Zhiyuan and Panigrahi, Abhishek},
  journal={arXiv preprint arXiv:2205.09745},
  year={2022}
}
@article{zhu2022understanding,
  title={Understanding Edge-of-Stability Training Dynamics with a Minimalist Example},
  author={Zhu, Xingyu and Wang, Zixuan and Wang, Xiang and Zhou, Mo and Ge, Rong},
  journal={arXiv preprint arXiv:2210.03294},
  year={2022}
}
@InProceedings{ahn2022understanding,
  title = 	 {Understanding the unstable convergence of gradient descent},
  author =       {Ahn, Kwangjun and Zhang, Jingzhao and Sra, Suvrit},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {247--257},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/ahn22a/ahn22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/ahn22a.html},
}
@article{chen2022gradient,
  title={On Gradient Descent Convergence beyond the Edge of Stability},
  author={Chen, Lei and Bruna, Joan},
  journal={arXiv preprint arXiv:2206.04172},
  year={2022}
}
@inproceedings{
    cohen2021gradient,
    title={Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability},
    author={Jeremy Cohen and Simran Kaur and Yuanzhi Li and J Zico Kolter and Ameet Talwalkar},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=jh-rTtvkGeM}
}
@inproceedings{
    wang2022analyzing,
    title={Analyzing Sharpness along {GD} Trajectory: Progressive Sharpening and Edge of Stability},
    author={Zixuan Wang and Zhouzi Li and Jian Li},
    booktitle={Advances in Neural Information Processing Systems},
    editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    year={2022},
    url={https://openreview.net/forum?id=thgItcQrJ4y}
}
@inproceedings{
    wang2022large,
    title={Large Learning Rate Tames Homogeneity: Convergence and Balancing Effect},
    author={Yuqing Wang and Minshuo Chen and Tuo Zhao and Molei Tao},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=3tbDrs77LJ5}
}
@article{ma2022multiscale,
  title={The Multiscale Structure of Neural Network Loss Functions: The Effect on Optimization and Origin},
  author={Ma, Chao and Wu, Lei and Ying, Lexing},
  journal={arXiv preprint arXiv:2204.11326},
  year={2022}
}
@inproceedings{
lyu2022understanding,
title={Understanding the Generalization Benefit of Normalization Layers: Sharpness Reduction},
author={Kaifeng Lyu and Zhiyuan Li and Sanjeev Arora},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=xp5VOBxTxZ}
}
@inproceedings{azulay2021implicit,
  title={On the implicit bias of initialization shape: Beyond infinitesimal mirror descent},
  author={Azulay, Shahar and Moroshko, Edward and Nacson, Mor Shpigel and Woodworth, Blake E and Srebro, Nathan and Globerson, Amir and Soudry, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={468--477},
  year={2021},
  organization={PMLR}
}
@inproceedings{chizat2020implicit,
  title={Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Conference on Learning Theory},
  pages={1305--1338},
  year={2020},
  organization={PMLR}
}
@article{vardi2022implicit,
  title={On the implicit bias in deep-learning algorithms},
  author={Vardi, Gal},
  journal={arXiv preprint arXiv:2208.12591},
  year={2022}
}
@article{telgarsky2022feature,
  title={Feature selection with gradient descent on two-layer networks in low-rotation regimes},
  author={Telgarsky, Matus},
  journal={arXiv preprint arXiv:2208.02789},
  year={2022}
}
@inproceedings{lyu2019gradient,
  title={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},
  author={Lyu, Kaifeng and Li, Jian},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@article{ji2020directional,
  title={Directional convergence and alignment in deep learning},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17176--17186},
  year={2020}
}
@article{lyu2021gradient,
  title={Gradient descent on two-layer nets: Margin maximization and simplicity bias},
  author={Lyu, Kaifeng and Li, Zhiyuan and Wang, Runzhe and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12978--12991},
  year={2021}
}
@article{bartlett2021deep,
  title={Deep learning: a statistical viewpoint},
  author={Bartlett, Peter L and Montanari, Andrea and Rakhlin, Alexander},
  journal={Acta numerica},
  volume={30},
  pages={87--201},
  year={2021},
  publisher={Cambridge University Press}
}
@article{dar2021farewell,
  title={A farewell to the bias-variance tradeoff? an overview of the theory of overparameterized machine learning},
  author={Dar, Yehuda and Muthukumar, Vidya and Baraniuk, Richard G},
  journal={arXiv preprint arXiv:2109.02355},
  year={2021}
}
@inproceedings{gunasekar2018characterizing,
  title={Characterizing implicit bias in terms of optimization geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={1832--1841},
  year={2018},
  organization={PMLR}
}
@inproceedings{
    yun2021a,
    title={A unifying view on implicit bias in training linear neural networks},
    author={Chulhee Yun and Shankar Krishnan and Hossein Mobahi},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=ZsZM-4iMQkH}
}
@inproceedings{negrea2020defense,
  title={In defense of uniform convergence: Generalization via derandomization with an application to interpolating predictors},
  author={Negrea, Jeffrey and Dziugaite, Gintare Karolina and Roy, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={7263--7272},
  year={2020},
  organization={PMLR}
}
@article{mitra2019understanding,
  title={Understanding overfitting peaks in generalization error: Analytical risk curves for {$\ell_2$} and {$\ell_1$} penalized interpolation},
  author={Mitra, Partha P},
  journal={arXiv preprint arXiv:1906.03667},
  year={2019}
}
@article{raskutti2011minimax,
  title={Minimax rates of estimation for high-dimensional linear regression over {$\ell_q$}-balls},
  author={Raskutti, Garvesh and Wainwright, Martin J and Yu, Bin},
  journal={IEEE transactions on information theory},
  volume={57},
  number={10},
  pages={6976--6994},
  year={2011},
  publisher={IEEE}
}
@article{lounici2011oracle,
  title={Oracle inequalities and optimal inference under group sparsity},
  author={Lounici, Karim and Pontil, Massimiliano and Van De Geer, Sara and Tsybakov, Alexandre B},
  journal={The annals of statistics},
  volume={39},
  number={4},
  pages={2164--2204},
  year={2011},
  publisher={Institute of Mathematical Statistics}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}
@article{advani2020high,
  title={High-dimensional dynamics of generalization error in neural networks},
  author={Advani, Madhu S and Saxe, Andrew M and Sompolinsky, Haim},
  journal={Neural Networks},
  volume={132},
  pages={428--446},
  year={2020},
  publisher={Elsevier}
}
@inproceedings{
    li2021towards,
    title={Towards Resolving the Implicit Bias of Gradient Descent for Matrix Factorization: Greedy Low-Rank Learning},
    author={Zhiyuan Li and Yuping Luo and Kaifeng Lyu},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=AHOs7Sm5H7R}
}

@article{gidel2019implicit,
  title={Implicit regularization of discrete gradient dynamics in linear neural networks},
  author={Gidel, Gauthier and Bach, Francis and Lacoste-Julien, Simon},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@inproceedings{
du2018gradient,
title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
author={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1eK3i09YQ},
}
@inproceedings{allen2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}
@book{wainwright2019high,
  title={High-dimensional statistics: A non-asymptotic viewpoint},
  author={Wainwright, Martin J},
  volume={48},
  year={2019},
  publisher={Cambridge University Press}
}
@article{li2022implicit,
  title={Implicit bias of gradient descent on reparametrized models: On equivalence to mirror descent},
  author={Li, Zhiyuan and Wang, Tianhao and Lee, Jason D and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34626--34640},
  year={2022}
}
@article{candes2005decoding,
  title={Decoding by linear programming},
  author={Candes, Emmanuel J and Tao, Terence},
  journal={IEEE transactions on information theory},
  volume={51},
  number={12},
  pages={4203--4215},
  year={2005},
  publisher={IEEE}
}
@inproceedings{
frei2023implicit,
title={Implicit Bias in Leaky ReLU Networks Trained on High-Dimensional Data },
author={Spencer Frei and Gal Vardi and Peter Bartlett and Nathan Srebro and Wei Hu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=JpbLyEI5EwW}
}
@inproceedings{razin2022implicit,
  title={Implicit regularization in hierarchical tensor factorization and deep convolutional neural networks},
  author={Razin, Noam and Maman, Asaf and Cohen, Nadav},
  booktitle={International Conference on Machine Learning},
  pages={18422--18462},
  year={2022},
  organization={PMLR}
}
@inproceedings{timor2023implicit,
  title={Implicit regularization towards rank minimization in relu networks},
  author={Timor, Nadav and Vardi, Gal and Shamir, Ohad},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={1429--1459},
  year={2023},
  organization={PMLR}
}
