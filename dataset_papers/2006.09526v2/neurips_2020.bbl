\begin{thebibliography}{10}

\bibitem{aldarmaki2019context}
Hanan Aldarmaki and Mona Diab.
\newblock Context-aware cross-lingual mapping.
\newblock In {\em Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 3906--3911, 2019.

\bibitem{artetxe2016learning}
Mikel Artetxe, Gorka Labaka, and Eneko Agirre.
\newblock Learning principled bilingual mappings of word embeddings while
  preserving monolingual invariance.
\newblock In {\em Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}, pages 2289--2294, 2016.

\bibitem{artetxe-etal-2018-robust}
Mikel Artetxe, Gorka Labaka, and Eneko Agirre.
\newblock A robust self-learning method for fully unsupervised cross-lingual
  mappings of word embeddings.
\newblock In {\em Proceedings of the 56th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 789--798,
  Melbourne, Australia, July 2018. Association for Computational Linguistics.

\bibitem{artetxe2019bilingual}
Mikel Artetxe, Gorka Labaka, and Eneko Agirre.
\newblock Bilingual lexicon induction through unsupervised machine translation.
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pages 5002--5007, 2019.

\bibitem{artetxe2019margin}
Mikel Artetxe and Holger Schwenk.
\newblock Margin-based parallel corpus mining with multilingual sentence
  embeddings.
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pages 3197--3203, 2019.

\bibitem{artetxe2019massively}
Mikel Artetxe and Holger Schwenk.
\newblock Massively multilingual sentence embeddings for zero-shot
  cross-lingual transfer and beyond.
\newblock {\em Transactions of the Association for Computational Linguistics},
  7:597--610, 2019.

\bibitem{barrault2019findings}
Lo{\"\i}c Barrault, Ond{\v{r}}ej Bojar, Marta~R Costa-juss{\`a}, Christian
  Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp
  Koehn, Shervin Malmasi, et~al.
\newblock Findings of the 2019 conference on machine translation (wmt19).
\newblock In {\em Proceedings of the Fourth Conference on Machine Translation
  (Volume 2: Shared Task Papers, Day 1)}, pages 1--61, 2019.

\bibitem{bojar-etal-2013-findings}
Ond{\v{r}}ej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann,
  Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and
  Lucia Specia.
\newblock Findings of the 2013 {W}orkshop on {S}tatistical {M}achine
  {T}ranslation.
\newblock In {\em Proceedings of the Eighth Workshop on Statistical Machine
  Translation}, pages 1--44, Sofia, Bulgaria, August 2013. Association for
  Computational Linguistics.

\bibitem{bojar2016findings}
Ond{\v{r}}ej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry
  Haddow, Matthias Huck, Antonio~Jimeno Yepes, Philipp Koehn, Varvara
  Logacheva, Christof Monz, et~al.
\newblock Findings of the 2016 conference on machine translation.
\newblock In {\em Proceedings of the First Conference on Machine Translation:
  Volume 2, Shared Task Papers}, pages 131--198, 2016.

\bibitem{bojar-etal-2018-findings}
Ond{\v{r}}ej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry
  Haddow, Philipp Koehn, and Christof Monz.
\newblock Findings of the 2018 conference on machine translation ({WMT}18).
\newblock In {\em Proceedings of the Third Conference on Machine Translation:
  Shared Task Papers}, pages 272--303, Belgium, Brussels, October 2018.
  Association for Computational Linguistics.

\bibitem{cettolo2017overview}
Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Niehues Jan, St{\"u}ker
  Sebastian, Sudoh Katsuitho, Yoshino Koichiro, and Federmann Christian.
\newblock Overview of the {IWSLT} 2017 evaluation campaign.
\newblock In {\em International Workshop on Spoken Language Translation}, pages
  2--14, 2017.

\bibitem{Cettolo2015TheI2}
Mauro Cettolo, Jan Niehues, Sebastian St{\"u}ker, Luisa Bentivogli, R.~Cattoni,
  and Marcello Federico.
\newblock The iwslt 2015 evaluation campaign.
\newblock 2015.

\bibitem{Chaudhary2019LowResourceCF}
Vishrav Chaudhary, Yuqing Tang, Francisco Guzm{\'a}n, Holger Schwenk, and
  Philipp Koehn.
\newblock Low-resource corpus filtering using multilingual sentence embeddings.
\newblock In {\em Proceedings of the Fourth Conference on Machine Translation
  (Volume 3: Shared Task Papers, Day 2)}, pages 261--266, 2019.

\bibitem{conneau2019unsupervised}
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
  Wenzek, Francisco Guzm{\'a}n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and
  Veselin Stoyanov.
\newblock Unsupervised cross-lingual representation learning at scale.
\newblock {\em arXiv preprint arXiv:1911.02116}, 2019.

\bibitem{conneau2019cross}
Alexis Conneau and Guillaume Lample.
\newblock Cross-lingual language model pretraining.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7057--7067, 2019.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

\bibitem{dong2019unified}
Li~Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu~Wang, Jianfeng Gao,
  Ming Zhou, and Hsiao-Wuen Hon.
\newblock Unified language model pre-training for natural language
  understanding and generation.
\newblock {\em arXiv preprint arXiv:1905.03197}, 2019.

\bibitem{garcia2020multilingual}
Xavier Garcia, Pierre Foret, Thibault Sellam, and Ankur~P Parikh.
\newblock A multilingual view of unsupervised machine translation.
\newblock {\em arXiv preprint arXiv:2002.02955}, 2020.

\bibitem{guo2018effective}
Mandy Guo, Qinlan Shen, Yinfei Yang, Heming Ge, Daniel Cer, Gustavo~Hernandez
  Abrego, Keith Stevens, Noah Constant, Yun-Hsuan Sung, Brian Strope, et~al.
\newblock Effective parallel corpus mining using bilingual sentence embeddings.
\newblock In {\em Proceedings of the Third Conference on Machine Translation:
  Research Papers}, pages 165--176, 2018.

\bibitem{guu2020realm}
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.
\newblock Realm: Retrieval-augmented language model pre-training.
\newblock {\em arXiv preprint arXiv:2002.08909}, 2020.

\bibitem{guzman2019flores}
Francisco Guzm{\'a}n, Peng-Jen Chen, Myle Ott, Juan Pino, Guillaume Lample,
  Philipp Koehn, Vishrav Chaudhary, and Marcâ€™Aurelio Ranzato.
\newblock The flores evaluation datasets for low-resource machine translation:
  Nepali--english and sinhala--english.
\newblock In {\em Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 6100--6113, 2019.

\bibitem{hangya2019unsupervised}
Viktor Hangya and Alexander Fraser.
\newblock Unsupervised parallel sentence extraction with parallel segment
  detection helps machine translation.
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pages 1224--1234, 2019.

\bibitem{hu2020xtreme}
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and
  Melvin Johnson.
\newblock Xtreme: A massively multilingual multi-task benchmark for evaluating
  cross-lingual generalization.
\newblock {\em arXiv preprint arXiv:2003.11080}, 2020.

\bibitem{johnson2019billion}
Jeff Johnson, Matthijs Douze, and Herv{\'e} J{\'e}gou.
\newblock Billion-scale similarity search with gpus.
\newblock {\em IEEE Transactions on Big Data}, 2019.

\bibitem{khandelwal2019generalization}
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.
\newblock Generalization through memorization: Nearest neighbor language
  models.
\newblock {\em arXiv preprint arXiv:1911.00172}, 2019.

\bibitem{khayrallah-xu-koehn:2018:WMT}
Huda Khayrallah, Hainan Xu, and Philipp Koehn.
\newblock The {JHU} parallel corpus filtering systems for {WMT} 2018.
\newblock In {\em Proceedings of the Third Conference on Machine Translation,
  Volume 2: Shared Task Papers}, pages 909--912, Belgium, Brussels, October
  2018. Association for Computational Linguistics.

\bibitem{kudo2018sentencepiece}
Taku Kudo and John Richardson.
\newblock Sentencepiece: A simple and language independent subword tokenizer
  and detokenizer for neural text processing.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 66--71, 2018.

\bibitem{kunchukuttan2018iit}
Anoop Kunchukuttan, Pratik Mehta, and Pushpak Bhattacharyya.
\newblock The iit bombay english-hindi parallel corpus.
\newblock In {\em Proceedings of the Eleventh International Conference on
  Language Resources and Evaluation (LREC 2018)}, 2018.

\bibitem{lample2019cross}
Guillaume Lample and Alexis Conneau.
\newblock Cross-lingual language model pretraining.
\newblock {\em arXiv preprint arXiv:1901.07291}, 2019.

\bibitem{lample2017unsupervised}
Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc'Aurelio Ranzato.
\newblock Unsupervised machine translation using monolingual corpora only.
\newblock {\em arXiv preprint arXiv:1711.00043}, 2017.

\bibitem{lample2018word}
Guillaume Lample, Alexis Conneau, Marc'Aurelio Ranzato, Ludovic Denoyer, and
  Herv{\'e} J{\'e}gou.
\newblock Word translation without parallel data.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{lewis2019bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock {\em arXiv preprint arXiv:1910.13461}, 2019.

\bibitem{li2019data}
Zuchao Li, Rui Wang, Kehai Chen, Masso Utiyama, Eiichiro Sumita, Zhuosheng
  Zhang, and Hai Zhao.
\newblock Data-dependent gaussian prior objective for language generation.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{liu2020multilingual}
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan
  Ghazvininejad, Mike Lewis, and Luke Zettlemoyer.
\newblock Multilingual denoising pre-training for neural machine translation.
\newblock {\em arXiv preprint arXiv:2001.08210}, 2020.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{mikolov2013exploiting}
Tomas Mikolov, Quoc~V Le, and Ilya Sutskever.
\newblock Exploiting similarities among languages for machine translation.
\newblock {\em arXiv preprint arXiv:1309.4168}, 2013.

\bibitem{nakazawa-etal-2019-overview}
Toshiaki Nakazawa, Nobushige Doi, Shohei Higashiyama, Chenchen Ding, Raj Dabre,
  Hideya Mino, Isao Goto, Win~Pa Pa, Anoop Kunchukuttan, Shantipriya Parida,
  Ond{\v{r}}ej Bojar, and Sadao Kurohashi.
\newblock Overview of the 6th workshop on {A}sian translation.
\newblock In {\em Proceedings of the 6th Workshop on Asian Translation}, pages
  1--35, Hong Kong, China, November 2019. Association for Computational
  Linguistics.

\bibitem{ondrej2017findings}
Bojar Ondrej, Rajen Chatterjee, Federmann Christian, Graham Yvette, Haddow
  Barry, Huck Matthias, Koehn Philipp, Liu Qun, Logacheva Varvara, Monz
  Christof, et~al.
\newblock Findings of the 2017 conference on machine translation (wmt17).
\newblock In {\em Second Conference onMachine Translation}, pages 169--214. The
  Association for Computational Linguistics, 2017.

\bibitem{ott2019fairseq}
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
  David Grangier, and Michael Auli.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock In {\em Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics (Demonstrations)},
  pages 48--53, 2019.

\bibitem{papineni2002bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In {\em Proceedings of the 40th annual meeting on association for
  computational linguistics}, pages 311--318. Association for Computational
  Linguistics, 2002.

\bibitem{peters2018deep}
Matthew~E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
  Kenton Lee, and Luke Zettlemoyer.
\newblock Deep contextualized word representations.
\newblock In {\em Proceedings of NAACL-HLT}, pages 2227--2237, 2018.

\bibitem{pires2019multilingual}
Telmo Pires, Eva Schlinger, and Dan Garrette.
\newblock How multilingual is multilingual bert?
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pages 4996--5001, 2019.

\bibitem{qi2018and}
Ye~Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig.
\newblock When and why are pre-trained word embeddings useful for neural
  machine translation?
\newblock In {\em Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 2 (Short Papers)}, pages 529--535, 2018.

\bibitem{radford2018gpt}
Alec Radford, Karthik Narasimhan, Time Salimans, and Ilya Sutskever.
\newblock Improving language understanding with unsupervised learning.
\newblock Technical report, OpenAI, 2018.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock Technical report, OpenAI, 2019.

\bibitem{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em arXiv preprint arXiv:1910.10683}, 2019.

\bibitem{ren2019explicit}
Shuo Ren, Yu~Wu, Shujie Liu, Ming Zhou, and Shuai Ma.
\newblock Explicit cross-lingual pre-training for unsupervised machine
  translation.
\newblock In {\em Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 770--779, 2019.

\bibitem{schwenk2018filtering}
Holger Schwenk.
\newblock Filtering and mining parallel data in a joint multilingual space.
\newblock In {\em Proceedings of the 56th Annual Meeting of the Association for
  Computational Linguistics (Volume 2: Short Papers)}, pages 228--234, 2018.

\bibitem{schwenk2019ccmatrix}
Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, and Armand
  Joulin.
\newblock Ccmatrix: Mining billions of high-quality parallel sentences on the
  web.
\newblock {\em arXiv preprint arXiv:1911.04944}, 2019.

\bibitem{sennrich-etal-2016-improving}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Improving neural machine translation models with monolingual data.
\newblock In {\em Proceedings of the 54th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 86--96, Berlin,
  Germany, August 2016. Association for Computational Linguistics.

\bibitem{song2019mass}
Kaitao Song, Xu~Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.
\newblock Mass: Masked sequence to sequence pre-training for language
  generation.
\newblock In {\em International Conference on Machine Learning}, pages
  5926--5936, 2019.

\bibitem{uszkoreit2010large}
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Dubiner.
\newblock Large scale parallel document mining for machine translation.
\newblock In {\em Proceedings of the 23rd International Conference on
  Computational Linguistics (Coling 2010)}, pages 1101--1109, 2010.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, 2017.

\bibitem{DBLP:wada}
Takashi Wada and Tomoharu Iwata.
\newblock Unsupervised cross-lingual word embedding by multilingual neural
  language models.
\newblock {\em CoRR}, abs/1809.02306, 2018.

\bibitem{wang2019cross}
Zihan Wang, Stephen Mayhew, Dan Roth, et~al.
\newblock Cross-lingual ability of multilingual bertg: An empirical study.
\newblock {\em arXiv preprint arXiv:1912.07840}, 2019.

\bibitem{wenzek2019ccnet}
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary,
  Francisco Guzman, Armand Joulin, and Edouard Grave.
\newblock Ccnet: Extracting high quality monolingual datasets from web crawl
  data.
\newblock {\em arXiv preprint arXiv:1911.00359}, 2019.

\bibitem{wu2019extract}
Jiawei Wu, Xin Wang, and William~Yang Wang.
\newblock Extract and edit: An alternative to back-translation for unsupervised
  neural machine translation.
\newblock In {\em Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 1173--1183, 2019.

\bibitem{wu2019emerging}
Shijie Wu, Alexis Conneau, Haoran Li, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Emerging cross-lingual structure in pretrained language models.
\newblock {\em arXiv preprint arXiv:1911.01464}, 2019.

\bibitem{xu2017zipporah}
Hainan Xu and Philipp Koehn.
\newblock Zipporah: a fast and scalable data cleaning system for noisy
  web-crawled parallel corpora.
\newblock In {\em Proceedings of the 2017 Conference on Empirical Methods in
  Natural Language Processing}, pages 2945--2950, Denmark, Cophenhagen,
  September 2017. Association for Computational Linguistics.

\bibitem{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ~R Salakhutdinov,
  and Quoc~V Le.
\newblock {XLNET}: Generalized autoregressive pretraining for language
  understanding.
\newblock In {\em Advances in neural information processing systems}, pages
  5754--5764, 2019.

\end{thebibliography}
