\begin{thebibliography}{81}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbe and Sandon(2020)]{abbe2020poly}
Emmanuel Abbe and Colin Sandon.
\newblock Poly-time universality and limitations of deep learning.
\newblock \emph{arXiv preprint arXiv:2001.02992}, 2020.

\bibitem[Abbe et~al.(2021)Abbe, Kamath, Malach, Sandon, and
  Srebro]{abbe2021power}
Emmanuel Abbe, Pritish Kamath, Eran Malach, Colin Sandon, and Nathan Srebro.
\newblock On the power of differentiable learning versus {PAC} and {SQ}
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Abbe et~al.(2022)Abbe, Cornacchia, H{\k{a}}z{\l}a, and
  Marquis]{abbe2022initial}
Emmanuel Abbe, Elisabetta Cornacchia, Jan H{\k{a}}z{\l}a, and Christopher
  Marquis.
\newblock An initial alignment between neural network and target is needed for
  gradient descent to learn.
\newblock \emph{arXiv preprint arXiv:2202.12846}, 2022.

\bibitem[Agarwal et~al.(2020)Agarwal, Anil, Hazan, Koren, and
  Zhang]{agarwal2020disentangling}
Naman Agarwal, Rohan Anil, Elad Hazan, Tomer Koren, and Cyril Zhang.
\newblock Disentangling adaptive gradient methods from learning rates.
\newblock \emph{arXiv preprint arXiv:2002.11803}, 2020.

\bibitem[Alekhnovich(2003)]{alekhnovich2003more}
Michael Alekhnovich.
\newblock More on average case vs approximation complexity.
\newblock In \emph{44th Annual IEEE Symposium on Foundations of Computer
  Science, 2003. Proceedings.}, pages 298--307. IEEE, 2003.

\bibitem[Allen-Zhu and Li(2019)]{allen2019can}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock What can {R}es{N}et learn efficiently, going beyond kernels?
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pages
  242--252. PMLR, 2019.

\bibitem[Andoni et~al.(2014)Andoni, Panigrahy, Valiant, and
  Zhang]{andoni2014learning}
Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li~Zhang.
\newblock Learning polynomials with neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  1908--1916. PMLR, 2014.

\bibitem[Anil et~al.(2022)Anil, Wu, Andreassen, Lewkowycz, Misra, Ramasesh,
  Slone, Gur-Ari, Dyer, and Neyshabur]{anil2022exploring}
Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay
  Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur.
\newblock Exploring length generalization in large language models.
\newblock \emph{arXiv preprint arXiv:2207.04901}, 2022.

\bibitem[Applebaum et~al.(2009)Applebaum, Cash, Peikert, and
  Sahai]{applebaum2009fast}
Benny Applebaum, David Cash, Chris Peikert, and Amit Sahai.
\newblock Fast cryptographic primitives and circular-secure encryption based on
  hard learning problems.
\newblock In \emph{Annual International Cryptology Conference}, pages 595--618.
  Springer, 2009.

\bibitem[Applebaum et~al.(2010)Applebaum, Barak, and
  Wigderson]{applebaum2010public}
Benny Applebaum, Boaz Barak, and Avi Wigderson.
\newblock Public-key cryptography from different assumptions.
\newblock In \emph{Proceedings of the forty-second ACM symposium on Theory of
  computing}, pages 171--180, 2010.

\bibitem[Arous et~al.(2021)Arous, Gheissari, and Jagannath]{arous2021online}
Gerard~Ben Arous, Reza Gheissari, and Aukosh Jagannath.
\newblock Online stochastic gradient descent on non-convex losses from
  high-dimensional inference.
\newblock \emph{J. Mach. Learn. Res.}, 22:\penalty0 106--1, 2021.

\bibitem[Ba et~al.(2022)Ba, Erdogdu, Suzuki, Wang, Wu, and Yang]{ba2022high}
Jimmy Ba, Murat~A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang.
\newblock High-dimensional asymptotics of feature learning: How one gradient
  step improves the representation.
\newblock \emph{arXiv preprint arXiv:2205.01445}, 2022.

\bibitem[Belilovsky et~al.(2019)Belilovsky, Eickenberg, and
  Oyallon]{belilovsky2019greedy}
Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon.
\newblock Greedy layerwise learning can scale to imagenet.
\newblock In \emph{International conference on machine learning}, pages
  583--593. PMLR, 2019.

\bibitem[Berry(1941)]{berry1941accuracy}
Andrew~C Berry.
\newblock The accuracy of the gaussian approximation to the sum of independent
  variates.
\newblock \emph{Transactions of the American Mathematical Society}, 49\penalty0
  (1):\penalty0 122--136, 1941.

\bibitem[Bogdanov et~al.(2019)Bogdanov, Sabin, and Vasudevan]{bogdanov2019xor}
Andrej Bogdanov, Manuel Sabin, and Prashant~Nalini Vasudevan.
\newblock Xor codes and sparse learning parity with noise.
\newblock \emph{Proceedings of the Thirtieth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 986--1004, 2019.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Cao et~al.(2019)Cao, Fang, Wu, Zhou, and Gu]{cao2019towards}
Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu.
\newblock Towards understanding the spectral bias of deep learning.
\newblock \emph{arXiv preprint arXiv:1912.01198}, 2019.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Damian et~al.(2022)Damian, Lee, and Soltanolkotabi]{damian2022neural}
Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi.
\newblock Neural networks can learn representations with gradient descent.
\newblock In \emph{Conference on Learning Theory}, pages 5413--5452. PMLR,
  2022.

\bibitem[Daniely and Malach(2020)]{daniely2020learning}
Amit Daniely and Eran Malach.
\newblock Learning parities with neural networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 20356--20365, 2020.

\bibitem[Diakonikolas et~al.(2020)Diakonikolas, Goel, Karmalkar, Klivans, and
  Soltanolkotabi]{diakonikolas2020approximation}
Ilias Diakonikolas, Surbhi Goel, Sushrut Karmalkar, Adam~R Klivans, and Mahdi
  Soltanolkotabi.
\newblock Approximation schemes for relu regression.
\newblock In \emph{Conference on Learning Theory}, pages 1452--1485. PMLR,
  2020.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018.

\bibitem[Edelman et~al.(2021)Edelman, Goel, Kakade, and
  Zhang]{edelman2021inductive}
Benjamin~L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang.
\newblock Inductive biases and variable creation in self-attention mechanisms.
\newblock \emph{arXiv preprint arXiv:2110.10090}, 2021.

\bibitem[Engel and Van~den Broeck(2001)]{engel2001statistical}
Andreas Engel and Christian Van~den Broeck.
\newblock \emph{Statistical mechanics of learning}.
\newblock Cambridge University Press, 2001.

\bibitem[Esseen(1942)]{esseen1942liapunov}
Carl-Gustav Esseen.
\newblock On the {L}iapunov limit error in the theory of probability.
\newblock \emph{Ark. Mat. Astr. Fys.}, 28:\penalty0 1--19, 1942.

\bibitem[Frei et~al.(2020)Frei, Cao, and Gu]{frei2020agnostic}
Spencer Frei, Yuan Cao, and Quanquan Gu.
\newblock Agnostic learning of a single neuron with gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 5417--5428, 2020.

\bibitem[Frei et~al.(2022)Frei, Chatterji, and Bartlett]{frei2022random}
Spencer Frei, Niladri~S Chatterji, and Peter~L Bartlett.
\newblock Random feature amplification: Feature learning and generalization in
  neural networks.
\newblock \emph{arXiv preprint arXiv:2202.07626}, 2022.

\bibitem[Gardner and Derrida(1989)]{gardner1989three}
Elizabeth Gardner and Bernard Derrida.
\newblock Three unfinished works on the optimal storage capacity of networks.
\newblock \emph{Journal of Physics A: Mathematical and General}, 22\penalty0
  (12):\penalty0 1983, 1989.

\bibitem[Glorot and Bengio(2010)]{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem[Goel et~al.(2019)Goel, Karmalkar, and Klivans]{goel2019time}
Surbhi Goel, Sushrut Karmalkar, and Adam Klivans.
\newblock Time/accuracy tradeoffs for learning a {ReLU} with respect to
  {G}aussian marginals.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Goldreich and Levin(1989)]{goldreich1989hard}
Oded Goldreich and Leonid~A Levin.
\newblock A hard-core predicate for all one-way functions.
\newblock In \emph{Proceedings of the twenty-first annual ACM symposium on
  Theory of computing}, pages 25--32, 1989.

\bibitem[Goldt et~al.(2019)Goldt, Advani, Saxe, Krzakala, and
  Zdeborov{\'a}]{goldt2019dynamics}
Sebastian Goldt, Madhu Advani, Andrew~M Saxe, Florent Krzakala, and Lenka
  Zdeborov{\'a}.
\newblock Dynamics of stochastic gradient descent for two-layer neural networks
  in the teacher-student setup.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Hahn(2020)]{hahn2020theoretical}
Michael Hahn.
\newblock Theoretical limitations of self-attention in neural sequence models.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  8:\penalty0 156--171, 2020.

\bibitem[Hansel et~al.(1992)Hansel, Mato, and Meunier]{hansel1992memorization}
D~Hansel, G~Mato, and C~Meunier.
\newblock Memorization without generalization in a multilayered neural network.
\newblock \emph{EPL (Europhysics Letters)}, 20\penalty0 (5):\penalty0 471,
  1992.

\bibitem[Hardy et~al.(1952)Hardy, Littlewood, P{\'o}lya, P{\'o}lya,
  et~al.]{hardy1952inequalities}
Godfrey~Harold Hardy, John~Edensor Littlewood, George P{\'o}lya, Gy{\"o}rgy
  P{\'o}lya, et~al.
\newblock \emph{Inequalities}.
\newblock Cambridge University Press, 1952.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034, 2015.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Ishai et~al.(2008)Ishai, Kushilevitz, Ostrovsky, and
  Sahai]{ishai2008cryptography}
Yuval Ishai, Eyal Kushilevitz, Rafail Ostrovsky, and Amit Sahai.
\newblock Cryptography with constant computational overhead.
\newblock In \emph{Proceedings of the 40th Annual ACM Symposium on the Theory
  of Computing}, pages 433--442, 2008.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Ji and Telgarsky(2019)]{ji2019polylogarithmic}
Ziwei Ji and Matus Telgarsky.
\newblock Polylogarithmic width suffices for gradient descent to achieve
  arbitrarily small test error with shallow relu networks.
\newblock \emph{arXiv preprint arXiv:1909.12292}, 2019.

\bibitem[Kabashima(1994)]{kabashima1994perfect}
Y~Kabashima.
\newblock Perfect loss of generalization due to noise in k= 2 parity machines.
\newblock \emph{Journal of Physics A: Mathematical and General}, 27\penalty0
  (6):\penalty0 1917, 1994.

\bibitem[Kamath et~al.(2020)Kamath, Montasser, and
  Srebro]{kamath2020approximate}
Pritish Kamath, Omar Montasser, and Nathan Srebro.
\newblock Approximate is good enough: Probabilistic variants of dimensional and
  margin complexity.
\newblock In \emph{Conference on Learning Theory}, pages 2236--2262. PMLR,
  2020.

\bibitem[Kearns(1998)]{kearns1998efficient}
Michael Kearns.
\newblock Efficient noise-tolerant learning from statistical queries.
\newblock \emph{Journal of the ACM (JACM)}, 45\penalty0 (6):\penalty0
  983--1006, 1998.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Klivans and Kothari(2014)]{klivans2014embedding}
Adam Klivans and Pravesh Kothari.
\newblock Embedding hard learning problems into gaussian space.
\newblock In \emph{Approximation, Randomization, and Combinatorial
  Optimization. Algorithms and Techniques (APPROX/RANDOM 2014)}. Schloss
  Dagstuhl-Leibniz-Zentrum fuer Informatik, 2014.

\bibitem[Klivans et~al.(2004)Klivans, O'Donnell, and
  Servedio]{klivans2004learning}
Adam~R Klivans, Ryan O'Donnell, and Rocco~A Servedio.
\newblock Learning intersections and thresholds of halfspaces.
\newblock \emph{Journal of Computer and System Sciences}, 68\penalty0
  (4):\penalty0 808--840, 2004.

\bibitem[Kol et~al.(2017)Kol, Raz, and Tal]{kol2017time}
Gillat Kol, Ran Raz, and Avishay Tal.
\newblock Time-space hardness of learning sparse parities.
\newblock In \emph{Proceedings of the 49th Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 1067--1080, 2017.

\bibitem[Kushilevitz and Mansour(1993)]{kushilevitz1993learning}
Eyal Kushilevitz and Yishay Mansour.
\newblock Learning decision trees using the fourier spectrum.
\newblock \emph{SIAM Journal on Computing}, 22\penalty0 (6):\penalty0
  1331--1348, 1993.

\bibitem[Laurent and Massart(2000)]{laurent2000adaptive}
Beatrice Laurent and Pascal Massart.
\newblock Adaptive estimation of a quadratic functional by model selection.
\newblock \emph{Annals of Statistics}, pages 1302--1338, 2000.

\bibitem[Liu et~al.(2022)Liu, Ash, Goel, Krishnamurthy, and
  Zhang]{liu2022transformers}
Bingbin Liu, Jordan~T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang.
\newblock Transformers learn shortcuts to automata.
\newblock \emph{arXiv preprint arXiv:2210.10749}, 2022.

\bibitem[Lu et~al.(2021)Lu, Grover, Abbeel, and Mordatch]{lu2021pretrained}
Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch.
\newblock Pretrained transformers as universal computation engines.
\newblock \emph{arXiv preprint arXiv:2103.05247}, 2021.

\bibitem[Malach and Shalev-Shwartz(2019)]{malach2019deeper}
Eran Malach and Shai Shalev-Shwartz.
\newblock Is deeper better only when shallow is good?
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Malach and Shalev-Shwartz(2022)]{malach2022hardness}
Eran Malach and Shai Shalev-Shwartz.
\newblock When hardness of approximation meets hardness of learning.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (91):\penalty0 1--24, 2022.

\bibitem[Malach et~al.(2021)Malach, Kamath, Abbe, and
  Srebro]{malach2021quantifying}
Eran Malach, Pritish Kamath, Emmanuel Abbe, and Nathan Srebro.
\newblock Quantifying the benefit of using differentiable learning over tangent
  kernels.
\newblock In \emph{International Conference on Machine Learning}, pages
  7379--7389. PMLR, 2021.

\bibitem[Mitchison and Durbin(1989)]{mitchison1989bounds}
GJ~Mitchison and RM~Durbin.
\newblock Bounds on the learning capacity of some multi-layer networks.
\newblock \emph{Biological Cybernetics}, 60\penalty0 (5):\penalty0 345--365,
  1989.

\bibitem[Nanda and Lieberum(2022)]{nanda2022mechanistic}
Neel Nanda and Tom Lieberum.
\newblock A mechanistic interpretability analysis of grokking.
\newblock \emph{Alignment Forum}, 2022.
\newblock URL
  \url{https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking}.

\bibitem[O'Donnell(2014)]{o2014analysis}
Ryan O'Donnell.
\newblock \emph{Analysis of {B}oolean functions}.
\newblock Cambridge University Press, 2014.

\bibitem[Opper(1994)]{opper1994learning}
Manfred Opper.
\newblock Learning and generalization in a two-layer neural network: The role
  of the vapnik-chervonvenkis dimension.
\newblock \emph{Physical review letters}, 72\penalty0 (13):\penalty0 2113,
  1994.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Py{T}orch: An imperative style, high-performance deep learning
  library.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems 32}, pages 8024--8035. Curran Associates,
  Inc., 2019.
\newblock URL
  \url{http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}.

\bibitem[Power et~al.(2022)Power, Burda, Edwards, Babuschkin, and
  Misra]{power2022grokking}
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.
\newblock Grokking: Generalization beyond overfitting on small algorithmic
  datasets.
\newblock \emph{arXiv preprint arXiv:2201.02177}, 2022.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rahaman et~al.(2019)Rahaman, Baratin, Arpit, Draxler, Lin, Hamprecht,
  Bengio, and Courville]{rahaman2019spectral}
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred
  Hamprecht, Yoshua Bengio, and Aaron Courville.
\newblock On the spectral bias of neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  5301--5310. PMLR, 2019.

\bibitem[Refinetti et~al.(2021)Refinetti, Goldt, Krzakala, and
  Zdeborov{\'a}]{refinetti2021classifying}
Maria Refinetti, Sebastian Goldt, Florent Krzakala, and Lenka Zdeborov{\'a}.
\newblock Classifying high-dimensional gaussian mixtures: Where kernel methods
  fail and neural networks succeed.
\newblock In \emph{International Conference on Machine Learning}, pages
  8936--8947. PMLR, 2021.

\bibitem[Rosen-Zvi et~al.(2002)Rosen-Zvi, Klein, Kanter, and
  Kinzel]{rosen2002mutual}
Michal Rosen-Zvi, Einat Klein, Ido Kanter, and Wolfgang Kinzel.
\newblock Mutual learning in a tree parity machine and its application to
  cryptography.
\newblock \emph{Physical Review E}, 66\penalty0 (6):\penalty0 066135, 2002.

\bibitem[Saad and Solla(1995{\natexlab{a}})]{saad1995dynamics}
David Saad and Sara Solla.
\newblock Dynamics of on-line gradient descent learning for multilayer neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 8,
  1995{\natexlab{a}}.

\bibitem[Saad and Solla(1995{\natexlab{b}})]{saad1995line}
David Saad and Sara~A Solla.
\newblock On-line learning in soft committee machines.
\newblock \emph{Physical Review E}, 52\penalty0 (4):\penalty0 4225,
  1995{\natexlab{b}}.

\bibitem[Shalev-Shwartz and Ben-David(2014)]{shalev2014understanding}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Shalev-Shwartz et~al.(2017)Shalev-Shwartz, Shamir, and
  Shammah]{shalev2017failures}
Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah.
\newblock Failures of gradient-based deep learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  3067--3075. PMLR, 2017.

\bibitem[Shi et~al.(2021)Shi, Wei, and Liang]{shi2021theoretical}
Zhenmei Shi, Junyi Wei, and Yingyu Liang.
\newblock A theoretical analysis on feature learning in neural networks:
  Emergence from inputs and advantage over fixed features.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Simonetti and Caticha(1996)]{simonetti1996line}
Roberta Simonetti and Nestor Caticha.
\newblock On-line learning in parity machines.
\newblock \emph{Journal of Physics A: Mathematical and General}, 29\penalty0
  (16):\penalty0 4859, 1996.

\bibitem[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch,
  Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2022beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar
  Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a}
  Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the
  capabilities of language models.
\newblock \emph{arXiv preprint arXiv:2206.04615}, 2022.

\bibitem[Telgarsky(2022)]{telgarsky2022feature}
Matus Telgarsky.
\newblock Feature selection with gradient descent on two-layer networks in
  low-rotation regimes.
\newblock \emph{arXiv preprint arXiv:2208.02789}, 2022.

\bibitem[Titsworth(1962)]{titsworth1962correlation}
Robert~C Titsworth.
\newblock \emph{Correlation properties of cyclic sequences}.
\newblock PhD thesis, California Institute of Technology, 1962.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wainwright(2019)]{wainwright2019high}
Martin~J Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Watkin et~al.(1993)Watkin, Rau, and Biehl]{watkin1993statistical}
Timothy~LH Watkin, Albrecht Rau, and Michael Biehl.
\newblock The statistical mechanics of learning a rule.
\newblock \emph{Reviews of Modern Physics}, 65\penalty0 (2):\penalty0 499,
  1993.

\bibitem[Wei et~al.(2019)Wei, Lee, Liu, and Ma]{wei2019regularization}
Colin Wei, Jason~D Lee, Qiang Liu, and Tengyu Ma.
\newblock Regularization matters: generalization and optimization of neural
  nets vs their induced kernel.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Yehudai and Ohad(2020)]{yehudai2020learning}
Gilad Yehudai and Shamir Ohad.
\newblock Learning a single neuron with gradient methods.
\newblock In \emph{Conference on Learning Theory}, pages 3756--3786. PMLR,
  2020.

\bibitem[Yehudai and Shamir(2019)]{yehudai2019power}
Gilad Yehudai and Ohad Shamir.
\newblock On the power and limitations of random features for understanding
  neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Karimireddy, Veit, Kim, Reddi, Kumar, and
  Sra]{zhang2020adaptive}
Jingzhao Zhang, Sai~Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank
  Reddi, Sanjiv Kumar, and Suvrit Sra.
\newblock Why are adaptive methods good for attention models?
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15383--15393, 2020.

\end{thebibliography}
