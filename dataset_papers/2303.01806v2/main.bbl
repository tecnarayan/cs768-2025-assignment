\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bai et~al.(2021)Bai, Yang, Han, Yang, Li, Mao, Niu, and
  Liu]{understanding_early}
Bai, Y., Yang, E., Han, B., Yang, Y., Li, J., Mao, Y., Niu, G., and Liu, T.
\newblock Understanding and improving early stopping for learning with noisy
  labels.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Baldock et~al.(2021)Baldock, Maennel, and Neyshabur]{baldock_2021}
Baldock, R. J.~N., Maennel, H., and Neyshabur, B.
\newblock Deep learning through the lens of example difficulty.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Cheng et~al.(2021)Cheng, Zhu, Li, Gong, Sun, and Liu]{cores}
Cheng, H., Zhu, Z., Li, X., Gong, Y., Sun, X., and Liu, Y.
\newblock Learning with instance-dependent label noise: {A} sample sieve
  approach.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Collier et~al.(2021)Collier, Mustafa, Kokiopoulou, Jenatton, and
  Berent]{het}
Collier, M., Mustafa, B., Kokiopoulou, E., Jenatton, R., and Berent, J.
\newblock Correlated input-dependent label noise in large-scale image
  classification.
\newblock In \emph{{IEEE} Conference on Computer Vision and Pattern Recognition
  ({CVPR})}, 2021.

\bibitem[Collier et~al.(2022)Collier, Jenatton, Kokiopoulou, and Berent]{tram}
Collier, M., Jenatton, R., Kokiopoulou, E., and Berent, J.
\newblock Transfer and marginalize: Explaining away label noise with privileged
  information.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[D'Amour et~al.(2020)D'Amour, Heller, Moldovan, Adlam, Alipanahi,
  Beutel, Chen, Deaton, Eisenstein, Hoffman, Hormozdiari, Houlsby, Hou, Jerfel,
  Karthikesalingam, Lucic, Ma, McLean, Mincu, Mitani, Montanari, Nado,
  Natarajan, Nielson, Osborne, Raman, Ramasamy, Sayres, Schrouff, Seneviratne,
  Sequeira, Suresh, Veitch, Vladymyrov, Wang, Webster, Yadlowsky, Yun, Zhai,
  and Sculley]{alex_underspecification}
D'Amour, A., Heller, K.~A., Moldovan, D., Adlam, B., Alipanahi, B., Beutel, A.,
  Chen, C., Deaton, J., Eisenstein, J., Hoffman, M.~D., Hormozdiari, F.,
  Houlsby, N., Hou, S., Jerfel, G., Karthikesalingam, A., Lucic, M., Ma, Y.,
  McLean, C.~Y., Mincu, D., Mitani, A., Montanari, A., Nado, Z., Natarajan, V.,
  Nielson, C., Osborne, T.~F., Raman, R., Ramasamy, K., Sayres, R., Schrouff,
  J., Seneviratne, M., Sequeira, S., Suresh, H., Veitch, V., Vladymyrov, M.,
  Wang, X., Webster, K., Yadlowsky, S., Yun, T., Zhai, X., and Sculley, D.
\newblock Underspecification presents challenges for credibility in modern
  machine learning.
\newblock \emph{CoRR}, abs/2011.03395, 2020.
\newblock URL \url{https://arxiv.org/abs/2011.03395}.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Kai, and
  Li]{dengImageNetLargescaleHierarchical2009}
Deng, J., Dong, W., Socher, R., Li, L.~J., Kai, L., and Li, F.~F.
\newblock {{ImageNet}}: {{A}} large-scale hierarchical image database.
\newblock In \emph{{{IEEE Conference}} on {{Computer Vision}} and {{Pattern
  Recognition}} ({{CVPR}})}, pp.\  248--255, 2009.

\bibitem[Geirhos et~al.(2020)Geirhos, Jacobsen, Michaelis, Zemel, Brendel,
  Bethge, and Wichmann]{shortcut_geirhos}
Geirhos, R., Jacobsen, J., Michaelis, C., Zemel, R.~S., Brendel, W., Bethge,
  M., and Wichmann, F.~A.
\newblock Shortcut learning in deep neural networks.
\newblock \emph{Nat. Mach. Intell.}, 2\penalty0 (11):\penalty0 665--673, 2020.

\bibitem[Han et~al.(2018)Han, Yao, Yu, Niu, Xu, Hu, Tsang, and
  Sugiyama]{coteaching}
Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I.~W., and Sugiyama,
  M.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{standard_distillation}
Hinton, G.~E., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{CoRR}, abs/1503.02531, 2015.

\bibitem[Kallus et~al.(2018)Kallus, Puli, and Shalit]{aahlad}
Kallus, N., Puli, A.~M., and Shalit, U.
\newblock Removing hidden confounding by experimental grounding.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Krizhevsky(2009)]{krizhevskyLearningMultipleLayers2009}
Krizhevsky, A.
\newblock Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}.
\newblock Technical report, University of Toronto, 2009.

\bibitem[Lambert et~al.(2018)Lambert, Sener, and Savarese]{gaussian_dropout}
Lambert, J., Sener, O., and Savarese, S.
\newblock Deep learning under privileged information using heteroscedastic
  dropout.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2018.

\bibitem[Li et~al.(2020)Li, Socher, and Hoi]{dividemix}
Li, J., Socher, R., and Hoi, S. C.~H.
\newblock Dividemix: Learning with noisy labels as semi-supervised learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Liu et~al.(2020)Liu, Niles{-}Weed, Razavian, and
  Fernandez{-}Granda]{early_learning_reg}
Liu, S., Niles{-}Weed, J., Razavian, N., and Fernandez{-}Granda, C.
\newblock Early-learning regularization prevents memorization of noisy labels.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Liu et~al.(2022)Liu, Zhu, Qu, and You]{sop}
Liu, S., Zhu, Z., Qu, Q., and You, C.
\newblock Robust training under label noise by over-parameterization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Lopez-Paz et~al.(2016)Lopez-Paz, Bottou, Sch\"olkopf, and
  Vapnik]{distill_pi}
Lopez-Paz, D., Bottou, L., Sch\"olkopf, B., and Vapnik, V.
\newblock Unifying distillation and privileged information.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[Maennel et~al.(2020)Maennel, Alabdulmohsin, Tolstikhin, Baldock,
  Bousquet, Gelly, and Keysers]{maennel_2020}
Maennel, H., Alabdulmohsin, I.~M., Tolstikhin, I.~O., Baldock, R. J.~N.,
  Bousquet, O., Gelly, S., and Keysers, D.
\newblock What do neural networks learn when trained with random labels?
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Makar et~al.(2022)Makar, Packer, Moldovan, Blalock, Halpern, and
  D'Amour]{shortcut_removal}
Makar, M., Packer, B., Moldovan, D., Blalock, D., Halpern, Y., and D'Amour, A.
\newblock Causally motivated shortcut removal using auxiliary labels.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics, ({AISTATS})}, 2022.

\bibitem[Minderer et~al.(2020)Minderer, Bachem, Houlsby, and
  Tschannen]{automatic_shortcut}
Minderer, M., Bachem, O., Houlsby, N., and Tschannen, M.
\newblock Automatic shortcut removal for self-supervised representation
  learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Nado et~al.(2021)Nado, Band, Collier, Djolonga, Dusenberry, Farquhar,
  Feng, Filos, Havasi, Jenatton, et~al.]{nado2021uncertainty}
Nado, Z., Band, N., Collier, M., Djolonga, J., Dusenberry, M.~W., Farquhar, S.,
  Feng, Q., Filos, A., Havasi, M., Jenatton, R., et~al.
\newblock Uncertainty baselines: Benchmarks for uncertainty \& robustness in
  deep learning.
\newblock \emph{arXiv preprint arXiv:2106.04015}, 2021.

\bibitem[Patrini et~al.(2017)Patrini, Rozza, Menon, Nock, and
  Qu]{matrix_estimation}
Patrini, G., Rozza, A., Menon, A.~K., Nock, R., and Qu, L.
\newblock Making deep neural networks robust to label noise: {A} loss
  correction approach.
\newblock In \emph{{IEEE} Conference on Computer Vision and Pattern
  Recognition, ({CVPR})}, 2017.

\bibitem[Pearl(2009)]{pearl}
Pearl, J.
\newblock \emph{Causality: Models, Reasoning and Inference}.
\newblock Cambridge University Press, 2009.

\bibitem[Peterson et~al.(2019)Peterson, Battleday, Griffiths, and
  Russakovsky]{cifarh}
Peterson, J.~C., Battleday, R.~M., Griffiths, T.~L., and Russakovsky, O.
\newblock Human uncertainty makes classification more robust.
\newblock In \emph{2019 {IEEE/CVF} International Conference on Computer Vision,
  {ICCV} 2019, Seoul, Korea (South), October 27 - November 2, 2019}, 2019.

\bibitem[Pezeshki et~al.(2021)Pezeshki, Kaba, Bengio, Courville, Precup, and
  Lajoie]{gradient_starvation}
Pezeshki, M., Kaba, S., Bengio, Y., Courville, A.~C., Precup, D., and Lajoie,
  G.
\newblock Gradient starvation: {A} learning proclivity in neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Prabhu et~al.(2022)Prabhu, Yenamandra, Singh, and
  Hoffman]{attention_conditioned_masking_consistency}
Prabhu, V., Yenamandra, S., Singh, A., and Hoffman, J.
\newblock Adapting self-supervised vision transformers by probing
  attention-conditioned masking consistency.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2022.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8748--8763. PMLR, 2021.

\bibitem[Rolnick et~al.(2017)Rolnick, Veit, Belongie, and
  Shavit]{rolnick2017deep}
Rolnick, D., Veit, A., Belongie, S., and Shavit, N.
\newblock Deep learning is robust to massive label noise.
\newblock \emph{arXiv preprint arXiv:1705.10694}, 2017.

\bibitem[Sheng et~al.(2008)Sheng, Provost, and Ipeirotis]{sheng_2008}
Sheng, V.~S., Provost, F., and Ipeirotis, P.~G.
\newblock Get another label? improving data quality and data mining using
  multiple, noisy labelers.
\newblock In \emph{Proceedings of the 14th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining (KDD)}, 2008.

\bibitem[Snow et~al.(2008)Snow, O{'}Connor, Jurafsky, and
  Ng]{snow-etal-2008-cheap}
Snow, R., O{'}Connor, B., Jurafsky, D., and Ng, A.
\newblock Cheap and fast {--} but is it good? evaluating non-expert annotations
  for natural language tasks.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing (EMNLP)}, October 2008.

\bibitem[Song et~al.(2020)Song, Kim, Park, and Lee]{label_noise_survey}
Song, H., Kim, M., Park, D., and Lee, J.
\newblock Learning from noisy labels with deep neural networks: {A} survey.
\newblock \emph{CoRR}, abs/2007.08199, 2020.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and Wojna]{ls}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{{IEEE} Conference on Computer Vision and Pattern
  Recognition, ({CVPR})}, 2016.

\bibitem[Toneva et~al.(2019)Toneva, Sordoni, des Combes, Trischler, Bengio, and
  Gordon]{toneva_2019}
Toneva, M., Sordoni, A., des Combes, R.~T., Trischler, A., Bengio, Y., and
  Gordon, G.~J.
\newblock An empirical study of example forgetting during deep neural network
  learning.
\newblock In \emph{nternational Conference on Learning Representations (ICLR)},
  2019.

\bibitem[Vapnik \& Izmailov(2015)Vapnik and Izmailov]{svm+}
Vapnik, V. and Izmailov, R.
\newblock Learning using privileged information: similarity control and
  knowledge transfer.
\newblock \emph{J. Mach. Learn. Res.}, 16:\penalty0 2023--2049, 2015.

\bibitem[Vapnik \& Vashist(2009)Vapnik and Vashist]{lupi_vapnik}
Vapnik, V. and Vashist, A.
\newblock A new learning paradigm: Learning using privileged information.
\newblock \emph{Neural Networks}, 22\penalty0 (5-6), 2009.

\bibitem[Veit et~al.(2017)Veit, Alldrin, Chechik, Krasin, Gupta, and
  Belongie]{google_old_noise}
Veit, A., Alldrin, N., Chechik, G., Krasin, I., Gupta, A., and Belongie, S.~J.
\newblock Learning from noisy large-scale datasets with minimal supervision.
\newblock In \emph{{IEEE} Conference on Computer Vision and Pattern Recognition
  ({CVPR})}, 2017.

\bibitem[Veitch et~al.(2021)Veitch, D'Amour, Yadlowsky, and
  Eisenstein]{counterfactual_inv}
Veitch, V., D'Amour, A., Yadlowsky, S., and Eisenstein, J.
\newblock Counterfactual invariance to spurious correlations in text
  classification.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Wei et~al.(2022)Wei, Zhu, Cheng, Liu, Niu, and Liu]{cifarn}
Wei, J., Zhu, Z., Cheng, H., Liu, T., Niu, G., and Liu, Y.
\newblock Learning with noisy labels revisited: A study using real-world human
  annotations.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem[Yang et~al.(2017)Yang, Zhou, Cai, and Ong]{miml_pi}
Yang, H., Zhou, J.~T., Cai, J., and Ong, Y.
\newblock {MIML-FCN+:} multi-instance multi-label learning via fully
  convolutional networks with privileged information.
\newblock In \emph{2017 {IEEE} Conference on Computer Vision and Pattern
  Recognition, ({CVPR})}, 2017.

\bibitem[Yang et~al.(2022)Yang, Sanghavi, Rahmanian, Bakus, and
  Vishwanathan]{yang2022toward}
Yang, S., Sanghavi, S., Rahmanian, H., Bakus, J., and Vishwanathan, S.
\newblock Toward understanding privileged features distillation in
  learning-to-rank.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2022.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{ZhangBengioHardtRechtVinyals.17}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Zhao et~al.(2022)Zhao, Yang, and He]{Zhao_2022}
Zhao, P., Yang, Y., and He, Q.-C.
\newblock High-dimensional linear regression via implicit regularization.
\newblock \emph{Biometrika}, 109\penalty0 (4):\penalty0 1033--1046, feb 2022.
\newblock \doi{10.1093/biomet/asac010}.
\newblock URL \url{https://doi.org/10.1093%2Fbiomet%2Fasac010}.

\end{thebibliography}
