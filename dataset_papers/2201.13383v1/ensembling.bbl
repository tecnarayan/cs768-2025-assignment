\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adlam and Pennington(2020{\natexlab{a}})]{NEURIPS2020_7d420e2b}
Ben Adlam and Jeffrey Pennington.
\newblock Understanding double descent requires a fine-grained bias-variance
  decomposition.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 11022--11032. Curran Associates, Inc., 2020{\natexlab{a}}.

\bibitem[Adlam and Pennington(2020{\natexlab{b}})]{adlam2020neural}
Ben Adlam and Jeffrey Pennington.
\newblock The neural tangent kernel in high dimensions: Triple descent and a
  multi-scale theory of generalization.
\newblock In \emph{International Conference on Machine Learning}, pages 74--84.
  PMLR, 2020{\natexlab{b}}.

\bibitem[Advani and Saxe(2017)]{advani2017highdimensional}
Madhu~S. Advani and Andrew~M. Saxe.
\newblock High-dimensional dynamics of generalization error in neural networks,
  2017.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and
  Tsigler]{Bartlett30063}
Peter~L. Bartlett, Philip~M. Long, G{\'a}bor Lugosi, and Alexander Tsigler.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (48):\penalty0 30063--30070, 2020.

\bibitem[Bauschke et~al.(2006)Bauschke, Combettes, and Noll]{bauschke2006joint}
Heinz Bauschke, Patrick Combettes, and Dominikus Noll.
\newblock Joint minimization with alternating bregman proximity operators.
\newblock \emph{Pacific Journal of Optimization}, 2006.

\bibitem[Bauschke et~al.(2003)Bauschke, Borwein, and
  Combettes]{bauschke2003bregman}
Heinz~H Bauschke, Jonathan~M Borwein, and Patrick~L Combettes.
\newblock Bregman monotone optimization algorithms.
\newblock \emph{SIAM Journal on control and optimization}, 42\penalty0
  (2):\penalty0 596--636, 2003.

\bibitem[Bauschke et~al.(2011)Bauschke, Combettes, et~al.]{bauschke2011convex}
Heinz~H Bauschke, Patrick~L Combettes, et~al.
\newblock \emph{Convex analysis and monotone operator theory in Hilbert
  spaces}, volume 408.
\newblock Springer, 2011.

\bibitem[Bayati and Montanari(2011{\natexlab{a}})]{bayati2011dynamics}
Mohsen Bayati and Andrea Montanari.
\newblock The dynamics of message passing on dense graphs, with applications to
  compressed sensing.
\newblock \emph{IEEE Transactions on Information Theory}, 57\penalty0
  (2):\penalty0 764--785, 2011{\natexlab{a}}.

\bibitem[Bayati and Montanari(2011{\natexlab{b}})]{bayati2011lasso}
Mohsen Bayati and Andrea Montanari.
\newblock {The LASSO risk for Gaussian matrices}.
\newblock \emph{IEEE Transactions on Information Theory}, 58\penalty0
  (4):\penalty0 1997--2017, 2011{\natexlab{b}}.

\bibitem[Belkin et~al.(2020)Belkin, Hsu, Ma, and Mandal]{Belkin10627}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reply to loog et al.: Looking beyond the peaking phenomenon.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (20):\penalty0 10627--10627, 2020.

\bibitem[Berthier et~al.(2020)Berthier, Montanari, and
  Nguyen]{berthier2020state}
Raphael Berthier, Andrea Montanari, and Phan-Minh Nguyen.
\newblock State evolution for approximate message passing with non-separable
  functions.
\newblock \emph{Information and Inference: A Journal of the IMA}, 9\penalty0
  (1):\penalty0 33--79, 2020.

\bibitem[Bottou(2012)]{bottou2012stochastic}
L{\'e}on Bottou.
\newblock Stochastic gradient descent tricks.
\newblock In \emph{Neural networks: Tricks of the trade}, pages 421--436.
  Springer, 2012.

\bibitem[Breiman(1996)]{Breiman1996}
Leo Breiman.
\newblock Bagging predictors.
\newblock \emph{Machine Learning}, 24:\penalty0 123--140, 1996.

\bibitem[Celentano et~al.(2020)Celentano, Montanari, and
  Wei]{celentano2020lasso}
Michael Celentano, Andrea Montanari, and Yuting Wei.
\newblock {The Lasso with general Gaussian designs with applications to
  hypothesis testing}.
\newblock \emph{Preprint arXiv:2007.13716}, 2020.

\bibitem[Chen et~al.(2020)Chen, Min, Belkin, and Karbasi]{chen2020multiple}
Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi.
\newblock Multiple descent: Design your own generalization curve.
\newblock \emph{arXiv preprint arXiv:2008.01036}, 2020.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{NEURIPS2019_ae614c55}
L\'{e}na\"{\i}c Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem[D'Ascoli et~al.(2020)D'Ascoli, Refinetti, Biroli, and
  Krzakala]{dascoli2020}
St{\'e}phane D'Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala.
\newblock Double trouble in double descent: Bias and variance(s) in the lazy
  regime.
\newblock In Hal Daum\'e~III and Aarti Singh, editors, \emph{Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 2280--2290. PMLR,
  13--18 Jul 2020.

\bibitem[d'Ascoli et~al.(2021)d'Ascoli, Sagun, and Biroli]{dAscoli_2021}
St{\'{e}}phane d'Ascoli, Levent Sagun, and Giulio Biroli.
\newblock Triple descent and the two kinds of overfitting: where and why do
  they appear?
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2021\penalty0 (12):\penalty0 124002, dec 2021.

\bibitem[Dhifallah and Lu(2020)]{dhifallah2020precise}
Oussama Dhifallah and Yue~M Lu.
\newblock A precise performance analysis of learning with random features.
\newblock \emph{arXiv:2008.11904}, 2020.

\bibitem[Donoho and Montanari(2016)]{donoho2016high}
David Donoho and Andrea Montanari.
\newblock High dimensional robust m-estimation: Asymptotic variance via
  approximate message passing.
\newblock \emph{Probability Theory and Related Fields}, 166\penalty0
  (3):\penalty0 935--969, 2016.

\bibitem[Donoho et~al.(2009)Donoho, Maleki, and Montanari]{donoho2009message}
David~L Donoho, Arian Maleki, and Andrea Montanari.
\newblock Message-passing algorithms for compressed sensing.
\newblock \emph{Proceedings of the National Academy of Sciences}, 106\penalty0
  (45):\penalty0 18914--18919, 2009.

\bibitem[Drucker et~al.(1994)Drucker, Cortes, Jackel, LeCun, and
  Vapnik]{drucker1994boosting}
Harris Drucker, Corinna Cortes, Lawrence~D Jackel, Yann LeCun, and Vladimir
  Vapnik.
\newblock Boosting and other ensemble methods.
\newblock \emph{Neural computation}, 6\penalty0 (6):\penalty0 1289--1301, 1994.

\bibitem[Geiger et~al.(2020)Geiger, Jacot, Spigler, Gabriel, Sagun, d’Ascoli,
  Biroli, Hongler, and Wyart]{geiger2020scaling}
Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun,
  St{\'e}phane d’Ascoli, Giulio Biroli, Cl{\'e}ment Hongler, and Matthieu
  Wyart.
\newblock Scaling description of generalization with number of parameters in
  deep learning.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2020\penalty0 (2):\penalty0 023401, 2020.

\bibitem[Gerace et~al.(2020)Gerace, Loureiro, Krzakala, Mezard, and
  Zdeborova]{pmlr-v119-gerace20a}
Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mezard, and Lenka
  Zdeborova.
\newblock Generalisation error in learning with random features and the hidden
  manifold model.
\newblock In Hal~Daumé III and Aarti Singh, editors, \emph{Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 3452--3462. PMLR,
  13--18 Jul 2020.

\bibitem[Gerbelot and Berthier(2021)]{gerbelot2021graph}
C{\'e}dric Gerbelot and Rapha{\"e}l Berthier.
\newblock Graph-based approximate message passing iterations.
\newblock \emph{arXiv preprint arXiv:2109.11905}, 2021.

\bibitem[Goldt et~al.(2020)Goldt, M\'ezard, Krzakala, and
  Zdeborov\'a]{PhysRevX.10.041044}
Sebastian Goldt, Marc M\'ezard, Florent Krzakala, and Lenka Zdeborov\'a.
\newblock Modeling the influence of data structure on learning in neural
  networks: The hidden manifold model.
\newblock \emph{Physical Review X}, 10:\penalty0 041044, Dec 2020.

\bibitem[Goldt et~al.(2021)Goldt, Loureiro, Reeves, Krzakala, M{\'e}zard, and
  Zdeborov{\'a}]{goldt2021gaussian}
Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc
  M{\'e}zard, and Lenka Zdeborov{\'a}.
\newblock The gaussian equivalence of generative models for learning with
  shallow neural networks.
\newblock \emph{Proceedings of Machine Learning Research}, 145:\penalty0 1--46,
  2021.

\bibitem[Hansen and Salamon(1990)]{Hansen1990}
Lars~Kai Hansen and Peter Salamon.
\newblock Neural network ensembles.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 12\penalty0 (10):\penalty0 993--1001, 1990.

\bibitem[Hu and Lu(2020)]{hu2020universality}
Hong Hu and Yue~M Lu.
\newblock Universality laws for high-dimensional learning with random features.
\newblock \emph{arXiv:2009.07669}, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{NEURIPS2018_5a4be1fa}
Arthur Jacot, Franck Gabriel, and Clement Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem[Jacot et~al.(2020)Jacot, Simsek, Spadaro, Hongler, and
  Gabriel]{jacot2020implicit}
Arthur Jacot, Berfin Simsek, Francesco Spadaro, Cl{\'e}ment Hongler, and Franck
  Gabriel.
\newblock Implicit regularization of random feature models.
\newblock In \emph{International Conference on Machine Learning}, pages
  4631--4640. PMLR, 2020.

\bibitem[Javanmard and Montanari(2013)]{javanmard2013state}
Adel Javanmard and Andrea Montanari.
\newblock State evolution for general approximate message passing algorithms,
  with applications to spatial coupling.
\newblock \emph{Information and Inference: A Journal of the IMA}, 2\penalty0
  (2):\penalty0 115--144, 2013.

\bibitem[Karoui(2010)]{10.1214/08-AOS648}
Noureddine~El Karoui.
\newblock {The spectrum of kernel random matrices}.
\newblock \emph{The Annals of Statistics}, 38\penalty0 (1):\penalty0 1 -- 50,
  2010.

\bibitem[Krogh and Sollich(1997)]{Krogh1997}
Anders Krogh and Peter Sollich.
\newblock Statistical mechanics of ensemble learning.
\newblock \emph{Phys. Rev. E}, 55:\penalty0 811--825, Jan 1997.

\bibitem[Krogh and Vedelsby(1995)]{NIPS1994_b8c37e33}
Anders Krogh and Jesper Vedelsby.
\newblock Neural network ensembles, cross validation, and active learning.
\newblock In G.~Tesauro, D.~Touretzky, and T.~Leen, editors, \emph{Advances in
  Neural Information Processing Systems}, volume~7. MIT Press, 1995.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{lakshminarayanan_deep_ensembles_2017}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles, 2017.

\bibitem[Lin and Dobriban(2021)]{JMLR:v22:20-1211}
Licong Lin and Edgar Dobriban.
\newblock {What Causes the Test Error? Going Beyond Bias-Variance via ANOVA}.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (155):\penalty0 1--82, 2021.

\bibitem[Loureiro et~al.(2021{\natexlab{a}})Loureiro, Gerbelot, Cui, Goldt,
  Krzakala, M\'ezard, and Zdeborov\'a]{Loureiro2021}
Bruno Loureiro, C\'edric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala,
  Marc M\'ezard, and Lenka Zdeborov\'a.
\newblock Capturing the learning curves of generic features maps for realistic
  data sets with a teacher-student model.
\newblock \emph{arXiv:2102.08127}, 2021{\natexlab{a}}.

\bibitem[Loureiro et~al.(2021{\natexlab{b}})Loureiro, Sicuro, Gerbelot, Pacco,
  Krzakala, and Zdeborov{\'a}]{loureiro2021learning}
Bruno Loureiro, Gabriele Sicuro, C{\'e}dric Gerbelot, Alessandro Pacco, Florent
  Krzakala, and Lenka Zdeborov{\'a}.
\newblock Learning gaussian mixtures with generalised linear models: Precise
  asymptotics in high-dimensions.
\newblock \emph{arXiv:2106.03791}, 2021{\natexlab{b}}.

\bibitem[Mei and Montanari(2021)]{mei2020generalization}
Song Mei and Andrea Montanari.
\newblock The generalization error of random features regression: Precise
  asymptotics and the double descent curve.
\newblock \emph{Communications on Pure and Applied Mathematics}, 2021.

\bibitem[M{\'e}zard et~al.(1987)M{\'e}zard, Parisi, and
  Virasoro]{mezard1987spin}
Marc M{\'e}zard, Giorgio Parisi, and Miguel Virasoro.
\newblock \emph{Spin glass theory and beyond: An Introduction to the Replica
  Method and Its Applications}, volume~9.
\newblock World Scientific Publishing Company, 1987.

\bibitem[Narkhede et~al.(2021)Narkhede, Bartakke, and
  Sutaone]{narkhede2021review}
Meenal~V Narkhede, Prashant~P Bartakke, and Mukul~S Sutaone.
\newblock A review on weight initialization strategies for neural networks.
\newblock \emph{Artificial intelligence review}, pages 1--32, 2021.

\bibitem[Neal et~al.(2018)Neal, Mittal, Baratin, Tantia, Scicluna,
  Lacoste-Julien, and Mitliagkas]{neal2018modern}
Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna,
  Simon Lacoste-Julien, and Ioannis Mitliagkas.
\newblock A modern take on the bias-variance tradeoff in neural networks.
\newblock \emph{arXiv preprint arXiv:1810.08591}, 2018.

\bibitem[Opitz and Maclin(1999)]{Opitz1999}
David Opitz and Richard Maclin.
\newblock Popular ensemble methods: An empirical study.
\newblock \emph{Journal of Artificial Intelligence Research}, 11:\penalty0
  169--198, 8 1999.

\bibitem[Parikh and Boyd(2014)]{parikh2014proximal}
Neal Parikh and Stephen Boyd.
\newblock Proximal algorithms.
\newblock \emph{Foundations and Trends in optimization}, 1\penalty0
  (3):\penalty0 127--239, 2014.

\bibitem[Pennington and Worah(2017)]{NIPS2017_0f3d014e}
Jeffrey Pennington and Pratik Worah.
\newblock Nonlinear random matrix theory for deep learning.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Perrone(1994)]{NIPS1993_0537fb40}
Michael Perrone.
\newblock Putting it all together: Methods for combining neural networks.
\newblock In J.~Cowan, G.~Tesauro, and J.~Alspector, editors, \emph{Advances in
  Neural Information Processing Systems}, volume~6. Morgan-Kaufmann, 1994.

\bibitem[Perrone and Cooper(1993)]{Perrone93whennetworks}
Michael~P. Perrone and Leaon~N. Cooper.
\newblock When networks disagree: Ensemble methods for hybrid neural networks.
\newblock pages 126--142. Chapman and Hall, 1993.

\bibitem[Rahimi and Recht(2007)]{rahimi2007random}
Ali Rahimi and Benjamin Recht.
\newblock {Random Features for Large-Scale Kernel Machines}.
\newblock In \emph{NIPS}, pages 1177--1184, 2007.

\bibitem[Rangan(2011)]{rangan2011generalized}
Sundeep Rangan.
\newblock Generalized approximate message passing for estimation with random
  linear mixing.
\newblock In \emph{2011 IEEE International Symposium on Information Theory
  Proceedings}, pages 2168--2172. IEEE, 2011.

\bibitem[Rosset et~al.(2004)Rosset, Zhu, and Hastie]{NIPS2003_0fe47339}
Saharon Rosset, Ji~Zhu, and Trevor Hastie.
\newblock Margin maximizing loss functions.
\newblock In S.~Thrun, L.~Saul, and B.~Sch\"{o}lkopf, editors, \emph{Advances
  in Neural Information Processing Systems}, volume~16. MIT Press, 2004.

\bibitem[Schapire(1990)]{schapire1990strength}
Robert~E Schapire.
\newblock The strength of weak learnability.
\newblock \emph{Machine learning}, 5\penalty0 (2):\penalty0 197--227, 1990.

\bibitem[Schwarze and Hertz(1992)]{Schwarze1992}
H~Schwarze and J~Hertz.
\newblock Generalization in a large committee machine.
\newblock \emph{Europhysics Letters ({EPL})}, 20\penalty0 (4):\penalty0
  375--380, oct 1992.

\bibitem[Spigler et~al.(2019)Spigler, Geiger, d'Ascoli, Sagun, Biroli, and
  Wyart]{Spigler_2019}
Stefano Spigler, Mario Geiger, Stephane d'Ascoli, Levent Sagun, Giulio Biroli,
  and Matthieu Wyart.
\newblock A jamming transition from under- to over-parametrization affects
  generalization in deep learning.
\newblock \emph{Journal of Physics A: Mathematical and Theoretical},
  52\penalty0 (47):\penalty0 474001, oct 2019.

\bibitem[Thrampoulidis et~al.(2018)Thrampoulidis, Abbasi, and
  Hassibi]{thrampoulidis2018precise}
Christos Thrampoulidis, Ehsan Abbasi, and Babak Hassibi.
\newblock Precise error analysis of regularized $ m $-estimators in high
  dimensions.
\newblock \emph{IEEE Transactions on Information Theory}, 64\penalty0
  (8):\penalty0 5592--5628, 2018.

\bibitem[Vershynin(2010)]{vershynin2010introduction}
Roman Vershynin.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock \emph{arXiv:1011.3027}, 2010.

\bibitem[Vershynin(2018)]{vershynin2018high}
Roman Vershynin.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[Zdeborov{\'a} and Krzakala(2016)]{zdeborova2016statistical}
Lenka Zdeborov{\'a} and Florent Krzakala.
\newblock Statistical physics of inference: Thresholds and algorithms.
\newblock \emph{Advances in Physics}, 65\penalty0 (5):\penalty0 453--552, 2016.

\end{thebibliography}
