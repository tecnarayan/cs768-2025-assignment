\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bengio et~al.(1994)Bengio, Simard, and Frasconi]{bengio1994.LearningLongtermDependencies}
Bengio, Y., Simard, P., and Frasconi, P.
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock \emph{IEEE Transactions on Neural Networks}, 5\penalty0 (2):\penalty0 157--166, March 1994.
\newblock ISSN 1941-0093.
\newblock \doi{10.1109/72.279181}.

\bibitem[Boyd \& Chua(1985)Boyd and Chua]{boyd1985.FadingMemoryProblem}
Boyd, S. and Chua, L.
\newblock Fading memory and the problem of approximating nonlinear operators with {{Volterra}} series.
\newblock \emph{IEEE Transactions on Circuits and Systems}, 32\penalty0 (11):\penalty0 1150--1161, November 1985.
\newblock ISSN 0098-4094.
\newblock \doi{10.1109/TCS.1985.1085649}.

\bibitem[Boyd et~al.(1984)Boyd, Chua, and Desoer]{boyd1984.AnalyticalFoundationsVolterra}
Boyd, S., Chua, L.~O., and Desoer, C.~A.
\newblock Analytical {{Foundations}} of {{Volterra Series}}.
\newblock \emph{IMA Journal of Mathematical Control and Information}, 1\penalty0 (3):\penalty0 243--282, January 1984.
\newblock ISSN 0265-0754.
\newblock \doi{10.1093/imamci/1.3.243}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, and Askell]{brown2020.LanguageModelsArea}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., and Askell, A.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Connor et~al.(1994)Connor, Martin, and Atlas]{connor1994.RecurrentNeuralNetworksa}
Connor, J.~T., Martin, R.~D., and Atlas, L.~E.
\newblock Recurrent neural networks and robust time series prediction.
\newblock \emph{IEEE transactions on neural networks}, 5\penalty0 (2):\penalty0 240--254, 1994.

\bibitem[Gu \& Dao(2023)Gu and Dao]{gu2023.MambaLinearTimeSequence}
Gu, A. and Dao, T.
\newblock Mamba: {{Linear-Time Sequence Modeling}} with {{Selective State Spaces}}, December 2023.

\bibitem[Gu et~al.(2020)Gu, Dao, Ermon, Rudra, and R{\'e}]{gu2020.HiPPORecurrentMemorya}
Gu, A., Dao, T., Ermon, S., Rudra, A., and R{\'e}, C.
\newblock {{HiPPO}}: {{Recurrent Memory}} with {{Optimal Polynomial Projections}}.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, volume~33, pp.\  1474--1487. Curran Associates, Inc., 2020.

\bibitem[Gu et~al.(2022{\natexlab{a}})Gu, Goel, Gupta, and R{\'e}]{gu2022.ParameterizationInitializationDiagonal}
Gu, A., Goel, K., Gupta, A., and R{\'e}, C.
\newblock On the {{Parameterization}} and {{Initialization}} of {{Diagonal State Space Models}}.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 35971--35983, December 2022{\natexlab{a}}.

\bibitem[Gu et~al.(2022{\natexlab{b}})Gu, Goel, and Re]{gu2022.EfficientlyModelingLonga}
Gu, A., Goel, K., and Re, C.
\newblock Efficiently {{Modeling Long Sequences}} with {{Structured State Spaces}}.
\newblock In \emph{International {{Conference}} on {{Learning Representations}}}, January 2022{\natexlab{b}}.

\bibitem[Hochreiter(1998)]{hochreiter1998.VanishingGradientProblem}
Hochreiter, S.
\newblock The {{Vanishing Gradient Problem During Learning Recurrent Neural Nets}} and {{Problem Solutions}}.
\newblock \emph{International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems}, 06\penalty0 (02):\penalty0 107--116, April 1998.
\newblock ISSN 0218-4885, 1793-6411.
\newblock \doi{10.1142/S0218488598000094}.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and Schmidhuber]{hochreiter1997.LongShorttermMemory}
Hochreiter, S. and Schmidhuber, J.
\newblock Long {{Short-term Memory}}.
\newblock \emph{Neural computation}, 9:\penalty0 1735--80, December 1997.
\newblock \doi{10.1162/neco.1997.9.8.1735}.

\bibitem[Jiang et~al.(2023)Jiang, Li, Li, and Wang]{jiang2023.BriefSurveyApproximationa}
Jiang, H., Li, Q., Li, Z., and Wang, S.
\newblock A {{Brief Survey}} on the {{Approximation Theory}} for {{Sequence Modelling}}.
\newblock \emph{Journal of Machine Learning}, 2\penalty0 (1):\penalty0 1--30, June 2023.
\newblock ISSN 2790-203X, 2790-2048.
\newblock \doi{10.4208/jml.221221}.

\bibitem[Li et~al.(2019)Li, Wei, and Ma]{li2019.ExplainingRegularizationEffect}
Li, Y., Wei, C., and Ma, T.
\newblock Towards explaining the regularization effect of initial large learning rate in training neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Li et~al.(2020)Li, Han, E, and Li]{li2020.CurseMemoryRecurrent}
Li, Z., Han, J., E, W., and Li, Q.
\newblock On the {{Curse}} of {{Memory}} in {{Recurrent Neural Networks}}: {{Approximation}} and {{Optimization Analysis}}.
\newblock In \emph{International {{Conference}} on {{Learning Representations}}}, October 2020.

\bibitem[Li et~al.(2022)Li, Han, E, and Li]{li2022.ApproximationOptimizationTheory}
Li, Z., Han, J., E, W., and Li, Q.
\newblock Approximation and {{Optimization Theory}} for {{Linear Continuous-Time Recurrent Neural Networks}}.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0 (42):\penalty0 1--85, 2022.
\newblock ISSN 1533-7928.

\bibitem[Martin \& Cundy(2018)Martin and Cundy]{martin2018.ParallelizingLinearRecurrent}
Martin, E. and Cundy, C.
\newblock Parallelizing {{Linear Recurrent Neural Nets Over Sequence Length}}.
\newblock In \emph{International {{Conference}} on {{Learning Representations}}}, February 2018.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and Socher]{merity2016.PointerSentinelMixtureb}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer {{Sentinel Mixture Models}}.
\newblock In \emph{International {{Conference}} on {{Learning Representations}}}, 2016.

\bibitem[Orvieto et~al.(2023{\natexlab{a}})Orvieto, De, Gulcehre, Pascanu, and Smith]{orvieto2023universality}
Orvieto, A., De, S., Gulcehre, C., Pascanu, R., and Smith, S.~L.
\newblock On the universality of linear recurrences followed by nonlinear projections.
\newblock \emph{arXiv preprint arXiv:2307.11888}, 2023{\natexlab{a}}.

\bibitem[Orvieto et~al.(2023{\natexlab{b}})Orvieto, Smith, Gu, Fernando, Gulcehre, Pascanu, and De]{orvieto2023.ResurrectingRecurrentNeurald}
Orvieto, A., Smith, S.~L., Gu, A., Fernando, A., Gulcehre, C., Pascanu, R., and De, S.
\newblock Resurrecting recurrent neural networks for long sequences.
\newblock In \emph{Proceedings of the 40th {{International Conference}} on {{Machine Learning}}}, volume 202 of \emph{{{ICML}}'23}, pp.\  26670--26698. JMLR.org, July 2023{\natexlab{b}}.

\bibitem[Peng et~al.(2023)Peng, Alcaide, Anthony, Albalak, Arcadinho, Cao, Cheng, Chung, Grella, GV, et~al.]{peng2023rwkv}
Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K.~K., et~al.
\newblock {{RWKV}}: {{Reinventing RNNs}} for the transformer era.
\newblock \emph{arXiv preprint arXiv:2305.13048}, 2023.

\bibitem[Poli et~al.(2023)Poli, Massaroli, Nguyen, Fu, Dao, Baccus, Bengio, Ermon, and Re]{poli2023.HyenaHierarchyLarger}
Poli, M., Massaroli, S., Nguyen, E., Fu, D.~Y., Dao, T., Baccus, S., Bengio, Y., Ermon, S., and Re, C.
\newblock Hyena {{Hierarchy}}: {{Towards Larger Convolutional Language Models}}.
\newblock In \emph{International {{Conference}} on {{Machine Learning}}}, June 2023.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and Williams]{rumelhart1986.LearningRepresentationsBackpropagating}
Rumelhart, D.~E., Hinton, G.~E., and Williams, R.~J.
\newblock Learning representations by back-propagating errors.
\newblock \emph{Nature}, 323\penalty0 (6088):\penalty0 533--536, October 1986.
\newblock ISSN 1476-4687.
\newblock \doi{10.1038/323533a0}.

\bibitem[Rusch \& Mishra(2022)Rusch and Mishra]{rusch2022.CoupledOscillatoryRecurrent}
Rusch, T.~K. and Mishra, S.
\newblock Coupled {{Oscillatory Recurrent Neural Network}} ({{coRNN}}): {{An}} accurate and (gradient) stable architecture for learning long time dependencies.
\newblock In \emph{International {{Conference}} on {{Learning Representations}}}, February 2022.

\bibitem[Siivola \& Honkela(2003)Siivola and Honkela]{siivola2003.StatespaceMethodLanguage}
Siivola, V. and Honkela, A.
\newblock A state-space method for language modeling.
\newblock In \emph{2003 {{IEEE Workshop}} on {{Automatic Speech Recognition}} and {{Understanding}} ({{IEEE Cat}}. {{No}}.{{03EX721}})}, pp.\  548--553, St Thomas, VI, USA, 2003. IEEE.
\newblock ISBN 978-0-7803-7980-0.
\newblock \doi{10.1109/ASRU.2003.1318499}.

\bibitem[Smith et~al.(2023)Smith, Warrington, and Linderman]{smith2023.SimplifiedStateSpace}
Smith, J. T.~H., Warrington, A., and Linderman, S.
\newblock Simplified {{State Space Layers}} for {{Sequence Modeling}}.
\newblock In \emph{International {{Conference}} on {{Learning Representations}}}, February 2023.

\bibitem[Smith \& Topin(2019)Smith and Topin]{smith2019.SuperconvergenceVeryFast}
Smith, L.~N. and Topin, N.
\newblock Super-convergence: {{Very}} fast training of neural networks using large learning rates.
\newblock In \emph{Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications}, volume 11006, pp.\  369--386. SPIE, 2019.

\bibitem[Stein \& Shakarchi(2003)Stein and Shakarchi]{stein2003.PrincetonLecturesAnalysis}
Stein, E.~M. and Shakarchi, R.
\newblock \emph{Princeton Lectures in Analysis}.
\newblock Princeton University Press Princeton, 2003.

\bibitem[Sun et~al.(2023)Sun, Dong, Huang, Ma, Xia, Xue, Wang, and Wei]{sun2023retentive}
Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F.
\newblock Retentive network: {{A}} successor to transformer for large language models.
\newblock \emph{arXiv preprint arXiv:2307.08621}, 2023.

\bibitem[Sutskever et~al.(2011)Sutskever, Martens, and Hinton]{sutskever.GeneratingTextRecurrent}
Sutskever, I., Martens, J., and Hinton, G.
\newblock Generating {{Text}} with {{Recurrent Neural Networks}}.
\newblock In \emph{International {{Conference}} on {{Machine Learning}}}, pp.\  1017--1024, January 2011.

\bibitem[Tay et~al.(2021)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang, Ruder, and Metzler]{tay2021.LongRangeArena}
Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D.
\newblock Long {{Range Arena}} : {{A Benchmark}} for {{Efficient Transformers}}.
\newblock In \emph{International {{Conference}} on {{Learning Representations}}}, January 2021.

\bibitem[Tolimieri et~al.(1989)Tolimieri, An, and Lu]{tolimieri1989.AlgorithmsDiscreteFourier}
Tolimieri, R., An, M., and Lu, C.
\newblock \emph{Algorithms for {{Discrete Fourier Transform}} and {{Convolution}}}.
\newblock Signal {{Processing}} and {{Digital Filtering}}. Springer New York, New York, NY, 1989.
\newblock ISBN 978-1-4757-3856-8 978-1-4757-3854-4.
\newblock \doi{10.1007/978-1-4757-3854-4}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017.AttentionAllYou}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is {{All}} you {{Need}}.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Wang \& Xue(2023)Wang and Xue]{wang2023.StatespaceModelsLayerwisea}
Wang, S. and Xue, B.
\newblock State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory.
\newblock In \emph{Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems}}}, November 2023.

\bibitem[Wang \& Yan(2023)Wang and Yan]{wang2023improve}
Wang, S. and Yan, Z.
\newblock Improve long-term memory learning through rescaling the error temporally.
\newblock \emph{arXiv preprint arXiv:2307.11462}, 2023.

\bibitem[Wang et~al.(2023)Wang, Li, and Li]{wang2023.InverseApproximationTheory}
Wang, S., Li, Z., and Li, Q.
\newblock Inverse {{Approximation Theory}} for {{Nonlinear Recurrent Neural Networks}}.
\newblock In \emph{The {{Twelfth International Conference}} on {{Learning Representations}}}, October 2023.

\end{thebibliography}
