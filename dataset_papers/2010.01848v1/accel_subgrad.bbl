\begin{thebibliography}{93}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alayrac et~al.(2016)Alayrac, Bojanowski, Agrawal, Sivic, Laptev, and
  Lacoste-Julien]{alayrac2016unsupervised}
J.-B. Alayrac, P.~Bojanowski, N.~Agrawal, J.~Sivic, I.~Laptev, and
  S.~Lacoste-Julien.
\newblock Unsupervised learning from narrated instruction videos.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4575--4583, 2016.

\bibitem[Amos et~al.(2017)Amos, Xu, and Kolter]{amos2017input}
B.~Amos, L.~Xu, and J.~Z. Kolter.
\newblock Input convex neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 146--155. JMLR. org, 2017.

\bibitem[Bach(2015)]{bach2015duality}
F.~Bach.
\newblock Duality between subgradient and conditional gradient methods.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (1):\penalty0
  115--129, 2015.

\bibitem[Bach et~al.(2012)Bach, Jenatton, Mairal, Obozinski,
  et~al.]{bach2012optimization}
F.~Bach, R.~Jenatton, J.~Mairal, G.~Obozinski, et~al.
\newblock Optimization with sparsity-inducing penalties.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  4\penalty0 (1):\penalty0 1--106, 2012.

\bibitem[Balasubramanian and Ghadimi(2018)]{balasubramanian2018zeroth}
K.~Balasubramanian and S.~Ghadimi.
\newblock Zeroth-order (non)-convex stochastic optimization via conditional
  gradient and gradient updates.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3455--3464, 2018.

\bibitem[Bansal and Gupta(2017)]{bansal2017potential}
N.~Bansal and A.~Gupta.
\newblock Potential-function proofs for first-order methods.
\newblock \emph{arXiv preprint arXiv:1712.04581}, 2017.

\bibitem[Bauschke et~al.(2018)Bauschke, Dao, and
  Lindstrom]{bauschke2018regularizing}
H.~H. Bauschke, M.~N. Dao, and S.~B. Lindstrom.
\newblock Regularizing with bregman--moreau envelopes.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (4):\penalty0
  3208--3228, 2018.

\bibitem[Beck and Teboulle(2009)]{beck2009fast}
A.~Beck and M.~Teboulle.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock \emph{SIAM journal on imaging sciences}, 2\penalty0 (1):\penalty0
  183--202, 2009.

\bibitem[Beck and Teboulle(2012)]{beck2012smoothing}
A.~Beck and M.~Teboulle.
\newblock Smoothing and first order methods: A unified framework.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0
  557--580, 2012.

\bibitem[Ben-Tal et~al.(2012)Ben-Tal, Bhadra, Bhattacharyya, and
  Nemirovski]{ben2012efficient}
A.~Ben-Tal, S.~Bhadra, C.~Bhattacharyya, and A.~Nemirovski.
\newblock Efficient methods for robust classification under uncertainty in
  kernel matrices.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (Oct):\penalty0 2923--2954, 2012.

\bibitem[Bertsekas(1999)]{bertsekas1999nonlinear}
D.~P. Bertsekas.
\newblock \emph{Nonlinear Programming}.
\newblock Athena Scientific Belmont, 2 edition, 1999.

\bibitem[Bishop(2006)]{bishop2006pattern}
C.~M. Bishop.
\newblock \emph{Pattern recognition and machine learning}.
\newblock springer, 2006.

\bibitem[Bradley and Mangasarian(1998)]{bradley1998feature}
P.~S. Bradley and O.~L. Mangasarian.
\newblock Feature selection via concave minimization and support vector
  machines.
\newblock In \emph{ICML}, volume~98, pages 82--90, 1998.

\bibitem[Braun et~al.(2019)Braun, Pokutta, and Zink]{braun2019lazifying}
G.~Braun, S.~Pokutta, and D.~Zink.
\newblock Lazifying conditional gradient algorithms.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (71):\penalty0 1--42, 2019.

\bibitem[Cai et~al.(2010)Cai, Cand{\`e}s, and Shen]{cai2010singular}
J.-F. Cai, E.~J. Cand{\`e}s, and Z.~Shen.
\newblock A singular value thresholding algorithm for matrix completion.
\newblock \emph{SIAM Journal on optimization}, 20\penalty0 (4):\penalty0
  1956--1982, 2010.

\bibitem[Carmon et~al.(2019)Carmon, Jin, Sidford, and Tian]{carmon2019variance}
Y.~Carmon, Y.~Jin, A.~Sidford, and K.~Tian.
\newblock Variance reduction for matrix games.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  11377--11388, 2019.

\bibitem[Chen et~al.(2013)Chen, Yang, Lin, Zhang, and Chang]{chen2013optimal}
J.~Chen, T.~Yang, Q.~Lin, L.~Zhang, and Y.~Chang.
\newblock Optimal stochastic strongly convex optimization with a logarithmic
  number of projections.
\newblock \emph{arXiv preprint arXiv:1304.5504}, 2013.

\bibitem[Chen et~al.(2018{\natexlab{a}})Chen, Harshaw, Hassani, and
  Karbasi]{chen2018projection}
L.~Chen, C.~Harshaw, H.~Hassani, and A.~Karbasi.
\newblock Projection-free online optimization with stochastic gradient: From
  convexity to submodularity.
\newblock In \emph{International Conference on Machine Learning}, pages
  814--823, 2018{\natexlab{a}}.

\bibitem[Chen et~al.(2018{\natexlab{b}})Chen, Shi, and Zhang]{chen2018optimal}
Y.~Chen, Y.~Shi, and B.~Zhang.
\newblock Optimal control via neural networks: A convex approach.
\newblock In \emph{International Conference on Learning Representations},
  2018{\natexlab{b}}.

\bibitem[Chen et~al.(2020)Chen, Shi, and Zhang]{chen2020input}
Y.~Chen, Y.~Shi, and B.~Zhang.
\newblock Input convex neural networks for optimal voltage regulation.
\newblock \emph{arXiv preprint arXiv:2002.08684}, 2020.

\bibitem[Clark and Contributors(2020)]{Cla2020Pillow}
A.~Clark and Contributors.
\newblock Pillow: Python image-processing library, 2020.
\newblock URL \url{https://pillow.readthedocs.io/en/stable/}.
\newblock Documentation.

\bibitem[Clarkson(2010)]{clarkson2010coresets}
K.~L. Clarkson.
\newblock Coresets, sparse greedy approximation, and the frank-wolfe algorithm.
\newblock \emph{ACM Transactions on Algorithms (TALG)}, 6\penalty0
  (4):\penalty0 1--30, 2010.

\bibitem[Cox et~al.(2017)Cox, Juditsky, and Nemirovski]{cox2017decomposition}
B.~Cox, A.~Juditsky, and A.~Nemirovski.
\newblock Decomposition techniques for bilinear saddle point problems and
  variational inequalities with affine monotone operators.
\newblock \emph{Journal of Optimization Theory and Applications}, 172\penalty0
  (2):\penalty0 402--435, 2017.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem[Devolder et~al.(2014)Devolder, Glineur, and
  Nesterov]{devolder2014first}
O.~Devolder, F.~Glineur, and Y.~Nesterov.
\newblock First-order methods of smooth convex optimization with inexact
  oracle.
\newblock \emph{Mathematical Programming}, 146\penalty0 (1-2):\penalty0 37--75,
  2014.

\bibitem[Duchi et~al.(2008)Duchi, Shalev-Shwartz, Singer, and
  Chandra]{duchi2008efficient}
J.~Duchi, S.~Shalev-Shwartz, Y.~Singer, and T.~Chandra.
\newblock Efficient projections onto the l 1-ball for learning in high
  dimensions.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pages 272--279, 2008.

\bibitem[Duchi et~al.(2012)Duchi, Bartlett, and
  Wainwright]{duchi2012randomized}
J.~C. Duchi, P.~L. Bartlett, and M.~J. Wainwright.
\newblock Randomized smoothing for stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0
  674--701, 2012.

\bibitem[Frank and Wolfe(1956)]{frank1956algorithm}
M.~Frank and P.~Wolfe.
\newblock An algorithm for quadratic programming.
\newblock \emph{Naval research logistics quarterly}, 3\penalty0 (1-2):\penalty0
  95--110, 1956.

\bibitem[Freund and Grigas(2016)]{freund2016new}
R.~M. Freund and P.~Grigas.
\newblock New analysis and results for the frank--wolfe method.
\newblock \emph{Mathematical Programming}, 155\penalty0 (1-2):\penalty0
  199--230, 2016.

\bibitem[Garber and Hazan(2013)]{garber2013linearly}
D.~Garber and E.~Hazan.
\newblock A linearly convergent conditional gradient algorithm with
  applications to online and stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1301.4666}, 2013.

\bibitem[Garber and Hazan(2015)]{garber2015faster}
D.~Garber and E.~Hazan.
\newblock Faster rates for the frank-wolfe method over strongly-convex sets.
\newblock In \emph{32nd International Conference on Machine Learning, ICML
  2015}, 2015.

\bibitem[Gidel et~al.(2017)Gidel, Jebara, and Lacoste-Julien]{gidel2017frank}
G.~Gidel, T.~Jebara, and S.~Lacoste-Julien.
\newblock Frank-wolfe algorithms for saddle point problems.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 362--371.
  PMLR, 2017.

\bibitem[Gidel et~al.(2018)Gidel, Pedregosa, and
  Lacoste-Julien]{gidel2018frank}
G.~Gidel, F.~Pedregosa, and S.~Lacoste-Julien.
\newblock Frank-wolfe splitting via augmented lagrangian method.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1456--1465, 2018.

\bibitem[Goldstein(1964)]{goldstein1964convex}
A.~A. Goldstein.
\newblock Convex programming in hilbert space.
\newblock \emph{Bulletin of the American Mathematical Society}, 70\penalty0
  (5):\penalty0 709--710, 1964.

\bibitem[Hammond(1984)]{hammond1984solving}
J.~H. Hammond.
\newblock \emph{Solving asymmetric variational inequality problems and systems
  of equations with generalized nonlinear programming algorithms}.
\newblock PhD thesis, Massachusetts Institute of Technology, 1984.

\bibitem[Harchaoui et~al.(2015)Harchaoui, Juditsky, and
  Nemirovski]{harchaoui2015conditional}
Z.~Harchaoui, A.~Juditsky, and A.~Nemirovski.
\newblock Conditional gradient algorithms for norm-regularized smooth convex
  optimization.
\newblock \emph{Mathematical Programming}, 152\penalty0 (1-2):\penalty0
  75--112, 2015.

\bibitem[Hassani et~al.(2019)Hassani, Karbasi, Mokhtari, and
  Shen]{hassani2019stochastic}
H.~Hassani, A.~Karbasi, A.~Mokhtari, and Z.~Shen.
\newblock Stochastic conditional gradient++: (non-)convex minimization and
  continuous submodular maximization.
\newblock \emph{arXiv preprint arXiv:1902.06992}, 2019.

\bibitem[Hazan and Kale(2012)]{hazan2012projection}
E.~Hazan and S.~Kale.
\newblock Projection-free online learning.
\newblock In \emph{Proceedings of the 29th International Coference on
  International Conference on Machine Learning}, pages 1843--1850, 2012.

\bibitem[Hazan and Luo(2016)]{hazan2016variance}
E.~Hazan and H.~Luo.
\newblock Variance-reduced and projection-free stochastic optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  1263--1271, 2016.

\bibitem[Hazan and Minasyan(2020)]{hazan2020faster}
E.~Hazan and E.~Minasyan.
\newblock Faster projection-free online learning.
\newblock \emph{arXiv preprint arXiv:2001.11568}, 2020.

\bibitem[He and Harchaoui(2015{\natexlab{a}})]{he2015semi}
N.~He and Z.~Harchaoui.
\newblock Semi-proximal mirror-prox for nonsmooth composite minimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3411--3419, 2015{\natexlab{a}}.

\bibitem[He and Harchaoui(2015{\natexlab{b}})]{he2015stochastic}
N.~He and Z.~Harchaoui.
\newblock Stochastic semi-proximal mirror-prox.
\newblock Workshop on Optimization for Machine Learning, 2015{\natexlab{b}}.
\newblock URL \url{https://opt-ml.org/papers/OPT2015_paper_27.pdf}.

\bibitem[Howard(2019)]{How2019Imagenette}
J.~Howard.
\newblock Imagenette, 2019.
\newblock URL \url{https://github.com/fastai/imagenette}.
\newblock Github repository with links to dataset.

\bibitem[Huber(1996)]{huber1996robust}
P.~J. Huber.
\newblock \emph{Robust statistical procedures}, volume~68.
\newblock SIAM, 1996.

\bibitem[Jaggi(2013)]{jaggi2013revisiting}
M.~Jaggi.
\newblock Revisiting frank-wolfe: Projection-free sparse convex optimization.
\newblock In \emph{Proceedings of the 30th international conference on machine
  learning}, pages 427--435, 2013.

\bibitem[Jain et~al.(2018)Jain, Thakkar, and Thakurta]{jain2018differentially}
P.~Jain, O.~D. Thakkar, and A.~Thakurta.
\newblock Differentially private matrix completion revisited.
\newblock In \emph{International Conference on Machine Learning}, pages
  2215--2224. PMLR, 2018.

\bibitem[Kulis et~al.(2009)Kulis, Sustik, and Dhillon]{kulis2009low}
B.~Kulis, M.~A. Sustik, and I.~S. Dhillon.
\newblock Low-rank kernel learning with bregman matrix divergences.
\newblock \emph{Journal of Machine Learning Research}, 10\penalty0
  (Feb):\penalty0 341--376, 2009.

\bibitem[Kundu et~al.(2018)Kundu, Bach, and Bhattacharya]{kundu2018convex}
A.~Kundu, F.~Bach, and C.~Bhattacharya.
\newblock Convex optimization over intersection of simple sets: improved
  convergence rate guarantees via an exact penalty approach.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 958--967. PMLR, 2018.

\bibitem[Lacoste-Julien(2016)]{lacoste2016convergence}
S.~Lacoste-Julien.
\newblock Convergence rate of frank-wolfe for non-convex objectives.
\newblock \emph{arXiv preprint arXiv:1607.00345}, 2016.

\bibitem[Lacoste-Julien et~al.(2012)Lacoste-Julien, Schmidt, and
  Bach]{lacoste2012simpler}
S.~Lacoste-Julien, M.~Schmidt, and F.~Bach.
\newblock A simpler approach to obtaining an {$O(1/t)$} convergence rate for
  the projected stochastic subgradient method.
\newblock \emph{arXiv preprint arXiv:1212.2002}, 2012.

\bibitem[Lacoste-Julien et~al.(2013)Lacoste-Julien, Jaggi, Schmidt, and
  Pletscher]{lacoste2013block}
S.~Lacoste-Julien, M.~Jaggi, M.~Schmidt, and P.~Pletscher.
\newblock Block-coordinate frank-wolfe optimization for structural svms.
\newblock In \emph{Proceedings of the 30th international conference on machine
  learning}, pages 53--61, 2013.

\bibitem[Lafond et~al.(2015)Lafond, Wai, and Moulines]{lafond2015online}
J.~Lafond, H.-T. Wai, and E.~Moulines.
\newblock On the online frank-wolfe algorithms for convex and non-convex
  optimizations.
\newblock \emph{arXiv preprint arXiv:1510.01171}, 2015.

\bibitem[Lan(2012)]{lan2012optimal}
G.~Lan.
\newblock An optimal method for stochastic composite optimization.
\newblock \emph{Mathematical Programming}, 133\penalty0 (1-2):\penalty0
  365--397, 2012.

\bibitem[Lan(2013)]{lan2013complexity}
G.~Lan.
\newblock The complexity of large-scale convex programming under a linear
  optimization oracle.
\newblock \emph{arXiv preprint arXiv:1309.5550}, 2013.

\bibitem[Lan(2016)]{lan2016gradient}
G.~Lan.
\newblock Gradient sliding for composite optimization.
\newblock \emph{Mathematical Programming}, 159\penalty0 (1-2):\penalty0
  201--235, 2016.

\bibitem[Lan and Zhou(2016)]{lan2016conditional}
G.~Lan and Y.~Zhou.
\newblock Conditional gradient sliding for convex optimization.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (2):\penalty0
  1379--1409, 2016.

\bibitem[Lan et~al.(2011)Lan, Lu, and Monteiro]{lan2011primal}
G.~Lan, Z.~Lu, and R.~D. Monteiro.
\newblock Primal-dual first-order methods with ${O}(1/\varepsilon)$
  iteration-complexity for cone programming.
\newblock \emph{Mathematical Programming}, 126\penalty0 (1):\penalty0 1--29,
  2011.

\bibitem[Lan et~al.(2017)Lan, Pokutta, Zhou, and Zink]{lan2017conditional}
G.~Lan, S.~Pokutta, Y.~Zhou, and D.~Zink.
\newblock Conditional accelerated lazy stochastic gradient descent.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1965--1974, 2017.

\bibitem[Levitin and Polyak(1966)]{levitin1966constrained}
E.~S. Levitin and B.~T. Polyak.
\newblock Constrained minimization methods.
\newblock \emph{USSR Computational mathematics and mathematical physics},
  6\penalty0 (5):\penalty0 1--50, 1966.

\bibitem[Locatello et~al.(2019)Locatello, Yurtsever, Fercoq, and
  Cevher]{locatello2019stochastic}
F.~Locatello, A.~Yurtsever, O.~Fercoq, and V.~Cevher.
\newblock Stochastic frank-wolfe for composite convex minimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  14246--14256, 2019.

\bibitem[Mahdavi et~al.(2012)Mahdavi, Yang, Jin, Zhu, and
  Yi]{mahdavi2012stochastic}
M.~Mahdavi, T.~Yang, R.~Jin, S.~Zhu, and J.~Yi.
\newblock Stochastic gradient descent with only one projection.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  494--502, 2012.

\bibitem[Moreau(1962)]{moreau1962functions}
J.~J. Moreau.
\newblock Functions convexes duales et points proximaux dans un espace
  hilbertien.
\newblock \emph{CR Acad. Sci. Paris Ser. A Math.}, 255:\penalty0 2897--2899,
  1962.

\bibitem[Moreau(1965)]{moreau1965proximite}
J.-J. Moreau.
\newblock Proximit{\'e} et dualit{\'e} dans un espace hilbertien.
\newblock \emph{Bulletin de la Soci{\'e}t{\'e} math{\'e}matique de France},
  93:\penalty0 273--299, 1965.

\bibitem[Nemirovski and Yudin(1983)]{nemirovsky1983problem}
A.~S. Nemirovski and D.~B. Yudin.
\newblock \emph{Problem complexity and method efficiency in optimization.}
\newblock Wiley-Interscience, 1 edition, 1983.

\bibitem[Nesterov(1998)]{nesterov1998introductory}
Y.~Nesterov.
\newblock \emph{Introductory lectures on convex programming volume {I}: Basic
  course}.
\newblock Lecture notes, 1998.

\bibitem[Nesterov(2005)]{nesterov2005smooth}
Y.~Nesterov.
\newblock Smooth minimization of non-smooth functions.
\newblock \emph{Mathematical programming}, 103\penalty0 (1):\penalty0 127--152,
  2005.

\bibitem[Nesterov(2013)]{nesterov2013gradient}
Y.~Nesterov.
\newblock Gradient methods for minimizing composite functions.
\newblock \emph{Mathematical Programming}, 140\penalty0 (1):\penalty0 125--161,
  2013.

\bibitem[Nesterov(2018)]{nesterov2018complexity}
Y.~Nesterov.
\newblock Complexity bounds for primal-dual methods minimizing the model of
  objective function.
\newblock \emph{Mathematical Programming}, 171\penalty0 (1-2):\penalty0
  311--330, 2018.

\bibitem[Nesterov(1983)]{nesterov1983method}
Y.~E. Nesterov.
\newblock A method for solving the convex programming problem with convergence
  rate ${O} (1/k^2)$.
\newblock In \emph{Dokl. akad. nauk Sssr}, volume 269, pages 543--547, 1983.

\bibitem[Nguyen(2014)]{nguyen2014efficient}
Q.~Nguyen.
\newblock \emph{Efficient learning with soft label information and multiple
  annotators}.
\newblock PhD thesis, University of Pittsburgh, 2014.

\bibitem[Palaniappan and Bach(2016)]{palaniappan2016stochastic}
B.~Palaniappan and F.~Bach.
\newblock Stochastic variance reduction methods for saddle-point problems.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1416--1424, 2016.

\bibitem[Pierucci et~al.(2014)Pierucci, Harchaoui, and
  Malick]{pierucci2014smoothing}
F.~Pierucci, Z.~Harchaoui, and J.~Malick.
\newblock A smoothing approach for composite conditional gradient with
  nonsmooth loss.
\newblock Technical report, [Research Report] RR-8662, INRIA Grenoble, 2014.

\bibitem[Ravi et~al.(2019)Ravi, Collins, and Singh]{ravi2019deterministic}
S.~N. Ravi, M.~D. Collins, and V.~Singh.
\newblock A deterministic nonsmooth frank wolfe algorithm with coreset
  guarantees.
\newblock \emph{Informs Journal on Optimization}, 1\penalty0 (2):\penalty0
  120--142, 2019.

\bibitem[Razzak(2019)]{razzak2019sparse}
M.~I. Razzak.
\newblock \emph{Sparse support matrix machines for the classification of
  corrupted data}.
\newblock PhD thesis, Queensland University of Technology, 2019.

\bibitem[Reddi et~al.(2016)Reddi, Sra, P{\'o}czos, and
  Smola]{reddi2016stochasticfw}
S.~J. Reddi, S.~Sra, B.~P{\'o}czos, and A.~Smola.
\newblock Stochastic frank-wolfe methods for nonconvex optimization.
\newblock In \emph{2016 54th Annual Allerton Conference on Communication,
  Control, and Computing (Allerton)}, pages 1244--1251. IEEE, 2016.

\bibitem[Sahu et~al.(2019)Sahu, Zaheer, and Kar]{sahu2019towards}
A.~K. Sahu, M.~Zaheer, and S.~Kar.
\newblock Towards gradient free and projection free stochastic optimization.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 3468--3477, 2019.

\bibitem[Schmidt et~al.(2011)Schmidt, Roux, and Bach]{schmidt2011convergence}
M.~Schmidt, N.~L. Roux, and F.~R. Bach.
\newblock Convergence rates of inexact proximal-gradient methods for convex
  optimization.
\newblock In \emph{Advances in neural information processing systems}, pages
  1458--1466, 2011.

\bibitem[Shamir and Zhang(2013)]{shamir2013stochastic}
O.~Shamir and T.~Zhang.
\newblock Stochastic gradient descent for non-smooth optimization: Convergence
  results and optimal averaging schemes.
\newblock In \emph{International conference on machine learning}, pages 71--79,
  2013.

\bibitem[Srebro et~al.(2005)Srebro, Rennie, and Jaakkola]{srebro2005maximum}
N.~Srebro, J.~Rennie, and T.~S. Jaakkola.
\newblock Maximum-margin matrix factorization.
\newblock In \emph{Advances in neural information processing systems}, pages
  1329--1336, 2005.

\bibitem[Thekumparampil et~al.(2019)Thekumparampil, Jain, Netrapalli, and
  Oh]{thekumparampil2019efficient}
K.~K. Thekumparampil, P.~Jain, P.~Netrapalli, and S.~Oh.
\newblock Efficient algorithms for smooth minimax optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  12659--12670, 2019.

\bibitem[Tseng(2008)]{tseng2008accelerated}
P.~Tseng.
\newblock Accelerated proximal gradient methods for convex optimization.
\newblock Technical report, University of Washington, Seattle, 2008.
\newblock URL \url{https://www.mit.edu/~dimitrib/PTseng/papers/apgm.pdf}.

\bibitem[Vinter and Zheng(2003)]{Vinter-Zheng}
R.~Vinter and H.~Zheng.
\newblock Some finance problems solved with nonsmooth optimization techniques.
\newblock \emph{Journal of optimization theory and applications}, 119\penalty0
  (1):\penalty0 1--18, 2003.

\bibitem[Wang et~al.(2013)Wang, He, Gao, and Xue]{wang2013efficient}
Z.~Wang, X.~He, D.~Gao, and X.~Xue.
\newblock An efficient kernel-based matrixized least squares support vector
  machine.
\newblock \emph{Neural Computing and Applications}, 22\penalty0 (1):\penalty0
  143--150, 2013.

\bibitem[White(1993)]{white1993extension}
D.~White.
\newblock Extension of the frank-wolfe algorithm to concave nondifferentiable
  objective functions.
\newblock \emph{Journal of optimization theory and applications}, 78\penalty0
  (2):\penalty0 283--301, 1993.

\bibitem[Wolf et~al.(2007)Wolf, Jhuang, and Hazan]{wolf2007modeling}
L.~Wolf, H.~Jhuang, and T.~Hazan.
\newblock Modeling appearances with low-rank svm.
\newblock In \emph{2007 IEEE Conference on Computer Vision and Pattern
  Recognition}, pages 1--6. IEEE, 2007.

\bibitem[Xie et~al.(2020)Xie, Shen, Zhang, Wang, and Qian]{xie2020efficient}
J.~Xie, Z.~Shen, C.~Zhang, B.~Wang, and H.~Qian.
\newblock Efficient projection-free online methods with stochastic recursive
  gradient.
\newblock In \emph{AAAI}, pages 6446--6453, 2020.

\bibitem[Yang and Lin(2018)]{yang2018rsg}
T.~Yang and Q.~Lin.
\newblock {RSG}: Beating subgradient method without smoothness and strong
  convexity.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 236--268, 2018.

\bibitem[Yang et~al.(2017)Yang, Lin, and Zhang]{yang2017richer}
T.~Yang, Q.~Lin, and L.~Zhang.
\newblock A richer theory of convex constrained optimization with reduced
  projections and improved rates.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 3901--3910. JMLR. org, 2017.

\bibitem[Yen et~al.(2016)Yen, Lin, Zhang, Ravikumar, and
  Dhillon]{yen2016convex}
I.~E.-H. Yen, X.~Lin, J.~Zhang, P.~Ravikumar, and I.~Dhillon.
\newblock A convex atomic-norm approach to multiple sequence alignment and
  motif discovery.
\newblock In \emph{International Conference on Machine Learning}, pages
  2272--2280, 2016.

\bibitem[Yosida(1965)]{yosida2012functional}
K.~Yosida.
\newblock \emph{Functional analysis}.
\newblock Springer Verlag, 1965.

\bibitem[Zhang et~al.(2013)Zhang, Yang, Jin, and He]{zhang2013logt}
L.~Zhang, T.~Yang, R.~Jin, and X.~He.
\newblock {$O(\log t)$} projections for stochastic optimization of smooth and
  strongly convex functions.
\newblock In \emph{International Conference on Machine Learning}, pages
  1121--1129, 2013.

\bibitem[Zhang(2003)]{zhang2003sequential}
T.~Zhang.
\newblock Sequential greedy approximation for certain convex optimization
  problems.
\newblock \emph{IEEE Transactions on Information Theory}, 49\penalty0
  (3):\penalty0 682--691, 2003.

\bibitem[Zhu et~al.(2004)Zhu, Rosset, Tibshirani, and Hastie]{zhu20041}
J.~Zhu, S.~Rosset, R.~Tibshirani, and T.~J. Hastie.
\newblock $1$-norm support vector machines.
\newblock In \emph{Advances in neural information processing systems}, pages
  49--56, 2004.

\end{thebibliography}
