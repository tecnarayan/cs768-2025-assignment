\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{vdGBRD14}

\bibitem[AGH{\etalchar{+}}14]{anandkumar2014tensor}
A.~Anandkumar, R.~Ge, D.~Hsu, S.~M. Kakade, and M.~Telgarsky.
\newblock Tensor decompositions for learning latent variable models.
\newblock {\em The Journal of Machine Learning Research}, 15:2773--2832, 2014.

\bibitem[Ben05]{bentkus2005lyapunov}
V.~Bentkus.
\newblock A {L}yapunov-type bound in {$R^d$}.
\newblock {\em Theory of Probability \& Its Applications}, 49(2):311--323,
  2005.

\bibitem[BM03]{burer2003nonlinear}
S.~Burer and R.~D. Monteiro.
\newblock A nonlinear programming algorithm for solving semidefinite programs
  via low-rank factorization.
\newblock {\em Mathematical Programming}, 95(2):329--357, 2003.

\bibitem[BM16]{barak2016noisy}
B.~Barak and A.~Moitra.
\newblock Noisy tensor completion via the sum-of-squares hierarchy.
\newblock In {\em Proceedings of the Conference on Learning Theory}, pages
  417--445, 2016.

\bibitem[CC17]{ChenCandes15solving}
Y.~Chen and E.~J. Cand\`es.
\newblock Solving random quadratic systems of equations is nearly as easy as
  solving linear systems.
\newblock {\em Communications on Pure and Applied Mathematics}, 70(5):822--883,
  2017.

\bibitem[CC18]{chen2016projected}
Y.~Chen and E.~Cand\`es.
\newblock The projected power method: An efficient algorithm for joint
  alignment from pairwise differences.
\newblock {\em Communications on Pure and Applied Mathematics},
  71(8):1648--1714, 2018.

\bibitem[CCF{\etalchar{+}}19]{chen2019noisy}
Y.~Chen, Y.~Chi, J.~Fan, C.~Ma, and Y.~Yan.
\newblock Noisy matrix completion: Understanding statistical guarantees for
  convex relaxation via nonconvex optimization.
\newblock {\em arXiv:1902.07698}, 2019.

\bibitem[CCFM19]{chen2018gradient}
Y.~Chen, Y.~Chi, J.~Fan, and C.~Ma.
\newblock Gradient descent with random initialization: Fast global convergence
  for nonconvex phase retrieval.
\newblock {\em Mathematical Programming}, 176(1-2):5--37, July 2019.

\bibitem[CFMY19]{chen2019inference}
Y.~Chen, J.~Fan, C.~Ma, and Y.~Yan.
\newblock Inference and uncertainty quantification for noisy matrix completion.
\newblock {\em Proceedings of the National Academy of Sciences of the U.S.A.},
  116(46):22931--22937, 2019.

\bibitem[CFMY20]{chen2020bridging}
Y.~Chen, J.~Fan, C.~Ma, and Y.~Yan.
\newblock Bridging convex and nonconvex optimization in robust {PCA}: Noise,
  outliers, and missing data.
\newblock {\em arXiv preprint arXiv:2001.05484}, 2020.

\bibitem[CG17]{cai2017confidence}
T.~T. Cai and Z.~Guo.
\newblock Confidence intervals for high-dimensional linear regression: Minimax
  rates and adaptivity.
\newblock {\em The Annals of statistics}, 45(2):615--646, 2017.

\bibitem[CL17]{chen2017memory}
J.~Chen and X.~Li.
\newblock Memory-efficient kernel {PCA} via partial matrix sampling and
  nonconvex optimization: a model-free analysis of local minima.
\newblock {\em arXiv:1711.01742}, 2017.

\bibitem[CLC19]{chi2018nonconvex}
Y.~Chi, Y.~M. Lu, and Y.~Chen.
\newblock Nonconvex optimization meets low-rank matrix factorization: An
  overview.
\newblock {\em IEEE Transactions on Signal Processing}, 67(20):5239 -- 5269,
  October 2019.

\bibitem[CLC{\etalchar{+}}20]{cai2019subspace}
C.~Cai, G.~Li, Y.~Chi, H.~V. Poor, and Y.~Chen.
\newblock Subspace estimation from unbalanced and incomplete data matrices:
  $\ell_{2,\infty}$ statistical guarantees.
\newblock {\em The Annals of Statistics, {\em to appear}}, 2020.

\bibitem[CLL19]{chen2019nonconvex}
J.~Chen, D.~Liu, and X.~Li.
\newblock Nonconvex rectangular matrix completion via gradient descent without
  $\ell_{2,\infty}$ regularization.
\newblock {\em arXiv preprint arXiv:1901.06116}, 2019.

\bibitem[CLPC19]{cai2019nonconvex}
C.~Cai, G.~Li, H.~V. Poor, and Y.~Chen.
\newblock Nonconvex low-rank tensor completion from noisy data.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1861--1872, 2019.

\bibitem[CLR16]{cai2016geometric}
T.~T. Cai, T.~Liang, and A.~Rakhlin.
\newblock Geometric inference for general high-dimensional linear inverse
  problems.
\newblock {\em The Annals of Statistics}, 44(4):1536--1563, 2016.

\bibitem[CLS15]{candes2014wirtinger}
E.~Cand\`es, X.~Li, and M.~Soltanolkotabi.
\newblock Phase retrieval via {Wirtinger} flow: Theory and algorithms.
\newblock {\em IEEE Transactions on Information Theory}, 61(4):1985--2007,
  April 2015.

\bibitem[CLW17]{cai2017fast}
J.-F. Cai, H.~Liu, and Y.~Wang.
\newblock Fast rank one alternating minimization algorithm for phase retrieval.
\newblock {\em arXiv preprint arXiv:1708.08751}, 2017.

\bibitem[CP10]{CanPla10}
E.~Cand\`es and Y.~Plan.
\newblock Matrix completion with noise.
\newblock {\em Proceedings of the IEEE}, 98(6):925 --936, June 2010.

\bibitem[CR09]{candes2009exact}
E.~J. Cand{\`e}s and B.~Recht.
\newblock Exact matrix completion via convex optimization.
\newblock {\em Foundations of Computational Mathematics}, 9(6):717, 2009.

\bibitem[CW15]{chen2015fast}
Y.~Chen and M.~J. Wainwright.
\newblock Fast low-rank estimation by projected gradient descent: General
  statistical and algorithmic guarantees.
\newblock {\em arXiv:1509.03025}, 2015.

\bibitem[CWC20]{cheng2020inference}
C.~Cheng, Y.~Wei, and Y.~Chen.
\newblock Tackling small eigen-gaps: Fine-grained eigenvector estimation and
  inference under heteroscedastic noise.
\newblock {\em arXiv preprint arXiv:2001.04620}, 2020.

\bibitem[Gro11]{Gross2011recovering}
D.~Gross.
\newblock Recovering low-rank matrices from few coefficients in any basis.
\newblock {\em IEEE Transactions on Information Theory}, 57(3):1548--1566,
  March 2011.

\bibitem[GRY11]{gandy2011tensor}
S.~Gandy, B.~Recht, and I.~Yamada.
\newblock Tensor completion and low-n-rank tensor recovery via convex
  optimization.
\newblock {\em Inverse Problems}, 27(2):025010, 2011.

\bibitem[HKZ12]{hsu2012tail}
D.~Hsu, S.~M. Kakade, and T.~Zhang.
\newblock A tail inequality for quadratic forms of sub-{G}aussian random
  vectors.
\newblock {\em Electronic Communications in Probability}, 17(52), 2012.

\bibitem[HMGW15]{huang2015provable}
B.~Huang, C.~Mu, D.~Goldfarb, and J.~Wright.
\newblock Provable models for robust low-rank tensor completion.
\newblock {\em Pacific Journal of Optimization}, 11(2):339--364, 2015.

\bibitem[HWZ20]{han2020optimal}
R.~Han, R.~Willett, and A.~Zhang.
\newblock An optimal statistical and computational framework for generalized
  tensor estimation.
\newblock {\em arXiv preprint arXiv:2002.11255}, 2020.

\bibitem[HZC20]{hao2020sparse}
B.~Hao, A.~Zhang, and G.~Cheng.
\newblock Sparse and low-rank tensor estimation via cubic sketchings.
\newblock {\em IEEE Transactions on Information Theory}, 2020.

\bibitem[JHZ{\etalchar{+}}16]{ji2016tensor}
T.-Y. Ji, T.-Z. Huang, X.-L. Zhao, T.-H. Ma, and G.~Liu.
\newblock Tensor completion using total variation and low-rank matrix
  factorization.
\newblock {\em Information Sciences}, 326:243--257, 2016.

\bibitem[JM14]{javanmard2014confidence}
A.~Javanmard and A.~Montanari.
\newblock Confidence intervals and hypothesis testing for high-dimensional
  regression.
\newblock {\em The Journal of Machine Learning Research}, 15(1):2869--2909,
  2014.

\bibitem[JNS13]{jain2013low}
P.~Jain, P.~Netrapalli, and S.~Sanghavi.
\newblock Low-rank matrix completion using alternating minimization.
\newblock In {\em Proceedings of the Forty-fifth Annual ACM Symposium on Theory
  of Computing}, pages 665--674, 2013.

\bibitem[JO14]{jain2014provable}
P.~Jain and S.~Oh.
\newblock Provable tensor factorization with missing data.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1431--1439, 2014.

\bibitem[JvdG18]{jankova2018biased}
J.~Jankov{\'a} and S.~van~de Geer.
\newblock De-biased sparse pca: Inference and testing for eigenstructure of
  large covariance matrices.
\newblock {\em arXiv preprint arXiv:1801.10567}, 2018.

\bibitem[KB09]{kolda2009tensor}
T.~G. Kolda and B.~W. Bader.
\newblock Tensor decompositions and applications.
\newblock {\em SIAM Review}, 51(3):455--500, 2009.

\bibitem[KMO10a]{KesMonSew2010}
R.~H. Keshavan, A.~Montanari, and S.~Oh.
\newblock Matrix completion from a few entries.
\newblock {\em IEEE Transactions on Information Theory}, 56(6):2980 --2998,
  June 2010.

\bibitem[KMO10b]{Se2010Noisy}
R.~H. Keshavan, A.~Montanari, and S.~Oh.
\newblock Matrix completion from noisy entries.
\newblock {\em The Journal of Machine Learning Research}, 11:2057--2078, 2010.

\bibitem[Kol11]{Koltchinskii2011oracle}
V.~Koltchinskii.
\newblock {\em Oracle inequalities in empirical risk minimization and sparse
  recovery problems}, volume 2033 of {\em Lecture Notes in Mathematics}.
\newblock Springer, Heidelberg, 2011.

\bibitem[KSS13]{kreimer2013tensor}
N.~Kreimer, A.~Stanton, and M.~D. Sacchi.
\newblock Tensor completion based on nuclear norm minimization for {5D} seismic
  data reconstruction.
\newblock {\em Geophysics}, 78(6):V273--V284, 2013.

\bibitem[Li13]{li2011compressed}
X.~Li.
\newblock Compressed sensing and matrix completion with constant proportion of
  corruptions.
\newblock {\em Constructive Approximation}, 37:73--99, 2013.

\bibitem[LM20]{liu2020tensor}
A.~Liu and A.~Moitra.
\newblock Tensor completion made practical.
\newblock {\em arXiv preprint arXiv:2006.03134}, 2020.

\bibitem[LMWY13]{liu2013tensor}
J.~Liu, P.~Musialski, P.~Wonka, and J.~Ye.
\newblock Tensor completion for estimating missing values in visual data.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  35(1):208--220, 2013.

\bibitem[MHWG14]{mu2014square}
C.~Mu, B.~Huang, J.~Wright, and D.~Goldfarb.
\newblock Square deal: Lower bounds and improved relaxations for tensor
  recovery.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, pages 73--81, 2014.

\bibitem[MLL17]{ma2017inter}
C.~Ma, J.~Lu, and H.~Liu.
\newblock Inter-subject analysis: Inferring sparse interactions with dense
  intra-graphs.
\newblock {\em arXiv preprint arXiv:1709.07036}, 2017.

\bibitem[MM18]{miolane2018distribution}
L.~Miolane and A.~Montanari.
\newblock The distribution of the lasso: Uniform control over sparse balls and
  adaptive parameter tuning.
\newblock {\em arXiv preprint arXiv:1811.01212}, 2018.

\bibitem[MS18]{montanari2018spectral}
A.~Montanari and N.~Sun.
\newblock Spectral algorithms for tensor completion.
\newblock {\em Communications on Pure and Applied Mathematics},
  71(11):2381--2425, 2018.

\bibitem[MWCC19]{ma2017implicit}
C.~Ma, K.~Wang, Y.~Chi, and Y.~Chen.
\newblock Implicit regularization in nonconvex statistical estimation: Gradient
  descent converges linearly for phase retrieval, matrix completion, and blind
  deconvolution.
\newblock {\em Foundations of Computational Mathematics}, pages 1--182, 2019.

\bibitem[NL17]{ning2017general}
Y.~Ning and H.~Liu.
\newblock A general theory of hypothesis tests and confidence regions for
  sparse high dimensional models.
\newblock {\em The Annals of Statistics}, 45(1):158--195, 2017.

\bibitem[NNS{\etalchar{+}}14]{netrapalli2014non}
P.~Netrapalli, U.~Niranjan, S.~Sanghavi, A.~Anandkumar, and P.~Jain.
\newblock Non-convex robust {PCA}.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1107--1115, 2014.

\bibitem[PS17]{potechin2017exact}
A.~Potechin and D.~Steurer.
\newblock Exact tensor completion with sum-of-squares.
\newblock In {\em Proceedings of the Conference on Learning Theory}, pages
  1619--1673, 2017.

\bibitem[QZEW19]{qu2019convolutional}
Q.~Qu, Y.~Zhang, Y.~C. Eldar, and J.~Wright.
\newblock Convolutional phase retrieval via gradient descent.
\newblock {\em IEEE Transactions on Information Theory}, 2019.

\bibitem[RPP13]{romera2013new}
B.~Romera-Paredes and M.~Pontil.
\newblock A new convex relaxation for tensor completion.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2967--2975, 2013.

\bibitem[RSZZ15]{ren2015asymptotic}
Z.~Ren, T.~Sun, C.-H. Zhang, and H.~H. Zhou.
\newblock Asymptotic normality and optimalities in estimation of large
  {G}aussian graphical models.
\newblock {\em The Annals of Statistics}, 43(3):991--1026, 2015.

\bibitem[SCC19]{sur2017likelihood}
P.~Sur, Y.~Chen, and E.~J. Cand{\`e}s.
\newblock The likelihood ratio test in high-dimensional logistic regression is
  asymptotically a rescaled chi-square.
\newblock {\em Probability Theory and Related Fields}, 175:487--558, 2019.

\bibitem[Sch92]{MR1176461}
B.~A. Schmitt.
\newblock Perturbation bounds for matrix square roots and {P}ythagorean sums.
\newblock {\em Linear Algebra and its Applications}, 174:215--227, 1992.

\bibitem[SDLF{\etalchar{+}}17]{sidiropoulos2017tensor}
N.~D. Sidiropoulos, L.~De~Lathauwer, X.~Fu, K.~Huang, E.~E. Papalexakis, and
  C.~Faloutsos.
\newblock Tensor decomposition for signal processing and machine learning.
\newblock {\em IEEE Transactions on Signal Processing}, 65(13):3551--3582,
  2017.

\bibitem[Sha03]{shao2003mathematical}
J.~Shao.
\newblock {\em Mathematical Statistics}.
\newblock Springer Texts in Statistics. Springer, 2003.

\bibitem[SHKM14]{semerci2014tensor}
O.~Semerci, N.~Hao, M.~E. Kilmer, and E.~L. Miller.
\newblock Tensor-based formulation and nuclear norm regularization for
  multienergy computed tomography.
\newblock {\em IEEE Transactions on Image Processing}, 23(4):1678--1693, 2014.

\bibitem[SQW18]{sun2018geometric}
J.~Sun, Q.~Qu, and J.~Wright.
\newblock A geometric analysis of phase retrieval.
\newblock {\em Foundations of Computational Mathematics}, 18(5):1131--1198,
  2018.

\bibitem[Sre04]{srebro2004learning}
N.~Srebro.
\newblock {\em Learning with matrix factorizations}.
\newblock PhD thesis, Massachusetts Institute of Technology, 2004.

\bibitem[THK10]{tomioka2010estimation}
R.~Tomioka, K.~Hayashi, and H.~Kashima.
\newblock Estimation of low-rank tensors via convex optimization.
\newblock {\em arXiv preprint arXiv:1010.0789}, 2010.

\bibitem[TMC20]{tong2020accelerating}
T.~Tong, C.~Ma, and Y.~Chi.
\newblock Accelerating ill-conditioned low-rank matrix estimation via scaled
  gradient descent.
\newblock {\em arXiv preprint arXiv:2005.08898}, 2020.

\bibitem[vdGBRD14]{van2014asymptotically}
S.~van~de Geer, P.~B{\"u}hlmann, Y.~Ritov, and R.~Dezeure.
\newblock On asymptotically optimal confidence regions and tests for
  high-dimensional models.
\newblock {\em The Annals of Statistics}, 42(3):1166--1202, 2014.

\bibitem[WG16]{wang2016solving3}
G.~Wang and G.~Giannakis.
\newblock Solving random systems of quadratic equations via truncated
  generalized gradient flow.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  568--576, 2016.

\bibitem[Xia18]{xia2018confidence}
D.~Xia.
\newblock Confidence interval of singular vectors for high-dimensional and
  low-rank matrix regression.
\newblock {\em arXiv preprint arXiv:1805.09871}, 2018.

\bibitem[Xia19]{xia2019data}
D.~Xia.
\newblock Data-dependent confidence regions of singular subspaces.
\newblock {\em arXiv preprint arXiv:1901.00304}, 2019.

\bibitem[XY19a]{xia2017polynomial}
D.~Xia and M.~Yuan.
\newblock On polynomial time methods for exact low-rank tensor completion.
\newblock {\em Foundations of Computational Mathematics}, pages 1265--1313,
  2019.

\bibitem[XY19b]{xia2019statistical}
D.~Xia and M.~Yuan.
\newblock Statistical inferences of linear forms for noisy matrix completion.
\newblock {\em arXiv preprint arXiv:1909.00116}, 2019.

\bibitem[XYZ20]{xia2017statistically}
D.~Xia, M.~Yuan, and C.-H. Zhang.
\newblock Statistically optimal and computationally efficient low rank tensor
  completion from noisy entries.
\newblock {\em The Annals of Statistics, \em{to appear}}, 2020.

\bibitem[YZ16]{yuan2016tensor}
M.~Yuan and C.-H. Zhang.
\newblock On tensor completion via nuclear norm minimization.
\newblock {\em Foundations of Computational Mathematics}, 16(4):1031--1068,
  2016.

\bibitem[YZ17]{yuan2017incoherent}
M.~Yuan and C.-H. Zhang.
\newblock Incoherent tensor norms and their applications in higher order tensor
  completion.
\newblock {\em IEEE Transactions on Information Theory}, 63(10):6753--6766,
  2017.

\bibitem[ZA17]{zhang2017exact}
Z.~Zhang and S.~Aeron.
\newblock Exact tensor completion using t-svd.
\newblock {\em IEEE Transactions on Signal Processing}, 65(6):1511--1526, 2017.

\bibitem[ZCL16]{zhang2016provable}
H.~Zhang, Y.~Chi, and Y.~Liang.
\newblock Provable non-convex phase retrieval with outliers: Median truncated
  {W}irtinger flow.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, pages 1022--1031, 2016.

\bibitem[ZCW18]{zhang2018heteroskedastic}
A.~Zhang, T.~T. Cai, and Y.~Wu.
\newblock Heteroskedastic {PCA}: Algorithm, optimality, and applications.
\newblock {\em arXiv preprint arXiv:1810.08316}, 2018.

\bibitem[Zha19]{zhang2019cross}
A.~Zhang.
\newblock Cross: Efficient low-rank tensor completion.
\newblock {\em The Annals of Statistics}, 47(2):936--964, 2019.

\bibitem[ZZ14]{zhang2014confidence}
C.-H. Zhang and S.~S. Zhang.
\newblock Confidence intervals for low dimensional parameters in high
  dimensional linear models.
\newblock {\em Journal of the Royal Statistical Society: Series B},
  76(1):217--242, 2014.

\bibitem[ZZLC17]{zhang2017nonconvex}
H.~Zhang, Y.~Zhou, Y.~Liang, and Y.~Chi.
\newblock A nonconvex approach for phase retrieval: Reshaped wirtinger flow and
  incremental algorithms.
\newblock {\em The Journal of Machine Learning Research}, 18(1):5164--5198,
  2017.

\end{thebibliography}
