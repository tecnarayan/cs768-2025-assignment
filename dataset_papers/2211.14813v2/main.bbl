\begin{thebibliography}{70}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bain et~al.(2021)Bain, Nagrani, Varol, and Zisserman]{Bain2021Frozen}
Bain, M., Nagrani, A., Varol, G., and Zisserman, A.
\newblock Frozen in time: {A} joint video and image encoder for end-to-end
  retrieval.
\newblock In \emph{ICCV}, pp.\  1708--1718, 2021.

\bibitem[Bucher et~al.(2019)Bucher, Vu, Cord, and P{\'{e}}rez]{Bucher2019Zero}
Bucher, M., Vu, T., Cord, M., and P{\'{e}}rez, P.
\newblock Zero-shot semantic segmentation.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J{\'{e}}gou, Mairal,
  Bojanowski, and Joulin]{Caron2021Emerging}
Caron, M., Touvron, H., Misra, I., J{\'{e}}gou, H., Mairal, J., Bojanowski, P.,
  and Joulin, A.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In \emph{ICCV}, pp.\  9630--9640, 2021.

\bibitem[Changpinyo et~al.(2021)Changpinyo, Sharma, Ding, and
  Soricut]{Changpinyo2021CC12M}
Changpinyo, S., Sharma, P., Ding, N., and Soricut, R.
\newblock Conceptual 12m: Pushing web-scale image-text pre-training to
  recognize long-tail visual concepts.
\newblock In \emph{CVPR}, pp.\  3558--3568, 2021.

\bibitem[Chefer et~al.(2021)Chefer, Gur, and Wolf]{chefer2021transformer}
Chefer, H., Gur, S., and Wolf, L.
\newblock Transformer interpretability beyond attention visualization.
\newblock In \emph{CVPR}, pp.\  782--791, 2021.

\bibitem[Chen et~al.(2015)Chen, Papandreou, Kokkinos, Murphy, and
  Yuille]{Chen2015Semantic}
Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., and Yuille, A.~L.
\newblock Semantic image segmentation with deep convolutional nets and fully
  connected crfs.
\newblock In \emph{ICLR}, 2015.

\bibitem[Chen et~al.(2018)Chen, Zhu, Papandreou, Schroff, and
  Adam]{Deeplabv3plus2018}
Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., and Adam, H.
\newblock Encoder-decoder with atrous separable convolution for semantic image
  segmentation.
\newblock In \emph{ECCV}, 2018.

\bibitem[Chen et~al.(2021)Chen, Xie, and He]{Chen2021An}
Chen, X., Xie, S., and He, K.
\newblock An empirical study of training self-supervised vision transformers.
\newblock In \emph{ICCV}, pp.\  9620--9629, 2021.

\bibitem[Chen et~al.(2020)Chen, Li, Yu, Kholy, Ahmed, Gan, Cheng, and
  Liu]{Chen2020UNITER}
Chen, Y., Li, L., Yu, L., Kholy, A.~E., Ahmed, F., Gan, Z., Cheng, Y., and Liu,
  J.
\newblock {UNITER:} universal image-text representation learning.
\newblock In \emph{ECCV}, volume 12375, pp.\  104--120, 2020.

\bibitem[Cheng et~al.(2021)Cheng, Schwing, and Kirillov]{Cheng2021Per}
Cheng, B., Schwing, A.~G., and Kirillov, A.
\newblock Per-pixel classification is not all you need for semantic
  segmentation.
\newblock In \emph{NeurIPS}, pp.\  17864--17875, 2021.

\bibitem[Cheng et~al.(2022)Cheng, Misra, Schwing, Kirillov, and
  Girdhar]{Cheng2021mask2former}
Cheng, B., Misra, I., Schwing, A.~G., Kirillov, A., and Girdhar, R.
\newblock Masked-attention mask transformer for universal image segmentation.
\newblock In \emph{CVPR}, pp.\  1290--1299, 2022.

\bibitem[Cordts et~al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson,
  Franke, Roth, and Schiele]{Cordts2016cityscapes}
Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,
  Franke, U., Roth, S., and Schiele, B.
\newblock The cityscapes dataset for semantic urban scene understanding.
\newblock In \emph{CVPR}, pp.\  3213--3223, 2016.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{Devlin2019BERT}
Devlin, J., Chang, M., Lee, K., and Toutanova, K.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL-HLT}, pp.\  4171--4186, 2019.

\bibitem[Ding et~al.(2022{\natexlab{a}})Ding, Xue, Xia, and
  Dai]{Ding2022Decoupling}
Ding, J., Xue, N., Xia, G., and Dai, D.
\newblock Decoupling zero-shot semantic segmentation.
\newblock In \emph{CVPR}, pp.\  11573--11582, 2022{\natexlab{a}}.

\bibitem[Ding et~al.(2022{\natexlab{b}})Ding, Wang, and Tu]{Ding2022Open}
Ding, Z., Wang, J., and Tu, Z.
\newblock Open-vocabulary panoptic segmentation with maskclip.
\newblock \emph{arXiv preprint arXiv:2208.08984}, 2022{\natexlab{b}}.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{Dosovitskiy2021An}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{ICLR}, 2021.

\bibitem[Everingham et~al.(2010)Everingham, Gool, Williams, Winn, and
  Zisserman]{Everingham2010VOC}
Everingham, M., Gool, L.~V., Williams, C. K.~I., Winn, J.~M., and Zisserman, A.
\newblock The pascal visual object classes {(VOC)} challenge.
\newblock \emph{Int. J. Comput. Vis.}, 88\penalty0 (2):\penalty0 303--338,
  2010.

\bibitem[Felzenszwalb \& Huttenlocher(2004)Felzenszwalb and
  Huttenlocher]{felzenszwalb2004efficient}
Felzenszwalb, P.~F. and Huttenlocher, D.~P.
\newblock Efficient graph-based image segmentation.
\newblock \emph{International journal of computer vision}, 59\penalty0
  (2):\penalty0 167--181, 2004.

\bibitem[Gan et~al.(2022)Gan, Li, Li, Wang, Liu, and Gao]{Gan2022VLP}
Gan, Z., Li, L., Li, C., Wang, L., Liu, Z., and Gao, J.
\newblock Vision-language pre-training: Basics, recent advances, and future
  trends.
\newblock \emph{arXiv preprint arXiv:2210.09263}, 2022.

\bibitem[Ghiasi et~al.(2021)Ghiasi, Gu, Cui, and Lin]{Ghiasi2021Scaling}
Ghiasi, G., Gu, X., Cui, Y., and Lin, T.-Y.
\newblock Scaling open-vocabulary image segmentation with image-level labels.
\newblock \emph{arXiv:2112.12143}, 2021.

\bibitem[Gu et~al.(2022)Gu, Lin, Kuo, and Cui]{Gu2021Open}
Gu, X., Lin, T.-Y., Kuo, W., and Cui, Y.
\newblock Open-vocabulary object detection via vision and language knowledge
  distillation.
\newblock In \emph{ICLR}, 2022.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'{a}}r, and
  Girshick]{He2022Masked}
He, K., Chen, X., Xie, S., Li, Y., Doll{\'{a}}r, P., and Girshick, R.~B.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{CVPR}, pp.\  15979--15988, 2022.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016gaussian}
Hendrycks, D. and Gimpel, K.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Huang et~al.(2020)Huang, Zeng, Liu, Fu, and Fu]{Huang2020PixelBERT}
Huang, Z., Zeng, Z., Liu, B., Fu, D., and Fu, J.
\newblock Pixel-bert: Aligning image pixels with text by deep multi-modal
  transformers.
\newblock \emph{arXiv preprint arXiv:2004.00849}, 2020.

\bibitem[Jain et~al.(2022)Jain, Li, Chiu, Hassani, Orlov, and
  Shi]{Jain2022OneFormer}
Jain, J., Li, J., Chiu, M., Hassani, A., Orlov, N., and Shi, H.
\newblock Oneformer: One transformer to rule universal image segmentation.
\newblock \emph{arXiv preprint arXiv:abs/2211.06220}, 2022.

\bibitem[Jang et~al.(2017)Jang, Gu, and Poole]{Jang2017Gumbel}
Jang, E., Gu, S., and Poole, B.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock In \emph{ICLR}, 2017.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and
  Duerig]{Jia2021Scaling}
Jia, C., Yang, Y., Xia, Y., Chen, Y., Parekh, Z., Pham, H., Le, Q.~V., Sung,
  Y., Li, Z., and Duerig, T.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In \emph{ICML}, volume 139, pp.\  4904--4916, 2021.

\bibitem[Kim et~al.(2021)Kim, Son, and Kim]{Kim2021ViLT}
Kim, W., Son, B., and Kim, I.
\newblock Vilt: Vision-and-language transformer without convolution or region
  supervision.
\newblock In \emph{ICML}, volume 139, pp.\  5583--5594, 2021.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Weinberger, Belongie, Koltun, and
  Ranftl]{li2022languagedriven}
Li, B., Weinberger, K.~Q., Belongie, S., Koltun, V., and Ranftl, R.
\newblock Language-driven semantic segmentation.
\newblock In \emph{ICLR}, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Li, Xiong, and Hoi]{Li2022BLIP}
Li, J., Li, D., Xiong, C., and Hoi, S. C.~H.
\newblock {BLIP:} bootstrapping language-image pre-training for unified
  vision-language understanding and generation.
\newblock In \emph{ICML}, volume 162, pp.\  12888--12900, 2022{\natexlab{b}}.

\bibitem[Li et~al.(2022{\natexlab{c}})Li, Gan, Lin, Lin, Liu, Liu, and
  Wang]{Li2022LAVENDER}
Li, L., Gan, Z., Lin, K., Lin, C., Liu, Z., Liu, C., and Wang, L.
\newblock {LAVENDER:} unifying video-language understanding as masked language
  modeling.
\newblock \emph{arXiv preprint arXiv:2206.07160}, 2022{\natexlab{c}}.

\bibitem[Li et~al.(2022{\natexlab{d}})Li, Zhang, Zhang, Yang, Li, Zhong, Wang,
  Yuan, Zhang, Hwang, Chang, and Gao]{Li2022Grounded}
Li, L.~H., Zhang, P., Zhang, H., Yang, J., Li, C., Zhong, Y., Wang, L., Yuan,
  L., Zhang, L., Hwang, J., Chang, K., and Gao, J.
\newblock Grounded language-image pre-training.
\newblock In \emph{CVPR}, pp.\  10955--10965, 2022{\natexlab{d}}.

\bibitem[Li et~al.(2021)Li, Gao, Niu, Xiao, Liu, Liu, Wu, and
  Wang]{Li2021UNIMO}
Li, W., Gao, C., Niu, G., Xiao, X., Liu, H., Liu, J., Wu, H., and Wang, H.
\newblock {UNIMO:} towards unified-modal understanding and generation via
  cross-modal contrastive learning.
\newblock In \emph{ACL/IJCNLP}, pp.\  2592--2607, 2021.

\bibitem[Li et~al.(2022{\natexlab{e}})Li, Liang, Zhao, Cui, Ouyang, Shao, Yu,
  and Yan]{Li2022Supervision}
Li, Y., Liang, F., Zhao, L., Cui, Y., Ouyang, W., Shao, J., Yu, F., and Yan, J.
\newblock Supervision exists everywhere: {A} data efficient contrastive
  language-image pre-training paradigm.
\newblock In \emph{ICLR}, 2022{\natexlab{e}}.

\bibitem[Liang et~al.(2022)Liang, Wu, Dai, Li, Zhao, Zhang, Zhang, Vajda, and
  Marculescu]{Liang2022Open}
Liang, F., Wu, B., Dai, X., Li, K., Zhao, Y., Zhang, H., Zhang, P., Vajda, P.,
  and Marculescu, D.
\newblock Open-vocabulary semantic segmentation with mask-adapted {CLIP}.
\newblock \emph{arXiv preprint arXiv:abs/2210.04150}, 2022.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
  Doll{\'{a}}r, and Zitnick]{Lin2014COCO}
Lin, T., Maire, M., Belongie, S.~J., Hays, J., Perona, P., Ramanan, D.,
  Doll{\'{a}}r, P., and Zitnick, C.~L.
\newblock Microsoft {COCO:} common objects in context.
\newblock In \emph{ECCV}, volume 8693, pp.\  740--755, 2014.

\bibitem[Long et~al.(2015)Long, Shelhamer, and Darrell]{Long2015FCN}
Long, J., Shelhamer, E., and Darrell, T.
\newblock Fully convolutional networks for semantic segmentation.
\newblock In \emph{CVPR}, pp.\  3431--3440, 2015.

\bibitem[L{\"{u}}ddecke \& Ecker(2022)L{\"{u}}ddecke and
  Ecker]{Lueddecke2022Image}
L{\"{u}}ddecke, T. and Ecker, A.~S.
\newblock Image segmentation using text and image prompts.
\newblock In \emph{CVPR}, pp.\  7076--7086, 2022.

\bibitem[Luo et~al.(2020)Luo, Ji, Shi, Huang, Duan, Li, Chen, and
  Zhou]{Luo2020UniVL}
Luo, H., Ji, L., Shi, B., Huang, H., Duan, N., Li, T., Chen, X., and Zhou, M.
\newblock Univl: {A} unified video and language pre-training model for
  multimodal understanding and generation.
\newblock \emph{arXiv preprint arXiv:2002.06353}, 2020.

\bibitem[Ma et~al.(2022)Ma, Yang, Wang, Zhang, and Xie]{Ma2022Open}
Ma, C., Yang, Y., Wang, Y., Zhang, Y., and Xie, W.
\newblock Open-vocabulary semantic segmentation with frozen vision-language
  models.
\newblock \emph{arXiv preprint arXiv:abs/2210.15138}, 2022.

\bibitem[Miech et~al.(2019)Miech, Zhukov, Alayrac, Tapaswi, Laptev, and
  Sivic]{Miech2019HowTo100M}
Miech, A., Zhukov, D., Alayrac, J., Tapaswi, M., Laptev, I., and Sivic, J.
\newblock Howto100m: Learning a text-video embedding by watching hundred
  million narrated video clips.
\newblock In \emph{ICCV}, pp.\  2630--2640, 2019.

\bibitem[Mottaghi et~al.(2014)Mottaghi, Chen, Liu, Cho, Lee, Fidler, Urtasun,
  and Yuille]{Mottaghi2014Context}
Mottaghi, R., Chen, X., Liu, X., Cho, N., Lee, S., Fidler, S., Urtasun, R., and
  Yuille, A.~L.
\newblock The role of context for object detection and semantic segmentation in
  the wild.
\newblock In \emph{CVPR}, pp.\  891--898, 2014.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, Krueger, and Sutskever]{Radford2021Learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{ICML}, volume 139, pp.\  8748--8763, 2021.

\bibitem[Rao et~al.(2022)Rao, Zhao, Chen, Tang, Zhu, Huang, Zhou, and
  Lu]{Rao2022DenseCLIP}
Rao, Y., Zhao, W., Chen, G., Tang, Y., Zhu, Z., Huang, G., Zhou, J., and Lu, J.
\newblock Denseclip: Language-guided dense prediction with context-aware
  prompting.
\newblock In \emph{CVPR}, pp.\  18061--18070, 2022.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{Ronneberger2015UNet}
Ronneberger, O., Fischer, P., and Brox, T.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{MICCAI}, pp.\  234--241, 2015.

\bibitem[Schuhmann et~al.(2021)Schuhmann, Vencu, Beaumont, Kaczmarczyk, Mullis,
  Katta, Coombes, Jitsev, and Komatsuzaki]{Schuhmann2021LAION}
Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A.,
  Coombes, T., Jitsev, J., and Komatsuzaki, A.
\newblock {LAION-400M:} open dataset of clip-filtered 400 million image-text
  pairs.
\newblock \emph{arXiv preprint arXiv:2111.02114}, 2021.

\bibitem[Selvaraju et~al.(2017)Selvaraju, Cogswell, Das, Vedantam, Parikh, and
  Batra]{selvaraju2017grad}
Selvaraju, R.~R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra,
  D.
\newblock Grad-cam: Visual explanations from deep networks via gradient-based
  localization.
\newblock In \emph{ICCV}, pp.\  618--626, 2017.

\bibitem[Sharma et~al.(2018)Sharma, Ding, Goodman, and Soricut]{Sharma2018CC}
Sharma, P., Ding, N., Goodman, S., and Soricut, R.
\newblock Conceptual captions: {A} cleaned, hypernymed, image alt-text dataset
  for automatic image captioning.
\newblock In \emph{ACL}, pp.\  2556--2565, 2018.

\bibitem[Sun et~al.(2019)Sun, Myers, Vondrick, Murphy, and
  Schmid]{Sun2019VideoBERT}
Sun, C., Myers, A., Vondrick, C., Murphy, K., and Schmid, C.
\newblock Videobert: {A} joint model for video and language representation
  learning.
\newblock In \emph{ICCV}, pp.\  7463--7472, 2019.

\bibitem[Tan \& Bansal(2019)Tan and Bansal]{tan2019lxmert}
Tan, H. and Bansal, M.
\newblock {LXMERT:} learning cross-modality encoder representations from
  transformers.
\newblock In \emph{EMNLP}, 2019.

\bibitem[Thomee et~al.(2016)Thomee, Shamma, Friedland, Elizalde, Ni, Poland,
  Borth, and Li]{Thomee2016YFCC100M}
Thomee, B., Shamma, D.~A., Friedland, G., Elizalde, B., Ni, K., Poland, D.,
  Borth, D., and Li, L.
\newblock {YFCC100M:} the new data in multimedia research.
\newblock \emph{Commun. {ACM}}, 59\penalty0 (2):\penalty0 64--73, 2016.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and
  J{\'{e}}gou]{Touvron2021Training}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J{\'{e}}gou,
  H.
\newblock Training data-efficient image transformers {\&} distillation through
  attention.
\newblock In \emph{ICML}, volume 139, pp.\  10347--10357, 2021.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{Vaswani2017}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, pp.\  5998--6008, 2017.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Chen, Wu, Chen, Dai, Liu, Jiang,
  Zhou, and Yuan]{wang2022bevt}
Wang, R., Chen, D., Wu, Z., Chen, Y., Dai, X., Liu, M., Jiang, Y.-G., Zhou, L.,
  and Yuan, L.
\newblock Bevt: Bert pretraining of video transformers.
\newblock In \emph{CVPR}, pp.\  14733--14743, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Yu, Yu, Dai, Tsvetkov, and
  Cao]{Wang2022SimVLM}
Wang, Z., Yu, J., Yu, A.~W., Dai, Z., Tsvetkov, Y., and Cao, Y.
\newblock Simvlm: Simple visual language model pretraining with weak
  supervision.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022},
  2022{\natexlab{b}}.

\bibitem[Wen et~al.(2022)Wen, Zhao, Zheng, Zhang, and Qi]{Wen2022SlotCon}
Wen, X., Zhao, B., Zheng, A., Zhang, X., and Qi, X.
\newblock Self-supervised visual representation learning with semantic
  grouping.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Wu et~al.(2020)Wu, Lin, Cohen, Bui, and Maji]{Wu2020PhraseCut}
Wu, C., Lin, Z., Cohen, S., Bui, T., and Maji, S.
\newblock Phrasecut: Language-based image segmentation in the wild.
\newblock In \emph{CVPR}, pp.\  10213--10222, 2020.

\bibitem[Xian et~al.(2019)Xian, Choudhury, He, Schiele, and
  Akata]{Xian2019Semantic}
Xian, Y., Choudhury, S., He, Y., Schiele, B., and Akata, Z.
\newblock Semantic projection network for zero- and few-label semantic
  segmentation.
\newblock In \emph{CVPR}, pp.\  8256--8265, 2019.

\bibitem[Xie et~al.(2021)Xie, Wang, Yu, Anandkumar, Alvarez, and
  Luo]{Xie2021SegFormer}
Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J.~M., and Luo, P.
\newblock Segformer: Simple and efficient design for semantic segmentation with
  transformers.
\newblock In \emph{NeurIPS}, pp.\  12077--12090, 2021.

\bibitem[Xu et~al.(2022{\natexlab{a}})Xu, De~Mello, Liu, Byeon, Breuel, Kautz,
  and Wang]{Xu2022GroupViT}
Xu, J., De~Mello, S., Liu, S., Byeon, W., Breuel, T., Kautz, J., and Wang, X.
\newblock Groupvit: Semantic segmentation emerges from text supervision.
\newblock In \emph{CVPR}, pp.\  18134--18144, 2022{\natexlab{a}}.

\bibitem[Xu et~al.(2022{\natexlab{b}})Xu, Zhang, Wei, Lin, Cao, Hu, and
  Bai]{Xu2021A}
Xu, M., Zhang, Z., Wei, F., Lin, Y., Cao, Y., Hu, H., and Bai, X.
\newblock A simple baseline for open vocabulary semantic segmentation with
  pre-trained vision-language model.
\newblock \emph{ECCV}, 2022{\natexlab{b}}.

\bibitem[Yao et~al.(2022)Yao, Huang, Hou, Lu, Niu, Xu, Liang, Li, Jiang, and
  Xu]{Yao2022FILIP}
Yao, L., Huang, R., Hou, L., Lu, G., Niu, M., Xu, H., Liang, X., Li, Z., Jiang,
  X., and Xu, C.
\newblock {FILIP:} fine-grained interactive language-image pre-training.
\newblock In \emph{ICLR}, 2022.

\bibitem[Zabari \& Hoshen(2021)Zabari and Hoshen]{Zabari2021Semantic}
Zabari, N. and Hoshen, Y.
\newblock Semantic segmentation in-the-wild without seeing any segmentation
  examples.
\newblock \emph{arXiv:2112.03185}, 2021.

\bibitem[Zeng et~al.(2022)Zeng, Zhang, and Li]{Zeng2022Multi}
Zeng, Y., Zhang, X., and Li, H.
\newblock Multi-grained vision language pre-training: Aligning texts with
  visual concepts.
\newblock In \emph{ICML}, volume 162, pp.\  25994--26009, 2022.

\bibitem[Zhao et~al.(2017)Zhao, Shi, Qi, Wang, and Jia]{Zhao2017Pyramid}
Zhao, H., Shi, J., Qi, X., Wang, X., and Jia, J.
\newblock Pyramid scene parsing network.
\newblock In \emph{CVPR}, pp.\  6230--6239, 2017.

\bibitem[Zheng et~al.(2021)Zheng, Lu, Zhao, Zhu, Luo, Wang, Fu, Feng, Xiang,
  Torr, and Zhang]{Zheng2021SETR}
Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J.,
  Xiang, T., Torr, P.~H., and Zhang, L.
\newblock Rethinking semantic segmentation from a sequence-to-sequence
  perspective with transformers.
\newblock In \emph{CVPR}, 2021.

\bibitem[Zhong et~al.(2022)Zhong, Yang, Zhang, Li, Codella, Li, Zhou, Dai,
  Yuan, Li, and Gao]{Zhong2022RegionCLIP}
Zhong, Y., Yang, J., Zhang, P., Li, C., Codella, N., Li, L.~H., Zhou, L., Dai,
  X., Yuan, L., Li, Y., and Gao, J.
\newblock Regionclip: Region-based language-image pretraining.
\newblock In \emph{CVPR}, pp.\  16772--16782, 2022.

\bibitem[Zhou et~al.(2017)Zhou, Zhao, Puig, Fidler, Barriuso, and
  Torralba]{Zhou2017scene}
Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., and Torralba, A.
\newblock Scene parsing through ade20k dataset.
\newblock In \emph{CVPR}, pp.\  633--641, 2017.

\bibitem[Zhou et~al.(2022{\natexlab{a}})Zhou, Loy, and Dai]{zhou2022maskclip}
Zhou, C., Loy, C.~C., and Dai, B.
\newblock Extract free dense labels from clip.
\newblock In \emph{ECCV}, 2022{\natexlab{a}}.

\bibitem[Zhou et~al.(2022{\natexlab{b}})Zhou, Wei, Wang, Shen, Xie, Yuille, and
  Kong]{Zhou2022iBOT}
Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A.~L., and Kong, T.
\newblock {iBOT}: Image {BERT} pre-training with online tokenizer.
\newblock In \emph{ICLR}, 2022{\natexlab{b}}.

\end{thebibliography}
