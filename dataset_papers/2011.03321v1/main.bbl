\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI Blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Adiwardana et~al.(2020)Adiwardana, Luong, So, Hall, Fiedel, Thoppilan,
  Yang, Kulshreshtha, Nemade, Lu, et~al.]{adiwardana2020towards}
Daniel Adiwardana, Minh-Thang Luong, David~R So, Jamie Hall, Noah Fiedel, Romal
  Thoppilan, Zi~Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et~al.
\newblock Towards a human-like open-domain chatbot.
\newblock \emph{arXiv preprint arXiv:2001.09977}, 2020.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,
  and Dean]{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
  Geoffrey Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock \emph{arXiv preprint arXiv:1701.06538}, 2017.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016.

\bibitem[Belkin et~al.(2019{\natexlab{a}})Belkin, Hsu, Ma, and
  Mandal]{belkin2019reconciling}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (32):\penalty0 15849--15854, 2019{\natexlab{a}}.

\bibitem[Efron and Stein(1981)]{efron1981jackknife}
Bradley Efron and Charles Stein.
\newblock The jackknife estimate of variance.
\newblock \emph{The Annals of Statistics}, pages 586--596, 1981.

\bibitem[Stone(1994)]{stone1994use}
Charles~J Stone.
\newblock The use of polynomial splines and their tensor products in
  multivariate function estimation.
\newblock \emph{The Annals of Statistics}, pages 118--171, 1994.

\bibitem[Huang et~al.(1998)]{huang1998projection}
Jianhua~Z Huang et~al.
\newblock Projection estimation in multiple regression with application to
  functional anova models.
\newblock \emph{The annals of statistics}, 26\penalty0 (1):\penalty0 242--272,
  1998.

\bibitem[Owen(2003)]{owen2003dimension}
Art~B Owen.
\newblock The dimension distribution and quadrature test functions.
\newblock \emph{Statistica Sinica}, pages 1--17, 2003.

\bibitem[Geman et~al.(1992)Geman, Bienenstock, and Doursat]{geman1992neural}
Stuart Geman, Elie Bienenstock, and Ren{\'e} Doursat.
\newblock Neural networks and the bias/variance dilemma.
\newblock \emph{Neural computation}, 4\penalty0 (1):\penalty0 1--58, 1992.

\bibitem[Vapnik(1999)]{vapnik1999overview}
Vladimir~N Vapnik.
\newblock An overview of statistical learning theory.
\newblock \emph{IEEE transactions on neural networks}, 10\penalty0
  (5):\penalty0 988--999, 1999.

\bibitem[Belkin et~al.(2018{\natexlab{a}})Belkin, Ma, and
  Mandal]{belkin2018understand}
Mikhail Belkin, Siyuan Ma, and Soumik Mandal.
\newblock To understand deep learning we need to understand kernel learning.
\newblock \emph{arXiv preprint arXiv:1802.01396}, 2018{\natexlab{a}}.

\bibitem[Nakkiran et~al.(2019)Nakkiran, Kaplun, Bansal, Yang, Barak, and
  Sutskever]{nakkiran2019deep}
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya
  Sutskever.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock \emph{arXiv preprint arXiv:1912.02292}, 2019.

\bibitem[Belkin et~al.(2018{\natexlab{b}})Belkin, Hsu, and
  Mitra]{belkin2018overfitting}
Mikhail Belkin, Daniel~J Hsu, and Partha Mitra.
\newblock Overfitting or perfect fitting? risk bounds for classification and
  regression rules that interpolate.
\newblock In \emph{Advances in neural information processing systems}, pages
  2300--2311, 2018{\natexlab{b}}.

\bibitem[Belkin et~al.(2018{\natexlab{c}})Belkin, Rakhlin, and
  Tsybakov]{belkin2018does}
Mikhail Belkin, Alexander Rakhlin, and Alexandre~B Tsybakov.
\newblock Does data interpolation contradict statistical optimality?
\newblock \emph{arXiv preprint arXiv:1806.09471}, 2018{\natexlab{c}}.

\bibitem[Liang and Rakhlin(2018)]{liang2018just}
Tengyuan Liang and Alexander Rakhlin.
\newblock Just interpolate: Kernel "ridgeless" regression can generalize.
\newblock \emph{arXiv preprint arXiv:1808.00387}, 2018.

\bibitem[Advani and Saxe(2017)]{advani2017high}
Madhu~S Advani and Andrew~M Saxe.
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock \emph{arXiv preprint arXiv:1710.03667}, 2017.

\bibitem[Geiger et~al.(2019)Geiger, Jacot, Spigler, Gabriel, Sagun, d'Ascoli,
  Biroli, Hongler, and Wyart]{geiger2019scaling}
Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun,
  St{\'e}phane d'Ascoli, Giulio Biroli, Cl{\'e}ment Hongler, and Matthieu
  Wyart.
\newblock Scaling description of generalization with number of parameters in
  deep learning.
\newblock \emph{arXiv preprint arXiv:1901.01608}, 2019.

\bibitem[Kobak et~al.(2018)Kobak, Lomond, and Sanchez]{kobak2018optimal}
Dmitry Kobak, Jonathan Lomond, and Benoit Sanchez.
\newblock Optimal ridge penalty for real-world high-dimensional data can be
  zero or negative due to the implicit ridge regularization.
\newblock \emph{arXiv preprint arXiv:1805.10939}, 2018.

\bibitem[Belkin et~al.(2019{\natexlab{b}})Belkin, Hsu, and Xu]{belkin2019two}
Mikhail Belkin, Daniel Hsu, and Ji~Xu.
\newblock Two models of double descent for weak features.
\newblock \emph{arXiv preprint arXiv:1903.07571}, 2019{\natexlab{b}}.

\bibitem[Hastie et~al.(2019)Hastie, Montanari, Rosset, and
  Tibshirani]{hastie2019surprises}
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan~J Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock \emph{arXiv preprint arXiv:1903.08560}, 2019.

\bibitem[Mitra(2019)]{mitra2019understanding}
Partha~P Mitra.
\newblock Understanding overfitting peaks in generalization error: Analytical
  risk curves for $ l\_2 $ and $ l\_1 $ penalized interpolation.
\newblock \emph{arXiv preprint arXiv:1906.03667}, 2019.

\bibitem[Mei and Montanari(2019)]{mei2019generalization}
Song Mei and Andrea Montanari.
\newblock The generalization error of random features regression: Precise
  asymptotics and double descent curve.
\newblock \emph{arXiv preprint arXiv:1908.05355}, 2019.

\bibitem[Adlam and Pennington(2020)]{adlamneural}
Ben Adlam and Jeffrey Pennington.
\newblock The neural tangent kernel in high dimensions: Triple descent and a
  multi-scale theory of generalization.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning (ICML 2020)}, 2020.

\bibitem[Neal et~al.(2018)Neal, Mittal, Baratin, Tantia, Scicluna,
  Lacoste-Julien, and Mitliagkas]{neal2018modern}
Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna,
  Simon Lacoste-Julien, and Ioannis Mitliagkas.
\newblock A modern take on the bias-variance tradeoff in neural networks.
\newblock \emph{arXiv preprint arXiv:1810.08591}, 2018.

\bibitem[Pennington and Worah(2017)]{pennington2017nonlinear}
Jeffrey Pennington and Pratik Worah.
\newblock Nonlinear random matrix theory for deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2637--2646, 2017.

\bibitem[Pennington and Worah(2018)]{pennington2018spectrum}
Jeffrey Pennington and Pratik Worah.
\newblock The spectrum of the fisher information matrix of a
  single-hidden-layer neural network.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5410--5419, 2018.

\bibitem[Adlam et~al.(2019)Adlam, Levinson, and Pennington]{adlam2019random}
Ben Adlam, Jake Levinson, and Jeffrey Pennington.
\newblock A random matrix perspective on mixtures of nonlinearities for deep
  learning.
\newblock \emph{arXiv preprint arXiv:1912.00827}, 2019.

\bibitem[Louart et~al.(2018)Louart, Liao, Couillet, et~al.]{louart2018random}
Cosme Louart, Zhenyu Liao, Romain Couillet, et~al.
\newblock A random matrix approach to neural networks.
\newblock \emph{The Annals of Applied Probability}, 28\penalty0 (2):\penalty0
  1190--1248, 2018.

\bibitem[P{\'e}ch{\'e} et~al.(2019)]{peche2019note}
S~P{\'e}ch{\'e} et~al.
\newblock A note on the pennington-worah distribution.
\newblock \emph{Electronic Communications in Probability}, 24, 2019.

\bibitem[Far et~al.(2006)Far, Oraby, Bryc, and Speicher]{far2006spectra}
Reza~Rashidi Far, Tamer Oraby, Wlodzimierz Bryc, and Roland Speicher.
\newblock Spectra of large block matrices.
\newblock \emph{arXiv preprint cs/0610045}, 2006.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem[Yang et~al.(2020)Yang, Yu, You, Steinhardt, and
  Ma]{yang2020rethinking}
Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi~Ma.
\newblock Rethinking bias-variance trade-off for generalization of neural
  networks.
\newblock \emph{arXiv preprint arXiv:2002.11328}, 2020.

\bibitem[Jacot et~al.(2020)Jacot, {\c{S}}im{\c{s}}ek, Spadaro, Hongler, and
  Gabriel]{jacot2020implicit}
Arthur Jacot, Berfin {\c{S}}im{\c{s}}ek, Francesco Spadaro, Cl{\'e}ment
  Hongler, and Franck Gabriel.
\newblock Implicit regularization of random feature models.
\newblock \emph{arXiv preprint arXiv:2002.08404}, 2020.

\bibitem[d'Ascoli et~al.(2020)d'Ascoli, Refinetti, Biroli, and
  Krzakala]{d2020double}
St{\'e}phane d'Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala.
\newblock Double trouble in double descent: Bias and variance (s) in the lazy
  regime.
\newblock \emph{arXiv preprint arXiv:2003.01054}, 2020.

\bibitem[Neal(1996)]{neal1996priors}
Radford~M Neal.
\newblock Priors for infinite networks.
\newblock In \emph{Bayesian Learning for Neural Networks}, pages 29--53.
  Springer, 1996.

\bibitem[Rahimi and Recht(2008)]{rahimi2008random}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{Advances in neural information processing systems}, pages
  1177--1184, 2008.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{ghorbani2019linearized}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Linearized two-layers neural networks in high dimension.
\newblock \emph{arXiv preprint arXiv:1904.12191}, 2019.

\bibitem[Helton et~al.(2018)Helton, Mai, and Speicher]{helton2018applications}
J~William Helton, Tobias Mai, and Roland Speicher.
\newblock Applications of realizations (aka linearizations) to free
  probability.
\newblock \emph{Journal of Functional Analysis}, 274\penalty0 (1):\penalty0
  1--79, 2018.

\bibitem[Mingo and Speicher(2017)]{mingo2017free}
James~A Mingo and Roland Speicher.
\newblock \emph{Free probability and random matrices}, volume~35.
\newblock Springer, 2017.

\bibitem[Erdos(2019)]{erdos2019matrix}
Laszlo Erdos.
\newblock The matrix dyson equation and its applications for random matrices.
\newblock \emph{arXiv preprint arXiv:1903.10060}, 2019.

\end{thebibliography}
