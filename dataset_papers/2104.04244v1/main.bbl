\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, Salakhutdinov, and Wang]{Arora19}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R.~R., and Wang, R.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~32, pp.\  8141--8150, 2019.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and Tsigler]{Bartlett20}
Bartlett, P.~L., Long, P.~M., Lugosi, G., and Tsigler, A.
\newblock Benign overfitting in linear regression.
\newblock 117\penalty0 (48):\penalty0 30063--30070, 2020.

\bibitem[Belkin et~al.(2018)Belkin, Ma, and Mandal]{Belkin18c}
Belkin, M., Ma, S., and Mandal, S.
\newblock To understand deep learning we need to understand kernel learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, volume~80, pp.\  541--549, 2018.

\bibitem[Belkin et~al.(2019)Belkin, Rakhlin, and Tsybakov]{Belkin19}
Belkin, M., Rakhlin, A., and Tsybakov, A.~B.
\newblock Does data interpolation contradict statistical optimality?
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics (AISTATS)}, pp.\  1611--1619, 2019.

\bibitem[Berg et~al.(1984)Berg, Christensen, and Ressel]{Berg84}
Berg, C., Christensen, J. P.~R., and Ressel, P.
\newblock \emph{Harmonic Analysis on Semigroups}.
\newblock Springer, 1984.

\bibitem[Blumenthal \& Getoor(1960)Blumenthal and Getoor]{Blumenthal60}
Blumenthal, R.~M. and Getoor, R.~K.
\newblock Some theorems on stable processes.
\newblock \emph{Transactions of the American Mathematical Society}, 95\penalty0
  (2):\penalty0 263--273, 1960.

\bibitem[B{\"u}hlmann \& Van De~Geer(2011)B{\"u}hlmann and Van
  De~Geer]{Buhlmann11}
B{\"u}hlmann, P. and Van De~Geer, S.
\newblock \emph{Statistics for high-dimensional data: methods, theory and
  applications}.
\newblock Springer Science \& Business Media, 2011.

\bibitem[Caponnetto \& De~Vito(2007)Caponnetto and De~Vito]{Caponnetto07}
Caponnetto, A. and De~Vito, E.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock \emph{Foundations of Computational Mathematics}, 7\penalty0
  (3):\penalty0 331--368, 2007.

\bibitem[Chen et~al.(2017)Chen, Stern, Wainwright, and Jordan]{Chen17}
Chen, J., Stern, M., Wainwright, M.~J., and Jordan, M.~I.
\newblock Kernel feature selection via conditional covariance minimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  6946--6955, 2017.

\bibitem[Christmann et~al.(2007)Christmann, Steinwart, et~al.]{Christmann07}
Christmann, A., Steinwart, I., et~al.
\newblock Consistency and robustness of kernel-based regression in convex risk
  minimization.
\newblock \emph{Bernoulli}, 13\penalty0 (3):\penalty0 799--819, 2007.

\bibitem[Daniely et~al.(2016)Daniely, Frostig, and Singer]{Daniely16}
Daniely, A., Frostig, R., and Singer, Y.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~29, 2016.

\bibitem[Dobriban \& Wager(2018)Dobriban and Wager]{Dobriban15}
Dobriban, E. and Wager, S.
\newblock High-dimensional asymptotics of prediction: Ridge regression and
  classification.
\newblock \emph{The Annals of Statistics}, 46\penalty0 (1):\penalty0 247 --
  279, 2018.

\bibitem[Driscoll \& Fornberg(2002)Driscoll and Fornberg]{Driscoll02}
Driscoll, T.~A. and Fornberg, B.
\newblock Interpolation in the limit of increasingly flat radial basis
  functions.
\newblock \emph{Computers \& Mathematics with Applications}, 43\penalty0
  (3-5):\penalty0 413--422, 2002.

\bibitem[Dua \& Graff(2017)Dua and Graff]{Dua17}
Dua, D. and Graff, C.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[El~Karoui et~al.(2010)]{ElKaroui10}
El~Karoui, N. et~al.
\newblock The spectrum of kernel random matrices.
\newblock \emph{Annals of Statistics}, 38\penalty0 (1):\penalty0 1--50, 2010.

\bibitem[Geifman et~al.(2020)Geifman, Yadav, Kasten, Galun, Jacobs, and
  Basri]{Geifman20}
Geifman, A., Yadav, A., Kasten, Y., Galun, M., Jacobs, D., and Basri, R.
\newblock On the similarity between the laplace and neural tangent kernels.
\newblock \emph{arXiv preprint arXiv:2007.01580}, 2020.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{Ghorbani19}
Ghorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A.
\newblock Linearized two-layers neural networks in high dimension.
\newblock \emph{arXiv preprint arXiv:1904.12191}, 2019.

\bibitem[Ghorbani et~al.(2020)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{Ghorbani20}
Ghorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A.
\newblock When do neural networks outperform kernel methods?
\newblock \emph{arXiv preprint arXiv:2006.13409}, 2020.

\bibitem[Hastie et~al.(2019)Hastie, Montanari, Rosset, and
  Tibshirani]{Hastie19}
Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R.~J.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock \emph{arXiv preprint arXiv:1903.08560}, 2019.

\bibitem[Ikramov \& Savel'eva(2000)Ikramov and Savel'eva]{Ikramov00}
Ikramov, K. and Savel'eva, N.
\newblock Conditionally definite matrices.
\newblock \emph{Journal of Mathematical Sciences}, 98:\penalty0 1--50, 2000.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{Jacot20}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{arXiv preprint arXiv:1806.07572}, 2018.

\bibitem[Larsson \& Fornberg(2005)Larsson and Fornberg]{Larsson05}
Larsson, E. and Fornberg, B.
\newblock Theoretical and computational aspects of multivariate interpolation
  with increasingly flat radial basis functions.
\newblock \emph{Computers \& Mathematics with Applications}, 49\penalty0
  (1):\penalty0 103--130, 2005.

\bibitem[Ledoux(2001)]{Ledoux2001}
Ledoux, M.
\newblock \emph{The Concentration of Measure Phenomenon}.
\newblock Mathematical surveys and monographs. American Mathematical Society,
  2001.
\newblock ISBN 9780821837924.

\bibitem[Lee et~al.(2018)Lee, Sohl-dickstein, Pennington, Novak, Schoenholz,
  and Bahri]{Lee18}
Lee, J., Sohl-dickstein, J., Pennington, J., Novak, R., Schoenholz, S., and
  Bahri, Y.
\newblock Deep neural networks as gaussian processes.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2018.

\bibitem[Lee et~al.(2014)Lee, Micchelli, and Yoon]{Lee2014}
Lee, Y.~J., Micchelli, C.~A., and Yoon, J.
\newblock On convergence of flat multivariate interpolation by translation
  kernels with finite smoothness.
\newblock \emph{Constructive Approximation}, 40\penalty0 (1):\penalty0 37--60,
  2014.

\bibitem[Li et~al.(2018)Li, Cheng, Wang, Morstatter, Trevino, Tang, and
  Liu]{Li18}
Li, J., Cheng, K., Wang, S., Morstatter, F., Trevino, R.~P., Tang, J., and Liu,
  H.
\newblock Feature selection: A data perspective.
\newblock \emph{ACM Computing Surveys (CSUR)}, 50\penalty0 (6):\penalty0 94,
  2018.

\bibitem[Liang et~al.(2020{\natexlab{a}})Liang, Rakhlin, and Zhai]{Liang20b}
Liang, T., Rakhlin, A., and Zhai, X.
\newblock On the multiple descent of minimum-norm interpolants and restricted
  lower isometry of kernels.
\newblock In \emph{Proceedings of the Conference on Learning Theory (COLT)},
  2020{\natexlab{a}}.

\bibitem[Liang et~al.(2020{\natexlab{b}})Liang, Rakhlin, et~al.]{Liang20a}
Liang, T., Rakhlin, A., et~al.
\newblock Just interpolate: Kernel “ridgeless” regression can generalize.
\newblock \emph{Annals of Statistics}, 48\penalty0 (3):\penalty0 1329--1347,
  2020{\natexlab{b}}.

\bibitem[Liu et~al.(2020)Liu, Liao, and Suykens]{Liu20}
Liu, F., Liao, Z., and Suykens, J.~A.
\newblock Kernel regression in high dimension: Refined analysis beyond double
  descent.
\newblock \emph{arXiv preprint arXiv:2010.02681}, 2020.

\bibitem[MacKay(1996)]{MacKay96}
MacKay, D. J.~C.
\newblock \emph{Bayesian Non-Linear Modeling for the Prediction Competition},
  pp.\  221--234.
\newblock Springer Netherlands, Dordrecht, 1996.
\newblock ISBN 978-94-015-8729-7.

\bibitem[Mei \& Montanari(2019)Mei and Montanari]{Mei19}
Mei, S. and Montanari, A.
\newblock The generalization error of random features regression: Precise
  asymptotics and double descent curve.
\newblock \emph{arXiv preprint arXiv:1908.05355}, 2019.

\bibitem[Mei et~al.(2021{\natexlab{a}})Mei, Misiakiewicz, and Montanari]{Mei21}
Mei, S., Misiakiewicz, T., and Montanari, A.
\newblock Generalization error of random features and kernel methods:
  hypercontractivity and kernel matrix concentration.
\newblock \emph{arXiv preprint arXiv:2101.10588}, 2021{\natexlab{a}}.

\bibitem[Mei et~al.(2021{\natexlab{b}})Mei, Misiakiewicz, and
  Montanari]{Mei21b}
Mei, S., Misiakiewicz, T., and Montanari, A.
\newblock Learning with invariances in random features and kernel models,
  2021{\natexlab{b}}.

\bibitem[Milman \& Schechtman(1986)Milman and Schechtman]{Milman86}
Milman, D.~V. and Schechtman, G.
\newblock Asymptotic theory of finite dimensional normed spaces.
\newblock 1986.

\bibitem[Muthukumar et~al.(2020)Muthukumar, Narang, Subramanian, Belkin, Hsu,
  and Sahai]{Muthukumar20}
Muthukumar, V., Narang, A., Subramanian, V., Belkin, M., Hsu, D., and Sahai, A.
\newblock Classification vs regression in overparameterized regimes: Does the
  loss function matter?
\newblock \emph{arXiv preprint arXiv:2005.08054}, 2020.

\bibitem[Neal(1996)]{Neal96}
Neal, R.~M.
\newblock \emph{Bayesian Learning for Neural Networks}.
\newblock Springer-Verlag, 1996.
\newblock ISBN 0387947248.

\bibitem[Novak et~al.(2019)Novak, Xiao, Bahri, Lee, Yang, Abolafia, Pennington,
  and Sohl-dickstein]{Novak19}
Novak, R., Xiao, L., Bahri, Y., Lee, J., Yang, G., Abolafia, D.~A., Pennington,
  J., and Sohl-dickstein, J.
\newblock Bayesian deep convolutional networks with many channels are gaussian
  processes.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=B1g30j0qF7}.

\bibitem[Schaback(2005)]{Schaback05}
Schaback, R.
\newblock Multivariate interpolation by polynomials and radial basis functions.
\newblock \emph{Constructive Approximation}, 21\penalty0 (3):\penalty0
  293--317, 2005.

\bibitem[Shankar et~al.(2020)Shankar, Fang, Guo, Fridovich-Keil, Ragan-Kelley,
  Schmidt, and Recht]{Shankar20}
Shankar, V., Fang, A., Guo, W., Fridovich-Keil, S., Ragan-Kelley, J., Schmidt,
  L., and Recht, B.
\newblock Neural kernels without tangents.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, volume 119, pp.\  8614--, 2020.

\bibitem[Smola \& Sch{\"o}lkopf(1998)Smola and Sch{\"o}lkopf]{Smola98}
Smola, A.~J. and Sch{\"o}lkopf, B.
\newblock \emph{Learning with kernels}, volume~4.
\newblock Citeseer, 1998.

\bibitem[Wahba(1990)]{Wahba90}
Wahba, G.
\newblock \emph{Spline models for observational data}.
\newblock SIAM, 1990.

\bibitem[Wainwright(2019)]{Wainwright19}
Wainwright, M.
\newblock \emph{High-Dimensional Statistics: A Non-Asymptotic Viewpoint}.
\newblock Cambridge University Press, 2019.
\newblock ISBN 9781108498029.

\bibitem[Wyner et~al.(2017)Wyner, Olson, Bleich, and Mease]{Wyner17}
Wyner, A.~J., Olson, M., Bleich, J., and Mease, D.
\newblock Explaining the success of adaboost and random forests as
  interpolating classifiers.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 18\penalty0
  (1):\penalty0 1558--1590, 2017.

\end{thebibliography}
