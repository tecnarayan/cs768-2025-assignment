\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020{\natexlab{a}})Agarwal, Henaff, Kakade, and
  Sun]{agarwal2020pc}
Agarwal, A., Henaff, M., Kakade, S., and Sun, W.
\newblock Pc-pg: Policy cover directed exploration for provable policy gradient
  learning.
\newblock \emph{arXiv preprint arXiv:2007.08459}, 2020{\natexlab{a}}.

\bibitem[Agarwal et~al.(2020{\natexlab{b}})Agarwal, Kakade, and
  Yang]{agarwal2019model}
Agarwal, A., Kakade, S., and Yang, L.~F.
\newblock Model-based reinforcement learning with a generative model is minimax
  optimal.
\newblock In \emph{Conference on Learning Theory}, pp.\  67--83,
  2020{\natexlab{b}}.

\bibitem[Agarwal et~al.(2020{\natexlab{c}})Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2020optimality}
Agarwal, A., Kakade, S.~M., Lee, J.~D., and Mahajan, G.
\newblock Optimality and approximation with policy gradient methods in markov
  decision processes.
\newblock In \emph{Conference on Learning Theory}, pp.\  64--66,
  2020{\natexlab{c}}.

\bibitem[Amortila et~al.(2020)Amortila, Jiang, and Xie]{amortila2020variant}
Amortila, P., Jiang, N., and Xie, T.
\newblock A variant of the wang-foster-kakade lower bound for the discounted
  setting, 2020.

\bibitem[Antos et~al.(2008)Antos, Szepesv{\'a}ri, and Munos]{antos2008learning}
Antos, A., Szepesv{\'a}ri, C., and Munos, R.
\newblock Learning near-optimal policies with bellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock \emph{Machine Learning}, 71\penalty0 (1):\penalty0 89--129, 2008.

\bibitem[Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang, and
  Yang]{ayoub2020model}
Ayoub, A., Jia, Z., Szepesvari, C., Wang, M., and Yang, L.~F.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock \emph{arXiv preprint arXiv:2006.01107}, 2020.

\bibitem[Azar et~al.(2012)Azar, Munos, and Kappen]{Azar12}
Azar, M., Munos, R., and Kappen, H.~J.
\newblock On the sample complexity of reinforcement learning with a generative
  model.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2012.

\bibitem[Baird(1995)]{baird1995residual}
Baird, L.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In \emph{International Conference on Machine Learning (ICML)}. 1995.

\bibitem[Bradtke \& Barto(1996)Bradtke and Barto]{bradtke1996linear}
Bradtke, S.~J. and Barto, A.~G.
\newblock Linear least-squares algorithms for temporal difference learning.
\newblock \emph{Machine learning}, 22\penalty0 (1-3):\penalty0 33--57, 1996.

\bibitem[Chen \& Jiang(2019)Chen and Jiang]{chen2019information}
Chen, J. and Jiang, N.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1042--1051, 2019.

\bibitem[Cui \& Yang(2020)Cui and Yang]{cui2020plug}
Cui, Q. and Yang, L.~F.
\newblock Is plug-in solver sample-efficient for feature-based reinforcement
  learning?
\newblock \emph{arXiv preprint arXiv:2010.05673}, 2020.

\bibitem[Du et~al.(2019)Du, Kakade, Wang, and Yang]{du2019good}
Du, S.~S., Kakade, S.~M., Wang, R., and Yang, L.~F.
\newblock Is a good representation sufficient for sample efficient
  reinforcement learning?
\newblock \emph{arXiv preprint arXiv:1910.03016}, 2019.

\bibitem[Du et~al.(2020)Du, Lee, Mahajan, and Wang]{du2020agnostic}
Du, S.~S., Lee, J.~D., Mahajan, G., and Wang, R.
\newblock Agnostic q-learning with function approximation in deterministic
  systems: Tight bounds on approximation error and sample complexity, 2020.

\bibitem[Duan \& Wang(2020)Duan and Wang]{duan2020minimax}
Duan, Y. and Wang, M.
\newblock Minimax-optimal off-policy evaluation with linear function
  approximation.
\newblock \emph{arXiv preprint arXiv:2002.09516}, 2020.

\bibitem[Hao et~al.(2020)Hao, Duan, Lattimore, Szepesv{\'a}ri, and
  Wang]{hao2020sparse}
Hao, B., Duan, Y., Lattimore, T., Szepesv{\'a}ri, C., and Wang, M.
\newblock Sparse feature selection makes batch reinforcement learning more
  sample efficient, 2020.

\bibitem[Jiang \& Li(2016)Jiang and Li]{jiang2016doubly}
Jiang, N. and Li, L.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  652--661. PMLR, 2016.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang17contextual}
Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R.~E.
\newblock Contextual decision processes with low {B}ellman rank are
  {PAC}-learnable.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{International Conference
  on Machine Learning (ICML)}, volume~70 of \emph{Proceedings of Machine
  Learning Research}, pp.\  1704--1713, International Convention Centre,
  Sydney, Australia, 06--11 Aug 2017. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v70/jiang17c.html}.

\bibitem[Jin et~al.(2020)Jin, Yang, Wang, and Jordan]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, 2020.

\bibitem[Lagoudakis \& Parr(2003)Lagoudakis and Parr]{lagoudakis2003least}
Lagoudakis, M.~G. and Parr, R.
\newblock Least-squares policy iteration.
\newblock \emph{Journal of machine learning research}, 4\penalty0
  (Dec):\penalty0 1107--1149, 2003.

\bibitem[Lattimore \& Szepesv{\'a}ri(2020)Lattimore and
  Szepesv{\'a}ri]{lattimore2020bandit}
Lattimore, T. and Szepesv{\'a}ri, C.
\newblock \emph{Bandit Algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Lattimore et~al.(2020)Lattimore, Szepesvari, and
  Weisz]{lattimore2020learning}
Lattimore, T., Szepesvari, C., and Weisz, G.
\newblock Learning with good feature representations in bandits and in {RL}
  with a generative model.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  5662--5670. PMLR,
  13--18 Jul 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/lattimore20a.html}.

\bibitem[Lazaric et~al.(2012)Lazaric, Ghavamzadeh, and
  Munos]{lazaric2012finite}
Lazaric, A., Ghavamzadeh, M., and Munos, R.
\newblock Finite-sample analysis of least-squares policy iteration.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (Oct):\penalty0 3041--3074, 2012.

\bibitem[Li et~al.(2020)Li, Wei, Chi, Gu, and Chen]{li2020breaking}
Li, G., Wei, Y., Chi, Y., Gu, Y., and Chen, Y.
\newblock Breaking the sample size barrier in model-based reinforcement
  learning with a generative model.
\newblock \emph{arXiv preprint arXiv:2005.12900}, 2020.

\bibitem[Li et~al.(2015)Li, Munos, and Szepesv{\'a}ri]{li2015toward}
Li, L., Munos, R., and Szepesv{\'a}ri, C.
\newblock Toward minimax off-policy value estimation.
\newblock 2015.

\bibitem[Li(2011)]{li2011concise}
Li, S.
\newblock Concise formulas for the area and volume of a hyperspherical cap.
\newblock \emph{Asian Journal of Mathematics and Statistics}, 4\penalty0
  (1):\penalty0 66--70, 2011.

\bibitem[Liu et~al.(2018)Liu, Li, Tang, and Zhou]{liu2018breaking}
Liu, Q., Li, L., Tang, Z., and Zhou, D.
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5356--5366, 2018.

\bibitem[Liu et~al.(2020)Liu, Swaminathan, Agarwal, and
  Brunskill]{liu2020provably}
Liu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E.
\newblock Provably good batch reinforcement learning without great exploration.
\newblock \emph{arXiv preprint arXiv:2007.08202}, 2020.

\bibitem[Marjani \& Proutiere(2020)Marjani and Proutiere]{marjani2020best}
Marjani, A.~A. and Proutiere, A.
\newblock Best policy identification in discounted mdps: Problem-specific
  sample complexity.
\newblock \emph{arXiv preprint arXiv:2009.13405}, 2020.

\bibitem[Munos(2003)]{munos2003error}
Munos, R.
\newblock Error bounds for approximate policy iteration.
\newblock In \emph{ICML}, volume~3, pp.\  560--567, 2003.

\bibitem[Munos(2005)]{munos2005error}
Munos, R.
\newblock Error bounds for approximate value iteration.
\newblock In \emph{AAAI Conference on Artificial Intelligence (AAAI)}, 2005.

\bibitem[Munos \& Szepesv{\'a}ri(2008)Munos and
  Szepesv{\'a}ri]{munos2008finite}
Munos, R. and Szepesv{\'a}ri, C.
\newblock Finite-time bounds for fitted value iteration.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0
  (May):\penalty0 815--857, 2008.

\bibitem[Precup(2000)]{precup2000eligibility}
Precup, D.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock \emph{Computer Science Department Faculty Publication Series}, pp.\
  ~80, 2000.

\bibitem[Pukelsheim(2006)]{pukelsheim2006optimal}
Pukelsheim, F.
\newblock \emph{Optimal design of experiments}.
\newblock SIAM, 2006.

\bibitem[Puterman(1994)]{puterman1994markov}
Puterman, M.~L.
\newblock \emph{Markov Decision Processes: Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons, Inc., New York, NY, USA, 1994.
\newblock ISBN 0471619779.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT Press, 2018.

\bibitem[Thomas \& Brunskill(2016)Thomas and Brunskill]{thomas2016data}
Thomas, P. and Brunskill, E.
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2139--2148, 2016.

\bibitem[Tsitsiklis \& Van~Roy(1996)Tsitsiklis and
  Van~Roy]{tsitsiklis1996feature}
Tsitsiklis, J.~N. and Van~Roy, B.
\newblock Feature-based methods for large scale dynamic programming.
\newblock \emph{Machine Learning}, 22\penalty0 (1-3):\penalty0 59--94, 1996.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Foster, and
  Kakade]{wang2020statistical}
Wang, R., Foster, D.~P., and Kakade, S.~M.
\newblock What are the statistical limits of offline rl with linear function
  approximation?
\newblock \emph{arXiv preprint arXiv:2010.11895}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Salakhutdinov, and
  Yang]{wang2020provably}
Wang, R., Salakhutdinov, R., and Yang, L.~F.
\newblock Provably efficient reinforcement learning with general value function
  approximation, 2020{\natexlab{b}}.

\bibitem[Weisz et~al.(2020)Weisz, Amortila, and
  Szepesv{\'a}ri]{weisz2020exponential}
Weisz, G., Amortila, P., and Szepesv{\'a}ri, C.
\newblock Exponential lower bounds for planning in mdps with
  linearly-realizable optimal action-value functions.
\newblock \emph{arXiv preprint arXiv:2010.01374}, 2020.

\bibitem[Wen \& {Van Roy}(2013)Wen and {Van Roy}]{WR13}
Wen, Z. and {Van Roy}, B.
\newblock Efficient exploration and value function generalization in
  deterministic systems.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2013.

\bibitem[Xie \& Jiang(2020{\natexlab{a}})Xie and Jiang]{xie2020Q}
Xie, T. and Jiang, N.
\newblock Q* approximation schemes for batch reinforcement learning: A
  theoretical comparison.
\newblock volume 124 of \emph{Proceedings of Machine Learning Research}, pp.\
  550--559, Virtual, 03--06 Aug 2020{\natexlab{a}}. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v124/xie20a.html}.

\bibitem[Xie \& Jiang(2020{\natexlab{b}})Xie and Jiang]{xie2020batch}
Xie, T. and Jiang, N.
\newblock Batch value-function approximation with only realizability.
\newblock \emph{arXiv preprint arXiv:2008.04990}, 2020{\natexlab{b}}.

\bibitem[Xie et~al.(2019)Xie, Ma, and Wang]{xie2019towards}
Xie, T., Ma, Y., and Wang, Y.-X.
\newblock Towards optimal off-policy evaluation for reinforcement learning with
  marginalized importance sampling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9668--9678, 2019.

\bibitem[Yang \& Wang(2020)Yang and Wang]{yang2020reinforcement}
Yang, L.~F. and Wang, M.
\newblock Reinforcement leaning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Yin et~al.(2020)Yin, Bai, and Wang]{yin2020near}
Yin, M., Bai, Y., and Wang, Y.-X.
\newblock Near optimal provable uniform convergence in off-policy evaluation
  for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2007.03760}, 2020.

\bibitem[Zanette et~al.(2019{\natexlab{a}})Zanette, Brunskill, and {J.
  Kochenderfer}]{zanette2019b}
Zanette, A., Brunskill, E., and {J. Kochenderfer}, M.
\newblock Almost horizon-free structure-aware best policy identification with a
  generative model.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2019{\natexlab{a}}.

\bibitem[Zanette et~al.(2019{\natexlab{b}})Zanette, Lazaric, {J. Kochenderfer},
  and Brunskill]{zanette19limiting}
Zanette, A., Lazaric, A., {J. Kochenderfer}, M., and Brunskill, E.
\newblock Limiting extrapolation in linear approximate value iteration.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2019{\natexlab{b}}.

\bibitem[Zanette et~al.(2020{\natexlab{a}})Zanette, Brandfonbrener, Pirotta,
  and Lazaric]{zanette2020frequentist}
Zanette, A., Brandfonbrener, D., Pirotta, M., and Lazaric, A.
\newblock Frequentist regret bounds for randomized least-squares value
  iteration.
\newblock In \emph{AISTATS}, 2020{\natexlab{a}}.

\bibitem[Zanette et~al.(2020{\natexlab{b}})Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020learning}
Zanette, A., Lazaric, A., Kochenderfer, M., and Brunskill, E.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2020{\natexlab{b}}.

\bibitem[Zanette et~al.(2020{\natexlab{c}})Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020provably}
Zanette, A., Lazaric, A., Kochenderfer, M.~J., and Brunskill, E.
\newblock Provably efficient reward-agnostic navigation with linear value
  iteration.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2020{\natexlab{c}}.

\bibitem[Zhou et~al.(2020)Zhou, He, and Gu]{zhou2020provably}
Zhou, D., He, J., and Gu, Q.
\newblock Provably efficient reinforcement learning for discounted mdps with
  feature mapping.
\newblock \emph{arXiv preprint arXiv:2006.13165}, 2020.

\end{thebibliography}
