\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Acar et~al.(2020)Acar, Zhao, Matas, Mattina, Whatmough, and
  Saligrama]{acar2020federated}
Acar, D. A.~E., Zhao, Y., Matas, R., Mattina, M., Whatmough, P., and Saligrama,
  V.
\newblock Federated learning based on dynamic regularization.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  242--252. PMLR, 2019.

\bibitem[Caldarola et~al.(2022)Caldarola, Caputo, and
  Ciccone]{caldarola2022improving}
Caldarola, D., Caputo, B., and Ciccone, M.
\newblock Improving generalization in federated learning by seeking flat
  minima.
\newblock In \emph{Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXIII}, pp.\  654--672.
  Springer, 2022.

\bibitem[Charles et~al.(2021)Charles, Garrett, Huo, Shmulyian, and
  Smith]{charles2021large}
Charles, Z., Garrett, Z., Huo, Z., Shmulyian, S., and Smith, V.
\newblock On large-cohort training for federated learning.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 20461--20475, 2021.

\bibitem[Chatterjee(2019)]{chatterjee2019coherent}
Chatterjee, S.
\newblock Coherent gradients: An approach to understanding generalization in
  gradient descent-based optimization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Chatterjee \& Zielinski(2020)Chatterjee and
  Zielinski]{chatterjee2020making}
Chatterjee, S. and Zielinski, P.
\newblock Making coherence out of nothing at all: measuring the evolution of
  gradient alignment.
\newblock \emph{arXiv preprint arXiv:2008.01217}, 2020.

\bibitem[Chen \& Chao(2021)Chen and Chao]{DBLP:conf/iclr/ChenC21}
Chen, H. and Chao, W.
\newblock Fedbe: Making bayesian model ensemble applicable to federated
  learning.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.
\newblock URL \url{https://openreview.net/forum?id=dgtpE6gKjHn}.

\bibitem[Deng et~al.(2020)Deng, Kamani, and Mahdavi]{deng2020distributionally}
Deng, Y., Kamani, M.~M., and Mahdavi, M.
\newblock Distributionally robust federated averaging.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 15111--15122, 2020.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh2017sharp}
Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1019--1028. PMLR, 2017.

\bibitem[Draxler et~al.(2018)Draxler, Veschgini, Salmhofer, and
  Hamprecht]{draxler2018essentially}
Draxler, F., Veschgini, K., Salmhofer, M., and Hamprecht, F.
\newblock Essentially no barriers in neural network energy landscape.
\newblock In \emph{International conference on machine learning}, pp.\
  1309--1318. PMLR, 2018.

\bibitem[Du et~al.(2021)Du, Yan, Feng, Zhou, Zhen, Goh, and
  Tan]{du2021efficient}
Du, J., Yan, H., Feng, J., Zhou, J.~T., Zhen, L., Goh, R. S.~M., and Tan, V.~Y.
\newblock Efficient sharpness-aware minimization for improved training of
  neural networks.
\newblock \emph{arXiv preprint arXiv:2110.03141}, 2021.

\bibitem[Entezari et~al.(2022)Entezari, Sedghi, Saukh, and
  Neyshabur]{entezari2021role}
Entezari, R., Sedghi, H., Saukh, O., and Neyshabur, B.
\newblock The role of permutation invariance in linear mode connectivity of
  neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Foret et~al.(2020)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2020sharpness}
Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock \emph{arXiv preprint arXiv:2010.01412}, 2020.

\bibitem[Fort \& Jastrzebski(2019)Fort and Jastrzebski]{fort2019large}
Fort, S. and Jastrzebski, S.
\newblock Large scale structure of neural network loss landscapes.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Fort et~al.(2019)Fort, Nowak, Jastrzebski, and
  Narayanan]{fort2019stiffness}
Fort, S., Nowak, P.~K., Jastrzebski, S., and Narayanan, S.
\newblock Stiffness: A new perspective on generalization in neural networks.
\newblock \emph{arXiv preprint arXiv:1901.09491}, 2019.

\bibitem[Franceschi et~al.(2017)Franceschi, Donini, Frasconi, and
  Pontil]{franceschi2017forward}
Franceschi, L., Donini, M., Frasconi, P., and Pontil, M.
\newblock Forward and reverse gradient-based hyperparameter optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1165--1173. PMLR, 2017.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{garipov2018loss}
Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D.~P., and Wilson, A.~G.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Guo et~al.(2022)Guo, Yang, Hatamizadeh, Xu, Xu, Li, Zhao, Xu, Harmon,
  Turkbey, et~al.]{guo2022auto}
Guo, P., Yang, D., Hatamizadeh, A., Xu, A., Xu, Z., Li, W., Zhao, C., Xu, D.,
  Harmon, S., Turkbey, E., et~al.
\newblock Auto-fedrl: Federated hyperparameter optimization for
  multi-institutional medical image segmentation.
\newblock \emph{arXiv preprint arXiv:2203.06338}, 2022.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Huang, G., Liu, Z., Van Der~Maaten, L., and Weinberger, K.~Q.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  4700--4708, 2017.

\bibitem[Huang et~al.(2021)Huang, Chu, Zhou, Wang, Liu, Pei, and
  Zhang]{huang2021personalized}
Huang, Y., Chu, L., Zhou, Z., Wang, L., Liu, J., Pei, J., and Zhang, Y.
\newblock Personalized cross-silo federated learning on non-iid data.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pp.\  7865--7873, 2021.

\bibitem[Izmailov et~al.(2018)Izmailov, Wilson, Podoprikhin, Vetrov, and
  Garipov]{izmailov2018averaging}
Izmailov, P., Wilson, A., Podoprikhin, D., Vetrov, D., and Garipov, T.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock In \emph{34th Conference on Uncertainty in Artificial Intelligence
  2018, UAI 2018}, pp.\  876--885, 2018.

\bibitem[Jastrz{\k{e}}bski et~al.(2018)Jastrz{\k{e}}bski, Kenton, Ballas,
  Fischer, Bengio, and Storkey]{jastrzkebski2018relation}
Jastrz{\k{e}}bski, S., Kenton, Z., Ballas, N., Fischer, A., Bengio, Y., and
  Storkey, A.
\newblock On the relation between the sharpest directions of dnn loss and the
  sgd step length.
\newblock \emph{arXiv preprint arXiv:1807.05031}, 2018.

\bibitem[Jastrzebski et~al.(2019)Jastrzebski, Szymczak, Fort, Arpit, Tabor,
  Cho, and Geras]{jastrzebski2019break}
Jastrzebski, S., Szymczak, M., Fort, S., Arpit, D., Tabor, J., Cho, K., and
  Geras, K.
\newblock The break-even point on optimization trajectories of deep neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Jastrzebski et~al.(2020)Jastrzebski, Szymczak, Fort, Arpit, Tabor,
  Cho, and Geras]{jastrzebski2020break}
Jastrzebski, S., Szymczak, M., Fort, S., Arpit, D., Tabor, J., Cho, K., and
  Geras, K.
\newblock The break-even point on optimization trajectories of deep neural
  networks.
\newblock \emph{arXiv preprint arXiv:2002.09572}, 2020.

\bibitem[Jomaa et~al.(2019)Jomaa, Grabocka, and Schmidt-Thieme]{jomaa2019hyp}
Jomaa, H.~S., Grabocka, J., and Schmidt-Thieme, L.
\newblock Hyp-rl: Hyperparameter optimization by reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1906.11527}, 2019.

\bibitem[Kantorovich(2006)]{kantorovich2006translocation}
Kantorovich, L.~V.
\newblock On the translocation of masses.
\newblock \emph{Journal of mathematical sciences}, 133\penalty0 (4):\penalty0
  1381--1382, 2006.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2020scaffold}
Karimireddy, S.~P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh,
  A.~T.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5132--5143. PMLR, 2020.

\bibitem[Keskar et~al.(2017)Keskar, Nocedal, Tang, Mudigere, and
  Smelyanskiy]{keskar2017large}
Keskar, N.~S., Nocedal, J., Tang, P. T.~P., Mudigere, D., and Smelyanskiy, M.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{5th International Conference on Learning Representations,
  ICLR 2017}, 2017.

\bibitem[Kwon et~al.(2021)Kwon, Kim, Park, and Choi]{kwon2021asam}
Kwon, J., Kim, J., Park, H., and Choi, I.~K.
\newblock Asam: Adaptive sharpness-aware minimization for scale-invariant
  learning of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5905--5914. PMLR, 2021.

\bibitem[Lewkowycz \& Gur-Ari(2020)Lewkowycz and
  Gur-Ari]{lewkowycz2020training}
Lewkowycz, A. and Gur-Ari, G.
\newblock On the training dynamics of deep networks with $ l\_2 $
  regularization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 4790--4799, 2020.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Zhou, Tian, and Tao]{li2022learning}
Li, S., Zhou, T., Tian, X., and Tao, D.
\newblock Learning to collaborate in decentralized learning of personalized
  models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  9766--9775, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Sahu, Talwalkar, and
  Smith]{DBLP:journals/spm/LiSTS20}
Li, T., Sahu, A.~K., Talwalkar, A., and Smith, V.
\newblock Federated learning: Challenges, methods, and future directions.
\newblock \emph{{IEEE} Signal Process. Mag.}, 37\penalty0 (3):\penalty0 50--60,
  2020{\natexlab{a}}.
\newblock \doi{10.1109/MSP.2020.2975749}.
\newblock URL \url{https://doi.org/10.1109/MSP.2020.2975749}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{li2020federated}
Li, T., Sahu, A.~K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V.
\newblock Federated optimization in heterogeneous networks.
\newblock \emph{Proceedings of Machine Learning and Systems}, 2:\penalty0
  429--450, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2020{\natexlab{c}})Li, Chen, and Yang]{li2020understanding}
Li, X., Chen, S., and Yang, J.
\newblock Understanding the disharmony between weight normalization family and
  weight decay.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pp.\  4715--4722, 2020{\natexlab{c}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Lu, Luo, Zhu, Shao, Li, Zhang, Wang,
  and Wu]{li2022towards}
Li, Z., Lu, J., Luo, S., Zhu, D., Shao, Y., Li, Y., Zhang, Z., Wang, Y., and
  Wu, C.
\newblock Towards effective clustered federated learning: A peer-to-peer
  framework with adaptive neighbor matching.
\newblock \emph{IEEE Transactions on Big Data}, 2022{\natexlab{b}}.

\bibitem[Li et~al.(2022{\natexlab{c}})Li, Lu, Luo, Zhu, Shao, Li, Zhang, and
  Wu]{li2022mining}
Li, Z., Lu, J., Luo, S., Zhu, D., Shao, Y., Li, Y., Zhang, Z., and Wu, C.
\newblock Mining latent relationships among clients: Peer-to-peer federated
  learning with adaptive neighbor matching.
\newblock \emph{arXiv preprint arXiv:2203.12285}, 2022{\natexlab{c}}.

\bibitem[Lin et~al.(2020)Lin, Kong, Stich, and Jaggi]{lin2020ensemble}
Lin, T., Kong, L., Stich, S.~U., and Jaggi, M.
\newblock Ensemble distillation for robust model fusion in federated learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 2351--2363, 2020.

\bibitem[Loshchilov \& Hutter(2018)Loshchilov and
  Hutter]{loshchilov2018decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Lyu et~al.(2022)Lyu, Li, and Arora]{lyu2022understanding}
Lyu, K., Li, Z., and Arora, S.
\newblock Understanding the generalization benefit of normalization layers:
  Sharpness reduction.
\newblock \emph{arXiv preprint arXiv:2206.07085}, 2022.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and
  Adams]{maclaurin2015gradient}
Maclaurin, D., Duvenaud, D., and Adams, R.
\newblock Gradient-based hyperparameter optimization through reversible
  learning.
\newblock In \emph{International conference on machine learning}, pp.\
  2113--2122. PMLR, 2015.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  1273--1282.
  PMLR, 2017.

\bibitem[Mostafa(2019)]{mostafa2019robust}
Mostafa, H.
\newblock Robust federated learning through representation matching and
  adaptive hyper-parameters.
\newblock \emph{arXiv preprint arXiv:1912.13075}, 2019.

\bibitem[Singh \& Jaggi(2020)Singh and Jaggi]{singh2020model}
Singh, S.~P. and Jaggi, M.
\newblock Model fusion via optimal transport.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 22045--22055, 2020.

\bibitem[Vlaar \& Frankle(2021)Vlaar and Frankle]{vlaar2021can}
Vlaar, T. and Frankle, J.
\newblock What can linear interpolation of neural network loss landscapes tell
  us?
\newblock \emph{arXiv preprint arXiv:2106.16004}, 2021.

\bibitem[Wan et~al.(2021)Wan, Zhu, Zhang, and Sun]{wan2021spherical}
Wan, R., Zhu, Z., Zhang, X., and Sun, J.
\newblock Spherical motion dynamics: Learning dynamics of normalized neural
  network using sgd and weight decay.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 6380--6391, 2021.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Yurochkin, Sun, Papailiopoulos,
  and Khazaeni]{wang2020federated}
Wang, H., Yurochkin, M., Sun, Y., Papailiopoulos, D., and Khazaeni, Y.
\newblock Federated learning with matched averaging.
\newblock \emph{arXiv preprint arXiv:2002.06440}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Liu, Liang, Joshi, and
  Poor]{wang2020tackling}
Wang, J., Liu, Q., Liang, H., Joshi, G., and Poor, H.~V.
\newblock Tackling the objective inconsistency problem in heterogeneous
  federated optimization.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 7611--7623, 2020{\natexlab{b}}.

\bibitem[Wang et~al.(2021)Wang, Charles, Xu, Joshi, McMahan, Al-Shedivat,
  Andrew, Avestimehr, Daly, Data, et~al.]{wang2021field}
Wang, J., Charles, Z., Xu, Z., Joshi, G., McMahan, H.~B., Al-Shedivat, M.,
  Andrew, G., Avestimehr, S., Daly, K., Data, D., et~al.
\newblock A field guide to federated optimization.
\newblock \emph{arXiv preprint arXiv:2107.06917}, 2021.

\bibitem[Wu et~al.(2022)Wu, Liang, Han, Bian, Zhao, and Huang]{wu2022drflm}
Wu, B., Liang, Z., Han, Y., Bian, Y., Zhao, P., and Huang, J.
\newblock Drflm: Distributionally robust federated learning with inter-client
  noise via local mixup.
\newblock \emph{arXiv preprint arXiv:2204.07742}, 2022.

\bibitem[Xia et~al.(2021)Xia, Yang, Li, Myronenko, Xu, Obinata, Mori, An,
  Harmon, Turkbey, et~al.]{xia2021auto}
Xia, Y., Yang, D., Li, W., Myronenko, A., Xu, D., Obinata, H., Mori, H., An,
  P., Harmon, S., Turkbey, E., et~al.
\newblock Auto-fedavg: learnable federated averaging for multi-institutional
  medical image segmentation.
\newblock \emph{arXiv preprint arXiv:2104.10195}, 2021.

\bibitem[Xie et~al.(2020)Xie, Sato, and Sugiyama]{xie2020understanding}
Xie, Z., Sato, I., and Sugiyama, M.
\newblock Understanding and scheduling weight decay.
\newblock \emph{arXiv preprint arXiv:2011.11152}, 2020.

\bibitem[Yan et~al.(2021)Yan, Wang, and Li]{yan2021critical}
Yan, G., Wang, H., and Li, J.
\newblock Critical learning periods in federated learning.
\newblock \emph{arXiv preprint arXiv:2109.05613}, 2021.

\bibitem[Yao et~al.(2020)Yao, Gholami, Keutzer, and Mahoney]{yao2020pyhessian}
Yao, Z., Gholami, A., Keutzer, K., and Mahoney, M.~W.
\newblock Pyhessian: Neural networks through the lens of the hessian.
\newblock In \emph{2020 IEEE international conference on big data (Big data)},
  pp.\  581--590. IEEE, 2020.

\bibitem[Yin et~al.(2018)Yin, Pananjady, Lam, Papailiopoulos, Ramchandran, and
  Bartlett]{yin2018gradient}
Yin, D., Pananjady, A., Lam, M., Papailiopoulos, D., Ramchandran, K., and
  Bartlett, P.
\newblock Gradient diversity: a key ingredient for scalable distributed
  learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1998--2007. PMLR, 2018.

\bibitem[Zhang et~al.(2018)Zhang, Wang, Xu, and Grosse]{zhang2018three}
Zhang, G., Wang, C., Xu, B., and Grosse, R.
\newblock Three mechanisms of weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Zielinski et~al.(2020)Zielinski, Krishnan, and
  Chatterjee]{zielinski2020weak}
Zielinski, P., Krishnan, S., and Chatterjee, S.
\newblock Weak and strong gradient directions: Explaining memorization,
  generalization, and hardness of examples at scale.
\newblock \emph{arXiv preprint arXiv:2003.07422}, 2020.

\bibitem[Zou \& Gu(2019)Zou and Gu]{zou2019improved}
Zou, D. and Gu, Q.
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\end{thebibliography}
