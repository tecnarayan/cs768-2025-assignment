\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achille et~al.(2017)Achille, Rovere, and Soatto]{achille2017critical}
Alessandro Achille, Matteo Rovere, and Stefano Soatto.
\newblock Critical learning periods in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1711.08856}, 2017.

\bibitem[Adiwardana et~al.(2020)Adiwardana, Luong, So, Hall, Fiedel, Thoppilan,
  Yang, Kulshreshtha, Nemade, Lu, et~al.]{adiwardana2020towards}
Daniel Adiwardana, Minh-Thang Luong, David~R So, Jamie Hall, Noah Fiedel, Romal
  Thoppilan, Zi~Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et~al.
\newblock Towards a human-like open-domain chatbot.
\newblock \emph{arXiv preprint arXiv:2001.09977}, 2020.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Cohen, Hu, and
  Luo]{arora2019implicit}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  7411--7422, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019exact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock \emph{arXiv preprint arXiv:1904.11955}, 2019{\natexlab{b}}.

\bibitem[Arora et~al.(2019{\natexlab{c}})Arora, Du, Hu, Li, and
  Wang]{arora2019fine}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock \emph{arXiv preprint arXiv:1901.08584}, 2019{\natexlab{c}}.

\bibitem[Bai and Lee(2020)]{bai2020Beyond}
Yu~Bai and Jason~D. Lee.
\newblock Beyond linearization: On quadratic and higher-order approximation of
  wide neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkllGyBFPH}.

\bibitem[Bartlett and Mendelson(2002)]{bartlett2002rademacher}
Peter~L Bartlett and Shahar Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Nov):\penalty0 463--482, 2002.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett2017spectrally}
Peter~L Bartlett, Dylan~J Foster, and Matus~J Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6241--6250, 2017.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, and Wanderman-Milne]{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, and Skye Wanderman-Milne.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Cao et~al.(2019)Cao, Fang, Wu, Zhou, and Gu]{cao2019towards}
Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu.
\newblock Towards understanding the spectral bias of deep learning.
\newblock \emph{arXiv preprint arXiv:1912.01198}, 2019.

\bibitem[Chizat and Bach(2020)]{chizat2020implicit}
Lenaic Chizat and Francis Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks
  trained with the logistic loss.
\newblock \emph{arXiv preprint arXiv:2002.04486}, 2020.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2019lazy}
Lenaic Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2933--2943, 2019.

\bibitem[Du et~al.(2020)Du, Hu, Kakade, Lee, and Lei]{du2020few}
Simon~S Du, Wei Hu, Sham~M Kakade, Jason~D Lee, and Qi~Lei.
\newblock Few-shot learning via learning the representation, provably.
\newblock \emph{arXiv preprint arXiv:2002.09434}, 2020.

\bibitem[Dziugaite and Roy(2017)]{dziugaite2017computing}
Gintare~Karolina Dziugaite and Daniel~M Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{arXiv preprint arXiv:1703.11008}, 2017.

\bibitem[El~Karoui(2010)]{el2010spectrum}
Noureddine El~Karoui.
\newblock The spectrum of kernel random matrices.
\newblock \emph{The Annals of Statistics}, 38\penalty0 (1):\penalty0 1--50,
  2010.

\bibitem[Frankle and Carbin(2019)]{frankle2019lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJl-b3RcF7}.

\bibitem[Gidel et~al.(2019)Gidel, Bach, and Lacoste-Julien]{gidel2019implicit}
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien.
\newblock Implicit regularization of discrete gradient dynamics in linear
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3196--3206, 2019.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar2017implicit}
Suriya Gunasekar, Blake~E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur,
  and Nati Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6151--6159, 2017.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018implicit}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock \emph{arXiv preprint arXiv:1806.00468}, 2018.

\bibitem[Gur-Ari et~al.(2018)Gur-Ari, Roberts, and Dyer]{gur2018gradient}
Guy Gur-Ari, Daniel~A Roberts, and Ethan Dyer.
\newblock Gradient descent happens in a tiny subspace.
\newblock \emph{arXiv preprint arXiv:1812.04754}, 2018.

\bibitem[Hu et~al.(2020)Hu, Li, and Yu]{hu2020Simple}
Wei Hu, Zhiyuan Li, and Dingli Yu.
\newblock Simple and effective regularization methods for training on noisily
  labeled data with generalization guarantee.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=Hke3gyHYwH}.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{arXiv preprint arXiv:1806.07572}, 2018.

\bibitem[Ji and Telgarsky(2019{\natexlab{a}})]{ji2019implicit}
Ziwei Ji and Matus Telgarsky.
\newblock The implicit bias of gradient descent on nonseparable data.
\newblock In \emph{Conference on Learning Theory}, pages 1772--1798,
  2019{\natexlab{a}}.

\bibitem[Ji and Telgarsky(2019{\natexlab{b}})]{ji2019gradient}
Ziwei Ji and Matus~Jan Telgarsky.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock In \emph{7th International Conference on Learning Representations,
  ICLR 2019}, 2019{\natexlab{b}}.

\bibitem[Klochkov and Zhivotovskiy(2020)]{klochkov2020uniform}
Yegor Klochkov and Nikita Zhivotovskiy.
\newblock Uniform hanson-wright type concentration inequalities for unbounded
  entries via the entropy method.
\newblock \emph{Electronic Journal of Probability}, 25, 2020.

\bibitem[Lampinen and Ganguli(2018)]{lampinen2018analytic}
Andrew~K Lampinen and Surya Ganguli.
\newblock An analytic theory of generalization dynamics and transfer learning
  in deep linear networks.
\newblock \emph{arXiv preprint arXiv:1809.10374}, 2018.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Sohl-Dickstein, and
  Pennington]{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel~S Schoenholz, Yasaman Bahri, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \emph{arXiv preprint arXiv:1902.06720}, 2019.

\bibitem[Lewkowycz et~al.(2020)Lewkowycz, Bahri, Dyer, Sohl-Dickstein, and
  Gur-Ari]{lewkowycz2020large}
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy
  Gur-Ari.
\newblock The large learning rate phase of deep learning: the catapult
  mechanism.
\newblock \emph{arXiv preprint arXiv:2003.02218}, 2020.

\bibitem[Li et~al.(2018)Li, Ma, and Zhang]{li2018algorithmic}
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock In \emph{Conference On Learning Theory}, pages 2--47, 2018.

\bibitem[Li et~al.(2019{\natexlab{a}})Li, Wei, and Ma]{li2019towards}
Yuanzhi Li, Colin Wei, and Tengyu Ma.
\newblock Towards explaining the regularization effect of initial large
  learning rate in training neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  11669--11680, 2019{\natexlab{a}}.

\bibitem[Li et~al.(2019{\natexlab{b}})Li, Wang, Yu, Du, Hu, Salakhutdinov, and
  Arora]{li2019enhanced}
Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon~S Du, Wei Hu, Ruslan Salakhutdinov,
  and Sanjeev Arora.
\newblock Enhanced convolutional neural tangent kernels.
\newblock \emph{arXiv preprint arXiv:1911.00809}, 2019{\natexlab{b}}.

\bibitem[Lyu and Li(2019)]{lyu2019gradient}
Kaifeng Lyu and Jian Li.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock \emph{arXiv preprint arXiv:1906.05890}, 2019.

\bibitem[McAllester(1999)]{mcallester1999pac}
David~A McAllester.
\newblock Pac-bayesian model averaging.
\newblock In \emph{Proceedings of the twelfth annual conference on
  Computational learning theory}, pages 164--170, 1999.

\bibitem[Mohri et~al.(2012)Mohri, Rostamizadeh, and
  Talwalkar]{mohri2012foundations}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock Foundations of machine learning.
\newblock \emph{MIT Press}, 2012.

\bibitem[Nakkiran et~al.(2019)Nakkiran, Kaplun, Kalimeris, Yang, Edelman,
  Zhang, and Barak]{nakkiran2019sgd}
Preetum Nakkiran, Gal Kaplun, Dimitris Kalimeris, Tristan Yang, Benjamin~L
  Edelman, Fred Zhang, and Boaz Barak.
\newblock Sgd on neural networks learns functions of increasing complexity.
\newblock \emph{arXiv preprint arXiv:1905.11604}, 2019.

\bibitem[Neyshabur et~al.(2017{\natexlab{a}})Neyshabur, Bhojanapalli,
  McAllester, and Srebro]{neyshabur2017pac}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro.
\newblock A {PAC-Bayesian} approach to spectrally-normalized margin bounds for
  neural networks.
\newblock \emph{arXiv preprint arXiv:1707.09564}, 2017{\natexlab{a}}.

\bibitem[Neyshabur et~al.(2017{\natexlab{b}})Neyshabur, Bhojanapalli,
  McAllester, and Srebro]{neyshabur2017exploring}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro.
\newblock Exploring generalization in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5947--5956, 2017{\natexlab{b}}.

\bibitem[Novak et~al.(2019)Novak, Xiao, Hron, Lee, Alemi, Sohl-Dickstein, and
  Schoenholz]{novak2019neural}
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander~A Alemi, Jascha
  Sohl-Dickstein, and Samuel~S Schoenholz.
\newblock Neural tangents: Fast and easy infinite neural networks in python.
\newblock \emph{arXiv preprint arXiv:1912.02803}, 2019.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI Blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rahaman et~al.(2018)Rahaman, Baratin, Arpit, Draxler, Lin, Hamprecht,
  Bengio, and Courville]{rahaman2018spectral}
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred~A
  Hamprecht, Yoshua Bengio, and Aaron Courville.
\newblock On the spectral bias of neural networks.
\newblock \emph{arXiv preprint arXiv:1806.08734}, 2018.

\bibitem[Razin and Cohen(2020)]{razin2020implicit}
Noam Razin and Nadav Cohen.
\newblock Implicit regularization in deep learning may not be explainable by
  norms.
\newblock \emph{arXiv preprint arXiv:2005.06398}, 2020.

\bibitem[Rudelson and Vershynin(2013)]{rudelson2013hanson}
Mark Rudelson and Roman Vershynin.
\newblock Hanson-wright inequality and sub-gaussian concentration.
\newblock \emph{Electronic Communications in Probability}, 18, 2013.

\bibitem[Saxe et~al.(2014)Saxe, McClelland, and Ganguli]{saxe2014exact}
AM~Saxe, JL~McClelland, and S~Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock International Conference on Learning Representations, 2014.

\bibitem[Schur(1911)]{schur1911bemerkungen}
Jssai Schur.
\newblock Bemerkungen zur theorie der beschr{\"a}nkten bilinearformen mit
  unendlich vielen ver{\"a}nderlichen.
\newblock \emph{Journal f{\"u}r die reine und angewandte Mathematik (Crelles
  Journal)}, 1911\penalty0 (140):\penalty0 1--28, 1911.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{Journal of Machine Learning Research}, 19\penalty0 (70), 2018.

\bibitem[Su and Yang(2019)]{su2019learning}
Lili Su and Pengkun Yang.
\newblock On learning over-parameterized neural networks: A functional
  approximation perspective.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2637--2646, 2019.

\bibitem[Tropp(2015)]{tropp2015introduction}
Joel~A Tropp.
\newblock An introduction to matrix concentration inequalities.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  8\penalty0 (1-2):\penalty0 1--230, 2015.

\bibitem[Vapnik and Chervonenkis(1971)]{vapnik1971uniform}
VN~Vapnik and A~Ya Chervonenkis.
\newblock On the uniform convergence of relative frequencies of events to their
  probabilities.
\newblock \emph{Theory of Probability \& Its Applications}, 16\penalty0
  (2):\penalty0 264--280, 1971.

\bibitem[Wainwright(2019)]{wainwright2019high}
Martin~J Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Xiao et~al.(2018)Xiao, Bahri, Sohl-Dickstein, Schoenholz, and
  Pennington]{xiao2018dynamical}
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and
  Jeffrey Pennington.
\newblock Dynamical isometry and a mean field theory of cnns: How to train
  10,000-layer vanilla convolutional neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  5393--5402, 2018.

\bibitem[Xu et~al.(2019{\natexlab{a}})Xu, Zhang, Luo, Xiao, and
  Ma]{xu2019frequency}
Zhi-Qin~John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, and Zheng Ma.
\newblock Frequency principle: Fourier analysis sheds light on deep neural
  networks.
\newblock \emph{arXiv preprint arXiv:1901.06523}, 2019{\natexlab{a}}.

\bibitem[Xu et~al.(2019{\natexlab{b}})Xu, Zhang, and Xiao]{xu2019training}
Zhi-Qin~John Xu, Yaoyu Zhang, and Yanyang Xiao.
\newblock Training behavior of deep neural network in frequency domain.
\newblock In \emph{International Conference on Neural Information Processing},
  pages 264--274. Springer, 2019{\natexlab{b}}.

\bibitem[Xu(2018)]{xu2018understanding}
Zhiqin~John Xu.
\newblock Understanding training and generalization in deep learning by fourier
  analysis.
\newblock \emph{arXiv preprint arXiv:1808.04295}, 2018.

\bibitem[Yang(2019)]{yang2019scaling}
Greg Yang.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian
  process behavior, gradient independence, and neural tangent kernel
  derivation.
\newblock \emph{arXiv preprint arXiv:1902.04760}, 2019.

\bibitem[Yang and Salman(2019)]{yang2019fine}
Greg Yang and Hadi Salman.
\newblock A fine-grained spectral perspective on neural networks.
\newblock \emph{arXiv preprint arXiv:1907.10599}, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2017.

\bibitem[Zhang et~al.(2019)Zhang, Xu, Luo, and Ma]{zhang2019type}
Yaoyu Zhang, Zhi-Qin~John Xu, Tao Luo, and Zheng Ma.
\newblock A type of generalization error induced by initialization in deep
  neural networks.
\newblock \emph{arXiv preprint arXiv:1905.07777}, 2019.

\end{thebibliography}
