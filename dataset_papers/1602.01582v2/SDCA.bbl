\begin{thebibliography}{19}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal \& Bottou(2014)Agarwal and Bottou]{agarwal2014lower}
Agarwal, Alekh and Bottou, Leon.
\newblock A lower bound for the optimization of finite sums.
\newblock In \emph{ICML}, 2014.

\bibitem[Allen-Zhu \& Yuan(2015)Allen-Zhu and Yuan]{allen2015univr}
Allen-Zhu, Zeyuan and Yuan, Yang.
\newblock Univr: A universal variance reduction framework for proximal
  stochastic gradient method.
\newblock \emph{arXiv preprint arXiv:1506.01972}, 2015.

\bibitem[Arjevani et~al.(2015)Arjevani, Shalev-Shwartz, and
  Shamir]{arjevani2015lower}
Arjevani, Yossi, Shalev-Shwartz, Shai, and Shamir, Ohad.
\newblock On lower and upper bounds for smooth and strongly convex optimization
  problems.
\newblock \emph{arXiv preprint arXiv:1503.06833}, 2015.

\bibitem[Csiba \& Richt{\'a}rik(2015)Csiba and Richt{\'a}rik]{csiba2015primal}
Csiba, Dominik and Richt{\'a}rik, Peter.
\newblock Primal method for erm with flexible mini-batching schemes and
  non-convex losses.
\newblock \emph{arXiv preprint arXiv:1506.02227}, 2015.

\bibitem[Defazio(2014)]{defazio2014new}
Defazio, Aaron.
\newblock \emph{New Optimisation Methods for Machine Learning}.
\newblock PhD thesis, Australian National Univer- sity, 2014.

\bibitem[Defazio et~al.(2014{\natexlab{a}})Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1646--1654, 2014{\natexlab{a}}.

\bibitem[Defazio et~al.(2014{\natexlab{b}})Defazio, Caetano, and
  Domke]{defazio2014finito}
Defazio, Aaron~J, Caetano, Tib{\'e}rio~S, and Domke, Justin.
\newblock Finito: A faster, permutable incremental gradient method for big data
  problems.
\newblock \emph{arXiv preprint arXiv:1407.2710}, 2014{\natexlab{b}}.

\bibitem[He \& Tak{\'a}{\v{c}}(2015)He and Tak{\'a}{\v{c}}]{he2015dual}
He, Xi and Tak{\'a}{\v{c}}, Martin.
\newblock Dual free sdca for empirical risk minimization with adaptive
  probabilities.
\newblock \emph{arXiv preprint arXiv:1510.06684}, 2015.

\bibitem[Jin et~al.(2015)Jin, Kakade, Musco, Netrapalli, and
  Sidford]{jin2015robust}
Jin, Chi, Kakade, Sham~M, Musco, Cameron, Netrapalli, Praneeth, and Sidford,
  Aaron.
\newblock Robust shift-and-invert preconditioning: Faster and more sample
  efficient algorithms for eigenvector computation.
\newblock \emph{arXiv preprint arXiv:1510.08896}, 2015.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{johnson2013accelerating}
Johnson, Rie and Zhang, Tong.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  315--323, 2013.

\bibitem[Kone{\v{c}}n{\`y} \& Richt{\'a}rik(2013)Kone{\v{c}}n{\`y} and
  Richt{\'a}rik]{konevcny2013semi}
Kone{\v{c}}n{\`y}, Jakub and Richt{\'a}rik, Peter.
\newblock Semi-stochastic gradient descent methods.
\newblock \emph{arXiv preprint arXiv:1312.1666}, 2013.

\bibitem[{Le Roux} et~al.(2012){Le Roux}, {Schmidt}, and {Bach}]{LSB12-sgdexp}
{Le Roux}, Nicolas, {Schmidt}, Mark, and {Bach}, Francis.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2663--2671, 2012.

\bibitem[Lin et~al.(2015)Lin, Mairal, and Harchaoui]{lin2015universal}
Lin, Hongzhou, Mairal, Julien, and Harchaoui, Zaid.
\newblock A universal catalyst for first-order optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3366--3374, 2015.

\bibitem[Shalev-Shwartz \& Zhang(2015)Shalev-Shwartz and
  Zhang]{ShalevZhangAcc2015}
Shalev-Shwartz, S. and Zhang, T.
\newblock Accelerated proximal stochastic dual coordinate ascent for
  regularized loss minimization.
\newblock \emph{Mathematical Programming SERIES A and B (to appear)}, 2015.

\bibitem[Shalev-Shwartz(2015)]{shalev2015sdca}
Shalev-Shwartz, Shai.
\newblock Sdca without duality.
\newblock \emph{arXiv preprint arXiv:1502.06177}, 2015.

\bibitem[Shalev-Shwartz \& Ben-David(2014)Shalev-Shwartz and Ben-David]{MLbook}
Shalev-Shwartz, Shai and Ben-David, Shai.
\newblock \emph{Understanding Machine Learning: From Theory to Algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Shalev-Shwartz \& Zhang(2013)Shalev-Shwartz and Zhang]{ShalevZh2013}
Shalev-Shwartz, Shai and Zhang, Tong.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock \emph{Journal of Machine Learning Research}, 14:\penalty0 567--599,
  Feb 2013.

\bibitem[Shamir(2015)]{shamir2014stochastic}
Shamir, Ohad.
\newblock A stochastic pca and svd algorithm with an exponential convergence
  rate.
\newblock In \emph{ICML}, 2015.

\bibitem[Xiao \& Zhang(2014)Xiao and Zhang]{xiao2014proximal}
Xiao, Lin and Zhang, Tong.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock \emph{SIAM Journal on Optimization}, 24\penalty0 (4):\penalty0
  2057--2075, 2014.

\end{thebibliography}
