\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2018)Arora, Ge, Neyshabur, and Zhang]{arora2018stronger}
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi~Zhang.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock In \emph{International Conference on Machine Learning}, pages
  254--263. PMLR, 2018.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  322--332. PMLR, 2019.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett2017spectrally}
Peter~L Bartlett, Dylan~J Foster, and Matus~J Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Bousquet and Elisseeff(2002)]{bousquet2002stability}
Olivier Bousquet and Andr{\'e} Elisseeff.
\newblock Stability and generalization.
\newblock \emph{The Journal of Machine Learning Research}, 2:\penalty0
  499--526, 2002.

\bibitem[Bousquet et~al.(2020)Bousquet, Klochkov, and
  Zhivotovskiy]{bousquet2020sharper}
Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy.
\newblock Sharper bounds for uniformly stable algorithms.
\newblock In \emph{Conference on Learning Theory}, pages 610--626. PMLR, 2020.

\bibitem[Bu et~al.(2020)Bu, Zou, and Veeravalli]{bu2020tightening}
Yuheng Bu, Shaofeng Zou, and Venugopal~V Veeravalli.
\newblock Tightening mutual information-based bounds on generalization error.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  1\penalty0 (1):\penalty0 121--130, 2020.

\bibitem[Cao and Gu(2019)]{cao2019generalization}
Yuan Cao and Quanquan Gu.
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Catoni(2007)]{catoni2007pac}
Olivier Catoni.
\newblock Pac-bayesian supervised classification: the thermodynamics of
  statistical learning.
\newblock \emph{{IMS} Lecture Notes Monograph Series}, 56:\penalty0 1--163,
  2007.
\newblock \doi{10.1214/074921707000000391}.

\bibitem[Chaudhari et~al.(2019)Chaudhari, Choromanska, Soatto, LeCun, Baldassi,
  Borgs, Chayes, Sagun, and Zecchina]{chaudhari2019entropy}
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi,
  Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2019\penalty0 (12):\penalty0 124018, 2019.

\bibitem[Chen et~al.(2020)Chen, Du, and Tong]{chen2020stationary}
Xi~Chen, Simon~S Du, and Xin~T Tong.
\newblock On stationary-point hitting time and ergodicity of stochastic
  gradient langevin dynamics.
\newblock \emph{Journal of Machine Learning Research}, 2020.

\bibitem[Chen et~al.(2018)Chen, Jin, and Yu]{chen2018stability}
Yuansi Chen, Chi Jin, and Bin Yu.
\newblock Stability and convergence trade-off of iterative optimization
  algorithms.
\newblock \emph{CoRR}, 2018.

\bibitem[Cover(1999)]{cover1999elements}
Thomas~M Cover.
\newblock \emph{Elements of information theory}.
\newblock John Wiley \& Sons, 1999.

\bibitem[Donsker and Varadhan(1983)]{donsker1983asymptotic}
Monroe~D Donsker and SR~Srinivasa Varadhan.
\newblock Asymptotic evaluation of certain markov process expectations for
  large time. iv.
\newblock \emph{Communications on pure and applied mathematics}, 36\penalty0
  (2):\penalty0 183--212, 1983.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International conference on machine learning}, pages
  1675--1685. PMLR, 2019.

\bibitem[Dubhashi and Panconesi(2009)]{dubhashi2009concentration}
Devdatt~P. Dubhashi and Alessandro Panconesi.
\newblock \emph{Concentration of Measure for the Analysis of Randomized
  Algorithms}.
\newblock Cambridge University Press, 2009.
\newblock ISBN 978-0-521-88427-3.

\bibitem[Duchi(2007)]{duchi2007derivations}
John Duchi.
\newblock Derivations for linear algebra and optimization.
\newblock \emph{Berkeley, California}, 3\penalty0 (1):\penalty0 2325--5870,
  2007.

\bibitem[Dziugaite and Roy(2017)]{dziugaite2017computing}
Gintare~Karolina Dziugaite and Daniel~M Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{UAI}, 2017.

\bibitem[Dziugaite et~al.(2021)Dziugaite, Hsu, Gharbieh, Arpino, and
  Roy]{dziugaite2021role}
Gintare~Karolina Dziugaite, Kyle Hsu, Waseem Gharbieh, Gabriel Arpino, and
  Daniel Roy.
\newblock On the role of data in pac-bayes bounds.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 604--612. PMLR, 2021.

\bibitem[Feldman and Vondrak(2019)]{feldman2019high}
Vitaly Feldman and Jan Vondrak.
\newblock High probability generalization bounds for uniformly stable
  algorithms with nearly optimal rate.
\newblock In \emph{Conference on Learning Theory}, pages 1270--1279. PMLR,
  2019.

\bibitem[Golowich et~al.(2018)Golowich, Rakhlin, and Shamir]{golowich2018size}
Noah Golowich, Alexander Rakhlin, and Ohad Shamir.
\newblock Size-independent sample complexity of neural networks.
\newblock In \emph{Conference On Learning Theory}, pages 297--299. PMLR, 2018.

\bibitem[Haghifam et~al.(2020)Haghifam, Negrea, Khisti, Roy, and
  Dziugaite]{haghifam2020sharpened}
Mahdi Haghifam, Jeffrey Negrea, Ashish Khisti, Daniel~M Roy, and
  Gintare~Karolina Dziugaite.
\newblock Sharpened generalization bounds based on conditional mutual
  information and an application to noisy, iterative algorithms.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 9925--9935, 2020.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt2016train}
Moritz Hardt, Ben Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In \emph{International conference on machine learning}, pages
  1225--1234. PMLR, 2016.

\bibitem[Harvey et~al.(2017)Harvey, Liaw, and Mehrabian]{harvey2017nearly}
Nick Harvey, Christopher Liaw, and Abbas Mehrabian.
\newblock Nearly-tight vc-dimension bounds for piecewise linear neural
  networks.
\newblock In \emph{Conference on learning theory}, pages 1064--1068. PMLR,
  2017.

\bibitem[Hasanpour et~al.(2016)Hasanpour, Rouhani, Fayyaz, and
  Sabokrou]{hasanpour2016lets}
Seyyed~Hossein Hasanpour, Mohammad Rouhani, Mohsen Fayyaz, and Mohammad
  Sabokrou.
\newblock Lets keep it simple, using simple architectures to outperform deeper
  and more complex architectures.
\newblock \emph{CoRR}, 2016.

\bibitem[Hodgkinson et~al.(2022)Hodgkinson, Simsekli, Khanna, and
  Mahoney]{hodgkinson2022generalization}
Liam Hodgkinson, Umut Simsekli, Rajiv Khanna, and Michael Mahoney.
\newblock Generalization bounds using lower tail exponents in stochastic
  optimizers.
\newblock In \emph{International Conference on Machine Learning}, pages
  8774--8795. PMLR, 2022.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Master's thesis, University of Tront}, 2009.

\bibitem[Kuzborskij and Lampert(2018)]{kuzborskij2018data}
Ilja Kuzborskij and Christoph Lampert.
\newblock Data-dependent stability of stochastic gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages
  2815--2824. PMLR, 2018.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lever et~al.(2013)Lever, Laviolette, and
  Shawe-Taylor]{lever2013tighter}
Guy Lever, Fran{\c{c}}ois Laviolette, and John Shawe-Taylor.
\newblock Tighter pac-bayes bounds through distribution-dependent priors.
\newblock \emph{Theoretical Computer Science}, 473:\penalty0 4--28, 2013.

\bibitem[Li et~al.(2020)Li, Luo, and Qiao]{li2019generalization}
Jian Li, Xuanyuan Luo, and Mingda Qiao.
\newblock On generalization error bounds of noisy gradient methods for
  non-convex learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Li et~al.(2017)Li, Tai, and Weinan]{li2017stochastic}
Qianxiao Li, Cheng Tai, and E~Weinan.
\newblock Stochastic modified equations and adaptive stochastic gradient
  algorithms.
\newblock In \emph{International Conference on Machine Learning}, pages
  2101--2110. PMLR, 2017.

\bibitem[Li et~al.(2021)Li, Malladi, and Arora]{li2021validity}
Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora.
\newblock On the validity of modeling sgd with stochastic differential
  equations (sdes).
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[London(2016)]{london2016generalization}
Ben London.
\newblock Generalization bounds for randomized learning with application to
  stochastic gradient descent.
\newblock In \emph{NIPS Workshop on Optimizing the Optimizers}, 2016.

\bibitem[London(2017)]{london2017pac}
Ben London.
\newblock A pac-bayesian analysis of randomized learning with application to
  stochastic gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[McAllester(1999)]{mcallester1999some}
David~A McAllester.
\newblock Some pac-bayesian theorems.
\newblock \emph{Machine Learning}, 37\penalty0 (3):\penalty0 355--363, 1999.

\bibitem[Mou et~al.(2018)Mou, Wang, Zhai, and Zheng]{mou2018generalization}
Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng.
\newblock Generalization bounds of sgld for non-convex learning: Two
  theoretical viewpoints.
\newblock In \emph{Conference on Learning Theory}, pages 605--638. PMLR, 2018.

\bibitem[Nagarajan and Kolter(2017)]{nagarajan2019generalization}
Vaishnavh Nagarajan and J~Zico Kolter.
\newblock Generalization in deep networks: The role of distance from
  initialization.
\newblock \emph{NeurIPS}, 2017.

\bibitem[Nagarajan and Kolter(2019)]{nagarajan2019uniform}
Vaishnavh Nagarajan and J~Zico Kolter.
\newblock Uniform convergence may be unable to explain generalization in deep
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Negrea et~al.(2019)Negrea, Haghifam, Dziugaite, Khisti, and
  Roy]{negrea2019information}
Jeffrey Negrea, Mahdi Haghifam, Gintare~Karolina Dziugaite, Ashish Khisti, and
  Daniel~M Roy.
\newblock Information-theoretic generalization bounds for sgld via
  data-dependent estimates.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Bhojanapalli, and
  Srebro]{neyshabur2018pac}
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.
\newblock A pac-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Oneto et~al.(2017)Oneto, Ridella, and Anguita]{oneto2017differential}
Luca Oneto, Sandro Ridella, and Davide Anguita.
\newblock Differential privacy and generalization: Sharper bounds with
  applications.
\newblock \emph{Pattern Recognition Letters}, 89:\penalty0 31--38, 2017.

\bibitem[Parrado-Hern{\'a}ndez et~al.(2012)Parrado-Hern{\'a}ndez, Ambroladze,
  Shawe-Taylor, and Sun]{parrado2012pac}
Emilio Parrado-Hern{\'a}ndez, Amiran Ambroladze, John Shawe-Taylor, and
  Shiliang Sun.
\newblock Pac-bayes bounds with data dependent priors.
\newblock \emph{The Journal of Machine Learning Research}, 13\penalty0
  (1):\penalty0 3507--3531, 2012.

\bibitem[Pensia et~al.(2018)Pensia, Jog, and Loh]{pensia2018generalization}
Ankit Pensia, Varun Jog, and Po-Ling Loh.
\newblock Generalization error bounds for noisy, iterative algorithms.
\newblock In \emph{2018 IEEE International Symposium on Information Theory
  (ISIT)}, pages 546--550. IEEE, 2018.

\bibitem[Raginsky et~al.(2017)Raginsky, Rakhlin, and
  Telgarsky]{raginsky2017non}
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky.
\newblock Non-convex learning via stochastic gradient langevin dynamics: a
  nonasymptotic analysis.
\newblock In \emph{Conference on Learning Theory}, pages 1674--1703. PMLR,
  2017.

\bibitem[Risken(1996)]{risken1996fokker}
Hannes Risken.
\newblock Fokker-planck equation.
\newblock In \emph{The Fokker-Planck Equation}, pages 63--95. Springer, 1996.

\bibitem[Simsekli et~al.(2020)Simsekli, Sener, Deligiannidis, and
  Erdogdu]{simsekli2020hausdorff}
Umut Simsekli, Ozan Sener, George Deligiannidis, and Murat~A Erdogdu.
\newblock Hausdorff dimension, heavy tails, and generalization in neural
  networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 5138--5151, 2020.

\bibitem[Wang et~al.(2021)Wang, Huang, Gao, and Calmon]{wang2021analyzing}
Hao Wang, Yizhe Huang, Rui Gao, and Flavio Calmon.
\newblock Analyzing the generalization capability of sgld using properties of
  gaussian channels.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Wei et~al.(2019)Wei, Lee, Liu, and Ma]{wei2019regularization}
Colin Wei, Jason~D Lee, Qiang Liu, and Tengyu Ma.
\newblock Regularization matters: Generalization and optimization of neural
  nets vs their induced kernel.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Welling and Teh(2011)]{welling2011bayesian}
Max Welling and Yee~W Teh.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In \emph{Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 681--688. Citeseer, 2011.

\bibitem[Wu et~al.(2021)Wu, Liang, Bian, Chen, Huang, and
  Yao]{wu2021generalization}
Bingzhe Wu, Zhicong Liang, Yatao Bian, ChaoChao Chen, Junzhou Huang, and Yuan
  Yao.
\newblock Generalization bounds for stochastic gradient langevin dynamics: A
  unified view via information leakage analysis.
\newblock \emph{CoRR}, 2021.

\bibitem[Yang et~al.(2019)Yang, Sun, and Roy]{yang2019fast}
Jun Yang, Shengyang Sun, and Daniel~M Roy.
\newblock Fast-rate pac-bayes generalization bounds via shifted rademacher
  processes.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Zantedeschi et~al.(2021)Zantedeschi, Viallard, Morvant, Emonet,
  Habrard, Germain, and Guedj]{zantedeschi2021learning}
Valentina Zantedeschi, Paul Viallard, Emilie Morvant, R{\'e}mi Emonet, Amaury
  Habrard, Pascal Germain, and Benjamin Guedj.
\newblock Learning stochastic majority votes by minimizing a pac-bayes
  generalization bound.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 455--467, 2021.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{ZhangBHRV17}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.

\bibitem[Zhan$\text{g}$ et~al.(2017)Zhan$\text{g}$, Liang, and
  Charikar]{zhang2017hitting}
Yuchen Zhan$\text{g}$, Percy Liang, and Moses Charikar.
\newblock A hitting time analysis of stochastic gradient langevin dynamics.
\newblock In \emph{Conference on Learning Theory}, pages 1980--2022. PMLR,
  2017.

\bibitem[Zhou et~al.(2019)Zhou, Veitch, Austern, Adams, and Orbanz]{ZhouVAAO19}
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan~P. Adams, and Peter Orbanz.
\newblock Non-vacuous generalization bounds at the imagenet scale: a
  pac-bayesian compression approach.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.

\bibitem[Zhu et~al.(2019)Zhu, Wu, Yu, Wu, and Ma]{zhu2019anisotropic}
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma.
\newblock The anisotropic noise in stochastic gradient descent: Its behavior of
  escaping from sharp minima and regularization effects.
\newblock In \emph{International Conference on Machine Learning}, pages
  7654--7663. PMLR, 2019.

\bibitem[Ziyin et~al.(2021)Ziyin, Liu, Mori, and Ueda]{ziyin2021strength}
Liu Ziyin, Kangqiao Liu, Takashi Mori, and Masahito Ueda.
\newblock Strength of minibatch noise in sgd.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\end{thebibliography}
