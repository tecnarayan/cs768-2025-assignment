\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2018)Arora, Ge, Neyshabur, and Zhang]{Arora18}
Arora, S., Ge, R., Neyshabur, B., and Zhang, Y.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock In \emph{35th International Conference on Machine Learning}, pp.\
  254--263, 2018.
\newblock URL \url{http://proceedings.mlr.press/v80/arora18b.html}.

\bibitem[Arpit et~al.(2017)Arpit, Jastrzebski, Ballas, Krueger, Bengio, Kanwal,
  Maharaj, Fischer, Courville, Bengio, and Lacoste{-}Julien]{Arpit17}
Arpit, D., Jastrzebski, S.~K., Ballas, N., Krueger, D., Bengio, E., Kanwal,
  M.~S., Maharaj, T., Fischer, A., Courville, A.~C., Bengio, Y., and
  Lacoste{-}Julien, S.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{34th International Conference on Machine Learning}, pp.\
  233--242, 2017.
\newblock URL \url{http://proceedings.mlr.press/v70/arpit17a.html}.

\bibitem[Balduzzi et~al.(2017)Balduzzi, Frean, Leary, Lewis, Ma, and
  McWilliams]{Balduzzi17}
Balduzzi, D., Frean, M., Leary, L., Lewis, J.~P., Ma, K.~W., and McWilliams, B.
\newblock The shattered gradients problem: If resnets are the answer, then what
  is the question?
\newblock In \emph{34th International Conference on Machine Learning}, pp.\
  342--350, 2017.
\newblock URL \url{http://proceedings.mlr.press/v70/balduzzi17b.html}.

\bibitem[Billingsley(1995)]{Billingsley95}
Billingsley, P.
\newblock \emph{Probability and Measure}.
\newblock Wiley, 1995.

\bibitem[{Borovykh}(2018)]{Borovykh18}
{Borovykh}, A.
\newblock A gaussian process perspective on convolutional neural networks.
\newblock \emph{arXiv preprint}, 2018.
\newblock URL \url{https://arxiv.org/abs/1810.10798}.

\bibitem[Chaudhari et~al.(2017)Chaudhari, Choromanska, Soatto, LeCun, Baldassi,
  Borgs, Chayes, Sagun, and Zecchina]{Chaudhari17}
Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y., Baldassi, C., Borgs, C.,
  Chayes, J., Sagun, L., and Zecchina, R.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=B1YfAfcgl}.

\bibitem[Chiani et~al.(2003)Chiani, Dardari, and Simon]{Chiani03}
Chiani, M., Dardari, D., and Simon, M.~K.
\newblock New exponential bounds and approximations for the computation of
  error probability in fading channels.
\newblock \emph{{IEEE} Transactions on Wireless Communications}, 2\penalty0
  (4):\penalty0 840--845, 2003.
\newblock URL \url{https://core.ac.uk/download/pdf/11174040.pdf}.

\bibitem[Duvenaud et~al.(2014)Duvenaud, Rippel, Adams, and
  Ghahramani]{Duvenaud14}
Duvenaud, D., Rippel, O., Adams, R., and Ghahramani, Z.
\newblock Avoiding pathologies in very deep networks.
\newblock In \emph{17th International Conference on Artificial Intelligence and
  Statistics}, pp.\  202--210, 2014.
\newblock URL \url{http://proceedings.mlr.press/v33/duvenaud14.html}.

\bibitem[Dziugaite \& Roy(2017)Dziugaite and Roy]{Dziugaite17}
Dziugaite, G.~K. and Roy, D.~M.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock In \emph{33rd Conference on Uncertainty in Artificial Intelligence},
  2017.
\newblock URL \url{http://auai.org/uai2017/proceedings/papers/173.pdf}.

\bibitem[Garriga-Alonso et~al.(2019)Garriga-Alonso, Rasmussen, and
  Aitchison]{Garriga18}
Garriga-Alonso, A., Rasmussen, C.~E., and Aitchison, L.
\newblock Deep convolutional networks as shallow gaussian processes.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bklfsi0cKm}.

\bibitem[Hanin(2018)]{Hanin18a}
Hanin, B.
\newblock Which neural net architectures give rise to exploding and vanishing
  gradients?
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pp.\
  580--589, 2018.
\newblock URL
  \url{http://papers.nips.cc/paper/7339-which-neural-net-architectures-give-rise-to-exploding-and-vanishing-gradients}.

\bibitem[Hanin \& Rolnick(2018)Hanin and Rolnick]{Hanin18b}
Hanin, B. and Rolnick, D.
\newblock How to start training: The effect of initialization and architecture.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pp.\
  569--579, 2018.
\newblock URL
  \url{http://papers.nips.cc/paper/7338-how-to-start-training-the-effect-of-initialization-and-architecture}.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{He15}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{International Conference on Computer Vision}, pp.\
  1026--1034, 2015.
\newblock URL \url{http://dx.doi.org/10.1109/ICCV.2015.123}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{He16}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{14th European Conference on Computer Vision}, pp.\
  630--645, 2016.
\newblock URL \url{https://doi.org/10.1007/978-3-319-46493-0\_38}.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{Hochreiter97}
Hochreiter, S. and Schmidhuber, J.
\newblock Flat minima.
\newblock \emph{Neural Computation}, 9\penalty0 (1):\penalty0 1--42, 1997.
\newblock URL \url{http://dx.doi.org/10.1162/neco.1997.9.1.1}.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{Ioffe15}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{32nd International Conference on Machine Learning}, pp.\
  448--456, 2015.
\newblock URL \url{http://jmlr.org/proceedings/papers/v37/ioffe15.html}.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{Keskar17}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=H1oyRlYgg}.

\bibitem[Langford \& Caruana(2002)Langford and Caruana]{Langford02}
Langford, J. and Caruana, R.
\newblock (not) bounding the true error.
\newblock In \emph{Advances in Neural Information Processing Systems 14}, pp.\
  809--816, 2002.
\newblock URL
  \url{http://papers.nips.cc/paper/1968-not-bounding-the-true-error}.

\bibitem[{Le Roux} \& Bengio(2007){Le Roux} and Bengio]{Leroux07}
{Le Roux}, N. and Bengio, Y.
\newblock Continuous neural networks.
\newblock In \emph{11th International Conference on Artificial Intelligence and
  Statistics}, pp.\  404--411, 2007.
\newblock URL \url{http://proceedings.mlr.press/v2/leroux07a.html}.

\bibitem[Lee et~al.(2018)Lee, Sohl-dickstein, Pennington, Novak, Schoenholz,
  and Bahri]{Lee18}
Lee, J., Sohl-dickstein, J., Pennington, J., Novak, R., Schoenholz, S., and
  Bahri, Y.
\newblock Deep neural networks as gaussian processes.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=B1EA-M-0Z}.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and Goldstein]{Li18}
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pp.\
  6391--6401, 2018.
\newblock URL
  \url{http://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets}.

\bibitem[Lu et~al.(2018)Lu, Su, and Karniadakis]{Lu18}
Lu, L., Su, Y., and Karniadakis, G.~E.
\newblock Collapse of deep and narrow neural nets.
\newblock \emph{arXiv preprint}, 2018.
\newblock URL \url{http://arxiv.org/abs/1808.04947}.

\bibitem[Matthews et~al.(2018)Matthews, Rowland, Hron, Turner, and
  Ghahramani]{Matthews18}
Matthews, A.~G., Rowland, M., Hron, J., Turner, R.~E., and Ghahramani, Z.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock \emph{arXiv preprint}, 2018.
\newblock URL \url{https://arxiv.org/abs/1804.11271}.

\bibitem[Morcos et~al.(2018)Morcos, Barrett, Rabinowitz, and
  Botvinick]{Morcos18}
Morcos, A.~S., Barrett, D.~G., Rabinowitz, N.~C., and Botvinick, M.
\newblock On the importance of single directions for generalization.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=r1iuQjxCZ}.

\bibitem[Neal(1996)]{Neal96}
Neal, R.~M.
\newblock \emph{Bayesian Learning for Neural Networks}.
\newblock Springer-Verlag, 1996.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, Mcallester, and
  Srebro]{Neyshabur17}
Neyshabur, B., Bhojanapalli, S., Mcallester, D., and Srebro, N.
\newblock Exploring generalization in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pp.\
  5947--5956, 2017.
\newblock URL
  \url{http://papers.nips.cc/paper/7176-exploring-generalization-in-deep-learning}.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Bhojanapalli, and
  Srebro]{Neyshabur18}
Neyshabur, B., Bhojanapalli, S., and Srebro, N.
\newblock A {PAC}-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=Skz_WfbCZ}.

\bibitem[Novak et~al.(2018)Novak, Bahri, Abolafia, Pennington, and
  Sohl-Dickstein]{Novak18}
Novak, R., Bahri, Y., Abolafia, D.~A., Pennington, J., and Sohl-Dickstein, J.
\newblock Sensitivity and generalization in neural networks: an empirical
  study.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=HJC2SzZCW}.

\bibitem[Novak et~al.(2019)Novak, Xiao, Bahri, Lee, Yang, Abolafia, Pennington,
  and Sohl-dickstein]{Novak19}
Novak, R., Xiao, L., Bahri, Y., Lee, J., Yang, G., Abolafia, D.~A., Pennington,
  J., and Sohl-dickstein, J.
\newblock Deep bayesian convolutional networks with many channels are gaussian
  processes.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=B1g30j0qF7}.

\bibitem[Philipp \& Carbonell(2018)Philipp and Carbonell]{Philipp18b}
Philipp, G. and Carbonell, J.~G.
\newblock The nonlinearity coefficient - predicting overfitting in deep neural
  networks.
\newblock \emph{arXiv preprint}, 2018.
\newblock URL \url{http://arxiv.org/abs/1806.00179}.

\bibitem[Philipp et~al.(2018)Philipp, Song, and Carbonell]{Philipp18a}
Philipp, G., Song, D., and Carbonell, J.~G.
\newblock Gradients explode - deep networks are shallow - resnet explained.
\newblock In \emph{International Conference on Learning Representations -
  Workshop Track}, 2018.
\newblock URL \url{https://openreview.net/forum?id=HkpYwMZRb}.

\bibitem[Poole et~al.(2016)Poole, Lahiri, Raghu, Sohl{-}Dickstein, and
  Ganguli]{Poole16}
Poole, B., Lahiri, S., Raghu, M., Sohl{-}Dickstein, J., and Ganguli, S.
\newblock Exponential expressivity in deep neural networks through transient
  chaos.
\newblock In \emph{Advances in Neural Information Processing Systems 29}, pp.\
  3360--3368, 2016.
\newblock URL
  \url{http://papers.nips.cc/paper/6322-exponential-expressivity-in-deep-neural-networks-through-transient-chaos}.

\bibitem[Raghu et~al.(2017)Raghu, Poole, Kleinberg, Ganguli, and
  Sohl-Dickstein]{Raghu17}
Raghu, M., Poole, B., Kleinberg, J., Ganguli, S., and Sohl-Dickstein, J.
\newblock On the expressive power of deep neural networks.
\newblock In \emph{34th International Conference on Machine Learning}, pp.\
  2847--2854, 2017.
\newblock URL \url{http://proceedings.mlr.press/v70/raghu17a.html}.

\bibitem[Rifai et~al.(2011)Rifai, Vincent, Muller, Glorot, and Bengio]{Rifai11}
Rifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y.
\newblock Contractive auto-encoders: Explicit invariance during feature
  extraction.
\newblock In \emph{28th International Conference on Machine Learning}, pp.\
  833--840, 2011.
\newblock URL
  \url{http://www.iro.umontreal.ca/~lisa/pointeurs/ICML2011_explicit_invariance.pdf}.

\bibitem[Schoenholz et~al.(2017)Schoenholz, Gilmer, Ganguli, and
  Sohl{-}Dickstein]{Schoenholz17}
Schoenholz, S.~S., Gilmer, J., Ganguli, S., and Sohl{-}Dickstein, J.
\newblock Deep information propagation.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=H1W1UN9gg}.

\bibitem[Smith \& Le(2018)Smith and Le]{Smith18}
Smith, S.~L. and Le, Q.~V.
\newblock A bayesian perspective on generalization and stochastic gradient
  descent.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=BJij4yg0Z}.

\bibitem[Sokolic et~al.(2017)Sokolic, Giryes, Sapiro, and Rodrigues]{Sokolic17}
Sokolic, J., Giryes, R., Sapiro, G., and Rodrigues, M. R.~D.
\newblock Robust large margin deep neural networks.
\newblock \emph{{IEEE} Transactions on Signal Processing}, 65\penalty0
  (16):\penalty0 4265--4280, 2017.
\newblock URL \url{https://doi.org/10.1109/TSP.2017.2708039}.

\bibitem[Vershynin(2010)]{Vershynin12}
Vershynin, R.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock \emph{arXiv preprint}, 2010.
\newblock URL \url{http://arxiv.org/abs/1011.3027}.

\bibitem[Williams(1997)]{Williams97}
Williams, C. K.~I.
\newblock Computing with infinite networks.
\newblock In \emph{Advances in Neural Information Processing Systems 9}, pp.\
  295--301, 1997.
\newblock URL
  \url{http://papers.nips.cc/paper/1197-computing-with-infinite-networks}.

\bibitem[Xiao et~al.(2018)Xiao, Bahri, Sohl-Dickstein, Schoenholz, and
  Pennington]{Xiao18}
Xiao, L., Bahri, Y., Sohl-Dickstein, J., Schoenholz, S., and Pennington, J.
\newblock Dynamical isometry and a mean field theory of {CNN}s: How to train
  10,000-layer vanilla convolutional neural networks.
\newblock In \emph{35th International Conference on Machine Learning}, pp.\
  5393--5402, 2018.
\newblock URL \url{http://proceedings.mlr.press/v80/xiao18a.html}.

\bibitem[Yang(2019)]{Yang19b}
Yang, G.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian
  process behavior, gradient independence, and neural tangent kernel
  derivation.
\newblock \emph{arXiv preprint}, 2019.
\newblock URL \url{http://arxiv.org/abs/1902.04760}.

\bibitem[Yang \& Schoenholz(2017)Yang and Schoenholz]{Yang17}
Yang, G. and Schoenholz, S.
\newblock Mean field residual networks: On the edge of chaos.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pp.\
  7103--7114, 2017.
\newblock URL
  \url{http://papers.nips.cc/paper/6879-mean-field-residual-networks-on-the-edge-of-chaos}.

\bibitem[Yang et~al.(2019)Yang, Pennington, Rao, Sohl-Dickstein, and
  Schoenholz]{Yang19a}
Yang, G., Pennington, J., Rao, V., Sohl-Dickstein, J., and Schoenholz, S.~S.
\newblock A mean field theory of batch normalization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=SyMDXnCcF7}.

\end{thebibliography}
