\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[glo(2023)]{gloo}
{Collective communications library with various primitives for multi-machine training}, 2023.
\newblock URL \url{https://github.com/facebookincubator/gloo}.

\bibitem[Alweiss et~al.(2021)Alweiss, Liu, and Sawhney]{alweiss2021discrepancy}
Ryan Alweiss, Yang~P Liu, and Mehtaab Sawhney.
\newblock Discrepancy minimization via a self-balancing walk.
\newblock In \emph{Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing}, pages 14--20, 2021.

\bibitem[Barp et~al.(2022)Barp, Simon-Gabriel, Girolami, and Mackey]{barp2022targeted}
Alessandro Barp, Carl-Johann Simon-Gabriel, Mark Girolami, and Lester Mackey.
\newblock Targeted separation and convergence with kernel discrepancies.
\newblock \emph{arXiv preprint arXiv:2209.12835}, 2022.

\bibitem[Bertsekas(2011)]{bertsekas2011incremental}
Dimitri~P. Bertsekas.
\newblock {Incremental Gradient, Subgradient, and Proximal Methods for Convex Optimization: A Survey}.
\newblock In \emph{{Optimization for Machine Learning}}. The MIT Press, 2011.

\bibitem[Bottou(2012)]{bottou2012stochastic}
L{\'e}on Bottou.
\newblock Stochastic gradient descent tricks.
\newblock In \emph{Neural networks: Tricks of the trade}, pages 421--436. Springer, 2012.

\bibitem[Cha et~al.(2023)Cha, Lee, and Yun]{cha2023tighter}
Jaeyoung Cha, Jaewook Lee, and Chulhee Yun.
\newblock {Tighter Lower Bounds for Shuffling SGD: Random Permutations and Beyond}.
\newblock In \emph{Proceedings of the 40th International Conference on Machine Learning}, ICML'23. JMLR.org, 2023.

\bibitem[Cooper et~al.(2023)Cooper, Lee, Barocas, Sa, Sen, and Zhang]{cooper2023variance}
A.~Feder Cooper, Katherine Lee, Solon Barocas, Christopher~De Sa, Siddhartha Sen, and Baobao Zhang.
\newblock {Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification}.
\newblock \emph{arXiv preprint arXiv:2301.11562}, 2023.

\bibitem[De~Sa(2020)]{desa2020shuffle}
Christopher De~Sa.
\newblock {Random Reshuffling is Not Always Better}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}.
\newblock In \emph{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186, 2019.

\bibitem[Dwivedi and Mackey(2021)]{dwivedi2021kernel}
Raaz Dwivedi and Lester Mackey.
\newblock Kernel thinning.
\newblock \emph{arXiv preprint arXiv:2105.05842}, 2021.

\bibitem[Dwivedi and Mackey(2022)]{dwivedi2022generalized}
Raaz Dwivedi and Lester Mackey.
\newblock {Generalized Kernel Thinning}.
\newblock In \emph{Tenth International Conference on Learning Representations}, 2022.

\bibitem[Graves et~al.(2017)Graves, Bellemare, Menick, Munos, and Kavukcuoglu]{graves2017automated}
Alex Graves, Marc~G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu.
\newblock Automated curriculum learning for neural networks.
\newblock In \emph{international conference on machine learning}, pages 1311--1320. PMLR, 2017.

\bibitem[G{\"{u}}rb{\"{u}}zbalaban et~al.(2019)G{\"{u}}rb{\"{u}}zbalaban, Ozdaglar, and Parrilo]{gurbuzbalaban2019convergence}
Mert G{\"{u}}rb{\"{u}}zbalaban, Asuman~E. Ozdaglar, and Pablo~A. Parrilo.
\newblock {Convergence Rate of Incremental Gradient and Incremental {Newton} Methods}.
\newblock \emph{{SIAM} Journal on Optimization}, 29\penalty0 (4):\penalty0 2542--2565, 2019.

\bibitem[G{\"u}rb{\"u}zbalaban et~al.(2021)G{\"u}rb{\"u}zbalaban, Ozdaglar, and Parrilo]{gurbuzbalaban2021random}
Mert G{\"u}rb{\"u}zbalaban, Asu Ozdaglar, and Pablo~A Parrilo.
\newblock Why random reshuffling beats stochastic gradient descent.
\newblock \emph{Mathematical Programming}, 186\penalty0 (1):\penalty0 49--84, 2021.

\bibitem[HaoChen and Sra(2019)]{haochen2019random}
Jeff~Z. HaoChen and Suvrit Sra.
\newblock {Random Shuffling Beats {SGD} after Finite Epochs}.
\newblock In \emph{Proceedings of the International Conference on Machine Learning}, volume~97, pages 2624--2633, 2019.

\bibitem[Harvey and Samadi(2014)]{harvey2014near}
Nick Harvey and Samira Samadi.
\newblock {Near-Optimal Herding}.
\newblock In \emph{Proceedings of The 27th Conference on Learning Theory}, volume~35, pages 1165--1182, 2014.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Huang et~al.(2021)Huang, Li, Milzarek, Pu, and Qiu]{huang2021distributed}
Kun Huang, Xiao Li, Andre Milzarek, Shi Pu, and Junwen Qiu.
\newblock {Distributed Random Reshuffling over Networks}.
\newblock \emph{arXiv preprint arXiv:2112.15287}, 2021.

\bibitem[Inan et~al.(2016)Inan, Khosravi, and Socher]{inan2016tying}
Hakan Inan, Khashayar Khosravi, and Richard Socher.
\newblock Tying word vectors and word classifiers: A loss framework for language modeling.
\newblock \emph{arXiv preprint arXiv:1611.01462}, 2016.

\bibitem[Li et~al.(2014)Li, Andersen, Park, Smola, Ahmed, Josifovski, Long, Shekita, and Su]{li2014ps}
Mu~Li, David~G. Andersen, Jun~Woo Park, Alexander~J. Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene~J. Shekita, and Bor-Yiing Su.
\newblock {Scaling Distributed Machine Learning with the Parameter Server}.
\newblock In \emph{Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation}, OSDI'14, page 583â€“598, USA, 2014. USENIX Association.
\newblock ISBN 9781931971164.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Lu et~al.(2021{\natexlab{a}})Lu, Meng, and De~Sa]{lu2021general}
Yucheng Lu, Si~Yi Meng, and Christopher De~Sa.
\newblock {A General Analysis of Example-Selection for Stochastic Gradient Descent}.
\newblock In \emph{International Conference on Learning Representations}, 2021{\natexlab{a}}.

\bibitem[Lu et~al.(2021{\natexlab{b}})Lu, Park, Chen, Wang, De~Sa, and Foster]{lu2021variance}
Yucheng Lu, Youngsuk Park, Lifan Chen, Yuyang Wang, Christopher De~Sa, and Dean Foster.
\newblock {Variance Reduced Training with Stratified Sampling for Forecasting Models}.
\newblock In \emph{Proceedings of the International Conference on Machine Learning}, pages 7145--7155. PMLR, 2021{\natexlab{b}}.

\bibitem[Lu et~al.(2022)Lu, Guo, and Sa]{lu2022grab}
Yucheng Lu, Wentao Guo, and Christopher~De Sa.
\newblock {GraB: Finding Provably Better Data Permutations than Random Reshuffling}.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=nDemfqKHTpK}.

\bibitem[Makridakis et~al.(2020)Makridakis, Spiliotis, and Assimakopoulos]{MAKRIDAKIS202054}
Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos.
\newblock The m4 competition: 100,000 time series and 61 forecasting methods.
\newblock \emph{International Journal of Forecasting}, 36\penalty0 (1):\penalty0 54--74, 2020.
\newblock ISSN 0169-2070.
\newblock \doi{https://doi.org/10.1016/j.ijforecast.2019.04.014}.
\newblock URL \url{https://www.sciencedirect.com/science/article/pii/S0169207019301128}.
\newblock M4 Competition.

\bibitem[Malinovsky et~al.(2022)Malinovsky, Mishchenko, and Richt{\'a}rik]{malinovsky2022server}
Grigory Malinovsky, Konstantin Mishchenko, and Peter Richt{\'a}rik.
\newblock {Server-Side Stepsizes and Sampling Without Replacement Provably Help in Federated Optimization}.
\newblock \emph{arXiv preprint arXiv:2201.11066}, 2022.

\bibitem[Matiisen et~al.(2019)Matiisen, Oliver, Cohen, and Schulman]{matiisen2019teacher}
Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman.
\newblock Teacher--student curriculum learning.
\newblock \emph{IEEE transactions on neural networks and learning systems}, 31\penalty0 (9):\penalty0 3732--3740, 2019.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and y~Arcas]{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized data.
\newblock In \emph{Artificial intelligence and statistics}, pages 1273--1282. PMLR, 2017.

\bibitem[Merity et~al.(2018)Merity, Keskar, and Socher]{merity2017regularizing}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher.
\newblock Regularizing and optimizing lstm language models.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Mishchenko et~al.(2020)Mishchenko, Khaled, and Richt{\'{a}}rik]{mishchenko2020random}
Konstantin Mishchenko, Ahmed Khaled, and Peter Richt{\'{a}}rik.
\newblock {Random Reshuffling: Simple Analysis with Vast Improvements}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Mohtashami et~al.(2022)Mohtashami, Stich, and Jaggi]{mohtashami2022characterizing}
Amirkeivan Mohtashami, Sebastian Stich, and Martin Jaggi.
\newblock {Characterizing \& Finding Good Data Orderings for Fast Convergence of Sequential Gradient Methods}.
\newblock \emph{arXiv preprint arXiv:2202.01838}, 2022.

\bibitem[Needell et~al.(2014)Needell, Ward, and Srebro]{needell2014stochastic}
Deanna Needell, Rachel Ward, and Nathan Srebro.
\newblock {Stochastic Gradient Descent, Weighted Sampling, and the Randomized Kaczmarz algorithm}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages 1017--1025, 2014.

\bibitem[NVIDIA(2023)]{nccl}
NVIDIA.
\newblock {NVIDIA Collective Communication Library}, 2023.
\newblock URL \url{https://https://developer.nvidia.com/nccl}.

\bibitem[{PyTorch Contributors}(2023)]{pytorchshuffle}
{PyTorch Contributors}.
\newblock {DataLoader API}, 2023.
\newblock URL \url{https://pytorch.org/docs/stable/data.html}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever]{radford2019language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Rajput et~al.(2022)Rajput, Lee, and Papailiopoulos]{rajput2021permutationbased}
Shashank Rajput, Kangwook Lee, and Dimitris Papailiopoulos.
\newblock {Permutation-Based SGD: Is Random Optimal?}
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Recht and R{\'{e}}(2012)]{recht2012toward}
Benjamin Recht and Christopher R{\'{e}}.
\newblock {Toward a Noncommutative Arithmetic-geometric Mean Inequality: Conjectures, Case-studies, and Consequences}.
\newblock In \emph{Conference on Learning Theory}, volume~23, pages 11.1--11.24, 2012.

\bibitem[Sadiev et~al.(2022)Sadiev, Malinovsky, Gorbunov, Sokolov, Khaled, Burlachenko, and Richt{\'a}rik]{sadiev2022federated}
Abdurakhmon Sadiev, Grigory Malinovsky, Eduard Gorbunov, Igor Sokolov, Ahmed Khaled, Konstantin Burlachenko, and Peter Richt{\'a}rik.
\newblock {Federated Optimization Algorithms with Random Reshuffling and Gradient Compression}.
\newblock \emph{arXiv preprint arXiv:2206.07021}, 2022.

\bibitem[Schmidt et~al.(2017)Schmidt, Roux, and Bach]{schmidt2017minimizing}
Mark Schmidt, Nicolas~Le Roux, and Francis~R. Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, 162\penalty0 (1-2):\penalty0 83--112, 2017.

\bibitem[Smith et~al.(2018)Smith, Kindermans, Ying, and Le]{smith2018don}
Samuel~L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc~V Le.
\newblock {Don't Decay the Learning Rate, Increase the Batch Size}.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Soviany et~al.(2022)Soviany, Ionescu, Rota, and Sebe]{soviany2022curriculum}
Petru Soviany, Radu~Tudor Ionescu, Paolo Rota, and Nicu Sebe.
\newblock {Curriculum learning: A survey}.
\newblock \emph{International Journal of Computer Vision}, pages 1--40, 2022.

\bibitem[Spiliotis et~al.(2020)Spiliotis, Kouloumos, Assimakopoulos, and Makridakis]{spiliotis2020forecasting}
Evangelos Spiliotis, Andreas Kouloumos, Vassilios Assimakopoulos, and Spyros Makridakis.
\newblock Are forecasting competitions data representative of the reality?
\newblock \emph{International Journal of Forecasting}, 36\penalty0 (1):\penalty0 37--53, 2020.

\bibitem[Stephen et~al.(2017)Stephen, Caiming, James, and Socher]{stephen2017pointer}
Merity Stephen, Xiong Caiming, Bradbury James, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock \emph{Proceedings of ICLR}, 2017.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman]{wang-etal-2018-glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural language understanding.
\newblock In \emph{Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}}, pages 353--355, Brussels, Belgium, November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/W18-5446}.
\newblock URL \url{https://aclanthology.org/W18-5446}.

\bibitem[Welling(2009)]{welling2009herding}
Max Welling.
\newblock Herding dynamical weights to learn.
\newblock In \emph{Proceedings of the 26th Annual International Conference on Machine Learning}, pages 1121--1128, 2009.

\bibitem[Ying et~al.(2017)Ying, Yuan, Vlaski, and Sayed]{ying2017performance}
Bicheng Ying, Kun Yuan, Stefan Vlaski, and Ali~H. Sayed.
\newblock On the performance of random reshuffling in stochastic learning.
\newblock In \emph{2017 Information Theory and Applications Workshop (ITA)}, pages 1--5. IEEE, 2017.

\bibitem[Yuan et~al.(2022)Yuan, He, Davis, Zhang, Dao, Chen, Liang, Re, and Zhang]{yuan2022decentralized}
Binhang Yuan, Yongjun He, Jared Davis, Tianyi Zhang, Tri Dao, Beidi Chen, Percy~S Liang, Christopher Re, and Ce~Zhang.
\newblock Decentralized training of foundation models in heterogeneous environments.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 25464--25477, 2022.

\bibitem[Yun et~al.(2021{\natexlab{a}})Yun, Rajput, and Sra]{yun2021minibatch}
Chulhee Yun, Shashank Rajput, and Suvrit Sra.
\newblock {Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond}.
\newblock In \emph{International Conference on Learning Representations}, 2021{\natexlab{a}}.

\bibitem[Yun et~al.(2021{\natexlab{b}})Yun, Sra, and Jadbabaie]{yun2021can}
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie.
\newblock {Open Problem: Can Single-Shuffle {SGD} be Better than Reshuffling {SGD} and {GD}?}
\newblock In \emph{Conference on Learning Theory}, 2021{\natexlab{b}}.

\end{thebibliography}
