\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu \& Hazan(2016)Allen-Zhu and Hazan]{allen2016variance}
Allen-Zhu, Z. and Hazan, E.
\newblock Variance reduction for faster non-convex optimization.
\newblock In \emph{ICML}, pp.\  699--707, 2016.

\bibitem[Arjevani et~al.(2019)Arjevani, Carmon, Duchi, Foster, Srebro, and
  Woodworth]{arjevani2019lower}
Arjevani, Y., Carmon, Y., Duchi, J.~C., Foster, D.~J., Srebro, N., and
  Woodworth, B.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1912.02365}, 2019.

\bibitem[Baxter \& Bartlett(2001)Baxter and Bartlett]{baxter2001infinite}
Baxter, J. and Bartlett, P.~L.
\newblock Infinite-horizon policy-gradient estimation.
\newblock \emph{Journal of Artificial Intelligence Research}, 15:\penalty0
  319--350, 2001.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{gym}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym, 2016.

\bibitem[Cheng et~al.(2019{\natexlab{a}})Cheng, Yan, and
  Boots]{cheng2019trajectory}
Cheng, C.-A., Yan, X., and Boots, B.
\newblock Trajectory-wise control variates for variance reduction in policy
  gradient methods.
\newblock \emph{arXiv preprint arXiv:1908.03263}, 2019{\natexlab{a}}.

\bibitem[Cheng et~al.(2019{\natexlab{b}})Cheng, Yan, Ratliff, and
  Boots]{cheng2019predictor}
Cheng, C.-A., Yan, X., Ratliff, N., and Boots, B.
\newblock Predictor-corrector policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1151--1161, 2019{\natexlab{b}}.

\bibitem[Cortes et~al.(2010)Cortes, Mansour, and Mohri]{cortes2010learning}
Cortes, C., Mansour, Y., and Mohri, M.
\newblock Learning bounds for importance weighting.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  442--450, 2010.

\bibitem[Cutkosky \& Orabona(2019)Cutkosky and Orabona]{cutkosky2019momentum}
Cutkosky, A. and Orabona, F.
\newblock Momentum-based variance reduction in non-convex sgd.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  15210--15219, 2019.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Defazio, A., Bach, F., and Lacoste-Julien, S.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1646--1654, 2014.

\bibitem[Deisenroth et~al.(2013)Deisenroth, Neumann, Peters,
  et~al.]{deisenroth2013survey}
Deisenroth, M.~P., Neumann, G., Peters, J., et~al.
\newblock A survey on policy search for robotics.
\newblock \emph{Foundations and Trends{\textregistered} in Robotics},
  2\penalty0 (1--2):\penalty0 1--142, 2013.

\bibitem[Du et~al.(2017)Du, Chen, Li, Xiao, and Zhou]{du2017stochastic}
Du, S.~S., Chen, J., Li, L., Xiao, L., and Zhou, D.
\newblock Stochastic variance reduction methods for policy evaluation.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1049--1058. JMLR. org, 2017.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018spider}
Fang, C., Li, C.~J., Lin, Z., and Zhang, T.
\newblock Spider: Near-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  689--699, 2018.

\bibitem[Fellows et~al.(2018)Fellows, Ciosek, and Whiteson]{fellows2018fourier}
Fellows, M., Ciosek, K., and Whiteson, S.
\newblock Fourier policy gradients.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1486--1495, 2018.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and
  Meger]{fujimoto2018addressing}
Fujimoto, S., Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{ICML}, pp.\  1587--1596, 2018.

\bibitem[Furmston et~al.(2016)Furmston, Lever, and
  Barber]{furmston2016approximate}
Furmston, T., Lever, G., and Barber, D.
\newblock Approximate newton methods for policy search in markov decision
  processes.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 8055--8105, 2016.

\bibitem[garage contributors(2019)]{garage}
garage contributors, T.
\newblock Garage: A toolkit for reproducible reinforcement learning research.
\newblock \url{https://github.com/rlworkgroup/garage}, 2019.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013stochastic}
Ghadimi, S. and Lan, G.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Greensmith et~al.(2004)Greensmith, Bartlett, and
  Baxter]{greensmith2004variance}
Greensmith, E., Bartlett, P.~L., and Baxter, J.
\newblock Variance reduction techniques for gradient estimates in reinforcement
  learning.
\newblock \emph{Journal of Machine Learning Research}, 5\penalty0
  (Nov):\penalty0 1471--1530, 2004.

\bibitem[Gu et~al.(2016)Gu, Lillicrap, Ghahramani, Turner, and Levine]{gu2016q}
Gu, S., Lillicrap, T., Ghahramani, Z., Turner, R.~E., and Levine, S.
\newblock Q-prop: Sample-efficient policy gradient with an off-policy critic.
\newblock \emph{arXiv preprint arXiv:1611.02247}, 2016.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1861--1870, 2018.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{johnson2013accelerating}
Johnson, R. and Zhang, T.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{NIPS}, pp.\  315--323, 2013.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Li(2017)]{li2017deep}
Li, Y.
\newblock Deep reinforcement learning: An overview.
\newblock \emph{arXiv preprint arXiv:1701.07274}, 2017.

\bibitem[Mao et~al.(2018)Mao, Venkatakrishnan, Schwarzkopf, and
  Alizadeh]{mao2018variance}
Mao, H., Venkatakrishnan, S.~B., Schwarzkopf, M., and Alizadeh, M.
\newblock Variance reduction for reinforcement learning in input-driven
  environments.
\newblock \emph{arXiv preprint arXiv:1807.02264}, 2018.

\bibitem[Metelli et~al.(2018)Metelli, Papini, Faccio, and
  Restelli]{metelli2018policy}
Metelli, A.~M., Papini, M., Faccio, F., and Restelli, M.
\newblock Policy optimization via importance sampling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5442--5454, 2018.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{nguyen2017sarah}
Nguyen, L.~M., Liu, J., Scheinberg, K., and Tak{\'a}{\v{c}}, M.
\newblock Sarah: A novel method for machine learning problems using stochastic
  recursive gradient.
\newblock In \emph{ICML}, pp.\  2613--2621, 2017.

\bibitem[Palaniappan \& Bach(2016)Palaniappan and
  Bach]{palaniappan2016stochastic}
Palaniappan, B. and Bach, F.
\newblock Stochastic variance reduction methods for saddle-point problems.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1416--1424, 2016.

\bibitem[Papini et~al.(2018)Papini, Binaghi, Canonaco, Pirotta, and
  Restelli]{papini2018stochastic}
Papini, M., Binaghi, D., Canonaco, G., Pirotta, M., and Restelli, M.
\newblock Stochastic variance-reduced policy gradient.
\newblock In \emph{35th International Conference on Machine Learning},
  volume~80, pp.\  4026--4035, 2018.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8024--8035, 2019.

\bibitem[Peters \& Schaal(2008)Peters and Schaal]{peters2008reinforcement}
Peters, J. and Schaal, S.
\newblock Reinforcement learning of motor skills with policy gradients.
\newblock \emph{Neural networks}, 21\penalty0 (4):\penalty0 682--697, 2008.

\bibitem[Pham et~al.(2020)Pham, Nguyen, Phan, Nguyen, van Dijk, and
  Tran-Dinh]{pham2020hybrid}
Pham, N.~H., Nguyen, L.~M., Phan, D.~T., Nguyen, P.~H., van Dijk, M., and
  Tran-Dinh, Q.
\newblock A hybrid stochastic policy gradient algorithm for reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2003.00430}, 2020.

\bibitem[Pirotta et~al.(2013)Pirotta, Restelli, and
  Bascetta]{pirotta2013adaptive}
Pirotta, M., Restelli, M., and Bascetta, L.
\newblock Adaptive step-size for policy gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1394--1402, 2013.

\bibitem[Reddi et~al.(2016)Reddi, Hefny, Sra, Poczos, and
  Smola]{reddi2016stochastic}
Reddi, S.~J., Hefny, A., Sra, S., Poczos, B., and Smola, A.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  314--323, 2016.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{robbins1951stochastic}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pp.\  400--407, 1951.

\bibitem[Schulman et~al.(2015{\natexlab{a}})Schulman, Levine, Abbeel, Jordan,
  and Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1889--1897, 2015{\natexlab{a}}.

\bibitem[Schulman et~al.(2015{\natexlab{b}})Schulman, Moritz, Levine, Jordan,
  and Abbeel]{schulman2015high}
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock \emph{arXiv preprint arXiv:1506.02438}, 2015{\natexlab{b}}.

\bibitem[Shalev-Shwartz et~al.(2016)Shalev-Shwartz, Shammah, and
  Shashua]{shalev2016safe}
Shalev-Shwartz, S., Shammah, S., and Shashua, A.
\newblock Safe, multi-agent, reinforcement learning for autonomous driving.
\newblock \emph{arXiv preprint arXiv:1610.03295}, 2016.

\bibitem[Shen et~al.(2019)Shen, Ribeiro, Hassani, Qian, and
  Mi]{shen2019hessian}
Shen, Z., Ribeiro, A., Hassani, H., Qian, H., and Mi, C.
\newblock Hessian aided policy gradient.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5729--5738, 2019.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
  A., Hubert, T., Baker, L., Lai, M., Bolton, A., et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{sutton2000policy}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1057--1063, 2000.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{IEEE/RSJ International Conference on Intelligent Robots and
  Systems}, pp.\  5026--5033, 2012.

\bibitem[Tran-Dinh et~al.(2019)Tran-Dinh, Pham, Phan, and
  Nguyen]{tran2019hybrid}
Tran-Dinh, Q., Pham, N.~H., Phan, D.~T., and Nguyen, L.~M.
\newblock A hybrid stochastic optimization framework for stochastic composite
  nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1907.03793}, 2019.

\bibitem[Wai et~al.(2019)Wai, Hong, Yang, Wang, and Tang]{wai2019variance}
Wai, H.-T., Hong, M., Yang, Z., Wang, Z., and Tang, K.
\newblock Variance reduced policy evaluation with smooth function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5776--5787, 2019.

\bibitem[Wang et~al.(2019{\natexlab{a}})Wang, Cai, Yang, and
  Wang]{wang2019neural}
Wang, L., Cai, Q., Yang, Z., and Wang, Z.
\newblock Neural policy gradient methods: Global optimality and rates of
  convergence.
\newblock \emph{arXiv preprint arXiv:1909.01150}, 2019{\natexlab{a}}.

\bibitem[Wang et~al.(2018)Wang, Li, and He]{wang2018deep}
Wang, W.~Y., Li, J., and He, X.
\newblock Deep reinforcement learning for nlp.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics: Tutorial Abstracts}, pp.\  19--21, 2018.

\bibitem[Wang et~al.(2019{\natexlab{b}})Wang, Ji, Zhou, Liang, and
  Tarokh]{wang2019spiderboost}
Wang, Z., Ji, K., Zhou, Y., Liang, Y., and Tarokh, V.
\newblock Spiderboost and momentum: Faster variance reduction algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2403--2413, 2019{\natexlab{b}}.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 229--256, 1992.

\bibitem[Wu et~al.(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade, Mordatch,
  and Abbeel]{wu2018variance}
Wu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A.~M., Kakade, S.,
  Mordatch, I., and Abbeel, P.
\newblock Variance reduction for policy gradient with action-dependent
  factorized baselines.
\newblock \emph{arXiv preprint arXiv:1803.07246}, 2018.

\bibitem[Xiong et~al.(2020)Xiong, Xu, Liang, and Zhang]{xiong2020non}
Xiong, H., Xu, T., Liang, Y., and Zhang, W.
\newblock Non-asymptotic convergence of adam-type reinforcement learning
  algorithms under markovian sampling.
\newblock \emph{arXiv preprint arXiv:2002.06286}, 2020.

\bibitem[Xu et~al.(2019{\natexlab{a}})Xu, Gao, and Gu]{xu2019improved}
Xu, P., Gao, F., and Gu, Q.
\newblock An improved convergence analysis of stochastic variance-reduced
  policy gradient.
\newblock In \emph{Proceedings of the Thirty-Fifth Conference on Uncertainty in
  Artificial Intelligence}, pp.\  191, 2019{\natexlab{a}}.

\bibitem[Xu et~al.(2019{\natexlab{b}})Xu, Gao, and Gu]{xu2019sample}
Xu, P., Gao, F., and Gu, Q.
\newblock Sample efficient policy gradient methods with recursive variance
  reduction.
\newblock \emph{arXiv preprint arXiv:1909.08610}, 2019{\natexlab{b}}.

\bibitem[Xu et~al.(2017)Xu, Liu, and Peng]{xu2017stochastic}
Xu, T., Liu, Q., and Peng, J.
\newblock Stochastic variance reduction for policy gradient estimation.
\newblock \emph{arXiv preprint arXiv:1710.06034}, 2017.

\bibitem[Yuan et~al.(2020)Yuan, Lian, Liu, and Zhou]{yuan2020stochastic}
Yuan, H., Lian, X., Liu, J., and Zhou, Y.
\newblock Stochastic recursive momentum for policy gradient methods.
\newblock \emph{arXiv preprint arXiv:2003.04302}, 2020.

\bibitem[Zhou et~al.(2018)Zhou, Xu, and Gu]{zhou2018stochastic}
Zhou, D., Xu, P., and Gu, Q.
\newblock Stochastic nested variance reduction for nonconvex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3921--3932, 2018.

\end{thebibliography}
