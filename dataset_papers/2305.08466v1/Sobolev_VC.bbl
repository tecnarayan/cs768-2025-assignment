\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Czarnecki et~al.(2017)Czarnecki, Osindero, Jaderberg, Swirszcz, and
  Pascanu]{czarnecki2017sobolev}
W.~Czarnecki, S.~Osindero, M.~Jaderberg, G.~Swirszcz, and R.~Pascanu.
\newblock Sobolev training for neural networks.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Son et~al.(2021)Son, Jang, Han, and Hwang]{son2021sobolev}
H.~Son, J.~Jang, W.~Han, and H.~Hwang.
\newblock Sobolev training for the neural network solutions of {PDEs}.
\newblock \emph{arXiv preprint arXiv:2101.08932}, 2021.

\bibitem[Vlassis and Sun(2021)]{vlassis2021sobolev}
N.~Vlassis and W.~Sun.
\newblock Sobolev training of thermodynamic-informed neural networks for
  interpretable elasto-plasticity models with level set hardening.
\newblock \emph{Computer Methods in Applied Mechanics and Engineering},
  377:\penalty0 113695, 2021.

\bibitem[Lagaris et~al.(1998)Lagaris, Likas, and Fotiadis]{Lagaris1998}
I.~Lagaris, A.~Likas, and D.~Fotiadis.
\newblock Artificial neural networks for solving ordinary and partial
  differential equations.
\newblock \emph{IEEE Transactions on Neural Networks}, 9\penalty0 (5):\penalty0
  987â€“1000, 1998.

\bibitem[E et~al.(2017)E, Han, and Jentzen]{weinan2017deep}
W.~E, J.~Han, and A.~Jentzen.
\newblock Deep learning-based numerical methods for high-dimensional parabolic
  partial differential equations and backward stochastic differential
  equations.
\newblock \emph{Communications in Mathematics and Statistics}, 5\penalty0
  (4):\penalty0 349--380, 2017.

\bibitem[Raissi et~al.(2019)Raissi, Perdikaris, and
  Karniadakis]{raissi2019physics}
M.~Raissi, P.~Perdikaris, and G.~Karniadakis.
\newblock Physics-informed neural networks: A deep learning framework for
  solving forward and inverse problems involving nonlinear partial differential
  equations.
\newblock \emph{Journal of Computational Physics}, 378:\penalty0 686--707,
  2019.

\bibitem[Lu et~al.(2021{\natexlab{a}})Lu, Jin, Pang, Zhang, and
  Karniadakis]{lu2021learning}
L.~Lu, P.~Jin, G.~Pang, Z.~Zhang, and G.~Karniadakis.
\newblock Learning nonlinear operators via {DeepONet} based on the universal
  approximation theorem of operators.
\newblock \emph{Nature machine intelligence}, 3\penalty0 (3):\penalty0
  218--229, 2021{\natexlab{a}}.

\bibitem[Liu et~al.(2022)Liu, Yang, Chen, Zhao, and Liao]{liu2022deep}
H.~Liu, H.~Yang, M.~Chen, T.~Zhao, and W.~Liao.
\newblock Deep nonparametric estimation of operators between infinite
  dimensional spaces.
\newblock \emph{arXiv preprint arXiv:2201.00217}, 2022.

\bibitem[Sau and Balasubramanian(2016)]{sau2016deep}
B.~Sau and V.~Balasubramanian.
\newblock Deep model compression: Distilling knowledge from noisy teachers.
\newblock \emph{arXiv preprint arXiv:1610.09650}, 2016.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
G.~Hinton, O.~Vinyals, and J.~Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Rusu et~al.(2015)Rusu, Colmenarejo, Gulcehre, Desjardins, Kirkpatrick,
  Pascanu, Mnih, Kavukcuoglu, and Hadsell]{rusu2015policy}
A.~A Rusu, S.~Colmenarejo, C.~Gulcehre, G.~Desjardins, J.~Kirkpatrick,
  R.~Pascanu, V.~Mnih, K.~Kavukcuoglu, and R.~Hadsell.
\newblock Policy distillation.
\newblock \emph{arXiv preprint arXiv:1511.06295}, 2015.

\bibitem[Finlay et~al.(2018)Finlay, Calder, Abbasi, and
  Oberman]{finlay2018lipschitz}
C.~Finlay, J.~Calder, B.~Abbasi, and A.~Oberman.
\newblock Lipschitz regularized deep neural networks generalize and are
  adversarially robust.
\newblock \emph{arXiv preprint arXiv:1808.09540}, 2018.

\bibitem[Werbos(1992)]{werbos1992approximate}
P.~Werbos.
\newblock Approximate dynamic programming for real-time control and neural
  modeling.
\newblock \emph{Handbook of intelligent control}, 1992.

\bibitem[Adler and Lunz(2018)]{adler2018banach}
J.~Adler and S.~Lunz.
\newblock Banach wasserstein {Gan}.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Gu and Rigazio(2014)]{gu2014towards}
S.~Gu and L.~Rigazio.
\newblock Towards deep neural network architectures robust to adversarial
  examples.
\newblock \emph{arXiv preprint arXiv:1412.5068}, 2014.

\bibitem[Mroueh et~al.(2018)Mroueh, Li, Sercu, Raj, and
  Cheng]{mroueh2018sobolev}
Y.~Mroueh, C.~Li, T.~Sercu, A.~Raj, and Y.~Cheng.
\newblock {Sobolev Gan}.
\newblock In \emph{International Conference on Learning Representations}.
  International Conference on Learning Representations, ICLR, 2018.

\bibitem[Anthony et~al.(1999)Anthony, Bartlett, et~al.]{anthony1999neural}
M.~Anthony, P.~Bartlett, et~al.
\newblock \emph{Neural network learning: Theoretical foundations}, volume~9.
\newblock cambridge university press Cambridge, 1999.

\bibitem[Abu-Mostafa(1989)]{abu1989vapnik}
Y.~Abu-Mostafa.
\newblock The {Vapnik-Chervonenkis} dimension: Information versus complexity in
  learning.
\newblock \emph{Neural Computation}, 1\penalty0 (3):\penalty0 312--317, 1989.

\bibitem[Pollard(1990)]{pollard1990empirical}
D.~Pollard.
\newblock Empirical processes: theory and applications.
\newblock Ims, 1990.

\bibitem[Goldberg and Jerrum(1993)]{goldberg1993bounding}
P.~Goldberg and M.~Jerrum.
\newblock Bounding the {Vapnik-Chervonenkis} dimension of concept classes
  parameterized by real numbers.
\newblock In \emph{Proceedings of the sixth annual conference on Computational
  learning theory}, pages 361--369, 1993.

\bibitem[Bartlett et~al.(1998)Bartlett, Maiorov, and Meir]{bartlett1998almost}
P.~Bartlett, V.~Maiorov, and R.~Meir.
\newblock Almost linear {VC} dimension bounds for piecewise polynomial
  networks.
\newblock \emph{Advances in neural information processing systems}, 11, 1998.

\bibitem[Bartlett et~al.(2019)Bartlett, Harvey, Liaw, and
  Mehrabian]{bartlett2019nearly}
P.~Bartlett, N.~Harvey, C.~Liaw, and A.~Mehrabian.
\newblock Nearly-tight {VC-dimension} and pseudodimension bounds for piecewise
  linear neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 20\penalty0
  (1):\penalty0 2285--2301, 2019.

\bibitem[Blumer et~al.(1989)Blumer, Ehrenfeucht, Haussler, and
  Warmuth]{blumer1989learnability}
A.~Blumer, A.~Ehrenfeucht, D.~Haussler, and M.~Warmuth.
\newblock {Learnability and the Vapnik-Chervonenkis} dimension.
\newblock \emph{Journal of the ACM (JACM)}, 36\penalty0 (4):\penalty0 929--965,
  1989.

\bibitem[Duan et~al.(2021)Duan, Jiao, Lai, Lu, and Yang]{duan2021convergence}
C.~Duan, Y.~Jiao, Y.~Lai, X.~Lu, and Z.~Yang.
\newblock Convergence rate analysis for {Deep Ritz} method.
\newblock \emph{arXiv preprint arXiv:2103.13330}, 2021.

\bibitem[Farrell et~al.(2021)Farrell, Liang, and Misra]{farrell2021deep}
M.~Farrell, T.~Liang, and S.~Misra.
\newblock Deep neural networks for estimation and inference.
\newblock \emph{Econometrica}, 89\penalty0 (1):\penalty0 181--213, 2021.

\bibitem[Evans(2022)]{evans2022partial}
L.~Evans.
\newblock \emph{Partial differential equations}, volume~19.
\newblock American Mathematical Society, 2022.

\bibitem[Brenner et~al.(2008)Brenner, Scott, and
  Scott]{brenner2008mathematical}
S.~Brenner, L.~Scott, and L.~Scott.
\newblock \emph{The mathematical theory of finite element methods}, volume~3.
\newblock Springer, 2008.

\bibitem[Lu et~al.(2021{\natexlab{b}})Lu, Shen, Yang, and Zhang]{lu2021deep}
J.~Lu, Z.~Shen, H.~Yang, and S.~Zhang.
\newblock Deep network approximation for smooth functions.
\newblock \emph{SIAM Journal on Mathematical Analysis}, 53\penalty0
  (5):\penalty0 5465--5506, 2021{\natexlab{b}}.

\bibitem[Hon and Yang(2022)]{hon2022simultaneous}
S.~Hon and H.~Yang.
\newblock Simultaneous neural network approximation for smooth functions.
\newblock \emph{Neural Networks}, 154:\penalty0 152--164, 2022.

\bibitem[Siegel(2022)]{siegel2022optimal}
J.~Siegel.
\newblock Optimal approximation rates for deep {ReLU} neural networks on
  {Sobolev} spaces.
\newblock \emph{arXiv preprint arXiv:2211.14400}, 2022.

\bibitem[G{\"u}hring et~al.(2020)G{\"u}hring, Kutyniok, and
  Petersen]{guhring2020error}
I.~G{\"u}hring, G.~Kutyniok, and P.~Petersen.
\newblock Error bounds for approximations with deep {ReLU} neural networks in
  ${W}^{s, p}$ norms.
\newblock \emph{Analysis and Applications}, 18\penalty0 (05):\penalty0
  803--859, 2020.

\bibitem[M{\"u}ller and Zeinhofer(2022)]{muller2022error}
J.~M{\"u}ller and M.~Zeinhofer.
\newblock Error estimates for the {Deep Ritz} method with boundary penalty.
\newblock In \emph{Mathematical and Scientific Machine Learning}, pages
  215--230. PMLR, 2022.

\bibitem[De~Ryck et~al.(2021)De~Ryck, Lanthaler, and
  Mishra]{de2021approximation}
T.~De~Ryck, S.~Lanthaler, and S.~Mishra.
\newblock On the approximation of functions by tanh neural networks.
\newblock \emph{Neural Networks}, 143:\penalty0 732--750, 2021.

\bibitem[G{\"u}hring and Raslan(2021)]{guhring2021approximation}
I.~G{\"u}hring and M.~Raslan.
\newblock Approximation rates for neural networks with encodable weights in
  smoothness spaces.
\newblock \emph{Neural Networks}, 134:\penalty0 107--130, 2021.

\bibitem[Jiao et~al.(2021)Jiao, Lai, Lo, Wang, and Yang]{jiao2021error}
Y.~Jiao, Y.~Lai, Y.~Lo, Y.~Wang, and Y.~Yang.
\newblock Error analysis of {Deep Ritz} methods for elliptic equations.
\newblock \emph{arXiv preprint arXiv:2107.14478}, 2021.

\bibitem[Vapnik and Chervonenkis(2015)]{vapnik2015uniform}
V.~Vapnik and A.~Chervonenkis.
\newblock On the uniform convergence of relative frequencies of events to their
  probabilities.
\newblock \emph{Measures of complexity: festschrift for alexey chervonenkis},
  pages 11--30, 2015.

\bibitem[Stein(1970)]{stein1970singular}
E.~Stein.
\newblock \emph{Singular integrals and differentiability properties of
  functions}, volume~2.
\newblock Princeton university press, 1970.

\bibitem[Shen et~al.(2022)Shen, Yang, and Zhang]{shen2022optimal}
Z.~Shen, H.~Yang, and S.~Zhang.
\newblock Optimal approximation rate of {ReLU} networks in terms of width and
  depth.
\newblock \emph{Journal de Math{\'e}matiques Pures et Appliqu{\'e}es},
  157:\penalty0 101--135, 2022.

\bibitem[Shen et~al.(2020)Shen, Yang, and Zhang]{shen2020deep}
Z.~Shen, H.~Yang, and S.~Zhang.
\newblock Deep network approximation characterized by number of neurons.
\newblock \emph{Communications in Computational Physics}, 28\penalty0 (5),
  2020.

\bibitem[Shen et~al.(2019)Shen, Yang, and Zhang]{shen2019nonlinear}
Z.~Shen, H.~Yang, and S.~Zhang.
\newblock Nonlinear approximation via compositions.
\newblock \emph{Neural Networks}, 119:\penalty0 74--84, 2019.

\bibitem[Wainwright(2019)]{wainwright2019high}
M.~Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge university press, 2019.

\end{thebibliography}
