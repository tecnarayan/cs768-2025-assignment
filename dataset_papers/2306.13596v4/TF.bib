%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{ICL_GD_google,
  title={Transformers learn in-context by gradient descent},
  author={von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  journal={arXiv:2212.07677},
  year={2022}
}
@article{tian2023scan,
  title={Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer},
  author={Tian, Yuandong and Wang, Yiping and Chen, Beidi and Du, Simon},
  journal={arXiv:2305.16380},
  year={2023}
}
@article{akyurek2022learning,
  title={What learning algorithm is in-context learning? investigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  journal={arXiv:2211.15661},
  year={2022}
}
@inproceedings{ICLRsub2022,
  title={A theoretical understanding of vision transformers: Learning, generalization, and sample complexity},
  author={Li, Hongkang and Wang, Meng and Liu, Sijia and Chen, Pin-Yu},
  booktitle={International Conference on Learning Representations},
  year={2023}
}
@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}
@inproceedings{oymak2019overparameterized,
  title={Overparameterized nonlinear learning: Gradient descent takes the shortest path?},
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  booktitle={International Conference on Machine Learning},
  pages={4951--4960},
  year={2019},
  organization={PMLR}
}
@article{giannou2023looped,
  title={Looped transformers as programmable computers},
  author={Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D and Papailiopoulos, Dimitris},
  journal={arXiv:2301.13196},
  year={2023}
}
@article{heckel2019denoising,
  title={Denoising and regularization via exploiting the structural bias of convolutional generators},
  author={Heckel, Reinhard and Soltanolkotabi, Mahdi},
  journal={arXiv:1910.14634},
  year={2019}
}
@article{soltanolkotabi2023implicit,
  title={Implicit Balancing and Regularization: Generalization and Convergence Guarantees for Overparameterized Asymmetric Matrix Sensing},
  author={Soltanolkotabi, Mahdi and St{\"o}ger, Dominik and Xie, Changzhi},
  journal={arXiv:2303.14244},
  year={2023}
}
@article{azizan2021stochastic,
  title={Stochastic mirror descent on overparameterized nonlinear models},
  author={Azizan, Navid and Lale, Sahin and Hassibi, Babak},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={33},
  number={12},
  pages={7717--7727},
  year={2021},
  publisher={IEEE}
}
@inproceedings{taheri2023generalization,
  title={On Generalization of Decentralized Learning with Separable Data},
  author={Taheri, Hossein and Thrampoulidis, Christos},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4917--4945},
  year={2023},
  organization={PMLR}
}
@article{kini2021label,
  title={Label-imbalanced and group-sensitive classification under overparameterization},
  author={Kini, Ganesh Ramachandra and Paraskevas, Orestis and Oymak, Samet and Thrampoulidis, Christos},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18970--18983},
  year={2021}
}
@inproceedings{azizanstochastic,
  title={Stochastic Gradient/Mirror Descent: Minimax Optimality and Implicit Regularization},
  author={Azizan, Navid and Hassibi, Babak},
  booktitle={International Conference on Learning Representations}
}
@article{li2023dissecting,
  title={Dissecting Chain-of-Thought: A Study on Compositional In-Context Learning of MLPs},
  author={Li, Yingcong and Sreenivasan, Kartik and Giannou, Angeliki and Papailiopoulos, Dimitris and Oymak, Samet},
  journal={arXiv preprint arXiv:2305.18869},
  year={2023}
}
@article{feng2023towards,
  title={Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective},
  author={Feng, Guhao and Gu, Yuntian and Zhang, Bohang and Ye, Haotian and He, Di and Wang, Liwei},
  journal={arXiv preprint arXiv:2305.15408},
  year={2023}
}
@inproceedings{li2023transformers,
  title={Transformers as algorithms: Generalization and stability in in-context learning},
  author={Li, Yingcong and Ildiz, M Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  booktitle={International Conference on Machine Learning},
  year={2023}
}
@article{garg2022can,
  title={What can transformers learn in-context? a case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30583--30598},
  year={2022}
}
%%%%%%%%%%%% 
@inproceedings{ICL_regression,
  title={What Can Transformers Learn In-Context? A Case Study of Simple Function Classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy and Valiant, Gregory},
  booktitle={Advances in Neural Information Processing Systems}
}

@inproceedings{ICL_tabular,
  title={TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second},
  author={Hollmann, Noah and M{\"u}ller, Samuel and Eggensperger, Katharina and Hutter, Frank},
  booktitle={NeurIPS 2022 First Table Representation Workshop}
}

@inproceedings{ICL_Bayesian,
  title={An Explanation of In-context Learning as Implicit Bayesian Inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  booktitle={International Conference on Learning Representations},
  year={2021}
}



@inproceedings{velivckovic2018graph,
  title={Graph Attention Networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{Jelassi,
  title={Vision Transformers provably learn spatial structure},
  author={Jelassi, Samy and Sander, Michael Eli and Li, Yuanzhi},
  booktitle={Advances in Neural Information Processing Systems}
}

@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}


@inproceedings{hopefullysoon,
  title={Attention Mechanism Enables Gradient-based Feature Selection},
  author={Oymak, Samet and Rawat, Ankit S and Soltanolkotabi, Mahdi and Thrampoulidis, Christos},
  booktitle={in preparation},
  year={2023}
}

@inproceedings{peng2020random,
  title={Random Feature Attention},
  author={Peng, Hao and Pappas, Nikolaos and Yogatama, Dani and Schwartz, Roy and Smith, Noah and Kong, Lingpeng},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{choromanski2020rethinking,
  title={Rethinking Attention with Performers},
  author={Choromanski, Krzysztof Marcin and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared Quincy and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}


@article{ergen2022convexifying,
  title={Convexifying Transformers: Improving optimization and understanding of transformer networks},
  author={Ergen, Tolga and Neyshabur, Behnam and Mehta, Harsh},
  journal={arXiv:2211.11052},
  year={2022}
}

@article{liu2022transformers,
  title={Transformers Learn Shortcuts to Automata},
  author={Liu, Bingbin and Ash, Jordan T and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
  journal={arXiv preprint arXiv:2210.10749},
  year={2022}
}



@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}



@article{weidinger2021ethical,
  title={Ethical and social risks of harm from language models},
  author={Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and others},
  journal={arXiv preprint arXiv:2112.04359},
  year={2021}
}




@article{mahdavi2022towards,
  title={Towards Better Out-of-Distribution Generalization of Neural Algorithmic Reasoning Tasks},
  author={Mahdavi, Sadegh and Swersky, Kevin and Kipf, Thomas and Hashemi, Milad and Thrampoulidis, Christos and Liao, Renjie},
  journal={arXiv preprint arXiv:2211.00692},
  year={2022}
}

@inproceedings{sun2020understanding,
  title={Understanding attention for text classification},
  author={Sun, Xiaobing and Lu, Wei},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={3418--3428},
  year={2020}
}



@inproceedings{yu2022metaformer,
  title={Metaformer is actually what you need for vision},
  author={Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10819--10829},
  year={2022}
}
@article{bartlett2002rademacher,
  title={Rademacher and Gaussian complexities: Risk bounds and structural results},
  author={Bartlett, Peter L and Mendelson, Shahar},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Nov},
  pages={463--482},
  year={2002}
}
@article{hanneke2019value,
  title={On the value of target data in transfer learning},
  author={Hanneke, Steve and Kpotufe, Samory},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{bartlett2002localized,
  title={Localized rademacher complexities},
  author={Bartlett, Peter L and Bousquet, Olivier and Mendelson, Shahar},
  booktitle={International Conference on Computational Learning Theory},
  pages={44--58},
  year={2002},
  organization={Springer}
}
@article{wang2018distributed,
  title={Distributed stochastic multi-task learning with graph regularization},
  author={Wang, Weiran and Wang, Jialei and Kolar, Mladen and Srebro, Nathan},
  journal={arXiv preprint arXiv:1802.03830},
  year={2018}
}
@article{pan2009survey,
  title={A survey on transfer learning},
  author={Pan, Sinno Jialin and Yang, Qiang},
  journal={IEEE Transactions on knowledge and data engineering},
  volume={22},
  number={10},
  pages={1345--1359},
  year={2009},
  publisher={IEEE}
}
@article{he2020deberta,
  title={Deberta: Decoding-enhanced bert with disentangled attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2006.03654},
  year={2020}
}
@article{reed2022generalist,
  title={A generalist agent},
  author={Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and others},
  journal={arXiv preprint arXiv:2205.06175},
  year={2022}
}
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{mingpt,
  title={minGPT},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={\url{https://github.com/\%20karpathy/minGPT}},
  year={2020}
}
@article{tolstikhin2021mlp,
  title={Mlp-mixer: An all-mlp architecture for vision},
  author={Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24261--24272},
  year={2021}
}
@article{guibas2021adaptive,
  title={Adaptive fourier neural operators: Efficient token mixers for transformers},
  author={Guibas, John and Mardani, Morteza and Li, Zongyi and Tao, Andrew and Anandkumar, Anima and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2111.13587},
  year={2021}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}
@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}
@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}
@article{liu2021pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={arXiv preprint arXiv:2107.13586},
  year={2021}
}
@article{gu2021ppt,
  title={Ppt: Pre-trained prompt tuning for few-shot learning},
  author={Gu, Yuxian and Han, Xu and Liu, Zhiyuan and Huang, Minlie},
  journal={arXiv preprint arXiv:2109.04332},
  year={2021}
}
@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}
@article{wu2021autoformer,
  title={Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting},
  author={Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22419--22430},
  year={2021}
}
@inproceedings{wu2021cvt,
  title={Cvt: Introducing convolutions to vision transformers},
  author={Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={22--31},
  year={2021}
}
@inproceedings{dai2019transformer,
  title={Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G and Le, Quoc and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2978--2988},
  year={2019}
}
@inproceedings{tay2020sparse,
  title={Sparse sinkhorn attention},
  author={Tay, Yi and Bahri, Dara and Yang, Liu and Metzler, Donald and Juan, Da-Cheng},
  booktitle={International Conference on Machine Learning},
  pages={9438--9447},
  year={2020},
  organization={PMLR}
}
@article{zheng2022linear,
  title={Linear Complexity Randomized Self-attention Mechanism},
  author={Zheng, Lin and Wang, Chong and Kong, Lingpeng},
  journal={International Conference on Machine Learning},
  year={2022}
}
@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10012--10022},
  year={2021}
}
@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}
@article{khan2021transformers,
  title={Transformers in vision: A survey},
  author={Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal={ACM Computing Surveys (CSUR)},
  year={2021},
  publisher={ACM New York, NY}
}
@article{tay2020efficient,
  title={Efficient transformers: A survey},
  author={Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal={ACM Computing Surveys (CSUR)},
  year={2020},
  publisher={ACM New York, NY}
}

@article{chan2015listen,
  title={Listen, attend and spell},
  author={Chan, William and Jaitly, Navdeep and Le, Quoc V and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1508.01211},
  year={2015}
}
@article{he2021pipetransformer,
  title={Pipetransformer: Automated elastic pipelining for distributed training of transformers},
  author={He, Chaoyang and Li, Shen and Soltanolkotabi, Mahdi and Avestimehr, Salman},
  journal={arXiv preprint arXiv:2102.03161},
  year={2021}
}
@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@inproceedings{xu2015show,
  title={Show, attend and tell: Neural image caption generation with visual attention},
  author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={2048--2057},
  year={2015},
  organization={PMLR}
}
@article{khan2022transformers,
  title={Transformers in vision: A survey},
  author={Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal={ACM computing surveys (CSUR)},
  volume={54},
  number={10s},
  pages={1--41},
  year={2022},
  publisher={ACM New York, NY}
}