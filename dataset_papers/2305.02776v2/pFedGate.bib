@article{struct-sparsity,
	author = {Maurer, Andreas and Pontil, Massimiliano},
	title = {Structured Sparsity and Generalization},
	year = {2012},
	volume = {13},
	number = {1},
	issn = {1532-4435},
	journal = {The Journal of Machine Learning Research},
	month = {mar},
	pages = {671â€“690},
	numpages = {20},

}

@inproceedings{chen2023FSReal,
title={{FS}-{REAL}: Towards Real-World Cross-Device Federated Learning},
author={Daoyuan Chen and Dawei Gao and Yuexiang Xie and Xuchen Pan and Zitao Li and Yaliang Li and Bolin Ding and Jingren Zhou},
booktitle={KDD},
year={2023},
}

@inproceedings{qin2023revistingpfl,
title={Revisiting Personalized Federated Learning: Robustness Against Backdoor Attacks},
author={Zeyu Qin and Liuyi Yao and Daoyuan Chen and Yaliang Li and Bolin Ding and Minhao Cheng},
booktitle={KDD},
year={2023},
}

@inproceedings{
chen2022pflbench,
title={p{FL}-Bench: A Comprehensive Benchmark for Personalized Federated Learning},
author={Daoyuan Chen and Dawei Gao and Weirui Kuang and Yaliang Li and Bolin Ding},
booktitle={NeurIPS},
year={2022},
}



@article{huang2022FedSpa,
	title={Achieving Personalized Federated Learning with Sparse Local Models},
	author={Huang, Tiansheng and Liu, Shiwei and Li, Shen and He, Fengxiang and Lin, Weiwei and Tao, Dacheng},
	journal={ArXiv},
	year={2022},
	volume={abs/2201.11380}
}

@inproceedings{NEURIPS2019_ecb287ff,
 author = {Yurochkin, Mikhail and Agarwal, Mayank and Ghosh, Soumya and Greenewald, Kristjan and Hoang, Nghia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 title = {Statistical Model Aggregation via Parameter Matching},
 volume = {32},
 year = {2019}
}

@inproceedings{fedpnas,
 author = {Minh, Hoang and Carl, Kingsford},
 booktitle = {Personalized neural architecture search for federated learning},
 title = {NeurIPS 2021, Workshop on New Frontiers in Federated Learning},
 year = {2021}
}

@inproceedings{nguyen2022federated,
	title={Federated learning with buffered asynchronous aggregation},
	author={Nguyen, John and Malik, Kshitiz and Zhan, Hongyuan and Yousefpour, Ashkan and Rabbat, Mike and Malek, Mani and Huba, Dzmitry},
	booktitle={International Conference on Artificial Intelligence and Statistics},
	pages={3581--3607},
	year={2022},
	organization={PMLR}
}

@article{ainsworth2022git,
	title={Git re-basin: Merging models modulo permutation symmetries},
	author={Ainsworth, Samuel K and Hayase, Jonathan and Srinivasa, Siddhartha},
	journal={arXiv preprint arXiv:2209.04836},
	year={2022}
}

@inproceedings{fedmn,
 author = {Tianchun, Wan and Wei, Cheng and Dongsheng, Luo and Wenchao, Yu and Jingchao, Ni and Liang, Tong and Haifeng, Chen and Xiang Zhang
},
 booktitle = {Personalized Federated Learning via Heterogeneous Modular Networks},
 title = {ICDM},
 year = {2022}
}


@inproceedings{fedmd,
  title={Fedmd: Heterogenous federated learning via model distillation},
author={Li, Daliang and Wang, Junpu},
journal={NeurIPS 2019 Workshop on Federated Learning for Data Privacy and Confidentiality},
year={2019}
}





@inproceedings{yurochkin2019bayesian,
  title={Bayesian nonparametric federated learning of neural networks},
  author={Yurochkin, Mikhail and Agarwal, Mayank and Ghosh, Soumya and Greenewald, Kristjan and Hoang, Nghia and Khazaeni, Yasaman},
  booktitle={International Conference on Machine Learning},
  pages={7252--7261},
  year={2019},
  organization={PMLR}
}

@article{Li2019FederatedLS,
	title={Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection},
	author={Q. Li and Zeyi Wen and B. He},
	journal={ArXiv},
	year={2019},
	volume={abs/1907.09693}
}

@inproceedings{meng2021cross,
	title={Cross-node federated graph neural network for spatio-temporal data modeling},
	author={Meng, Chuizheng and Rambhatla, Sirisha and Liu, Yan},
	booktitle={KDD},
	pages={1202--1211},
	year={2021}
}

@inproceedings{hong2022efficient,
  title={Efficient Split-Mix Federated Learning for On-Demand and In-Situ Customization},
  author={Hong, Junyuan and Wang, Haotao and Wang, Zhangyang and Zhou, Jiayu},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{
ozkara2021quped,
title={QuPeD: Quantized Personalization via Distillation with Applications to Federated Learning},
author={Kaan Ozkara and Navjot Singh and Deepesh Data and Suhas Diggavi},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
}

@article{singhal2021federated,
  title={Federated reconstruction: Partially local federated learning},
  author={Singhal, Karan and Sidahmed, Hakim and Garrett, Zachary and Wu, Shanshan and Rush, John and Prakash, Sushant},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


@inproceedings{yang2021flop,
	title={Flop: Federated learning on medical datasets using partial networks},
	author={Yang, Qian and Zhang, Jianyi and Hao, Weituo and Spell, Gregory P and Carin, Lawrence},
	booktitle={KDD},
	pages={3845--3853},
	year={2021}
}

@inproceedings{hong2021federated,
	title={Federated Adversarial Debiasing for Fair and Transferable Representations},
	author={Hong, Junyuan and Zhu, Zhuangdi and Yu, Shuyang and Wang, Zhangyang and Dodge, Hiroko and Zhou, Jiayu},
	booktitle={KDD},
	year={2021}
}

@inproceedings{muhammad2020fedfast,
	title={Fedfast: Going beyond average for faster training of federated recommender systems},
	author={Muhammad, Khalil and Wang, Qinqin and O'Reilly-Morgan, Diarmuid and Tragos, Elias and Smyth, Barry and Hurley, Neil and Geraci, James and Lawlor, Aonghus},
	booktitle={KDD},
	pages={1234--1242},
	year={2020}
}

@inproceedings{yu2021fed2,
	title={Fed2: Feature-Aligned Federated Learning},
	author={Yu, Fuxun and Zhang, Weishan and Qin, Zhuwei and Xu, Zirui and Wang, Di and Liu, Chenchen and Tian, Zhi and Chen, Xiang},
	booktitle={KDD},
	pages={2066--2074},
	year={2021}
}


@article{duchi2014privacy,
	title={Privacy aware learning},
	author={Duchi, John C and Jordan, Michael I and Wainwright, Martin J},
	journal={JACM},
	volume={61},
	number={6},
	pages={1--57},
	year={2014},
	publisher={ACM New York, NY, USA}
}

@article{mcmahan2017learning,
	title={Learning differentially private recurrent language models},
	author={McMahan, H Brendan and Ramage, Daniel and Talwar, Kunal and Zhang, Li},
	journal={arXiv preprint arXiv:1710.06963},
	year={2017}
}

@inproceedings{agarwal2018cpsgd,
	title={cpSGD: Communication-efficient and differentially-private distributed sgd},
	author={Agarwal, Naman and Suresh, Ananda Theertha and Yu, Felix Xinnan X and Kumar, Sanjiv and McMahan, Brendan},
	booktitle={NeurIPS},
	pages={7564--7575},
	year={2018}
}

@article{zhu2019federated,
	title={Federated heavy hitters discovery with differential privacy},
	author={Zhu, Wennan and Kairouz, Peter and Sun, Haicheng and McMahan, Brendan and Li, Wei},
	journal={arXiv preprint arXiv:1902.08534},
	year={2019}
}

@inproceedings{zhuDataFreeKnowledgeDistillation2021,
	title={Data-Free Knowledge Distillation for Heterogeneous Federated Learning},
	author={Zhu, Zhuangdi and Hong, Junyuan and Zhou, Jiayu},
	booktitle={ICML},
	year={2021}
}

@inproceedings{liDittoFairRobust2021,
	title={Ditto: Fair and robust federated learning through personalization},
	author={Li, Tian and Hu, Shengyuan and Beirami, Ahmad and Smith, Virginia},
	booktitle={ICML},
	pages={6357--6368},
	year={2021}
}

@inproceedings{huangPersonalizedFederatedLearning2020,
	title = {Personalized Cross-Silo Federated Learning on Non-IID Data},
	author = {Huang, Yutao and Chu, Lingyang and Zhou, Zirui and Wang, Lanjun and Liu, Jiangchuan and Pei, Jian and Zhang, Yong},
	year = {2021},
	volume={35},
	number={9},
	pages={7865--7873},
	booktitle = {AAAI}
}


@inproceedings{zhang2020personalized,
	title={Personalized Federated Learning with First Order Model Optimization},
	author={Zhang, Michael and Sapra, Karan and Fidler, Sanja and Yeung, Serena and Alvarez, Jose M},
	booktitle={ICLR},
	year={2020}
}

@inproceedings{hamer2020fedboost,
	title={Fedboost: A communication-efficient algorithm for federated learning},
	author={Hamer, Jenny and Mohri, Mehryar and Suresh, Ananda Theertha},
	booktitle={ICML},
	pages={3973--3983},
	year={2020},
	organization={PMLR}
}

@inproceedings{rothchild2020fetchsgd,
	title={Fetchsgd: Communication-efficient federated learning with sketching},
	author={Rothchild, Daniel and Panda, Ashwinee and Ullah, Enayat and Ivkin, Nikita and Stoica, Ion and Braverman, Vladimir and Gonzalez, Joseph and Arora, Raman},
	booktitle={ICML},
	pages={8253--8265},
	year={2020},
	organization={PMLR}
}

@article{alistarh2017qsgd,
	title={QSGD: Communication-efficient SGD via gradient quantization and encoding},
	author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
	journal={NeurIPS},
	volume={30},
	pages={1709--1720},
	year={2017}
}

@inproceedings{reisizadeh2020fedpaq,
	title={Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization},
	author={Reisizadeh, Amirhossein and Mokhtari, Aryan and Hassani, Hamed and Jadbabaie, Ali and Pedarsani, Ramtin},
	booktitle={AISTATS},
	pages={2021--2031},
	year={2020},
	organization={PMLR}
}

@inproceedings{li2021fedmask,
	title={FedMask: Joint Computation and Communication-Efficient Personalized Federated Learning via Heterogeneous Masking},
	author={Li, Ang and Sun, Jingwei and Zeng, Xiao and Zhang, Mi and Li, Hai and Chen, Yiran},
	booktitle={SenSys},
	pages={42--55},
	year={2021}
}


@article{liang2020think,
	title={Think locally, act globally: Federated learning with local and global representations},
	author={Liang, Paul Pu and Liu, Terrance and Ziyin, Liu and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
	journal={arXiv preprint arXiv:2001.01523},
	year={2020}
}

@InProceedings{Liam2021Shared,
	title = 	 {Exploiting Shared Representations for Personalized Federated Learning},
	author =       {Collins, Liam and Hassani, Hamed and Mokhtari, Aryan and Shakkottai, Sanjay},
	booktitle = 	 {ICML},
	pages = 	 {2089--2099},
	year = 	 {2021},
	volume = 	 {139},
	month = 	 {18--24 Jul},
}


@InProceedings{diaoHeteroFLComputationCommunication2020,
	title={HeteroFL: Computation and communication efficient federated learning for heterogeneous clients},
	author={Diao, Enmao and Ding, Jie and Tarokh, Vahid},
	booktitle={ICLR},
	year={2021}
}

@inproceedings{ghoshEfficientFrameworkClustered2020a,
	title = {An efficient framework for clustered federated learning},
	author = {Ghosh, Avishek and Chung, Jichan and Yin, Dong and Ramchandran, Kannan},
	year = {2020},
	booktitle = {NeurIPS},
	volume={33},
}

@article{linEnsembleDistillationRobust2021,
	title = {Ensemble distillation for robust model fusion in federated learning},
	author = {Lin, Tao and Kong, Lingjing and Stich, Sebastian U and Jaggi, Martin},
	year = {2020},
	journal = {NeurIPS},
	volume={33}
}

@article{annavaramGroupKnowledgeTransfer,
	title={Group knowledge transfer: Federated learning of large cnns at the edge},
	author={He, Chaoyang and Annavaram, Murali and Avestimehr, Salman},
	journal={NeurIPS},
	volume={33},
	year={2020}
}


@inproceedings{zhang2021parameterized,
	title={Parameterized Knowledge Transfer for Personalized Federated Learning},
	author={Zhang, Jie and Guo, Song and Ma, Xiaosong and Wang, Haozhao and Xu, Wenchao and Wu, Feijie},
	booktitle={NeurIPS},
	volume={34},
	year={2021}
}


@article{yangFedStegFederatedTransfer2020,
	title = {{{FedSteg}}: {{A Federated Transfer Learning Framework}} for {{Secure Image Steganalysis}}},
	shorttitle = {{{FedSteg}}},
	author = {Yang, Hongwei and He, Hui and Zhang, Weizhe and Cao, Xiaochun},
	year = {2020},
	journal = {IEEE TNSE}
}


@book{hardy1952inequalities,
	title={Inequalities},
	author={Hardy, Godfrey Harold and Littlewood, John Edensor and P{\'o}lya, George and P{\'o}lya, Gy{\"o}rgy and others},
	year={1952},
	publisher={Cambridge university press}
}


@article{dai2019hyper,
	title={Hyper-sphere quantization: Communication-efficient sgd for federated learning},
	author={Dai, Xinyan and Yan, Xiao and Zhou, Kaiwen and Yang, Han and Ng, Kelvin KW and Cheng, James and Fan, Yu},
	journal={arXiv preprint arXiv:1911.04655},
	year={2019}
}

@article{basu2020qsparse,
	title={Qsparse-local-SGD: Distributed SGD with quantization, sparsification, and local computations},
	author={Basu, Debraj and Data, Deepesh and Karakus, Can and Diggavi, Suhas N},
	journal={IEEE Journal on Selected Areas in Information Theory},
	volume={1},
	number={1},
	pages={217--226},
	year={2020},
	publisher={IEEE}
}

@article{li2020a,
	title={Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization},
	author={Li, Zhize and Kovalev, Dmitry and Qian, Xun and Richt{\'a}rik, Peter},
	journal={arXiv preprint arXiv:2002.11364},
	year={2020}
}

@article{stich2018local,
	title={Local SGD converges fast and communicates little},
	author={Stich, Sebastian U},
	journal={arXiv preprint arXiv:1805.09767},
	year={2018}
}

@article{wang2018cooperative,
	title={Cooperative SGD: A unified framework for the design and analysis of communication-efficient SGD algorithms},
	author={Wang, Jianyu and Joshi, Gauri},
	journal={arXiv preprint arXiv:1808.07576},
	year={2018}
}

@article{zhou2017convergence,
	title={On the convergence properties of a $ K $-step averaging stochastic gradient descent algorithm for nonconvex optimization},
	author={Zhou, Fan and Cong, Guojing},
	journal={arXiv preprint arXiv:1708.01012},
	year={2017}
}

@article{lin2018don,
	title={Don't Use Large Mini-Batches, Use Local SGD},
	author={Lin, Tao and Stich, Sebastian U and Patel, Kumar Kshitij and Jaggi, Martin},
	journal={arXiv preprint arXiv:1808.07217},
	year={2018}
}

@article{hanzely2020federated,
	title={Federated learning of a mixture of global and local models},
	author={Hanzely, Filip and Richt{\'a}rik, Peter},
	journal={arXiv preprint arXiv:2002.05516},
	year={2020}
}

@article{zhao2018federated,
	title={Federated learning with non-iid data},
	author={Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
	journal={arXiv preprint arXiv:1806.00582},
	year={2018}
}


@article{karimireddy2019scaffold,
	title={Scaffold: Stochastic controlled averaging for on-device federated learning},
	author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank J and Stich, Sebastian U and Suresh, Ananda Theertha},
	journal={arXiv preprint arXiv:1910.06378},
	year={2019}
}

@article{haddadpour2019convergence,
	title={On the convergence of local descent methods in federated learning},
	author={Haddadpour, Farzin and Mahdavi, Mehrdad},
	journal={arXiv preprint arXiv:1910.14425},
	year={2019}
}

@mastersthesis{Krizhevsky09learningmultiple,
	author = {Alex Krizhevsky},
	title = {Learning multiple layers of features from tiny images},
	institution = {},
	type ={MSc thesis},
	year = {2009}
}


@inproceedings{smithFederatedMultiTaskLearning2018,
	title = {Federated {{Multi}}-{{Task Learning}}},
	author = {Smith, Virginia and Chiang, Chao-Kai and Sanjabi, Maziar and Talwalkar, Ameet},
	year = {2017},
	pages = {4427--4437},
	booktitle = {NeurIPS},
	volume={30}
}

@article{Kulkarni2020SurveyOP,
	title={Survey of Personalization Techniques for Federated Learning},
	author={V. Kulkarni and Milind Kulkarni and A. Pant},
	journal={2020 Fourth World Conference on Smart Trends in Systems, Security and Sustainability (WorldS4)},
	year={2020},
	pages={794-797}
}


@article{li2017meta,
	title={Meta-sgd: Learning to learn quickly for few-shot learning},
	author={Li, Zhenguo and Zhou, Fengwei and Chen, Fei and Li, Hang},
	journal={arXiv preprint arXiv:1707.09835},
	year={2017}
}

@article{behl2019alpha,
	title={Alpha maml: Adaptive model-agnostic meta-learning},
	author={Behl, Harkirat Singh and Baydin, At{\i}l{\i}m G{\"u}ne{\c{s}} and Torr, Philip HS},
	journal={arXiv preprint arXiv:1905.07435},
	year={2019}
}

@article{zhou2019efficient,
	title={Efficient meta learning via minibatch proximal update},
	author={Zhou, Pan and Yuan, Xiaotong and Xu, Huan and Yan, Shuicheng and Feng, Jiashi},
	journal={NeurIPS},
	volume={32},
	pages={1534--1544},
	year={2019}
}

@article{ma2022convergence,
  title={On the convergence of clustered federated learning},
  author={Ma, Jie and Long, Guodong and Zhou, Tianyi and Jiang, Jing and Zhang, Chengqi},
  journal={arXiv preprint arXiv:2202.06187},
  year={2022}
}

@inproceedings{fallah2020convergence,
	title={On the convergence theory of gradient-based model-agnostic meta-learning algorithms},
	author={Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman},
	booktitle={AISTATS},
	pages={1082--1092},
	year={2020},
	organization={PMLR}
}

@article{nichol2018first,
	title={On first-order meta-learning algorithms},
	author={Nichol, Alex and Achiam, Joshua and Schulman, John},
	journal={arXiv preprint arXiv:1803.02999},
	year={2018}
}


@article{arivazhagan2019federated,
	title={Federated learning with personalization layers},
	author={Arivazhagan, Manoj Ghuhan and Aggarwal, Vinay and Singh, Aaditya Kumar and Choudhary, Sunav},
	journal={arXiv preprint arXiv:1912.00818},
	year={2019}
}

@article{Yang2019FederatedML,
	title={Federated Machine Learning: Concept and Applications},
	author={Qiang Yang and Yang Liu and Tianjian Chen and Yongxin Tong},
	journal={arXiv: Artificial Intelligence},
	year={2019}
}


@article{lake2015human,
	title={Human-level concept learning through probabilistic program induction},
	author={Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
	journal={Science},
	volume={350},
	number={6266},
	pages={1332--1338},
	year={2015},
	publisher={American Association for the Advancement of Science}
}

@article{Mansour2020ThreeAF,
	title={Three Approaches for Personalization with Applications to Federated Learning},
	author={Y. Mansour and M. Mohri and J. Ro and A. T. Suresh},
	journal={ArXiv},
	year={2020},
	volume={abs/2002.10619}
}

@article{deng2020adaptive,
	title={Adaptive Personalized Federated Learning},
	author={Deng, Yuyang and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad},
	journal={arXiv preprint arXiv:2003.13461},
	year={2020}
}

@article{Dinh2020PersonalizedFL,
	title={Personalized Federated Learning with Moreau Envelopes},
	author={Canh T. Dinh and N. H. Tran and T. D. Nguyen},
	journal={ArXiv},
	year={2020},
	volume={abs/2006.08848}
}

@inproceedings{Huang2020PersonalizedCF,
	title={Personalized Cross-Silo Federated Learning on Non-IID Data},
	author={Yutao Huang and Lingyang Chu and Z. Zhou and Lanjun Wang and J. Liu and Jian Pei and Yanxin Zhang},
	year={2020}
}


@article{Klein2015ADC,
	title={A Dynamic Convolutional Layer for short rangeweather prediction},
	author={B. Klein and Lior Wolf and Y. Afek},
	journal={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	year={2015},
	pages={4840-4848}
}

@article{Riegler2015ConditionedRM,
	title={Conditioned Regression Models for Non-blind Single Image Super-Resolution},
	author={Gernot Riegler and S. Schulter and M. R{\"u}ther and H. Bischof},
	journal={2015 IEEE International Conference on Computer Vision (ICCV)},
	year={2015},
	pages={522-530}
}

@article{Ha2017HyperNetworks,
	title={HyperNetworks},
	author={David Ha and Andrew M. Dai and Quoc V. Le},
	journal={ArXiv},
	year={2017},
	volume={abs/1609.09106}
}

@article{Brock2018SMASHOM,
	title={SMASH: One-Shot Model Architecture Search through HyperNetworks},
	author={A. Brock and T. Lim and J. M. Ritchie and N. Weston},
	journal={ArXiv},
	year={2018},
	volume={abs/1708.05344}
}

@article{Lorraine2018StochasticHO,
	title={Stochastic Hyperparameter Optimization through Hypernetworks},
	author={Jonathan Lorraine and D. Duvenaud},
	journal={ArXiv},
	year={2018},
	volume={abs/1802.09419}
}

@inproceedings{Oswald2019COL,
	title={C ONTINUAL LEARNING WITH HYPERNETWORKS},
	author={J. Oswald and Christian Henning and J. Sacramento and Benjamin F. Grewe},
	year={2019}
}

@article{Littwin2019DeepMF,
	title={Deep Meta Functionals for Shape Representation},
	author={G. Littwin and L. Wolf},
	journal={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
	year={2019},
	pages={1824-1833}
}

@article{Nachmani2019HyperGraphNetworkDF,
	title={Hyper-Graph-Network Decoders for Block Codes},
	author={Eliya Nachmani and L. Wolf},
	journal={ArXiv},
	year={2019},
	volume={abs/1909.09036}
}

@inproceedings{
	navon2021learning,
	title={Learning the Pareto Front with Hypernetworks},
	author={Aviv Navon and Aviv Shamsian and Gal Chechik and Ethan Fetaya},
	booktitle={ICLR},
	year={2021},
	url={https://openreview.net/forum?id=NjF772F4ZZR}
}

@article{zhang2019lookahead,
	title={Lookahead optimizer: k steps forward, 1 step back},
	author={Zhang, Michael and Lucas, James and Ba, Jimmy and Hinton, Geoffrey E},
	journal={NeurIPS},
	volume={32},
	pages={9597--9608},
	year={2019}
}
@article{baxter2000model,
	title={A model of inductive bias learning},
	author={Baxter, Jonathan},
	journal={JAIR},
	volume={12},
	pages={149--198},
	year={2000}
}
@article{konevcny2015federated,
	title={Federated optimization: Distributed optimization beyond the datacenter},
	author={Kone{\v{c}}n{\`y}, Jakub and McMahan, Brendan and Ramage, Daniel},
	journal={arXiv preprint arXiv:1511.03575},
	year={2015}
}
@article{konevcny2016federated,
	title={Federated learning: Strategies for improving communication efficiency},
	author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
	journal={arXiv preprint arXiv:1610.05492},
	year={2016}
}
@inproceedings{smith2017federated,
	title={Federated multi-task learning},
	author={Smith, Virginia and Chiang, Chao-Kai and Sanjabi, Maziar and Talwalkar, Ameet S},
	booktitle={NeurIPS},
	pages={4424--4434},
	year={2017}
}

@article{Maaten2008VisualizingDU,
	title={Visualizing Data using t-SNE},
	author={L. V. D. Maaten and Geoffrey E. Hinton},
	journal={JMLR},
	year={2008},
	volume={9},
	pages={2579-2605}
}

@article{Hsu2019MeasuringTE,
	title={Measuring the Effects of Non-Identical Data Distribution for Federated Visual Classification},
	author={T. H. Hsu and Hang Qi and M. Brown},
	journal={ArXiv},
	year={2019},
	volume={abs/1909.06335}
}

@article{Wu2020PersonalizedFL,
	title={Personalized Federated Learning for Intelligent IoT Applications: A Cloud-Edge Based Framework},
	author={Qiong Wu and Kaiwen He and X. Chen},
	journal={IEEE Open Journal of the Computer Society},
	year={2020},
	volume={1},
	pages={35-44}
}

@techreport{cifar,
	title = {Learning multiple layers of features from tiny images},
	author = {Alex Krizhevsky and Geoffrey Hinton},
	year = {2009},
	institution = {University of Toronto}
}

@inproceedings{spectral_normalization,
	author    = {Takeru Miyato and
	Toshiki Kataoka and
	Masanori Koyama and
	Yuichi Yoshida},
	title     = {Spectral Normalization for Generative Adversarial Networks},
	booktitle = {ICLR, {ICLR}},
	year      = {2018}
}

@article{CLT_for_ML,
	author = {Halbert White},
	journal = {Econometrica},
	pages = {1--25},
	publisher = {[Wiley, Econometric Society]},
	title = {Maximum Likelihood Estimation of Misspecified Models},
	volume = {50},
	year = {1982}
}

@INPROCEEDINGS{PCAdenoising,
	author={D. D. {Muresan} and T. W. {Parks}},
	booktitle={Proceedings 2003 International Conference on Image Processing (Cat. No.03CH37429)}, 
	title={Adaptive principal components and image denoising}, 
	year={2003}}

@article{mothukuri2021survey,
	title={A survey on security and privacy of federated learning},
	author={Mothukuri, Viraaji and Parizi, Reza M and Pouriyeh, Seyedamin and Huang, Yan and Dehghantanha, Ali and Srivastava, Gautam},
	journal={Future Generation Computer Systems},
	volume={115},
	pages={619--640},
	year={2021},
	publisher={Elsevier}
}
@article{li2020acceleration,
	title={Acceleration for compressed gradient descent in distributed and federated optimization},
	author={Li, Zhize and Kovalev, Dmitry and Qian, Xun and Richt{\'a}rik, Peter},
	journal={arXiv preprint arXiv:2002.11364},
	year={2020}
}
@inproceedings{klocek2019hypernetwork,
	title={Hypernetwork functional image representation},
	author={Klocek, Sylwester and Maziarka, {\L}ukasz and Wo{\l}czyk, Maciej and Tabor, Jacek and Nowak, Jakub and {\'S}mieja, Marek},
	booktitle={International Conference on Artificial Neural Networks},
	pages={496--510},
	year={2019},
	organization={Springer}
}
@inproceedings{suarez2017character,
	title={Character-level language modeling with recurrent highway hypernetworks},
	author={Suarez, Joseph},
	booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
	pages={3269--3278},
	year={2017}
}
@article{von2019continual,
	title={Continual learning with hypernetworks},
	author={von Oswald, Johannes and Henning, Christian and Sacramento, Jo{\~a}o and Grewe, Benjamin F},
	journal={arXiv preprint arXiv:1906.00695},
	year={2019}
}
@inproceedings{DBLP:conf/nips/NachmaniW19,
	author    = {Eliya Nachmani and
	Lior Wolf},
	editor    = {Hanna M. Wallach and
	Hugo Larochelle and
	Alina Beygelzimer and
	Florence d'Alch{\'{e}}{-}Buc and
	Emily B. Fox and
	Roman Garnett},
	title     = {Hyper-Graph-Network Decoders for Block Codes},
	booktitle = {NeurIPS 32: Annual Conference
	on Neural Information Processing Systems 2019, NeurIPS 2019, December
	8-14, 2019, Vancouver, BC, Canada},
	pages     = {2326--2336},
	year      = {2019},
	url       = {https://proceedings.neurips.cc/paper/2019/hash/a9be4c2a4041cadbf9d61ae16dd1389e-Abstract.html},
	timestamp = {Thu, 21 Jan 2021 15:15:19 +0100},
	biburl    = {https://dblp.org/rec/conf/nips/NachmaniW19.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{MacKay2019SelfTuningNB,
	title={Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions},
	author={Matthew MacKay and Paul Vicol and Jonathan Lorraine and D. Duvenaud and Roger B. Grosse},
	journal={ArXiv},
	year={2019},
	volume={abs/1903.03088}
}

@article{Bae2020DeltaSTNEB,
	title={Delta-STN: Efficient Bilevel Optimization for Neural Networks using Structured Response Jacobians},
	author={Juhan Bae and Roger B. Grosse},
	journal={ArXiv},
	year={2020},
	volume={abs/2010.13514}
}

@article{huo2020faster,
	title={Faster on-device training using new federated momentum algorithm},
	author={Huo, Zhouyuan and Yang, Qian and Gu, Bin and Huang, Lawrence Carin and others},
	journal={arXiv preprint arXiv:2002.02090},
	year={2020}
}

@article{lecun1998gradient,
	title={Gradient-based learning applied to document recognition},
	author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
	journal={Proceedings of the IEEE},
	volume={86},
	number={11},
	pages={2278--2324},
	year={1998},
	publisher={Ieee}
}

@inproceedings{tang18a,
	title =    {{$D^2$: Decentralized Training over Decentralized Data}},
	author =   {Tang, Hanlin and Lian, Xiangru and Yan, Ming and Zhang, Ce and Liu, Ji},
	booktitle =    {ICML},
	year =   {2018}
}

@article{tan2021towards,
  title={Towards personalized federated learning},
  author={Tan, Alysa Ziying and Yu, Han and Cui, Lizhen and Yang, Qiang},
  journal={arXiv preprint arXiv:2103.00710},
  year={2021}
}

@inproceedings{neglia2020,
	title={Decentralized gradient methods: does topology matter?},
	author={Giovanni Neglia and Chuan Xu and Don Towsley and Gianmarco Calbi},
	booktitle={AISTATS},
	year={2020}
}


@inproceedings{li2021ditto,
	title={Ditto: Fair and robust federated learning through personalization},
	author={Li, Tian and Hu, Shengyuan and Beirami, Ahmad and Smith, Virginia},
	booktitle={ICML},
	pages={6357--6368},
	year={2021},
	organization={PMLR}
}


@InProceedings{acar2021debiasing,
	title = 	 {Debiasing Model Updates for Improving Personalized Federated Training},
	author =       {Acar, Durmus Alp Emre and Zhao, Yue and Zhu, Ruizhao and Matas, Ramon and Mattina, Matthew and Whatmough, Paul and Saligrama, Venkatesh},
	booktitle = 	 {ICML},
	pages = 	 {21--31},
	year = 	 {2021},
	editor = 	 {Meila, Marina and Zhang, Tong},
	volume = 	 {139},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {7},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v139/acar21a/acar21a.pdf},
	url = 	 {https://proceedings.mlr.press/v139/acar21a.html},
	abstract = 	 {We propose a novel method for federated learning that is customized specifically to the objective of a given edge device. In our proposed method, a server trains a global meta-model by collaborating with devices without actually sharing data. The trained global meta-model is then personalized locally by each device to meet its specific objective. Different from the conventional federated learning setting, training customized models for each device is hindered by both the inherent data biases of the various devices, as well as the requirements imposed by the federated architecture. We propose gradient correction methods leveraging prior works, and explicitly de-bias the meta-model in the distributed heterogeneous data setting to learn personalized device models. We present convergence guarantees of our method for strongly convex, convex and nonconvex meta objectives. We empirically evaluate the performance of our method on benchmark datasets and demonstrate significant communication savings.}
}


@InProceedings{shamsian2021personalized,
	title = 	 {Personalized Federated Learning using Hypernetworks},
	author =       {Shamsian, Aviv and Navon, Aviv and Fetaya, Ethan and Chechik, Gal},
	booktitle = 	 {ICML},
	pages = 	 {9489--9502},
	year = 	 {2021},
	editor = 	 {Meila, Marina and Zhang, Tong},
	volume = 	 {139},
	month = 	 {7},
}


@inproceedings{li2020fedbn,
	title={Fed{BN}: Federated Learning on Non-{IID} Features via Local Batch Normalization},
	author={Li, Xiaoxiao and JIANG, Meirui and Zhang, Xiaofei and Kamp, Michael and Dou, Qi},
	booktitle={ICLR},
	year={2020}
}

@inproceedings{huang2021personalized,
	title={Personalized cross-silo federated learning on non-iid data},
	author={Huang, Yutao and Chu, Lingyang and Zhou, Zirui and Wang, Lanjun and Liu, Jiangchuan and Pei, Jian and Zhang, Yong},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={35},
	number={9},
	pages={7865--7873},
	year={2021}
}

@inproceedings{zhang2021asysqn,
  title={AsySQN: Faster Vertical Federated Learning Algorithms with Better Computation Resource Utilization},
  author={Zhang, Qingsong and Gu, Bin and Deng, Cheng and Gu, Songxiang and Bo, Liefeng and Pei, Jian and Huang, Heng},
  booktitle={KDD},
  pages={3917--3927},
  year={2021}
}


@inproceedings{Lian2018,
	Author = {Xiangru Lian and Wei Zhang and Ce Zhang and Ji Liu},
	Booktitle = {ICML},
	Title = {{Asynchronous Decentralized Parallel Stochastic Gradient Descent}},
	Year = {2018}}

@InProceedings{pmlr-v130-haddadpour21a,
	title = 	 {Federated Learning with Compression: Unified Analysis and Sharp Guarantees},
	author =       {Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mokhtari, Aryan and Mahdavi, Mehrdad},
	booktitle = 	 {ICML},
	year = 	 {2021}
}

@INPROCEEDINGS{Bellet2018a,
	author = {Aur\'{e}lien Bellet and Rachid Guerraoui and Mahsa Taziki and Marc Tommasi},
	title = {{P}ersonalized and {P}rivate {P}eer-to-{P}eer {M}achine {L}earning},
	booktitle = {{AISTATS}},
	year = {2018}
}

@INPROCEEDINGS{Vanhaesebrouck2017a,
	author = {Paul Vanhaesebrouck and Aur\'{e}lien Bellet and Marc Tommasi},
	title = {{D}ecentralized {C}ollaborative {L}earning of {P}ersonalized {M}odels over {N}etworks},
	booktitle = {{AISTATS}},
	year = {2017}
}

@INPROCEEDINGS{neglia19infocom,
	author={Neglia, Giovanni and Calbi, Gianmarco and Towsley, Don and Vardoyan, Gayane},
	booktitle={IEEE INFOCOM 2019 - IEEE Conference on Computer Communications}, 
	title={The Role of Network Topology for Distributed Machine Learning}, 
	year={2019},
	volume={},
	number={},
	pages={2350-2358},
	doi={10.1109/INFOCOM.2019.8737602}}

@misc{cyffers2021privacy,
	title={Privacy Amplification by Decentralization}, 
	author={Edwige Cyffers and Aur\'{e}lien Bellet},
	year={2021},
	eprint={2012.05326},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	note = {Presented at the Privacy Preserving Machine Learning workshop (in conjunction with NeurIPS 2020)}
}

@misc{carbonFL,
	title={Can Federated Learning Save The Planet?}, 
	author={Xinchi Qiu and Titouan Parcollet and Daniel J. Beutel and Taner Topal and Akhil Mathur and Nicholas D. Lane},
	year={2020},
	eprint={2010.06537},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	note = {Presented at NeurIPS 2020 Workshop Tackling Climate Change with Machine Learning.}
}


@inproceedings{marfoq20neurips,
	author = {Marfoq, Othmane and Xu, Chuan and Neglia, Giovanni and Vidal, Richard},
	booktitle = {NeurIPS},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
	pages = {19478--19487},
	publisher = {Curran Associates, Inc.},
	title = {Throughput-Optimal Topology Design for Cross-Silo Federated Learning},
	url = {https://proceedings.neurips.cc/paper/2020/file/e29b722e35040b88678e25a1ec032a21-Paper.pdf},
	volume = {33},
	year = {2020},
	Bdsk-Url-1 = {https://proceedings.neurips.cc/paper/2020/file/e29b722e35040b88678e25a1ec032a21-Paper.pdf}}


@inproceedings{lian17,
	author = {Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
	title = {Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent},
	year = {2017},
	isbn = {9781510860964},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
	pages = {5336â€“5346},
	numpages = {11},
	location = {Long Beach, California, USA},
	series = {NIPS'17}
}

@inproceedings{cortes_sample_bias2,
	author = {Cortes, Corinna and Mansour, Yishay and Mohri, Mehryar},
	booktitle = {NeurIPS},
	editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Learning Bounds for Importance Weighting},
	url = {https://proceedings.neurips.cc/paper/2010/file/59c33016884a62116be975a9bb8257e3-Paper.pdf},
	volume = {23},
	year = {2010}
}

@inproceedings{cortes_sample_bias1,
	author = {Cortes, Corinna and Mohri, Mehryar and Riley, Michael and Rostamizadeh, Afshin},
	title = {Sample Selection Bias Correction Theory},
	year = {2008},
	booktitle = {ALT}
}

@inproceedings{Sugiyama_sample_bias,
	author = {Sugiyama, Masashi and Nakajima, Shinichi and Kashima, Hisashi and Buenau, Paul and Kawanabe, Motoaki},
	booktitle = {NIPS},
	title = {Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation},
	year = {2008}
}

@inproceedings{weightedERM,
	author = {Robin Vogel and Mastane Achab and St\'{e}phan Cl\'{e}men\c{c}on and Charles Tillier},
	booktitle = {ESANN},
	title = {Weighted Emprirical Risk Minimization: Transfer Learning based on Importance Sampling},
	year = {2020}
}

@inproceedings{ghosh2020efficient,
	author = {Avishek Ghosh and Jichan Chung and Dong Yin and Kannan Ramchandran},
	title = {An Efficient Framework for Clustered Federated Learning},
	year = {2020},
	booktitle = {NeurIPS}
}


@article{JMLR:v6:ando05a,
	author  = {Rie Kubota Ando and Tong Zhang},
	title   = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	journal = {JMLR},
	year    = {2005},
	volume  = {6},
	number  = {61},
	pages   = {1817-1853}
}

@inproceedings{smith2017federated,
	author = {Smith, Virginia and Chiang, Chao-Kai and Sanjabi, Maziar and Talwalkar, Ameet},
	title = {Federated Multi-Task Learning},
	year = {2017},
	isbn = {9781510860964},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	abstract = {Federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices. In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues. Our method and theory for the first time consider issues of high communication cost, stragglers, and fault tolerance for distributed multi-task learning. The resulting method achieves significant speedups compared to alternatives in the federated setting, as we demonstrate through simulations on real-world federated datasets.},
	booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
	pages = {4427â€“4437},
	numpages = {11},
	location = {Long Beach, California, USA},
	series = {NIPS'17}
}

@article{fs,
  title = {FederatedScope: A Flexible Federated Learning Platform for Heterogeneity},
  author = {Xie, Yuexiang and Wang, Zhen and Gao, Dawei and Chen, Daoyuan and Yao, Liuyi and Kuang, Weirui and Li, Yaliang and Ding, Bolin and Zhou, Jingren},
  journal={Proceedings of the VLDB Endowment},
  volume={16},
  number={5},
  pages={1059--1072},
  year={2023}
}

@inproceedings{fsgnn,
author = {Wang, Zhen and Kuang, Weirui and Xie, Yuexiang and Yao, Liuyi and Li, Yaliang and Ding, Bolin and Zhou, Jingren},
title = {FederatedScope-GNN: Towards a Unified, Comprehensive and Efficient Package for Federated Graph Learning},
year = {2022},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4110â€“4120},
numpages = {11},
}





@inproceedings{briggs2020federated,
  title={Federated learning with hierarchical clustering of local updates to improve training on non-IID data},
  author={Briggs, Christopher and Fan, Zhong and Andras, Peter},
  booktitle={IJCNN},
  pages={1--9},
  year={2020},
  organization={IEEE}
}

@inproceedings{zhang2010convex,
	title={A Convex Formulation for Learning Task Relationships in Multi-task Learning},
	author={Zhang, Yu and Yeung, Dit Yan},
	booktitle={Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, UAI 2010},
	pages={733},
	year={2010}
}

@InProceedings{pmlr-v108-zantedeschi20a,
	title = {Fully Decentralized Joint Learning of Personalized Models and Collaboration Graphs},
	author = {Zantedeschi, Valentina and Bellet, Aur\'{e}lien and Tommasi, Marc},
	pages = {864--874},
	year = {2020}, 
	editor = {Silvia Chiappa and Roberto Calandra},
	volume = {108},
	series = {Proceedings of Machine Learning Research},
	address = {Online},
	month = {8}, 
	publisher = {PMLR},
	pdf = {http://proceedings.mlr.press/v108/zantedeschi20a/zantedeschi20a.pdf},
	url = {http://proceedings.mlr.press/v108/zantedeschi20a.html},
	abstract = {We consider the fully decentralized machine learning scenario where many users with personal datasets collaborate to learn models through local peer-to-peer exchanges, without a central coordinator. We propose to train personalized models that leverage a collaboration graph describing the relationships between user personal tasks, which we learn jointly with the models. Our fully decentralized optimization procedure alternates between training nonlinear models given the graph in a greedy boosting manner, and updating the collaboration graph (with controlled sparsity) given the models. Throughout the process, users exchange messages only with a small number of peers (their direct neighbors when updating the models, and a few random users when updating the graph), ensuring that the procedure naturally scales with the number of users. Overall, our approach is communication-efficient and avoids exchanging personal data. We provide an extensive analysis of the convergence rate, memory and communication complexity of our approach, and demonstrate its benefits compared to competing techniques on synthetic and real datasets.}
} 

@article{hanzely2020federated,
	title={Federated Learning of a Mixture of Global and Local Models},
	author={Filip Hanzely and Peter Richt\'{a}rik},
	year={2020},
	eprint={2002.05516},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@article{sattler2020clustered,
	title={Clustered Federated Learning: Model-Agnostic Distributed Multitask Optimization Under Privacy Constraints},
	author={Sattler, Felix and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
	journal={TNNLS},
	year={2020},
	publisher={IEEE}
}

@article{ward1963hierarchical,
	title={Hierarchical grouping to optimize an objective function},
	author={Ward Jr, Joe H},
	journal={JASA},
	volume={58},
	number={301},
	pages={236--244},
	year={1963},
	publisher={Taylor \& Francis}
}

@inproceedings{mcmahan2017communication,
	title={Communication-efficient learning of deep networks from decentralized data},
	author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
	booktitle={AISTATS},
	pages={1273--1282},
	year={2017},
	organization={PMLR}
}

@article{konevcny2016federated,
	title={Federated learning: Strategies for improving communication efficiency},
	author={Kone{\v{c}}n{\'y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
	journal={arXiv preprint arXiv:1610.05492},
	year={2016},
	note = {Presented at NIPS 2016 Workshop on Private Multi-Party Machine Learning}
}

@article{li2020federated,
	title={Federated learning: Challenges, methods, and future directions},
	author={Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
	journal={IEEE Signal Processing Magazine},
	volume={37},
	number={3},
	pages={50--60},
	year={2020},
	publisher={IEEE}
}

@article{kairouz2019advances,
	year = {2021},
	volume = {14},
	journal = {Foundations and TrendsÂ® in Machine Learning},
	title = {Advances and Open Problems in Federated Learning},
	issn = {1935-8237},
	number = {1â€“2},
	pages = {1-210},
	author = {Peter Kairouz and H. Brendan McMahan and Brendan Avent and AurÃ©lien Bellet and Mehdi Bennis and Arjun Nitin Bhagoji and Kallista Bonawitz and Zachary Charles and Graham Cormode and Rachel Cummings and Rafael G. L. Dâ€™Oliveira and Hubert Eichner and Salim El Rouayheb and David Evans and Josh Gardner and Zachary Garrett and AdriÃ  GascÃ³n and Badih Ghazi and Phillip B. Gibbons and Marco Gruteser and Zaid Harchaoui and Chaoyang He and Lie He and Zhouyuan Huo and Ben Hutchinson and Justin Hsu and Martin Jaggi and Tara Javidi and Gauri Joshi and Mikhail Khodak and Jakub KonecnÃ½ and Aleksandra Korolova and Farinaz Koushanfar and Sanmi Koyejo and TancrÃ¨de Lepoint and Yang Liu and Prateek Mittal and Mehryar Mohri and Richard Nock and Ayfer Ã–zgÃ¼r and Rasmus Pagh and Hang Qi and Daniel Ramage and Ramesh Raskar and Mariana Raykova and Dawn Song and Weikang Song and Sebastian U. Stich and Ziteng Sun and Ananda Theertha Suresh and Florian TramÃ¨r and Praneeth Vepakomma and Jianyu Wang and Li Xiong and Zheng Xu and Qiang Yang and Felix X. Yu and Han Yu and Sen Zhao}
}

@inproceedings{stich2018local,
	title={Local SGD Converges Fast and Communicates Little},
	author={Stich, Sebastian U},
	booktitle={ICLR},
	year={2018}
}

@inproceedings{sim2019investigation,
	title={An Investigation Into On-device Personalization of End-to-end Automatic Speech Recognition Models},
	author={Sim, Khe Chai and Zadrazil, Petr and Beaufays, Fran{\c{c}}oise},
	booktitle={INTERSPEECH},
	year={2019}
}

@article{jiangImprovingFederatedLearning2019,
	title = {Improving {{Federated Learning Personalization}} via {{Model Agnostic Meta Learning}}},
	author = {Jiang, Yihan and Kone{\v c}n{\'y}, Jakub and Rush, Keith and Kannan, Sreeram},
	year = {2019},
	journal = {arXiv:1909.12488}
}

@inproceedings{feddst,
	title={Federated dynamic sparse training: Computing less, communicating less, yet learning better},
	author={Bibikar, Sameer and Vikalo, Haris and Wang, Zhangyang and Chen, Xiaohan},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={36},
	number={6},
	pages={6080--6088},
	year={2022}
}

@inproceedings{zerofl,
	title={Zero{FL}: Efficient On-Device Training for  Federated Learning with Local Sparsity},
	author={Xinchi Qiu and Javier Fernandez-Marques and Pedro PB Gusmao and Yan Gao and Titouan Parcollet and Nicholas Donald Lane},
	booktitle={International Conference on Learning Representations},
	year={2022},
}

@inproceedings{fallahPersonalizedFederatedLearning2020,
	author = {Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman},
	booktitle = {NeurIPS},
	pages = {3557--3568},
	title = {Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach},
	volume = {33},
	year = {2020}
}


@inproceedings{fallah2020personalized,
	title={Personalized federated learning: A meta-learning approach},
	author={Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman},
	booktitle={NeurIPS 2020},
	year={2020}
}

@article{deng2020model,
  title={Model compression and hardware acceleration for neural networks: A comprehensive survey},
  author={Deng, Lei and Li, Guoqi and Han, Song and Shi, Luping and Xie, Yuan},
  journal={Proceedings of the IEEE},
  volume={108},
  number={4},
  pages={485--532},
  year={2020},
  publisher={IEEE}
}

@inproceedings{khodak2019adaptive,
	title={Adaptive gradient-based meta-learning methods},
	author={Khodak, Mikhail and Balcan, Maria-Florina F and Talwalkar, Ameet S},
	booktitle={NeurIPS},
	pages={5917--5928},
	year={2019}
}

@inproceedings{hanzely2020lower,
	title={Lower bounds and optimal algorithms for personalized federated learning},
	author={Hanzely, Filip and Hanzely, Slavom{\'\i}r and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter},
	booktitle={34th Conference on Neural Information Processing Systems (NeurIPS 2020)},
	year={2020}
}

@inproceedings{karimireddy2020scaffold,
	title={SCAFFOLD: Stochastic controlled averaging for federated learning},
	author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
	booktitle={ICML},
	pages={5132--5143},
	year={2020},
	organization={PMLR}
}

@inproceedings{dinh2020personalized,
	title={Personalized Federated Learning with Moreau Envelopes},
	author={Dinh, Canh T and Tran, Nguyen H and Nguyen, Tuan Dung},
	booktitle={NeurIPS},
	year={2020}
}

@article{mansour2020three,
	title={Three approaches for personalization with applications to federated learning},
	author={Mansour, Yishay and Mohri, Mehryar and Ro, Jae and Suresh, Ananda Theertha},
	journal={arXiv preprint arXiv:2002.10619},
	year={2020}
}

@article{deng2020adaptive,
	title={Adaptive Personalized Federated Learning},
	author={Deng, Yuyang and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad},
	journal={arXiv preprint arXiv:2003.13461},
	year={2020}
}

@misc{corinzia2019variational,
	title={Variational Federated Multi-Task Learning}, 
	author={Luca Corinzia and Joachim M. Buhmann},
	year={2019},
	eprint={1906.06268},
	archivePrefix={arXiv},
}

@inproceedings{finn2017model,
	title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
	author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	booktitle={ICML},
	year={2017}
}

@inproceedings{shokri2017membership,
	title={Membership inference attacks against machine learning models},
	author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
	booktitle={2017 IEEE Symposium on Security and Privacy (SP)},
	pages={3--18},
	year={2017},
	organization={IEEE}
}

@inproceedings{fredrikson2015model,
	title={Model inversion attacks that exploit confidence information and basic countermeasures},
	author={Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
	booktitle={Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
	pages={1322--1333},
	year={2015}
}

@inproceedings{bost2015machine,
	title={Machine learning classification over encrypted data.},
	author={Bost, Raphael and Popa, Raluca Ada and Tu, Stephen and Goldwasser, Shafi},
	booktitle={NDSS},
	volume={4324},
	pages={4325},
	year={2015}
}

@inproceedings{bonawitz2017practical,
	title={Practical secure aggregation for privacy-preserving machine learning},
	author={Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
	booktitle={Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
	pages={1175--1191},
	year={2017}
}

@inproceedings{abadi2016deep,
	title={Deep learning with differential privacy},
	author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
	booktitle={Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
	pages={308--318},
	year={2016}
}

@INPROCEEDINGS{tran19,
	author={N. H. {Tran} and W. {Bao} and A. {Zomaya} and M. N. H. {Nguyen} and C. S. {Hong}},
	booktitle={IEEE INFOCOM 2019 - IEEE Conference on Computer Communications}, 
	title={Federated Learning over Wireless Networks: Optimization Model Design and Analysis}, 
	year={2019},
	volume={},
	number={},
	pages={1387-1395},}


@INPROCEEDINGS{kang19,
	author={Kang, Jiawen and Xiong, Zehui and Niyato, Dusit and Yu, Han and Liang, Ying-Chang and Kim, Dong In},
	booktitle={2019 IEEE VTS Asia Pacific Wireless Communications Symposium (APWCS)}, 
	title={Incentive Design for Efficient Federated Learning in Mobile Networks: A Contract Theory Approach}, 
	year={2019},
	volume={},
	number={},
	pages={1-5},
	doi={10.1109/VTS-APWCS.2019.8851649}}

@inproceedings{nikolaenko2013privacy,
	title={Privacy-preserving ridge regression on hundreds of millions of records},
	author={Nikolaenko, Valeria and Weinsberg, Udi and Ioannidis, Stratis and Joye, Marc and Boneh, Dan and Taft, Nina},
	booktitle={2013 IEEE Symposium on Security and Privacy},
	pages={334--348},
	year={2013},
	organization={IEEE}
}

@ARTICLE{Boyd03fastestmixing,
	author = {Stephen Boyd and Persi Diaconis and Lin Xiao},
	title = {Fastest Mixing Markov Chain on A Graph},
	journal = {SIAM REVIEW},
	year = {2003},
	volume = {46},
	pages = {667--689}
}

@book{nestrov2003introduction,
	title =     {Introductory Lectures on Convex Optimization: A Basic Course},
	author =    {Y. Nesterov},
	publisher = {Springer},
	year =      {2003},
	series =    {Applied Optimization},
	edition =   {1},
	volume =    {},
	url =       {http://gen.lib.rus.ec/book/index.php?md5=488d3c36f629a6e021fc011675df02ef}
}

@misc{bubeck2015convex,
	title={Convex Optimization: Algorithms and Complexity}, 
	author={S\'{e}bastien Bubeck},
	year={2015},
	eprint={1405.4980},
	archivePrefix={arXiv},
	primaryClass={math.OC}
}

@article{erdos59a,
	added-at = {2010-05-05T00:38:27.000+0200},
	author = {Erd\"{o}s, P. and R\'{e}nyi, A.},
	biburl = {https://www.bibsonomy.org/bibtex/25aab47a7be9ec47644735f8e0a4607b6/alex},
	interhash = {99061fc859ba540d4485abfbce44f298},
	intrahash = {5aab47a7be9ec47644735f8e0a4607b6},
	journal = {Publicationes Mathematicae Debrecen},
	keywords = {graph sna},
	pages = 290,
	timestamp = {2010-05-05T00:38:27.000+0200},
	title = {On Random Graphs I},
	volume = 6,
	year = 1959
}

@article{karimireddy2020mime,
	title={Mime: Mimicking Centralized Stochastic Algorithms in Federated Learning},
	author={Karimireddy, Sai Praneeth and Jaggi, Martin and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank J and Stich, Sebastian U and Suresh, Ananda Theertha},
	journal={arXiv preprint arXiv:2008.03606},
	year={2020}
}

@inproceedings{wang2020tackling,
	title={Tackling the objective inconsistency problem in heterogeneous federated optimization},
	author={Wang, Jianyu and Liu, Qinghua and Liang, Hao and Joshi, Gauri and Poor, H Vincent},
	booktitle={34th Conference on Neural Information Processing Systems (NeurIPS 2020)},
	year={2020}
}

@inproceedings{li2019convergence,
	title={On the Convergence of FedAvg on Non-IID Data},
	author={Li, Xiang and Huang, Kaixuan and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
	booktitle={ICLR},
	year={2019}
}

@book{shalev2014understanding,
	title={Understanding machine learning: From theory to algorithms},
	author={Shalev-Shwartz, Shai and Ben-David, Shai},
	year={2014},
	publisher={Cambridge university press}
}

@inproceedings{mohri2019agnostic,
	title={Agnostic Federated Learning},
	author={Mohri, Mehryar and Sivek, Gary and Suresh, Ananda Theertha},
	booktitle={ICML},
	pages={4615--4625},
	year={2019}
}

@inproceedings{gopfert2019can,
	title={When can unlabeled data improve the learning rate?},
	author={G{\"o}pfert, Christina and Ben-David, Shai and Bousquet, Olivier and Gelly, Sylvain and Tolstikhin, Ilya and Urner, Ruth},
	booktitle={Conference on Learning Theory},
	pages={1500--1518},
	year={2019},
	organization={PMLR}
}

@inproceedings{marcel2010torchvision,
	author = {Marcel, S\'{e}bastien and Rodriguez, Yann},
	title = {Torchvision the Machine-Vision Package of Torch},
	year = {2010},
	isbn = {9781605589336},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1873951.1874254},
	doi = {10.1145/1873951.1874254},
	abstract = {This paper presents Torchvision an open source machine vision package for Torch. Torch is a machine learning library providing a series of the state-of-the-art algorithms such as Neural Networks, Support Vector Machines, Gaussian Mixture Models, Hidden Markov Models and many others. Torchvision provides additional functionalities to manipulate and process images with standard image processing algorithms. Hence, the resulting images can be used directly with the Torch machine learning algorithms as Torchvision is fully integrated with Torch. Both Torch and Torchvision are written in C++ language and are publicly available under the Free-BSD License.},
	booktitle = {Proceedings of the 18th ACM International Conference on Multimedia},
	pages = {1485â€“1488},
	numpages = {4},
	keywords = {pattern recognition, face detection and recognition, open source, machine learning, vision},
	location = {Firenze, Italy},
	series = {MM '10}
}

@incollection{paszke2019pytorch,
	title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	booktitle = {NeurIPS 32},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages = {8024--8035},
	year = {2019},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@book{mohri2012foundations,
	author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
	title = {Foundations of Machine Learning},
	year = {2012},
	isbn = {026201825X},
	publisher = {The MIT Press},
	abstract = {This graduate-level textbook introduces fundamental concepts and methods in machine learning. It describes several important modern algorithms, provides the theoretical underpinnings of these algorithms, and illustrates key aspects for their application. The authors aim to present novel theoretical tools and concepts while giving concise proofs even for relatively advanced topics. Foundations of Machine Learning fills the need for a general textbook that also offers theoretical details and an emphasis on proofs. Certain topics that are often treated with insufficient attention are discussed in more detail here; for example, entire chapters are devoted to regression, multi-class classification, and ranking. The first three chapters lay the theoretical foundation for what follows, but each remaining chapter is mostly self-contained. The appendix offers a concise probability review, a short introduction to convex optimization, tools for concentration bounds, and several basic properties of matrices and norms used in the book. The book is intended for graduate students and researchers in machine learning, statistics, and related areas; it can be used either as a textbook or as a reference text for a research seminar. }
}

@inproceedings{singh2008unlabeled,
	author = {Singh, Aarti and Nowak, Robert and Zhu, Jerry},
	booktitle = {NeurIPS},
	editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
	pages = {1513--1520},
	publisher = {Curran Associates, Inc.},
	title = {Unlabeled data: Now it helps, now it doesn\textquotesingle t},
	url = {https://proceedings.neurips.cc/paper/2008/file/07871915a8107172b3b5dc15a6574ad3-Paper.pdf},
	volume = {21},
	year = {2009}
}

@article{epstein2019generalization,
	title={Generalization bounds for unsupervised and semi-supervised learning with autoencoders},
	author={Epstein, Baruch and Meir, Ron},
	journal={arXiv preprint arXiv:1902.01449},
	year={2019}
}

@article{Mey2019ImprovabilityTS,
	title={Improvability Through Semi-Supervised Learning: A Survey of Theoretical Results},
	author={A. Mey and M. Loog},
	journal={ArXiv},
	year={2019},
	volume={abs/1908.09574}
}

@inproceedings{BenDavid2008DoesUD,
	title={Does Unlabeled Data Provably Help? Worst-case Analysis of the Sample Complexity of Semi-Supervised Learning},
	author={Shai Ben-David and Tyler Lu and D. P{\'a}l},
	booktitle={COLT},
	year={2008}
}

@inproceedings{Darnstdt2013UnlabeledDD,
	title={Unlabeled Data Does Provably Help},
	author={Malte Darnst{\"a}dt and H. U. Simon and Bal{\'a}zs Sz{\"o}r{\'e}nyi},
	booktitle={STACS},
	year={2013}
}

@inproceedings{Zhou2011ClusteredMTLviaASO,
	author = {Zhou, Jiayu and Chen, Jianhui and Ye, Jieping},
	booktitle = {NeurIPS},
	editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K. Q. Weinberger},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Clustered Multi-Task Learning Via Alternating Structure Optimization},
	url = {https://proceedings.neurips.cc/paper/2011/file/a516a87cfcaef229b342c437fe2b95f7-Paper.pdf},
	volume = {24},
	year = {2011}
}


@inproceedings{mairal2013optimization,
	title={Optimization with first-order surrogate functions},
	author={Mairal, Julien},
	booktitle={ICML},
	pages={783--791},
	year={2013}
}

@inproceedings{stich2019local,
	title={Local SGD Converges Fast and Communicates Little},
	author={Stich, Sebastian Urban},
	booktitle={ICLR 2019 ICLR 2019 ICLR},
	number={CONF},
	year={2019}
}

@article{caldas2018leaf,
	title={Leaf: A benchmark for federated settings},
	author={Caldas, Sebastian and Duddu, Sai Meher Karthik and Wu, Peter and Li, Tian and Kone{\v{c}}n{\'y}, Jakub and McMahan, H Brendan and Smith, Virginia and Talwalkar, Ameet},
	journal={arXiv preprint arXiv:1812.01097},
	year={2018},
}




@article{nedic2018network,
	author={A. {Nedi\'{c}} and A. {Olshevsky} and M. G. {Rabbat}},
	journal={Proceedings of the IEEE}, 
	title={Network Topology and Communication-Computation Tradeoffs in Decentralized Optimization}, 
	year={2018},
	volume={106},
	number={5},
	pages={953-976},
	doi={10.1109/JPROC.2018.2817461}
}

@inproceedings{Koloskova2020AUT,
	title={A Unified Theory of Decentralized SGD with Changing Topology and Local Updates},
	author={Anastasia Koloskova and N. Loizou and Sadra Boreiri and M. Jaggi and S. Stich},
	booktitle={ICML},
	year={2020}
}


@article{enshaei2015diagonal,
	author = {Sharareh Enshaei and Wah June Leong and Mahboubeh Farid},
	title = {Diagonal quasi-Newton method via variational principle under generalized Frobenius norm},
	journal = {Optimization Methods and Software},
	volume = {31},
	number = {6},
	pages = {1258-1271},
	year  = {2016},
	publisher = {Taylor & Francis},
	doi = {10.1080/10556788.2016.1196205},
	URL = {https://doi.org/10.1080/10556788.2016.1196205},
	eprint = {https://doi.org/10.1080/10556788.2016.1196205}
}

@Article{HochSchm97LSTM,
	author      = {Sepp Hochreiter and J{\"u}rgen Schmidhuber},
	journal     = {Neural Computation},
	title       = {Long Short-Term Memory},
	year        = {1997},
	number      = {8},
	pages       = {1735--1780},
	volume      = {9},
	optabstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	optdoi      = {10.1162/neco.1997.9.8.1735},
	opteprint   = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
	opturl      = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
}

@inproceedings{sandler2018mobilenetv2,
	title={Mobilenetv2: Inverted residuals and linear bottlenecks},
	author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={4510--4520},
	year={2018}
}

@article{jiang2022model,
  title={Model pruning enables efficient federated learning on edge devices},
  author={Jiang, Yuang and Wang, Shiqiang and Valls, Victor and Ko, Bong Jun and Lee, Wei-Han and Leung, Kin K and Tassiulas, Leandros},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}

@inproceedings{
yuan2022what,
title={What Do We Mean by Generalization in Federated Learning?},
author={Honglin Yuan and Warren Richard Morningstar and Lin Ning and Karan Singhal},
booktitle={International Conference on Learning Representations},
year={2022},
}

@inproceedings{cohen2017emnist,
	title={EMNIST: Extending MNIST to handwritten letters},
	author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Van Schaik, Andre},
	booktitle={IJCNN},
	pages={2921--2926},
	year={2017},
	organization={IEEE}
}

@inproceedings{chai2019towards,
  title={Towards taming the resource and data heterogeneity in federated learning},
  author={Chai, Zheng and Fayyaz, Hannan and Fayyaz, Zeshan and Anwar, Ali and Zhou, Yi and Baracaldo, Nathalie and Ludwig, Heiko and Cheng, Yue},
  booktitle={2019 USENIX Conference on Operational Machine Learning (OpML 19)},
  pages={19--21},
  year={2019}
}

@inproceedings{chai2020tifl,
  title={Tifl: A tier-based federated learning system},
  author={Chai, Zheng and Ali, Ahsan and Zawad, Syed and Truex, Stacey and Anwar, Ali and Baracaldo, Nathalie and Zhou, Yi and Ludwig, Heiko and Yan, Feng and Cheng, Yue},
  booktitle={Proceedings of the 29th International Symposium on High-Performance Parallel and Distributed Computing},
  pages={125--136},
  year={2020}
}


@inproceedings{Sahu2018OnTC,
	title={Federated Optimization in Heterogeneous Networks}, 
	author={Tian Li and Anit Kumar Sahu and Manzil Zaheer and Maziar Sanjabi and Ameet Talwalkar and Virginia Smith},
	year={2020},
	booktitle ={Third MLSys Conference}
}


@article{Lange2012Optimization,
	ISSN = {10618600},
	URL = {http://www.jstor.org/stable/1390605},
	abstract = {The well-known EM algorithm is an optimization transfer algorithm that depends on the notion of incomplete or missing data. By invoking convexity arguments, one can construct a variety of other optimization transfer algorithms that do not involve missing data. These algorithms all rely on a majorizing or minorizing function that serves as a surrogate for the objective function. Optimizing the surrogate function drives the objective function in the correct direction. This article illustrates this general principle by a number of specific examples drawn from the statistical literature. Because optimization transfer algorithms often exhibit the slow convergence of EM algorithms, two methods of accelerating optimization transfer are discussed and evaluated in the context of specific problems.},
	author = {Kenneth Lange and David R. Hunter and Ilsoon Yang},
	journal = {Journal of Computational and Graphical Statistics},
	number = {1},
	pages = {1--20},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
	title = {Optimization Transfer Using Surrogate Objective Functions},
	volume = {9},
	year = {2000}
}

@inproceedings{Wang2020Federated,
	title={Federated Learning with Matched Averaging},
	author={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},
	booktitle={ICLR},
	year={2020},
	url={https://openreview.net/forum?id=BkluqlSFDS}
}

@inproceedings{reddi2021adaptive,
	title={Adaptive Federated Optimization},
	author={Sashank J. Reddi and Zachary Charles and Manzil Zaheer and Zachary Garrett and Keith Rush and Jakub Kone{\v{c}}n{\'y} and Sanjiv Kumar and Hugh Brendan McMahan},
	booktitle={ICLR},
	year={2021},
}

@inproceedings{li2006pachinko,
	author = {Li, Wei and McCallum, Andrew},
	title = {Pachinko Allocation: DAG-Structured Mixture Models of Topic Correlations},
	year = {2006},
	isbn = {1595933832},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1143844.1143917},
	doi = {10.1145/1143844.1143917},
	abstract = {Latent Dirichlet allocation (LDA) and other related topic models are increasingly popular tools for summarization and manifold discovery in discrete data. However, LDA does not capture correlations between topics. In this paper, we introduce the pachinko allocation model (PAM), which captures arbitrary, nested, and possibly sparse correlations between topics using a directed acyclic graph (DAG). The leaves of the DAG represent individual words in the vocabulary, while each interior node represents a correlation among its children, which may be words or other interior nodes (topics). PAM provides a flexible alternative to recent work by Blei and Lafferty (2006), which captures correlations only between pairs of topics. Using text data from newsgroups, historic NIPS proceedings and other research paper corpora, we show improved performance of PAM in document classification, likelihood of held-out data, the ability to support finer-grained topics, and topical keyword coherence.},
	booktitle = {ICML},
	pages = {577â€“584},
	numpages = {8},
	location = {Pittsburgh, Pennsylvania, USA},
	series = {ICML '06}
}

@article{dinh2021fedu,
	title={FedU: A Unified Framework for Federated Multi-Task Learning with Laplacian Regularization},
	author={Dinh, Canh T and Vu, Tung T and Tran, Nguyen H and Dao, Minh N and Zhang, Hongyu},
	journal={arXiv preprint arXiv:2102.07148},
	year={2021}
}

@book{lauritzen1996graphical,
	title = "Graphical models",
	author = "Lauritzen, {Steffen L.}",
	year = "1996",
	language = "English",
	isbn = "0198522193",
	series = "Oxford Statistical Science Series",
	publisher = "Clarendon Press",
	number = "17",
}

@article{luo2019SwitchableNnorm,
	title={Differentiable Learning-to-Normalize via Switchable Normalization},
	author={Ping Luo and Jiamin Ren and Zhanglin Peng and Ruimao Zhang and Jingyu Li},
	journal={ICLR},
	year={2019}
}


@inproceedings{haddadpour2021federated,
	title={Federated learning with compression: Unified analysis and sharp guarantees},
	author={Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mokhtari, Aryan and Mahdavi, Mehrdad},
	booktitle={AISTATS},
	pages={2350--2358},
	year={2021},
	organization={PMLR}
}

@inproceedings{
	li2021fedbn,
	title={Fed{\{}BN{\}}: Federated Learning on Non-{\{}IID{\}} Features via Local Batch Normalization},
	author={Xiaoxiao Li and Meirui Jiang and Xiaofei Zhang and Michael Kamp and Qi Dou},
	booktitle={ICLR},
	year={2021},
}

@article{marfoq2021fedEM,
	title={Federated multi-task learning under a mixture of distributions},
	author={Marfoq, Othmane and Neglia, Giovanni and Bellet, Aur{\'e}lien and Kameni, Laetitia and Vidal, Richard},
	journal={NeurIPS},
	volume={34},
	year={2021}
}

@article{hubara2016binarized,
	title={Binarized neural networks},
	author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
	journal={NeurIPS},
	volume={29},
	year={2016}
}

@inproceedings{
	Li2020Fair,
	title={Fair Resource Allocation in Federated Learning},
	author={Tian Li and Maziar Sanjabi and Ahmad Beirami and Virginia Smith},
	booktitle={ICLR},
	year={2020},
}

@incollection{zhu2020deepLeakage,
  title={Deep leakage from gradients},
  author={Zhu, Ligeng and Han, Song},
  booktitle={NeurIPS},
	volume={32},
  year={2019},
  publisher={Springer}
}

@inproceedings{li2021hermes,
  title={Hermes: an efficient federated learning framework for heterogeneous mobile clients},
  author={Li, Ang and Sun, Jingwei and Li, Pengcheng and Pu, Yu and Li, Hai and Chen, Yiran},
  booktitle={MOBICOM},
  pages={420--437},
  year={2021}
}

@inproceedings{NTK,
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl\'{e}ment},
	title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
	year = {2018},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
	pages = {8580â€“8589},
	numpages = {10},
}