\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International conference on machine learning}, pp.\  242--252. PMLR, 2019.

\bibitem[Antor{\'a}n et~al.(2022)Antor{\'a}n, Janz, Allingham, Daxberger, Barbano, Nalisnick, and Hern{\'a}ndez-Lobato]{antoran2022adapting}
Antor{\'a}n, J., Janz, D., Allingham, J.~U., Daxberger, E., Barbano, R.~R., Nalisnick, E., and Hern{\'a}ndez-Lobato, J.~M.
\newblock Adapting the linearised laplace model evidence for modern deep learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  796--821. PMLR, 2022.

\bibitem[Antorán et~al.(2023)Antorán, Padhy, Barbano, Nalisnick, Janz, and Hernández-Lobato]{antorán2023samplingbased}
Antorán, J., Padhy, S., Barbano, R., Nalisnick, E., Janz, D., and Hernández-Lobato, J.~M.
\newblock Sampling-based inference for large linear models, with application to linearised laplace, 2023.

\bibitem[Bergamin et~al.(2024)Bergamin, Moreno-Mu{\~n}oz, Hauberg, and Arvanitidis]{bergamin2024riemannian}
Bergamin, F., Moreno-Mu{\~n}oz, P., Hauberg, S., and Arvanitidis, G.
\newblock Riemannian laplace approximations for bayesian neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and Wierstra]{blundell2015weight}
Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D.
\newblock Weight uncertainty in neural network.
\newblock In \emph{International conference on machine learning}, pp.\  1613--1622. PMLR, 2015.

\bibitem[Bombari et~al.(2022)Bombari, Amani, and Mondelli]{bombari2022memorization}
Bombari, S., Amani, M.~H., and Mondelli, M.
\newblock Memorization and optimization in deep neural networks with minimum over-parameterization.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem[Brier(1950)]{VERIFICATIONOFFORECASTSEXPRESSEDINTERMSOFPROBABILITY}
Brier, G.~W.
\newblock Verification of forecasts expressed in terms of probability.
\newblock \emph{Monthly Weather Review}, 78\penalty0 (1):\penalty0 1 -- 3, 1950.
\newblock \doi{10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2}.
\newblock URL \url{https://journals.ametsoc.org/view/journals/mwre/78/1/1520-0493_1950_078_0001_vofeit_2_0_co_2.xml}.

\bibitem[Daxberger et~al.(2021{\natexlab{a}})Daxberger, Kristiadi, Immer, Eschenhagen, Bauer, and Hennig]{NEURIPS2021_a7c95857}
Daxberger, E., Kristiadi, A., Immer, A., Eschenhagen, R., Bauer, M., and Hennig, P.
\newblock Laplace redux - effortless {B}ayesian deep learning.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~34, pp.\  20089--20103. Curran Associates, Inc., 2021{\natexlab{a}}.

\bibitem[Daxberger et~al.(2021{\natexlab{b}})Daxberger, Kristiadi, Immer, Eschenhagen, Bauer, and Hennig]{laplace2021}
Daxberger, E., Kristiadi, A., Immer, A., Eschenhagen, R., Bauer, M., and Hennig, P.
\newblock Laplace redux--effortless {B}ayesian deep learning.
\newblock In \emph{{N}eur{IPS}}, 2021{\natexlab{b}}.

\bibitem[Daxberger et~al.(2021{\natexlab{c}})Daxberger, Nalisnick, Allingham, Antor{\'a}n, and Hern{\'a}ndez-Lobato]{daxberger2021bayesian}
Daxberger, E., Nalisnick, E., Allingham, J.~U., Antor{\'a}n, J., and Hern{\'a}ndez-Lobato, J.~M.
\newblock Bayesian deep learning via subnetwork inference.
\newblock In \emph{International Conference on Machine Learning}, pp.\  2510--2521. PMLR, 2021{\natexlab{c}}.

\bibitem[Daxberger et~al.(2022)Daxberger, Kristiadi, Immer, Eschenhagen, Bauer, and Hennig]{daxberger2022laplace}
Daxberger, E., Kristiadi, A., Immer, A., Eschenhagen, R., Bauer, M., and Hennig, P.
\newblock Laplace redux -- effortless bayesian deep learning, 2022.

\bibitem[Deng et~al.(2022)Deng, Zhou, and Zhu]{deng2022accelerated}
Deng, Z., Zhou, F., and Zhu, J.
\newblock Accelerated linearized laplace approximation for bayesian deep learning, 2022.

\bibitem[Devroye et~al.(1996)Devroye, Gy{\"o}rfi, and Lugosi]{devroye1996probabilistic}
Devroye, L., Gy{\"o}rfi, L., and Lugosi, G.
\newblock \emph{A probabilistic theory of pattern recognition}.
\newblock Springer Science \& Business Media, 1996.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh2017sharp}
Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1019--1028. PMLR, 2017.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International conference on machine learning}, pp.\  1675--1685. PMLR, 2019.

\bibitem[George et~al.(2018)George, Laurent, Bouthillier, Ballas, and Vincent]{george2018fast}
George, T., Laurent, C., Bouthillier, X., Ballas, N., and Vincent, P.
\newblock Fast approximate natural gradient descent in a kronecker factored eigenbasis.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Girolami \& Calderhead(2011)Girolami and Calderhead]{girolami2011riemann}
Girolami, M. and Calderhead, B.
\newblock Riemann manifold langevin and hamiltonian monte carlo methods.
\newblock \emph{Journal of the Royal Statistical Society Series B: Statistical Methodology}, 73\penalty0 (2):\penalty0 123--214, 2011.

\bibitem[Hansen \& Salamon(1990)Hansen and Salamon]{hansen1990neural}
Hansen, L.~K. and Salamon, P.
\newblock Neural network ensembles.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 12\penalty0 (10):\penalty0 993--1001, 1990.

\bibitem[Hauberg(2018)]{hauberg:SN:2018}
Hauberg, S.
\newblock Directional statistics with the spherical normal distribution.
\newblock In \emph{Proceedings of FUSION 2018}, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hsu(2002)]{hsu2002stochastic}
Hsu, E.~P.
\newblock \emph{Stochastic analysis on manifolds}.
\newblock American Mathematical Soc., 2002.

\bibitem[Immer et~al.(2021{\natexlab{a}})Immer, Bauer, Fortuin, R{\"a}tsch, and Emtiyaz]{immer2021scalable}
Immer, A., Bauer, M., Fortuin, V., R{\"a}tsch, G., and Emtiyaz, K.~M.
\newblock Scalable marginal likelihood estimation for model selection in deep learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\  4563--4573, 2021{\natexlab{a}}.

\bibitem[Immer et~al.(2021{\natexlab{b}})Immer, Korzepa, and Bauer]{immer2021improving}
Immer, A., Korzepa, M., and Bauer, M.
\newblock Improving predictions of {B}ayesian neural nets via local linearization.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics (AISTATS)}, pp.\  703--711, 2021{\natexlab{b}}.

\bibitem[Izmailov et~al.(2020)Izmailov, Maddox, Kirichenko, Garipov, Vetrov, and Wilson]{izmailov2020subspace}
Izmailov, P., Maddox, W.~J., Kirichenko, P., Garipov, T., Vetrov, D., and Wilson, A.~G.
\newblock Subspace inference for bayesian deep learning.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  1169--1179. PMLR, 2020.

\bibitem[Izmailov et~al.(2021)Izmailov, Nicholson, Lotfi, and Wilson]{izmailov2021dangers}
Izmailov, P., Nicholson, P., Lotfi, S., and Wilson, A.~G.
\newblock Dangers of bayesian model averaging under covariate shift.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 3309--3322, 2021.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural networks.
\newblock In \emph{Advances in neural information processing systems}, 2018.

\bibitem[Jang et~al.(2022)Jang, Lee, Park, and Noh]{jang2022reparametrization}
Jang, C., Lee, S., Park, F., and Noh, Y.-K.
\newblock A reparametrization-invariant sharpness measure based on information geometry.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 27893--27905, 2022.

\bibitem[Khan et~al.(2019)Khan, Immer, Abedi, and Korzepa]{khan2019approximate}
Khan, M. E.~E., Immer, A., Abedi, E., and Korzepa, M.
\newblock Approximate inference turns deep networks into {G}aussian processes.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 32, 2019.

\bibitem[Kim et~al.(2022)Kim, Park, Kim, and Yang]{kim2022scale}
Kim, S., Park, S., Kim, K.-S., and Yang, E.
\newblock Scale-invariant bayesian neural networks with connectivity tangent kernel.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Kim et~al.(2024)Kim, Kim, and Yang]{kim2024gex}
Kim, S., Kim, K., and Yang, E.
\newblock Gex: A flexible method for approximating influence via geometric ensemble.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Kristiadi et~al.(2020)Kristiadi, Hein, and Hennig]{pmlr-v119-kristiadi20a}
Kristiadi, A., Hein, M., and Hennig, P.
\newblock Being {B}ayesian, even just a bit, fixes overconfidence in {R}e{LU} networks.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th International Conference on Machine Learning}, volume 119 of \emph{Proceedings of Machine Learning Research}, pp.\  5436--5446. PMLR, 13--18 Jul 2020.

\bibitem[Kristiadi et~al.(2023)Kristiadi, Dangel, and Hennig]{kristiadi2023geometry}
Kristiadi, A., Dangel, F., and Hennig, P.
\newblock The geometry of neural nets' parameter spaces under reparametrization.
\newblock \emph{arXiv preprint arXiv:2302.07384}, 2023.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and Blundell]{lakshminarayanan2017simple}
Lakshminarayanan, B., Pritzel, A., and Blundell, C.
\newblock Simple and scalable predictive uncertainty estimation using deep ensembles.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Lanczos(1950)]{lanczos:hal-01712947}
Lanczos, C.
\newblock {An iteration method for the solution of the eigenvalue problem of linear differential and integral operators}.
\newblock \emph{{Journal of Research of the National Bureau of Standards}}, 45\penalty0 (4), October 1950.
\newblock \doi{10.6028/jres.045.026}.
\newblock URL \url{https://hal.science/hal-01712947}.

\bibitem[Lawrence(2001)]{lawrence2001variational}
Lawrence, N.~D.
\newblock \emph{Variational inference in probabilistic models}.
\newblock PhD thesis, Citeseer, 2001.

\bibitem[LeCun et~al.(1989)LeCun, Boser, Denker, Henderson, Howard, Hubbard, and Jackel]{lecun1989backpropagation}
LeCun, Y., Boser, B., Denker, J.~S., Henderson, D., Howard, R.~E., Hubbard, W., and Jackel, L.~D.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock \emph{Neural computation}, 1\penalty0 (4):\penalty0 541--551, 1989.

\bibitem[Lee \& Lee(2012)Lee and Lee]{lee2012smooth}
Lee, J.~M. and Lee, J.~M.
\newblock \emph{Smooth manifolds}.
\newblock Springer, 2012.

\bibitem[Li et~al.(2015)Li, Chen, Carlson, and Carin]{li2015preconditioned}
Li, C., Chen, C., Carlson, D., and Carin, L.
\newblock Preconditioned stochastic gradient langevin dynamics for deep neural networks, 2015.

\bibitem[Li et~al.(2021)Li, Wang, and Arora]{li2021happens}
Li, Z., Wang, T., and Arora, S.
\newblock What happens after sgd reaches zero loss?--a mathematical framework.
\newblock \emph{arXiv preprint arXiv:2110.06914}, 2021.

\bibitem[Lippe(2022)]{lippe2022uvadlc}
Lippe, P.
\newblock {UvA Deep Learning Tutorials}.
\newblock \url{https://uvadlc-notebooks.readthedocs.io/en/latest/}, 2022.

\bibitem[Liu et~al.(2020)Liu, Zhu, and Belkin]{liu2020linearity}
Liu, C., Zhu, L., and Belkin, M.
\newblock On the linearity of large non-linear models: when and why the tangent kernel is constant.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 15954--15964, 2020.

\bibitem[MacKay(1998)]{mackay1998choice}
MacKay, D.~J.
\newblock Choice of basis for laplace approximation.
\newblock \emph{Machine learning}, 33:\penalty0 77--86, 1998.

\bibitem[MacKay(1992)]{mackay1992laplace}
MacKay, D. J.~C.
\newblock A practical {B}ayesian framework for backpropagation networks.
\newblock \emph{Neural Computation}, 4\penalty0 (3):\penalty0 448--472, 1992.

\bibitem[Maddox et~al.(2019)Maddox, Izmailov, Garipov, Vetrov, and Wilson]{maddox2019simple}
Maddox, W.~J., Izmailov, P., Garipov, T., Vetrov, D.~P., and Wilson, A.~G.
\newblock A simple baseline for bayesian uncertainty in deep learning.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Maruyama(1955)]{maruyama1955continuous}
Maruyama, G.
\newblock Continuous markov processes and stochastic equations.
\newblock \emph{Rendiconti del Circolo Matematico di Palermo}, 4:\penalty0 48--90, 1955.

\bibitem[Miani et~al.(2022)Miani, Warburg, Moreno-Muñoz, Detlefsen, and Hauberg]{miani:neurips:2022}
Miani, M., Warburg, F., Moreno-Muñoz, P., Detlefsen, N.~S., and Hauberg, S.
\newblock Laplacian autoencoders for learning stochastic representations.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem[Naeini et~al.(2015)Naeini, Cooper, and Hauskrecht]{naeini2015obtaining}
Naeini, M.~P., Cooper, G., and Hauskrecht, M.
\newblock Obtaining well calibrated probabilities using bayesian binning.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~29, 2015.

\bibitem[Nguyen et~al.(2021)Nguyen, Mondelli, and Montufar]{nguyen2021tight}
Nguyen, Q., Mondelli, M., and Montufar, G.~F.
\newblock Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep relu networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  8119--8129. PMLR, 2021.

\bibitem[Oymak \& Soltanolkotabi(2019)Oymak and Soltanolkotabi]{oymak2019overparameterized}
Oymak, S. and Soltanolkotabi, M.
\newblock Overparameterized nonlinear learning: Gradient descent takes the shortest path?
\newblock In \emph{International Conference on Machine Learning}, pp.\  4951--4960. PMLR, 2019.

\bibitem[Oymak \& Soltanolkotabi(2020)Oymak and Soltanolkotabi]{oymak2020toward}
Oymak, S. and Soltanolkotabi, M.
\newblock Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory}, 1\penalty0 (1):\penalty0 84--105, 2020.

\bibitem[Papamarkou et~al.(2024)Papamarkou, Skoularidou, Palla, Aitchison, Arbel, Dunson, Filippone, Fortuin, Hennig, Hubin, et~al.]{papamarkou2024position}
Papamarkou, T., Skoularidou, M., Palla, K., Aitchison, L., Arbel, J., Dunson, D., Filippone, M., Fortuin, V., Hennig, P., Hubin, A., et~al.
\newblock Position paper: Bayesian deep learning in the age of large-scale ai.
\newblock \emph{arXiv preprint arXiv:2402.00809}, 2024.

\bibitem[Petzka et~al.(2019)Petzka, Adilova, Kamp, and Sminchisescu]{petzka2019reparameterization}
Petzka, H., Adilova, L., Kamp, M., and Sminchisescu, C.
\newblock A reparameterization-invariant flatness measure for deep neural networks.
\newblock \emph{arXiv preprint arXiv:1912.00058}, 2019.

\bibitem[Ritter et~al.(2018)Ritter, Botev, and Barber]{ritter2018scalable}
Ritter, H., Botev, A., and Barber, D.
\newblock A scalable laplace approximation for neural networks.
\newblock In \emph{6th international conference on learning representations, ICLR 2018-conference track proceedings}, volume~6. International Conference on Representation Learning, 2018.

\bibitem[Sharma et~al.(2023)Sharma, Farquhar, Nalisnick, and Rainforth]{sharma2023bayesian}
Sharma, M., Farquhar, S., Nalisnick, E., and Rainforth, T.
\newblock {Do Bayesian Neural Networks Need To Be Fully Stochastic?}
\newblock In \emph{International Conference on Artificial Intelligence and Statistics (AISTATS, notable paper award)}, 2023.

\bibitem[Warburg et~al.(2023)Warburg, Miani, Brack, and Hauberg]{warburg:metric:2023}
Warburg, F., Miani, M., Brack, S., and Hauberg, S.
\newblock Bayesian metric learning for uncertainty quantification in image retrieval.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem[Yu et~al.(2023)Yu, Hartmann, Williams, Girolami, and Klami]{yu2023riemannian}
Yu, H., Hartmann, M., Williams, B., Girolami, M., and Klami, A.
\newblock Riemannian laplace approximation with the fisher metric, 2023.

\end{thebibliography}
