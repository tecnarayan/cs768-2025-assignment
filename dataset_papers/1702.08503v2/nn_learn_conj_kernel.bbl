\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andoni et~al.(2014)Andoni, Panigrahy, Valiant, and
  Zhang]{andoni2014learning}
A.~Andoni, R.~Panigrahy, G.~Valiant, and L.~Zhang.
\newblock Learning polynomials with neural networks.
\newblock In \emph{Proceedings of the 31st International Conference on Machine
  Learning}, pages 1908--1916, 2014.

\bibitem[Anselmi et~al.(2015)Anselmi, Rosasco, Tan, and
  Poggio]{anselmi2015deep}
F.~Anselmi, L.~Rosasco, C.~Tan, and T.~Poggio.
\newblock Deep convolutional networks are hierarchical kernel machines.
\newblock \emph{arXiv:1508.01084}, 2015.

\bibitem[Arora et~al.(2014)Arora, Bhaskara, Ge, and Ma]{arora2014provable}
Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma.
\newblock Provable bounds for learning some deep representations.
\newblock In \emph{ICML}, pages 584--592, 2014.

\bibitem[Arora et~al.(2016)Arora, Ge, Ma, and Risteski]{arora2016provable}
Sanjeev Arora, Rong Ge, Tengyu Ma, and Andrej Risteski.
\newblock Provable learning of noisy-or networks.
\newblock \emph{arXiv preprint arXiv:1612.08795}, 2016.

\bibitem[Bach(2014)]{bach2014breaking}
F.~Bach.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock \emph{arXiv:1412.8690}, 2014.

\bibitem[Bach(2015)]{bach2015equivalence}
F.~Bach.
\newblock On the equivalence between kernel quadrature rules and random feature
  expansions.
\newblock 2015.

\bibitem[Blum et~al.(1994)Blum, Furst, Jackson, Kearns, Mansour, and
  Rudich]{blum1994weakly}
A.~Blum, M.~Furst, J.~Jackson, M.~Kearns, Y.~Mansour, and Steven Rudich.
\newblock Weakly learning {DNF} and characterizing statistical query learning
  using fourier analysis.
\newblock In \emph{Proceedings of the twenty-sixth annual ACM symposium on
  Theory of computing}, pages 253--262. ACM, 1994.

\bibitem[Blum and Rivest(1989)]{BlumRi89}
Avrim Blum and Ronald~L. Rivest.
\newblock Training a 3-node neural net is {NP-Complete}.
\newblock In David~S. Touretzky, editor, \emph{Advances in Neural Information
  Processing Systems I}, pages 494--501. Morgan Kaufmann, 1989.

\bibitem[Bshouty et~al.(1998)Bshouty, Tamon, and Wilson]{bshouty1998learning}
Nader~H Bshouty, Christino Tamon, and David~K Wilson.
\newblock On learning width two branching programs.
\newblock \emph{Information Processing Letters}, 65\penalty0 (4):\penalty0
  217--222, 1998.

\bibitem[Cho and Saul(2009)]{cho2009kernel}
Y.~Cho and L.K. Saul.
\newblock Kernel methods for deep learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  342--350, 2009.

\bibitem[Daniely and Shalev-Shwartz(2016)]{danielySh2014}
A.~Daniely and S.~Shalev-Shwartz.
\newblock Complexity theoretic limitations on learning {DNF}s.
\newblock In \emph{COLT}, 2016.

\bibitem[Daniely et~al.(2014)Daniely, Linial, and
  Shalev-Shwartz]{daniely2013average}
A.~Daniely, N.~Linial, and S.~Shalev-Shwartz.
\newblock From average case complexity to improper learning complexity.
\newblock In \emph{STOC}, 2014.

\bibitem[Daniely et~al.(2016)Daniely, Frostig, and Singer]{daniely2016toward}
Amit Daniely, Roy Frostig, and Yoram Singer.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In \emph{NIPS}, 2016.

\bibitem[Daniely et~al.(2017)Daniely, Frostig, Gupta, and
  Singer]{daniely2017random}
Amit Daniely, Roy Frostig, Vineet Gupta, and Yoram Singer.
\newblock Random features for compositional kernels.
\newblock \emph{arXiv preprint arXiv:1703.07872}, 2017.

\bibitem[Glorot and Bengio(2010)]{glorot2010understanding}
X.~Glorot and Y.~Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{International conference on artificial intelligence and
  statistics}, pages 249--256, 2010.

\bibitem[Hazan and Jaakkola(2015)]{hazan2015steps}
T.~Hazan and T.~Jaakkola.
\newblock Steps toward deep kernel methods from infinite neural networks.
\newblock \emph{arXiv:1508.05133}, 2015.

\bibitem[Kamath()]{kamathbounds}
Gautam~C Kamath.
\newblock Bounds on the expectation of the maximum of samples from a gaussian,
  2015.
\newblock \emph{URL http://www. gautamkamath. com/writings/gaussian\_max. pdf}.

\bibitem[Kar and Karnick(2012)]{kar2012random}
P.~Kar and H.~Karnick.
\newblock Random feature maps for dot product kernels.
\newblock \emph{arXiv:1201.6530}, 2012.

\bibitem[Kearns and Valiant(1994)]{KearnsVa94}
M.~Kearns and L.G. Valiant.
\newblock Cryptographic limitations on learning {B}oolean formulae and finite
  automata.
\newblock \emph{Journal of the Association for Computing Machinery},
  41\penalty0 (1):\penalty0 67--95, January 1994.

\bibitem[Kharitonov(1993)]{Kharitonov93}
Michael Kharitonov.
\newblock Cryptographic hardness of distribution-specific learning.
\newblock In \emph{Proceedings of the twenty-fifth annual ACM symposium on
  Theory of computing}, pages 372--381. ACM, 1993.

\bibitem[Klivans and Sherstov(2006)]{KlivansSh06}
A.R. Klivans and A.A. Sherstov.
\newblock Cryptographic hardness for learning intersections of halfspaces.
\newblock In \emph{FOCS}, 2006.

\bibitem[Klivans and Sherstov(2007)]{klivans2007unconditional}
A.R. Klivans and A.A. Sherstov.
\newblock Unconditional lower bounds for learning intersections of halfspaces.
\newblock \emph{Machine Learning}, 69\penalty0 (2-3):\penalty0 97--114, 2007.

\bibitem[Livni et~al.(2014)Livni, Shalev-Shwartz, and
  Shamir]{livni2014computational}
R.~Livni, S.~Shalev-Shwartz, and O.~Shamir.
\newblock On the computational efficiency of training neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  855--863, 2014.

\bibitem[Mairal et~al.(2014)Mairal, Koniusz, Harchaoui, and
  Schmid]{mairal2014convolutional}
J.~Mairal, P.~Koniusz, Z.~Harchaoui, and Cordelia Schmid.
\newblock Convolutional kernel networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2627--2635, 2014.

\bibitem[Neal(2012)]{neal2012bayesian}
R.M. Neal.
\newblock \emph{Bayesian learning for neural networks}, volume 118.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Pennington et~al.(2015)Pennington, Yu, and
  Kumar]{pennington2015spherical}
J.~Pennington, F.~Yu, and S.~Kumar.
\newblock Spherical random features for polynomial kernels.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1837--1845, 2015.

\bibitem[Rahimi and Recht(2007)]{RahimiRe07}
A.~Rahimi and B.~Recht.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{NIPS}, pages 1177--1184, 2007.

\bibitem[Rahimi and Recht(2009)]{rahimi2009weighted}
A.~Rahimi and B.~Recht.
\newblock Weighted sums of random kitchen sinks: Replacing minimization with
  randomization in learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  1313--1320, 2009.

\bibitem[Scarselli and Tsoi(1998)]{scarselli1998universal}
Franco Scarselli and Ah~Chung Tsoi.
\newblock Universal approximation using feedforward neural networks: A survey
  of some existing methods, and some new results.
\newblock \emph{Neural networks}, 11\penalty0 (1):\penalty0 15--37, 1998.

\bibitem[Shalev-Shwartz and Ben-David(2014)]{shalev2014understanding}
S.~Shalev-Shwartz and S.~Ben-David.
\newblock \emph{Understanding Machine Learning: From Theory to Algorithms}.
\newblock Cambridge University Press, 2014.

\bibitem[Vershynin(2010)]{vershynin2010introduction}
Roman Vershynin.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock \emph{arXiv preprint arXiv:1011.3027 v7}, 2010.

\bibitem[Williams(1997)]{williams1997infinite}
C.K.I. Williams.
\newblock Computation with infinite neural networks.
\newblock pages 295--301, 1997.

\bibitem[Zhang et~al.(2015)Zhang, Lee, Wainwright, and
  Jordan]{zhang2015learning}
Yuchen Zhang, Jason~D Lee, Martin~J Wainwright, and Michael~I Jordan.
\newblock Learning halfspaces and neural networks with random initialization.
\newblock \emph{arXiv preprint arXiv:1511.07948}, 2015.

\bibitem[Zhang et~al.(2016{\natexlab{a}})Zhang, Lee, and Jordan]{zhang2016l1}
Yuchen Zhang, Jason~D Lee, and Michael~I Jordan.
\newblock l1-regularized neural networks are improperly learnable in polynomial
  time.
\newblock In \emph{NIPS}, 2016{\natexlab{a}}.

\bibitem[Zhang et~al.(2016{\natexlab{b}})Zhang, Liang, and
  Wainwright]{zhang2016convexified}
Yuchen Zhang, Percy Liang, and Martin~J Wainwright.
\newblock Convexified convolutional neural networks.
\newblock \emph{arXiv preprint arXiv:1609.01000}, 2016{\natexlab{b}}.

\end{thebibliography}
