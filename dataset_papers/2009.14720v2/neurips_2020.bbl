\begin{thebibliography}{10}

\bibitem{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock {\em arXiv preprint arXiv:1412.6572}, 2014.

\bibitem{carlini2017towards}
Nicholas Carlini and David Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock In {\em Security and Privacy (SP), 2017 IEEE Symposium on}, pages
  39--57. IEEE, 2017.

\bibitem{madry2018towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{papernot2016transferability}
Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow.
\newblock Transferability in machine learning: from phenomena to black-box
  attacks using adversarial samples.
\newblock {\em arXiv preprint arXiv:1605.07277}, 2016.

\bibitem{ilyas2019adversarial}
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon
  Tran, and Aleksander Madry.
\newblock Adversarial examples are not bugs, they are features.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  125--136, 2019.

\bibitem{Inkawhich2020Transferable}
Nathan Inkawhich, Kevin Liang, Lawrence Carin, and Yiran Chen.
\newblock Transferable perturbations of deep feature distributions.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{li2015convergent}
Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John~E Hopcroft.
\newblock Convergent learning: Do different neural networks learn the same
  representations?
\newblock In {\em FE@ NIPS}, pages 196--212, 2015.

\bibitem{tsipras2018robustness}
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
  Aleksander Madry.
\newblock Robustness may be at odds with accuracy.
\newblock {\em arXiv preprint arXiv:1805.12152}, 2018.

\bibitem{breiman1996bagging}
Leo Breiman.
\newblock Bagging predictors.
\newblock {\em Machine learning}, 24(2):123--140, 1996.

\bibitem{dietterich2000ensemble}
Thomas~G Dietterich.
\newblock Ensemble methods in machine learning.
\newblock In {\em International workshop on multiple classifier systems}, pages
  1--15. Springer, 2000.

\bibitem{bagnall2017training}
Alexander Bagnall, Razvan Bunescu, and Gordon Stewart.
\newblock Training ensembles to detect adversarial examples.
\newblock {\em arXiv preprint arXiv:1712.04006}, 2017.

\bibitem{pang2019improving}
Tianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu.
\newblock Improving adversarial robustness via promoting ensemble diversity.
\newblock {\em arXiv preprint arXiv:1901.08846}, 2019.

\bibitem{kariyappa2019improving}
Sanjay Kariyappa and Moinuddin~K Qureshi.
\newblock Improving adversarial robustness of ensembles with diversity
  training.
\newblock {\em arXiv preprint arXiv:1901.09981}, 2019.

\bibitem{tramer2020adaptive}
Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry.
\newblock On adaptive attacks to adversarial example defenses.
\newblock {\em arXiv preprint arXiv:2002.08347}, 2020.

\bibitem{papernot2016limitations}
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z~Berkay
  Celik, and Ananthram Swami.
\newblock The limitations of deep learning in adversarial settings.
\newblock In {\em 2016 IEEE European symposium on security and privacy
  (EuroS\&P)}, pages 372--387. IEEE, 2016.

\bibitem{dong2017discovering}
Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Xiaolin Hu, and Jun Zhu.
\newblock Discovering adversarial examples with momentum.
\newblock {\em arXiv preprint arXiv:1710.06081}, 2017.

\bibitem{zheng2019distributionally}
Tianhang Zheng, Changyou Chen, and Kui Ren.
\newblock Distributionally adversarial attack.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 2253--2260, 2019.

\bibitem{hansen1990neural}
Lars~Kai Hansen and Peter Salamon.
\newblock Neural network ensembles.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  12(10):993--1001, 1990.

\bibitem{kuncheva2003measures}
Ludmila~I Kuncheva and Christopher~J Whitaker.
\newblock Measures of diversity in classifier ensembles and their relationship
  with the ensemble accuracy.
\newblock {\em Machine learning}, 51(2):181--207, 2003.

\bibitem{lakshminarayanan2017simple}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In {\em Advances in neural information processing systems}, pages
  6402--6413, 2017.

\bibitem{sinha2020dibs}
Samarth Sinha, Homanga Bharadhwaj, Anirudh Goyal, Hugo Larochelle, Animesh
  Garg, and Florian Shkurti.
\newblock Dibs: Diversity inducing information bottleneck in model ensembles.
\newblock {\em arXiv preprint arXiv:2003.04514}, 2020.

\bibitem{wen2020batchensemble}
Yeming Wen, Dustin Tran, and Jimmy Ba.
\newblock Batchensemble: an alternative approach to efficient ensemble and
  lifelong learning.
\newblock {\em arXiv preprint arXiv:2002.06715}, 2020.

\bibitem{dusenberry2020efficient}
Michael~W Dusenberry, Ghassen Jerfel, Yeming Wen, Yi-an Ma, Jasper Snoek,
  Katherine Heller, Balaji Lakshminarayanan, and Dustin Tran.
\newblock Efficient and scalable bayesian neural nets with rank-1 factors.
\newblock {\em arXiv preprint arXiv:2005.07186}, 2020.

\bibitem{tramer2017space}
Florian Tram{\`e}r, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick
  McDaniel.
\newblock The space of transferable adversarial examples.
\newblock {\em arXiv preprint arXiv:1704.03453}, 2017.

\bibitem{Xie2020Intriguing}
Cihang Xie and Alan Yuille.
\newblock Intriguing properties of adversarial training at scale.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{tramer2017ensemble}
Florian Tram{\`e}r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan
  Boneh, and Patrick McDaniel.
\newblock Ensemble adversarial training: Attacks and defenses.
\newblock {\em arXiv preprint arXiv:1705.07204}, 2017.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem{dong2018boosting}
Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and
  Jianguo Li.
\newblock Boosting adversarial attacks with momentum.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 9185--9193, 2018.

\bibitem{xie2019improving}
Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and
  Alan~L Yuille.
\newblock Improving transferability of adversarial examples with input
  diversity.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 2730--2739, 2019.

\bibitem{wu2020skip}
Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, and Xingjun Ma.
\newblock Skip connections matter: On the transferability of adversarial
  examples generated with resnets.
\newblock {\em arXiv preprint arXiv:2002.05990}, 2020.

\bibitem{hamonrobustness}
Ronan Hamon, Henrik Junklewitz, and Ignacio Sanchez.
\newblock Robustness and explainability of artificial intelligence.
\newblock 2020.

\bibitem{strubell2018linguistically}
Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, and Andrew McCallum.
\newblock Linguistically-informed self-attention for semantic role labeling.
\newblock {\em arXiv preprint arXiv:1804.08199}, 2018.

\bibitem{danks2020}
David Danks.
\newblock How adversarial attacks could destabilize military ai systems.
\newblock
  \url{https://spectrum.ieee.org/automaton/artificial-intelligence/embedded-ai/adversarial-attacks-and-ai-systems},
  2020.
\newblock [Online; accessed 2-June-2020].

\bibitem{oh2019towards}
Seong~Joon Oh, Bernt Schiele, and Mario Fritz.
\newblock Towards reverse-engineering black-box neural networks.
\newblock In {\em Explainable AI: Interpreting, Explaining and Visualizing Deep
  Learning}, pages 121--144. Springer, 2019.

\bibitem{tramer2016stealing}
Florian Tram{\`e}r, Fan Zhang, Ari Juels, Michael~K Reiter, and Thomas
  Ristenpart.
\newblock Stealing machine learning models via prediction apis.
\newblock In {\em 25th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$
  Security 16)}, pages 601--618, 2016.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock In {\em NIPS-W}, 2017.

\bibitem{ding2019advertorch}
Gavin~Weiguang Ding, Luyu Wang, and Xiaomeng Jin.
\newblock {AdverTorch} v0.1: An adversarial robustness toolbox based on
  pytorch.
\newblock {\em arXiv preprint arXiv:1902.07623}, 2019.

\bibitem{athalye2018obfuscated}
Anish Athalye, Nicholas Carlini, and David Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock {\em arXiv preprint arXiv:1802.00420}, 2018.

\bibitem{carlini2019evaluating}
Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas
  Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey
  Kurakin.
\newblock On evaluating adversarial robustness.
\newblock {\em arXiv preprint arXiv:1902.06705}, 2019.

\end{thebibliography}
