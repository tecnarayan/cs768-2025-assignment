\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Athalye \& Carlini(2018)Athalye and Carlini]{athalye2018cvpr}
Athalye, A. and Carlini, N.
\newblock On the robustness of the cvpr 2018 white-box adversarial example
  defenses.
\newblock In \emph{Computer Vision: Challenges and Opportunities for Privacy
  and Security,}, 2018.

\bibitem[Athalye et~al.(2018{\natexlab{a}})Athalye, Carlini, and
  Wagner]{athalye2018obfuscated}
Athalye, A., Carlini, N., and Wagner, D.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock In \emph{International Conference on Machine Learning},
  2018{\natexlab{a}}.

\bibitem[Athalye et~al.(2018{\natexlab{b}})Athalye, Engstrom, Ilyas, and
  Kwok]{athalye2017synthesizing}
Athalye, A., Engstrom, L., Ilyas, A., and Kwok, K.
\newblock Synthesizing robust adversarial examples.
\newblock In \emph{International Conference on Machine Learning},
  2018{\natexlab{b}}.

\bibitem[Bafna et~al.(2018)Bafna, Murtagh, and Vyas]{bafna2018thwarting}
Bafna, M., Murtagh, J., and Vyas, N.
\newblock Thwarting adversarial examples: An l0-robust sparse fourier
  transform.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10096--10106, 2018.

\bibitem[Biggio et~al.(2013)Biggio, Corona, Maiorca, Nelson, {\v{S}}rndi{\'c},
  Laskov, Giacinto, and Roli]{biggio2013evasion}
Biggio, B., Corona, I., Maiorca, D., Nelson, B., {\v{S}}rndi{\'c}, N., Laskov,
  P., Giacinto, G., and Roli, F.
\newblock Evasion attacks against machine learning at test time.
\newblock In \emph{Joint European conference on machine learning and knowledge
  discovery in databases}, pp.\  387--402. Springer, 2013.

\bibitem[Brendel et~al.(2018)Brendel, Rauber, and Bethge]{brendel2017decision}
Brendel, W., Rauber, J., and Bethge, M.
\newblock Decision-based adversarial attacks: Reliable attacks against
  black-box machine learning models.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Brendel et~al.(2019)Brendel, Rauber, Matthias, Ustyuzhaninov, and
  Bethge]{brendel2019bethge}
Brendel, W., Rauber, J., Matthias, K., Ustyuzhaninov, I., and Bethge, M.
\newblock Accurate, reliable and fast robustness evaluation.
\newblock \emph{33rd Conference on Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Cand{\`e}s \& Recht(2009)Cand{\`e}s and Recht]{candes2009exact}
Cand{\`e}s, E.~J. and Recht, B.
\newblock Exact matrix completion via convex optimization.
\newblock \emph{Foundations of Computational mathematics}, 9\penalty0
  (6):\penalty0 717, 2009.

\bibitem[Cao \& Gong(2017)Cao and Gong]{cao2017mitigating}
Cao, X. and Gong, N.~Z.
\newblock Mitigating evasion attacks to deep neural networks via region-based
  classification.
\newblock In \emph{Proceedings of the 33rd Annual Computer Security
  Applications Conference}, pp.\  278--287, 2017.

\bibitem[Carlini(2019)]{carlini2019ami}
Carlini, N.
\newblock Is {AmI} (attacks meet interpretability) robust to adversarial
  examples?
\newblock \emph{arXiv preprint arXiv:1902.02322}, 2019.

\bibitem[Carlini \& Wagner(2016)Carlini and Wagner]{carlini2016defensive}
Carlini, N. and Wagner, D.
\newblock Defensive distillation is not robust to adversarial examples.
\newblock \emph{arXiv preprint arXiv:1607.04311}, 2016.

\bibitem[Carlini \& Wagner(2017{\natexlab{a}})Carlini and
  Wagner]{carlini2017adversarial}
Carlini, N. and Wagner, D.
\newblock Adversarial examples are not easily detected: Bypassing ten detection
  methods.
\newblock In \emph{Proceedings of the 10th ACM Workshop on Artificial
  Intelligence and Security}, pp.\  3--14, 2017{\natexlab{a}}.

\bibitem[Carlini \& Wagner(2017{\natexlab{b}})Carlini and
  Wagner]{carlini2017towards}
Carlini, N. and Wagner, D.
\newblock Towards evaluating the robustness of neural networks.
\newblock In \emph{2017 IEEE symposium on security and privacy}, pp.\  39--57.
  IEEE, 2017{\natexlab{b}}.

\bibitem[Carlini et~al.(2019)Carlini, Athalye, Papernot, Brendel, Rauber,
  Tsipras, Goodfellow, and Madry]{carlini2019evaluating}
Carlini, N., Athalye, A., Papernot, N., Brendel, W., Rauber, J., Tsipras, D.,
  Goodfellow, I., and Madry, A.
\newblock On evaluating adversarial robustness.
\newblock \emph{arXiv preprint arXiv:1902.06705}, 2019.

\bibitem[Chatterjee et~al.(2015)]{chatterjee2015matrix}
Chatterjee, S. et~al.
\newblock Matrix estimation by universal singular value thresholding.
\newblock \emph{The Annals of Statistics}, 43\penalty0 (1):\penalty0 177--214,
  2015.

\bibitem[Chen et~al.(2017)Chen, Zhang, Sharma, Yi, and Hsieh]{chen2017zoo}
Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., and Hsieh, C.-J.
\newblock Zoo: Zeroth order optimization based black-box attacks to deep neural
  networks without training substitute models.
\newblock In \emph{Proceedings of the 10th ACM Workshop on Artificial
  Intelligence and Security}, pp.\  15--26, 2017.

\bibitem[Chen et~al.(2018)Chen, Sharma, Zhang, Yi, and Hsieh]{chen2018ead}
Chen, P.-Y., Sharma, Y., Zhang, H., Yi, J., and Hsieh, C.-J.
\newblock Ead: elastic-net attacks to deep neural networks via adversarial
  examples.
\newblock In \emph{Thirty-second AAAI conference on artificial intelligence},
  2018.

\bibitem[Croce \& Hein(2020)Croce and Hein]{croce2020reliable}
Croce, F. and Hein, M.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks.
\newblock In \emph{ICML}, 2020.

\bibitem[Dhillon et~al.(2018)Dhillon, Azizzadenesheli, Lipton, Bernstein,
  Kossaifi, Khanna, and Anandkumar]{dhillon2018stochastic}
Dhillon, G.~S., Azizzadenesheli, K., Lipton, Z.~C., Bernstein, J.~D., Kossaifi,
  J., Khanna, A., and Anandkumar, A.
\newblock Stochastic activation pruning for robust adversarial defense.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Dong et~al.(2018)Dong, Liao, Pang, Su, Zhu, Hu, and
  Li]{dong2018boosting}
Dong, Y., Liao, F., Pang, T., Su, H., Zhu, J., Hu, X., and Li, J.
\newblock Boosting adversarial attacks with momentum.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  9185--9193, 2018.

\bibitem[Engstrom et~al.(2019)Engstrom, Ilyas, Santurkar, Tsipras, Tran, and
  Madry]{engstrom2019adversarial}
Engstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Tran, B., and Madry, A.
\newblock Adversarial robustness as a prior for learned representations.
\newblock \emph{arXiv preprint arXiv:1906.00945}, 2019.

\bibitem[Gowal et~al.(2019)Gowal, Uesato, Qin, Huang, Mann, and
  Kohli]{gowal2019alternative}
Gowal, S., Uesato, J., Qin, C., Huang, P.-S., Mann, T., and Kohli, P.
\newblock An alternative surrogate loss for {PGD}-based adversarial testing.
\newblock \emph{arXiv preprint arXiv:1910.09338}, 2019.

\bibitem[Guo et~al.(2018)Guo, Rana, Cisse, and van~der
  Maaten]{guo2018countering}
Guo, C., Rana, M., Cisse, M., and van~der Maaten, L.
\newblock Countering adversarial images using input transformations.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[He et~al.(2017)He, Wei, Chen, Carlini, and Song]{he2017adversarial}
He, W., Wei, J., Chen, X., Carlini, N., and Song, D.
\newblock Adversarial example defense: Ensembles of weak defenses are not
  strong.
\newblock In \emph{11th $\{$USENIX$\}$ Workshop on Offensive Technologies
  ($\{$WOOT$\}$ 17)}, 2017.

\bibitem[He et~al.(2018)He, Li, and Song]{he2018decision}
He, W., Li, B., and Song, D.
\newblock Decision boundary analysis of adversarial examples.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Hosseini et~al.(2019)Hosseini, Kannan, and
  Poovendran]{hosseini2019odds}
Hosseini, H., Kannan, S., and Poovendran, R.
\newblock Are odds really odd? bypassing statistical detection of adversarial
  examples.
\newblock \emph{arXiv preprint arXiv:1907.12138}, 2019.

\bibitem[Hu et~al.(2019)Hu, Yu, Guo, Chao, and Weinberger]{hu2019new}
Hu, S., Yu, T., Guo, C., Chao, W.-L., and Weinberger, K.~Q.
\newblock A new defense against adversarial images: Turning a weakness into a
  strength.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1633--1644, 2019.

\bibitem[Ilyas et~al.(2018)Ilyas, Engstrom, Athalye, and Lin]{ilyas2018black}
Ilyas, A., Engstrom, L., Athalye, A., and Lin, J.
\newblock Black-box adversarial attacks with limited queries and information.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2137--2146, 2018.

\bibitem[Jacobsen et~al.(2019)Jacobsen, Behrmannn, Carlini, Tramer, and
  Papernot]{jacobsen2019exploiting}
Jacobsen, J.-H., Behrmannn, J., Carlini, N., Tramer, F., and Papernot, N.
\newblock Exploiting excessive invariance caused by norm-bounded adversarial
  robustness.
\newblock \emph{arXiv preprint arXiv:1903.10484}, 2019.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational {Bayes}.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[Kurakin et~al.(2016)Kurakin, Goodfellow, and
  Bengio]{kurakin2016adversarial}
Kurakin, A., Goodfellow, I., and Bengio, S.
\newblock Adversarial machine learning at scale.
\newblock \emph{arXiv preprint arXiv:1611.01236}, 2016.

\bibitem[Li et~al.(2019)Li, Bradshaw, and Sharma]{li2018generative}
Li, Y., Bradshaw, J., and Sharma, Y.
\newblock Are generative classifiers more robust to adversarial attacks?
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[Pang et~al.(2018)Pang, Du, and Zhu]{pang2018max}
Pang, T., Du, C., and Zhu, J.
\newblock Max-mahalanobis linear discriminant analysis networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4016--4025, 2018.

\bibitem[Pang et~al.(2019)Pang, Xu, Du, Chen, and Zhu]{pang2019improving}
Pang, T., Xu, K., Du, C., Chen, N., and Zhu, J.
\newblock Improving adversarial robustness via promoting ensemble diversity.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Pang et~al.(2020{\natexlab{a}})Pang, Xu, Dong, Du, Chen, and
  Zhu]{pang2020rethinking}
Pang, T., Xu, K., Dong, Y., Du, C., Chen, N., and Zhu, J.
\newblock Rethinking softmax cross-entropy loss for adversarial robustness.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.

\bibitem[Pang et~al.(2020{\natexlab{b}})Pang, Xu, and Zhu]{pang2020mixup}
Pang, T., Xu, K., and Zhu, J.
\newblock Mixup inference: Better exploiting mixup to defend adversarial
  attacks.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{b}}.

\bibitem[Papernot et~al.(2016{\natexlab{a}})Papernot, Faghri, Carlini,
  Goodfellow, Feinman, Kurakin, Xie, Sharma, Brown, Roy,
  et~al.]{papernot2016technical}
Papernot, N., Faghri, F., Carlini, N., Goodfellow, I., Feinman, R., Kurakin,
  A., Xie, C., Sharma, Y., Brown, T., Roy, A., et~al.
\newblock Technical report on the {CleverHans} {v2.1.0} adversarial examples
  library.
\newblock \emph{arXiv preprint arXiv:1610.00768}, 2016{\natexlab{a}}.

\bibitem[Papernot et~al.(2016{\natexlab{b}})Papernot, McDaniel, and
  Goodfellow]{papernot2016transferability}
Papernot, N., McDaniel, P., and Goodfellow, I.
\newblock Transferability in machine learning: from phenomena to black-box
  attacks using adversarial samples.
\newblock \emph{arXiv preprint arXiv:1605.07277}, 2016{\natexlab{b}}.

\bibitem[Papernot et~al.(2016{\natexlab{c}})Papernot, McDaniel, Wu, Jha, and
  Swami]{papernot2016distillation}
Papernot, N., McDaniel, P., Wu, X., Jha, S., and Swami, A.
\newblock Distillation as a defense to adversarial perturbations against deep
  neural networks.
\newblock In \emph{2016 IEEE Symposium on Security and Privacy (SP)}, pp.\
  582--597. IEEE, 2016{\natexlab{c}}.

\bibitem[Prakash et~al.(2018)Prakash, Moran, Garber, DiLillo, and
  Storer]{prakash2018deflecting}
Prakash, A., Moran, N., Garber, S., DiLillo, A., and Storer, J.
\newblock Deflecting adversarial attacks with pixel deflection.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  8571--8580, 2018.

\bibitem[Rauber et~al.(2017)Rauber, Brendel, and Bethge]{rauber2017foolbox}
Rauber, J., Brendel, W., and Bethge, M.
\newblock Foolbox: A python toolbox to benchmark the robustness of machine
  learning models.
\newblock \emph{arXiv preprint arXiv:1707.04131}, 2017.

\bibitem[Roth et~al.(2019)Roth, Kilcher, and Hofmann]{roth2019odds}
Roth, K., Kilcher, Y., and Hofmann, T.
\newblock The odds are odd: A statistical test for detecting adversarial
  examples.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Sabour et~al.(2016)Sabour, Cao, Faghri, and
  Fleet]{sabour2015adversarial}
Sabour, S., Cao, Y., Faghri, F., and Fleet, D.~J.
\newblock Adversarial manipulation of deep representations.
\newblock \emph{International Conference on Learning Representations}, 2016.

\bibitem[Santurkar et~al.(2019)Santurkar, Tsipras, Tran, Ilyas, Engstrom, and
  Madry]{santurkar2019computer}
Santurkar, S., Tsipras, D., Tran, B., Ilyas, A., Engstrom, L., and Madry, A.
\newblock Computer vision with a single (robust) classifier.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Schott et~al.(2019{\natexlab{a}})Schott, Rauber, Bethge, and
  Brendel]{schott2018towards}
Schott, L., Rauber, J., Bethge, M., and Brendel, W.
\newblock Towards the first adversarially robust neural network model on
  {MNIST}.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Schott et~al.(2019{\natexlab{b}})Schott, Rauber, Brendel, and
  Bethge]{lukas_abs}
Schott, L., Rauber, J., Brendel, W., and Bethge, M.
\newblock Robust perception through analysis by synthesis.
\newblock \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.

\bibitem[Sen et~al.(2020)Sen, Ravindran, and Raghunathan]{sen2020empir}
Sen, S., Ravindran, B., and Raghunathan, A.
\newblock {\{}EMPIR{\}}: Ensembles of mixed precision deep networks for
  increased robustness against adversarial attacks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,
  and Fergus, R.
\newblock Intriguing properties of neural networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2014.

\bibitem[Tram{\`e}r \& Boneh(2019)Tram{\`e}r and Boneh]{tramer2019adversarial}
Tram{\`e}r, F. and Boneh, D.
\newblock Adversarial training and robustness for multiple perturbations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5858--5868, 2019.

\bibitem[Tram\`er et~al.(2018)Tram\`er, Kurakin, Papernot, Goodfellow, Boneh,
  and McDaniel]{tramer2017ensemble}
Tram\`er, F., Kurakin, A., Papernot, N., Goodfellow, I., Boneh, D., and
  McDaniel, P.
\newblock Ensemble adversarial training: Attacks and defenses.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Tsipras et~al.(2019)Tsipras, Santurkar, Engstrom, Turner, and
  Madry]{tsipras2018robustness}
Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A.
\newblock Robustness may be at odds with accuracy.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Uesato et~al.(2018)Uesato, O'Donoghue, Oord, and
  Kohli]{uesato2018adversarial}
Uesato, J., O'Donoghue, B., Oord, A. v.~d., and Kohli, P.
\newblock Adversarial risk and the dangers of evaluating against weak attacks.
\newblock \emph{arXiv preprint arXiv:1802.05666}, 2018.

\bibitem[Verma \& Swami(2019)Verma and Swami]{verma2019error}
Verma, G. and Swami, A.
\newblock Error correcting output codes improve probability estimation and
  adversarial robustness of deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8643--8653, 2019.

\bibitem[Willetts et~al.(2019)Willetts, Camuto, Roberts, and
  Holmes]{willetts2019disentangling}
Willetts, M., Camuto, A., Roberts, S., and Holmes, C.
\newblock Disentangling improves vaes' robustness to adversarial attacks, 2019.

\bibitem[Xiao et~al.(2020)Xiao, Zhong, and Zheng]{xiao2019resisting}
Xiao, C., Zhong, P., and Zheng, C.
\newblock Resisting adversarial attacks by $k$-winners-take-all.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Yang et~al.(2019{\natexlab{a}})Yang, Zhang, Katabi, and
  Xu]{yang2019me}
Yang, Y., Zhang, G., Katabi, D., and Xu, Z.
\newblock Me-net: Towards effective adversarial robustness with matrix
  estimation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7025--7034, 2019{\natexlab{a}}.

\bibitem[Yang et~al.(2019{\natexlab{b}})Yang, Li, Chen, and
  Song]{yang2018characterizing}
Yang, Z., Li, B., Chen, P.-Y., and Song, D.
\newblock Characterizing audio adversarial examples using temporal dependency.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.

\bibitem[Yin et~al.(2020)Yin, Kolouri, and Rohde]{yin2020adversarial}
Yin, X., Kolouri, S., and Rohde, G.~K.
\newblock Adversarial example detection and classification with asymmetrical
  adversarial training.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\end{thebibliography}
