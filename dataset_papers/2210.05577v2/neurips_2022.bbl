\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu and Li(2022)]{ZhLi20}
Z.~Allen-Zhu and Y.~Li.
\newblock Feature purification: How adversarial training performs robust deep
  learning.
\newblock \emph{2021 IEEE 62nd Annual Symposium on Foundations of Computer
  Science (FOCS)}, pages 977--988, 2022.

\bibitem[Andriushchenko et~al.(2020)Andriushchenko, Croce, Flammarion, and
  Hein]{And+20}
M.~Andriushchenko, F.~Croce, N.~Flammarion, and M.~Hein.
\newblock Square attack: {A} query-efficient black-box adversarial attack via
  random search.
\newblock In A.~Vedaldi, H.~Bischof, T.~Brox, and J.~Frahm, editors,
  \emph{Computer Vision - {ECCV} 2020 - 16th European Conference, Glasgow, UK,
  August 23-28, 2020, Proceedings, Part {XXIII}}, volume 12368 of \emph{Lecture
  Notes in Computer Science}, pages 484--501. Springer, 2020.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, and Wang]{Aro+19a}
S.~Arora, S.~Du, W.~Hu, Z.~Li, and R.~Wang.
\newblock {Fine-Grained Analysis of Optimization and Generalization for
  Overparameterized Two-Layer Neural Networks}.
\newblock In K.~Chaudhuri and R.~Salakhutdinov, editors, \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pages 322--332. PMLR, 09--15
  Jun 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{Aro+19b}
S.~Arora, S.~S. Du, W.~Hu, Z.~Li, R.~Salakhutdinov, and R.~Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In H.~M. Wallach, H.~Larochelle, A.~Beygelzimer,
  F.~d'Alch{\'{e}}{-}Buc, E.~B. Fox, and R.~Garnett, editors, \emph{Advances in
  Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pages 8139--8148, 2019{\natexlab{b}}.

\bibitem[Atanasov et~al.(2022)Atanasov, Bordelon, and Pehlevan]{ABP21}
A.~Atanasov, B.~Bordelon, and C.~Pehlevan.
\newblock Neural networks as kernel learners: The silent alignment effect.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Bai et~al.(2021)Bai, Yan, Jiang, Xia, and Wang]{Bai+21}
Y.~Bai, X.~Yan, Y.~Jiang, S.~Xia, and Y.~Wang.
\newblock Clustering effect of adversarial robust models.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~N. Dauphin, P.~Liang, and J.~W.
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems 34:
  Annual Conference on Neural Information Processing Systems 2021, NeurIPS
  2021, December 6-14, 2021, virtual}, pages 29590--29601, 2021.

\bibitem[Baratin et~al.(2021)Baratin, George, Laurent, Hjelm, Lajoie, Vincent,
  and Lacoste{-}Julien]{Bar+21}
A.~Baratin, T.~George, C.~Laurent, R.~D. Hjelm, G.~Lajoie, P.~Vincent, and
  S.~Lacoste{-}Julien.
\newblock Implicit regularization via neural feature alignment.
\newblock In A.~Banerjee and K.~Fukumizu, editors, \emph{The 24th International
  Conference on Artificial Intelligence and Statistics, {AISTATS} 2021, April
  13-15, 2021, Virtual Event}, volume 130 of \emph{Proceedings of Machine
  Learning Research}, pages 2269--2277. {PMLR}, 2021.

\bibitem[Basri et~al.(2019)Basri, Jacobs, Kasten, and Kritchman]{Bas+19}
R.~Basri, D.~W. Jacobs, Y.~Kasten, and S.~Kritchman.
\newblock The convergence rate of neural networks for learned functions of
  different frequencies.
\newblock In H.~M. Wallach, H.~Larochelle, A.~Beygelzimer,
  F.~d'Alch{\'{e}}{-}Buc, E.~B. Fox, and R.~Garnett, editors, \emph{Advances in
  Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pages 4763--4772, 2019.

\bibitem[Basri et~al.(2020)Basri, Galun, Geifman, Jacobs, Kasten, and
  Kritchman]{Bas+20}
R.~Basri, M.~Galun, A.~Geifman, D.~W. Jacobs, Y.~Kasten, and S.~Kritchman.
\newblock Frequency bias in neural networks for input of non-uniform density.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 685--694. {PMLR},
  2020.

\bibitem[Bietti and Mairal(2019)]{BiMa19}
A.~Bietti and J.~Mairal.
\newblock On the inductive bias of neural tangent kernels.
\newblock In H.~M. Wallach, H.~Larochelle, A.~Beygelzimer,
  F.~d'Alch{\'{e}}{-}Buc, E.~B. Fox, and R.~Garnett, editors, \emph{Advances in
  Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pages 12873--12884, 2019.

\bibitem[Blondel et~al.(2021)Blondel, Berthet, Cuturi, Frostig, Hoyer,
  Llinares-L{\'o}pez, Pedregosa, and Vert]{Blo+21}
M.~Blondel, Q.~Berthet, M.~Cuturi, R.~Frostig, S.~Hoyer, F.~Llinares-L{\'o}pez,
  F.~Pedregosa, and J.-P. Vert.
\newblock Efficient and modular implicit differentiation.
\newblock \emph{arXiv preprint arXiv:2105.15183}, 2021.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{Brad+18}
J.~Bradbury, R.~Frostig, P.~Hawkins, M.~J. Johnson, C.~Leary, D.~Maclaurin,
  G.~Necula, A.~Paszke, J.~Vander{P}las, S.~Wanderman-{M}ilne, and Q.~Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Carlini and Wagner(2017)]{CaWa17}
N.~Carlini and D.~A. Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock In \emph{2017 {IEEE} Symposium on Security and Privacy, {SP} 2017,
  San Jose, CA, USA, May 22-26, 2017}, pages 39--57. {IEEE} Computer Society,
  2017.

\bibitem[Chen et~al.(2017)Chen, Zhang, Sharma, Yi, and Hsieh]{Che+17}
P.~Chen, H.~Zhang, Y.~Sharma, J.~Yi, and C.~Hsieh.
\newblock {ZOO:} zeroth order optimization based black-box attacks to deep
  neural networks without training substitute models.
\newblock In B.~M. Thuraisingham, B.~Biggio, D.~M. Freeman, B.~Miller, and
  A.~Sinha, editors, \emph{Proceedings of the 10th {ACM} Workshop on Artificial
  Intelligence and Security, AISec@CCS 2017, Dallas, TX, USA, November 3,
  2017}, pages 15--26. {ACM}, 2017.

\bibitem[Chen et~al.(2021)Chen, Gong, and Wang]{Chen+21}
W.~Chen, X.~Gong, and Z.~Wang.
\newblock Neural architecture search on imagenet in four {GPU} hours: {A}
  theoretically inspired perspective.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{COB19}
L.~Chizat, E.~Oyallon, and F.~R. Bach.
\newblock On lazy training in differentiable programming.
\newblock In H.~M. Wallach, H.~Larochelle, A.~Beygelzimer,
  F.~d'Alch{\'{e}}{-}Buc, E.~B. Fox, and R.~Garnett, editors, \emph{Advances in
  Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pages 2933--2943, 2019.

\bibitem[Croce et~al.(2021)Croce, Andriushchenko, Sehwag, Debenedetti,
  Flammarion, Chiang, Mittal, and Hein]{robustbench20}
F.~Croce, M.~Andriushchenko, V.~Sehwag, E.~Debenedetti, N.~Flammarion,
  M.~Chiang, P.~Mittal, and M.~Hein.
\newblock Robustbench: a standardized adversarial robustness benchmark.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 2)}, 2021.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang, and Zhai]{Du+19b}
S.~S. Du, J.~D. Lee, H.~Li, L.~Wang, and X.~Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In K.~Chaudhuri and R.~Salakhutdinov, editors, \emph{Proceedings of
  the 36th International Conference on Machine Learning, {ICML} 2019, 9-15 June
  2019, Long Beach, California, {USA}}, volume~97 of \emph{Proceedings of
  Machine Learning Research}, pages 1675--1685. {PMLR}, 2019{\natexlab{a}}.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Zhai, P{\'{o}}czos, and
  Singh]{Du+19a}
S.~S. Du, X.~Zhai, B.~P{\'{o}}czos, and A.~Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net,
  2019{\natexlab{b}}.

\bibitem[Fort et~al.(2020)Fort, Dziugaite, Paul, Kharaghani, 0001, and
  Ganguli]{Fort+20}
S.~Fort, G.~K. Dziugaite, M.~Paul, S.~Kharaghani, D.~M.~R. 0001, and
  S.~Ganguli.
\newblock Deep learning versus kernel learning: an empirical study of loss
  landscape geometry and the time evolution of the neural tangent kernel.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.-F. Balcan, and H.-T.
  Lin, editors, \emph{Advances in Neural Information Processing Systems 33:
  Annual Conference on Neural Information Processing Systems 2020, NeurIPS
  2020, December 6-12, 2020, virtual}, 2020.

\bibitem[Geiger et~al.(2020)Geiger, Spigler, Jacot, and Wyart]{Gei+19}
M.~Geiger, S.~Spigler, A.~Jacot, and M.~Wyart.
\newblock Disentangling feature and lazy training in deep neural networks.
\newblock \emph{Journal Of Statistical Mechanics-Theory And Experiment},
  2020\penalty0 (11):\penalty0 113301, 2020.

\bibitem[Goh(2019)]{Goh19}
G.~Goh.
\newblock A discussion of 'adversarial examples are not bugs, they are
  features': Two examples of useful, non-robust features.
\newblock \emph{Distill}, 2019.
\newblock https://distill.pub/2019/advex-bugs-discussion/response-3.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and Szegedy]{GSS15}
I.~J. Goodfellow, J.~Shlens, and C.~Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock In Y.~Bengio and Y.~LeCun, editors, \emph{3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}, 2015.

\bibitem[Heek et~al.(2020)Heek, Levskaya, Oliver, Ritter, Rondepierre, Steiner,
  and van {Z}ee]{Hee+20}
J.~Heek, A.~Levskaya, A.~Oliver, M.~Ritter, B.~Rondepierre, A.~Steiner, and
  M.~van {Z}ee.
\newblock {F}lax: A neural network library and ecosystem for {JAX}, 2020.
\newblock URL \url{http://github.com/google/flax}.

\bibitem[Ilyas et~al.(2018)Ilyas, Engstrom, Athalye, and Lin]{Ily+18}
A.~Ilyas, L.~Engstrom, A.~Athalye, and J.~Lin.
\newblock Black-box adversarial attacks with limited queries and information.
\newblock In J.~G. Dy and A.~Krause, editors, \emph{Proceedings of the 35th
  International Conference on Machine Learning, {ICML} 2018,
  Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pages 2142--2151. {PMLR},
  2018.

\bibitem[Ilyas et~al.(2019)Ilyas, Santurkar, Tsipras, Engstrom, Tran, and
  Madry]{Ily+19}
A.~Ilyas, S.~Santurkar, D.~Tsipras, L.~Engstrom, B.~Tran, and A.~Madry.
\newblock Adversarial examples are not bugs, they are features.
\newblock In H.~M. Wallach, H.~Larochelle, A.~Beygelzimer,
  F.~d'Alch{\'{e}}{-}Buc, E.~B. Fox, and R.~Garnett, editors, \emph{Advances in
  Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pages 125--136, 2019.

\bibitem[Jacot et~al.(2018)Jacot, Hongler, and Gabriel]{JHG18}
A.~Jacot, C.~Hongler, and F.~Gabriel.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In S.~Bengio, H.~M. Wallach, H.~Larochelle, K.~Grauman,
  N.~Cesa{-}Bianchi, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems 31: Annual Conference on Neural Information
  Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr{\'{e}}al,
  Canada}, pages 8580--8589, 2018.

\bibitem[Kim et~al.(2021)Kim, Lee, and Ro]{KLR21}
J.~Kim, B.-K. Lee, and Y.~M. Ro.
\newblock Distilling robust and non-robust features in adversarial examples by
  information bottleneck.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W. Vaughan, editors,
  \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Kurakin et~al.(2017)Kurakin, Goodfellow, and Bengio]{KGB17}
A.~Kurakin, I.~J. Goodfellow, and S.~Bengio.
\newblock Adversarial examples in the physical world.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings},
  2017.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{Lee+19}
J.~Lee, L.~Xiao, S.~Schoenholz, Y.~Bahri, R.~Novak, J.~Sohl-Dickstein, and
  J.~Pennington.
\newblock {Wide Neural Networks of Any Depth Evolve as Linear Models Under
  Gradient Descent}.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem[Liu et~al.(2020)Liu, Zhu, and Belkin]{Liu+20}
C.~Liu, L.~Zhu, and M.~Belkin.
\newblock On the linearity of large non-linear models: when and why the tangent
  kernel is constant.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 15954--15964. Curran Associates, Inc., 2020.

\bibitem[Long(2021)]{Long21}
P.~M. Long.
\newblock Properties of the after kernel.
\newblock \emph{CoRR}, abs/2105.10585, 2021.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and Vladu]{Mad+18}
A.~Madry, A.~Makelov, L.~Schmidt, D.~Tsipras, and A.~Vladu.
\newblock {Towards Deep Learning Models Resistant to Adversarial Attacks}.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Nakkiran(2019)]{Nakk19}
P.~Nakkiran.
\newblock Adversarial robustness may be at odds with simplicity.
\newblock \emph{CoRR}, abs/1901.00532, 2019.
\newblock URL \url{http://arxiv.org/abs/1901.00532}.

\bibitem[Nguyen et~al.(2021)Nguyen, Novak, Xiao, and Lee]{Ngu+21}
T.~Nguyen, R.~Novak, L.~Xiao, and J.~Lee.
\newblock Dataset distillation with infinitely wide convolutional networks.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W. Vaughan, editors,
  \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Novak et~al.(2020)Novak, Xiao, Hron, Lee, Alemi, Sohl{-}Dickstein, and
  Schoenholz]{Nov+20}
R.~Novak, L.~Xiao, J.~Hron, J.~Lee, A.~A. Alemi, J.~Sohl{-}Dickstein, and S.~S.
  Schoenholz.
\newblock Neural tangents: Fast and easy infinite neural networks in python.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.

\bibitem[Ortiz{-}Jim{\'{e}}nez et~al.(2021)Ortiz{-}Jim{\'{e}}nez,
  Moosavi{-}Dezfooli, and Frossard]{Jim+21}
G.~Ortiz{-}Jim{\'{e}}nez, S.~Moosavi{-}Dezfooli, and P.~Frossard.
\newblock What can linearized neural networks actually say about
  generalization?
\newblock \emph{CoRR}, abs/2106.06770, 2021.

\bibitem[Papernot et~al.(2016)Papernot, McDaniel, and Goodfellow]{PMG16}
N.~Papernot, P.~D. McDaniel, and I.~J. Goodfellow.
\newblock Transferability in machine learning: from phenomena to black-box
  attacks using adversarial samples.
\newblock \emph{CoRR}, abs/1605.07277, 2016.

\bibitem[Papernot et~al.(2017)Papernot, McDaniel, Goodfellow, Jha, Celik, and
  Swami]{Pap+17}
N.~Papernot, P.~D. McDaniel, I.~J. Goodfellow, S.~Jha, Z.~B. Celik, and
  A.~Swami.
\newblock Practical black-box attacks against machine learning.
\newblock In R.~Karri, O.~Sinanoglu, A.~Sadeghi, and X.~Yi, editors,
  \emph{Proceedings of the 2017 {ACM} on Asia Conference on Computer and
  Communications Security, AsiaCCS 2017, Abu Dhabi, United Arab Emirates, April
  2-6, 2017}, pages 506--519. {ACM}, 2017.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, K{\"{o}}pf, Yang, DeVito,
  Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{Pas+19}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~K{\"{o}}pf, E.~Z. Yang,
  Z.~DeVito, M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang,
  J.~Bai, and S.~Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In H.~M. Wallach, H.~Larochelle, A.~Beygelzimer,
  F.~d'Alch{\'{e}}{-}Buc, E.~B. Fox, and R.~Garnett, editors, \emph{Advances in
  Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pages 8024--8035, 2019.

\bibitem[Rahaman et~al.(2019)Rahaman, Baratin, Arpit, Draxler, Lin, Hamprecht,
  Bengio, and Courville]{Rah+19}
N.~Rahaman, A.~Baratin, D.~Arpit, F.~Draxler, M.~Lin, F.~A. Hamprecht,
  Y.~Bengio, and A.~C. Courville.
\newblock On the spectral bias of neural networks.
\newblock In K.~Chaudhuri and R.~Salakhutdinov, editors, \emph{Proceedings of
  the 36th International Conference on Machine Learning, {ICML} 2019, 9-15 June
  2019, Long Beach, California, {USA}}, volume~97 of \emph{Proceedings of
  Machine Learning Research}, pages 5301--5310. {PMLR}, 2019.

\bibitem[Shankar et~al.(2020)Shankar, Fang, Guo, Fridovich-Keil, Ragan-Kelley,
  Schmidt, and Recht]{Sha+20}
V.~Shankar, A.~Fang, W.~Guo, S.~Fridovich-Keil, J.~Ragan-Kelley, L.~Schmidt,
  and B.~Recht.
\newblock Neural kernels without tangents.
\newblock In H.~D. III and A.~Singh, editors, \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 8614--8623. PMLR,
  13--18 Jul 2020.

\bibitem[Simon{-}Gabriel et~al.(2019)Simon{-}Gabriel, Ollivier, Bottou,
  Sch{\"{o}}lkopf, and Lopez{-}Paz]{Gab+19}
C.~Simon{-}Gabriel, Y.~Ollivier, L.~Bottou, B.~Sch{\"{o}}lkopf, and
  D.~Lopez{-}Paz.
\newblock First-order adversarial vulnerability of neural networks and input
  dimension.
\newblock In K.~Chaudhuri and R.~Salakhutdinov, editors, \emph{Proceedings of
  the 36th International Conference on Machine Learning, {ICML} 2019, 9-15 June
  2019, Long Beach, California, {USA}}, volume~97 of \emph{Proceedings of
  Machine Learning Research}, pages 5809--5817. {PMLR}, 2019.

\bibitem[Springer et~al.(2021)Springer, Mitchell, and Kenyon]{SMK21}
J.~M. Springer, M.~Mitchell, and G.~T. Kenyon.
\newblock A little robustness goes a long way: Leveraging robust features for
  targeted transfer attacks.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W. Vaughan, editors,
  \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{Sze+14}
C.~Szegedy, W.~Zaremba, I.~Sutskever, J.~Bruna, D.~Erhan, I.~J. Goodfellow, and
  R.~Fergus.
\newblock Intriguing properties of neural networks.
\newblock In Y.~Bengio and Y.~LeCun, editors, \emph{2nd International
  Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April
  14-16, 2014, Conference Track Proceedings}, 2014.

\bibitem[Tancik et~al.(2020)Tancik, Srinivasan, Mildenhall, Fridovich{-}Keil,
  Raghavan, Singhal, Ramamoorthi, Barron, and Ng]{Tan+20}
M.~Tancik, P.~P. Srinivasan, B.~Mildenhall, S.~Fridovich{-}Keil, N.~Raghavan,
  U.~Singhal, R.~Ramamoorthi, J.~T. Barron, and R.~Ng.
\newblock Fourier features let networks learn high frequency functions in low
  dimensional domains.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem[Tsipras et~al.(2019)Tsipras, Santurkar, Engstrom, Turner, and
  Madry]{Tsi+19}
D.~Tsipras, S.~Santurkar, L.~Engstrom, A.~Turner, and A.~Madry.
\newblock Robustness may be at odds with accuracy.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}, 2019.

\bibitem[Wong et~al.(2020)Wong, Rice, and Kolter]{Won+20}
E.~Wong, L.~Rice, and J.~Z. Kolter.
\newblock Fast is better than free: Revisiting adversarial training.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.

\bibitem[Yuan and Wu(2021)]{YuWu21}
C.-H. Yuan and S.-H. Wu.
\newblock Neural tangent generalization attacks.
\newblock In M.~Meila and T.~Zhang, editors, \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 12230--12240. PMLR,
  18--24 Jul 2021.

\bibitem[Zhang et~al.(2019)Zhang, Yu, Jiao, Xing, Ghaoui, and Jordan]{Zha+19}
H.~Zhang, Y.~Yu, J.~Jiao, E.~P. Xing, L.~E. Ghaoui, and M.~I. Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In K.~Chaudhuri and R.~Salakhutdinov, editors, \emph{Proceedings of
  the 36th International Conference on Machine Learning, {ICML} 2019, 9-15 June
  2019, Long Beach, California, {USA}}, volume~97 of \emph{Proceedings of
  Machine Learning Research}, pages 7472--7482. {PMLR}, 2019.

\end{thebibliography}
