\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Absil et~al.(2009)Absil, Mahony, and Sepulchre]{absil2009optimization}
P.-A. Absil, R.~Mahony, and R.~Sepulchre.
\newblock \emph{Optimization algorithms on matrix manifolds}.
\newblock Princeton University Press, 2009.

\bibitem[Arora et~al.(2019)Arora, Cohen, Hu, and Luo]{arora2019implicit}
S.~Arora, N.~Cohen, W.~Hu, and Y.~Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  7411--7422, 2019.

\bibitem[Baldi and Hornik(1989)]{baldi1989neural}
P.~Baldi and K.~Hornik.
\newblock Neural networks and principal component analysis: {L}earning from
  examples without local minima.
\newblock \emph{Neural networks}, 2\penalty0 (1):\penalty0 53--58, 1989.

\bibitem[Bamler and Mandt(2018)]{bamler2018improving}
R.~Bamler and S.~Mandt.
\newblock Improving optimization for models with continuous symmetry breaking.
\newblock \emph{arXiv preprint arXiv:1803.03234}, 2018.

\bibitem[Dai et~al.(2018)Dai, Wang, Aston, Hua, and Wipf]{dai2018connections}
B.~Dai, Y.~Wang, J.~Aston, G.~Hua, and D.~Wipf.
\newblock Connections with robust pca and the role of emergent sparsity in
  variational autoencoder models.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 1573--1614, 2018.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock {Image{N}et: {A} Large-Scale Hierarchical Image Database}.
\newblock In \emph{CVPR09}, 2009.

\bibitem[Ge et~al.(2015)Ge, Huang, Jin, and Yuan]{pmlr-v40-Ge15}
R.~Ge, F.~Huang, C.~Jin, and Y.~Yuan.
\newblock Escaping from saddle points --- online stochastic gradient for tensor
  decomposition.
\newblock In P.~Gr√ºnwald, E.~Hazan, and S.~Kale, editors, \emph{Proceedings of
  The 28th Conference on Learning Theory}, volume~40 of \emph{Proceedings of
  Machine Learning Research}, pages 797--842, Paris, France, 03--06 Jul 2015.
  PMLR.
\newblock URL \url{http://proceedings.mlr.press/v40/Ge15.html}.

\bibitem[Gemp et~al.(2020)Gemp, McWilliams, Vernade, and
  Graepel]{gemp2020eigengame}
I.~Gemp, B.~McWilliams, C.~Vernade, and T.~Graepel.
\newblock Eigengame: Pca as a nash equilibrium.
\newblock \emph{arXiv preprint arXiv:2010.00554}, 2020.

\bibitem[Gidel et~al.(2019)Gidel, Bach, and Lacoste-Julien]{gidel2019implicit}
G.~Gidel, F.~Bach, and S.~Lacoste-Julien.
\newblock Implicit regularization of discrete gradient dynamics in deep linear
  neural networks.
\newblock \emph{arXiv preprint arXiv:1904.13262}, 2019.

\bibitem[Higgins et~al.(2017)Higgins, Matthey, Pal, Burgess, Glorot, Botvinick,
  Mohamed, and Lerchner]{higgins2017beta}
I.~Higgins, L.~Matthey, A.~Pal, C.~Burgess, X.~Glorot, M.~Botvinick,
  S.~Mohamed, and A.~Lerchner.
\newblock beta-{VAE}: {L}earning basic visual concepts with a constrained
  variational framework.
\newblock \emph{Iclr}, 2\penalty0 (5):\penalty0 6, 2017.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{ntk}
A.~Jacot, F.~Gabriel, and C.~Hongler.
\newblock Neural {T}angent {K}ernel: {C}onvergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem[Jin et~al.(2017)Jin, Ge, Netrapalli, Kakade, and
  Jordan]{pmlr-v70-jin17a}
C.~Jin, R.~Ge, P.~Netrapalli, S.~M. Kakade, and M.~I. Jordan.
\newblock How to escape saddle points efficiently.
\newblock In D.~Precup and Y.~W. Teh, editors, \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pages 1724--1732, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v70/jin17a.html}.

\bibitem[Khalil and Grizzle(2002)]{khalil2002nonlinear}
H.~K. Khalil and J.~W. Grizzle.
\newblock \emph{Nonlinear systems}, volume~3.
\newblock Prentice hall Upper Saddle River, NJ, 2002.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma and Welling(2013)]{kingma2013auto}
D.~P. Kingma and M.~Welling.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Kumar and Poole(2020)]{kumar2020implicit}
A.~Kumar and B.~Poole.
\newblock On implicit regularization in $\beta$-{VAE}s.
\newblock \emph{arXiv preprint arXiv:2002.00041}, 2020.

\bibitem[Kunin et~al.(2019)Kunin, Bloom, Goeva, and Seed]{ae-loss-landscape}
D.~Kunin, J.~M. Bloom, A.~Goeva, and C.~Seed.
\newblock Loss landscapes of regularized linear autoencoders.
\newblock \emph{arXiv preprint arXiv:1901.08168}, 2019.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{mnist}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lucas et~al.(2019)Lucas, Tucker, Grosse, and Norouzi]{dont-blame-elbo}
J.~Lucas, G.~Tucker, R.~B. Grosse, and M.~Norouzi.
\newblock Don't blame the {ELBO}! {A} linear vae perspective on posterior
  collapse.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9403--9413, 2019.

\bibitem[Oftadeh et~al.(2020)Oftadeh, Shen, Wang, and
  Shell]{oftadeheliminating}
R.~Oftadeh, J.~Shen, A.~Wang, and D.~Shell.
\newblock Eliminating the invariance on the loss landscape of linear
  autoencoders.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pages 5726--5734, 2020.

\bibitem[Oja(1982)]{oja1982simplified}
E.~Oja.
\newblock Simplified neuron model as a principal component analyzer.
\newblock \emph{Journal of mathematical biology}, 15\penalty0 (3):\penalty0
  267--273, 1982.

\bibitem[Pearson(1901)]{pearson1901liii}
K.~Pearson.
\newblock Liii. on lines and planes of closest fit to systems of points in
  space.
\newblock \emph{The London, Edinburgh, and Dublin Philosophical Magazine and
  Journal of Science}, 2\penalty0 (11):\penalty0 559--572, 1901.

\bibitem[Pfau et~al.(2018)Pfau, Petersen, Agarwal, Barrett, and
  Stachenfeld]{pfau2018spectral}
D.~Pfau, S.~Petersen, A.~Agarwal, D.~G. Barrett, and K.~L. Stachenfeld.
\newblock Spectral inference networks: {U}nifying deep and spectral learning.
\newblock \emph{arXiv preprint arXiv:1806.02215}, 2018.

\bibitem[Rippel et~al.(2014)Rippel, Gelbart, and Adams]{nested-dropout}
O.~Rippel, M.~Gelbart, and R.~Adams.
\newblock Learning ordered representations with nested dropout.
\newblock In \emph{International Conference on Machine Learning}, pages
  1746--1754, 2014.

\bibitem[Rolinek et~al.(2019)Rolinek, Zietlow, and
  Martius]{rolinek2019variational}
M.~Rolinek, D.~Zietlow, and G.~Martius.
\newblock Variational autoencoders pursue pca directions (by accident).
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 12406--12415, 2019.

\bibitem[Sanger(1989)]{generalized-hebbian-alg}
T.~D. Sanger.
\newblock Optimal unsupervised learning in a single-layer linear feedforward
  neural network.
\newblock \emph{Neural Networks}, 2\penalty0 (6):\penalty0 459--473, 1989.
\newblock \doi{10.1016/0893-6080(89)90044-0}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/0893608089900440}.

\bibitem[Saxe et~al.(2019)Saxe, McClelland, and Ganguli]{saxe2018}
A.~M. Saxe, J.~L. McClelland, and S.~Ganguli.
\newblock A mathematical theory of semantic development in deep neural
  networks.
\newblock \emph{Proceedings of the National Academy of Sciences of the United
  States of America}, 116\penalty0 (23):\penalty0 11537--11546, Jun 4, 2019.
\newblock \doi{10.1073/pnas.1820226116}.
\newblock URL \url{https://www.ncbi.nlm.nih.gov/pubmed/31101713}.

\bibitem[Tang(2019)]{tang2019exponentially}
C.~Tang.
\newblock Exponentially convergent stochastic k-{PCA} without variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  12393--12404, 2019.

\bibitem[Tipping and Bishop(1999)]{tipping1999probabilistic}
M.~E. Tipping and C.~M. Bishop.
\newblock Probabilistic principal component analysis.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 61\penalty0 (3):\penalty0 611--622, 1999.

\end{thebibliography}
