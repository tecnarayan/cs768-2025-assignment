@inproceedings{harutyunyan2016q,
  title={Q($\lambda$) with Off-Policy Corrections},
  author={Harutyunyan, Anna and Bellemare, Marc G and Stepleton, Tom and Munos, R{\'e}mi},
  booktitle={International Conference on Algorithmic Learning Theory (ALT)},
  year={2016},
}

@inproceedings{lehnert2018value,
  title={On value function representation of long horizon problems},
  author={Lehnert, Lucas and Laroche, Romain and van Seijen, Harm},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2018}
}

@inproceedings{kapturowski2018recurrent,
  title={Recurrent experience replay in distributed reinforcement learning},
  author={Kapturowski, Steven and Ostrovski, Georg and Quan, John and Munos, Remi and Dabney, Will},
  booktitle={Proceedings of the International Conference on Learning Representations},
  year={2018}
}

@inproceedings{jiang2016structural,
  title={On Structural Properties of {MDP}s that Bound Loss Due to Shallow Planning.},
  author={Jiang, Nan and Singh, Satinder P and Tewari, Ambuj},
  booktitle={Proceedings of the International Joint Conference on Artificial Intelligence},
  year={2016}
}

@article{van2019general,
  title={General non-linear {B}ellman equations},
  author={van Hasselt, Hado and Quan, John and Hessel, Matteo and Xu, Zhongwen and Borsa, Diana and Barreto, Andr{\'e}},
  journal={arXiv},
  year={2019}
}

@article{fedus2019hyperbolic,
  title={Hyperbolic discounting and learning over multiple horizons},
  author={Fedus, William and Gelada, Carles and Bengio, Yoshua and Bellemare, Marc G and Larochelle, Hugo},
  journal={arXiv},
  year={2019}
}

@inproceedings{van2019using,
  title={Using a logarithmic mapping to enable lower discount factors in reinforcement learning},
  author={Van Seijen, Harm and Fatemi, Mehdi and Tavakoli, Arash},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@inproceedings{amit2020discount,
  title={Discount Factor as a Regularizer in Reinforcement Learning},
  author={Amit, Ron and Meir, Ron and Ciosek, Kamil},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2020},
}

@incollection{sutton1995td,
  title={{TD} models: Modeling the world at a mixture of time scales},
  author={Sutton, Richard S},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={1995},
}

@book{grinstead2012introduction,
  title={Introduction to probability},
  author={Grinstead, Charles Miller and Snell, James Laurie},
  year={2012},
  publisher={American Mathematical Soc.}
}

@book{ross2014introduction,
  title={Introduction to probability models},
  author={Ross, Sheldon M},
  year={2014},
  publisher={Academic press}
}

@article{nachum2019algaedice,
  title={Algae{DICE}: Policy gradient from arbitrary experience},
  author={Nachum, Ofir and Dai, Bo and Kostrikov, Ilya and Chow, Yinlam and Li, Lihong and Schuurmans, Dale},
  journal={arXiv},
  year={2019}
}

@article{yang2020off,
  title={Off-policy evaluation via the regularized lagrangian},
  author={Yang, Mengjiao and Nachum, Ofir and Dai, Bo and Li, Lihong and Schuurmans, Dale},
  journal={arXiv},
  year={2020}
}

@article{bellman1957markovian,
  title={A {M}arkovian decision process},
  author={Bellman, Richard},
  journal={Journal of mathematics and mechanics},
  pages={679--684},
  year={1957},
}

@article{prokhorov1997adaptive,
  title={Adaptive critic designs},
  author={Prokhorov, Danil V and Wunsch, Donald C},
  journal={IEEE transactions on Neural Networks},
  volume={8},
  number={5},
  pages={997--1007},
  year={1997},
  publisher={IEEE}
}



@article{xu2018meta,
  title={Meta-gradient reinforcement learning},
  author={Xu, Zhongwen and van Hasselt, Hado P and Silver, David},
  journal={Advances in Neural Information Processing Systems},
  year={2018}
}

@article{franccois2015discount,
  title={How to discount deep reinforcement learning: Towards new dynamic strategies},
  author={Fran{\c{c}}ois-Lavet, Vincent and Fonteneau, Raphael and Ernst, Damien},
  journal={NIPS Deep Reinforcement Learning Workshop},
  year={2015}
}



@article{laroche2018reinforcement,
  title={In reinforcement learning, all objective functions are not equal},
  author={Laroche, Romain and van Seijen, Harm},
  year={2018}
}

@article{pohlen2018observe,
  title={Observe and look further: Achieving consistent performance on atari},
  author={Pohlen, Tobias and Piot, Bilal and Hester, Todd and Azar, Mohammad Gheshlaghi and Horgan, Dan and Budden, David and Barth-Maron, Gabriel and Van Hasselt, Hado and Quan, John and Ve{\v{c}}er{\'\i}k, Mel and Hessel, Matteo  and Munos, R\'emi and Pietquin, Olivier},
  journal={arXiv},
  year={2018}
}

@inproceedings{munos2016safe,
  title={Safe and efficient off-policy reinforcement learning},
  author={Munos, R{\'e}mi and Stepleton, Tom and Harutyunyan, Anna and Bellemare, Marc},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2016}
}

@inproceedings{tang2020taylor,
  title={Taylor expansion policy optimization},
  author={Tang, Yunhao and Valko, Michal and Munos, R{\'e}mi},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2020}
}

@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in Neural Information Processing Systems},
  year={2000}
}

@book{sutton1998introduction,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT Press}
}

@book{casella2002statistical,
  title={Statistical inference},
  author={Casella, George and Berger, Roger L},
  volume={2},
  year={2002},
  publisher={Duxbury Pacific Grove, CA}
}

@article{precup2000eligibility,
  title={Eligibility traces for off-policy policy evaluation},
  author={Precup, Doina},
  journal={Computer Science Department Faculty Publication Series},
  pages={80},
  year={2000}
}

@inproceedings{jiang2016doubly,
  title={Doubly robust off-policy value evaluation for reinforcement learning},
  author={Jiang, Nan and Li, Lihong},
  booktitle={International Conference on Machine Learning},
  pages={652--661},
  year={2016}
}

@inproceedings{espeholt2018impala,
  title={Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures},
  author={Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others},
  booktitle={Proceeedings of the International Conference on Machine Learning},
  year={2018}
}

@inproceedings{rowland2020adaptive,
  title={Adaptive trade-offs in off-policy learning},
  author={Rowland, Mark and Dabney, Will and Munos, R{\'e}mi},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={34--44},
  year={2020}
}

@inproceedings{hessel2018rainbow,
  title={Rainbow: Combining improvements in deep reinforcement learning},
  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{de2003linear,
  title={The linear programming approach to approximate dynamic programming},
  author={De Farias, Daniela Pucci and Van Roy, Benjamin},
  journal={Operations research},
  volume={51},
  number={6},
  pages={850--865},
  year={2003},
  publisher={INFORMS}
}

@article{hasselt2010double,
  title={Double {Q}-learning},
  author={Hasselt, Hado},
  journal={Advances in Neural Information Processing Systems},
  year={2010},
}

@article{imani2018off,
  title={An off-policy policy gradient theorem using emphatic weightings},
  author={Imani, Ehsan and Graves, Eric and White, Martha},
  journal={arXiv preprint arXiv:1811.09013},
  year={2018}
}

@inproceedings{fox2015taming,
  title={Taming the Noise in Reinforcement Learning via Soft Updates},
  author={Fox, Roy and Pakman, Ari and Tishby, Naftali},
  booktitle={Proceedings of the Conference on Uncertainty in Artificial Intelligence},
  year={2016}
}

@inproceedings{ziebart2008maximum,
  title={Maximum Entropy Inverse Reinforcement Learning},
  author={Ziebart, Brian D. and Maas, Andrew L. and Bagnell, J. Andrew and Dey, Anind K.},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2008}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{van2016deep,
  title={Deep reinforcement learning with double {Q}-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  number={1},
  year={2016}
}

@article{huang2019importance,
  title={From importance sampling to doubly robust policy gradient},
  author={Huang, Jiawei and Jiang, Nan},
  journal={arXiv preprint arXiv:1910.09066},
  year={2019}
}

@article{wang2016sample,
  title={Sample efficient actor-critic with experience replay},
  author={Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
  journal={arXiv preprint arXiv:1611.01224},
  year={2016}
}

@article{degris2012off,
  title={Off-policy actor-critic},
  author={Degris, Thomas and White, Martha and Sutton, Richard S},
  journal={arXiv preprint arXiv:1205.4839},
  year={2012}
}

@inproceedings{silver2014deterministic,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2014}
}

@inproceedings{imani2018off,
  title={An off-policy policy gradient theorem using emphatic weightings},
  author={Imani, Ehsan and Graves, Eric and White, Martha},
  booktitle={Advances in Neural Information Processing Systems},
  pages={96--106},
  year={2018}
}

@article{gruslys2017reactor,
  title={The reactor: A fast and sample-efficient actor-critic agent for reinforcement learning},
  author={Gruslys, Audrunas and Dabney, Will and Azar, Mohammad Gheshlaghi and Piot, Bilal and Bellemare, Marc and Munos, Remi},
  journal={arXiv preprint arXiv:1704.04651},
  year={2017}
}

@misc{dhariwal2017openai,
  title={Open{AI} baselines},
  author={Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai and Zhokhov, Peter},
  year={2017}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2015}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv},
  year={2017}
}

@article{peng2019advantage,
  title={Advantage-weighted regression: Simple and scalable off-policy reinforcement learning},
  author={Peng, Xue Bin and Kumar, Aviral and Zhang, Grace and Levine, Sergey},
  journal={arXiv preprint arXiv:1910.00177},
  year={2019}
}

@article{grill2020monte,
  title={Monte-Carlo tree search as regularized policy optimization},
  author={Grill, Jean-Bastien and Altch{\'e}, Florent and Tang, Yunhao and Hubert, Thomas and Valko, Michal and Antonoglou, Ioannis and Munos, R{\'e}mi},
  journal={arXiv preprint arXiv:2007.12509},
  year={2020}
}

@article{silver2018general,
  title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={Science},
  volume={362},
  number={6419},
  pages={1140--1144},
  year={2018},
  publisher={American Association for the Advancement of Science}
}
@article{oh2018self,
  title={Self-imitation learning},
  author={Oh, Junhyuk and Guo, Yijie and Singh, Satinder and Lee, Honglak},
  journal={arXiv preprint arXiv:1806.05635},
  year={2018}
}

@book{puterman2014markov,
  title={Markov decision processes: Discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@inproceedings{nota2019policy,
  title={Is the policy gradient a gradient?},
  author={Nota, Chris and Thomas, Philip S},
  booktitle={Proceedings of the International Conference on Autonomous Agenst and Multiagent Systems},
  year={2019}
}

@article{romoff2019separating,
  title={Separating value functions across time-scales},
  author={Romoff, Joshua and Henderson, Peter and Touati, Ahmed and Brunskill, Emma and Pineau, Joelle and Ollivier, Yann},
  journal={Proceedings of the International Conference on Machine Learning},
  year={2019}
}

@inproceedings{van2019use,
  title={When to use parametric models in reinforcement learning?},
  author={van Hasselt, Hado P and Hessel, Matteo and Aslanides, John},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14322--14333},
  year={2019}
}

@book{camacho2013model,
  title={Model predictive control},
  author={Camacho, Eduardo F and Alba, Carlos Bordons},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{ghosh2020operator,
  title={An operator view of policy gradient methods},
  author={Ghosh, Dibya and Machado, Marlos C and Roux, Nicolas Le},
  journal={arXiv preprint arXiv:2006.11266},
  year={2020}
}

@article{tucker2018mirage,
  title={The mirage of action-dependent baselines in reinforcement learning},
  author={Tucker, George and Bhupatiraju, Surya and Gu, Shixiang and Turner, Richard E and Ghahramani, Zoubin and Levine, Sergey},
  journal={arXiv preprint arXiv:1802.10031},
  year={2018}
}

@inproceedings{pardo2018time,
  title={Time limits in reinforcement learning},
  author={Pardo, Fabio and Tavakoli, Arash and Levdik, Vitaly and Kormushev, Petar},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2018},
}

@techreport{agarwal2019reinforcement,
  title={Reinforcement learning: Theory and algorithms},
  author={Agarwal, Alekh and Jiang, Nan and Kakade, Sham M},
  year={2019},
  institution={Technical Report, Department of Computer Science, University of Washington}
}

@incollection{hordijk2002blackwell,
  title={Blackwell optimality},
  author={Hordijk, Arie and Yushkevich, Alexander A},
  booktitle={Handbook of Markov decision processes},
  pages={231--267},
  year={2002},
  publisher={Springer}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie,  Charles and Sadik, Amir  and   Antonoglou, Ioannis and  King, Helen and   Kumaran, Dharshan and   Wierstra, Daan and  Legg, Shane and  Hassabis, Demis},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{o2016combining,
  title={Combining policy gradient and {Q}-learning},
  author={O'Donoghue, Brendan and Munos, Remi and Kavukcuoglu, Koray and Mnih, Volodymyr},
  booktitle={Proceedings of the International Conference on Learning Representations},
  year={2016}
}

@article{efroni2018beyond,
  title={Beyond the one step greedy approach in reinforcement learning},
  author={Efroni, Yonathan and Dalal, Gal and Scherrer, Bruno and Mannor, Shie},
  journal={arXiv preprint arXiv:1802.03654},
  year={2018}
}

@article{tomar2019multi,
  title={Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning},
  author={Tomar, Manan and Efroni, Yonathan and Ghavamzadeh, Mohammad},
  journal={arXiv preprint arXiv:1910.02919},
  year={2019}
}

@inproceedings{efroni2018multiple,
  title={Multiple-step greedy policies in approximate and online reinforcement learning},
  author={Efroni, Yonathan and Dalal, Gal and Scherrer, Bruno and Mannor, Shie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5238--5247},
  year={2018}
}

@inproceedings{nachum2019dualdice,
  title={Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections},
  author={Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2318--2328},
  year={2019}
}

@article{nachum2020reinforcement,
  title={Reinforcement learning via fenchel-rockafellar duality},
  author={Nachum, Ofir and Dai, Bo},
  journal={arXiv preprint arXiv:2001.01866},
  year={2020}
}

@inproceedings{bertsekas1995neuro,
  title={Neuro-dynamic programming: an overview},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  booktitle={Proceedings of 1995 34th IEEE Conference on Decision and Control},
  volume={1},
  pages={560--564},
  year={1995},
  organization={IEEE}
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2016}
}

@article{schulman2015high,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1506.02438},
  year={2015}
}

@article{sinha2020experience,
  title={Experience Replay with Likelihood-free Importance Weights},
  author={Sinha, Samarth and Song, Jiaming and Garg, Animesh and Ermon, Stefano},
  journal={arXiv},
  year={2020}
}

@inproceedings{schaul2015prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  booktitle={Proceedings of the International Conference on Learning Representations},
  year={2015}
}

@article{janner2020gamma,
  title={Gamma-Models: Generative Temporal Difference Learning for Infinite-Horizon Prediction},
  author={Janner, Michael and Mordatch, Igor and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@article{tassa2018deepmind,
  title={Deepmind control suite},
  author={Tassa, Yuval and Doron, Yotam and Muldal, Alistair and Erez, Tom and Li, Yazhe and Casas, Diego de Las and Budden, David and Abdolmaleki, Abbas and Merel, Josh and Lefrancq, Andrew and others},
  journal={arXiv preprint arXiv:1801.00690},
  year={2018}
}



@incollection{coumans2015bullet,
  title={Bullet physics simulation},
  author={Coumans, Erwin},
  booktitle={ACM SIGGRAPH 2015 Courses},
  pages={1},
  year={2015}
}


@inproceedings{todorov2012mujoco,
  title={Mu{J}o{C}o: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={International Conference on Intelligent Robots and Systems},
  year={2012},
}

@article{brockman2016openai,
  title={Open{AI} gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv},
  year={2016}
}

@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
  booktitle={Proceeedings of the International Conference on Machine Learning},
  year={2018}
}

@inproceedings{fujimoto2018addressing,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Van Hoof, Herke and Meger, David},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2018}
}


@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={ICML},
  volume={2},
  pages={267--274},
  year={2002}
}

@article{SpinningUp2018,
    author = {Achiam, Joshua and OpenAI},
    title = {{Spinning Up in Deep Reinforcement Learning}},
    year = {2018},
    journal = {https://github.com/openai/spinningup}
}

@inproceedings{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  booktitle={Proceedings of the International Conference on Learning Representations},
  year={2015}
}

@inproceedings{kearns2000bias,
  title={Bias-Variance Error Bounds for Temporal Difference Updates.},
  author={Kearns, Michael J and Singh, Satinder P},
  booktitle={Proceedings of the Conference on Learning Theory},
  year={2000},
}