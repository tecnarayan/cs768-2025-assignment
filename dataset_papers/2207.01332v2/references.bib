
@article{martin2000synaptic,
  title={Synaptic plasticity and memory: an evaluation of the hypothesis},
  author={Martin, Stephen J. and Grimwood, Paul D. and Morris, Richard G.M.},
  journal={Annual Review of Neuroscience},
  volume={23},
  number={1},
  pages={649--711},
  year={2000}  
}

@inproceedings{alemi_learning_2018,
	title = {Learning nonlinear dynamics in efficient, balanced spiking networks using local plasticity rules},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Alemi, Alireza and Machens, Christian and Deneve, Sophie and Slotine, Jean-Jacques},
	year = {2018},
}

@article{baydin_automatic_2018,
	title = {Automatic differentiation in machine learning: a survey},
	volume = {18},
	journal = {Journal of Marchine Learning Research},
	author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
	year = {2018},
	pages = {1--43},
}

@book{bryson_applied_1969,
	title = {Applied optimal control: optimization, estimation, and control},
	publisher = {Blaisdell Pub. Co.},
	author = {Bryson, A. E. and Ho, Y. C.},
	year = {1969},
}

@inproceedings{loshchilov_sgdr_2017,
	title = {{SGDR}: {Stochastic} gradient descent with restarts},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Loshchilov, Ilya and Hutter, Frank},
	year = {2017},
}

@inproceedings{ioffe_batch_2015,
	title = {Batch normalization: accelerating deep network training by reducing internal covariate shift},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Ioffe, Sergey and Szegedy, Christian},
	year = {2015},
}

@inproceedings{wu_group_2018,
	title = {Group normalization},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision}},
	author = {Wu, Yuxin and He, Kaiming},
	year = {2018},
}


@inproceedings{silver_learning_2021,
	title = {Learning by directional gradient descent},
	abstract = {How should state be constructed from a sequence of observations, so as to best achieve some objective? Most deep learning methods update the parameters of the state representation by gradient...},
	author = {Silver, David and Goyal, Anirudh and Danihelka, Ivo and Hessel, Matteo and Hasselt, Hado van},
	year = {2021},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/QIWNLNCX/Silver et al. - 2021 - Learning by Directional Gradient Descent.pdf:application/pdf;Snapshot:/Users/alexander/Zotero/storage/QHSL4IYK/forum.html:text/html},
}

@article{baldi_gradient_1995,
	title = {Gradient descent learning algorithm overview: {A} general dynamical systems perspective},
	volume = {6},
	number = {1},
	journal = {IEEE Transactions on neural networks},
	author = {Baldi, Pierre},
	year = {1995},
	note = {Publisher: IEEE},
	pages = {182--195},
}

@article{baydin_gradients_2022,
	title = {Gradients without backpropagation},
	journal = {arXiv preprint arXiv:2202.08587},
	author = {Baydin, Atılım Güneş and Pearlmutter, Barak A and Syme, Don and Wood, Frank and Torr, Philip},
	year = {2022},
}

@article{kirkpatrick_overcoming_2017,
	title = {Overcoming catastrophic forgetting in neural networks},
	volume = {114},
	number = {13},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
	year = {2017},
	keywords = {artificial intelligence, deep learning, continual learning, stability plasticity, synaptic consolidation},
	pages = {3521--3526},
}

@article{fusi_cascade_2005,
	title = {Cascade models of synaptically stored memories},
	volume = {45},
	number = {4},
	journal = {Neuron},
	author = {Fusi, Stefano and Drew, Patrick J. and Abbott, Larry F.},
	year = {2005},
	pages = {599--611},
}

@article{benna_computational_2016,
	title = {Computational principles of synaptic memory consolidation},
	volume = {19},
	number = {12},
	journal = {Nature Neuroscience},
	author = {Benna, Marcus K. and Fusi, Stefano},
	year = {2016},
	pages = {1697--1706},
}

@inproceedings{zenke_continual_2017,
	title = {Continual learning through synaptic intelligence},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
	year = {2017},
}

@article{ziegler_synaptic_2015,
	title = {Synaptic consolidation: from synapses to behavioral modeling},
	volume = {35},
	number = {3},
	journal = {Journal of Neuroscience},
	author = {Ziegler, Lorric and Zenke, Friedemann and Kastner, David B. and Gerstner, Wulfram},
	year = {2015},
	pages = {1319--1334},
}

@article{deneve_brain_2017,
	title = {The brain as an efficient and robust adaptive learner},
	volume = {94},
	number = {5},
	journal = {Neuron},
	author = {Denève, Sophie and Alemi, Alireza and Bourdoukan, Ralph},
	year = {2017},
	pages = {969--977},
}

@article{gilra_predicting_2017,
	title = {Predicting non-linear dynamics by stable local learning in a recurrent spiking neural network},
	volume = {6},
	journal = {eLife},
	author = {Gilra, Aditya and Gerstner, Wulfram},
	year = {2017},
	pages = {e28295},
}

@inproceedings{lecun_theoretical_1988,
	title = {A theoretical framework for back-propagation},
	booktitle = {Proceedings of the 1998 {Connectionist} {Models} {Summer} {School}},
	author = {LeCun, Yann},
	year = {1988},
}

@inproceedings{lee_meta-learning_2019,
	title = {Meta-learning with differentiable convex optimization},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Lee, Kwonjoon and Maji, Subhransu and Ravichandran, Avinash and Soatto, Stefano},
	year = {2019},
}

@article{el_ghaoui_implicit_2021,
	title = {Implicit deep learning},
	volume = {3},
	number = {3},
	journal = {SIAM Journal on Mathematics of Data Science},
	author = {El Ghaoui, Laurent and Gu, Fangda and Travacca, Bertrand and Askari, Armin and Tsai, Alicia},
	year = {2021},
	pages = {930--958},
}

@article{gilbert_top-down_2013,
	title = {Top-down influences on visual processing},
	volume = {14},
	number = {5},
	journal = {Nature Reviews Neuroscience},
	author = {Gilbert, Charles D. and Li, Wu},
	year = {2013},
	pages = {350--363},
}

@article{meulemans_minimizing_2022,
	title = {Minimizing control for credit assignment with strong feedback},
	journal = {arXiv preprint arXiv:2204.07249},
	author = {Meulemans, Alexander and Farinha, Matilde Tristany and Cervera, Maria R. and Sacramento, João and Grewe, Benjamin F.},
	year = {2022},
}

@inproceedings{huang_implicit2_2021,
	title = {Implicit$^{\textrm{2}}$: implicit layers for implicit representations},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Huang, Zhichun and Bai, Shaojie and Kolter, J. Zico},
	year = {2021},
}

@inproceedings{podlaski_biological_2020,
	title = {Biological credit assignment through dynamic inversion of feedforward networks},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Podlaski, William F. and Machens, Christian K.},
	year = {2020},
}

@inproceedings{zhao_meta-learning_2020,
	title = {Meta-learning via hypernetworks},
	abstract = {Recent developments in few-shot learning have shown that during fast adaption, gradient-based meta-learners mostly rely on embedding features of powerful pretrained networks. This leads us to research ways to effectively adapt features and utilize the meta-learner’s full potential. Here, we demonstrate the effectiveness of hypernetworks in this context. We propose a soft weight-sharing hypernetwork architecture and show that training the hypernetwork with a variant of MAML is tightly linked to meta-learning a curvature matrix used to condition gradients during fast adaptation. We achieve similar results as state-of-art model-agnostic methods in the overparametrized case, while outperforming many MAML variants without using different optimization schemes in the compressive regime. Furthermore, we empirically show that hypernetworks do leverage the inner loop optimization for better adaptation, and analyse how they naturally try to learn the shared curvature of constructed tasks on a toy problem when using our proposed training algorithm.},
	booktitle = {Workshop on {Meta}-{Learning} at {NeurIPS}},
	author = {Zhao, Dominic and Kobayashi, Seijin and Sacramento, João and von Oswald, Johannes},
	year = {2020},
	file = {Zhao et al. - Meta-Learning via Hypernetworks.pdf:/Users/alexander/Zotero/storage/2PTE7X9Q/Zhao et al. - Meta-Learning via Hypernetworks.pdf:application/pdf},
}

@article{gould_deep_2021,
	title = {Deep declarative networks},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Gould, Stephen and Hartley, Richard and Campbell, Dylan John},
	year = {2021},
}

@inproceedings{chen_neural_2018,
	title = {Neural ordinary differential equations},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K.},
	year = {2018},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/3M7RAZ68/Chen et al. - 2018 - Neural Ordinary Differential Equations.pdf:application/pdf},
}

@phdthesis{amos_differentiable_2019,
	type = {{PhD} {Thesis}},
	title = {Differentiable optimization-based modeling for machine learning},
	school = {Carnegie Mellon University},
	author = {Amos, Brandon},
	year = {2019},
}

@book{dontchev_implicit_2009,
	title = {Implicit functions and solution mappings},
	publisher = {Springer},
	author = {Dontchev, Asen L. and Rockafellar, R. Tyrrell},
	year = {2009},
	file = {Dontchev et Rockafellar - Implicit Functions and Solution Mappings.pdf:/Users/nicolas/Zotero/storage/2F4VEKVR/Dontchev et Rockafellar - Implicit Functions and Solution Mappings.pdf:application/pdf},
}

@article{pineda_recurrent_1989,
	title = {Recurrent backpropagation and the dynamical approach to adaptive neural computation},
	volume = {1},
	abstract = {Error backpropagation in feedforward neural network models is a popular learning algorithm that has its roots in nonlinear estimation and optimization. It is being used routinely to calculate error gradients in nonlinear systems with hundreds of thousands of parameters. However, the classical architecture for backpropagation has severe restrictions. The extension of backpropagation to networks with recurrent connections will be reviewed. It is now possible to efficiently compute the error gradients for networks that have temporal dynamics, which opens applications to a host of problems in systems identification and control.},
	number = {2},
	journal = {Neural Computation},
	author = {Pineda, Fernando J.},
	year = {1989},
	pages = {161--172},
}

@inproceedings{linsley_stable_2020,
	title = {Stable and expressive recurrent vision models},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Linsley, Drew and Karkada Ashok, Alekh and Govindarajan, Lakshmi Narasimhan and Liu, Rex and Serre, Thomas},
	year = {2020},
}

@article{linsley_learning_2018,
	title = {Learning long-range spatial dependencies with horizontal gated recurrent units},
	journal = {Advances in Neural Information Processing Systems},
	author = {Linsley, Drew and Kim, Junkyung and Veerabadran, Vijay and Windolf, Charles and Serre, Thomas},
	year = {2018},
}

@article{spoerer_recurrent_2017,
	title = {Recurrent convolutional neural networks: a better model of biological object recognition},
	volume = {8},
	journal = {Frontiers in psychology},
	author = {Spoerer, Courtney J and McClure, Patrick and Kriegeskorte, Nikolaus},
	year = {2017},
	note = {Publisher: Frontiers},
	pages = {1551},
}

@article{lindsay_how_2018,
	title = {How biological attention mechanisms improve task performance in a large-scale visual system model},
	volume = {7},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.38105},
	doi = {10.7554/eLife.38105},
	abstract = {How does attentional modulation of neural activity enhance performance? Here we use a deep convolutional neural network as a large-scale model of the visual system to address this question. We model the feature similarity gain model of attention, in which attentional modulation is applied according to neural stimulus tuning. Using a variety of visual tasks, we show that neural modulations of the kind and magnitude observed experimentally lead to performance changes of the kind and magnitude observed experimentally. We find that, at earlier layers, attention applied according to tuning does not successfully propagate through the network, and has a weaker impact on performance than attention applied according to values computed for optimally modulating higher areas. This raises the question of whether biological attention might be applied at least in part to optimize function rather than strictly according to tuning. We suggest a simple experiment to distinguish these alternatives.},
	urldate = {2022-05-17},
	journal = {eLife},
	author = {Lindsay, Grace W and Miller, Kenneth D},
	editor = {van Gerven, Marcel and Behrens, Timothy E and Peelen, Marius V},
	month = oct,
	year = {2018},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {convolutional neural networks, gain modulation, visual attention},
	pages = {e38105},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/TBQ6SJWP/Lindsay and Miller - 2018 - How biological attention mechanisms improve task p.pdf:application/pdf},
}

@article{liao_bridging_2016,
	title = {Bridging the gaps between residual learning, recurrent neural networks and visual cortex},
	journal = {arXiv preprint arXiv:1604.03640},
	author = {Liao, Qianli and Poggio, Tomaso},
	year = {2016},
}

@article{kirchberger_essential_2021,
	title = {The essential role of recurrent processing for figure-ground perception in mice},
	volume = {7},
	number = {27},
	journal = {Science Advances},
	author = {Kirchberger, Lisa and Mukherjee, Sreedeep and Schnabel, Ulf H. and van Beest, Enny H. and Barsegyan, Areg and Levelt, Christiaan N. and Heimel, J. Alexander and Lorteije, Jeannette A. M. and van der Togt, Chris and Self, Matthew W. and Roelfsema, Pieter R.},
	year = {2021},
	pages = {eabe1833},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/K76GTX2U/Kirchberger et al. - The essential role of recurrent processing for fig.pdf:application/pdf},
}

@article{larkum_cellular_2013,
	title = {A cellular mechanism for cortical associations: an organizing principle for the cerebral cortex},
	volume = {36},
	abstract = {A basic feature of intelligent systems such as the cerebral cortex is the ability to freely associate aspects of perceived experience with an internal representation of the world and make predictions about the future. Here, a hypothesis is presented that the extraordinary performance of the cortex derives from an associative mechanism built in at the cellular level to the basic cortical neuronal unit: the pyramidal cell. The mechanism is robustly triggered by coincident input to opposite poles of the neuron, is exquisitely matched to the large- and fine-scale architecture of the cortex, and is tightly controlled by local microcircuits of inhibitory neurons targeting subcellular compartments. This article explores the experimental evidence and the implications for how the cortex operates.},
	number = {3},
	journal = {Trends in Neurosciences},
	author = {Larkum, Matthew},
	year = {2013},
	keywords = {feedback, binding, calcium spike, dendrite, neocortex, pyramidal neuron},
	pages = {141--151},
	file = {ScienceDirect Snapshot:/Users/alexander/Zotero/storage/M76NB22D/S0166223612002032.html:text/html},
}

@article{manita_top-down_2015,
	title = {A top-down cortical circuit for accurate sensory perception},
	volume = {86},
	abstract = {A fundamental issue in cortical processing of sensory information is whether top-down control circuits from higher brain areas to primary sensory areas not only modulate but actively engage in perception. Here, we report the identification of a neural circuit for top-down control in the mouse somatosensory system. The circuit consisted of a long-range reciprocal projection between M2 secondary motor cortex and S1 primary somatosensory cortex. In vivo physiological recordings revealed that sensory stimulation induced sequential S1 to M2 followed by M2 to S1 neural activity. The top-down projection from M2 to S1 initiated dendritic spikes and persistent firing of S1 layer 5 (L5) neurons. Optogenetic inhibition of M2 input to S1 decreased L5 firing and the accurate perception of tactile surfaces. These findings demonstrate that recurrent input to sensory areas is essential for accurate perception and provide a physiological model for one type of top-down control circuit.},
	number = {5},
	journal = {Neuron},
	author = {Manita, Satoshi and Suzuki, Takayuki and Homma, Chihiro and Matsumoto, Takashi and Odagawa, Maya and Yamada, Kazuyuki and Ota, Keisuke and Matsubara, Chie and Inutsuka, Ayumu and Sato, Masaaki and Ohkura, Masamichi and Yamanaka, Akihiro and Yanagawa, Yuchio and Nakai, Junichi and Hayashi, Yasunori and Larkum, Matthew E. and Murayama, Masanori},
	year = {2015},
	pages = {1304--1316},
	file = {Full Text:/Users/alexander/Zotero/storage/PCYZ25LC/Manita et al. - 2015 - A Top-Down Cortical Circuit for Accurate Sensory P.pdf:application/pdf;ScienceDirect Snapshot:/Users/alexander/Zotero/storage/ARJ4VR3F/S0896627315004134.html:text/html},
}

@article{marques_functional_2018,
	title = {The functional organization of cortical feedback inputs to primary visual cortex},
	volume = {21},
	abstract = {Cortical feedback is thought to mediate cognitive processes like attention, prediction, and awareness. Understanding its function requires identifying the organizational logic of feedback axons relaying different signals. We measured retinotopic specificity in inputs from the lateromedial visual area in mouse primary visual cortex (V1) by mapping receptive fields in feedback boutons and relating them to those of neurons in their vicinity. Lateromedial visual area inputs in layer 1 targeted, on average, retinotopically matched locations in V1, but many of them relayed distal visual information. Orientation-selective axons overspread around the retinotopically matched location perpendicularly to their preferred orientation. Direction-selective axons were biased to visual areas shifted from the retinotopically matched position along the angle of their antipreferred direction. Our results show that feedback inputs show tuning-dependent retinotopic specificity. By targeting locations that would be activated by stimuli orthogonal to or opposite to a cell’s own tuning, feedback could potentially enhance visual representations in time and space.},
	number = {5},
	journal = {Nature Neuroscience},
	author = {Marques, Tiago and Nguyen, Julia and Fioreze, Gabriela and Petreanu, Leopoldo},
	year = {2018},
	keywords = {Neuroscience, Visual system},
	pages = {757--764},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/EWNRRFRK/Marques et al. - 2018 - The functional organization of cortical feedback i.pdf:application/pdf;Snapshot:/Users/alexander/Zotero/storage/QWM9CZ8P/s41593-018-0135-z.html:text/html},
}

@article{lecun_mnist_1998,
	title = {The {MNIST} database of handwritten digits},
	journal = {Available at http://yann. lecun. com/exdb/mnist},
	author = {LeCun, Yann},
	year = {1998},
}

@article{bai_multiscale_2020,
	title = {Multiscale deep equilibrium models},
	journal = {Advances in Neural Information Processing Systems},
	author = {Bai, Shaojie and Koltun, Vladlen and Kolter, J. Zico},
	year = {2020},
}

@article{bai_deep_2019,
	title = {Deep equilibrium models},
	journal = {Advances in Neural Information Processing Systems},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	year = {2019},
}

@article{van_bergen_going_2020,
	title = {Going in circles is the way forward: the role of recurrence in visual inference},
	volume = {65},
	abstract = {Biological visual systems exhibit abundant recurrent connectivity. State-of-the-art neural network models for visual recognition, by contrast, rely heavily or exclusively on feedforward computation. Any finite-time recurrent neural network (RNN) can be unrolled along time to yield an equivalent feedforward neural network (FNN). This important insight suggests that computational neuroscientists may not need to engage recurrent computation, and that computer-vision engineers may be limiting themselves to a special case of FNN if they build recurrent models. Here we argue, to the contrary, that FNNs are a special case of RNNs and that computational neuroscientists and engineers should engage recurrence to understand how brains and machines can (1) achieve greater and more flexible computational depth (2) compress complex computations into limited hardware (3) integrate priors and priorities into visual inference through expectation and attention (4) exploit sequential dependencies in their data for better inference and prediction and (5) leverage the power of iterative computation.},
	journal = {Current Opinion in Neurobiology},
	author = {van Bergen, Ruben S. and Kriegeskorte, Nikolaus},
	year = {2020},
	pages = {176--193},
	file = {ScienceDirect Snapshot:/Users/alexander/Zotero/storage/CYXGTUU7/S0959438820301768.html:text/html;Submitted Version:/Users/alexander/Zotero/storage/KXGKXEBQ/van Bergen and Kriegeskorte - 2020 - Going in circles is the way forward the role of r.pdf:application/pdf},
}

@inproceedings{almeida_backpropagation_1989,
	title = {Backpropagation in perceptrons with feedback},
	abstract = {Backpropagation has shown to be an efficient learning rule for graded perceptrons. However, as initially introduced, it was limited to feedforward structures. Extension of backpropagation to systems with feedback was done by this author, in [4]. In this paper, this extension is presented, and the error propagation circuit is interpreted as the transpose of the linearized perceptron network. The error propagation network is shown to always be stable during training, and a sufficient condition for the stability of the perceptron network is derived. Finally, potentially useful relationships with Hopfield networks and Boltzmann machines are discussed.},
	booktitle = {Neural {Computers}},
	publisher = {Springer Berlin Heidelberg},
	author = {Almeida, Luís B.},
	editor = {Eckmiller, Rolf and v.d. Malsburg, Christoph},
	year = {1989},
	pages = {199--208},
}

@inproceedings{linsley_recurrent_2020,
	title = {Recurrent neural circuits for contour detection},
	abstract = {We introduce a deep recurrent neural network architecture that approximates visual cortical circuits. We show that this architecture, which we refer to as the gamma-net, learns to solve contour detection tasks with better sample efficiency than state-of-the-art feedforward networks, while also exhibiting a classic perceptual illusion, known as the orientation-tilt illusion. Correcting this illusion significantly reduces gamma-net contour detection accuracy by driving it to prefer low-level edges over high-level object boundary contours. Overall, our study suggests that the orientation-tilt illusion is a byproduct of neural circuits that help biological visual systems achieve robust and efficient contour detection, and that incorporating these circuits in artificial neural networks can improve computer vision.},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Linsley, Drew and Kim, Junkyung and Ashok, Alekh and Serre, Thomas},
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/alexander/Zotero/storage/5KL96IB2/Linsley et al. - 2020 - Recurrent neural circuits for contour detection.pdf:application/pdf;arXiv.org Snapshot:/Users/alexander/Zotero/storage/78RMDR28/2010.html:text/html},
}

@inproceedings{
huang2021textrmimplicit,
title={\$({\textbackslash}textrm\{Implicit\}){\textasciicircum}2\$: Implicit Layers for Implicit Representations},
author={Zhichun Huang and Shaojie Bai and J Zico Kolter},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
}

@article{douglas_neuronal_2004,
	title = {Neuronal circuits of the neocortex},
	volume = {27},
	abstract = {We explore the extent to which neocortical circuits generalize, i.e., to what extent can neocortical neurons and the circuits they form be considered as canonical? We find that, as has long been suspected by cortical neuroanatomists, the same basic laminar and tangential organization of the excitatory neurons of the neocortex is evident wherever it has been sought. Similarly, the inhibitory neurons show characteristic morphology and patterns of connections throughout the neocortex. We offer a simple model of cortical processing that is consistent with the major features of cortical circuits: The superficial layer neurons within local patches of cortex, and within areas, cooperate to explore all possible interpretations of different cortical input and cooperatively select an interpretation consistent with their various cortical and subcortical inputs.},
	number = {1},
	journal = {Annual Review of Neuroscience},
	author = {Douglas, Rodney J. and Martin, Kevan A.C.},
	year = {2004},
	pages = {419--451},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/ERQ8P9V4/Douglas and Martin - 2004 - Neuronal Circuits of the Neocortex.pdf:application/pdf},
}

@inproceedings{kubilius_brain-like_2019,
	title = {Brain-like object recognition with high-performing shallow recurrent {ANNs}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Kubilius, Jonas and Schrimpf, Martin and Kar, Kohitij and Rajalingham, Rishi and Hong, Ha and Majaj, Najib and Issa, Elias and Bashivan, Pouya and Prescott-Roy, Jonathan and Schmidt, Kailyn and Nayebi, Aran and Bear, Daniel and Yamins, Daniel L. and DiCarlo, James J.},
	year = {2019},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/2ISG3KNN/Kubilius et al. - 2019 - Brain-Like Object Recognition with High-Performing.pdf:application/pdf},
}

@article{nayebi_task-driven_2018,
	title = {Task-driven convolutional recurrent models of the visual system},
	volume = {31},
	journal = {Advances in Neural Information Processing Systems},
	author = {Nayebi, Aran and Bear, Daniel and Kubilius, Jonas and Kar, Kohitij and Ganguli, Surya and Sussillo, David and DiCarlo, James J. and Yamins, Daniel L.},
	year = {2018},
}

@article{felleman_distributed_1991,
	title = {Distributed hierarchical processing in the primate cerebral cortex},
	volume = {1},
	number = {1},
	journal = {Cerebral Cortex},
	author = {Felleman, D. J. and van Essen, D. C.},
	year = {1991},
	pages = {1--47},
}

@article{seijdel_necessity_2021,
	title = {On the {Necessity} of {Recurrent} {Processing} during {Object} {Recognition}: {It} {Depends} on the {Need} for {Scene} {Segmentation}},
	volume = {41},
	copyright = {Copyright © 2021 the authors. SfN exclusive license.},
	issn = {0270-6474, 1529-2401},
	shorttitle = {On the {Necessity} of {Recurrent} {Processing} during {Object} {Recognition}},
	url = {https://www.jneurosci.org/content/41/29/6281},
	doi = {10.1523/JNEUROSCI.2851-20.2021},
	abstract = {Although feedforward activity may suffice for recognizing objects in isolation, additional visual operations that aid object recognition might be needed for real-world scenes. One such additional operation is figure-ground segmentation, extracting the relevant features and locations of the target object while ignoring irrelevant features. In this study of 60 human participants (female and male), we show objects on backgrounds of increasing complexity to investigate whether recurrent computations are increasingly important for segmenting objects from more complex backgrounds. Three lines of evidence show that recurrent processing is critical for recognition of objects embedded in complex scenes. First, behavioral results indicated a greater reduction in performance after masking objects presented on more complex backgrounds, with the degree of impairment increasing with increasing background complexity. Second, electroencephalography (EEG) measurements showed clear differences in the evoked response potentials between conditions around time points beyond feedforward activity, and exploratory object decoding analyses based on the EEG signal indicated later decoding onsets for objects embedded in more complex backgrounds. Third, deep convolutional neural network performance confirmed this interpretation. Feedforward and less deep networks showed a higher degree of impairment in recognition for objects in complex backgrounds compared with recurrent and deeper networks. Together, these results support the notion that recurrent computations drive figure-ground segmentation of objects in complex scenes.
SIGNIFICANCE STATEMENT The incredible speed of object recognition suggests that it relies purely on a fast feedforward buildup of perceptual activity. However, this view is contradicted by studies showing that disruption of recurrent processing leads to decreased object recognition performance. Here, we resolve this issue by showing that how object recognition is resolved and whether recurrent processing is crucial depends on the context in which it is presented. For objects presented in isolation or in simple environments, feedforward activity could be sufficient for successful object recognition. However, when the environment is more complex, additional processing seems necessary to select the elements that belong to the object and by that segregate them from the background.},
	language = {en},
	number = {29},
	urldate = {2022-03-15},
	journal = {Journal of Neuroscience},
	author = {Seijdel, Noor and Loke, Jessica and Klundert, Ron van de and Meer, Matthew van der and Quispel, Eva and Gaal, Simon van and Haan, Edward H. F. de and Scholte, H. Steven},
	month = jul,
	year = {2021},
	pmid = {34088797},
	note = {Publisher: Society for Neuroscience
Section: Research Articles},
	keywords = {object recognition, deep convolutional neural network, natural scene statistics, scene segmentation, visual categorization, visual perception},
	pages = {6281--6289},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/PBG5554L/Seijdel et al. - 2021 - On the Necessity of Recurrent Processing during Ob.pdf:application/pdf;Snapshot:/Users/alexander/Zotero/storage/EPQSGAZE/6281.html:text/html},
}

@article{werbos_backpropagation_1990,
	title = {Backpropagation through time: what it does and how to do it},
	volume = {78},
	abstract = {Basic backpropagation, which is a simple method now being widely used in areas like pattern recognition and fault diagnosis, is reviewed. The basic equations for backpropagation through time, and applications to areas like pattern recognition involving dynamic systems, systems identification, and control are discussed. Further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations, or true recurrent networks, and other practical issues arising with the method are described. Pseudocode is provided to clarify the algorithms. The chain rule for ordered derivatives-the theorem which underlies backpropagation-is briefly discussed. The focus is on designing a simpler version of backpropagation which can be translated into computer code and applied directly by neutral network users.{\textless}{\textgreater}},
	number = {10},
	journal = {Proceedings of the IEEE},
	author = {Werbos, Paul},
	year = {1990},
	keywords = {Neural networks, Backpropagation, Pattern recognition, Supervised learning, Artificial neural networks, Equations, Control systems, Books, Fluid dynamics, Power system modeling},
	pages = {1550--1560},
	file = {IEEE Xplore Abstract Record:/Users/alexander/Zotero/storage/Y82YCJUF/58337.html:text/html;IEEE Xplore Full Text PDF:/Users/alexander/Zotero/storage/L2LRR2RZ/Werbos - 1990 - Backpropagation through time what it does and how.pdf:application/pdf},
}

@inproceedings{zintgraf_fast_2019,
	title = {Fast context adaptation via meta-learning},
	abstract = {We propose CAVIA for meta-learning, a simple extension to MAML that is less prone to metaoverﬁtting, easier to parallelise, and more interpretable. CAVIA partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, only the context parameters are updated, leading to a low-dimensional task representation. We show empirically that CAVIA outperforms MAML for regression, classiﬁcation, and reinforcement learning. Our experiments also highlight weaknesses in current benchmarks, in that the amount of adaptation needed in some cases is small.},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Zintgraf, Luisa and Shiarlis, Kyriacos and Kurin, Vitaly and Hofmann, Katja and Whiteson, Shimon},
	year = {2019},
	file = {Zintgraf et al. - Fast Context Adaptation via Meta-Learning.pdf:/Users/alexander/Zotero/storage/JNU6TQXW/Zintgraf et al. - Fast Context Adaptation via Meta-Learning.pdf:application/pdf},
}

@article{richards_deep_2019,
	title = {A deep learning framework for neuroscience},
	volume = {22},
	abstract = {Systems neuroscience seeks explanations for how the brain implements a wide variety of perceptual, cognitive and motor tasks. Conversely, artificial intelligence attempts to design computational systems based on the tasks they will have to solve. In artificial neural networks, the three components specified by design are the objective functions, the learning rules and the architectures. With the growing success of deep learning, which utilizes brain-inspired architectures, these three designed components have increasingly become central to how we model, engineer and optimize complex artificial learning systems. Here we argue that a greater focus on these components would also benefit systems neuroscience. We give examples of how this optimization-based framework can drive theoretical and experimental progress in neuroscience. We contend that this principled perspective on systems neuroscience will help to generate more rapid progress.},
	number = {11},
	journal = {Nature Neuroscience},
	author = {Richards, Blake A. and Lillicrap, Timothy P. and Beaudoin, Philippe and Bengio, Yoshua and Bogacz, Rafal and Christensen, Amelia and Clopath, Claudia and Costa, Rui Ponte and de Berker, Archy and Ganguli, Surya and Gillon, Colleen J. and Hafner, Danijar and Kepecs, Adam and Kriegeskorte, Nikolaus and Latham, Peter and Lindsay, Grace W. and Miller, Kenneth D. and Naud, Richard and Pack, Christopher C. and Poirazi, Panayiota and Roelfsema, Pieter and Sacramento, João and Saxe, Andrew and Scellier, Benjamin and Schapiro, Anna C. and Senn, Walter and Wayne, Greg and Yamins, Daniel and Zenke, Friedemann and Zylberberg, Joel and Therien, Denis and Kording, Konrad P.},
	year = {2019},
	pages = {1761--1770},
}

@article{richards_dendritic_2019,
	series = {Neurobiology of {Learning} and {Plasticity}},
	title = {Dendritic solutions to the credit assignment problem},
	volume = {54},
	abstract = {Guaranteeing that synaptic plasticity leads to effective learning requires a means for assigning credit to each neuron for its contribution to behavior. The ‘credit assignment problem’ refers to the fact that credit assignment is non-trivial in hierarchical networks with multiple stages of processing. One difficulty is that if credit signals are integrated with other inputs, then it is hard for synaptic plasticity rules to distinguish credit-related activity from non-credit-related activity. A potential solution is to use the spatial layout and non-linear properties of dendrites to distinguish credit signals from other inputs. In cortical pyramidal neurons, evidence hints that top-down feedback signals are integrated in the distal apical dendrites and have a distinct impact on spike-firing and synaptic plasticity. This suggests that the distal apical dendrites of pyramidal neurons help the brain to solve the credit assignment problem.},
	journal = {Current Opinion in Neurobiology},
	author = {Richards, Blake A. and Lillicrap, Timothy P.},
	year = {2019},
	pages = {28--36},
}

@article{lillicrap_backpropagation_2020,
	title = {Backpropagation and the brain},
	volume = {21},
	abstract = {During learning, the brain modifies synapses to improve behaviour. In the cortex, synapses are embedded within multilayered networks, making it difficult to determine the effect of an individual synaptic modification on the behaviour of the system. The backpropagation algorithm solves this problem in deep artificial neural networks, but historically it has been viewed as biologically problematic. Nonetheless, recent developments in neuroscience and the successes of artificial neural networks have reinvigorated interest in whether backpropagation offers insights for understanding learning in the cortex. The backpropagation algorithm learns quickly by computing synaptic updates using feedback connections to deliver error signals. Although feedback connections are ubiquitous in the cortex, it is difficult to see how they could deliver the error signals required by strict formulations of backpropagation. Here we build on past and recent developments to argue that feedback connections may instead induce neural activities whose differences can be used to locally approximate these signals and hence drive effective learning in deep networks in the brain.},
	language = {en},
	number = {6},
	journal = {Nature Reviews Neuroscience},
	author = {Lillicrap, Timothy P. and Santoro, Adam and Marris, Luke and Akerman, Colin J. and Hinton, Geoffrey},
	year = {2020},
	pages = {335--346},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/PCT4JCDB/Lillicrap et al. - 2020 - Backpropagation and the brain.pdf:application/pdf;Snapshot:/Users/alexander/Zotero/storage/JWLMHSL4/s41583-020-0277-3.html:text/html},
}

@article{lillicrap_backpropagation_2019,
	title = {Backpropagation through time and the brain},
	volume = {55},
	abstract = {It has long been speculated that the backpropagation-of-error algorithm (backprop) may be a model of how the brain learns. Backpropagation-through-time (BPTT) is the canonical temporal-analogue to backprop used to assign credit in recurrent neural networks in machine learning, but there's even less conviction about whether BPTT has anything to do with the brain. Even in machine learning the use of BPTT in classic neural network architectures has proven insufficient for some challenging temporal credit assignment (TCA) problems that we know the brain is capable of solving. Nonetheless, recent work in machine learning has made progress in solving difficult TCA problems by employing novel memory-based and attention-based architectures and algorithms, some of which are brain inspired. Importantly, these recent machine learning methods have been developed in the context of, and with reference to BPTT, and thus serve to strengthen BPTT's position as a useful normative guide for thinking about temporal credit assignment in artificial and biological systems alike.},
	journal = {Current Opinion in Neurobiology},
	author = {Lillicrap, Timothy P. and Santoro, Adam},
	year = {2019},
	pages = {82--89},
	file = {ScienceDirect Full Text PDF:/Users/alexander/Zotero/storage/X2BSQ73J/Lillicrap and Santoro - 2019 - Backpropagation through time and the brain.pdf:application/pdf;ScienceDirect Snapshot:/Users/alexander/Zotero/storage/HCDMPIEE/S0959438818302009.html:text/html},
}

@article{rusakov_noisy_2020,
	title = {Noisy synaptic conductance: {Bug} or a feature?},
	volume = {43},
	number = {6},
	journal = {Trends in Neurosciences},
	author = {Rusakov, Dmitri A. and Savtchenko, Leonid P. and Latham, Peter E.},
	year = {2020},
	pages = {363--372},
}

@inproceedings{dold_lagrangian_2019,
	title = {Lagrangian dynamics of dendritic microcircuits enables real-time backpropagation of errors},
	booktitle = {Computational and {Systems} {Neuroscience} ({Cosyne})},
	author = {Dold, Dominik and Kungl, Akos F. and Sacramento, João and Petrovici, Mihai A. and Schindler, Kaspar and Binas, Jonathan and Bengio, Yoshua and Senn, Walter},
	year = {2019},
}

@inproceedings{meulemans_credit_2021,
	title = {Credit assignment in neural networks through deep feedback control},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Meulemans, Alexander and Tristany Farinha, Matilde and Garcia Ordonez, Javier and Vilimelis Aceituno, Pau and Sacramento, João and Grewe, Benjamin F.},
	year = {2021},
}

@inproceedings{carreira-perpinan_distributed_2014,
	title = {Distributed optimization of deeply nested systems},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	author = {Carreira-Perpinan, Miguel and Wang, Weiran},
	year = {2014},
}

@phdthesis{scellier_deep_2021,
	type = {{PhD} {Thesis}},
	title = {A deep learning theory for neural networks grounded in physics},
	school = {Université de Montréal},
	author = {Scellier, Benjamin},
	year = {2021},
}

@article{scellier_equilibrium_2017,
	title = {Equilibrium propagation: bridging the gap between energy-based models and backpropagation},
	volume = {11},
	language = {en},
	journal = {Frontiers in Computational Neuroscience},
	author = {Scellier, Benjamin and Bengio, Yoshua},
	year = {2017},
}

@book{bertsekas_constrained_2014,
	title = {Constrained optimization and {Lagrange} multiplier methods},
	publisher = {Academic Press},
	author = {Bertsekas, Dimitri P.},
	year = {2014},
	file = {1982 - Constrained Optimization and Lagrange Multiplier M.pdf:/Users/alexander/Zotero/storage/D9UTSYMW/1982 - Constrained Optimization and Lagrange Multiplier M.pdf:application/pdf},
}

@article{friston_free-energy_2009,
	title = {The free-energy principle: a rough guide to the brain?},
	volume = {13},
	number = {7},
	journal = {Trends in Cognitive Sciences},
	author = {Friston, Karl},
	year = {2009},
	pages = {293--301},
}

@article{whittington_theories_2019,
	title = {Theories of error back-propagation in the brain},
	volume = {23},
	abstract = {This review article summarises recently proposed theories on how neural circuits in the brain could approximate the error back-propagation algorithm used by artificial neural networks. Computational models implementing these theories achieve learning as efficient as artificial neural networks, but they use simple synaptic plasticity rules based on activity of presynaptic and postsynaptic neurons. The models have similarities, such as including both feedforward and feedback connections, allowing information about error to propagate throughout the network. Furthermore, they incorporate experimental evidence on neural connectivity, responses, and plasticity. These models provide insights on how brain networks might be organised such that modification of synaptic weights on multiple levels of cortical hierarchy leads to improved performance on tasks.},
	number = {3},
	journal = {Trends in Cognitive Sciences},
	author = {Whittington, James C. R. and Bogacz, Rafal},
	year = {2019},
	keywords = {synaptic plasticity, deep learning, neural networks, predictive coding},
	pages = {235--250},
	file = {ScienceDirect Full Text PDF:/Users/alexander/Zotero/storage/CLX5L8SD/Whittington and Bogacz - 2019 - Theories of Error Back-Propagation in the Brain.pdf:application/pdf;ScienceDirect Snapshot:/Users/alexander/Zotero/storage/XG2QWB7S/S1364661319300129.html:text/html},
}

@article{whittington_approximation_2017,
	title = {An approximation of the error backpropagation algorithm in a predictive coding network with local {Hebbian} synaptic plasticity},
	volume = {29},
	number = {5},
	journal = {Neural Computation},
	author = {Whittington, James C. R. and Bogacz, Rafal},
	year = {2017},
	pages = {1229--1262},
}

@phdthesis{werbos_beyond_1974,
	type = {Ph.{D}. thesis},
	title = {Beyond regression: new tools for prediction and analysis in the behavioral sciences},
	school = {Harvard University},
	author = {Werbos, Paul},
	year = {1974},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	number = {6088},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	year = {1986},
	pages = {533--536},
}

@book{nocedal_numerical_2006,
	title = {Numerical optimization},
	publisher = {Springer},
	author = {Nocedal, Jorge and Wright, Stephen J.},
	year = {2006},
	keywords = {Mathematical optimization},
	file = {Nocedal and Wright - 2006 - Numerical optimization.pdf:/Users/alexander/Zotero/storage/CIZQYZPT/Nocedal and Wright - 2006 - Numerical optimization.pdf:application/pdf},
}

@article{lake_human-level_2015,
	title = {Human-level concept learning through probabilistic program induction},
	volume = {350},
	number = {6266},
	journal = {Science},
	author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
	year = {2015},
	pages = {1332--1338},
}

@inproceedings{rajeswaran_meta-learning_2019,
	title = {Meta-learning with implicit gradients},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Rajeswaran, Aravind and Finn, Chelsea and Kakade, Sham and Levine, Sergey},
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Optimization and Control},
	file = {Rajeswaran et al. - 2019 - Meta-Learning with Implicit Gradients.pdf:/Users/alexander/Zotero/storage/2T49DD2C/Rajeswaran et al. - 2019 - Meta-Learning with Implicit Gradients.pdf:application/pdf},
}

@inproceedings{lorraine_optimizing_2020,
	title = {Optimizing millions of hyperparameters by implicit differentiation},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Lorraine, Jonathan and Vicol, Paul and Duvenaud, David},
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/alexander/Zotero/storage/QYUH7BT5/Lorraine et al. - 2019 - Optimizing Millions of Hyperparameters by Implicit.pdf:application/pdf;arXiv.org Snapshot:/Users/alexander/Zotero/storage/62CR69L8/1911.html:text/html},
}

@inproceedings{luketina_scalable_2016,
	title = {Scalable gradient-based tuning of continuous regularization hyperparameters},
	abstract = {Hyperparameter selection generally relies on running multiple full training trials, with selection based on validation set performance. We propose a gradient-based approach for locally adjusting hyperparameters during training of the model. Hyperparameters are adjusted so as to make the model parameter gradients, and hence updates, more advantageous for the validation cost. We explore the approach for tuning regularization hyperparameters and find that in experiments on MNIST, SVHN and CIFAR-10, the resulting regularization levels are within the optimal regions. The additional computational cost depends on how frequently the hyperparameters are trained, but the tested scheme adds only 30\% computational overhead regardless of the model size. Since the method is significantly less computationally demanding compared to similar gradient-based approaches to hyperparameter optimization, and consistently finds good hyperparameter values, it can be a useful tool for training neural network models.},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Luketina, Jelena and Berglund, Mathias and Greff, Klaus and Raiko, Tapani},
	year = {2016},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/alexander/Zotero/storage/JAXWP8KN/Luketina et al. - 2016 - Scalable Gradient-Based Tuning of Continuous Regul.pdf:application/pdf},
}

@article{nichol_first-order_2018,
	title = {On first-order meta-learning algorithms},
	abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
	journal = {arXiv preprint arXiv:1803.02999},
	author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
	year = {2018},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/alexander/Zotero/storage/DZEA6XHD/Nichol et al. - 2018 - On First-Order Meta-Learning Algorithms.pdf:application/pdf;arXiv.org Snapshot:/Users/alexander/Zotero/storage/GR83K4MV/1803.html:text/html},
}

@inproceedings{finn_model-agnostic_2017,
	title = {Model-agnostic meta-learning for fast adaptation of deep networks},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	year = {2017},
}


@article{zucchet_contrastive_2022,
	title = {A contrastive rule for meta-learning},
	abstract = {Meta-learning algorithms leverage regularities that are present on a set of tasks to speed up and improve the performance of a subsidiary learning process. Recent work on deep neural networks has shown that prior gradient-based learning of meta-parameters can greatly improve the efficiency of subsequent learning. Here, we present a gradient-based meta-learning algorithm based on equilibrium propagation. Instead of explicitly differentiating the learning process, our contrastive meta-learning rule estimates meta-parameter gradients by executing the subsidiary process more than once. This avoids reversing the learning dynamics in time and computing second-order derivatives. In spite of this, and unlike previous first-order methods, our rule recovers an arbitrarily accurate meta-parameter update given enough compute. As such, contrastive meta-learning is a candidate rule for biologically-plausible meta-learning. We establish theoretical bounds on its performance and present experiments on a set of standard benchmarks and neural network architectures.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Zucchet, Nicolas and Schug, Simon and von Oswald, Johannes and Zhao, Dominic and Sacramento, João},
	year = {2022},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Computer Science - Neural and Evolutionary Computing},
	file = {Zucchet et al. - 2021 - A contrastive rule for meta-learning.pdf:/Users/nicolas/Zotero/storage/YDMRHLZ7/Zucchet et al. - 2021 - A contrastive rule for meta-learning.pdf:application/pdf},
}


@inproceedings{liao_reviving_2018,
	title = {Reviving and improving recurrent back-propagation},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Liao, Renjie and Xiong, Yuwen and Fetaya, Ethan and Zhang, Lisa and Yoon, KiJung and Pitkow, Xaq and Urtasun, Raquel and Zemel, Richard},
	year = {2018},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/ZPPDQKEG/Liao et al. - 2018 - Reviving and Improving Recurrent Back-Propagation.pdf:application/pdf;Snapshot:/Users/alexander/Zotero/storage/N5K9VND8/liao18c.html:text/html},
}

@inproceedings{akrout_deep_2019,
	title = {Deep learning without weight transport},
	abstract = {Current algorithms for deep learning probably cannot run in the brain because
they rely on weight transport, where forward-path neurons transmit their synaptic
weights to a feedback path, in a way that is likely impossible biologically. An algorithm called feedback alignment achieves deep learning without weight transport by using random feedback weights, but it performs poorly on hard visual-recognition tasks. Here we describe two mechanisms — a neural circuit called a weight mirror and a modification of an algorithm proposed by Kolen and Pollack in 1994 — both of which let the feedback path learn appropriate synaptic weights quickly and accurately even in large networks, without weight transport or complex wiring. Tested on the ImageNet visual-recognition task, these mechanisms outperform both feedback alignment and the newer sign-symmetry method, and nearly match backprop, the standard algorithm of deep learning, which uses weight transport.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Akrout, Mohamed and Wilson, Collin and Humphreys, Peter and Lillicrap, Timothy P. and Tweed, Douglas B.},
	year = {2019},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/DLFK7W89/Akrout et al. - 2019 - Deep Learning without Weight Transport.pdf:application/pdf},
}

@inproceedings{kolen_back-propagation_1994,
	title = {Back-propagation without weight transport},
	abstract = {In back-propagation (Rumelhart et al, 1985) connection weights are used to both compute node activations and error gradients for hidden units. Grossberg (1987) has argued that the dual use of the same synaptic connections ("weight transport") constitutes a bidirectional flow of information through synapses, which is biologically implausable. In this paper we formally and empirically demonstrate the feasibility of an architecture equivalent to back-propagation, but without the assumption of weight transport. Through coordinated training with weight decay, a reciprocal layer of weights evolves into a copy of the forward connections and acts as the conduit for backward flowing corrective information. Examination of the networks trained with dual weights suggests that functional synchronization, and not weight synchronization, is crucial to the operation of back-propagation methods.  Introduction  Back-propagation (Rumelhart et al, 1985) is a popular gradient descent method for tuning the ...},
	booktitle = {Proceedings of 1994 {IEEE} {International} {Conference} on {Neural} {Networks} ({ICNN}’94)},
	author = {Kolen, John F. and Pollack, Jordan B.},
	year = {1994},
	file = {Citeseer - Full Text PDF:/Users/alexander/Zotero/storage/QGVBUGP3/Kolen and Pollack - 1994 - Back-Propagation Without Weight Transport.pdf:application/pdf;Citeseer - Snapshot:/Users/alexander/Zotero/storage/Q82HHEQ7/summary.html:text/html},
}

@inproceedings{meulemans_theoretical_2020,
	title = {A theoretical framework for target propagation},
	abstract = {The success of deep learning, a brain-inspired form of AI, has sparked interest in understanding how the brain could similarly learn across multiple layers of neurons. However, the majority of biologically-plausible learning algorithms have not yet reached the performance of backpropagation (BP), nor are they built on strong theoretical foundations. Here, we analyze target propagation (TP), a popular but not yet fully understood alternative to BP, from the standpoint of mathematical optimization. Our theory shows that TP is closely related to Gauss-Newton optimization and thus substantially differs from BP. Furthermore, our analysis reveals a fundamental limitation of difference target propagation (DTP), a well-known variant of TP, in the realistic scenario of non-invertible neural networks. We provide a first solution to this problem through a novel reconstruction loss that improves feedback weight training, while simultaneously introducing architectural flexibility by allowing for direct feedback connections from the output to each hidden layer. Our theory is corroborated by experimental results that show significant improvements in performance and in the alignment of forward weight updates with loss gradients, compared to DTP.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Meulemans, Alexander and Carzaniga, Francesco and Suykens, Johan and Sacramento, João and Grewe, Benjamin F.},
	year = {2020},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/F5VPSJH7/Meulemans et al. - 2020 - A Theoretical Framework for Target Propagation.pdf:application/pdf},
}

@techreport{krizhevsky_learning_2009,
	title = {Learning multiple layers of features from tiny images},
	author = {Krizhevsky, Alex and Hinton, Geoffrey},
	year = {2009},
	file = {Krizhevsky - Learning Multiple Layers of Features from Tiny Ima.pdf:/Users/alexander/Zotero/storage/564KP94S/Krizhevsky - Learning Multiple Layers of Features from Tiny Ima.pdf:application/pdf},
}

@article{rao_predictive_1999,
	title = {Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects},
	volume = {2},
	number = {1},
	journal = {Nature Neuroscience},
	author = {Rao, Rajesh P. N. and Ballard, Dana H.},
	year = {1999},
	pages = {79--87},
}

@inproceedings{sacramento_dendritic_2018,
	title = {Dendritic cortical microcircuits approximate the backpropagation algorithm},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Sacramento, João and Costa, Rui P. and Bengio, Yoshua and Senn, Walter},
	year = {2018},
}

@article{grossberg_competitive_1987,
	title = {Competitive learning: {From} interactive activation to adaptive resonance},
	volume = {11},
	number = {1},
	journal = {Cognitive science},
	author = {Grossberg, Stephen},
	year = {1987},
	pages = {23--63},
}

@inproceedings{nokland_direct_2016,
	title = {Direct feedback alignment provides learning in deep neural networks},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Nøkland, Arild},
	year = {2016},
}

@article{lillicrap_random_2016,
	title = {Random synaptic feedback weights support error backpropagation for deep learning},
	volume = {7},
	abstract = {The brain processes information through multiple layers of neurons. This deep architecture is representationally powerful, but complicates learning because it is difficult to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame by multiplying error signals with all the synaptic weights on each neuron’s axon and further downstream. However, this involves a precise, symmetric backward connectivity pattern, which is thought to be impossible in the brain. Here we demonstrate that this strong architectural constraint is not required for effective error propagation. We present a surprisingly simple mechanism that assigns blame by multiplying errors by even random synaptic weights. This mechanism can transmit teaching signals across multiple layers of neurons and performs as effectively as backpropagation on a variety of tasks. Our results help reopen questions about how the brain could use error signals and dispel long-held assumptions about algorithmic constraints on learning.},
	language = {en},
	number = {1},
	journal = {Nature Communications},
	author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
	year = {2016},
	pages = {13276},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/Y8CI26JS/Lillicrap et al. - 2016 - Random synaptic feedback weights support error bac.pdf:application/pdf;Snapshot:/Users/alexander/Zotero/storage/BDE3538U/ncomms13276.html:text/html},
}

@article{cohen_absolute_1983,
	title = {Absolute stability of global pattern formation and parallel memory storage by competitive neural networks},
	volume = {SMC-13},
	abstract = {Systems that are competitive and possess symmetric interactions admit a global Lyapunov function. However, a global Lyapunov function whose equilibrium set can be effectively analyzed has not yet been discovered. It remains an open question whether the Lyapunov function approach, which requires a study of equilibrium points, or an alternative global approach, such as the Lyapunov functional approach, which sidesteps a direct study of equilibrium points will ultimately handle all of the physically important cases.},
	number = {5},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics},
	author = {Cohen, Michael A. and Grossberg, Stephen},
	year = {1983},
	keywords = {Neural networks, Mathematical model, neural nets, Trajectory, stability, Stability analysis, Equations, competitive neural networks, equilibrium set, global Lyapunov function, global pattern formation, parallel memory storage, Pattern formation, symmetric interactions, Symmetric matrices},
	pages = {815--826},
}

@article{hopfield_neurons_1984,
	title = {Neurons with graded response have collective computational properties like those of two-state neurons},
	volume = {81},
	abstract = {A model for a large network of "neurons" with a graded response (or sigmoid input-output relation) is studied. This deterministic system has collective properties in very close correspondence with the earlier stochastic model based on McCulloch - Pitts neurons. The content- addressable memory and other emergent collective properties of the original model also are present in the graded response model. The idea that such collective properties are used in biological systems is given added credence by the continued presence of such properties for more nearly biological "neurons." Collective analog electrical circuits of the kind described will certainly function. The collective states of the two models have a simple correspondence. The original model will continue to be useful for simulations, because its connection to graded response systems is established. Equations that include the effect of action potentials in the graded response system are also developed.},
	language = {en},
	number = {10},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Hopfield, J. J.},
	year = {1984},
	pages = {3088--3092},
}

@article{abarbanel_machine_2018,
	title = {Machine learning: {Deepest} learning as statistical data assimilation problems},
	volume = {30},
	number = {8},
	journal = {Neural computation},
	author = {Abarbanel, Henry D. I. and Rozdeba, Paul J. and Shirman, Sasha},
	year = {2018},
	pages = {2025--2055},
}

@inproceedings{fathony_multiplicative_2020,
	title = {Multiplicative filter networks},
	abstract = {Although deep networks are typically used to approximate functions over high dimensional inputs, recent work has increased interest in neural networks as function approximators for...},
	language = {en},
	author = {Fathony, Rizal and Sahu, Anit Kumar and Willmott, Devin and Kolter, J. Zico},
	month = sep,
	year = {2020},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/Z2AH5MGZ/Fathony et al. - 2020 - Multiplicative Filter Networks.pdf:application/pdf;Snapshot:/Users/alexander/Zotero/storage/23NK8KEY/forum.html:text/html},
}

@article{pozzi_attention-gated_2020,
	title = {Attention-gated brain propagation: how the brain can implement reward-based error backpropagation},
	journal = {Advances in Neural Information Processing Systems},
	author = {Pozzi, Isabella and Bohte, Sander and Roelfsema, Pieter},
	year = {2020},
	pages = {2516--2526},
}

@article{bastos_canonical_2012,
	title = {Canonical microcircuits for predictive coding},
	volume = {76},
	number = {4},
	journal = {Neuron},
	author = {Bastos, Andre M. and Usrey, W. Martin and Adams, Rick A. and Mangun, George R. and Fries, Pascal and Friston, Karl J.},
	year = {2012},
	pages = {695--711},
}

@article{mikulasch_dendritic_2022,
	title = {Dendritic predictive coding: {A} theory of cortical computation with spiking neurons},
	journal = {arXiv preprint arXiv:2205.05303},
	author = {Mikulasch, Fabian A. and Rudelt, Lucas and Wibral, Michael and Priesemann, Viola},
	year = {2022},
}

@article{keller_predictive_2018,
	title = {Predictive processing: a canonical cortical computation},
	volume = {100},
	number = {2},
	journal = {Neuron},
	author = {Keller, Georg B. and Mrsic-Flogel, Thomas D.},
	year = {2018},
	keywords = {canonical microcircuit, cortex, predictive coding, predictive processing, sensory processing},
	pages = {424--435},
}

@article{crick_recent_1989,
	title = {The recent excitement about neural networks},
	volume = {337},
	abstract = {The recent excitement about neural networks},
	journal = {Nature},
	author = {Crick, Francis},
	year = {1989},
	pages = {129--132},
}

@article{payeur_burst-dependent_2021,
	title = {Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits},
	volume = {24},
	number = {7},
	journal = {Nature Neuroscience},
	author = {Payeur, Alexandre and Guerguiev, Jordan and Zenke, Friedemann and Richards, Blake A. and Naud, Richard},
	year = {2021},
	pages = {1010--1019},
}

@article{kording_supervised_2001,
	title = {Supervised and unsupervised learning with two sites of synaptic integration},
	volume = {11},
	number = {3},
	journal = {Journal of Computational Neuroscience},
	author = {Körding, Konrad P and König, Peter},
	year = {2001},
	pages = {207--215},
}

@article{bellec_solution_2020,
	title = {A solution to the learning dilemma for recurrent networks of spiking neurons},
	volume = {11},
	abstract = {Abstract
            Recurrently connected networks of spiking neurons underlie the astounding information processing capabilities of the brain. Yet in spite of extensive research, how they can learn through synaptic plasticity to carry out complex network computations remains unclear. We argue that two pieces of this puzzle were provided by experimental data from neuroscience. A mathematical result tells us how these pieces need to be combined to enable biologically plausible online network learning through gradient descent, in particular deep reinforcement learning. This learning method–called e-prop–approaches the performance of backpropagation through time (BPTT), the best-known method for training recurrent neural networks in machine learning. In addition, it suggests a method for powerful on-chip learning in energy-efficient spike-based hardware for artificial intelligence.},
	number = {1},
	journal = {Nature Communications},
	author = {Bellec, Guillaume and Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
	year = {2020},
	pages = {3625},
	file = {Full Text:/Users/alexander/Zotero/storage/4F8MV8MW/Bellec et al. - 2020 - A solution to the learning dilemma for recurrent n.pdf:application/pdf},
}

@article{roelfsema_control_2018,
	title = {Control of synaptic plasticity in deep cortical networks},
	volume = {19},
	abstract = {Humans and many other animals have an enormous capacity to learn about sensory stimuli and to master new skills. However, many of the mechanisms that enable us to learn remain to be understood. One of the greatest challenges of systems neuroscience is to explain how synaptic connections change to support maximally adaptive behaviour. Here, we provide an overview of factors that determine the change in the strength of synapses, with a focus on synaptic plasticity in sensory cortices. We review the influence of neuromodulators and feedback connections in synaptic plasticity and suggest a specific framework in which these factors can interact to improve the functioning of the entire network.},
	number = {3},
	journal = {Nature Reviews Neuroscience},
	author = {Roelfsema, Pieter R. and Holtmaat, Anthony},
	year = {2018},
	pages = {166--180},
}

@inproceedings{lee_difference_2015,
	title = {Difference target propagation},
	booktitle = {Joint {European} {Conference} on {Machine} {Learning} and {Knowledge} {Discovery} in {Databases}},
	author = {Lee, Dong-Hyun and Zhang, Saizheng and Fischer, Asja and Bengio, Yoshua},
	year = {2015},
}

@article{guerguiev_towards_2017,
	title = {Towards deep learning with segregated dendrites},
	volume = {6},
	abstract = {Deep learning has led to significant advances in artificial intelligence, in part, by adopting strategies motivated by neurophysiology. However, it is unclear whether deep learning could occur in the real brain. Here, we show that a deep learning algorithm that utilizes multi-compartment neurons might help us to understand how the neocortex optimizes cost functions. Like neocortical pyramidal neurons, neurons in our model receive sensory information and higher-order feedback in electrotonically segregated compartments. Thanks to this segregation, neurons in different layers of the network can coordinate synaptic weight updates. As a result, the network learns to categorize images better than a single layer network. Furthermore, we show that our algorithm takes advantage of multilayer architectures to identify useful higher-order representations—the hallmark of deep learning. This work demonstrates that deep learning can be achieved using segregated dendritic compartments, which may help to explain the morphology of neocortical pyramidal neurons.},
	journal = {eLife},
	author = {Guerguiev, Jordan and Lillicrap, Timothy P. and Richards, Blake A.},
	year = {2017},
	keywords = {deep learning, feedback alignment, neocortex, credit assignment, dendritic morphology, target propagation},
	pages = {e22901},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/UHCHX2WU/Guerguiev et al. - 2017 - Towards deep learning with segregated dendrites.pdf:application/pdf},
}

@article{wang_meta-learning_2021,
	title = {Meta-learning in natural and artificial intelligence},
	volume = {38},
	journal = {Current Opinion in Behavioral Sciences},
	author = {Wang, Jane X.},
	year = {2021},
	pages = {90--95},
	file = {ScienceDirect Full Text PDF:/Users/alexander/Zotero/storage/BGEXU323/Wang - 2021 - Meta-learning in natural and artificial intelligen.pdf:application/pdf;ScienceDirect Snapshot:/Users/alexander/Zotero/storage/DHRHU36F/S2352154621000024.html:text/html},
}

@article{wang_prefrontal_2018,
	title = {Prefrontal cortex as a meta-reinforcement learning system},
	volume = {21},
	number = {6},
	journal = {Nature Neuroscience},
	author = {Wang, Jane X. and Kurth-Nelson, Zeb and Kumaran, Dharshan and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Hassabis, Demis and Botvinick, Matthew},
	year = {2018},
	pages = {860--868},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/4P4CWGMT/Wang et al. - 2018 - Prefrontal cortex as a meta-reinforcement learning.pdf:application/pdf;Snapshot:/Users/alexander/Zotero/storage/X4AXL5V8/signup.html:text/html},
}

@book{thrun_learning_1998,
	title = {Learning to learn},
	publisher = {Springer US},
	author = {Thrun, Sebastian and Pratt, Lorien},
	year = {1998},
}

@article{behrens_what_2018,
	title = {What is a cognitive map? {Organizing} knowledge for flexible behavior},
	volume = {100},
	number = {2},
	journal = {Neuron},
	author = {Behrens, Timothy E. J. and Muller, Timothy H and Whittington, James C. R. and Mark, Shirley and Baram, Alon B. and Stachenfeld, Kimberly L and Kurth-Nelson, Zeb},
	year = {2018},
	pages = {490--509},
}

@inproceedings{bartunov_assessing_2018,
	title = {Assessing the scalability of biologically-motivated deep learning algorithms and architectures},
	booktitle = {Advances in neural information processing systems},
	author = {Bartunov, Sergey and Santoro, Adam and Richards, Blake and Marris, Luke and Hinton, Geoffrey E. and Lillicrap, Timothy P.},
	year = {2018},
}

@article{tsuda_modeling_2020,
	title = {A modeling framework for adaptive lifelong learning with transfer and savings through gating in the prefrontal cortex},
	volume = {117},
	number = {47},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Tsuda, Ben and Tye, Kay M and Siegelmann, Hava T and Sejnowski, Terrence J},
	year = {2020},
	note = {Publisher: National Acad Sciences},
	pages = {29872--29882},
}

@article{baldi_contrastive_1991,
	title = {Contrastive learning and neural oscillations},
	volume = {3},
	number = {4},
	journal = {Neural Computation},
	author = {Baldi, Pierre and Pineda, Fernando},
	year = {1991},
	pages = {526--545},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/TSTYEEF2/Baldi and Pineda - 1991 - Contrastive Learning and Neural Oscillations.pdf:application/pdf;Snapshot:/Users/alexander/Zotero/storage/W85ZTXQU/Contrastive-Learning-and-Neural-Oscillations.html:text/html},
}

@inproceedings{sitzmann_implicit_2020,
	title = {Implicit neural representations with periodic activation functions},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Sitzmann, Vincent and Martel, Julien and Bergman, Alexander and Lindell, David and Wetzstein, Gordon},
	year = {2020},
}

@inproceedings{he_deep_2016,
	title = {Deep residual learning for image recognition},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/K9EEYPHC/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf;Snapshot:/Users/alexander/Zotero/storage/422F97FT/He_Deep_Residual_Learning_CVPR_2016_paper.html:text/html},
}

@article{bogacz_tutorial_2017,
	title = {A tutorial on the free-energy framework for modelling perception and learning},
	volume = {76},
	abstract = {This paper provides an easy to follow tutorial on the free-energy framework for modelling perception developed by Friston, which extends the predictive coding model of Rao and Ballard. These models assume that the sensory cortex infers the most likely values of attributes or features of sensory stimuli from the noisy inputs encoding the stimuli. Remarkably, these models describe how this inference could be implemented in a network of very simple computational elements, suggesting that this inference could be performed by biological networks of neurons. Furthermore, learning about the parameters describing the features and their uncertainty is implemented in these models by simple rules of synaptic plasticity based on Hebbian learning. This tutorial introduces the free-energy framework using very simple examples, and provides step-by-step derivations of the model. It also discusses in more detail how the model could be implemented in biological neural circuits. In particular, it presents an extended version of the model in which the neurons only sum their inputs, and synaptic plasticity only depends on activity of pre-synaptic and post-synaptic neurons.},
	journal = {Journal of Mathematical Psychology},
	author = {Bogacz, Rafal},
	year = {2017},
	pages = {198--211},
	file = {Bogacz_2017_A tutorial on the free-energy framework for modelling perception and learning.pdf:/Users/alexander/Zotero/storage/MZXWBLB9/Bogacz_2017_A tutorial on the free-energy framework for modelling perception and learning.pdf:application/pdf;ScienceDirect Snapshot:/Users/alexander/Zotero/storage/WX9CBZLZ/S0022249615000759.html:text/html},
}

@article{zucchet_beyond_2022,
	title = {Beyond backpropagation: implicit gradients for bilevel optimization},
	abstract = {This paper reviews gradient-based techniques to solve bilevel optimization problems. Bilevel optimization is a general way to frame the learning of systems that are implicitly deﬁned through a quantity that they minimize. This characterization can be applied to neural networks, optimizers, algorithmic solvers and even physical systems, and allows for greater modeling ﬂexibility compared to an explicit deﬁnition of such systems. Here we focus on gradient-based approaches that solve such problems. We distinguish them in two categories: those rooted in implicit diﬀerentiation, and those that leverage the equilibrium propagation theorem. We present the mathematical foundations that are behind such methods, introduce the gradient-estimation algorithms in detail and compare the competitive advantages of the diﬀerent approaches.},
	journal = {arXiv preprint arXiv:2205.03076},
	author = {Zucchet, Nicolas and Sacramento, João},
	year = {2022},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {Zucchet and Sacramento - 2022 - Beyond backpropagation implicit gradients for bil.pdf:/Users/alexander/Zotero/storage/Z54RBH6W/Zucchet and Sacramento - 2022 - Beyond backpropagation implicit gradients for bil.pdf:application/pdf},
}

@incollection{neal_view_1998,
	title = {A view of the {EM} algorithm that justifies incremental, sparse, and other variants},
	abstract = {The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.},
	booktitle = {Learning in {Graphical} {Models}},
	publisher = {Springer Netherlands},
	author = {Neal, Radford M. and Hinton, Geoffrey E.},
	year = {1998},
	keywords = {Data Item, Incremental Algorithm, Incremental Variant, Standard Algorithm, Unobserved Variable},
	pages = {355--368},
}

@article{tzikas_variational_2008,
	title = {The variational approximation for {Bayesian} inference},
	volume = {25},
	abstract = {The influence of this Thomas Bayes' work was immense. It was from here that "Bayesian" ideas first spread through the mathematical world, as Bayes's own article was ignored until 1780 and played no important role in scientific debate until the 20th century. It was also this article of Laplace's that introduced the mathematical techniques for the asymptotic analysis of posterior distributions that are still employed today. And it was here that the earliest example of optimum estimation can be found, the derivation and characterization of an estimator that minimized a particular measure of posterior expected loss. After more than two centuries, we mathematicians, statisticians cannot only recognize our roots in this masterpiece of our science, we can still learn from it.},
	number = {6},
	journal = {IEEE Signal Processing Magazine},
	author = {Tzikas, Dimitris G. and Likas, Aristidis C. and Galatsanos, Nikolaos P.},
	year = {2008},
	keywords = {Approximation algorithms, Autobiographies, Bayesian methods, Inference algorithms, Iterative algorithms, Life estimation, Loss measurement, Maximum likelihood estimation, Particle measurements, Signal processing algorithms},
	pages = {131--146},
	file = {IEEE Xplore Abstract Record:/Users/alexander/Zotero/storage/S3SWTHYD/4644060.html:text/html},
}

@article{shepherd_dendrodendritic_2009,
	title = {Dendrodendritic synapses: past, present, and future},
	volume = {1170},
	number = {1},
	journal = {Annals of the New York Academy of Sciences},
	author = {Shepherd, Gordon M.},
	year = {2009},
	pages = {215--223},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/8NA9ZV2K/Shepherd - 2009 - Symposium Overview and Historical Perspective.pdf:application/pdf;Snapshot:/Users/alexander/Zotero/storage/4GSBKCV8/j.1749-6632.2009.03937.html:text/html},
}

@article{magee_synaptic_2020,
	title = {Synaptic plasticity forms and functions},
	volume = {43},
	journal = {Annual Review of Neuroscience},
	author = {Magee, Jeffrey C. and Grienberger, Christine},
	year = {2020},
	pages = {95--117},
}

@article{hennequin_inhibitory_2017,
	title = {Inhibitory plasticity: balance, control, and codependence},
	volume = {40},
	journal = {Annual Review of Neuroscience},
	author = {Hennequin, Guillaume and Agnes, Everton J. and Vogels, Tim P.},
	year = {2017},
	pages = {557--579},
}

@article{goldman_neural_2010,
	title = {Neural integrator models},
	journal = {Encyclopedia of Neuroscience},
	author = {Goldman, Mark S. and Compte, Albert and Wang, Xiao-Jing},
	year = {2010},
	pages = {165--178},
}

@article{friston_free_2006,
	title = {A free energy principle for the brain},
	volume = {100},
	abstract = {By formulating Helmholtz’s ideas about perception, in terms of modern-day theories, one arrives at a model of perceptual inference and learning that can explain a remarkable range of neurobiological facts: using constructs from statistical physics, the problems of inferring the causes of sensory input and learning the causal structure of their generation can be resolved using exactly the same principles. Furthermore, inference and learning can proceed in a biologically plausible fashion. The ensuing scheme rests on Empirical Bayes and hierarchical models of how sensory input is caused. The use of hierarchical models enables the brain to construct prior expectations in a dynamic and context-sensitive fashion. This scheme provides a principled way to understand many aspects of cortical organisation and responses.},
	number = {1-3},
	journal = {Journal of Physiology-Paris},
	author = {Friston, Karl and Kilner, James and Harrison, Lee},
	year = {2006},
	pages = {70--87},
	file = {Friston et al. - 2006 - A free energy principle for the brain.pdf:/Users/nicolas/Zotero/storage/S3INXKR5/Friston et al. - 2006 - A free energy principle for the brain.pdf:application/pdf},
}

@inproceedings{goutte_adaptive_1998,
	title = {Adaptive regularization of neural networks using conjugate gradient},
	abstract = {Recently we suggested a regularization scheme which iteratively adapts regularization parameters by minimizing validation error using simple gradient descent. In this contribution we present an improved algorithm based on the conjugate gradient technique. Numerical experiments with feed-forward neural networks successfully demonstrate improved generalization ability and lower computational cost.},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	author = {Goutte, C. and Larsen, J.},
	year = {1998},
	file = {Goutte et Larsen - 1998 - Adaptive regularization of neural networks using c.pdf:/Users/alexander/Zotero/storage/NSTKQZPJ/Goutte et Larsen - 1998 - Adaptive regularization of neural networks using c.pdf:application/pdf},
}

@inproceedings{bourdoukan_enforcing_2015,
	title = {Enforcing balance allows local supervised learning in spiking recurrent networks},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/hash/3871bd64012152bfb53fdf04b401193f-Abstract.html},
	abstract = {To predict sensory inputs or control motor trajectories, the brain must constantly learn temporal dynamics based on error feedback. However, it remains unclear how such supervised learning is implemented in biological neural networks. Learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics. The most commonly used learning rules, such as temporal back-propagation, are not local and thus not biologically plausible. Furthermore, reproducing the Poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition. Such balance is easily destroyed during learning. Using a top-down approach, we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input. The network uses two types of recurrent connections: fast and slow. The fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule. The slow connections are trained to minimize the error feedback using a current-based Hebbian learning rule. Importantly, the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron, in turn resulting in a local learning rule for the slow connections. This demonstrates that spiking networks can learn complex dynamics using purely local learning rules, using E/I balance as the key rather than an additional constraint. The resulting network implements a given function within the predictive coding scheme, with minimal dimensions and activity.},
	urldate = {2022-05-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bourdoukan, Ralph and Denève, Sophie},
	year = {2015},
	file = {Bourdoukan_Denève_2015_Enforcing balance allows local supervised learning in spiking recurrent networks.pdf:/Users/alexander/Zotero/storage/C7TTRB8D/Bourdoukan_Denève_2015_Enforcing balance allows local supervised learning in spiking recurrent networks.pdf:application/pdf},
}

@inproceedings{vinyals_matching_2016,
	title = {Matching networks for one shot learning},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy P. and Kavukcuoglu, Koray and Wierstra, Daan},
	year = {2016},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/alexander/Zotero/storage/HAXLUTAV/Vinyals et al. - 2017 - Matching Networks for One Shot Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/alexander/Zotero/storage/LVMDL7KX/1606.html:text/html},
}

@book{sarkka_applied_2019,
	title = {Applied stochastic differential equations},
	publisher = {Cambridge University Press},
	author = {Särkkä, Simo and Solin, Arno},
	year = {2019},
	file = {Särkkä and Solin - 2019 - Applied Stochastic Differential Equations.pdf:/Users/nicolas/Zotero/storage/9QGE4WRA/Särkkä and Solin - 2019 - Applied Stochastic Differential Equations.pdf:application/pdf},
}

@article{lu_inverses_2002,
	title = {Inverses of 2 × 2 block matrices},
	volume = {43},
	abstract = {In this paper, the authors give explicit inverse formulae for 2 × 2 block matrices with three different partitions. Then these results are applied to obtain inverses of block triangular matrices and various structured matrices such as Hamiltonian, per-Hermitian, and centro-Hermitian matrices.},
	number = {1},
	journal = {Computers \& Mathematics with Applications},
	author = {Lu, Tzon-Tzer and Shiou, Sheng-Hua},
	year = {2002},
	keywords = {2 × 2 block matrix, Inverse matrix, Structured matrix},
	pages = {119--129},
	file = {ScienceDirect Snapshot:/Users/nicolas/Zotero/storage/XLJ39IU8/S0898122101002784.html:text/html},
}

@inproceedings{kingma_adam_2015,
	title = {Adam: {A} method for stochastic optimization},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	year = {2015},
	file = {Snapshot:/Users/nicolas/Zotero/storage/VN7CJUWL/search.html:text/html},
}

