\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu \& Hazan(2016)Allen-Zhu and Hazan]{allen2016variance}
Allen-Zhu, Zeyuan and Hazan, Elad.
\newblock Variance reduction for faster non-convex optimization.
\newblock \emph{arXiv preprint arXiv:1603.05643}, 2016.

\bibitem[Andoni et~al.(2014)Andoni, Panigrahy, Valiant, and
  Zhang]{valiant2014learning}
Andoni, Alexandr, Panigrahy, Rina, Valiant, Gregory, and Zhang, Li.
\newblock Learning polynomials with neural networks.
\newblock In \emph{Proceedings of the 31th International Conference on Machine
  Learning}, pp.\  1908--1916, 2014.

\bibitem[Auer et~al.(1996)Auer, Herbster, Warmuth,
  et~al.]{auer1996exponentially}
Auer, Peter, Herbster, Mark, Warmuth, Manfred~K, et~al.
\newblock Exponentially many local minima for single neurons.
\newblock \emph{Advances in neural information processing systems}, pp.\
  316--322, 1996.

\bibitem[Baum(1990)]{baum1990polynomial}
Baum, Eric~B.
\newblock A polynomial time algorithm that learns two hidden unit nets.
\newblock \emph{Neural Computation}, 2\penalty0 (4):\penalty0 510--522, 1990.

\bibitem[Blum \& Rivest(1993)Blum and Rivest]{blum1993training}
Blum, Avrim~L and Rivest, Ronald~L.
\newblock Training a 3-node neural network is np-complete.
\newblock In \emph{Machine learning: From theory to applications}, pp.\  9--28.
  Springer, 1993.

\bibitem[Brown et~al.(2001)Brown, Cai, and DasGupta]{brown2001interval}
Brown, Lawrence~D, Cai, T~Tony, and DasGupta, Anirban.
\newblock Interval estimation for a binomial proportion.
\newblock \emph{Statistical science}, pp.\  101--117, 2001.

\bibitem[Cho \& Saul(2009)Cho and Saul]{cho2009kernel}
Cho, Youngmin and Saul, Lawrence~K.
\newblock Kernel methods for deep learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  342--350, 2009.

\bibitem[Choromanska et~al.(2015)Choromanska, Henaff, Mathieu, Arous, and
  LeCun]{choromanska2015loss}
Choromanska, Anna, Henaff, Mikael, Mathieu, Michael, Arous, G{\'e}rard~Ben, and
  LeCun, Yann.
\newblock The loss surfaces of multilayer networks.
\newblock In \emph{AISTATS}, 2015.

\bibitem[Daniely et~al.(2014)Daniely, Linial, and
  Shalev-Shwartz]{daniely2014average}
Daniely, Amit, Linial, Nati, and Shalev-Shwartz, Shai.
\newblock From average case complexity to improper learning complexity.
\newblock In \emph{Proceedings of the 46th Annual ACM Symposium on Theory of
  Computing}, pp.\  441--448. ACM, 2014.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, John, Hazan, Elad, and Singer, Yoram.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Garey \& Johnson(1990)Garey and Johnson]{Garey:1990}
Garey, Michael~R. and Johnson, David~S.
\newblock \emph{Computers and Intractability; A Guide to the Theory of
  NP-Completeness}.
\newblock W. H. Freeman \& Co., New York, NY, USA, 1990.
\newblock ISBN 0716710455.

\bibitem[Ge et~al.(2015)Ge, Huang, Jin, and Yuan]{ge2015escaping}
Ge, Rong, Huang, Furong, Jin, Chi, and Yuan, Yang.
\newblock Escaping from saddle points-online stochastic gradient for tensor
  decomposition.
\newblock In \emph{COLT}, pp.\  797--842, 2015.

\bibitem[Ge et~al.(2016)Ge, Lee, and Ma]{ge2016matrix}
Ge, Rong, Lee, Jason~D, and Ma, Tengyu.
\newblock Matrix completion has no spurious local minimum.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2973--2981, 2016.

\bibitem[Haeffele \& Vidal(2015)Haeffele and Vidal]{haeffele2015global}
Haeffele, Benjamin~D and Vidal, Ren{\'e}.
\newblock Global optimality in tensor factorization, deep learning, and beyond.
\newblock \emph{arXiv preprint arXiv:1506.07540}, 2015.

\bibitem[Hardt \& Ma(2016)Hardt and Ma]{hardt2016identity}
Hardt, Moritz and Ma, Tengyu.
\newblock Identity matters in deep learning.
\newblock \emph{arXiv preprint arXiv:1611.04231}, 2016.

\bibitem[Hardt et~al.(2016)Hardt, Ma, and Recht]{hardt2016gradient}
Hardt, Moritz, Ma, Tengyu, and Recht, Benjamin.
\newblock Gradient descent learns linear dynamical systems.
\newblock \emph{arXiv preprint arXiv:1609.05191}, 2016.

\bibitem[Hinton et~al.(2012)Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior,
  Vanhoucke, Nguyen, Sainath, et~al.]{hinton2012deep}
Hinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George~E, Mohamed, Abdel-rahman,
  Jaitly, Navdeep, Senior, Andrew, Vanhoucke, Vincent, Nguyen, Patrick,
  Sainath, Tara~N, et~al.
\newblock Deep neural networks for acoustic modeling in speech recognition: The
  shared views of four research groups.
\newblock \emph{IEEE Signal Processing Magazine}, 29\penalty0 (6):\penalty0
  82--97, 2012.

\bibitem[Janzamin et~al.(2015)Janzamin, Sedghi, and
  Anandkumar]{janzamin2015beating}
Janzamin, Majid, Sedghi, Hanie, and Anandkumar, Anima.
\newblock Beating the perils of non-convexity: Guaranteed training of neural
  networks using tensor methods.
\newblock \emph{arXiv preprint arXiv:1506.08473}, 2015.

\bibitem[Kakade et~al.(2011)Kakade, Kanade, Shamir, and Kalai]{NIPS2011_4429}
Kakade, Sham~M, Kanade, Varun, Shamir, Ohad, and Kalai, Adam.
\newblock Efficient learning of generalized linear and single index models with
  isotonic regression.
\newblock In \emph{Advances in Neural Information Processing Systems 24}, pp.\
  927--935. 2011.

\bibitem[Kawaguchi(2016)]{kawaguchi2016deep}
Kawaguchi, Kenji.
\newblock Deep learning without poor local minima.
\newblock In \emph{Advances In Neural Information Processing Systems}, pp.\
  586--594, 2016.

\bibitem[Klivans(2008)]{klivans2008cryptographic}
Klivans, Adam.
\newblock Cryptographic hardness of learning.
\newblock In \emph{Encyclopedia of Algorithms}, pp.\  210--212. Springer, 2008.

\bibitem[Klivans et~al.(2009)Klivans, Long, and Tang]{klivans2009baum}
Klivans, Adam~R, Long, Philip~M, and Tang, Alex~K.
\newblock Baumâ€™s algorithm learns intersections of halfspaces with respect to
  log-concave distributions.
\newblock In \emph{Approximation, Randomization, and Combinatorial
  Optimization. Algorithms and Techniques}, pp.\  588--600. Springer, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1097--1105, 2012.

\bibitem[Lee et~al.(2016)Lee, Simchowitz, Jordan, and Recht]{LeeSJR16}
Lee, Jason~D., Simchowitz, Max, Jordan, Michael~I., and Recht, Benjamin.
\newblock Gradient descent only converges to minimizers.
\newblock In \emph{Proceedings of the 29th Conference on Learning Theory}, pp.\
   1246--1257, 2016.

\bibitem[Lin et~al.(2013)Lin, Chen, and Yan]{lin2013network}
Lin, Min, Chen, Qiang, and Yan, Shuicheng.
\newblock Network in network.
\newblock \emph{arXiv preprint arXiv:1312.4400}, 2013.

\bibitem[Livni et~al.(2014)Livni, Shalev-Shwartz, and
  Shamir]{livni2014computational}
Livni, Roi, Shalev-Shwartz, Shai, and Shamir, Ohad.
\newblock On the computational efficiency of training neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  855--863, 2014.

\bibitem[Mei et~al.(2016)Mei, Bai, and Montanari]{mei2016landscape}
Mei, Song, Bai, Yu, and Montanari, Andrea.
\newblock The landscape of empirical risk for non-convex losses.
\newblock \emph{arXiv preprint arXiv:1607.06534}, 2016.

\bibitem[Milletari et~al.(2016)Milletari, Navab, and Ahmadi]{milletari2016v}
Milletari, Fausto, Navab, Nassir, and Ahmadi, Seyed-Ahmad.
\newblock V-net: Fully convolutional neural networks for volumetric medical
  image segmentation.
\newblock In \emph{3D Vision (3DV), 2016 Fourth International Conference on},
  pp.\  565--571. IEEE, 2016.

\bibitem[Nesterov(2004)]{nesterov2004introductory}
Nesterov, Yurii.
\newblock Introductory lectures on convex optimization.
\newblock pp.\  22--29, 2004.

\bibitem[Safran \& Shamir(2016)Safran and Shamir]{safran2015quality}
Safran, Itay and Shamir, Ohad.
\newblock On the quality of the initial basin in overspecified neural networks.
\newblock In \emph{Proceedings of the 33nd International Conference on Machine
  Learning}, pp.\  774--782, 2016.

\bibitem[Shamir(2016)]{shamir2016distribution}
Shamir, Ohad.
\newblock Distribution-specific hardness of learning neural networks.
\newblock \emph{arXiv preprint arXiv:1609.01037}, 2016.

\bibitem[Soudry \& Carmon(2016)Soudry and Carmon]{soudry2016no}
Soudry, Daniel and Carmon, Yair.
\newblock No bad local minima: Data independent training error guarantees for
  multilayer neural networks.
\newblock \emph{arXiv preprint arXiv:1605.08361}, 2016.

\bibitem[Wu et~al.(2016)Wu, Schuster, Chen, Le, Norouzi, Macherey, Krikun, Cao,
  Gao, Macherey, Klingner, Shah, Johnson, Liu, Kaiser, Gouws, Kato, Kudo,
  Kazawa, Stevens, Kurian, Patil, Wang, Young, Smith, Riesa, Rudnick, Vinyals,
  Corrado, Hughes, and Dean]{WuSCLNMKCGMKSJL16}
Wu, Yonghui, Schuster, Mike, Chen, Zhifeng, Le, Quoc~V., Norouzi, Mohammad,
  Macherey, Wolfgang, Krikun, Maxim, Cao, Yuan, Gao, Qin, Macherey, Klaus,
  Klingner, Jeff, Shah, Apurva, Johnson, Melvin, Liu, Xiaobing, Kaiser, Lukasz,
  Gouws, Stephan, Kato, Yoshikiyo, Kudo, Taku, Kazawa, Hideto, Stevens, Keith,
  Kurian, George, Patil, Nishant, Wang, Wei, Young, Cliff, Smith, Jason, Riesa,
  Jason, Rudnick, Alex, Vinyals, Oriol, Corrado, Greg, Hughes, Macduff, and
  Dean, Jeffrey.
\newblock Google's neural machine translation system: Bridging the gap between
  human and machine translation.
\newblock \emph{CoRR}, abs/1609.08144, 2016.

\bibitem[Xu et~al.(2016)Xu, Hsu, and Maleki]{xu2016global}
Xu, Ji, Hsu, Daniel~J, and Maleki, Arian.
\newblock Global analysis of expectation maximization for mixtures of two
  gaussians.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2676--2684, 2016.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{ZhangBHRV16}
Zhang, Chiyuan, Bengio, Samy, Hardt, Moritz, Recht, Benjamin, and Vinyals,
  Oriol.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{CoRR}, abs/1611.03530, 2016.
\newblock URL \url{http://arxiv.org/abs/1611.03530}.

\bibitem[Zhang et~al.(2017)Zhang, Panigrahy, Sachdeva, and
  Rahimi]{zhang2017electron}
Zhang, Qiuyi, Panigrahy, Rina, Sachdeva, Sushant, and Rahimi, Ali.
\newblock Electron-proton dynamics in deep learning.
\newblock \emph{arXiv preprint arXiv:1702.00458}, 2017.

\end{thebibliography}
