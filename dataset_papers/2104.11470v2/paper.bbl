\begin{thebibliography}{10}

\bibitem{Al-Dujaili2020Sign}
Abdullah Al-Dujaili and Una-May O'Reilly.
\newblock Sign bits are all you need for black-box attacks.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{andriushchenko2020square}
Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein.
\newblock Square attack: a query-efficient black-box adversarial attack via
  random search.
\newblock In {\em European Conference on Computer Vision}, pages 484--501.
  Springer, 2020.

\bibitem{athalye2018obfuscated}
Anish Athalye, Nicholas Carlini, and David Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock In {\em International Conference on Machine Learning}, pages
  274--283, 2018.

\bibitem{athalye2018synthesizing}
Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok.
\newblock Synthesizing robust adversarial examples.
\newblock In {\em International conference on machine learning}, pages
  284--293. PMLR, 2018.

\bibitem{byun2021small}
Junyoung Byun, Hyojun Go, and Changick Kim.
\newblock Small input noise is enough to defend against query-based black-box
  attacks, 2021.

\bibitem{carlini2019evaluating}
Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas
  Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey
  Kurakin.
\newblock On evaluating adversarial robustness.
\newblock {\em arXiv preprint arXiv:1902.06705}, 2019.

\bibitem{chen2020hopskipjumpattack}
Jianbo Chen, Michael~I Jordan, and Martin~J Wainwright.
\newblock Hopskipjumpattack: A query-efficient decision-based attack.
\newblock In {\em 2020 ieee symposium on security and privacy (sp)}, pages
  1277--1294. IEEE, 2020.

\bibitem{chen2020stateful}
Steven Chen, Nicholas Carlini, and David Wagner.
\newblock Stateful detection of black-box adversarial attacks.
\newblock In {\em Proceedings of the 1st ACM Workshop on Security and Privacy
  on Artificial Intelligence}, pages 30--39, 2020.

\bibitem{chen2020boosting}
Weilun Chen, Zhaoxiang Zhang, Xiaolin Hu, and Baoyuan Wu.
\newblock Boosting decision-based black-box adversarial attacks with random
  sign flip.
\newblock In {\em European Conference on Computer Vision}, pages 276--293.
  Springer, 2020.

\bibitem{cheng2018queryefficient}
Minhao Cheng, Thong Le, Pin-Yu Chen, Huan Zhang, JinFeng Yi, and Cho-Jui Hsieh.
\newblock Query-efficient hard-label black-box attack: An optimization-based
  approach.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{cheng2019sign}
Minhao Cheng, Simranjit Singh, Patrick Chen, Pin-Yu Chen, Sijia Liu, and
  Cho-Jui Hsieh.
\newblock Sign-opt: A query-efficient hard-label adversarial attack.
\newblock {\em arXiv preprint arXiv:1909.10773}, 2019.

\bibitem{Cheng2020Sign-OPT}
Minhao Cheng, Simranjit Singh, Patrick~H. Chen, Pin-Yu Chen, Sijia Liu, and
  Cho-Jui Hsieh.
\newblock Sign-opt: A query-efficient hard-label adversarial attack.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{cohen2019certified}
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter.
\newblock Certified adversarial robustness via randomized smoothing.
\newblock In {\em International Conference on Machine Learning}, pages
  1310--1320, 2019.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. IEEE, 2009.

\bibitem{dong2020benchmarking}
Yinpeng Dong, Qi-An Fu, Xiao Yang, Tianyu Pang, Hang Su, Zihao Xiao, and Jun
  Zhu.
\newblock Benchmarking adversarial robustness on image classification.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 321--331, 2020.

\bibitem{duchi2015optimal}
John~C Duchi, Michael~I Jordan, Martin~J Wainwright, and Andre Wibisono.
\newblock Optimal rates for zero-order convex optimization: The power of two
  function evaluations.
\newblock {\em IEEE Transactions on Information Theory}, 61(5):2788--2806,
  2015.

\bibitem{robustness}
Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris
  Tsipras.
\newblock Robustness (python library), 2019.

\bibitem{fan2020sparse}
Yanbo Fan, Baoyuan Wu, Tuanhui Li, Yong Zhang, Mingyang Li, Zhifeng Li, and
  Yujiu Yang.
\newblock Sparse adversarial attack via perturbation factorization.
\newblock In {\em Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part XXII 16}, pages 35--50.
  Springer, 2020.

\bibitem{feng2020boosting}
Yan Feng, Baoyuan Wu, Yanbo Fan, Li~Liu, Zhifeng Li, and Shutao Xia.
\newblock Boosting black-box attack with partially transferred conditional
  adversarial distribution, 2020.

\bibitem{GeirhosRMBWB19}
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix~A.
  Wichmann, and Wieland Brendel.
\newblock Imagenet-trained cnns are biased towards texture; increasing shape
  bias improves accuracy and robustness.
\newblock In {\em ICLR}, 2019.

\bibitem{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock {\em arXiv preprint arXiv:1412.6572}, 2014.

\bibitem{gowal2020uncovering}
Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli.
\newblock Uncovering the limits of adversarial training against norm-bounded
  adversarial examples.
\newblock {\em arXiv preprint arXiv:2010.03593}, 2020.

\bibitem{guo2019simple}
Chuan Guo, Jacob Gardner, Yurong You, Andrew~Gordon Wilson, and Kilian
  Weinberger.
\newblock Simple black-box adversarial attacks.
\newblock In {\em International Conference on Machine Learning}, pages
  2484--2493, 2019.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{he2019parametric}
Zhezhi He, Adnan~Siraj Rakin, and Deliang Fan.
\newblock Parametric noise injection: Trainable randomness to improve deep
  neural network robustness against adversarial attack.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 588--597, 2019.

\bibitem{ilyas2018black}
Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin.
\newblock Black-box adversarial attacks with limited queries and information.
\newblock In {\em International Conference on Machine Learning}, pages
  2137--2146, 2018.

\bibitem{ilyas2018prior}
Andrew Ilyas, Logan Engstrom, and Aleksander Madry.
\newblock Prior convictions: Black-box adversarial attacks with bandits and
  priors.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{Kurakin_2018}
Alexey Kurakin, Ian~J. Goodfellow, and Samy Bengio.
\newblock Adversarial examples in the physical world.
\newblock {\em Artificial Intelligence Safety and Security}, page 99â€“112, Jul
  2018.

\bibitem{lecuyer2019certified}
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman
  Jana.
\newblock Certified robustness to adversarial examples with differential
  privacy.
\newblock In {\em 2019 IEEE Symposium on Security and Privacy (SP)}, pages
  656--672. IEEE, 2019.

\bibitem{li2020blacklight}
Huiying Li, Shawn Shan, Emily Wenger, Jiayun Zhang, Haitao Zheng, and Ben~Y
  Zhao.
\newblock Blacklight: Defending black-box adversarial attacks on deep neural
  networks.
\newblock {\em arXiv preprint arXiv:2006.14042}, 2020.

\bibitem{liu2018signsgd}
Sijia Liu, Pin-Yu Chen, Xiangyi Chen, and Mingyi Hong.
\newblock signsgd via zeroth-order oracle.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{liu2018towards}
Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh.
\newblock Towards robust neural networks via random self-ensemble.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 369--385, 2018.

\bibitem{madry2018towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{moon2019parsimonious}
Seungyong Moon, Gaon An, and Hyun~Oh Song.
\newblock Parsimonious black-box adversarial attacks via efficient
  combinatorial optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  4636--4645. PMLR, 2019.

\bibitem{nesterov2017random}
Yurii Nesterov and Vladimir Spokoiny.
\newblock Random gradient-free minimization of convex functions.
\newblock {\em Foundations of Computational Mathematics}, 17(2):527--566, 2017.

\bibitem{pang2020advmind}
Ren Pang, Xinyang Zhang, Shouling Ji, Xiapu Luo, and Ting Wang.
\newblock Advmind: Inferring adversary intent of black-box attacks.
\newblock In {\em Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 1899--1907, 2020.

\bibitem{pang2019improving}
Tianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu.
\newblock Improving adversarial robustness via promoting ensemble diversity.
\newblock In {\em International Conference on Machine Learning}, pages
  4970--4979. PMLR, 2019.

\bibitem{rice2020overfitting}
Leslie Rice, Eric Wong, and Zico Kolter.
\newblock Overfitting in adversarially robust deep learning.
\newblock In {\em International Conference on Machine Learning}, pages
  8093--8104. PMLR, 2020.

\bibitem{rusak2020simple}
Evgenia Rusak, Lukas Schott, Roland~S Zimmermann, Julian Bitterwolf, Oliver
  Bringmann, Matthias Bethge, and Wieland Brendel.
\newblock A simple way to make neural networks robust against diverse image
  corruptions.
\newblock In {\em European Conference on Computer Vision}, pages 53--69.
  Springer, 2020.

\bibitem{salman2019provably}
Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien
  Bubeck, and Greg Yang.
\newblock Provably robust deep learning via adversarially trained smoothed
  classifiers.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  11292--11303, 2019.

\bibitem{salman2020denoised}
Hadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor, and J~Zico Kolter.
\newblock Denoised smoothing: A provable defense for pretrained classifiers.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{smith2017cyclical}
Leslie~N Smith.
\newblock Cyclical learning rates for training neural networks.
\newblock In {\em 2017 IEEE winter conference on applications of computer
  vision (WACV)}, pages 464--472. IEEE, 2017.

\bibitem{SokolicGSR17}
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel R.~D. Rodrigues.
\newblock Generalization error of invariant classifiers.
\newblock In Aarti Singh and Xiaojin~(Jerry) Zhu, editors, {\em AISTATS}, pages
  1094--1103, 2017.

\bibitem{stutz2020confidence}
David Stutz, Matthias Hein, and Bernt Schiele.
\newblock Confidence-calibrated adversarial training: Generalizing to unseen
  attacks.
\newblock In {\em International Conference on Machine Learning}, pages
  9155--9166. PMLR, 2020.

\bibitem{szegedy2016rethinking}
C~Szegedy, V~Vanhoucke, S~Ioffe, J~Shlens, and Z~Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em 2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 2818--2826, 2016.

\bibitem{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock {\em arXiv preprint arXiv:1312.6199}, 2013.

\bibitem{NEURIPS2020_11f38f8e}
Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry.
\newblock On adaptive attacks to adversarial example defenses.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 1631--1643. Curran Associates, Inc., 2020.

\bibitem{tramer2017ensemble}
Florian Tram{\`e}r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan
  Boneh, and Patrick McDaniel.
\newblock Ensemble adversarial training: Attacks and defenses.
\newblock {\em arXiv preprint arXiv:1705.07204}, 2017.

\bibitem{xie2018mitigating}
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille.
\newblock Mitigating adversarial effects through randomization.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{xie2019feature}
Cihang Xie, Yuxin Wu, Laurens van~der Maaten, Alan~L Yuille, and Kaiming He.
\newblock Feature denoising for improving adversarial robustness.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 501--509, 2019.

\bibitem{yang2020dverge}
Huanrui Yang, Jingyang Zhang, Hongliang Dong, Nathan Inkawhich, Andrew Gardner,
  Andrew Touchet, Wesley Wilkes, Heath Berry, and Hai Li.
\newblock Dverge: Diversifying vulnerabilities for enhanced robust generation
  of ensembles.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock {\em arXiv preprint arXiv:1605.07146}, 2016.

\end{thebibliography}
