\begin{thebibliography}{61}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Akyurek and Akyurek(2022)}]{Akyurek2022:GPT3AdditionScratchpadFormat}
Ekin Akyurek and Afra~Feyza Akyurek. 2022.
\newblock \href {https://lingo.csail.mit.edu//blog/arithmetic_gpt3} {Notes on
  teaching gpt-3 adding numbers}.

\bibitem[{Akyurek et~al.(2023)Akyurek, Schuurmans, Andreas, Ma, and
  Zhou}]{Ekin2023:InContetLearning}
Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2023.
\newblock \href {https://openreview.net/forum?id=0g0X4H8yN4I} {What learning
  algorithm is in-context learning? investigations with linear models}.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}.

\bibitem[{Allal et~al.(2023)Allal, Li, Kocetkov, Mou, Akiki, Ferrandis,
  Muennighoff, Mishra, Gu, Dey, Umapathi et~al.}]{Allal2023:SantaCoder}
Loubna~Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki,
  Carlos~Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan
  Dey, Logesh~Kumar Umapathi, et~al. 2023.
\newblock \href {https://arxiv.org/abs/2301.03988} {Santacoder: don't reach for
  the stars!}
\newblock \emph{ArXiv}, abs/2301.03988.

\bibitem[{Anil et~al.(2022)Anil, Wu, Andreassen, Lewkowycz, Misra, Ramasesh,
  Slone, Gur-Ari, Dyer, and Neyshabur}]{Anil2022:LengthFailure}
Cem Anil, Yuhuai Wu, Anders~Johan Andreassen, Aitor Lewkowycz, Vedant Misra,
  Vinay~Venkatesh Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam
  Neyshabur. 2022.
\newblock \href {https://openreview.net/forum?id=zSkYVeX7bC4} {Exploring length
  generalization in large language models}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Ba et~al.(2016)Ba, Kiros, and Hinton}]{Ba2016:LayerNorm}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E. Hinton. 2016.
\newblock \href {https://arxiv.org/abs/1607.06450} {Layer normalization}.
\newblock \emph{ArXiv}, abs/1607.06450.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei}]{Brown2020:GPT3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei. 2020.
\newblock \href
  {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}
  {Language models are few-shot learners}.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}.

\bibitem[{Bueno et~al.(2022)Bueno, Gemmell, Dalton, Lotufo, and
  Nogueira}]{Bueno2022:MarkupTokens}
Mirelle~Candida Bueno, Carlos Gemmell, Jeff Dalton, Roberto Lotufo, and Rodrigo
  Nogueira. 2022.
\newblock \href {https://aclanthology.org/2022.mathnlp-1.3} {Induced natural
  language rationales and interleaved markup tokens enable extrapolation in
  large language models}.
\newblock In \emph{Proceedings of the 1st Workshop on Mathematical Natural
  Language Processing (MathNLP)}, pages 17--24, Abu Dhabi, United Arab Emirates
  (Hybrid). Association for Computational Linguistics.

\bibitem[{Chi et~al.(2023)Chi, Fan, Rudnicky, and
  Ramadge}]{Chi2023:DissectingLenGen}
Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.acl-long.756} {Dissecting
  transformer length extrapolation via the lens of receptive field analysis}.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 13522--13537,
  Toronto, Canada. Association for Computational Linguistics.

\bibitem[{Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann et~al.}]{Chowdhery2022:PaLM}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al. 2022.
\newblock \href {https://arxiv.org/abs/2204.02311} {Palm: Scaling language
  modeling with pathways}.
\newblock \emph{ArXiv}, abs/2204.02311.

\bibitem[{Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma et~al.}]{Chung2022:ScalingSFT}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al. 2022.
\newblock \href {https://arxiv.org/abs/2210.11416} {Scaling
  instruction-finetuned language models}.
\newblock \emph{ArXiv}, abs/2210.11416.

\bibitem[{Csord{\'a}s et~al.(2021)Csord{\'a}s, Irie, and
  Schmidhuber}]{Csordas2021:DevilInDetail}
R{\'o}bert Csord{\'a}s, Kazuki Irie, and Juergen Schmidhuber. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.emnlp-main.49} {The devil is
  in the detail: Simple tricks improve systematic generalization of
  transformers}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 619--634, Online and Punta Cana,
  Dominican Republic. Association for Computational Linguistics.

\bibitem[{Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov}]{Dai2019:TransformerXL}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan
  Salakhutdinov. 2019.
\newblock \href {https://doi.org/10.18653/v1/P19-1285} {Transformer-{XL}:
  Attentive language models beyond a fixed-length context}.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 2978--2988, Florence, Italy.
  Association for Computational Linguistics.

\bibitem[{Deletang et~al.(2023)Deletang, Ruoss, Grau-Moya, Genewein, Wenliang,
  Catt, Cundy, Hutter, Legg, Veness, and Ortega}]{Deletang2023:Chomskey}
Gregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li~Kevin
  Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness,
  and Pedro~A Ortega. 2023.
\newblock \href {https://openreview.net/forum?id=WbxHAzkeQcn} {Neural networks
  and the chomsky hierarchy}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova}]{Devlin2019:BERT}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
\newblock \href {https://doi.org/10.18653/v1/N19-1423} {{BERT}: Pre-training of
  deep bidirectional transformers for language understanding}.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota. Association for Computational Linguistics.

\bibitem[{Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann,
  Askell, Bai, Chen, Conerly, DasSarma et~al.}]{Elhage2021:TransformerCircuits}
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben
  Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma,
  et~al. 2021.
\newblock \href {https://transformer-circuits.pub/2021/framework/index.html} {A
  mathematical framework for transformer circuits}.
\newblock \emph{Transformer Circuits Thread}.

\bibitem[{Furrer et~al.(2020)Furrer, van Zee, Scales, and
  Sch{\"{a}}rli}]{Furrer2020:CompGenSM}
Daniel Furrer, Marc van Zee, Nathan Scales, and Nathanael Sch{\"{a}}rli. 2020.
\newblock \href {https://arxiv.org/abs/2007.08970} {Compositional
  generalization in semantic parsing: Pre-training vs. specialized
  architectures}.
\newblock \emph{ArXiv}, abs/2007.08970.

\bibitem[{Gibson(1998)}]{Gibson1998:Locality}
Edward Gibson. 1998.
\newblock \href {https://doi.org/https://doi.org/10.1016/S0010-0277(98)00034-1}
  {Linguistic complexity: locality of syntactic dependencies}.
\newblock \emph{Cognition}, 68(1):1--76.

\bibitem[{Gontier et~al.(2020)Gontier, Sinha, Reddy, and
  Pal}]{Gontier2020:GPTCLUTRR}
Nicolas Gontier, Koustuv Sinha, Siva Reddy, and Christopher Pal. 2020.
\newblock \href
  {https://proceedings.neurips.cc/paper/2020/hash/fc84ad56f9f547eb89c72b9bac209312-Abstract.html}
  {Measuring systematic generalization in neural proof generation with
  transformers}.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}.

\bibitem[{Graves et~al.(2016)Graves, Wayne, Reynolds, Harley, Danihelka,
  Grabska-Barwi{\'n}ska, Colmenarejo, Grefenstette, Ramalho, Agapiou
  et~al.}]{Graves2014:NTM}
Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka
  Grabska-Barwi{\'n}ska, Sergio~G{\'o}mez Colmenarejo, Edward Grefenstette,
  Tiago Ramalho, John Agapiou, et~al. 2016.
\newblock \href {https://doi.org/10.1038/nature20101} {Hybrid computing using a
  neural network with dynamic external memory}.
\newblock \emph{Nature}, 538(7626):471--476.

\bibitem[{Haviv et~al.(2022)Haviv, Ram, Press, Izsak, and
  Levy}]{Haviv2022:NoPETransformer}
Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. 2022.
\newblock \href {https://aclanthology.org/2022.findings-emnlp.99} {Transformer
  language models without positional encodings still learn positional
  information}.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2022}, pages 1382--1390, Abu Dhabi, United Arab Emirates. Association
  for Computational Linguistics.

\bibitem[{Hendrycks and Gimpel(2020)}]{Hendrycks2020:GeLU}
Dan Hendrycks and Kevin Gimpel. 2020.
\newblock Gaussian error linear units (gelus).
\newblock \emph{ArXiv}, abs/1606.08415.

\bibitem[{Hupkes et~al.(2020)Hupkes, Dankers, Mul, and
  Bruni}]{Hupkes2020:Compositionality}
Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. 2020.
\newblock \href {https://doi.org/10.1613/jair.1.11674} {Compositionality
  decomposed: How do neural networks generalise?}
\newblock \emph{Journal of Artificial Intelligence Research}, 67:757--795.

\bibitem[{Irie et~al.(2019)Irie, Zeyer, Schl{\"{u}}ter, and
  Ney}]{Irie2019:LMwithDeepTransform}
Kazuki Irie, Albert Zeyer, Ralf Schl{\"{u}}ter, and Hermann Ney. 2019.
\newblock \href {https://doi.org/10.21437/INTERSPEECH.2019-2225} {Language
  modeling with deep transformers}.
\newblock In \emph{Interspeech 2019, 20th Annual Conference of the
  International Speech Communication Association, Graz, Austria, 15-19
  September 2019}, pages 3905--3909.

\bibitem[{Kaiser and Sutskever(2016)}]{Kaiser2015:NeuralGPU}
Lukasz Kaiser and Ilya Sutskever. 2016.
\newblock \href {http://arxiv.org/abs/1511.08228} {Neural gpus learn
  algorithms}.
\newblock In \emph{4th International Conference on Learning Representations,
  {ICLR} 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track
  Proceedings}.

\bibitem[{Kiyono et~al.(2021)Kiyono, Kobayashi, Suzuki, and
  Inui}]{Kiyono2021:ShiftingAbsolutePositionEmbedding}
Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, and Kentaro Inui. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.emnlp-main.266} {{SHAPE}:
  {S}hifted absolute position embedding for transformers}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 3309--3321, Online and Punta Cana,
  Dominican Republic. Association for Computational Linguistics.

\bibitem[{Lake and Baroni(2018)}]{Lake2018:SCAN}
Brenden~M. Lake and Marco Baroni. 2018.
\newblock \href {http://proceedings.mlr.press/v80/lake18a.html} {Generalization
  without systematicity: On the compositional skills of sequence-to-sequence
  recurrent networks}.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, volume~80 of \emph{Proceedings of Machine Learning Research}, pages
  2879--2888. {PMLR}.

\bibitem[{Li et~al.(2023)Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone,
  Akiki, Li, Chim, Liu, Zheltonozhskii, Zhuo et~al.}]{Li2023:Starcoder}
Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,
  Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu,
  Evgenii Zheltonozhskii, Terry~Yue Zhuo, et~al. 2023.
\newblock \href {https://arxiv.org/abs/2305.06161} {Starcoder: may the source
  be with you!}
\newblock \emph{ArXiv}, abs/2305.06161.

\bibitem[{Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga,
  Zhang, Narayanan, Wu, Kumar et~al.}]{Liang2022:HELM}
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
  Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
  et~al. 2022.
\newblock \href {https://arxiv.org/abs/2211.09110} {Holistic evaluation of
  language models}.
\newblock \emph{ArXiv}, abs/2211.09110.

\bibitem[{Likhomanenko et~al.(2021)Likhomanenko, Xu, Synnaeve, Collobert, and
  Rogozhnikov}]{Likhomanenko2021:CAPE}
Tatiana Likhomanenko, Qiantong Xu, Gabriel Synnaeve, Ronan Collobert, and Alex
  Rogozhnikov. 2021.
\newblock \href {https://openreview.net/forum?id=n-FqqWXnWW} {{CAPE}: Encoding
  relative positions with continuous augmented positional embeddings}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Lindner et~al.(2023)Lindner, Kramár, Rahtz, McGrath, and
  Mikulik}]{lindner2023tracr}
David Lindner, János Kramár, Matthew Rahtz, Thomas McGrath, and Vladimir
  Mikulik. 2023.
\newblock \href {https://arxiv.org/abs/2301.05062} {Tracr: Compiled
  transformers as a laboratory for interpretability}.
\newblock \emph{ArXiv}, abs/2301.05062.

\bibitem[{Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le,
  Zoph, Wei, and Roberts}]{Longpre2023:FLAN}
Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny
  Zhou, Quoc~V. Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023.
\newblock \href {https://arxiv.org/abs/2301.13688} {The flan collection:
  Designing data and methods for effective instruction tuning}.
\newblock \emph{ArXiv}, abs/2301.13688.

\bibitem[{Luo et~al.(2021)Luo, Kulmizev, and Mao}]{Luo2021:PositionalArtefacts}
Ziyang Luo, Artur Kulmizev, and Xiaoxi Mao. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.acl-long.413} {Positional
  artefacts propagate through masked language model embeddings}.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 5312--5327,
  Online. Association for Computational Linguistics.

\bibitem[{Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin,
  Bieber, Dohan, Lewkowycz, Bosma, Luan, Sutton, and
  Odena}]{Nye2021:Scratchpad}
Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,
  David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan,
  Charles Sutton, and Augustus Odena. 2021.
\newblock \href {https://arxiv.org/abs/2112.00114} {Show your work: Scratchpads
  for intermediate computation with language models}.
\newblock \emph{ArXiv}, abs/2112.00114.

\bibitem[{Ontanon et~al.(2022)Ontanon, Ainslie, Fisher, and
  Cvicek}]{Ontanon2022:MakeTransformerCompositional}
Santiago Ontanon, Joshua Ainslie, Zachary Fisher, and Vaclav Cvicek. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-long.251} {Making
  transformers solve compositional tasks}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 3591--3607,
  Dublin, Ireland. Association for Computational Linguistics.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell,
  Welinder, Christiano, Leike, and Lowe}]{Ouyang2022:InstructGPT}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
  Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
  Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
\newblock \href {https://arxiv.org/abs/2203.02155} {Training language models to
  follow instructions with human feedback}.
\newblock \emph{ArXiv}, abs/2203.02155.

\bibitem[{Park et~al.(2020)Park, Yun, Lee, and
  Shin}]{Park2020:UniversalApproximation}
Sejun Park, Chulhee Yun, Jaeho Lee, and Jinwoo Shin. 2020.
\newblock Minimum width for universal approximation.
\newblock \emph{ArXiv}.

\bibitem[{Press et~al.(2022)Press, Smith, and Lewis}]{Press2022:ALiBi}
Ofir Press, Noah~A. Smith, and Mike Lewis. 2022.
\newblock \href {https://openreview.net/forum?id=R8sQPpGCv0} {Train short, test
  long: Attention with linear biases enables input length extrapolation}.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever}]{Radford2019:Language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever. 2019.
\newblock \href
  {https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
  {Language models are unsupervised multitask learners}.
\newblock \emph{OpenAI Blog}, 1(8):9.

\bibitem[{Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu}]{Raffel2020:T5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu. 2020.
\newblock \href {http://jmlr.org/papers/v21/20-074.html} {Exploring the limits
  of transfer learning with a unified text-to-text transformer}.
\newblock \emph{Journal of Machine Learning Research}, 21:140:1--140:67.

\bibitem[{Saxton et~al.(2019)Saxton, Grefenstette, Hill, and
  Kohli}]{Saxton2018:DeepmindMath}
David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019.
\newblock \href {https://openreview.net/forum?id=H1gR5iR5FX} {Analysing
  mathematical reasoning abilities of neural models}.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net.

\bibitem[{Scao et~al.(2022{\natexlab{a}})Scao, Fan, Akiki, Pavlick, Ili{\'c},
  Hesslow, Castagn{\'e}, Luccioni, Yvon, Gall{\'e} et~al.}]{Scao2022:BLOOM}
Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c},
  Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois
  Yvon, Matthias Gall{\'e}, et~al. 2022{\natexlab{a}}.
\newblock \href {https://arxiv.org/abs/2211.05100} {Bloom: A 176b-parameter
  open-access multilingual language model}.
\newblock \emph{ArXiv}, abs/2211.05100.

\bibitem[{Scao et~al.(2022{\natexlab{b}})Scao, Wang, Hesslow, Saulnier, Bekman,
  Bari, Biderman, Elsahar, Phang, Press, Raffel, Sanh, Shen, Sutawika, Tae,
  Yong, Launay, and Beltagy}]{Scao2022:LMinOneMillionGPUHours}
Teven~Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman,
  M~Saiful Bari, Stella Biderman, Hady Elsahar, Jason Phang, Ofir Press, Colin
  Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng~Xin
  Yong, Julien Launay, and Iz~Beltagy. 2022{\natexlab{b}}.
\newblock \href {https://openreview.net/forum?id=rI7BL3fHIZq} {What language
  model to train if you have one million {GPU} hours?}
\newblock In \emph{Challenges {\&} Perspectives in Creating Large Language
  Models}.

\bibitem[{Shaw et~al.(2018)Shaw, Uszkoreit, and
  Vaswani}]{Shaw2018:RelativePosEnc}
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.
\newblock \href {https://doi.org/10.18653/v1/N18-2074} {Self-attention with
  relative position representations}.
\newblock In \emph{Proceedings of the 2018 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 2 (Short Papers)}, pages 464--468, New Orleans,
  Louisiana. Association for Computational Linguistics.

\bibitem[{Shen et~al.(2018)Shen, Zhou, Long, Jiang, Pan, and
  Zhang}]{Shen2017:DiSelfAttentionNetwork}
Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Shirui Pan, and Chengqi Zhang.
  2018.
\newblock \href
  {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16126} {Disan:
  Directional self-attention network for rnn/cnn-free language understanding}.
\newblock In \emph{Proceedings of the Thirty-Second {AAAI} Conference on
  Artificial Intelligence, (AAAI-18), the 30th innovative Applications of
  Artificial Intelligence (IAAI-18), and the 8th {AAAI} Symposium on
  Educational Advances in Artificial Intelligence (EAAI-18), New Orleans,
  Louisiana, USA, February 2-7, 2018}, pages 5446--5455. {AAAI} Press.

\bibitem[{Sinha et~al.(2022)Sinha, Kazemnejad, Reddy, Pineau, Hupkes, and
  Williams}]{Sinha2022:CuriousCaseAPE}
Koustuv Sinha, Amirhossein Kazemnejad, Siva Reddy, Joelle Pineau, Dieuwke
  Hupkes, and Adina Williams. 2022.
\newblock \href {https://aclanthology.org/2022.findings-emnlp.326} {The curious
  case of absolute position embeddings}.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2022}, pages 4449--4472, Abu Dhabi, United Arab Emirates. Association
  for Computational Linguistics.

\bibitem[{Sinha et~al.(2019)Sinha, Sodhani, Dong, Pineau, and
  Hamilton}]{Sinha2019:CLUTRR}
Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William~L.
  Hamilton. 2019.
\newblock \href {https://doi.org/10.18653/v1/D19-1458} {{CLUTRR}: A diagnostic
  benchmark for inductive reasoning from text}.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 4506--4515, Hong Kong,
  China. Association for Computational Linguistics.

\bibitem[{Su et~al.(2021)Su, Lu, Pan, Wen, and Liu}]{Su2021:Rotary}
Jianlin Su, Yu~Lu, Shengfeng Pan, Bo~Wen, and Yunfeng Liu. 2021.
\newblock \href {https://arxiv.org/abs/2104.09864} {Roformer: Enhanced
  transformer with rotary position embedding}.
\newblock \emph{ArXiv}, abs/2104.09864.

\bibitem[{Tay et~al.(2022)Tay, Dehghani, Rao, Fedus, Abnar, Chung, Narang,
  Yogatama, Vaswani, and Metzler}]{Tay2022:PerplexityDontTransfer}
Yi~Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung~Won
  Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler.
  2022.
\newblock \href {https://openreview.net/forum?id=f2OYVDyfIB} {Scale
  efficiently: Insights from pretraining and finetuning transformers}.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net.

\bibitem[{Taylor et~al.(2022)Taylor, Kardas, Cucurull, Scialom, Hartshorn,
  Saravia, Poulton, Kerkez, and Stojnic}]{Taylor2022:Galactica}
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony
  Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic.
  2022.
\newblock \href {https://arxiv.org/abs/2211.09085} {Galactica: A large language
  model for science}.
\newblock \emph{ArXiv}, abs/2211.09085.

\bibitem[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and
  Lample}]{Touvron2023:LLaMA}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
  Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
  Lample. 2023.
\newblock \href {https://arxiv.org/abs/2302.13971} {{LLaMA}: Open and efficient
  foundation language models}.
\newblock \emph{ArXiv}, abs/2302.13971.

\bibitem[{Tsai et~al.(2019)Tsai, Bai, Yamada, Morency, and
  Salakhutdinov}]{Tsai2019:TransformerDissection}
Yao-Hung~Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and
  Ruslan Salakhutdinov. 2019.
\newblock \href {https://doi.org/10.18653/v1/D19-1443} {Transformer dissection:
  An unified understanding for transformer{'}s attention via the lens of
  kernel}.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 4344--4353, Hong Kong,
  China. Association for Computational Linguistics.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{Vaswani2017:Transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.
\newblock \href
  {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
  {Attention is all you need}.
\newblock In \emph{Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
  Long Beach, CA, {USA}}, pages 5998--6008.

\bibitem[{Wang et~al.(2022)Wang, Mishra, Alipoormolabashi, Kordi, Mirzaei,
  Naik, Ashok, Dhanasekaran, Arunkumar, Stap
  et~al.}]{Wang2022:SuperNaturalInstructions}
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza
  Mirzaei, Atharva Naik, Arjun Ashok, Arut~Selvan Dhanasekaran, Anjana
  Arunkumar, David Stap, et~al. 2022.
\newblock \href {https://aclanthology.org/2022.emnlp-main.340}
  {Super-{N}atural{I}nstructions: Generalization via declarative instructions
  on 1600+ {NLP} tasks}.
\newblock In \emph{Proc. of EMNLP}, pages 5085--5109, Abu Dhabi, United Arab
  Emirates. Association for Computational Linguistics.

\bibitem[{Wei et~al.(2022{\natexlab{a}})Wei, Bosma, Zhao, Guu, Yu, Lester, Du,
  Dai, and Le}]{Wei2022:SFT}
Jason Wei, Maarten Bosma, Vincent~Y. Zhao, Kelvin Guu, Adams~Wei Yu, Brian
  Lester, Nan Du, Andrew~M. Dai, and Quoc~V. Le. 2022{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=gEZrGCozdqR} {Finetuned
  language models are zero-shot learners}.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net.

\bibitem[{Wei et~al.(2022{\natexlab{b}})Wei, Wang, Schuurmans, Bosma, brian
  ichter, Xia, Chi, Le, and Zhou}]{Wei2022:ChainOfThought}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia,
  Ed~H. Chi, Quoc~V Le, and Denny Zhou. 2022{\natexlab{b}}.
\newblock \href {https://openreview.net/forum?id=_VjQlMeSB_J} {Chain of thought
  prompting elicits reasoning in large language models}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Weiss et~al.(2021)Weiss, Goldberg, and Yahav}]{weiss2021thinking}
Gail Weiss, Yoav Goldberg, and Eran Yahav. 2021.
\newblock \href {http://proceedings.mlr.press/v139/weiss21a.html} {Thinking
  like transformers}.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning, {ICML} 2021, 18-24 July 2021, Virtual Event}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 11080--11090. {PMLR}.

\bibitem[{Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Le~Scao, Gugger, Drame, Lhoest, and Rush}]{Wolf2020:Huggingface}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven Le~Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander Rush. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.emnlp-demos.6} {Transformers:
  State-of-the-art natural language processing}.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 38--45, Online.
  Association for Computational Linguistics.

\bibitem[{Yang et~al.(2019)Yang, Wang, Wong, Chao, and
  Tu}]{Yang2019:SelfAttentionWordOrder}
Baosong Yang, Longyue Wang, Derek~F. Wong, Lidia~S. Chao, and Zhaopeng Tu.
  2019.
\newblock \href {https://doi.org/10.18653/v1/P19-1354} {Assessing the ability
  of self-attention networks to learn word order}.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 3635--3644, Florence, Italy.
  Association for Computational Linguistics.

\bibitem[{Yehudai et~al.(2021)Yehudai, Fetaya, Meirom, Chechik, and
  Maron}]{Yehudai2020:GraphSizeGen}
Gilad Yehudai, Ethan Fetaya, Eli~A. Meirom, Gal Chechik, and Haggai Maron.
  2021.
\newblock \href {http://proceedings.mlr.press/v139/yehudai21a.html} {From local
  structures to size generalization in graph neural networks}.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning, {ICML} 2021, 18-24 July 2021, Virtual Event}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 11975--11986. {PMLR}.

\bibitem[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura, Sridhar, Wang,
  and Zettlemoyer}]{Zhang2022:OPT}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, Todor Mihaylov,
  Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit~Singh Koura, Anjali
  Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022.
\newblock \href {https://arxiv.org/abs/2205.01068} {Opt: Open pre-trained
  transformer language models}.
\newblock \emph{ArXiv}, abs/2205.01068.

\bibitem[{Zhang et~al.(2023)Zhang, Backurs, Bubeck, Eldan, Gunasekar, and
  Wagner}]{Zhang2023:LEGO}
Yi~Zhang, Arturs Backurs, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, and
  Tal Wagner. 2023.
\newblock \href {https://arxiv.org/abs/2206.04301} {Unveiling transformers with
  lego: a synthetic reasoning task}.
\newblock \emph{ArXiv}, abs/2206.04301.

\end{thebibliography}
