@article{li2023self,
 author = {Li, Xian and Yu, Ping and Zhou, Chunting and Schick, Timo and Zettlemoyer, Luke and Levy, Omer and Weston, Jason and Lewis, Mike},
 journal = {ArXiv preprint},
 title = {Self-alignment with instruction backtranslation},
 url = {https://arxiv.org/abs/2308.06259},
 volume = {abs/2308.06259},
 year = {2023}
}

@article{zhong2023self,
 author = {Zhong, Qihuang and Ding, Liang and Liu, Juhua and Du, Bo and Tao, Dacheng},
 journal = {ArXiv preprint},
 title = {Self-Evolution Learning for Discriminative Language Model Pretraining},
 url = {https://arxiv.org/abs/2305.15275},
 volume = {abs/2305.15275},
 year = {2023}
}

@article{kung2023active,
 author = {Kung, Po-Nien and Yin, Fan and Wu, Di and Chang, Kai-Wei and Peng, Nanyun},
 journal = {ArXiv preprint},
 title = {Active instruction tuning: Improving cross-task generalization by training on prompt sensitive tasks},
 url = {https://arxiv.org/abs/2311.00288},
 volume = {abs/2311.00288},
 year = {2023}
}

@article{peng2023towards,
 author = {Peng, Keqin and Ding, Liang and Zhong, Qihuang and Shen, Li and Liu, Xuebo and Zhang, Min and Ouyang, Yuanxin and Tao, Dacheng},
 journal = {ArXiv preprint},
 title = {Towards making the most of chatgpt for machine translation},
 url = {https://arxiv.org/abs/2303.13780},
 volume = {abs/2303.13780},
 year = {2023}
}

@inproceedings{zhou2020uncertainty,
 address = {Online},
 author = {Zhou, Yikai  and
Yang, Baosong  and
Wong, Derek F.  and
Wan, Yu  and
Chao, Lidia S.},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/2020.acl-main.620},
 pages = {6934--6944},
 publisher = {Association for Computational Linguistics},
 title = {Uncertainty-Aware Curriculum Learning for Neural Machine Translation},
 url = {https://aclanthology.org/2020.acl-main.620},
 year = {2020}
}

@inproceedings{hendrycks2020measuring,
 author = {Dan Hendrycks and
Collin Burns and
Steven Basart and
Andy Zou and
Mantas Mazeika and
Dawn Song and
Jacob Steinhardt},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/HendrycksBBZMSS21.bib},
 booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
 publisher = {OpenReview.net},
 timestamp = {Wed, 23 Jun 2021 01:00:00 +0200},
 title = {Measuring Massive Multitask Language Understanding},
 url = {https://openreview.net/forum?id=d7KBjmI3GmQ},
 year = {2021}
}

@article{longpre2023flan,
 author = {Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
 journal = {ArXiv preprint},
 title = {The flan collection: Designing data and methods for effective instruction tuning},
 url = {https://arxiv.org/abs/2301.13688},
 volume = {abs/2301.13688},
 year = {2023}
}

@misc{alpaca,
 author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
 howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
 journal = {GitHub repository},
 publisher = {GitHub},
 title = {Stanford Alpaca: An Instruction-following LLaMA model},
 year = {2023}
}

@article{peng2023instruction,
 author = {Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
 journal = {ArXiv preprint},
 title = {Instruction Tuning with GPT-4},
 url = {https://arxiv.org/abs/2304.03277},
 volume = {abs/2304.03277},
 year = {2023}
}

@misc{chiang2023vicuna,
 author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
 title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
 url = {https://lmsys.org/blog/2023-03-30-vicuna/},
 year = {2023}
}

@article{xu2023wizardlm,
 author = {Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
 journal = {ArXiv preprint},
 title = {Wizardlm: Empowering large language models to follow complex instructions},
 url = {https://arxiv.org/abs/2304.12244},
 volume = {abs/2304.12244},
 year = {2023}
}



@inproceedings{
zhou2023lima,
title={{LIMA}: Less Is More for Alignment},
author={Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and LILI YU and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=KBMOKmX2he}
}

@inproceedings{wei2021finetuned,
 author = {Jason Wei and
Maarten Bosma and
Vincent Y. Zhao and
Kelvin Guu and
Adams Wei Yu and
Brian Lester and
Nan Du and
Andrew M. Dai and
Quoc V. Le},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/WeiBZGYLDDL22.bib},
 booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
 publisher = {OpenReview.net},
 timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
 title = {Finetuned Language Models are Zero-Shot Learners},
 url = {https://openreview.net/forum?id=gEZrGCozdqR},
 year = {2022}
}

@article{chung2022scaling,
 author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
 journal = {ArXiv preprint},
 title = {Scaling instruction-finetuned language models},
 url = {https://arxiv.org/abs/2210.11416},
 volume = {abs/2210.11416},
 year = {2022}
}

@article{sun2023principle,
 author = {Sun, Zhiqing and Shen, Yikang and Zhou, Qinhong and Zhang, Hongxin and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang},
 journal = {ArXiv preprint},
 title = {Principle-driven self-alignment of language models from scratch with minimal human supervision},
 url = {https://arxiv.org/abs/2305.03047},
 volume = {abs/2305.03047},
 year = {2023}
}

@article{honovich2022unnatural,
 author = {Honovich, Or and Scialom, Thomas and Levy, Omer and Schick, Timo},
 journal = {ArXiv preprint},
 title = {Unnatural instructions: Tuning language models with (almost) no human labor},
 url = {https://arxiv.org/abs/2212.09689},
 volume = {abs/2212.09689},
 year = {2022}
}


@inproceedings{
chen2023alpagasus,
title={AlpaGasus: Training a Better Alpaca with Fewer Data},
author={Lichang Chen and Shiyang Li and Jun Yan and Hai Wang and Kalpa Gunaratna and Vikas Yadav and Zheng Tang and Vijay Srinivasan and Tianyi Zhou and Heng Huang and Hongxia Jin},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=FdVXgSJhvz}
}

@article{cao2023instruction,
 author = {Cao, Yihan and Kang, Yanbin and Sun, Lichao},
 journal = {ArXiv preprint},
 title = {Instruction mining: High-quality instruction data selection for large language models},
 url = {https://arxiv.org/abs/2307.06290},
 volume = {abs/2307.06290},
 year = {2023}
}

@article{li2023quantity,
 author = {Li, Ming and Zhang, Yong and Li, Zhitao and Chen, Jiuhai and Chen, Lichang and Cheng, Ning and Wang, Jianzong and Zhou, Tianyi and Xiao, Jing},
 journal = {ArXiv preprint},
 title = {From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning},
 url = {https://arxiv.org/abs/2308.12032},
 volume = {abs/2308.12032},
 year = {2023}
}

@article{wu2023self,
 author = {Wu, Shengguang and Lu, Keming and Xu, Benfeng and Lin, Junyang and Su, Qi and Zhou, Chang},
 journal = {ArXiv preprint},
 title = {Self-Evolved Diverse Data Sampling for Efficient Instruction Tuning},
 url = {https://arxiv.org/abs/2311.08182},
 volume = {abs/2311.08182},
 year = {2023}
}



@article{wang2023far,
  title={How far can camels go? exploring the state of instruction tuning on open resources},
  author={Wang, Yizhong and Ivison, Hamish and Dasigi, Pradeep and Hessel, Jack and Khot, Tushar and Chandu, Khyathi and Wadden, David and MacMillan, Kelsey and Smith, Noah A and Beltagy, Iz and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  url = {https://arxiv.org/abs/2306.04751},
  year={2024}
}




@misc{ivison2023camels,
 archiveprefix = {arXiv},
 author = {Hamish Ivison and Yizhong Wang and Valentina Pyatkin and Nathan Lambert and Matthew Peters and Pradeep Dasigi and Joel Jang and David Wadden and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},
 eprint = {2311.10702},
 primaryclass = {cs.CL},
 title = {Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2},
 url = {https://arxiv.org/abs/2311.10702},
 year = {2023}
}

@article{cobbe2021training,
 author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
 journal = {ArXiv preprint},
 title = {Training verifiers to solve math word problems},
 url = {https://arxiv.org/abs/2110.14168},
 volume = {abs/2110.14168},
 year = {2021}
}

@article{suzgun2022challenging,
 author = {Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
 journal = {ArXiv preprint},
 title = {Challenging big-bench tasks and whether chain-of-thought can solve them},
 url = {https://arxiv.org/abs/2210.09261},
 volume = {abs/2210.09261},
 year = {2022}
}

@article{wei2022chain,
 author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
 journal = {ArXiv preprint},
 title = {Chain-of-thought prompting elicits reasoning in large language models},
 url = {https://arxiv.org/abs/2201.11903},
 volume = {abs/2201.11903},
 year = {2022}
}

@article{chen2021evaluating,
 author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
 journal = {ArXiv preprint},
 title = {Evaluating large language models trained on code},
 url = {https://arxiv.org/abs/2107.03374},
 volume = {abs/2107.03374},
 year = {2021}
}




@inproceedings{
dubois2023alpacafarm,
title={AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback},
author={Yann Dubois and Xuechen Li and Rohan Taori and Tianyi Zhang and Ishaan Gulrajani and Jimmy Ba and Carlos Guestrin and Percy Liang and Tatsunori Hashimoto},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=4hturzLcKX}
}


@article{touvron2023llama,
 author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
 journal = {ArXiv preprint},
 title = {Llama 2: Open foundation and fine-tuned chat models},
 url = {https://arxiv.org/abs/2307.09288},
 volume = {abs/2307.09288},
 year = {2023}
}


@article{liu2023makes,
 author = {Liu, Wei and Zeng, Weihao and He, Keqing and Jiang, Yong and He, Junxian},
 journal = {ArXiv preprint},
 title = {What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning},
 url = {https://arxiv.org/abs/2312.15685},
 volume = {abs/2312.15685},
 year = {2023}
}

@inproceedings{jiao-etal-2023-parrot,
 abstract = {Large language models (LLMs) like ChatGPT have exhibited remarkable abilities on a wide range of natural language processing (NLP) tasks, including various machine translation abilities accomplished during chat. However, these models are only accessible through restricted APIs, which creates barriers to new research and advancements in the field. Therefore, we propose ParroT, a framework to enhance and regulate the translation abilities during chat based on open-source LLMs (e.g., LLaMA), human-written translation and feedback data. Specifically, ParroT reformulates translation data into the instruction-following style, and introduces a {``}Hint{''} field for incorporating extra requirements to regulate the translation process. Accordingly, we propose three instruction types for finetuning ParroT models, including translation instruction, contrastive instruction, and error-guided instruction. Experiments on Flores subsets and WMT22 test sets suggest that translation instruction improves the translation performance of vanilla LLMs significantly while error-guided instruction can lead to further improvement, which demonstrates the importance of learning from low-quality translations annotated by humans. We also demonstrate the potential of automatic evaluation tools in providing quality information of translations, when constructing error-guided instructions for directions that lack human annotation data. Please refer to our Github project for more implementation details: https://github.com/wxjiao/ParroT.},
 address = {Singapore},
 author = {Jiao, Wenxiang  and
Huang, Jen-tse  and
Wang, Wenxuan  and
He, Zhiwei  and
Liang, Tian  and
Wang, Xing  and
Shi, Shuming  and
Tu, Zhaopeng},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
 doi = {10.18653/v1/2023.findings-emnlp.1001},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 pages = {15009--15020},
 publisher = {Association for Computational Linguistics},
 title = {{P}arro{T}: Translating during Chat using Large Language Models tuned with Human Translation and Feedback},
 url = {https://aclanthology.org/2023.findings-emnlp.1001},
 year = {2023}
}

@article{zeng2023tim,
 author = {Zeng, Jiali and Meng, Fandong and Yin, Yongjing and Zhou, Jie},
 journal = {ArXiv preprint},
 title = {Tim: Teaching large language models to translate with comparison},
 url = {https://arxiv.org/abs/2307.04408},
 volume = {abs/2307.04408},
 year = {2023}
}


@inproceedings{
xu2024a,
title={A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models},
author={Haoran Xu and Young Jin Kim and Amr Sharaf and Hany Hassan Awadalla},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=farT6XXntP}
}


@article{gao2024towards,
 author = {Gao, Pengzhi and He, Zhongjun and Wu, Hua and Wang, Haifeng},
 journal = {ArXiv preprint},
 title = {Towards Boosting Many-to-Many Multilingual Machine Translation with Large Language Models},
 url = {https://arxiv.org/abs/2401.05861},
 volume = {abs/2401.05861},
 year = {2024}
}

@inproceedings{liu2023knn,
 author = {Liu, Shudong and Liu, Xuebo and Wong, Derek F and Li, Zhaocong and Jiao, Wenxiang and Chao, Lidia S and Zhang, Min},
 booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 pages = {1878--1891},
 title = {kNN-TL: k-nearest-neighbor transfer learning for low-resource neural machine translation},
 year = {2023}
}

@inproceedings{hinton2002stochastic,
 author = {Geoffrey E. Hinton and
Sam T. Roweis},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/HintonR02.bib},
 booktitle = {Advances in Neural Information Processing Systems 15 [Neural Information
Processing Systems, {NIPS} 2002, December 9-14, 2002, Vancouver, British
Columbia, Canada]},
 editor = {Suzanna Becker and
Sebastian Thrun and
Klaus Obermayer},
 pages = {833--840},
 publisher = {{MIT} Press},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Stochastic Neighbor Embedding},
 url = {https://proceedings.neurips.cc/paper/2002/hash/6150ccc6069bea6b5716254057a194ef-Abstract.html},
 year = {2002}
}

@article{jiang2023mistral,
 author = {Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
 journal = {ArXiv preprint},
 title = {Mistral 7B},
 url = {https://arxiv.org/abs/2310.06825},
 volume = {abs/2310.06825},
 year = {2023}
}

@article{touvron2023llama1,
 author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
 journal = {ArXiv preprint},
 title = {Llama: Open and efficient foundation language models},
 url = {https://arxiv.org/abs/2302.13971},
 volume = {abs/2302.13971},
 year = {2023}
}

@article{achiam2023gpt,
 author = {Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
 journal = {ArXiv preprint},
 title = {Gpt-4 technical report},
 url = {https://arxiv.org/abs/2303.08774},
 volume = {abs/2303.08774},
 year = {2023}
}

@article{penedo2023refinedweb,
 author = {Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
 journal = {ArXiv preprint},
 title = {The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only},
 url = {https://arxiv.org/abs/2306.01116},
 volume = {abs/2306.01116},
 year = {2023}
}

@misc{zhao2024long,
 author = {Hao Zhao and Maksym Andriushchenko and Francesco Croce and Nicolas Flammarion},
 journal = {ArXiv preprint},
 title = {Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning},
 url = {https://arxiv.org/abs/2402.04833},
 volume = {abs/2402.04833},
 year = {2024}
}

@article{yang-etal-2023-BigTranslate,
 author = {Wen Yang and
Chong Li and
Jiajun Zhang and
Chengqing Zong},
 journal = {ArXiv preprint},
 title = {BigTranslate: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages},
 url = {https://arxiv.org/abs/2305.18098},
 volume = {abs/2305.18098},
 year = {2023}
}

@article{chen2023improving,
 author = {Chen, Yijie and Liu, Yijin and Meng, Fandong and Chen, Yufeng and Xu, Jinan and Zhou, Jie},
 journal = {ArXiv preprint},
 title = {Improving translation faithfulness of large language models via augmenting instructions},
 url = {https://arxiv.org/abs/2308.12674},
 volume = {abs/2308.12674},
 year = {2023}
}

@article{bayling,
 author = {Shaolei Zhang and Qingkai Fang and Zhuocheng Zhang and Zhengrui Ma and Yan Zhou and Langlin Huang and Mengyu Bu and Shangtong Gui and Yunji Chen and Xilin Chen and Yang Feng},
 journal = {ArXiv preprint},
 title = {BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models},
 url = {https://arxiv.org/abs/2306.10968},
 volume = {abs/2306.10968},
 year = {2023}
}

@inproceedings{liu-etal-2020-norm,
 address = {Online},
 author = {Liu, Xuebo  and
Lai, Houtim  and
Wong, Derek F.  and
Chao, Lidia S.},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/2020.acl-main.41},
 pages = {427--436},
 publisher = {Association for Computational Linguistics},
 title = {Norm-Based Curriculum Learning for Neural Machine Translation},
 url = {https://aclanthology.org/2020.acl-main.41},
 year = {2020}
}

@inproceedings{liu-etal-2019-shared,
 address = {Florence, Italy},
 author = {Liu, Xuebo  and
Wong, Derek F.  and
Liu, Yang  and
Chao, Lidia S.  and
Xiao, Tong  and
Zhu, Jingbo},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/P19-1352},
 pages = {3613--3622},
 publisher = {Association for Computational Linguistics},
 title = {Shared-Private Bilingual Word Embeddings for Neural Machine Translation},
 url = {https://aclanthology.org/P19-1352},
 year = {2019}
}

@inproceedings{liu-etal-2021-copying,
 address = {Online},
 author = {Liu, Xuebo  and
Wang, Longyue  and
Wong, Derek F.  and
Ding, Liang  and
Chao, Lidia S.  and
Shi, Shuming  and
Tu, Zhaopeng},
 booktitle = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
 doi = {10.18653/v1/2021.findings-acl.373},
 pages = {4265--4275},
 publisher = {Association for Computational Linguistics},
 title = {On the Copying Behaviors of Pre-Training for Neural Machine Translation},
 url = {https://aclanthology.org/2021.findings-acl.373},
 year = {2021}
}

@inproceedings{wang-etal-2022-breaking,
 address = {Abu Dhabi, United Arab Emirates},
 author = {Wang, Zhijun  and
Liu, Xuebo  and
Zhang, Min},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 pages = {6473--6484},
 publisher = {Association for Computational Linguistics},
 title = {Breaking the Representation Bottleneck of {C}hinese Characters: Neural Machine Translation with Stroke Sequence Modeling},
 url = {https://aclanthology.org/2022.emnlp-main.434},
 year = {2022}
}

@inproceedings{liu2021understanding,
 author = {Xuebo Liu and
Longyue Wang and
Derek F. Wong and
Liang Ding and
Lidia S. Chao and
Zhaopeng Tu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/0002WWDCT21.bib},
 booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
 publisher = {OpenReview.net},
 timestamp = {Tue, 12 Apr 2022 01:00:00 +0200},
 title = {Understanding and Improving Encoder Layer Fusion in Sequence-to-Sequence
Learning},
 url = {https://openreview.net/forum?id=n1HD8M6WGn},
 year = {2021}
}

@inproceedings{ConsistTL2022,
 address = {Abu Dhabi, United Arab Emirates},
 author = {Li, Zhaocong  and
Liu, Xuebo  and
Wong, Derek F.  and
Chao, Lidia S.  and
Zhang, Min},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 pages = {8383--8394},
 publisher = {Association for Computational Linguistics},
 title = {{C}onsist{TL}: Modeling Consistency in Transfer Learning for Low-Resource Neural Machine Translation},
 url = {https://aclanthology.org/2022.emnlp-main.574},
 year = {2022}
}

@inproceedings{liu-etal-2021-complementarity-pre,
 address = {Punta Cana, Dominican Republic},
 author = {Liu, Xuebo  and
Wang, Longyue  and
Wong, Derek F.  and
Ding, Liang  and
Chao, Lidia S.  and
Shi, Shuming  and
Tu, Zhaopeng},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
 doi = {10.18653/v1/2021.findings-emnlp.247},
 pages = {2900--2907},
 publisher = {Association for Computational Linguistics},
 title = {On the Complementarity between Pre-Training and Back-Translation for Neural Machine Translation},
 url = {https://aclanthology.org/2021.findings-emnlp.247},
 year = {2021}
}

@inproceedings{peng-etal-2023-towards,
 abstract = {ChatGPT shows remarkable capabilities for machine translation (MT). Several prior studies have shown that it achieves comparable results to commercial systems for high-resource languages, but lags behind in complex tasks, e.g, low-resource and distant-language-pairs translation. However, they usually adopt simple prompts which can not fully elicit the capability of ChatGPT. In this report, we aim to further mine ChatGPT{'}s translation ability by revisiting several aspects: temperature, task information, and domain information, and correspondingly propose two (simple but effective) prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts (DSP). We show that: 1) The performance of ChatGPT depends largely on temperature, and a lower temperature usually can achieve better performance; 2) Emphasizing the task information further improves ChatGPT{'}s performance, particularly in complex MT tasks; 3) Introducing domain information can elicit ChatGPT{'}s generalization ability and improve its performance in the specific domain; 4) ChatGPT tends to generate hallucinations for non-English-centric MT tasks, which can be partially addressed by our proposed prompts but still need to be highlighted for the MT/NLP community. We also explore the effects of advanced in-context learning strategies and find a (negative but interesting) observation: the powerful chain-of-thought prompt leads to word-by-word translation behavior, thus bringing significant translation degradation.},
 address = {Singapore},
 author = {Peng, Keqin  and
Ding, Liang  and
Zhong, Qihuang  and
Shen, Li  and
Liu, Xuebo  and
Zhang, Min  and
Ouyang, Yuanxin  and
Tao, Dacheng},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
 doi = {10.18653/v1/2023.findings-emnlp.373},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 pages = {5622--5633},
 publisher = {Association for Computational Linguistics},
 title = {Towards Making the Most of {C}hat{GPT} for Machine Translation},
 url = {https://aclanthology.org/2023.findings-emnlp.373},
 year = {2023}
}

@inproceedings{ding2021understanding,
 author = {Liang Ding and
Longyue Wang and
Xuebo Liu and
Derek F. Wong and
Dacheng Tao and
Zhaopeng Tu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/DingW0WTT21.bib},
 booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
 publisher = {OpenReview.net},
 timestamp = {Tue, 12 Apr 2022 01:00:00 +0200},
 title = {Understanding and Improving Lexical Choice in Non-Autoregressive Translation},
 url = {https://openreview.net/forum?id=ZTFeSBIX9C},
 year = {2021}
}

@inproceedings{ding-etal-2021-progressive,
 address = {Online},
 author = {Ding, Liang  and
Wang, Longyue  and
Liu, Xuebo  and
Wong, Derek F.  and
Tao, Dacheng  and
Tu, Zhaopeng},
 booktitle = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
 doi = {10.18653/v1/2021.findings-acl.247},
 pages = {2797--2803},
 publisher = {Association for Computational Linguistics},
 title = {Progressive Multi-Granularity Training for Non-Autoregressive Translation},
 url = {https://aclanthology.org/2021.findings-acl.247},
 year = {2021}
}

@article{li2023one,
 author = {Li, Yunshui and Hui, Binyuan and Xia, Xiaobo and Yang, Jiaxi and Yang, Min and Zhang, Lei and Si, Shuzheng and Liu, Junhao and Liu, Tongliang and Huang, Fei and others},
 journal = {ArXiv preprint},
 title = {One shot learning as instruction data prospector for large language models},
 url = {https://arxiv.org/abs/2312.10302},
 volume = {abs/2312.10302},
 year = {2023}
}

@article{xia2024less,
 author = {Xia, Mengzhou and Malladi, Sadhika and Gururangan, Suchin and Arora, Sanjeev and Chen, Danqi},
 journal = {ArXiv preprint},
 title = {Less: Selecting influential data for targeted instruction tuning},
 url = {https://arxiv.org/abs/2402.04333},
 volume = {abs/2402.04333},
 year = {2024}
}

@article{costa2022no,
 author = {Costa-juss{\`a}, Marta R and Cross, James and {\c{C}}elebi, Onur and Elbayad, Maha and Heafield, Kenneth and Heffernan, Kevin and Kalbassi, Elahe and Lam, Janice and Licht, Daniel and Maillard, Jean and others},
 journal = {ArXiv preprint},
 title = {No language left behind: Scaling human-centered machine translation},
 url = {https://arxiv.org/abs/2207.04672},
 volume = {abs/2207.04672},
 year = {2022}
}

@inproceedings{han2023understanding,
    title = "Understanding In-Context Learning via Supportive Pretraining Data",
    author = "Han, Xiaochuang  and
      Simig, Daniel  and
      Mihaylov, Todor  and
      Tsvetkov, Yulia  and
      Celikyilmaz, Asli  and
      Wang, Tianlu",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.708",
    doi = "10.18653/v1/2023.acl-long.708",
    pages = "12660--12673",
    abstract = "In-context learning (ICL) improves language models{'} performance on a variety of NLP tasks by simply demonstrating a handful of examples at inference time. It is not well understood why ICL ability emerges, as the model has never been specifically trained on such demonstrations. Unlike prior work that explores implicit mechanisms behind ICL, we study ICL via investigating the pretraining data. Specifically, we first adapt an iterative, gradient-based approach to find a small subset of pretraining data that supports ICL. We observe that a continued pretraining on this small subset significantly improves the model{'}s ICL ability, by up to 18{\%}. We then compare the supportive subset constrastively with random subsets of pretraining data and discover: (1) The supportive pretraining data to ICL do not have a higher domain relevance to downstream tasks. (2) The supportive pretraining data have a higher mass of rarely occurring, long-tail tokens. (3) The supportive pretraining data are challenging examples where the information gain from long-range context is below average, indicating learning to incorporate difficult long-range context encourages ICL. Our work takes a first step towards understanding ICL via analyzing instance-level pretraining data. Our insights have a potential to enhance the ICL ability of language models by actively guiding the construction of pretraining data in the future.",
}


@inproceedings{gururangan2020don,
 address = {Online},
 author = {Gururangan, Suchin  and
Marasovi{\'c}, Ana  and
Swayamdipta, Swabha  and
Lo, Kyle  and
Beltagy, Iz  and
Downey, Doug  and
Smith, Noah A.},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/2020.acl-main.740},
 pages = {8342--8360},
 publisher = {Association for Computational Linguistics},
 title = {Don{'}t Stop Pretraining: Adapt Language Models to Domains and Tasks},
 url = {https://aclanthology.org/2020.acl-main.740},
 year = {2020}
}

@inproceedings{
chen2024skill,
title={Skill-it! A data-driven skills framework for understanding and training language models},
author={Mayee F Chen and Nicholas Roberts and Kush Bhatia and Jue WANG and Ce Zhang and Frederic Sala and Christopher Re},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=IoizwO1NLf}
}


@article{xie2023data,
 author = {Xie, Sang Michael and Santurkar, Shibani and Ma, Tengyu and Liang, Percy S},
 journal = {Advances in Neural Information Processing Systems},
 pages = {34201--34227},
 title = {Data selection for language models via importance resampling},
 volume = {36},
 year = {2023},
 url = {https://arxiv.org/abs/2302.03169},
}

@inproceedings{post-2018-call,
 address = {Brussels, Belgium},
 author = {Post, Matt},
 booktitle = {Proceedings of the Third Conference on Machine Translation: Research Papers},
 doi = {10.18653/v1/W18-6319},
 pages = {186--191},
 publisher = {Association for Computational Linguistics},
 title = {A Call for Clarity in Reporting {BLEU} Scores},
 url = {https://aclanthology.org/W18-6319},
 year = {2018}
}

@inproceedings{ott2018scaling,
 address = {Brussels, Belgium},
 author = {Ott, Myle  and
Edunov, Sergey  and
Grangier, David  and
Auli, Michael},
 booktitle = {Proceedings of the Third Conference on Machine Translation: Research Papers},
 doi = {10.18653/v1/W18-6301},
 pages = {1--9},
 publisher = {Association for Computational Linguistics},
 title = {Scaling Neural Machine Translation},
 url = {https://aclanthology.org/W18-6301},
 year = {2018}
}

@inproceedings{rei-etal-2022-comet,
 address = {Abu Dhabi, United Arab Emirates (Hybrid)},
 author = {Rei, Ricardo  and
C. de Souza, Jos{\'e} G.  and
Alves, Duarte  and
Zerva, Chrysoula  and
Farinha, Ana C  and
Glushkova, Taisiya  and
Lavie, Alon  and
Coheur, Luisa  and
Martins, Andr{\'e} F. T.},
 booktitle = {Proceedings of the Seventh Conference on Machine Translation (WMT)},
 pages = {578--585},
 publisher = {Association for Computational Linguistics},
 title = {{COMET}-22: Unbabel-{IST} 2022 Submission for the Metrics Shared Task},
 url = {https://aclanthology.org/2022.wmt-1.52},
 year = {2022}
}



@article{Orca,
  title={Orca: Progressive Learning from Complex
Explanation Traces of GPT-4},
  author={Subhabrata, Mukherjee and Arindam, Mitra},
  journal={https://arxiv.org/pdf/2306.02707},
  url = {https://arxiv.org/pdf/2306.02707},
  year={2023}
}