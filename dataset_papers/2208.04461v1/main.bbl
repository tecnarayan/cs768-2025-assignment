\begin{thebibliography}{10}

\bibitem{allingham2021sparse}
James~Urquhart Allingham, Florian Wenzel, Zelda~E Mariet, Basil Mustafa, Joan
  Puigcerver, Neil Houlsby, Ghassen Jerfel, Vincent Fortuin, Balaji
  Lakshminarayanan, and Jasper Snoek.
\newblock {Sparse MOEs meet efficient ensembles}.
\newblock {\em arXiv preprint arXiv:2110.03360}, 2021.

\bibitem{andoni2006near}
Alexandr Andoni and Piotr Indyk.
\newblock Near-optimal hashing algorithms for approximate nearest neighbor in
  high dimensions.
\newblock In {\em 2006 47th annual IEEE symposium on foundations of computer
  science (FOCS'06)}, pages 459--468. IEEE, 2006.

\bibitem{andoni2015practical}
Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig
  Schmidt.
\newblock Practical and optimal lsh for angular distance.
\newblock In {\em Proceedings of the 28th International Conference on Neural
  Information Processing Systems-Volume 1}, pages 1225--1233, 2015.

\bibitem{andoni2018approximate}
Alexandr Andoni, Piotr Indyk, and Ilya Razenshteyn.
\newblock Approximate nearest neighbor search in high dimensions.
\newblock In {\em Proceedings of the International Congress of Mathematicians:
  Rio de Janeiro 2018}, pages 3287--3318. World Scientific, 2018.

\bibitem{artetxe2021efficient}
Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam
  Shleifer, Xi~Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru,
  et~al.
\newblock Efficient large scale language modeling with mixtures of experts.
\newblock {\em arXiv preprint arXiv:2112.10684}, 2021.

\bibitem{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em arXiv preprint arXiv:2005.14165}, 2020.

\bibitem{charikar2002similarity}
Moses~S Charikar.
\newblock Similarity estimation techniques from rounding algorithms.
\newblock In {\em Proceedings of the thiry-fourth annual ACM symposium on
  Theory of computing}, pages 380--388, 2002.

\bibitem{chen2020mongoose}
Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan~Lingjie Li, Tri
  Dao, Zhao Song, Anshumali Shrivastava, and Christopher Re.
\newblock Mongoose: A learnable lsh framework for efficient neural network
  training.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{chen2015compressing}
Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen.
\newblock Compressing neural networks with the hashing trick.
\newblock In {\em International conference on machine learning}, pages
  2285--2294. PMLR, 2015.

\bibitem{chen2005condition}
Zizhong Chen and Jack~J Dongarra.
\newblock Condition numbers of gaussian random matrices.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  27(3):603--620, 2005.

\bibitem{datar2004locality}
Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab~S Mirrokni.
\newblock Locality-sensitive hashing scheme based on p-stable distributions.
\newblock In {\em Proceedings of the twentieth annual symposium on
  Computational geometry}, pages 253--262, 2004.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{du2021glam}
Nan Du, Yanping Huang, Andrew~M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,
  Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, et~al.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock {\em arXiv preprint arXiv:2112.06905}, 2021.

\bibitem{fedus2021switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock {\em arXiv preprint arXiv:2101.03961}, 2021.

\bibitem{ghazi2019recursive}
Badih Ghazi, Rina Panigrahy, and Joshua Wang.
\newblock Recursive sketches for modular deep learning.
\newblock In {\em International Conference on Machine Learning}, pages
  2211--2220. PMLR, 2019.

\bibitem{har2011geometric}
Sariel Har-Peled.
\newblock {\em Geometric approximation algorithms}.
\newblock Number 173. American Mathematical Soc., 2011.

\bibitem{rmsprop}
Geoffrey Hinton.
\newblock {Lecture Notes, Toronto, Hinton, 2012,
  \url{http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}}.

\bibitem{hoefler2021sparsity}
Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste.
\newblock Sparsity in deep learning: Pruning and growth for efficient inference
  and training in neural networks.
\newblock {\em arXiv preprint arXiv:2102.00554}, 2021.

\bibitem{jacobs1995methods}
Robert~A Jacobs.
\newblock Methods for combining experts' probability assessments.
\newblock {\em Neural computation}, 7(5):867--888, 1995.

\bibitem{jacobs1991adaptive}
Robert~A Jacobs, Michael~I Jordan, Steven~J Nowlan, and Geoffrey~E Hinton.
\newblock Adaptive mixtures of local experts.
\newblock {\em Neural computation}, 3(1):79--87, 1991.

\bibitem{jaszczur2021sparse}
Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, {\L}ukasz Kaiser,
  Wojciech Gajewski, Henryk Michalewski, and Jonni Kanerva.
\newblock Sparse is enough in scaling transformers.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{jordan1994hierarchical}
Michael~I Jordan and Robert~A Jacobs.
\newblock {Hierarchical mixtures of experts and the EM algorithm}.
\newblock {\em Neural computation}, 6(2):181--214, 1994.

\bibitem{kang1996statistical}
Kukjin Kang and Jong-Hoon Oh.
\newblock Statistical mechanics of the mixture of experts.
\newblock {\em Advances in neural information processing systems}, 9, 1996.

\bibitem{khan2021transformers}
Salman Khan, Muzammal Naseer, Munawar Hayat, Syed~Waqas Zamir, Fahad~Shahbaz
  Khan, and Mubarak Shah.
\newblock Transformers in vision: A survey.
\newblock {\em arXiv preprint arXiv:2101.01169}, 2021.

\bibitem{kim2020fastformers}
Young~Jin Kim and Hany~Hassan Awadalla.
\newblock Fastformers: Highly efficient transformer models for natural language
  understanding.
\newblock {\em arXiv preprint arXiv:2010.13382}, 2020.

\bibitem{cifar}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{lepikhin2021gshard}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping
  Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
\newblock Gshard: Scaling giant models with conditional computation and
  automatic sharding.
\newblock In {\em 9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.

\bibitem{li2020train}
Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and
  Joseph~E Gonzalez.
\newblock Train large, then compress: Rethinking model size for efficient
  training and inference of transformers.
\newblock {\em arXiv preprint arXiv:2002.11794}, 2020.

\bibitem{nie2021dense}
Xiaonan Nie, Shijie Cao, Xupeng Miao, Lingxiao Ma, Jilong Xue, Youshan Miao,
  Zichao Yang, Zhi Yang, and Bin Cui.
\newblock Dense-to-sparse gate for mixture-of-experts.
\newblock {\em arXiv preprint arXiv:2112.14397}, 2021.

\bibitem{panigrahy2021sketch}
Rina Panigrahy, Xin Wang, and Manzil Zaheer.
\newblock Sketch based memory for neural networks.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 3169--3177. PMLR, 2021.

\bibitem{rae2016scaling}
Jack~W Rae, Jonathan~J Hunt, Tim Harley, Ivo Danihelka, Andrew Senior, Greg
  Wayne, Alex Graves, and Timothy~P Lillicrap.
\newblock Scaling memory-augmented neural networks with sparse reads and
  writes.
\newblock {\em arXiv preprint arXiv:1610.09027}, 2016.

\bibitem{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em arXiv preprint arXiv:1910.10683}, 2019.

\bibitem{riquelme2021scaling}
Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe
  Jenatton, Andr{\'e} Susano~Pinto, Daniel Keysers, and Neil Houlsby.
\newblock Scaling vision with sparse mixture of experts.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{roller2021hash}
Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston.
\newblock Hash layers for large sparse models.
\newblock {\em arXiv preprint arXiv:2106.04426}, 2021.

\bibitem{rudelson2009smallest}
Mark Rudelson and Roman Vershynin.
\newblock Smallest singular value of a random rectangular matrix.
\newblock {\em Communications on Pure and Applied Mathematics: A Journal Issued
  by the Courant Institute of Mathematical Sciences}, 62(12):1707--1739, 2009.

\bibitem{shazeer2018mesh-tensorflow}
Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn
  Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young,
  Ryan Sepassi, and Blake Hechtman.
\newblock Mesh-tensorflow: Deep learning for supercomputers.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, {\em Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc~V. Le,
  Geoffrey~E. Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock In {\em ICLR (Poster)}. OpenReview.net, 2017.

\bibitem{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em The journal of machine learning research}, 15(1):1929--1958,
  2014.

\bibitem{valiant2011estimating}
Gregory Valiant and Paul Valiant.
\newblock Estimating the unseen: an n/log (n)-sample estimator for entropy and
  support size, shown optimal via new clts.
\newblock In {\em Proceedings of the forty-third annual ACM symposium on Theory
  of computing}, pages 685--694, 2011.

\bibitem{wang2021ernie}
Shuohuan Wang, Yu~Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun
  Feng, Junyuan Shang, Yanbin Zhao, Chao Pang, et~al.
\newblock Ernie 3.0 titan: Exploring larger-scale knowledge enhanced
  pre-training for language understanding and generation.
\newblock {\em arXiv preprint arXiv:2112.12731}, 2021.

\bibitem{wen2022transformers}
Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and
  Liang Sun.
\newblock Transformers in time series: A survey.
\newblock {\em arXiv preprint arXiv:2202.07125}, 2022.

\bibitem{wu2020visual}
Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan,
  Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter Vajda.
\newblock Visual transformers: Token-based image representation and processing
  for computer vision.
\newblock {\em arXiv preprint arXiv:2006.03677}, 2020.

\end{thebibliography}
