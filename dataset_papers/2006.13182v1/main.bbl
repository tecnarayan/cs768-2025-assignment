\begin{thebibliography}{54}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{}\fi

\bibitem[{Allen-Zhu et~al.(2018{\natexlab{a}})Allen-Zhu, Li and
  Liang}]{allen2018learning}
\text{Allen-Zhu, Z.}, \text{Li, Y.} and \text{Liang, Y.} (2018{\natexlab{a}}).
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock \textit{arXiv preprint arXiv:1811.04918}.

\bibitem[{Allen-Zhu et~al.(2018{\natexlab{b}})Allen-Zhu, Li and
  Song}]{allen2018convergence}
\text{Allen-Zhu, Z.}, \text{Li, Y.} and \text{Song, Z.} (2018{\natexlab{b}}).
\newblock A convergence theory for deep learning via over-parameterization.
\newblock \textit{arXiv preprint arXiv:1811.03962}.

\bibitem[{Amit and Meir(2017)}]{amit2017meta}
\text{Amit, R.} and \text{Meir, R.} (2017).
\newblock Meta-learning by adjusting priors based on extended {PAC}-{B}ayes
  theory.
\newblock \textit{arXiv preprint arXiv:1711.01244}.

\bibitem[{Arora et~al.(2019)Arora, Du, Hu, Li and Wang}]{arora2019fine}
\text{Arora, S.}, \text{Du, S.~S.}, \text{Hu, W.}, \text{Li, Z.} and
  \text{Wang, R.} (2019).
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock \textit{arXiv preprint arXiv:1901.08584}.

\bibitem[{Bai and Lee(2019)}]{bai2019beyond}
\text{Bai, Y.} and \text{Lee, J.~D.} (2019).
\newblock Beyond linearization: On quadratic and higher-order approximation of
  wide neural networks.
\newblock \textit{arXiv preprint arXiv:1910.01619}.

\bibitem[{Cai et~al.(2019)Cai, Yang, Lee and Wang}]{cai2019neural}
\text{Cai, Q.}, \text{Yang, Z.}, \text{Lee, J.~D.} and \text{Wang, Z.} (2019).
\newblock Neural temporal-difference learning converges to global optima.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Cao and Gu(2019{\natexlab{a}})}]{cao2019bounds}
\text{Cao, Y.} and \text{Gu, Q.} (2019{\natexlab{a}}).
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock \textit{arXiv preprint arXiv:1905.13210}.

\bibitem[{Cao and Gu(2019{\natexlab{b}})}]{cao2019generalization}
\text{Cao, Y.} and \text{Gu, Q.} (2019{\natexlab{b}}).
\newblock A generalization theory of gradient descent for learning
  over-parameterized deep {ReLU} networks.
\newblock \textit{arXiv preprint arXiv:1902.01384}.

\bibitem[{Chizat and Bach(2018)}]{chizat2018note}
\text{Chizat, L.} and \text{Bach, F.} (2018).
\newblock A note on lazy training in supervised differentiable programming.
\newblock \textit{arXiv preprint arXiv:1812.07956}.

\bibitem[{Daniely(2017)}]{daniely2017sgd}
\text{Daniely, A.} (2017).
\newblock {SGD} learns the conjugate kernel class of the network.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Du et~al.(2018{\natexlab{a}})Du, Lee, Li, Wang and
  Zhai}]{du2018gradient1}
\text{Du, S.~S.}, \text{Lee, J.~D.}, \text{Li, H.}, \text{Wang, L.} and
  \text{Zhai, X.} (2018{\natexlab{a}}).
\newblock Gradient descent finds global minima of deep neural networks.
\newblock \textit{arXiv preprint arXiv:1811.03804}.

\bibitem[{Du et~al.(2018{\natexlab{b}})Du, Zhai, Poczos and
  Singh}]{du2018gradient}
\text{Du, S.~S.}, \text{Zhai, X.}, \text{Poczos, B.} and \text{Singh, A.}
  (2018{\natexlab{b}}).
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \textit{arXiv preprint arXiv:1810.02054}.

\bibitem[{Ekeland and Temam(1999)}]{ekeland1999convex}
\text{Ekeland, I.} and \text{Temam, R.} (1999).
\newblock \textit{Convex analysis and variational problems}, vol.~28.
\newblock SIAM.

\bibitem[{Evgeniou and Pontil(2004)}]{evgeniou2004regularized}
\text{Evgeniou, T.} and \text{Pontil, M.} (2004).
\newblock Regularized multi--task learning.
\newblock In \textit{International Conference on Knowledge Discovery and Data
  Mining}.

\bibitem[{Facchinei and Pang(2007)}]{facchinei2007finite}
\text{Facchinei, F.} and \text{Pang, J.-S.} (2007).
\newblock \textit{Finite-dimensional variational inequalities and
  complementarity problems}.
\newblock Springer Science \& Business Media.

\bibitem[{Fallah et~al.(2019)Fallah, Mokhtari and
  Ozdaglar}]{fallah2019convergence}
\text{Fallah, A.}, \text{Mokhtari, A.} and \text{Ozdaglar, A.} (2019).
\newblock On the convergence theory of gradient-based model-agnostic
  meta-learning algorithms.
\newblock \textit{arXiv preprint arXiv:1908.10400}.

\bibitem[{Fan et~al.(2019)Fan, Ma and Zhong}]{fan2019selective}
\text{Fan, J.}, \text{Ma, C.} and \text{Zhong, Y.} (2019).
\newblock A selective overview of deep learning.
\newblock \textit{arXiv preprint arXiv:1904.05526}.

\bibitem[{Finn et~al.(2017{\natexlab{a}})Finn, Abbeel and
  Levine}]{finn2017model}
\text{Finn, C.}, \text{Abbeel, P.} and \text{Levine, S.} (2017{\natexlab{a}}).
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Finn et~al.(2019)Finn, Rajeswaran, Kakade and
  Levine}]{finn2019online}
\text{Finn, C.}, \text{Rajeswaran, A.}, \text{Kakade, S.} and \text{Levine, S.}
  (2019).
\newblock Online meta-learning.
\newblock \textit{arXiv preprint arXiv:1902.08438}.

\bibitem[{Finn et~al.(2018)Finn, Xu and Levine}]{finn2018probabilistic}
\text{Finn, C.}, \text{Xu, K.} and \text{Levine, S.} (2018).
\newblock Probabilistic model-agnostic meta-learning.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Finn et~al.(2017{\natexlab{b}})Finn, Yu, Zhang, Abbeel and
  Levine}]{finn2017one}
\text{Finn, C.}, \text{Yu, T.}, \text{Zhang, T.}, \text{Abbeel, P.} and
  \text{Levine, S.} (2017{\natexlab{b}}).
\newblock One-shot visual imitation learning via meta-learning.
\newblock \textit{arXiv preprint arXiv:1709.04905}.

\bibitem[{Gupta et~al.(2018)Gupta, Mendonca, Liu, Abbeel and
  Levine}]{gupta2018meta}
\text{Gupta, A.}, \text{Mendonca, R.}, \text{Liu, Y.}, \text{Abbeel, P.} and
  \text{Levine, S.} (2018).
\newblock Meta-reinforcement learning of structured exploration strategies.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel and
  Levine}]{haarnoja2017reinforcement}
\text{Haarnoja, T.}, \text{Tang, H.}, \text{Abbeel, P.} and \text{Levine, S.}
  (2017).
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Jacot et~al.(2018)Jacot, Gabriel and Hongler}]{jacot2018neural}
\text{Jacot, A.}, \text{Gabriel, F.} and \text{Hongler, C.} (2018).
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Kakade and Langford(2002)}]{kakade2002approximately}
\text{Kakade, S.} and \text{Langford, J.} (2002).
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Khodak et~al.(2019)Khodak, Balcan and Talwalkar}]{khodak2019provable}
\text{Khodak, M.}, \text{Balcan, M.-F.} and \text{Talwalkar, A.} (2019).
\newblock Provable guarantees for gradient-based meta-learning.
\newblock \textit{arXiv preprint arXiv:1902.10644}.

\bibitem[{Konda(2002)}]{konda2002actor}
\text{Konda, V.} (2002).
\newblock \textit{Actor-Critic Algorithms}.
\newblock Ph.D. thesis, Massachusetts Institute of Technology.

\bibitem[{Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Sohl-Dickstein and
  Pennington}]{lee2019wide}
\text{Lee, J.}, \text{Xiao, L.}, \text{Schoenholz, S.~S.}, \text{Bahri, Y.},
  \text{Sohl-Dickstein, J.} and \text{Pennington, J.} (2019).
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \textit{arXiv preprint arXiv:1902.06720}.

\bibitem[{Li and Liang(2018)}]{li2018learning}
\text{Li, Y.} and \text{Liang, Y.} (2018).
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Li et~al.(2017)Li, Zhou, Chen and Li}]{li2017meta}
\text{Li, Z.}, \text{Zhou, F.}, \text{Chen, F.} and \text{Li, H.} (2017).
\newblock Meta-{SGD}: Learning to learn quickly for few-shot learning.
\newblock \textit{arXiv preprint arXiv:1707.09835}.

\bibitem[{Liu et~al.(2019)Liu, Cai, Yang and Wang}]{liu2019neural}
\text{Liu, B.}, \text{Cai, Q.}, \text{Yang, Z.} and \text{Wang, Z.} (2019).
\newblock Neural proximal/trust region policy optimization attains globally
  optimal policy.
\newblock \textit{arXiv preprint arXiv:1906.10306}.

\bibitem[{Mendonca et~al.(2019)Mendonca, Gupta, Kralev, Abbeel, Levine and
  Finn}]{mendonca2019guided}
\text{Mendonca, R.}, \text{Gupta, A.}, \text{Kralev, R.}, \text{Abbeel, P.},
  \text{Levine, S.} and \text{Finn, C.} (2019).
\newblock Guided meta-policy search.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Nagabandi et~al.(2018)Nagabandi, Finn and Levine}]{nagabandi2018deep}
\text{Nagabandi, A.}, \text{Finn, C.} and \text{Levine, S.} (2018).
\newblock Deep online learning via meta-learning: Continual adaptation for
  model-based {RL}.
\newblock \textit{arXiv preprint arXiv:1812.07671}.

\bibitem[{Nichol et~al.(2018)Nichol, Achiam and Schulman}]{nichol2018first}
\text{Nichol, A.}, \text{Achiam, J.} and \text{Schulman, J.} (2018).
\newblock On first-order meta-learning algorithms.
\newblock \textit{arXiv preprint arXiv:1803.02999}.

\bibitem[{Nichol and Schulman(2018)}]{nichol2018reptile}
\text{Nichol, A.} and \text{Schulman, J.} (2018).
\newblock Reptile: A scalable meta-learning algorithm.
\newblock \textit{arXiv preprint arXiv:1803.02999}, \textbf{2} 2.

\bibitem[{Pan and Yang(2009)}]{pan2009survey}
\text{Pan, S.~J.} and \text{Yang, Q.} (2009).
\newblock A survey on transfer learning.
\newblock \textit{IEEE Transactions on knowledge and data engineering},
  \textbf{22} 1345--1359.

\bibitem[{Pentina and Lampert(2014)}]{pentina2014pac}
\text{Pentina, A.} and \text{Lampert, C.} (2014).
\newblock A {PAC}-{B}ayesian bound for lifelong learning.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Rajeswaran et~al.(2019)Rajeswaran, Finn, Kakade and
  Levine}]{rajeswaran2019meta}
\text{Rajeswaran, A.}, \text{Finn, C.}, \text{Kakade, S.~M.} and \text{Levine,
  S.} (2019).
\newblock Meta-learning with implicit gradients.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Rakelly et~al.(2018)Rakelly, Shelhamer, Darrell, Efros and
  Levine}]{rakelly2018few}
\text{Rakelly, K.}, \text{Shelhamer, E.}, \text{Darrell, T.}, \text{Efros,
  A.~A.} and \text{Levine, S.} (2018).
\newblock Few-shot segmentation propagation with guided networks.
\newblock \textit{arXiv preprint arXiv:1806.07373}.

\bibitem[{Rockafellar(1968)}]{rockafellar1968integrals}
\text{Rockafellar, R.} (1968).
\newblock Integrals which are convex functionals.
\newblock \textit{Pacific journal of mathematics}, \textbf{24} 525--539.

\bibitem[{Rudin(2006)}]{rudin2006real}
\text{Rudin, W.} (2006).
\newblock \textit{Real and complex analysis}.
\newblock McGraw-Hill.

\bibitem[{Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan and
  Moritz}]{schulman2015trust}
\text{Schulman, J.}, \text{Levine, S.}, \text{Abbeel, P.}, \text{Jordan, M.}
  and \text{Moritz, P.} (2015).
\newblock Trust region policy optimization.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford and
  Klimov}]{schulman2017proximal}
\text{Schulman, J.}, \text{Wolski, F.}, \text{Dhariwal, P.}, \text{Radford, A.}
  and \text{Klimov, O.} (2017).
\newblock Proximal policy optimization algorithms.
\newblock \textit{arXiv preprint arXiv:1707.06347}.

\bibitem[{Sutton(1988)}]{sutton1988learning}
\text{Sutton, R.~S.} (1988).
\newblock Learning to predict by the methods of temporal differences.
\newblock \textit{Machine learning}, \textbf{3} 9--44.

\bibitem[{Sutton and Barto(2018)}]{sutton2018reinforcement}
\text{Sutton, R.~S.} and \text{Barto, A.~G.} (2018).
\newblock \textit{Reinforcement learning: An introduction}.
\newblock MIT press.

\bibitem[{Taylor and Stone(2009)}]{taylor2009transfer}
\text{Taylor, M.~E.} and \text{Stone, P.} (2009).
\newblock Transfer learning for reinforcement learning domains: A survey.
\newblock \textit{Journal of Machine Learning Research}, \textbf{10}
  1633--1685.

\bibitem[{Thrun and Pratt(2012)}]{thrun2012learning}
\text{Thrun, S.} and \text{Pratt, L.} (2012).
\newblock \textit{Learning to learn}.
\newblock Springer Science \& Business Media.

\bibitem[{Wang et~al.(2020)Wang, Sun and Li}]{meta-learning-convergence-kernel}
\text{Wang, H.}, \text{Sun, R.} and \text{Li, B.} (2020).
\newblock Global convergence and induced kernels of gradient-based
  meta-learning with neural nets.
\newblock \textit{To appear on arXiv}.

\bibitem[{Weiss et~al.(2016)Weiss, Khoshgoftaar and Wang}]{weiss2016survey}
\text{Weiss, K.}, \text{Khoshgoftaar, T.~M.} and \text{Wang, D.} (2016).
\newblock A survey of transfer learning.
\newblock \textit{Journal of Big Data}, \textbf{3} 9.

\bibitem[{Wu et~al.(2018)Wu, Ma and Weinan}]{wu2018sgd}
\text{Wu, L.}, \text{Ma, C.} and \text{Weinan, E.} (2018).
\newblock How {SGD} selects the global minima in over-parameterized learning: A
  dynamical stability perspective.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Xu et~al.(2018)Xu, Ratner, Dragan, Levine and Finn}]{xu2018learning}
\text{Xu, K.}, \text{Ratner, E.}, \text{Dragan, A.}, \text{Levine, S.} and
  \text{Finn, C.} (2018).
\newblock Learning a prior over intent via meta-inverse reinforcement learning.
\newblock \textit{arXiv preprint arXiv:1805.12573}.

\bibitem[{Yoon et~al.(2018)Yoon, Kim, Dia, Kim, Bengio and
  Ahn}]{yoon2018bayesian}
\text{Yoon, J.}, \text{Kim, T.}, \text{Dia, O.}, \text{Kim, S.}, \text{Bengio,
  Y.} and \text{Ahn, S.} (2018).
\newblock Bayesian model-agnostic meta-learning.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Yu et~al.(2018)Yu, Abbeel, Levine and Finn}]{yu2018one}
\text{Yu, T.}, \text{Abbeel, P.}, \text{Levine, S.} and \text{Finn, C.} (2018).
\newblock One-shot hierarchical imitation learning of compound visuomotor
  tasks.
\newblock \textit{arXiv preprint arXiv:1810.11043}.

\bibitem[{Zou et~al.(2018)Zou, Cao, Zhou and Gu}]{zou2018stochastic}
\text{Zou, D.}, \text{Cao, Y.}, \text{Zhou, D.} and \text{Gu, Q.} (2018).
\newblock Stochastic gradient descent optimizes over-parameterized deep {ReLU}
  networks.
\newblock \textit{arXiv preprint arXiv:1811.08888}.

\end{thebibliography}
