\begin{thebibliography}{}

\bibitem[Arora et~al., 2019a]{arora2018convergence}
Arora, S., Cohen, N., Golowich, N., and Hu, W. (2019a).
\newblock A convergence analysis of gradient descent for deep linear neural
  networks.
\newblock {\em International Conference on Learning Representations}.

\bibitem[Arora et~al., 2019b]{arora2018theoretical}
Arora, S., Li, Z., and Lyu, K. (2019b).
\newblock Theoretical analysis of auto rate-tuning by batch normalization.
\newblock {\em International Conference on Learning Representations}.

\bibitem[Bahri et~al., 2020]{bahri2020statistical}
Bahri, Y., Kadmon, J., Pennington, J., Schoenholz, S.~S., Sohl-Dickstein, J.,
  and Ganguli, S. (2020).
\newblock Statistical mechanics of deep learning.
\newblock {\em Annual Review of Condensed Matter Physics}.

\bibitem[Bartlett et~al., 2019]{bartlett2019gradient}
Bartlett, P.~L., Helmbold, D.~P., and Long, P.~M. (2019).
\newblock Gradient descent with identity initialization efficiently learns
  positive-definite linear transformations by deep residual networks.
\newblock {\em Neural computation}.

\bibitem[Bietti and Mairal, 2019]{bietti2019inductive}
Bietti, A. and Mairal, J. (2019).
\newblock On the inductive bias of neural tangent kernels.
\newblock {\em Advances in Neural Information Processing Systems}.

\bibitem[Bjorck et~al., 2018]{bjorck2018understanding}
Bjorck, J., Gomes, C., Selman, B., and Weinberger, K.~Q. (2018).
\newblock Understanding batch normalization.
\newblock {\em Advances in Neural Information Processing Systems}.

\bibitem[Bougerol et~al., 2012]{bougerol2012products}
Bougerol, P. et~al. (2012).
\newblock {\em Products of random matrices with applications to Schr{\"o}dinger
  operators}.
\newblock Springer Science \& Business Media.

\bibitem[Daneshmand et~al., 2020]{daneshmand2020batch}
Daneshmand, H., Kohler, J., Bach, F., Hofmann, T., and Lucchi, A. (2020).
\newblock Batch normalization provably avoids rank collapse for randomly
  initialised deep networks.
\newblock {\em Advances in Neural Information Processing Systems}.

\bibitem[De~Palma et~al., 2019]{de2018random}
De~Palma, G., Kiani, B.~T., and Lloyd, S. (2019).
\newblock Random deep neural networks are biased towards simple functions.
\newblock {\em Advances in Neural Information Processing Systems}.

\bibitem[Eberle, 2009]{eberle2009markov}
Eberle, A. (2009).
\newblock Markov processes.
\newblock {\em Lecture Notes at University of Bonn}.

\bibitem[Frankle et~al., 2020]{frankle2020training}
Frankle, J., Schwab, D.~J., and Morcos, A.~S. (2020).
\newblock Training batchnorm and only batchnorm: On the expressive power of
  random features in cnns.
\newblock {\em arXiv preprint arXiv:2003.00152}.

\bibitem[Garriga-Alonso et~al., 2019]{garriga2018deep}
Garriga-Alonso, A., Rasmussen, C.~E., and Aitchison, L. (2019).
\newblock Deep convolutional networks as shallow gaussian processes.
\newblock {\em International Conference on Learning Representations}.

\bibitem[Glorot and Bengio, 2010]{glorot2010understanding}
Glorot, X. and Bengio, Y. (2010).
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}.

\bibitem[Hazan and Jaakkola, 2015]{hazan2015steps}
Hazan, T. and Jaakkola, T. (2015).
\newblock Steps toward deep kernel methods from infinite neural networks.
\newblock {\em arXiv preprint arXiv:1508.05133}.

\bibitem[He et~al., 2016]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J. (2016).
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}.

\bibitem[Huang et~al., 2017]{huang2017densely}
Huang, G., Liu, Z., Van Der~Maaten, L., and Weinberger, K.~Q. (2017).
\newblock Densely connected convolutional networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}.

\bibitem[Huang et~al., 2014]{huang2014kernel}
Huang, P.-S., Avron, H., Sainath, T.~N., Sindhwani, V., and Ramabhadran, B.
  (2014).
\newblock Kernel methods match deep neural networks on timit.
\newblock In {\em ICASSP}.

\bibitem[Ioffe and Szegedy, 2015]{ioffe2015batch}
Ioffe, S. and Szegedy, C. (2015).
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em ICML}.

\bibitem[Jacot et~al., 2018]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C. (2018).
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock {\em Advances in Neural Information Processing Systems}.

\bibitem[Karakida et~al., 2019]{karakida2019normalization}
Karakida, R., Akaho, S., and Amari, S.-i. (2019).
\newblock The normalization method for alleviating pathological sharpness in
  wide neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Kemeny and Snell, 1976]{kemeny1976markov}
Kemeny, J.~G. and Snell, J.~L. (1976).
\newblock {\em Markov chains}.
\newblock Springer-Verlag, New York.

\bibitem[Khasminskii, 2011]{khasminskii2011stochastic}
Khasminskii, R. (2011).
\newblock {\em Stochastic stability of differential equations}, volume~66.
\newblock Springer Science \& Business Media.

\bibitem[Klambauer et~al., 2017]{klambauer2017self}
Klambauer, G., Unterthiner, T., Mayr, A., and Hochreiter, S. (2017).
\newblock Self-normalizing neural networks.
\newblock {\em Advances in Neural Information Processing Systems}.

\bibitem[Kohler et~al., 2018]{kohler2018exponential}
Kohler, J., Daneshmand, H., Lucchi, A., Zhou, M., Neymeyr, K., and Hofmann, T.
  (2018).
\newblock Exponential convergence rates for batch normalization: The power of
  length-direction decoupling in non-convex optimization.
\newblock {\em arXiv preprint arXiv:1805.10694}.

\bibitem[Krizhevsky et~al., 2009]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al. (2009).
\newblock Learning multiple layers of features from tiny images.

\bibitem[Kushner and Yin, 2003]{kushner2003stochastic}
Kushner, H. and Yin, G.~G. (2003).
\newblock {\em Stochastic approximation and recursive algorithms and
  applications}.
\newblock Springer Science \& Business Media.

\bibitem[Kushner, 1967]{kushner1967stochastic}
Kushner, H.~J. (1967).
\newblock Stochastic stability and control.
\newblock Technical report.

\bibitem[Lee et~al., 2019]{lee2017deep}
Lee, J., Bahri, Y., Novak, R., Schoenholz, S.~S., Pennington, J., and
  Sohl-Dickstein, J. (2019).
\newblock Deep neural networks as gaussian processes.
\newblock {\em International Conference on Learning Representations}.

\bibitem[Matthews et~al., 2018]{matthews2018gaussian}
Matthews, A. G. d.~G., Rowland, M., Hron, J., Turner, R.~E., and Ghahramani, Z.
  (2018).
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock {\em arXiv preprint arXiv:1804.11271}.

\bibitem[Neal, 1996]{neal2012bayesian}
Neal, R.~M. (1996).
\newblock {\em Bayesian learning for neural networks}.
\newblock Springer Science \& Business Media.

\bibitem[Paszke et~al., 2019]{NEURIPS2019_9015}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S. (2019).
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Pennington et~al., 2018]{pennington2018emergence}
Pennington, J., Schoenholz, S., and Ganguli, S. (2018).
\newblock The emergence of spectral universality in deep networks.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}.

\bibitem[Santurkar et~al., 2018]{santurkar2018does}
Santurkar, S., Tsipras, D., Ilyas, A., and Madry, A. (2018).
\newblock How does batch normalization help optimization?(no, it is not about
  internal covariate shift).
\newblock {\em Advances in Neural Information Processing Systems}.

\bibitem[Sawa, 1972]{sawa1972finite}
Sawa, T. (1972).
\newblock Finite-sample properties of the k-class estimators.
\newblock {\em Econometrica: Journal of the Econometric Society}, pages
  653--680.

\bibitem[Saxe et~al., 2014]{saxe2013exact}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S. (2014).
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock {\em International Conference on Learning Representations}.

\bibitem[Schoenholz et~al., 2017]{schoenholz2016deep}
Schoenholz, S.~S., Gilmer, J., Ganguli, S., and Sohl-Dickstein, J. (2017).
\newblock Deep information propagation.
\newblock {\em International Conference on Learning Representations}.

\bibitem[Silver et~al., 2017]{silver2017mastering}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
  A., Hubert, T., Baker, L., Lai, M., Bolton, A., et~al. (2017).
\newblock Mastering the game of go without human knowledge.
\newblock {\em nature}.

\bibitem[Srivastava et~al., 2014]{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R. (2014).
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em The Journal of Machine Learning Research}.

\bibitem[Yang et~al., 2019]{yang2019mean}
Yang, G., Pennington, J., Rao, V., Sohl-Dickstein, J., and Schoenholz, S.~S.
  (2019).
\newblock A mean field theory of batch normalization.
\newblock {\em International Conference on Learning Representations}.

\end{thebibliography}
