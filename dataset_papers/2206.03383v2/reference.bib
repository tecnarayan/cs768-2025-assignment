@article{uehara2021pessimistic,
  title={Pessimistic Model-based Offline Reinforcement Learning under Partial Coverage},
  author={Uehara, Masatoshi and Sun, Wen},
  journal={arXiv preprint arXiv:2107.06226},
  year={2021}
}

@inproceedings{jin2021pessimism,
  title={Is pessimism provably efficient for offline rl?},
  author={Jin, Ying and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={5084--5096},
  year={2021},
  organization={PMLR}
}

@article{kumar2020conservative,
  title={Conservative q-learning for offline reinforcement learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.04779},
  year={2020}
}

@inproceedings{fujimoto2019off,
  title={Off-policy deep reinforcement learning without exploration},
  author={Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle={International Conference on Machine Learning},
  pages={2052--2062},
  year={2019},
  organization={PMLR}
}

@article{ma2021offline,
  title={Offline Reinforcement Learning with Value-based Episodic Memory},
  author={Ma, Xiaoteng and Yang, Yiqin and Hu, Hao and Liu, Qihan and Yang, Jun and Zhang, Chongjie and Zhao, Qianchuan and Liang, Bin},
  journal={arXiv preprint arXiv:2110.09796},
  year={2021}
}

@article{rathnam2021comparison,
  title={Comparison and Unification of Three Regularization Methods in Batch Reinforcement Learning},
  author={Rathnam, Sarah and Murphy, Susan A and Doshi-Velez, Finale},
  journal={arXiv preprint arXiv:2109.08134},
  year={2021}
}

@inproceedings{amit2020discount,
  title={Discount factor as a regularizer in reinforcement learning},
  author={Amit, Ron and Meir, Ron and Ciosek, Kamil},
  booktitle={International conference on machine learning},
  pages={269--278},
  year={2020},
  organization={PMLR}
}

@inproceedings{jiang2015dependence,
  title={The dependence of effective planning horizon on model accuracy},
  author={Jiang, Nan and Kulesza, Alex and Singh, Satinder and Lewis, Richard},
  booktitle={Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
  pages={1181--1189},
  year={2015},
  organization={Citeseer}
}

@article{petrik2008biasing,
  title={Biasing approximate dynamic programming with a lower discount factor},
  author={Petrik, Marek and Scherrer, Bruno},
  journal={Advances in neural information processing systems},
  volume={21},
  pages={1265--1272},
  year={2008}
}

@inproceedings{chen2018improving,
  title={Improving offline value-function approximations for pomdps by reducing discount factors},
  author={Chen, Yi-Chun and Kochenderfer, Mykel J and Spaan, Matthijs TJ},
  booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={3531--3536},
  year={2018},
  organization={IEEE}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@article{shalev2016safe,
  title={Safe, multi-agent, reinforcement learning for autonomous driving},
  author={Shalev-Shwartz, Shai and Shammah, Shaked and Shashua, Amnon},
  journal={arXiv preprint arXiv:1610.03295},
  year={2016}
}

@article{singh2020cog,
  title={Cog: Connecting new skills to past experience with offline reinforcement learning},
  author={Singh, Avi and Yu, Albert and Yang, Jonathan and Zhang, Jesse and Kumar, Aviral and Levine, Sergey},
  journal={arXiv preprint arXiv:2010.14500},
  year={2020}
}


@article{swaminathan2015batch,
  title={Batch learning from logged bandit feedback through counterfactual risk minimization},
  author={Swaminathan, Adith and Joachims, Thorsten},
  journal={The Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={1731--1755},
  year={2015},
  publisher={JMLR. org}
}


@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}


@article{peng2019advantage,
  title={Advantage-weighted regression: Simple and scalable off-policy reinforcement learning},
  author={Peng, Xue Bin and Kumar, Aviral and Zhang, Grace and Levine, Sergey},
  journal={arXiv preprint arXiv:1910.00177},
  year={2019}
}

@article{yu2021combo,
  title={Combo: Conservative offline model-based policy optimization},
  author={Yu, Tianhe and Kumar, Aviral and Rafailov, Rafael and Rajeswaran, Aravind and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:2102.08363},
  year={2021}
}

@article{fujimoto2021td3,
  title={A Minimalist Approach to Offline Reinforcement Learning},
  author={Fujimoto, Scott and Gu, Shixiang Shane},
  journal={arXiv preprint arXiv:2106.06860},
  year={2021}
}

@article{nair2020accelerating,
  title={Accelerating online reinforcement learning with offline datasets},
  author={Nair, Ashvin and Dalal, Murtaza and Gupta, Abhishek and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.09359},
  year={2020}
}

@article{wu2021uncertainty,
  title={Uncertainty Weighted Actor-Critic for Offline Reinforcement Learning},
  author={Wu, Yue and Zhai, Shuangfei and Srivastava, Nitish and Susskind, Joshua and Zhang, Jian and Salakhutdinov, Ruslan and Goh, Hanlin},
  journal={arXiv preprint arXiv:2105.08140},
  year={2021}
}

@article{an2021uncertainty,
  title={Uncertainty-based offline reinforcement learning with diversified q-ensemble},
  author={An, Gaon and Moon, Seungyong and Kim, Jang-Hyun and Song, Hyun Oh},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{yu2020mopo,
  title={Mopo: Model-based offline policy optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  journal={arXiv preprint arXiv:2005.13239},
  year={2020}
}

@article{kidambi2020morel,
  title={Morel: Model-based offline reinforcement learning},
  author={Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  journal={arXiv preprint arXiv:2005.05951},
  year={2020}
}

@article{fedus2019hyperbolic,
  title={Hyperbolic discounting and learning over multiple horizons},
  author={Fedus, William and Gelada, Carles and Bengio, Yoshua and Bellemare, Marc G and Larochelle, Hugo},
  journal={arXiv preprint arXiv:1902.06865},
  year={2019}
}

@article{rashidinejad2021bridging,
  title={Bridging offline reinforcement learning and imitation learning: A tale of pessimism},
  author={Rashidinejad, Paria and Zhu, Banghua and Ma, Cong and Jiao, Jiantao and Russell, Stuart},
  journal={arXiv preprint arXiv:2103.12021},
  year={2021}
}

@inproceedings{yang2019sample,
  title={Sample-optimal parametric Q-learning using linearly additive features},
  author={Yang, Lin and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={6995--7004},
  year={2019},
  organization={PMLR}
}

@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2137--2143},
  year={2020},
  organization={PMLR}
}

@article{abbasi2011improved,
  title={Improved algorithms for linear stochastic bandits},
  author={Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
  journal={Advances in neural information processing systems},
  volume={24},
  pages={2312--2320},
  year={2011}
}

@article{fu2020d4rl,
  title={D4rl: Datasets for deep data-driven reinforcement learning},
  author={Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2004.07219},
  year={2020}
}

@article{xu2018meta,
  title={Meta-Gradient Reinforcement Learning},
  author={Xu, Zhongwen and van Hasselt, Hado P and Silver, David},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  pages={2396--2407},
  year={2018}
}

@inproceedings{sherstan2020gamma,
  title={Gamma-nets: Generalizing value estimation over timescale},
  author={Sherstan, Craig and Dohare, Shibhansh and MacGlashan, James and G{\"u}nther, Johannes and Pilarski, Patrick M},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={5717--5725},
  year={2020}
}

@inproceedings{romoff2019separating,
  title={Separating value functions across time-scales},
  author={Romoff, Joshua and Henderson, Peter and Touati, Ahmed and Brunskill, Emma and Pineau, Joelle and Ollivier, Yann},
  booktitle={International Conference on Machine Learning},
  pages={5468--5477},
  year={2019},
  organization={PMLR}
}

@article{yang2021believe,
  title={Believe What You See: Implicit Constraint Approach for Offline Multi-Agent Reinforcement Learning},
  author={Yang, Yiqin and Ma, Xiaoteng and Li, Chenghao and Zheng, Zewu and Zhang, Qiyuan and Huang, Gao and Yang, Jun and Zhao, Qianchuan},
  journal={arXiv preprint arXiv:2106.03400},
  year={2021}
}

@inproceedings{ghasemipour2021emaq,
  title={Emaq: Expected-max q-learning operator for simple yet effective offline and online rl},
  author={Ghasemipour, Seyed Kamyar Seyed and Schuurmans, Dale and Gu, Shixiang Shane},
  booktitle={International Conference on Machine Learning},
  pages={3682--3691},
  year={2021},
  organization={PMLR}
}

@article{siegel2020keep,
  title={Keep doing what worked: Behavioral modelling priors for offline reinforcement learning},
  author={Siegel, Noah Y and Springenberg, Jost Tobias and Berkenkamp, Felix and Abdolmaleki, Abbas and Neunert, Michael and Lampe, Thomas and Hafner, Roland and Heess, Nicolas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:2002.08396},
  year={2020}
}

@article{kostrikov2021offline,
  title={Offline reinforcement learning with implicit q-learning},
  author={Kostrikov, Ilya and Nair, Ashvin and Levine, Sergey},
  journal={arXiv preprint arXiv:2110.06169},
  year={2021}
}

@book{geer2000empirical,
  title={Empirical Processes in M-estimation},
  author={Geer, Sara A and van de Geer, Sara and Williams, D},
  volume={6},
  year={2000},
  publisher={Cambridge university press}
}

@inproceedings{agarwal2020optimistic,
  title={An optimistic perspective on offline reinforcement learning},
  author={Agarwal, Rishabh and Schuurmans, Dale and Norouzi, Mohammad},
  booktitle={International Conference on Machine Learning},
  pages={104--114},
  year={2020},
  organization={PMLR}
}

@article{wu2019behavior,
  title={Behavior regularized offline reinforcement learning},
  author={Wu, Yifan and Tucker, George and Nachum, Ofir},
  journal={arXiv preprint arXiv:1911.11361},
  year={2019}
}

@article{xie2021bellman,
  title={Bellman-consistent Pessimism for Offline Reinforcement Learning},
  author={Xie, Tengyang and Cheng, Ching-An and Jiang, Nan and Mineiro, Paul and Agarwal, Alekh},
  journal={arXiv preprint arXiv:2106.06926},
  year={2021}
}

@article{brandfonbrener2021offline,
  title={Offline RL Without Off-Policy Evaluation},
  author={Brandfonbrener, David and Whitney, William F and Ranganath, Rajesh and Bruna, Joan},
  journal={arXiv preprint arXiv:2106.08909},
  year={2021}
}

@article{kumar2019stabilizing,
  title={Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction},
  author={Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={11784--11794},
  year={2019}
}

@article{argenson2020model,
  title={Model-based offline planning},
  author={Argenson, Arthur and Dulac-Arnold, Gabriel},
  journal={arXiv preprint arXiv:2008.05556},
  year={2020}
}

@article{zhang2020deeper,
  title={A deeper look at discounting mismatch in actor-critic algorithms},
  author={Zhang, Shangtong and Laroche, Romain and van Seijen, Harm and Whiteson, Shimon and Combes, Remi Tachet des},
  journal={arXiv preprint arXiv:2010.01069},
  year={2020}
}

@book{wainwright2019high,
  title={High-dimensional statistics: A non-asymptotic viewpoint},
  author={Wainwright, Martin J},
  volume={48},
  year={2019},
  publisher={Cambridge University Press}
}

@article{hong2020two,
  title={A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic},
  author={Hong, Mingyi and Wai, Hoi-To and Wang, Zhaoran and Yang, Zhuoran},
  journal={arXiv preprint arXiv:2007.05170},
  year={2020}
}
