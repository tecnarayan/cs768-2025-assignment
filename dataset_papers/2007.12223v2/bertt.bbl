\begin{thebibliography}{10}

\bibitem{gpt2}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI Blog}, 1(8):9, 2019.

\bibitem{turing}
Corby Rosset.
\newblock Turing-nlg: A 17-billion-parameter language model by microsoft.
\newblock {\em Microsoft Research Blog}, 2(13), 2020.

\bibitem{gpt3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
  Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter,
  Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
  Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
  Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners, 2020.

\bibitem{strubell-acl}
Emma Strubell, Ananya Ganesh, and Andrew McCallum.
\newblock Energy and policy considerations for deep learning in {NLP}.
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pages 3645--3650, Florence, Italy, July 2019.
  Association for Computational Linguistics.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{jin2019bert}
Di~Jin, Zhijing Jin, Joey~Tianyi Zhou, and Peter Szolovits.
\newblock Is bert really robust? natural language attack on text classification
  and entailment.
\newblock {\em arXiv preprint arXiv:1907.11932}, 2, 2019.

\bibitem{moco}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In {\em The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2020.

\bibitem{simclr}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock {\em arXiv preprint arXiv:2002.05709}, 2020.

\bibitem{erhan2010does}
Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal
  Vincent, and Samy Bengio.
\newblock Why does unsupervised pre-training help deep learning?
\newblock {\em Journal of Machine Learning Research}, 11(Feb):625--660, 2010.

\bibitem{snip}
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr.
\newblock {SNIP}: {SINGLE}-{SHOT} {NETWORK} {PRUNING} {BASED} {ON} {CONNECTION}
  {SENSITIVITY}.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{dettmers}
Tim Dettmers and Luke Zettlemoyer.
\newblock Sparse networks from scratch: Faster training without losing
  performance.
\newblock {\em arXiv preprint arXiv:1907.04840}, 2019.

\bibitem{grasp}
Chaoqi Wang, Guodong Zhang, and Roger Grosse.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{evci2019rigging}
Utku Evci, Trevor Gale, Jacob Menick, Pablo~Samuel Castro, and Erich Elsen.
\newblock Rigging the lottery: Making all tickets winners.
\newblock In {\em International Conference on Machine Learning}, 2020.

\bibitem{you2019drawing}
Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen,
  Yingyan Lin, Zhangyang Wang, and Richard~G Baraniuk.
\newblock Drawing early-bird tickets: Towards more efficient training of deep
  networks.
\newblock {\em arXiv preprint arXiv:1909.11957}, 2019.

\bibitem{frankle2018the}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{frankle2019lottery}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel~M Roy, and Michael Carbin.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In {\em International Conference on Machine Learning}, 2020.

\bibitem{yu2019playing}
Haonan Yu, Sergey Edunov, Yuandong Tian, and Ari~S Morcos.
\newblock Playing the lottery with rewards and multiple languages: lottery
  tickets in rl and nlp.
\newblock {\em arXiv preprint arXiv:1906.02768}, 2019.

\bibitem{Renda2020Comparing}
Alex Renda, Jonathan Frankle, and Michael Carbin.
\newblock Comparing rewinding and fine-tuning in neural network pruning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{Frankle2020The}
Jonathan Frankle, David~J. Schwab, and Ari~S. Morcos.
\newblock The early phase of neural network training.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{chen2020adversarial}
Tianlong Chen, Sijia Liu, Shiyu Chang, Yu~Cheng, Lisa Amini, and Zhangyang
  Wang.
\newblock Adversarial robustness: From self-supervised pre-training to
  fine-tuning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 699--708, 2020.

\bibitem{morcos2019one}
Ari Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian.
\newblock One ticket to win them all: generalizing lottery ticket
  initializations across datasets and optimizers.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4933--4943, 2019.

\bibitem{mehta2019sparse}
Rahul Mehta.
\newblock Sparse transfer learning via winning lottery tickets.
\newblock {\em arXiv preprint arXiv:1905.07785}, 2019.

\bibitem{Desai_2019}
Shrey Desai, Hongyuan Zhan, and Ahmed Aly.
\newblock Evaluating lottery tickets under distributional shifts.
\newblock {\em Proceedings of the 2nd Workshop on Deep Learning Approaches for
  Low-Resource NLP (DeepLo 2019)}, 2019.

\bibitem{gale2019state}
Trevor Gale, Erich Elsen, and Sara Hooker.
\newblock The state of sparsity in deep neural networks.
\newblock {\em arXiv preprint arXiv:1902.09574}, 2019.

\bibitem{prasanna2020bert}
Sai Prasanna, Anna Rogers, and Anna Rumshisky.
\newblock When bert plays the lottery, all tickets are winning.
\newblock {\em arXiv preprint arXiv:2005.00561}, 2020.

\bibitem{Fan2020Reducing}
Angela Fan, Edouard Grave, and Armand Joulin.
\newblock Reducing transformer depth on demand with structured dropout.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{guo2020reweighted}
Fu-Ming Guo, Sijia Liu, Finlay~S Mungall, Xue Lin, and Yanzhi Wang.
\newblock Reweighted proximal pruning for large-scale language representation.
\newblock {\em arXiv preprint arXiv:1909.12486}, 2019.

\bibitem{ganesh2020compressing}
Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad~Ali Khan, Yin Yang, Deming Chen,
  Marianne Winslett, Hassan Sajjad, and Preslav Nakov.
\newblock Compressing large-scale transformer-based models: A case study on
  bert.
\newblock {\em arXiv preprint arXiv:2002.11985}, 2020.

\bibitem{michel2019sixteen}
Paul Michel, Omer Levy, and Graham Neubig.
\newblock Are sixteen heads really better than one?
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  14014--14024, 2019.

\bibitem{mccarley2019structured}
J.~S. McCarley, Rishav Chakravarti, and Avirup Sil.
\newblock Structured pruning of a bert-based question answering model.
\newblock {\em arXiv preprint arXiv:1910.06360}, 2019.

\bibitem{zafrir2019q8bert}
Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
\newblock Q8bert: Quantized 8bit bert.
\newblock {\em arXiv preprint arXiv:1910.06188}, 2019.

\bibitem{shen2019qbert}
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
  Michael~W. Mahoney, and Kurt Keutzer.
\newblock Q-bert: Hessian based ultra low precision quantization of bert.
\newblock In {\em Association for the Advancement of Artificial Intelligence},
  pages 8815--8821, 2020.

\bibitem{wu2018deep}
Junru Wu, Yue Wang, Zhenyu Wu, Zhangyang Wang, Ashok Veeraraghavan, and Yingyan
  Lin.
\newblock Deep k-means: Re-training and parameter sharing with harder cluster
  assignments for compressing deep convolutions.
\newblock In {\em International Conference on Machine Learning}, pages
  5363--5372, 2018.

\bibitem{yang2019legonet}
Zhaohui Yang, Yunhe Wang, Chuanjian Liu, Hanting Chen, Chunjing Xu, Boxin Shi,
  Chao Xu, and Chang Xu.
\newblock Legonet: Efficient convolutional neural networks with lego filters.
\newblock In {\em International Conference on Machine Learning}, pages
  7005--7014, 2019.

\bibitem{Lan2020ALBERT:}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}, 2019.

\bibitem{chen2019data}
Hanting Chen, Yunhe Wang, Chang Xu, Zhaohui Yang, Chuanjian Liu, Boxin Shi,
  Chunjing Xu, Chao Xu, and Qi~Tian.
\newblock Data-free learning of student networks.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 3514--3522, 2019.

\bibitem{wang2018adversarial}
Yunhe Wang, Chang Xu, Chao Xu, and Dacheng Tao.
\newblock Adversarial learning of portable student networks.
\newblock In {\em Association for the Advancement of Artificial Intelligence},
  2018.

\bibitem{jiao2020tinybert}
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang
  Wang, and Qun Liu.
\newblock Tinybert: Distilling bert for natural language understanding.
\newblock {\em arXiv preprint arXiv:1909.10351}, 2019.

\bibitem{sun2020mobilebert}
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.
\newblock {M}obile{BERT}: a compact task-agnostic {BERT} for resource-limited
  devices.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}, pages 2158--2170, Online, July 2020. Association
  for Computational Linguistics.

\bibitem{tang2019distilling}
Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin.
\newblock Distilling task-specific knowledge from bert into simple neural
  networks.
\newblock {\em arXiv preprint arXiv:1903.12136}, 2019.

\bibitem{mukherjee2019distilling}
Subhabrata Mukherjee and Ahmed~Hassan Awadallah.
\newblock Distilling transformers into simple neural networks with unlabeled
  transfer data.
\newblock {\em arXiv preprint arXiv:1910.01769}, 2019.

\bibitem{liu2019attentive}
Linqing Liu, Huan Wang, Jimmy Lin, Richard Socher, and Caiming Xiong.
\newblock Attentive student meets multi-task teacher: Improved knowledge
  distillation for pretrained models.
\newblock {\em arXiv preprint arXiv:1911.03588}, 2019.

\bibitem{Sun_2019}
Siqi Sun, Yu~Cheng, Zhe Gan, and Jingjing Liu.
\newblock Patient knowledge distillation for bert model compression.
\newblock {\em Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, 2019.

\bibitem{blalock2020state}
Davis Blalock, Jose Javier~Gonzalez Ortiz, Jonathan Frankle, and John Guttag.
\newblock What is the state of neural network pruning?
\newblock {\em arXiv preprint arXiv:2003.03033}, 2020.

\bibitem{han2015deep}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock {\em arXiv preprint arXiv:1510.00149}, 2015.

\bibitem{liu2018rethinking}
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell.
\newblock Rethinking the value of network pruning.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{Wolf2019HuggingFacesTS}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, R'emi Louf, Morgan Funtowicz, and
  Jamie Brew.
\newblock Huggingface's transformers: State-of-the-art natural language
  processing.
\newblock {\em ArXiv}, abs/1910.03771, 2019.

\bibitem{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
  Samuel~R. Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{RajpurkarZLL16}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock Squad: 100, 000+ questions for machine comprehension of text.
\newblock In {\em EMNLP}, pages 2383--2392, 2016.

\bibitem{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{elsen2020fast}
Erich Elsen, Marat Dukhan, Trevor Gale, and Karen Simonyan.
\newblock Fast sparse convnets.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 14629--14638, 2020.

\end{thebibliography}
