\begin{thebibliography}{24}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Chang et~al.(2018)Chang, Meng, Haber, Ruthotto, Begert, and
  Holtham]{rev1}
Chang, B., Meng, L., Haber, E., Ruthotto, L., Begert, D., and Holtham, E.
\newblock Reversible architectures for arbitrarily deep residual neural
  networks.
\newblock In \emph{AAAI}, 2018.

\bibitem[Chang et~al.(2019)Chang, Chen, Haber, and Chi]{antisymmetricrnn}
Chang, B., Chen, M., Haber, E., and Chi, E.~H.
\newblock Antisymmetricrnn: A dynamical system view on recurrent neural
  networks.
\newblock In \emph{ICLR}, 2019.

\bibitem[Chen(2001)]{chen2001stability}
Chen, G.
\newblock Stability of nonlinear systems.
\newblock \emph{Wiley Encyclopedia of Electrical and Electronics Engineering},
  2001.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and Duvenaud]{neuralode}
Chen, R. T.~Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D.
\newblock {Neural Ordinary Differential Equations}.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Dupont et~al.(2019)Dupont, Doucet, and Teh]{aneuralode}
Dupont, E., Doucet, A., and Teh, Y.~W.
\newblock Augmented neural odes.
\newblock In \emph{NeurIPS}, pp.\  3134--3144, 2019.

\bibitem[E(2017)]{eproposal}
E, W.
\newblock A proposal on machine learning via dynamical systems.
\newblock \emph{Communications in Mathematics and Statistics}, 5\penalty0
  (1):\penalty0 1--11, 2017.

\bibitem[Haber \& Ruthotto(2017)Haber and Ruthotto]{rev2}
Haber, E. and Ruthotto, L.
\newblock Stable architectures for deep neural networks.
\newblock \emph{Inverse Problems}, 34\penalty0 (1):\penalty0 014004, 2017.

\bibitem[Han et~al.(2018)Han, Jentzen, and E]{Han8505}
Han, J., Jentzen, A., and E, W.
\newblock Solving high-dimensional partial differential equations using deep
  learning.
\newblock \emph{PNAS}, 115\penalty0 (34):\penalty0 8505--8510, 2018.

\bibitem[Hanshu et~al.(2019)Hanshu, Jiawei, Vincent, and
  Jiashi]{RobustNeuralODE}
Hanshu, Y., Jiawei, D., Vincent, T., and Jiashi, F.
\newblock On robustness of neural ordinary differential equations.
\newblock In \emph{ICLR}, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{ECCV}, pp.\  630--645. Springer, 2016.

\bibitem[Hendrycks \& Dietterich(2019)Hendrycks and Dietterich]{CIFAR-C}
Hendrycks, D. and Dietterich, T.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock \emph{ICLR}, 2019.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and Goldstein]{visualization}
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{NeurIPS}, pp.\  6389--6399, 2018.

\bibitem[Liu et~al.(2019)Liu, Si, Cao, Kumar, and Hsieh]{NeuralSDE}
Liu, X., Si, S., Cao, Q., Kumar, S., and Hsieh, C.-J.
\newblock Neural sde: Stabilizing neural ode networks with stochastic noise.
\newblock \emph{arXiv:1906.02355}, 2019.

\bibitem[Lu et~al.(2018)Lu, Zhong, Li, and Dong]{lu-lm}
Lu, Y., Zhong, A., Li, Q., and Dong, B.
\newblock {Beyond Finite Layer Neural Networks: Bridging Deep Architectures and
  Numerical Differential Equations}.
\newblock In \emph{ICML}, 2018.

\bibitem[Lu et~al.(2019)Lu, Li, He, Sun, Dong, Qin, Wang, and
  Liu]{lu-transformer}
Lu, Y., Li, Z., He, D., Sun, Z., Dong, B., Qin, T., Wang, L., and Liu, T.-Y.
\newblock Understanding and improving transformer from a multi-particle dynamic
  system point of view.
\newblock \emph{arXiv:1906.02762}, 2019.

\bibitem[Lyapunov(1992)]{lyapunov}
Lyapunov, A.~M.
\newblock The general problem of the stability of motion.
\newblock \emph{International journal of control}, 55\penalty0 (3):\penalty0
  531--534, 1992.

\bibitem[Reshniak \& Webster(2019)Reshniak and Webster]{ImplicitResNet}
Reshniak, V. and Webster, C.
\newblock Robust learning with implicit residual networks.
\newblock \emph{arXiv:1905.10479}, 2019.

\bibitem[Su et~al.(2018)Su, Zhang, Chen, Yi, Chen, and Gao]{compareCVmodels}
Su, D., Zhang, H., Chen, H., Yi, J., Chen, P.-Y., and Gao, Y.
\newblock Is robustness the cost of accuracy?--a comprehensive study on the
  robustness of 18 deep image classification models.
\newblock In \emph{ECCV}, pp.\  631--648, 2018.

\bibitem[Tao et~al.(2018)Tao, Sun, Du, and Liu]{nonlocal}
Tao, Y., Sun, Q., Du, Q., and Liu, W.
\newblock Nonlocal neural networks, nonlocal diffusion and nonlocal modeling.
\newblock In \emph{NeurIPS}, pp.\  496--506, 2018.

\bibitem[Wang et~al.(2019)Wang, Yuan, Shi, and Osher]{EnResNet}
Wang, B., Yuan, B., Shi, Z., and Osher, S.~J.
\newblock Enresnet: Resnet ensemble via the feynman-kac formalism.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'a}r, Tu, and He]{resnext}
Xie, S., Girshick, R., Doll{\'a}r, P., Tu, Z., and He, K.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In \emph{CVPR}, pp.\  1492--1500, 2017.

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, Zhang, Lu, Zhu, and
  Dong]{lu-adv}
Zhang, D., Zhang, T., Lu, Y., Zhu, Z., and Dong, B.
\newblock You only propagate once: Painless adversarial training using maximal
  principle.
\newblock In \emph{NeurIPS}, 2019{\natexlab{a}}.

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Han, Wynter, Low, and
  Kankanhalli]{smallResNet}
Zhang, J., Han, B., Wynter, L., Low, K.~H., and Kankanhalli, M.
\newblock Towards robust resnet: A small step but a giant leap.
\newblock In \emph{IJCAI}, 2019{\natexlab{b}}.

\bibitem[Zhu et~al.(2018)Zhu, Chang, and Fu]{zhu-rk}
Zhu, M., Chang, B., and Fu, C.
\newblock {Convolutional Neural Networks combined with Runge-Kutta Methods}.
\newblock \emph{arXiv.org}, February 2018.

\end{thebibliography}
