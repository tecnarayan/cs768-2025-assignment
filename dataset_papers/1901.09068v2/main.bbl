\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abernethy et~al.(2008)Abernethy, Hazan, and Rakhlin]{AbernethyHR08}
Abernethy, J.~D., Hazan, E., and Rakhlin, A.
\newblock Competing in the dark: An efficient algorithm for bandit linear
  optimization.
\newblock In Servedio, R.~A. and Zhang, T. (eds.), \emph{Proc. of the 21st
  Annual Conference on Learning Theory, COLT}, pp.\  263--274. Omnipress, 2008.

\bibitem[Abernethy et~al.(2012)Abernethy, Hazan, and Rakhlin]{AbernethyHR12}
Abernethy, J.~D., Hazan, E., and Rakhlin, A.
\newblock Interior-point methods for full-information and bandit online
  learning.
\newblock \emph{{IEEE} Trans. Information Theory}, 58\penalty0 (7):\penalty0
  4164--4175, 2012.

\bibitem[Baydin et~al.(2018)Baydin, Cornish, Rubio, Schmidt, and
  Wood]{baydin-2018-hypergradient}
Baydin, A.~G., Cornish, R., Rubio, D.~M., Schmidt, M., and Wood, F.
\newblock Online learning rate adaptation with hypergradient descent.
\newblock In \emph{Sixth International Conference on Learning Representations
  (ICLR), Vancouver, Canada, April 30 -- May 3, 2018}, 2018.

\bibitem[Cesa-Bianchi \& Lugosi(2006)Cesa-Bianchi and Lugosi]{Cesa-BianchiL06}
Cesa-Bianchi, N. and Lugosi, G.
\newblock \emph{Prediction, learning, and games}.
\newblock Cambridge University Press, 2006.

\bibitem[Chang \& Lin(2001)Chang and Lin]{ChangL01}
Chang, C.-C. and Lin, C.-J.
\newblock \emph{{LIBSVM}: a library for support vector machines}, 2001.
\newblock Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}.

\bibitem[Duchi et~al.(2010)Duchi, Hazan, and Singer]{DuchiHS10}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock Technical Report 2010-24, UC Berkeley Electrical Engineering and
  Computer Science, 2010.
\newblock Available at
  \url{http://cs.berkeley.edu/~jduchi/projects/DuchiHaSi10.pdf}.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{DuchiHS11}
Duchi, J.~C., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2121--2159,
  2011.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{Ghadimi13}
Ghadimi, S. and Lan, G.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{KarimiNS16}
Karimi, H., Nutini, J., and Schmidt, M.
\newblock {Linear convergence of gradient and proximal-gradient methods under
  the {P}olyak-\L{}ojasiewicz condition}.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pp.\  795--811. Springer, 2016.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{KingmaB15}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem[Koolen et~al.(2014)Koolen, van Erven, and Gr\"{u}nwald]{KoolenvEG14}
Koolen, W.~M., van Erven, T., and Gr\"{u}nwald, P.
\newblock Learning the learning rate for prediction with expert advice.
\newblock In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.~D., and
  Weinberger, K.~Q. (eds.), \emph{Advances in Neural Information Processing
  Systems 27}, pp.\  2294--2302. Curran Associates, Inc., 2014.

\bibitem[Li \& Orabona(2019)Li and Orabona]{LiO19}
Li, X. and Orabona, F.
\newblock On the convergence of stochastic gradient descent with adaptive
  stepsizes.
\newblock In \emph{{Proc. of the 22nd International Conference on Artificial
  Intelligence and Statistics, AISTATS}}, 2019.

\bibitem[McMahan(2017)]{McMahan17}
McMahan, H.~B.
\newblock A survey of algorithms and analysis for adaptive online learning.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 3117--3166, 2017.

\bibitem[Mohri \& Yang(2016)Mohri and Yang]{mohri2016accelerating}
Mohri, M. and Yang, S.
\newblock Accelerating online convex optimization via adaptive prediction.
\newblock In Gretton, A. and Robert, C.~C. (eds.), \emph{Proc. of the 19th
  International Conference on Artificial Intelligence and Statistics, AISTATS},
  volume~51 of \emph{Proceedings of Machine Learning Research}, pp.\  848--856,
  Cadiz, Spain, 09--11 May 2016. PMLR.

\bibitem[Nesterov(2003)]{Nesterov2003}
Nesterov, Y.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer, 2003.

\bibitem[Polyak(1987)]{Polyak87}
Polyak, B.~T.
\newblock \emph{Introduction to Optimization}.
\newblock Optimization Software Inc, New York, 1987.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{ReddiKK18}
Reddi, S.~J., Kale, S., and Kumar, S.
\newblock On the convergence of {Adam} and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Rosenbrock(1960)]{Rosenbrock60}
Rosenbrock, H.~H.
\newblock An automatic method for finding the greatest or least value of a
  function.
\newblock \emph{The Computer Journal}, 3\penalty0 (3):\penalty0 175--184, 1960.

\bibitem[Schraudolph(1999)]{Schraudolph99}
Schraudolph, N.~N.
\newblock Local gain adaptation in stochastic gradient descent.
\newblock In \emph{In Proc. Intl. Conf. Artificial Neural Networks}, pp.\
  569--574. IEE, London, 1999.

\bibitem[Shalev-Shwartz(2007)]{Shalev-Shwartz07}
Shalev-Shwartz, S.
\newblock Online learning: Theory, algorithms, and applications.
\newblock Technical report, The Hebrew University, 2007.
\newblock PhD thesis.

\bibitem[Song et~al.(2013)Song, Chaudhuri, and Sarwate]{SongCS13}
Song, S., Chaudhuri, K., and Sarwate, A.~D.
\newblock Stochastic gradient descent with differentially private updates.
\newblock In \emph{Global Conference on Signal and Information Processing
  (GlobalSIP), 2013 IEEE}, pp.\  245--248. IEEE, 2013.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{TielemanH12}
Tieleman, T. and Hinton, G.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural Networks for Machine Learning}, 2012.

\bibitem[van Erven \& Koolen(2016)van Erven and Koolen]{vanErvenK16}
van Erven, T. and Koolen, W.~M.
\newblock {MetaGrad}: Multiple learning rates in online learning.
\newblock In Lee, D.~D., Sugiyama, M., Luxburg, U.~V., Guyon, I., and Garnett,
  R. (eds.), \emph{Advances in Neural Information Processing Systems 29}, pp.\
  3666--3674. Curran Associates, Inc., 2016.

\bibitem[Ward et~al.(2018)Ward, Wu, and Bottou]{WardWB18}
Ward, R., Wu, X., and Bottou, L.
\newblock {AdaGrad} stepsizes: Sharp convergence over nonconvex landscapes,
  from any initialization.
\newblock \emph{arXiv preprint arXiv:1806.01811}, 2018.

\bibitem[Zeiler(2012)]{Zeiler12}
Zeiler, M.~D.
\newblock {ADADELTA}: an adaptive learning rate method.
\newblock \emph{arXiv preprint arXiv:1212.5701}, 2012.

\end{thebibliography}
