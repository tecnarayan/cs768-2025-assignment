\begin{thebibliography}{88}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and
  Liang]{allen:2019-learning-in-wide-nets}
Allen-Zhu, Z., Li, Y., and Liang, Y.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Arora et~al.(2018)Arora, Cohen, and
  Hazan]{arora:2018-deep-linear-net-optimization}
Arora, S., Cohen, N., and Hazan, E.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  244--253. PMLR, 2018.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Cohen, Hu, and
  Luo]{arora:2019-implicit-reg-in-deep-mfac}
Arora, S., Cohen, N., Hu, W., and Luo, Y.
\newblock Implicit regularization in deep matrix factorization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32,
  2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, and
  Wang]{arora:2019-NTK-gen-bound}
Arora, S., Du, S., Hu, W., Li, Z., and Wang, R.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  322--332. PMLR, 2019{\natexlab{b}}.

\bibitem[Arora et~al.(2019{\natexlab{c}})Arora, Khandeparkar, Khodak,
  Plevrakis, and Saunshi]{arora:2019-cl-learning-theory}
Arora, S., Khandeparkar, H., Khodak, M., Plevrakis, O., and Saunshi, N.
\newblock A theoretical analysis of contrastive unsupervised representation
  learning.
\newblock \emph{arXiv preprint arXiv:1902.09229}, 2019{\natexlab{c}}.

\bibitem[Ash et~al.(2021)Ash, Goel, Krishnamurthy, and
  Misra]{ash:2021-investigating-negative-samples}
Ash, J.~T., Goel, S., Krishnamurthy, A., and Misra, D.
\newblock Investigating the role of negatives in contrastive representation
  learning.
\newblock \emph{arXiv preprint arXiv:2106.09943}, 2021.

\bibitem[Assran et~al.(2022{\natexlab{a}})Assran, Balestriero, Duval, Bordes,
  Misra, Bojanowski, Vincent, Rabbat, and
  Ballas]{assran:2022-ssl-hidden-cluster-prior}
Assran, M., Balestriero, R., Duval, Q., Bordes, F., Misra, I., Bojanowski, P.,
  Vincent, P., Rabbat, M., and Ballas, N.
\newblock The hidden uniform cluster prior in self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2210.07277}, 2022{\natexlab{a}}.

\bibitem[Assran et~al.(2022{\natexlab{b}})Assran, Caron, Misra, Bojanowski,
  Bordes, Vincent, Joulin, Rabbat, and Ballas]{assran:2022-msn}
Assran, M., Caron, M., Misra, I., Bojanowski, P., Bordes, F., Vincent, P.,
  Joulin, A., Rabbat, M., and Ballas, N.
\newblock Masked siamese networks for label-efficient learning.
\newblock In \emph{Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXI}, pp.\  456--473.
  Springer, 2022{\natexlab{b}}.

\bibitem[Assran et~al.(2023)Assran, Duval, Misra, Bojanowski, Vincent, Rabbat,
  LeCun, and Ballas]{assran:2023-i-jepa}
Assran, M., Duval, Q., Misra, I., Bojanowski, P., Vincent, P., Rabbat, M.,
  LeCun, Y., and Ballas, N.
\newblock Self-supervised learning from images with a joint-embedding
  predictive architecture.
\newblock \emph{arXiv preprint arXiv:2301.08243}, 2023.

\bibitem[Atanasov et~al.(2021)Atanasov, Bordelon, and
  Pehlevan]{atanasov:2021-silent-alignment}
Atanasov, A., Bordelon, B., and Pehlevan, C.
\newblock Neural networks as kernel learners: The silent alignment effect.
\newblock \emph{arXiv preprint arXiv:2111.00034}, 2021.

\bibitem[Bachman et~al.(2019)Bachman, Hjelm, and
  Buchwalter]{bachman:2019-cl-from-multiple-views}
Bachman, P., Hjelm, R.~D., and Buchwalter, W.
\newblock Learning representations by maximizing mutual information across
  views.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Baevski et~al.(2022)Baevski, Hsu, Xu, Babu, Gu, and
  Auli]{baevski:2022-data2vec}
Baevski, A., Hsu, W.-N., Xu, Q., Babu, A., Gu, J., and Auli, M.
\newblock Data2vec: A general framework for self-supervised learning in speech,
  vision and language.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1298--1312. PMLR, 2022.

\bibitem[Balestriero \& LeCun(2022)Balestriero and
  LeCun]{balestriero:2022-ssl-and-spectral-methods}
Balestriero, R. and LeCun, Y.
\newblock Contrastive and non-contrastive self-supervised learning recover
  global and local spectral embedding methods.
\newblock \emph{arXiv preprint arXiv:2205.11508}, 2022.

\bibitem[Bardes et~al.(2021)Bardes, Ponce, and LeCun]{bardes:2021-vicreg}
Bardes, A., Ponce, J., and LeCun, Y.
\newblock Vicreg: Variance-invariance-covariance regularization for
  self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2105.04906}, 2021.

\bibitem[Bordelon \& Pehlevan(2022)Bordelon and
  Pehlevan]{bordelon:2022-feature-learning-dmft}
Bordelon, B. and Pehlevan, C.
\newblock Self-consistent dynamical field theory of kernel evolution in wide
  neural networks.
\newblock \emph{arXiv preprint arXiv:2205.09653}, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown-2020-gpt3}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Cabannes et~al.(2023)Cabannes, Kiani, Balestriero, LeCun, and
  Bietti]{cabannes:2023-ssl-interplay}
Cabannes, V., Kiani, B.~T., Balestriero, R., LeCun, Y., and Bietti, A.
\newblock The ssl interplay: Augmentations, inductive bias, and generalization.
\newblock \emph{arXiv preprint arXiv:2302.02774}, 2023.

\bibitem[Canatar et~al.(2021)Canatar, Bordelon, and
  Pehlevan]{canatar:2021-spectral-bias}
Canatar, A., Bordelon, B., and Pehlevan, C.
\newblock Spectral bias and task-model alignment explain generalization in
  kernel regression and infinitely wide neural networks.
\newblock \emph{Nature communications}, 12\penalty0 (1):\penalty0 1--12, 2021.

\bibitem[Cao et~al.(2019)Cao, Fang, Wu, Zhou, and Gu]{cao:2019-spectral-bias}
Cao, Y., Fang, Z., Wu, Y., Zhou, D.-X., and Gu, Q.
\newblock Towards understanding the spectral bias of deep learning.
\newblock \emph{arXiv preprint arXiv:1912.01198}, 2019.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{caron:2020-swav}
Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J{\'e}gou, Mairal,
  Bojanowski, and Joulin]{caron:2021-dino}
Caron, M., Touvron, H., Misra, I., J{\'e}gou, H., Mairal, J., Bojanowski, P.,
  and Joulin, A.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pp.\  9650--9660, 2021.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and
  Hinton]{chen:2020-simclr}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pp.\
  1597--1607. PMLR, 2020.

\bibitem[Chen et~al.(2021)Chen, Luo, and
  Li]{chen:2021-properties-of-contrastive-losses}
Chen, T., Luo, C., and Li, L.
\newblock Intriguing properties of contrastive losses.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 11834--11845, 2021.

\bibitem[Chen \& He(2021)Chen and He]{chen:2021-simsiam}
Chen, X. and He, K.
\newblock Exploring simple siamese representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  15750--15758, 2021.

\bibitem[Chi et~al.(2019)Chi, Lu, and Chen]{chi:2019-nonconvex-opt-meets-mfac}
Chi, Y., Lu, Y.~M., and Chen, Y.
\newblock Nonconvex optimization meets low-rank matrix factorization: An
  overview.
\newblock \emph{IEEE Transactions on Signal Processing}, 67\penalty0
  (20):\penalty0 5239--5269, 2019.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery:2022-palm}
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,
  Barham, P., Chung, H.~W., Sutton, C., Gehrmann, S., et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Coates et~al.(2011)Coates, Ng, and Lee]{coates:2011-stl10}
Coates, A., Ng, A., and Lee, H.
\newblock An analysis of single-layer networks in unsupervised feature
  learning.
\newblock In \emph{Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pp.\  215--223. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du:2019-convergence}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1675--1685. PMLR, 2019.

\bibitem[Du et~al.(2018)Du, Hu, and Lee]{du:2018-deep-linear-net-optimization}
Du, S.~S., Hu, W., and Lee, J.~D.
\newblock Algorithmic regularization in learning deep homogeneous models:
  Layers are automatically balanced.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Fort et~al.(2020)Fort, Dziugaite, Paul, Kharaghani, Roy, and
  Ganguli]{fort:2020-deep-learning-versus-kernel-learning}
Fort, S., Dziugaite, G.~K., Paul, M., Kharaghani, S., Roy, D.~M., and Ganguli,
  S.
\newblock Deep learning versus kernel learning: an empirical study of loss
  landscape geometry and the time evolution of the neural tangent kernel.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 5850--5861, 2020.

\bibitem[Fukumizu(1998)]{fukumizu:1998-deep-linear-nets}
Fukumizu, K.
\newblock Effect of batch learning in multilayer neural networks.
\newblock \emph{Gen}, 1\penalty0 (04):\penalty0 1E--03, 1998.

\bibitem[Garrido et~al.(2022)Garrido, Balestriero, Najman, and
  Lecun]{garrido:2022-rankme}
Garrido, Q., Balestriero, R., Najman, L., and Lecun, Y.
\newblock Rankme: Assessing the downstream performance of pretrained
  self-supervised representations by their rank.
\newblock \emph{arXiv preprint arXiv:2210.02885}, 2022.

\bibitem[Geirhos et~al.(2020)Geirhos, Narayanappa, Mitzkus, Bethge, Wichmann,
  and Brendel]{geirhos:2020-ssl-vs-sl}
Geirhos, R., Narayanappa, K., Mitzkus, B., Bethge, M., Wichmann, F.~A., and
  Brendel, W.
\newblock On the surprising similarities between supervised and self-supervised
  models.
\newblock \emph{arXiv preprint arXiv:2010.08377}, 2020.

\bibitem[Grigg et~al.(2021)Grigg, Busbridge, Ramapuram, and
  Webb]{grigg:2021-ssl-vs-sl}
Grigg, T.~G., Busbridge, D., Ramapuram, J., and Webb, R.
\newblock Do self-supervised and supervised methods learn similar visual
  representations?
\newblock \emph{arXiv preprint arXiv:2110.00528}, 2021.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond,
  Buchatskaya, Doersch, Avila~Pires, Guo, Gheshlaghi~Azar,
  et~al.]{grill:2020-byol}
Grill, J.-B., Strub, F., Altch{\'e}, F., Tallec, C., Richemond, P.,
  Buchatskaya, E., Doersch, C., Avila~Pires, B., Guo, Z., Gheshlaghi~Azar, M.,
  et~al.
\newblock Bootstrap your own latent-a new approach to self-supervised learning.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 21271--21284, 2020.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar:2017-implicit-reg-in-mf}
Gunasekar, S., Woodworth, B.~E., Bhojanapalli, S., Neyshabur, B., and Srebro,
  N.
\newblock Implicit regularization in matrix factorization.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[HaoChen et~al.(2021)HaoChen, Wei, Gaidon, and
  Ma]{haochen:2021-provable-guarantees-for-cl}
HaoChen, J.~Z., Wei, C., Gaidon, A., and Ma, T.
\newblock Provable guarantees for self-supervised deep learning with spectral
  contrastive loss.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 5000--5011, 2021.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he:2020-moco}
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pp.\  9729--9738, 2020.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'a}r, and
  Girshick]{he:2022-mae}
He, K., Chen, X., Xie, S., Li, Y., Doll{\'a}r, P., and Girshick, R.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  16000--16009, 2022.

\bibitem[Henaff(2020)]{henaff:2020-contastive-coding}
Henaff, O.
\newblock Data-efficient image recognition with contrastive predictive coding.
\newblock In \emph{International conference on machine learning}, pp.\
  4182--4192. PMLR, 2020.

\bibitem[Hjelm et~al.(2018)Hjelm, Fedorov, Lavoie-Marchildon, Grewal, Bachman,
  Trischler, and Bengio]{hjelm:2018-deep-infomax}
Hjelm, R.~D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P.,
  Trischler, A., and Bengio, Y.
\newblock Learning deep representations by mutual information estimation and
  maximization.
\newblock \emph{arXiv preprint arXiv:1808.06670}, 2018.

\bibitem[Horace~He(2021)]{he:2021-functorch}
Horace~He, R.~Z.
\newblock functorch: Jax-like composable function transforms for pytorch.
\newblock \url{https://github.com/pytorch/functorch}, 2021.

\bibitem[Jacot et~al.(2018)Jacot, Hongler, and Gabriel]{jacot:2018}
Jacot, A., Hongler, C., and Gabriel, F.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Jacot et~al.(2021)Jacot, Ged, Simsek, Hongler, and
  Gabriel]{jacot:2021-saddle-to-saddle}
Jacot, A., Ged, F., Simsek, B., Hongler, C., and Gabriel, F.
\newblock Saddle-to-saddle dynamics in deep linear networks: Small
  initialization training, symmetry, and sparsity.
\newblock \emph{arXiv preprint arXiv:2106.15933}, 2021.

\bibitem[Jacot et~al.(2022)Jacot, Golikov, Hongler, and
  Gabriel]{jacot:2022-dnns-with-weight-decay}
Jacot, A., Golikov, E., Hongler, C., and Gabriel, F.
\newblock Feature learning in $l_{2}$-regularized dnns: Attraction/repulsion
  and sparsity.
\newblock \emph{arXiv preprint arXiv:2205.15809}, 2022.

\bibitem[Jing et~al.(2021)Jing, Vincent, LeCun, and
  Tian]{jing:2021-ssl-dim-collapse}
Jing, L., Vincent, P., LeCun, Y., and Tian, Y.
\newblock Understanding dimensional collapse in contrastive self-supervised
  learning.
\newblock \emph{arXiv preprint arXiv:2110.09348}, 2021.

\bibitem[Johnson et~al.(2022)Johnson, Hanchi, and
  Maddison]{johnson:2022-optimal-contrastive-reps}
Johnson, D.~D., Hanchi, A.~E., and Maddison, C.~J.
\newblock Contrastive learning can find an optimal basis for approximately
  view-invariant functions.
\newblock \emph{arXiv preprint arXiv:2210.01883}, 2022.

\bibitem[Kardar(2007)]{kardar:2007-stat-phys-of-fields}
Kardar, M.
\newblock \emph{Statistical physics of fields}.
\newblock Cambridge University Press, 2007.

\bibitem[Kiani et~al.(2022)Kiani, Balestriero, Chen, Lloyd, and
  LeCun]{kiani:2022-joint-embedding}
Kiani, B.~T., Balestriero, R., Chen, Y., Lloyd, S., and LeCun, Y.
\newblock Joint embedding self-supervised learning in the kernel regime.
\newblock \emph{arXiv preprint arXiv:2209.14884}, 2022.

\bibitem[Krizhevsky(2009)]{krizhevsky:2009}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Landau(1944)]{landau:1944-turbulence}
Landau, L.~D.
\newblock On the problem of turbulence.
\newblock In \emph{Dokl. Akad. Nauk USSR}, volume~44, pp.\  311, 1944.

\bibitem[Lee et~al.(2018)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl{-}Dickstein]{lee:2018-nngp}
Lee, J., Bahri, Y., Novak, R., Schoenholz, S.~S., Pennington, J., and
  Sohl{-}Dickstein, J.
\newblock Deep neural networks as gaussian processes.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl{-}Dickstein,
  and Pennington]{lee:2019-ntk}
Lee, J., Xiao, L., Schoenholz, S.~S., Bahri, Y., Novak, R., Sohl{-}Dickstein,
  J., and Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Li et~al.(2018)Li, Ma, and Zhang]{li:2018-matrix-sensing}
Li, Y., Ma, T., and Zhang, H.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock In \emph{Conference On Learning Theory}, pp.\  2--47. PMLR, 2018.

\bibitem[Long(2021)]{long:2021-after-kernel}
Long, P.~M.
\newblock Properties of the after kernel.
\newblock \emph{arXiv preprint arXiv:2105.10585}, 2021.

\bibitem[Loureiro et~al.(2021)Loureiro, Gerbelot, Cui, Goldt, Krzakala, Mezard,
  and Zdeborov{\'a}]{loureiro:2021-learning-curves}
Loureiro, B., Gerbelot, C., Cui, H., Goldt, S., Krzakala, F., Mezard, M., and
  Zdeborov{\'a}, L.
\newblock Learning curves of generic features maps for realistic datasets with
  a teacher-student model.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 18137--18151, 2021.

\bibitem[Mallinar et~al.(2022)Mallinar, Simon, Abedsoltan, Pandit, Belkin, and
  Nakkiran]{mallinar:2022-taxonomy}
Mallinar, N., Simon, J.~B., Abedsoltan, A., Pandit, P., Belkin, M., and
  Nakkiran, P.
\newblock Benign, tempered, or catastrophic: A taxonomy of overfitting.
\newblock \emph{arXiv preprint arXiv:2207.06569}, 2022.

\bibitem[Mei \& Montanari(2019)Mei and Montanari]{mei:2019-double-descent}
Mei, S. and Montanari, A.
\newblock The generalization error of random features regression: Precise
  asymptotics and the double descent curve.
\newblock \emph{Communications on Pure and Applied Mathematics}, 2019.

\bibitem[Nozawa \& Sato(2021)Nozawa and
  Sato]{nozawa:2021-understanding-negative-samples}
Nozawa, K. and Sato, I.
\newblock Understanding negative samples in instance discriminative
  self-supervised representation learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 5784--5797, 2021.

\bibitem[Pokle et~al.(2022)Pokle, Tian, Li, and
  Risteski]{pokle:2022-noncontrastive-cl-landscapes}
Pokle, A., Tian, J., Li, Y., and Risteski, A.
\newblock Contrasting the landscape of contrastive and non-contrastive
  learning.
\newblock \emph{arXiv preprint arXiv:2203.15702}, 2022.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford:2021-clip}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8748--8763. PMLR, 2021.

\bibitem[Rahimi et~al.(2007)Rahimi, Recht, et~al.]{rahimi:2007}
Rahimi, A., Recht, B., et~al.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~3, pp.\ ~5, 2007.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and
  Chen]{ramesh:2022-dalle2}
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock \emph{arXiv preprint arXiv:2204.06125}, 2022.

\bibitem[Roberts et~al.(2022)Roberts, Yaida, and
  Hanin]{roberts:2022-principles-of-dl-theory}
Roberts, D.~A., Yaida, S., and Hanin, B.
\newblock \emph{Frontmatter}.
\newblock Cambridge University Press, 2022.

\bibitem[Saunshi et~al.(2022)Saunshi, Ash, Goel, Misra, Zhang, Arora, Kakade,
  and Krishnamurthy]{saunshi:2022-understanding-cl-req-ind-bias}
Saunshi, N., Ash, J., Goel, S., Misra, D., Zhang, C., Arora, S., Kakade, S.,
  and Krishnamurthy, A.
\newblock Understanding contrastive learning requires incorporating inductive
  biases.
\newblock \emph{arXiv preprint arXiv:2202.14037}, 2022.

\bibitem[Saxe et~al.(2013)Saxe, McClelland, and
  Ganguli]{saxe:2013-deep-linear-nets}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6120}, 2013.

\bibitem[Shawe-Taylor et~al.(2004)Shawe-Taylor, Cristianini,
  et~al.]{shawe:2004}
Shawe-Taylor, J., Cristianini, N., et~al.
\newblock \emph{Kernel methods for pattern analysis}.
\newblock Cambridge university press, 2004.

\bibitem[Simon et~al.(2021)Simon, Dickens, Karkada, and
  DeWeese]{simon:2021-eigenlearning}
Simon, J.~B., Dickens, M., Karkada, D., and DeWeese, M.~R.
\newblock The eigenlearning framework: A conservation law perspective on kernel
  ridge regression and wide neural networks.
\newblock \emph{arXiv preprint arXiv:2110.03922}, 2021.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry:2018-implicit-bias-of-gd}
Soudry, D., Hoffer, E., Nacson, M.~S., Gunasekar, S., and Srebro, N.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Tancik et~al.(2020)Tancik, Srinivasan, Mildenhall, Fridovich-Keil,
  Raghavan, Singhal, Ramamoorthi, Barron, and
  Ng]{tancik:2020-nerf-with-fourier-features}
Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N.,
  Singhal, U., Ramamoorthi, R., Barron, J., and Ng, R.
\newblock Fourier features let networks learn high frequency functions in low
  dimensional domains.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7537--7547, 2020.

\bibitem[Tian et~al.(2020)Tian, Yu, Chen, and
  Ganguli]{tian:2020-understanding-ssl}
Tian, Y., Yu, L., Chen, X., and Ganguli, S.
\newblock Understanding self-supervised learning with dual deep networks.
\newblock \emph{arXiv preprint arXiv:2010.00578}, 2020.

\bibitem[Tian et~al.(2021)Tian, Chen, and
  Ganguli]{tian:2021-understanding-ssl-no-neg-pairs}
Tian, Y., Chen, X., and Ganguli, S.
\newblock Understanding self-supervised learning dynamics without contrastive
  pairs.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10268--10278. PMLR, 2021.

\bibitem[Tosh et~al.(2021{\natexlab{a}})Tosh, Krishnamurthy, and
  Hsu]{tosh:2021-cl-and-posterior-info}
Tosh, C., Krishnamurthy, A., and Hsu, D.
\newblock Contrastive estimation reveals topic posterior information to linear
  models.
\newblock \emph{J. Mach. Learn. Res.}, 22:\penalty0 281--1, 2021{\natexlab{a}}.

\bibitem[Tosh et~al.(2021{\natexlab{b}})Tosh, Krishnamurthy, and
  Hsu]{tosh:2021-cl-and-redundancy}
Tosh, C., Krishnamurthy, A., and Hsu, D.
\newblock Contrastive learning, multi-view redundancy, and linear models.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  1179--1206. PMLR,
  2021{\natexlab{b}}.

\bibitem[Tsai et~al.(2020)Tsai, Wu, Salakhutdinov, and
  Morency]{tsai:2020-ssl-multi-view}
Tsai, Y.-H.~H., Wu, Y., Salakhutdinov, R., and Morency, L.-P.
\newblock Self-supervised learning from a multi-view perspective.
\newblock \emph{arXiv preprint arXiv:2006.05576}, 2020.

\bibitem[Tsai et~al.(2021)Tsai, Ma, Yang, Zhao, Morency, and
  Salakhutdinov]{tsai:2021-ssl-rel-pred-coding}
Tsai, Y.-H.~H., Ma, M.~Q., Yang, M., Zhao, H., Morency, L.-P., and
  Salakhutdinov, R.
\newblock Self-supervised representation learning with relative predictive
  coding.
\newblock \emph{arXiv preprint arXiv:2103.11275}, 2021.

\bibitem[Vyas et~al.(2022)Vyas, Bansal, and
  Nakkiran]{vyas:2022-limitations-of-the-ntk-for-generalization}
Vyas, N., Bansal, Y., and Nakkiran, P.
\newblock Limitations of the ntk for understanding generalization in deep
  learning.
\newblock \emph{arXiv preprint arXiv:2206.10012}, 2022.

\bibitem[Wang \& Isola(2020)Wang and
  Isola]{wang:2020-understanding-cl-thru-align-and-unif}
Wang, T. and Isola, P.
\newblock Understanding contrastive representation learning through alignment
  and uniformity on the hypersphere.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9929--9939. PMLR, 2020.

\bibitem[Wei et~al.(2022)Wei, Hu, and Steinhardt]{wei:2022-more-than-a-toy}
Wei, A., Hu, W., and Steinhardt, J.
\newblock More than a toy: Random matrix models predict how real-world neural
  representations generalize.
\newblock In \emph{International Conference on Machine Learning}, Proceedings
  of Machine Learning Research, 2022.

\bibitem[Wei et~al.(2020)Wei, Shen, Chen, and
  Ma]{wei:2020-self-training-theory}
Wei, C., Shen, K., Chen, Y., and Ma, T.
\newblock Theoretical analysis of self-training with deep networks on unlabeled
  data.
\newblock \emph{arXiv preprint arXiv:2010.03622}, 2020.

\bibitem[Wen \& Li(2021)Wen and Li]{wen:2021-cl-feature-learning}
Wen, Z. and Li, Y.
\newblock Toward understanding the feature learning process of self-supervised
  contrastive learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11112--11122. PMLR, 2021.

\bibitem[Wu et~al.(2018)Wu, Xiong, Yu, and
  Lin]{wu:2018-unsupervised-feature-learning}
Wu, Z., Xiong, Y., Yu, S.~X., and Lin, D.
\newblock Unsupervised feature learning via non-parametric instance
  discrimination.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  3733--3742, 2018.

\bibitem[Yang(2019)]{yang:2019-tensor-programs-I}
Yang, G.
\newblock Tensor programs {I:} wide feedforward or recurrent neural networks of
  any architecture are gaussian processes.
\newblock \emph{CoRR}, abs/1910.12478, 2019.
\newblock URL \url{http://arxiv.org/abs/1910.12478}.

\bibitem[Yang \& Hu(2021)Yang and Hu]{yang:2021-tensor-programs-IV}
Yang, G. and Hu, E.~J.
\newblock Tensor programs iv: Feature learning in infinite-width neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11727--11737. PMLR, 2021.

\bibitem[Yang \& Salman(2019)Yang and
  Salman]{yang:2019-hypercube-spectral-bias}
Yang, G. and Salman, H.
\newblock A fine-grained spectral perspective on neural networks.
\newblock \emph{arXiv preprint arXiv:1907.10599}, 2019.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{you:2017-LARS}
You, Y., Gitman, I., and Ginsburg, B.
\newblock Large batch training of convolutional networks.
\newblock \emph{arXiv preprint arXiv:1708.03888}, 2017.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and
  Deny]{zbontar:2021-barlow-twins}
Zbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12310--12320. PMLR, 2021.

\bibitem[Ziyin et~al.(2022)Ziyin, Lubana, Ueda, and
  Tanaka]{ziyin:2022-loss-landscapes}
Ziyin, L., Lubana, E.~S., Ueda, M., and Tanaka, H.
\newblock What shapes the loss landscape of self-supervised learning?
\newblock \emph{arXiv preprint arXiv:2210.00638}, 2022.

\end{thebibliography}
