\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Sutton et~al.(1999{\natexlab{a}})Sutton, Precup, and
  Singh]{Sutton1999}
R.~S. Sutton, D.~Precup, and S.~Singh.
\newblock Between {MDPs} and {semi-MDPs}: A framework for temporal abstraction
  in reinforcement learning.
\newblock \emph{Artificial Intelligence}, 112\penalty0 (1-2):\penalty0
  181--211, 1999{\natexlab{a}}.

\bibitem[Precup(2000)]{Precup2000}
D.~Precup.
\newblock \emph{Temporal abstraction in reinforcement learning}.
\newblock PhD thesis, University of Massachusetts Amherst, 2000.

\bibitem[Newman and Girvan(2004)]{Newman2004}
M.~E. Newman and M.~Girvan.
\newblock Finding and evaluating community structure in networks.
\newblock \emph{Physical Review E}, 69\penalty0 (2):\penalty0 026113, 2004.

\bibitem[Leicht and Newman(2008)]{Leicht2008}
E.~A. Leicht and M.~E. Newman.
\newblock Community structure in directed networks.
\newblock \emph{Physical Review Letters}, 100\penalty0 (11):\penalty0 118703,
  2008.

\bibitem[Arenas et~al.(2007)Arenas, Duch, Fern{\'a}ndez, and
  G{\'o}mez]{Arenas2007}
A.~Arenas, J.~Duch, A.~Fern{\'a}ndez, and S.~G{\'o}mez.
\newblock Size reduction of complex networks preserving modularity.
\newblock \emph{New Journal of Physics}, 9\penalty0 (6):\penalty0 176, 2007.

\bibitem[Brandes et~al.(2006)Brandes, Delling, Gaertler, G{\"o}rke, Hoefer,
  Nikoloski, and Wagner]{Brandes2006}
U.~Brandes, D.~Delling, M.~Gaertler, R.~G{\"o}rke, M.~Hoefer, Z.~Nikoloski, and
  D.~Wagner.
\newblock Maximizing modularity is hard.
\newblock \emph{arXiv preprint:physics/0608255}, 2006.

\bibitem[Blondel et~al.(2008)Blondel, Guillaume, Lambiotte, and
  Lefebvre]{Blondel2008}
V.~D. Blondel, J.-L. Guillaume, R.~Lambiotte, and E.~Lefebvre.
\newblock Fast unfolding of communities in large networks.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2008\penalty0 (10):\penalty0 10008, 2008.

\bibitem[Lancichinetti and Fortunato(2009)]{Lancichinetti2009}
A.~Lancichinetti and S.~Fortunato.
\newblock Community detection algorithms: a comparative analysis.
\newblock \emph{Physical Review E}, 80\penalty0 (5):\penalty0 056117, 2009.

\bibitem[Menache et~al.(2002)Menache, Mannor, and Shimkin]{Menache2002}
I.~Menache, S.~Mannor, and N.~Shimkin.
\newblock Q-{C}ut --- {D}ynamic discovery of sub-goals in reinforcement
  learning.
\newblock In \emph{Proceedings of the 13th European Conference on Machine
  Learning}, ECML '02, pages 295--306. Springer-Verlag, 2002.

\bibitem[{\c{S}}im{\c{s}}ek and Barto(2004)]{Simsek2004}
{\"O}.~{\c{S}}im{\c{s}}ek and A.~G. Barto.
\newblock Using relative novelty to identify useful temporal abstractions in
  reinforcement learning.
\newblock In \emph{Proceedings of the 21st International Conference on Machine
  Learning}, ICML '04, pages 95--102. ACM, 2004.

\bibitem[{\c{S}}im{\c{s}}ek and Barto(2009)]{Simsek2009}
{\"O}.~{\c{S}}im{\c{s}}ek and A.~G. Barto.
\newblock Skill characterization based on betweenness.
\newblock In \emph{Advances in Neural Information Processing Systems 21},
  NeurIPS '09, pages 1497--1504. Curran Associates, Inc., 2009.

\bibitem[Moradi et~al.(2010)Moradi, Shiri, and Entezari]{Moradi2010}
P.~Moradi, M.~E. Shiri, and N.~Entezari.
\newblock Automatic skill acquisition in reinforcement learning agents using
  connection bridge centrality.
\newblock In \emph{International Conference on Future Generation Communication
  and Networking}, pages 51--62. Springer, 2010.

\bibitem[Rad et~al.(2010)Rad, Hasler, and Moradi]{Rad2010}
A.~A. Rad, M.~Hasler, and P.~Moradi.
\newblock Automatic skill acquisition in reinforcement learning using
  connection graph stability centrality.
\newblock In \emph{Proceedings of 2010 IEEE International Symposium on Circuits
  and Systems}, pages 697--700. IEEE, 2010.

\bibitem[Imanian and Moradi(2011)]{Imanian2011}
M.~A. Imanian and P.~Moradi.
\newblock Autonomous subgoal discovery in reinforcement learning agents using
  bridgeness centrality measure.
\newblock \emph{International Journal of Electrical and Computer Sciences},
  11\penalty0 (5):\penalty0 54--62, 2011.

\bibitem[Moradi et~al.(2012)Moradi, Shiri, Rad, Khadivi, and
  Hasler]{Moradi2012}
P.~Moradi, M.~E. Shiri, A.~A. Rad, A.~Khadivi, and M.~Hasler.
\newblock Automatic skill acquisition in reinforcement learning using graph
  centrality measures.
\newblock \emph{Intelligent Data Analysis}, 16\penalty0 (1):\penalty0 113--135,
  2012.

\bibitem[Metzen(2013)]{Metzen2013}
J.~H. Metzen.
\newblock Online skill discovery using graph-based clustering.
\newblock In \emph{Proceedings of the Tenth European Workshop on Reinforcement
  Learning}, EWRL '13, pages 77--88. PMLR, 2013.

\bibitem[{\c{S}}im{\c{s}}ek et~al.(2005){\c{S}}im{\c{s}}ek, Wolfe, and
  Barto]{Simsek2005}
{\"O}.~{\c{S}}im{\c{s}}ek, A.~P. Wolfe, and A.~G. Barto.
\newblock Identifying useful subgoals in reinforcement learning by local graph
  partitioning.
\newblock In \emph{Proceedings of the 22nd International Conference on Machine
  learning}, ICML '05, pages 816--823. ACM, 2005.

\bibitem[Kazemitabar and Beigy(2009)]{Kazemitabar2009}
S.~J. Kazemitabar and H.~Beigy.
\newblock Using strongly connected components as a basis for autonomous skill
  acquisition in reinforcement learning.
\newblock In \emph{6th International Symposium on Neural Networks}, ISSN '09,
  pages 794--803. Springer, 2009.

\bibitem[Entezari et~al.(2010)Entezari, Shiri, and Moradi]{Entezari2010}
N.~Entezari, M.~E. Shiri, and P.~Moradi.
\newblock A local graph clustering algorithm for discovering subgoals in
  reinforcement learning.
\newblock In \emph{International Conference on Future Generation Communication
  and Networking}, pages 41--50. Springer, 2010.

\bibitem[Bacon and Precup(2013)]{Bacon2013}
P.-L. Bacon and D.~Precup.
\newblock Using label propagation for learning temporally abstract actions in
  reinforcement learning.
\newblock In \emph{Proceedings of the Workshop on Multiagent Interaction
  Networks}, MAIN '13, 2013.

\bibitem[Taghizadeh and Beigy(2013)]{Taghizadeh2013}
N.~Taghizadeh and H.~Beigy.
\newblock A novel graphical approach to automatic abstraction in reinforcement
  learning.
\newblock \emph{Robotics and Autonomous Systems}, 61\penalty0 (8):\penalty0
  821--835, 2013.

\bibitem[Kazemitabar et~al.(2018)Kazemitabar, Taghizadeh, and
  Beigy]{Kazemitabar2018}
S.~J. Kazemitabar, N.~Taghizadeh, and H.~Beigy.
\newblock A graph-theoretic approach toward autonomous skill acquisition in
  reinforcement learning.
\newblock \emph{Evolving Systems}, 9\penalty0 (3):\penalty0 227--244, 2018.

\bibitem[Ramesh et~al.(2019)Ramesh, Tomar, and Ravindran]{Ramesh2019}
R.~Ramesh, M.~Tomar, and B.~Ravindran.
\newblock Successor options: an option discovery framework for reinforcement
  learning.
\newblock In \emph{Proceedings of the 28th International Joint Conference on
  Artificial Intelligence}, IJCAI '19, pages 3304--3310. AAAI Press, 2019.

\bibitem[Mannor et~al.(2004)Mannor, Menache, Hoze, and Klein]{Mannor2004}
S.~Mannor, I.~Menache, A.~Hoze, and U.~Klein.
\newblock Dynamic abstraction in reinforcement learning via clustering.
\newblock In \emph{Proceedings of the 21st International Conference on Machine
  Learning}, ICML '04, pages 71--78. ACM, 2004.

\bibitem[Davoodabadi and Beigy(2011)]{Davoodabadi2011}
M.~Davoodabadi and H.~Beigy.
\newblock A new method for discovering subgoals and constructing options in
  reinforcement learning.
\newblock In \emph{Proceedings of the 5th Indian International Conference on
  Artificial Intelligence}, pages 441--450, 2011.

\bibitem[Shoeleh and Asadpour(2017)]{Shoeleh2017}
F.~Shoeleh and M.~Asadpour.
\newblock Graph based skill acquisition and transfer learning for continuous
  reinforcement learning domains.
\newblock \emph{Pattern Recognition Letters}, 87:\penalty0 104--116, 2017.

\bibitem[Xu et~al.(2018)Xu, Yang, and Li]{Xu2018}
X.~Xu, M.~Yang, and G.~Li.
\newblock Constructing temporally extended actions through incremental
  community detection.
\newblock \emph{Computational Intelligence and Neuroscience}, 2018.

\bibitem[Farahani and Mozayani(2019)]{Davoodabadi2019}
M.~D. Farahani and N.~Mozayani.
\newblock Automatic construction and evaluation of macro-actions in
  reinforcement learning.
\newblock \emph{Applied Soft Computing}, 82:\penalty0 105574, 2019.

\bibitem[Machado et~al.(2017)Machado, Bellemare, and Bowling]{Machado2017}
M.~C. Machado, M.~G. Bellemare, and M.~Bowling.
\newblock A {Laplacian} framework for option discovery in reinforcement
  learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, ICML '17, pages 2295--2304. PMLR, 2017.

\bibitem[Jinnai et~al.(2019)Jinnai, Park, Abel, and Konidaris]{Jinnai2019}
Y.~Jinnai, J.~W. Park, D.~Abel, and G.~Konidaris.
\newblock Discovering options for exploration by minimizing cover time.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, ICML '19, pages 3130--3139. PMLR, 2019.

\bibitem[Bacon et~al.(2017)Bacon, Harb, and Precup]{Bacon2017}
P.-L. Bacon, J.~Harb, and D.~Precup.
\newblock The option-critic architecture.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~31, 2017.

\bibitem[Sutton et~al.(1999{\natexlab{b}})Sutton, McAllester, Singh, and
  Mansour]{Sutton1999PolicyGradient}
R.~S. Sutton, D.~McAllester, S.~Singh, and Y.~Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems 12},
  NeurIPS '99. MIT Press, 1999{\natexlab{b}}.

\bibitem[Riemer et~al.(2018)Riemer, Liu, and Tesauro]{Riemer2018}
M.~Riemer, M.~Liu, and G.~Tesauro.
\newblock Learning abstract options.
\newblock In \emph{Advances in Neural Information Processing Systems 31},
  NeurIPS '18. Curran Associates, Inc., 2018.

\bibitem[Fox et~al.(2017)Fox, Krishnan, Stoica, and Goldberg]{Fox2017}
R.~Fox, S.~Krishnan, I.~Stoica, and K.~Goldberg.
\newblock Multi-level discovery of deep options.
\newblock \emph{arXiv preprint:1703.08294}, 2017.

\bibitem[Levy et~al.(2019)Levy, Konidaris, Platt, and Saenko]{Levy2019}
A.~Levy, G.~Konidaris, R.~Platt, and K.~Saenko.
\newblock Learning multi-level hierarchies with hindsight.
\newblock In \emph{Proceedings of the 7th International Conference on Learning
  Representations}, ICLR '19, 2019.

\bibitem[Dietterich(2000)]{Dietterich2000}
T.~G. Dietterich.
\newblock Hierarchical reinforcement learning with the {MAXQ} value function
  decomposition.
\newblock \emph{Journal of Artificial Intelligence Resesearch}, 13\penalty0
  (1):\penalty0 227--303, 2000.

\bibitem[McGovern et~al.(1997)McGovern, Sutton, and Fagg]{McGovern1997}
A.~McGovern, R.~S. Sutton, and A.~H. Fagg.
\newblock Roles of macro-actions in accelerating reinforcement learning.
\newblock In \emph{Grace Hopper Celebration of Women in Computing}, volume~1,
  pages 13--18, 1997.

\bibitem[Sutton and Precup(1998)]{Precup1998}
R.~S. Sutton and D.~Precup.
\newblock Intra-option learning about temporally abstract actions.
\newblock In \emph{Proceedings of the 15th International Conference on Machine
  Learning}, pages 556--564. Morgan Kaufman, 1998.

\bibitem[Watkins(1989)]{Watkins1989}
C.~J. C.~H. Watkins.
\newblock \emph{Learning from delayed rewards}.
\newblock PhD thesis, King's College, Cambridge United Kingdom, 1989.

\bibitem[Mahadevan and Maggioni(2007)]{Mahadevan2007}
S.~Mahadevan and M.~Maggioni.
\newblock Proto-value functions: A {Laplacian} framework for learning
  representation and control in {Markov} decision processes.
\newblock \emph{Journal of Machine Learning Research}, 8\penalty0 (10), 2007.

\bibitem[Konidaris and Barto(2009)]{Konidaris2009}
G.~Konidaris and A.~Barto.
\newblock Skill discovery in continuous reinforcement learning domains using
  skill chaining.
\newblock In \emph{Advances in Neural Information Processing Systems 22},
  NeurIPS '09, pages 1015--1023. Curran Associates, Inc., 2009.

\bibitem[Farahani and Mozayani(2020)]{Davoodabadi2020}
D.~M. Farahani and N.~Mozayani.
\newblock Evaluating skills in hierarchical reinforcement learning.
\newblock \emph{International Journal of Machine Learning and Cybernetics},
  11\penalty0 (10):\penalty0 2407--2420, 2020.

\end{thebibliography}
