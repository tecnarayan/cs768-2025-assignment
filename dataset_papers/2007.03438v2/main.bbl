\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Baird(1995)]{Baird95residualalgorithms}
Leemon Baird.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In \emph{In Proceedings of the Twelfth International Conference on
  Machine Learning}, pages 30--37. Morgan Kaufmann, 1995.

\bibitem[Bas-Serrano and Neu(2019)]{basserrano2019faster}
Joan Bas-Serrano and Gergely Neu.
\newblock Faster saddle-point optimization for solving large-scale {Markov}
  decision processes, 2019.
\newblock arXiv:1909.10904.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock {OpenAI Gym}.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Chen et~al.(2018)Chen, Li, and Wang]{chen2018scalable}
Yichen Chen, Lihong Li, and Mengdi Wang.
\newblock Scalable bilinear $\pi$ learning using state and action features.
\newblock \emph{arXiv preprint arXiv:1804.10328}, 2018.

\bibitem[Dai et~al.(2016)Dai, He, Pan, Boots, and Song]{dai2016learning}
Bo~Dai, Niao He, Yunpeng Pan, Byron Boots, and Le~Song.
\newblock Learning from conditional distributions via dual embeddings.
\newblock \emph{arXiv preprint arXiv:1607.04579}, 2016.

\bibitem[Dai et~al.(2018)Dai, Shaw, Li, Xiao, He, Liu, Chen, and
  Song]{dai18sbeed}
Bo~Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and
  Le~Song.
\newblock {SBEED}: Convergent reinforcement learning with nonlinear function
  approximation.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, pages 1133--1142, 2018.

\bibitem[Du et~al.(2017)Du, Chen, Li, Xiao, and Zhou]{du2017stochastic}
Simon~S Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou.
\newblock Stochastic variance reduction methods for policy evaluation.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1049--1058. JMLR. org, 2017.

\bibitem[Duan and Wang(2020)]{duan2020minimaxoptimal}
Yaqi Duan and Mengdi Wang.
\newblock Minimax-optimal off-policy evaluation with linear function
  approximation, 2020.
\newblock arXiv:2002.09516.

\bibitem[Farajtabar et~al.(2018)Farajtabar, Chow, and
  Ghavamzadeh]{farajtabar2018more}
Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh.
\newblock More robust doubly robust off-policy evaluation.
\newblock \emph{arXiv preprint arXiv:1802.03493}, 2018.

\bibitem[Fonteneau et~al.(2013)Fonteneau, Murphy, Wehenkel, and
  Ernst]{fonteneau13batch}
Raphael Fonteneau, Susan~A. Murphy, Louis Wehenkel, and Damien Ernst.
\newblock Batch mode reinforcement learning based on the synthesis of
  artificial trajectories.
\newblock \emph{Annals of Operations Research}, 208\penalty0 (1):\penalty0
  383--416, 2013.

\bibitem[Hasselt et~al.(2016)Hasselt, Guez, and Silver]{van2016deep}
H.~Van Hasselt, A.~Guez, and D.~Silver.
\newblock Deep reinforcement learning with double {Q}-learning.
\newblock In \emph{AAAI}, volume~16, pages 2094--2100, 2016.

\bibitem[Jiang and Li(2015)]{jiang2015doubly}
Nan Jiang and Lihong Li.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1511.03722}, 2015.

\bibitem[Kallus and Uehara(2019{\natexlab{a}})]{kallus2019double}
Nathan Kallus and Masatoshi Uehara.
\newblock Double reinforcement learning for efficient off-policy evaluation in
  {Markov} decision processes.
\newblock \emph{arXiv preprint arXiv:1908.08526}, 2019{\natexlab{a}}.

\bibitem[Kallus and Uehara(2019{\natexlab{b}})]{kallus2019efficiently}
Nathan Kallus and Masatoshi Uehara.
\newblock Efficiently breaking the curse of horizon: Double reinforcement
  learning in infinite-horizon processes.
\newblock \emph{arXiv preprint arXiv:1909.05850}, 2019{\natexlab{b}}.

\bibitem[Lagoudakis and Parr(2003)]{lagoudakis2003least}
Michail~G Lagoudakis and Ronald Parr.
\newblock Least-squares policy iteration.
\newblock \emph{Journal of machine learning research}, 4\penalty0
  (Dec):\penalty0 1107--1149, 2003.

\bibitem[Liu et~al.(2015)Liu, Liu, Ghavamzadeh, Mahadevan, and
  Petrik]{LiuLiuGhaMahetal15}
Bo~Liu, Ji~Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik.
\newblock Finite-sample analysis of proximal gradient {TD} algorithms.
\newblock In \emph{Proc. The 31st Conf. Uncertainty in Artificial Intelligence,
  Amsterdam, Netherlands}, 2015.

\bibitem[Liu et~al.(2018)Liu, Li, Tang, and Zhou]{liu2018breaking}
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou.
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5356--5366, 2018.

\bibitem[Meyn and Tweedie(2012)]{meyn2012markov}
Sean~P Meyn and Richard~L Tweedie.
\newblock \emph{Markov Chains and Stochastic Stability}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{munos2016safe}
R.~Munos, T.~Stepleton, A.~Harutyunyan, and M.~Bellemare.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1054--1062, 2016.

\bibitem[Murphy et~al.(2001)Murphy, van~der Laan, and Robins]{Murphy01MM}
S.~Murphy, M.~van~der Laan, and J.~Robins.
\newblock Marginal mean models for dynamic regimes.
\newblock \emph{Journal of American Statistical Association}, 96\penalty0
  (456):\penalty0 1410--1423, 2001.

\bibitem[Nachum et~al.(2019{\natexlab{a}})Nachum, Chow, Dai, and
  Li]{NacChoDaiLi19}
Ofir Nachum, Yinlam Chow, Bo~Dai, and Lihong Li.
\newblock {DualDICE}: Behavior-agnostic estimation of discounted stationary
  distribution corrections.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2315--2325, 2019{\natexlab{a}}.

\bibitem[Nachum et~al.(2019{\natexlab{b}})Nachum, Dai, Kostrikov, Chow, Li, and
  Schuurmans]{algae}
Ofir Nachum, Bo~Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale
  Schuurmans.
\newblock {AlgaeDICE}: Policy gradient from arbitrary experience,
  2019{\natexlab{b}}.

\bibitem[Precup et~al.(2000)Precup, Sutton, and Singh]{Precup00ET}
Doina Precup, Richard~S. Sutton, and Satinder~P. Singh.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock In \emph{Proceedings of the 17th International Conference on Machine
  Learning}, pages 759--766, 2000.

\bibitem[Puterman(1994)]{puterman1994markov}
Martin~L Puterman.
\newblock \emph{Markov Decision Processes: Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons, Inc., 1994.

\bibitem[Rockafellar(1974)]{rockafellar1974augmented}
R~Tyrrell Rockafellar.
\newblock Augmented {Lagrange} multiplier functions and duality in nonconvex
  programming.
\newblock \emph{SIAM Journal on Control}, 12\penalty0 (2):\penalty0 268--285,
  1974.

\bibitem[Tang et~al.(2020)Tang, Feng, Li, Zhou, and Liu]{tang20doubly}
Ziyang Tang, Yihao Feng, Lihong Li, Dengyong Zhou, and Qiang Liu.
\newblock Doubly robust bias reduction in infinite horizon off-policy
  estimation.
\newblock In \emph{Proceedings of the 8th International Conference on Learning
  Representations}, 2020.

\bibitem[Thomas et~al.(2015)Thomas, Theocharous, and Ghavamzadeh]{Thomas15HCPE}
P.~Thomas, G.~Theocharous, and M.~Ghavamzadeh.
\newblock High confidence off-policy evaluation.
\newblock In \emph{Proceedings of the 29th Conference on Artificial
  Intelligence}, 2015.

\bibitem[Uehara and Jiang(2019)]{uehara2019minimax}
Masatoshi Uehara and Nan Jiang.
\newblock Minimax weight and {Q}-function learning for off-policy evaluation.
\newblock \emph{arXiv preprint arXiv:1910.12809}, 2019.

\bibitem[Wang(2017)]{wang2017randomized}
Mengdi Wang.
\newblock Randomized linear programming solves the discounted {Markov} decision
  problem in nearly-linear (sometimes sublinear) running time.
\newblock \emph{arXiv preprint arXiv:1704.01869}, 2017.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Dai, Li, and
  Schuurmans]{zhang2020gendice}
Ruiyi Zhang, Bo~Dai, Lihong Li, and Dale Schuurmans.
\newblock {GenDICE}: Generalized offline estimation of stationary values.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Liu, and
  Whiteson]{zhang2020gradientdice}
Shangtong Zhang, Bo~Liu, and Shimon Whiteson.
\newblock {GradientDICE}: Rethinking generalized offline estimation of
  stationary values.
\newblock \emph{arXiv preprint arXiv:2001.11113}, 2020{\natexlab{b}}.

\end{thebibliography}
