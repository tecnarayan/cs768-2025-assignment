\begin{thebibliography}{68}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Krishnamurthy, and Sun]{agarwal2020flambe}
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun.
\newblock Flambe: Structural complexity and representation learning of low rank {MDP}s.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 20095--20107, 2020.

\bibitem[Arora et~al.(2012)Arora, Dekel, and Tewari]{arora2012online}
Raman Arora, Ofer Dekel, and Ambuj Tewari.
\newblock Online bandit learning against an adaptive adversary: from regret to policy regret.
\newblock \emph{arXiv preprint arXiv:1206.6400}, 2012.

\bibitem[Arora et~al.(2018)Arora, Dinitz, Marinov, and Mohri]{arora2018policy}
Raman Arora, Michael Dinitz, Teodor~Vanislavov Marinov, and Mehryar Mohri.
\newblock Policy regret in repeated games.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Audibert and Bubeck(2009)]{audibert2009minimax}
Jean-Yves Audibert and S{\'e}bastien Bubeck.
\newblock Minimax policies for adversarial and stochastic bandits.
\newblock In \emph{COLT}, pages 217--226, 2009.

\bibitem[Awasthi et~al.(2022)Awasthi, Bhatia, Gollapudi, and Kollias]{awasthi2022congested}
Pranjal Awasthi, Kush Bhatia, Sreenivas Gollapudi, and Kostas Kollias.
\newblock Congested bandits: Optimal routing via short-term resets.
\newblock In \emph{International Conference on Machine Learning}, pages 1078--1100. PMLR, 2022.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages 263--272. PMLR, 2017.

\bibitem[Bai and Jin(2020)]{bai2020provable}
Yu~Bai and Chi Jin.
\newblock Provable self-play algorithms for competitive reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages 551--560. PMLR, 2020.

\bibitem[Bai et~al.(2020)Bai, Jin, and Yu]{bai2020near}
Yu~Bai, Chi Jin, and Tiancheng Yu.
\newblock Near-optimal reinforcement learning with self-play.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 2159--2170, 2020.

\bibitem[Baker et~al.(2019)Baker, Kanitscheider, Markov, Wu, Powell, McGrew, and Mordatch]{baker2019emergent}
Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi~Wu, Glenn Powell, Bob McGrew, and Igor Mordatch.
\newblock Emergent tool use from multi-agent autocurricula.
\newblock \emph{arXiv preprint arXiv:1909.07528}, 2019.

\bibitem[Balcan et~al.(2005)Balcan, Blum, Hartline, and Mansour]{balcan2005mechanism}
M-F Balcan, Avrim Blum, Jason~D Hartline, and Yishay Mansour.
\newblock Mechanism design via machine learning.
\newblock In \emph{46th Annual IEEE Symposium on Foundations of Computer Science (FOCS'05)}, pages 605--614. IEEE, 2005.

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, D{\k{e}}biak, Dennison, Farhi, Fischer, Hashme, Hesse, et~al.]{berner2019dota}
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys{\l}aw D{\k{e}}biak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.06680}, 2019.

\bibitem[Bhatia and Sridharan(2020)]{bhatia2020online}
Kush Bhatia and Karthik Sridharan.
\newblock Online learning with dynamics: A minimax perspective.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 15020--15030, 2020.

\bibitem[Blum et~al.(2014)Blum, Haghtalab, and Procaccia]{blum2014learning}
Avrim Blum, Nika Haghtalab, and Ariel~D Procaccia.
\newblock Learning optimal commitment to overcome insecurity.
\newblock \emph{Advances in Neural Information Processing Systems}, 27, 2014.

\bibitem[Brafman and Tennenholtz(2002)]{brafman2002r}
Ronen~I Brafman and Moshe Tennenholtz.
\newblock R-max-a general polynomial time algorithm for near-optimal reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0 (Oct):\penalty0 213--231, 2002.

\bibitem[Braverman et~al.(2019)Braverman, Mao, Schneider, and Weinberg]{braverman2019multi}
Mark Braverman, Jieming Mao, Jon Schneider, and S~Matthew Weinberg.
\newblock Multi-armed bandit problems with strategic arms.
\newblock In \emph{Conference on Learning Theory}, pages 383--416. PMLR, 2019.

\bibitem[Brown and Sandholm(2018)]{brown2018superhuman}
Noam Brown and Tuomas Sandholm.
\newblock Superhuman {AI} for heads-up no-limit poker: Libratus beats top professionals.
\newblock \emph{Science}, 359\penalty0 (6374):\penalty0 418--424, 2018.

\bibitem[Cole and Roughgarden(2014)]{cole2014sample}
Richard Cole and Tim Roughgarden.
\newblock The sample complexity of revenue maximization.
\newblock In \emph{Proceedings of the forty-sixth annual ACM symposium on Theory of computing}, pages 243--252, 2014.

\bibitem[Conitzer and Sandholm(2002)]{conitzer2002complexity}
Vincent Conitzer and Tuomas Sandholm.
\newblock Complexity of mechanism design.
\newblock \emph{arXiv preprint cs/0205075}, 2002.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Christoph Dann, Tor Lattimore, and Emma Brunskill.
\newblock Unifying {PAC} and regret: Uniform {PAC} bounds for episodic reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Dekel et~al.(2014)Dekel, Ding, Koren, and Peres]{dekel2014bandits}
Ofer Dekel, Jian Ding, Tomer Koren, and Yuval Peres.
\newblock Bandits with switching costs: ${T}^{2/3}$ regret.
\newblock In \emph{Proceedings of the forty-sixth annual ACM symposium on Theory of computing}, pages 459--467, 2014.

\bibitem[Dinh et~al.(2023)Dinh, Mguni, Tran-Thanh, Wang, and Yang]{dinh2023online}
Le~Cong Dinh, David~Henry Mguni, Long Tran-Thanh, Jun Wang, and Yaodong Yang.
\newblock Online {M}arkov decision processes with non-oblivious strategic adversary.
\newblock \emph{Autonomous Agents and Multi-Agent Systems}, 37\penalty0 (1):\penalty0 15, 2023.

\bibitem[Domingues et~al.(2021)Domingues, M{\'e}nard, Kaufmann, and Valko]{domingues2021episodic}
Omar~Darwiche Domingues, Pierre M{\'e}nard, Emilie Kaufmann, and Michal Valko.
\newblock Episodic reinforcement learning in finite {MDP}s: Minimax lower bounds revisited.
\newblock In \emph{Algorithmic Learning Theory}, pages 578--598. PMLR, 2021.

\bibitem[D{\"u}tting et~al.(2019)D{\"u}tting, Feng, Narasimhan, Parkes, and Ravindranath]{dutting2019optimal}
Paul D{\"u}tting, Zhe Feng, Harikrishna Narasimhan, David Parkes, and Sai~Srivatsa Ravindranath.
\newblock Optimal auctions through deep learning.
\newblock In \emph{International Conference on Machine Learning}, pages 1706--1715. PMLR, 2019.

\bibitem[Filar and Vrieze(2012)]{filar2012competitive}
Jerzy Filar and Koos Vrieze.
\newblock \emph{Competitive Markov decision processes}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Geer(2000)]{geer2000empirical}
Sara~A Geer.
\newblock \emph{Empirical Processes in M-estimation}, volume~6.
\newblock Cambridge university press, 2000.

\bibitem[Hansen et~al.(2013)Hansen, Miltersen, and Zwick]{hansen2013strategy}
Thomas~Dueholm Hansen, Peter~Bro Miltersen, and Uri Zwick.
\newblock Strategy iteration is strongly polynomial for 2-player turn-based stochastic games with a constant discount factor.
\newblock \emph{Journal of the ACM (JACM)}, 60\penalty0 (1):\penalty0 1--16, 2013.

\bibitem[Heidari et~al.(2016)Heidari, Kearns, and Roth]{heidari2016tight}
Hoda Heidari, Michael~J Kearns, and Aaron Roth.
\newblock Tight policy regret bounds for improving and decaying bandits.
\newblock In \emph{IJCAI}, pages 1562--1570, 2016.

\bibitem[Hu and Wellman(2003)]{hu2003nash}
Junling Hu and Michael~P Wellman.
\newblock Nash q-learning for general-sum stochastic games.
\newblock \emph{Journal of machine learning research}, 4\penalty0 (Nov):\penalty0 1039--1069, 2003.

\bibitem[Jaderberg et~al.(2019)Jaderberg, Czarnecki, Dunning, Marris, Lever, Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, et~al.]{jaderberg2019human}
Max Jaderberg, Wojciech~M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio~Garcia Castaneda, Charles Beattie, Neil~C Rabinowitz, Ari~S Morcos, Avraham Ruderman, et~al.
\newblock Human-level performance in 3d multiplayer games with population-based reinforcement learning.
\newblock \emph{Science}, 364\penalty0 (6443):\penalty0 859--865, 2019.

\bibitem[Jin et~al.(2020)Jin, Krishnamurthy, Simchowitz, and Yu]{jin2020reward}
Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu.
\newblock Reward-free exploration for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages 4870--4879. PMLR, 2020.

\bibitem[Jin et~al.(2021)Jin, Liu, Wang, and Yu]{jin2021v}
Chi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu.
\newblock V-learning--a simple, efficient, decentralized algorithm for multiagent {RL}.
\newblock \emph{arXiv preprint arXiv:2110.14555}, 2021.

\bibitem[Jin et~al.(2022)Jin, Liu, and Yu]{jin2022power}
Chi Jin, Qinghua Liu, and Tiancheng Yu.
\newblock The power of exploiter: Provable multi-agent {RL} in large state spaces.
\newblock In \emph{International Conference on Machine Learning}, pages 10251--10279. PMLR, 2022.

\bibitem[Koren et~al.(2017{\natexlab{a}})Koren, Livni, and Mansour]{koren2017bandits}
Tomer Koren, Roi Livni, and Yishay Mansour.
\newblock Bandits with movement costs and adaptive pricing.
\newblock In \emph{Conference on Learning Theory}, pages 1242--1268. PMLR, 2017{\natexlab{a}}.

\bibitem[Koren et~al.(2017{\natexlab{b}})Koren, Livni, and Mansour]{koren2017multi}
Tomer Koren, Roi Livni, and Yishay Mansour.
\newblock Multi-armed bandits with metric movement costs.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017{\natexlab{b}}.

\bibitem[Kwon et~al.(2021)Kwon, Efroni, Caramanis, and Mannor]{kwon2021rl}
Jeongyeol Kwon, Yonathan Efroni, Constantine Caramanis, and Shie Mannor.
\newblock {RL} for latent {MDP}s: Regret guarantees and a lower bound.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 24523--24534, 2021.

\bibitem[Letchford et~al.(2009)Letchford, Conitzer, and Munagala]{letchford2009learning}
Joshua Letchford, Vincent Conitzer, and Kamesh Munagala.
\newblock Learning and approximating the optimal strategy to commit to.
\newblock In \emph{Algorithmic Game Theory: Second International Symposium, SAGT 2009, Paphos, Cyprus, October 18-20, 2009. Proceedings 2}, pages 250--262. Springer, 2009.

\bibitem[Levine et~al.(2017)Levine, Crammer, and Mannor]{levine2017rotting}
Nir Levine, Koby Crammer, and Shie Mannor.
\newblock Rotting bandits.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Lindner et~al.(2021)Lindner, Heidari, and Krause]{lindner2021addressing}
David Lindner, Hoda Heidari, and Andreas Krause.
\newblock Addressing the long-term impact of ml decisions via policy regret.
\newblock \emph{arXiv preprint arXiv:2106.01325}, 2021.

\bibitem[Littman(1994)]{littman1994markov}
Michael~L Littman.
\newblock Markov games as a framework for multi-agent reinforcement learning.
\newblock In \emph{Machine learning proceedings 1994}, pages 157--163. Elsevier, 1994.

\bibitem[Liu et~al.(2021)Liu, Yu, Bai, and Jin]{liu2021sharp}
Qinghua Liu, Tiancheng Yu, Yu~Bai, and Chi Jin.
\newblock A sharp analysis of model-based reinforcement learning with self-play.
\newblock In \emph{International Conference on Machine Learning}, pages 7001--7010. PMLR, 2021.

\bibitem[Liu et~al.(2022)Liu, Wang, and Jin]{liu2022learning}
Qinghua Liu, Yuanhao Wang, and Chi Jin.
\newblock Learning {M}arkov games with adversarial opponents: Efficient algorithms and fundamental limits.
\newblock In \emph{International Conference on Machine Learning}, pages 14036--14053. PMLR, 2022.

\bibitem[Liu et~al.(2023)Liu, Netrapalli, Szepesvari, and Jin]{liu2023optimistic}
Qinghua Liu, Praneeth Netrapalli, Csaba Szepesvari, and Chi Jin.
\newblock Optimistic {MLE}: A generic model-based algorithm for partially observable sequential decision making.
\newblock In \emph{Proceedings of the 55th Annual ACM Symposium on Theory of Computing}, pages 363--376, 2023.

\bibitem[Malik et~al.(2022)Malik, Li, and Singh]{malik2022complete}
Dhruv Malik, Yuanzhi Li, and Aarti Singh.
\newblock Complete policy regret bounds for tallying bandits.
\newblock In \emph{Conference on Learning Theory}, pages 5146--5174. PMLR, 2022.

\bibitem[Malik et~al.(2023)Malik, Igoe, Li, and Singh]{malik2023weighted}
Dhruv Malik, Conor Igoe, Yuanzhi Li, and Aarti Singh.
\newblock Weighted tallying bandits: overcoming intractability via repeated exposure optimality.
\newblock In \emph{International Conference on Machine Learning}, pages 23590--23609. PMLR, 2023.

\bibitem[Merhav et~al.(2002)Merhav, Ordentlich, Seroussi, and Weinberger]{merhav2002sequential}
Neri Merhav, Erik Ordentlich, Gadiel Seroussi, and Marcelo~J Weinberger.
\newblock On sequential strategies for loss functions with memory.
\newblock \emph{IEEE Transactions on Information Theory}, 48\penalty0 (7):\penalty0 1947--1958, 2002.

\bibitem[Morav{\v{c}}{\'\i}k et~al.(2017)Morav{\v{c}}{\'\i}k, Schmid, Burch, Lis{\`y}, Morrill, Bard, Davis, Waugh, Johanson, and Bowling]{moravvcik2017deepstack}
Matej Morav{\v{c}}{\'\i}k, Martin Schmid, Neil Burch, Viliam Lis{\`y}, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling.
\newblock Deepstack: Expert-level artificial intelligence in heads-up no-limit poker.
\newblock \emph{Science}, 356\penalty0 (6337):\penalty0 508--513, 2017.

\bibitem[Nash(1950)]{doi:10.1073/pnas.36.1.48}
John~F. Nash.
\newblock Equilibrium points in $n$-person games.
\newblock \emph{Proceedings of the National Academy of Sciences}, 36\penalty0 (1):\penalty0 48--49, 1950.
\newblock \doi{10.1073/pnas.36.1.48}.

\bibitem[Qiao et~al.(2022)Qiao, Yin, Min, and Wang]{qiao2022sample}
Dan Qiao, Ming Yin, Ming Min, and Yu-Xiang Wang.
\newblock Sample-efficient reinforcement learning with loglog (t) switching cost.
\newblock In \emph{International Conference on Machine Learning}, pages 18031--18061. PMLR, 2022.

\bibitem[Ramponi and Restelli(2022)]{ramponi2022learning}
Giorgia Ramponi and Marcello Restelli.
\newblock Learning in {M}arkov games: can we exploit a general-sum opponent?
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 1665--1675. PMLR, 2022.

\bibitem[Russo and Van~Roy(2013)]{russo2013eluder}
Daniel Russo and Benjamin Van~Roy.
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Seznec et~al.(2019)Seznec, Locatelli, Carpentier, Lazaric, and Valko]{seznec2019rotting}
Julien Seznec, Andrea Locatelli, Alexandra Carpentier, Alessandro Lazaric, and Michal Valko.
\newblock Rotting bandits are no harder than stochastic ones.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence and Statistics}, pages 2564--2572. PMLR, 2019.

\bibitem[Shalev-Shwartz et~al.(2016)Shalev-Shwartz, Shammah, and Shashua]{shalev2016safe}
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua.
\newblock Safe, multi-agent, reinforcement learning for autonomous driving.
\newblock \emph{arXiv preprint arXiv:1610.03295}, 2016.

\bibitem[Shapley(1953)]{shapley1953stochastic}
Lloyd~S Shapley.
\newblock Stochastic games.
\newblock \emph{Proceedings of the national academy of sciences}, 39\penalty0 (10):\penalty0 1095--1100, 1953.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, et~al.]{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of {G}o with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang, Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et~al.
\newblock Mastering the game of {G}o without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Silver et~al.(2018)Silver, Hubert, Schrittwieser, Antonoglou, Lai, Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2018general}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et~al.
\newblock A general reinforcement learning algorithm that masters chess, shogi, and go through self-play.
\newblock \emph{Science}, 362\penalty0 (6419):\penalty0 1140--1144, 2018.

\bibitem[Tian et~al.(2021)Tian, Wang, Yu, and Sra]{tian2021online}
Yi~Tian, Yuanhao Wang, Tiancheng Yu, and Suvrit Sra.
\newblock Online learning in unknown {M}arkov games.
\newblock In \emph{International conference on machine learning}, pages 10279--10288. PMLR, 2021.

\bibitem[Tirinzoni et~al.(2023)Tirinzoni, Al-Marjani, and Kaufmann]{tirinzoni2023optimistic}
Andrea Tirinzoni, Aymen Al-Marjani, and Emilie Kaufmann.
\newblock Optimistic {PAC} reinforcement learning: the instance-dependent view.
\newblock In \emph{International Conference on Algorithmic Learning Theory}, pages 1460--1480. PMLR, 2023.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik, Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Oriol Vinyals, Igor Babuschkin, Wojciech~M Czarnecki, Micha{\"e}l Mathieu, Andrew Dudzik, Junyoung Chung, David~H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et~al.
\newblock Grandmaster level in {StarCraft II} using multi-agent reinforcement learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Von~Stackelberg(2010)]{stackelberg1934marktform}
Heinrich Von~Stackelberg.
\newblock \emph{Market structure and equilibrium}.
\newblock Springer Science \& Business Media, 2010.

\bibitem[Wei et~al.(2017)Wei, Hong, and Lu]{wei2017online}
Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu.
\newblock Online reinforcement learning in stochastic games.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Wei et~al.(2020)Wei, Lee, Zhang, and Luo]{wei2020linear}
Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo.
\newblock Linear last-iterate convergence in constrained saddle-point optimization.
\newblock \emph{arXiv preprint arXiv:2006.09517}, 2020.

\bibitem[Xie et~al.(2020)Xie, Chen, Wang, and Yang]{xie2020learning}
Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang.
\newblock Learning zero-sum simultaneous-move {M}arkov games using function approximation and correlated equilibrium.
\newblock In \emph{Conference on learning theory}, pages 3674--3682. PMLR, 2020.

\bibitem[Yang and Wang(2020)]{yang2020overview}
Yaodong Yang and Jun Wang.
\newblock An overview of multi-agent reinforcement learning from game theoretical perspective.
\newblock \emph{arXiv preprint arXiv:2011.00583}, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Yang, and Ba{\c{s}}ar]{zhang2021multi}
Kaiqing Zhang, Zhuoran Yang, and Tamer Ba{\c{s}}ar.
\newblock Multi-agent reinforcement learning: A selective overview of theories and algorithms.
\newblock \emph{Handbook of reinforcement learning and control}, pages 321--384, 2021.

\bibitem[Zhang(2006)]{zhang2006}
Tong Zhang.
\newblock From $\epsilon$-entropy to {KL}-entropy: Analysis of minimum information complexity density estimation.
\newblock 2006.

\bibitem[Zhang et~al.(2023)Zhang, Chen, Lee, and Du]{zhang2023settling}
Zihan Zhang, Yuxin Chen, Jason~D Lee, and Simon~S Du.
\newblock Settling the sample complexity of online reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2307.13586}, 2023.

\bibitem[Zheng et~al.(2020)Zheng, Trott, Srinivasa, Naik, Gruesbeck, Parkes, and Socher]{zheng2020ai}
Stephan Zheng, Alexander Trott, Sunil Srinivasa, Nikhil Naik, Melvin Gruesbeck, David~C Parkes, and Richard Socher.
\newblock The {AI} economist: Improving equality and productivity with {AI}-driven tax policies.
\newblock \emph{arXiv preprint arXiv:2004.13332}, 2020.

\end{thebibliography}
