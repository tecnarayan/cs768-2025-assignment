\begin{thebibliography}{10}

\bibitem{alberti2019fusion}
Chris Alberti, Jeffrey Ling, Michael Collins, and David Reitter.
\newblock Fusion of detected objects in text for visual question answering.
\newblock {\em arXiv preprint arXiv:1908.05054}, 2019.

\bibitem{anderson2018bottom}
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
  Gould, and Lei Zhang.
\newblock Bottom-up and top-down attention for image captioning and visual
  question answering.
\newblock In {\em CVPR}, 2018.

\bibitem{andreas2016neural}
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein.
\newblock Neural module networks.
\newblock In {\em CVPR}, 2016.

\bibitem{antol2015vqa}
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
  C~Lawrence~Zitnick, and Devi Parikh.
\newblock Vqa: Visual question answering.
\newblock In {\em ICCV}, 2015.

\bibitem{athalye2018obfuscated}
Anish Athalye, Nicholas Carlini, and David Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock {\em arXiv preprint arXiv:1802.00420}, 2018.

\bibitem{cadene2019murel}
Remi Cadene, Hedi Ben-Younes, Matthieu Cord, and Nicolas Thome.
\newblock Murel: Multimodal relational reasoning for visual question answering.
\newblock In {\em CVPR}, 2019.

\bibitem{cao2020behind}
Jize Cao, Zhe Gan, Yu~Cheng, Licheng Yu, Yen-Chun Chen, and Jingjing Liu.
\newblock Behind the scene: Revealing the secrets of pre-trained
  vision-and-language models.
\newblock {\em arXiv preprint arXiv:2005.07310}, 2020.

\bibitem{carmon2019unlabeled}
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John~C Duchi, and Percy~S
  Liang.
\newblock Unlabeled data improves adversarial robustness.
\newblock In {\em NeurIPS}, 2019.

\bibitem{chen2017attacking}
Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, and Cho-Jui Hsieh.
\newblock Attacking visual language grounding with adversarial examples: A case
  study on neural image captioning.
\newblock {\em arXiv preprint arXiv:1712.02051}, 2017.

\bibitem{chen2020adversarial}
Tianlong Chen, Sijia Liu, Shiyu Chang, Yu~Cheng, Lisa Amini, and Zhangyang
  Wang.
\newblock Adversarial robustness: From self-supervised pre-training to
  fine-tuning.
\newblock {\em arXiv preprint arXiv:2003.12862}, 2020.

\bibitem{chen2019meta}
Wenhu Chen, Zhe Gan, Linjie Li, Yu~Cheng, William Wang, and Jingjing Liu.
\newblock Meta module network for compositional visual reasoning.
\newblock {\em arXiv preprint arXiv:1910.03230}, 2019.

\bibitem{chen2019uniter}
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed~El Kholy, Faisal Ahmed, Zhe Gan,
  Yu~Cheng, and Jingjing Liu.
\newblock Uniter: Learning universal image-text representations.
\newblock {\em arXiv preprint arXiv:1909.11740}, 2019.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em NAACL}, 2019.

\bibitem{fukui2016multimodal}
Akira Fukui, Dong~Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and
  Marcus Rohrbach.
\newblock Multimodal compact bilinear pooling for visual question answering and
  visual grounding.
\newblock {\em arXiv preprint arXiv:1606.01847}, 2016.

\bibitem{gan2019multi}
Zhe Gan, Yu~Cheng, Ahmed~EI Kholy, Linjie Li, Jingjing Liu, and Jianfeng Gao.
\newblock Multi-step reasoning via recurrent dual attention for visual dialog.
\newblock {\em arXiv preprint arXiv:1902.00579}, 2019.

\bibitem{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock {\em arXiv preprint arXiv:1412.6572}, 2014.

\bibitem{goyal2017making}
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
\newblock Making the v in vqa matter: Elevating the role of image understanding
  in visual question answering.
\newblock In {\em CVPR}, 2017.

\bibitem{hao2020towards}
Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao.
\newblock Towards learning a generic agent for vision-and-language navigation
  via pre-training.
\newblock {\em arXiv preprint arXiv:2002.10638}, 2020.

\bibitem{hendrycks2019using}
Dan Hendrycks, Kimin Lee, and Mantas Mazeika.
\newblock Using pre-training can improve model robustness and uncertainty.
\newblock {\em arXiv preprint arXiv:1901.09960}, 2019.

\bibitem{hu2017learning}
Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko.
\newblock Learning to reason: End-to-end module networks for visual question
  answering.
\newblock In {\em ICCV}, 2017.

\bibitem{huang2020pixel}
Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu.
\newblock Pixel-bert: Aligning image pixels with text by deep multi-modal
  transformers.
\newblock {\em arXiv preprint arXiv:2004.00849}, 2020.

\bibitem{hudson2018compositional}
Drew~A Hudson and Christopher~D Manning.
\newblock Compositional attention networks for machine reasoning.
\newblock {\em arXiv preprint arXiv:1803.03067}, 2018.

\bibitem{hudson2019gqa}
Drew~A Hudson and Christopher~D Manning.
\newblock Gqa: A new dataset for real-world visual reasoning and compositional
  question answering.
\newblock In {\em CVPR}, 2019.

\bibitem{jiang2019smart}
Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo
  Zhao.
\newblock Smart: Robust and efficient fine-tuning for pre-trained natural
  language models through principled regularized optimization.
\newblock {\em arXiv preprint arXiv:1911.03437}, 2019.

\bibitem{johnson2017inferring}
Justin Johnson, Bharath Hariharan, Laurens van~der Maaten, Judy Hoffman,
  Li~Fei-Fei, C~Lawrence~Zitnick, and Ross Girshick.
\newblock Inferring and executing programs for visual reasoning.
\newblock In {\em ICCV}, 2017.

\bibitem{kim2018bilinear}
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang.
\newblock Bilinear attention networks.
\newblock In {\em NeurIPS}, 2018.

\bibitem{kim2016hadamard}
Jin-Hwa Kim, Kyoung-Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha, and
  Byoung-Tak Zhang.
\newblock Hadamard product for low-rank bilinear pooling.
\newblock {\em arXiv preprint arXiv:1610.04325}, 2016.

\bibitem{krishna2017visual}
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
  Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David~A Shamma, et~al.
\newblock Visual genome: Connecting language and vision using crowdsourced
  dense image annotations.
\newblock {\em IJCV}, 2017.

\bibitem{lee2018stacked}
Kuang-Huei Lee, Xi~Chen, Gang Hua, Houdong Hu, and Xiaodong He.
\newblock Stacked cross attention for image-text matching.
\newblock In {\em ECCV}, 2018.

\bibitem{li2019unicoder}
Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and Ming Zhou.
\newblock Unicoder-vl: A universal encoder for vision and language by
  cross-modal pre-training.
\newblock {\em arXiv preprint arXiv:1908.06066}, 2019.

\bibitem{li2020hero}
Linjie Li, Yen-Chun Chen, Yu~Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu.
\newblock Hero: Hierarchical encoder for video+ language omni-representation
  pre-training.
\newblock {\em arXiv preprint arXiv:2005.00200}, 2020.

\bibitem{li2019relation}
Linjie Li, Zhe Gan, Yu~Cheng, and Jingjing Liu.
\newblock Relation-aware graph attention network for visual question answering.
\newblock {\em arXiv preprint arXiv:1903.12314}, 2019.

\bibitem{li2019visualbert}
Liunian~Harold Li, Mark Yatskar, Da~Yin, Cho-Jui Hsieh, and Kai-Wei Chang.
\newblock Visualbert: A simple and performant baseline for vision and language.
\newblock {\em arXiv preprint arXiv:1908.03557}, 2019.

\bibitem{li2019improving}
Pengcheng Li, Jinfeng Yi, Bowen Zhou, and Lijun Zhang.
\newblock Improving the robustness of deep neural networks via adversarial
  training with triplet loss.
\newblock {\em arXiv preprint arXiv:1905.11713}, 2019.

\bibitem{li2020oscar}
Xiujun Li, Xi~Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan
  Wang, Houdong Hu, Li~Dong, Furu Wei, et~al.
\newblock Oscar: Object-semantics aligned pre-training for vision-language
  tasks.
\newblock {\em arXiv preprint arXiv:2004.06165}, 2020.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em ECCV}, 2014.

\bibitem{liu2020adversarial}
Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu~Wang, Hoifung Poon, and
  Jianfeng Gao.
\newblock Adversarial training for large neural language models.
\newblock {\em arXiv preprint arXiv:2004.08994}, 2020.

\bibitem{lu2019vilbert}
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
\newblock Vilbert: Pretraining task-agnostic visiolinguistic representations
  for vision-and-language tasks.
\newblock In {\em NeurIPS}, 2019.

\bibitem{lu201912}
Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee.
\newblock 12-in-1: Multi-task vision and language representation learning.
\newblock {\em arXiv preprint arXiv:1912.02315}, 2019.

\bibitem{lu2016hierarchical}
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.
\newblock Hierarchical question-image co-attention for visual question
  answering.
\newblock In {\em NeurIPS}, 2016.

\bibitem{luo2020univilm}
Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Xilin
  Chen, and Ming Zhou.
\newblock Univilm: A unified video and language pre-training model for
  multimodal understanding and generation.
\newblock {\em arXiv preprint arXiv:2002.06353}, 2020.

\bibitem{madry2017towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock {\em arXiv preprint arXiv:1706.06083}, 2017.

\bibitem{majumdar2020improving}
Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, and
  Dhruv Batra.
\newblock Improving vision-and-language navigation with image-text pairs from
  the web.
\newblock {\em arXiv preprint arXiv:2004.14973}, 2020.

\bibitem{mao2019metric}
Chengzhi Mao, Ziyuan Zhong, Junfeng Yang, Carl Vondrick, and Baishakhi Ray.
\newblock Metric learning for adversarial robustness.
\newblock In {\em NeurIPS}, 2019.

\bibitem{miyato2016adversarial}
Takeru Miyato, Andrew~M Dai, and Ian Goodfellow.
\newblock Adversarial training methods for semi-supervised text classification.
\newblock {\em arXiv preprint arXiv:1605.07725}, 2016.

\bibitem{miyato2018virtual}
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii.
\newblock Virtual adversarial training: a regularization method for supervised
  and semi-supervised learning.
\newblock {\em PAMI}, 2018.

\bibitem{murahari2019large}
Vishvak Murahari, Dhruv Batra, Devi Parikh, and Abhishek Das.
\newblock Large-scale pretraining for visual dialog: A simple state-of-the-art
  baseline.
\newblock {\em arXiv preprint arXiv:1912.02379}, 2019.

\bibitem{norcliffe2018learning}
Will Norcliffe-Brown, Stathis Vafeias, and Sarah Parisot.
\newblock Learning conditioned graph structures for interpretable visual
  question answering.
\newblock In {\em NeurIPS}, 2018.

\bibitem{ordonez2011im2text}
Vicente Ordonez, Girish Kulkarni, and Tamara~L Berg.
\newblock Im2text: Describing images using 1 million captioned photographs.
\newblock In {\em NeurIPS}, 2011.

\bibitem{pang2020boosting}
Tianyu Pang, Xiao Yang, Yinpeng Dong, Kun Xu, Hang Su, and Jun Zhu.
\newblock Boosting adversarial training with hypersphere embedding.
\newblock {\em arXiv preprint arXiv:2002.08619}, 2020.

\bibitem{peng2018dynamic}
Gao Peng, Zhengkai Jiang, Haoxuan You, Pan Lu, Steven Hoi, Xiaogang Wang, and
  Hongsheng Li.
\newblock Dynamic fusion with intra-and inter-modality attention flow for
  visual question answering.
\newblock {\em arXiv preprint arXiv:1812.05252}, 2018.

\bibitem{plummer2015flickr30k}
Bryan~A Plummer, Liwei Wang, Chris~M Cervantes, Juan~C Caicedo, Julia
  Hockenmaier, and Svetlana Lazebnik.
\newblock Flickr30k entities: Collecting region-to-phrase correspondences for
  richer image-to-sentence models.
\newblock In {\em ICCV}, 2015.

\bibitem{ramakrishnan2018overcoming}
Sainandan Ramakrishnan, Aishwarya Agrawal, and Stefan Lee.
\newblock Overcoming language priors in visual question answering with
  adversarial regularization.
\newblock In {\em NeurIPS}, 2018.

\bibitem{ribeiro2018semantically}
Marco~Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
\newblock Semantically equivalent adversarial rules for debugging nlp models.
\newblock In {\em ACL}, 2018.

\bibitem{santoro2017simple}
Adam Santoro, David Raposo, David~G Barrett, Mateusz Malinowski, Razvan
  Pascanu, Peter Battaglia, and Timothy Lillicrap.
\newblock A simple neural network module for relational reasoning.
\newblock In {\em NeurIPS}, 2017.

\bibitem{shafahi2019adversarial}
Ali Shafahi, Mahyar Najibi, Mohammad~Amin Ghiasi, Zheng Xu, John Dickerson,
  Christoph Studer, Larry~S Davis, Gavin Taylor, and Tom Goldstein.
\newblock Adversarial training for free!
\newblock In {\em NeurIPS}, 2019.

\bibitem{shah2019cycle}
Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh.
\newblock Cycle-consistency for robust visual question answering.
\newblock In {\em CVPR}, 2019.

\bibitem{sharma2018conceptual}
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset
  for automatic image captioning.
\newblock In {\em ACL}, 2018.

\bibitem{stanforth2019labels}
Robert Stanforth, Alhussein Fawzi, Pushmeet Kohli, et~al.
\newblock Are labels required for improving adversarial robustness?
\newblock {\em arXiv preprint arXiv:1905.13725}, 2019.

\bibitem{su2019vl}
Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai.
\newblock Vl-bert: Pre-training of generic visual-linguistic representations.
\newblock {\em arXiv preprint arXiv:1908.08530}, 2019.

\bibitem{suhr2018corpus}
Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi.
\newblock A corpus for reasoning about natural language grounded in
  photographs.
\newblock {\em arXiv preprint arXiv:1811.00491}, 2018.

\bibitem{sun2019contrastive}
Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid.
\newblock Contrastive bidirectional transformer for temporal representation
  learning.
\newblock {\em arXiv preprint arXiv:1906.05743}, 2019.

\bibitem{sun2019videobert}
Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid.
\newblock Videobert: A joint model for video and language representation
  learning.
\newblock In {\em ICCV}, 2019.

\bibitem{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock {\em arXiv preprint arXiv:1312.6199}, 2013.

\bibitem{tan2019lxmert}
Hao Tan and Mohit Bansal.
\newblock Lxmert: Learning cross-modality encoder representations from
  transformers.
\newblock In {\em EMNLP}, 2019.

\bibitem{tramer2017ensemble}
Florian Tram{\`e}r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan
  Boneh, and Patrick McDaniel.
\newblock Ensemble adversarial training: Attacks and defenses.
\newblock {\em arXiv preprint arXiv:1705.07204}, 2017.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em NeurIPS}, 2017.

\bibitem{wang2019improving}
Dilin Wang, Chengyue Gong, and Qiang Liu.
\newblock Improving neural language modeling via adversarial training.
\newblock {\em arXiv preprint arXiv:1906.03805}, 2019.

\bibitem{wang2020vd}
Yue Wang, Shafiq Joty, Michael~R Lyu, Irwin King, Caiming Xiong, and Steven~CH
  Hoi.
\newblock Vd-bert: A unified vision and dialog transformer with bert.
\newblock {\em arXiv preprint arXiv:2004.13278}, 2020.

\bibitem{wong2020fast}
Eric Wong, Leslie Rice, and J~Zico Kolter.
\newblock Fast is better than free: Revisiting adversarial training.
\newblock {\em arXiv preprint arXiv:2001.03994}, 2020.

\bibitem{xia2020xgpt}
Qiaolin Xia, Haoyang Huang, Nan Duan, Dongdong Zhang, Lei Ji, Zhifang Sui,
  Edward Cui, Taroon Bharti, and Ming Zhou.
\newblock Xgpt: Cross-modal generative pre-training for image captioning.
\newblock {\em arXiv preprint arXiv:2003.01473}, 2020.

\bibitem{xie2019adversarial}
Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan Yuille, and Quoc~V Le.
\newblock Adversarial examples improve image recognition.
\newblock {\em arXiv preprint arXiv:1911.09665}, 2019.

\bibitem{xie2019feature}
Cihang Xie, Yuxin Wu, Laurens van~der Maaten, Alan~L Yuille, and Kaiming He.
\newblock Feature denoising for improving adversarial robustness.
\newblock In {\em CVPR}, 2019.

\bibitem{xie2019visual}
Ning Xie, Farley Lai, Derek Doran, and Asim Kadav.
\newblock Visual entailment: A novel task for fine-grained image understanding.
\newblock {\em arXiv preprint arXiv:1901.06706}, 2019.

\bibitem{xie2019unsupervised}
Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc~V Le.
\newblock Unsupervised data augmentation for consistency training.
\newblock 2019.

\bibitem{xu2019exact}
Yan Xu, Baoyuan Wu, Fumin Shen, Yanbo Fan, Yong Zhang, Heng~Tao Shen, and Wei
  Liu.
\newblock Exact adversarial attack to image captioning via structured output
  learning with latent variables.
\newblock In {\em CVPR}, 2019.

\bibitem{yang2016stacked}
Zichao Yang, Xiaodong He, Jianfeng Gao, Li~Deng, and Alex Smola.
\newblock Stacked attention networks for image question answering.
\newblock In {\em CVPR}, 2016.

\bibitem{yu2016modeling}
Licheng Yu, Patrick Poirson, Shan Yang, Alexander~C Berg, and Tamara~L Berg.
\newblock Modeling context in referring expressions.
\newblock In {\em ECCV}, 2016.

\bibitem{yu2019deep}
Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi~Tian.
\newblock Deep modular co-attention networks for visual question answering.
\newblock In {\em CVPR}, 2019.

\bibitem{yu2017multi}
Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao.
\newblock Multi-modal factorized bilinear pooling with co-attention learning
  for visual question answering.
\newblock In {\em ICCV}, 2017.

\bibitem{zellers2019recognition}
Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock From recognition to cognition: Visual commonsense reasoning.
\newblock In {\em CVPR}, 2019.

\bibitem{zhang2019you}
Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong.
\newblock You only propagate once: Accelerating adversarial training via
  maximal principle.
\newblock In {\em NeurIPS}, 2019.

\bibitem{zhang2019defense}
Haichao Zhang and Jianyu Wang.
\newblock Defense against adversarial attacks using feature scattering-based
  adversarial training.
\newblock In {\em NeurIPS}, 2019.

\bibitem{zhang2019theoretically}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric~P Xing, Laurent~El Ghaoui, and
  Michael~I Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock {\em arXiv preprint arXiv:1901.08573}, 2019.

\bibitem{zhou2019unified}
Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason~J Corso, and Jianfeng
  Gao.
\newblock Unified vision-language pre-training for image captioning and vqa.
\newblock {\em arXiv preprint arXiv:1909.11059}, 2019.

\bibitem{zhu2019freelb}
Chen Zhu, Yu~Cheng, Zhe Gan, Siqi Sun, Thomas Goldstein, and Jingjing Liu.
\newblock Freelb: Enhanced adversarial training for language understanding.
\newblock {\em arXiv preprint arXiv:1909.11764}, 2019.

\end{thebibliography}
