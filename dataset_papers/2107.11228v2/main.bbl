\begin{thebibliography}{97}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{Conference on Neural Information Processing Systems}, pages
  6389--6399, 2018.

\bibitem[Ballard et~al.(2017)Ballard, Das, Martiniani, Mehta, Sagun, Stevenson,
  and Wales]{ballard2017energy}
Andrew~J Ballard, Ritankar Das, Stefano Martiniani, Dhagash Mehta, Levent
  Sagun, Jacob~D Stevenson, and David~J Wales.
\newblock Energy landscapes for machine learning.
\newblock \emph{Physical Chemistry Chemical Physics}, 19\penalty0
  (20):\penalty0 12585--12603, 2017.

\bibitem[Keskar et~al.(2017)Keskar, Nocedal, Tang, Mudigere, and
  Smelyanskiy]{keskar2016large}
Nitish~Shirish Keskar, Jorge Nocedal, Ping Tak~Peter Tang, Dheevatsa Mudigere,
  and Mikhail Smelyanskiy.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Yao et~al.(2018{\natexlab{a}})Yao, Gholami, Keutzer, and
  Mahoney]{YGKM18_TRv1}
Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael~W Mahoney.
\newblock Large batch size training of neural networks with adversarial
  training and second-order information.
\newblock Technical Report Preprint: arXiv:1810.01021, 2018{\natexlab{a}}.

\bibitem[Yao et~al.(2018{\natexlab{b}})Yao, Gholami, Lei, Keutzer, and
  Mahoney]{yao2018hessian}
Zhewei Yao, Amir Gholami, Qi~Lei, Kurt Keutzer, and Michael~W Mahoney.
\newblock Hessian-based analysis of large batch training and robustness to
  adversaries.
\newblock In \emph{Conference on Neural Information Processing Systems},
  volume~31, pages 4949--4959, 2018{\natexlab{b}}.

\bibitem[Li and Yuan(2017)]{li2017convergence}
Yuanzhi Li and Yang Yuan.
\newblock Convergence analysis of two-layer neural networks with {ReLU}
  activation.
\newblock In \emph{Conference on Neural Information Processing Systems}, pages
  597--607, 2017.

\bibitem[Santurkar et~al.(2018)Santurkar, Tsipras, Ilyas, and
  M{\k{a}}dry]{santurkar2018does}
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander M{\k{a}}dry.
\newblock How does batch normalization help optimization?
\newblock In \emph{Neural Information Processing Systems}, pages 2488--2498,
  2018.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh2017sharp}
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{International Conference on Machine Learning}, pages
  1019--1028, 2017.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Bhojanapalli, and
  Srebro]{neyshabur2017pac}
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.
\newblock A {PAC}-{B}ayesian approach to spectrally-normalized margin bounds
  for neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Foret et~al.(2020)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2020sharpness}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock \emph{arXiv preprint arXiv:2010.01412}, 2020.

\bibitem[Yao et~al.(2020)Yao, Gholami, Keutzer, and Mahoney]{yao2020pyhessian}
Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael~W Mahoney.
\newblock Py{H}essian: Neural networks through the lens of the hessian.
\newblock In \emph{IEEE International Conference on Big Data (Big Data)}, pages
  581--590, 2020.

\bibitem[Engel and den Broeck(2001)]{EB01_BOOK}
Andreas Engel and Christian P. L.~Van den Broeck.
\newblock \emph{Statistical mechanics of learning}.
\newblock Cambridge University Press, New York, NY, USA, 2001.

\bibitem[Martin and Mahoney(2017)]{martin2017rethinking}
Charles~H Martin and Michael~W Mahoney.
\newblock Rethinking generalization requires revisiting old ideas: statistical
  mechanics approaches and complex learning behavior.
\newblock Technical Report Preprint: arXiv:1710.09553, 2017.

\bibitem[Granziol(2020)]{granziol2020flatness}
Diego Granziol.
\newblock Flatness is a false friend.
\newblock Technical Report Preprint: arXiv:2006.09091, 2020.

\bibitem[Tsipras et~al.(2019)Tsipras, Santurkar, Engstrom, Turner, and
  Madry]{tsipras2018robustness}
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
  Aleksander Madry.
\newblock Robustness may be at odds with accuracy.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Martin and
  Mahoney(2021{\natexlab{a}})]{martin2018implicit_JMLRversion}
Charles~H Martin and Michael~W Mahoney.
\newblock Implicit self-regularization in deep neural networks: Evidence from
  random matrix theory and implications for learning.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (165):\penalty0 1--73, 2021{\natexlab{a}}.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{garipov2018loss}
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, and
  Andrew~Gordon Wilson.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock In \emph{Conference on Neural Information Processing Systems}, pages
  8803--8812, 2018.

\bibitem[Draxler et~al.(2018)Draxler, Veschgini, Salmhofer, and
  Hamprecht]{draxler2018essentially}
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht.
\newblock Essentially no barriers in neural network energy landscape.
\newblock In \emph{International conference on machine learning}, pages
  1309--1318, 2018.

\bibitem[Kornblith et~al.(2019)Kornblith, Norouzi, Lee, and
  Hinton]{kornblith2019similarity}
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton.
\newblock Similarity of neural network representations revisited.
\newblock In \emph{International Conference on Machine Learning}, pages
  3519--3529, 2019.

\bibitem[Advani et~al.(2013)Advani, Lahiri, and Ganguli]{Advani_2013}
Madhu Advani, Subhaneil Lahiri, and Surya Ganguli.
\newblock Statistical mechanics of complex neural systems and high dimensional
  data.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2013\penalty0 (03):\penalty0 P03014, mar 2013.
\newblock \doi{10.1088/1742-5468/2013/03/p03014}.
\newblock URL \url{https://doi.org/10.1088/1742-5468/2013/03/p03014}.

\bibitem[Belkin et~al.(2019)Belkin, Hsu, Ma, and Mandal]{belkin2019reconciling}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (32):\penalty0 15849--15854, 2019.

\bibitem[Belkin et~al.(2020)Belkin, Hsu, and Xu]{belkin2020two}
Mikhail Belkin, Daniel Hsu, and Ji~Xu.
\newblock Two models of double descent for weak features.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 2\penalty0
  (4):\penalty0 1167--1180, 2020.

\bibitem[Liao et~al.(2020)Liao, Couillet, and Mahoney]{liao2020random}
Zhenyu Liao, Romain Couillet, and Michael~W Mahoney.
\newblock A random matrix analysis of random fourier features: beyond the
  {G}aussian kernel, a precise phase transition, and the corresponding double
  descent.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2020.

\bibitem[Derezi{\'n}ski et~al.(2020{\natexlab{a}})Derezi{\'n}ski, Liang, and
  Mahoney]{derezinski2019exact}
Micha{\l} Derezi{\'n}ski, Feynman Liang, and Michael~W Mahoney.
\newblock Exact expressions for double descent and implicit regularization via
  surrogate random design.
\newblock In \emph{Conference on Neural Information Processing Systems},
  volume~33, 2020{\natexlab{a}}.

\bibitem[Amit et~al.(1985)Amit, Gutfreund, and Sompolinsky]{amit1985storing}
Daniel~J Amit, Hanoch Gutfreund, and Haim Sompolinsky.
\newblock Storing infinite numbers of patterns in a spin-glass model of neural
  networks.
\newblock \emph{Physical Review Letters}, 55\penalty0 (14):\penalty0 1530,
  1985.

\bibitem[Choromanska et~al.(2015)Choromanska, Henaff, Mathieu, Arous, and
  LeCun]{choromanska2015loss}
Anna Choromanska, Mikael Henaff, Michael Mathieu, G{\'e}rard~Ben Arous, and
  Yann LeCun.
\newblock The loss surfaces of multilayer networks.
\newblock In \emph{Artificial intelligence and statistics}, pages 192--204,
  2015.

\bibitem[Agliari et~al.(2014)Agliari, Barra, Galluzzi, Tantari, and
  Tavani]{agliari2014walk}
Elena Agliari, Adriano Barra, Andrea Galluzzi, Daniele Tantari, and Flavia
  Tavani.
\newblock A walk in the statistical mechanical formulation of neural networks.
\newblock In \emph{International Joint Conference on Computational
  Intelligence}, pages 210--217, 2014.

\bibitem[Fuhs and Touretzky(2006)]{fuhs2006spin}
Mark~C Fuhs and David~S Touretzky.
\newblock A spin glass model of path integration in rat medial entorhinal
  cortex.
\newblock \emph{The Journal of neuroscience: the official journal of the
  Society for Neuroscience}, 26\penalty0 (16):\penalty0 4266--4276, 2006.

\bibitem[Hudetz et~al.(2014)Hudetz, Humphries, and Binder]{hudetz2014spin}
Anthony~G Hudetz, Colin~J Humphries, and Jeffrey~R Binder.
\newblock Spin-glass model predicts metastable brain states that diminish in
  anesthesia.
\newblock \emph{Frontiers in systems neuroscience}, 8:\penalty0 234, 2014.

\bibitem[Recio and Torres(2016)]{recio2016emergence}
Ibon Recio and Joaqu{\'\i}n~J Torres.
\newblock Emergence of low noise frustrated states in {E}/{I} balanced neural
  networks.
\newblock \emph{Neural Networks}, 84:\penalty0 91--101, 2016.

\bibitem[Bryngelson and Wolynes(1987)]{bryngelson1987spin}
Joseph~D Bryngelson and Peter~G Wolynes.
\newblock Spin glasses and the statistical mechanics of protein folding.
\newblock \emph{Proceedings of the National Academy of sciences}, 84\penalty0
  (21):\penalty0 7524--7528, 1987.

\bibitem[Garstecki et~al.(1999)Garstecki, Hoang, and
  Cieplak]{garstecki1999energy}
Piotr Garstecki, Trinh~Xuan Hoang, and Marek Cieplak.
\newblock Energy landscapes, supergraphs, and “folding funnels” in spin
  systems.
\newblock \emph{Physical Review E}, 60\penalty0 (3):\penalty0 3219, 1999.

\bibitem[Klemm et~al.(2008)Klemm, Flamm, and Stadler]{klemm2007funnels}
Konstantin Klemm, Christoph Flamm, and Peter~F Stadler.
\newblock Funnels in energy landscapes.
\newblock \emph{The European Physical Journal B}, 63\penalty0 (3):\penalty0
  387--391, 2008.

\bibitem[Brooks et~al.(2001)Brooks, Onuchic, and Wales]{brooks2001taking}
Charles~L Brooks, Jos{\'e}~N Onuchic, and David~J Wales.
\newblock Taking a walk on a landscape.
\newblock \emph{Science}, 293\penalty0 (5530):\penalty0 612--613, 2001.

\bibitem[Wales(2003)]{wales_book}
D.~J. Wales.
\newblock \emph{Energy Landscapes: Applications to Clusters, Biomolecules and
  Glasses}.
\newblock Cambridge University Press, 2003.

\bibitem[Stillinger(2016)]{stillinger_book}
F.~H. Stillinger.
\newblock \emph{Energy Landscapes, Inherent Structures, and Condensed-Matter
  Phenomena}.
\newblock Princeton University Press, 2016.

\bibitem[Neal et~al.(2018)Neal, Mittal, Baratin, Tantia, Scicluna,
  Lacoste-Julien, and Mitliagkas]{neal2018modern}
Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna,
  Simon Lacoste-Julien, and Ioannis Mitliagkas.
\newblock A modern take on the bias-variance tradeoff in neural networks.
\newblock Technical Report Preprint: arXiv:1810.08591, 2018.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{IEEE conference on computer vision and pattern recognition},
  pages 770--778, 2016.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Nakkiran et~al.(2019)Nakkiran, Kaplun, Bansal, Yang, Barak, and
  Sutskever]{nakkiran2019deep}
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya
  Sutskever.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Martin and Mahoney(2021{\natexlab{b}})]{MM21a_simpsons_TR}
Charles~H Martin and Michael~W Mahoney.
\newblock Post-mortem on a deep learning contest: a {S}impson's paradox and the
  complementary roles of scale metrics versus shape metrics.
\newblock Technical Report Preprint: arXiv:2106.00734, 2021{\natexlab{b}}.

\bibitem[Miceli-Barone et~al.(2017)Miceli-Barone, Haddow, Germann, and
  Sennrich]{barone2017regularization}
Antonio~Valerio Miceli-Barone, Barry Haddow, Ulrich Germann, and Rico Sennrich.
\newblock Regularization techniques for fine-tuning in neural machine
  translation.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, pages 1489--1494, 2017.

\bibitem[Gal and Ghahramani(2016)]{gal2016theoretically}
Yarin Gal and Zoubin Ghahramani.
\newblock A theoretically grounded application of dropout in recurrent neural
  networks.
\newblock In \emph{Conference on Neural Information Processing Systems},
  volume~29, pages 1019--1027, 2016.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch {SGD}: Training {I}mage{N}et in 1 hour.
\newblock Technical Report Preprint: arXiv:1706.02677, 2017.

\bibitem[Xing et~al.(2018)Xing, Arpit, Tsirigotis, and Bengio]{xing2018walk}
Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio.
\newblock A walk with {SGD}.
\newblock Technical Report Preprint: arXiv:1802.08770, 2018.

\bibitem[Kleinberg et~al.(2018)Kleinberg, Li, and
  Yuan]{kleinberg2018alternative}
Bobby Kleinberg, Yuanzhi Li, and Yang Yuan.
\newblock An alternative view: When does sgd escape local minima?
\newblock In \emph{International Conference on Machine Learning}, pages
  2698--2707, 2018.

\bibitem[Mahoney and Martin(2019)]{martin2019traditional}
Michael Mahoney and Charles Martin.
\newblock Traditional and heavy tailed self regularization in neural network
  models.
\newblock In \emph{International Conference on Machine Learning}, pages
  4284--4293, 2019.

\bibitem[Martin and Mahoney(2020)]{martin2020heavy}
Charles~H Martin and Michael~W Mahoney.
\newblock Heavy-tailed universality predicts trends in test accuracies for very
  large pre-trained deep neural networks.
\newblock In \emph{SIAM International Conference on Data Mining}, pages
  505--513. SIAM, 2020.

\bibitem[Martin et~al.(2021)Martin, Peng, and
  Mahoney]{martin2020predicting_NatComm}
Charles~H Martin, Tongsu~Serena Peng, and Michael~W Mahoney.
\newblock Predicting trends in the quality of state-of-the-art neural networks
  without access to training or testing data.
\newblock \emph{Nature Communications}, 12\penalty0 (1):\penalty0 1--13, 2021.

\bibitem[Simsekli et~al.(2019)Simsekli, Sagun, and
  Gurbuzbalaban]{simsekli2019tail}
Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban.
\newblock A tail-index analysis of stochastic gradient noise in deep neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  5827--5837, 2019.

\bibitem[Hodgkinson and Mahoney(2017)]{hodgkinson2020multiplicative}
Liam Hodgkinson and Michael~W Mahoney.
\newblock Multiplicative noise and heavy tails in stochastic optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  1019--1028, 2017.

\bibitem[Fort et~al.(2019)Fort, Hu, and Lakshminarayanan]{fort2019deep}
Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan.
\newblock Deep ensembles: A loss landscape perspective.
\newblock \emph{arXiv preprint arXiv:1912.02757}, 2019.

\bibitem[Fort et~al.(2020)Fort, Dziugaite, Paul, Kharaghani, Roy, and
  Ganguli]{fort2020deep}
Stanislav Fort, Gintare~Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani,
  Daniel~M Roy, and Surya Ganguli.
\newblock Deep learning versus kernel learning: an empirical study of loss
  landscape geometry and the time evolution of the neural tangent kernel.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2020.

\bibitem[Sun et~al.(2020)Sun, Li, Liang, Ding, and Srikant]{sun2020global}
Ruoyu Sun, Dawei Li, Shiyu Liang, Tian Ding, and Rayadurgam Srikant.
\newblock The global landscape of neural networks: An overview.
\newblock \emph{IEEE Signal Processing Magazine}, 37\penalty0 (5):\penalty0
  95--108, 2020.

\bibitem[Dauphin et~al.(2014)Dauphin, Pascanu, Gulcehre, Cho, Ganguli, and
  Bengio]{dauphin2014identifying}
Yann~N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli,
  and Yoshua Bengio.
\newblock Identifying and attacking the saddle point problem in
  high-dimensional non-convex optimization.
\newblock In \emph{Conference on Neural Information Processing Systems}, pages
  2933--2941, 2014.

\bibitem[Ge et~al.(2015)Ge, Huang, Jin, and Yuan]{ge2015escaping}
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.
\newblock Escaping from saddle points—online stochastic gradient for tensor
  decomposition.
\newblock In \emph{Conference on learning theory}, pages 797--842, 2015.

\bibitem[Du et~al.(2017)Du, Jin, Jordan, P{\'o}czos, Singh, and
  Lee]{du2017gradient}
SS~Du, C~Jin, MI~Jordan, B~P{\'o}czos, A~Singh, and JD~Lee.
\newblock Gradient descent can take exponential time to escape saddle points.
\newblock In \emph{Conference on Neural Information Processing Systems}, pages
  1068--1078, 2017.

\bibitem[Jin et~al.(2017)Jin, Ge, Netrapalli, Kakade, and
  Jordan]{jin2017escape}
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham~M Kakade, and Michael~I Jordan.
\newblock How to escape saddle points efficiently.
\newblock In \emph{International Conference on Machine Learning}, pages
  1724--1732, 2017.

\bibitem[Safran and Shamir(2018)]{safran2018spurious}
Itay Safran and Ohad Shamir.
\newblock Spurious local minima are common in two-layer relu neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  4433--4441, 2018.

\bibitem[Soltanolkotabi et~al.(2018)Soltanolkotabi, Javanmard, and
  Lee]{soltanolkotabi2018theoretical}
Mahdi Soltanolkotabi, Adel Javanmard, and Jason~D Lee.
\newblock Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks.
\newblock \emph{IEEE Transactions on Information Theory}, 65\penalty0
  (2):\penalty0 742--769, 2018.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  1675--1685, 2019.

\bibitem[Novak et~al.(2018)Novak, Bahri, Abolafia, Pennington, and
  Sohl-Dickstein]{novak2018sensitivity}
Roman Novak, Yasaman Bahri, Daniel~A Abolafia, Jeffrey Pennington, and Jascha
  Sohl-Dickstein.
\newblock Sensitivity and generalization in neural networks: an empirical
  study.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Liu et~al.(2019)Liu, Bai, Jiang, Chen, and Wang]{liu2020understanding}
Jinlong Liu, Yunzhi Bai, Guoqing Jiang, Ting Chen, and Huayan Wang.
\newblock Understanding why neural networks generalize well through {GSNR} of
  parameters.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[McAllester(1999)]{mcallester1999pac}
David~A McAllester.
\newblock {PAC}-{B}ayesian model averaging.
\newblock In \emph{Annual Conference on Computational Learning Theory}, pages
  164--170, 1999.

\bibitem[Dziugaite and Roy(2017)]{dziugaite2017computing}
Gintare~Karolina Dziugaite and Daniel~M Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock In \emph{Annual Conference on Uncertainty in Artificial Intelligence
  (UAI)}, 2017.

\bibitem[Jiang et~al.(2019)Jiang, Neyshabur, Mobahi, Krishnan, and
  Bengio]{jiang2019fantastic}
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy
  Bengio.
\newblock Fantastic generalization measures and where to find them.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Geiger et~al.(2019)Geiger, Spigler, d'Ascoli, Sagun, Baity-Jesi,
  Biroli, and Wyart]{geiger2019jamming}
Mario Geiger, Stefano Spigler, St{\'e}phane d'Ascoli, Levent Sagun, Marco
  Baity-Jesi, Giulio Biroli, and Matthieu Wyart.
\newblock Jamming transition as a paradigm to understand the loss landscape of
  deep neural networks.
\newblock \emph{Physical Review E}, 100\penalty0 (1):\penalty0 012115, 2019.

\bibitem[Sagun et~al.(2016)Sagun, Bottou, and LeCun]{sagun2016eigenvalues}
Levent Sagun, Leon Bottou, and Yann LeCun.
\newblock Eigenvalues of the {H}essian in deep learning: Singularity and
  beyond.
\newblock Technical Report Preprint: arXiv:1611.07476, 2016.

\bibitem[Gur-Ari et~al.(2018)Gur-Ari, Roberts, and Dyer]{gur2018gradient}
Guy Gur-Ari, Daniel~A Roberts, and Ethan Dyer.
\newblock Gradient descent happens in a tiny subspace.
\newblock Technical Report Preprint: arXiv:1812.04754, 2018.

\bibitem[Papyan(2020)]{papyan2020traces}
Vardan Papyan.
\newblock Traces of class/cross-class structure pervade deep learning spectra.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (252):\penalty0 1--64, 2020.

\bibitem[Fort and Ganguli(2019)]{fort2019emergent}
Stanislav Fort and Surya Ganguli.
\newblock Emergent properties of the local geometry of neural loss landscapes.
\newblock Technical Report Preprint: arXiv:1910.05929, 2019.

\bibitem[He et~al.(2019)He, Huang, and Yuan]{he2019asymmetric}
Haowei He, Gao Huang, and Yang Yuan.
\newblock Asymmetric valleys: Beyond sharp and flat local minima.
\newblock In \emph{Conference on Neural Information Processing Systems}, pages
  2553--2564, 2019.

\bibitem[Chaudhari et~al.(2019)Chaudhari, Choromanska, Soatto, LeCun, Baldassi,
  Borgs, Chayes, Sagun, and Zecchina]{chaudhari2019entropy}
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi,
  Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina.
\newblock Entropy-{SGD}: biasing gradient descent into wide valleys.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  12\penalty0 (12):\penalty0 124018, 2019.

\bibitem[Izmailov et~al.(2018)Izmailov, Wilson, Podoprikhin, Vetrov, and
  Garipov]{izmailov2018averaging}
P~Izmailov, AG~Wilson, D~Podoprikhin, D~Vetrov, and T~Garipov.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, pages
  876--885, 2018.

\bibitem[Dong et~al.(2019)Dong, Yao, Gholami, Mahoney, and
  Keutzer]{dong2019hawq}
Zhen Dong, Zhewei Yao, Amir Gholami, Michael~W Mahoney, and Kurt Keutzer.
\newblock {HAWQ}: Hessian aware quantization of neural networks with
  mixed-precision.
\newblock In \emph{IEEE/CVF International Conference on Computer Vision}, pages
  293--302, 2019.

\bibitem[Shen et~al.(2020)Shen, Dong, Ye, Ma, Yao, Gholami, Mahoney, and
  Keutzer]{shen2020q}
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
  Michael~W Mahoney, and Kurt Keutzer.
\newblock Q-{BERT}: Hessian based ultra low precision quantization of bert.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, volume~34,
  pages 8815--8821, 2020.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Vinyals, and
  Saxe]{goodfellow2014qualitatively}
Ian~J Goodfellow, Oriol Vinyals, and Andrew~M Saxe.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock Technical Report Preprint: arXiv:1412.6544, 2014.

\bibitem[Cooper(2018)]{cooper2018loss}
Yaim Cooper.
\newblock The loss landscape of overparameterized neural networks.
\newblock Technical Report Preprint: arXiv:1804.10200, 2018.

\bibitem[Freeman and Bruna(2017)]{freeman2016topology}
C~Daniel Freeman and Joan Bruna.
\newblock Topology and geometry of half-rectified network optimization.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Nguyen(2019)]{nguyen2019connected}
Quynh Nguyen.
\newblock On connected sublevel sets in deep learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  4790--4799, 2019.

\bibitem[Kuditipudi et~al.(2019)Kuditipudi, Wang, Lee, Zhang, Li, Hu, Ge, and
  Arora]{kuditipudi2019explaining}
Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi~Zhang, Zhiyuan Li, Wei Hu, Rong
  Ge, and Sanjeev Arora.
\newblock Explaining landscape connectivity of low-cost solutions for
  multilayer nets.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 14601--14610, 2019.

\bibitem[Shevchenko and Mondelli(2020)]{shevchenko2020landscape}
Alexander Shevchenko and Marco Mondelli.
\newblock Landscape connectivity and dropout stability of {SGD} solutions for
  over-parameterized neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  8773--8784, 2020.

\bibitem[Frankle et~al.(2020)Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020linear}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel Roy, and Michael Carbin.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{International Conference on Machine Learning}, pages
  3259--3269, 2020.

\bibitem[Fort and Jastrzebski(2019)]{fort2019large}
Stanislav Fort and Stanislaw Jastrzebski.
\newblock Large scale structure of neural network loss landscapes.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 6709--6717, 2019.

\bibitem[Muthukumar et~al.(2020)Muthukumar, Vodrahalli, Subramanian, and
  Sahai]{muthukumar2020harmless}
Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai.
\newblock Harmless interpolation of noisy data in regression.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  1\penalty0 (1):\penalty0 67--83, 2020.

\bibitem[Mei and Montanari()]{mei2019generalization}
Song Mei and Andrea Montanari.
\newblock The generalization error of random features regression: Precise
  asymptotics and the double descent curve.
\newblock \emph{Communications on Pure and Applied Mathematics}.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and
  Tsigler]{bartlett2020benign}
Peter~L Bartlett, Philip~M Long, G{\'a}bor Lugosi, and Alexander Tsigler.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (48):\penalty0 30063--30070, 2020.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Chen et~al.(2020)Chen, Min, Belkin, and Karbasi]{chen2020multiple}
Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi.
\newblock Multiple descent: Design your own generalization curve.
\newblock Technical Report Preprint: arXiv:2008.01036, 2020.

\bibitem[Adlam and Pennington(2020)]{adlam2020neural}
Ben Adlam and Jeffrey Pennington.
\newblock The neural tangent kernel in high dimensions: Triple descent and a
  multi-scale theory of generalization.
\newblock In \emph{International Conference on Machine Learning}, pages 74--84,
  2020.

\bibitem[d'Ascoli et~al.(2020)d'Ascoli, Sagun, and Biroli]{d2020triple}
St{\'e}phane d'Ascoli, Levent Sagun, and Giulio Biroli.
\newblock Triple descent and the two kinds of overfitting: Where \& why do they
  appear?
\newblock In \emph{Conference on Neural Information Processing Systems}, 2020.

\bibitem[Derezi{\'n}ski et~al.(2020{\natexlab{b}})Derezi{\'n}ski, Khanna, and
  Mahoney]{DKM20_TR}
M.~Derezi{\'n}ski, R.~Khanna, and M.~W. Mahoney.
\newblock Improved guarantees and a multiple-descent curve for {C}olumn
  {S}ubset {S}election and the {N}ystrom method.
\newblock Technical Report Preprint: arXiv:2002.09073, 2020{\natexlab{b}}.

\bibitem[Yang et~al.(2020)Yang, Yu, You, Steinhardt, and
  Ma]{yang2020rethinking}
Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi~Ma.
\newblock Rethinking bias-variance trade-off for generalization of neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  10767--10777, 2020.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Cettolo et~al.(2012)Cettolo, Girardi, and Federico]{cettolo2012wit3}
Mauro Cettolo, Christian Girardi, and Marcello Federico.
\newblock Wit3: Web inventory of transcribed and translated talks.
\newblock In \emph{Conference of european association for machine translation},
  pages 261--268, 2012.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pages
  5998--6008, 2017.

\end{thebibliography}
