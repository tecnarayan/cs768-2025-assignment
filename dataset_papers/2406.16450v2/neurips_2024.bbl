\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{scalinglaw}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Ainslie et~al.(2023)Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy,
  Lebr{\'o}n, and Sanghai]{gqa}
Joshua Ainslie, James Lee-Thorp, Michiel de~Jong, Yury Zemlyanskiy, Federico
  Lebr{\'o}n, and Sumit Sanghai.
\newblock Gqa: Training generalized multi-query transformer models from
  multi-head checkpoints.
\newblock \emph{arXiv preprint arXiv:2305.13245}, 2023.

\bibitem[Vaswani et~al.(2017{\natexlab{a}})Vaswani, Shazeer, Parmar, Uszkoreit,
  Jones, Gomez, Kaiser, and Polosukhin]{transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30,
  2017{\natexlab{a}}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{gpt-2}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{gpt-3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Martin, Stone, Albert,
  Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{llama-2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{a}}.

\bibitem[Smith et~al.(2022)Smith, Patwary, Norick, LeGresley, Rajbhandari,
  Casper, Liu, Prabhumoye, Zerveas, Korthikanti, et~al.]{megatron}
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam
  Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,
  Vijay Korthikanti, et~al.
\newblock Using deepspeed and megatron to train megatron-turing nlg 530b, a
  large-scale generative language model.
\newblock \emph{arXiv preprint arXiv:2201.11990}, 2022.

\bibitem[Meta(2024)]{llama-3}
Meta.
\newblock Llama 3.
\newblock https://llama.meta.com/llama3/, 2024.

\bibitem[Team et~al.(2024)Team, Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak,
  Sifre, Rivi{\`e}re, Kale, Love, et~al.]{gemma}
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,
  Shreya Pathak, Laurent Sifre, Morgane Rivi{\`e}re, Mihir~Sanjay Kale,
  Juliette Love, et~al.
\newblock Gemma: Open models based on gemini research and technology.
\newblock \emph{arXiv preprint arXiv:2403.08295}, 2024.

\bibitem[Sharma et~al.(2023)Sharma, Ash, and Misra]{laser}
Pratyusha Sharma, Jordan~T Ash, and Dipendra Misra.
\newblock The truth is in there: Improving reasoning in language models with
  layer-selective rank reduction.
\newblock \emph{arXiv preprint arXiv:2312.13558}, 2023.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and
  Chen]{lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Arora et~al.(2019)Arora, Cohen, Hu, and Luo]{implicitrank}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Dao et~al.(2022{\natexlab{a}})Dao, Chen, Sohoni, Desai, Poli, Grogan,
  Liu, Rao, Rudra, and R{\'e}]{monarch}
Tri Dao, Beidi Chen, Nimit~S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan,
  Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher R{\'e}.
\newblock Monarch: Expressive structured matrices for efficient and accurate
  training.
\newblock In \emph{International Conference on Machine Learning}, pages
  4690--4721. PMLR, 2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2018)Zhang, Zhou, Lin, and Sun]{shufflenet}
Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
\newblock Shufflenet: An extremely efficient convolutional neural network for
  mobile devices.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 6848--6856, 2018.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{mobilenetv2}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4510--4520, 2018.

\bibitem[Williams et~al.(2009)Williams, Waterman, and Patterson]{roofline}
Samuel Williams, Andrew Waterman, and David Patterson.
\newblock Roofline: an insightful visual performance model for multicore
  architectures.
\newblock \emph{Communications of the ACM}, 52\penalty0 (4):\penalty0 65--76,
  2009.

\bibitem[Hong et~al.(2023)Hong, Dai, Xu, Mao, Li, Liu, Chen, Dong, and
  Wang]{flashdecoding++}
Ke~Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen,
  Hanyu Dong, and Yu~Wang.
\newblock Flashdecoding++: Faster large language model inference on gpus.
\newblock \emph{arXiv preprint arXiv:2311.01282}, 2023.

\bibitem[Saxe et~al.(2013)Saxe, McClelland, and Ganguli]{Saxe2013ExactST}
Andrew~M. Saxe, James~L. McClelland, and Surya Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{CoRR}, abs/1312.6120, 2013.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:17272965}.

\bibitem[Baldi and Hornik(1989)]{baldi1989neural}
Pierre Baldi and Kurt Hornik.
\newblock Neural networks and principal component analysis: Learning from
  examples without local minima.
\newblock \emph{Neural networks}, 2\penalty0 (1):\penalty0 53--58, 1989.

\bibitem[Frankle and Carbin(2019)]{FrankleC19}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{ICLR}, 2019.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{linformer}
Sinong Wang, Belinda~Z Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, et~al.]{bigbird}
Manzil Zaheer, Guru Guruganesh, Kumar~Avinava Dubey, Joshua Ainslie, Chris
  Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li~Yang,
  et~al.
\newblock Big bird: Transformers for longer sequences.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 17283--17297, 2020.

\bibitem[Shazeer(2019)]{mqa}
Noam Shazeer.
\newblock Fast transformer decoding: One write-head is all you need.
\newblock \emph{arXiv preprint arXiv:1911.02150}, 2019.

\bibitem[Kwon et~al.(2023)Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang,
  and Stoica]{pagedattention}
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody~Hao Yu,
  Joseph Gonzalez, Hao Zhang, and Ion Stoica.
\newblock Efficient memory management for large language model serving with
  pagedattention.
\newblock In \emph{Proceedings of the 29th Symposium on Operating Systems
  Principles}, pages 611--626, 2023.

\bibitem[Dao et~al.(2022{\natexlab{b}})Dao, Fu, Ermon, Rudra, and
  R{\'e}]{flashattention}
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 16344--16359, 2022{\natexlab{b}}.

\bibitem[Dao et~al.(2023)Dao, Haziza, Massa, and Sizov]{flashdecoding}
Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov.
\newblock Flash-deociding, 2023.
\newblock URL \url{https://crfm.stanford.edu/2023/10/12/flashdecoding.html}.

\bibitem[Lee-Thorp et~al.(2021)Lee-Thorp, Ainslie, Eckstein, and Ontanon]{fnet}
James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon.
\newblock Fnet: Mixing tokens with fourier transforms.
\newblock \emph{arXiv preprint arXiv:2105.03824}, 2021.

\bibitem[Choromanski et~al.(2022)Choromanski, Lin, Chen, Zhang, Sehanobish,
  Likhosherstov, Parker-Holder, Sarlos, Weller, and Weingarten]{block-toeplitz}
Krzysztof Choromanski, Han Lin, Haoxian Chen, Tianyi Zhang, Arijit Sehanobish,
  Valerii Likhosherstov, Jack Parker-Holder, Tamas Sarlos, Adrian Weller, and
  Thomas Weingarten.
\newblock From block-toeplitz matrices to differential equations on graphs:
  towards a general theory for scalable masked transformers.
\newblock In \emph{International Conference on Machine Learning}, pages
  3962--3983. PMLR, 2022.

\bibitem[Luo et~al.(2021)Luo, Li, Cai, He, Peng, Zheng, Ke, Wang, and
  Liu]{kernelized_attention}
Shengjie Luo, Shanda Li, Tianle Cai, Di~He, Dinglan Peng, Shuxin Zheng, Guolin
  Ke, Liwei Wang, and Tie-Yan Liu.
\newblock Stable, fast and accurate: Kernelized attention with relative
  positional encoding.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 22795--22807, 2021.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,
  and Dean]{moe}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
  Geoffrey Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock \emph{arXiv preprint arXiv:1701.06538}, 2017.

\bibitem[Fedus et~al.(2022)Fedus, Zoph, and Shazeer]{switchtransformer}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock \emph{The Journal of Machine Learning Research}, 23\penalty0
  (1):\penalty0 5232--5270, 2022.

\bibitem[Belcak and Wattenhofer(2023)]{belcak2023fast}
Peter Belcak and Roger Wattenhofer.
\newblock Fast feedforward networks.
\newblock \emph{arXiv preprint arXiv:2308.14711}, 2023.

\bibitem[Liu et~al.(2023)Liu, Li, Hall, Liang, and Ma]{sophia}
Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma.
\newblock Sophia: A scalable stochastic second-order optimizer for language
  model pre-training.
\newblock \emph{arXiv preprint arXiv:2305.14342}, 2023.

\bibitem[Zhao et~al.(2024)Zhao, Zhang, Chen, Wang, Anandkumar, and
  Tian]{galore}
Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and
  Yuandong Tian.
\newblock Galore: Memory-efficient llm training by gradient low-rank
  projection.
\newblock \emph{arXiv preprint arXiv:2403.03507}, 2024.

\bibitem[Xia et~al.(2023)Xia, Gao, Zeng, and Chen]{shearedllama}
Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen.
\newblock Sheared llama: Accelerating language model pre-training via
  structured pruning.
\newblock \emph{arXiv preprint arXiv:2310.06694}, 2023.

\bibitem[Cheng et~al.(2015)Cheng, Yu, Feris, Kumar, Choudhary, and
  Chang]{circulant}
Yu~Cheng, Felix~X Yu, Rogerio~S Feris, Sanjiv Kumar, Alok Choudhary, and Shi-Fu
  Chang.
\newblock An exploration of parameter redundancy in deep networks with
  circulant projections.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 2857--2865, 2015.

\bibitem[Denil et~al.(2013)Denil, Shakibi, Dinh, Ranzato, and
  De~Freitas]{denil2013predicting}
Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, and Nando
  De~Freitas.
\newblock Predicting parameters in deep learning.
\newblock \emph{Advances in neural information processing systems}, 26, 2013.

\bibitem[Tai et~al.(2015)Tai, Xiao, Zhang, Wang, et~al.]{lowrankcnn}
Cheng Tai, Tong Xiao, Yi~Zhang, Xiaogang Wang, et~al.
\newblock Convolutional neural networks with low-rank regularization.
\newblock \emph{arXiv preprint arXiv:1511.06067}, 2015.

\bibitem[Yang et~al.(2020)Yang, Tang, Wen, Yan, Hu, Li, Li, and
  Chen]{lowrankortho}
Huanrui Yang, Minxue Tang, Wei Wen, Feng Yan, Daniel Hu, Ang Li, Hai Li, and
  Yiran Chen.
\newblock Learning low-rank deep neural networks via singular vector
  orthogonality regularization and singular value sparsification.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition workshops}, pages 678--679, 2020.

\bibitem[Xu et~al.(2020)Xu, Li, Zhang, Wen, Wang, Qi, Chen, Lin, and
  Xiong]{lowranktrained}
Yuhui Xu, Yuxi Li, Shuai Zhang, Wei Wen, Botao Wang, Yingyong Qi, Yiran Chen,
  Weiyao Lin, and Hongkai Xiong.
\newblock Trp: Trained rank pruning for efficient deep neural networks.
\newblock \emph{arXiv preprint arXiv:2004.14566}, 2020.

\bibitem[Khodak et~al.(2021)Khodak, Tenenholtz, Mackey, and Fusi]{lowrankfd}
Mikhail Khodak, Neil Tenenholtz, Lester Mackey, and Nicolo Fusi.
\newblock Initialization and regularization of factorized neural layers.
\newblock \emph{arXiv preprint arXiv:2105.01029}, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, June 2016.

\bibitem[Moczulski et~al.(2015)Moczulski, Denil, Appleyard, and
  de~Freitas]{acdc}
Marcin Moczulski, Misha Denil, Jeremy Appleyard, and Nando de~Freitas.
\newblock Acdc: A structured efficient linear layer.
\newblock \emph{arXiv preprint arXiv:1511.05946}, 2015.

\bibitem[Dao et~al.(2019)Dao, Gu, Eichhorn, Rudra, and R{\'e}]{butterfly}
Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R{\'e}.
\newblock Learning fast algorithms for linear transforms using butterfly
  factorizations.
\newblock In \emph{International conference on machine learning}, pages
  1517--1527. PMLR, 2019.

\bibitem[Vaswani et~al.(2017{\natexlab{b}})Vaswani, Shazeer, Parmar, Uszkoreit,
  Jones, Gomez, Kaiser, and Polosukhin]{attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30,
  2017{\natexlab{b}}.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{roformer}
Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:\penalty0 127063, 2024.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Lavril, Izacard, Martinet,
  Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{b}}.

\bibitem[Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli,
  Alobeidli, Pannier, Almazrouei, and Launay]{refinedweb}
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,
  Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
  and Julien Launay.
\newblock The refinedweb dataset for falcon llm: outperforming curated corpora
  with web data, and web data only.
\newblock \emph{arXiv preprint arXiv:2306.01116}, 2023.

\bibitem[Gu and Dao(2023)]{mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock \emph{arXiv preprint arXiv:2312.00752}, 2023.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, et~al.]{opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem[Beck et~al.(2024)Beck, P{\"o}ppel, Spanring, Auer, Prudnikova, Kopp,
  Klambauer, Brandstetter, and Hochreiter]{xlstm}
Maximilian Beck, Korbinian P{\"o}ppel, Markus Spanring, Andreas Auer,
  Oleksandra Prudnikova, Michael Kopp, G{\"u}nter Klambauer, Johannes
  Brandstetter, and Sepp Hochreiter.
\newblock xlstm: Extended long short-term memory.
\newblock \emph{arXiv preprint arXiv:2405.04517}, 2024.

\bibitem[Peng et~al.(2023)Peng, Alcaide, Anthony, Albalak, Arcadinho, Biderman,
  Cao, Cheng, Chung, Grella, et~al.]{rwkv}
Bo~Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella
  Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et~al.
\newblock Rwkv: Reinventing rnns for the transformer era.
\newblock \emph{arXiv preprint arXiv:2305.13048}, 2023.

\bibitem[Bi et~al.(2024)Bi, Chen, Chen, Chen, Dai, Deng, Ding, Dong, Du, Fu,
  et~al.]{deepseekscaling}
Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng,
  Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et~al.
\newblock Deepseek llm: Scaling open-source language models with longtermism.
\newblock \emph{arXiv preprint arXiv:2401.02954}, 2024.

\bibitem[Narayanan et~al.(2021)Narayanan, Shoeybi, Casper, LeGresley, Patwary,
  Korthikanti, Vainbrand, Kashinkunti, Bernauer, Catanzaro, et~al.]{megatronlm}
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
  Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie
  Bernauer, Bryan Catanzaro, et~al.
\newblock Efficient large-scale language model training on gpu clusters using
  megatron-lm.
\newblock In \emph{Proceedings of the International Conference for High
  Performance Computing, Networking, Storage and Analysis}, pages 1--15, 2021.

\bibitem[Ioffe and Szegedy(2015)]{bn}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, pages
  448--456. pmlr, 2015.

\bibitem[NVIDIA(2022)]{transformerengine}
NVIDIA.
\newblock Transformerengine.
\newblock https://github.com/NVIDIA/TransformerEngine, 2022.

\end{thebibliography}
