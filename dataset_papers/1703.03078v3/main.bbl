\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akrour et~al.(2016)Akrour, Abdolmaleki, Abdulsamad, and
  Neumann]{peters_quadratic_models_paper}
Akrour, R., Abdolmaleki, A., Abdulsamad, H., and Neumann, G.
\newblock Model-free trajectory optimization for reinforcement learning.
\newblock In \emph{ICML}, 2016.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{gym}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock {OpenAI} gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Chebotar et~al.(2017)Chebotar, Kalakrishnan, Yahya, Li, Schaal, and
  Levine]{chebotar-icra2017}
Chebotar, Y., Kalakrishnan, M., Yahya, A., Li, A., Schaal, S., and Levine, S.
\newblock Path integral guided policy search.
\newblock In \emph{ICRA}, 2017.

\bibitem[Daniel et~al.(2013)Daniel, Neumann, Kroemer, and
  Peters]{daniel2013learning}
Daniel, Christian, Neumann, Gerhard, Kroemer, Oliver, and Peters, Jan.
\newblock Learning sequential motor tasks.
\newblock In \emph{ICRA}, 2013.

\bibitem[Deisenroth et~al.(2011)Deisenroth, Rasmussen, and Fox]{DeisenrothRF11}
Deisenroth, M., Rasmussen, C., and Fox, D.
\newblock Learning to control a low-cost manipulator using data-efficient
  reinforcement learning.
\newblock In \emph{RSS}, 2011.

\bibitem[Deisenroth et~al.(2013)Deisenroth, Neumann, and Peters]{policysearch}
Deisenroth, M., Neumann, G., and Peters, J.
\newblock A survey on policy search for robotics.
\newblock \emph{Foundations and Trends in Robotics}, 2\penalty0 (1-2):\penalty0
  1--142, 2013.

\bibitem[Deisenroth et~al.(2014)Deisenroth, Fox, and Rasmussen]{pilco}
Deisenroth, M., Fox, D., and Rasmussen, C.
\newblock Gaussian processes for data-efficient learning in robotics and
  control.
\newblock \emph{PAMI}, 2014.

\bibitem[Farshidian et~al.(2014)Farshidian, Neunert, and
  Buchli]{Farshidianetal}
Farshidian, F., Neunert, M., and Buchli, J.
\newblock Learning of closed-loop motion control.
\newblock In \emph{IROS}, 2014.

\bibitem[Gu et~al.(2016)Gu, Lillicrap, Sutskever, and Levine]{GuLSL16}
Gu, S., Lillicrap, T., Sutskever, I., and Levine, S.
\newblock Continuous deep {Q}-learning with model-based acceleration.
\newblock \emph{CoRR}, abs/1603.00748, 2016.

\bibitem[Heess et~al.(2015)Heess, Wayne, Silver, Lillicrap, Tassa, and
  Erez]{stochastic_value_gradients}
Heess, N., Wayne, G., Silver, D., Lillicrap, T., Tassa, Y., and Erez, T.
\newblock Learning continuous control policies by stochastic value gradients.
\newblock In \emph{NIPS}, 2015.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{KingmaW13}
Kingma, D. and Welling, M.
\newblock Auto-encoding variational {B}ayes.
\newblock \emph{CoRR}, abs/1312.6114, 2013.

\bibitem[Kober et~al.(2013)Kober, Bagnell, and Peters]{kbp-rlrs-13}
Kober, J., Bagnell, J., and Peters, J.
\newblock Reinforcement learning in robotics: a survey.
\newblock \emph{International Journal of Robotic Research}, 32\penalty0
  (11):\penalty0 1238--1274, 2013.

\bibitem[Levine \& Abbeel(2014)Levine and Abbeel]{LevineA14}
Levine, S. and Abbeel, P.
\newblock Learning neural network policies with guided policy search under
  unknown dynamics.
\newblock In \emph{NIPS}, 2014.

\bibitem[Levine et~al.(2015)Levine, Wagener, and Abbeel]{lwa-lnnpg-15}
Levine, S., Wagener, N., and Abbeel, P.
\newblock Learning contact-rich manipulation skills with guided policy search.
\newblock In \emph{ICRA}, 2015.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{Levine:2016}
Levine, S., Finn, C., Darrell, T., and Abbeel, P.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{JMLR}, 17\penalty0 (1), 2016.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lhphe-ccdrl-16}
Lillicrap, T., Hunt, J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver,
  D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{ICLR}, 2016.

\bibitem[Lioutikov et~al.(2014)Lioutikov, Paraschos, Neumann, and
  Peters]{peters_linear_gaussian_controller_paper}
Lioutikov, R., Paraschos, A., Neumann, G., and Peters, J.
\newblock Sample-based information-theoretic stochastic optimal control.
\newblock In \emph{ICRA}, 2014.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih_et_al_atari}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., and Riedmiller, M.
\newblock Playing {A}tari with deep reinforcement learning.
\newblock In \emph{NIPS Workshop on Deep Learning}, 2013.

\bibitem[Montgomery \& Levine(2016)Montgomery and Levine]{MontgomeryL16}
Montgomery, W. and Levine, S.
\newblock Guided policy search via approximate mirror descent.
\newblock In \emph{NIPS}, 2016.

\bibitem[Montgomery et~al.(2017)Montgomery, Ajay, Finn, Abbeel, and
  Levine]{montgomery_ajay_icra_paper}
Montgomery, W., Ajay, A., Finn, C., Abbeel, P., and Levine, S.
\newblock Reset-free guided policy search: efficient deep reinforcement
  learning with stochastic initial states.
\newblock In \emph{ICRA}, 2017.

\bibitem[Pan \& Theodorou(2014)Pan and Theodorou]{pddp}
Pan, Y. and Theodorou, E.
\newblock Probabilistic differential dynamic programming.
\newblock In \emph{NIPS}, 2014.

\bibitem[Pastor et~al.(2009)Pastor, Hoffmann, Asfour, and
  Schaal]{peter_pastor_demonstration}
Pastor, P., Hoffmann, H., Asfour, T., and Schaal, S.
\newblock Learning and generalization of motor skills by learning from
  demonstration.
\newblock In \emph{ICRA}, 2009.

\bibitem[Peters \& Schaal(2008)Peters and
  Schaal]{peters_schaal_2008_reinforcement_learning_pg}
Peters, J. and Schaal, S.
\newblock Reinforcement learning of motor skills with policy gradients.
\newblock \emph{Neural Networks}, 21\penalty0 (4), 2008.

\bibitem[Peters et~al.(2010)Peters, M\"ulling, and Altun]{PetersMA10}
Peters, J., M\"ulling, K., and Altun, Y.
\newblock Relative entropy policy search.
\newblock In \emph{AAAI}, 2010.

\bibitem[Schaal et~al.(2003)Schaal, Peters, Nakanishi, and Ijspeert]{rl_dmps}
Schaal, S., Peters, J., Nakanishi, J., and Ijspeert, A.
\newblock Control, planning, learning, and imitation with dynamic movement
  primitives.
\newblock In \emph{IROS Workshop on Bilateral Paradigms on Humans and
  Humanoids}, 2003.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Moritz, Jordan, and
  Abbeel]{slmja-trpo-15}
Schulman, J., Levine, S., Moritz, P., Jordan, M., and Abbeel, P.
\newblock Trust region policy optimization.
\newblock In \emph{ICML}, 2015.

\bibitem[Schulman et~al.(2016)Schulman, Moritz, Levine, Jordan, and
  Abbeel]{trpo-gae}
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock In \emph{ICLR}, 2016.

\bibitem[Sutton(1990)]{dyna-q}
Sutton, R.
\newblock Integrated architectures for learning, planning, and reacting based
  on approximating dynamic programming.
\newblock In \emph{ICML}, 1990.

\bibitem[Tassa et~al.(2012)Tassa, Erez, and Todorov]{synthesis}
Tassa, Y., Erez, T., and Todorov, E.
\newblock Synthesis and stabilization of complex behaviors.
\newblock In \emph{IROS}, 2012.

\bibitem[Theodorou et~al.(2010)Theodorou, Buchli, and Schaal]{TheodorouBS10}
Theodorou, E., Buchli, J., and Schaal, S.
\newblock A generalized path integral control approach to reinforcement
  learning.
\newblock \emph{JMLR}, 11, 2010.

\end{thebibliography}
