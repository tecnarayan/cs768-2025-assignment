\begin{thebibliography}{}

\bibitem[Agakov and Barber, 2004]{agakov2004auxiliary}
Agakov, F.~V. and Barber, D. (2004).
\newblock An auxiliary variational method.
\newblock In {\em International Conference on Neural Information Processing},
  pages 561--566. Springer.

\bibitem[Agapiou et~al., 2017]{agapiou2017importance}
Agapiou, S., Papaspiliopoulos, O., Sanz-Alonso, D., and Stuart, A. (2017).
\newblock Importance sampling: Intrinsic dimension and computational cost.
\newblock {\em Statistical Science}, pages 405--431.

\bibitem[Akyildiz and M{\'\i}guez, 2021]{akyildiz2021convergence}
Akyildiz, {\"O}.~D. and M{\'\i}guez, J. (2021).
\newblock Convergence rates for optimised adaptive importance samplers.
\newblock {\em Statistics and Computing}, 31(2):1--17.

\bibitem[Andrieu et~al., 2010]{andrieu2010particle}
Andrieu, C., Doucet, A., and Holenstein, R. (2010).
\newblock Particle {M}arkov chain {M}onte {C}arlo methods.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 72(3):269--342.

\bibitem[Andrieu et~al., 2018]{andrieu2018uniform}
Andrieu, C., Lee, A., Vihola, M., et~al. (2018).
\newblock Uniform ergodicity of the iterated conditional {SMC} and geometric
  ergodicity of particle {G}ibbs samplers.
\newblock {\em Bernoulli}, 24(2):842--872.

\bibitem[Bingham et~al., 2019]{bingham2019pyro}
Bingham, E., Chen, J.~P., Jankowiak, M., Obermeyer, F., Pradhan, N.,
  Karaletsos, T., Singh, R., Szerlip, P., Horsfall, P., and Goodman, N.~D.
  (2019).
\newblock Pyro: Deep universal probabilistic programming.
\newblock {\em The Journal of Machine Learning Research}, 20(1):973--978.

\bibitem[Burda et~al., 2016]{burda:grosse:2015}
Burda, Y., Grosse, R., and Salakhutdinov, R. (2016).
\newblock Importance weighted autoencoders.
\newblock In {\em The 4th International Conference on Learning Representations
  (ICLR)}.

\bibitem[Che et~al., 2020]{che:bengio:2020}
Che, T., Zhang, R., Sohl-Dickstein, J., Larochelle, H., Paull, L., Cao, Y., and
  Bengio, Y. (2020).
\newblock Your {GAN} is secretly an energy-based model and you should use
  discriminator driven latent sampling.
\newblock {\em arXiv preprint arXiv:2003.06060}.

\bibitem[Chopin and Robert, 2010]{chopin:robert:2010}
Chopin, N. and Robert, C.~P. (2010).
\newblock Properties of nested sampling.
\newblock {\em Biometrika}, 97(3):741--755.

\bibitem[Craiu and Lemieux, 2007]{craiu2007acceleration}
Craiu, R.~V. and Lemieux, C. (2007).
\newblock Acceleration of the multiple-try metropolis algorithm using
  antithetic and stratified sampling.
\newblock {\em Statistics and computing}, 17(2):109.

\bibitem[Cremer et~al., 2017]{cremer2017reinterpreting}
Cremer, C., Morris, Q., and Duvenaud, D. (2017).
\newblock Reinterpreting importance-weighted autoencoders.
\newblock {\em arXiv preprint arXiv:1704.02916}.

\bibitem[Del~Moral et~al., 2006]{del2006sequential}
Del~Moral, P., Doucet, A., and Jasra, A. (2006).
\newblock {Sequential Monte Carlo samplers}.
\newblock {\em Journal of the Royal Statistical Society: Series B},
  68(3):411--436.

\bibitem[Ding and Freedman, 2019]{ding2019learning}
Ding, X. and Freedman, D.~J. (2019).
\newblock Learning deep generative models with annealed importance sampling.
\newblock {\em arXiv preprint arXiv:1906.04904}.

\bibitem[Douc et~al., 2011]{douc2011sequential}
Douc, R., Garivier, A., Moulines, E., Olsson, J., et~al. (2011).
\newblock Sequential monte carlo smoothing for general state space hidden
  markov models.
\newblock {\em Annals of Applied Probability}, 21(6):2109--2145.

\bibitem[Douc et~al., 2018]{douc:moulines:priouret:2018}
Douc, R., Moulines, E., Priouret, P., and Soulier, P. (2018).
\newblock {\em {M}arkov {C}hains}.
\newblock Springer Series in Operations Research and Financial Engineering.
  Springer, Cham.

\bibitem[El~Moselhy and Marzouk, 2012]{el2012bayesian}
El~Moselhy, T.~A. and Marzouk, Y.~M. (2012).
\newblock Bayesian inference with optimal maps.
\newblock {\em Journal of Computational Physics}, 231(23):7815--7850.

\bibitem[Franca et~al., 2019]{francca2019conformal}
Franca, G., Sulam, J., Robinson, D.~P., and Vidal, R. (2019).
\newblock Conformal symplectic and relativistic optimization.
\newblock {\em arXiv preprint arXiv:1903.04100}.

\bibitem[Hall and Heyde, 1980]{hallheydebook}
Hall, P. and Heyde, C. (1980).
\newblock {\em Martingale Limit Theory and Its Application}.
\newblock Academic Press.

\bibitem[Hartman, 1982]{Har_1982}
Hartman, P. (1982).
\newblock {\em {Ordinary Differential Equations: Second Edition}}.
\newblock {Classics in Applied Mathematics}. Society for Industrial and Applied
  Mathematics (SIAM, 3600 Market Street, Floor 6, Philadelphia, PA 19104).

\bibitem[Jia and Seljak, 2020]{jia2020normalizing}
Jia, H. and Seljak, U. (2020).
\newblock Normalizing constant estimation with {G}aussianized bridge sampling.
\newblock In {\em Symposium on Advances in Approximate Bayesian Inference},
  pages 1--14. PMLR.

\bibitem[Kingma and Welling, 2013]{kingma:welling:2013}
Kingma, D.~P. and Welling, M. (2013).
\newblock Auto-encoding variational {B}ayes.
\newblock {\em arXiv preprint arXiv:1312.6114}.

\bibitem[Kingma and Welling, 2019]{kingma2019introduction}
Kingma, D.~P. and Welling, M. (2019).
\newblock An introduction to variational autoencoders.
\newblock {\em arXiv preprint arXiv:1906.02691}.

\bibitem[Lawson et~al., 2019]{lawson2019energy}
Lawson, D., Tucker, G., Dai, B., and Ranganath, R. (2019).
\newblock Energy-inspired models: Learning with sampler-induced distributions.
\newblock {\em arXiv preprint arXiv:1910.14265}.

\bibitem[Levy et~al., 2018]{levy:hoffman:sohl}
Levy, D., Hoffman, M.~D., and Sohl{-}Dickstein, J. (2018).
\newblock Generalizing {H}amiltonian {M}onte {C}arlo with neural networks.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}. OpenReview.net.

\bibitem[Lindsten et~al., 2015]{lindsten2015uniform}
Lindsten, F., Douc, R., and Moulines, E. (2015).
\newblock Uniform ergodicity of the particle {G}ibbs sampler.
\newblock {\em Scandinavian Journal of Statistics}, 42(3):775--797.

\bibitem[Liu et~al., 2018]{liu2018large}
Liu, Z., Luo, P., Wang, X., and Tang, X. (2018).
\newblock Large-scale celebfaces attributes (celeba) dataset.
\newblock {\em Retrieved August}, 15(2018):11.

\bibitem[Maddison et~al., 2018]{maddison2018hamiltonian}
Maddison, C.~J., Paulin, D., Teh, Y.~W., O'Donoghue, B., and Doucet, A. (2018).
\newblock Hamiltonian descent methods.
\newblock {\em arXiv preprint arXiv:1809.05042}.

\bibitem[M{\"u}ller et~al., 2019]{muller2018neural}
M{\"u}ller, T., McWilliams, B., Rousselle, F., Gross, M., and Nov{\'a}k, J.
  (2019).
\newblock Neural importance sampling.
\newblock {\em ACM Transactions on Graphics}, 38(145).

\bibitem[Neal, 2001]{neal-annealed:2001}
Neal, R.~M. (2001).
\newblock Annealed importance sampling.
\newblock {\em Statistics and Computing}, 11:125--139.

\bibitem[Owen and Zhou, 2000]{owen:zhou:2000}
Owen, A. and Zhou, Y. (2000).
\newblock Safe and effective importance sampling.
\newblock {\em Journal of the American Statistical Association},
  95(449):135--143.

\bibitem[Papamakarios et~al., 2019]{papamakarios2019normalizing}
Papamakarios, G., Nalisnick, E., Rezende, D.~J., Mohamed, S., and
  Lakshminarayanan, B. (2019).
\newblock Normalizing flows for probabilistic modeling and inference.
\newblock {\em arXiv preprint arXiv:1912.02762}.

\bibitem[Prangle, 2019]{prangle2019distilling}
Prangle, D. (2019).
\newblock Distilling importance sampling.
\newblock {\em arXiv preprint arXiv:1910.03632}.

\bibitem[Rotskoff and Vanden-Eijnden, 2019]{rotskoff:vanden-eijden:2019}
Rotskoff, G. and Vanden-Eijnden, E. (2019).
\newblock Dynamical computation of the density of states and {B}ayes factors
  using nonequilibrium importance sampling.
\newblock {\em Physical Review Letters}, 122(15):150602.

\bibitem[Rubin, 1987]{rubin1987comment}
Rubin, D.~B. (1987).
\newblock Comment: A noniterative sampling/importance resampling alternative to
  the data augmentation algorithm for creating a few imputations when fractions
  of missing information are modest: The {SIR} algorithm.
\newblock {\em Journal of the American Statistical Association},
  82(398):542--543.

\bibitem[Ruiz et~al., 2020]{ruiz:titsias:doucet:2020}
Ruiz, F.~J., Titsias, M.~K., Cemgil, T., and Doucet, A. (2020).
\newblock Unbiased gradient estimation for variational auto-encoders using
  coupled {M}arkov chains.
\newblock {\em arXiv preprint arXiv:2010.01845}.

\bibitem[Shestopaloff et~al., 2018]{shestopaloff:neal:2018}
Shestopaloff, A.~Y., Neal, R.~M., et~al. (2018).
\newblock Sampling latent states for high-dimensional non-linear state space
  models with the embedded hmm method.
\newblock {\em Bayesian Analysis}, 13(3):797--822.

\bibitem[Skare et~al., 2003]{skare2003improved}
Skare, {\O}., B{\o}lviken, E., and Holden, L. (2003).
\newblock Improved sampling-importance resampling and reduced bias importance
  sampling.
\newblock {\em Scandinavian Journal of Statistics}, 30(4):719--737.

\bibitem[Skilling, 2006]{skilling2006nested}
Skilling, J. (2006).
\newblock Nested sampling for general {B}ayesian computation.
\newblock {\em Bayesian Analysis}, 1(4):833--859.

\bibitem[Smith and Gelfand, 1992]{smith1992bayesian}
Smith, A.~F. and Gelfand, A.~E. (1992).
\newblock Bayesian statistics without tears: a sampling--resampling
  perspective.
\newblock {\em The American Statistician}, 46(2):84--88.

\bibitem[So, 2006]{so2006bayesian}
So, M.~K. (2006).
\newblock Bayesian analysis of nonlinear and non-gaussian state space models
  via multiple-try sampling methods.
\newblock {\em Statistics and Computing}, 16(2):125--141.

\bibitem[Tierney, 1994]{tierney:1994}
Tierney, L. (1994).
\newblock {Markov Chains for Exploring Posterior Distributions}.
\newblock {\em The Annals of Statistics}, 22(4):1701--1728.

\bibitem[Tokdar and Kass, 2010]{tokdar2010importance}
Tokdar, S.~T. and Kass, R.~E. (2010).
\newblock Importance sampling: a review.
\newblock {\em Wiley Interdisciplinary Reviews: Computational Statistics},
  2(1):54--60.

\bibitem[Turner et~al., 2019]{turner:hung:2019}
Turner, R., Hung, J., Frank, E., Saatchi, Y., and Yosinski, J. (2019).
\newblock Metropolis-{H}astings generative adversarial networks.
\newblock In {\em International Conference on Machine Learning}, pages
  6345--6353. PMLR.

\bibitem[Wirnsberger et~al., 2020]{wirnsberger2020targeted}
Wirnsberger, P., Ballard, A.~J., Papamakarios, G., Abercrombie, S.,
  Racani{\`e}re, S., Pritzel, A., Rezende, D.~J., and Blundell, C. (2020).
\newblock Targeted free energy estimation via learned mappings.
\newblock {\em arXiv preprint arXiv:2002.04913}.

\bibitem[Wu et~al., 2016]{wu:burda:grosse:2016}
Wu, Y., Burda, Y., Salakhutdinov, R., and Grosse, R. (2016).
\newblock On the quantitative analysis of decoder-based generative models.
\newblock {\em arXiv preprint arXiv:1611.04273}.

\end{thebibliography}
