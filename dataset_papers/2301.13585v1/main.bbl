\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2019)Agarwal, Shah, Shen, and
  Song]{agarwal2019robustness}
Anish Agarwal, Devavrat Shah, Dennis Shen, and Dogyoon Song.
\newblock On robustness of principal component regression.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Ayme et~al.(2022)Ayme, Boyer, Dieuleveut, and Scornet]{ayme2022near}
Alexis Ayme, Claire Boyer, Aymeric Dieuleveut, and Erwan Scornet.
\newblock Near-optimal rate of consistency for linear models with missing
  values.
\newblock In \emph{International Conference on Machine Learning}, pages
  1211--1243. PMLR, 2022.

\bibitem[Bach and Moulines(2013)]{bach2013non}
Francis Bach and Eric Moulines.
\newblock Non-strongly-convex smooth stochastic approximation with convergence
  rate o (1/n).
\newblock \emph{Advances in neural information processing systems}, 26, 2013.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and
  Tsigler]{bartlett2020benign}
Peter~L Bartlett, Philip~M Long, G{\'a}bor Lugosi, and Alexander Tsigler.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (48):\penalty0 30063--30070, 2020.

\bibitem[Caponnetto and De~Vito(2007)]{caponnetto2007optimal}
Andrea Caponnetto and Ernesto De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock \emph{Foundations of Computational Mathematics}, 7\penalty0
  (3):\penalty0 331--368, 2007.

\bibitem[Chizat and Bach(2020)]{chizat2020implicit}
Lenaic Chizat and Francis Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks
  trained with the logistic loss.
\newblock In \emph{Conference on Learning Theory}, pages 1305--1338. PMLR,
  2020.

\bibitem[Dieuleveut et~al.(2017)Dieuleveut, Flammarion, and
  Bach]{dieuleveut2017harder}
Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach.
\newblock Harder, better, faster, stronger convergence rates for least-squares
  regression.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 3520--3570, 2017.

\bibitem[Gal and Ghahramani(2016)]{gal2016theoretically}
Yarin Gal and Zoubin Ghahramani.
\newblock A theoretically grounded application of dropout in recurrent neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Hsu et~al.(2012)Hsu, Kakade, and Zhang]{hsu2012random}
Daniel Hsu, Sham~M Kakade, and Tong Zhang.
\newblock Random design analysis of ridge regression.
\newblock In \emph{Conference on learning theory}, pages 9--1. JMLR Workshop
  and Conference Proceedings, 2012.

\bibitem[Ipsen et~al.(2022)Ipsen, Mattei, and Frellsen]{ipsen2022deal}
Niels~Bruun Ipsen, Pierre-Alexandre Mattei, and Jes Frellsen.
\newblock How to deal with missing data in supervised deep learning?
\newblock In \emph{ICLR 2022-10th International Conference on Learning
  Representations}, 2022.

\bibitem[Johnstone(2001)]{johnstone2001}
Iain~M. Johnstone.
\newblock {On the distribution of the largest eigenvalue in principal
  components analysis}.
\newblock \emph{The Annals of Statistics}, 29\penalty0 (2):\penalty0 295 --
  327, 2001.
\newblock \doi{10.1214/aos/1009210544}.
\newblock URL \url{https://doi.org/10.1214/aos/1009210544}.

\bibitem[Josse et~al.(2019)Josse, Prost, Scornet, and
  Varoquaux]{josse2019consistency}
Julie Josse, Nicolas Prost, Erwan Scornet, and Ga{\"e}l Varoquaux.
\newblock On the consistency of supervised learning with missing values.
\newblock \emph{arXiv preprint arXiv:1902.06931}, 2019.

\bibitem[Le~Morvan et~al.(2020{\natexlab{a}})Le~Morvan, Josse, Moreau, Scornet,
  and Varoquaux]{lemorvan:hal-02888867}
Marine Le~Morvan, Julie Josse, Thomas Moreau, Erwan Scornet, and Ga{\"e}l
  Varoquaux.
\newblock {NeuMiss networks: differentiable programming for supervised learning
  with missing values}.
\newblock In \emph{{NeurIPS 2020 - 34th Conference on Neural Information
  Processing Systems}}, Vancouver / Virtual, Canada, December
  2020{\natexlab{a}}.
\newblock URL \url{https://hal.archives-ouvertes.fr/hal-02888867}.

\bibitem[Le~Morvan et~al.(2020{\natexlab{b}})Le~Morvan, Prost, Josse, Scornet,
  and Varoquaux]{le2020linear}
Marine Le~Morvan, Nicolas Prost, Julie Josse, Erwan Scornet, and Ga{\"e}l
  Varoquaux.
\newblock Linear predictor on linearly-generated data with missing values: non
  consistency and solutions.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3165--3174. PMLR, 2020{\natexlab{b}}.

\bibitem[Le~Morvan et~al.(2021)Le~Morvan, Josse, Scornet, and
  Varoquaux]{le2021sa}
Marine Le~Morvan, Julie Josse, Erwan Scornet, and Ga{\"e}l Varoquaux.
\newblock Whatâ€™sa good imputation to predict with missing values?
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 11530--11540, 2021.

\bibitem[Mourtada(2019)]{mourtada2019exact}
Jaouad Mourtada.
\newblock Exact minimax risk for linear least squares, and the lower tail of
  sample covariance matrices.
\newblock \emph{arXiv preprint arXiv:1912.10754}, 2019.

\bibitem[Mourtada and Rosasco(2022)]{mourtada2022elementary}
Jaouad Mourtada and Lorenzo Rosasco.
\newblock An elementary analysis of ridge regression with random design.
\newblock \emph{arXiv preprint arXiv:2203.08564}, 2022.

\bibitem[Muzellec et~al.(2020)Muzellec, Josse, Boyer, and
  Cuturi]{muzellec2020missing}
Boris Muzellec, Julie Josse, Claire Boyer, and Marco Cuturi.
\newblock Missing data imputation using optimal transport.
\newblock In \emph{International Conference on Machine Learning}, pages
  7130--7140. PMLR, 2020.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,
  Cournapeau, Brucher, Perrot, and Duchesnay]{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2825--2830,
  2011.

\bibitem[Pesme et~al.(2021)Pesme, Pillaud-Vivien, and
  Flammarion]{pesme2021implicit}
Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion.
\newblock Implicit bias of sgd for diagonal linear networks: a provable benefit
  of stochasticity.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 29218--29230, 2021.

\bibitem[Polyak and Juditsky(1992)]{polyak1992acceleration}
Boris~T Polyak and Anatoli~B Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock \emph{SIAM journal on control and optimization}, 30\penalty0
  (4):\penalty0 838--855, 1992.

\bibitem[Rubin(1976)]{RUBIN76}
DONALD~B. Rubin.
\newblock {Inference and missing data}.
\newblock \emph{Biometrika}, 63\penalty0 (3):\penalty0 581--592, 12 1976.
\newblock ISSN 0006-3444.
\newblock \doi{10.1093/biomet/63.3.581}.
\newblock URL \url{https://doi.org/10.1093/biomet/63.3.581}.

\bibitem[Sridharan et~al.(2008)Sridharan, Shalev-Shwartz, and
  Srebro]{sridharan2008fast}
Karthik Sridharan, Shai Shalev-Shwartz, and Nathan Srebro.
\newblock Fast rates for regularized objectives.
\newblock \emph{Advances in neural information processing systems}, 21, 2008.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The journal of machine learning research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Van~Buuren and Groothuis-Oudshoorn(2011)]{van2011mice}
Stef Van~Buuren and Karin Groothuis-Oudshoorn.
\newblock mice: Multivariate imputation by chained equations in r.
\newblock \emph{Journal of statistical software}, 45:\penalty0 1--67, 2011.

\bibitem[Yao et~al.(2007)Yao, Rosasco, and Caponnetto]{yao2007early}
Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto.
\newblock On early stopping in gradient descent learning.
\newblock \emph{Constructive Approximation}, 26\penalty0 (2):\penalty0
  289--315, 2007.

\bibitem[Yoon et~al.(2018)Yoon, Jordon, and Schaar]{yoon2018gain}
Jinsung Yoon, James Jordon, and Mihaela Schaar.
\newblock Gain: Missing data imputation using generative adversarial nets.
\newblock In \emph{International conference on machine learning}, pages
  5689--5698. PMLR, 2018.

\end{thebibliography}
