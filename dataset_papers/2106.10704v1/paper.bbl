\begin{thebibliography}{71}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjovsky et~al.(2016)Arjovsky, Shah, and Bengio]{UnitaryEvolutionRNNs}
Arjovsky, M., Shah, A., and Bengio, Y.
\newblock Unitary evolution recurrent neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1120--1128, 2016.

\bibitem[Bakry \& {\'E}mery(1985)Bakry and {\'E}mery]{BaE85}
Bakry, D. and {\'E}mery, M.
\newblock Diffusions hypercontractives.
\newblock In Az{\'e}ma, J. and Yor, M. (eds.), \emph{S{\'e}minaire de
  Probabilit{\'e}s XIX 1983/84}, pp.\  177--206, Berlin, Heidelberg, 1985.
  Springer Berlin Heidelberg.
\newblock ISBN 978-3-540-39397-9.

\bibitem[Bakry et~al.(2013)Bakry, Gentil, and Ledoux]{BGL13}
Bakry, D., Gentil, I., and Ledoux, M.
\newblock \emph{Analysis and geometry of {M}arkov diffusion operators}, volume
  348.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Bansal et~al.(2018)Bansal, Chen, and Wang]{OrthReg_CNNs}
Bansal, N., Chen, X., and Wang, Z.
\newblock Can we gain more from orthogonality regularizations in training deep
  {CNN}s?
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pp.\  4266--4276, 2018.

\bibitem[Beskos et~al.(2013)Beskos, Pillai, Roberts, Sanz-Serna, and
  Stuart]{Beskos2013}
Beskos, A., Pillai, N., Roberts, G., Sanz-Serna, J.-M., and Stuart, A.
\newblock Optimal tuning of the hybrid {M}onte {C}arlo algorithm.
\newblock \emph{Bernoulli}, 19\penalty0 (5A):\penalty0 1501--1534, 2013.

\bibitem[Bhattacharya(1982)]{Bha82}
Bhattacharya, R.~N.
\newblock On the functional central limit theorem and the law of the iterated
  logarithm for {M}arkov processes.
\newblock \emph{Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und verwandte
  Gebiete}, 60\penalty0 (2):\penalty0 185--201, 1982.

\bibitem[Bou-Rabee \& Sanz-Serna(2018)Bou-Rabee and Sanz-Serna]{BRSS2017}
Bou-Rabee, N. and Sanz-Serna, J.
\newblock Geometric integrators and the {H}amiltonian {M}onte {C}arlo method.
\newblock \emph{Acta Numerica}, 27:\penalty0 113--206, 2018.

\bibitem[Brock et~al.(2017)Brock, Lim, Ritchie, and Weston]{NeuralPhotoEditing}
Brock, A., Lim, T., Ritchie, J.~M., and Weston, N.~J.
\newblock Neural photo editing with introspective adversarial networks.
\newblock \emph{ICLR}, 2017.

\bibitem[Chaudhari et~al.(2017)Chaudhari, Choromanska, Soatto, LeCun, Baldassi,
  Borgs, Chayes, Sagun, and Zecchina]{Chaudhari2017}
Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y., Baldassi, C., Borgs, C.,
  Chayes, J., Sagun, L., and Zecchina, R.
\newblock Entropy-{SGD}: {B}iasing gradient descent into wide valleys.
\newblock \emph{ICLR}, 2017.

\bibitem[Cheng et~al.(2017)Cheng, Chatterji, Bartlett, and Jordan]{Cheng2017}
Cheng, X., Chatterji, N.~S., Bartlett, P.~L., and Jordan, M.~I.
\newblock {U}nderdamped {L}angevin {MCMC}: A non-asymptotic analysis.
\newblock \emph{arXiv:1707.03663}, 2017.

\bibitem[Choromanska et~al.(2015)Choromanska, Henaff, Mathieu, Arous, and
  LeCun]{Choromanska2015}
Choromanska, A., Henaff, M., Mathieu, M., Arous, G.~B., and LeCun, Y.
\newblock The loss surfaces of multilayer networks.
\newblock \emph{AISTATS}, 2015.

\bibitem[d'Ascoli et~al.(2019)d'Ascoli, Sagun, Bruna, and Biroli]{haystack}
d'Ascoli, S., Sagun, L., Bruna, J., and Biroli, G.
\newblock Finding the needle in the haystack with convolutions: on the benefits
  of architectural bias.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Faou \& Leli{\`e}vre(2009)Faou and Leli{\`e}vre]{FaL09}
Faou, E. and Leli{\`e}vre, T.
\newblock Conservative stochastic differential equations: {M}athematical and
  numerical analysis.
\newblock \emph{Mathematics of computation}, 78\penalty0 (268):\penalty0
  2047--2074, 2009.

\bibitem[Graham \& Storkey(2017)Graham and Storkey]{GS2017}
Graham, M. and Storkey, A.
\newblock Asymptotically exact inference in differentiable generative models.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics}, volume~54, pp.\  499--508, 2017.

\bibitem[Gray et~al.(2006)Gray, Abbena, and Salamon]{gray2006}
Gray, A., Abbena, E., and Salamon, S.
\newblock \emph{Modern Differential Geometry of Curves and Surfaces with
  MATHEMATICA, 3rd edition}.
\newblock 2006.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{Pytorchinit}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: {S}urpassing human-level performance on
  {I}magenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  1026--1034, 2015.

\bibitem[Hoerl \& Kennard(1970)Hoerl and Kennard]{ridge}
Hoerl, A. and Kennard, R.
\newblock Ridge regression: {B}iased estimation for nonorthogonal problems.
\newblock \emph{Technometrics}, 12:\penalty0 55--67, 1970.

\bibitem[Huang et~al.(2018)Huang, Liu, Lang, Wei~Yu, and
  Li]{CNN_stiefelmanifold}
Huang, L., Liu, X., Lang, B., Wei~Yu, A., and Li, B.
\newblock Orthogonal weight normalization: Solution to optimization over
  multiple dependent stiefel manifolds in deep neural networks.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{BatchNorm}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  448--456, 2015.

\bibitem[Jastrz{\c{e}}bski et~al.(2018)Jastrz{\c{e}}bski, Kenton, Arpit,
  Ballas, Fischer, Bengio, and Storkey]{Jastrzebski2018}
Jastrz{\c{e}}bski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio,
  Y., and Storkey, A.
\newblock Three factors influencing minima in {SGD}.
\newblock \emph{ICANN}, 2018.

\bibitem[Jia et~al.(2019)Jia, Li, Wen, Liu, and Tao]{OrthDNN}
Jia, K., Li, S., Wen, Y., Liu, T., and Tao, D.
\newblock Orthogonal deep neural networks.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2019.

\bibitem[Kawaguchi(2016)]{kawaguchi2016}
Kawaguchi, K.
\newblock Deep learning without poor local minima.
\newblock \emph{NeurIPS}, 2016.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, and
  M.~Smelyanskiy]{largebatch}
Keskar, N., Mudigere, D., Nocedal, J., and M.~Smelyanskiy, P.~T.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{ICLR}, 2017.

\bibitem[Kipnis \& Varadhan(1986)Kipnis and Varadhan]{KiV86}
Kipnis, C. and Varadhan, S. R.~S.
\newblock Central limit theorem for additive functionals of reversible {M}arkov
  processes and applications to simple exclusions.
\newblock \emph{Communications in Mathematical Physics}, 104\penalty0
  (1):\penalty0 1--19, 1986.

\bibitem[Kirkpatrick et~al.(1983)Kirkpatrick, Gelatt, and Vecchi]{annealing}
Kirkpatrick, S., Gelatt, C., and Vecchi, M.
\newblock Optimization by simulated annealing.
\newblock \emph{Science}, 220:\penalty0 671--680, 1983.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and Hinton]{cifar10}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lee(2018)]{Lee18}
Lee, J.~M.
\newblock \emph{Introduction to {R}iemannian manifolds}, volume~2.
\newblock Springer, 2018.

\bibitem[Leimkuhler \& Matthews(2015)Leimkuhler and Matthews]{LeMa2015}
Leimkuhler, B. and Matthews, C.
\newblock \emph{Molecular Dynamics: With Deterministic and Stochastic Numerical
  Methods}.
\newblock Interdisciplinary Applied Mathematics. Springer, 2015.

\bibitem[Leimkuhler \& Matthews(2016)Leimkuhler and Matthews]{LeM16}
Leimkuhler, B. and Matthews, C.
\newblock Efficient molecular dynamics using geodesic integration and
  solvent--solute splitting.
\newblock \emph{Proceedings of the Royal Society A: Mathematical, Physical and
  Engineering Sciences}, 472\penalty0 (2189):\penalty0 20160138, 2016.

\bibitem[Leimkuhler \& Reich(2004)Leimkuhler and Reich]{LeR04}
Leimkuhler, B. and Reich, S.
\newblock \emph{Simulating {H}amiltonian dynamics}, volume~14.
\newblock Cambridge university press, 2004.

\bibitem[Leimkuhler et~al.(2016)Leimkuhler, Matthews, and Stoltz]{LMS16}
Leimkuhler, B., Matthews, C., and Stoltz, G.
\newblock The computation of averages from equilibrium and nonequilibrium
  {L}angevin molecular dynamics.
\newblock \emph{IMA Journal of Numerical Analysis}, 36\penalty0 (1):\penalty0
  13--79, 2016.

\bibitem[Leimkuhler et~al.(2019)Leimkuhler, Matthews, and Vlaar]{LMV}
Leimkuhler, B., Matthews, C., and Vlaar, T.
\newblock {P}artitioned integrators for thermodynamic parameterization of
  neural networks.
\newblock \emph{Foundations of Data Science}, 1\penalty0 (4):\penalty0
  457--489, 2019.

\bibitem[Leli{\`e}vre \& Stoltz(2016)Leli{\`e}vre and Stoltz]{LelS16}
Leli{\`e}vre, T. and Stoltz, G.
\newblock Partial differential equations and stochastic methods in molecular
  dynamics.
\newblock \emph{Acta Numerica}, 25:\penalty0 681--880, 2016.

\bibitem[Leli{\`e}vre et~al.(2010)Leli{\`e}vre, Stoltz, and Rousset]{LSR10}
Leli{\`e}vre, T., Stoltz, G., and Rousset, M.
\newblock \emph{{F}ree energy computations: {A} mathematical perspective}.
\newblock Imperial College Press, 2010.
\newblock ISBN 9781848162488.

\bibitem[Leli{\`e}vre et~al.(2012)Leli{\`e}vre, Rousset, and Stoltz]{LRS12}
Leli{\`e}vre, T., Rousset, M., and Stoltz, G.
\newblock Langevin dynamics with constraints and computation of free energy
  differences.
\newblock \emph{Mathematics of computation}, 81\penalty0 (280):\penalty0
  2071--2125, 2012.

\bibitem[Leli{\`e}vre et~al.(2020)Leli{\`e}vre, Stoltz, and Zhang]{LSZ2020}
Leli{\`e}vre, T., Stoltz, G., and Zhang, W.
\newblock Multiple projection {MCMC} algorithms on submanifolds.
\newblock \emph{arXiv:2003.09402}, 2020.

\bibitem[Li et~al.(2019)Li, Haque, Anil, Lucas, Grosse, and
  Jacobsen]{LipschitzCNNs}
Li, Q., Haque, S., Anil, C., Lucas, J., Grosse, R., and Jacobsen, J.
\newblock Preventing gradient attenuation in {L}ipschitz constrained
  convolutional networks.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{SGDR}
Loshchilov, I. and Hutter, F.
\newblock Stochastic gradient descent with warm restarts.
\newblock \emph{ICLR}, 2017.

\bibitem[Marcus et~al.(1993)Marcus, Santorini, and Marcinkiewicz]{PennTreebank}
Marcus, M.~P., Santorini, B., and Marcinkiewicz, M.~A.
\newblock Building a large annotated corpus of {E}nglish: The {P}enn
  {T}reebank.
\newblock \emph{Computational Linguistics}, 19\penalty0 (2):\penalty0 313--330,
  1993.

\bibitem[Marinari \& Parisi(1992)Marinari and Parisi]{ST}
Marinari, E. and Parisi, G.
\newblock Simulated tempering: a new {M}onte {C}arlo scheme.
\newblock \emph{Europhysics Letters}, 1992.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and Socher]{Wikitext2}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer sentinel mixture models.
\newblock \emph{ICLR}, 2017.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and Srebro]{neyshabur2015}
Neyshabur, B., Tomioka, R., and Srebro, N.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Workshop Track Proceedings}, 2015.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and Bengio]{PascanuRNNs}
Pascanu, R., Mikolov, T., and Bengio, Y.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{International conference on machine learning}, pp.\
  1310--1318, 2013.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{Pytorch}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in {P}y{T}orch.
\newblock 2017.

\bibitem[Pavliotis(2014)]{Pav14}
Pavliotis, G.~A.
\newblock \emph{Stochastic processes and applications: diffusion processes, the
  {F}okker-{P}lanck and {L}angevin equations}, volume~60.
\newblock Springer, 2014.

\bibitem[Pennington et~al.(2017)Pennington, Schoenholz, and
  Ganguli]{Pennington2017}
Pennington, J., Schoenholz, S., and Ganguli, S.
\newblock Resurrecting the sigmoid in deep learning through dynamical isometry:
  theory and practice.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4785--4795, 2017.

\bibitem[Pennington et~al.(2018)Pennington, Schoenholz, and
  Ganguli]{Pennington2018}
Pennington, J., Schoenholz, S., and Ganguli, S.
\newblock The emergence of spectral universality in deep networks.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1924--1932, 2018.

\bibitem[Persson(2006)]{levelsetmethod}
Persson, P.
\newblock The level set method. {L}ecture notes {MIT} 16.920{J} / 2.097{J} /
  6.339{J}, {N}umerical {M}ethods for {P}artial {D}ifferential {E}quations,
  October 2006.

\bibitem[Rodr\'{i}guez et~al.(2017)Rodr\'{i}guez, Gonz\`{a}lez, Cucurull,
  Gonfaus, and Roca]{Rodriguez2017}
Rodr\'{i}guez, P., Gonz\`{a}lez, J., Cucurull, G., Gonfaus, J.~M., and Roca, X.
\newblock Regularizing {CNN}s with locally constrained decorrelations.
\newblock \emph{ICLR}, 2017.

\bibitem[Santurkar et~al.(2018)Santurkar, Tsipras, Ilyas, and Madry]{BNhelpopt}
Santurkar, S., Tsipras, D., Ilyas, A., and Madry, A.
\newblock How does batch normalization help optimization?
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2483--2493, 2018.

\bibitem[Saxe et~al.(2013)Saxe, McClelland, and Ganguli]{Saxe2013}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{arXiv:1312.6120}, 2013.

\bibitem[Smith(2017)]{cyclicallr}
Smith, L.~N.
\newblock Cyclical learning rates for training neural networks.
\newblock \emph{Worshop on Application of Computer Vision}, 2017.

\bibitem[Srebro \& Shraibman(2005)Srebro and Shraibman]{maxnorm}
Srebro, N. and Shraibman, A.
\newblock Rank, trace-norm and max-norm.
\newblock In \emph{International Conference on Computational Learning Theory},
  pp.\  545--560. Springer, 2005.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{dropout2}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The journal of machine learning research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{sutskever2013}
Sutskever, I., Martens, J., Dahl, G., and Hinton, G.
\newblock On the importance of initialization and momentum in deep learning.
\newblock \emph{ICML}, 2013.

\bibitem[Tibshirani(1996)]{LASSO1}
Tibshirani, R.
\newblock Regression shrinkage and selection via the {L}asso.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 58\penalty0 (1):\penalty0 267--288, 1996.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{Transformer}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5998--6008, 2017.

\bibitem[Villani(2009)]{Vil09}
Villani, C.
\newblock Hypocoercivity.
\newblock \emph{Memoirs of the American Mathematical Society}, 202\penalty0
  (950), 2009.

\bibitem[Vorontsov et~al.(2017)Vorontsov, Trabelsi, Kadoury, and
  Pal]{RNNsLongTermDependencies}
Vorontsov, E., Trabelsi, C., Kadoury, S., and Pal, C.
\newblock On orthogonality and learning recurrent networks with long term
  dependencies.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-{V}olume 70}, pp.\  3570--3578. JMLR. org, 2017.

\bibitem[Welling \& Teh(2011)Welling and Teh]{WeT11}
Welling, M. and Teh, Y.~W.
\newblock {B}ayesian learning via stochastic gradient {L}angevin dynamics.
\newblock In \emph{Proceedings of the 28th International Conference on Machine
  Learning (ICML-11)}, pp.\  681--688, 2011.

\bibitem[Wenzel et~al.(2020)Wenzel, Roth, Veeling, Swiatkowski, Tran, Mandt,
  Snoek, Salimans, Jenatton, and Nowozin]{coldposterior}
Wenzel, F., Roth, K., Veeling, B.~S., Swiatkowski, J., Tran, L., Mandt, S.,
  Snoek, J., Salimans, T., Jenatton, R., and Nowozin, S.
\newblock How good is the {B}ayes posterior in deep neural networks really?
\newblock \emph{arXiv:2002.02405}, 2020.

\bibitem[Williams(1995)]{LASSO2}
Williams, P.
\newblock Bayesian regularization and pruning using a {L}aplace prior.
\newblock \emph{Neural computation}, 7\penalty0 (1):\penalty0 117--143, 1995.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro†, and
  Recht]{Wilson2017}
Wilson, A., Roelofs, R., Stern, M., Srebro†, N., and Recht, B.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock \emph{NeurIPS}, 2017.

\bibitem[Wu et~al.(2017)Wu, Zhu, and E]{Wu2017}
Wu, L., Zhu, Z., and E, W.
\newblock Towards understanding generalization of deep learning: Perspective of
  loss landscapes.
\newblock \emph{ICML}, 2017.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{FashionMNIST}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-{MNIST}: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv:1708.07747}, 2017.

\bibitem[Xiao et~al.(2018)Xiao, Bahri, Sohl-Dickstein, Schoenholz, and
  Pennington]{10000CNN}
Xiao, L., Bahri, Y., Sohl-Dickstein, J., Schoenholz, S., and Pennington, J.
\newblock Dynamical isometry and a mean field theory of {CNN}s: {H}ow to train
  10,000-layer vanilla convolutional neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5393--5402, 2018.

\bibitem[Xie et~al.(2017)Xie, Xiong, and Pu]{BeyondGoodInit}
Xie, D., Xiong, J., and Pu, S.
\newblock All you need is beyond a good init: Exploring better solution for
  training extremely deep convolutional neural networks with orthonormality and
  modulation.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  6176--6185, 2017.

\bibitem[Yao et~al.(2019)Yao, Gholami, Keutzer, and Mahoney]{PyHessian}
Yao, Z., Gholami, A., Keutzer, K., and Mahoney, M.
\newblock Py{H}essian: Neural networks through the lens of the {H}essian.
\newblock \emph{arXiv:1912.07145}, 2019.

\bibitem[Zappa et~al.(2018)Zappa, Holmes-Cerfon, and Goodman]{ZHG2018}
Zappa, E., Holmes-Cerfon, M., and Goodman, J.
\newblock {M}onte {C}arlo on manifolds: {S}ampling densities and integrating
  functions.
\newblock \emph{Communications on Pure and Applied Mathematics}, 71\penalty0
  (12):\penalty0 2609--2647, 2018.

\bibitem[Zhang et~al.(2015)Zhang, Choromanska, and LeCun]{elasticaveragingSGD}
Zhang, S., Choromanska, A., and LeCun, Y.
\newblock Deep learning with elastic averaging {SGD}.
\newblock \emph{NeurIPS}, 2015.

\bibitem[Zhou et~al.(2006)Zhou, Do, and Kovacevic]{Zhou2006}
Zhou, J., Do, M., and Kovacevic, J.
\newblock Special paraunitary matrices, {C}ayley transform, and
  multidimensional orthogonal filter banks.
\newblock \emph{IEEE Transactions on Image Processing}, 15\penalty0
  (2):\penalty0 511--519, 2006.

\end{thebibliography}
