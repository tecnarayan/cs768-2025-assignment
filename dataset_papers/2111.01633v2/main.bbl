\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and
  Zheng]{tensorflow2015-whitepaper}
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
  G.~S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp,
  A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M.,
  Levenberg, J., Man\'{e}, D., Monga, R., Moore, S., Murray, D., Olah, C.,
  Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.,
  Vanhoucke, V., Vasudevan, V., Vi\'{e}gas, F., Vinyals, O., Warden, P.,
  Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock Software available from tensorflow.org.

\bibitem[Allamanis \& Sutton(2014)Allamanis and Sutton]{idiomsFSE14}
Allamanis, M. and Sutton, C.
\newblock Mining idioms from source code.
\newblock In \emph{Proceedings of the 22Nd ACM SIGSOFT International Symposium
  on Foundations of Software Engineering}, FSE 2014, pp.\  472--483, New York,
  NY, USA, 2014. ACM.
\newblock ISBN 978-1-4503-3056-5.
\newblock \doi{10.1145/2635868.2635901}.

\bibitem[Alon et~al.(2019)Alon, Zilberstein, Levy, and Yahav]{alon2019code2vec}
Alon, U., Zilberstein, M., Levy, O., and Yahav, E.
\newblock code2vec: Learning distributed representations of code.
\newblock \emph{Proceedings of the ACM on Programming Languages}, 3\penalty0
  (POPL):\penalty0 1--29, 2019.

\bibitem[Balog et~al.(2017)Balog, Gaunt, Brockschmidt, Nowozin, and
  Tarlow]{balog2016deepcoder}
Balog, M., Gaunt, A.~L., Brockschmidt, M., Nowozin, S., and Tarlow, D.
\newblock Deepcoder: Learning to write programs.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2017.

\bibitem[Bielik et~al.(2016)Bielik, Raychev, and Vechev]{bielik2016phog}
Bielik, P., Raychev, V., and Vechev, M.
\newblock {PHOG}: {P}robabilistic model for code.
\newblock In \emph{ICML}, pp.\  19--24, 2016.

\bibitem[Black et~al.(2021)Black, Gao, Wang, Leahy, and Biderman]{gpt-neo}
Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S.
\newblock {GPT-Neo: Large Scale Autoregressive Language Modeling with
  Mesh-Tensorflow}, March 2021.

\bibitem[Brockschmidt et~al.(2018)Brockschmidt, Allamanis, Gaunt, and
  Polozov]{brockschmidt2018generative}
Brockschmidt, M., Allamanis, M., Gaunt, A.~L., and Polozov, O.
\newblock Generative code modeling with graphs.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Ponde, Kaplan, Edwards,
  Burda, Joseph, Brockman, et~al.]{codex}
Chen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan, J., Edwards, H.,
  Burda, Y., Joseph, N., Brockman, G., et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Chen et~al.(2018)Chen, Liu, and Song]{chen2018tree}
Chen, X., Liu, C., and Song, D.
\newblock Tree-to-tree neural networks for program translation.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Chen et~al.(2020)Chen, Wang, Bastani, Dillig, and
  Feng]{chen2020program}
Chen, Y., Wang, C., Bastani, O., Dillig, I., and Feng, Y.
\newblock Program synthesis using deduction-guided reinforcement learning.
\newblock In \emph{International Conference on Computer Aided Verification},
  pp.\  587--610. Springer, 2020.

\bibitem[Dai et~al.(2018)Dai, Tian, Dai, Skiena, and Song]{dai2018syntax}
Dai, H., Tian, Y., Dai, B., Skiena, S., and Song, L.
\newblock Syntax-directed variational autoencoder for structured data.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Devlin et~al.(2017)Devlin, Uesato, Bhupatiraju, Singh, Mohamed, and
  Kohli]{devlin2017robustfill}
Devlin, J., Uesato, J., Bhupatiraju, S., Singh, R., Mohamed, A.-r., and Kohli,
  P.
\newblock Robustfill: Neural program learning under noisy i/o.
\newblock In \emph{International conference on machine learning}, pp.\
  990--998. PMLR, 2017.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{bert}
Devlin, J., Chang, M., Lee, K., and Toutanova, K.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL-HLT (1)}, 2019.

\bibitem[Feng et~al.(2020)Feng, Guo, Tang, Duan, Feng, Gong, Shou, Qin, Liu,
  Jiang, et~al.]{feng2020codebert}
Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B.,
  Liu, T., Jiang, D., et~al.
\newblock Codebert: A pre-trained model for programming and natural languages.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: Findings}, pp.\  1536--1547, 2020.

\bibitem[Gao et~al.(2021)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,
  He, Thite, Nabeshima, Presser, and Leahy]{Gao2021ThePA}
Gao, L., Biderman, S.~R., Black, S., Golding, L., Hoppe, T., Foster, C., Phang,
  J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{ArXiv}, abs/2101.00027, 2021.

\bibitem[Gemmell et~al.(2020)Gemmell, Rossetto, and
  Dalton]{gemmell2020relevance}
Gemmell, C., Rossetto, F., and Dalton, J.
\newblock Relevance transformer: Generating concise code snippets with
  relevance feedback.
\newblock In \emph{Proceedings of the 43rd International ACM SIGIR Conference
  on Research and Development in Information Retrieval}, pp.\  2005--2008,
  2020.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Basart, Kadavath, Mazeika, Arora,
  Guo, Burns, Puranik, He, Song, and Steinhardt]{hendrycksapps2021}
Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E.,
  Burns, C., Puranik, S., He, H., Song, D., and Steinhardt, J.
\newblock Measuring coding challenge competence with apps.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Hopcroft et~al.(2001)Hopcroft, Motwani, and
  Ullman]{hopcroft2001introduction}
Hopcroft, J.~E., Motwani, R., and Ullman, J.~D.
\newblock Introduction to automata theory, languages, and computation.
\newblock \emph{Acm Sigact News}, 32\penalty0 (1):\penalty0 60--65, 2001.

\bibitem[Javalang(2020)]{javalang}
Javalang.
\newblock {Pure Python Java parser and tools}, March 2020.
\newblock {https://pypi.org/project/javalang/}.

\bibitem[Knuth(1968)]{MST:Knuth68}
Knuth, D.
\newblock Semantics of context-free languages.
\newblock \emph{Mathematical Systems Theory}, 2\penalty0 (2):\penalty0
  127--145, June 1968.

\bibitem[Kusner et~al.(2017)Kusner, Paige, and
  Hern{\'a}ndez-Lobato]{kusner2017grammar}
Kusner, M.~J., Paige, B., and Hern{\'a}ndez-Lobato, J.~M.
\newblock Grammar variational autoencoder.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1945--1954. PMLR, 2017.

\bibitem[Li et~al.(2016)Li, Tarlow, Brockschmidt, and Zemel]{Li2016GatedGS}
Li, Y., Tarlow, D., Brockschmidt, M., and Zemel, R.
\newblock Gated graph sequence neural networks.
\newblock \emph{CoRR}, abs/1511.05493, 2016.

\bibitem[Ling et~al.(2016)Ling, Blunsom, Grefenstette, Hermann,
  Ko{\v{c}}isk{\`y}, Wang, and Senior]{ling2016latent}
Ling, W., Blunsom, P., Grefenstette, E., Hermann, K.~M., Ko{\v{c}}isk{\`y}, T.,
  Wang, F., and Senior, A.
\newblock Latent predictor networks for code generation.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association
  for Computational Linguistics, (Volume 1)}, pp.\  599--609, 2016.

\bibitem[Lu et~al.(2021)Lu, Guo, Ren, Huang, Svyatkovskiy, Blanco, Clement,
  Drain, Jiang, Tang, et~al.]{lu2021codexglue}
Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C.,
  Drain, D., Jiang, D., Tang, D., et~al.
\newblock Codexglue: A machine learning benchmark dataset for code
  understanding and generation.
\newblock \emph{arXiv preprint arXiv:2102.04664}, 2021.

\bibitem[Maddison \& Tarlow(2014)Maddison and Tarlow]{ICML:MT14}
Maddison, C. and Tarlow, D.
\newblock Structured generative models of natural source code.
\newblock In \emph{ICML}, pp.\  II–649–II–657, 2014.

\bibitem[Murali et~al.(2018)Murali, Qi, Chaudhuri, and Jermaine]{bayou}
Murali, V., Qi, L., Chaudhuri, S., and Jermaine, C.
\newblock Neural sketch learning for conditional program generation.
\newblock In \emph{ICLR}, 2018.

\bibitem[Nguyen \& Nguyen(2015)Nguyen and Nguyen]{nguyenICSE15}
Nguyen, A.~T. and Nguyen, T.~N.
\newblock Graph-based statistical language model for code.
\newblock In \emph{Proceedings of the 37th International Conference on Software
  Engineering - Volume 1}, ICSE '15, pp.\  858--868, Piscataway, NJ, USA, 2015.
  IEEE Press.
\newblock ISBN 978-1-4799-1934-5.

\bibitem[Nguyen et~al.(2013)Nguyen, Nguyen, Nguyen, and Nguyen]{nguyenFSE13}
Nguyen, T.~T., Nguyen, A.~T., Nguyen, H.~A., and Nguyen, T.~N.
\newblock A statistical semantic language model for source code.
\newblock In \emph{Proceedings of the 2013 9th Joint Meeting on Foundations of
  Software Engineering}, ESEC/FSE 2013, pp.\  532--542, New York, NY, USA,
  2013. ACM.
\newblock ISBN 978-1-4503-2237-9.
\newblock \doi{10.1145/2491411.2491458}.

\bibitem[Odena \& Sutton(2020)Odena and Sutton]{OdenaS20}
Odena, A. and Sutton, C.
\newblock Learning to represent programs with property signatures.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.

\bibitem[Parisotto et~al.(2017)Parisotto, Mohamed, Singh, Li, Zhou, and
  Kohli]{parisotto2016neuro}
Parisotto, E., Mohamed, A.-r., Singh, R., Li, L., Zhou, D., and Kohli, P.
\newblock Neuro-symbolic program synthesis.
\newblock In \emph{ICLR}, 2017.

\bibitem[Polikarpova et~al.(2016)Polikarpova, Kuraj, and Solar-Lezama]{synquid}
Polikarpova, N., Kuraj, I., and Solar-Lezama, A.
\newblock Program synthesis from polymorphic refinement types.
\newblock \emph{ACM SIGPLAN Notices}, 51\penalty0 (6):\penalty0 522--538, 2016.

\bibitem[Raychev et~al.(2014)Raychev, Vechev, and Yahav]{raychev2014code}
Raychev, V., Vechev, M., and Yahav, E.
\newblock Code completion with statistical language models.
\newblock In \emph{Proceedings of the 35th ACM SIGPLAN Conference on
  Programming Language Design and Implementation}, pp.\  419--428, 2014.

\bibitem[Shah et~al.(2020)Shah, Zhan, Sun, Verma, Yue, and Chaudhuri]{near}
Shah, A., Zhan, E., Sun, J.~J., Verma, A., Yue, Y., and Chaudhuri, S.
\newblock Learning differentiable programs with admissible neural heuristics.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Si et~al.(2019)Si, Yang, Dai, Naik, and Song]{si2019learning}
Si, X., Yang, Y., Dai, H., Naik, M., and Song, L.
\newblock Learning a meta-solver for syntax-guided program synthesis.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and
  Le]{sutskever2014sequence}
Sutskever, I., Vinyals, O., and Le, Q.~V.
\newblock Sequence to sequence learning with neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3104--3112, 2014.

\bibitem[Svyatkovskiy et~al.(2020)Svyatkovskiy, Deng, Fu, and
  Sundaresan]{svyatkovskiy2020intellicode}
Svyatkovskiy, A., Deng, S.~K., Fu, S., and Sundaresan, N.
\newblock Intellicode compose: Code generation using transformer.
\newblock In \emph{Proceedings of the 28th ACM Joint Meeting on European
  Software Engineering Conference and Symposium on the Foundations of Software
  Engineering}, pp.\  1433--1443, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5998--6008, 2017.

\bibitem[Yin \& Neubig(2017)Yin and Neubig]{YinN17}
Yin, P. and Neubig, G.
\newblock A syntactic neural model for general-purpose code generation.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics, {ACL} 2017, Vancouver, Canada, July 30 -
  August 4, Volume 1: Long Papers}, pp.\  440--450, 2017.

\end{thebibliography}
