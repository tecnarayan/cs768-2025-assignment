\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjovsky \& Bottou(2017)Arjovsky and Bottou]{Arjovsky2017TowardsPM}
Martin Arjovsky and L{\'e}on Bottou.
\newblock Towards principled methods for training generative adversarial
  networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and Bottou]{Arjovsky2017WGAN}
Martin Arjovsky, Soumith Chintala, and L{\'e}on Bottou.
\newblock {W}asserstein generative adversarial networks.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[Bi{\'n}kowski et~al.(2018)Bi{\'n}kowski, Sutherland, Arbel, and
  Gretton]{Binkowski2016MMDGAN}
Miko{\l}aj Bi{\'n}kowski, Dougal~J. Sutherland, Michael~N. Arbel, and Athur
  Gretton.
\newblock Demystifying {MMD GAN}s.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Brock et~al.(2019)Brock, Donahue, and Simonyan]{Brock2019}
Andrew Brock, Jeff Donahue, and Karen Simonyan.
\newblock Large scale {GAN} training for high fidelity natural image synthesis.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Chen et~al.(2019{\natexlab{a}})Chen, Lucic, Houlsby, and
  Gelly]{chen2018on}
Ting Chen, Mario Lucic, Neil Houlsby, and Sylvain Gelly.
\newblock On self modulation for generative adversarial networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019{\natexlab{a}}.

\bibitem[Chen et~al.(2019{\natexlab{b}})Chen, Zhai, Ritter, Lucic, and
  Houlsby]{ChenSS2019}
Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, and Neil Houlsby.
\newblock Self-supervised gans via auxiliary rotation loss.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2019{\natexlab{b}}.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{imagenet_cvpr09}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2009.

\bibitem[Donahue et~al.(2017)Donahue, Kr{\"a}henb{\"u}hl, and Darrell]{ali2017}
Jeff Donahue, Philipp Kr{\"a}henb{\"u}hl, and Trevor Darrell.
\newblock Adversarial feature learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Durugkar et~al.(2017)Durugkar, Gemp, and
  Mahadevan]{Durugkar2016GenerativeMN}
Ishan~P. Durugkar, Ian Gemp, and Sridhar Mahadevan.
\newblock Generative multi-adversarial networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Gidaris et~al.(2018)Gidaris, Singh, and Komodakis]{Gidaris2018}
Spyros Gidaris, Praveer Singh, and Nikos Komodakis.
\newblock Unsupervised representation learning by predicting image rotations.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, Vancouver, Canada, April 2018.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{GlorotAISTATS2010}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics (AISTATS 2010)}, 2010.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2014.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{gulrajani_NIPS2017}
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron~C
  Courville.
\newblock Improved training of {Wasserstein} {GAN}s.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{heuselttur2017}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock {GANs} trained by a two time-scale update rule converge to a local
  nash equilibrium.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[Husz{\'a}r(2015)]{FID}
Ferenc Husz{\'a}r.
\newblock How (not) to train your generative model: Scheduled sampling,
  likelihood, adversary?
\newblock \emph{arXiv: 1511.05101}, 2015.

\bibitem[Karras et~al.(2018)Karras, Aila, Laine, and
  Lehtinen]{karras2018progressive}
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
\newblock Progressive growing of {GAN}s for improved quality, stability, and
  variation.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{adamopt}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem[Krizhevsky(2009)]{Cifar10_Krizhevsky09learningmultiple}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Kurach et~al.(2018)Kurach, Lu\u{c}i\'{c}, Zhai, Michalski, and
  Gelly]{Kurach2018GANlandscape}
Karol Kurach, Mario Lu\u{c}i\'{c}, Xiaohua Zhai, Marcin Michalski, and Sylvain
  Gelly.
\newblock The {GAN} landscape: Losses, architectures, regularization, and
  normalization.
\newblock \emph{arXiv: 1807.04720}, 2018.

\bibitem[Lin et~al.(2019)Lin, Chang, Chen, Juan, Wei, and Chen]{lin2019cocogan}
Chieh~Hubert Lin, Chia{-}Che Chang, Yu{-}Sheng Chen, Da{-}Cheng Juan, Wei Wei,
  and Hwann{-}Tzong Chen.
\newblock {COCO-GAN:} generation by parts via conditional coordinating.
\newblock In \emph{IEEE International Conference on Computer Vision (ICCV)},
  2019.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{Liu_Celeba}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{International Conference on Computer Vision (ICCV)}, 2015.

\bibitem[Lu\u{c}i\'{c} et~al.(2018)Lu\u{c}i\'{c}, Kurach, Michalski, Gelly, and
  Bousquet]{LucicEqualGANs}
Mario Lu\u{c}i\'{c}, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier
  Bousquet.
\newblock Are {GAN}s created equal? {A} large-scale study.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2018.

\bibitem[Mao et~al.(2016)Mao, Li, Xie, Lau, and Wang]{MaoLXLW16}
Xudong Mao, Qing Li, Haoran Xie, Raymond Y.~K. Lau, and Zhen Wang.
\newblock Multi-class generative adversarial networks with the {L2} loss
  function.
\newblock \emph{arXiv:1611.04076}, 2016.

\bibitem[Mescheder et~al.(2018)Mescheder, Geiger, and
  Nowozin]{MeschederICML2018}
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin.
\newblock Which training methods for {GAN}s do actually converge?
\newblock In \emph{International Conference on Machine learning (ICML)}, 2018.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and
  Yoshida]{miyato2018spectral}
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Nowozin et~al.(2016)Nowozin, Cseke, and Tomioka]{Nowozin2016fGANTG}
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka.
\newblock {f-GAN}: Training generative neural samplers using variational
  divergence minimization.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2016.

\bibitem[Odena et~al.(2017)Odena, Olah, and Shlens]{odena17a}
Augustus Odena, Christopher Olah, and Jonathon Shlens.
\newblock Conditional image synthesis with auxiliary classifier {GAN}s.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Radford et~al.(2016)Radford, Metz, and
  Chintala]{Radford2016UnsupervisedRL}
Alec Radford, Luke Metz, and Soumith Chintala.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[Roth et~al.(2017)Roth, Lucchi, Nowozin, and Hofmann]{Roth_NIPS2017}
Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann.
\newblock Stabilizing training of generative adversarial networks through
  regularization.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[Sajjadi et~al.(2018)Sajjadi, Parascandolo, and
  Arash~Mehrjou]{SajParMehSch18}
Mehdi S.~M. Sajjadi, Giambattista Parascandolo, and Bernhard~Sch{\"o}lkopf
  Arash~Mehrjou.
\newblock Tempered adversarial networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  Chen, and Chen]{SalimansNIPS2016}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,
  Xi~Chen, and Xi~Chen.
\newblock Improved techniques for training {GAN}s.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2016.

\bibitem[S{\o}nderby et~al.(2017)S{\o}nderby, Caballero, Theis, Shi, and
  Husz{\'a}r]{Sonderby2016AmortisedMI}
Casper~Kaae S{\o}nderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc
  Husz{\'a}r.
\newblock Amortised map inference for image super-resolution.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{JMLR:v15:srivastava14a}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 2014.

\bibitem[Theis et~al.(2016)Theis, van~den Oord, and Bethge]{Theis2016a}
Lucas Theis, Aaron van~den Oord, and Matthias Bethge.
\newblock A note on the evaluation of generative models.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[Tompson et~al.(2015)Tompson, Goroshin, Jain, LeCun, and
  Bregler]{Tompson2015EfficientOL}
Jonathan Tompson, Ross Goroshin, Arjun Jain, Yann LeCun, and Christoph Bregler.
\newblock Efficient object localization using convolutional networks.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2015.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv: 1708.07747}, 2017.

\bibitem[Zhang et~al.(2018)Zhang, Goodfellow, Metaxas, and
  Odena]{Zhang_SAGAN18}
Han Zhang, Ian~J. Goodfellow, Dimitris~N. Metaxas, and Augustus Odena.
\newblock Self-attention generative adversarial networks.
\newblock \emph{arXiv: 1805.08318}, 2018.

\end{thebibliography}
