\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2016)Arora, Ge, Koehler, Ma, and Moitra]{arora2016topic}
Sanjeev Arora, Rong Ge, Frederic Koehler, Tengyu Ma, and Ankur Moitra.
\newblock Provable algorithms for inference in topic models.
\newblock In Maria~Florina Balcan and Kilian~Q. Weinberger (eds.),
  \emph{Proceedings of The 33rd International Conference on Machine Learning},
  volume~48 of \emph{Proceedings of Machine Learning Research}, pp.\
  2859--2867, New York, New York, USA, 20--22 Jun 2016. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v48/arorab16.html}.

\bibitem[Awasthi \& Risteski(2015)Awasthi and Risteski]{awasthi2015variational}
Pranjal Awasthi and Andrej Risteski.
\newblock On some provably correct cases of variational inference for topic
  models.
\newblock In C.~Cortes, N.~Lawrence, D.~Lee, M.~Sugiyama, and R.~Garnett
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~28.
  Curran Associates, Inc., 2015.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2015/file/68a83eeb494a308fe5295da69428a507-Paper.pdf}.

\bibitem[Belinkov(2022)]{belinkov2022probing}
Yonatan Belinkov.
\newblock Probing classifiers: Promises, shortcomings, and advances.
\newblock \emph{Computational Linguistics}, 48\penalty0 (1):\penalty0 207--219,
  March 2022.
\newblock \doi{10.1162/coli_a_00422}.
\newblock URL \url{https://aclanthology.org/2022.cl-1.7}.

\bibitem[Bhattamishra et~al.(2020{\natexlab{a}})Bhattamishra, Ahuja, and
  Goyal]{bhattamishra2020ability}
Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal.
\newblock On the {A}bility and {L}imitations of {T}ransformers to {R}ecognize
  {F}ormal {L}anguages.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  7096--7116, Online, November
  2020{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.576}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.576}.

\bibitem[Bhattamishra et~al.(2020{\natexlab{b}})Bhattamishra, Patel, and
  Goyal]{bhattamishra2020computational}
Satwik Bhattamishra, Arkil Patel, and Navin Goyal.
\newblock On the computational power of transformers and its implications in
  sequence modeling.
\newblock In \emph{Proceedings of the 24th Conference on Computational Natural
  Language Learning}, pp.\  455--475, Online, November 2020{\natexlab{b}}.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.conll-1.37}.
\newblock URL \url{https://aclanthology.org/2020.conll-1.37}.

\bibitem[Blei et~al.(2003)Blei, Ng, and Jordan]{blei2003latent}
David~M. Blei, Andrew~Y. Ng, and Michael~I. Jordan.
\newblock Latent dirichlet allocation.
\newblock \emph{J. Mach. Learn. Res.}, 3\penalty0 (null):\penalty0 993â€“1022,
  mar 2003.
\newblock ISSN 1532-4435.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  1877--1901. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Clark et~al.(2019)Clark, Khandelwal, Levy, and Manning]{clark2019bert}
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher~D. Manning.
\newblock What does {BERT} look at? an analysis of {BERT}{'}s attention.
\newblock In \emph{Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing
  and Interpreting Neural Networks for NLP}, pp.\  276--286, Florence, Italy,
  August 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/W19-4828}.
\newblock URL \url{https://aclanthology.org/W19-4828}.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pp.\  4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://aclanthology.org/N19-1423}.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2021an}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[Ebrahimi et~al.(2020)Ebrahimi, Gelda, and Zhang]{ebrahimi2020self}
Javid Ebrahimi, Dhruv Gelda, and Wei Zhang.
\newblock How can self-attention networks recognize {D}yck-n languages?
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pp.\  4301--4306, Online, November 2020. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2020.findings-emnlp.384}.
\newblock URL \url{https://aclanthology.org/2020.findings-emnlp.384}.

\bibitem[Edelman et~al.(2022)Edelman, Goel, Kakade, and
  Zhang]{edelman2022inductive}
Benjamin~L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang.
\newblock Inductive biases and variable creation in self-attention mechanisms.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato (eds.), \emph{Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of
  \emph{Proceedings of Machine Learning Research}, pp.\  5793--5831. PMLR,
  17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/edelman22a.html}.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann,
  Askell, Bai, Chen, Conerly, DasSarma, Drain, Ganguli, Hatfield-Dodds,
  Hernandez, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan,
  McCandlish, and Olah]{elhage2021mathematical}
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben
  Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn
  Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson
  Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark,
  Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 2021.
\newblock https://transformer-circuits.pub/2021/framework/index.html.

\bibitem[Gers \& Schmidhuber(2001)Gers and Schmidhuber]{gers2001lstm}
F.~Gers and J.~Schmidhuber.
\newblock Lstm recurrent networks learn simple context-free and
  context-sensitive languages.
\newblock \emph{IEEE transactions on neural networks}, 12 6:\penalty0 1333--40,
  2001.

\bibitem[Hewitt \& Liang(2019)Hewitt and Liang]{hewitt2019designing}
John Hewitt and Percy Liang.
\newblock Designing and interpreting probes with control tasks.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pp.\  2733--2743, Hong Kong,
  China, November 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-1275}.
\newblock URL \url{https://aclanthology.org/D19-1275}.

\bibitem[Hewitt \& Manning(2019)Hewitt and Manning]{hewitt2019structural}
John Hewitt and Christopher~D. Manning.
\newblock {A} structural probe for finding syntax in word representations.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pp.\  4129--4138,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1419}.
\newblock URL \url{https://www.aclweb.org/anthology/N19-1419}.

\bibitem[Hewitt et~al.(2020)Hewitt, Hahn, Ganguli, Liang, and
  Manning]{hewitt2020rnns}
John Hewitt, Michael Hahn, Surya Ganguli, Percy Liang, and Christopher~D.
  Manning.
\newblock {RNN}s can generate bounded hierarchical languages with optimal
  memory.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  1978--2010, Online, November
  2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.156}.
\newblock URL \url{https://www.aclweb.org/anthology/2020.emnlp-main.156}.

\bibitem[Htut et~al.(2019)Htut, Phang, Bordia, and Bowman]{htut2019attention}
Phu~Mon Htut, Jason Phang, Shikha Bordia, and Samuel~R. Bowman.
\newblock Do attention heads in bert track syntactic dependencies?
\newblock \emph{ArXiv}, abs/1911.12246, 2019.

\bibitem[Jelassi et~al.(2022)Jelassi, Sander, and Li]{jelassi2022vision}
Samy Jelassi, Michael~Eli Sander, and Yuanzhi Li.
\newblock Vision transformers provably learn spatial structure.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=eMW9AkXaREI}.

\bibitem[Jumper et~al.(2021)Jumper, Evans, Pritzel, Green, Figurnov,
  Ronneberger, Tunyasuvunakool, Bates, {\v{Z}}{\'\i}dek, Potapenko,
  et~al.]{jumper2021highly}
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov,
  Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin
  {\v{Z}}{\'\i}dek, Anna Potapenko, et~al.
\newblock Highly accurate protein structure prediction with alphafold.
\newblock \emph{Nature}, 596\penalty0 (7873):\penalty0 583--589, 2021.

\bibitem[Kovaleva et~al.(2019)Kovaleva, Romanov, Rogers, and
  Rumshisky]{kovaleva2019revealing}
Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky.
\newblock Revealing the dark secrets of {BERT}.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pp.\  4365--4374, Hong Kong,
  China, November 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-1445}.
\newblock URL \url{https://aclanthology.org/D19-1445}.

\bibitem[Li \& Gong(2021)Li and Gong]{li2021robust}
Xian Li and Hongyu Gong.
\newblock Robust optimization for multilingual translation with imbalanced
  data.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  25086--25099. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/d324a0cc02881779dcda44a675fdcaaa-Paper.pdf}.

\bibitem[Li \& Risteski(2021)Li and Risteski]{li2021limitations}
Yuchen Li and Andrej Risteski.
\newblock The limitations of limited context for constituency parsing.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pp.\  2675--2687,
  Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.208}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.208}.

\bibitem[Liu et~al.(2022)Liu, Hsu, Ravikumar, and Risteski]{liu2022masked}
Bingbin Liu, Daniel Hsu, Pradeep~Kumar Ravikumar, and Andrej Risteski.
\newblock Masked prediction: A parameter identifiability view.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=Hbvlb4D1aFC}.

\bibitem[Liu et~al.(2023)Liu, Ash, Goel, Krishnamurthy, and
  Zhang]{liu2022shortcuts}
Bingbin Liu, Jordan~T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang.
\newblock Transformers learn shortcuts to automata.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=De4FYqjFueZ}.

\bibitem[Liu et~al.(2020)Liu, Liu, Gao, Chen, and Han]{liu2020understanding}
Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han.
\newblock Understanding the difficulty of training transformers.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  5747--5763, Online, November
  2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.463}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.463}.

\bibitem[Luo et~al.(2022)Luo, Weng, Wu, Zhou, and Ge]{luo2022one}
Zeping Luo, Cindy Weng, Shiyou Wu, Mo~Zhou, and Rong Ge.
\newblock One objective for all models--self-supervised learning for topic
  models.
\newblock \emph{arXiv preprint arXiv:2203.03539}, 2022.

\bibitem[Meng et~al.(2022)Meng, Zhang, Huang, Zhang, and Han]{meng2022topic}
Yu~Meng, Yunyi Zhang, Jiaxin Huang, Yu~Zhang, and Jiawei Han.
\newblock Topic discovery via latent space clustering of pretrained language
  model representations.
\newblock In \emph{Proceedings of the ACM Web Conference 2022}, WWW '22, pp.\
  3143â€“3152, New York, NY, USA, 2022. Association for Computing Machinery.
\newblock ISBN 9781450390965.
\newblock \doi{10.1145/3485447.3512034}.
\newblock URL \url{https://doi.org/10.1145/3485447.3512034}.

\bibitem[Merrill(2019)]{merrill2019sequential}
William Merrill.
\newblock Sequential neural networks as automata.
\newblock In \emph{Proceedings of the Workshop on Deep Learning and Formal
  Languages: Building Bridges}, pp.\  1--13, Florence, August 2019. Association
  for Computational Linguistics.
\newblock \doi{10.18653/v1/W19-3901}.
\newblock URL \url{https://www.aclweb.org/anthology/W19-3901}.

\bibitem[Nguyen \& Salazar(2019)Nguyen and Salazar]{nguyen2019transformers}
Toan~Q. Nguyen and Julian Salazar.
\newblock Transformers without tears: Improving the normalization of
  self-attention.
\newblock In \emph{Proceedings of the 16th International Conference on Spoken
  Language Translation}, Hong Kong, November 2-3 2019. Association for
  Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2019.iwslt-1.17}.

\bibitem[Press \& Wolf(2017)Press and Wolf]{press2017using}
Ofir Press and Lior Wolf.
\newblock Using the output embedding to improve language models.
\newblock In \emph{Proceedings of the 15th Conference of the {E}uropean Chapter
  of the Association for Computational Linguistics: Volume 2, Short Papers},
  pp.\  157--163, Valencia, Spain, April 2017. Association for Computational
  Linguistics.
\newblock URL \url{https://aclanthology.org/E17-2025}.

\bibitem[Sia et~al.(2020)Sia, Dalmia, and Mielke]{sia2020tired}
Suzanna Sia, Ayush Dalmia, and Sabrina~J. Mielke.
\newblock Tired of topic models? clusters of pretrained word embeddings make
  for fast and good topics too!
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  1728--1736, Online, November
  2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.135}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.135}.

\bibitem[Siegelmann \& Sontag(1992)Siegelmann and Sontag]{siegelmann1992rnn}
Hava~T. Siegelmann and Eduardo~D. Sontag.
\newblock On the computational power of neural nets.
\newblock In \emph{Proceedings of the Fifth Annual Workshop on Computational
  Learning Theory}, COLT '92, pp.\  440â€“449, New York, NY, USA, 1992.
  Association for Computing Machinery.
\newblock ISBN 089791497X.
\newblock \doi{10.1145/130385.130432}.
\newblock URL \url{https://doi.org/10.1145/130385.130432}.

\bibitem[Snell et~al.(2021)Snell, Zhong, Klein, and
  Steinhardt]{snell2021approximating}
Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt.
\newblock Approximating how single head attention learns, 2021.

\bibitem[Sontag \& Roy(2011)Sontag and Roy]{sontag2011complexity}
David Sontag and Dan Roy.
\newblock Complexity of inference in latent dirichlet allocation.
\newblock In J.~Shawe-Taylor, R.~Zemel, P.~Bartlett, F.~Pereira, and K.Q.
  Weinberger (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~24. Curran Associates, Inc., 2011.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2011/file/3871bd64012152bfb53fdf04b401193f-Paper.pdf}.

\bibitem[Sun \& Marasovi{\'c}(2021)Sun and Marasovi{\'c}]{sun2021effective}
Kaiser Sun and Ana Marasovi{\'c}.
\newblock Effective attention sheds light on interpretability.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL-IJCNLP 2021}, pp.\  4126--4135, Online, August 2021. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2021.findings-acl.361}.
\newblock URL \url{https://aclanthology.org/2021.findings-acl.361}.

\bibitem[Sun \& Lu(2020)Sun and Lu]{sun2020understanding}
Xiaobing Sun and Wei Lu.
\newblock Understanding attention for text classification.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  3418--3428, Online, July 2020.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.312}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.312}.

\bibitem[Suzgun et~al.(2019)Suzgun, Belinkov, Shieber, and
  Gehrmann]{suzgun2019lstm}
Mirac Suzgun, Yonatan Belinkov, Stuart Shieber, and Sebastian Gehrmann.
\newblock {LSTM} networks can perform dynamic counting.
\newblock In \emph{Proceedings of the Workshop on Deep Learning and Formal
  Languages: Building Bridges}, pp.\  44--54, Florence, August 2019.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/W19-3905}.
\newblock URL \url{https://www.aclweb.org/anthology/W19-3905}.

\bibitem[Talebpour et~al.(2023)Talebpour, Garc{\'i}a Seco~de Herrera, and
  Jameel]{talebpour2023topics}
Mozhgan Talebpour, Alba Garc{\'i}a Seco~de Herrera, and Shoaib Jameel.
\newblock Topics in contextualised attention embeddings.
\newblock In Jaap Kamps, Lorraine Goeuriot, Fabio Crestani, Maria Maistro,
  Hideo Joho, Brian Davis, Cathal Gurrin, Udo Kruschwitz, and Annalina Caputo
  (eds.), \emph{Advances in Information Retrieval}, pp.\  221--238, Cham, 2023.
  Springer Nature Switzerland.
\newblock ISBN 978-3-031-28238-6.

\bibitem[Tenney et~al.(2019)Tenney, Das, and Pavlick]{tenney2019bert}
Ian Tenney, Dipanjan Das, and Ellie Pavlick.
\newblock {BERT} rediscovers the classical {NLP} pipeline.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  4593--4601, Florence, Italy, July 2019.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P19-1452}.
\newblock URL \url{https://aclanthology.org/P19-1452}.

\bibitem[Thompson \& Mimno(2020)Thompson and Mimno]{thompson2020topic}
Laure Thompson and David Mimno.
\newblock Topic modeling with contextualized word representation clusters,
  2020.

\bibitem[Tosh et~al.(2021)Tosh, Krishnamurthy, and Hsu]{tosh2021contrastive}
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu.
\newblock Contrastive estimation reveals topic posterior information to linear
  models.
\newblock \emph{J. Mach. Learn. Res.}, 22\penalty0 (1), jan 2021.
\newblock ISSN 1532-4435.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett (eds.), \emph{Advances in Neural Information
  Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.

\bibitem[Vig \& Belinkov(2019)Vig and Belinkov]{vig2019analyzing}
Jesse Vig and Yonatan Belinkov.
\newblock Analyzing the structure of attention in a transformer language model.
\newblock In \emph{Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing
  and Interpreting Neural Networks for NLP}, pp.\  63--76, Florence, Italy,
  August 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/W19-4808}.
\newblock URL \url{https://aclanthology.org/W19-4808}.

\bibitem[Wei et~al.(2021)Wei, Chen, and Ma]{wei2021statistically}
Colin Wei, Yining Chen, and Tengyu Ma.
\newblock Statistically meaningful approximation: a case study on approximating
  turing machines with transformers, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.13163}.

\bibitem[Weiss et~al.(2018)Weiss, Goldberg, and Yahav]{weiss2018practical}
Gail Weiss, Yoav Goldberg, and Eran Yahav.
\newblock On the practical computational power of finite precision {RNN}s for
  language recognition.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, pp.\  740--745,
  Melbourne, Australia, July 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P18-2117}.
\newblock URL \url{https://www.aclweb.org/anthology/P18-2117}.

\bibitem[WikimediaFoundation(2023)]{wikidump}
WikimediaFoundation.
\newblock Wikimedia downloads.
\newblock \emph{Wikimedia Downloads}, 2023.
\newblock URL \url{https://dumps.wikimedia.org}.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Scao, Gugger, Drame, Lhoest, and Rush]{wolf2020transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pp.\  38--45, Online,
  October 2020. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/2020.emnlp-demos.6}.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan,
  Wang, and Liu]{xiong2020layer}
Ruibin Xiong, Yunchang Yang, Di~He, Kai Zheng, Shuxin Zheng, Chen Xing,
  Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu.
\newblock On layer normalization in the transformer architecture.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, ICML'20. JMLR.org, 2020.

\bibitem[Yao et~al.(2021)Yao, Peng, Papadimitriou, and Narasimhan]{yao2021self}
Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan.
\newblock Self-attention networks can process bounded hierarchical languages.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pp.\  3770--3785,
  Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.292}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.292}.

\bibitem[Yun et~al.(2020)Yun, Bhojanapalli, Rawat, Reddi, and
  Kumar]{yun2020are}
Chulhee Yun, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank Reddi, and Sanjiv
  Kumar.
\newblock Are transformers universal approximators of sequence-to-sequence
  functions?
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=ByxRM0Ntvr}.

\bibitem[Zhang et~al.(2020)Zhang, Karimireddy, Veit, Kim, Reddi, Kumar, and
  Sra]{zhang2020adaptive}
Jingzhao Zhang, Sai~Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank
  Reddi, Sanjiv Kumar, and Suvrit Sra.
\newblock Why are adaptive methods good for attention models?
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  15383--15393. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/b05b57f6add810d3b7490866d74c0053-Paper.pdf}.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Backurs, Bubeck, Eldan,
  Gunasekar, and Wagner]{zhang2022unveiling}
Yi~Zhang, Arturs Backurs, SÃ©bastien Bubeck, Ronen Eldan, Suriya Gunasekar, and
  Tal Wagner.
\newblock Unveiling transformers with lego: a synthetic reasoning task,
  2022{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2206.04301}.

\bibitem[Zhang et~al.(2023)Zhang, Liu, Cai, Wang, and Wang]{zhang2023analysis}
Yufeng Zhang, Boyi Liu, Qi~Cai, Lingxiao Wang, and Zhaoran Wang.
\newblock An analysis of attention via the lens of exchangeability and latent
  variable models, 2023.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Fang, Chen, and
  Namazi~Rad]{zhang2022neural}
Zihan Zhang, Meng Fang, Ling Chen, and Mohammad~Reza Namazi~Rad.
\newblock Is neural topic modelling better than clustering? an empirical study
  on clustering with contextual embeddings for topics.
\newblock In \emph{Proceedings of the 2022 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pp.\  3886--3893, Seattle, United States, July
  2022{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.naacl-main.285}.
\newblock URL \url{https://aclanthology.org/2022.naacl-main.285}.

\end{thebibliography}
