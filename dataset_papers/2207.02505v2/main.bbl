\begin{thebibliography}{10}

\bibitem{andreoli2019convolution}
J.-M. Andreoli.
\newblock Convolution, attention and structure embedding.
\newblock {\em arXiv}, 2019.

\bibitem{barabasi1999emergence}
A.-L. Barabasi and R.~Albert.
\newblock Emergence of scaling in random networks.
\newblock {\em arXiv}, 1999.

\bibitem{bhattamishra2020on}
S.~Bhattamishra, A.~Patel, and N.~Goyal.
\newblock On the computational power of transformers and its implications in
  sequence modeling.
\newblock In {\em CoNLL}, 2020.

\bibitem{bhojanapalli2020low}
S.~Bhojanapalli, C.~Yun, A.~S. Rawat, S.~J. Reddi, and S.~Kumar.
\newblock Low-rank bottleneck in multi-head attention models.
\newblock In {\em ICML}, 2020.

\bibitem{brody2022how}
S.~Brody, U.~Alon, and E.~Yahav.
\newblock How attentive are graph attention networks?
\newblock In {\em ICLR}, 2022.

\bibitem{bronstein2017geometric}
M.~M. Bronstein, J.~Bruna, Y.~LeCun, A.~Szlam, and P.~Vandergheynst.
\newblock Geometric deep learning: Going beyond euclidean data.
\newblock {\em {IEEE} Signal Process. Mag.}, 2017.

\bibitem{brown2020language}
T.~B. Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, S.~Agarwal,
  A.~Herbert{-}Voss, G.~Krueger, T.~Henighan, R.~Child, A.~Ramesh, D.~M.
  Ziegler, J.~Wu, C.~Winter, C.~Hesse, M.~Chen, E.~Sigler, M.~Litwin, S.~Gray,
  B.~Chess, J.~Clark, C.~Berner, S.~McCandlish, A.~Radford, I.~Sutskever, and
  D.~Amodei.
\newblock Language models are few-shot learners.
\newblock In {\em NeurIPS}, 2020.

\bibitem{cai2020a}
C.~Cai and Y.~Wang.
\newblock A note on over-smoothing for graph neural networks.
\newblock {\em arXiv}, 2020.

\bibitem{chen2021decision}
L.~Chen, K.~Lu, A.~Rajeswaran, K.~Lee, A.~Grover, M.~Laskin, P.~Abbeel,
  A.~Srinivas, and I.~Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock In {\em NeurIPS}, 2021.

\bibitem{chen2020can}
Z.~Chen, L.~Chen, S.~Villar, and J.~Bruna.
\newblock Can graph neural networks count substructures?
\newblock In {\em NeurIPS}, 2020.

\bibitem{choromanski2021rethinking}
K.~M. Choromanski, V.~Likhosherstov, D.~Dohan, X.~Song, A.~Gane,
  T.~Sarl{\'{o}}s, P.~Hawkins, J.~Q. Davis, A.~Mohiuddin, L.~Kaiser, D.~B.
  Belanger, L.~J. Colwell, and A.~Weller.
\newblock Rethinking attention with performers.
\newblock In {\em ICLR}, 2021.

\bibitem{choromanski2017the}
K.~M. Choromanski, M.~Rowland, and A.~Weller.
\newblock The unreasonable effectiveness of structured random orthogonal
  embeddings.
\newblock In {\em NeurIPS}, 2017.

\bibitem{cohen2016group}
T.~Cohen and M.~Welling.
\newblock Group equivariant convolutional networks.
\newblock In {\em ICML}, 2016.

\bibitem{cohen2017steerable}
T.~S. Cohen and M.~Welling.
\newblock Steerable cnns.
\newblock In {\em ICLR}, 2017.

\bibitem{cordonnier2020on}
J.~Cordonnier, A.~Loukas, and M.~Jaggi.
\newblock On the relationship between self-attention and convolutional layers.
\newblock In {\em ICLR}, 2020.

\bibitem{dai2021coatnet}
Z.~Dai, H.~Liu, Q.~V. Le, and M.~Tan.
\newblock Coatnet: Marrying convolution and attention for all data sizes.
\newblock In {\em NeurIPS}, 2021.

\bibitem{devlin2019bert}
J.~Devlin, M.~Chang, K.~Lee, and K.~Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em NAACL-HLT}, 2019.

\bibitem{dosovitskiy2021animage}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit,
  and N.~Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em ICLR}, 2021.

\bibitem{dwivedi2020a}
V.~P. Dwivedi and X.~Bresson.
\newblock A generalization of transformer networks to graphs.
\newblock {\em arXiv}, 2020.

\bibitem{dwivedi2020benchmarking}
V.~P. Dwivedi, C.~K. Joshi, T.~Laurent, Y.~Bengio, and X.~Bresson.
\newblock Benchmarking graph neural networks.
\newblock {\em arXiv}, 2020.

\bibitem{gilmer2017neural}
J.~Gilmer, S.~S. Schoenholz, P.~F. Riley, O.~Vinyals, and G.~E. Dahl.
\newblock Neural message passing for quantum chemistry.
\newblock In {\em ICML}, 2017.

\bibitem{gorban2016approximation}
A.~N. Gorban, I.~Y. Tyukin, D.~V. Prokhorov, and K.~I. Sofeikov.
\newblock Approximation with random bases: Pro et contra.
\newblock {\em Inf. Sci.}, 2016.

\bibitem{hanin2017approximating}
B.~Hanin and M.~Sellke.
\newblock Approximating continuous functions by relu nets of minimal width.
\newblock {\em arXiv}, 2017.

\bibitem{he2021masked}
K.~He, X.~Chen, S.~Xie, Y.~Li, P.~Doll{\'{a}}r, and R.~B. Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock {\em arXiv}, 2021.

\bibitem{hendrycks2018gaussian}
D.~Hendrycks and K.~Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock {\em arXiv}, 2018.

\bibitem{hornik1989multilayer}
K.~Hornik, M.~B. Stinchcombe, and H.~White.
\newblock Multilayer feedforward networks are universal approximators.
\newblock {\em Neural Networks}, 1989.

\bibitem{hu2021ogb}
W.~Hu, M.~Fey, H.~Ren, M.~Nakata, Y.~Dong, and J.~Leskovec.
\newblock {OGB-LSC:} {A} large-scale challenge for machine learning on graphs.
\newblock {\em arXiv}, 2021.

\bibitem{huang2016deep}
G.~Huang, Y.~Sun, Z.~Liu, D.~Sedra, and K.~Q. Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In {\em ECCV}, 2016.

\bibitem{hussain2021edge}
M.~S. Hussain, M.~J. Zaki, and D.~Subramanian.
\newblock Edge-augmented graph transformers: Global self-attention is enough
  for graphs.
\newblock {\em arXiv}, 2021.

\bibitem{andrew2021perceiverio}
A.~Jaegle, S.~Borgeaud, J.~Alayrac, C.~Doersch, C.~Ionescu, D.~Ding,
  S.~Koppula, D.~Zoran, A.~Brock, E.~Shelhamer, O.~J. H{\'{e}}naff, M.~M.
  Botvinick, A.~Zisserman, O.~Vinyals, and J.~Carreira.
\newblock Perceiver {IO:} {A} general architecture for structured inputs {\&}
  outputs.
\newblock {\em arXiv}, 2021.

\bibitem{andrew2021perceiver}
A.~Jaegle, F.~Gimeno, A.~Brock, O.~Vinyals, A.~Zisserman, and J.~Carreira.
\newblock Perceiver: General perception with iterative attention.
\newblock In {\em ICML}, 2021.

\bibitem{katharopoulos2020transformers}
A.~Katharopoulos, A.~Vyas, N.~Pappas, and F.~Fleuret.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock In {\em ICML}, 2020.

\bibitem{keriven2019universal}
N.~Keriven and G.~Peyr{\'{e}}.
\newblock Universal invariant and equivariant graph neural networks.
\newblock In {\em NeurIPS}, 2019.

\bibitem{kim2021transformers}
J.~Kim, S.~Oh, and S.~Hong.
\newblock Transformers generalize deepsets and can be extended to graphs and
  hypergraphs.
\newblock In {\em NeurIPS}, 2021.

\bibitem{kingma2015adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In {\em ICLR}, 2015.

\bibitem{kipf2017semi}
T.~N. Kipf and M.~Welling.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock In {\em ICLR}, 2017.

\bibitem{zico2019champs}
Z.~Kolter, S.~Bai, D.~Wilmott, M.~Kornbluth, and J.~Mailoa.
\newblock First place solution of 2019 champs predicting molecular properties
  challenge.
\newblock
  \url{https://www.kaggle.com/c/champs-scalar-coupling/discussion/106575},
  2019.

\bibitem{kreuzer2021rethinking}
D.~Kreuzer, D.~Beaini, W.~L. Hamilton, V.~L{\'{e}}tourneau, and P.~Tossou.
\newblock Rethinking graph transformers with spectral attention.
\newblock {\em NeurIPS}, 2021.

\bibitem{lee2019set}
J.~Lee, Y.~Lee, J.~Kim, A.~R. Kosiorek, S.~Choi, and Y.~W. Teh.
\newblock Set transformer: {A} framework for attention-based
  permutation-invariant neural networks.
\newblock In {\em ICML}, 2019.

\bibitem{li2018deeper}
Q.~Li, Z.~Han, and X.~Wu.
\newblock Deeper insights into graph convolutional networks for semi-supervised
  learning.
\newblock In {\em AAAI}, 2018.

\bibitem{likhosherstov2021on}
V.~Likhosherstov, K.~Choromanski, and A.~Weller.
\newblock On the expressive power of self-attention matrices.
\newblock {\em arXiv}, 2021.

\bibitem{lim2022sign}
D.~Lim, J.~Robinson, L.~Zhao, T.~E. Smidt, S.~Sra, H.~Maron, and S.~Jegelka.
\newblock Sign and basis invariant networks for spectral graph representation
  learning.
\newblock {\em arXiv}, 2022.

\bibitem{lin2021mesh}
K.~Lin, L.~Wang, and Z.~Liu.
\newblock Mesh graphormer.
\newblock In {\em ICCV}, 2021.

\bibitem{loshchilov2019decoupled}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em ICLR}, 2019.

\bibitem{ma2021luna}
X.~Ma, X.~Kong, S.~Wang, C.~Zhou, J.~May, H.~Ma, and L.~Zettlemoyer.
\newblock Luna: Linear unified nested attention.
\newblock {\em NeurIPS}, 2021.

\bibitem{maron2019provably}
H.~Maron, H.~Ben{-}Hamu, H.~Serviansky, and Y.~Lipman.
\newblock Provably powerful graph networks.
\newblock In {\em NeurIPS}, 2019.

\bibitem{maron2019invariant}
H.~Maron, H.~Ben{-}Hamu, N.~Shamir, and Y.~Lipman.
\newblock Invariant and equivariant graph networks.
\newblock In {\em ICLR}, 2019.

\bibitem{maron2019on}
H.~Maron, E.~Fetaya, N.~Segol, and Y.~Lipman.
\newblock On the universality of invariant networks.
\newblock In {\em ICML}, 2019.

\bibitem{maron2020onlearning}
H.~Maron, O.~Litany, G.~Chechik, and E.~Fetaya.
\newblock On learning sets of symmetric elements.
\newblock In {\em ICML}, 2020.

\bibitem{min2022transformer}
E.~Min, R.~Chen, Y.~Bian, T.~Xu, K.~Zhao, W.~Huang, P.~Zhao, J.~Huang,
  S.~Ananiadou, and Y.~Rong.
\newblock Transformer for graphs: An overview from architecture perspective.
\newblock {\em arXiv}, 2022.

\bibitem{nguyen2022universal}
D.~Q. Nguyen, T.~D. Nguyen, and D.~Phung.
\newblock Universal graph transformer self-attention networks.
\newblock In {\em WWW}, 2022.

\bibitem{oono2020graph}
K.~Oono and T.~Suzuki.
\newblock Graph neural networks exponentially lose expressive power for node
  classification.
\newblock In {\em ICLR}, 2020.

\bibitem{pan2022permutation}
H.~Pan and R.~Kondor.
\newblock Permutation equivariant layers for higher order interactions.
\newblock In {\em AISTATS}, 2022.

\bibitem{park2022grpe}
W.~Park, W.~Chang, D.~Lee, J.~Kim, and S.~won Hwang.
\newblock Grpe: Relative positional encoding for graph transformer.
\newblock {\em arXiv}, 2022.

\bibitem{peng2021random}
H.~Peng, N.~Pappas, D.~Yogatama, R.~Schwartz, N.~A. Smith, and L.~Kong.
\newblock Random feature attention.
\newblock In {\em ICLR}, 2021.

\bibitem{ravanbakhsh2017equivariance}
S.~Ravanbakhsh, J.~G. Schneider, and B.~P{\'{o}}czos.
\newblock Equivariance through parameter-sharing.
\newblock In {\em ICML}, 2017.

\bibitem{reed2022ageneralist}
S.~Reed, K.~{\.{Z}}o{\/{l}}na, E.~Parisotto, S.~G. Colmenarejo, A.~Novikov,
  G.~Barth-Maron, M.~Gim{\'{e}}nez, Y.~Sulsky, J.~Kay, J.~T. Springenberg,
  T.~Eccles, J.~Bruce, A.~Razavi, A.~Edwards, N.~Heess, Y.~Chen, R.~Hadsell,
  O.~Vinyals, M.~Bordbar, and N.~de~Freitas.
\newblock A generalist agent.
\newblock {\em arXiv}, 2022.

\bibitem{rong2020self}
Y.~Rong, Y.~Bian, T.~Xu, W.~Xie, Y.~Wei, W.~Huang, and J.~Huang.
\newblock Self-supervised graph transformer on large-scale molecular data.
\newblock In {\em NeurIPS}, 2020.

\bibitem{rozemberczki2021multi}
B.~Rozemberczki, C.~Allen, and R.~Sarkar.
\newblock Multi-scale attributed node embedding.
\newblock {\em J. Complex Networks}, 2021.

\bibitem{serviansky2020set}
H.~Serviansky, N.~Segol, J.~Shlomi, K.~Cranmer, E.~Gross, H.~Maron, and
  Y.~Lipman.
\newblock Set2graph: Learning graphs from sets.
\newblock In {\em NeurIPS}, 2020.

\bibitem{schur2018pitfalls}
O.~Shchur, M.~Mumme, A.~Bojchevski, and S.~G{\"{u}}nnemann.
\newblock Pitfalls of graph neural network evaluation.
\newblock {\em arXiv}, 2018.

\bibitem{shen2021efficient}
Z.~Shen, M.~Zhang, H.~Zhao, S.~Yi, and H.~Li.
\newblock Efficient attention: Attention with linear complexities.
\newblock In {\em WACV}, 2021.

\bibitem{shi2022benchmarking}
Y.~Shi, S.~Zheng, G.~Ke, Y.~Shen, J.~You, J.~He, S.~Luo, C.~Liu, D.~He, and
  T.~Liu.
\newblock Benchmarking graphormer on large-scale molecular modeling datasets.
\newblock {\em arXiv}, 2022.

\bibitem{steiner2021how}
A.~Steiner, A.~Kolesnikov, X.~Zhai, R.~Wightman, J.~Uszkoreit, and L.~Beyer.
\newblock How to train your vit? data, augmentation, and regularization in
  vision transformers.
\newblock {\em arXiv}, 2021.

\bibitem{tay2020efficient}
Y.~Tay, M.~Dehghani, D.~Bahri, and D.~Metzler.
\newblock Efficient transformers: {A} survey.
\newblock {\em arXiv}, 2020.

\bibitem{thomas2018tensor}
N.~Thomas, T.~E. Smidt, S.~Kearnes, L.~Yang, L.~Li, K.~Kohlhoff, and P.~Riley.
\newblock Tensor field networks: Rotation- and translation-equivariant neural
  networks for 3d point clouds.
\newblock {\em arXiv}, 2018.

\bibitem{tompson2015efficient}
J.~Tompson, R.~Goroshin, A.~Jain, Y.~LeCun, and C.~Bregler.
\newblock Efficient object localization using convolutional networks.
\newblock In {\em CVPR}, 2015.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In {\em NeurIPS}, 2017.

\bibitem{velikovic2018graph}
P.~Velickovic, G.~Cucurull, A.~Casanova, A.~Romero, P.~Li{\`{o}}, and
  Y.~Bengio.
\newblock Graph attention networks.
\newblock In {\em ICLR}, 2018.

\bibitem{wang2022deepnet}
H.~Wang, S.~Ma, L.~Dong, S.~Huang, D.~Zhang, and F.~Wei.
\newblock Deepnet: Scaling transformers to 1, 000 layers.
\newblock {\em arXiv}, 2022.

\bibitem{wang2020linformer}
S.~Wang, B.~Z. Li, M.~Khabsa, H.~Fang, and H.~Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock {\em arXiv}, 2020.

\bibitem{wang2019self}
X.~Wang, Z.~Tu, L.~Wang, and S.~Shi.
\newblock Self-attention with structural position representations.
\newblock In {\em EMNLP-IJCNLP}, 2019.

\bibitem{weiler20183d}
M.~Weiler, M.~Geiger, M.~Welling, W.~Boomsma, and T.~Cohen.
\newblock 3d steerable cnns: Learning rotationally equivariant features in
  volumetric data.
\newblock In {\em NeurIPS}, 2018.

\bibitem{wies2021which}
N.~Wies, Y.~Levine, D.~Jannai, and A.~Shashua.
\newblock Which transformer architecture fits my data? {A} vocabulary
  bottleneck in self-attention.
\newblock In {\em ICML}, 2021.

\bibitem{xiong2020on}
R.~Xiong, Y.~Yang, D.~He, K.~Zheng, S.~Zheng, C.~Xing, H.~Zhang, Y.~Lan,
  L.~Wang, and T.~Liu.
\newblock On layer normalization in the transformer architecture.
\newblock In {\em ICML}, 2020.

\bibitem{xiong2021nystromformer}
Y.~Xiong, Z.~Zeng, R.~Chakraborty, M.~Tan, G.~Fung, Y.~Li, and V.~Singh.
\newblock Nystr{\"{o}}mformer: {A} nystr{\"{o}}m-based algorithm for
  approximating self-attention.
\newblock In {\em AAAI}, 2021.

\bibitem{xu2019how}
K.~Xu, W.~Hu, J.~Leskovec, and S.~Jegelka.
\newblock How powerful are graph neural networks?
\newblock In {\em ICLR}, 2019.

\bibitem{ying2021do}
C.~Ying, T.~Cai, S.~Luo, S.~Zheng, G.~Ke, D.~He, Y.~Shen, and T.~Liu.
\newblock Do transformers really perform bad for graph representation?
\newblock In {\em NeurIPS}, 2021.

\bibitem{yu2016orthogonal}
F.~X. Yu, A.~T. Suresh, K.~M. Choromanski, D.~N. Holtmann{-}Rice, and S.~Kumar.
\newblock Orthogonal random features.
\newblock In {\em NeurIPS}, 2016.

\bibitem{yuan2021incorporating}
K.~Yuan, S.~Guo, Z.~Liu, A.~Zhou, F.~Yu, and W.~Wu.
\newblock Incorporating convolution designs into visual transformers.
\newblock In {\em ICCV}, 2021.

\bibitem{yun2020are}
C.~Yun, S.~Bhojanapalli, A.~S. Rawat, S.~J. Reddi, and S.~Kumar.
\newblock Are transformers universal approximators of sequence-to-sequence
  functions?
\newblock In {\em ICLR}, 2020.

\bibitem{zaheer2017deep}
M.~Zaheer, S.~Kottur, S.~Ravanbakhsh, B.~P{\'{o}}czos, R.~Salakhutdinov, and
  A.~J. Smola.
\newblock Deep sets.
\newblock In {\em NeurIPS}, 2017.

\bibitem{zopf20221_wl}
M.~Zopf.
\newblock 1-wl expressiveness is (almost) all you need.
\newblock {\em arXiv}, 2022.

\end{thebibliography}
