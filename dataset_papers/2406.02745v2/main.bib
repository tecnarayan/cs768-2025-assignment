@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@inproceedings{george2021fast,
    author = {Thomas George and C\'esar Laurent and Xavier Bouthillier and Nicolas Ballas and Pascal Vincent},
title = {Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis},
year = {2018},
booktitle = {Proceedings of the 32nd Conference on Neural Information Processing Systems},
}

@misc{perotti2018thermodynamics,
      title={Thermodynamics of the Minimum Description Length on Community Detection}, 
      author={Juan Ignacio Perotti and Claudio Juan Tessone and Aaron Clauset and Guido Caldarelli},
      year={2018},
      eprint={1806.07005},
      archivePrefix={arXiv},
      primaryClass={physics.soc-ph}
}

@InProceedings{clip2021radford,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/radford21a.html},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}

@INPROCEEDINGS{he2016resnet,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  keywords={Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation},
  doi={10.1109/CVPR.2016.90}}



@misc{srikanth2023empirical,
      title={An Empirical Study of Automated Mislabel Detection in Real World Vision Datasets}, 
      author={Maya Srikanth and Jeremy Irvin and Brian Wesley Hill and Felipe Godoy and Ishan Sabane and Andrew Y. Ng},
      year={2023},
      eprint={2312.02200},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{koh2017understanding,
  title={Understanding black-box predictions via influence functions},
  author={Koh, Pang Wei and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={1885--1894},
  year={2017},
  organization={PMLR}
}

@inproceedings{el2n2021paul,
 author = {Paul, Mansheej and Ganguli, Surya and Dziugaite, Gintare Karolina},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {20596--20607},
 publisher = {Curran Associates, Inc.},
 title = {Deep Learning on a Data Diet: Finding Important Examples Early in Training},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/ac56f8fe9eea3e4a365f29f0f1957c55-Paper.pdf},
 volume = {34},
 year = {2021}
}



@article{mdl1978rissanen,
title = {Modeling by shortest data description},
journal = {Automatica},
volume = {14},
number = {5},
pages = {465-471},
year = {1978},
issn = {0005-1098},
doi = {https://doi.org/10.1016/0005-1098(78)90005-5},
url = {https://www.sciencedirect.com/science/article/pii/0005109878900055},
author = {J. Rissanen},
keywords = {Modeling, parameter estimation, identification, statistics, stochastic systems},
abstract = {The number of digits it takes to write down an observed sequence x1, …, xN of a time series depends on the model with its parameters that one assumes to have generated the observed data. Accordingly, by finding the model which minimizes the description length one obtains estimates of both the integer-valued structure parameters and the real-valued system parameters.}
}

@inbook{akaike1973information,
  added-at = {2017-04-14T16:55:40.000+0200},
  address = {New York, NY},
  author = {Akaike, Hirotogu},
  biburl = {https://www.bibsonomy.org/bibtex/27a0be27d77d022668d19a554218ddb23/becker},
  booktitle = {Selected Papers of Hirotugu Akaike},
  interhash = {2782e1db136f4c3bf43d25c7a6fa90ca},
  intrahash = {7a0be27d77d022668d19a554218ddb23},
  keywords = {aic akaike comparison criterion diss information inthesis model selection},
  pages = {199--213},
  publisher = {Springer New York},
  timestamp = {2017-12-20T07:53:07.000+0100},
  title = {Information Theory and an Extension of the Maximum Likelihood Principle},
  year = 1973
}

@misc{bibas2020deep,
      title={Deep pNML: Predictive Normalized Maximum Likelihood for Deep Neural Networks}, 
      author={Koby Bibas and Yaniv Fogel and Meir Feder},
      year={2020},
      eprint={1904.12286},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{bd831960-ac2b-396a-8c8f-de3944255f11,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2285666},
 abstract = {This paper treats essentially the first derivative of an estimator viewed as functional and the ways in which it can be used to study local robustness properties. A theory of robust estimation "near" strict parametric models is briefly sketched and applied to some classical situations. Relations between von Mises functionals, the jackknife and U-statistics are indicated. A number of classical and new estimators are discussed, including trimmed and Winsorized means, Huber-estimators, and more generally maximum likelihood and M-estimators. Finally, a table with some numerical robustness properties is given.},
 author = {Frank R. Hampel},
 journal = {Journal of the American Statistical Association},
 number = {346},
 pages = {383--393},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {The Influence Curve and Its Role in Robust Estimation},
 urldate = {2024-02-01},
 volume = {69},
 year = {1974}
}



@article{10.1214/aos/1176344136,
author = {Gideon Schwarz},
title = {{Estimating the Dimension of a Model}},
volume = {6},
journal = {The Annals of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {461 -- 464},
keywords = {Akaike information criterion, asymptotics, dimension},
year = {1978},
doi = {10.1214/aos/1176344136},
URL = {https://doi.org/10.1214/aos/1176344136}
}


@inproceedings{10.1145/168304.168306, author = {Hinton, Geoffrey E. and van Camp, Drew}, title = {Keeping the neural networks simple by minimizing the description length of the weights}, year = {1993}, isbn = {0897916115}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/168304.168306}, doi = {10.1145/168304.168306}, booktitle = {Proceedings of the Sixth Annual Conference on Computational Learning Theory}, pages = {5–13}, numpages = {9}, location = {Santa Cruz, California, USA}, series = {COLT '93} }


@inproceedings{NIPS1993_9e3cfc48,
 author = {Hinton, Geoffrey E and Zemel, Richard},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Autoencoders, Minimum Description Length and Helmholtz Free Energy},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/file/9e3cfc48eccf81a0d57663e129aef3cb-Paper.pdf},
 volume = {6},
 year = {1993}
}


@misc{xu2020knowledge,
      title={Knowledge Distillation Meets Self-Supervision}, 
      author={Guodong Xu and Ziwei Liu and Xiaoxiao Li and Chen Change Loy},
      year={2020},
      eprint={2006.07114},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{hinton2015distilling,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{ovadia2019trust,
 author = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D. and Nowozin, Sebastian and Dillon, Joshua and Lakshminarayanan, Balaji and Snoek, Jasper},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Can you trust your model\textquotesingle s uncertainty?  Evaluating predictive uncertainty under dataset shift},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/8558cb408c1d76621371888657d2eb1d-Paper.pdf},
 volume = {32},
 year = {2019}
}


@misc{zhang2017understanding,
      title={Understanding deep learning requires rethinking generalization}, 
      author={Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
      year={2017},
      eprint={1611.03530},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{grathwol2020jem,
title={Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One},
author = {Will Grathwohl and Kuan-Chieh Wang and J{\"o}rn-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky},
booktitle={Proceedings of ICLR},
year={2020},
}

@misc{kim2021comparing,
      title={Comparing Kullback-Leibler Divergence and Mean Squared Error Loss in Knowledge Distillation}, 
      author={Taehyeon Kim and Jaehoon Oh and NakYil Kim and Sangwook Cho and Se-Young Yun},
      year={2021},
      eprint={2105.08919},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@TECHREPORT{Krizhevsky2009learning,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {},
    year = {2009}
}

@misc{izmailov2019averaging,
      title={Averaging Weights Leads to Wider Optima and Better Generalization}, 
      author={Pavel Izmailov and Dmitrii Podoprikhin and Timur Garipov and Dmitry Vetrov and Andrew Gordon Wilson},
      year={2019},
      eprint={1803.05407},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{park2023trak,
  title = {TRAK: Attributing Model Behavior at Scale},
  author = {Sung Min Park and Kristian Georgiev and Andrew Ilyas and Guillaume Leclerc and Aleksander Madry},
  booktitle = {International Conference on Machine Learning (ICML)},
  year = {2023}
}

@inproceedings{NIPS2016_8d8818c8,
 author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Exploration via Bootstrapped DQN},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/8d8818c8e140c64c743113f563cf750f-Paper.pdf},
 volume = {29},
 year = {2016}
}

@inproceedings{PlattProbabilisticOutputs1999,
  added-at = {2009-07-08T16:22:45.000+0200},
  author = {Platt, J.},
  biburl = {https://www.bibsonomy.org/bibtex/2b13a556c2a6c1a3a2a30fe889ea9b738/zeno},
  booktitle = {Advances in Large Margin Classifiers},
  interhash = {60601962d5858c7ee9a68e7347fe59b1},
  intrahash = {b13a556c2a6c1a3a2a30fe889ea9b738},
  keywords = {2000 svm},
  timestamp = {2009-12-18T12:44:08.000+0100},
  title = {Probabilistic outputs for support vector machines and comparison to regularized
    likelihood methods},
  year = 2000
}



@inproceedings{10.5555/3104482.3104568, author = {Welling, Max and Teh, Yee Whye}, title = {Bayesian learning via stochastic gradient langevin dynamics}, year = {2011}, isbn = {9781450306195}, publisher = {Omnipress}, address = {Madison, WI, USA}, abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a "sampling threshold" and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.}, booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning}, pages = {681–688}, numpages = {8}, location = {Bellevue, Washington, USA}, series = {ICML'11} }


# v1.5 report
@article{zhang2023openood,
  title={OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection},
  author={Zhang, Jingyang and Yang, Jingkang and Wang, Pengyun and Wang, Haoqi and Lin, Yueqian and Zhang, Haoran and Sun, Yiyou and Du, Xuefeng and Zhou, Kaiyang and Zhang, Wayne and Li, Yixuan and Liu, Ziwei and Chen, Yiran and Li, Hai},
  journal={arXiv preprint arXiv:2306.09301},
  year={2023}
}

@article{kolmogorov65,
  added-at = {2011-05-09T23:10:52.000+0200},
  author = {Kolmogorov, A. N.},
  biburl = {https://www.bibsonomy.org/bibtex/2020f5b5306bf85d0c65cee5e8284244f/josephausterwei},
  interhash = {2943e8971f1bdd4f74f8bee3faf02dd0},
  intrahash = {020f5b5306bf85d0c65cee5e8284244f},
  journal = {Problems of Information Transmission},
  keywords = {imported},
  pages = {1-7},
  timestamp = {2011-05-09T23:11:39.000+0200},
  title = {Three approaches to the quantitative definition of information},
  volume = 1,
  year = 1965
}

@article{SOLOMONOFF19641,
title = {A formal theory of inductive inference. Part I},
journal = {Information and Control},
volume = {7},
number = {1},
pages = {1-22},
year = {1964},
issn = {0019-9958},
doi = {https://doi.org/10.1016/S0019-9958(64)90223-2},
url = {https://www.sciencedirect.com/science/article/pii/S0019995864902232},
author = {R.J. Solomonoff},
abstract = {1. Summary
In Part I, four ostensibly different theoretical models of induction are presented, in which the problem dealt with is the extrapolation of a very long sequence of symbols—presumably containing all of the information to be used in the induction. Almost all, if not all problems in induction can be put in this form. Some strong heuristic arguments have been obtained for the equivalence of the last three models. One of these models is equivalent to a Bayes formulation, in which a priori probabilities are assigned to sequences of symbols on the basis of the lengths of inputs to a universal Turing machine that are required to produce the sequence of interest as output. Though it seems likely, it is not certain whether the first of the four models is equivalent to the other three. Few rigorous results are presented. Informal investigations are made of the properties of these models. There are discussions of their consistency and meaningfulness, of their degree of independence of the exact nature of the Turing machine used, and of the accuracy of their predictions in comparison to those of other induction methods. In Part II these models are applied to the solution of three problems—prediction of the Bernoulli sequence, extrapolation of a certain kind of Markov chain, and the use of phrase structure grammars for induction. Though some approximations are used, the first of these problems is treated most rigorously. The result is Laplace's rule of succession. The solution to the second problem uses less certain approximations, but the properties of the solution that are discussed, are fairly independent of these approximations. The third application, using phrase structure grammars, is least exact of the three. First a formal solution is presented. Though it appears to have certain deficiencies, it is hoped that presentation of this admittedly inadequate model will suggest acceptable improvements in it. This formal solution is then applied in an approximate way to the determination of the “optimum” phrase structure grammar for a given set of strings. The results that are obtained are plausible, but subject to the uncertainties of the approximation used.}
}

@book{li2019kolmogorov, author = {Li, Ming and Vitanyi, Paul}, title = {An Introduction to Kolmogorov Complexity and Its Applications}, year = {2019}, isbn = {3030112977}, publisher = {Springer Publishing Company, Incorporated}, edition = {4th}, abstract = {This must-read textbook presents an essential introduction to Kolmogorov complexity (KC), a central theory and powerful tool in information science that deals with the quantity of information in individual objects. The text covers both the fundamental concepts and the most important practical applications, supported by a wealth of didactic features. This thoroughly revised and enhanced fourth edition includes new and updated material on, amongst other topics, the Miller-Yu theorem, the Gacs-Kucera theorem, the Day-Gacs theorem, increasing randomness, short lists computable from an input string containing the incomputable Kolmogorov complexity of the input, the Lovasz local lemma, sorting, the algorithmic full Slepian-Wolf theorem for individual strings, multiset normalized information distance and normalized web distance, and conditional universal distribution. Topics and features: describes the mathematical theory of KC, including the theories of algorithmic complexity and algorithmic probability; presents a general theory of inductive reasoning and its applications, and reviews the utility of the incompressibility method; covers the practical application of KC in great detail, including the normalized information distance (the similarity metric) and information diameter of multisets in phylogeny, language trees, music, heterogeneous files, and clustering; discusses the many applications of resource-bounded KC, and examines different physical theories from a KC point of view; includes numerous examples that elaborate the theory, and a range of exercises of varying difficulty (with solutions); offers explanatory asides on technical issues, and extensive historical sections; suggests structures for several one-semester courses in the preface. As the definitive textbook on Kolmogorov complexity, this comprehensive and self-contained work is an invaluable resource for advanced undergraduate students, graduate students, and researchers in all fields of science.} }


@ARTICLE{lecun1998lenet,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  keywords={Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
  doi={10.1109/5.726791}}


@article{deng2012mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

# v1.0 report
@article{yang2022openood,
    author = {Yang, Jingkang and Wang, Pengyun and Zou, Dejian and Zhou, Zitang and Ding, Kunyuan and Peng, Wenxuan and Wang, Haoqi and Chen, Guangyao and Li, Bo and Sun, Yiyou and Du, Xuefeng and Zhou, Kaiyang and Zhang, Wayne and Hendrycks, Dan and Li, Yixuan and Liu, Ziwei},
    title = {OpenOOD: Benchmarking Generalized Out-of-Distribution Detection},
    year = {2022}
}

@article{yang2022fsood,
    title = {Full-Spectrum Out-of-Distribution Detection},
    author = {Yang, Jingkang and Zhou, Kaiyang and Liu, Ziwei},
    journal={arXiv preprint arXiv:2204.05306},
    year = {2022}
}

@article{yang2021oodsurvey,
    title={Generalized Out-of-Distribution Detection: A Survey},
    author={Yang, Jingkang and Zhou, Kaiyang and Li, Yixuan and Liu, Ziwei},
    journal={arXiv preprint arXiv:2110.11334},
    year={2021}
}

@inproceedings{bitterwolf2023ninco,
    title={In or Out? Fixing ImageNet Out-of-Distribution Detection Evaluation},
    author={Julian Bitterwolf and Maximilian Mueller and Matthias Hein},
    booktitle={ICML},
    year={2023},
    url={https://proceedings.mlr.press/v202/bitterwolf23a.html}
}

@article{rissanen1986complexity,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/3035559},
 abstract = {As a modification of the notion of algorithmic complexity, the stochastic complexity of a string of data, relative to a class of probabilistic models, is defined to be the fewest number of binary digits with which the data can be encoded by taking advantage of the selected models. The computation of the stochastic complexity produces a model, which may be taken to incorporate all the statistical information in the data that can be extracted with the chosen model class. This model, for example, allows for optimal prediction, and its parameters are optimized both in their values and their number. A fundamental theorem is proved which gives a lower bound for the code length and, therefore, for prediction errors as well. Finally, the notions of "prior information" and the "useful information" in the data are defined in a new way, and a related construct gives a universal test statistic for hypothesis testing.},
 author = {Jorma Rissanen},
 journal = {The Annals of Statistics},
 number = {3},
 pages = {1080--1100},
 publisher = {Institute of Mathematical Statistics},
 title = {Stochastic Complexity and Modeling},
 urldate = {2024-01-31},
 volume = {14},
 year = {1986}
}

@article{bartlett2003rademacher, author = {Bartlett, Peter L. and Mendelson, Shahar}, title = {Rademacher and Gaussian Complexities: Risk Bounds and Structural Results}, year = {2003}, issue_date = {3/1/2003}, publisher = {JMLR.org}, volume = {3}, number = {null}, issn = {1532-4435}, abstract = {We investigate the use of certain data-dependent estimates of the complexity of a function class, called Rademacher and Gaussian complexities. In a decision theoretic setting, we prove general risk bounds in terms of these complexities. We consider function classes that can be expressed as combinations of functions from basis classes and show how the Rademacher and Gaussian complexities of such a function class can be bounded in terms of the complexity of the basis classes. We give examples of the application of these techniques in finding data-dependent risk bounds for decision trees, neural networks and support vector machines.}, journal = {J. Mach. Learn. Res.}, month = {mar}, pages = {463–482}, numpages = {20}, keywords = {error bounds, Rademacher averages, maximum discrepancy, data-dependent complexity} }

@article{vapnik1971uniform,
author = {Vapnik, V. N. and Chervonenkis, A. Ya.},
title = {On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities},
journal = {Theory of Probability \& Its Applications},
volume = {16},
number = {2},
pages = {264-280},
year = {1971},
doi = {10.1137/1116025},

URL = { 
        https://doi.org/10.1137/1116025
    
},
eprint = { 
        https://doi.org/10.1137/1116025
    
}

}

@inproceedings{huang2021importance,
  title={On the Importance of Gradients for Detecting Distributional Shifts in the Wild},
  author={Huang, Rui and Geng, Andrew and Li, Yixuan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}



@InProceedings{pmlr-v70-guo17a,
  title = 	 {On Calibration of Modern Neural Networks},
  author =       {Chuan Guo and Geoff Pleiss and Yu Sun and Kilian Q. Weinberger},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1321--1330},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/guo17a/guo17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/guo17a.html},
  abstract = 	 {Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a single-parameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.}
}



@InProceedings{gal2016dropout,
  title = 	 {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  author = 	 {Gal, Yarin and Ghahramani, Zoubin},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1050--1059},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/gal16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/gal16.html},
  abstract = 	 {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.}
}


@inproceedings{maddox2019swag,
 author = {Maddox, Wesley J and Izmailov, Pavel and Garipov, Timur and Vetrov, Dmitry P and Wilson, Andrew Gordon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Simple Baseline for Bayesian Uncertainty in Deep Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/118921efba23fc329e6560b27861f0c2-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{balaji2017ensembles,
 author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf},
 volume = {30},
 year = {2017}
}


@article{hendrycks2019robustness,
  title={Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
  author={Dan Hendrycks and Thomas Dietterich},
  journal={Proceedings of the International Conference on Learning Representations},
  year={2019}
}

@Inbook{Wuthrich2023,
author="W{\"u}thrich, Mario V.
and Merz, Michael",
title="Exponential Dispersion Family",
bookTitle="Statistical Foundations of Actuarial Learning and its Applications",
year="2023",
publisher="Springer International Publishing",
address="Cham",
pages="13--47",
abstract="This chapter introduces and discusses the exponential family (EF) and the exponential dispersion family (EDF). The EF and the EDF are by far the most important classes of distribution functions for regression modeling. They include, among others, the Gaussian, the binomial, the Poisson, the gamma, the inverse Gaussian distributions, as well as Tweedie's models. We introduce these families of distribution functions, discuss their properties and provide several examples. Moreover, we introduce the Kullback--Leibler (KL) divergence and the Bregman divergence, which are important tools in model evaluation and model selection.",
isbn="978-3-031-12409-9",
doi="10.1007/978-3-031-12409-9_2",
url="https://doi.org/10.1007/978-3-031-12409-9_2"
}


@InProceedings{pmlr-v206-fisher23a,
  title = 	 {Influence Diagnostics under Self-concordance},
  author =       {Fisher, Jillian and Liu, Lang and Pillutla, Krishna and Choi, Yejin and Harchaoui, Zaid},
  booktitle = 	 {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {10028--10076},
  year = 	 {2023},
  editor = 	 {Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem},
  volume = 	 {206},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--27 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v206/fisher23a/fisher23a.pdf},
  url = 	 {https://proceedings.mlr.press/v206/fisher23a.html},
  abstract = 	 {Influence diagnostics such as influence functions and approximate maximum influence perturbations are popular in machine learning and in AI domain applications. Influence diagnostics are powerful statistical tools to identify influential datapoints or subsets of datapoints. We establish finite-sample statistical bounds, as well as computational complexity bounds, for influence functions and approximate maximum influence perturbations using efficient inverse-Hessian-vector product implementations. We illustrate our results with generalized linear models and large attention based models on synthetic and real data.}
}

@inproceedings{foster2001competitive,
    author = {Robert A Stine and Dean P Foster},
    title = {The competitive complexity ratio},
    booktitle = {Proceedings of the 2001 Conference on Information Sciences and Systems},
    year = {2001}
}


@article{JMLR:v24:21-1133,
  author  = {Raaz Dwivedi and Chandan Singh and Bin Yu and Martin Wainwright},
  title   = {Revisiting minimum description length complexity in overparameterized models},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {268},
  pages   = {1--59},
  url     = {http://jmlr.org/papers/v24/21-1133.html}
}

@inproceedings{blitzer-etal-2007-biographies,
    title = "Biographies, {B}ollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification",
    author = "Blitzer, John  and
      Dredze, Mark  and
      Pereira, Fernando",
    booktitle = "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P07-1056",
    pages = "440--447",
}

@Article{Esteva2017,
author={Esteva, Andre
and Kuprel, Brett
and Novoa, Roberto A.
and Ko, Justin
and Swetter, Susan M.
and Blau, Helen M.
and Thrun, Sebastian},
title={Dermatologist-level classification of skin cancer with deep neural networks},
journal={Nature},
year={2017},
month={Feb},
day={01},
volume={542},
number={7639},
pages={115-118},
abstract={An artificial intelligence trained to classify images of skin lesions as benign lesions or malignant skin cancers achieves the accuracy of board-certified dermatologists.},
issn={1476-4687},
doi={10.1038/nature21056},
url={https://doi.org/10.1038/nature21056}
}


@INPROCEEDINGS{8437543,
  author={Fogel, Yaniv and Feder, Meir},
  booktitle={2018 IEEE International Symposium on Information Theory (ISIT)}, 
  title={Universal Batch Learning with Log-Loss}, 
  year={2018},
  volume={},
  number={},
  pages={21-25},
  keywords={Mutual information;Training;Statistical learning;Loss measurement;Upper bound;Electrical engineering},
  doi={10.1109/ISIT.2018.8437543}}


@inproceedings{NEURIPS2022_552260cf,
 author = {Tanno, Ryutaro and F. Pradier, Melanie and Nori, Aditya and Li, Yingzhen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {13132--13145},
 publisher = {Curran Associates, Inc.},
 title = {Repairing Neural Networks by Leaving the Right Past Behind},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/552260cfb5e292e511eaa780806ac984-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@InProceedings{pmlr-v89-schulam19a,
  title = 	 {Can You Trust This Prediction? Auditing Pointwise Reliability After Learning},
  author =       {Schulam, Peter and Saria, Suchi},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1022--1031},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/schulam19a/schulam19a.pdf},
  url = 	 {https://proceedings.mlr.press/v89/schulam19a.html},
  abstract = 	 {To use machine learning in high stakes applications (e.g. medicine), we need tools for building confidence in the system and evaluating whether it is reliable. Methods to improve model reliability often require new learning algorithms (e.g. using Bayesian inference to obtain uncertainty estimates). An alternative is to audit a model after it is trained. In this paper, we describe resampling uncertainty estimation (RUE), an algorithm to audit the pointwise reliability of predictions. Intuitively, RUE estimates the amount that a prediction would change if the model had been fit on different training data. The algorithm uses the gradient and Hessian of the model’s loss function to create an ensemble of predictions. Experimentally, we show that RUE more effectively detects inaccurate predictions than existing tools for auditing reliability subsequent to training. We also show that RUE can create predictive distributions that are competitive with state-of-the-art methods like Monte Carlo dropout, probabilistic backpropagation, and deep ensembles, but does not depend on specific algorithms at train-time like these methods do.}
}


@misc{koh2021stronger,
      title={Stronger Data Poisoning Attacks Break Data Sanitization Defenses}, 
      author={Pang Wei Koh and Jacob Steinhardt and Percy Liang},
      year={2021},
      eprint={1811.00741},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@InProceedings{pmlr-v97-brunet19a,
  title = 	 {Understanding the Origins of Bias in Word Embeddings},
  author =       {Brunet, Marc-Etienne and Alkalay-Houlihan, Colleen and Anderson, Ashton and Zemel, Richard},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {803--811},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/brunet19a/brunet19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/brunet19a.html},
  abstract = 	 {Popular word embedding algorithms exhibit stereotypical biases, such as gender bias. The widespread use of these algorithms in machine learning systems can amplify stereotypes in important contexts. Although some methods have been developed to mitigate this problem, how word embedding biases arise during training is poorly understood. In this work we develop a technique to address this question. Given a word embedding, our method reveals how perturbing the training corpus would affect the resulting embedding bias. By tracing the origins of word embedding bias back to the original training documents, one can identify subsets of documents whose removal would most reduce bias. We demonstrate our methodology on Wikipedia and New York Times corpora, and find it to be very accurate.}
}


@misc{feldman2021does,
      title={Does Learning Require Memorization? A Short Tale about a Long Tail}, 
      author={Vitaly Feldman},
      year={2021},
      eprint={1906.05271},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{feldman2020neural,
      title={What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation}, 
      author={Vitaly Feldman and Chiyuan Zhang},
      year={2020},
      eprint={2008.03703},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{basu2020influence,
  title={Influence functions in deep learning are fragile},
  author={Basu, Samyadeep and Pope, Philip and Feizi, Soheil},
  journal={arXiv preprint arXiv:2006.14651},
  year={2020}
}

@article{75272a7e-1c8b-3ed5-9350-a7ee81abee59,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2286747},
 abstract = {Characteristics of observations which cause them to be influential in a least squares analysis are investigated and related to residual variances, residual correlations, and the convex hull of the observed values of the independent variables. It is shown how deleting an observation can substantially alter an analysis by changing the partial F-tests, the studentized residuals, the residual variances, the convex hull of the independent variables, and the estimated parameter vector. Outliers are discussed briefly, and an example is presented.},
 author = {R. Dennis Cook},
 journal = {Journal of the American Statistical Association},
 number = {365},
 pages = {169--174},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Influential Observations in Linear Regression},
 urldate = {2024-02-01},
 volume = {74},
 year = {1979}
}



@article{naeini2015ece, title={Obtaining Well Calibrated Probabilities Using Bayesian Binning}, volume={29}, url={https://ojs.aaai.org/index.php/AAAI/article/view/9602}, DOI={10.1609/aaai.v29i1.9602}, abstractNote={ &lt;p&gt; Learning probabilistic predictive models that are well calibrated is critical for many prediction and decision-making tasks in artificial intelligence. In this paper we present a new non-parametric calibration method called Bayesian Binning into Quantiles (BBQ) which addresses key limitations of existing calibration methods. The method post processes the output of a binary classification algorithm; thus, it can be readily combined with many existing classification algorithms. The method is computationally tractable, and empirically accurate, as evidenced by the set of experiments reported here on both real and simulated datasets. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Mahdi Pakdaman Naeini and Gregory Cooper and Milos Hauskrecht}, year={2015}, month={Feb.} }

@misc{zhou2021amortized,
      title={Amortized Conditional Normalized Maximum Likelihood: Reliable Out of Distribution Uncertainty Estimation}, 
      author={Aurick Zhou and Sergey Levine},
      year={2021},
      eprint={2011.02696},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{jorgensen1987dispersion,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2345415},
 abstract = {We study general properties of the class of exponential dispersion models, which is the multivariate generalization of the error distribution of Nelder and Wedderburn's (1972) generalized linear models. Since any given moment generating function generates an exponential dispersion model, there exists a multitude of exponential dispersion models, and some new examples are introduced. General results on convolution and asymptotic normality of exponential dispersion models are presented. Asymptotic theory is discussed, including a new small-dispersion asymptotic framework, which extends the domain of application of large-sample theory. Procedures for constructing new exponential dispersion models for correlated data are introduced, including models for longitudinal data and variance components. The results of the paper unify and generalize standard results for distributions such as the Poisson, the binomial, the negative binomial, the normal, the gamma, and the inverse Gaussian distributions.},
 author = {Bent Jorgensen},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {2},
 pages = {127--162},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Exponential Dispersion Models},
 urldate = {2024-01-28},
 volume = {49},
 year = {1987}
}

@misc{ilyas2022datamodels,
      title={Datamodels: Predicting Predictions from Training Data}, 
      author={Andrew Ilyas and Sung Min Park and Logan Engstrom and Guillaume Leclerc and Aleksander Madry},
      year={2022},
      eprint={2202.00622},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@inproceedings{bae2022influence,
 author = {Bae, Juhan and Ng, Nathan and Lo, Alston and Ghassemi, Marzyeh and Grosse, Roger B},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {17953--17967},
 publisher = {Curran Associates, Inc.},
 title = {If Influence Functions are the Answer, Then What is the Question?},
 volume = {35},
 year = {2022}
}

@inproceedings{wei2021learning,
title={Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations},
author={Jiaheng Wei and Zhaowei Zhu and Hao Cheng and Tongliang Liu and Gang Niu and Yang Liu},
year = {2022},
booktitle = {Proceedings of the International Conference on Learning Representations},
}

@ARTICLE{barron1998minimum,
  author={Barron, A. and Rissanen, J. and Bin Yu},
  journal={IEEE Transactions on Information Theory}, 
  title={The minimum description length principle in coding and modeling}, 
  year={1998},
  volume={44},
  number={6},
  pages={2743-2760},
  keywords={Probability distribution;Stochastic processes;Data compression;Context modeling;Statistical distributions;Maximum likelihood decoding;Source coding;Predictive coding;Linear regression;Maximum likelihood estimation},
  doi={10.1109/18.720554}}


@ARTICLE{rissanen1996fisher,
  author={Rissanen, J.J.},
  journal={IEEE Transactions on Information Theory}, 
  title={Fisher information and stochastic complexity}, 
  year={1996},
  volume={42},
  number={1},
  pages={40-47},
  keywords={Stochastic processes;Channel capacity;Entropy;Statistics;Senior members;Mutual information;Markov processes;Bayesian methods;Information theory},
  doi={10.1109/18.481776}}


@misc{grunwald2004tutorial,
      title={A tutorial introduction to the minimum description length principle}, 
      author={Peter Grunwald},
      year={2004},
      eprint={math/0406077},
      archivePrefix={arXiv},
      primaryClass={math.ST}
}

@inproceedings{NIPS2011_7eb3c8be,
 author = {Graves, Alex},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Practical Variational Inference for Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf},
 volume = {24},
 year = {2011}
}

@inproceedings{ritter2018laplace,
author = {Hippolyt Ritter and Aleksandar Botev and David Barber},
title = {A Scalable Laplace Approximation for Neural Networks},
year = {2018},
booktitle = {Proceedings of ICLR}
}


@phdthesis{mackay1992bayesian, author = {Mackay, David John Cameron}, title = {Bayesian methods for adaptive models}, year = {1992}, publisher = {California Institute of Technology}, address = {USA}, abstract = {The Bayesian framework for model comparison and regularisation is demonstrated by studying interpolation and classification problems modelled with both linear and non-linear models. This framework quantitatively embodies 'Occam's razor'. Over-complex and under-regularised models are automatically inferred to be less probable, even though their flexibility allows them to fit the data better. When applied to 'neural networks', the Bayesian framework makes possible (1) objective comparison of solutions using alternative network architectures; (2) objective stopping rules for network pruning or growing procedures; (3) objective choice of type of weight decay terms (or regularisers); (4) on-line techniques for optimising weight decay (or regularisation constant) magnitude; (5) a measure of the effective number of well-determined parameters in a model; (6) quantified estimates of the error bars on network parameters and on network output. In the case of classification models, it is shown that the careful incorporation of error bar information into a classifier's predictions yields improved performance.Comparisons of the inferences of the Bayesian framework with more traditional cross-validation methods help detect poor underlying assumptions in learning models.The relationship of the Bayesian learning framework to 'active learning' is examined. Objective functions are discussed which measure the expected informativeness of candidate data measurements, in the context of both interpolation and classification problems.The concepts and methods described in this thesis are quite general and will be applicable to other data modelling problems whether they involve regression, classification or density estimation.}, note = {UMI Order No. GAX92-32200} }


@article{kass1995bayes,
  added-at = {2023-12-13T09:19:47.000+0100},
  author = {Kass, Robert E. and Raftery, Adrian E.},
  biburl = {https://www.bibsonomy.org/bibtex/20a1dfd0f204e42300e16a9ebd7a94969/admin},
  doi = {10.1080/01621459.1995.10476572},
  eprint = {http://www.tandfonline.com/doi/pdf/10.1080/01621459.1995.10476572},
  interhash = {cd122c30c9c556c1186f628b8478c515},
  intrahash = {0a1dfd0f204e42300e16a9ebd7a94969},
  journal = {Journal of the American Statistical Association},
  keywords = {},
  number = 430,
  pages = {773-795},
  publisher = {Taylor & Francis},
  timestamp = {2023-12-13T09:19:47.000+0100},
  title = {Bayes Factors},
  url = {/brokenurl#         http://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476572    },
  volume = 90,
  year = 1995
}

@misc{bojarski2016end,
      title={End to End Learning for Self-Driving Cars}, 
      author={Mariusz Bojarski and Davide Del Testa and Daniel Dworakowski and Bernhard Firner and Beat Flepp and Prasoon Goyal and Lawrence D. Jackel and Mathew Monfort and Urs Muller and Jiakai Zhang and Xin Zhang and Jake Zhao and Karol Zieba},
      year={2016},
      eprint={1604.07316},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{ren2021simple,
      title={A Simple Fix to Mahalanobis Distance for Improving Near-OOD Detection}, 
      author={Jie Ren and Stanislav Fort and Jeremiah Liu and Abhijit Guha Roy and Shreyas Padhy and Balaji Lakshminarayanan},
      year={2021},
      eprint={2106.09022},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{angwin2016bias,
    author= {Julia Angwin and Jeff Larson and Surya Mattu and Lauren Kirchner},
    year  = {2016},
    publisher = {ProPublica},
    title = {Machine Bias},
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI and : and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mo Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@Article{Ghassemi2022,
author={Ghassemi, Marzyeh
and Mohamed, Shakir},
title={Machine learning and health need better values},
journal={npj Digital Medicine},
year={2022},
month={Apr},
day={22},
volume={5},
number={1},
pages={51},
abstract={Health care is a human process that generates data from human lives, as well as the care they receive. Machine learning has worked in health to bring new technology into this sociotechnical environment, using data to support a vision of healthier living for everyone. Interdisciplinary fields of research like machine learning for health bring different values and judgements together, requiring that those value choices be deliberate and measured. More than just abstract ideas, our values are the basis upon which we choose our research topics, set up research collaborations, execute our research methodologies, make assessments of scientific and technical correctness, proceed to product development, and finally operationalize deployments and describe policy. For machine learning to achieve its aims of supporting healthier living while minimizing harm, we believe that a deeper introspection of our field's values and contentions is overdue. In this perspective, we highlight notable areas in need of attention within the field. We believe deliberate and informed introspection will lead our community to renewed opportunities for understanding disease, new partnerships with clinicians and patients, and allow us to better support people and communities to live healthier, dignified lives.},
issn={2398-6352},
doi={10.1038/s41746-022-00595-9},
url={https://doi.org/10.1038/s41746-022-00595-9}
}



@book{griewank2008evaluating,
  title={Evaluating derivatives: principles and techniques of algorithmic differentiation},
  author={Griewank, Andreas and Walther, Andrea},
  year={2008},
  publisher={SIAM}
}

@book{bernardo1994bayesian,
  title={Bayesian Theory},
  author={Jos\'e M. Bernardo and Adrian F. M. Smith},
  year={1994},
  publisher={Wiley}
}

@book{krantz2002implicit,
  title={The implicit function theorem: history, theory, and applications},
  author={Krantz, Steven George and Parks, Harold R},
  year={2002},
  publisher={Springer Science \& Business Media}
}

@misc{farnia2017minimax,
      title={A Minimax Approach to Supervised Learning}, 
      author={Farzan Farnia and David Tse},
      year={2017},
      eprint={1606.02206},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@INPROCEEDINGS{fogel2019universal,
  author={Fogel, Yaniv and Feder, Meir},
  booktitle={2019 IEEE International Symposium on Information Theory (ISIT)}, 
  title={Universal Learning of Individual Data}, 
  year={2019},
  volume={},
  number={},
  pages={2289-2293},
  keywords={Data handling;Learning (artificial intelligence);Maximum likelihood estimation;Supervised learning;Minimax techniques},
  doi={10.1109/ISIT.2019.8849222}}


@article{sequentially2008roos,
author = {Roos, Teemu and Rissanen, Jorma},
year = {2008},
month = {01},
pages = {},
title = {On sequentially normalized maximum likelihood models}
}

@INPROCEEDINGS{roos2008bayesian,
  author={Roos, Teemu and Silander, Tomi and Kontkanen, Petri and Myllymaki, Petri},
  booktitle={2008 Information Theory and Applications Workshop}, 
  title={Bayesian network structure learning using factorized NML universal models}, 
  year={2008},
  volume={},
  number={},
  pages={272-276},
  keywords={Computational modeling;Bayesian methods;Data models;Complexity theory;Stochastic processes;Predictive models;Approximation methods},
  doi={10.1109/ITA.2008.4601061}}


@inproceedings{clusterability2021zhu,
title = {Clusterability as an Alternative to Anchor Points When Learning with Noisy Labels},
author = {Zhaowei Zhu and Yiwen Song and Yang Liu},
year = {2021},
booktitle = {Proceedings of ICML}
}

@article{universal1987shtarkov,
    author = {Yurii Mikhailovich Shtar'kov},
    title = {Universal Sequential Coding of Single Messages},
    journal = {Problemy Peredachi Informatsii},
    year = {1987},
    volume = {23},
    issue = {3},
    pages = {175-186}
}

@book{elements1991cover,
title = {Elements of Information Theory},
author = {Thomas Cover and Joy Thomas},
year      = {1991},
Publisher = {Wiley Interscience},
}

@inproceedings{tracin2020pruthi,
 author = {Pruthi, Garima and Liu, Frederick and Kale, Satyen and Sundararajan, Mukund},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {19920--19930},
 publisher = {Curran Associates, Inc.},
 title = {Estimating Training Data Influence by Tracing Gradient Descent},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/e6385d39ec9394f2f3a354d9d2b88eec-Paper.pdf},
 volume = {33},
 year = {2020}
}


@misc{grosse2023studying,
      title={Studying Large Language Model Generalization with Influence Functions}, 
      author={Roger Grosse and Juhan Bae and Cem Anil and Nelson Elhage and Alex Tamkin and Amirhossein Tajdini and Benoit Steiner and Dustin Li and Esin Durmus and Ethan Perez and Evan Hubinger and Kamilė Lukošiūtė and Karina Nguyen and Nicholas Joseph and Sam McCandlish and Jared Kaplan and Samuel R. Bowman},
      year={2023},
      eprint={2308.03296},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}
