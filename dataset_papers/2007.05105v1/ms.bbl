\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amodei et~al.(2016)Amodei, Anubhai, Battenberg, Case, Casper,
  Catanzaro, Chen, Chrzanowski, Coates, Diamos, Elsen, Engel, Fan, Fougner,
  Han, Hannun, Jun, LeGresley, Lin, Narang, Ng, Ozair, Prenger, Raiman,
  Satheesh, Seetapun, Sengupta, Wang, Wang, Wang, Xiao, Yogatama, Zhan, and
  Zhu]{Amodei:2016}
Amodei, D., Anubhai, R., Battenberg, E., Case, C., Casper, J., Catanzaro, B.,
  Chen, J., Chrzanowski, M., Coates, A., Diamos, G., Elsen, E., Engel, J.~H.,
  Fan, L., Fougner, C., Han, T., Hannun, A.~Y., Jun, B., LeGresley, P., Lin,
  L., Narang, S., Ng, A.~Y., Ozair, S., Prenger, R., Raiman, J., Satheesh, S.,
  Seetapun, D., Sengupta, S., Wang, Y., Wang, Z., Wang, C., Xiao, B., Yogatama,
  D., Zhan, J., and Zhu, Z.
\newblock Deep speech 2: End-to-end speech recognition in {English} and
  {Mandarin}.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning}, 2016.

\bibitem[Balles \& Hennig(2018)Balles and Hennig]{Balles:2018}
Balles, L. and Hennig, P.
\newblock Dissecting {Adam}: The sign, magnitude and variance of stochastic
  gradients.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, 2018.

\bibitem[Charles \& Papailiopoulos(2018)Charles and
  Papailiopoulos]{Charles:2018}
Charles, Z. and Papailiopoulos, D.
\newblock Stability and generalization of learning algorithms that converge to
  global optima.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, 2018.

\bibitem[De et~al.(2017)De, Yadav, Jacobs, and Goldstein]{De:2017}
De, S., Yadav, A., Jacobs, D., and Goldstein, T.
\newblock Automated inference with adaptive batches.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics}, 2017.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and Lacoste-Julien]{Defazio:2014}
Defazio, A., Bach, F., and Lacoste-Julien, S.
\newblock {SAGA}: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems 27}, 2014.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{Deng:2009}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  2009.

\bibitem[Devarakonda et~al.(2017)Devarakonda, Naumov, and
  Garland]{Devarakonda:2017}
Devarakonda, A., Naumov, M., and Garland, M.
\newblock {AdaBatch}: Adaptive batch sizes for training deep neural networks.
\newblock arXiv:1712.02029, 2017.

\bibitem[Everingham et~al.(2010)Everingham, Gool, Williams, Winn, and
  Zisserman]{Everingham:2010}
Everingham, M., Gool, L.~V., Williams, C. K.~I., Winn, J., and Zisserman, A.
\newblock The pascal visual object classes ({VOC}) challenge.
\newblock \emph{International Journal of Computer Vision}, 88\penalty0
  (2):\penalty0 303--338, 2010.

\bibitem[Ghorbani \& Krishnan(2019)Ghorbani and Krishnan]{Ghorbani:2019}
Ghorbani, B. and Krishnan, S.
\newblock An investigation into neural net optimization via hessian eigenvalue
  density.
\newblock arXiv:1901.10159, 2019.

\bibitem[Golmant et~al.(2018)Golmant, Vemuri, Yao, Feinberg, Gholami, Rothauge,
  Mahoney, and Gonzalez]{Golmalt:2018}
Golmant, N., Vemuri, N., Yao, Z., Feinberg, V., Gholami, A., Rothauge, K.,
  Mahoney, M.~W., and Gonzalez, J.
\newblock On the computational inefficiency of large batch sizes for stochastic
  gradient descent.
\newblock arXiv:1811.12941, 2018.

\bibitem[Goyal et~al.(2017)Goyal, Doll\'{a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{Goyal:2017}
Goyal, P., Doll\'{a}r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola,
  A., Tulloch, A., Jia, Y., and He, K.
\newblock Accurate, large minibatch {SGD}: Training {ImageNet} in one hour.
\newblock arXiv:1706.02677, 2017.

\bibitem[He et~al.(2016{\natexlab{a}})He, Zhang, Ren, and Sun]{He:2016}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 2016{\natexlab{a}}.

\bibitem[He et~al.(2016{\natexlab{b}})He, Zhang, Ren, and Sun]{He:2016b}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{European conference on computer vision}, 2016{\natexlab{b}}.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry]{Hoffer:2017}
Hoffer, E., Hubara, I., and Soudry, D.
\newblock Train longer, generalize better: Closing the generalization gap in
  large batch training of neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, 2017.

\bibitem[Jain et~al.(2018)Jain, Kakade, Kidambi, Netrapalli, and
  Sidford]{Jain:2018}
Jain, P., Kakade, S.~M., Kidambi, R., Netrapalli, P., and Sidford, A.
\newblock Parallelizing stochastic gradient descent for least squares
  regression: Mini-batching, averaging, and model misspecification.
\newblock \emph{Journal of Machince Learning Research}, 18\penalty0
  (223):\penalty0 1--42, 2018.

\bibitem[Jastrz{\k{e}}bski et~al.(2018)Jastrz{\k{e}}bski, Kenton, Arpit,
  Ballas, Fischer, Bengio, and Storkey]{Jastrzebski:2018}
Jastrz{\k{e}}bski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio,
  Y., and Storkey, A.~J.
\newblock Three factors influencing minima in {SGD}.
\newblock In \emph{Proceedings of the 27th International Conference on
  Artificial Neural Networks}, 2018.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{Johnson:2013}
Johnson, R. and Zhang, T.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems 26}, 2013.

\bibitem[Johnson \& Guestrin(2018)Johnson and Guestrin]{Johnson:2018}
Johnson, T.~B. and Guestrin, C.
\newblock Training deep models faster with robust, approximate importance
  sampling.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, 2018.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{Karimi:2016}
Karimi, H., Nutini, J., and Schmidt, M.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, 2016.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{Kingma:2015}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{Proceedings of the 3rd International Conference on Learning
  Representations}, 2015.

\bibitem[Kloeden \& Platen(1992)Kloeden and Platen]{Kloeden:1992}
Kloeden, P.~E. and Platen, E.
\newblock \emph{Numerical Solution of Stochastic Differential Equations}.
\newblock Springer, 1992.

\bibitem[Krizhevsky(2009)]{Krizhevsky:2009}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Krizhevsky(2014)]{Krizhevsky:2014}
Krizhevsky, A.
\newblock One weird trick for parallelizing convolutional neural networks.
\newblock arXiv:1404.5997, 2014.

\bibitem[Lei et~al.(2017)Lei, Ju, Chen, and Jordan]{Lei:2017}
Lei, L., Ju, C., Chen, J., and Jordan, M.~I.
\newblock Nonconvex finite-sum optimization via {SCSG} methods.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, 2017.

\bibitem[Li et~al.(2014)Li, Zhang, Chen, and Smola]{Li:2014}
Li, M., Zhang, T., Chen, Y., and Smola, A.~J.
\newblock Efficient mini-batch training for stochastic optimization.
\newblock In \emph{Proceedings of the 20th ACM SIGKDD Interational Conference
  on Knowledge Discovery and Data Mining}, 2014.

\bibitem[Lin et~al.(2019)Lin, Zhang, Ma, He, Zhang, Zha, and Li]{Lin:2019}
Lin, H., Zhang, H., Ma, Y., He, T., Zhang, Z., Zha, S., and Li, M.
\newblock Dynamic mini-batch {SGD} for elastic distributed training: Learning
  in the limbo of resources.
\newblock arXiv:1904.12043, 2019.

\bibitem[Ma et~al.(2018)Ma, Bassily, and Belkin]{Ma:2018}
Ma, S., Bassily, R., and Belkin, M.
\newblock The power of interpolation: Understanding the effectiveness of {SGD}
  in modern over-parametrized learning.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, 2018.

\bibitem[McCandlish et~al.(2018)McCandlish, Kaplan, Amodei, and
  Team]{McCandlish:2018}
McCandlish, S., Kaplan, J., Amodei, D., and Team, O.~D.
\newblock An empirical model of large-batch training.
\newblock arXiv:1812.06162, 2018.

\bibitem[Needell et~al.(2014)Needell, Ward, and Srebro]{Needell:2014}
Needell, D., Ward, R., and Srebro, N.
\newblock Stochastic gradient descent, weighted sampling, and the randomized
  {Kaczmarz} algorithm.
\newblock In \emph{Advances in Neural Information Processing Systems 27}, 2014.

\bibitem[Panayotov et~al.(2015)Panayotov, Chen, Povey, and
  Khudanpur]{Panayotov:2015}
Panayotov, V., Chen, G., Povey, D., and Khudanpur, S.
\newblock Librispeech: An {ASR} corpus based on public domain audio books.
\newblock In \emph{IEEE International Conference on Acoustics, Speech and
  Signal Processing}, 2015.

\bibitem[Reddi et~al.(2016)Reddi, Hefny, Sra, P\'{o}czo\'{o}s, and
  Smola]{Reddi:2016}
Reddi, S.~J., Hefny, A., Sra, S., P\'{o}czo\'{o}s, B., and Smola, A.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning}, 2016.

\bibitem[Redmon \& Farhadi(2018)Redmon and Farhadi]{Redmon:2018}
Redmon, J. and Farhadi, A.
\newblock {YOLOv3}: An incremental improvement.
\newblock arXiv:1804.02767, 2018.

\bibitem[Sagun et~al.(2017)Sagun, Evci, Guney, Dauphin, and Bottou]{Sagun:2017}
Sagun, L., Evci, U., Guney, V.~U., Dauphin, Y., and Bottou, L.
\newblock Empirical analysis of the hessian of over-parametrized neural
  networks.
\newblock arXiv:1706.04454, 2017.

\bibitem[Schaul et~al.(2013)Schaul, Zhang, and LeCun]{Schaul:2013}
Schaul, T., Zhang, S., and LeCun, Y.
\newblock No more pesky learning rates.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning}, 2013.

\bibitem[Shallue et~al.(2019)Shallue, Lee, Antognini, Sohl-Dickstein, Frostig,
  and Dahl]{Shallue:2019}
Shallue, C.~J., Lee, J., Antognini, J., Sohl-Dickstein, J., Frostig, R., and
  Dahl, G.~E.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock \emph{Journal of Machince Learning Research}, 20\penalty0
  (112):\penalty0 1--49, 2019.

\bibitem[Smith et~al.(2018)Smith, Kindermans, Ying, and Le]{Smith:2018}
Smith, S., Kindermans, P., Ying, C., and Le, Q.~V.
\newblock Don't decay the learning rate, increase the batch size.
\newblock In \emph{Proceedings of the 6th International Conference on Learning
  Representations}, 2018.

\bibitem[Sun(2019)]{Sun:2019}
Sun, R.
\newblock Optimization for deep learning: theory and algorithms.
\newblock arXiv:1912.08957, 2019.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{Vaswani:2017}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, 2017.

\bibitem[Yin et~al.(2018)Yin, Pananjady, Lam, Papailiopoulos, Ramchandran, and
  Bartlett]{Yin:2018}
Yin, D., Pananjady, A., Lam, M., Papailiopoulos, D., Ramchandran, K., and
  Bartlett, P.
\newblock Gradient diversity: A key ingredient for scalable distributed
  learning.
\newblock In \emph{Proceedings of the 21st International Conference on
  Artificial Intelligence and Statistics}, 2018.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{You:2017}
You, Y., Gitman, I., and Ginsburg, B.
\newblock Large batch training of convolutional networks.
\newblock arXiv:1708.03888, 2017.

\bibitem[You et~al.(2018)You, Hseu, Ying, Demmel, Keutzer, and Hsieh]{You:2018}
You, Y., Hseu, J., Ying, C., Demmel, J., Keutzer, K., and Hsieh, C.
\newblock Large-batch training for {LSTM} and beyond.
\newblock In \emph{NeurIPS Workshop on Systems for ML and Open Source
  Software}, 2018.

\bibitem[You et~al.(2020)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song,
  Demmel, Keutzer, and Hsieh]{You:2020}
You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X.,
  Demmel, J., Keutzer, K., and Hsieh, C.-J.
\newblock Large batch optimization for deep learning: Training bert in 76
  minutes.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Yuan et~al.(2019)Yuan, Yan, Jin, and Yang]{Yuan:2019}
Yuan, Z., Yan, Y., Jin, R., and Yang, T.
\newblock Stagewise training accelerates convergence of testing error over sgd.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, 2019.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and Lopez-Paz]{Zhang:2018}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock Mixup: Beyond empirical risk minimization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Zhang et~al.(2019)Zhang, He, Zhang, Zhang, Xie, and Li]{Zhang:2019b}
Zhang, Z., He, T., Zhang, H., Zhang, Z., Xie, J., and Li, M.
\newblock Bag of freebies for training object detection neural networks.
\newblock arXiv:1902.04103, 2019.

\bibitem[Zhao \& Zhang(2015)Zhao and Zhang]{Zhao:2015}
Zhao, P. and Zhang, T.
\newblock Stochastic optimization with importance sampling for regularized loss
  minimization.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning}, 2015.

\end{thebibliography}
