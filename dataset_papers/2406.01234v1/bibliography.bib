@misc{cohen2020solving,
      title={Solving Linear Programs in the Current Matrix Multiplication Time}, 
      author={Michael B. Cohen and Yin Tat Lee and Zhao Song},
      year={2020},
      eprint={1810.07896},
      archivePrefix={arXiv},
      primaryClass={cs.DS}
}

@article{audibert_explorationexploitation_2009,
	title = {Exploration–exploitation tradeoff using variance estimates in multi-armed bandits},
	volume = {410},
	number = {19},
	journal = {Theoretical Computer Science},
	author = {Audibert, Jean-Yves and Munos, Rémi and Szepesvári, Csaba},
	year = {2009},
	note = {Publisher: Elsevier},
	pages = {1876--1902},
	file = {exlporation exploitation variance.pdf:/home/victor/travail/phd/biblio/bandits/exlporation exploitation variance.pdf:application/pdf},
}

@article{abbasi2019exploration,
  title={Exploration-Enhanced {POLITEX}},
  author={Abbasi-Yadkori, Yasin and Lazic, Nevena and Szepesvari, Csaba and Weisz, Gellert},
  journal={arXiv preprint arXiv:1908.10479},
  year={2019}
}

@inproceedings{maillard2011finite,
  title={A finite-time analysis of multi-armed bandits problems with kullback-leibler divergences},
  author={Maillard, Odalric-Ambrym and Munos, R{\'e}mi and Stoltz, Gilles},
  booktitle={Proceedings of the 24th annual Conference On Learning Theory},
  pages={497--514},
  year={2011}
}
@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}
@article{jaksch2010near,
  author={Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  title={Near-optimal regret bounds for reinforcement learning},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Apr},
  pages={1563--1600},
  year={2010}
}

@article{agrawal_optimistic_2023,
	title = {Optimistic {Posterior} {Sampling} for {Reinforcement} {Learning}: {Worst}-{Case} {Regret} {Bounds}},
	volume = {48},
	number = {1},
	journal = {Mathematics of Operations Research},
	author = {Agrawal, Shipra and Jia, Randy},
	year = {2023},
	note = {Publisher: INFORMS},
	pages = {363--392},
}

@inproceedings{gopalan_thompson_2015,
	title = {Thompson sampling for learning parameterized markov decision processes},
	booktitle = {Conference on learning theory},
	publisher = {PMLR},
	author = {Gopalan, Aditya and Mannor, Shie},
	year = {2015},
	pages = {861--898},
}

@article{osband_posterior_2016,
	title = {Posterior sampling for reinforcement learning without episodes},
	journal = {arXiv preprint arXiv:1608.02731},
	author = {Osband, Ian and Van Roy, Benjamin},
	year = {2016},
}

@inproceedings{Berge2010TopologicalSI,
  title={Topological Spaces: Including a Treatment of Multi-Valued Functions, Vector Spaces and Convexity},
  author={Claude Berge},
  year={1957},
}

@article{csiszar_simple_2006,
	title = {A simple proof of {Sanov}’s theorem},
	volume = {37},
	number = {4},
	journal = {Bulletin of the Brazilian Mathematical Society},
	author = {Csiszar, Imre},
	year = {2006},
}

@article{pardalos_checking_1988,
	title = {Checking local optimality in constrained quadratic programming is {NP}-hard},
	volume = {7},
	journal = {Operations Research Letters},
	author = {Pardalos, Panos M. and Schnitger, Georg},
	year = {1988},
	pages = {33--35},
}

@article{selvi_convex_2022,
	title = {Convex maximization via adjustable robust optimization},
	volume = {34},
	number = {4},
	journal = {INFORMS Journal on Computing},
	author = {Selvi, Aras and Ben-Tal, Aharon and Brekelmans, Ruud and den Hertog, Dick},
	year = {2022},
	note = {Publisher: INFORMS},
	pages = {2091--2105},
}

@inproceedings{fruit_near_2018,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'18},
	title = {Near optimal exploration-exploitation in non-communicating {Markov} decision processes},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro},
	year = {2018},
	pages = {2998--3008},
}

@article{howard_time-uniform_2020,
	title = {Time-uniform {Chernoff} bounds via nonnegative supermartingales},
	author = {Howard, Steven R and Ramdas, Aaditya and McAuliffe, Jon and Sekhon, Jasjeet},
	year = {2020},
}

@article{peligrad_new_2020,
	title = {A new {CLT} for additive functionals of {Markov} chains},
	volume = {130},
	number = {9},
	journal = {Stochastic Processes and their Applications},
	author = {Peligrad, Magda},
	year = {2020},
	note = {Publisher: Elsevier},
	pages = {5695--5708},
}

@article{maxwell_central_2000,
	title = {Central limit theorems for additive functionals of {Markov} chains},
	journal = {Annals of probability},
	author = {Maxwell, Michael and Woodroofe, Michael},
	year = {2000},
	note = {Publisher: JSTOR},
	pages = {713--724},
}

@misc{kakade_variance_2020,
	title = {Variance {Reduction} {Methods} for {Sublinear} {Reinforcement} {Learning}},
	abstract = {There is a technical issue in the analysis that is not easily fixable. We, therefore, withdraw the submission. Sorry for the inconvenience.},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Kakade, Sham and Wang, Mengdi and Yang, Lin F.},
	month = jun,
	year = {2020},
}

@inproceedings{lattimore_pac_2012,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{PAC} {Bounds} for {Discounted} {MDPs}},
	language = {en},
	booktitle = {Algorithmic {Learning} {Theory}},
	publisher = {Springer},
	author = {Lattimore, Tor and Hutter, Marcus},
	editor = {Bshouty, Nader H. and Stoltz, Gilles and Vayatis, Nicolas and Zeugmann, Thomas},
	year = {2012},
	pages = {320--334},
}

@inproceedings{munos_influence_1999,
	title = {Influence and variance of a {Markov} chain: {Application} to adaptive discretization in optimal control},
	volume = {2},
	booktitle = {Proceedings of the 38th {IEEE} {Conference} on {Decision} and {Control} ({Cat}. {No}. {99CH36304})},
	publisher = {IEEE},
	author = {Munos, Rémi and Moore, Andrew},
	year = {1999},
	pages = {1464--1469},
}

@article{wang_near_2022,
	title = {Near {Sample}-{Optimal} {Reduction}-based {Policy} {Learning} for {Average} {Reward} {MDP}},
	journal = {arXiv preprint arXiv:2212.00603},
	author = {Wang, Jinghan and Wang, Mengdi and Yang, Lin F},
	year = {2022},
}

@inproceedings{zhang_sharper_2023,
	title = {Sharper {Model}-free {Reinforcement} {Learning} for {Average}-reward {Markov} {Decision} {Processes}},
	booktitle = {The {Thirty} {Sixth} {Annual} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Zhang, Zihan and Xie, Qiaomin},
	year = {2023},
	pages = {5476--5477},
}

@article{jonsson2020planning,
  title={Planning in markov decision processes with gap-dependent sample complexity},
  author={Jonsson, Anders and Kaufmann, Emilie and M{\'e}nard, Pierre and Darwiche Domingues, Omar and Leurent, Edouard and Valko, Michal},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1253--1263},
  year={2020}
}

@misc{boone_sliding_2023,
	title = {The {Sliding} {Regret} in {Stochastic} {Bandits}: {Discriminating} {Index} and {Randomized} {Policies}},
	publisher = {arXiv},
	author = {Boone, Victor},
	month = nov,
	year = {2023},
	note = {arXiv:2311.18437 [cs, eess, math, stat]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@inproceedings{baudry_sub-sampling_2020,
	title = {Sub-sampling for {Efficient} {Non}-{Parametric} {Bandit} {Exploration}},
	volume = {33},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Baudry, Dorian and Kaufmann, Emilie and Maillard, Odalric-Ambrym},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {5468--5478},
}

@article{baudry_general_2023,
	title = {A {General} {Recipe} for the {Analysis} of {Randomized} {Multi}-{Armed} {Bandit} {Algorithms}},
	journal = {arXiv preprint arXiv:2303.06058},
	author = {Baudry, Dorian and Suzuki, Kazuya and Honda, Junya},
	year = {2023},
}

@phdthesis{baudry_non-parametric_2022,
	type = {{PhD} {Thesis}},
	title = {Non-parametric algorithms for multi-armed bandits},
	author = {Baudry, Dorian},
	year = {2022},
}

@article{cappe_kullback-leibler_2013,
	title = {Kullback-{Leibler} upper confidence bounds for optimal sequential allocation},
	journal = {The Annals of Statistics},
	author = {Cappé, Olivier and Garivier, Aurélien and Maillard, Odalric-Ambrym and Munos, Rémi and Stoltz, Gilles},
	year = {2013},
	note = {Publisher: JSTOR},
	pages = {1516--1541},
}

@inproceedings{boone_regret_2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {The {Regret} of {Exploration} and the {Control} of {Bad} {Episodes} in {Reinforcement} {Learning}},
	volume = {202},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Boone, Victor and Gaujal, Bruno},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	month = jul,
	year = {2023},
	pages = {2824--2856},
}
@misc{agrawal_analysis_2012,
	title = {Analysis of {Thompson} {Sampling} for the multi-armed bandit problem},
	urldate = {2023-03-20},
	publisher = {arXiv},
	author = {Agrawal, Shipra and Goyal, Navin},
	month = apr,
	year = {2012},
	note = {arXiv:1111.1797 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Data Structures and Algorithms, 68W40, 68Q25, F.2.0},
	annote = {Comment: This version corrects some minor errors, and reorganizes some content},
}

@inproceedings{boone_identification_2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Identification of {Blackwell} {Optimal} {Policies} for {Deterministic} {MDPs}},
	volume = {206},
	abstract = {This paper investigates a new learning problem, the identification of Blackwell optimal policies on deterministic MDPs (DMDPs): A learner has to return a Blackwell optimal policy with fixed confidence using a minimal number of queries. First, we characterize the maximal set of DMDPs for which the identification is possible. Then, we focus on the analysis of algorithms based on product-form confidence regions. We minimize the number of queries by efficiently visiting the state-action pairs with respect to the shape of confidence sets. Furthermore, these confidence sets are themselves optimized to achieve better performances. The performances of our methods compare to the lower bounds up to a factor n{\textasciicircum}2 in the worst case – where n is the number of states, and constant in certain classes of DMDPs.},
	booktitle = {Proceedings of {The} 26th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Boone, Victor and Gaujal, Bruno},
	editor = {Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem},
	month = apr,
	year = {2023},
	pages = {7392--7424},
}
@misc{garivier_explore_2018,
	title = {Explore {First}, {Exploit} {Next}: {The} {True} {Shape} of {Regret} in {Bandit} {Problems}},
	author = {Garivier, Aurélien and Ménard, Pierre and Stoltz, Gilles},
	year = {2018},
	note = {\_eprint: 1602.07182},
}

@inproceedings{pesquerel_imed-rl_2022,
	title = {{IMED}-{RL}: {Regret} optimal learning of ergodic {Markov} decision processes},
	volume = {35},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Pesquerel, Fabien and Maillard, Odalric-Ambrym},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {26363--26374},
}

@article{honda_non-asymptotic_2015,
	title = {Non-asymptotic analysis of a new bandit algorithm for semi-bounded rewards.},
	volume = {16},
	journal = {J. Mach. Learn. Res.},
	author = {Honda, Junya and Takemura, Akimichi},
	year = {2015},
	pages = {3721--3756},
}

@misc{honda_asymptotically_2010,
	title = {An {Asymptotically} {Optimal} {Policy} for {Finite} {Support} {Models} in the {Multiarmed} {Bandit} {Problem}},
	author = {Honda, Junya and Takemura, Akimichi},
	year = {2010},
	note = {\_eprint: 0905.2776},
}
@phdthesis{maillard_mathematics_2019,
	type = {Habilitation à diriger des recherches},
	title = {Mathematics of {Statistical} {Sequential} {Decision} {Making}},
	school = {Université de Lille Nord de France},
	author = {Maillard, Odalric-Ambrym},
	month = feb,
	year = {2019},
	keywords = {Markov Decision Process, Bandits manchots, Concentration de la mesure, Concentration of measure, Finite-time Statistics, Multi-armed Bandits, Processus decisionnel de Markov, Statistique non-asymptotique},
}

@book{massart_concentration_2007,
	address = {Berlin},
	title = {Concentration inequalities and model selection},
	volume = {volume 1896 of Lecture Notes in Mathematics},
	shorttitle = {Lectures from the 33rd {Summer} {School} on {Probability} {Theory} held in {Saint}-{Flour}, {July} 6–23, 2003, {With} a foreword by {Jean} {Picard}.},
	publisher = {Springer},
	author = {Massart, Pierre},
	year = {2007},
}

@phdthesis{fruit_exploration-exploitation_2019,
	type = {{PhD} {Thesis}},
	title = {Exploration-exploitation dilemma in {Reinforcement} {Learning} under various form of prior knowledge},
	school = {Université de Lille 1, Sciences et Technologies; CRIStAL UMR 9189},
	author = {Fruit, Ronan},
	year = {2019},
}

@incollection{benaim_dynamics_1999,
	title = {Dynamics of stochastic approximation algorithms},
	booktitle = {Seminaire de probabilites {XXXIII}},
	publisher = {Springer},
	author = {Benaïm, Michel},
	year = {1999},
	pages = {1--68},
}

@article{biggar_replicator_2022,
	title = {The {Replicator} {Dynamic}, {Chain} {Components} and the {Response} {Graph}},
	journal = {arXiv preprint arXiv:2209.15230},
	author = {Biggar, Oliver and Shames, Iman},
	year = {2022},
}

@article{bishop_examples_2014,
	title = {Examples concerning {Abel} and {Ces{\`a}ro} limits},
	volume = {420},
	number = {2},
	journal = {Journal of Mathematical Analysis and Applications},
	author = {Bishop, Christopher J and Feinberg, Eugene A and Zhang, Junyu},
	year = {2014},
	note = {Publisher: Elsevier},
	pages = {1654--1661},
}

@article{bertsekas_dynamic_2011,
	title = {Dynamic programming and optimal control 3rd edition, volume ii},
	journal = {Belmont, MA: Athena Scientific},
	author = {Bertsekas, Dimitri P and {others}},
	year = {2011},
}

@book{bertsekas_dynamic_2012,
	title = {Dynamic programming and optimal control: {Volume} {I}},
	volume = {1},
	publisher = {Athena scientific},
	author = {Bertsekas, Dimitri},
	year = {2012},
}

@book{piunovskiy_continuous-time_2020,
	address = {Cham},
	series = {Probability {Theory} and {Stochastic} {Modelling}},
	title = {Continuous-{Time} {Markov} {Decision} {Processes}: {Borel} {Space} {Models} and {General} {Control} {Strategies}},
	volume = {97},
	isbn = {978-3-030-54986-2 978-3-030-54987-9},
	shorttitle = {Continuous-{Time} {Markov} {Decision} {Processes}},
	language = {en},
	urldate = {2022-06-20},
	publisher = {Springer International Publishing},
	author = {Piunovskiy, Alexey and Zhang, Yi},
	year = {2020},
}

@article{mertikopoulos_learning_2019,
	title = {Learning in games with continuous action sets and unknown payoff functions},
	volume = {173},
	number = {1},
	journal = {Mathematical Programming},
	author = {Mertikopoulos, Panayotis and Zhou, Zhengyuan},
	year = {2019},
	note = {Publisher: Springer},
	pages = {465--507},
}

@article{sorin_exponential_2009,
	title = {Exponential weight algorithm in continuous time},
	volume = {116},
	journal = {Math. Program.},
	author = {Sorin, Sylvain},
	month = jan,
	year = {2009},
	pages = {513--528},
}

@article{papadimitriou_complexity_1994,
	title = {On the complexity of the parity argument and other inefficient proofs of existence},
	volume = {48},
	number = {3},
	journal = {Journal of Computer and system Sciences},
	author = {Papadimitriou, Christos H},
	year = {1994},
	note = {Publisher: Elsevier},
	pages = {498--532},
}

@inproceedings{gordon_no-regret_2008,
	title = {No-regret learning in convex games},
	booktitle = {Proceedings of the 25th international conference on {Machine} learning},
	author = {Gordon, Geoffrey J and Greenwald, Amy and Marks, Casey},
	year = {2008},
	pages = {360--367},
}

@incollection{greenwald_general_2003,
	title = {A general class of no-regret learning algorithms and game-theoretic equilibria},
	booktitle = {Learning theory and kernel machines},
	publisher = {Springer},
	author = {Greenwald, Amy and Jafari, Amir},
	year = {2003},
	pages = {2--12},
}

@article{stoltz_learning_2007,
	title = {Learning correlated equilibria in games with compact sets of strategies},
	volume = {59},
	number = {1},
	journal = {Games and Economic Behavior},
	author = {Stoltz, Gilles and Lugosi, G{\'a}bor},
	year = {2007},
	note = {Publisher: Elsevier},
	pages = {187--208},
}

@book{nisan_algorithmic_2007,
	address = {Cambridge ; New York},
	title = {Algorithmic game theory},
	isbn = {978-0-521-87282-9},
	language = {en},
	publisher = {Cambridge University Press},
	editor = {Nisan, Noam},
	year = {2007},
	note = {OCLC: ocn122526907},
	keywords = {Algorithms, Game theory},
}

@article{piliouras_evolutionary_2021,
	title = {Evolutionary {Dynamics} and \$\${\textbackslash}backslash\${Phi} \$-{Regret} {Minimization} in {Games}},
	journal = {arXiv preprint arXiv:2106.14668},
	author = {Piliouras, Georgios and Rowland, Mark and Omidshafiei, Shayegan and Elie, Romuald and Hennes, Daniel and Connor, Jerome and Tuyls, Karl},
	year = {2021},
}

@article{aumann_correlated_1987,
	title = {Correlated equilibrium as an expression of {Bayesian} rationality},
	journal = {Econometrica: Journal of the Econometric Society},
	author = {Aumann, Robert J},
	year = {1987},
	note = {Publisher: JSTOR},
	pages = {1--18},
}

@article{aumann_subjectivity_1974,
	title = {Subjectivity and correlation in randomized strategies},
	volume = {1},
	number = {1},
	journal = {Journal of mathematical Economics},
	author = {Aumann, Robert J},
	year = {1974},
	note = {Publisher: Elsevier},
	pages = {67--96},
}

@book{katok_introduction_1997,
	title = {Introduction to the modern theory of dynamical systems},
	number = {54},
	publisher = {Cambridge university press},
	author = {Katok, Anatole and Hasselblatt, Boris},
	year = {1997},
}

@inproceedings{vovk_aggregating_1990,
	title = {Aggregating strategies},
	booktitle = {{COLT} '90},
	author = {Vovk, Vladimir},
	year = {1990},
}

@inproceedings{littlestone_weighted_1989,
	title = {The weighted majority algorithm},
	booktitle = {30th {Annual} {Symposium} on {Foundations} of {Computer} {Science}},
	author = {Littlestone, N. and Warmuth, M.K.},
	year = {1989},
	pages = {256--261},
}

@inproceedings{auer_gambling_1995,
	title = {Gambling in a rigged casino: {The} adversarial multi-armed bandit problem},
	booktitle = {Proceedings of {IEEE} 36th {Annual} {Foundations} of {Computer} {Science}},
	author = {Auer, P. and Cesa-Bianchi, N. and Freund, Y. and Schapire, R.E.},
	year = {1995},
	pages = {322--331},
}

@article{thiparat_chotibut_route_2019,
	title = {The route to chaos in routing games: {Population} increase drives period-doubling instability, chaos \&amp; inefficiency with {Price} of {Anarchy} equal to one},
	shorttitle = {The route to chaos in routing games},
	abstract = {We study a learning dynamic model of routing (congestion) games to explore how an increase in the total demand influences system performance. We focus on non-atomic routing games with two parallel edges of linear cost, where all agents evolve using Multiplicative Weights Updates with a fixed learning rate. Previous game-theoretic equilibrium analysis suggests that system performance is close to optimal in the large population limit, as seen by the Price of Anarchy reduction. In this work, however, we reveal a rather undesirable consequence of non-equilibrium phenomena driven by population increase. As the total demand rises, we prove that the learning dynamics unavoidably become non-equilibrating, typically chaotic. The Price of Anarchy predictions of near-optimal performance no longer apply. To the contrary, the time-average social cost may converge to its worst possible value in the large population limit.},
	language = {en},
	urldate = {2022-07-01},
	author = {Thiparat Chotibut and Falniowski, Fryderyk and Micha{\l } Misiurewicz and Piliouras, Georgios},
	year = {2019},
	note = {Publisher: Unpublished},
}

@article{andrade_learning_2021,
	title = {Learning in {Matrix} {Games} can be {Arbitrarily} {Complex}},
	abstract = {A growing number of machine learning architectures, such as Generative Adversarial Networks, rely on the design of games which implement a desired functionality via a Nash equilibrium. In practice these games have an implicit complexity (e.g. from underlying datasets and the deep networks used) that makes directly computing a Nash equilibrium impractical or impossible. For this reason, numerous learning algorithms have been developed with the goal of iteratively converging to a Nash equilibrium. Unfortunately, the dynamics generated by the learning process can be very intricate and instances of training failure hard to interpret. In this paper we show that, in a strong sense, this dynamic complexity is inherent to games. Specifically, we prove that replicator dynamics, the continuous-time analogue of Multiplicative Weights Update, even when applied in a very restricted class of games{\textemdash}known as finite matrix games{\textemdash}is rich enough to be able to approximate arbitrary dynamical systems. Our results are positive in the sense that they show the nearly boundless dynamic modelling capabilities of current machine learning practices, but also negative in implying that these capabilities may come at the cost of interpretability. As a concrete example, we show how replicator dynamics can effectively reproduce the well-known strange attractor of Lonrenz dynamics (the {\textquotedblleft}butterfly effect{\textquotedblright}, Fig 1) while achieving no regret.},
	language = {en},
	urldate = {2021-12-10},
	journal = {arXiv:2103.03405 [nlin]},
	author = {Andrade, Gabriel P. and Frongillo, Rafael and Piliouras, Georgios},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.03405},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Science and Game Theory, Mathematics - Dynamical Systems, Nonlinear Sciences - Chaotic Dynamics},
}

@article{flokas_no-regret_2020,
	title = {No-regret learning and mixed {Nash} equilibria: {They} do not mix},
	shorttitle = {No-regret learning and mixed {Nash} equilibria},
	abstract = {Understanding the behavior of no-regret dynamics in general N -player games is a fundamental question in online learning and game theory. A folk result in the field states that, in finite games, the empirical frequency of play under no-regret learning converges to the game{\textquoteright}s set of coarse correlated equilibria. By contrast, our understanding of how the day-to-day behavior of the dynamics correlates to the game{\textquoteright}s Nash equilibria is much more limited, and only partial results are known for certain classes of games (such as zero-sum or congestion games). In this paper, we study the dynamics of follow the regularized leader (FTRL), arguably the most well-studied class of no-regret dynamics, and we establish a sweeping negative result showing that the notion of mixed Nash equilibrium is antithetical to no-regret learning. Specifically, we show that any Nash equilibrium which is not strict (in that every player has a unique best response) cannot be stable and attracting under the dynamics of FTRL. This result has significant implications for predicting the outcome of a learning process as it shows unequivocally that only strict (and hence, pure) Nash equilibria can emerge as stable limit points thereof.},
	language = {en},
	urldate = {2021-12-10},
	journal = {arXiv:2010.09514 [cs, math]},
	author = {Flokas, Lampros and Vlatakis-Gkaragkounis, Emmanouil-Vasileios and Lianeas, Thanasis and Mertikopoulos, Panayotis and Piliouras, Georgios},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.09514},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Science and Game Theory, Mathematics - Optimization and Control, Primary 91A26, 37N40, secondary 91A68, 68Q32, 68T05},
	annote = {Comment: 24 pages, 7 figures, 1 table},
}

@inproceedings{piliouras_optimization_2014,
	title = {Optimization {Despite} {Chaos}: {Convex} {Relaxations} to {Complex} {Limit} {Sets} via {Poincar{\'e}} {Recurrence}},
	isbn = {978-1-61197-338-9 978-1-61197-340-2},
	shorttitle = {Optimization {Despite} {Chaos}},
	abstract = {It is well understood that decentralized systems can, through network interactions, give rise to complex behavior patterns that do not reflect their equilibrium properties. The challenge of any analytic investigation is to identify and characterize persistent properties despite the inherent irregularities of such systems and to do so efficiently. We develop a novel framework to address this challenge.},
	language = {en},
	urldate = {2021-12-10},
	booktitle = {Proceedings of the {Twenty}-{Fifth} {Annual} {ACM}-{SIAM} {Symposium} on {Discrete} {Algorithms}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Piliouras, Georgios and Shamma, Jeff S.},
	month = jan,
	year = {2014},
	pages = {861--873},
}

@article{hofbauer_time_2009,
	title = {Time {Average} {Replicator} and {Best}-{Reply} {Dynamics}},
	volume = {34},
	issn = {0364-765X, 1526-5471},
	language = {en},
	number = {2},
	urldate = {2021-12-10},
	journal = {Mathematics of Operations Research},
	author = {Hofbauer, Josef and Sorin, Sylvain and Viossat, Yannick},
	month = may,
	year = {2009},
	pages = {263--269},
}

@article{hofbauer_evolutionary_nodate,
	title = {{EVOLUTIONARY} {GAME} {DYNAMICS}},
	abstract = {Evolutionary game dynamics is the application of population dynamical methods to game theory. It has been introduced by evolutionary biologists, anticipated in part by classical game theorists. In this survey, we present an overview of the many brands of deterministic dynamical systems motivated by evolutionary game theory, including ordinary differential equations (and, in particular, the replicator equation), differential inclusions (the best response dynamics), difference equations (as, for instance, fictitious play) and reaction-diffusion systems. A recurrent theme (the so-called {\textquoteleft}folk theorem of evolutionary game theory{\textquoteright}) is the close connection of the dynamical approach with the Nash equilibrium, but we show that a static, equilibriumbased viewpoint is, on principle, unable to always account for the long-term behaviour of players adjusting their behaviour to maximise their payoff.},
	language = {en},
	author = {Hofbauer, Josef and Sigmund, Karl},
	pages = {41},
}

@article{mertikopoulos_cycles_2017,
	title = {Cycles in adversarial regularized learning},
	abstract = {Regularized learning is a fundamental technique in online optimization, machine learning and many other fields of computer science. A natural question that arises in these settings is how regularized learning algorithms behave when faced against each other. We study a natural formulation of this problem by coupling regularized learning dynamics in zero-sum games. We show that the system{\textquoteright}s behavior is Poincar{\'e} recurrent, implying that almost every trajectory revisits any (arbitrarily small) neighborhood of its starting point infinitely often. This cycling behavior is robust to the agents{\textquoteright} choice of regularization mechanism (each agent could be using a different regularizer), to positive-affine transformations of the agents{\textquoteright} utilities, and it also persists in the case of networked competition, i.e., for zero-sum polymatrix games.},
	language = {en},
	urldate = {2021-12-10},
	journal = {arXiv:1709.02738 [cs]},
	author = {Mertikopoulos, Panayotis and Papadimitriou, Christos and Piliouras, Georgios},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.02738},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Science and Game Theory},
	annote = {Comment: 22 pages, 4 figures},
}

@article{czechowski_poincare-bendixson_2021,
	title = {Poincar{\textbackslash}'\{e\}-{Bendixson} {Limit} {Sets} in {Multi}-{Agent} {Learning}},
	abstract = {A key challenge of evolutionary game theory and multi-agent learning is to characterize the limit behaviour of game dynamics. Whereas convergence is often a property of learning algorithms in games satisfying a particular reward structure (e.g. zero-sum), it is well known, that for general payoffs even basic learning models, such as the replicator dynamics, are not guaranteed to converge. Worse yet, chaotic behavior is possible even in rather simple games, such as variants of Rock-Paper-Scissors games [35]. Although chaotic behavior in learning dynamics can be precluded by the celebrated Poincar{\'e}-Bendixson theorem, it is only applicable to low-dimensional settings. Are there other characteristics of a game, which can force regularity in the limit sets of learning? In this paper, we show that behaviors consistent with the Poincar{\'e}Bendixson theorem (limit cycles, but no chaotic attractor) follows purely based on the topological structure of the interaction graph, even for high-dimensional settings with arbitrary number of players and arbitrary payoff matrices. We prove our result for a wide class of follow-the-regularized leader (FoReL) dynamics, which generalize replicator dynamics, for games where each player has two strategies at disposal, and for interaction graphs where payoffs of each agent are only affected by one other agent (i.e. interaction graphs of indegree one). Since chaos has been observed in a game with only two players and three strategies, this class of non-chaotic games is in a sense maximal. Moreover, we provide simple conditions under which such behavior translates to social welfare guarantees, implying that FoReL learning achieves time average social welfare which is at least as good as that of a Nash equilibrium; and connecting the topology of the dynamics to the Price of Anarchy analysis.},
	language = {en},
	urldate = {2021-12-10},
	journal = {arXiv:2102.00053 [cs]},
	author = {Czechowski, Aleksander and Piliouras, Georgios},
	month = jan,
	year = {2021},
	note = {arXiv: 2102.00053},
	keywords = {Computer Science - Computer Science and Game Theory, 91A22, 91A26, Computer Science - Multiagent Systems},
}

@article{plank_qualitative_1997,
	title = {Some qualitative differences between the replicator dynamics of two player and n player games},
	volume = {30},
	issn = {0362546X},
	language = {en},
	number = {3},
	urldate = {2021-12-10},
	journal = {Nonlinear Analysis: Theory, Methods \& Applications},
	author = {Plank, Manfred},
	month = dec,
	year = {1997},
	pages = {1411--1417},
}

@article{mertikopoulos_learning_2016,
	title = {Learning in {Games} via {Reinforcement} and {Regularization}},
	volume = {41},
	issn = {0364-765X, 1526-5471},
	abstract = {We investigate a class of reinforcement learning dynamics in which players adjust their strategies based on their actions{\textquoteright} cumulative payoffs over time {\textendash} specifically, by playing mixed strategies that maximize their expected cumulative payoff minus a strongly convex, regularizing penalty term. In contrast to the class of penalty functions used to define smooth best responses in models of stochastic fictitious play, the regularizers used in this paper need not be infinitely steep at the boundary of the simplex; in fact, dropping this requirement gives rise to an important dichotomy between steep and nonsteep cases. In this general setting, our main results extend several properties of the replicator dynamics such as the elimination of dominated strategies, the asymptotic stability of strict Nash equilibria and the convergence of timeaveraged trajectories to interior Nash equilibria in zero-sum games.},
	language = {en},
	number = {4},
	urldate = {2021-12-10},
	journal = {Mathematics of Operations Research},
	author = {Mertikopoulos, Panayotis and Sandholm, William H.},
	month = nov,
	year = {2016},
	pages = {1297--1324},
}

@article{shalev-shwartz_online_2011,
	title = {Online {Learning} and {Online} {Convex} {Optimization}},
	volume = {4},
	issn = {1935-8237, 1935-8245},
	abstract = {Online learning is a well established learning paradigm which has both theoretical and practical appeals. The goal of online learning is to make a sequence of accurate predictions given knowledge of the correct answer to previous prediction tasks and possibly additional available information. Online learning has been studied in several research fields including game theory, information theory, and machine learning. It also became of great interest to practitioners due the recent emergence of large scale applications such as online advertisement placement and online web ranking. In this survey we provide a modern overview of online learning. Our goal is to give the reader a sense of some of the interesting ideas and in particular to underscore the centrality of convexity in deriving efficient online learning algorithms. We do not mean to be comprehensive but rather to give a high-level, rigorous yet easy to follow, survey.},
	language = {en},
	number = {2},
	urldate = {2021-12-10},
	journal = {Foundations and Trends{\textregistered} in Machine Learning},
	author = {Shalev-Shwartz, Shai},
	year = {2011},
	pages = {107--194},
}

@article{bilenne_fast_2020,
	title = {Fast {Optimization} {With} {Zeroth}-{Order} {Feedback} in {Distributed}, {Multi}-{User} {MIMO} {Systems}},
	volume = {68},
	issn = {1053-587X, 1941-0476},
	abstract = {In this paper, we develop a gradient-free optimization methodology for efficient resource allocation in Gaussian MIMO multiple access channels. Our approach combines two main ingredients: (i ) an entropic semidefinite optimization based on matrix exponential learning (MXL); and (ii ) a one-shot gradient estimator which achieves low variance through the reuse of past information. This novel algorithm, which we call gradientfree MXL with callbacks (MXL0+), retains the convergence speed of gradient-based methods while requiring minimal feedback per iteration{\textemdash}a single scalar. In more detail, in a MIMO multiple access channel with K users and M transmit antennas per user, the MXL0+ algorithm achieves $\varepsilon$-optimality within poly(K, M)/$\varepsilon$2 iterations (on average and with high probability), even when implemented in a fully distributed, asynchronous manner. For crossvalidation, we also perform a series of numerical experiments in medium- to large-scale MIMO networks under realistic channel conditions. Throughout our experiments, the performance of MXL0+ matches{\textemdash}and sometimes exceeds{\textemdash}that of gradientbased MXL methods, all the while operating with a vastly reduced communication overhead. In view of these findings, the MXL0+ algorithm appears to be uniquely suited for distributed massive MIMO systems where gradient calculations can become prohibitively expensive.},
	language = {en},
	urldate = {2021-12-10},
	journal = {IEEE Transactions on Signal Processing},
	author = {Bilenne, Olivier and Mertikopoulos, Panayotis and Belmega, Elena Veronica},
	year = {2020},
	pages = {6085--6100},
}

@article{bravo_bandit_2018,
	title = {Bandit learning in concave \${N}\$-person games},
	abstract = {This paper examines the long-run behavior of learning with bandit feedback in non-cooperative concave games. The bandit framework accounts for extremely low-information environments where the agents may not even know they are playing a game; as such, the agents{\textquoteright} most sensible choice in this setting would be to employ a no-regret learning algorithm. In general, this does not mean that the players{\textquoteright} behavior stabilizes in the long run: no-regret learning may lead to cycles, even with perfect gradient information. However, if a standard monotonicity condition is satisfied, our analysis shows that noregret learning based on mirror descent with bandit feedback converges to Nash equilibrium with probability 1. We also derive an upper bound for the convergence rate of the process that nearly matches the best attainable rate for single-agent bandit stochastic optimization.},
	language = {en},
	urldate = {2021-12-10},
	journal = {arXiv:1810.01925 [cs, math]},
	author = {Bravo, Mario and Leslie, David S. and Mertikopoulos, Panayotis},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.01925},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Science and Game Theory, Mathematics - Optimization and Control, Primary 91A10, 91A26, secondary 68Q32, 68T02},
	annote = {Comment: 24 pages, 1 figure},
}

@article{flaxman_online_2004,
	title = {Online convex optimization in the bandit setting: gradient descent without a gradient},
	shorttitle = {Online convex optimization in the bandit setting},
	abstract = {We study a general online convex optimization problem. We have a convex set S and an unknown sequence of cost functions c1, c2, . . . , and in each round, we choose a feasible point xt in S, and learn the cost ct(xt). If the function ct is also revealed after each round then, as Zinke?vich shows in [23], gradient descent can be used on these funct?ions to get regret bounds of O( n). That is, after n rounds, the total cost incurred will be O( n) more than the cost of the best single feasible decision chosen with the benefit of hindsight, minx?S ct(x).},
	language = {en},
	urldate = {2021-12-10},
	journal = {arXiv:cs/0408007},
	author = {Flaxman, Abraham D. and Kalai, Adam Tauman and McMahan, H. Brendan},
	month = aug,
	year = {2004},
	note = {arXiv: cs/0408007},
	keywords = {Computer Science - Machine Learning, Computer Science - Computational Complexity},
	annote = {Comment: 12 pages},
}

@article{wilson_smoothing_1969,
	title = {Smoothing derivatives of functions and applications},
	volume = {139},
	journal = {Transactions of the American Mathematical Society},
	author = {Wilson, F. Wesley},
	year = {1969},
	pages = {413--428},
}

@article{milionis_nash_2022,
	title = {Nash, {Conley}, and {Computation}: {Impossibility} and {Incompleteness} in {Game} {Dynamics}},
	shorttitle = {Nash, {Conley}, and {Computation}},
	abstract = {Under what conditions do the behaviors of players, who play a game repeatedly, converge to a Nash equilibrium? If one assumes that the players{\textquoteright} behavior is a discrete-time or continuous-time rule whereby the current mixed strategy profile is mapped to the next, this becomes a problem in the theory of dynamical systems. We apply this theory, and in particular the concepts of chain recurrence, attractors, and Conley index, to prove a general impossibility result: there exist games for which any dynamics is bound to have starting points that do not end up at a Nash equilibrium. We also prove a stronger result for ??-approximate Nash equilibria: there are games such that no game dynamics can converge (in an appropriate sense) to ??-Nash equilibria, and in fact the set of such games has positive measure. Further numerical results demonstrate that this holds for any ?? between zero and 0.09. Our results establish that, although the notions of Nash equilibria (and its computation-inspired approximations) are universally applicable in all games, they are also fundamentally incomplete as predictors of long term behavior, regardless of the choice of dynamics.},
	language = {en},
	urldate = {2022-03-31},
	journal = {arXiv:2203.14129 [cs, econ, math]},
	author = {Milionis, Jason and Papadimitriou, Christos and Piliouras, Georgios and Spendlove, Kelly},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.14129},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Science and Game Theory, Mathematics - Dynamical Systems, Economics - Theoretical Economics},
	annote = {Comment: 25 pages},
}

@book{conley_isolated_1978,
	series = {{CBMS} {Regional} {Conference} {Series} in {Mathematics}},
	title = {Isolated {Invariant} {Sets} and the {Morse} {Index}},
	volume = {38},
	isbn = {978-0-8218-1688-2 978-0-8218-8883-4 978-1-4704-2398-8},
	abstract = {Advancing research. Creating connections.},
	language = {en},
	urldate = {2022-04-01},
	publisher = {American Mathematical Society},
	author = {Conley, Charles},
	month = dec,
	year = {1978},
	note = {ISSN: 0160-7642, 2380-5668},
}

@inproceedings{giannou_survival_2021,
	title = {Survival of the strictest: {Stable} and unstable equilibria under regularized learning with partial information},
	booktitle = {Conference on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Giannou, Angeliki and Vlatakis-Gkaragkounis, Emmanouil Vasileios and Mertikopoulos, Panayotis},
	year = {2021},
	pages = {2147--2148},
}

@article{palaiopanos_multiplicative_2017,
	title = {Multiplicative weights update with constant step-size in congestion games: {Convergence}, limit cycles and chaos},
	volume = {30},
	journal = {Advances in Neural Information Processing Systems},
	author = {Palaiopanos, Gerasimos and Panageas, Ioannis and Piliouras, Georgios},
	year = {2017},
}

@article{arora_multiplicative_2012,
	title = {The multiplicative weights update method: a meta-algorithm and applications},
	volume = {8},
	number = {1},
	journal = {Theory of computing},
	author = {Arora, Sanjeev and Hazan, Elad and Kale, Satyen},
	year = {2012},
	note = {Publisher: Theory of Computing Exchange},
	pages = {121--164},
}

@inproceedings{rakhlin_optimization_2013,
	title = {Optimization, {Learning}, and {Games} with {Predictable} {Sequences}},
	volume = {26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rakhlin, Sasha and Sridharan, Karthik},
	editor = {Burges, C. J. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
}

@article{daskalakis_complexity_nodate,
	title = {The {Complexity} of {Computing} a {Nash} {Equilibrium}},
	abstract = {In 1951, John F. Nash proved that every game has a Nash equilibrium [43]. His proof is non-constructive, relying on Brouwer{\textquoteright}s fixed point theorem, thus leaving open the questions: Is there a polynomial-time algorithm for computing Nash equilibria? And is this reliance on Brouwer inherent? Many algorithms have since been proposed for finding Nash equilibria, but none known to run in polynomial time. In 1991 the complexity class PPAD, for which Brouwer{\textquoteright}s problem is complete, was introduced [48], motivated largely by the classification problem for Nash equilibria; but whether the Nash problem is complete for this class remained open. In this paper we resolve these questions: We show that finding a Nash equilibrium in three-player games is indeed PPAD-complete; and we do so by a reduction from Brouwer{\textquoteright}s problem, thus establishing that the two problems are computationally equivalent. Our reduction simulates a (stylized) Brouwer function by a graphical game [33], relying on {\textquotedblleft}gadgets,{\textquotedblright} graphical games performing various arithmetic and logical operations. We then show how to simulate this graphical game by a three-player game, where each of the three players is essentially a color class in a coloring of the underlying graph. Subsequent work [8] established, by improving our construction, that even two-player games are PPAD-complete; here we show that this result follows easily from our proof.},
	language = {en},
	author = {Daskalakis, Constantinos and Goldberg, Paul W and Papadimitriou, Christos H},
	pages = {70},
}

@article{lin_smooth_1996,
	title = {A smooth converse {Lyapunov} theorem for robust stability},
	volume = {34},
	number = {1},
	journal = {SIAM Journal on Control and Optimization},
	author = {Lin, Yuandan and Sontag, Eduardo D and Wang, Yuan},
	year = {1996},
	note = {Publisher: SIAM},
	pages = {124--160},
}

@inproceedings{boone_darwin_2019,
	title = {From {Darwin} to {Poincar{\'e}} and von {Neumann}: {Recurrence} and cycles in evolutionary and algorithmic game theory},
	booktitle = {International {Conference} on {Web} and {Internet} {Economics}},
	publisher = {Springer},
	author = {Boone, Victor and Piliouras, Georgios},
	year = {2019},
	pages = {85--99},
}

@article{hart_uncoupled_2003,
	title = {Uncoupled dynamics do not lead to {Nash} equilibrium},
	volume = {93},
	number = {5},
	journal = {American Economic Review},
	author = {Hart, Sergiu and Mas-Colell, Andreu},
	year = {2003},
	pages = {1830--1836},
}

@article{hart_stochastic_2006,
	title = {Stochastic uncoupled dynamics and {Nash} equilibrium},
	volume = {57},
	number = {2},
	journal = {Games and economic behavior},
	author = {Hart, Sergiu and Mas-Colell, Andreu},
	year = {2006},
	note = {Publisher: Elsevier},
	pages = {286--303},
}

@article{hannan_approximation_1957,
	title = {Approximation to {Bayes} risk in repeated play},
	volume = {3},
	number = {2},
	journal = {Contributions to the Theory of Games},
	author = {Hannan, James},
	year = {1957},
	pages = {97--139},
}

@article{nash_non-cooperative_1951,
	title = {Non-cooperative games},
	journal = {Annals of mathematics},
	author = {Nash, John},
	year = {1951},
	note = {Publisher: JSTOR},
	pages = {286--295},
}

@article{nash_jr_equilibrium_1950,
	title = {Equilibrium points in n-person games},
	volume = {36},
	number = {1},
	journal = {Proceedings of the national academy of sciences},
	author = {Nash Jr, John F},
	year = {1950},
	note = {Publisher: National Acad Sciences},
	pages = {48--49},
}

@book{shoham_multiagent_2008,
	title = {Multiagent systems: {Algorithmic}, game-theoretic, and logical foundations},
	publisher = {Cambridge University Press},
	author = {Shoham, Yoav and Leyton-Brown, Kevin},
	year = {2008},
}

@article{goodfellow_generative_2014,
	title = {Generative adversarial nets},
	volume = {27},
	journal = {Advances in neural information processing systems},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	year = {2014},
}

@article{blum_learning_2007,
	title = {Learning, regret minimization, and equilibria},
	author = {Blum, Avrim and Monsour, Yishay},
	year = {2007},
	note = {Publisher: Carnegie Mellon University},
}

@article{madry_towards_2017,
	title = {Towards deep learning models resistant to adversarial attacks},
	journal = {arXiv preprint arXiv:1706.06083},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	year = {2017},
}

@article{heliou_learning_2017,
	title = {Learning with bandit feedback in potential games},
	volume = {30},
	journal = {Advances in Neural Information Processing Systems},
	author = {Heliou, Am{\'e}lie and Cohen, Johanne and Mertikopoulos, Panayotis},
	year = {2017},
}

@article{leslie_generalised_2006,
	title = {Generalised weakened fictitious play},
	volume = {56},
	number = {2},
	journal = {Games and Economic Behavior},
	author = {Leslie, David S and Collins, Edmund J},
	year = {2006},
	note = {Publisher: Elsevier},
	pages = {285--298},
}

@misc{daskalakis_last-iterate_2020,
	title = {Last-{Iterate} {Convergence}: {Zero}-{Sum} {Games} and {Constrained} {Min}-{Max} {Optimization}},
	shorttitle = {Last-{Iterate} {Convergence}},
	abstract = {Motivated by applications in Game Theory, Optimization, and Generative Adversarial Networks, recent work of Daskalakis et al [8] and follow-up work of Liang and Stokes [11] have established that a variant of the widely used Gradient Descent/Ascent procedure, called {\textquotedblleft}Optimistic Gradient Descent/Ascent (OGDA){\textquotedblright}, exhibits last-iterate convergence to saddle points in unconstrained convex-concave min-max optimization problems. We show that the same holds true in the more general problem of constrained min-max optimization under a variant of the no-regret Multiplicative-Weights-Update method called {\textquotedblleft}Optimistic Multiplicative-Weights Update (OMWU){\textquotedblright}. This answers an open question of Syrgkanis et al [19].},
	language = {en},
	urldate = {2022-07-01},
	publisher = {arXiv},
	author = {Daskalakis, Constantinos and Panageas, Ioannis},
	month = dec,
	year = {2020},
	note = {arXiv:1807.04252 [cs, math, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Science and Game Theory, Mathematics - Optimization and Control},
	annote = {Comment: Appeared in ITCS 2019},
}

@misc{daskalakis_training_2018,
	title = {Training {GANs} with {Optimism}},
	abstract = {We address the issue of limit cycling behavior in training Generative Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for training Wasserstein GANs. Recent theoretical results have shown that optimistic mirror decent (OMD) can enjoy faster regret rates in the context of zero-sum games. WGANs is exactly a context of solving a zero-sum game with simultaneous noregret dynamics. Moreover, we show that optimistic mirror decent addresses the limit cycling problem in training WGANs. We formally show that in the case of bi-linear zero-sum games the last iterate of OMD dynamics converges to an equilibrium, in contrast to GD dynamics which are bound to cycle. We also portray the huge qualitative difference between GD and OMD dynamics with toy examples, even when GD is modified with many adaptations proposed in the recent literature, such as gradient penalty or momentum. We apply OMD WGAN training to a bioinformatics problem of generating DNA sequences. We observe that models trained with OMD achieve consistently smaller KL divergence with respect to the true underlying distribution, than models trained with GD variants. Finally, we introduce a new algorithm, Optimistic Adam, which is an optimistic variant of Adam. We apply it to WGAN training on CIFAR10 and observe improved performance in terms of inception score as compared to Adam.},
	language = {en},
	urldate = {2022-07-01},
	publisher = {arXiv},
	author = {Daskalakis, Constantinos and Ilyas, Andrew and Syrgkanis, Vasilis and Zeng, Haoyang},
	month = feb,
	year = {2018},
	note = {arXiv:1711.00141 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Science and Game Theory},
}

@article{syrgkanis_fast_2015,
	title = {Fast convergence of regularized learning in games},
	volume = {28},
	journal = {Advances in Neural Information Processing Systems},
	author = {Syrgkanis, Vasilis and Agarwal, Alekh and Luo, Haipeng and Schapire, Robert E},
	year = {2015},
}

@inproceedings{giannou_convergence_2021,
	title = {The convergence rate of regularized learning in games: {From} bandits and uncertainty to optimism and beyond},
	booktitle = {{NeurIPS} 2021-35th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Giannou, Angeliki and Vlatakis-Gkaragkounis, Emmanouil and Mertikopoulos, Panayotis},
	year = {2021},
}

@article{chotibut_family_2021,
	title = {Family of chaotic maps from game theory},
	volume = {36},
	issn = {1468-9367, 1468-9375},
	language = {en},
	number = {1},
	urldate = {2022-07-01},
	journal = {Dynamical Systems},
	author = {Chotibut, Thiparat and Falniowski, Fryderyk and Misiurewicz, Micha{\l } and Piliouras, Georgios},
	month = jan,
	year = {2021},
	pages = {48--63},
}

@article{shalev-shwartz_convex_2006,
	title = {Convex repeated games and {Fenchel} duality},
	volume = {19},
	journal = {Advances in neural information processing systems},
	author = {Shalev-Shwartz, Shai and Singer, Yoram},
	year = {2006},
}

@inproceedings{mertikopoulos_optimistic_2019,
	address = {New Orleans, United States},
	title = {Optimistic {Mirror} {Descent} in {Saddle}-{Point} {Problems}: {Going} the {Extra} ({Gradient}) {Mile}},
	booktitle = {{ICLR} 2019 - 7th {International} {Conference} on {Learning} {Representations}},
	author = {Mertikopoulos, Panayotis and Lecouat, Bruno and Zenati, Houssam and Foo, Chuan-Sheng and Chandrasekhar, Vijay and Piliouras, Georgios},
	month = may,
	year = {2019},
	pages = {1--23},
}

@article{audibert_minimax_nodate,
	title = {Minimax policies for adversarial and stochastic bandits},
	abstract = {We fill in a long open gap in the characterization of the minimax rate for the multi-armed bandit problem. Concretely, we remove an extraneous logarithmic factor in the previously known upper bound and propose a new family of randomized algorithms based on an implicit normalization, as well as a new analysis. We also consider the stochastic case, and prove that an appropriate modification of the upper confidence bound policy UCB1 (Auer et al., 2002) achieves the distribution-free optimal rate while still having a distribution-dependent rate logarithmic in the number of plays.},
	language = {en},
	author = {Audibert, Jean-Yves and Bubeck, Sebastien},
	pages = {10},
}

@article{kaufmann_thompson_2012,
	title = {Thompson {Sampling}: {An} {Asymptotically} {Optimal} {Finite} {Time} {Analysis}},
	shorttitle = {Thompson {Sampling}},
	abstract = {The question of the optimality of Thompson Sampling for solving the stochastic multi-armed bandit problem had been open since 1933. In this paper we answer it positively for the case of Bernoulli rewards by providing the first finite-time analysis that matches the asymptotic rate given in the Lai and Robbins lower bound for the cumulative regret. The proof is accompanied by a numerical comparison with other optimal policies, experiments that have been lacking in the literature until now for the Bernoulli case.},
	language = {en},
	urldate = {2021-12-10},
	journal = {arXiv:1205.4217 [cs, stat]},
	author = {Kaufmann, Emilie and Korda, Nathaniel and Munos, R{\'e}mi},
	month = jul,
	year = {2012},
	note = {arXiv: 1205.4217},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 15 pages, 2 figures, submitted to ALT (Algorithmic Learning Theory)},
}

@article{lai_asymptotically_1985,
	title = {Asymptotically efficient adaptive allocation rules},
	volume = {6},
	issn = {01968858},
	language = {en},
	number = {1},
	urldate = {2021-12-10},
	journal = {Advances in Applied Mathematics},
	author = {Lai, T.L and Robbins, Herbert},
	month = mar,
	year = {1985},
	pages = {4--22},
}

@unpublished{bubeck_regret_2012,
	title = {Regret {Analysis} of {Stochastic} and {Nonstochastic} {Multi}-armed {Bandit} {Problems}},
	abstract = {Multi-armed bandit problems are the most basic examples of sequential decision problems with an exploration-exploitation trade-off. This is the balance between staying with the option that gave highest payoffs in the past and exploring new options that might give higher payoffs in the future. Although the study of bandit problems dates back to the Thirties, exploration-exploitation trade-offs arise in several modern applications, such as ad placement, website optimization, and packet routing. Mathematically, a multi-armed bandit is defined by the payoff process associated with each option. In this survey, we focus on two extreme cases in which the analysis of regret is particularly simple and elegant: i.i.d. payoffs and adversarial payoffs. Besides the basic setting of finitely many actions, we also analyze some of the most important variants and extensions, such as the contextual bandit model.},
	language = {en},
	urldate = {2021-12-10},
	author = {Bubeck, S{\'e}bastien and Cesa-Bianchi, Nicol{\`o}},
	month = nov,
	year = {2012},
	note = {arXiv: 1204.5721},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: To appear in Foundations and Trends in Machine Learning},
}

@article{reverdy_modeling_2014,
	title = {Modeling {Human} {Decision} {Making} in {Generalized} {Gaussian} {Multiarmed} {Bandits}},
	volume = {102},
	issn = {0018-9219, 1558-2256},
	abstract = {In this paper, we present a formal model of human multiarmed bandit problem on graphs, we generalize the UCL decision making in explore{\textendash}exploit tasks using the context of algorithm to the block UCL algorithm and the graphical block multiarmed bandit problems, where the decision maker must UCL algorithm, respectively. We show that these algorithms choose among multiple options with uncertain rewards. We also achieve logarithmic cumulative expected regret and address the standard multiarmed bandit problem, the multi- require a sublogarithmic expected number of transitions armed bandit problem with transition costs, and the multi- among arms. We further illustrate the performance of these armed bandit problem on graphs. We focus on the case of algorithms with numerical examples.},
	language = {en},
	number = {4},
	urldate = {2021-12-10},
	journal = {Proceedings of the IEEE},
	author = {Reverdy, Paul B. and Srivastava, Vaibhav and Leonard, Naomi Ehrich},
	month = apr,
	year = {2014},
	pages = {544--571},
}

@inproceedings{srivastava_optimal_2013,
	address = {Monticello, IL},
	title = {On optimal foraging and multi-armed bandits},
	isbn = {978-1-4799-3410-2 978-1-4799-3409-6},
	abstract = {We consider two variants of the standard multiarmed bandit problem, namely, the multi-armed bandit problem with transition costs and the multi-armed bandit problem on graphs. We develop block allocation algorithms for these problems that achieve an expected cumulative regret that is uniformly dominated by a logarithmic function of time, and an expected cumulative number of transitions from one arm to another arm uniformly dominated by a double-logarithmic function of time. We observe that the multi-armed bandit problem with transition costs and the associated block allocation algorithm capture the key features of popular animal foraging models in literature.},
	language = {en},
	urldate = {2021-12-10},
	booktitle = {2013 51st {Annual} {Allerton} {Conference} on {Communication}, {Control}, and {Computing} ({Allerton})},
	publisher = {IEEE},
	author = {Srivastava, Vaibhav and Reverdy, Paul and Leonard, Naomi E.},
	month = oct,
	year = {2013},
	pages = {494--499},
}

@article{russo_tutorial_2020,
	title = {A {Tutorial} on {Thompson} {Sampling}},
	abstract = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, product recommendation, assortment, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not effective and relations to alternative algorithms.},
	language = {en},
	urldate = {2021-12-10},
	journal = {arXiv:1707.02038 [cs]},
	author = {Russo, Daniel and Van Roy, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng},
	month = jul,
	year = {2020},
	note = {arXiv: 1707.02038},
	keywords = {Computer Science - Machine Learning},
}

@article{russo_learning_2014,
	title = {Learning to {Optimize} via {Posterior} {Sampling}},
	volume = {39},
	issn = {0364-765X, 1526-5471},
	abstract = {This paper considers the use of a simple posterior sampling algorithm to balance between exploration and exploitation when learning to optimize actions such as in multi-armed bandit problems. The algorithm, also known as Thompson Sampling, offers significant advantages over the popular upper confidence bound (UCB) approach, and can be applied to problems with finite or infinite action spaces and complicated relationships among action rewards. We make two theoretical contributions. The first establishes a connection between posterior sampling and UCB algorithms. This result lets us convert regret bounds developed for UCB algorithms into Bayes risk bounds for posterior sampling. Our second theoretical contribution is a Bayes risk bound for posterior sampling that applies broadly and can be specialized to many model classes. This bound depends on a new notion we refer to as the margin dimension, which measures the degree of dependence among action rewards. Compared to UCB algorithm Bayes risk bounds for specific model classes, our general bound matches the best available for linear models and is stronger than the best available for generalized linear models. Further, our analysis provides insight into performance advantages of posterior sampling, which are highlighted through simulation results that demonstrate performance surpassing recently proposed UCB algorithms.},
	language = {en},
	number = {4},
	urldate = {2021-12-10},
	journal = {Mathematics of Operations Research},
	author = {Russo, Daniel and Van Roy, Benjamin},
	month = nov,
	year = {2014},
	pages = {1221--1243},
}

@article{kaufmann_mixture_2021,
	title = {Mixture {Martingales} {Revisited} with {Applications} to {Sequential} {Tests} and {Confidence} {Intervals}},
	abstract = {This paper presents new deviation inequalities that are valid uniformly in time under adaptive sampling in a multi-armed bandit model. The deviations are measured using the Kullback-Leibler divergence in a given one-dimensional exponential family, and may take into account several arms at a time. They are obtained by constructing for each arm a mixture martingale based on a hierarchical prior, and by multiplying those martingales. Our deviation inequalities allow us to analyze stopping rules based on generalized likelihood ratios for a large class of sequential identification problems, and to construct tight confidence intervals for some functions of the means of the arms.},
	urldate = {2021-12-31},
	journal = {Journal of Machine Learning Research},
	author = {Kaufmann, Emilie and Koolen, Wouter M.},
	month = dec,
	year = {2021},
	note = {Publisher: Microtome Publishing},
	keywords = {adaptive sequential testing, best arm identification, mixture methods, multi-armed bandits, test martingales},
}

@article{rigollet_chapter_nodate,
	title = {Chapter 1: {Sub}-{Gaussian} {Random} {Variables}},
	language = {en},
	author = {Rigollet, Philippe},
	pages = {20},
}

@inproceedings{bubeck_pure_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Pure {Exploration} in {Multi}-armed {Bandits} {Problems}},
	isbn = {978-3-642-04414-4},
	abstract = {We consider the framework of stochastic multi-armed bandit problems and study the possibilities and limitations of strategies that perform an online exploration of the arms. The strategies are assessed in terms of their simple regret, a regret notion that captures the fact that exploration is only constrained by the number of available rounds (not necessarily known in advance), in contrast to the case when the cumulative regret is considered and when exploitation needs to be performed at the same time. We believe that this performance criterion is suited to situations when the cost of pulling an arm is expressed in terms of resources rather than rewards. We discuss the links between the simple and the cumulative regret. The main result is that the required exploration{\textendash}exploitation trade-offs are qualitatively different, in view of a general lower bound on the simple regret in terms of the cumulative regret.},
	language = {en},
	booktitle = {Algorithmic {Learning} {Theory}},
	publisher = {Springer},
	author = {Bubeck, S{\'e}bastien and Munos, R{\'e}mi and Stoltz, Gilles},
	editor = {Gavald{\`a}, Ricard and Lugosi, G{\'a}bor and Zeugmann, Thomas and Zilles, Sandra},
	year = {2009},
	pages = {23--37},
}


@inproceedings{antos_active_2008,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Active {Learning} in {Multi}-armed {Bandits}},
	isbn = {978-3-540-87987-9},
	abstract = {In this paper we consider the problem of actively learning the mean values of distributions associated with a finite number of options (arms). The algorithms can select which option to generate the next sample from in order to produce estimates with equally good precision for all the distributions. When an algorithm uses sample means to estimate the unknown values then the optimal solution, assuming full knowledge of the distributions, is to sample each option proportional to its variance. In this paper we propose an incremental algorithm that asymptotically achieves the same loss as an optimal rule. We prove that the excess loss suffered by this algorithm, apart from logarithmic factors, scales as n - 3/2, which we conjecture to be the optimal rate. The performance of the algorithm is illustrated in a simple problem.},
	language = {en},
	booktitle = {Algorithmic {Learning} {Theory}},
	publisher = {Springer},
	author = {Antos, Andr{\'a}s and Grover, Varun and Szepesv{\'a}ri, Csaba},
	editor = {Freund, Yoav and Gy{\"o}rfi, L{\'a}szl{\'o} and Tur{\'a}n, Gy{\"o}rgy and Zeugmann, Thomas},
	year = {2008},
	pages = {287--302},
}

@inproceedings{garivier_optimal_2016,
	title = {Optimal {Best} {Arm} {Identification} with {Fixed} {Confidence}},
	abstract = {We give a complete characterization of the complexity of best-arm identification in one-parameter bandit problems. We prove a new, tight lower bound on the sample complexity. We propose the {\textquoteleft}Track-and-Stop{\textquoteright} strategy, which we prove to be asymptotically optimal. It consists in a new sampling rule (which tracks the optimal proportions of arm draws highlighted by the lower bound) and in a stopping rule named after Chernoff, for which we give a new analysis.},
	language = {en},
	urldate = {2022-01-27},
	booktitle = {Conference on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Garivier, Aur{\'e}lien and Kaufmann, Emilie},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {998--1027},
}

@article{kaufmann_complexity_2016,
	title = {On the {Complexity} of {Best}-{Arm} {Identification} in {Multi}-{Armed} {Bandit} {Models}},
	volume = {17},
	number = {1},
	journal = {Journal of Machine Learning Research},
	author = {Kaufmann, Emilie and Capp{\'e}, Olivier and Garivier, Aur{\'e}lien},
	year = {2016},
	pages = {1--42},
}

@inproceedings{li_breaking_2020,
	title = {Breaking the {Sample} {Size} {Barrier} in {Model}-{Based} {Reinforcement} {Learning} with a {Generative} {Model}},
	volume = {33},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {12861--12872},
}

@inproceedings{russo_eluder_2013,
	title = {Eluder {Dimension} and the {Sample} {Complexity} of {Optimistic} {Exploration}},
	volume = {26},
	urldate = {2022-03-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Russo, Daniel and Van Roy, Benjamin},
	year = {2013},
}

@article{thompson_likelihood_1933,
	title = {On the {Likelihood} that {One} {Probability} {Exceeds} {Another} in {View} of the {Evidence} of {Two} {Samples}},
	volume = {25},
	issn = {0006-3444},
	number = {3-4},
	journal = {Biometrika},
	author = {Thompson, William R},
	month = dec,
	year = {1933},
	pages = {285--294},
}

@article{auer_using_2002,
	title = {Using {Confidence} {Bounds} for {Exploitation}-{Exploration} {Trade}-offs},
	volume = {3},
	journal = {J. Mach. Learn. Res.},
	author = {Auer, Peter},
	year = {2002},
	pages = {397--422},
}

@article{rusmevichientong_linearly_2010,
	title = {Linearly {Parameterized} {Bandits}},
	abstract = {We consider bandit problems involving a large (possibly infinite) collection of arms, in which the expected reward of each arm is a linear function of an \$r\$-dimensional random vector \${\textbackslash}mathbf\{Z\} {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}r\$, where \$r {\textbackslash}geq 2\$. The objective is to minimize the cumulative regret and Bayes risk. When the set of arms corresponds to the unit sphere, we prove that the regret and Bayes risk is of order \${\textbackslash}Theta(r {\textbackslash}sqrt\{T\})\$, by establishing a lower bound for an arbitrary policy, and showing that a matching upper bound is obtained through a policy that alternates between exploration and exploitation phases. The phase-based policy is also shown to be effective if the set of arms satisfies a strong convexity condition. For the case of a general set of arms, we describe a near-optimal policy whose regret and Bayes risk admit upper bounds of the form \$O(r {\textbackslash}sqrt\{T\} {\textbackslash}log{\textasciicircum}\{3/2\} T)\$.},
	urldate = {2022-03-11},
	journal = {arXiv:0812.3465 [cs]},
	author = {Rusmevichientong, Paat and Tsitsiklis, John N.},
	month = feb,
	year = {2010},
	note = {arXiv: 0812.3465},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 40 pages; updated results and references},
}

@inproceedings{dani_stochastic_2008,
	title = {Stochastic {Linear} {Optimization} under {Bandit} {Feedback}},
	booktitle = {{COLT}},
	author = {Dani, Varsha and Hayes, Thomas P. and Kakade, Sham M.},
	year = {2008},
}

@article{slivkins_introduction_2022,
	title = {Introduction to {Multi}-{Armed} {Bandits}},
	abstract = {Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a brief review of the further developments; many of the chapters conclude with exercises.},
	language = {en},
	urldate = {2022-03-11},
	journal = {arXiv:1904.07272 [cs, stat]},
	author = {Slivkins, Aleksandrs},
	month = jan,
	year = {2022},
	note = {arXiv: 1904.07272},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Data Structures and Algorithms},
	annote = {Comment: Published with Foundations and Trends(R) in Machine Learning, November 2019. The present version is a revision of the "Foundations and Trends" publication. It contains numerous edits for presentation and accuracy (based in part on readers' feedback), updated and expanded literature reviews, and some new exercises},
}

@inproceedings{chapelle_empirical_2011,
	title = {An {Empirical} {Evaluation} of {Thompson} {Sampling}},
	booktitle = {{NIPS}},
	author = {Chapelle, Olivier and Li, Lihong},
	year = {2011},
}

@inproceedings{dani_price_2007,
	title = {The {Price} of {Bandit} {Information} for {Online} {Optimization}},
	volume = {20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dani, Varsha and Kakade, Sham M and Hayes, Thomas},
	editor = {Platt, J. and Koller, D. and Singer, Y. and Roweis, S.},
	year = {2007},
}

@inproceedings{kleinberg_multi-armed_2008,
	address = {New York, NY, USA},
	series = {{STOC} '08},
	title = {Multi-{Armed} {Bandits} in {Metric} {Spaces}},
	isbn = {978-1-60558-047-0},
	abstract = {In a multi-armed bandit problem, an online algorithm chooses from a set of strategies in a sequence of \$n\$ trials so as to maximize the total payoff of the chosen strategies. While the performance of bandit algorithms with a small finite strategy set is quite well understood, bandit problems with large strategy sets are still a topic of very active investigation, motivated by practical applications such as online auctions and web advertisement. The goal of such research is to identify broad and natural classes of strategy sets and payoff functions which enable the design of efficient solutions.In this work we study a very general setting for the multi-armed bandit problem in which the strategies form a metric space, and the payoff function satisfies a Lipschitz condition with respect to the metric. We refer to this problem as the "Lipschitz MAB problem". We present a complete solution for the multi-armed problem in this setting. That is, for every metric space (L,X) we define an isometry invariant Max Min COV(X) which bounds from below the performance of Lipschitz MAB algorithms for \$X\$, and we present an algorithm which comes arbitrarily close to meeting this bound. Furthermore, our technique gives even better results for benign payoff functions.},
	booktitle = {Proceedings of the {Fortieth} {Annual} {ACM} {Symposium} on {Theory} of {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Kleinberg, Robert and Slivkins, Aleksandrs and Upfal, Eli},
	year = {2008},
	note = {event-place: Victoria, British Columbia, Canada},
	keywords = {covering dimension, metric spaces, multi-armed bandit problem, online learning},
	pages = {681--690},
}

@article{bubeck_x-armed_2011,
	title = {X-{Armed} {Bandits}},
	volume = {12},
	issn = {1532-4435},
	abstract = {We consider a generalization of stochastic bandits where the set of arms, X, is allowed to be a generic measurable space and the mean-payoff function is "locally Lipschitz" with respect to a dissimilarity function that is known to the decision maker. Under this condition we construct an arm selection policy, called HOO (hierarchical optimistic optimization), with improved regret bounds compared to previous results for a large class of problems. In particular, our results imply that if X is the unit hypercube in a Euclidean space and the mean-payoff function has a finite number of global maxima around which the behavior of the function is locally continuous with a known smoothness degree, then the expected regret of HOO is bounded up to a logarithmic factor by ?n, that is, the rate of growth of the regret is independent of the dimension of the space. We also prove the minimax optimality of our algorithm when the dissimilarity is a metric. Our basic strategy has quadratic computational complexity as a function of the number of time steps and does not rely on the doubling trick. We also introduce a modified strategy, which relies on the doubling trick but runs in linearithmic time. Both results are improvements with respect to previous approaches.},
	number = {null},
	journal = {J. Mach. Learn. Res.},
	author = {Bubeck, S{\'e}bastien and Munos, R{\'e}mi and Stoltz, Gilles and Szepesv{\'a}ri, Csaba},
	month = jul,
	year = {2011},
	note = {Publisher: JMLR.org},
	pages = {1655--1695},
}

@inproceedings{filippi_parametric_2010,
	title = {Parametric {Bandits}: {The} {Generalized} {Linear} {Case}},
	volume = {23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Filippi, Sarah and Cappe, Olivier and Garivier, Aur{\'e}lien and Szepesv{\'a}ri, Csaba},
	editor = {Lafferty, J. and Williams, C. and Shawe-Taylor, J. and Zemel, R. and Culotta, A.},
	year = {2010},
}

@article{robbins_aspects_1952,
	title = {Some aspects of the sequential design of experiments},
	volume = {58},
	number = {5},
	journal = {Bulletin of the American Mathematical Society},
	author = {Robbins, Herbert},
	year = {1952},
	note = {Publisher: American Mathematical Society},
	pages = {527 -- 535},
}

@article{auer_finite-time_2002,
	title = {Finite-{Time} {Analysis} of the {Multiarmed} {Bandit} {Problem}},
	volume = {47},
	issn = {0885-6125},
	abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
	number = {2{\textendash}3},
	journal = {Mach. Learn.},
	author = {Auer, Peter and Cesa-Bianchi, Nicol{\`o} and Fischer, Paul},
	month = may,
	year = {2002},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {adaptive allocation rules, bandit problems, finite horizon regret},
	pages = {235--256},
}

@inproceedings{bubeck_bounded_2013,
	address = {Princeton, NJ, USA},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Bounded regret in stochastic multi-armed bandits},
	volume = {30},
	abstract = {We study the stochastic multi-armed bandit problem when one knows the value $\mu${\textasciicircum}(*) of an optimal arm, as a well as a positive lower bound on the smallest positive gap ?. We propose a new randomized policy that attains a regret uniformly bounded over time in this setting. We also prove several lower bounds, which show in particular that bounded regret is not possible if one only knows ?, and bounded regret of order 1/?is not possible if one only knows $\mu${\textasciicircum}(*).},
	booktitle = {Proceedings of the 26th {Annual} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Bubeck, S{\'e}bastien and Perchet, Vianney and Rigollet, Philippe},
	editor = {Shalev-Shwartz, Shai and Steinwart, Ingo},
	month = jun,
	year = {2013},
	pages = {122--134},
}

@article{liu_regret_2021,
	title = {Regret {Bounds} for {Discounted} {MDPs}},
	abstract = {Reinforcement learning (RL) has traditionally been understood from an episodic perspective; the concept of non-episodic RL, where there is no restart and therefore no reliable recovery, remains elusive. A fundamental question in non-episodic RL is how to measure the performance of a learner and derive algorithms to maximize such performance. Conventional wisdom is to maximize the difference between the average reward received by the learner and the maximal long-term average reward. In this paper, we argue that if the total time budget is relatively limited compared to the complexity of the environment, such comparison may fail to reflect the finite-time optimality of the learner. We propose a family of measures, called $\gamma$-regret, which we believe to better capture the finite-time optimality. We give motivations and derive lower and upper bounds for such measures.},
	language = {en},
	urldate = {2021-12-10},
	journal = {arXiv:2002.05138 [cs, stat]},
	author = {Liu, Shuang and Su, Hao},
	month = may,
	year = {2021},
	note = {arXiv: 2002.05138},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{dong_q-learning_2019,
	title = {Q-learning with {UCB} {Exploration} is {Sample} {Efficient} for {Infinite}-{Horizon} {MDP}},
	abstract = {A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently, Jin et al. {\textbackslash}cite\{jin2018q\} proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards {\textbackslash}emph\{without\} accessing a generative model. We show that the {\textbackslash}textit\{sample complexity of exploration\} of our algorithm is bounded by \${\textbackslash}tilde\{O\}(\{{\textbackslash}frac\{SA\}\{{\textbackslash}epsilon{\textasciicircum}2(1-{\textbackslash}gamma){\textasciicircum}7\}\})\$. This improves the previously best known result of \${\textbackslash}tilde\{O\}(\{{\textbackslash}frac\{SA\}\{{\textbackslash}epsilon{\textasciicircum}4(1-{\textbackslash}gamma){\textasciicircum}8\}\})\$ in this setting achieved by delayed Q-learning {\textbackslash}cite\{strehl2006pac\}, and matches the lower bound in terms of \${\textbackslash}epsilon\$ as well as \$S\$ and \$A\$ except for logarithmic factors.},
	language = {en},
	urldate = {2021-12-10},
	journal = {arXiv:1901.09311 [cs, stat]},
	author = {Dong, Kefan and Wang, Yuanhao and Chen, Xiaoyu and Wang, Liwei},
	month = sep,
	year = {2019},
	note = {arXiv: 1901.09311},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{li_blackwell_2020,
	title = {Blackwell {Online} {Learning} for {Markov} {Decision} {Processes}},
	abstract = {This work provides a novel interpretation of Markov Decision Processes (MDP) from the online optimization viewpoint. In such an online optimization context, the policy of the MDP is viewed as the decision variable while the corresponding value function is treated as payoff feedback from the environment. Based on this interpretation, we construct a Blackwell game induced by MDP, which bridges the gap among regret minimization, Blackwell approachability theory, and learning theory for MDP. Specifically, from the approachability theory, we propose 1) Blackwell value iteration for offline planning and 2) Blackwell Q-learning for online learning in MDP, both of which are shown to converge to the optimal solution. Our theoretical guarantees are corroborated by numerical experiments.},
	language = {en},
	urldate = {2021-12-10},
	journal = {arXiv:2012.14043 [cs]},
	author = {Li, Tao and Peng, Guanze and Zhu, Quanyan},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.14043},
	keywords = {Computer Science - Machine Learning},
}

@article{marjani_navigating_2021,
	title = {Navigating to the {Best} {Policy} in {Markov} {Decision} {Processes}},
	abstract = {We investigate the classical active pure exploration problem in Markov Decision Processes, where the agent sequentially selects actions and, from the resulting system trajectory, aims at identifying the best policy as fast as possible. We propose an information-theoretic lower bound on the average number of steps required before a correct answer can be given with probability at least 1 - $\delta$. This lower bound involves a non-convex optimization problem, for which we propose a convex relaxation. We further provide an algorithm whose sample complexity matches the relaxed lower bound up to a factor 2. This algorithm addresses general communicating MDPs; we propose a variant with reduced exploration rate (and hence faster convergence) under an additional ergodicity assumption. This work extends previous results relative to the generative setting [MP20], where the agent could at each step observe the random outcome of any (state, action) pair. In contrast, we show here how to deal with the navigation constraints. Our analysis relies on an ergodic theorem for non-homogeneous Markov chains which we consider of wide interest in the analysis of Markov Decision Processes.},
	language = {en},
	urldate = {2021-12-10},
	journal = {arXiv:2106.02847 [cs, stat]},
	author = {Marjani, Aymen Al and Garivier, Aur{\'e}lien and Proutiere, Alexandre},
	month = oct,
	year = {2021},
	note = {arXiv: 2106.02847},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{jin_is_2018,
	title = {Is {Q}-{Learning} {Provably} {Efficient}?},
	volume = {December 2018},
	language = {en},
	journal = {Advances in Neural Information Processing Systems},
	author = {Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
	month = dec,
	year = {2018},
	pages = {11},
}

@article{osband_more_2013,
	title = {({More}) {Efficient} {Reinforcement} {Learning} via {Posterior} {Sampling}},
	abstract = {Most provably-efficient learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for efficient exploration, posterior sampling for reinforcement learning (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally efficient and allows an agent to encode prior knowledge in a natural way. We establish an \${\textbackslash}tilde\{O\}({\textbackslash}tau S {\textbackslash}sqrt\{AT\})\$ bound on the expected regret, where \$T\$ is time, \${\textbackslash}tau\$ is the episode length and \$S\$ and \$A\$ are the cardinalities of the state and action spaces. This bound is one of the first for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL significantly outperforms existing algorithms with similar regret bounds.},
	language = {en},
	urldate = {2021-12-10},
	journal = {arXiv:1306.0940 [cs, stat]},
	author = {Osband, Ian and Russo, Daniel and Van Roy, Benjamin},
	month = dec,
	year = {2013},
	note = {arXiv: 1306.0940},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 10 pages},
}

@article{ipsen_uniform_1994,
	title = {Uniform {Stability} of {Markov} {Chains}},
	volume = {15},
	issn = {0895-4798, 1095-7162},
	abstract = {By deriving a new set of tight perturbation bounds, it is shown that all stationary probabilities of a finite irreducible Markov chain react essentially in the same way to perturbations in the transition probabilities. In particular, if at least one stationary probability is insensitive in a relative sense, then all stationary probabilities must be insensitive in an absolute sense. New measures of sensitivity are related to more traditional ones, and it is shown that all relevant condition numbers for the Markov chain problem are small multiples of each other. Finally, the implications of these findings to the computation of stationary probabilities by direct methods are discussed, and the results are applied to stability issues in nearly transient chains.},
	language = {en},
	number = {4},
	urldate = {2021-12-10},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Ipsen, Ilse C. F. and Meyer, Carl D.},
	month = oct,
	year = {1994},
	pages = {1061--1074},
}

@book{puterman_markov_1994,
	edition = {1},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Markov {Decision} {Processes}: {Discrete} {Stochastic} {Dynamic} {Programming}},
	isbn = {978-0-471-61977-2 978-0-470-31688-7},
	shorttitle = {Markov {Decision} {Processes}},
	language = {en},
	urldate = {2021-12-15},
	publisher = {Wiley},
	author = {Puterman, Martin L.},
	month = apr,
	year = {1994},
}

@article{cochet-terrasson_numerical_1998,
	title = {Numerical {Computation} of {Spectral} {Elements} in {Max}-{Plus} {Algebra} *},
	volume = {31},
	issn = {14746670},
	abstract = {We describe the specialization to max-plus algebra of Howard{\textquoteright}s policy improvement scheme, which yields an algorithm to compute the solutions of spectral problems in the max-plus semiring. Experimentally, the algorithm shows a remarkable (almost linear) average execution time.},
	language = {en},
	number = {18},
	urldate = {2021-12-17},
	journal = {IFAC Proceedings Volumes},
	author = {Cochet-Terrasson, Jean and Cohen, Guy and Gaubert, St{\'e}phane and McGettrick, Michael and Quadrat, Jean-Pierre},
	month = jul,
	year = {1998},
	pages = {667--674},
}

@article{dasdan_faster_1998,
	title = {Faster maximum and minimum mean cycle algorithms for system-performance analysis},
	volume = {17},
	issn = {02780070},
	abstract = {Maximum and minimum mean cycle problems are important problems with many applications in performance analysis of synchronous and asynchronous digital systems including rate analysis of embedded systems, in discrete-event systems, and in graph theory. Karp's algorithm is one of the fastest and commonest algorithms for both of these problems. We present this paper mainly in the context of the maximum mean cycle problem. We show that Karp's algorithm processes more vertices and arcs than needed to nd the maximum cycle mean of a digraph. This observation motivated us to propose a new graph unfolding scheme that remedies this de ciency and leads to three faster algorithms with di erent characteristics. Asymptotic analysis tells us that our algorithms always run faster than Karp's algorithm. Experiments on benchmark graphs con rm this fact for most of the graphs. Like Karp's algorithm, they are also applicable to both the maximum and minimum mean cycle problems. Moreover, one of them is among the fastest to date.},
	language = {en},
	number = {10},
	urldate = {2021-12-17},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Dasdan, A. and Gupta, R.K.},
	month = oct,
	year = {1998},
	pages = {889--899},
}

@article{cuninghame-green_maximum_1996,
	title = {Maximum cycle-means of weighted digraphs},
	volume = {11},
	issn = {1005-1031, 1993-0445},
	language = {en},
	number = {2},
	urldate = {2021-12-17},
	journal = {Applied Mathematics},
	author = {Cuninghame-Green, R. A. and Lin, Yixun},
	month = jun,
	year = {1996},
	pages = {225--234},
}

@article{karp_characterization_1978,
	title = {A characterization of the minimum cycle mean in a digraph},
	volume = {23},
	issn = {0012-365X},
	abstract = {Let C = (V,E) be a digraph with n vertices. Let f be a function from E into the real numbers, associating with each edge e ? E a weight?(e). Given any sequence of edges $\sigma$ = e1,e2,{\textellipsis},ep define w($\sigma$), the weight of $\sigma$, as ?i = 1p ?(ei), and define m($\sigma$), the mean weight of $\sigma$, as w($\sigma$)?p. Let $\lambda$* = minCm(C) where C ranges over all directed cycles in G; $\lambda$* is called the minimum cycle mean. We give a simple characterization of $\lambda$*, as well as an algorithm for computing it efficiently.},
	language = {en},
	number = {3},
	urldate = {2021-12-17},
	journal = {Discrete Mathematics},
	author = {Karp, Richard M.},
	month = jan,
	year = {1978},
	pages = {309--311},
}

@inproceedings{auer_near-optimal_2009,
	title = {Near-optimal {Regret} {Bounds} for {Reinforcement} {Learning}},
	volume = {21},
	urldate = {2022-01-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
	year = {2009},
}

@article{wei_breaking_nodate,
	title = {Breaking the {Sample} {Size} {Barrier} in   {Model}-{Based} {Reinforcement} {Learning}},
	language = {en},
	author = {Wei, Yuting},
	pages = {78},
}

@article{li_breaking_2021,
	title = {Breaking the {Sample} {Size} {Barrier} in {Model}-{Based} {Reinforcement} {Learning} with a {Generative} {Model}},
	abstract = {This paper is concerned with the sample efficiency of reinforcement learning, assuming access to a generative model (or simulator). We first consider \${\textbackslash}gamma\$-discounted infinite-horizon Markov decision processes (MDPs) with state space \${\textbackslash}mathcal\{S\}\$ and action space \${\textbackslash}mathcal\{A\}\$. Despite a number of prior works tackling this problem, a complete picture of the trade-offs between sample complexity and statistical accuracy is yet to be determined. In particular, all prior results suffer from a severe sample size barrier, in the sense that their claimed statistical guarantees hold only when the sample size exceeds at least \${\textbackslash}frac\{{\textbar}{\textbackslash}mathcal\{S\}{\textbar}{\textbar}{\textbackslash}mathcal\{A\}{\textbar}\}\{(1-{\textbackslash}gamma){\textasciicircum}2\}\$. The current paper overcomes this barrier by certifying the minimax optimality of two algorithms -- a perturbed model-based algorithm and a conservative model-based algorithm -- as soon as the sample size exceeds the order of \${\textbackslash}frac\{{\textbar}{\textbackslash}mathcal\{S\}{\textbar}{\textbar}{\textbackslash}mathcal\{A\}{\textbar}\}\{1-{\textbackslash}gamma\}\$ (modulo some log factor). Moving beyond infinite-horizon MDPs, we further study time-inhomogeneous finite-horizon MDPs, and prove that a plain model-based planning algorithm suffices to achieve minimax-optimal sample complexity given any target accuracy level. To the best of our knowledge, this work delivers the first minimax-optimal guarantees that accommodate the entire range of sample sizes (beyond which finding a meaningful policy is information theoretically infeasible).},
	language = {en},
	urldate = {2022-01-19},
	journal = {arXiv:2005.12900 [cs, math, stat]},
	author = {Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
	month = dec,
	year = {2021},
	note = {arXiv: 2005.12900},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control, Mathematics - Statistics Theory, Computer Science - Information Theory},
	annote = {Comment: add a new conservative model-based algorithm, and add new results for finite-horizon MDPs},
}

@inproceedings{mahadevan_average-reward_1996,
	series = {{AAAI}'96},
	title = {An {Average}-{Reward} {Reinforcement} {Learning} {Algorithm} for {Computing} {Bias}-{Optimal} {Policies}},
	isbn = {0-262-51091-X},
	abstract = {Average-reward reinforcement learning (ARL) is an undiscounted optimality framework that is generally applicable to a broad range of control tasks. ARL computes gain-optimal control policies that maximize the expected payoff per step. However, gainoptimality has some intrinsic limitations as an optimality criterion, since for example, it cannot distinguish between different policies that all reach an absorbing goal state, but incur varying costs. A more selective criterion is bias optimality, which can filter gain-optimal policies to select those that reach absorbing goals with the minimum cost. While several ARL algorithms for computing gain-optimal policies have been proposed, none of these algorithms can guarantee bias optimality, since this requires solving at least two nested optimality equations. In this paper, we describe a novel model-based ARL algorithm for computing bias-optimal policies. We test the proposed algorithm using an admission control queuing system, and show that it is able to utilize the queue much more efficiently than a gain-optimal method by learning bias-optimal policies.},
	booktitle = {Proceedings of the {Thirteenth} {National} {Conference} on {Artificial} {Intelligence} - {Volume} 1},
	publisher = {AAAI Press},
	author = {Mahadevan, Sridhar},
	year = {1996},
	note = {event-place: Portland, Oregon},
	pages = {875--880},
}

@inproceedings{agarwal_model-based_2020,
	title = {Model-{Based} {Reinforcement} {Learning} with a {Generative} {Model} is {Minimax} {Optimal}},
	abstract = {This work considers the sample and computational complexity of obtaining an ??$\epsilon${\textbackslash}epsilon-optimal policy in a discounted Markov Decision Process (MDP), given only access to a generative model. In this model, the learner accesses the underlying transition model via a sampling oracle that provides a sample of the next state, when given any state-action pair as input. We are interested in a basic and unresolved question in model based planning: is this na{\"i}ve {\textquotedblleft}plug-in{\textquotedblright} approach {\textemdash} where we build the maximum likelihood estimate of the transition model in the MDP from observations and then find an optimal policy in this empirical MDP {\textemdash} non-asymptotically, minimax optimal? Our main result answers this question positively. With regards to computation, our result provides a simpler approach towards minimax optimal planning: in comparison to prior model-free results,  we show that using {\textbackslash}emph\{any\} high accuracy, black-box planning oracle in the empirical model suffices to obtain the minimax error rate. The key proof technique uses a leave-one-out analysis, in a novel {\textquotedblleft}absorbing MDP{\textquotedblright} construction, to decouple the statistical dependency issues that arise in the analysis of model-based planning; this construction may be helpful more generally.},
	language = {en},
	urldate = {2022-01-27},
	booktitle = {Proceedings of {Thirty} {Third} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Agarwal, Alekh and Kakade, Sham and Yang, Lin F.},
	month = jul,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {67--83},
}

@inproceedings{fiechter_efficient_1994,
	address = {New York, NY, USA},
	series = {{COLT} '94},
	title = {Efficient reinforcement learning},
	isbn = {978-0-89791-655-4},
	abstract = {In this paper we propose a new formal model for studying reinforcement learning, based on Valiant's PAC framework. In our model the learner does not have direct access to every state of the environment. Instead, every sequence of experiments starts in a fixed initial state and the learner is provided with a {\textquotedblleft}reset{\textquotedblright} operation that interrupts the current sequence of experiments and starts a new one (from the initial state). We do not require the agent to learn the optimal policy but only a good approximation of it with high probability. More precisely, we require the learner to produce a policy whose expected value from the initial state is $\varepsilon$-close to that of the optimal policy, with probability no less than 1-$\delta$. For this model, we describe an algorithm that produces such an ($\varepsilon$,$\delta$)-optimal policy for any environment, in time polynomial in N,K,1/$\varepsilon$,1/$\delta$,1/(1-$\beta$) and rmax, where N is the number of states of the environment, K is the maximum number of actions in a state, $\beta$ is the discount factor and rmax is the maximum reward on any transition.},
	urldate = {2022-01-27},
	booktitle = {Proceedings of the seventh annual conference on {Computational} learning theory},
	publisher = {Association for Computing Machinery},
	author = {Fiechter, Claude-Nicolas},
	month = jul,
	year = {1994},
	pages = {88--97},
}

@inproceedings{gabillon_multi-bandit_2011,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'11},
	title = {Multi-bandit best arm identification},
	isbn = {978-1-61839-599-3},
	abstract = {We study the problem of identifying the best arm in each of the bandits in a multi-bandit multi-armed setting. We first propose an algorithm called Gap-based Exploration (GapE) that focuses on the arms whose mean is close to the mean of the best arm in the same bandit (i.e., small gap). We then introduce an algorithm, called GapE-V, which takes into account the variance of the arms in addition to their gap. We prove an upper-bound on the probability of error for both algorithms. Since GapE and GapE-V need to tune an exploration parameter that depends on the complexity of the problem, which is often unknown in advance, we also introduce variations of these algorithms that estimate this complexity online. Finally, we evaluate the performance of these algorithms and compare them to other allocation strategies on a number of synthetic problems.},
	urldate = {2022-01-27},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro and Bubeck, S{\'e}bastien},
	month = dec,
	year = {2011},
	pages = {2222--2230},
}

@inproceedings{marjani_adaptive_2021,
	title = {Adaptive {Sampling} for {Best} {Policy} {Identification} in {Markov} {Decision} {Processes}},
	abstract = {We investigate the problem of best-policy identification in discounted Markov Decision Processes (MDPs) when the learner has access to a generative model. The objective is to devise a learning algorithm returning the best policy as early as possible. We first derive a problem-specific lower bound of the sample complexity satisfied by any learning algorithm. This lower bound corresponds to an optimal sample allocation that solves a non-convex program, and hence, is hard to exploit in the design of efficient algorithms. We then provide a simple and tight upper bound of the sample complexity lower bound, whose corresponding nearly-optimal sample allocation becomes explicit. The upper bound depends on specific functionals of the MDP such as the sub-optimality gaps and the variance of the next-state value function, and thus really captures the hardness of the MDP. Finally, we devise KLB-TS (KL Ball Track-and-Stop), an algorithm tracking this nearly-optimal allocation, and provide asymptotic guarantees for its sample complexity (both almost surely and in expectation). The advantages of KLB-TS against state-of-the-art algorithms are discussed and illustrated numerically.},
	language = {en},
	urldate = {2022-01-27},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Marjani, Aymen Al and Proutiere, Alexandre},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {7459--7468},
}
@article{theocharous2017posterior,
  title={Posterior Sampling for Large Scale Reinforcement Learning},
  author={Theocharous, Georgios and Wen, Zheng and Abbasi-Yadkori, Yasin and Vlassis, Nikos},
  journal={arXiv preprint arXiv:1711.07979},
  year={2017}
}



@article{ortner_online_2010,
	title = {Online regret bounds for {Markov} decision processes with deterministic transitions},
	volume = {411},
	issn = {0304-3975},
	abstract = {We consider an upper confidence bound algorithm for learning in Markov decision processes with deterministic transitions. For this algorithm we derive upper bounds on the online regret with respect to an ($\varepsilon$-)optimal policy that are logarithmic in the number of steps taken. We also present a corresponding lower bound. As an application, multi-armed bandits with switching cost are considered.},
	number = {29},
	journal = {Theoretical Computer Science},
	author = {Ortner, Ronald},
	year = {2010},
	keywords = {Labeled digraph, Markov decision process, Regret},
	pages = {2684--2695},
	annote = {Algorithmic Learning Theory (ALT 2008)},
}

@article{shah_non-asymptotic_2020,
	title = {Non-{Asymptotic} {Analysis} of {Monte} {Carlo} {Tree} {Search}},
	volume = {48},
	issn = {0163-5999},
	abstract = {In this work, we consider the popular tree-based search strategy within the framework of reinforcement learning, the Monte Carlo Tree Search (MCTS), in the context of infinite-horizon discounted cost Markov Decision Process (MDP) with deterministic transitions. While MCTS is believed to provide an approximate value function for a given state with enough simulations, cf. [5, 6], the claimed proof of this property is incomplete. This is due to the fact that the variant of MCTS, the Upper Confidence Bound for Trees (UCT), analyzed in prior works utilizes "logarithmic" bonus term for balancing exploration and exploitation within the tree-based search, following the insights from stochastic multi-arm bandit (MAB) literature, cf. [1, 3]. In effect, such an approach assumes that the regret of the underlying recursively dependent non-stationary MABs concentrates around their mean exponentially in the number of steps, which is unlikely to hold as pointed out in [2], even for stationary MABs.},
	number = {1},
	urldate = {2022-01-27},
	journal = {ACM SIGMETRICS Performance Evaluation Review},
	author = {Shah, Devavrat and Xie, Qiaomin and Xu, Zhi},
	month = jul,
	year = {2020},
	pages = {31--32},
}

@inproceedings{kaufmann_monte-carlo_2017,
	title = {Monte-{Carlo} {Tree} {Search} by {Best} {Arm} {Identification}},
	volume = {30},
	urldate = {2022-03-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kaufmann, Emilie and Koolen, Wouter M},
	year = {2017},
}

@article{li_towards_2006,
	title = {Towards a {Unified} {Theory} of {State} {Abstraction} for {MDPs}},
	abstract = {State abstraction (or state aggregation) has been extensively studied in the fields of artificial intelligence and operations research. Instead of working in the ground state space, the decision maker usually finds solutions in the abstract state space much faster by treating groups of states as a unit by ignoring irrelevant state information. A number of abstractions have been proposed and studied in the reinforcement-learning and planning literatures, and positive and negative results are known. We provide a unified treatment of state abstraction for Markov decision processes. We study five particular abstraction schemes, some of which have been proposed in the past in different forms, and analyze their usability for planning and learning.},
	language = {en},
	journal = {In Proceedings of the Ninth International Symposium on Artificial Intelligence and Mathematics},
	author = {Li, Lihong and Walsh, Thomas J and Littman, Michael L},
	year = {2006},
	pages = {531--539},
}

@inproceedings{bartlett_regal_2009,
	address = {Arlington, Virginia, USA},
	series = {{UAI} '09},
	title = {{REGAL}: a regularization based algorithm for reinforcement learning in weakly communicating {MDPs}},
	isbn = {978-0-9749039-5-8},
	shorttitle = {{REGAL}},
	abstract = {We provide an algorithm that achieves the optimal regret rate in an unknown weakly communicating Markov Decision Process (MDP). The algorithm proceeds in episodes where, in each episode, it picks a policy using regularization based on the span of the optimal bias vector. For an MDP with S states and A actions whose optimal bias vector has span bounded by H, we show a regret bound of {\~O}(HS?AT). We also relate the span to various diameter-like quantities associated with the MDP, demonstrating how our results improve on previous regret bounds.},
	urldate = {2022-03-08},
	booktitle = {Proceedings of the {Twenty}-{Fifth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {AUAI Press},
	author = {Bartlett, Peter L. and Tewari, Ambuj},
	month = jun,
	year = {2009},
	pages = {35--42},
}

@inproceedings{azar_minimax_2017,
	title = {Minimax {Regret} {Bounds} for {Reinforcement} {Learning}},
	abstract = {We consider the problem of provably optimal exploration in reinforcement learning for finite horizon MDPs. We show that an optimistic modification to value iteration achieves a regret bound of ???~(????????-------?+??2??2??+????--?)O{\textasciitilde}(HSAT+H2S2A+HT){\textbackslash}tilde \{O\}( {\textbackslash}sqrt\{HSAT\} + H{\textasciicircum}2S{\textasciicircum}2A+H{\textbackslash}sqrt\{T\}) where ??HH is the time horizon, ??SS the number of states, ??AA the number of actions and ??TT the number of time-steps. This result improves over the best previous known bound ???~(????????---?)O{\textasciitilde}(HSAT){\textbackslash}tilde \{O\}(HS {\textbackslash}sqrt\{AT\}) achieved by the UCRL2 algorithm. The key significance of our new results is that when ??>=??3??3??T>=H3S3AT{\textbackslash}geq H{\textasciicircum}3S{\textasciicircum}3A and ????>=??SA>=HSA{\textbackslash}geq H, it leads to a regret of ???~(????????-------?)O{\textasciitilde}(HSAT){\textbackslash}tilde\{O\}({\textbackslash}sqrt\{HSAT\}) that matches the established lower bound of $\Omega$(????????-------?)$\Omega$(HSAT){\textbackslash}Omega({\textbackslash}sqrt\{HSAT\}) up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in ??SS), and we define Bernstein-based {\textquotedblleft}exploration bonuses{\textquotedblright} that use the empirical variance of the estimated values at the next states (to improve scaling in ??HH).},
	language = {en},
	urldate = {2022-03-08},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {263--272},
}

@article{auer_logarithmic_2006,
	title = {Logarithmic {Online} {Regret} {Bounds} for {Undiscounted} {Reinforcement} {Learning}},
	abstract = {We present a learning algorithm for undiscounted reinforcement learning. Our interest lies in bounds for the algorithm's online performance after some finite number of steps. In the spirit of similar methods already successfully applied for the exploration-exploitation tradeoff in multi-armed bandit problems, we use upper confidence bounds to show that our UCRL algorithm achieves logarithmic online regret in the number of steps taken with respect to an optimal policy.},
	language = {en},
	urldate = {2022-03-08},
	journal = {Proceedings of the 19th International Conference on Neural Information Processing Systems},
	author = {Auer, Peter and Ortner, Ronald},
	month = dec,
	year = {2006},
}

@article{strehl_analysis_2008,
	series = {Learning {Theory} 2005},
	title = {An analysis of model-based {Interval} {Estimation} for {Markov} {Decision} {Processes}},
	volume = {74},
	issn = {0022-0000},
	abstract = {Several algorithms for learning near-optimal policies in Markov Decision Processes have been analyzed and proven efficient. Empirical results have suggested that Model-based Interval Estimation (MBIE) learns efficiently in practice, effectively balancing exploration and exploitation. This paper presents a theoretical analysis of MBIE and a new variation called MBIE-EB, proving their efficiency even under worst-case conditions. The paper also introduces a new performance metric, average loss, and relates it to its less {\textquotedblleft}online{\textquotedblright} cousins from the literature.},
	language = {en},
	number = {8},
	urldate = {2022-03-08},
	journal = {Journal of Computer and System Sciences},
	author = {Strehl, Alexander L. and Littman, Michael L.},
	month = dec,
	year = {2008},
	keywords = {Learning theory, Markov Decision Processes, Reinforcement learning},
	pages = {1309--1331},
}

@inproceedings{strehl_pac_2006,
	address = {New York, NY, USA},
	series = {{ICML} '06},
	title = {{PAC} model-free reinforcement learning},
	isbn = {978-1-59593-383-6},
	abstract = {For a Markov Decision Process with finite state (size S) and action spaces (size A per state), we propose a new algorithm---Delayed Q-Learning. We prove it is PAC, achieving near optimal performance except for {\~O}(SA) timesteps using O(SA) space, improving on the {\~O}(S2 A) bounds of best previous algorithms. This result proves efficient reinforcement learning is possible without learning a model of the MDP from experience. Learning takes place from a single continuous thread of experience---no resets nor parallel sampling is used. Beyond its smaller storage and experience requirements, Delayed Q-learning's per-experience computation cost is much less than that of previous PAC algorithms.},
	urldate = {2022-03-08},
	booktitle = {Proceedings of the 23rd international conference on {Machine} learning},
	publisher = {Association for Computing Machinery},
	author = {Strehl, Alexander L. and Li, Lihong and Wiewiora, Eric and Langford, John and Littman, Michael L.},
	month = jun,
	year = {2006},
	pages = {881--888},
}

@article{kearns_near-optimal_2002,
	title = {Near-{Optimal} {Reinforcement} {Learning} in {Polynomial} {Time}},
	volume = {49},
	abstract = {We present new algorithms for reinforcement learning and prove that they have polynomial bounds on the resources required to achieve near-optimal return in general Markov decision processes. After observing that the number of actions required to approach the optimal return is lower bounded by the mixing time T of the optimal policy (in the undiscounted case) or by the horizon time T (in the discounted case), we then give algorithms requiring a number of actions and total computation time that are only polynomial in T and the number of states and actions, for both the undiscounted and discounted cases. An interesting aspect of our algorithms is their explicit handling of the Exploration-Exploitation trade-off.},
	language = {en},
	journal = {Machine Learning},
	author = {Kearns, Michael},
	month = nov,
	year = {2002},
	pages = {209,232},
}

@article{brafman_r-max_2002,
	title = {R-max {\textendash} {A} {General} {Polynomial} {Time} {Algorithm} for {Near}-{Optimal} {Reinforcement} {Learning}},
	volume = {3},
	abstract = {R-max is a very simple model-based reinforcement learning algorithm which can attain near-optimal average reward in polynomial time. In R-max, the agent always maintains a complete, but possibly inaccurate model of its environment and acts based on the optimal policy derived from this model. The model is initialized in an optimistic fashion: all actions in all states return the maximal possible reward (hence the name). During execution, it is updated based on the agent{\textquoteright}s observations. R-max improves upon several previous algorithms: (1) It is simpler and more general than Kearns and Singh{\textquoteright}s E3 algorithm, covering zero-sum stochastic games. (2) It has a built-in mechanism for resolving the exploration vs. exploitation dilemma. (3) It formally justifies the {\textquotedblleft}optimism under uncertainty{\textquotedblright} bias used in many RL algorithms. (4) It is simpler, more general, and more efficient than Brafman and Tennenholtz{\textquoteright}s LSG algorithm for learning in single controller stochastic games. (5) It generalizes the algorithm by Monderer and Tennenholtz for learning in repeated games. (6) It is the only algorithm for learning in repeated games, to date, which is provably efficient, considerably improving and simplifying previous algorithms by Banos and by Megiddo.},
	language = {en},
	journal = {Journal of Machine Learning Research},
	author = {Brafman, Ronen I and Tennenholtz, Moshe},
	year = {2002},
	pages = {212--231},
}

@article{dann_unifying_2018,
	title = {Unifying {PAC} and {Regret}: {Uniform} {PAC} {Bounds} for {Episodic} {Reinforcement} {Learning}},
	shorttitle = {Unifying {PAC} and {Regret}},
	abstract = {Statistical performance bounds for reinforcement learning (RL) algorithms can be critical for high-stakes applications like healthcare. This paper introduces a new framework for theoretically measuring the performance of such algorithms called Uniform-PAC, which is a strengthening of the classical Probably Approximately Correct (PAC) framework. In contrast to the PAC framework, the uniform version may be used to derive high probability regret guarantees and so forms a bridge between the two setups that has been missing in the literature. We demonstrate the benefits of the new framework for finite-state episodic MDPs with a new algorithm that is Uniform-PAC and simultaneously achieves optimal regret and PAC guarantees except for a factor of the horizon.},
	urldate = {2022-03-08},
	journal = {arXiv:1703.07710 [cs, stat]},
	author = {Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
	month = jan,
	year = {2018},
	note = {arXiv: 1703.07710},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: appears in Neural Information Processing Systems 2017},
}

@article{agrawal_posterior_2020,
	title = {Posterior sampling for reinforcement learning: worst-case regret bounds},
	shorttitle = {Posterior sampling for reinforcement learning},
	abstract = {We present an algorithm based on posterior sampling (aka Thompson sampling) that achieves near-optimal worst-case regret bounds when the underlying Markov Decision Process (MDP) is communicating with a finite, though unknown, diameter. Our main result is a high probability regret upper bound of \${\textbackslash}tilde\{O\}(DS{\textbackslash}sqrt\{AT\})\$ for any communicating MDP with \$S\$ states, \$A\$ actions and diameter \$D\$. Here, regret compares the total reward achieved by the algorithm to the total expected reward of an optimal infinite-horizon undiscounted average reward policy, in time horizon \$T\$. This result closely matches the known lower bound of \${\textbackslash}Omega({\textbackslash}sqrt\{DSAT\})\$. Our techniques involve proving some novel results about the anti-concentration of Dirichlet distribution, which may be of independent interest.},
	urldate = {2022-03-08},
	journal = {arXiv:1705.07041 [cs]},
	author = {Agrawal, Shipra and Jia, Randy},
	month = mar,
	year = {2020},
	note = {arXiv: 1705.07041},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: This revision fixes an error due to use of some incorrect results (Lemma C.1 and Lemma C.2) in the earlier version. The regret bounds in this version are worse by a factor of sqrt(S) as compared to the previous version},
}

@article{dann_sample_2016,
	title = {Sample {Complexity} of {Episodic} {Fixed}-{Horizon} {Reinforcement} {Learning}},
	abstract = {Recently, there has been significant progress in understanding reinforcement learning in discounted infinite-horizon Markov decision processes (MDPs) by deriving tight sample complexity bounds. However, in many real-world applications, an interactive learning agent operates for a fixed or bounded period of time, for example tutoring students for exams or handling customer service requests. Such scenarios can often be better treated as episodic fixed-horizon MDPs, for which only looser bounds on the sample complexity exist. A natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability (PAC guarantee). In this paper, we derive an upper PAC bound \${\textbackslash}tilde O({\textbackslash}frac\{{\textbar}{\textbackslash}mathcal S{\textbar}{\textasciicircum}2 {\textbar}{\textbackslash}mathcal A{\textbar} H{\textasciicircum}2\}\{{\textbackslash}epsilon{\textasciicircum}2\} {\textbackslash}ln{\textbackslash}frac 1 {\textbackslash}delta)\$ and a lower PAC bound \${\textbackslash}tilde {\textbackslash}Omega({\textbackslash}frac\{{\textbar}{\textbackslash}mathcal S{\textbar} {\textbar}{\textbackslash}mathcal A{\textbar} H{\textasciicircum}2\}\{{\textbackslash}epsilon{\textasciicircum}2\} {\textbackslash}ln {\textbackslash}frac 1 \{{\textbackslash}delta + c\})\$ that match up to log-terms and an additional linear dependency on the number of states \${\textbar}{\textbackslash}mathcal S{\textbar}\$. The lower bound is the first of its kind for this setting. Our upper bound leverages Bernstein's inequality to improve on previous bounds for episodic finite-horizon MDPs which have a time-horizon dependency of at least \$H{\textasciicircum}3\$.},
	urldate = {2022-03-08},
	journal = {arXiv:1510.08906 [cs, stat]},
	author = {Dann, Christoph and Brunskill, Emma},
	month = may,
	year = {2016},
	note = {arXiv: 1510.08906},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: 28 pages, appeared in Neural Information Processing Systems (NIPS) 2015, updated version with fixed typos and modified Lemma 1 and Lemma C.5},
}

@article{filippi_optimism_2010,
	title = {Optimism in {Reinforcement} {Learning} and {Kullback}-{Leibler} {Divergence}},
	abstract = {We consider model-based reinforcement learning in finite Markov Decision Processes (MDPs), focussing on so-called optimistic strategies. In MDPs, optimism can be implemented by carrying out extended value iterations under a constraint of consistency with the estimated model transition probabilities. The UCRL2 algorithm by Auer, Jaksch and Ortner (2009), which follows this strategy, has recently been shown to guarantee near-optimal regret bounds. In this paper, we strongly argue in favor of using the Kullback-Leibler (KL) divergence for this purpose. By studying the linear maximization problem under KL constraints, we provide an efficient algorithm, termed KL-UCRL, for solving KL-optimistic extended value iteration. Using recent deviation bounds on the KL divergence, we prove that KL-UCRL provides the same guarantees as UCRL2 in terms of regret. However, numerical experiments on classical benchmarks show a significantly improved behavior, particularly when the MDP has reduced connectivity. To support this observation, we provide elements of comparison between the two algorithms based on geometric considerations.},
	language = {en},
	urldate = {2022-03-08},
	journal = {2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
	author = {Filippi, Sarah and Capp{\'e}, Olivier and Garivier, Aur{\'e}lien},
	month = sep,
	year = {2010},
	note = {arXiv: 1004.5229},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Statistics Theory},
	pages = {115--122},
	annote = {Comment: This work has been accepted and presented at ALLERTON 2010; Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on, Monticello (Illinois) : {\textbackslash}'Etats-Unis (2010)},
}

@article{jin_provably_2019,
	title = {Provably {Efficient} {Reinforcement} {Learning} with {Linear} {Function} {Approximation}},
	abstract = {Modern Reinforcement Learning (RL) is commonly applied to practical problems with an enormous number of states, where function approximation must be deployed to approximate either the value function or the policy. The introduction of function approximation raises a fundamental set of challenges involving computational and statistical efficiency, especially given the need to manage the exploration/exploitation tradeoff. As a result, a core RL question remains open: how can we design provably efficient RL algorithms that incorporate function approximation? This question persists even in a basic setting with linear dynamics and linear rewards, for which only linear function approximation is needed. This paper presents the first provable RL algorithm with both polynomial runtime and polynomial sample complexity in this linear setting, without requiring a "simulator" or additional assumptions. Concretely, we prove that an optimistic modification of Least-Squares Value Iteration (LSVI)---a classical algorithm frequently studied in the linear setting---achieves \${\textbackslash}tilde\{{\textbackslash}mathcal\{O\}\}({\textbackslash}sqrt\{d{\textasciicircum}3H{\textasciicircum}3T\})\$ regret, where \$d\$ is the ambient dimension of feature space, \$H\$ is the length of each episode, and \$T\$ is the total number of steps. Importantly, such regret is independent of the number of states and actions.},
	language = {en},
	urldate = {2022-03-08},
	journal = {arXiv:1907.05388 [cs, math, stat]},
	author = {Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I.},
	month = aug,
	year = {2019},
	note = {arXiv: 1907.05388},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
}

@article{gheshlaghi_azar_minimax_2013,
	title = {Minimax {PAC} bounds on the sample complexity of reinforcement learning with a generative model},
	volume = {91},
	issn = {1573-0565},
	abstract = {We consider the problems of learning the optimal action-value function and the optimal policy in discounted-reward Markov decision processes (MDPs). We prove new PAC bounds on the sample-complexity of two well-known model-based reinforcement learning (RL) algorithms in the presence of a generative model of the MDP: value iteration and policy iteration. The first result indicates that for an MDP with N state-action pairs and the discount factor $\gamma$?[0,1) only O(Nlog(N/$\delta$)/((1-$\gamma$)3$\varepsilon$2)) state-transition samples are required to find an $\varepsilon$-optimal estimation of the action-value function with the probability (w.p.) 1-$\delta$. Further, we prove that, for small values of $\varepsilon$, an order of O(Nlog(N/$\delta$)/((1-$\gamma$)3$\varepsilon$2)) samples is required to find an $\varepsilon$-optimal policy w.p. 1-$\delta$. We also prove a matching lower bound of $\Theta$(Nlog(N/$\delta$)/((1-$\gamma$)3$\varepsilon$2)) on the sample complexity of estimating the optimal action-value function with $\varepsilon$ accuracy. To the best of our knowledge, this is the first minimax result on the sample complexity of RL: the upper bounds match the lower bound in terms of~N, $\varepsilon$, $\delta$ and 1/(1-$\gamma$) up to a constant factor. Also, both our lower bound and upper bound improve on the state-of-the-art in terms of their dependence on 1/(1-$\gamma$).},
	language = {en},
	number = {3},
	urldate = {2022-03-08},
	journal = {Machine Learning},
	author = {Gheshlaghi Azar, Mohammad and Munos, R{\'e}mi and Kappen, Hilbert J.},
	month = jun,
	year = {2013},
	pages = {325--349},
}

@inproceedings{tewari_optimistic_2007,
	title = {Optimistic {Linear} {Programming} gives {Logarithmic} {Regret} for {Irreducible} {MDPs}},
	abstract = {An algorithm called Optimistic Linear Programming (OLP) for learning to optimize average reward in an irreducible but otherwise unknown Markov decision process (MDP), which is closely related to an algorithm proposed by Burnetas and Katehakis with four key differences: OLP is simpler, it does not require knowledge of the supports of transition probabilities, the proof of the regret bound is simpler and the regretbound is a constant factor larger than the regret of their algorithm. We present an algorithm called Optimistic Linear Programming (OLP) for learning to optimize average reward in an irreducible but otherwise unknown Markov decision process (MDP). OLP uses its experience so far to estimate the MDP. It chooses actions by optimistically maximizing estimated future rewards over a set of next-state transition probabilities that are close to the estimates, a computation that corresponds to solving linear programs. We show that the total expected reward obtained by OLP up to time T is within C(P) log T of the reward obtained by the optimal policy, where C(P) is an explicit, MDP-dependent constant. OLP is closely related to an algorithm proposed by Burnetas and Katehakis with four key differences: OLP is simpler, it does not require knowledge of the supports of transition probabilities, the proof of the regret bound is simpler, but our regret bound is a constant factor larger than the regret of their algorithm. OLP is also similar in flavor to an algorithm recently proposed by Auer and Ortner. But OLP is simpler and its regret bound has a better dependence on the size of the MDP.},
	booktitle = {{NIPS}},
	author = {Tewari, Ambuj and Bartlett, P.},
	year = {2007},
}

@article{osband_lower_2016,
	title = {On {Lower} {Bounds} for {Regret} in {Reinforcement} {Learning}},
	abstract = {This is a brief technical note to clarify the state of lower bounds on regret for reinforcement learning. In particular, this paper: - Reproduces a lower bound on regret for reinforcement learning, similar to the result of Theorem 5 in the journal UCRL2 paper (Jaksch et al 2010). - Clarifies that the proposed proof of Theorem 6 in the REGAL paper (Bartlett and Tewari 2009) does not hold using the standard techniques without further work. We suggest that this result should instead be considered a conjecture as it has no rigorous proof. - Suggests that the conjectured lower bound given by (Bartlett and Tewari 2009) is incorrect and, in fact, it is possible to improve the scaling of the upper bound to match the weaker lower bounds presented in this paper. We hope that this note serves to clarify existing results in the field of reinforcement learning and provides interesting motivation for future work.},
	language = {en},
	urldate = {2022-03-08},
	journal = {arXiv:1608.02732 [cs, stat]},
	author = {Osband, Ian and Van Roy, Benjamin},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.02732},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{fruit_efcient_2018,
	title = {Efficient {Bias}-{Span}-{Constrained} {Exploration}-{Exploitation} in {Reinforcement} {Learning}},
	abstract = {We introduce SCAL, an algorithm designed to perform efficient exploration-exploitation in any unknown weakly-communicating Markov decision process (MDP) for which an upper bound c on the span of the optimal bias function is known. For an MDP with S states, A actions and $\Gamma$ <= S possibl?e next states, we prove a regret bound of O(c $\Gamma$SAT ), which significantly improves over existing algorithms (e.g., UCRL and PSRL), whose regret scales linearly with the MDP diameter D. In fact, the optimal bias span is finite and often much smaller than D (e.g., D = $\infty$ in non-communicating MDPs). A similar result was originally derived by Bartlett and Tewari (2009) for REGAL.C, for which no tractable algorithm is available. In this paper, we relax the optimization problem at the core of REGAL.C, we carefully analyze its properties, and we provide the first computationally efficient algorithm to solve it. Finally, we report numerical simulations supporting our theoretical findings and showing how SCAL significantly outperforms UCRL in MDPs with large diameter and small span.},
	language = {en},
	journal = {Proceedings of the 35 th International Conference on Machine Learning},
	author = {Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro and Ortner, Ronald},
	year = {2018},
}

@article{ouyang_learning_2017,
	title = {Learning {Unknown} {Markov} {Decision} {Processes}: {A} {Thompson} {Sampling} {Approach}},
	shorttitle = {Learning {Unknown} {Markov} {Decision} {Processes}},
	abstract = {We consider the problem of learning an unknown Markov Decision Process (MDP) that is weakly communicating in the infinite horizon setting. We propose a Thompson Sampling-based reinforcement learning algorithm with dynamic episodes (TSDE). At the beginning of each episode, the algorithm generates a sample from the posterior distribution over the unknown model parameters. It then follows the optimal stationary policy for the sampled model for the rest of the episode. The duration of each episode is dynamically determined by two stopping criteria. The first stopping criterion controls the growth rate of episode length. The second stopping criterion happens when the number of visits to any state-action pair is doubled. We establish \${\textbackslash}tilde O(HS{\textbackslash}sqrt\{AT\})\$ bounds on expected regret under a Bayesian setting, where \$S\$ and \$A\$ are the sizes of the state and action spaces, \$T\$ is time, and \$H\$ is the bound of the span. This regret bound matches the best available bound for weakly communicating MDPs. Numerical results show it to perform better than existing algorithms for infinite horizon MDPs.},
	language = {en},
	urldate = {2022-03-08},
	journal = {arXiv:1709.04570 [cs]},
	author = {Ouyang, Yi and Gagrani, Mukul and Nayyar, Ashutosh and Jain, Rahul},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.04570},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Accepted to NIPS 2017},
}

@article{simchowitz_non-asymptotic_2019,
	title = {Non-{Asymptotic} {Gap}-{Dependent} {Regret} {Bounds} for {Tabular} {MDPs}},
	abstract = {This paper establishes that optimistic algorithms attain gap-dependent and non-asymptotic logarithmic regret for episodic MDPs. In contrast to prior work, our bounds do not suffer a dependence on diameter-like quantities or ergodicity, and smoothly interpolate between the gap dependent logarithmic-regret, and the \${\textbackslash}widetilde\{{\textbackslash}mathcal\{O\}\}({\textbackslash}sqrt\{HSAT\})\$-minimax rate. The key technique in our analysis is a novel "clipped" regret decomposition which applies to a broad family of recent optimistic algorithms for episodic MDPs.},
	urldate = {2022-03-08},
	journal = {arXiv:1905.03814 [cs, math, stat]},
	author = {Simchowitz, Max and Jamieson, Kevin},
	month = oct,
	year = {2019},
	note = {arXiv: 1905.03814},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control, Mathematics - Statistics Theory},
}

@article{jiang_contextual_2017,
	title = {Contextual {Decision} {Processes} with low {Bellman} rank are {PAC}-{Learnable}},
	abstract = {This paper studies systematic exploration for reinforcement learning (RL) with rich observations and function approximation. We introduce contextual decision processes (CDPs), that unify most prior RL settings. Our first contribution is a complexity measure, the Bellman rank, that we show enables tractable learning of near-optimal behavior in CDPs and is naturally small for many well-studied RL models. Our second contribution is a new RL algorithm that does systematic exploration to learn near-optimal behavior in CDPs with low Bellman rank. The algorithm requires a number of samples that is polynomial in all relevant parameters but independent of the number of unique contexts. Our approach uses Bellman error minimization with optimistic exploration and provides new insights into efficient exploration for RL with function approximation.},
	language = {en},
	journal = {Proceedings of the 34 th International Conference on Machine Learning},
	author = {Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
	year = {2017},
}

@article{efroni_tight_2019,
	title = {Tight {Regret} {Bounds} for {Model}-{Based} {Reinforcement} {Learning} with {Greedy} {Policies}},
	abstract = {State-of-the-art efficient model-based Reinforcement Learning (RL) algorithms typically act by iteratively solving empirical models, i.e., by performing {\textbackslash}emph\{full-planning\} on Markov Decision Processes (MDPs) built by the gathered experience. In this paper, we focus on model-based RL in the finite-state finite-horizon MDP setting and establish that exploring with {\textbackslash}emph\{greedy policies\} -- act by {\textbackslash}emph\{1-step planning\} -- can achieve tight minimax performance in terms of regret, \${\textbackslash}tilde\{{\textbackslash}mathcal\{O\}\}({\textbackslash}sqrt\{HSAT\})\$. Thus, full-planning in model-based RL can be avoided altogether without any performance degradation, and, by doing so, the computational complexity decreases by a factor of \$S\$. The results are based on a novel analysis of real-time dynamic programming, then extended to model-based RL. Specifically, we generalize existing algorithms that perform full-planning to such that act by 1-step planning. For these generalizations, we prove regret bounds with the same rate as their full-planning counterparts.},
	urldate = {2022-03-08},
	journal = {arXiv:1905.11527 [cs, stat]},
	author = {Efroni, Yonathan and Merlis, Nadav and Ghavamzadeh, Mohammad and Mannor, Shie},
	month = oct,
	year = {2019},
	note = {arXiv: 1905.11527},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: NeurIPS 2019},
}

@article{zhang_is_2021,
	title = {Is {Reinforcement} {Learning} {More} {Difficult} {Than} {Bandits}? {A} {Near}-optimal {Algorithm} {Escaping} the {Curse} of {Horizon}},
	shorttitle = {Is {Reinforcement} {Learning} {More} {Difficult} {Than} {Bandits}?},
	urldate = {2022-03-08},
	journal = {arXiv:2009.13503 [cs, stat]},
	author = {Zhang, Zihan and Ji, Xiangyang and Du, Simon S.},
	month = jun,
	year = {2021},
	note = {arXiv: 2009.13503},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{dyagilev_efficient_2008,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Efficient {Reinforcement} {Learning} in {Parameterized} {Models}: {Discrete} {Parameter} {Case}},
	isbn = {978-3-540-89722-4},
	shorttitle = {Efficient {Reinforcement} {Learning} in {Parameterized} {Models}},
	abstract = {We consider reinforcement learning in the parameterized setup, where the model is known to belong to a finite set of Markov Decision Processes (MDPs) under the discounted return criteria. We propose an on-line algorithm for learning in such parameterized models, the Parameter Elimination (PEL) algorithm, and analyze its performance in terms of the total mistake bound criterion. The algorithm relies on Wald{\textquoteright}s sequential probability ratio test to eliminate unlikely parameters, and uses an optimistic policy for effective exploration. We establish that, with high probability, the total mistake bound for the algorithm is linear (up to a logarithmic term) in the size Open image in new window of the parameter space, independently of the cardinality of the state and action spaces. We further demonstrate that much better dependence on Open image in new window is possible, depending on the specific information structure of the problem.},
	language = {en},
	booktitle = {Recent {Advances} in {Reinforcement} {Learning}},
	publisher = {Springer},
	author = {Dyagilev, Kirill and Mannor, Shie and Shimkin, Nahum},
	editor = {Girgin, Sertan and Loth, Manuel and Munos, R{\'e}mi and Preux, Philippe and Ryabko, Daniil},
	year = {2008},
	keywords = {Action Space, Markov Decision Process, Reinforcement Learn, Reinforcement Learn Algorithm, Sequential Probability Ratio Test},
	pages = {41--54},
}

@inproceedings{dann_policy_2019,
	title = {Policy {Certificates}: {Towards} {Accountable} {Reinforcement} {Learning}},
	shorttitle = {Policy {Certificates}},
	abstract = {The performance of a reinforcement learning algorithm can vary drastically during learning because of exploration. Existing algorithms provide little information about the quality of their current policy before executing it, and thus have limited use in high-stakes applications like healthcare. We address this lack of accountability by proposing that algorithms output policy certificates. These certificates bound the sub-optimality and return of the policy in the next episode, allowing humans to intervene when the certified quality is not satisfactory. We further introduce two new algorithms with certificates and present a new framework for theoretical analysis that guarantees the quality of their policies and certificates. For tabular MDPs, we show that computing certificates can even improve the sample-efficiency of optimism-based exploration. As a result, one of our algorithms is the first to achieve minimax-optimal PAC bounds up to lower-order terms, and this algorithm also matches (and in some settings slightly improves upon) existing minimax regret bounds.},
	language = {en},
	urldate = {2022-03-08},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Dann, Christoph and Li, Lihong and Wei, Wei and Brunskill, Emma},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {1507--1516},
}

@inproceedings{yang_reinforcement_2020,
	title = {Reinforcement {Learning} in {Feature} {Space}: {Matrix} {Bandit}, {Kernels}, and {Regret} {Bound}},
	shorttitle = {Reinforcement {Learning} in {Feature} {Space}},
	abstract = {Exploration in reinforcement learning (RL) suffers from the curse of dimensionality when the state-action space is large. A common practice is to parameterize the high-dimensional value and policy functions using given features. However existing methods either have no theoretical guarantee or suffer a regret that is exponential in the planning horizon ??HH.In this paper, we propose an online RL algorithm, namely the MatrixRL, that leverages ideas from linear bandit to learn a low-dimensional representation of the probability transition model while carefully balancing the exploitation-exploration tradeoff. We show that MatrixRL achieves a regret bound ??(??2??log????--?)O(H2dlog?TT)\{O\}{\textbackslash}big(H{\textasciicircum}2d{\textbackslash}log T{\textbackslash}sqrt\{T\}{\textbackslash}big) where ??dd is the number of features, independent with the number of state-action pairs. MatrixRL has an equivalent kernelized version, which is able to work with an arbitrary kernel Hilbert space without using explicit features. In this case, the kernelized MatrixRL satisfies a regret bound ??(??2{\textbackslash}wt??log????--?)O(H2{\textbackslash}wtdlog?TT)\{O\}{\textbackslash}big(H{\textasciicircum}2{\textbackslash}wt\{d\}{\textbackslash}log T{\textbackslash}sqrt\{T\}{\textbackslash}big), where {\textbackslash}wt??{\textbackslash}wtd{\textbackslash}wt\{d\} is the effective dimension of the kernel space.},
	language = {en},
	urldate = {2022-03-08},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yang, Lin and Wang, Mengdi},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {10746--10756},
}

@article{osband_model-based_2014,
	title = {Model-based {Reinforcement} {Learning} and the {Eluder} {Dimension}},
	abstract = {We consider the problem of learning to optimize an unknown Markov decision process (MDP). We show that, if the MDP can be parameterized within some known function class, we can obtain regret bounds that scale with the dimensionality, rather than cardinality, of the system. We characterize this dependence explicitly as \${\textbackslash}tilde\{O\}({\textbackslash}sqrt\{d\_K d\_E T\})\$ where \$T\$ is time elapsed, \$d\_K\$ is the Kolmogorov dimension and \$d\_E\$ is the {\textbackslash}emph\{eluder dimension\}. These represent the first unified regret bounds for model-based reinforcement learning and provide state of the art guarantees in several important settings. Moreover, we present a simple and computationally efficient algorithm {\textbackslash}emph\{posterior sampling for reinforcement learning\} (PSRL) that satisfies these bounds.},
	urldate = {2022-03-08},
	journal = {arXiv:1406.1853 [cs, stat]},
	author = {Osband, Ian and Van Roy, Benjamin},
	month = oct,
	year = {2014},
	note = {arXiv: 1406.1853},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zhang_almost_2020,
	title = {Almost {Optimal} {Model}-{Free} {Reinforcement} {Learning} via {Reference}-{Advantage} {Decomposition}},
	abstract = {We study the reinforcement learning problem in the setting of finite-horizon episodic Markov Decision Processes (MDPs) with \$S\$ states, \$A\$ actions, and episode length \$H\$. We propose a model-free algorithm UCB-Advantage and prove that it achieves \${\textbackslash}tilde\{O\}({\textbackslash}sqrt\{H{\textasciicircum}2SAT\})\$ regret where \$T = KH\$ and \$K\$ is the number of episodes to play. Our regret bound improves upon the results of [Jin et al., 2018] and matches the best known model-based algorithms as well as the information theoretic lower bound up to logarithmic factors. We also show that UCB-Advantage achieves low local switching cost and applies to concurrent reinforcement learning, improving upon the recent results of [Bai et al., 2019].},
	urldate = {2022-03-08},
	journal = {arXiv:2004.10019 [cs, stat]},
	author = {Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang},
	month = jun,
	year = {2020},
	note = {arXiv: 2004.10019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Data Structures and Algorithms},
	annote = {Comment: 26 pages},
}

@phdthesis{dann_strategic_2019,
	address = {Pittsburgh, PA},
	title = {Strategic {Exploration} in {Reinforcement} {Learning} - {New} {Algorithms} and {Learning} {Guarantees}},
	abstract = {Reinforcement learning (RL) focuses on an essential aspect of intelligent behavior {\textendash} how
an agent can learn to make good decisions given experience and rewards in a stochastic
world. Yet popular RL algorithms that have enabled exciting successes in domains with
good simulators (Go, Atari, etc) still often fail to learn in other domains because they rely on
simple heuristics for exploration. This provides additional empirical justification for essential
questions around RL, specifically around algorithms that learn in a provably efficient manner
through strategic exploration in any considered domain. This thesis provides new algorithms
and theory that enable good performance with respect to existing theoretical frameworks for
evaluating RL algorithms (specifically, probably approximately correct) and introduces new
stronger evaluation criteria, that may be particularly of interest as RL is applied to more real
world problems.
For the first line of work on probably approximately correct (PAC) RL algorithms, we
introduce a series of algorithms for episodic tabular domains with substantially better PAC
sample complexity bounds that culminate in a new algorithm with close to minimax optimal
PAC and regret bounds. Look up tables are required by most sample efficient and computa-
tionally tractable algorithms, but cannot represent many practical domains. We therefore also
present a new RL algorithm that can learn a good policy in environments with high dimensional
observations and hidden deterministic states; unlike predecessors, this algorithm provably
explores not only in a statistically but also computationally efficient manner assuming access
to function classes with efficient optimization oracles.
To make progress it is critical to have the right measures of success. While empirical
demonstrations are quite clear, we find that for theoretical properties, two of the most commonly
used learning frameworks, PAC guarantees and regret guarantees, each allow undesirable
algorithm behavior (e.g. ignoring new observations that could improve the policy). We present
a new stronger learning framework called Uniform-PAC that unifies the existing frameworks
and prevents undesirable algorithm properties.
One caveat of all existing learning frameworks is that for any particular episode, we do not
know how well the algorithm will perform. To address this, we introduce the IPOC framework
that requires algorithms to provide a certificate before each episode bounding how suboptimal
the current policy can be. Such certifications may be of substantial interest in high stakes
scenarios when an organization may wish to track or even pause an online RL system should
the potential expected performance bound drop below a required expected outcome.},
	language = {en},
	school = {Carnegie Mellon University},
	author = {Dann, Christoph},
	month = sep,
	year = {2019},
}

@misc{noauthor_settling_nodate,
	title = {Settling the {Horizon}-{Dependence} of {Sample} {Complexity} in {Reinforcement} {Learning} {\textbar} {IEEE} {Conference} {Publication} {\textbar} {IEEE} {Xplore}},
	urldate = {2022-03-08},
}

@article{xiong_near-optimal_2021,
	title = {Near-{Optimal} {Randomized} {Exploration} for {Tabular} {MDP}},
	abstract = {We study exploration using randomized value functions in Thompson Sampling (TS)-like algorithms in reinforcement learning. This type of algorithms enjoys appealing empirical performance. We show that when we use 1) a single random seed in each episode, and 2) a Bernstein-type magnitude of noise, we obtain a worst-case \${\textbackslash}widetilde\{O\}{\textbackslash}left(H{\textbackslash}sqrt\{SAT\}{\textbackslash}right)\$ regret bound for episodic time-inhomogeneous Markov Decision Process where \$S\$ is the size of state space, \$A\$ is the size of action space, \$H\$ is the planning horizon and \$T\$ is the number of interactions. This bound polynomially improves all existing bounds for TS-like algorithms based on randomized value functions, and for the first time, matches the \${\textbackslash}Omega{\textbackslash}left(H{\textbackslash}sqrt\{SAT\}{\textbackslash}right)\$ lower bound up to logarithmic factors. Our result highlights that randomized exploration can be near-optimal, which was previously only achieved by optimistic algorithms. To achieve the desired result, we develop 1) a new clipping operation to ensure both the probability being optimistic and the probability being pessimistic are lower bounded by a constant, and 2) a new recursion formula for the absolute value of estimations errors to analyze the regret.},
	language = {en},
	urldate = {2022-03-08},
	journal = {arXiv:2102.09703 [cs]},
	author = {Xiong, Zhihan and Shen, Ruoqi and Cui, Qiwen and Du, Simon S.},
	month = nov,
	year = {2021},
	note = {arXiv: 2102.09703},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 42 pages},
}

@inproceedings{wu_reinforcement_2021,
	title = {On {Reinforcement} {Learning} with {Adversarial} {Corruption} and {Its} {Application} to {Block} {MDP}},
	abstract = {We study reinforcement learning (RL) in episodic tabular MDPs with adversarial corruptions, where some episodes can be adversarially corrupted. When the total number of corrupted episodes is known, we propose an algorithm, Corruption Robust Monotonic Value Propagation ({\textbackslash}textsf\{CR-MVP\}), which achieves a regret bound of ???~((??????-----?+??2??+??????)){\textbackslash}polylog(??))O{\textasciitilde}((SAK+S2A+CSA)){\textbackslash}polylog(H)){\textbackslash}tilde\{O\}{\textbackslash}left({\textbackslash}left({\textbackslash}sqrt\{SAK\}+S{\textasciicircum}2A+CSA){\textbackslash}right){\textbackslash}polylog(H){\textbackslash}right), where ??SS is the number of states, ??AA is the number of actions, ??HH is the planning horizon, ??KK is the number of episodes, and ??CC is the corruption level. We also provide a corresponding lower bound, which indicates that our upper bound is tight. Finally, as an application, we study RL with rich observations in the block MDP model. We provide the first algorithm that achieves a ??--?K{\textbackslash}sqrt\{K\}-type regret in this setting and is computationally efficient.},
	language = {en},
	urldate = {2022-03-08},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wu, Tianhao and Yang, Yunchang and Du, Simon and Wang, Liwei},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {11296--11306},
}

@article{neu_unifying_2020,
	title = {A {Unifying} {View} of {Optimism} in {Episodic} {Reinforcement} {Learning}},
	abstract = {The principle of optimism in the face of uncertainty underpins many theoretically successful reinforcement learning algorithms. In this paper we provide a general framework for designing, analyzing and implementing such algorithms in the episodic reinforcement learning problem. This framework is built upon Lagrangian duality, and demonstrates that every model-optimistic algorithm that constructs an optimistic MDP has an equivalent representation as a value-optimistic dynamic programming algorithm. Typically, it was thought that these two classes of algorithms were distinct, with model-optimistic algorithms benefiting from a cleaner probabilistic analysis while value-optimistic algorithms are easier to implement and thus more practical. With the framework developed in this paper, we show that it is possible to get the best of both worlds by providing a class of algorithms which have a computationally efficient dynamic-programming implementation and also a simple probabilistic analysis. Besides being able to capture many existing algorithms in the tabular setting, our framework can also address largescale problems under realizable function approximation, where it enables a simple model-based analysis of some recently proposed methods.},
	urldate = {2022-03-08},
	journal = {arXiv:2007.01891 [cs, stat]},
	author = {Neu, Gergely and Pike-Burke, Ciara},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.01891},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_pdf_nodate,
	title = {[{PDF}] {Reinforcement} {Learning} with {General} {Value} {Function} {Approximation}: {Provably} {Efficient} {Approach} via {Bounded} {Eluder} {Dimension} {\textbar} {Semantic} {Scholar}},
	urldate = {2022-03-08},
}

@inproceedings{metelli_propagating_2019,
	title = {Propagating {Uncertainty} in {Reinforcement} {Learning} via {Wasserstein} {Barycenters}},
	abstract = {This paper proposes a Bayesian framework in which approximate posterior distributions are employed to model the uncertainty of the value function and Wasserstein barycenters are used to propagate it across state-action pairs, and shows how it can be extended to deal with continuous domains. How does the uncertainty of the value function propagate when performing temporal difference learning? In this paper, we address this question by proposing a Bayesian framework in which we employ approximate posterior distributions to model the uncertainty of the value function and Wasserstein barycenters to propagate it across state-action pairs. Leveraging on these tools, we present an algorithm, Wasserstein Q-Learning (WQL), starting in the tabular case and then, we show how it can be extended to deal with continuous domains. Furthermore, we prove that, under mild assumptions, a slight variation of WQL enjoys desirable theoretical properties in the tabular setting. Finally, we present an experimental campaign to show the effectiveness of WQL on finite problems, compared to several RL algorithms, some of which are specifically designed for exploration, along with some preliminary results on Atari games.},
	booktitle = {{NeurIPS}},
	author = {Metelli, Alberto Maria and Likmeta, Amarildo and Restelli, Marcello},
	year = {2019},
}

@article{wang_provably_2020,
	title = {Provably {Efficient} {Reinforcement} {Learning} with {General} {Value} {Function} {Approximation}},
	abstract = {Value function approximation has demonstrated phenomenal empirical success in reinforcement learning (RL). Nevertheless, despite a handful of recent progress on developing theory for RL with linear...},
	language = {en},
	urldate = {2022-03-08},
	author = {Wang, Ruosong and Salakhutdinov, Ruslan and Yang, Lin F.},
	month = jan,
	year = {2020},
}

@article{wang_reinforcement_2020,
	title = {Reinforcement {Learning} with {General} {Value} {Function} {Approximation}: {Provably} {Efficient} {Approach} via {Bounded} {Eluder} {Dimension}},
	shorttitle = {Reinforcement {Learning} with {General} {Value} {Function} {Approximation}},
	abstract = {Value function approximation has demonstrated phenomenal empirical success in reinforcement learning (RL). Nevertheless, despite a handful of recent progress on developing theory for RL with linear function approximation, the understanding of general function approximation schemes largely remains missing. In this paper, we establish a provably efficient RL algorithm with general value function approximation. We show that if the value functions admit an?approximation with a function class F , our algorithm achieves a regret bound of O(poly(dH) T ) where d is a complexity measure of F that depends on the eluder dimension [Russo and Van Roy, 2013] and log-covering numbers, H is the planning horizon, and T is the number interactions with the environment. Our theory generalizes recent progress on RL with linear value function approximation and does not make explicit assumptions on the model of the environment. Moreover, our algorithm is model-free and provides a framework to justify the effectiveness of algorithms used in practice.},
	language = {en},
	urldate = {2022-03-08},
	journal = {arXiv:2005.10804 [cs, math, stat]},
	author = {Wang, Ruosong and Salakhutdinov, Ruslan and Yang, Lin F.},
	month = jun,
	year = {2020},
	note = {arXiv: 2005.10804},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
}

@inproceedings{osband_generalization_2016,
	title = {Generalization and {Exploration} via {Randomized} {Value} {Functions}},
	abstract = {We propose randomized least-squares value iteration (RLSVI) {\textendash} a new reinforcement learning algorithm designed to explore and generalize efficiently via linearly parameterized value functions. We explain why versions of least-squares value iteration that use Boltzmann or epsilon-greedy exploration can be highly inefficient, and we present computational results that demonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish an upper bound on the expected regret of RLSVI that demonstrates near-optimality in a tabula rasa learning context. More broadly, our results suggest that randomized value functions offer a promising approach to tackling a critical challenge in reinforcement learning: synthesizing efficient exploration and effective generalization.},
	language = {en},
	urldate = {2022-03-08},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Osband, Ian and Roy, Benjamin Van and Wen, Zheng},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {2377--2386},
}

@inproceedings{osband_why_2017,
	title = {Why is {Posterior} {Sampling} {Better} than {Optimism} for {Reinforcement} {Learning}?},
	abstract = {Computational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms existing algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an ???~(????????-----?)O{\textasciitilde}(HSAT){\textbackslash}tilde\{O\}(H{\textbackslash}sqrt\{SAT\}) Bayesian regret bound for PSRL in finite-horizon episodic Markov decision processes. This improves upon the best previous Bayesian regret bound of ???~(????????---?)O{\textasciitilde}(HSAT){\textbackslash}tilde\{O\}(H S {\textbackslash}sqrt\{AT\}) for any reinforcement learning algorithm. Our theoretical results are supported by extensive empirical evaluation.},
	language = {en},
	urldate = {2022-03-08},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Osband, Ian and Roy, Benjamin Van},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {2701--2710},
}

@inproceedings{sidford_near-optimal_2018,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'18},
	title = {Near-optimal time and sample complexities for solving {Markov} decision processes with a generative model},
	abstract = {In this paper we consider the problem of computing an e-optimal policy of a discounted Markov Decision Process (DMDP) provided we can only access its transition function through a generative sampling model that given any state-action pair samples from the transition function in O(1) time. Given such a DMDP with states S, actions A, discount factor $\gamma$ ? (0,1), and rewards in range [0,1] we provide an algorithm which computes an $\varepsilon$-optimal policy with probability 1 {\textendash} $\delta$ where both the time spent and number of sample taken are upper bounded by {\textbackslash}[O{\textbackslash}left[{\textbackslash}frac\{{\textbar}{\textbackslash}cS{\textbar}{\textbar}{\textbackslash}cA{\textbar}\}\{(1-{\textbackslash}gamma){\textasciicircum}3 {\textbackslash}epsilon{\textasciicircum}2\} {\textbackslash}log {\textbackslash}left({\textbackslash}frac\{{\textbar}{\textbackslash}cS{\textbar}{\textbar}{\textbackslash}cA{\textbar}\}\{(1-{\textbackslash}gamma){\textbackslash}delta {\textbackslash}epsilon\} {\textbackslash}right) {\textbackslash}log{\textbackslash}left({\textbackslash}frac\{1\}\{(1-{\textbackslash}gamma){\textbackslash}epsilon\}{\textbackslash}right){\textbackslash}right] {\textasciitilde}.{\textbackslash}] For fixed values of $\varepsilon$, this improves upon the previous best known bounds by a factor of (1 {\textendash} $\gamma$)-1 and matches the sample complexity lower bounds proved in [AMK13] up to logarithmic factors. We also extend our method to computing $\varepsilon$-optimal policies for finite-horizon MDP with a generative model and provide a nearly matching sample complexity lower bound.},
	urldate = {2022-03-08},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Sidford, Aaron and Wang, Mengdi and Wu, Xian and Yang, Lin F. and Ye, Yinyu},
	month = dec,
	year = {2018},
	pages = {5192--5202},
}

@article{strehl_reinforcement_2009,
	title = {Reinforcement {Learning} in {Finite} {MDPs}: {PAC} {Analysis}},
	volume = {10},
	issn = {1533-7928},
	shorttitle = {Reinforcement {Learning} in {Finite} {MDPs}},
	abstract = {We study the problem of learning near-optimal behavior in finite Markov Decision Processes (MDPs) with a polynomial number of samples. These "PAC-MDP" algorithms include the well-known E3 and R-MAX algorithms as well as the more recent Delayed Q-learning algorithm. We summarize the current state-of-the-art by presenting bounds for the problem in a unified theoretical framework. A more refined analysis for upper and lower bounds is presented to yield insight into the differences between the model-free Delayed Q-learning and the model-based R-MAX.},
	number = {84},
	urldate = {2022-03-08},
	journal = {Journal of Machine Learning Research},
	author = {Strehl, Alexander L. and Li, Lihong and Littman, Michael L.},
	year = {2009},
	pages = {2413--2444},
}

@inproceedings{szita_model-based_2010,
	address = {Madison, WI, USA},
	series = {{ICML}'10},
	title = {Model-based reinforcement learning with nearly tight exploration complexity bounds},
	isbn = {978-1-60558-907-7},
	abstract = {One might believe that model-based algorithms of reinforcement learning can propagate the obtained experience more quickly, and are able to direct exploration better. As a consequence, fewer exploratory actions should be enough to learn a good policy. Strangely enough, current theoretical results for model-based algorithms do not support this claim: In a finite Markov decision process with N states, the best bounds on the number of exploratory steps necessary are of order O(N2 log N), in contrast to the O(N log N) bound available for the model-free, delayed Q-learning algorithm. In this paper we show that Mormax, a modified version of the Rmax algorithm needs to make at most O(N log N) exploratory steps. This matches the lower bound up to logarithmic factors, as well as the upper bound of the state-of-the-art model-free algorithm, while our new bound improves the dependence on other problem parameters.},
	urldate = {2022-03-08},
	booktitle = {Proceedings of the 27th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Szita, Istvan and Szepesvari, Csaba},
	month = jun,
	year = {2010},
	pages = {1031--1038},
}

@inproceedings{tarbouriech_active_2019,
	title = {Active {Exploration} in {Markov} {Decision} {Processes}},
	abstract = {We introduce the active exploration problem in Markov decision processes (MDPs). Each state of the MDP is characterized by a random value and the learner should gather samples to estimate the mean value of each state as accurately as possible. Similarly to active exploration in multi-armed bandit (MAB), states may have different levels of noise, so that the higher the noise, the more samples are needed. As the noise level is initially unknown, we need to trade off the exploration of the environment to estimate the noise and the exploitation of these estimates to compute a policy maximizing the accuracy of the mean predictions. We introduce a novel learning algorithm to solve this problem showing that active exploration in MDPs may be significantly more difficult than in MAB. We also derive a heuristic procedure to mitigate the negative effect of slowly mixing policies. Finally, we validate our findings on simple numerical simulations.},
	language = {en},
	urldate = {2022-03-08},
	booktitle = {Proceedings of the {Twenty}-{Second} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Tarbouriech, Jean and Lazaric, Alessandro},
	month = apr,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {974--982},
}

@inproceedings{zanette_tighter_2019,
	title = {Tighter {Problem}-{Dependent} {Regret} {Bounds} in {Reinforcement} {Learning} without {Domain} {Knowledge} using {Value} {Function} {Bounds}},
	abstract = {Strong worst-case performance bounds for episodic reinforcement learning exist but fortunately in practice RL algorithms perform much better than such bounds would predict. Algorithms and theory that provide strong problem-dependent bounds could help illuminate the key features of what makes a RL problem hard and reduce the barrier to using RL algorithms in practice. As a step towards this we derive an algorithm and analysis for finite horizon discrete MDPs with state-of-the-art worst-case regret bounds and substantially tighter bounds if the RL environment has special features but without apriori knowledge of the environment from the algorithm. As a result of our analysis, we also help address an open learning theory question~{\textbackslash}cite\{jiang2018open\} about episodic MDPs with a constant upper-bound on the sum of rewards, providing a regret bound function of the number of episodes with no dependence on the horizon.},
	language = {en},
	urldate = {2022-03-08},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zanette, Andrea and Brunskill, Emma},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {7304--7312},
}

@inproceedings{strehl_theoretical_2005,
	address = {New York, NY, USA},
	series = {{ICML} '05},
	title = {A {Theoretical} {Analysis} of {Model}-{Based} {Interval} {Estimation}},
	isbn = {1-59593-180-5},
	abstract = {Several algorithms for learning near-optimal policies in Markov Decision Processes have been analyzed and proven efficient. Empirical results have suggested that Model-based Interval Estimation (MBIE) learns efficiently in practice, effectively balancing exploration and exploitation. This paper presents the first theoretical analysis of MBIE, proving its efficiency even under worst-case conditions. The paper also introduces a new performance metric, average loss, and relates it to its less "online" cousins from the literature.},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Machine} {Learning}},
	publisher = {Association for Computing Machinery},
	author = {Strehl, Alexander L. and Littman, Michael L.},
	year = {2005},
	note = {event-place: Bonn, Germany},
	pages = {856--863},
}

@phdthesis{kakade_sample_2003,
	type = {{PhD} thesis},
	title = {On the {Sample} {Complexity} of {Reinforcement} {Learning}},
	abstract = {This thesis is a detailed investigation into the following question: how much data must an agent collect in order to perform "reinforcement learning" successfully? This question is analogous to the classical issue of the sample complexity in supervised learning, but is harder because of the increased realism of the reinforcement learning setting. This thesis summarizes recent sample complexity results in the reinforcement learning literature and builds on these results to provide novel algorithms with strong performance guarantees. We focus on a variety of reasonable performance criteria and sampling models by which agents may access the environment. For instance, in a policy search setting, we consider the problem of how much simulated experience is required to reliably choose a "good" policy among a restricted class of policies {\textbackslash}Pi (as in Kearns, Mansour, and Ng [2000]). In a more online setting, we consider the case in which an agent is placed in an environment and must follow one unbroken chain of experience with no access to "offline" simulation (as in Kearns and Singh [1998]). We build on the sample based algorithms suggested by Kearns, Mansour, and Ng [2000]. Their sample complexity bounds have no dependence on the size of the state space, an exponential dependence on the planning horizon time, and linear dependence on the complexity of {\textbackslash}Pi . We suggest novel algorithms with more restricted guarantees whose sample complexities are again independent of the size of the state space and depend linearly on the complexity of the policy class {\textbackslash}Pi , but have only a polynomial dependence on the horizon time. We pay particular attention to the tradeoffs made by such algorithms.},
	author = {Kakade, Sham M.},
	year = {2003},
}

@inproceedings{strehl_empirical_2004,
	address = {Boca Raton, FL, USA},
	title = {An empirical evaluation of interval estimation for {Markov} decision processes},
	isbn = {978-0-7695-2236-4},
	abstract = {This paper takes an empirical approach to evaluating three model-based reinforcement-learning methods. All methods intend to speed the learning process by mixing exploitation of learned knowledge with exploration of possibly promising alternatives. We consider -greedy exploration, which is computationally cheap and popular, but unfocused in its exploration effort; R-Max exploration, a simplification of an exploration scheme that comes with a theoretical guarantee of efficiency; and a well-grounded approach, model-based interval estimation, that better integrates exploration and exploitation. Our experiments indicate that effective exploration can result in dramatic improvements in the observed rate of learning.},
	language = {en},
	urldate = {2022-03-09},
	booktitle = {16th {IEEE} {International} {Conference} on {Tools} with {Artificial} {Intelligence}},
	publisher = {IEEE Comput. Soc},
	author = {Strehl, A.L. and Littman, M.L.},
	year = {2004},
	pages = {128--135},
}

@article{burnetas_optimal_1997,
	title = {Optimal {Adaptive} {Policies} for {Markov} {Decision} {Processes}},
	volume = {22},
	journal = {Mathematics of Operations Research - MOR},
	author = {Burnetas, Apostolos and Katehakis, Michael},
	month = feb,
	year = {1997},
	pages = {222--255},
}

@article{ibrahimi_efficient_nodate,
	title = {Efficient {Reinforcement} {Learning} for {High} {Dimensional} {Linear} {Quadratic} {Systems}},
	abstract = {We study the problem of adaptive control of a high dimensional linear quadratic (LQ) system. Previous work established the asymptotic convergence to an optimal controller for various adaptive control s?chemes. More recently, for the average cost LQ problem, a regret bound of O( T ) was shown, apart form logarithmic factors. However, this bound scales exponentially with p, the dimension of the state space. In this work we consider the case where the matrices describing the dynamic of the LQ system are sparse and their dimensions are la?rge. We present an adaptive control scheme that achieves a regret bound of O(p T ), apart from logarithmic factors. In particular, our algorithm has an average cost of (1 + ) times the optimum cost after T = polylog(p)O(1/ 2). This is in comparison to previous work on the dense dynamics where the algorithm requires time that scales exponentially with dimension in order to achieve regret of times the optimal cost. We believe that our result has prominent applications in the emerging area of computational advertising, in particular targeted online advertising and advertising in social networks.},
	language = {en},
	author = {Ibrahimi, Morteza and Javanmard, Adel and Roy, Benjamin V},
	pages = {9},
}

@inproceedings{osband_near-optimal_2014,
	title = {Near-optimal {Reinforcement} {Learning} in {Factored} {MDPs}},
	volume = {27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Osband, Ian and Van Roy, Benjamin},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. and Weinberger, K. Q.},
	year = {2014},
}

@article{yang_function_2020,
	title = {On {Function} {Approximation} in {Reinforcement} {Learning}: {Optimism} in the {Face} of {Large} {State} {Spaces}},
	shorttitle = {On {Function} {Approximation} in {Reinforcement} {Learning}},
	abstract = {The classical theory of reinforcement learning (RL) has focused on tabular and linear representations of value functions. Further progress hinges on combining RL with modern function approximators such as kernel functions and deep neural networks, and indeed there have been many empirical successes that have exploited such combinations in large-scale applications. There are profound challenges, however, in developing a theory to support this enterprise, most notably the need to take into consideration the exploration-exploitation tradeoff at the core of RL in conjunction with the computational and statistical tradeoffs that arise in modern function-approximation-based learning systems. We approach these challenges by studying an optimistic modification of the least-squares value iteration algorithm, in the context of the action-value function represented by a kernel function or an overparameterized neural network. We establish both polynomial runtime complexity and polynomial sample complexity for this algorithm, without additional assumptions on the data-generating model. In particular, we prove that the algorithm incurs an \${\textbackslash}tilde\{{\textbackslash}mathcal\{O\}\}({\textbackslash}delta\_\{{\textbackslash}mathcal\{F\}\} H{\textasciicircum}2 {\textbackslash}sqrt\{T\})\$ regret, where \${\textbackslash}delta\_\{{\textbackslash}mathcal\{F\}\}\$ characterizes the intrinsic complexity of the function class \${\textbackslash}mathcal\{F\}\$, \$H\$ is the length of each episode, and \$T\$ is the total number of episodes. Our regret bounds are independent of the number of states, a result which exhibits clearly the benefit of function approximation in RL.},
	urldate = {2022-03-10},
	journal = {arXiv:2011.04622 [cs, math, stat]},
	author = {Yang, Zhuoran and Jin, Chi and Wang, Zhaoran and Wang, Mengdi and Jordan, Michael I.},
	month = dec,
	year = {2020},
	note = {arXiv: 2011.04622},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control, Mathematics - Statistics Theory, Computer Science - Artificial Intelligence},
	annote = {Comment: 76 pages. The short version of this work appears in NeurIPS 2020},
}

@article{ortner_regret_2020,
	title = {Regret {Bounds} for {Reinforcement} {Learning} via {Markov} {Chain} {Concentration}},
	volume = {67},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	abstract = {We give a simple optimistic algorithm for which it is easy to derive regret bounds of O(sqrt\{t-mix SAT\}) steps in uniformly ergodic Markov decision processes with S states, A actions, and mixing time parameter t-mix. These bounds are the first regret bounds in the general, non-episodic setting with an optimal dependence on all given parameters. They could only be improved by using an alternative mixing time parameter.},
	language = {en},
	urldate = {2022-03-10},
	journal = {Journal of Artificial Intelligence Research},
	author = {Ortner, Ronald},
	month = jan,
	year = {2020},
	keywords = {markov decision processes, reinforcement learning},
	pages = {115--128},
}

@article{tossou_near-optimal_2019,
	title = {Near-optimal {Optimistic} {Reinforcement} {Learning} using {Empirical} {Bernstein} {Inequalities}},
	abstract = {We study model-based reinforcement learning in an unknown finite communicating Markov decision process. We propose a simple algorithm that leverages a variance based confidence interval. We show that the proposed algorithm, UCRL-V, achieves the optimal regret \${\textbackslash}tilde\{{\textbackslash}mathcal\{O\}\}({\textbackslash}sqrt\{DSAT\})\$ up to logarithmic factors, and so our work closes a gap with the lower bound without additional assumptions on the MDP. We perform experiments in a variety of environments that validates the theoretical bounds as well as prove UCRL-V to be better than the state-of-the-art algorithms.},
	urldate = {2022-03-10},
	journal = {arXiv:1905.12425 [cs, stat]},
	author = {Tossou, Aristide and Basu, Debabrota and Dimitrakakis, Christos},
	month = dec,
	year = {2019},
	note = {arXiv: 1905.12425},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Science and Game Theory, Computer Science - Artificial Intelligence},
	annote = {Comment: the algorithm has been simplified (no need to look at lower bound of the reward and transitions). Proof has been significantly clean-up. The previous "assumption" is clarified as a condition of the algorithm well-known as sub-modularity. The proof that the bounds satisfy the submodularity is clean-up},
}

@inproceedings{zhang_regret_2019,
	title = {Regret {Minimization} for {Reinforcement} {Learning} by {Evaluating} the {Optimal} {Bias} {Function}},
	volume = {32},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Zihan and Ji, Xiangyang},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alch{\'e}-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
}

@inproceedings{jin_provably_2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Provably efficient reinforcement learning with linear function approximation},
	volume = {125},
	abstract = {Modern Reinforcement Learning (RL) is commonly applied to practical problems with an enormous number of states, where {\textbackslash}emphfunction approximation must be deployed to approximate either the value function or the policy. The introduction of function approximation raises a fundamental set of challenges involving computational and statistical efficiency, especially given the need to manage the exploration/exploitation tradeoff. As a result, a core RL question remains open: how can we design provably efficient RL algorithms that incorporate function approximation? This question persists even in a basic setting with linear dynamics and linear rewards, for which only linear function approximation is needed. This paper presents the first provable RL algorithm with both polynomial runtime and polynomial sample complexity in this linear setting, without requiring a {\textquotedblleft}simulator{\textquotedblright} or additional assumptions. Concretely, we prove that an optimistic modification of Least-Squares Value Iteration (LSVI){\textemdash}a classical algorithm frequently studied in the linear setting{\textemdash}achieves \${\textbackslash}tilde{\textbackslash}mathcalO({\textbackslash}sqrtd{\textasciicircum}3H{\textasciicircum}3T)\$ regret, where \$d\$ is the ambient dimension of feature space, \$H\$ is the length of each episode, and \$T\$ is the total number of steps. Importantly, such regret is independent of the number of states and actions.},
	booktitle = {Proceedings of {Thirty} {Third} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
	editor = {Abernethy, Jacob and Agarwal, Shivani},
	month = jul,
	year = {2020},
	pages = {2137--2143},
}

@inproceedings{rosenberg_online_2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Online {Convex} {Optimization} in {Adversarial} {Markov} {Decision} {Processes}},
	volume = {97},
	abstract = {We consider online learning in episodic loop-free Markov decision processes (MDPs), where the loss function can change arbitrarily between episodes, and the transition function is not known to the learner. We show \${\textbackslash}tildeO(L{\textbar}X{\textbar}{\textbackslash}sqrt{\textbar}A{\textbar}T)\$ regret bound, where \$T\$ is the number of episodes, \$X\$ is the state space, \$A\$ is the action space, and \$L\$ is the length of each episode. Our online algorithm is implemented using entropic regularization methodology, which allows to extend the original adversarial MDP model to handle convex performance criteria (different ways to aggregate the losses of a single episode) , as well as improve previous regret bounds.},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Rosenberg, Aviv and Mansour, Yishay},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	pages = {5478--5486},
}

@inproceedings{bourel_tightening_2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Tightening {Exploration} in {Upper} {Confidence} {Reinforcement} {Learning}},
	volume = {119},
	abstract = {The upper confidence reinforcement learning (UCRL2) algorithm introduced in {\textbackslash}citepjaksch2010near is a popular method to perform regret minimization in unknown discrete Markov Decision Processes under the average-reward criterion. Despite its nice and generic theoretical regret guarantees, this algorithm and its variants have remained until now mostly theoretical as numerical experiments in simple environments exhibit long burn-in phases before the learning takes place. In pursuit of practical efficiency, we present UCRL3, following the lines of UCRL2, but with two key modifications: First, it uses state-of-the-art time-uniform concentration inequalities to compute confidence sets on the reward and (component-wise) transition distributions for each state-action pair. Furthermore, to tighten exploration, it uses an adaptive computation of the support of each transition distribution, which in turn enables us to revisit the extended value iteration procedure of UCRL2 to optimize over distributions with reduced support by disregarding low probability transitions, while still ensuring near-optimism. We demonstrate, through numerical experiments in standard environments, that reducing exploration this way yields a substantial numerical improvement compared to UCRL2 and its variants. On the theoretical side, these key modifications enable us to derive a regret bound for UCRL3 improving on UCRL2, that for the first time makes appear notions of local diameter and local effective support, thanks to variance-aware concentration bounds.},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Bourel, Hippolyte and Maillard, Odalric and Talebi, Mohammad Sadegh},
	editor = {III, Hal Daum{\'e} and Singh, Aarti},
	month = jul,
	year = {2020},
	pages = {1056--1066},
}

@inproceedings{wei_model-free_2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Model-free {Reinforcement} {Learning} in {Infinite}-horizon {Average}-reward {Markov} {Decision} {Processes}},
	volume = {119},
	abstract = {Model-free reinforcement learning is known to be memory and computation efficient and more amendable to large scale problems. In this paper, two model-free algorithms are introduced for learning infinite-horizon average-reward Markov Decision Processes (MDPs). The first algorithm reduces the problem to the discounted-reward version and achieves \${\textbackslash}mathcalO(T{\textasciicircum}2/3)\$ regret after \$T\$ steps, under the minimal assumption of weakly communicating MDPs. To our knowledge, this is the first model-free algorithm for general MDPs in this setting. The second algorithm makes use of recent advances in adaptive algorithms for adversarial multi-armed bandits and improves the regret to \${\textbackslash}mathcalO({\textbackslash}sqrtT)\$, albeit with a stronger ergodic assumption. This result significantly improves over the \${\textbackslash}mathcalO(T{\textasciicircum}3/4)\$ regret achieved by the only existing model-free algorithm by Abbasi-Yadkori et al. (2019) for ergodic MDPs in the infinite-horizon average-reward setting.},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wei, Chen-Yu and Jahromi, Mehdi Jafarnia and Luo, Haipeng and Sharma, Hiteshi and Jain, Rahul},
	editor = {III, Hal Daum{\'e} and Singh, Aarti},
	month = jul,
	year = {2020},
	pages = {10170--10180},
}

@inproceedings{yang_q-learning_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Q-learning with {Logarithmic} {Regret}},
	volume = {130},
	abstract = {This paper presents the first non-asymptotic result showing a model-free algorithm can achieve logarithmic cumulative regret for episodic tabular reinforcement learning if there exists a strictly positive sub-optimality gap. We prove that the optimistic Q-learning studied in [Jin et al. 2018] enjoys a \${\textbackslash}mathcalO{\textbackslash}!{\textbackslash}left({\textbackslash}fracSA{\textbackslash}cdot {\textbackslash}mathrmpoly{\textbackslash}left(H{\textbackslash}right){\textbackslash}Delta\_{\textbackslash}min{\textbackslash}log{\textbackslash}left(SAT{\textbackslash}right){\textbackslash}right)\$ cumulative regret bound where \$S\$ is the number of states, \$A\$ is the number of actions, \$H\$ is the planning horizon, \$T\$ is the total number of steps, and $_{\textrm{{\textbackslash}min}}$ is the minimum sub-optimality gap of the optimal Q-function. This bound matches the information theoretical lower bound in terms of \$S,A,T\$ up to a \${\textbackslash}log{\textbackslash}left(SA{\textbackslash}right)\$ factor. We further extend our analysis to the discounted setting and obtain a similar logarithmic cumulative regret bound.},
	booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Yang, Kunhe and Yang, Lin and Du, Simon},
	editor = {Banerjee, Arindam and Fukumizu, Kenji},
	month = apr,
	year = {2021},
	pages = {1576--1584},
}

@article{talebi_variance-aware_2018,
	title = {Variance-{Aware} {Regret} {Bounds} for {Undiscounted} {Reinforcement} {Learning} in {MDPs}},
	journal = {Journal of Machine Learning Research},
	author = {Talebi, Mohammad Sadegh and Maillard, Odalric-Ambrym},
	month = apr,
	year = {2018},
	note = {Publisher: Microtome Publishing},
	keywords = {Markov Decision Processes, Bellman Optimality, Concentration Inequalities, Regret Minimization, Undiscounted Reinforcement Learning},
	pages = {1--36},
}

@techreport{huang_short_2021,
	title = {A {Short} {Note} on the {Relationship} of {Information} {Gain} and {Eluder} {Dimension}},
	abstract = {Eluder dimension and information gain are two widely used methods of complexity measures in bandit and reinforcement learning. Eluder dimension was originally proposed as a general complexity measure of function classes, but the common examples of where it is known to be small are function spaces (vector spaces). In these cases, the primary tool to upper bound the eluder dimension is the elliptic potential lemma. Interestingly, the elliptic potential lemma also features prominently in the analysis of linear bandits/reinforcement learning and their nonparametric generalization, the information gain. We show that this is not a coincidence {\textendash} eluder dimension and information gain are equivalent in a precise sense for reproducing kernel Hilbert spaces.},
	institution = {ArXiv Report},
	author = {Huang, Kaixuan and Kakade, Sham M. and Lee, Jason D. and Lei, Qi},
	year = {2021},
}

@article{jin_bellman_2021,
	title = {Bellman {Eluder} {Dimension}: {New} {Rich} {Classes} of {RL} {Problems}, and {Sample}-{Efficient} {Algorithms}},
	shorttitle = {Bellman {Eluder} {Dimension}},
	abstract = {Finding the minimal structural assumptions that empower sample-efficient learning is one of the most important research directions in Reinforcement Learning (RL). This paper advances our understanding of this fundamental question by introducing a new complexity measure -- Bellman Eluder (BE) dimension. We show that the family of RL problems of low BE dimension is remarkably rich, which subsumes a vast majority of existing tractable RL problems including but not limited to tabular MDPs, linear MDPs, reactive POMDPs, low Bellman rank problems as well as low Eluder dimension problems. This paper further designs a new optimization-based algorithm -- GOLF, and reanalyzes a hypothesis elimination-based algorithm -- OLIVE (proposed in Jiang et al., 2017). We prove that both algorithms learn the near-optimal policies of low BE dimension problems in a number of samples that is polynomial in all relevant parameters, but independent of the size of state-action space. Our regret and sample complexity results match or improve the best existing results for several well-known subclasses of low BE dimension problems.},
	urldate = {2022-03-16},
	journal = {arXiv:2102.00815 [cs, stat]},
	author = {Jin, Chi and Liu, Qinghua and Miryoosefi, Sobhan},
	month = jul,
	year = {2021},
	note = {arXiv: 2102.00815},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
}

@article{li_eluder_2021,
	title = {Eluder {Dimension} and {Generalized} {Rank}},
	abstract = {We study the relationship between the eluder dimension for a function class and a generalized notion of rank, defined for any monotone "activation" \${\textbackslash}sigma : {\textbackslash}mathbb\{R\} {\textbackslash}to {\textbackslash}mathbb\{R\}\$, which corresponds to the minimal dimension required to represent the class as a generalized linear model. When \${\textbackslash}sigma\$ has derivatives bounded away from \$0\$, it is known that \${\textbackslash}sigma\$-rank gives rise to an upper bound on eluder dimension for any function class; we show however that eluder dimension can be exponentially smaller than \${\textbackslash}sigma\$-rank. We also show that the condition on the derivative is necessary; namely, when \${\textbackslash}sigma\$ is the \${\textbackslash}mathrm\{relu\}\$ activation, we show that eluder dimension can be exponentially larger than \${\textbackslash}sigma\$-rank.},
	urldate = {2022-03-16},
	journal = {arXiv:2104.06970 [cs, stat]},
	author = {Li, Gene and Kamath, Pritish and Foster, Dylan J. and Srebro, Nathan},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.06970},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Technical Note},
}

@inproceedings{du_bilinear_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Bilinear {Classes}: {A} {Structural} {Framework} for {Provable} {Generalization} in {RL}},
	volume = {139},
	abstract = {This work introduces Bilinear Classes, a new structural framework, which permit generalization in reinforcement learning in a wide variety of settings through the use of function approximation. The framework incorporates nearly all existing models in which a polynomial sample complexity is achievable, and, notably, also includes new models, such as the Linear Q*/V* model in which both the optimal Q-function and the optimal V-function are linear in some known feature space. Our main result provides an RL algorithm which has polynomial sample complexity for Bilinear Classes; notably, this sample complexity is stated in terms of a reduction to the generalization error of an underlying supervised learning sub-problem. These bounds nearly match the best known sample complexity bounds for existing models. Furthermore, this framework also extends to the infinite dimensional (RKHS) setting: for the the Linear Q*/V* model, linear MDPs, and linear mixture MDPs, we provide sample complexities that have no explicit dependence on the explicit feature dimension (which could be infinite), but instead depends only on information theoretic quantities.},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Du, Simon and Kakade, Sham and Lee, Jason and Lovett, Shachar and Mahajan, Gaurav and Sun, Wen and Wang, Ruosong},
	editor = {Meila, Marina and Zhang, Tong},
	month = jul,
	year = {2021},
	pages = {2826--2836},
}

@inproceedings{ok_exploration_2018,
	title = {Exploration in {Structured} {Reinforcement} {Learning}},
	volume = {31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ok, Jungseul and Proutiere, Alexandre and Tranos, Damianos},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
}

@article{tranos_regret_2021,
	title = {Regret {Analysis} in {Deterministic} {Reinforcement} {Learning}},
	abstract = {We consider Markov Decision Processes (MDPs) with deterministic transitions and study the problem of regret minimization, which is central to the analysis and design of optimal learning algorithms. We present logarithmic problemspecific regret lower bounds that explicitly depend on the system parameter (in contrast to previous minimax approaches) and thus, truly quantify the fundamental limit of performance achievable by any learning algorithm. Deterministic MDPs can be interpreted as graphs and analyzed in terms of their cycles, a fact which we leverage in order to identify a class of deterministic MDPs whose regret lower bound can be determined numerically. We further exemplify this result on a deterministic line search problem, and a deterministic MDP with statedependent rewards, whose regret lower bounds we can state explicitly. These bounds share similarities with the known problem-specific bound of the multi-armed bandit problem and suggest that navigation on a deterministic MDP need not have an effect on the performance of a learning algorithm.},
	language = {en},
	urldate = {2022-03-16},
	journal = {arXiv:2106.14338 [cs, stat]},
	author = {Tranos, Damianos and Proutiere, Alexandre},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.14338},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{gong_duality_2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {A {Duality} {Approach} for {Regret} {Minimization} in {Average}-{Award} {Ergodic} {Markov} {Decision} {Processes}},
	volume = {120},
	abstract = {In light of the Bellman duality, we propose a novel value-policy gradient algorithm to explore and act in infinite-horizon Average-reward Markov Decision Process (AMDP) and show that it has sublinear regret. The algorithm is motivated by the Bellman saddle point formulation. It learns the optimal state-action distribution, which encodes a randomized policy, by interacting with the environment along a single trajectory and making primal-dual updates. The key to the analysis is to establish a connection between the min-max duality gap of Bellman saddle point and the cumulative regret of the learning agent. We show that, for ergodic AMDPs with finite state space \${\textbackslash}mathcalS\$ and action space \${\textbackslash}mathcalA\$ and uniformly bounded mixing times, the algorithm{\textquoteright}s \$T\$-time step regret is \$\$ R(T)={\textbackslash}tilde{\textbackslash}mathcalO{\textbackslash}left( {\textbackslash}left(t\_mix{\textasciicircum}*{\textbackslash}right){\textasciicircum}2 {\textbackslash}tau{\textasciicircum}{\textbackslash}frac32 {\textbackslash}sqrt({\textbackslash}tau{\textasciicircum}3 + {\textbar}{\textbackslash}mathcalA{\textbar}) {\textbar}{\textbackslash}mathcalS{\textbar} T {\textbackslash}right), \$\$ where \$t\_mix{\textasciicircum}*\$ is the worst-case mixing time, $\tau$ is an ergodicity parameter, \$T\$ is the number of time steps and \${\textbackslash}tilde{\textbackslash}mathcalO\$ hides polylog factors.},
	booktitle = {Proceedings of the 2nd {Conference} on {Learning} for {Dynamics} and {Control}},
	publisher = {PMLR},
	author = {Gong, Hao and Wang, Mengdi},
	editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
	month = jun,
	year = {2020},
	pages = {862--883},
}

@article{maurer_empirical_2009,
	title = {Empirical {Bernstein} {Bounds} and {Sample} {Variance} {Penalization}},
	abstract = {We give improved constants for data dependent and variance sensitive confidence bounds, called empirical Bernstein bounds, and extend these inequalities to hold uniformly over classes of functionswhose growth function is polynomial in the sample size n. The bounds lead us to consider sample variance penalization, a novel learning method which takes into account the empirical variance of the loss function. We give conditions under which sample variance penalization is effective. In particular, we present a bound on the excess risk incurred by the method. Using this, we argue that there are situations in which the excess risk of our method is of order 1/n, while the excess risk of empirical risk minimization is of order 1/sqrt/\{n\}. We show some experimental results, which confirm the theory. Finally, we discuss the potential application of our results to sample compression schemes.},
	urldate = {2022-03-24},
	journal = {arXiv:0907.3740 [stat]},
	author = {Maurer, Andreas and Pontil, Massimiliano},
	month = jul,
	year = {2009},
	note = {arXiv: 0907.3740},
	keywords = {Statistics - Machine Learning},
	annote = {Comment: 10 pages, 1 figure, Proc. Computational Learning Theory Conference (COLT 2009)},
}

@article{abounadi_learning_2002,
	title = {Learning algorithms for {Markov} decision processes with average cost},
	volume = {40},
	issn = {0363-0129},
	abstract = {This paper gives the first rigorous convergence analysis of analogues of Watkins's Q-learning algorithm, applied to average cost control of finite-state Markov chains. We discuss two algorithms which may be viewed as stochastic approximation counterparts of two existing algorithms for recursively computing the value function of the average cost problem - the traditional relative value iteration (RVI) algorithm and a recent algorithm of Bertsekas based on the stochastic shortest path (SSP) formulation of the problem. Both synchronous and asynchronous implementations are considered and analyzed using the ODE method. This involves establishing asymptotic stability of associated ODE limits. The SSP algorithm also uses ideas from two-time-scale stochastic approximation.},
	language = {English (US)},
	number = {3},
	journal = {SIAM Journal on Control and Optimization},
	author = {Abounadi, Jinane and Bertsekas, Dimitrib and Borkar, V. S.},
	year = {2002},
	note = {Publisher: Society for Industrial and Applied Mathematics Publications},
	keywords = {Average cost control, Controlled Markov chains, Dynamic programming, Q-learning, Simulation-based algorithms, Stochastic approximation},
	pages = {681--698},
	annote = {Copyright: Copyright 2008 Elsevier B.V., All rights reserved.},
}

@article{borkar_analog_1997,
	title = {An analog scheme for fixed point computation. {I}. {Theory}},
	volume = {44},
	issn = {1558-1268},
	abstract = {An analog system for fixed point computation is described. The system is derived from a continuous time analog of the classical over-relaxed fixed point iteration. The dynamical system is proved to converge for nonexpansive mappings under all p norms, p/spl isin/(1,/spl infin/). This extends previously established results to not necessarily differentiable maps which are nonexpansive under the /spl infin/-norm. The system will always converge to a single fixed point in a connected set of fixed points. This allows the system to function as a complementary paradigm to energy minimization techniques for optimization in the analog domain. It is shown that the proposed technique is applicable to a large class of dynamic programming computations.},
	number = {4},
	journal = {IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications},
	author = {Borkar, V.S. and Soumyanatha, K.},
	month = apr,
	year = {1997},
	note = {Conference Name: IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications},
	keywords = {Dynamic programming, Adaptive control, Analog computers, Automatic control, Control system analysis, Control systems, Linear systems, Perturbation methods, Robust control, Robust stability},
	pages = {351--355},
}

@article{abounadi_stochastic_2002,
	title = {Stochastic {Approximation} for {Nonexpansive} {Maps}: {Application} to \textit{{Q}} -{Learning} {Algorithms}},
	volume = {41},
	issn = {0363-0129, 1095-7138},
	shorttitle = {Stochastic {Approximation} for {Nonexpansive} {Maps}},
	abstract = {We discuss synchronous and asynchronous iterations of the form xk+1 = xk + $\gamma$(k)(h(xk) + wk), where h is a suitable map and \{wk\} is a deterministic or stochastic sequence satisfying suitable conditions. In particular, in the stochastic case, these are stochastic approximation iterations that can be analyzed using the ODE approach based either on Kushner and Clark{\textquoteright}s lemma for the synchronous case or on Borkar{\textquoteright}s theorem for the asynchronous case. However, the analysis requires that the iterates \{xk\} be bounded, a fact which is usually hard to prove. We develop a novel framework for proving boundedness in the deterministic framework, which is also applicable to the stochastic case when the deterministic hypotheses can be verified in the almost sure sense. This is based on scaling ideas and on the properties of Lyapunov functions. We then combine the boundedness property with Borkar{\textquoteright}s stability analysis of ODEs involving nonexpansive mappings to prove convergence (with probability 1 in the stochastic case). We also apply our convergence analysis to Q-learning algorithms for stochastic shortest path problems and are able to relax some of the assumptions of the currently available results.},
	language = {en},
	number = {1},
	urldate = {2022-03-29},
	journal = {SIAM Journal on Control and Optimization},
	author = {Abounadi, Jinane and Bertsekas, Dimitri P. and Borkar, Vivek},
	month = jan,
	year = {2002},
	pages = {1--22},
}

@article{paulin_concentration_2015,
	title = {Concentration inequalities for {Markov} chains by {Marton} couplings and spectral methods},
	volume = {20},
	number = {none},
	journal = {Electronic Journal of Probability},
	author = {Paulin, Daniel},
	year = {2015},
	note = {Publisher: Institute of Mathematical Statistics and Bernoulli Society},
	keywords = {Concentration inequalities, coupling, Markov chain, mixing time, spectral gap},
	pages = {1 -- 32},
}

@inproceedings{moulos_hoeffding_2020,
	title = {A {Hoeffding} {Inequality} for {Finite} {State} {Markov} {Chains} and its {Applications} to {Markovian} {Bandits}},
	booktitle = {2020 {IEEE} {International} {Symposium} on {Information} {Theory} ({ISIT})},
	author = {Moulos, Vrettos},
	year = {2020},
	pages = {2777--2782},
}

@article{fruit_improved_2020,
	title = {Improved {Analysis} of {UCRL2} with {Empirical} {Bernstein} {Inequality}},
	volume = {abs/2007.05456},
	journal = {ArXiv},
	author = {Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro},
	year = {2020},
}

@article{freedman_tail_1975,
	title = {On tail probabilities for martingales},
	journal = {the Annals of Probability},
	author = {Freedman, David A},
	year = {1975},
	note = {Publisher: JSTOR},
	pages = {100--118},
}

@book{levin_markov_2017,
	title = {Markov chains and mixing times},
	volume = {107},
	publisher = {American Mathematical Soc.},
	author = {Levin, David A and Peres, Yuval},
	year = {2017},
}

@article{azuma_weighted_1967,
	title = {Weighted sums of certain dependent random variables},
	volume = {19},
	number = {3},
	journal = {Tohoku Mathematical Journal},
	author = {Azuma, Kazuoki},
	year = {1967},
	note = {Publisher: Tohoku University, Mathematical Institute},
	pages = {357 -- 367},
}

@article{weissman_inequalities_2003,
	title = {Inequalities for the {L1} deviation of the empirical distribution},
	journal = {Hewlett-Packard Labs, Tech. Rep},
	author = {Weissman, Tsachy and Ordentlich, Erik and Seroussi, Gadiel and Verdu, Sergio and Weinberger, Marcelo J},
	year = {2003},
}
