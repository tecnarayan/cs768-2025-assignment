\begin{thebibliography}{10}

\bibitem{abbe2017community}
E.~Abbe.
\newblock Community detection and stochastic block models: recent developments.
\newblock {\em The Journal of Machine Learning Research}, 18(1):6446--6531,
  2017.

\bibitem{mmsbm}
E.~M. Airoldi, D.~Blei, S.~Fienberg, and E.~Xing.
\newblock Mixed membership stochastic blockmodels.
\newblock {\em Advances in neural information processing systems}, 21, 2008.

\bibitem{longformer}
I.~Beltagy, M.~E. Peters, and A.~Cohan.
\newblock Longformer: The long-document transformer.
\newblock {\em CoRR}, abs/2004.05150, 2020.

\bibitem{ste}
Y.~Bengio, N.~L{\'{e}}onard, and A.~C. Courville.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock {\em CoRR}, abs/1308.3432, 2013.

\bibitem{bhojanapalli2020}
S.~Bhojanapalli, C.~Yun, A.~S. Rawat, S.~J. Reddi, and S.~Kumar.
\newblock Low-rank bottleneck in multi-head attention models.
\newblock {\em CoRR}, abs/2002.07028, 2020.

\bibitem{bollobas1998random}
B.~Bollob{\'a}s.
\newblock Random graphs.
\newblock In {\em Modern graph theory}, pages 215--252. Springer, 1998.

\bibitem{gpt-3}
T.~B. Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, S.~Agarwal,
  A.~Herbert{-}Voss, G.~Krueger, T.~Henighan, R.~Child, A.~Ramesh, D.~M.
  Ziegler, J.~Wu, C.~Winter, C.~Hesse, M.~Chen, E.~Sigler, M.~Litwin, S.~Gray,
  B.~Chess, J.~Clark, C.~Berner, S.~McCandlish, A.~Radford, I.~Sutskever, and
  D.~Amodei.
\newblock Language models are few-shot learners.
\newblock {\em CoRR}, abs/2005.14165, 2020.

\bibitem{qqp}
Z.~Chen, H.~Zhang, X.~Zhang, and L.~Zhao.
\newblock Quora question pairs.
\newblock 2017.

\bibitem{sparsetransformer}
R.~Child, S.~Gray, A.~Radford, and I.~Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock {\em CoRR}, abs/1904.10509, 2019.

\bibitem{performer}
K.~Choromanski, V.~Likhosherstov, D.~Dohan, X.~Song, A.~Gane, T.~Sarl{\'{o}}s,
  P.~Hawkins, J.~Davis, A.~Mohiuddin, L.~Kaiser, D.~Belanger, L.~J. Colwell,
  and A.~Weller.
\newblock Rethinking attention with performers.
\newblock {\em CoRR}, abs/2009.14794, 2020.

\bibitem{clark2019}
K.~Clark, U.~Khandelwal, O.~Levy, and C.~D. Manning.
\newblock What does {BERT} look at? an analysis of bert's attention.
\newblock {\em CoRR}, abs/1906.04341, 2019.

\bibitem{correia2019}
G.~M. Correia, V.~Niculae, and A.~F.~T. Martins.
\newblock Adaptively sparse transformers.
\newblock {\em CoRR}, abs/1909.00015, 2019.

\bibitem{smyrf}
G.~Daras, N.~Kitaev, A.~Odena, and A.~G. Dimakis.
\newblock {SMYRF:} efficient attention using asymmetric clustering.
\newblock {\em CoRR}, abs/2010.05315, 2020.

\bibitem{bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

\bibitem{ViT}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit,
  and N.~Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{funke2019sbm}
T.~Funke and T.~Becker.
\newblock Stochastic block models: A comparison of variants and inference
  methods.
\newblock {\em PLOS ONE}, 14(4):1--40, 04 2019.

\bibitem{kaiming2015delving}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock {\em CoRR}, abs/1502.01852, 2015.

\bibitem{LSTM}
S.~Hochreiter and J.~Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem{huang2021efficient}
L.~Huang, S.~Cao, N.~Parulian, H.~Ji, and L.~Wang.
\newblock Efficient attentions for long document summarization.
\newblock In {\em Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 1419--1436, 2021.

\bibitem{dcsbm}
B.~Karrer and M.~E. Newman.
\newblock Stochastic blockmodels and community structure in networks.
\newblock {\em Physical review E}, 83(1):016107, 2011.

\bibitem{lineartransformer}
A.~Katharopoulos, A.~Vyas, N.~Pappas, and F.~Fleuret.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock {\em CoRR}, abs/2006.16236, 2020.

\bibitem{reformer}
N.~Kitaev, L.~Kaiser, and A.~Levskaya.
\newblock Reformer: The efficient transformer.
\newblock {\em CoRR}, abs/2001.04451, 2020.

\bibitem{image}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{osbm}
P.~Latouche, E.~Birmel{\'e}, and C.~Ambroise.
\newblock Overlapping stochastic block models with application to the french
  political blogosphere.
\newblock {\em The Annals of Applied Statistics}, 5(1):309--336, 2011.

\bibitem{pathfinder}
D.~Linsley, J.~Kim, V.~Veerabadran, and T.~Serre.
\newblock Learning long-range spatial dependencies with horizontal
  gated-recurrent units.
\newblock {\em CoRR}, abs/1805.08315, 2018.

\bibitem{text}
A.~L. Maas, R.~E. Daly, P.~T. Pham, D.~Huang, A.~Y. Ng, and C.~Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In {\em Proceedings of the 49th Annual Meeting of the Association for
  Computational Linguistics: Human Language Technologies}, pages 142--150,
  Portland, Oregon, USA, June 2011. Association for Computational Linguistics.

\bibitem{martins2020}
A.~F.~T. Martins, M.~V. Treviso, A.~Farinhas, V.~Niculae, M.~A.~T. Figueiredo,
  and P.~M.~Q. Aguiar.
\newblock Sparse and continuous attention mechanisms.
\newblock {\em CoRR}, abs/2006.07214, 2020.

\bibitem{listops}
N.~Nangia and S.~R. Bowman.
\newblock Listops: {A} diagnostic dataset for latent tree learning.
\newblock {\em CoRR}, abs/1804.06028, 2018.

\bibitem{narang2021}
S.~Narang, H.~W. Chung, Y.~Tay, W.~Fedus, T.~F{\'{e}}vry, M.~Matena, K.~Malkan,
  N.~Fiedel, N.~Shazeer, Z.~Lan, Y.~Zhou, W.~Li, N.~Ding, J.~Marcus,
  A.~Roberts, and C.~Raffel.
\newblock Do transformer modifications transfer across implementations and
  applications?
\newblock {\em CoRR}, abs/2102.11972, 2021.

\bibitem{Transformer-NMT}
M.~Ott, S.~Edunov, D.~Grangier, and M.~Auli.
\newblock Scaling neural machine translation.
\newblock In {\em Proceedings of the Third Conference on Machine Translation:
  Research Papers}, pages 1--9, Brussels, Belgium, Oct. 2018. Association for
  Computational Linguistics.

\bibitem{hsbm}
T.~P. Peixoto.
\newblock Hierarchical block structures and high-resolution model selection in
  large networks.
\newblock {\em Physical Review X}, 4(1):011047, 2014.

\bibitem{retrieval}
D.~R. Radev, P.~Muthukrishnan, and V.~Qazvinian.
\newblock The {ACL} {A}nthology network.
\newblock In {\em Proceedings of the 2009 Workshop on Text and Citation
  Analysis for Scholarly Digital Libraries ({NLPIR}4{DL})}, pages 54--61,
  Suntec City, Singapore, Aug. 2009. Association for Computational Linguistics.

\bibitem{qnli}
P.~Rajpurkar, J.~Zhang, K.~Lopyrev, and P.~Liang.
\newblock {SQ}u{AD}: 100,000+ questions for machine comprehension of text.
\newblock In {\em Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}, pages 2383--2392, Austin, Texas, Nov. 2016.
  Association for Computational Linguistics.

\bibitem{MSATranformer}
R.~M. Rao, J.~Liu, R.~Verkuil, J.~Meier, J.~Canny, P.~Abbeel, T.~Sercu, and
  A.~Rives.
\newblock Msa transformer.
\newblock In M.~Meila and T.~Zhang, editors, {\em Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of {\em Proceedings
  of Machine Learning Research}, pages 8844--8856. PMLR, 18--24 Jul 2021.

\bibitem{fastrg}
K.~Rohe, J.~Tao, X.~Han, and N.~Binkiewicz.
\newblock A note on quickly sampling a sparse matrix with low rank expectation.
\newblock {\em The Journal of Machine Learning Research}, 19(1):3040--3052,
  2018.

\bibitem{sparsebert}
H.~Shi, J.~Gao, X.~Ren, H.~Xu, X.~Liang, Z.~Li, and J.~T. Kwok.
\newblock Sparsebert: Rethinking the importance analysis in self-attention.
\newblock {\em CoRR}, abs/2102.12871, 2021.

\bibitem{sst-2}
R.~Socher, A.~Perelygin, J.~Wu, J.~Chuang, C.~D. Manning, A.~Y. Ng, and
  C.~Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In {\em Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pages 1631--1642, 2013.

\bibitem{lra}
Y.~Tay, M.~Dehghani, S.~Abnar, Y.~Shen, D.~Bahri, P.~Pham, J.~Rao, L.~Yang,
  S.~Ruder, and D.~Metzler.
\newblock Long range arena: {A} benchmark for efficient transformers.
\newblock {\em CoRR}, abs/2011.04006, 2020.

\bibitem{etsurvey}
Y.~Tay, M.~Dehghani, D.~Bahri, and D.~Metzler.
\newblock Efficient transformers: {A} survey.
\newblock {\em CoRR}, abs/2009.06732, 2020.

\bibitem{transformer}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock {\em CoRR}, abs/1706.03762, 2017.

\bibitem{gat}
P.~Veli{\v{c}}kovi{\'{c}}, G.~Cucurull, A.~Casanova, A.~Romero, P.~Li{\`{o}},
  and Y.~Bengio.
\newblock {Graph Attention Networks}.
\newblock {\em International Conference on Learning Representations}, 2018.
\newblock accepted as poster.

\bibitem{walker1977}
A.~J. Walker.
\newblock An efficient method for generating discrete random variables with
  general distributions.
\newblock {\em ACM Trans. Math. Softw.}, 3(3):253â€“256, sep 1977.

\bibitem{glue}
A.~Wang, A.~Singh, J.~Michael, F.~Hill, O.~Levy, and S.~R. Bowman.
\newblock {GLUE:} {A} multi-task benchmark and analysis platform for natural
  language understanding.
\newblock {\em CoRR}, abs/1804.07461, 2018.

\bibitem{wang2019dgl}
M.~Wang, D.~Zheng, Z.~Ye, Q.~Gan, M.~Li, X.~Song, J.~Zhou, C.~Ma, L.~Yu,
  Y.~Gai, T.~Xiao, T.~He, G.~Karypis, J.~Li, and Z.~Zhang.
\newblock Deep graph library: A graph-centric, highly-performant package for
  graph neural networks.
\newblock {\em arXiv preprint arXiv:1909.01315}, 2019.

\bibitem{linformer}
S.~Wang, B.~Z. Li, M.~Khabsa, H.~Fang, and H.~Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock {\em CoRR}, abs/2006.04768, 2020.

\bibitem{sparsert}
Z.~Wang.
\newblock Sparsert: Accelerating unstructured sparsity on gpus for deep
  learning inference.
\newblock {\em arXiv preprint arXiv:2008.11849}, 2020.

\bibitem{mnli}
A.~Williams, N.~Nangia, and S.~Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In {\em Proceedings of the 2018 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pages 1112--1122, New Orleans,
  Louisiana, June 2018. Association for Computational Linguistics.

\bibitem{nystromformer}
Y.~Xiong, Z.~Zeng, R.~Chakraborty, M.~Tan, G.~Fung, Y.~Li, and V.~Singh.
\newblock Nystr{\"{o}}mformer: {A} nystr{\"{o}}m-based algorithm for
  approximating self-attention.
\newblock {\em CoRR}, abs/2102.03902, 2021.

\bibitem{yun2019}
C.~Yun, S.~Bhojanapalli, A.~S. Rawat, S.~J. Reddi, and S.~Kumar.
\newblock Are transformers universal approximators of sequence-to-sequence
  functions?
\newblock {\em CoRR}, abs/1912.10077, 2019.

\bibitem{yun2020}
C.~Yun, Y.~Chang, S.~Bhojanapalli, A.~S. Rawat, S.~J. Reddi, and S.~Kumar.
\newblock O(n) connections are expressive enough: Universal approximability of
  sparse transformers.
\newblock {\em CoRR}, abs/2006.04862, 2020.

\bibitem{bigbird}
M.~Zaheer, G.~Guruganesh, A.~Dubey, J.~Ainslie, C.~Alberti,
  S.~Onta{\~{n}}{\'{o}}n, P.~Pham, A.~Ravula, Q.~Wang, L.~Yang, and A.~Ahmed.
\newblock Big bird: Transformers for longer sequences.
\newblock {\em CoRR}, abs/2007.14062, 2020.

\bibitem{realnews}
R.~Zellers, A.~Holtzman, H.~Rashkin, Y.~Bisk, A.~Farhadi, F.~Roesner, and
  Y.~Choi.
\newblock Defending against neural fake news.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{zhang2021multi}
P.~Zhang, X.~Dai, J.~Yang, B.~Xiao, L.~Yuan, L.~Zhang, and J.~Gao.
\newblock Multi-scale vision longformer: A new vision transformer for
  high-resolution image encoding.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 2998--3008, 2021.

\bibitem{latency-aware}
M.~Zhu and Y.~Xie.
\newblock Taming unstructured sparsity on gpus via latency-aware optimization.
\newblock In {\em 2020 57th ACM/IEEE Design Automation Conference (DAC)}, pages
  1--6, 2020.

\bibitem{bookcorpus}
Y.~Zhu, R.~Kiros, R.~Zemel, R.~Salakhutdinov, R.~Urtasun, A.~Torralba, and
  S.~Fidler.
\newblock Aligning books and movies: Towards story-like visual explanations by
  watching movies and reading books.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 19--27, 2015.

\end{thebibliography}
