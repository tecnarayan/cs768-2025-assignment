\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbeel \& Ng(2004)Abbeel and Ng]{abbeel2004apprenticeship}
Abbeel, P. and Ng, A.~Y.
\newblock {A}pprenticeship {L}earning via {I}nverse {R}einforcement {L}earning.
\newblock In \emph{{I}nternational {C}onference on {M}achine {L}earning ({ICML})}, 2004.

\bibitem[Akrour et~al.(2012)Akrour, Schoenauer, and Sebag]{akrour2012april}
Akrour, R., Schoenauer, M., and Sebag, M.
\newblock {APRIL}: {A}ctive {P}reference-learning based {R}einforcement {L}earning.
\newblock In \emph{European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)}, 2012.

\bibitem[An et~al.(2023)An, Lee, Zuo, Kosaka, Kim, and Song]{an2023direct}
An, G., Lee, J., Zuo, X., Kosaka, N., Kim, K.-M., and Song, H.~O.
\newblock {D}irect {P}reference-based {P}olicy {O}ptimization without {R}eward {M}odeling.
\newblock In \emph{{A}dvances in {N}eural {I}nformation {P}rocessing {S}ystems ({NeurIPS})}, 2023.

\bibitem[B{\i}y{\i}k et~al.(2019)B{\i}y{\i}k, Lazar, Sadigh, and Pedarsani]{biyik2019green}
B{\i}y{\i}k, E., Lazar, D.~A., Sadigh, D., and Pedarsani, R.
\newblock {T}he {G}reen {C}hoice: {L}earning and {I}nfluencing {H}uman {D}ecisions on {S}hared {R}oads.
\newblock In \emph{{IEEE} {C}onference on {D}ecision and {C}ontrol ({CDC})}, 2019.

\bibitem[Bradley \& Terry(1952)Bradley and Terry]{bradley1952rank}
Bradley, R.~A. and Terry, M.~E.
\newblock {R}ank {A}nalysis of {I}ncomplete {B}lock {D}esigns: {I}. {T}he {M}ethod of {P}aired {C}omparisons.
\newblock \emph{{B}iometrika}, 39\penalty0 (3/4):\penalty0 324--345, 1952.

\bibitem[Brown et~al.(2019)Brown, Goo, Nagarajan, and Niekum]{brown2019extrapolating}
Brown, D., Goo, W., Nagarajan, P., and Niekum, S.
\newblock {E}xtrapolating {B}eyond {S}uboptimal {D}emonstrations via {I}nverse {R}einforcement {L}earning from {O}bservations.
\newblock In \emph{{I}nternational {C}onference on {M}achine {L}earning ({ICML})}, 2019.

\bibitem[Burges et~al.(2005)Burges, Shaked, Renshaw, Lazier, Deeds, Hamilton, and Hullender]{burges2005learning}
Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N., and Hullender, G.
\newblock {L}earning to {R}ank using {G}radient {D}escent.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2005.

\bibitem[Cao et~al.(2021)Cao, Wong, and Lin]{cao2021weak}
Cao, Z., Wong, K., and Lin, C.-T.
\newblock {W}eak {H}uman {P}reference {S}upervision {F}or {D}eep {R}einforcement {L}earning.
\newblock In \emph{{IEEE} {T}ransactions on {N}eural {N}etworks and {L}earning {S}ystems ({TNNLS})}, 2021.

\bibitem[Casper et~al.(2023)Casper, Davies, Shi, Gilbert, Scheurer, Rando, Freedman, Korbak, Lindner, Freire, et~al.]{casper2023open}
Casper, S., Davies, X., Shi, C., Gilbert, T.~K., Scheurer, J., Rando, J., Freedman, R., Korbak, T., Lindner, D., Freire, P., et~al.
\newblock {O}pen {P}roblems and {F}undamental {L}imitations of {R}einforcement {L}earning from {H}uman {F}eedback.
\newblock In \emph{{arXiv} preprint arXiv:2307.15217}, 2023.

\bibitem[Chebotar et~al.(2019)Chebotar, Handa, Makoviychuk, Macklin, Issac, Ratliff, and Fox]{chebotar2019closing}
Chebotar, Y., Handa, A., Makoviychuk, V., Macklin, M., Issac, J., Ratliff, N., and Fox, D.
\newblock {C}losing the {S}im-to-{R}eal {L}oop: {A}dapting {S}imulation {R}andomization with {R}eal {W}orld {E}xperience.
\newblock In \emph{International Conference on Robotics and Automation (ICRA)}, 2019.

\bibitem[Chen et~al.(2022)Chen, Zhong, Yang, Wang, and Wang]{chen2022human}
Chen, X., Zhong, H., Yang, Z., Wang, Z., and Wang, L.
\newblock {H}uman-in-the-loop: {P}rovably {E}fficient {P}reference-based {R}einforcement {L}earning with {G}eneral {F}unction {A}pproximation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and Amodei]{christiano2017deep}
Christiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.
\newblock Deep {R}einforcement {L}earning from {H}uman {P}references.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.
\newblock {D}4{RL}: {D}atasets for {D}eep {D}ata-{D}riven {R}einforcement {L}earning.
\newblock In \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[F{\"u}rnkranz et~al.(2012)F{\"u}rnkranz, H{\"u}llermeier, Cheng, and Park]{furnkranz2012preference}
F{\"u}rnkranz, J., H{\"u}llermeier, E., Cheng, W., and Park, S.-H.
\newblock {P}reference-based {R}einforcement {L}earning: {A} {F}ormal {F}ramework and a {P}olicy {I}teration {A}lgorithm.
\newblock In \emph{{M}achine {L}earning}, volume~89, pp.\  123--156. Springer, 2012.

\bibitem[Gulcehre et~al.(2020)Gulcehre, Wang, Novikov, Paine, G{\'o}mez, Zolna, Agarwal, Merel, Mankowitz, Paduraru, et~al.]{gulcehre2020rl}
Gulcehre, C., Wang, Z., Novikov, A., Paine, T., G{\'o}mez, S., Zolna, K., Agarwal, R., Merel, J.~S., Mankowitz, D.~J., Paduraru, C., et~al.
\newblock {RL} {U}nplugged: {A} {S}uite of {B}enchmarks for {O}ffline {R}einforcement {L}earning.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock {S}oft {A}ctor-{C}ritic: {O}ff-policy {M}aximum {E}ntropy {D}eep {R}einforcement {L}earning with a {S}tochastic {A}ctor.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Hejna \& Sadigh(2024)Hejna and Sadigh]{hejna2023inverse}
Hejna, J. and Sadigh, D.
\newblock {I}nverse {P}reference {L}earning: {P}reference-based {RL} without a {R}eward {F}unction.
\newblock In \emph{{A}dvances in {N}eural {I}nformation {P}rocessing {S}ystems ({NeurIPS})}, 2024.

\bibitem[Hejna et~al.(2023)Hejna, Rafailov, Sikchi, Finn, Niekum, Knox, and Sadigh]{hejna2023contrastive}
Hejna, J., Rafailov, R., Sikchi, H., Finn, C., Niekum, S., Knox, W.~B., and Sadigh, D.
\newblock {C}ontrastive {P}reference {L}earning: {L}earning from {H}uman {F}eedback without {RL}.
\newblock In \emph{arXiv preprint arXiv:2310.13639}, 2023.

\bibitem[Hejna~III \& Sadigh(2023)Hejna~III and Sadigh]{hejna2023few}
Hejna~III, D.~J. and Sadigh, D.
\newblock {F}ew-{S}hot {P}reference {L}earning for {H}uman-in-the-{L}oop {RL}.
\newblock In \emph{{C}onference on {R}obot {L}earning ({CoRL})}, 2023.

\bibitem[Hwang et~al.(2023)Hwang, Lee, Kee, Kim, Lee, and Oh]{hwang2023sequential}
Hwang, M., Lee, G., Kee, H., Kim, C.~W., Lee, K., and Oh, S.
\newblock Sequential {P}reference {R}anking for {E}fficient {R}einforcement {L}earning from {H}uman {F}eedback.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem[Ibarz et~al.(2018)Ibarz, Leike, Pohlen, Irving, Legg, and Amodei]{ibarz2018reward}
Ibarz, B., Leike, J., Pohlen, T., Irving, G., Legg, S., and Amodei, D.
\newblock {R}eward {L}earning from {H}uman {P}references and {D}emonstrations in {A}tari.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog, Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, et~al.]{kalashnikov2018scalable}
Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E., Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., et~al.
\newblock {S}calable {D}eep {R}einforcement {L}earning for {V}ision-{B}ased {R}obotic {M}anipulation.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2018.

\bibitem[Kang et~al.(2023)Kang, Shi, Liu, He, and Wang]{kang2023beyond}
Kang, Y., Shi, D., Liu, J., He, L., and Wang, D.
\newblock {B}eyond {R}eward: {O}ffline {P}reference-guided {P}olicy {O}ptimization.
\newblock In \emph{{arXiv} preprint arXiv:2305.16217}, 2023.

\bibitem[Kaufmann et~al.(2023)Kaufmann, Weng, Bengs, and H{\"u}llermeier]{kaufmann2023survey}
Kaufmann, T., Weng, P., Bengs, V., and H{\"u}llermeier, E.
\newblock {A} {S}urvey of {R}einforcement {L}earning from {H}uman {F}eedback.
\newblock In \emph{{arXiv} preprint arXiv:2312.14925}, 2023.

\bibitem[Kim et~al.(2022)Kim, Park, Shin, Lee, Abbeel, and Lee]{kim2022preference}
Kim, C., Park, J., Shin, J., Lee, H., Abbeel, P., and Lee, K.
\newblock {P}reference {T}ransformer: {M}odeling {H}uman {P}references using {T}ransformers for {RL}.
\newblock In \emph{{I}nternational {C}onference on {L}earning {R}epresentations ({ICLR})}, 2022.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock {A}dam: {A} {M}ethod for {S}tochastic {O}ptimization.
\newblock In \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kiran et~al.(2021)Kiran, Sobh, Talpaert, Mannion, Al~Sallab, Yogamani, and P{\'e}rez]{kiran2021deep}
Kiran, B.~R., Sobh, I., Talpaert, V., Mannion, P., Al~Sallab, A.~A., Yogamani, S., and P{\'e}rez, P.
\newblock {D}eep {R}einforcement {L}earning for {A}utonomous {D}riving: {A} {S}urvey.
\newblock In \emph{{IEEE} {T}ransactions on {I}ntelligent {T}ransportation {S}ystems ({TITS})}, 2021.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Nair, and Levine]{kostrikov2021offline}
Kostrikov, I., Nair, A., and Levine, S.
\newblock {Offline} {R}einforcement {L}earning with {I}mplicit {Q}-{L}earning.
\newblock In \emph{Advances in Neural Information Processing Systems Workshop (NeurIPS Workshop)}, 2021.

\bibitem[Lee et~al.(2021{\natexlab{a}})Lee, Smith, Dragan, and Abbeel]{lee2021b}
Lee, K., Smith, L., Dragan, A., and Abbeel, P.
\newblock {B-Pref}: {B}enchmarking {P}reference-{B}ased {R}einforcement {L}earning.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2021{\natexlab{a}}.

\bibitem[Lee et~al.(2021{\natexlab{b}})Lee, Smith, and Abbeel]{lee2021pebble}
Lee, K., Smith, L.~M., and Abbeel, P.
\newblock Pebble: {F}eedback-{E}fficient {I}nteractive {R}einforcement {L}earning via {R}elabeling {E}xperience and {U}nsupervised {P}re-training.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021{\natexlab{b}}.

\bibitem[Li et~al.(2023)Li, Misra, Kolobov, and Cheng]{li2023survival}
Li, A., Misra, D., Kolobov, A., and Cheng, C.-A.
\newblock {S}urvival {I}nstinct in {O}ffline {R}einforcement {L}earning.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem[Liang et~al.(2021)Liang, Shu, Lee, and Abbeel]{liang2021reward}
Liang, X., Shu, K., Lee, K., and Abbeel, P.
\newblock {R}eward {U}ncertainty for {E}xploration in {P}reference-based {R}einforcement {L}earning.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2021.

\bibitem[Loshchilov \& Hutter(2018)Loshchilov and Hutter]{loshchilov2018decoupled}
Loshchilov, I. and Hutter, F.
\newblock {D}ecoupled {W}eight {D}ecay {R}egularization.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2018.

\bibitem[Mazoure et~al.(2023)Mazoure, Eysenbach, Nachum, and Tompson]{mazoure2023contrastive}
Mazoure, B., Eysenbach, B., Nachum, O., and Tompson, J.
\newblock {C}ontrastive {V}alue {L}earning: {I}mplicit {M}odels for {S}imple {O}ffline {RL}.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2023.

\bibitem[McKinney et~al.(2023)McKinney, Duan, Krueger, and Gleave]{mckinney2023fragility}
McKinney, L., Duan, Y., Krueger, D., and Gleave, A.
\newblock {O}n {T}he {F}ragility of {L}earned {R}eward {F}unctions.
\newblock In \emph{{arXiv} preprint arXiv:2301.03652}, 2023.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou, Wierstra, and Riedmiller]{mnih2013playing}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M.
\newblock Playing {A}tari with {D}eep {R}einforcement {L}earning.
\newblock In \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Myers et~al.(2022)Myers, Biyik, Anari, and Sadigh]{myers2022learning}
Myers, V., Biyik, E., Anari, N., and Sadigh, D.
\newblock {L}earning {M}ultimodal {R}ewards from {R}ankings.
\newblock In \emph{{C}onference on {R}obot {L}earning ({CoRL})}, 2022.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock {T}raining {L}anguage {M}odels to {F}ollow {I}nstructions with {H}uman {F}eedback.
\newblock In \emph{{A}dvances in {N}eural {I}nformation {P}rocessing {S}ystems ({NeurIPS})}, 2022.

\bibitem[Palan et~al.(2019)Palan, Shevchuk, Charles~Landolfi, and Sadigh]{palan2019learning}
Palan, M., Shevchuk, G., Charles~Landolfi, N., and Sadigh, D.
\newblock {L}earning {R}eward {F}unctions by {I}ntegrating {H}uman {D}emonstrations and {P}references.
\newblock In \emph{Robotics: Science and Systems (RSS)}, 2019.

\bibitem[Park et~al.(2021)Park, Seo, Shin, Lee, Abbeel, and Lee]{park2021surf}
Park, J., Seo, Y., Shin, J., Lee, H., Abbeel, P., and Lee, K.
\newblock {SURF}: {S}emi-supervised {R}eward {L}earning with {D}ata {A}ugmentation for {F}eedback-efficient {P}reference-based {R}einforcement {L}earning.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2021.

\bibitem[Plackett(1975)]{plackett1975analysis}
Plackett, R.~L.
\newblock {T}he {A}nalysis of {P}ermutations.
\newblock \emph{Journal of the Royal Statistical Society Series C: Applied Statistics}, 24\penalty0 (2):\penalty0 193--202, 1975.

\bibitem[Shin et~al.(2022)Shin, Dragan, and Brown]{shin2022benchmarks}
Shin, D., Dragan, A., and Brown, D.~S.
\newblock {B}enchmarks and {A}lgorithms for {O}ffline {P}reference-{B}ased {R}eward {L}earning.
\newblock In \emph{{T}ransactions on {M}achine {L}earning {R}esearch ({TMLR})}, 2022.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang, Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et~al.
\newblock {M}astering the {G}ame of {G}o without {H}uman {K}nowledge.
\newblock \emph{Nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Sinha et~al.(2022)Sinha, Mandlekar, and Garg]{sinha2022s4rl}
Sinha, S., Mandlekar, A., and Garg, A.
\newblock {S}4{RL}: {S}urprisingly {S}imple {S}elf-{S}upervision for {O}ffline {R}einforcement {L}earning in {R}obotics.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2022.

\bibitem[Song et~al.(2024)Song, Yu, Li, Yu, Huang, Li, and Wang]{song2023preference}
Song, F., Yu, B., Li, M., Yu, H., Huang, F., Li, Y., and Wang, H.
\newblock {P}reference {R}anking {O}ptimization for {H}uman {A}lignment.
\newblock In \emph{Association for the Advancement of Artificial Intelligence (AAAI)}, 2024.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, and Christiano]{stiennon2020learning}
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P.~F.
\newblock {L}earning to {S}ummarize from {H}uman {F}eedback.
\newblock In \emph{{A}dvances in {N}eural {I}nformation {P}rocessing {S}ystems ({NeurIPS})}, 2020.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{{R}einforcement {L}earning: {A}n {I}ntroduction}.
\newblock MIT press, 2018.

\bibitem[Swezey et~al.(2021)Swezey, Grover, Charron, and Ermon]{swezey2021pirank}
Swezey, R., Grover, A., Charron, B., and Ermon, S.
\newblock {P}i{R}ank: {S}calable {L}earning to {R}ank via {D}ifferentiable {S}orting.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Tan et~al.(2018)Tan, Zhang, Coumans, Iscen, Bai, Hafner, Bohez, and Vanhoucke]{tan2018sim}
Tan, J., Zhang, T., Coumans, E., Iscen, A., Bai, Y., Hafner, D., Bohez, S., and Vanhoucke, V.
\newblock {S}im-to-{R}eal: {L}earning {A}gile {L}ocomotion for {Q}uadruped {R}obots.
\newblock In \emph{arXiv preprint arXiv:1804.10332}, 2018.

\bibitem[Tarasov et~al.(2023)Tarasov, Nikulin, Akimov, Kurenkov, and Kolesnikov]{tarasov2022corl}
Tarasov, D., Nikulin, A., Akimov, D., Kurenkov, V., and Kolesnikov, S.
\newblock {CORL}: {R}esearch-oriented {D}eep {O}ffline {R}einforcement {L}earning {L}ibrary.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track}, 2023.

\bibitem[Tassa et~al.(2018)Tassa, Doron, Muldal, Erez, Li, Casas, Budden, Abdolmaleki, Merel, Lefrancq, et~al.]{tassa2018deepmind}
Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d.~L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., et~al.
\newblock {D}eep{M}ind {C}ontrol {S}uite.
\newblock In \emph{arXiv preprint arXiv:1801.00690}, 2018.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock {L}lama 2: {O}pen {F}oundation and {F}ine-{T}uned {C}hat {M}odels.
\newblock In \emph{{arXiv} preprint arXiv:2307.09288}, 2023.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik, Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung, J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al.
\newblock {G}randmaster level in {S}tar{C}raft {II} using multi-agent reinforcement learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[White et~al.(2023)White, Wu, Novoseller, Lawhern, Waytowich, and Cao]{white2023rating}
White, D., Wu, M., Novoseller, E., Lawhern, V., Waytowich, N.~R., and Cao, Y.
\newblock {Rating}-based {R}einforcement {L}earning.
\newblock In \emph{International Conference on Machine Learning Workshop (ICML Workshop)}, 2023.

\bibitem[Wilson et~al.(2012)Wilson, Fern, and Tadepalli]{wilson2012bayesian}
Wilson, A., Fern, A., and Tadepalli, P.
\newblock {A} {B}ayesian {A}pproach for {P}olicy {L}earning from {T}rajectory {P}reference {Q}ueries.
\newblock In \emph{{A}dvances in {N}eural {I}nformation {P}rocessing {S}ystems ({NeurIPS})}, 2012.

\bibitem[Wirth et~al.(2017)Wirth, Akrour, Neumann, F{\"u}rnkranz, et~al.]{wirth2017survey}
Wirth, C., Akrour, R., Neumann, G., F{\"u}rnkranz, J., et~al.
\newblock {A} {S}urvey of {P}reference-{B}ased {R}einforcement {L}earning {M}ethods.
\newblock In \emph{Journal of Machine Learning Research (JMLR)}, 2017.

\bibitem[Xia et~al.(2008)Xia, Liu, Wang, Zhang, and Li]{xia2008listwise}
Xia, F., Liu, T.-Y., Wang, J., Zhang, W., and Li, H.
\newblock {L}istwise {A}pproach to {L}earning to {R}ank: {T}heory and {A}lgorithm.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2008.

\bibitem[Xu \& Li(2007)Xu and Li]{xu2007adarank}
Xu, J. and Li, H.
\newblock {A}darank: A {B}oosting {A}lgorithm for {I}nformation {R}etrieval.
\newblock In \emph{International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)}, 2007.

\bibitem[Yu et~al.(2020)Yu, Quillen, He, Julian, Hausman, Finn, and Levine]{yu2020meta}
Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S.
\newblock {M}eta-{W}orld: {A} {B}enchmark and {E}valuation for {M}ulti-{T}ask and {M}eta {R}einforcement {L}earning.
\newblock In \emph{Conference on Robot Learning (CoRL)}, pp.\  1094--1100. PMLR, 2020.

\bibitem[Yu et~al.(2021{\natexlab{a}})Yu, Kumar, Chebotar, Hausman, Levine, and Finn]{yu2021conservative}
Yu, T., Kumar, A., Chebotar, Y., Hausman, K., Levine, S., and Finn, C.
\newblock {C}onservative {D}ata {S}haring for {M}ulti-{T}ask {O}ffline {R}einforcement {L}earning.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2021{\natexlab{a}}.

\bibitem[Yu et~al.(2021{\natexlab{b}})Yu, Kumar, Rafailov, Rajeswaran, Levine, and Finn]{yu2021combo}
Yu, T., Kumar, A., Rafailov, R., Rajeswaran, A., Levine, S., and Finn, C.
\newblock {C}ombo: {C}onservative {O}ffline {M}odel-{B}ased {P}olicy {O}ptimization.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2021{\natexlab{b}}.

\bibitem[Zhang(2023)]{anonymous2023efficient}
Zhang, J.
\newblock {E}fficient {O}ffline {P}reference-{B}ased {R}einforcement {L}earning with {T}ransition-{D}ependent {D}iscounting, 2023.
\newblock URL \url{https://openreview.net/forum?id=7kKyELnAhn}.

\bibitem[Zhao et~al.(2023)Zhao, Joshi, Liu, Khalman, Saleh, and Liu]{zhao2023slic}
Zhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and Liu, P.~J.
\newblock {SLiC}-{HF}: {S}equence {L}ikelihood {C}alibration with {H}uman {F}eedback.
\newblock In \emph{arXiv preprint arXiv:2305.10425}, 2023.

\bibitem[Zhu et~al.(2023)Zhu, Jordan, and Jiao]{zhu2023principled}
Zhu, B., Jordan, M., and Jiao, J.
\newblock {P}rincipled {R}einforcement {L}earning with {H}uman {F}eedback from {P}airwise or {K}-wise {C}omparisons.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2023.

\end{thebibliography}
