@misc{zhu2018texygen,
      title={Texygen: A Benchmarking Platform for Text Generation Models}, 
      author={Yaoming Zhu and Sidi Lu and Lei Zheng and Jiaxian Guo and Weinan Zhang and Jun Wang and Yong Yu},
      year={2018},
      eprint={1802.01886},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{zheng2024llamafactory,
  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},
  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Yongqiang Ma},
  journal={arXiv preprint arXiv:2403.13372},
  year={2024},
  url={http://arxiv.org/abs/2403.13372}
}

@article{ye2024justice,
  title={Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge},
  author={Ye, Jiayi and Wang, Yanbo and Huang, Yue and Chen, Dongping and Zhang, Qihui and Moniz, Nuno and Gao, Tian and Geyer, Werner and Huang, Chao and Chen, Pin-Yu and others},
  journal={arXiv preprint arXiv:2410.02736},
  year={2024}
}

@misc{GPT-4,
    title={GPT-4},
    author={OpenAI},
    year={2023},
    note={\url{https://openai.com/gpt-4}}
}

@online{ChatGPT,
    title={ChatGPT},
    author={OpenAI},
    year={2023},
    note ={\url{https://openai.com/product/chatgpt}}
}

@online{MistralAI,
    title={Mistral AI},
    author={OpenAI},
    note ={\url{https://mistral.ai/company/}},
    year={2024},
}

@online{honest_sycophancy,
    title={Reducing sycophancy and improving honesty via activation steering},
    author={Nina Rimsky},
    year={2024},
    note ={\url{https://www.alignmentforum.org/posts/zt6hRsDE84HeBKh7E/reducing-sycophancy-and-improving-honesty-via-activation}}
}




@article{rlhf,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@misc{lin2023generating,
      title={Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models}, 
      author={Zhen Lin and Shubhendu Trivedi and Jimeng Sun},
      year={2023},
      eprint={2305.19187},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@misc{xiong2024llms,
      title={Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs}, 
      author={Miao Xiong and Zhiyuan Hu and Xinyang Lu and Yifei Li and Jie Fu and Junxian He and Bryan Hooi},
      year={2024},
      eprint={2306.13063},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lyu2024calibrating,
      title={Calibrating Large Language Models with Sample Consistency}, 
      author={Qing Lyu and Kumar Shridhar and Chaitanya Malaviya and Li Zhang and Yanai Elazar and Niket Tandon and Marianna Apidianaki and Mrinmaya Sachan and Chris Callison-Burch},
      year={2024},
      eprint={2402.13904},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{ling2024uncertainty,
      title={Uncertainty Quantification for In-Context Learning of Large Language Models}, 
      author={Chen Ling and Xujiang Zhao and Xuchao Zhang and Wei Cheng and Yanchi Liu and Yiyou Sun and Mika Oishi and Takao Osaki and Katsushi Matsuda and Jie Ji and Guangji Bai and Liang Zhao and Haifeng Chen},
      year={2024},
      eprint={2402.10189},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xiao2022uncertainty,
      title={Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis}, 
      author={Yuxin Xiao and Paul Pu Liang and Umang Bhatt and Willie Neiswanger and Ruslan Salakhutdinov and Louis-Philippe Morency},
      year={2022},
      eprint={2210.04714},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{cheng2024ai,
      title={Can AI Assistants Know What They Don't Know?}, 
      author={Qinyuan Cheng and Tianxiang Sun and Xiangyang Liu and Wenwei Zhang and Zhangyue Yin and Shimin Li and Linyang Li and Zhengfu He and Kai Chen and Xipeng Qiu},
      year={2024},
      eprint={2401.13275},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{huang2024fakegpt,
      title={FakeGPT: Fake News Generation, Explanation and Detection of Large Language Models}, 
      author={Yue Huang and Lichao Sun},
      year={2024},
      eprint={2310.05046},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{LLAMA2,
    title={LLAMA 2},
    author={Meta},
    year={2023},
    note={\url{https://llama.meta.com/llama2}}
}

@misc{LLAMA3,
    title={LLAMA 3},
    author={Meta},
    year={2023},
    note={\url{https://llama.meta.com/llama3}}
}

@misc{zheng2024promptdriven,
      title={On Prompt-Driven Safeguarding for Large Language Models}, 
      author={Chujie Zheng and Fan Yin and Hao Zhou and Fandong Meng and Jie Zhou and Kai-Wei Chang and Minlie Huang and Nanyun Peng},
      year={2024},
      eprint={2401.18018},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{yin2023large,
      title={Do Large Language Models Know What They Don't Know?}, 
      author={Zhangyue Yin and Qiushi Sun and Qipeng Guo and Jiawen Wu and Xipeng Qiu and Xuanjing Huang},
      year={2023},
      eprint={2305.18153},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{qian2023communicative,
  title={Communicative agents for software development},
  author={Qian, Chen and Cong, Xin and Yang, Cheng and Chen, Weize and Su, Yusheng and Xu, Juyuan and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2307.07924},
  year={2023}
}

@article{zhang2023siren,
  title={Siren's song in the AI ocean: a survey on hallucination in large language models},
  author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others},
  journal={arXiv preprint arXiv:2309.01219},
  year={2023}
}

@article{kasneci2023chatgpt,
  title={ChatGPT for good? On opportunities and challenges of large language models for education},
  author={Kasneci, Enkelejda and Se{\ss}ler, Kathrin and K{\"u}chemann, Stefan and Bannert, Maria and Dementieva, Daryna and Fischer, Frank and Gasser, Urs and Groh, Georg and G{\"u}nnemann, Stephan and H{\"u}llermeier, Eyke and others},
  journal={Learning and individual differences},
  volume={103},
  pages={102274},
  year={2023},
  publisher={Elsevier}
}


@misc{Claude,
    title={Claude},
    author={Anthropic},
    year={2023},
    note={\url{https://www.anthropic.com/claude}}
}

@misc{chen2024mllmasajudge,
      title={MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark}, 
      author={Dongping Chen and Ruoxi Chen and Shilin Zhang and Yinuo Liu and Yaochen Wang and Huichi Zhou and Qihui Zhang and Pan Zhou and Yao Wan and Lichao Sun},
      year={2024},
      eprint={2402.04788},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{liu2023alignbench,
  title={Alignbench: Benchmarking chinese alignment of large language models},
  author={Liu, Xiao and Lei, Xuanyu and Wang, Shengyuan and Huang, Yue and Feng, Zhuoer and Wen, Bosi and Cheng, Jiale and Ke, Pei and Xu, Yifan and Tam, Weng Lam and others},
  journal={arXiv preprint arXiv:2311.18743},
  year={2023}
}

@article{huang2023harnessing,
  title={Harnessing the power of chatgpt in fake news: An in-depth exploration in generation, detection and explanation},
  author={Huang, Yue and Sun, Lichao},
  journal={arXiv preprint arXiv:2310.05046},
  year={2023}
}

@misc{sun2023principledriven,
      title={Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision}, 
      author={Zhiqing Sun and Yikang Shen and Qinhong Zhou and Hongxin Zhang and Zhenfang Chen and David Cox and Yiming Yang and Chuang Gan},
      year={2023},
      eprint={2305.03047},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lee2023rlaif,
      title={RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback}, 
      author={Harrison Lee and Samrat Phatale and Hassan Mansoor and Thomas Mesnard and Johan Ferret and Kellie Lu and Colton Bishop and Ethan Hall and Victor Carbune and Abhinav Rastogi and Sushant Prakash},
      year={2023},
      eprint={2309.00267},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{petridis2023constitutionmaker,
      title={ConstitutionMaker: Interactively Critiquing Large Language Models by Converting Feedback into Principles}, 
      author={Savvas Petridis and Ben Wedin and James Wexler and Aaron Donsbach and Mahima Pushkarna and Nitesh Goyal and Carrie J. Cai and Michael Terry},
      year={2023},
      eprint={2310.15428},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}


@misc{liu2023agentbench,
      title={AgentBench: Evaluating LLMs as Agents}, 
      author={Xiao Liu and Hao Yu and Hanchen Zhang and Yifan Xu and Xuanyu Lei and Hanyu Lai and Yu Gu and Hangliang Ding and Kaiwen Men and Kejuan Yang and Shudan Zhang and Xiang Deng and Aohan Zeng and Zhengxiao Du and Chenhui Zhang and Sheng Shen and Tianjun Zhang and Yu Su and Huan Sun and Minlie Huang and Yuxiao Dong and Jie Tang},
      year={2023},
      eprint={2308.03688},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}



@inproceedings{curri,
author = {Bengio, Yoshua and Louradour, J\'{e}r\^{o}me and Collobert, Ronan and Weston, Jason},
title = {Curriculum learning},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553380},
doi = {10.1145/1553374.1553380},
abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculum learning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {41–48},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@misc{zheng2023judging,
      title={Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ke2023critiquellm,
      title={CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation}, 
      author={Pei Ke and Bosi Wen and Zhuoer Feng and Xiao Liu and Xuanyu Lei and Jiale Cheng and Shengyuan Wang and Aohan Zeng and Yuxiao Dong and Hongning Wang and Jie Tang and Minlie Huang},
      year={2023},
      eprint={2311.18702},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@online{text-embedding-ada-002,
    title        = {text-embedding-ada-002},
    author       = {OpenAI},
    year={2024},
    note =  {\url{https://platform.openai.com/docs/guides/embeddings}}
}


@misc{kim2024prometheus,
      title={Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models}, 
      author={Seungone Kim and Juyoung Suk and Shayne Longpre and Bill Yuchen Lin and Jamin Shin and Sean Welleck and Graham Neubig and Moontae Lee and Kyungjae Lee and Minjoon Seo},
      year={2024},
      eprint={2405.01535},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{Anthropic,
    title={Anthropic},
    year={2024},
    note={\url{https://www.anthropic.com/}}
}

@online{metaailab,
    title={AI at Meta},
    year={2024},
    author={Meta},
    note ={\url{https://ai.meta.com}}
}

@online{OpenAI,
    title={OpenAI},
    year={2024},
    note ={\url{https://openai.com/}}
}





@article{abdi2010principal,
  title={Principal component analysis},
  author={Abdi, Herv{\'e} and Williams, Lynne J},
  journal={Wiley interdisciplinary reviews: computational statistics},
  volume={2},
  number={4},
  pages={433--459},
  year={2010},
  publisher={Wiley Online Library}
}

@misc{Ernie,
    title={Ernie},
    author={Baidu},
    year={2023},
    note={\url{https://yiyan.baidu.com/welcome}}
}



@misc{tonmoy2024comprehensive,
      title={A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models}, 
      author={S. M Towhidul Islam Tonmoy and S M Mehedi Zaman and Vinija Jain and Anku Rani and Vipula Rawte and Aman Chadha and Amitava Das},
      year={2024},
      eprint={2401.01313},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2023aligning,
      title={Aligning Large Language Models with Human: A Survey}, 
      author={Yufei Wang and Wanjun Zhong and Liangyou Li and Fei Mi and Xingshan Zeng and Wenyong Huang and Lifeng Shang and Xin Jiang and Qun Liu},
      year={2023},
      eprint={2307.12966},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sun2024easytohard,
      title={Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision}, 
      author={Zhiqing Sun and Longhui Yu and Yikang Shen and Weiyang Liu and Yiming Yang and Sean Welleck and Chuang Gan},
      year={2024},
      eprint={2403.09472},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lai2024alarm,
      title={ALaRM: Align Language Models via Hierarchical Rewards Modeling}, 
      author={Yuhang Lai and Siyuan Wang and Shujun Liu and Xuanjing Huang and Zhongyu Wei},
      year={2024},
      eprint={2403.06754},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{handa2024bayesian,
      title={Bayesian Preference Elicitation with Language Models}, 
      author={Kunal Handa and Yarin Gal and Ellie Pavlick and Noah Goodman and Jacob Andreas and Alex Tamkin and Belinda Z. Li},
      year={2024},
      eprint={2403.05534},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ji2024aligner,
      title={Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction}, 
      author={Jiaming Ji and Boyuan Chen and Hantao Lou and Donghai Hong and Borong Zhang and Xuehai Pan and Juntao Dai and Yaodong Yang},
      year={2024},
      eprint={2402.02416},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{li2024quantifying,
  title={Quantifying AI Psychology: A Psychometrics Benchmark for Large Language Models},
  author={Li, Yuan and Huang, Yue and Wang, Hongyi and Zhang, Xiangliang and Zou, James and Sun, Lichao},
  journal={arXiv preprint arXiv:2406.17675},
  year={2024}
}

@article{wu2024unigen,
  title={UniGen: A Unified Framework for Textual Dataset Generation Using Large Language Models},
  author={Wu, Siyuan and Huang, Yue and Gao, Chujie and Chen, Dongping and Zhang, Qihui and Wan, Yao and Zhou, Tianyi and Zhang, Xiangliang and Gao, Jianfeng and Xiao, Chaowei and others},
  journal={arXiv preprint arXiv:2406.18966},
  year={2024}
}

@article{chen2024gui,
  title={GUI-WORLD: A Dataset for GUI-oriented Multimodal LLM-based Agents},
  author={Chen, Dongping and Huang, Yue and Wu, Siyuan and Tang, Jingyu and Chen, Liuyi and Bai, Yilin and He, Zhigang and Wang, Chenlong and Zhou, Huichi and Li, Yiqiang and others},
  journal={arXiv preprint arXiv:2406.10819},
  year={2024}
}

@article{huang2024obscureprompt,
  title={ObscurePrompt: Jailbreaking Large Language Models via Obscure Input},
  author={Huang, Yue and Tang, Jingyu and Chen, Dongping and Tang, Bingda and Wan, Yao and Sun, Lichao and Zhang, Xiangliang},
  journal={arXiv preprint arXiv:2406.13662},
  year={2024}
}

@article{wu2024can,
  title={Can Large Language Models Automatically Jailbreak GPT-4V?},
  author={Wu, Yuanwei and Huang, Yue and Liu, Yixin and Li, Xiang and Zhou, Pan and Sun, Lichao},
  journal={arXiv preprint arXiv:2407.16686},
  year={2024}
}

@article{shi2024optimization,
  title={Optimization-based Prompt Injection Attack to LLM-as-a-Judge},
  author={Shi, Jiawen and Yuan, Zenghui and Liu, Yinuo and Huang, Yue and Zhou, Pan and Sun, Lichao and Gong, Neil Zhenqiang},
  journal={arXiv preprint arXiv:2403.17710},
  year={2024}
}

@misc{wang2023donotanswer,
      title={Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs}, 
      author={Yuxia Wang and Haonan Li and Xudong Han and Preslav Nakov and Timothy Baldwin},
      year={2023},
      eprint={2308.13387},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zou2023universal,
      title={Universal and Transferable Adversarial Attacks on Aligned Language Models}, 
      author={Andy Zou and Zifan Wang and Nicholas Carlini and Milad Nasr and J. Zico Kolter and Matt Fredrikson},
      year={2023},
      eprint={2307.15043},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{schulman2017proximal,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{rafailov2023direct,
      title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model}, 
      author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
      year={2023},
      eprint={2305.18290},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{cheng2023blackbox,
      title={Black-Box Prompt Optimization: Aligning Large Language Models without Model Training}, 
      author={Jiale Cheng and Xiao Liu and Kehan Zheng and Pei Ke and Hongning Wang and Yuxiao Dong and Jie Tang and Minlie Huang},
      year={2023},
      eprint={2311.04155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shen2023large,
      title={Large Language Model Alignment: A Survey}, 
      author={Tianhao Shen and Renren Jin and Yufei Huang and Chuang Liu and Weilong Dong and Zishan Guo and Xinwei Wu and Yan Liu and Deyi Xiong},
      year={2023},
      eprint={2309.15025},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{huang2024vaccine,
      title={Vaccine: Perturbation-aware Alignment for Large Language Model}, 
      author={Tiansheng Huang and Sihao Hu and Ling Liu},
      year={2024},
      eprint={2402.01109},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{wolf2023fundamental,
  title={Fundamental limitations of alignment in large language models},
  author={Wolf, Yotam and Wies, Noam and Levine, Yoav and Shashua, Amnon},
  journal={arXiv preprint arXiv:2304.11082},
  year={2023}
}

@article{zhou2024lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@misc{ranaldi2023large,
      title={When Large Language Models contradict humans? Large Language Models' Sycophantic Behaviour}, 
      author={Leonardo Ranaldi and Giulia Pucci},
      year={2023},
      eprint={2311.09410},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wei2024simple,
      title={Simple synthetic data reduces sycophancy in large language models}, 
      author={Jerry Wei and Da Huang and Yifeng Lu and Denny Zhou and Quoc V. Le},
      year={2024},
      eprint={2308.03958},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2024i,
      title={I Think, Therefore I am: Benchmarking Awareness of Large Language Models Using AwareBench}, 
      author={Yuan Li and Yue Huang and Yuli Lin and Siyuan Wu and Yao Wan and Lichao Sun},
      year={2024},
      eprint={2401.17882},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{liu2023deid,
  title={Deid-gpt: Zero-shot medical text de-identification by gpt-4},
  author={Liu, Zhengliang and Huang, Yue and Yu, Xiaowei and Zhang, Lu and Wu, Zihao and Cao, Chao and Dai, Haixing and Zhao, Lin and Li, Yiwei and Shu, Peng and others},
  journal={arXiv preprint arXiv:2303.11032},
  year={2023}
}

@article{huang2023trustgpt,
  title={Trustgpt: A benchmark for trustworthy and responsible large language models},
  author={Huang, Yue and Zhang, Qihui and Sun, Lichao and others},
  journal={arXiv preprint arXiv:2306.11507},
  year={2023}
}

@misc{zhu2023promptbench,
      title={PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts}, 
      author={Kaijie Zhu and Jindong Wang and Jiaheng Zhou and Zichen Wang and Hao Chen and Yidong Wang and Linyi Yang and Wei Ye and Yue Zhang and Neil Zhenqiang Gong and Xing Xie},
      year={2023},
      eprint={2306.04528},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2023privacy,
      title={Privacy in Large Language Models: Attacks, Defenses and Future Directions}, 
      author={Haoran Li and Yulin Chen and Jinglong Luo and Yan Kang and Xiaojin Zhang and Qi Hu and Chunkit Chan and Yangqiu Song},
      year={2023},
      eprint={2310.10383},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhang2023safetybench,
      title={SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions}, 
      author={Zhexin Zhang and Leqi Lei and Lindong Wu and Rui Sun and Yongkang Huang and Chong Long and Xiao Liu and Xuanyu Lei and Jie Tang and Minlie Huang},
      year={2023},
      eprint={2309.07045},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wei2023jailbroken,
      title={Jailbroken: How Does LLM Safety Training Fail?}, 
      author={Alexander Wei and Nika Haghtalab and Jacob Steinhardt},
      year={2023},
      eprint={2307.02483},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@misc{huang2023survey,
      title={A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions}, 
      author={Lei Huang and Weijiang Yu and Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qin and Ting Liu},
      year={2023},
      eprint={2311.05232},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{perez2022discovering,
  doi = {10.48550/ARXIV.2212.09251},
  url = {https://arxiv.org/abs/2212.09251},
  author = {Perez, Ethan and Ringer, Sam and Lukošiūtė, Kamilė and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and Jones, Andy and Chen, Anna and Mann, Ben and Israel, Brian and Seethor, Bryan and McKinnon, Cameron and Olah, Christopher and Yan, Da and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Khundadze, Guro and Kernion, Jackson and Landis, James and Kerr, Jamie and Mueller, Jared and Hyun, Jeeyoon and Landau, Joshua and Ndousse, Kamal and Goldberg, Landon and Lovitt, Liane and Lucas, Martin and Sellitto, Michael and Zhang, Miranda and Kingsland, Neerav and Elhage, Nelson and Joseph, Nicholas and Mercado, Noemí and DasSarma, Nova and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Johnston, Scott and Kravec, Shauna and {El Showk}, Sheer and Lanham, Tamera and Telleen-Lawton, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Clark, Jack and Bowman, Samuel R. and Askell, Amanda and Grosse, Roger and Hernandez, Danny and Ganguli, Deep and Hubinger, Evan and Schiefer, Nicholas and Kaplan, Jared},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Discovering Language Model Behaviors with Model-Written Evaluations},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{Yue2023metatool,
  title={Metatool benchmark for large language models: Deciding whether to use tools and which to use},
  author={Huang, Yue and Shi, Jiawen and Li, Yuan and Fan, Chenrui and Wu, Siyuan and Zhang, Qihui and Liu, Yixin and Zhou, Pan and Wan, Yao and Gong, Neil Zhenqiang and others},
  journal={arXiv preprint arXiv:2310.03128},
  year={2023}
}


@article{zhang2024mm,
  title={Mm-llms: Recent advances in multimodal large language models},
  author={Zhang, Duzhen and Yu, Yahan and Li, Chenxing and Dong, Jiahua and Su, Dan and Chu, Chenhui and Yu, Dong},
  journal={arXiv preprint arXiv:2401.13601},
  year={2024}
}

@article{rissling2013demand,
  title={Demand and modality of directed attention modulate “pre-attentive” sensory processes in schizophrenia patients and nonpsychiatric controls},
  author={Rissling, Anthony J and Park, Sung-Hyouk and Young, Jared W and Rissling, Michelle B and Sugar, Catherine A and Sprock, Joyce and Mathias, Daniel J and Pela, Marlena and Sharp, Richard F and Braff, David L and others},
  journal={Schizophrenia research},
  volume={146},
  number={1-3},
  pages={326--335},
  year={2013},
  publisher={Elsevier}
}

@article{peng2023instruction,
  title={Instruction tuning with gpt-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@misc{askell2021general,
      title={A General Language Assistant as a Laboratory for Alignment}, 
      author={Amanda Askell and Yuntao Bai and Anna Chen and Dawn Drain and Deep Ganguli and Tom Henighan and Andy Jones and Nicholas Joseph and Ben Mann and Nova DasSarma and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Jackson Kernion and Kamal Ndousse and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Jared Kaplan},
      year={2021},
      eprint={2112.00861},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{yang2023alignment,
      title={Alignment for Honesty}, 
      author={Yuqing Yang and Ethan Chern and Xipeng Qiu and Graham Neubig and Pengfei Liu},
      year={2023},
      eprint={2312.07000},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{chen2023misinfo,
  title={Combating misinformation in the age of llms: Opportunities and challenges},
  author={Chen, Canyu and Shu, Kai},
  journal={arXiv preprint arXiv:2311.05656},
  year={2023}
}

@article{evans2021truthful,
  title={Truthful AI: Developing and governing AI that does not lie},
  author={Evans, Owain and Cotton-Barratt, Owen and Finnveden, Lukas and Bales, Adam and Balwit, Avital and Wills, Peter and Righetti, Luca and Saunders, William},
  journal={arXiv preprint arXiv:2110.06674},
  year={2021}
}

@article{park2023ai,
  title={AI deception: A survey of examples, risks, and potential solutions},
  author={Park, Peter S and Goldstein, Simon and O'Gara, Aidan and Chen, Michael and Hendrycks, Dan},
  journal={arXiv preprint arXiv:2308.14752},
  year={2023}
}

@article{qin2023toolllm,
  title={Toolllm: Facilitating large language models to master 16000+ real-world apis},
  author={Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and others},
  journal={arXiv preprint arXiv:2307.16789},
  year={2023}
}

@article{Tang_Deng_Lin_Han_Liang_Sun_2023,
  title={Toolalpaca: Generalized tool learning for language models with 3000 simulated cases},
  author={Tang, Qiaoyu and Deng, Ziliang and Lin, Hongyu and Han, Xianpei and Liang, Qiao and Sun, Le},
  journal={arXiv preprint arXiv:2306.05301},
  year={2023}
}

@article{yang2024gpt4tools,
  title={Gpt4tools: Teaching large language model to use tools via self-instruction},
  author={Yang, Rui and Song, Lin and Li, Yanwei and Zhao, Sijie and Ge, Yixiao and Li, Xiu and Shan, Ying},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{li2023halueval,
  title={Halueval: A large-scale hallucination evaluation benchmark for large language models},
  author={Li, Junyi and Cheng, Xiaoxue and Zhao, Xin and Nie, Jian-Yun and Wen, Ji-Rong},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

@article{liu2023trustworthy,
  title={Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment},
  author={Liu, Yang and Yao, Yuanshun and Ton, Jean-Francois and Zhang, Xiaoying and Cheng, Ruocheng Guo Hao and Klochkov, Yegor and Taufiq, Muhammad Faaiz and Li, Hang},
  journal={arXiv preprint arXiv:2308.05374},
  year={2023}
}

@article{sun2024trustllm,
  title={Trustllm: Trustworthiness in large language models},
  author={Huang, Yue and Sun, Lichao and Wang, Haoran and Wu, Siyuan and Zhang, Qihui and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan and Li, Xiner and others},
  journal={arXiv preprint arXiv:2401.05561},
  year={2024}
}

@article{wang2023decodingtrust,
  title={Decodingtrust: A comprehensive assessment of trustworthiness in gpt models},
  author={Wang, Boxin and Chen, Weixin and Pei, Hengzhi and Xie, Chulin and Kang, Mintong and Zhang, Chenhui and Xu, Chejian and Xiong, Zidi and Dutta, Ritik and Schaeffer, Rylan and others},
  journal={arXiv preprint arXiv:2306.11698},
  year={2023}
}

@article{ouyang2022RLHF,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@misc{miyai2024unsolvable,
      title={Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models}, 
      author={Atsuyuki Miyai and Jingkang Yang and Jingyang Zhang and Yifei Ming and Qing Yu and Go Irie and Yixuan Li and Hai Li and Ziwei Liu and Kiyoharu Aizawa},
      year={2024},
      eprint={2403.20331},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}



@misc{ji2024alignmentsurvey,
      title={AI Alignment: A Comprehensive Survey}, 
      author={Jiaming Ji and Tianyi Qiu and Boyuan Chen and Borong Zhang and Hantao Lou and Kaile Wang and Yawen Duan and Zhonghao He and Jiayi Zhou and Zhaowei Zhang and Fanzhi Zeng and Kwan Yee Ng and Juntao Dai and Xuehai Pan and Aidan O'Gara and Yingshan Lei and Hua Xu and Brian Tse and Jie Fu and Stephen McAleer and Yaodong Yang and Yizhou Wang and Song-Chun Zhu and Yike Guo and Wen Gao},
      year={2024},
      eprint={2310.19852},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}


@misc{sharma2023understanding,
      title={Towards Understanding Sycophancy in Language Models}, 
      author={Mrinank Sharma and Meg Tong and Tomasz Korbak and David Duvenaud and Amanda Askell and Samuel R. Bowman and Newton Cheng and Esin Durmus and Zac Hatfield-Dodds and Scott R. Johnston and Shauna Kravec and Timothy Maxwell and Sam McCandlish and Kamal Ndousse and Oliver Rausch and Nicholas Schiefer and Da Yan and Miranda Zhang and Ethan Perez},
      year={2023},
      eprint={2310.13548},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{deng2024gotcha,
      title={Gotcha! Don't trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions}, 
      author={Yang Deng and Yong Zhao and Moxin Li and See-Kiong Ng and Tat-Seng Chua},
      year={2024},
      eprint={2402.15062},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xu2024earth,
      title={The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation}, 
      author={Rongwu Xu and Brian S. Lin and Shujian Yang and Tianqi Zhang and Weiyan Shi and Tianwei Zhang and Zhixuan Fang and Wei Xu and Han Qiu},
      year={2024},
      eprint={2312.09085},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@misc{hou2024chatglmrlhf,
      title={ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback}, 
      author={Zhenyu Hou and Yilin Niu and Zhengxiao Du and Xiaohan Zhang and Xiao Liu and Aohan Zeng and Qinkai Zheng and Minlie Huang and Hongning Wang and Jie Tang and Yuxiao Dong},
      year={2024},
      eprint={2404.00934},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{feng2024dont,
      title={Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration}, 
      author={Shangbin Feng and Weijia Shi and Yike Wang and Wenxuan Ding and Vidhisha Balachandran and Yulia Tsvetkov},
      year={2024},
      eprint={2402.00367},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gemini2023,
  author = {Gemini Team and Anil, R. and Borgeaud, S. and Wu, Y. and Alayrac, J.-B. and Yu, J. and Soricut, R. and Schalkwyk, J. and Dai, A. M. and Hauth, A. and Millican, K. and others},
  title = {Gemini: A family of highly capable multimodal models},
  journal = {arXiv preprint arXiv:2312.11805},
  year = {2023}
}


@misc{bai2022training,
      title={Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback}, 
      author={Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
      year={2022},
      eprint={2204.05862},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{liu2024large,
  title={How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?},
  author={Liu, Ryan and Sumers, Theodore R and Dasgupta, Ishita and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:2402.07282},
  year={2024}
}

@misc{zeng2023glm130b,
      title={GLM-130B: An Open Bilingual Pre-trained Model}, 
      author={Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and Weng Lam Tam and Zixuan Ma and Yufei Xue and Jidong Zhai and Wenguang Chen and Peng Zhang and Yuxiao Dong and Jie Tang},
      year={2023},
      eprint={2210.02414},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@misc{ai2024yi,
      title={Yi: Open Foundation Models by 01.AI}, 
      author={01. AI and : and Alex Young and Bei Chen and Chao Li and Chengen Huang and Ge Zhang and Guanwei Zhang and Heng Li and Jiangcheng Zhu and Jianqun Chen and Jing Chang and Kaidong Yu and Peng Liu and Qiang Liu and Shawn Yue and Senbin Yang and Shiming Yang and Tao Yu and Wen Xie and Wenhao Huang and Xiaohui Hu and Xiaoyi Ren and Xinyao Niu and Pengcheng Nie and Yuchi Xu and Yudong Liu and Yue Wang and Yuxuan Cai and Zhenyu Gu and Zhiyuan Liu and Zonghong Dai},
      year={2024},
      eprint={2403.04652},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{khurana2023natural,
  title={Natural language processing: State of the art, current trends and challenges},
  author={Khurana, Diksha and Koli, Aditya and Khatter, Kiran and Singh, Sukhdev},
  journal={Multimedia tools and applications},
  volume={82},
  number={3},
  pages={3713--3744},
  year={2023},
  publisher={Springer}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@inproceedings{chen2022knowprompt,
  title={Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization for relation extraction},
  author={Chen, Xiang and Zhang, Ningyu and Xie, Xin and Deng, Shumin and Yao, Yunzhi and Tan, Chuanqi and Huang, Fei and Si, Luo and Chen, Huajun},
  booktitle={Proceedings of the ACM Web conference 2022},
  pages={2778--2788},
  year={2022}
}

@misc{white2023prompengineering,
      title={A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT}, 
      author={Jules White and Quchen Fu and Sam Hays and Michael Sandborn and Carlos Olea and Henry Gilbert and Ashraf Elnashar and Jesse Spencer-Smith and Douglas C. Schmidt},
      year={2023},
      eprint={2302.11382},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and others},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

@misc{dong2023ICLsurvey,
      title={A Survey on In-context Learning}, 
      author={Qingxiu Dong and Lei Li and Damai Dai and Ce Zheng and Zhiyong Wu and Baobao Chang and Xu Sun and Jingjing Xu and Lei Li and Zhifang Sui},
      year={2023},
      eprint={2301.00234},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{sadat2023delucionqa,
  title={Delucionqa: Detecting hallucinations in domain-specific question answering},
  author={Sadat, Mobashir and Zhou, Zhengyu and Lange, Lukas and Araki, Jun and Gundroo, Arsalan and Wang, Bingqing and Menon, Rakesh R and Parvez, Md Rizwan and Feng, Zhe},
  journal={arXiv preprint arXiv:2312.05200},
  year={2023}
}

@article{zhao2023domain,
  title={Domain specialization as the key to make large language models disruptive: A comprehensive survey},
  author={Zhao, Xujiang and Lu, Jiaying and Deng, Chengyuan and Zheng, Can and Wang, Junxiang and Chowdhury, Tanmoy and Yun, Li and Cui, Hejie and Xuchao, Zhang and Zhao, Tianjiao and others},
  journal={arXiv preprint arXiv:2305.18703},
  year={2023}
}

@article{kim2024aligning,
  title={Aligning Language Models to Explicitly Handle Ambiguity},
  author={Kim, Hyuhng Joon and Kim, Youna and Park, Cheonbok and Kim, Junyeob and Park, Choonghyun and Yoo, Kang Min and Lee, Sang-goo and Kim, Taeuk},
  journal={arXiv preprint arXiv:2404.11972},
  year={2024}
}


@article{zhuang2024toolqa,
  title={Toolqa: A dataset for llm question answering with external tools},
  author={Zhuang, Yuchen and Yu, Yue and Wang, Kuan and Sun, Haotian and Zhang, Chao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@book{lurz2009philosophy,
  title={The philosophy of animal minds},
  author={Lurz, Robert W},
  year={2009},
  publisher={Cambridge University Press}
}

@article{mahowald2024dissociating,
  title={Dissociating language and thought in large language models},
  author={Mahowald, Kyle and Ivanova, Anna A and Blank, Idan A and Kanwisher, Nancy and Tenenbaum, Joshua B and Fedorenko, Evelina},
  journal={Trends in Cognitive Sciences},
  year={2024},
  publisher={Elsevier}
}

@article{berglund2023taken,
  title={Taken out of context: On measuring situational awareness in LLMs},
  author={Berglund, Lukas and Stickland, Asa Cooper and Balesni, Mikita and Kaufmann, Max and Tong, Meg and Korbak, Tomasz and Kokotajlo, Daniel and Evans, Owain},
  journal={arXiv preprint arXiv:2309.00667},
  year={2023}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}