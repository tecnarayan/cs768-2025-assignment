\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abernethy et~al.(2011)Abernethy, Bartlett, and
  Hazan]{abernethy2011blackwell}
Jacob Abernethy, Peter~L Bartlett, and Elad Hazan.
\newblock Blackwell approachability and no-regret learning are equivalent.
\newblock In \emph{Proceedings of the 24th Annual Conference on Learning
  Theory}, pages 27--46, 2011.

\bibitem[Agrawal and Devanur(2014)]{agrawal2014bandits}
Shipra Agrawal and Nikhil~R Devanur.
\newblock Bandits with concave rewards and convex knapsacks.
\newblock In \emph{Proceedings of the fifteenth ACM conference on Economics and
  computation}, pages 989--1006, 2014.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 263--272. JMLR. org, 2017.

\bibitem[Badanidiyuru et~al.(2013)Badanidiyuru, Kleinberg, and
  Slivkins]{badanidiyuru2013bandits}
Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins.
\newblock Bandits with knapsacks.
\newblock In \emph{2013 IEEE 54th Annual Symposium on Foundations of Computer
  Science}, pages 207--216. IEEE, 2013.

\bibitem[Bai and Jin(2020)]{bai2020provable}
Yu~Bai and Chi Jin.
\newblock Provable self-play algorithms for competitive reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2002.04017}, 2020.

\bibitem[Bai et~al.(2020)Bai, Jin, and Yu]{bai2020near}
Yu~Bai, Chi Jin, and Tiancheng Yu.
\newblock Near-optimal reinforcement learning with self-play.
\newblock \emph{arXiv preprint arXiv:2006.12007}, 2020.

\bibitem[Blackwell(1956)]{blackwell1956analog}
David Blackwell.
\newblock An analog of the minimax theorem for vector payoffs.
\newblock \emph{Pacific Journal of Mathematics}, 6\penalty0 (1):\penalty0 1--8,
  1956.

\bibitem[Brafman and Tennenholtz(2002)]{brafman2002r}
Ronen~I Brafman and Moshe Tennenholtz.
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Oct):\penalty0 213--231, 2002.

\bibitem[Brantley et~al.(2020)Brantley, Dudik, Lykouris, Miryoosefi,
  Simchowitz, Slivkins, and Sun]{brantley2020constrained}
Kiant{\'e} Brantley, Miroslav Dudik, Thodoris Lykouris, Sobhan Miryoosefi, Max
  Simchowitz, Aleksandrs Slivkins, and Wen Sun.
\newblock Constrained episodic reinforcement learning in concave-convex and
  knapsack settings.
\newblock \emph{arXiv preprint arXiv:2006.05051}, 2020.

\bibitem[Chen et~al.(2020)Chen, Hu, Li, and Wang]{chen2020efficient}
Xiaoyu Chen, Jiachen Hu, Lihong Li, and Liwei Wang.
\newblock Efficient reinforcement learning in factored mdps with application to
  constrained rl.
\newblock \emph{arXiv preprint arXiv:2008.13319}, 2020.

\bibitem[Cheung et~al.(2019)Cheung, Simchi-Levi, and Zhu]{cheung2019non}
Wang~Chi Cheung, David Simchi-Levi, and Ruihao Zhu.
\newblock Non-stationary reinforcement learning: The blessing of (more)
  optimism.
\newblock \emph{Available at SSRN 3397818}, 2019.

\bibitem[Ding et~al.(2020)Ding, Wei, Yang, Wang, and
  Jovanovi{\'c}]{ding2020provably}
Dongsheng Ding, Xiaohan Wei, Zhuoran Yang, Zhaoran Wang, and Mihailo~R
  Jovanovi{\'c}.
\newblock Provably efficient safe exploration via primal-dual policy
  optimization.
\newblock \emph{arXiv preprint arXiv:2003.00534}, 2020.

\bibitem[Domingues et~al.(2020)Domingues, M{\'e}nard, Kaufmann, and
  Valko]{domingues2020episodic}
Omar~Darwiche Domingues, Pierre M{\'e}nard, Emilie Kaufmann, and Michal Valko.
\newblock Episodic reinforcement learning in finite mdps: Minimax lower bounds
  revisited.
\newblock \emph{arXiv preprint arXiv:2010.03531}, 2020.

\bibitem[Efroni et~al.(2020)Efroni, Mannor, and Pirotta]{efroni2020exploration}
Yonathan Efroni, Shie Mannor, and Matteo Pirotta.
\newblock Exploration-exploitation in constrained mdps.
\newblock \emph{arXiv preprint arXiv:2003.02189}, 2020.

\bibitem[Jenatton et~al.(2016)Jenatton, Huang, and
  Archambeau]{jenatton2016adaptive}
Rodolphe Jenatton, Jim Huang, and C{\'e}dric Archambeau.
\newblock Adaptive algorithms for online convex optimization with long-term
  constraints.
\newblock In \emph{International Conference on Machine Learning}, pages
  402--411. PMLR, 2016.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock {Is Q-learning provably efficient?}
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4863--4873, 2018.

\bibitem[Jin et~al.(2020)Jin, Krishnamurthy, Simchowitz, and Yu]{jin2020reward}
Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu.
\newblock Reward-free exploration for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2002.02794}, 2020.

\bibitem[Littman(1994)]{littman1994markov}
Michael~L Littman.
\newblock Markov games as a framework for multi-agent reinforcement learning.
\newblock In \emph{Machine learning proceedings 1994}, pages 157--163.
  Elsevier, 1994.

\bibitem[Liu et~al.(2020)Liu, Yu, Bai, and Jin]{liu2020sharp}
Qinghua Liu, Tiancheng Yu, Yu~Bai, and Chi Jin.
\newblock A sharp analysis of model-based reinforcement learning with
  self-play.
\newblock \emph{arXiv preprint arXiv:2010.01604}, 2020.

\bibitem[Mannor et~al.(2014)Mannor, Perchet, and
  Stoltz]{mannor2014approachability}
Shie Mannor, Vianney Perchet, and Gilles Stoltz.
\newblock Approachability in unknown games: Online learning meets
  multi-objective optimization.
\newblock In \emph{Conference on Learning Theory}, pages 339--355, 2014.

\bibitem[Maurer and Pontil(2009)]{maurer2009empirical}
Andreas Maurer and Massimiliano Pontil.
\newblock Empirical bernstein bounds and sample variance penalization.
\newblock \emph{arXiv preprint arXiv:0907.3740}, 2009.

\bibitem[Neumann(1928)]{neumann1928theorie}
J~v Neumann.
\newblock Zur theorie der gesellschaftsspiele.
\newblock \emph{Mathematische annalen}, 100\penalty0 (1):\penalty0 295--320,
  1928.

\bibitem[Osband and Van~Roy(2014)]{osband2014near}
Ian Osband and Benjamin Van~Roy.
\newblock Near-optimal reinforcement learning in factored mdps.
\newblock \emph{Advances in Neural Information Processing Systems},
  27:\penalty0 604--612, 2014.

\bibitem[Qiu et~al.(2020)Qiu, Wei, Yang, Ye, and Wang]{qiu2020upper}
Shuang Qiu, Xiaohan Wei, Zhuoran Yang, Jieping Ye, and Zhaoran Wang.
\newblock Upper confidence primal-dual reinforcement learning for cmdp with
  adversarial loss.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Rockafellar(1970)]{rockafellar1970convex}
R~Tyrrell Rockafellar.
\newblock \emph{Convex analysis}.
\newblock Number~28. Princeton university press, 1970.

\bibitem[Shapley(1953)]{shapley1953stochastic}
Lloyd~S Shapley.
\newblock Stochastic games.
\newblock \emph{Proceedings of the national academy of sciences}, 39\penalty0
  (10):\penalty0 1095--1100, 1953.

\bibitem[Shimkin(2016)]{shimkin2016online}
Nahum Shimkin.
\newblock An online convex optimization approach to blackwell's
  approachability.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 4434--4456, 2016.

\bibitem[Singh et~al.(2020)Singh, Gupta, and Shroff]{singh2020learning}
Rahul Singh, Abhishek Gupta, and Ness~B Shroff.
\newblock Learning in markov decision processes under constraints.
\newblock \emph{arXiv preprint arXiv:2002.12435}, 2020.

\bibitem[Tian et~al.(2020{\natexlab{a}})Tian, Qian, and Sra]{tian2020towards}
Yi~Tian, Jian Qian, and Suvrit Sra.
\newblock Towards minimax optimal reinforcement learning in factored markov
  decision processes.
\newblock \emph{Advances in Neural Information Processing Systems}, 33,
  2020{\natexlab{a}}.

\bibitem[Tian et~al.(2020{\natexlab{b}})Tian, Wang, Yu, and
  Sra]{tian2020provably}
Yi~Tian, Yuanhao Wang, Tiancheng Yu, and Suvrit Sra.
\newblock Provably efficient online agnostic learning in markov games.
\newblock \emph{arXiv preprint arXiv:2010.15020}, 2020{\natexlab{b}}.

\bibitem[Wu et~al.(2020)Wu, Braverman, and Yang]{wu2020accommodating}
Jingfeng Wu, Vladimir Braverman, and Lin~F Yang.
\newblock Accommodating picky customers: Regret bound and exploration
  complexity for multi-objective reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2011.13034}, 2020.

\bibitem[Xie et~al.(2020)Xie, Chen, Wang, and Yang]{xie2020learning}
Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang.
\newblock {Learning Zero-Sum Simultaneous-Move Markov Games Using Function
  Approximation and Correlated Equilibrium}.
\newblock \emph{arXiv preprint arXiv:2002.07066}, 2020.

\bibitem[Yuan and Lamperski(2018)]{yuan2018online}
Jianjun Yuan and Andrew Lamperski.
\newblock Online convex optimization for cumulative constraints.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6137--6146, 2018.

\end{thebibliography}
