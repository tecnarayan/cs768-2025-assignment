\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, Kudlur, Levenberg, Monga, Moore, Murray, Steiner,
  Tucker, Vasudevan, Warden, Wicke, Yu, and Zheng]{TF:2016}
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
  Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
  Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek~G. Murray,
  Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan
  Yu, and Xiaoqiang Zheng.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \emph{12th USENIX Symposium on Operating Systems Design and
  Implementation (OSDI 16)}, pages 265--283, 2016.
\newblock URL
  \url{https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf}.

\bibitem[Alizadeh et~al.(2020)Alizadeh, Behboodi, van Baalen, Louizos,
  Blankevoort, and Welling]{L1RobustQ:2020}
Milad Alizadeh, Arash Behboodi, Mart van Baalen, Christos Louizos, Tijmen
  Blankevoort, and Max Welling.
\newblock Gradient $\ell_1$ regularization for quantization robustness.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=ryxK0JBtPr}.

\bibitem[Bagdasaryan et~al.(2020)Bagdasaryan, Veit, Hua, Estrin, and
  Shmatikov]{BackdoorFL:2020}
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
  Shmatikov.
\newblock How to backdoor federated learning.
\newblock In Silvia Chiappa and Roberto Calandra, editors, \emph{Proceedings of
  the Twenty Third International Conference on Artificial Intelligence and
  Statistics}, volume 108 of \emph{Proceedings of Machine Learning Research},
  pages 2938--2948. PMLR, 26--28 Aug 2020.
\newblock URL \url{http://proceedings.mlr.press/v108/bagdasaryan20a.html}.

\bibitem[Banner et~al.(2019)Banner, Nahshan, and Soudry]{ACIQ:2020}
Ron Banner, Yury Nahshan, and Daniel Soudry.
\newblock Post training 4-bit quantization of convolutional networks for
  rapid-deployment.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/c0a62e133894cdce435bcb4a5df1db2d-Paper.pdf}.

\bibitem[Bonawitz et~al.(2017)Bonawitz, Ivanov, Kreuter, Marcedone, McMahan,
  Patel, Ramage, Segal, and Seth]{SecureAgg:2017}
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H~Brendan
  McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth.
\newblock Practical secure aggregation for privacy-preserving machine learning.
\newblock In \emph{proceedings of the 2017 ACM SIGSAC Conference on Computer
  and Communications Security}, pages 1175--1191, 2017.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{GPT3:2020}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 1877--1901. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Chen et~al.(2019)Chen, Carvalho, Baracaldo, Ludwig, Edwards, Lee,
  Molloy, and Srivastava]{BDActivation}
Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin
  Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava.
\newblock Detecting backdoor attacks on deep neural networks by activation
  clustering.
\newblock In \emph{SafeAI@AAAI}, 2019.
\newblock URL \url{http://ceur-ws.org/Vol-2301/paper_18.pdf}.

\bibitem[Choi et~al.(2018)Choi, Wang, Venkataramani, Chuang, Srinivasan, and
  Gopalakrishnan]{PACT:2018}
Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang,
  Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan.
\newblock Pact: Parameterized clipping activation for quantized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1805.06085}, 2018.

\bibitem[Choi et~al.(2016)Choi, El-Khamy, and Lee]{clusterquant}
Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee.
\newblock Towards the limit of network quantization.
\newblock \emph{arXiv preprint arXiv:1612.01543}, 2016.

\bibitem[Chou et~al.(2018)Chou, Tram{\`{e}}r, Pellegrino, and Boneh]{SentiNet}
Edward Chou, Florian Tram{\`{e}}r, Giancarlo Pellegrino, and Dan Boneh.
\newblock Sentinet: Detecting physical attacks against deep learning systems.
\newblock \emph{CoRR}, abs/1812.00292, 2018.
\newblock URL \url{http://arxiv.org/abs/1812.00292}.

\bibitem[Choukroun et~al.(2019)Choukroun, Kravchik, Yang, and
  Kisilev]{MSE:2019}
Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev.
\newblock Low-bit quantization of neural networks for efficient inference.
\newblock In \emph{2019 IEEE/CVF International Conference on Computer Vision
  Workshop (ICCVW)}, pages 3009--3018, 2019.
\newblock \doi{10.1109/ICCVW.2019.00363}.

\bibitem[Courbariaux et~al.(2015)Courbariaux, Bengio, and
  David]{BinaryConnect:2015}
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.
\newblock Binaryconnect: Training deep neural networks with binary weights
  during propagations.
\newblock In C.~Cortes, N.~Lawrence, D.~Lee, M.~Sugiyama, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~28.
  Curran Associates, Inc., 2015.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2015/file/3e15cc11f979ed25912dff5b0669f2cd-Paper.pdf}.

\bibitem[Dong et~al.(2020)Dong, Yao, Arfeen, Gholami, Mahoney, and
  Keutzer]{HAWQv2:2020}
Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael~W Mahoney, and
  Kurt Keutzer.
\newblock Hawq-v2: Hessian aware trace-weighted quantization of neural
  networks.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 18518--18529. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/d77c703536718b95308130ff2e5cf9ee-Paper.pdf}.

\bibitem[Feldstein(2019)]{Feldstein:Repression}
Steven Feldstein.
\newblock The road to digital unfreedom: How artificial intelligence is
  reshaping repression.
\newblock \emph{Journal of Democracy}, 30\penalty0 (1):\penalty0 40--52, 2019.

\bibitem[Fiesler et~al.(1990)Fiesler, Choudry, and
  Caulfield]{firstquantization}
Emile Fiesler, Amar Choudry, and H~John Caulfield.
\newblock Weight discretization paradigm for optical neural networks.
\newblock In \emph{Optical interconnections and networks}, volume 1281, pages
  164--173. International Society for Optics and Photonics, 1990.

\bibitem[Garg et~al.(2020)Garg, Kumar, Goel, and Liang]{AWPBackdoor:2020}
Siddhant Garg, Adarsh Kumar, Vibhor Goel, and Yingyu Liang.
\newblock Can adversarial weight perturbations inject neural backdoors.
\newblock In \emph{Proceedings of the 29th ACM International Conference on
  Information \& Knowledge Management}, CIKM '20, page 2029–2032, New York,
  NY, USA, 2020. Association for Computing Machinery.
\newblock \doi{10.1145/3340531.3412130}.
\newblock URL \url{https://doi.org/10.1145/3340531.3412130}.

\bibitem[Gong et~al.(2014)Gong, Liu, Yang, and Bourdev]{firstclusterquant}
Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev.
\newblock Compressing deep convolutional networks using vector quantization.
\newblock \emph{arXiv preprint arXiv:1412.6115}, 2014.

\bibitem[Gu et~al.(2017)Gu, Dolan{-}Gavitt, and Garg]{BadNet:2017}
Tianyu Gu, Brendan Dolan{-}Gavitt, and Siddharth Garg.
\newblock Badnets: Identifying vulnerabilities in the machine learning model
  supply chain.
\newblock \emph{CoRR}, abs/1708.06733, 2017.
\newblock URL \url{http://arxiv.org/abs/1708.06733}.

\bibitem[{He} et~al.(2016){He}, {Zhang}, {Ren}, and {Sun}]{ResNet}
K.~{He}, X.~{Zhang}, S.~{Ren}, and J.~{Sun}.
\newblock Deep residual learning for image recognition.
\newblock In \emph{2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 770--778, 2016.
\newblock \doi{10.1109/CVPR.2016.90}.

\bibitem[Hinton et~al.(2012)Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior,
  Vanhoucke, Nguyen, Sainath, and Kingsbury]{Hinton:Speech}
Geoffrey Hinton, Li~Deng, Dong Yu, George~E. Dahl, Abdel-rahman Mohamed,
  Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara~N.
  Sainath, and Brian Kingsbury.
\newblock Deep neural networks for acoustic modeling in speech recognition: The
  shared views of four research groups.
\newblock \emph{IEEE Signal Processing Magazine}, 29\penalty0 (6):\penalty0
  82--97, 2012.
\newblock \doi{10.1109/MSP.2012.2205597}.

\bibitem[Hong et~al.(2019)Hong, Frigo, Kaya, Giuffrida, and
  Dumitra\c{s}]{TBD:2019}
Sanghyun Hong, Pietro Frigo, Yi\u{g}itcan Kaya, Cristiano Giuffrida, and Tudor
  Dumitra\c{s}.
\newblock Terminal brain damage: Exposing the graceless degradation in deep
  neural networks under hardware fault attacks.
\newblock In \emph{Proceedings of the 28th USENIX Conference on Security
  Symposium}, SEC'19, page 497–514, USA, 2019. USENIX Association.
\newblock ISBN 9781939133069.

\bibitem[Jacob et~al.(2018)Jacob, Kligys, Chen, Zhu, Tang, Howard, Adam, and
  Kalenichenko]{qatfirst}
Benoit Jacob, Skirmantas Kligys, Bo~Chen, Menglong Zhu, Matthew Tang, Andrew
  Howard, Hartwig Adam, and Dmitry Kalenichenko.
\newblock Quantization and training of neural networks for efficient
  integer-arithmetic-only inference.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 2704--2713, 2018.

\bibitem[Kim et~al.(2014)Kim, Daly, Kim, Fallin, Lee, Lee, Wilkerson, Lai, and
  Mutlu]{rowhammer}
Yoongu Kim, Ross Daly, Jeremie Kim, Chris Fallin, Ji~Hye Lee, Donghyuk Lee,
  Chris Wilkerson, Konrad Lai, and Onur Mutlu.
\newblock Flipping bits in memory without accessing them: An experimental study
  of dram disturbance errors.
\newblock \emph{ACM SIGARCH Computer Architecture News}, 42\penalty0
  (3):\penalty0 361--372, 2014.

\bibitem[Kingma and Ba(2015)]{Adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR (Poster)}, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.6980}.

\bibitem[Krizhevsky and Hinton(2009)]{CIFAR10}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Master's thesis, Department of Computer Science, University of
  Toronto}, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{AlexNet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In F.~Pereira, C.~J.~C. Burges, L.~Bottou, and K.~Q. Weinberger,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~25.
  Curran Associates, Inc., 2012.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf}.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{braindamage}
Yann LeCun, John~S Denker, and Sara~A Solla.
\newblock Optimal brain damage.
\newblock In \emph{Advances in neural information processing systems}, pages
  598--605, 1990.

\bibitem[Li et~al.(2016)Li, Kadav, Durdanovic, Samet, and Graf]{pruning}
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans~Peter Graf.
\newblock Pruning filters for efficient convnets.
\newblock \emph{arXiv preprint arXiv:1608.08710}, 2016.

\bibitem[Li et~al.(2021)Li, Gong, Tan, Yang, Hu, Zhang, Yu, Wang, and
  Gu]{BRECQ:2021}
Yuhang Li, Ruihao Gong, Xu~Tan, Yang Yang, Peng Hu, Qi~Zhang, Fengwei Yu, Wei
  Wang, and Shi Gu.
\newblock Brecq: Pushing the limit of post-training quantization by block
  reconstruction.
\newblock \emph{arXiv preprint arXiv:2102.05426}, 2021.

\bibitem[Liu et~al.(2018)Liu, Ma, Aafer, Lee, Zhai, Wang, and
  Zhang]{TrojanNN:2018}
Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang,
  and Xiangyu Zhang.
\newblock Trojaning attack on neural networks.
\newblock In \emph{25nd Annual Network and Distributed System Security
  Symposium, {NDSS} 2018, San Diego, California, USA, February 18-221, 2018}.
  The Internet Society, 2018.

\bibitem[Liu et~al.(2019)Liu, Lee, Tao, Ma, Aafer, and Zhang]{ABS:2020}
Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xiangyu
  Zhang.
\newblock Abs: Scanning neural networks for back-doors by artificial brain
  stimulation.
\newblock In \emph{Proceedings of the 2019 ACM SIGSAC Conference on Computer
  and Communications Security}, CCS '19, page 1265–1282, New York, NY, USA,
  2019. Association for Computing Machinery.
\newblock ISBN 9781450367479.
\newblock \doi{10.1145/3319535.3363216}.
\newblock URL \url{https://doi.org/10.1145/3319535.3363216}.

\bibitem[Morgan et~al.(1991)]{Margan:1991}
Nelson Morgan et~al.
\newblock Experimental determination of precision requirements for
  back-propagation training of artificial neural networks.
\newblock In \emph{Proc. Second Int'l. Conf. Microelectronics for Neural
  Networks,}, pages 9--16. Citeseer, 1991.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{PyTorch:2019}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems 32}, pages 8024--8035. Curran Associates,
  Inc., 2019.
\newblock URL
  \url{http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}.

\bibitem[PyTorch(2021)]{PTQ:PT}
PyTorch.
\newblock {Quantization - PyTorch Documentation}.
\newblock \url{https://pytorch.org/docs/stable/quantization.html}, 2021.
\newblock Accessed: 2021-05-26.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{CLIP:2021}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock \emph{arXiv preprint arXiv:2103.00020}, 2021.

\bibitem[Rastegari et~al.(2016)Rastegari, Ordonez, Redmon, and
  Farhadi]{XORNet:2016}
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors,
  \emph{Computer Vision -- ECCV 2016}, pages 525--542, Cham, 2016. Springer
  International Publishing.
\newblock ISBN 978-3-319-46493-0.

\bibitem[{Sandler} et~al.(2018){Sandler}, {Howard}, {Zhu}, {Zhmoginov}, and
  {Chen}]{MobileNet}
M.~{Sandler}, A.~{Howard}, M.~{Zhu}, A.~{Zhmoginov}, and L.~{Chen}.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pages 4510--4520, 2018.
\newblock \doi{10.1109/CVPR.2018.00474}.

\bibitem[Simonyan and Zisserman(2015)]{VGG}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Strubell et~al.(2019)Strubell, Ganesh, and McCallum]{Energy:Strubell}
Emma Strubell, Ananya Ganesh, and Andrew McCallum.
\newblock Energy and policy considerations for deep learning in nlp.
\newblock \emph{arXiv preprint arXiv:1906.02243}, 2019.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{Inceptionv3}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem[Tang et~al.(2021)Tang, Wang, Tang, and Zhang]{TaCT}
Di~Tang, XiaoFeng Wang, Haixu Tang, and Kehuan Zhang.
\newblock Demon in the variant: Statistical analysis of dnns for robust
  backdoor contamination detection.
\newblock In \emph{30th {USENIX} Security Symposium ({USENIX} Security 21)},
  pages 1541--1558. {USENIX} Association, August 2021.
\newblock ISBN 978-1-939133-24-3.
\newblock URL
  \url{https://www.usenix.org/conference/usenixsecurity21/presentation/tang-di}.

\bibitem[TensorFlow(2021)]{PTQ:TF}
TensorFlow.
\newblock {Post-training quantization | TensorFlow Lite}.
\newblock
  \url{https://www.tensorflow.org/lite/performance/post_training_quantization},
  2021.
\newblock Accessed: 2021-05-26.

\bibitem[Wang et~al.(2019{\natexlab{a}})Wang, Yao, Shan, Li, Viswanath, Zheng,
  and Zhao]{NC:2019}
Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao
  Zheng, and Ben~Y. Zhao.
\newblock Neural cleanse: Identifying and mitigating backdoor attacks in neural
  networks.
\newblock In \emph{2019 IEEE Symposium on Security and Privacy (SP)}, pages
  707--723, 2019{\natexlab{a}}.
\newblock \doi{10.1109/SP.2019.00031}.

\bibitem[Wang et~al.(2019{\natexlab{b}})Wang, Zhang, Liu, Liu, and
  Miao]{Ethnicity:Wang}
Cunrui Wang, Qingling Zhang, Wanquan Liu, Yu~Liu, and Lixin Miao.
\newblock Facial feature discovery for ethnicity recognition.
\newblock \emph{Wiley Interdisciplinary Reviews: Data Mining and Knowledge
  Discovery}, 9\penalty0 (1):\penalty0 e1278, 2019{\natexlab{b}}.

\bibitem[Yao et~al.(2020)Yao, Rakin, and Fan]{DeepHammer:2020}
Fan Yao, Adnan~Siraj Rakin, and Deliang Fan.
\newblock Deephammer: Depleting the intelligence of deep neural networks
  through targeted chain of bit flips.
\newblock In \emph{29th {USENIX} Security Symposium ({USENIX} Security 20)},
  pages 1463--1480. {USENIX} Association, August 2020.
\newblock ISBN 978-1-939133-17-5.
\newblock URL
  \url{https://www.usenix.org/conference/usenixsecurity20/presentation/yao}.

\bibitem[Zhang et~al.(2018)Zhang, Yang, Ye, and Hua]{LQNet:2018}
Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua.
\newblock Lq-nets: Learned quantization for highly accurate and compact deep
  neural networks.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, September 2018.

\bibitem[Zhao et~al.(2019)Zhao, Hu, Dotzel, De~Sa, and Zhang]{OCS:2019}
Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De~Sa, and Zhiru Zhang.
\newblock {Improving Neural Network Quantization without Retraining using
  Outlier Channel Splitting}.
\newblock \emph{International Conference on Machine Learning (ICML)}, pages
  7543--7552, June 2019.

\bibitem[Zhou et~al.(2018)Zhou, Kantarcioglu, and Xi]{Noise4AdvExample:2018}
Yan Zhou, Murat Kantarcioglu, and Bowei Xi.
\newblock {Breaking Transferability of Adversarial Samples with Randomness}.
\newblock \emph{arXiv preprint arXiv:1805.04613}, 2018.

\end{thebibliography}
