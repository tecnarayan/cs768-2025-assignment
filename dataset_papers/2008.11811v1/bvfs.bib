
@inproceedings{turchetta2016safe,
  title={Safe exploration in finite Markov decision processes with Gaussian processes},
  author={Turchetta, Matteo and Berkenkamp, Felix and Krause, Andreas},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4312--4320},
  year={2016}
}


@article{moldovan2012safe,
  title={Safe exploration in Markov decision processes},
  author={Moldovan, Teodor Mihai and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1205.4810},
  year={2012}
}


@article{sukhbaatar2017intrinsic,
  title={Intrinsic motivation and automatic curricula via asymmetric self-play},
  author={Sukhbaatar, Sainbayar and Lin, Zeming and Kostrikov, Ilya and Synnaeve, Gabriel and Szlam, Arthur and Fergus, Rob},
  journal={arXiv preprint arXiv:1703.05407},
  year={2017}
}

% constrained MDPs
@book{altman1999constrained,
  title={Constrained Markov decision processes},
  author={Altman, Eitan},
  volume={7},
  year={1999},
  publisher={CRC Press}
}

% risk
@article{geibel2005risk,
  title={Risk-sensitive reinforcement learning applied to control under constraints.},
  author={Geibel, Peter and Wysotzki, Fritz},
  journal={J. Artif. Intell. Res.(JAIR)},
  volume={24},
  pages={81--108},
  year={2005}
}


% @inproceedings{berkenkamp2017safe,
  title={Safe model-based reinforcement learning with stability guarantees},
  author={Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela and Krause, Andreas},
  booktitle={Advances in Neural Information Processing Systems},
  pages={908--919},
  year={2017}
}


@article{koller2018learning,
  title={Learning-based Model Predictive Control for Safe Exploration and Reinforcement Learning},
  author={Koller, Torsten and Berkenkamp, Felix and Turchetta, Matteo and Krause, Andreas},
  journal={arXiv preprint arXiv:1803.08287},
  year={2018}
}

% control theory
@article{khalil1996noninear,
  title={NonLinear systems},
  author={Khalil, Hassan K},
  year={1996}
}


% hamiltonian-jacobi for continous
@article{mitchell2003application,
  title={Application of level set methods to control and reachability problems in continuous and hybrid systems.},
  author={Mitchell, Ian Michael},
  year={2003}
}

@article{chow2018lyapunov,
  title={A Lyapunov-based Approach to Safe Reinforcement Learning},
  author={Chow, Yinlam and Nachum, Ofir and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad},
  journal={arXiv preprint arXiv:1805.07708},
  year={2018}
}


@article{achiam2017constrained,
  title={Constrained policy optimization},
  author={Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1705.10528},
  year={2017}
}


@article{florensa2017reverse,
  title={Reverse curriculum generation for reinforcement learning},
  author={Florensa, Carlos and Held, David and Wulfmeier, Markus and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1707.05300},
  year={2017}
}


@article{held2017automatic,
  title={Automatic goal generation for reinforcement learning agents},
  author={Held, David and Geng, Xinyang and Florensa, Carlos and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1705.06366},
  year={2017}
}

@article{eysenbach2017leave,
  title={Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning},
  author={Eysenbach, Benjamin and Gu, Shixiang and Ibarz, Julian and Levine, Sergey},
  journal={arXiv preprint arXiv:1711.06782},
  year={2017}
}

@article{liang2018accelerated,
  title={Accelerated Primal-Dual Policy Optimization for Safe Reinforcement Learning},
  author={Liang, Qingkai and Que, Fanyu and Modiano, Eytan},
  journal={arXiv preprint arXiv:1802.06480},
  year={2018}
}


@article{krakovna2018measuring,
  title={Measuring and avoiding side effects using relative reachability},
  author={Krakovna, Victoria and Orseau, Laurent and Martic, Miljan and Legg, Shane},
  journal={arXiv preprint arXiv:1806.01186},
  year={2018}
}


@article{o2017uncertainty,
  title={The Uncertainty Bellman Equation and Exploration},
  author={O'Donoghue, Brendan and Osband, Ian and Munos, Remi and Mnih, Volodymyr},
  journal={arXiv preprint arXiv:1709.05380},
  year={2017}
}


@article{perkins2002lyapunov,
  title={Lyapunov design for safe reinforcement learning},
  author={Perkins, Theodore J and Barto, Andrew G},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Dec},
  pages={803--832},
  year={2002}
}


@article{dalal2018safe,
  title={Safe Exploration in Continuous Action Spaces},
  author={Dalal, Gal and Dvijotham, Krishnamurthy and Vecerik, Matej and Hester, Todd and Paduraru, Cosmin and Tassa, Yuval},
  journal={arXiv preprint arXiv:1801.08757},
  year={2018}
}


@article{mossalam2016multi,
  title={Multi-objective deep reinforcement learning},
  author={Mossalam, Hossam and Assael, Yannis M and Roijers, Diederik M and Whiteson, Shimon},
  journal={arXiv preprint arXiv:1610.02707},
  year={2016}
}


@article{van2016effective,
  title={Effective multi-step temporal-difference learning for non-linear function approximation},
  author={van Seijen, Harm},
  journal={arXiv preprint arXiv:1608.05151},
  year={2016}
}

@inproceedings{fu2017ex2,
  title={Ex2: Exploration with exemplar models for deep reinforcement learning},
  author={Fu, Justin and Co-Reyes, John and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2577--2587},
  year={2017}
}

@article{white2016unifying,
  title={Unifying task specification in reinforcement learning},
  author={White, Martha},
  journal={arXiv preprint arXiv:1609.01995},
  year={2016}
}



@article{lipton2016combating,
  title={Combating Reinforcement Learning's Sisyphean Curse with Intrinsic Fear},
  author={Lipton, Zachary C and Azizzadenesheli, Kamyar and Kumar, Abhishek and Li, Lihong and Gao, Jianfeng and Deng, Li},
  journal={arXiv preprint arXiv:1611.01211},
  year={2016}
}


@inproceedings{yoshida2013reinforcement,
  title={Reinforcement learning with state-dependent discount factor},
  author={Yoshida, Naoto and Uchibe, Eiji and Doya, Kenji},
  booktitle={Development and Learning and Epigenetic Robotics (ICDL), 2013 IEEE Third Joint International Conference on},
  pages={1--6},
  year={2013},
  organization={IEEE}
}

@article{schlegel2018general,
  title={General Value Function Networks},
  author={Schlegel, Matthew and White, Adam and Patterson, Andrew and White, Martha},
  journal={arXiv preprint arXiv:1807.06763},
  year={2018}
}


@inproceedings{wachi2018safe,
  title={Safe Exploration and Optimization of Constrained MDPs using Gaussian Processes},
  author={Wachi, Akifumi and Sui, Yanan and Yue, Yisong and Ono, Masahiro},
  booktitle={AAAI Conference on Artificial Intelligence (AAAI)},
  year={2018}
}


@article{chow2019lyapunov,
  title={Lyapunov-based Safe Policy Optimization for Continuous Control},
  author={Chow, Yinlam and Nachum, Ofir and Faust, Aleksandra and Ghavamzadeh, Mohammad and Duenez-Guzman, Edgar},
  journal={arXiv preprint arXiv:1901.10031},
  year={2019}
}


@article{franccois2018introduction,
  title={An Introduction to Deep Reinforcement Learning},
  author={Fran{\c{c}}ois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G and Pineau, Joelle and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={11},
  number={3-4},
  pages={219--354},
  year={2018},
  publisher={Now Publishers, Inc.}
}


@article{tessler2018reward,
  title={Reward constrained policy optimization},
  author={Tessler, Chen and Mankowitz, Daniel J and Mannor, Shie},
  journal={arXiv preprint arXiv:1805.11074},
  year={2018}
}


@inproceedings{abbeel2004apprenticeship,
  title={Apprenticeship learning via inverse reinforcement learning},
  author={Abbeel, Pieter and Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={1},
  year={2004},
  organization={ACM}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}


@article{uhlenbeck1930theory,
  title={On the theory of the Brownian motion},
  author={Uhlenbeck, George E and Ornstein, Leonard S},
  journal={Physical review},
  volume={36},
  number={5},
  pages={823},
  year={1930},
  publisher={APS}
}


@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}


@article{kemeny1976finite,
  title={Finite markov chains, undergraduate texts in mathematics},
  author={Kemeny, John G and Snell, James Laurie},
  year={1976}
}

@article{leike2017ai,
  title={AI safety gridworlds},
  author={Leike, Jan and Martic, Miljan and Krakovna, Victoria and Ortega, Pedro A and Everitt, Tom and Lefrancq, Andrew and Orseau, Laurent and Legg, Shane},
  journal={arXiv preprint arXiv:1711.09883},
  year={2017}
}

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}


@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016}
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}



@incollection{peng1994incremental,
  title={Incremental multi-step Q-learning},
  author={Peng, Jing and Williams, Ronald J},
  booktitle={Machine Learning Proceedings 1994},
  pages={226--232},
  year={1994},
  publisher={Elsevier}
}


@book{sarsa-ref,
  title={On-line Q-learning using connectionist systems},
  author={Rummery, Gavin A and Niranjan, Mahesan},
  volume={37},
  year={1994},
  publisher={University of Cambridge, Department of Engineering Cambridge, England}
}

@article{sutton1988learning,
  title={Learning to predict by the methods of temporal differences},
  author={Sutton, Richard S},
  journal={Machine learning},
  volume={3},
  number={1},
  pages={9--44},
  year={1988},
  publisher={Springer}
}


@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}


@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}



@misc{pytorchrl,
  author = {Kostrikov, Ilya},
  title = {PyTorch Implementations of Reinforcement Learning Algorithms},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail}},
}



@misc{baselines,
  author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai and Zhokhov, Peter},
  title = {OpenAI Baselines},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openai/baselines}},
}


@article{neu2017unified,
  title={A unified view of entropy-regularized markov decision processes},
  author={Neu, Gergely and Jonsson, Anders and G{\'o}mez, Vicen{\c{c}}},
  journal={arXiv preprint arXiv:1705.07798},
  year={2017}
}


@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}


@inproceedings{duan2016benchmarking,
  title={Benchmarking deep reinforcement learning for continuous control},
  author={Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={1329--1338},
  year={2016}
}


@article{tamar2013policy,
  title={Policy evaluation with variance related risk criteria in markov decision processes},
  author={Tamar, Aviv and Di Castro, Dotan and Mannor, Shie},
  journal={arXiv preprint arXiv:1301.0104},
  year={2013}
}


@inproceedings{chow2015risk,
  title={Risk-sensitive and robust decision-making: a CVaR optimization approach},
  author={Chow, Yinlam and Tamar, Aviv and Mannor, Shie and Pavone, Marco},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1522--1530},
  year={2015}
}


@inproceedings{konda2000actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay R and Tsitsiklis, John N},
  booktitle={Advances in neural information processing systems},
  pages={1008--1014},
  year={2000}
}


@misc{rep-checklist,
  author = {Pineau, Joelle},
  title = {The machine learning reproducibility checklist},
  year = {2018},
  howpublished = {\url{https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf}},
}



@article{morimura2010,
author = {Morimura, Tetsuro and Uchibe, Eiji and Yoshimoto, Junichiro and Peters, Jan and Doya, Kenji},
title = {Derivatives of Logarithmic Stationary Distributions for Policy Gradient Reinforcement Learning},
journal = {Neural Computation},
volume = {22},
number = {2},
pages = {342-376},
year = {2010},
doi = {10.1162/neco.2009.12-08-922},
note ={PMID: 19842990},
URL = { https://doi.org/10.1162/neco.2009.12-08-922},
eprint = { https://doi.org/10.1162/neco.2009.12-08-922},
abstract = { Most conventional policy gradient reinforcement learning (PGRL) algorithms neglect (or do not explicitly make use of) a term in the average reward gradient with respect to the policy parameter. That term involves the derivative of the stationary state distribution that corresponds to the sensitivity of its distribution to changes in the policy parameter. Although the bias introduced by this omission can be reduced by setting the forgetting rate γ for the value functions close to 1, these algorithms do not permit γ to be set exactly at γ = 1. In this article, we propose a method for estimating the log stationary state distribution derivative (LSD) as a useful form of the derivative of the stationary state distribution through backward Markov chain formulation and a temporal difference learning framework. A new policy gradient (PG) framework with an LSD is also proposed, in which the average reward gradient can be estimated by setting γ = 0, so it becomes unnecessary to learn the value functions. We also test the performance of the proposed algorithms using simple benchmark tasks and show that these can improve the performances of existing PG methods. }
}




@book{bertsekas1995dynamic,
  title={Dynamic programming and optimal control},
  author={Bertsekas, Dimitri P and Bertsekas, Dimitri P and Bertsekas, Dimitri P and Bertsekas, Dimitri P},
  volume={1},
  number={2},
  year={1995},
  publisher={Athena scientific Belmont, MA}
}

% TRPO
@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015}
}

% AKTOR
@inproceedings{wu2017scalable,
  title={Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation},
  author={Wu, Yuhuai and Mansimov, Elman and Grosse, Roger B and Liao, Shun and Ba, Jimmy},
  booktitle={Advances in neural information processing systems},
  pages={5279--5288},
  year={2017}
}


@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={ICML},
  volume={2},
  pages={267--274},
  year={2002}
}

@article{le2019batch,
  title={Batch policy learning under constraints},
  author={Le, Hoang M and Voloshin, Cameron and Yue, Yisong},
  journal={arXiv preprint arXiv:1903.08738},
  year={2019}
}

@article{thomas2019preventing,
  title={Preventing undesirable behavior of intelligent machines},
  author={Thomas, Philip S and da Silva, Bruno Castro and Barto, Andrew G and Giguere, Stephen and Brun, Yuriy and Brunskill, Emma},
  journal={Science},
  volume={366},
  number={6468},
  pages={999--1004},
  year={2019},
  publisher={American Association for the Advancement of Science}
}

@article{misra2019kinematic,
  title={Kinematic State Abstraction and Provably Efficient Rich-Observation Reinforcement Learning},
  author={Misra, Dipendra and Henaff, Mikael and Krishnamurthy, Akshay and Langford, John},
  journal={arXiv preprint arXiv:1911.05815},
  year={2019}
}

@misc{
chow2020safe,
title={Safe Policy Learning for Continuous Control},
author={Yinlam Chow and Ofir Nachum and Aleksandra Faust and Edgar Duenez-Guzman and Mohammad Ghavamzadeh},
year={2020},
url={https://openreview.net/forum?id=HkxeThNFPH}
}