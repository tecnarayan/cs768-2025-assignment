@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(NeurIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TOMS   = {ACM TOMS})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String{ICML  = {ICML}}
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})
@String(ACL = {ACL})
@String{UAI = {UAI}}
@String{JMLR = {JMLR}}
@String{STOC = {STOC}}
@String{TVLSI= {TVLSI}}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle=ICCV,
  pages={10012--10022},
  year={2021}
}

@InProceedings{Wang_2021_ICCV,
    author    = {Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
    title     = {Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions},
    booktitle = ICCV,
    year      = {2021},
    pages     = {568-578}
}

@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle=ICLR,
year={2021},
pages={1-21}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle=ICML,
  pages={10347--10357},
  year={2021},
}

@inproceedings{devlin2019bert,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  editor    = {Jill Burstein and
               Christy Doran and
               Thamar Solorio},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  booktitle = {NAACL-HLT},
  pages     = {4171--4186},
  year      = {2019},
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NeurIPS},
  volume={30},
  year={2017},
  pages={5998--6008},
}

@inproceedings{
dehghani2018universal,
title={Universal Transformers},
author={Mostafa Dehghani and Stephan Gouws and Oriol Vinyals and Jakob Uszkoreit and Lukasz Kaiser},
booktitle=ICLR,
year={2019},
pages={1-23}
}

@article{Hochreiter1997LongSM,
  title={Long Short-Term Memory},
  author={Sepp Hochreiter and J{\"u}rgen Schmidhuber},
  journal={Neural Computation},
  year={1997},
  volume={9},
  pages={1735-1780}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle=NeurIPS,
  volume={25},
  year={2012},
  pages = {1106--1114},
}

@inproceedings{zadeh2020gobo,
  title={Gobo: Quantizing attention-based nlp models for low latency and energy efficient inference},
  author={Zadeh, Ali Hadi and Edo, Isak and Awad, Omar Mohamed and Moshovos, Andreas},
  booktitle={MICRO},
  pages={811--824},
  year={2020},
}

@inproceedings{bai2021binarybert,
  title={BinaryBERT: Pushing the Limit of BERT Quantization},
  author={Bai, Haoli and Zhang, Wei and Hou, Lu and Shang, Lifeng and Jin, Jin and Jiang, Xin and Liu, Qun and Lyu, Michael and King, Irwin},
  booktitle=ACL,
  pages={4334--4348},
  year={2021}
}

@inproceedings{shen2020q,
  title={Q-bert: Hessian based ultra low precision quantization of bert},
  author={Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={AAAI},
  volume={34},
  pages={8815--8821},
  year={2020}
}

@inproceedings{zafrir2019q8bert,
  title={Q8bert: Quantized 8bit bert},
  author={Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe},
  booktitle={EMC2-NIPS},
  pages={36--39},
  year={2019},
}

@inproceedings{
qin2022bibert,
title={Bi{BERT}: Accurate Fully Binarized {BERT}},
author={Haotong Qin and Yifu Ding and Mingyuan Zhang and Qinghua YAN and Aishan Liu and Qingqing Dang and Ziwei Liu and Xianglong Liu},
booktitle=ICLR,
year={2022},
pages={1-24}
}

@article{zhou2016dorefa,
  title={Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients},
  author={Zhou, Shuchang and Wu, Yuxin and Ni, Zekun and Zhou, Xinyu and Wen, He and Zou, Yuheng},
  journal={arXiv preprint arXiv:1606.06160},
  year={2016}
}

@inproceedings{courbariaux2015binaryconnect,
  title={Binaryconnect: Training deep neural networks with binary weights during propagations},
  author={Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
  booktitle={NeurIPS},
  volume={28},
  year={2015},
  pages = {3123--3131},
}

@article{hubara2017quantized,
  title={Quantized neural networks: Training neural networks with low precision weights and activations},
  author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal=JMLR,
  volume={18},
  number={1},
  pages={6869--6898},
  year={2017},
}

@inproceedings{rastegari2016xnor,
  title={Xnor-net: Imagenet classification using binary convolutional neural networks},
  author={Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
  booktitle=ECCV,
  pages={525--542},
  year={2016},
}

@inproceedings{andoni2015practical,
  title={Practical and optimal LSH for angular distance},
  author={Andoni, Alexandr and Indyk, Piotr and Laarhoven, Thijs and Razenshteyn, Ilya and Schmidt, Ludwig},
  booktitle={NeurIPS},
  volume={28},
  year={2015},
  pages = {1225--1233},
}

@inproceedings{peng2020random,
  title={Random Feature Attention},
  author={Peng, Hao and Pappas, Nikolaos and Yogatama, Dani and Schwartz, Roy and Smith, Noah and Kong, Lingpeng},
  booktitle=ICLR,
  year={2021},
  pages={1-19}
}


@inproceedings{lu2021soft,
  title={Soft: Softmax-free transformer with linear complexity},
  author={Lu, Jiachen and Yao, Jinghan and Zhang, Junge and Zhu, Xiatian and Xu, Hang and Gao, Weiguo and Xu, Chunjing and Xiang, Tao and Zhang, Li},
  booktitle=NeurIPS,
  volume={34},
  year={2021},
  pages={21297--21309},
}

@inproceedings{jaegle2021perceiver,
  title={Perceiver: General perception with iterative attention},
  author={Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  booktitle=ICML,
  pages={4651--4664},
  year={2021},
}

@inproceedings{charikar2002similarity,
  title={Similarity estimation techniques from rounding algorithms},
  author={Charikar, Moses S},
  booktitle=STOC,
  pages={380--388},
  year={2002}
}

@inproceedings{liu2012supervised,
  title={Supervised hashing with kernels},
  author={Liu, Wei and Wang, Jun and Ji, Rongrong and Jiang, Yu-Gang and Chang, Shih-Fu},
  booktitle=CVPR,
  pages={2074--2081},
  year={2012},
}

@article{linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@inproceedings{reformer,
  author    = {Nikita Kitaev and
               Lukasz Kaiser and
               Anselm Levskaya},
  title     = {Reformer: The Efficient Transformer},
  booktitle = {ICLR},
  year      = {2020},
  pages = {1-12}
}

@article{liu2019roberta,
    title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
    author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and
              Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and
              Luke Zettlemoyer and Veselin Stoyanov},
    journal={arXiv preprint arXiv:1907.11692},
    year = {2019},
}



@article{sparse_transformer_child,
  author    = {Rewon Child and
               Scott Gray and
               Alec Radford and
               Ilya Sutskever},
  title     = {Generating Long Sequences with Sparse Transformers},
  journal   = {CoRR},
  volume    = {abs/1904.10509},
  year      = {2019},
  eprinttype = {arXiv},
  eprint    = {1904.10509},
}

@article{longformer,
  author    = {Iz Beltagy and
               Matthew E. Peters and
               Arman Cohan},
  title     = {Longformer: The Long-Document Transformer},
  journal   = {CoRR},
  volume    = {abs/2004.05150},
  year      = {2020},
  eprinttype = {arXiv},
  eprint    = {2004.05150},
}




@inproceedings{sinkhorn,
  author    = {Yi Tay and
               Dara Bahri and
               Liu Yang and
               Donald Metzler and
               Da{-}Cheng Juan},
  title     = {Sparse Sinkhorn Attention},
  booktitle = {ICML},
  volume    = {119},
  pages     = {9438--9447},
  year      = {2020},
}

@article{han2016eie,
  title={EIE: Efficient inference engine on compressed deep neural network},
  author={Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J},
  journal={ACM SIGARCH Computer Architecture News},
  volume={44},
  number={3},
  pages={243--254},
  year={2016},
}

@inproceedings{horowitz20141,
  title={1.1 computing's energy problem (and what we can do about it)},
  author={Horowitz, Mark},
  booktitle={ISSCC},
  pages={10--14},
  year={2014},
}

@article{lian2019high,
  title={High-performance FPGA-based CNN accelerator with block-floating-point arithmetic},
  author={Lian, Xiaocong and Liu, Zhenyu and Song, Zhourui and Dai, Jiwu and Zhou, Wei and Ji, Xiangyang},
  journal=TVLSI,
  volume={27},
  number={8},
  pages={1874--1885},
  year={2019},
}

@inproceedings{NIPS2007_013a006f,
 author = {Rahimi, Ali and Recht, Benjamin},
 booktitle = {NeurIPS},
 title = {Random Features for Large-Scale Kernel Machines},
 volume = {20},
 year = {2007},
 pages = {1177--1184},
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle=ICML,
  pages={5156--5165},
  year={2020},
}

@inproceedings{wang2021pyramid,
  title={Pyramid vision transformer: A versatile backbone for dense prediction without convolutions},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  booktitle={ICCV},
  pages={568--578},
  year={2021}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@inproceedings{choromanski2021rethinking,
title={Rethinking Attention with Performers},
author={Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller},
booktitle={ICLR},
year={2021},
pages={1-38}
}

@inproceedings{xiong2021nystromformer,
  title={Nystr{\"o}mformer: A Nyst{\"o}m-based Algorithm for Approximating Self-Attention},
  author={Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  booktitle={AAAI},
  volume={35},
  pages={14138},
  year={2021}
}

@inproceedings{shu2021adder,
  title={Adder Attention for Vision Transformer},
  author={Shu, Han and Wang, Jiahao and Chen, Hanting and Li, Lin and Yang, Yujiu and Wang, Yunhe},
  booktitle={NeurIPS},
  volume={34},
  year={2021},
  pages={19899--19909},
}

%Group-net
@inproceedings{zhuang2019structured,
  title={Structured Binary Neural Network for Accurate Image Classification and Semantic Segmentation},
  author={Zhuang, Bohan and Shen, Chunhua and Tan, Mingkui and Liu, Lingqiao and Reid, Ian},
  booktitle=CVPR,
  pages={413--422},
  year={2019}
}

% Loss-aware
@inproceedings{hou2018loss,
  title={Loss-aware Weight Quantization of Deep Networks},
  author={Hou, Lu and Kwok, James T},
  booktitle=ICLR,
  year={2018},
  pages={1-16}
}

% Regularization
@inproceedings{ding2019regularizing,
  title={Regularizing activation distribution for training binarized deep networks},
  author={Ding, Ruizhou and Chin, Ting-Wu and Liu, Zeye and Marculescu, Diana},
  booktitle=CVPR,
  pages={11408--11417},
  year={2019}
}

@inproceedings{martinez2020training,
  title={Training binary neural networks with real-to-binary convolutions},
  author={Martinez, Brais and Yang, Jing and Bulat, Adrian and Tzimiropoulos, Georgios},
  booktitle=ICLR,
  year={2020},
  pages={1-11}
}

@inproceedings{Liu_2018_ECCV,
author = {Liu, Zechun and Wu, Baoyuan and Luo, Wenhan and Yang, Xin and Liu, Wei and Cheng, Kwang-Ting},
title = {Bi-Real Net: Enhancing the Performance of 1-bit CNNs with Improved Representational Capability and Advanced Training Algorithm},
booktitle = ECCV,
pages={722--737},
year = {2018}
}

@inproceedings{lin2017towards,
  title={Towards Accurate Binary Convolutional Neural Network},
  author={Lin, Xiaofan and Zhao, Cong and Pan, Wei},
  booktitle={NeurIPS},
  pages={344--352},
  year={2017}
}

@inproceedings{jung2019learning,
  title={Learning to quantize deep networks by optimizing quantization intervals with task loss},
  author={Jung, Sangil and Son, Changyong and Lee, Seohyung and Son, Jinwoo and Han, Jae-Joon and Kwak, Youngjun and Hwang, Sung Ju and Choi, Changkyu},
  booktitle=CVPR,
  pages={4350--4359},
  year={2019}
}

@inproceedings{esser2020learned,
  title={Learned step size quantization},
  author={Esser, Steven K and McKinstry, Jeffrey L and Bablani, Deepika and Appuswamy, Rathinakumar and Modha, Dharmendra S},
  booktitle=ICLR,
  year={2020},
  pages={1-12}
}

@inproceedings{bulat2019xnor,
  title={XNOR-Net++: Improved binary neural networks},
  author={Adrian Bulat and Georgios Tzimiropoulos},
  booktitle={BMVC},
  year={2019},
  pages = {62},
}

@article{pvtv2,
  title={Pvtv2: Improved baselines with pyramid vision transformer},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  journal={Computational Visual Media},
  volume={8},
  number={3},
  pages={415--424},
  year={2022}
}

@inproceedings{bai2019proxquant,
  title={Proxquant: Quantized neural networks via proximal operators},
  author={Bai, Yu and Wang, Yu-Xiang and Liberty, Edo},
  booktitle={ICLR},
  year={2019},
  pages={1-19}
}

@article{zhuang2021effective,
  title={Effective training of convolutional neural networks with low-bitwidth weights and activations},
  author={Zhuang, Bohan and Tan, Mingkui and Liu, Jing and Liu, Lingqiao and Reid, Ian and Shen, Chunhua},
  journal={TPAMI},
  year={2021}
}

@inproceedings{mishra2017apprentice,
  title={Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy},
  author={Mishra, Asit and Marr, Debbie},
  booktitle={ICLR},
  year={2018},
  pages={1-17}
}

@article{yan2020deep,
  title={Deep multi-view enhancement hashing for image retrieval},
  author={Yan, Chenggang and Gong, Biao and Wei, Yuxuan and Gao, Yue},
  journal={TPAMI},
  volume={43},
  number={4},
  pages={1445--1451},
  year={2020},
}

@inproceedings{eban2020structured,
  title={Structured multi-hashing for model compression},
  author={Eban, Elad and Movshovitz-Attias, Yair and Wu, Hao and Sandler, Mark and Poon, Andrew and Idelbayev, Yerlan and Carreira-Perpin{\'a}n, Miguel A},
  booktitle={CVPR},
  pages={11903--11912},
  year={2020}
}

@article{lu2020deep,
  title={Deep fuzzy hashing network for efficient image retrieval},
  author={Lu, Huimin and Zhang, Ming and Xu, Xing and Li, Yujie and Shen, Heng Tao},
  journal={IEEE transactions on fuzzy systems},
  volume={29},
  number={1},
  pages={166--176},
  year={2020},
}

@inproceedings{wu2019deep,
  title={Deep incremental hashing network for efficient image retrieval},
  author={Wu, Dayan and Dai, Qi and Liu, Jing and Li, Bo and Wang, Weiping},
  booktitle={CVPR},
  pages={9069--9077},
  year={2019}
}

@article{hu2021video,
  title={Video moment localization via deep cross-modal hashing},
  author={Hu, Yupeng and Liu, Meng and Su, Xiaobin and Gao, Zan and Nie, Liqiang},
  journal={TIP},
  volume={30},
  pages={4667--4677},
  year={2021},
}

@inproceedings{wang2021prototype,
  title={Prototype-supervised Adversarial Network for Targeted Attack of Deep Hashing},
  author={Wang, Xunguang and Zhang, Zheng and Wu, Baoyuan and Shen, Fumin and Lu, Guangming},
  booktitle={CVPR},
  pages={16357--16366},
  year={2021}
}

@inproceedings{hubara2016binarized,
	title={Binarized neural networks},
	author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
	booktitle={NeurIPS},
	pages={4107--4115},
	year={2016}
}

@article{bengio2013estimating,
  title={Estimating or propagating gradients through stochastic neurons},
  author={Bengio, Yoshua},
  journal={arXiv preprint arXiv:1305.2982},
  year={2013}
}

@inproceedings{
  zhen2022cosformer,
  title={cosFormer: Rethinking Softmax In Attention},
  author={Zhen Qin and Weixuan Sun and Hui Deng and Dongxu Li and Yunshen Wei and Baohong Lv and Junjie Yan and Lingpeng Kong and Yiran Zhong},
  booktitle={ICLR},
  year={2022},
  pages={1-15}
}

@inproceedings{gionis1999similarity,
author = {Gionis, Aristides and Indyk, Piotr and Motwani, Rajeev},
title = {Similarity Search in High Dimensions via Hashing},
year = {1999},
booktitle = {Vldb},
pages = {518–529},
numpages = {12},
}

@article{wang2012semi,
  title={Semi-supervised hashing for large-scale search},
  author={Wang, Jun and Kumar, Sanjiv and Chang, Shih-Fu},
  journal={TPAMI},
  volume={34},
  number={12},
  pages={2393--2406},
  year={2012},
}

@article{kulis2011kernelized,
  title={Kernelized locality-sensitive hashing},
  author={Kulis, Brian and Grauman, Kristen},
  journal={TPAMI},
  volume={34},
  number={6},
  pages={1092--1104},
  year={2011}
}

@inproceedings{weiss2008spectral,
  title={Spectral hashing},
  author={Weiss, Yair and Torralba, Antonio and Fergus, Rob},
  booktitle={NeurIPS},
  volume={21},
  year={2008},
  pages={1-8}
}

@inproceedings{zhuang2016fast,
  title={Fast training of triplet-based deep binary embedding networks},
  author={Zhuang, Bohan and Lin, Guosheng and Shen, Chunhua and Reid, Ian},
  booktitle={CVPR},
  pages={5955--5964},
  year={2016}
}

@inproceedings{liu2011hashing,
  title={Hashing with graphs},
  author={Liu, Wei and Wang, Jun and Kumar, Sanjiv and Chang, Shih-Fu},
  booktitle={ICML},
  year={2011},
  pages = {1--8},
}

@inproceedings{liu2014discrete,
  title={Discrete graph hashing},
  author={Liu, Wei and Mu, Cun and Kumar, Sanjiv and Chang, Shih-Fu},
  booktitle={NeurIPS},
  volume={27},
  year={2014},
  pages = {3419--3427},
}

@inproceedings{adamw,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={ICLR},
  year={2019},
  pages={1-18}
}

@inproceedings{OlivaNPXTHS15,
  author    = {Junier B. Oliva and
               Willie Neiswanger and
               Barnab{\'{a}}s P{\'{o}}czos and
               Eric P. Xing and
               Hy Trac and
               Shirley Ho and
               Jeff G. Schneider},
  title     = {Fast Function to Function Regression},
  booktitle = {AISTATS},
  volume    = {38},
  year      = {2015}
}



@article{AvronCW17,
  author    = {Haim Avron and
               Kenneth L. Clarkson and
               David P. Woodruff},
  title     = {Faster Kernel Ridge Regression Using Sketching and Preconditioning},
  journal   = {SIAM},
  volume    = {38},
  number    = {4},
  pages     = {1116--1138},
  year      = {2017}
}

@inproceedings{lin2014fast,
  title={Fast supervised hashing with decision trees for high-dimensional data},
  author={Lin, Guosheng and Shen, Chunhua and Shi, Qinfeng and Van den Hengel, Anton and Suter, David},
  booktitle={CVPR},
  pages={1963--1970},
  year={2014}
}

@inproceedings{chen2020addernet,
  title={AdderNet: Do we really need multiplications in deep learning?},
  author={Chen, Hanting and Wang, Yunhe and Xu, Chunjing and Shi, Boxin and Xu, Chao and Tian, Qi and Xu, Chang},
  booktitle={CVPR},
  pages={1468--1477},
  year={2020}
}

@inproceedings{twins,
	title={Twins: Revisiting the Design of Spatial Attention in Vision Transformers},
	author={Xiangxiang Chu and Zhi Tian and Yuqing Wang and Bo Zhang and Haibing Ren and Xiaolin Wei and Huaxia Xia and Chunhua Shen},
	booktitle={NeurIPS},
	year={2021},
 pages = {9355--9366},
}

@article{zhu1997algorithm,
  title={Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization},
  author={Zhu, Ciyou and Byrd, Richard H and Lu, Peihuang and Nocedal, Jorge},
  journal=TOMS,
  volume={23},
  number={4},
  pages={550--560},
  year={1997}
}

@inproceedings{datar2004locality,
  title={Locality-sensitive hashing scheme based on p-stable distributions},
  author={Datar, Mayur and Immorlica, Nicole and Indyk, Piotr and Mirrokni, Vahab S},
  booktitle={SoCG},
  pages={253--262},
  year={2004}
}

@inproceedings{norouzi2011minimal,
  title={Minimal loss hashing for compact binary codes},
  author={Norouzi, Mohammad and Fleet, David J},
  booktitle={ICML},
  year={2011},
  pages = {353--360},
}

@article{gong2012iterative,
  title={Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval},
  author={Gong, Yunchao and Lazebnik, Svetlana and Gordo, Albert and Perronnin, Florent},
  journal={TPAMI},
  volume={35},
  number={12},
  pages={2916--2929},
  year={2012}
}

@inproceedings{daras2020smyrf,
  title={Smyrf-efficient attention using asymmetric clustering},
  author={Daras, Giannis and Kitaev, Nikita and Odena, Augustus and Dimakis, Alexandros G},
  booktitle=NIPS,
  volume={33},
  pages={6476--6489},
  year={2020}
}

@inproceedings{sun2021sparse,
  title={Sparse Attention with Learning to Hash},
  author={Sun, Zhiqing and Yang, Yiming and Yoo, Shinjae},
  booktitle=ICLR,
  year={2022},
  pages={1-20}
}

@inproceedings{vyas2020fast,
  title={Fast transformers with clustered attention},
  author={Vyas, Apoorv and Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois},
  booktitle=NIPS,
  volume={33},
  pages={21665--21674},
  year={2020}
}

@inproceedings{sharma2018bit,
  title={Bit fusion: Bit-level dynamically composable architecture for accelerating deep neural network},
  author={Sharma, Hardik and Park, Jongse and Suda, Naveen and Lai, Liangzhen and Chau, Benson and Chandra, Vikas and Esmaeilzadeh, Hadi},
  booktitle={ISCA},
  pages={764--775},
  year={2018},
}

@inproceedings{tay2020long,
  title={Long Range Arena: A Benchmark for Efficient Transformers},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  booktitle=ICLR,
  year={2021},
  pages={1-19}
}

@inproceedings{zhu2021long,
  title={Long-short transformer: Efficient transformers for language and vision},
  author={Zhu, Chen and Ping, Wei and Xiao, Chaowei and Shoeybi, Mohammad and Goldstein, Tom and Anandkumar, Anima and Catanzaro, Bryan},
  booktitle=NIPS,
  volume={34},
  pages={17723--17736},
  year={2021}
}

@inproceedings{ren2021combiner,
  title={Combiner: Full attention transformer with sparse computation cost},
  author={Ren, Hongyu and Dai, Hanjun and Dai, Zihang and Yang, Mengjiao and Leskovec, Jure and Schuurmans, Dale and Dai, Bo},
  booktitle=NIPS,
  volume={34},
  pages={22470--22482},
  year={2021}
}