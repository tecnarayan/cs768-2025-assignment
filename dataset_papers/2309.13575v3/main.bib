@inproceedings{blundell2015weight,
  title={Weight uncertainty in neural network},
  author={Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  booktitle={International conference on machine learning},
  pages={1613--1622},
  year={2015},
  organization={PMLR}
}

@inproceedings{achterhold2018variational,
  title={Variational network quantization},
  author={Achterhold, Jan and Koehler, Jan Mathias and Schmeink, Anke and Genewein, Tim},
  booktitle={International Conference on Learning Representations},
  year={2018}
}



@inproceedings{subia2022weight,
  title={Weight Fixing Networks},
  author={Subia-Waud, Christopher and Dasmahapatra, Srinandan},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XI},
  pages={415--431},
  year={2022},
  organization={Springer}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}


@article{li2019additive,
  title={Additive powers-of-two quantization: An efficient non-uniform discretization for neural networks},
  author={Li, Yuhang and Dong, Xin and Wang, Wei},
  journal={arXiv preprint arXiv:1909.13144},
  year={2019}
}


@article{maddox2019simple,
  title={A simple baseline for bayesian uncertainty in deep learning},
  author={Maddox, Wesley J and Izmailov, Pavel and Garipov, Timur and Vetrov, Dmitry P and Wilson, Andrew Gordon},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@book{neal2012bayesian,
  title={Bayesian learning for neural networks},
  author={Neal, Radford M},
  volume={118},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{mitchell1988bayesian,
  title={Bayesian variable selection in linear regression},
  author={Mitchell, Toby J and Beauchamp, John J},
  journal={Journal of the american statistical association},
  volume={83},
  number={404},
  pages={1023--1032},
  year={1988},
  publisher={Taylor \& Francis}
}

@inproceedings{jung2019learning,
  title={Learning to quantize deep networks by optimizing quantization intervals with task loss},
  author={Jung, Sangil and Son, Changyong and Lee, Seohyung and Son, Jinwoo and Han, Jae-Joon and Kwak, Youngjun and Hwang, Sung Ju and Choi, Changkyu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4350--4359},
  year={2019}
}
@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International conference on machine learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}


@inproceedings{oh2021automated,
  title={Automated log-scale quantization for low-cost deep neural networks},
  author={Oh, Sangyun and Sim, Hyeonuk and Lee, Sugil and Lee, Jongeun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={742--751},
  year={2021}
}


@inproceedings{Yamamoto2021,
  title={Learnable companding quantization for accurate low-bit neural networks},
  author={Yamamoto, Kohei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5029--5038},
  year={2021}
}

@article{zhou2016dorefa,
  title={Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients},
  author={Zhou, Shuchang and Wu, Yuxin and Ni, Zekun and Zhou, Xinyu and Wen, He and Zou, Yuheng},
  journal={arXiv preprint arXiv:1606.06160},
  year={2016}
}



@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2704--2713},
  year={2018}
}

@article{li2022q,
  title={Q-vit: Fully differentiable quantization for vision transformer},
  author={Li, Zhexin and Yang, Tong and Wang, Peisong and Cheng, Jian},
  journal={arXiv preprint arXiv:2201.07703},
  year={2022}
}



@article{mackay1992practical,
  title={A practical Bayesian framework for backpropagation networks},
  author={MacKay, David JC},
  journal={Neural computation},
  volume={4},
  number={3},
  pages={448--472},
  year={1992},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{foret2020sharpness,
  title={Sharpness-aware minimization for efficiently improving generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2010.01412},
  year={2020}
}



@article{kaddour2022flat,
  title={When Do Flat Minima Optimizers Work?},
  author={Kaddour, Jean and Liu, Linqing and Silva, Ricardo and Kusner, Matt J},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16577--16595},
  year={2022}
}


@inproceedings{gal2016dropout,
  title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning},
  pages={1050--1059},
  year={2016},
  organization={PMLR}
}

@article{jospin2022hands,
  title={Hands-on Bayesian neural networks—A tutorial for deep learning users},
  author={Jospin, Laurent Valentin and Laga, Hamid and Boussaid, Farid and Buntine, Wray and Bennamoun, Mohammed},
  journal={IEEE Computational Intelligence Magazine},
  volume={17},
  number={2},
  pages={29--48},
  year={2022},
  publisher={IEEE}
}

@article{miyashita2016convolutional,
  title={Convolutional neural networks using logarithmic data representation},
  author={Miyashita, Daisuke and Lee, Edward H and Murmann, Boris},
  journal={arXiv preprint arXiv:1603.01025},
  year={2016}
}


@inproceedings{przewlocka2023energy,
  title={Energy Efficient Hardware Acceleration of Neural Networks with Power-of-Two Quantisation},
  author={Przewlocka-Rus, Dominika and Kryjak, Tomasz},
  booktitle={Computer Vision and Graphics: Proceedings of the International Conference on Computer Vision and Graphics ICCVG 2022},
  pages={225--236},
  year={2023},
  organization={Springer}
}


@inproceedings{vogel2019bit,
  title={Bit-shift-based accelerator for cnns with selectable accuracy and throughput},
  author={Vogel, Sebastian and Raghunath, Rajatha B and Guntoro, Andre and Van Laerhoven, Kristof and Ascheid, Gerd},
  booktitle={2019 22nd Euromicro Conference on Digital System Design (DSD)},
  pages={663--667},
  year={2019},
  organization={IEEE}
}


@article{ovadia2019can,
  title={Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift},
  author={Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, David and Nowozin, Sebastian and Dillon, Joshua and Lakshminarayanan, Balaji and Snoek, Jasper},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@inproceedings{welling2011bayesian,
  title={Bayesian learning via stochastic gradient Langevin dynamics},
  author={Welling, Max and Teh, Yee W},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={681--688},
  year={2011}
}

@inproceedings{YuhangLiXinDong2020,
  title={Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks},
  author={Li, Yuhang and Dong, Xin and Wang, Wei},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{Wu2018,
abstract = {Many existing compression approaches have been focused and evaluated on convolutional neural networks (CNNs) where fully-connected layers contain the most parameters (e.g., LeNet and AlexNet). However, the current trend of pushing CNNs deeper with convolutions has created a pressing demand to achieve higher compression gains on CNNs where convolutions dominate the parameter amount (e.g., GoogLeNet, ResNet and Wide ResNet). Further, convolutional layers always account for most energy consumption in run time. To this end, this paper investigates the relatively less-explored direction of compressing convolutional layers in deep CNNs. We introduce a novel spectrally relaxed $\kappa$-means regularization, that tends to approximately make hard assignments of convolutional layer weights to K learned cluster centers during re-training. Compression is then achieved through weight-sharing, by only recording K cluster centers and weight assignment indexes. Our proposed pipeline, termed Deep $\kappa$-Means, has well-aligned goals between re-training and compression stages. We further propose an improved set of metrics to estimate energy consumption of CNN hardware implementations, whose estimation results are verified to be consistent with previously proposed energy estimation tool extrapolated from actual hardware measurements. We have evaluated Deep $\kappa$-Means in compressing several CNN models in terms of both compression ratio and energy consumption reduction, observing promising results without incurring accuracy loss.},
annote = {Here the authors take a conv layer of shape:

sxsxcxm 


and reshape to: sxN 

where n = sxcxm 

Then they assign clusters to centroids for each input n for each layer. 

They then, during re-training, add a spectral relaxation of k-means (a closed form evaluation)

They also importantly introduce some energy aware metrics: 

comp cost + weight rep cost + activation rep cost 

Which they define the weight rep cost as 

number of times weights used X index set of weights X bits of weights 

The big flaw in this paper is that they compress convolution only

It also lacks details on how many 'k' they use, since they instead report only cluster rates = k/{\#}numparams},
archivePrefix = {arXiv},
arxivId = {1806.09228},
author = {Wu, Junru and Wang, Yue and Wu, Zhenyu and Wang, Zhangyang and Veeraraghavan, Ashok and Lin, Yingyan},
eprint = {1806.09228},
file = {:Users/chrisculley/Documents/Mendeley Desktop/1806.09228.pdf:pdf},
isbn = {9781510867963},
journal = {35th Int. Conf. Mach. Learn. ICML 2018},
pages = {8523--8532},
title = {{Deep $\kappa$-means: Re-training and parameter sharing with harder cluster assignments for compressing deep convolutions}},
volume = {12},
year = {2018}
}
@article{ullrich2017soft,
  title={Soft weight-sharing for neural network compression},
  author={Ullrich, Karen and Meeds, Edward and Welling, Max},
  journal={arXiv preprint arXiv:1702.04008},
  year={2017}
}

@inproceedings{Stock2019,
  title={And the Bit Goes Down: Revisiting the Quantization of Neural Networks},
  author={Stock, Pierre and Joulin, Armand and Gribonval, R{\'e}mi and Graham, Benjamin and J{\'e}gou, Herv{\'e}},
  booktitle={ICLR 2020-Eighth International Conference on Learning Representations},
  pages={1--11},
  year={2020}
}

@article{fan2020training,
  title={Training with quantization noise for extreme model compression},
  author={Fan, Angela and Stock, Pierre and Graham, Benjamin and Grave, Edouard and Gribonval, R{\'e}mi and Jegou, Herve and Joulin, Armand},
  journal={arXiv preprint arXiv:2004.07320},
  year={2020}
}


@article{zhou2017,
  title={Incremental network quantization: Towards lossless cnns with low-precision weights},
  author={Zhou, Aojun and Yao, Anbang and Guo, Yiwen and Xu, Lin and Chen, Yurong},
  journal={arXiv preprint arXiv:1702.03044},
  year={2017}
}

@article{fortuin2022priors,
  title={Priors in bayesian deep learning: A review},
  author={Fortuin, Vincent},
  journal={International Statistical Review},
  volume={90},
  number={3},
  pages={563--591},
  year={2022},
  publisher={Wiley Online Library}
}


@inproceedings{hernandez2015probabilistic,
  title={Probabilistic backpropagation for scalable learning of bayesian neural networks},
  author={Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Adams, Ryan},
  booktitle={International conference on machine learning},
  pages={1861--1869},
  year={2015},
  organization={PMLR}
}
@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{jouppi2017datacenter,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th annual international symposium on computer architecture},
  pages={1--12},
  year={2017}
}



@inproceedings{wu2018deep,
  title={Deep k-means: Re-training and parameter sharing with harder cluster assignments for compressing deep convolutions},
  author={Wu, Junru and Wang, Yue and Wu, Zhenyu and Wang, Zhangyang and Veeraraghavan, Ashok and Lin, Yingyan},
  booktitle={International Conference on Machine Learning},
  pages={5363--5372},
  year={2018},
  organization={PMLR}
}



@article{han2016eie,
  title={EIE: Efficient inference engine on compressed deep neural network},
  author={Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J},
  journal={ACM SIGARCH Computer Architecture News},
  volume={44},
  number={3},
  pages={243--254},
  year={2016},
  publisher={ACM New York, NY, USA}
}


@inproceedings{horowitz20141,
  title={1.1 computing's energy problem (and what we can do about it)},
  author={Horowitz, Mark},
  booktitle={2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC)},
  pages={10--14},
  year={2014},
  organization={IEEE}
}
 
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}



@article{sze2017efficient,
  title={Efficient processing of deep neural networks: A tutorial and survey},
  author={Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S},
  journal={Proceedings of the IEEE},
  volume={105},
  number={12},
  pages={2295--2329},
  year={2017},
  publisher={Ieee}
}


@article{chen2016eyeriss,
  title={Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks},
  author={Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel S and Sze, Vivienne},
  journal={IEEE journal of solid-state circuits},
  volume={52},
  number={1},
  pages={127--138},
  year={2016},
  publisher={IEEE}
}

ß

 @article{barsbey2021heavy,
  title={Heavy tails in SGD and compressibility of overparametrized neural networks},
  author={Barsbey, Melih and Sefidgaran, Milad and Erdogdu, Murat A and Richard, Gael and Simsekli, Umut},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={29364--29378},
  year={2021}
}

@inproceedings{gurbuzbalaban2021heavy,
  title={The heavy-tail phenomenon in SGD},
  author={Gurbuzbalaban, Mert and Simsekli, Umut and Zhu, Lingjiong},
  booktitle={International Conference on Machine Learning},
  pages={3964--3975},
  year={2021},
  organization={PMLR}
}

@inproceedings{lee2017lognet,
  title={Lognet: Energy-efficient neural networks using logarithmic computation},
  author={Lee, Edward H and Miyashita, Daisuke and Chai, Elaina and Murmann, Boris and Wong, S Simon},
  booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5900--5904},
  year={2017},
  organization={IEEE}
}



@inproceedings{hodgkinson2021multiplicative,
  title={Multiplicative noise and heavy tails in stochastic optimization},
  author={Hodgkinson, Liam and Mahoney, Michael},
  booktitle={International Conference on Machine Learning},
  pages={4262--4274},
  year={2021},
  organization={PMLR}
}
