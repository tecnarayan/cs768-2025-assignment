\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anil et~al.(2019)Anil, Lucas, and Grosse]{anil2019sorting}
Anil, C., Lucas, J., and Grosse, R.
\newblock Sorting out lipschitz function approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  291--301. PMLR, 2019.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Bahdanau, D., Cho, K., and Bengio, Y.
\newblock Neural machine translation by jointly learning to align and translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and Telgarsky]{bartlett2017spectrally}
Bartlett, P.~L., Foster, D.~J., and Telgarsky, M.~J.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Behrmann et~al.(2019)Behrmann, Grathwohl, Chen, Duvenaud, and Jacobsen]{behrmann2019invertible}
Behrmann, J., Grathwohl, W., Chen, R.~T., Duvenaud, D., and Jacobsen, J.-H.
\newblock Invertible residual networks.
\newblock In \emph{International conference on machine learning}, pp.\  573--582. PMLR, 2019.

\bibitem[Bird et~al.(2009)Bird, Klein, and Loper]{bird2009natural}
Bird, S., Klein, E., and Loper, E.
\newblock \emph{Natural language processing with Python: analyzing text with the natural language toolkit}.
\newblock " O'Reilly Media, Inc.", 2009.

\bibitem[Bogachev \& Ruas(2007)Bogachev and Ruas]{bogachev2007measure}
Bogachev, V.~I. and Ruas, M. A.~S.
\newblock \emph{Measure theory}, volume~1.
\newblock Springer, 2007.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Carlini \& Wagner(2017)Carlini and Wagner]{carlini2017towards}
Carlini, N. and Wagner, D.
\newblock Towards evaluating the robustness of neural networks.
\newblock In \emph{2017 ieee symposium on security and privacy (sp)}, pp.\  39--57. Ieee, 2017.

\bibitem[Catellier et~al.(2023)Catellier, Vaiter, and Garreau]{catellier2023robustness}
Catellier, R., Vaiter, S., and Garreau, D.
\newblock On the robustness of text vectorizers.
\newblock \emph{arXiv preprint arXiv:2303.07203}, 2023.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and Duvenaud]{chen2018neural}
Chen, R.~T., Rubanova, Y., Bettencourt, J., and Duvenaud, D.~K.
\newblock Neural ordinary differential equations.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Chen et~al.(2019)Chen, Behrmann, Duvenaud, and Jacobsen]{chen2019residual}
Chen, R.~T., Behrmann, J., Duvenaud, D.~K., and Jacobsen, J.-H.
\newblock Residual flows for invertible generative modeling.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Cisse et~al.(2017)Cisse, Bojanowski, Grave, Dauphin, and Usunier]{cisse2017parseval}
Cisse, M., Bojanowski, P., Grave, E., Dauphin, Y., and Usunier, N.
\newblock Parseval networks: Improving robustness to adversarial examples.
\newblock In \emph{International conference on machine learning}, pp.\  854--863. PMLR, 2017.

\bibitem[Dasoulas et~al.(2021)Dasoulas, Scaman, and Virmaux]{dasoulas2021lipschitz}
Dasoulas, G., Scaman, K., and Virmaux, A.
\newblock Lipschitz normalization for self-attention layers with application to graph neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  2456--2466. PMLR, 2021.

\bibitem[De~Bie et~al.(2019)De~Bie, Peyr{\'e}, and Cuturi]{debie2019stochastic}
De~Bie, G., Peyr{\'e}, G., and Cuturi, M.
\newblock Stochastic deep networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1556--1565. PMLR, 2019.

\bibitem[Dehghani et~al.(2023)Dehghani, Djolonga, Mustafa, Padlewski, Heek, Gilmer, Steiner, Caron, Geirhos, Alabdulmohsin, et~al.]{dehghani2023scaling}
Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., Steiner, A.~P., Caron, M., Geirhos, R., Alabdulmohsin, I., et~al.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock In \emph{International Conference on Machine Learning}, pp.\  7480--7512. PMLR, 2023.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv:1810.04805}, 2018.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv:2010.11929}, 2020.

\bibitem[Fazlyab et~al.(2019)Fazlyab, Robey, Hassani, Morari, and Pappas]{fazlyab2019efficient}
Fazlyab, M., Robey, A., Hassani, H., Morari, M., and Pappas, G.
\newblock Efficient and accurate estimation of lipschitz constants for deep neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Federer(2014)]{federer2014geometric}
Federer, H.
\newblock \emph{Geometric Measure Theory}.
\newblock Classics in Mathematics. Springer Berlin Heidelberg, 2014.
\newblock ISBN 9783642620102.
\newblock URL \url{https://books.google.fr/books?id=jld-BgAAQBAJ}.

\bibitem[Geshkovski et~al.(2023{\natexlab{a}})Geshkovski, Letrouit, Polyanskiy, and Rigollet]{geshkovski2023emergence}
Geshkovski, B., Letrouit, C., Polyanskiy, Y., and Rigollet, P.
\newblock The emergence of clusters in self-attention dynamics.
\newblock \emph{arXiv preprint arXiv:2305.05465}, 2023{\natexlab{a}}.

\bibitem[Geshkovski et~al.(2023{\natexlab{b}})Geshkovski, Letrouit, Polyanskiy, and Rigollet]{geshkovski2023layernorm}
Geshkovski, B., Letrouit, C., Polyanskiy, Y., and Rigollet, P.
\newblock A mathematical perspective on transformers.
\newblock 2023{\natexlab{b}}.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and Szegedy]{goodfellow2014explaining}
Goodfellow, I.~J., Shlens, J., and Szegedy, C.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014.

\bibitem[Gupta \& Verma(2023)Gupta and Verma]{gupta2023certvit}
Gupta, K. and Verma, S.
\newblock Certvit: Certified robustness of pre-trained vision transformers.
\newblock \emph{arXiv preprint arXiv:2302.10287}, 2023.

\bibitem[Hein \& Andriushchenko(2017)Hein and Andriushchenko]{hein2017formal}
Hein, M. and Andriushchenko, M.
\newblock Formal guarantees on the robustness of a classifier against adversarial manipulation.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Hosseini et~al.(2023)Hosseini, Hsu, and Taghvaei]{hosseini2023conditional}
Hosseini, B., Hsu, A.~W., and Taghvaei, A.
\newblock Conditional optimal transport on function spaces.
\newblock \emph{arXiv preprint arXiv:2311.05672}, 2023.

\bibitem[Jia et~al.(2023)Jia, Chen, Mao, Duan, Gu, Zhang, Xue, and Cao]{jia2023revisiting}
Jia, X., Chen, Y., Mao, X., Duan, R., Gu, J., Zhang, R., Xue, H., and Cao, X.
\newblock Revisiting and exploring efficient fast adversarial training via law: Lipschitz regularization and auto weight averaging.
\newblock \emph{arXiv preprint arXiv:2308.11443}, 2023.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
Jiang, A.~Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.~S., Casas, D. d.~l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Kim et~al.(2021)Kim, Papamakarios, and Mnih]{kim2021lipschitz}
Kim, H., Papamakarios, G., and Mnih, A.
\newblock The lipschitz constant of self-attention, 2021.

\bibitem[Kurakin et~al.(2016)Kurakin, Goodfellow, and Bengio]{kurakin2016adversarial}
Kurakin, A., Goodfellow, I., and Bengio, S.
\newblock Adversarial machine learning at scale.
\newblock \emph{arXiv preprint arXiv:1611.01236}, 2016.

\bibitem[Latorre et~al.(2020)Latorre, Rolland, and Cevher]{latorre2020lipschitz}
Latorre, F., Rolland, P., and Cevher, V.
\newblock Lipschitz constant estimation of neural networks via sparse polynomial optimization.
\newblock \emph{arXiv preprint arXiv:2004.08688}, 2020.

\bibitem[Lee et~al.(2019)Lee, Lee, Kim, Kosiorek, Choi, and Teh]{lee2019set}
Lee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., and Teh, Y.~W.
\newblock Set transformer: A framework for attention-based permutation-invariant neural networks.
\newblock In \emph{International conference on machine learning}, pp.\  3744--3753. PMLR, 2019.

\bibitem[Leino et~al.(2021)Leino, Wang, and Fredrikson]{leino2021globally}
Leino, K., Wang, Z., and Fredrikson, M.
\newblock Globally-robust neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  6212--6222. PMLR, 2021.

\bibitem[Likhosherstov et~al.(2021)Likhosherstov, Choromanski, and Weller]{likhosherstov2021expressive}
Likhosherstov, V., Choromanski, K., and Weller, A.
\newblock On the expressive power of self-attention matrices.
\newblock \emph{arXiv preprint arXiv:2106.03764}, 2021.

\bibitem[Liu et~al.(2018)Liu, Saleh, Pot, Goodrich, Sepassi, Kaiser, and Shazeer]{liu2018generating}
Liu, P.~J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R., Kaiser, L., and Shazeer, N.
\newblock Generating wikipedia by summarizing long sequences.
\newblock \emph{arXiv:1801.10198}, 2018.

\bibitem[Lu et~al.(2019)Lu, Li, He, Sun, Dong, Qin, Wang, and Liu]{lu2019understanding}
Lu, Y., Li, Z., He, D., Sun, Z., Dong, B., Qin, T., Wang, L., and Liu, T.-Y.
\newblock Understanding and improving transformer from a multi-particle dynamic system point of view.
\newblock \emph{arXiv preprint arXiv:1906.02762}, 2019.

\bibitem[Miyato et~al.(2015)Miyato, Maeda, Koyama, Nakae, and Ishii]{miyato2015distributional}
Miyato, T., Maeda, S.-i., Koyama, M., Nakae, K., and Ishii, S.
\newblock Distributional smoothing with virtual adversarial training.
\newblock \emph{arXiv preprint arXiv:1507.00677}, 2015.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and Yoshida]{miyato2018spectral}
Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y.
\newblock Spectral normalization for generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1802.05957}, 2018.

\bibitem[Moosavi-Dezfooli et~al.(2016)Moosavi-Dezfooli, Fawzi, and Frossard]{moosavi2016deepfool}
Moosavi-Dezfooli, S.-M., Fawzi, A., and Frossard, P.
\newblock Deepfool: a simple and accurate method to fool deep neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  2574--2582, 2016.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and Srebro]{neyshabur2017exploring}
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N.
\newblock Exploring generalization in deep learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Papernot et~al.(2016)Papernot, McDaniel, Wu, Jha, and Swami]{papernot2016distillation}
Papernot, N., McDaniel, P., Wu, X., Jha, S., and Swami, A.
\newblock Distillation as a defense to adversarial perturbations against deep neural networks.
\newblock In \emph{2016 IEEE symposium on security and privacy (SP)}, pp.\  582--597. IEEE, 2016.

\bibitem[Pevny \& Kovarik(2019)Pevny and Kovarik]{pevny2019approximation}
Pevny, T. and Kovarik, V.
\newblock Approximation capability of neural networks on spaces of probability measures and tree-structured domains.
\newblock \emph{arXiv preprint arXiv:1906.00764}, 2019.

\bibitem[Peyr{\'e} et~al.(2019)Peyr{\'e}, Cuturi, et~al.]{peyre2019computational}
Peyr{\'e}, G., Cuturi, M., et~al.
\newblock Computational optimal transport: With applications to data science.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 11\penalty0 (5-6):\penalty0 355--607, 2019.

\bibitem[Qi et~al.(2023)Qi, Wang, Chen, Shi, and Zhang]{qi2023lipsformer}
Qi, X., Wang, J., Chen, Y., Shi, Y., and Zhang, L.
\newblock Lipsformer: Introducing lipschitz continuity to vision transformers.
\newblock \emph{arXiv preprint arXiv:2304.09856}, 2023.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rosca et~al.(2020)Rosca, Weber, Gretton, and Mohamed]{rosca2020case}
Rosca, M., Weber, T., Gretton, A., and Mohamed, S.
\newblock A case for new neural network smoothness constraints.
\newblock \emph{PMLR}, 2020.

\bibitem[Sander et~al.(2022)Sander, Ablin, Blondel, and Peyr{\'e}]{sander2022sinkformers}
Sander, M.~E., Ablin, P., Blondel, M., and Peyr{\'e}, G.
\newblock Sinkformers: Transformers with doubly stochastic attention.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  3515--3530. PMLR, 2022.

\bibitem[Santambrogio(2015)]{santambrogio2015optimal}
Santambrogio, F.
\newblock Optimal transport for applied mathematicians.
\newblock \emph{Birk{\"a}user, NY}, 55\penalty0 (58-63):\penalty0 94, 2015.

\bibitem[Sokoli{\'c} et~al.(2017)Sokoli{\'c}, Giryes, Sapiro, and Rodrigues]{sokolic2017robust}
Sokoli{\'c}, J., Giryes, R., Sapiro, G., and Rodrigues, M.~R.
\newblock Robust large margin deep neural networks.
\newblock \emph{IEEE Transactions on Signal Processing}, 65\penalty0 (16):\penalty0 4265--4280, 2017.

\bibitem[Sra et~al.(2012)Sra, Nowozin, and Wright]{sra2012optimization}
Sra, S., Nowozin, S., and Wright, S.~J.
\newblock \emph{Optimization for machine learning}.
\newblock Mit Press, 2012.

\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan, Goodfellow, and Fergus]{szegedy2013intriguing}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R.
\newblock Intriguing properties of neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6199}, 2013.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Tsuzuku et~al.(2018)Tsuzuku, Sato, and Sugiyama]{tsuzuku2018lipschitz}
Tsuzuku, Y., Sato, I., and Sugiyama, M.
\newblock Lipschitz-margin training: Scalable certification of perturbation invariance for deep neural networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Virmaux \& Scaman(2018)Virmaux and Scaman]{virmaux2018lipschitz}
Virmaux, A. and Scaman, K.
\newblock Lipschitz regularity of deep neural networks: analysis and efficient estimation.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[von Luxburg \& Bousquet(2004)von Luxburg and Bousquet]{von2004distance}
von Luxburg, U. and Bousquet, O.
\newblock Distance-based classification with lipschitz functions.
\newblock \emph{J. Mach. Learn. Res.}, 5\penalty0 (Jun):\penalty0 669--695, 2004.

\bibitem[Vuckovic et~al.(2020)Vuckovic, Baratin, and des Combes]{Vuckovic2020AMT}
Vuckovic, J., Baratin, A., and des Combes, R.~T.
\newblock A mathematical theory of attention.
\newblock \emph{ArXiv}, abs/2007.02876, 2020.

\bibitem[Vuckovic et~al.(2021)Vuckovic, Baratin, and Combes]{vuckovic2021regularity}
Vuckovic, J., Baratin, A., and Combes, R. T.~d.
\newblock On the regularity of attention.
\newblock \emph{arXiv preprint arXiv:2102.05628}, 2021.

\bibitem[Vyas et~al.(2020)Vyas, Katharopoulos, and Fleuret]{vyas2020fast}
Vyas, A., Katharopoulos, A., and Fleuret, F.
\newblock Fast transformers with clustered attention.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 21665--21674, 2020.

\bibitem[Weng et~al.(2018)Weng, Zhang, Chen, Yi, Su, Gao, Hsieh, and Daniel]{weng2018evaluating}
Weng, T.-W., Zhang, H., Chen, P.-Y., Yi, J., Su, D., Gao, Y., Hsieh, C.-J., and Daniel, L.
\newblock Evaluating the robustness of neural networks: An extreme value theory approach.
\newblock \emph{arXiv preprint arXiv:1801.10578}, 2018.

\bibitem[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, et~al.]{wolf2019huggingface}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et~al.
\newblock Huggingface's transformers: State-of-the-art natural language processing.
\newblock \emph{arXiv preprint arXiv:1910.03771}, 2019.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan, Wang, and Liu]{xiong2020layernorm}
Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T.
\newblock On layer normalization in the transformer architecture.
\newblock In \emph{International Conference on Machine Learning}, pp.\  10524--10533. PMLR, 2020.

\bibitem[Ye et~al.(2023)Ye, Ma, Cao, and Tang]{ye2023mitigating}
Ye, W., Ma, Y., Cao, X., and Tang, K.
\newblock Mitigating transformer overconfidence via lipschitz regularization.
\newblock \emph{arXiv preprint arXiv:2306.06849}, 2023.

\bibitem[Zhai et~al.(2022)Zhai, Kolesnikov, Houlsby, and Beyer]{zhai2022scaling}
Zhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L.
\newblock Scaling vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  12104--12113, 2022.

\bibitem[Zhang \& Sennrich(2019)Zhang and Sennrich]{zhang2019root}
Zhang, B. and Sennrich, R.
\newblock Root mean square layer normalization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Zhang et~al.(2015)Zhang, Zhao, and LeCun]{Zhang2015CharacterlevelCN}
Zhang, X., Zhao, J.~J., and LeCun, Y.
\newblock Character-level convolutional networks for text classification.
\newblock In \emph{NIPS}, 2015.

\bibitem[Zhao et~al.(2020)Zhao, Jiang, Jia, Torr, and Koltun]{zhao2020point}
Zhao, H., Jiang, L., Jia, J., Torr, P., and Koltun, V.
\newblock Point transformer. arxiv.
\newblock \emph{arXiv preprint arXiv:2012.09164}, 2020.

\bibitem[Zweig \& Bruna(2021)Zweig and Bruna]{zweig2021functional}
Zweig, A. and Bruna, J.
\newblock A functional perspective on learning symmetric functions with neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  13023--13032. PMLR, 2021.

\end{thebibliography}
