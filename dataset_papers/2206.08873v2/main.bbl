\begin{thebibliography}{}

\bibitem[Aliprantis and Border, 2006]{aliprantis2006infinite}
Aliprantis, C. and Border, K.~C. (2006).
\newblock {\em Infinite dimensional analysis : a hitchhiker's guide}.
\newblock Springer, Berlin New York.

\bibitem[Ambrosio et~al., 2008]{ambrosio2008gradient}
Ambrosio, L., Gigli, N., and Savar{\'e}, G. (2008).
\newblock {\em Gradient flows: in metric spaces and in the space of probability
  measures}.
\newblock Springer Science \& Business Media.

\bibitem[Arbel et~al., 2019]{arbel2019maximum}
Arbel, M., Korba, A., Salim, A., and Gretton, A. (2019).
\newblock Maximum mean discrepancy gradient flow.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 6481--6491.

\bibitem[Arjovsky et~al., 2017]{arjovsky2017wasserstein}
Arjovsky, M., Chintala, S., and Bottou, L. (2017).
\newblock {W}asserstein generative adversarial networks.
\newblock In {\em International conference on machine learning}, pages
  214--223. PMLR.

\bibitem[Attouch and Brezis, 1986]{attouch1986duality}
Attouch, H. and Brezis, H. (1986).
\newblock Duality for the sum of convex functions in general {B}anach spaces.
\newblock In {\em Aspects of Mathematics and its Applications}, volume~34,
  pages 125--133. Elsevier.

\bibitem[Attouch et~al., 2014]{attouch2014variational}
Attouch, H., Buttazzo, G., and Michaille, G. (2014).
\newblock {\em Variational Analysis in Sobolev and {BV} Spaces}.
\newblock Society for Industrial and Applied Mathematics.

\bibitem[Bauschke et~al., 2017]{bauschke2017descent}
Bauschke, H.~H., Bolte, J., and Teboulle, M. (2017).
\newblock A descent lemma beyond {L}ipschitz gradient continuity: first-order
  methods revisited and applications.
\newblock {\em Mathematics of Operations Research}, 42(2):330--348.

\bibitem[Bauschke et~al., 2003]{Bauschke2003}
Bauschke, H.~H., Borwein, J.~M., and Combettes, P.~L. (2003).
\newblock Bregman monotone optimization algorithms.
\newblock {\em {SIAM} Journal on Control and Optimization}, 42(2):596--636.

\bibitem[Beck and Teboulle, 2003]{beck2003mirror}
Beck, A. and Teboulle, M. (2003).
\newblock Mirror descent and nonlinear projected subgradient methods for convex
  optimization.
\newblock {\em Operations Research Letters}, 31(3):167--175.

\bibitem[Birnbaum et~al., 2011]{Birnbaum2011}
Birnbaum, B., Devanur, N.~R., and Xiao, L. (2011).
\newblock Distributed algorithms via gradient descent for fisher markets.
\newblock In {\em Proceedings of the 12th {ACM} conference on Electronic
  commerce - {EC} {\textquotesingle}11}. {ACM} Press.

\bibitem[Borwein and Goebel, 2003]{borwein2003notions}
Borwein, J. and Goebel, R. (2003).
\newblock Notions of relative interior in banach spaces.
\newblock {\em Journal of Mathematical Sciences}, 115(4).

\bibitem[Braides, 2002]{braides2002gamma}
Braides, A. (2002).
\newblock {\em $\Gamma$-convergence for Beginners}, volume~22.
\newblock Oxford University Press.

\bibitem[Butnariu and Resmerita, 2006]{butnariu2006bregman}
Butnariu, D. and Resmerita, E. (2006).
\newblock Bregman distances, totally convex functions, and a method for solving
  operator equations in {Banach} spaces.
\newblock {\em Abstract and Applied Analysis}, 2006:1--39.

\bibitem[Carlier, 2022]{Carlier2022OnTL}
Carlier, G. (2022).
\newblock On the linear convergence of the multimarginal sinkhorn algorithm.
\newblock {\em SIAM Journal on Optimization}.

\bibitem[Chen and Teboulle, 1993]{chen1993convergence}
Chen, G. and Teboulle, M. (1993).
\newblock Convergence analysis of a proximal-like minimization algorithm using
  {B}regman functions.
\newblock {\em SIAM Journal on Optimization}, 3(3):538--543.

\bibitem[Chen et~al., 2016]{chen2016entropic}
Chen, Y., Georgiou, T., and Pavon, M. (2016).
\newblock Entropic and displacement interpolation: a computational approach
  using the {H}ilbert metric.
\newblock {\em SIAM Journal on Applied Mathematics}, 76(6):2375--2396.

\bibitem[Chizat, 2021]{chizat2021convergence}
Chizat, L. (2021).
\newblock Convergence rates of gradient methods for convex optimization in the
  space of measures.
\newblock {\em arXiv:2105.08368 [math]}.
\newblock arXiv: 2105.08368.

\bibitem[Chizat and Bach, 2018]{chizat2018global}
Chizat, L. and Bach, F. (2018).
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock {\em Advances in neural information processing systems}, 31.

\bibitem[Chu et~al., 2019]{chu2019proba}
Chu, C., Blanchet, J., and Glynn, P. (2019).
\newblock Probability functional descent: A unifying perspective on {GAN}s,
  variational inference, and reinforcement learning.
\newblock In {\em International Conference on Machine Learning (ICML)},
  volume~97, pages 1213--1222.

\bibitem[Csiszar, 1975]{csiszar1975}
Csiszar, I. (1975).
\newblock {$I$}-divergence geometry of probability distributions and
  minimization problems.
\newblock {\em The Annals of Probability}, 3(1):146--158.

\bibitem[Dal~Maso, 1987]{dal1987gamma}
Dal~Maso, G. (1987).
\newblock $\gamma$-convergence and $\mu$-capacities.
\newblock {\em Annali della Scuola Normale Superiore di Pisa-Classe di
  Scienze}, 14(3):423--464.

\bibitem[Dziugaite et~al., 2015]{dziugaite2015training}
Dziugaite, G.~K., Roy, D.~M., and Ghahramani, Z. (2015).
\newblock Training generative neural networks via maximum mean discrepancy
  optimization.
\newblock {\em Uncertainty in Artificial Intelligence}.

\bibitem[Eggermont, 1993]{eggermont1993maximum}
Eggermont, P. P.~B. (1993).
\newblock Maximum entropy regularization for {Fredholm} integral equations of
  the first kind.
\newblock {\em SIAM Journal on Mathematical Analysis}, 24(6):1557--1576.

\bibitem[Franklin and Lorenz, 1989]{Franklin1989}
Franklin, J. and Lorenz, J. (1989).
\newblock On the scaling of multidimensional matrices.
\newblock {\em Linear Algebra and its Applications}, 114-115:717--735.

\bibitem[Korba et~al., 2021]{korba2021kernel}
Korba, A., Aubin-Frankowski, P.-C., Majewski, S., and Ablin, P. (2021).
\newblock Kernel stein discrepancy descent.
\newblock In {\em International Conference on Machine Learning}, pages
  5719--5730. PMLR.

\bibitem[Korba et~al., 2020]{korba2020non}
Korba, A., Salim, A., Arbel, M., Luise, G., and Gretton, A. (2020).
\newblock A non-asymptotic analysis for stein variational gradient descent.
\newblock {\em Advances in Neural Information Processing Systems},
  33:4672--4682.

\bibitem[Kunstner et~al., 2021]{Kunstner2021Homeomorphic}
Kunstner, F., Kumar, R., and Schmidt, M.~W. (2021).
\newblock Homeomorphic-invariance of {EM}: Non-asymptotic convergence in {KL}
  divergence for exponential families via mirror descent.
\newblock In {\em AISTATS}.

\bibitem[Lan et~al., 2011]{lan2011PrimaldualFM}
Lan, G., Lu, Z., and Monteiro, R. D.~C. (2011).
\newblock Primal-dual first-order methods with $o(1/e)$ iteration-complexity
  for cone programming.
\newblock {\em Mathematical Programming}, 126:1--29.

\bibitem[L{\'{e}}ger, 2020]{Leger2020}
L{\'{e}}ger, F. (2020).
\newblock A gradient descent perspective on {S}inkhorn.
\newblock {\em Applied Mathematics {\&} Optimization}, 84(2):1843--1855.

\bibitem[Lu et~al., 2018]{lu2018relatively}
Lu, H., Freund, R.~M., and Nesterov, Y. (2018).
\newblock Relatively smooth convex optimization by first-order methods, and
  applications.
\newblock {\em SIAM Journal on Optimization}, 28(1):333--354.

\bibitem[Lucy, 1974]{Lucy1974}
Lucy, L.~B. (1974).
\newblock An iterative technique for the rectification of observed
  distributions.
\newblock {\em The Astronomical Journal}, 79:745.

\bibitem[Luise et~al., 2019]{luise2019sinkhorn}
Luise, G., Salzo, S., Pontil, M., and Ciliberto, C. (2019).
\newblock {S}inkhorn barycenters with free support via {F}rank-{W}olfe
  algorithm.
\newblock {\em Advances in neural information processing systems}, 32.

\bibitem[Maddison et~al., 2021]{Maddison2021}
Maddison, C.~J., Paulin, D., Teh, Y.~W., and Doucet, A. (2021).
\newblock Dual space preconditioning for gradient descent.
\newblock {\em {SIAM} Journal on Optimization}, 31(1):991--1016.

\bibitem[Mei et~al., 2018]{mei2018mean}
Mei, S., Montanari, A., and Nguyen, P.-M. (2018).
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock {\em Proceedings of the National Academy of Sciences},
  115(33):E7665--E7671.

\bibitem[Mensch and Peyr{\'e}, 2020]{menschpeyre}
Mensch, A. and Peyr{\'e}, G. (2020).
\newblock Online {S}inkhorn: Optimal transport distances from sample streams.
\newblock {\em Advances in Neural Information Processing Systems},
  33:1657--1667.

\bibitem[Milgrom and Segal, 2002]{Milgrom02envelopetheorems}
Milgrom, P. and Segal, I. (2002).
\newblock Envelope theorems for arbitrary choice sets.
\newblock {\em Econometrica}, 70(2):583--610.

\bibitem[Mishchenko, 2019]{mishchenko2019sinkhorn}
Mishchenko, K. (2019).
\newblock Sinkhorn algorithm as a special case of stochastic mirror descent.
\newblock {\em arXiv preprint arXiv:1909.06918, NeurIPS 2019 OTML Workshop}.

\bibitem[Neal and Hinton, 1998]{Neal1998}
Neal, R.~M. and Hinton, G.~E. (1998).
\newblock A view of the {EM} algorithm that justifies incremental, sparse, and
  other variants.
\newblock In {\em Learning in Graphical Models}, pages 355--368. Springer
  Netherlands.

\bibitem[Nutz, 2021]{Nutz2021IntroductionTE}
Nutz, M. (2021).
\newblock Introduction to entropic optimal transport.
\newblock \url{http://www.math.columbia.edu/~mnutz/docs/EOT_lecture_notes.pdf}.

\bibitem[Otto, 2001]{otto2001geometry}
Otto, F. (2001).
\newblock The geometry of dissipative evolution equations: the porous medium
  equation.

\bibitem[Peypouquet, 2015]{peypouquet2015convex}
Peypouquet, J. (2015).
\newblock {\em Convex optimization in normed spaces: theory, methods and
  examples}.
\newblock Springer.

\bibitem[Peyré and Cuturi, 2019]{peyre2019computational}
Peyré, G. and Cuturi, M. (2019).
\newblock Computational optimal transport: With applications to data science.
\newblock {\em Foundations and Trends® in Machine Learning}, 11(5-6):355--607.

\bibitem[Phelps, 1989]{Phelps1989}
Phelps, R.~R. (1989).
\newblock {\em Convex Functions, Monotone Operators and Differentiability}.
\newblock Springer Berlin Heidelberg.

\bibitem[Resmerita, 2005]{resmerita2005regularization}
Resmerita, E. (2005).
\newblock Regularization of ill-posed problems in {Banach} spaces: convergence
  rates.
\newblock {\em Inverse Problems}, 21(4):1303--1314.

\bibitem[Resmerita and Anderssen, 2007]{Resmerita2007}
Resmerita, E. and Anderssen, R.~S. (2007).
\newblock Joint additive {K}ullback--{L}eibler residual minimization and
  regularization for linear inverse problems.
\newblock {\em Mathematical Methods in the Applied Sciences},
  30(13):1527--1544.

\bibitem[Richardson, 1972]{Richardson1972}
Richardson, W.~H. (1972).
\newblock Bayesian-based iterative method of image restoration.
\newblock {\em Journal of the Optical Society of America}, 62(1):55.

\bibitem[Rotskoff and Vanden-Eijnden, 2018]{rotskoff2018neural}
Rotskoff, G.~M. and Vanden-Eijnden, E. (2018).
\newblock Trainability and accuracy of neural networks: An interacting particle
  system approach.
\newblock Technical report.
\newblock (\url{https://arxiv.org/abs/1805.00915}).

\bibitem[Ruschendorf, 1995]{Ruschendorf1995}
Ruschendorf, L. (1995).
\newblock Convergence of the iterative proportional fitting procedure.
\newblock {\em The Annals of Statistics}, 23(4).

\bibitem[Salim et~al., 2020]{salim2020wasserstein}
Salim, A., Korba, A., and Luise, G. (2020).
\newblock The wasserstein proximal gradient algorithm.
\newblock {\em Advances in Neural Information Processing Systems},
  33:12356--12366.

\bibitem[Santambrogio, 2015]{Santambrogio2015}
Santambrogio, F. (2015).
\newblock {\em Optimal Transport for Applied Mathematicians}.
\newblock Springer International Publishing.

\bibitem[Smola et~al., 2007]{smola2007hilbert}
Smola, A., Gretton, A., Song, L., and Sch{\"o}lkopf, B. (2007).
\newblock A {Hilbert} space embedding for distributions.
\newblock In {\em International Conference on Algorithmic Learning Theory},
  pages 13--31. Springer.

\bibitem[Steinwart and Christmann, 2008]{steinwart2008support}
Steinwart, I. and Christmann, A. (2008).
\newblock {\em Support vector machines}.
\newblock Springer Science \& Business Media.

\bibitem[Trillos and Sanz-Alonso, 2020]{trillos2020bayesian}
Trillos, N.~G. and Sanz-Alonso, D. (2020).
\newblock The {B}ayesian update: variational formulations and gradient flows.
\newblock {\em Bayesian Analysis}, 15(1):29--56.

\bibitem[Villani, 2003]{villani2003topics}
Villani, C. (2003).
\newblock {\em Topics in optimal transportation}, volume~58.
\newblock American Mathematical Soc.

\bibitem[Wibisono, 2018]{wibisono2018sampling}
Wibisono, A. (2018).
\newblock Sampling as optimization in the space of measures: The langevin
  dynamics as a composite optimization problem.
\newblock In {\em Conference on Learning Theory}, pages 2093--3027. PMLR.

\end{thebibliography}
