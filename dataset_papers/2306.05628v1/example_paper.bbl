\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anonymous(2023)]{anonymous2023double}
Anonymous.
\newblock Double wins: Boosting accuracy and efficiency of graph neural
  networks by reliable knowledge distillation.
\newblock In \emph{Submitted to The Eleventh International Conference on
  Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=NGIFt6BNvLe}.
\newblock under review.

\bibitem[Belkin \& Niyogi(2001)Belkin and Niyogi]{belkin2001laplacian}
Belkin, M. and Niyogi, P.
\newblock Laplacian eigenmaps and spectral techniques for embedding and
  clustering.
\newblock In \emph{Nips}, volume~14, pp.\  585--591, 2001.

\bibitem[Chen et~al.(2020)Chen, Bian, Xiao, Rong, Xu, and Huang]{chen2020self}
Chen, Y., Bian, Y., Xiao, X., Rong, Y., Xu, T., and Huang, J.
\newblock On self-distilling graph neural network.
\newblock \emph{arXiv preprint arXiv:2011.02255}, 2020.

\bibitem[Feng et~al.(2022)Feng, Li, Yuan, and Wang]{feng2022freekd}
Feng, K., Li, C., Yuan, Y., and Wang, G.
\newblock Freekd: Free-direction knowledge distillation for graph neural
  networks.
\newblock In \emph{Proceedings of the 28th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining}, pp.\  357--366, 2022.

\bibitem[Giles et~al.(1998)Giles, Bollacker, and Lawrence]{giles1998citeseer}
Giles, C.~L., Bollacker, K.~D., and Lawrence, S.
\newblock Citeseer: An automatic citation indexing system.
\newblock In \emph{Proceedings of the third ACM conference on Digital
  libraries}, pp.\  89--98, 1998.

\bibitem[Gou et~al.(2021)Gou, Yu, Maybank, and Tao]{gou2021knowledge}
Gou, J., Yu, B., Maybank, S.~J., and Tao, D.
\newblock Knowledge distillation: A survey.
\newblock \emph{International Journal of Computer Vision}, 129\penalty0
  (6):\penalty0 1789--1819, 2021.

\bibitem[Hamilton et~al.(2017)Hamilton, Ying, and
  Leskovec]{hamilton2017inductive}
Hamilton, W., Ying, Z., and Leskovec, J.
\newblock Inductive representation learning on large graphs.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1024--1034, 2017.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, Dean,
  et~al.]{hinton2015distilling}
Hinton, G., Vinyals, O., Dean, J., et~al.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2\penalty0 (7), 2015.

\bibitem[Hu et~al.(2020)Hu, Fey, Zitnik, Dong, Ren, Liu, Catasta, and
  Leskovec]{hu2020open}
Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M., and
  Leskovec, J.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock \emph{arXiv preprint arXiv:2005.00687}, 2020.

\bibitem[Jia et~al.(2020)Jia, Lin, Ying, You, Leskovec, and
  Aiken]{jia2020redundancy}
Jia, Z., Lin, S., Ying, R., You, J., Leskovec, J., and Aiken, A.
\newblock Redundancy-free computation for graph neural networks.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pp.\  997--1005, 2020.

\bibitem[Joshi et~al.(2021)Joshi, Liu, Xun, Lin, and
  Foo]{joshi2021representation}
Joshi, C.~K., Liu, F., Xun, X., Lin, J., and Foo, C.-S.
\newblock On representation knowledge distillation for graph neural networks.
\newblock \emph{arXiv preprint arXiv:2111.04964}, 2021.

\bibitem[Kipf \& Welling(2016)Kipf and Welling]{kipf2016semi}
Kipf, T.~N. and Welling, M.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock \emph{arXiv preprint arXiv:1609.02907}, 2016.

\bibitem[Lassance et~al.(2020)Lassance, Bontonou, Hacene, Gripon, Tang, and
  Ortega]{lassance2020deep}
Lassance, C., Bontonou, M., Hacene, G.~B., Gripon, V., Tang, J., and Ortega, A.
\newblock Deep geometric knowledge distillation with graphs.
\newblock In \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pp.\  8484--8488. IEEE, 2020.

\bibitem[Liu et~al.(2020)Liu, Gao, and Ji]{liu2020towards}
Liu, M., Gao, H., and Ji, S.
\newblock Towards deeper graph neural networks.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pp.\  338--348, 2020.

\bibitem[McCallum et~al.(2000)McCallum, Nigam, Rennie, and
  Seymore]{mccallum2000automating}
McCallum, A.~K., Nigam, K., Rennie, J., and Seymore, K.
\newblock Automating the construction of internet portals with machine
  learning.
\newblock \emph{Information Retrieval}, 3\penalty0 (2):\penalty0 127--163,
  2000.

\bibitem[McInnes et~al.(2018)McInnes, Healy, and Melville]{mcinnes2018umap}
McInnes, L., Healy, J., and Melville, J.
\newblock Umap: Uniform manifold approximation and projection for dimension
  reduction.
\newblock \emph{arXiv preprint arXiv:1802.03426}, 2018.

\bibitem[Ren et~al.(2021)Ren, Ji, Niu, and Lei]{ren2021multi}
Ren, Y., Ji, J., Niu, L., and Lei, M.
\newblock Multi-task self-distillation for graph-based semi-supervised
  learning.
\newblock \emph{arXiv preprint arXiv:2112.01174}, 2021.

\bibitem[Sen et~al.(2008)Sen, Namata, Bilgic, Getoor, Galligher, and
  Eliassi-Rad]{sen2008collective}
Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B., and Eliassi-Rad, T.
\newblock Collective classification in network data.
\newblock \emph{AI magazine}, 29\penalty0 (3):\penalty0 93--93, 2008.

\bibitem[Shchur et~al.(2018)Shchur, Mumme, Bojchevski, and
  G{\"u}nnemann]{shchur2018pitfalls}
Shchur, O., Mumme, M., Bojchevski, A., and G{\"u}nnemann, S.
\newblock Pitfalls of graph neural network evaluation.
\newblock \emph{arXiv preprint arXiv:1811.05868}, 2018.

\bibitem[Veli{\v{c}}kovi{\'c} et~al.(2017)Veli{\v{c}}kovi{\'c}, Cucurull,
  Casanova, Romero, Lio, and Bengio]{velivckovic2017graph}
Veli{\v{c}}kovi{\'c}, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and
  Bengio, Y.
\newblock Graph attention networks.
\newblock \emph{arXiv preprint arXiv:1710.10903}, 2017.

\bibitem[Wang et~al.(2019)Wang, Zheng, Ye, Gan, Li, Song, Zhou, Ma, Yu, Gai,
  He, Karypis, Li, and Zhang]{wang2019dgl}
Wang, M., Zheng, D., Ye, Z., Gan, Q., Li, M., Song, X., Zhou, J., Ma, C., Yu,
  L., Gai, Y., He, T., Karypis, G., Li, J., and Zhang, Z.
\newblock Deep graph library: A graph-centric, highly-performant package for
  graph neural networks.
\newblock \emph{arXiv preprint arXiv:1909.01315}, 2019.

\bibitem[Wu et~al.(2021{\natexlab{a}})Wu, Lin, Gao, Tan, Li,
  et~al.]{wu2021graphmixup}
Wu, L., Lin, H., Gao, Z., Tan, C., Li, S., et~al.
\newblock Graphmixup: Improving class-imbalanced node classification on graphs
  by self-supervised context prediction.
\newblock \emph{arXiv preprint arXiv:2106.11133}, 2021{\natexlab{a}}.

\bibitem[Wu et~al.(2021{\natexlab{b}})Wu, Lin, Gao, Tan, Li,
  et~al.]{wu2021self}
Wu, L., Lin, H., Gao, Z., Tan, C., Li, S., et~al.
\newblock Self-supervised on graphs: Contrastive, generative, or predictive.
\newblock \emph{arXiv preprint arXiv:2105.07342}, 2021{\natexlab{b}}.

\bibitem[Wu et~al.(2022{\natexlab{a}})Wu, Lin, Huang, and Li]{wu2022knowledge}
Wu, L., Lin, H., Huang, Y., and Li, S.~Z.
\newblock Knowledge distillation improves graph structure augmentation for
  graph neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{a}}.

\bibitem[Wu et~al.(2022{\natexlab{b}})Wu, Xia, Lin, Gao, Liu, Zhao, and
  Li]{wu2022teaching}
Wu, L., Xia, J., Lin, H., Gao, Z., Liu, Z., Zhao, G., and Li, S.~Z.
\newblock Teaching yourself: Graph self-distillation on neighborhood for node
  classification.
\newblock \emph{arXiv preprint arXiv:2210.02097}, 2022{\natexlab{b}}.

\bibitem[Wu et~al.(2023{\natexlab{a}})Wu, Lin, Hu, Tan, Gao, Liu, and
  Li]{wu2023beyond}
Wu, L., Lin, H., Hu, B., Tan, C., Gao, Z., Liu, Z., and Li, S.~Z.
\newblock Beyond homophily and homogeneity assumption: Relation-based frequency
  adaptive graph neural networks.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  2023{\natexlab{a}}.

\bibitem[Wu et~al.(2023{\natexlab{b}})Wu, Lin, Huang, Fan, and
  Li]{wu2023extracting}
Wu, L., Lin, H., Huang, Y., Fan, T., and Li, S.~Z.
\newblock Extracting low-/high- frequency knowledge from graph neural networks
  and injecting it into mlps: An effective gnn-to-mlp distillation framework.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2023{\natexlab{b}}.

\bibitem[Wu et~al.(2020)Wu, Pan, Chen, Long, Zhang, and
  Philip]{wu2020comprehensive}
Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Philip, S.~Y.
\newblock A comprehensive survey on graph neural networks.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  2020.

\bibitem[Yan et~al.(2020)Yan, Wang, Guo, and Lou]{yan2020tinygnn}
Yan, B., Wang, C., Guo, G., and Lou, Y.
\newblock Tinygnn: Learning efficient graph neural networks.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pp.\  1848--1856, 2020.

\bibitem[Yang et~al.(2020{\natexlab{a}})Yang, Wang, Yao, Liu, and
  Abdelzaher]{yang2020revisiting}
Yang, C., Wang, R., Yao, S., Liu, S., and Abdelzaher, T.
\newblock Revisiting over-smoothing in deep gcns.
\newblock \emph{arXiv preprint arXiv:2003.13663}, 2020{\natexlab{a}}.

\bibitem[Yang et~al.(2021)Yang, Liu, and Shi]{yang2021extract}
Yang, C., Liu, J., and Shi, C.
\newblock Extract the knowledge of graph neural networks and go beyond it: An
  effective knowledge distillation framework.
\newblock In \emph{Proceedings of the Web Conference 2021}, pp.\  1227--1237,
  2021.

\bibitem[Yang et~al.(2020{\natexlab{b}})Yang, Qiu, Song, Tao, and
  Wang]{yang2020distilling}
Yang, Y., Qiu, J., Song, M., Tao, D., and Wang, X.
\newblock Distilling knowledge from graph convolutional networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  7074--7083, 2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Lin, Liu, Zhou, Tang, Liang,
  and Xing]{zhang2020iterative}
Zhang, H., Lin, S., Liu, W., Zhou, P., Tang, J., Liang, X., and Xing, E.~P.
\newblock Iterative graph self-distillation.
\newblock \emph{arXiv preprint arXiv:2010.12609}, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2021)Zhang, Liu, Sun, and Shah]{zhang2021graph}
Zhang, S., Liu, Y., Sun, Y., and Shah, N.
\newblock Graph-less neural networks: Teaching old mlps new tricks via
  distillation.
\newblock \emph{arXiv preprint arXiv:2110.08727}, 2021.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Miao, Shao, Jiang, Chen, Ruas,
  and Cui]{zhang2020reliable}
Zhang, W., Miao, X., Shao, Y., Jiang, J., Chen, L., Ruas, O., and Cui, B.
\newblock Reliable data distillation on graph convolutional network.
\newblock In \emph{Proceedings of the 2020 ACM SIGMOD International Conference
  on Management of Data}, pp.\  1399--1414, 2020{\natexlab{b}}.

\bibitem[Zhou et~al.(2020)Zhou, Cui, Hu, Zhang, Yang, Liu, Wang, Li, and
  Sun]{zhou2020graph}
Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., Wang, L., Li, C., and
  Sun, M.
\newblock Graph neural networks: A review of methods and applications.
\newblock \emph{AI Open}, 1:\penalty0 57--81, 2020.

\end{thebibliography}
