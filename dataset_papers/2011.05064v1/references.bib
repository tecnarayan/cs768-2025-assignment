@misc{Waa2018Contrastive,
    title={Contrastive Explanations for Reinforcement Learning in terms of Expected Consequences},
    author={Jasper van der Waa and Jurriaan van Diggelen and Karel van den Bosch and Mark Neerincx},
    year={2018},
    eprint={1807.08706},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@inproceedings{
fu2018learning,
title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},
author={Justin Fu and Katie Luo and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rkHywl-A-},
}
@article{kim2017interpretability,
  title={Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)},
  author={Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
  journal={arXiv preprint arXiv:1711.11279},
  year={2017}
}

@misc{wulfmeier2019regularized,
    title={Regularized Hierarchical Policies for Compositional Transfer in Robotics},
    author={Markus Wulfmeier and Abbas Abdolmaleki and Roland Hafner and Jost Tobias Springenberg and Michael Neunert and Tim Hertweck and Thomas Lampe and Noah Siegel and Nicolas Heess and Martin Riedmiller},
    year={2019},
    eprint={1906.11228},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@ARTICLE{Niroui2019,
author={F. {Niroui} and K. {Zhang} and Z. {Kashino} and G. {Nejat}},
journal={IEEE Robotics and Automation Letters},
title={Deep Reinforcement Learning Robot for Search and Rescue Applications: Exploration in Unknown Cluttered Environments},
year={2019},
volume={4},
number={2},
pages={610-617},
abstract={Rescue robots can be used in urban search and rescue (USAR) applications to perform the important task of exploring unknown cluttered environments. Due to the unpredictable nature of these environments, deep learning techniques can be used to perform these tasks. In this letter, we present the first use of deep learning to address the robot exploration task in USAR applications. In particular, we uniquely combine the traditional approach of frontier-based exploration with deep reinforcement learning to allow a robot to autonomously explore unknown cluttered environments. Experiments conducted with a mobile robot in unknown cluttered environments of varying sizes and layouts showed that the proposed exploration approach can effectively determine appropriate frontier locations to navigate to, while being robust to different environment layouts and sizes. Furthermore, a comparison study with other frontier exploration approaches showed that our learning-based frontier exploration technique was able to explore more of an environment earlier on, allowing for potential identification of a larger number of victims at the beginning of the time-critical exploration task.},
keywords={learning (artificial intelligence);path planning;rescue robots;urban search and rescue applications;deep reinforcement learning robot;mobile robot;USAR applications;robot exploration task;rescue robots;unknown cluttered environments;time-critical exploration task;learning-based frontier exploration technique;Computer architecture;Robot sensing systems;Microprocessors;Task analysis;Navigation;Layout;Autonomous agents;deep learning in robotics and automation;search and rescue robots},
doi={10.1109/LRA.2019.2891991},
ISSN={},
month={April},}

@misc{OpenAI2016,
author = {OpenAI},
mendeley-groups = {PhD Literatures},
title = {{Faulty Reward Functions in the Wild}},
url = {https://openai.com/blog/faulty-reward-functions/},
urldate = {2019-10-23},
year = {2016}
}

@incollection{Malle2001,
author = {Malle, Bertram},
booktitle = {Intentions and Intentionality},
chapter = {13},
doi = {10.7551/mitpress/3838.003.0019},
file = {:user/HS221/hy00185/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - (PDF) Folk Explanations of Intentional Action.pdf:pdf},
mendeley-groups = {PhD Literatures},
title = {{Folk Explanations of Intentional Action}},
url = {https://www.researchgate.net/publication/246208961{\_}Folk{\_}Explanations{\_}of{\_}Intentional{\_}Action},
year = {2001}
}

@inproceedings{Mittelstadt2019,
abstract = {Recent work on interpretability in machine learning and AI has focused on the building of simplified models that approximate the true criteria used to make decisions. These models are a useful pedagogical device for teaching trained professionals how to predict what decisions will be made by the complex system, and most importantly how the system might break. However, when considering any such model it's important to remember Box's maxim that "All models are wrong but some are useful." We focus on the distinction between these models and explanations in philosophy and sociology. These models can be understood as a "do it yourself kit" for explanations, allowing a practitioner to directly answer "what if questions" or generate contrastive explanations without external assistance. Although a valuable ability, giving these models as explanations appears more difficult than necessary, and other forms of explanation may not have the same trade-offs. We contrast the different schools of thought on what makes an explanation, and suggest that machine learning might benefit from viewing the problem more broadly.},
archivePrefix = {arXiv},
arxivId = {1811.01439},
author = {Mittelstadt, Brent and Russell, Chris and Wachter, Sandra},
booktitle = {FAT* 2019 - Proceedings of the 2019 Conference on Fairness, Accountability, and Transparency},
doi = {10.1145/3287560.3287574},
eprint = {1811.01439},
file = {:user/HS221/hy00185/Documents/p279-Mittelstadt.pdf:pdf},
isbn = {9781450361255},
keywords = {Accountability,Explanations,Interpretability,Philosophy of Science},
mendeley-groups = {PhD Literatures},
month = {jan},
pages = {279--288},
publisher = {Association for Computing Machinery, Inc},
title = {{Explaining explanations in AI}},
year = {2019}
}

@techreport{Montavon2017,
abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. It introduces some recently proposed techniques of interpretation, along with theory, tricks and recommendations, to make most efficient use of these techniques on real data. It also discusses a number of practical applications.},
author = {Montavon, Gr{\'{e}}goire and Samek, Wojciech and M{\"{u}}ller, Klaus-Robert},
file = {:home/herman/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Montavon, Samek, M{\"{u}}ller - Unknown - Methods for Interpreting and Understanding Deep Neural Networks.pdf:pdf},
keywords = {Taylor decomposition,activation maximization,deep neural networks,layer-wise relevance propagation,sensitivity analysis},
mendeley-groups = {Explanability,PhD Literatures},
title = {{Methods for Interpreting and Understanding Deep Neural Networks}}
}

@techreport{Guo19,
abstract = {Understanding and interpreting how machine learning (ML) models make decisions have been a big challenge. While recent research has proposed various technical approaches to provide some clues as to how an ML model makes individual predictions, they cannot provide users with an ability to inspect a model as a complete entity. In this work, we propose a novel technical approach that augments a Bayesian non-parametric regression mixture model with multiple elastic nets. Using the enhanced mixture model, we can extract generalizable insights for a target model through a global approximation. To demonstrate the utility of our approach, we evaluate it on different ML models in the context of image recognition. The empirical results indicate that our proposed approach not only outperforms the state-of-the-art techniques in explaining individual decisions but also provides users with an ability to discover the vulnerabilities of the target ML models.},
author = {Guo, Wenbo and Huang, Sui and Tao, Yunzhe and Xing, Xinyu and Lin, Lin},
file = {:home/herman/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo et al. - Unknown - Explaining Deep Learning Models-A Bayesian Non-parametric Approach.pdf:pdf},
mendeley-groups = {Explanability,PhD Literatures},
title = {{Explaining Deep Learning Models-A Bayesian Non-parametric Approach}}
}

@inproceedings{Ribeiro2016,
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
archivePrefix = {arXiv},
arxivId = {1602.04938},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/2939672.2939778},
eprint = {1602.04938},
file = {:user/HS221/hy00185/Documents/p1135-ribeiro.pdf:pdf},
isbn = {9781450342322},
mendeley-groups = {Explanability,PhD Literatures},
pages = {1135--1144},
title = {{"Why Should I Trust You?" Explaining the Predictions of Any Classifier}},
url = {http://dx.doi.org/10.1145/2939672.2939778},
volume = {13-17-Augu},
year = {2016}
}

@misc{shrikumar2017learning,
    title={Learning Important Features Through Propagating Activation Differences},
    author={Avanti Shrikumar and Peyton Greenside and Anshul Kundaje},
    year={2017},
    eprint={1704.02685},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{Vasic2019,
abstract = {Deep Reinforcement Learning (DRL) has led to many recent breakthroughs on complex control tasks, such as defeating the best human player in the game of Go. However, decisions made by the DRL agent are not explainable, hindering its applicability in safety-critical settings. Viper, a recently proposed technique, constructs a decision tree policy by mimicking the DRL agent. Decision trees are interpretable as each action made can be traced back to the decision rule path that lead to it. However, one global decision tree approximating the DRL policy has significant limitations with respect to the geometry of decision boundaries. We propose Mo{\"{E}}T, a more expressive, yet still interpretable model based on Mixture of Experts, consisting of a gating function that partitions the state space, and multiple decision tree experts that specialize on different partitions. We propose a training procedure to support non-differentiable decision tree experts and integrate it into imitation learning procedure of Viper. We evaluate our algorithm on four OpenAI gym environments, and show that the policy constructed in such a way is more performant and better mimics the DRL agent by lowering mispredictions and increasing the reward. We also show that Mo{\"{E}}T policies are amenable for verification using off-the-shelf automated theorem provers such as Z3.},
archivePrefix = {arXiv},
arxivId = {1906.06717},
author = {Vasic, Marko and Petrovic, Andrija and Wang, Kaiyuan and Nikolic, Mladen and Singh, Rishabh and Khurshid, Sarfraz},
eprint = {1906.06717},
file = {:user/HS221/hy00185/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vasic et al. - Unknown - MO¨ETMO¨ MO¨ET Interpretable and Verifiable Reinforcement Learning via Mixture of Expert Trees.pdf:pdf},
mendeley-groups = {PhD Literatures/TBR},
pages = {1--15},
title = {{MoET: Interpretable and Verifiable Reinforcement Learning via Mixture of Expert Trees}},
url = {http://arxiv.org/abs/1906.06717},
year = {2019}
}


@inproceedings{Bastani2018,
abstract = {While deep reinforcement learning has successfully solved many challenging control tasks, its real-world applicability has been limited by the inability to ensure the safety of learned policies. We propose an approach to verifiable reinforcement learning by training decision tree policies, which can represent complex policies (since they are nonparametric), yet can be efficiently verified using existing techniques (since they are highly structured). The challenge is that decision tree policies are difficult to train. We propose VIPER, an algorithm that combines ideas from model compression and imitation learning to learn decision tree policies guided by a DNN policy (called the oracle) and its Q-function, and show that it substantially outperforms two baselines. We use VIPER to (i) learn a provably robust decision tree policy for a variant of Atari Pong with a symbolic state space, (ii) learn a decision tree policy for a toy game based on Pong that provably never loses, and (iii) learn a provably stable decision tree policy for cart-pole. In each case, the decision tree policy achieves performance equal to that of the original DNN policy.},
archivePrefix = {arXiv},
arxivId = {1805.08328},
author = {Bastani, Osbert and Pu, Yewen and Solar-Lezama, Armando},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1805.08328},
file = {:user/HS221/hy00185/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bastani, Pu, Solar-Lezama - Unknown - Verifiable Reinforcement Learning via Policy Extraction(2).pdf:pdf},
issn = {10495258},
mendeley-groups = {PhD Literatures/TBR},
pages = {2494--2504},
title = {{Verifiable reinforcement learning via policy extraction}},
volume = {2018-Decem},
year = {2018}
}

@article{Hinton2015,
abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
eprint = {1503.02531},
file = {:user/HS221/hy00185/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton, Vinyals, Dean - 2015 - Distilling the Knowledge in a Neural Network.pdf:pdf},
mendeley-groups = {PhD Literatures/TBR},
month = {mar},
title = {{Distilling the Knowledge in a Neural Network}},
url = {http://arxiv.org/abs/1503.02531},
year = {2015}
}

@article{Topin2019,
abstract = {Though reinforcement learning has greatly benefited from the incorporation of neural networks, the inability to verify the correctness of such systems limits their use. Current work in explainable deep learning focuses on explaining only a single decision in terms of input features, making it unsuitable for explaining a sequence of decisions. To address this need, we introduce Abstracted Policy Graphs, which are Markov chains of abstract states. This representation concisely summarizes a policy so that individual decisions can be explained in the context of expected future transitions. Additionally, we propose a method to generate these Abstracted Policy Graphs for deterministic policies given a learned value function and a set of observed transitions, potentially off-policy transitions used during training. Since no restrictions are placed on how the value function is generated, our method is compatible with many existing reinforcement learning methods. We prove that the worst-case time complexity of our method is quadratic in the number of features and linear in the number of provided transitions, O(|F|2|tr samples|). By applying our method to a family of domains, we show that our method scales well in practice and produces Abstracted Policy Graphs which reliably capture relationships within these domains.},
archivePrefix = {arXiv},
arxivId = {1905.12044},
author = {Topin, Nicholay and Veloso, Manuela},
doi = {10.1609/aaai.v33i01.33012514},
eprint = {1905.12044},
file = {:user/HS221/hy00185/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Topin, Veloso - Unknown - Generation of Policy-Level Explanations for Reinforcement Learning.pdf:pdf},
issn = {2159-5399},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
mendeley-groups = {PhD Literatures/TBR},
pages = {2514--2521},
title = {{Generation of Policy-Level Explanations for Reinforcement Learning}},
url = {www.aaai.org},
volume = {33},
year = {2019}
}

@techreport{Mott2019,
abstract = {Inspired by recent work in attention models for image captioning and question answering, we present a soft attention model for the reinforcement learning domain. This model uses a soft, top-down attention mechanism to create a bottleneck in the agent, forcing it to focus on task-relevant information by sequentially querying its view of the environment. The output of the attention mechanism allows direct observation of the information used by the agent to select its actions, enabling easier interpretation of this model than of traditional models. We analyze different strategies that the agents learn and show that a handful of strategies arise repeatedly across different games. We also show that the model learns to query separately about space and content ("where" vs. "what"). We demonstrate that an agent using this mechanism can achieve performance competitive with state-of-the-art models on ATARI tasks while still being interpretable.},
archivePrefix = {arXiv},
arxivId = {1906.02500v1},
author = {Mott, Alex and Zoran, Daniel and Chrzanowski, Mike and Wierstra, Daan and Rezende, Danilo J},
eprint = {1906.02500v1},
file = {:user/HS221/hy00185/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mott et al. - Unknown - Towards Interpretable Reinforcement Learning Using Attention Augmented Agents.pdf:pdf},
mendeley-groups = {PhD Literatures/TBR},
title = {{Towards Interpretable Reinforcement Learning Using Attention Augmented Agents}},
year = {2019}
}

@techreport{Annasamy2019IDQN,
abstract = {Deep reinforcement learning techniques have demonstrated superior performance in a wide variety of environments. As improvements in training algorithms continue at a brisk pace, theoretical or empirical studies on understanding what these networks seem to learn, are far behind. In this paper we propose an interpretable neural network architecture for Q-learning which provides a global explanation of the model's behavior using key-value memories, attention and reconstructible em-beddings. With a directed exploration strategy, our model can reach training rewards comparable to the state-of-the-art deep Q-learning models. However, results suggest that the features extracted by the neural network are extremely shallow and subsequent testing using out-of-sample examples shows that the agent can easily overfit to trajectories seen during training.},
archivePrefix = {arXiv},
arxivId = {1809.05630v2},
author = {Annasamy, Raghuram Mandyam and Sycara, Katia},
eprint = {1809.05630v2},
file = {:user/HS221/hy00185/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Annasamy, Sycara - Unknown - Towards Better Interpretability in Deep Q-Networks.pdf:pdf},
mendeley-groups = {PhD Literatures/Explanability},
title = {{Towards Better Interpretability in Deep Q-Networks}},
url = {www.aaai.org},
year = {2019}
}

@techreport{Oh2017VPN,
abstract = {This paper proposes a novel deep reinforcement learning (RL) architecture, called Value Prediction Network (VPN), which integrates model-free and model-based RL methods into a single neural network. In contrast to typical model-based RL methods, VPN learns a dynamics model whose abstract states are trained to make option-conditional predictions of future values (discounted sum of rewards) rather than of future observations. Our experimental results show that VPN has several advantages over both model-free and model-based baselines in a stochastic environment where careful planning is required but building an accurate observation-prediction model is difficult. Furthermore, VPN outperforms Deep Q-Network (DQN) on several Atari games even with short-lookahead planning, demonstrating its potential as a new way of learning a good state representation.},
author = {Oh, Junhyuk and Singh, Satinder and Lee, Honglak},
file = {:user/HS221/hy00185/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Oh, Singh, Lee - Unknown - Value Prediction Network.pdf:pdf},
mendeley-groups = {PhD Literatures},
title = {{Value Prediction Network}}
}

@techreport{Kipf2019,
abstract = {A structured understanding of our world in terms of objects, relations, and hierarchies is an important component of human cognition. Learning such a structured world model from raw sensory data remains a challenge. As a step towards this goal, we introduce Contrastively-trained Structured World Models (C-SWMs). C-SWMs utilize a contrastive approach for representation learning in environments with compositional structure. We structure each state embedding as a set of object representations and their relations, modeled by a graph neural network. This allows objects to be discovered from raw pixel observations without direct supervision as part of the learning process. We evaluate C-SWMs on compositional environments involving multiple interacting objects that can be manipulated independently by an agent, simple Atari games, and a multi-object physics simulation. Our experiments demonstrate that C-SWMs can overcome limitations of models based on pixel reconstruction and outperform typical representatives of this model class in highly structured environments, while learning interpretable object-based representations.},
archivePrefix = {arXiv},
arxivId = {1911.12247v2},
author = {Kipf, Thomas and {Van Der Pol}, Elise and Welling, Max},
eprint = {1911.12247v2},
file = {:user/HS221/hy00185/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kipf, Van Der Pol, Welling - Unknown - CONTRASTIVE LEARNING OF STRUCTURED WORLD MODELS.pdf:pdf},
mendeley-groups = {PhD Literatures/TBR},
title = {{Contrastive Learning of Strucutred World Models}}
}

@techreport{Lu2018delusional,
abstract = {We identify a fundamental source of error in Q-learning and other forms of dynamic programming with function approximation. Delusional bias arises when the approximation architecture limits the class of expressible greedy policies. Since standard Q-updates make globally uncoordinated action choices with respect to the expressible policy class, inconsistent or even conflicting Q-value estimates can result, leading to pathological behaviour such as over/under-estimation, instability and even divergence. To solve this problem, we introduce a new notion of policy consistency and define a local backup process that ensures global consistency through the use of information sets-sets that record constraints on policies consistent with backed-up Q-values. We prove that both the model-based and model-free algorithms using this backup remove delusional bias, yielding the first known algorithms that guarantee optimal results under general conditions. These algorithms furthermore only require polynomially many information sets (from a potentially exponential support). Finally, we suggest other practical heuristics for value-iteration and Q-learning that attempt to reduce delusional bias.},
author = {Google, Tyler Lu and Schuurmans, Dale and Ai, Google and Boutilier, Craig},
file = {:user/HS221/hy00185/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Google et al. - Unknown - Non-delusional Q-learning and Value Iteration.pdf:pdf},
mendeley-groups = {PhD Literatures},
title = {{Non-delusional Q-learning and Value Iteration}}
}

@paper{Marom2018AAAI,
	author = {Ofir Marom and Benjamin Rosman},
	title = {Belief Reward Shaping in Reinforcement Learning},
	conference = {AAAI Conference on Artificial Intelligence},
	year = {2018},
	keywords = {reward shaping; Bayesian statistics},
	abstract = {A key challenge in many reinforcement learning problems is delayed rewards, which can significantly slow down learning. Although reward shaping has previously been introduced to accelerate learning by bootstrapping an agent with additional information, this can lead to problems with convergence. We present a novel Bayesian reward shaping framework that augments the reward distribution with prior beliefs that decay with experience. Formally, we prove that under suitable conditions a Markov decision process augmented with our framework is consistent with the optimal policy of the original MDP when using the Q-learning algorithm. However, in general our method integrates seamlessly with any reinforcement learning algorithm that learns a value or action-value function through experience. Experiments are run on a gridworld and a more complex backgammon domain that show that we can learn tasks significantly faster when we specify intuitive priors on the reward distribution.},

	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16912}
}

@inproceedings{Ng1999ICML,
author = {Ng, Andrew Y. and Harada, Daishi and Russell, Stuart J.},
title = {Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping},
year = {1999},
isbn = {1558606122},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Sixteenth International Conference on Machine Learning},
pages = {278–287},
numpages = {10},
series = {ICML ’99}
}

@article{Wiewiora2003,
   title={Potential-Based Shaping and Q-Value Initialization are Equivalent},
   volume={19},
   ISSN={1076-9757},
   url={http://dx.doi.org/10.1613/jair.1190},
   DOI={10.1613/jair.1190},
   journal={Journal of Artificial Intelligence Research},
   publisher={AI Access Foundation},
   author={Wiewiora, E.},
   year={2003},
   month={Sep},
   pages={205–208}
}

@techreport{Moreno2018,
abstract = {1 Learning neural belief states An important challenge in reinforcement learning arises in domains where the agent's observations are partial or noisy measurements of the state of the environment. In such domains, a policy that depends only on the current observation x t is generally suboptimal; an optimal policy must in principle depend on the entire history of observations and actions h t = (x 1 , a 1 ,. .. , a t−1 , x t). Alternatively, an optimal policy can depend on a statistic b t of the history h t , as long as b t is sufficient for predicting future observations; in a POMDP, b t is known as a belief state [1, 2, 3]. Ideally, a rich belief state should capture the agent's memory of the past (e.g. where the agent has been) as well as represent the agent's remaining uncertainty about the world (e.g. what the agent has not yet seen but may be able to infer). The most commonly used solution for tackling POMDPs in deep RL is to endow agents with memory (e.g. LSTMs), which could in principle learn such a representation implicitly through model-free reinforcement learning. However, the memory formed is often limited, and the reward signal alone may be too weak to form a good approximation to the belief state. Enriching the learning signal with auxiliary losses (see e.g. [4, 5]) often increases performance, but does not capture a clear or interpretable notion of uncertainty. Similar observations were made in [6], where the belief state is represented by a collection of particles. [7] also investigates agents with predictive modeling of the environment, but filtering state-space models do not provide access to a full belief state, single samples only. In contrast, our approach is to learn a neural belief state, i.e. a representation that fully parametrizes the state distribution. Our design of agent architectures is guided by the fact that the posterior p(s t | h t) over the state of the environment s t given the history h t is itself a belief state, and so is any statistic b t = $\phi$(h t) for which p(s t | b t) = p(s t | h t). Our proposed agent architectures (Fig. 1a and 1b) consist of a recurrent model b t = f (b t−1 , a t−1 , x t) that aggregates the history and computes the belief state b t , and a conditional generative model p(s t | b t) that predicts a distribution of the state given b t. In addition to maximizing reward, we train the agent with an auxiliary loss that encourages p(s t | b t) to become the posterior over the true state of the environment. We investigate two training schemes for achieving this: (i) Supervised with privileged information, where the true current state of the environment is provided as a target to the generative model at training time (but not at test time). (ii) Unsupervised, where the state is defined implicitly by reconstructing future observations (no privileged information). 2 Experiments and discussion Our first environment is MiniPacman [8], a 15 × 19 grid world where the agent (Pacman) navigates a maze and tries to eat all the food while being chased by enemies. The observation is restricted to a 5 × 5 window around Pacman (Fig. 2a). The second is Numpad, an environment with continuous states and actions. A torque-actuated spherical robot (6 DoF, 3 action dimensions) has to visit a sequence of numbered tiles on a platform which light up if they were stepped on in the right order and turn off when stepped on out of sequence. The robot only observes its location on a platform, an indicator of which tiles are currently active, and a partial specification of the sequence of tiles it has to visit (Fig. 2b and 2c, also appendix A). This task is difficult because the agent needs to search for the correct sequence and memorize previous unsuccessful attempts, while also learning to actuate the rolling ball so as to produce temporally coherent behavior which includes turning and speeding up.},
archivePrefix = {arXiv},
arxivId = {1611.05397},
author = {Moreno, Pol and Humplik, Jan and Papamakarios, George and {\'{A}}vila, Bernardo and Lars, Pires and Nicolas, Buesing and Th{\'{e}}ophane, Heess and Deepmind, Weber},
eprint = {1611.05397},
file = {:user/HS221/hy00185/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moreno et al. - Unknown - Neural belief states for partially observed domains.pdf:pdf},
mendeley-groups = {PhD Literatures/Explanability},
title = {{Neural belief states for partially observed domains}}
}

@techreport{Juozapaitis2019,
abstract = {We study reward decomposition for explaining the decisions of reinforcement learning (RL) agents. The approach decomposes rewards into sums of semantically meaningful reward types, so that actions can be compared in terms of trade-offs among the types. In particular, we introduce the concept of minimum sufficient explanations for compactly explaining why one action is preferred over another in terms of the types. Many prior RL algorithms for decomposed rewards produced inconsistent decomposed values, which can be ill-suited to explanation. We exploit an off-policy variant of Q-learning that provably converges to an optimal policy and the correct decomposed action values. We illustrate the approach in a number of domains, showing its utility for explanations.},
author = {Juozapaitis, Zoe and Koul, Anurag and Fern, Alan and Erwig, Martin and Doshi-Velez, Finale},
file = {:home/hmhyau/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Juozapaitis et al. - Unknown - Explainable Reinforcement Learning via Reward Decomposition.pdf:pdf},
mendeley-groups = {PhD Literatures/Explanability},
title = {{Explainable Reinforcement Learning via Reward Decomposition}},
year = {2019}
}

@techreport{Erwig2019,
abstract = {Adaptation Based Programming (ABP) allows programmers to employ "choice points" at program locations where they are uncertain about how to best code the program logic. Reinforcement learning (RL) is then used to automatically learn to make choice-point decisions to optimize the reward achieved by the program. In this paper, we consider a new approach to explaining the learned decisions of adaptive programs. The key idea is to include simple program annotations that define multiple semantically meaningful reward types, which compose to define the overall reward signal used for learning. Using these reward types we define the notion of reward difference explanations (RDXs), which aim to explain why at a choice point an alternative A was selected over another alternative B. An RDX gives the difference in the predicted future reward of each type when selecting A versus B and then continuing to run the adaptive program. Significant differences can provide insight into why A was or was not preferred to B. We describe a SARSA-style learning algorithm for learning to optimize the choices at each choice point, while also learning side information for producing RDXs. We demonstrate this explanation approach through a case study in a synthetic domain, which shows the general promise of the approach and highlights future research questions.},
author = {Erwig, Martin and Fern, Alan and Murali, Magesh and Koul, Anurag},
file = {:home/hmhyau/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Erwig et al. - Unknown - Explaining Deep Adaptive Programs via Reward Decomposition.pdf:pdf},
mendeley-groups = {PhD Literatures/Explanability},
title = {{Explaining Deep Adaptive Programs via Reward Decomposition}}
}

@incollection{LundbergNIPS2017SHAP,
title = {A Unified Approach to Interpreting Model Predictions},
author = {Lundberg, Scott M and Lee, Su-In},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {4765--4774},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf}
}

@ARTICLE{Samek2017,
  author={W. {Samek} and A. {Binder} and G. {Montavon} and S. {Lapuschkin} and K. {Müller}},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Evaluating the Visualization of What a Deep Neural Network Has Learned}, 
  year={2017},
  volume={28},
  number={11},
  pages={2660-2673},}
  
@misc{sundararajan2017axiomatic,
    title={Axiomatic Attribution for Deep Networks},
    author={Mukund Sundararajan and Ankur Taly and Qiqi Yan},
    year={2017},
    eprint={1703.01365},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@ARTICLE{bach-plos15,
    author = {Bach, Sebastian AND Binder, Alexander AND Montavon, Gr{\'e}goire AND Klauschen, Frederick AND M{\"u}ller, Klaus-Robert AND Samek, Wojciech},
    journal = {PLoS ONE},
    publisher = {Public Library of Science},
    title = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
    year = {2015},
    month = {07},
    volume = {10},
    url = {http://dx.doi.org/10.1371%2Fjournal.pone.0130140},
    pages = {e0130140},
    number = {7},
    doi = {10.1371/journal.pone.0130140}
}

@article{SelvarajuDVCPB16,
  author    = {Ramprasaath R. Selvaraju and
               Abhishek Das and
               Ramakrishna Vedantam and
               Michael Cogswell and
               Devi Parikh and
               Dhruv Batra},
  title     = {Grad-CAM: Why did you say that? Visual Explanations from Deep Networks
               via Gradient-based Localization},
  journal   = {CoRR},
  volume    = {abs/1610.02391},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.02391},
  archivePrefix = {arXiv},
  eprint    = {1610.02391},
  timestamp = {Mon, 13 Aug 2018 16:46:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SelvarajuDVCPB16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@online{EUdataregulations2018,
    title = {2018 reform of EU data protection rules},
    url = {https://ec.europa.eu/commission/sites/beta-political/files/data-protection-factsheet-changes_en.pdf},
    organization = {European Commission},
    date = {2018-05-25},
    urldate = {2019-06-17},
    year={2018}
}

@article{mnih2015humanlevel,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2015-08-26T14:46:40.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}

@book{SuttonBartoBook,
author = {Sutton, Richard S. and Barto, Andrew G.},
title = {Reinforcement Learning: An Introduction},
year = {2018},
isbn = {0262039249},
publisher = {A Bradford Book},
address = {Cambridge, MA, USA}
}

@article{Langley2019,
abstract = {In this paper, we pose a new challenge for AI researchers – to develop intelligent systems that support justified agency. We illustrate this ability with examples and relate it to two more basic topics that are receiving increased attention – agents that explain their decisions and ones that follow societal norms. In each case, we describe the target abilities, consider design alternatives, note some open questions, and review prior research. After this, we return to justified agency, offering a hypothesis about its relation to explanatory and normative behavior. We conclude by proposing testbeds and experiments to evaluate this empirical claim and encouraging other researchers to contribute to this crucial area.},
author = {Langley, Pat},
doi = {10.1609/aaai.v33i01.33019775},
file = {:home/hmhyau/Documents/5049-Article Text-8112-1-10-20190709.pdf:pdf},
issn = {2159-5399},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
keywords = {"Explainable agents, Normative Agents, Justified a},
pages = {9775--9779},
title = {{Explainable, Normative, and Justified Agency}},
volume = {33},
year = {2019}
}

@article{Miller2019,
title = "Explanation in artificial intelligence: Insights from the social sciences",
journal = "Artificial Intelligence",
volume = "267",
pages = "1 - 38",
year = "2019",
issn = "0004-3702",
doi = "https://doi.org/10.1016/j.artint.2018.07.007",
url = "http://www.sciencedirect.com/science/article/pii/S0004370218305988",
author = "Tim Miller",
keywords = "Explanation, Explainability, Interpretability, Explainable AI, Transparency",
abstract = "There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence."
}

@inproceedings{Verma18,
  author    = {Abhinav Verma and
               Vijayaraghavan Murali and
               Rishabh Singh and
               Pushmeet Kohli and
               Swarat Chaudhuri},
  editor    = {Jennifer G. Dy and
               Andreas Krause},
  title     = {Programmatically Interpretable Reinforcement Learning},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
               10-15, 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {5052--5061},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v80/verma18a.html},
  timestamp = {Wed, 03 Apr 2019 18:17:30 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/VermaMSKC18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{friedman2008,
author = "Friedman, Jerome H. and Popescu, Bogdan E.",
doi = "10.1214/07-AOAS148",
fjournal = "Annals of Applied Statistics",
journal = "Ann. Appl. Stat.",
month = "09",
number = "3",
pages = "916--954",
publisher = "The Institute of Mathematical Statistics",
title = "Predictive learning via rule ensembles",
url = "https://doi.org/10.1214/07-AOAS148",
volume = "2",
year = "2008"
}

@article{Quinlan1986decisiontree,
author = {Quinlan, J. R.},
title = {Induction of Decision Trees},
year = {1986},
issue_date = {March 1986},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {1},
number = {1},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1022643204877},
doi = {10.1023/A:1022643204877},
journal = {Mach. Learn.},
month = mar,
pages = {81–106},
numpages = {26},
keywords = {classification, decision trees, information theory, induction, knowledge acquisition, expert systems}
}

@article{Hastie1986,
author = "Hastie, Trevor and Tibshirani, Robert",
doi = "10.1214/ss/1177013604",
fjournal = "Statistical Science",
journal = "Statist. Sci.",
month = "08",
number = "3",
pages = "297--310",
publisher = "The Institute of Mathematical Statistics",
title = "Generalized Additive Models",
url = "https://doi.org/10.1214/ss/1177013604",
volume = "1",
year = "1986"
}

@article{Gilpin2018,
  author    = {Leilani H. Gilpin and
               David Bau and
               Ben Z. Yuan and
               Ayesha Bajwa and
               Michael Specter and
               Lalana Kagal},
  title     = {Explaining Explanations: An Approach to Evaluating Interpretability
               of Machine Learning},
  journal   = {CoRR},
  volume    = {abs/1806.00069},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.00069},
  archivePrefix = {arXiv},
  eprint    = {1806.00069},
  timestamp = {Mon, 13 Aug 2018 16:48:49 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-00069.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{lipton2016mythos,
    title={The Mythos of Model Interpretability},
    author={Zachary C. Lipton},
    year={2016},
    eprint={1606.03490},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{OpenAIGym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}

@inbook{Barto1990Cartpole,
author = {Barto, Andrew G. and Sutton, Richard S. and Anderson, Charles W.},
title = {Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems},
year = {1990},
isbn = {0818620153},
publisher = {IEEE Press},
booktitle = {Artificial Neural Networks: Concept Learning},
pages = {81–93},
numpages = {13}
}

@misc{kingma2014adam,
    title={Adam: A Method for Stochastic Optimization},
    author={Diederik P. Kingma and Jimmy Ba},
    year={2014},
    eprint={1412.6980},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{Dietterich2000Taxi,
author = {Dietterich, Thomas G.},
title = {Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition},
year = {2000},
issue_date = {August 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {13},
number = {1},
issn = {1076-9757},
journal = {J. Artif. Int. Res.},
month = nov,
pages = {227–303},
numpages = {77}
}
@inproceedings{hasselt2010double,
  title={Double Q-learning},
  author={Hasselt, Hado V},
  booktitle={Advances in neural information processing systems},
  pages={2613--2621},
  year={2010}
}
@article{dayan1993improving,
  title={Improving generalization for temporal difference learning: The successor representation},
  author={Dayan, Peter},
  journal={Neural Computation},
  volume={5},
  number={4},
  pages={613--624},
  year={1993},
  publisher={MIT Press}
}

@misc{andrychowicz2018hindsight,
      title={Hindsight Experience Replay}, 
      author={Marcin Andrychowicz and Filip Wolski and Alex Ray and Jonas Schneider and Rachel Fong and Peter Welinder and Bob McGrew and Josh Tobin and Pieter Abbeel and Wojciech Zaremba},
      year={2018},
      eprint={1707.01495},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{alvarezmelis2018robust,
      title={Towards Robust Interpretability with Self-Explaining Neural Networks}, 
      author={David Alvarez-Melis and Tommi S. Jaakkola},
      year={2018},
      eprint={1806.07538},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Teso2019TowardFE,
  title={Toward Faithful Explanatory Active Learning with Self-explainable Neural Nets},
  author={Stefano Teso},
  year={2019}
}

@misc{alshedivat2020contextual,
      title={Contextual Explanation Networks}, 
      author={Maruan Al-Shedivat and Avinava Dubey and Eric P. Xing},
      year={2020},
      eprint={1705.10301},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{langley2019explainable,
  title={Explainable, normative, and justified agency},
  author={Langley, Pat},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={9775--9779},
  year={2019}
}
  
@article{rudin2019stop,
  title={Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
  author={Rudin, Cynthia},
  journal={Nature Machine Intelligence},
  volume={1},
  number={5},
  pages={206--215},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{de2019block,
  title={Block neural autoregressive flow},
  author={De Cao, Nicola and Titov, Ivan and Aziz, Wilker},
  journal={arXiv preprint arXiv:1904.04676},
  year={2019}
}
