\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Balasubramanian \& Ghadimi(2021)Balasubramanian and
  Ghadimi]{balasubramanian2021zeroth}
Balasubramanian, K. and Ghadimi, S.
\newblock Zeroth-order nonconvex stochastic optimization: Handling constraints,
  high dimensionality, and saddle points.
\newblock \emph{Foundations of Computational Mathematics}, pp.\  1--42, 2021.

\bibitem[Besbes et~al.(2015)Besbes, Gur, and Zeevi]{OR'15:dynamic-function-VT}
Besbes, O., Gur, Y., and Zeevi, A.~J.
\newblock Non-stationary stochastic optimization.
\newblock \emph{Operations Research}, 63\penalty0 (5):\penalty0 1227--1244,
  2015.

\bibitem[Cardoso et~al.(2019)Cardoso, Abernethy, Wang, and
  Xu]{ICML'19:drift-game}
Cardoso, A.~R., Abernethy, J.~D., Wang, H., and Xu, H.
\newblock Competing against {N}ash equilibria in adversarially changing
  zero-sum games.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, pp.\  921--930, 2019.

\bibitem[Cesa-bianchi et~al.(2012)Cesa-bianchi, Gaillard, Lugosi, and
  Stoltz]{cesa2012mirror}
Cesa-bianchi, N., Gaillard, P., Lugosi, G., and Stoltz, G.
\newblock Mirror descent meets fixed share (and feels no regret).
\newblock \emph{Advances in Neural Information Processing Systems},
  25:\penalty0 980--988, 2012.

\bibitem[Cesa{-}Bianchi et~al.(2012)Cesa{-}Bianchi, Gaillard, Lugosi, and
  Stoltz]{conf/nips/Cesa-BianchiGLS12}
Cesa{-}Bianchi, N., Gaillard, P., Lugosi, G., and Stoltz, G.
\newblock Mirror descent meets fixed share (and feels no regret).
\newblock In \emph{Advances in Neural Information Processing Systems 25
  (NIPS)}, pp.\  989--997, 2012.

\bibitem[Chen \& Teboulle(1993)Chen and Teboulle]{OPT'93:Bregman}
Chen, G. and Teboulle, M.
\newblock Convergence analysis of a proximal-like minimization algorithm using
  bregman functions.
\newblock \emph{SIAM Journal on Optimization}, 3\penalty0 (3):\penalty0
  538--543, 1993.

\bibitem[Chen et~al.(2021)Chen, Luo, and Wei]{COLT'21:impossible-tuning}
Chen, L., Luo, H., and Wei, C.
\newblock Impossible tuning made possible: {A} new expert algorithm and its
  applications.
\newblock In \emph{Proceedings of the 34th Conference on Learning Theory
  (COLT)}, pp.\  1216--1259, 2021.

\bibitem[Chen \& Peng(2020)Chen and Peng]{NIPS'20:faster-hedge}
Chen, X. and Peng, B.
\newblock Hedging in games: Faster convergence of external and swap regrets.
\newblock In \emph{Advances in Neural Information Processing Systems 33
  (NeurIPS)}, pp.\  18990--18999, 2020.

\bibitem[Chiang et~al.(2012)Chiang, Yang, Lee, Mahdavi, Lu, Jin, and
  Zhu]{COLT'12:variation-Yang}
Chiang, C.-K., Yang, T., Lee, C.-J., Mahdavi, M., Lu, C.-J., Jin, R., and Zhu,
  S.
\newblock Online optimization with gradual variations.
\newblock In \emph{Proceedings of the 25th Conference On Learning Theory
  (COLT)}, pp.\  6.1--6.20, 2012.

\bibitem[Daskalakis \& Panageas(2019)Daskalakis and
  Panageas]{daskalakis2019last}
Daskalakis, C. and Panageas, I.
\newblock Last-iterate convergence: Zero-sum games and constrained min-max
  optimization.
\newblock In \emph{Proceedings of the 10th Innovations in Theoretical Computer
  Science (ITCS) Conference}, 2019.

\bibitem[Daskalakis et~al.(2015)Daskalakis, Deckelbaum, and
  Kim]{SODA'11:fast-games}
Daskalakis, C., Deckelbaum, A., and Kim, A.
\newblock Near-optimal no-regret algorithms for zero-sum games.
\newblock \emph{Games and Economic Behavior}, 92:\penalty0 327--348, 2015.

\bibitem[Daskalakis et~al.(2021)Daskalakis, Fishelson, and
  Golowich]{NIPS'21:general-game}
Daskalakis, C., Fishelson, M., and Golowich, N.
\newblock Near-optimal no-regret learning in general games.
\newblock In \emph{Advances in Neural Information Processing Systems 34
  (NeurIPS)}, pp.\  to appear, 2021.

\bibitem[Duvocelle et~al.(2021)Duvocelle, Mertikopoulos, Staudigl, and
  Vermeulen]{OR'21:time-varying-games}
Duvocelle, B., Mertikopoulos, P., Staudigl, M., and Vermeulen, D.
\newblock Multi-agent online learning in time-varying games.
\newblock \emph{Mathematics of Operations Research}, to appear, 2021.

\bibitem[Fiez et~al.(2021)Fiez, Sim, Skoulakis, Piliouras, and
  Ratliff]{NIPS'21:periodic-game}
Fiez, T., Sim, R., Skoulakis, S., Piliouras, G., and Ratliff, L.~J.
\newblock Online learning in periodic zero-sum games.
\newblock In \emph{Advances in Neural Information Processing Systems 34
  (NeurIPS)}, pp.\  to appear, 2021.

\bibitem[Freund \& Schapire(1999)Freund and
  Schapire]{1999:Schapire-game-theory}
Freund, Y. and Schapire, R.~E.
\newblock Adaptive game playing using multiplicative weights.
\newblock \emph{Games and Economic Behavior}, 29\penalty0 (1-2):\penalty0
  79--103, 1999.

\bibitem[Herbster \& Warmuth(1998)Herbster and Warmuth]{herbster1998tracking}
Herbster, M. and Warmuth, M.~K.
\newblock Tracking the best expert.
\newblock \emph{Machine learning}, 32\penalty0 (2):\penalty0 151--178, 1998.

\bibitem[Hsieh et~al.(2021)Hsieh, Antonakopoulos, and
  Mertikopoulos]{COLT'21:Hsieh}
Hsieh, Y.-G., Antonakopoulos, K., and Mertikopoulos, P.
\newblock Adaptive learning in continuous games: Optimal regret bounds and
  convergence to {N}ash equilibrium.
\newblock In \emph{Proceedings of the 34th Conference on Learning Theory
  (COLT)}, pp.\  2388--2422, 2021.

\bibitem[Luo \& Schapire(2015)Luo and Schapire]{COLT'15:Luo-AdaNormalHedge}
Luo, H. and Schapire, R.~E.
\newblock Achieving all with no parameters: {AdaNormalHedge}.
\newblock In \emph{Proceedings of the 28th Annual Conference Computational
  Learning Theory (COLT)}, pp.\  1286--1304, 2015.

\bibitem[Pogodin \& Lattimore(2019)Pogodin and Lattimore]{UAI'19:FIRST-ORDER}
Pogodin, R. and Lattimore, T.
\newblock On first-order bounds, variance and gap-dependent bounds for
  adversarial bandits.
\newblock In \emph{Proceedings of the 35th Conference on Uncertainty in
  Artificial Intelligence (UAI)}, pp.\  894--904, 2019.

\bibitem[Rakhlin \& Sridharan(2013)Rakhlin and Sridharan]{conf/nips/RakhlinS13}
Rakhlin, A. and Sridharan, K.
\newblock Optimization, learning, and games with predictable sequences.
\newblock In \emph{Advances in Neural Information Processing Systems 26
  (NIPS)}, pp.\  3066--3074, 2013.

\bibitem[Roy et~al.(2019)Roy, Chen, Balasubramanian, and
  Mohapatra]{Arxiv'19:online-saddle-point}
Roy, A., Chen, Y., Balasubramanian, K., and Mohapatra, P.
\newblock Online and bandit algorithms for nonstationary stochastic
  saddle-point optimization.
\newblock \emph{arXiv preprint arXiv:1912.01698}, 2019.

\bibitem[Syrgkanis et~al.(2015)Syrgkanis, Agarwal, Luo, and
  Schapire]{NIPS'15FastConv}
Syrgkanis, V., Agarwal, A., Luo, H., and Schapire, R.~E.
\newblock Fast convergence of regularized learning in games.
\newblock In \emph{Advances in Neural Information Processing Systems 28
  (NIPS)}, pp.\  2989--2997, 2015.

\bibitem[von Neumann(1928)]{1928:von-Neumann}
von Neumann, J.
\newblock Zur theorie der gesellschaftsspiele.
\newblock \emph{Mathematische Annalen}, 100:\penalty0 295--320, 1928.

\bibitem[Wei et~al.(2016)Wei, Hong, and Lu]{NIPS'16:Wei-non-stationary-expert}
Wei, C.-Y., Hong, Y.-T., and Lu, C.-J.
\newblock Tracking the best expert in non-stationary stochastic environments.
\newblock In \emph{Advances in Neural Information Processing Systems 29
  (NIPS)}, pp.\  3972--3980, 2016.

\bibitem[Wei et~al.(2021)Wei, Lee, Zhang, and Luo]{ICLR'21-last-iteration}
Wei, C.-Y., Lee, C.-W., Zhang, M., and Luo, H.
\newblock Linear last-iterate convergence in constrained saddle-point
  optimization.
\newblock In \emph{Proceedings of the 9th International Conference on Learning
  Representations (ICLR)}, 2021.

\bibitem[Yang et~al.(2016)Yang, Zhang, Jin, and Yi]{ICML'16:Yang-smooth}
Yang, T., Zhang, L., Jin, R., and Yi, J.
\newblock Tracking slowly moving clairvoyant: Optimal dynamic regret of online
  learning with true and noisy gradient.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning (ICML)}, pp.\  449--457, 2016.

\bibitem[Zhang et~al.(2018)Zhang, Lu, and Zhou]{NIPS'18:Zhang-Ader}
Zhang, L., Lu, S., and Zhou, Z.-H.
\newblock Adaptive online learning in dynamic environments.
\newblock In \emph{Advances in Neural Information Processing Systems 31
  (NeurIPS)}, pp.\  1330--1340, 2018.

\bibitem[Zhang et~al.(2020)Zhang, Zhao, and Zhou]{UAI'20:simple}
Zhang, Y.-J., Zhao, P., and Zhou, Z.-H.
\newblock A simple online algorithm for competing with dynamic comparators.
\newblock In \emph{Proceedings of the 36th Conference on Uncertainty in
  Artificial Intelligence (UAI)}, pp.\  390--399, 2020.

\bibitem[Zhao \& Zhang(2021)Zhao and Zhang]{L4DC'21:sc_smooth}
Zhao, P. and Zhang, L.
\newblock Improved analysis for dynamic regret of strongly convex and smooth
  functions.
\newblock In \emph{Proceedings of the 3rd Conference on Learning for Dynamics
  and Control (L4DC)}, pp.\  48--59, 2021.

\bibitem[Zhao et~al.(2020)Zhao, Zhang, Zhang, and Zhou]{NIPS'20:sword}
Zhao, P., Zhang, Y.-J., Zhang, L., and Zhou, Z.-H.
\newblock Dynamic regret of convex and smooth functions.
\newblock In \emph{Advances in Neural Information Processing Systems 33
  (NeurIPS)}, pp.\  12510--12520, 2020.

\bibitem[Zhao et~al.(2021)Zhao, Zhang, Zhang, and Zhou]{JMLR:sword++}
Zhao, P., Zhang, Y.-J., Zhang, L., and Zhou, Z.-H.
\newblock Adaptivity and non-stationarity: Problem-dependent dynamic regret for
  online convex optimization.
\newblock \emph{ArXiv preprint}, arXiv:2112.14368, 2021.

\bibitem[Zinkevich(2003)]{ICML'03:zinkvich}
Zinkevich, M.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{Proceedings of the 20th International Conference on Machine
  Learning (ICML)}, pp.\  928--936, 2003.

\end{thebibliography}
