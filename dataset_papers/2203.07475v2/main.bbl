\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ab~Azar et~al.(2020)Ab~Azar, Shahmansoorian, and Davoudi]{ab2020}
Ab~Azar, N., Shahmansoorian, A., and Davoudi, M.
\newblock From inverse optimal control to inverse reinforcement learning: A
  historical review.
\newblock \emph{Annual Reviews in Control}, 50:\penalty0 119--138, 2020.
\newblock \doi{10.1016/j.arcontrol.2020.06.001}.

\bibitem[Abbeel \& Ng(2004)Abbeel and Ng]{abbeel2004}
Abbeel, P. and Ng, A.~Y.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In \emph{Proceedings of the Twenty-First International Conference on
  Machine Learning}, pp.\  1--8, Banff, Alberta, Canada, July 2004. Association
  for Computing Machinery.
\newblock \doi{10.1145/1015330.1015430}.

\bibitem[Abbeel et~al.(2010)Abbeel, Coates, and Ng]{abbeel2010}
Abbeel, P., Coates, A., and Ng, A.~Y.
\newblock Autonomous helicopter aerobatics through apprenticeship learning.
\newblock \emph{The International Journal of Robotics Research}, 29\penalty0
  (13):\penalty0 1608--1639, 2010.
\newblock \doi{10.1177/0278364910371999}.

\bibitem[Aguirregabiria(2005)]{aguirregabiria2005}
Aguirregabiria, V.
\newblock Nonparametric identification of behavioral responses to
  counterfactual policy interventions in dynamic discrete decision processes.
\newblock \emph{Economics Letters}, 87\penalty0 (3):\penalty0 393--398, 2005.
\newblock \doi{10.1016/j.econlet.2004.12.014}.

\bibitem[Aguirregabiria \& Mira(2010)Aguirregabiria and
  Mira]{aguirregabiria2010}
Aguirregabiria, V. and Mira, P.
\newblock Dynamic discrete choice structural models: A survey.
\newblock \emph{Journal of Econometrics}, 156\penalty0 (1):\penalty0 38--67,
  2010.
\newblock \doi{10.1016/j.jeconom.2009.09.007}.

\bibitem[Aigner(1996)]{aigner1979}
Aigner, M.
\newblock \emph{Combinatorial Theory}.
\newblock Classics in Mathematics. Springer Berlin Heidelberg, reprint of the
  1979 edition, 1996.
\newblock ISBN 3540617876.

\bibitem[Akrour et~al.(2012)Akrour, Schoenauer, and Sebag]{akrour2012}
Akrour, R., Schoenauer, M., and Sebag, M.
\newblock {APRIL}: Active preference learning-based reinforcement learning.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases: ECML
  PKDD 2012, Proceedings, Part II}, volume 7524 of \emph{Lecture Notes in
  Computer Science}, pp.\  116--131, Bristol, UK, 2012. Springer.
\newblock \doi{10.1007/978-3-642-33486-3_8}.

\bibitem[Amin et~al.(2017)Amin, Jiang, and Singh]{amin2017}
Amin, K., Jiang, N., and Singh, S.~P.
\newblock Repeated inverse reinforcement learning.
\newblock In \emph{Proceedings of the 31st Conference on Neural Information
  Processing Systems}, volume~30, pp.\  1813--1822, Long Beach, California,
  USA, 2017. Curran Associates, Inc., Red Hook, NY, USA.

\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and
  Man{\'e}]{amodei2016}
Amodei, D., Olah, C., Steinhardt, J., Christiano, P.~F., Schulman, J., and
  Man{\'e}, D.
\newblock Concrete problems in {AI} safety.
\newblock \emph{arXiv preprint}, arXiv:1606.06565 [cs.AI], 2016.

\bibitem[Arcidiacono \& Miller(2020)Arcidiacono and Miller]{arcidiacono2020}
Arcidiacono, P. and Miller, R.~A.
\newblock Identifying dynamic discrete choice models off short panels.
\newblock \emph{Journal of Econometrics}, 215\penalty0 (2):\penalty0 473--485,
  2020.
\newblock \doi{10.1016/j.jeconom.2018.12.025}.

\bibitem[Armstrong \& Mindermann(2018)Armstrong and Mindermann]{armstrong2017}
Armstrong, S. and Mindermann, S.
\newblock Occam's razor is insufficient to infer the preferences of irrational
  agents.
\newblock In \emph{Proceedings of the 32nd Conference on Neural Information
  Processing Systems}, volume~31, pp.\  5603--5614, Montr\'{e}al, Canada, 2018.
  Curran Associates, Inc., Red Hook, NY, USA.

\bibitem[B{\i}y{\i}k et~al.(2022)B{\i}y{\i}k, Losey, Palan, Landolfi, Shevchuk,
  and Sadigh]{biyik2020}
B{\i}y{\i}k, E., Losey, D.~P., Palan, M., Landolfi, N.~C., Shevchuk, G., and
  Sadigh, D.
\newblock Learning reward functions from diverse sources of human feedback:
  Optimally integrating demonstrations and preferences.
\newblock \emph{The International Journal of Robotics Research}, 41\penalty0
  (1):\penalty0 45--67, 2022.
\newblock \doi{10.1177/02783649211041652}.

\bibitem[Cao et~al.(2021)Cao, Cohen, and Szpruch]{cao2021}
Cao, H., Cohen, S.~N., and Szpruch, L.
\newblock Identifiability in inverse reinforcement learning.
\newblock In \emph{Proceedings of the 35th Conference on Neural Information
  Processing Systems}, volume~34, pp.\  12362--12373, Virtual, 2021. Curran
  Associates, Inc., Red Hook, NY, USA.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017}
Christiano, P.~F., Leike, J., Brown, T.~B., Martic, M., Legg, S., and Amodei,
  D.
\newblock Deep reinforcement learning from human preferences.
\newblock In \emph{Proceedings of the 31st Conference on Neural Information
  Processing Systems}, volume~30, pp.\  4302–4310, Long Beach, California,
  USA, 2017. Curran Associates, Inc., Red Hook, NY, USA.

\bibitem[Dabney et~al.(2018)Dabney, Rowland, Bellemare, and Munos]{dabney2018}
Dabney, W., Rowland, M., Bellemare, M.~G., and Munos, R.
\newblock Distributional reinforcement learning with quantile regression.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume 32(1), pp.\  2892--2901, New Orleans, Lousiana, USA,
  April 2018. AAAI Press.

\bibitem[Dulac-Arnold et~al.(2019)Dulac-Arnold, Mankowitz, and
  Hester]{dulac2019}
Dulac-Arnold, G., Mankowitz, D., and Hester, T.
\newblock Challenges of real-world reinforcement learning.
\newblock In \emph{Reinforcement Learning for Real Life, International
  Conference on Machine Learning Workshop}, Long Beach, California, USA, June
  2019.
\newblock \doi{10.48550/arXiv.1904.12901}.

\bibitem[Dvijotham \& Todorov(2010)Dvijotham and Todorov]{dvijotham2010}
Dvijotham, K. and Todorov, E.
\newblock Inverse optimal control with linearly-solvable {MDPs}.
\newblock In \emph{Proceedings of the 27th International Conference on Machine
  Learning}, pp.\  335--342, Haifa, Israel, June 2010. Omnipress, Madison,
  Wisconsin, USA.

\bibitem[Fishburn(1970)]{fishburn1970}
Fishburn, P.~C.
\newblock \emph{Utility Theory for Decision Making}.
\newblock John Wiley \& Sons, Inc., 1970.
\newblock ISBN 0471260606.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and Levine]{haarnoja2017}
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, volume~70 of \emph{Proceedings of Machine Learning Research}, pp.\
   1352--1361, Sydney, Australia, August 2017. PMLR.

\bibitem[Ibarz et~al.(2018)Ibarz, Leike, Pohlen, Irving, Legg, and
  Amodei]{ibarz2018}
Ibarz, B., Leike, J., Pohlen, T., Irving, G., Legg, S., and Amodei, D.
\newblock Reward learning from human preferences and demonstrations in {A}tari.
\newblock In \emph{Proceedings of the 32nd Conference on Neural Information
  Processing Systems}, volume~31, pp.\  8022–8034, Montr\'{e}al, Canada,
  2018. Curran Associates, Inc., Red Hook, NY, USA.

\bibitem[Jeon et~al.(2020)Jeon, Milli, and Dragan]{jeon2020}
Jeon, H.~J., Milli, S., and Dragan, A.
\newblock Reward-rational (implicit) choice: A unifying formalism for reward
  learning.
\newblock In \emph{Proceedings of the 34th Conference on Neural Information
  Processing Systems}, volume~33, pp.\  4415--4426, Virtual, 2020. Curran
  Associates, Inc., Red Hook, NY, USA.

\bibitem[Kim et~al.(2021)Kim, Garg, Shiragur, and Ermon]{kim2021}
Kim, K., Garg, S., Shiragur, K., and Ermon, S.
\newblock Reward identification in inverse reinforcement learning.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, volume 139 of \emph{Proceedings of Machine Learning Research},
  pp.\  5496--5505, Virtual, July 2021. PMLR.

\bibitem[Koppol et~al.(2020)Koppol, Admoni, and Simmons]{koppol2020}
Koppol, P., Admoni, H., and Simmons, R.
\newblock Iterative interactive reward learning.
\newblock In \emph{Participatory Approaches to Machine Learning, International
  Conference on Machine Learning Workshop}, Virtual, July 2020.

\bibitem[Krasheninnikov et~al.(2019)Krasheninnikov, Shah, and van
  Hoof]{krasheninnikov2021combining}
Krasheninnikov, D., Shah, R., and van Hoof, H.
\newblock Combining reward information from multiple sources.
\newblock In \emph{Workshop on Learning with Rich Experience (LIRE), NeurIPS
  2019}, Vancouver, Canada, December 2019.
\newblock \textit{arXiv preprint,} arXiv:2103.12142 [cs.LG].

\bibitem[Leike et~al.(2018)Leike, Krueger, Everitt, Martic, Maini, and
  Legg]{leike2018}
Leike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., and Legg, S.
\newblock Scalable agent alignment via reward modeling: A research direction.
\newblock \emph{arXiv preprint}, arXiv:1811.07871 [cs.LG], 2018.

\bibitem[Lewbel(2019)]{lewbel2019}
Lewbel, A.
\newblock The identification zoo: Meanings of identification in econometrics.
\newblock \emph{Journal of Economic Literature}, 57\penalty0 (4):\penalty0
  835--903, 2019.
\newblock \doi{10.1257/jel.20181361}.

\bibitem[Manski(1995)]{manski1995}
Manski, C.~F.
\newblock \emph{Identification Problems in the Social Sciences}.
\newblock Harvard University Press, 1995.
\newblock ISBN 9780674442849.

\bibitem[Manski(2003)]{manski2003}
Manski, C.~F.
\newblock \emph{Partial Identification of Probability Distributions}.
\newblock Springer, 2003.
\newblock ISBN 0387004548.

\bibitem[Morimura et~al.(2010{\natexlab{a}})Morimura, Sugiyama, Kashima,
  Hachiya, and Tanaka]{morimura2010nonpara}
Morimura, T., Sugiyama, M., Kashima, H., Hachiya, H., and Tanaka, T.
\newblock Nonparametric return distribution approximation for reinforcement
  learning.
\newblock In \emph{Proceedings of the 27th International Conference on Machine
  Learning}, pp.\  799--806, Haifa, Israel, June 2010{\natexlab{a}}. Omnipress,
  Madison, Wisconsin, USA.

\bibitem[Morimura et~al.(2010{\natexlab{b}})Morimura, Sugiyama, Kashima,
  Hachiya, and Tanaka]{morimura2010para}
Morimura, T., Sugiyama, M., Kashima, H., Hachiya, H., and Tanaka, T.
\newblock Parametric return density estimation for reinforcement learning.
\newblock In \emph{Proceedings of the Twenty-Sixth Conference on Uncertainty in
  Artificial Intelligence}, pp.\  368--375, Catalina Island, California, USA,
  2010{\natexlab{b}}. AUAI Press, Arlington, Virginia, USA.

\bibitem[Ng \& Russell(2000)Ng and Russell]{ng2000}
Ng, A.~Y. and Russell, S.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{Proceedings of the Seventeenth International Conference on
  Machine Learning}, volume~1, pp.\  663--670, Stanford, California, USA, 2000.
  Morgan Kaufmann Publishers Inc., San Francisco, California, USA.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{ng1999}
Ng, A.~Y., Harada, D., and Russell, S.
\newblock Policy invariance under reward transformations: Theory and
  application to reward shaping.
\newblock In \emph{Proceedings of the Sixteenth International Conference on
  Machine Learning}, pp.\  278--287, Bled, Slovenia, 1999. Morgan Kaufmann
  Publishers Inc., San Francisco, CA, USA.

\bibitem[Orsini et~al.(2021)Orsini, Raichuk, Hussenot, Vincent, Dadashi,
  Girgin, Geist, Bachem, Pietquin, and Andrychowicz]{orsini2021}
Orsini, M., Raichuk, A., Hussenot, L., Vincent, D., Dadashi, R., Girgin, S.,
  Geist, M., Bachem, O., Pietquin, O., and Andrychowicz, M.
\newblock What matters for adversarial imitation learning?
\newblock In \emph{Proceedings of the 35th Conference on Neural Information
  Processing Systems}, volume~34, pp.\  14656--14668, Virtual, 2021. Curran
  Associates, Inc., Red Hook, NY, USA.

\bibitem[Palan et~al.(2019)Palan, Landolfi, Shevchuk, and Sadigh]{palan2019}
Palan, M., Landolfi, N.~C., Shevchuk, G., and Sadigh, D.
\newblock Learning reward functions by integrating human demonstrations and
  preferences.
\newblock In \emph{Proceedings of Robotics: Science and Systems}, Freiburg im
  Breisgau, Germany, June 2019.
\newblock \doi{10.15607/RSS.2019.XV.023}.

\bibitem[Ramachandran \& Amir(2007)Ramachandran and Amir]{ramachandran2007}
Ramachandran, D. and Amir, E.
\newblock Bayesian inverse reinforcement learning.
\newblock In \emph{Proceedings of the 20th International Joint Conference on
  Artifical Intelligence}, pp.\  2586--2591, Hyderabad, India, 2007. Morgan
  Kaufmann Publishers Inc., San Francisco, California, USA.

\bibitem[Rothkopf \& Dimitrakakis(2011)Rothkopf and Dimitrakakis]{rothkopf2011}
Rothkopf, C.~A. and Dimitrakakis, C.
\newblock Preference elicitation and inverse reinforcement learning.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases: ECML
  PKDD 2011, Proceedings, Part III}, volume 6913 of \emph{Lecture Notes in
  Computer Science}, pp.\  34--48, Athens, Greece, 2011. Springer.
\newblock \doi{10.1007/978-3-642-23808-6_3}.

\bibitem[Russell(1998)]{russell1998}
Russell, S.
\newblock Learning agents for uncertain environments (extended abstract).
\newblock In \emph{Proceedings of the Eleventh Annual Conference on
  Computational Learning Theory}, pp.\  101--103, Madison, Wisconsin, USA,
  1998. Association for Computing Machinery.
\newblock \doi{10.1145/279943.279964}.

\bibitem[Russell \& Norvig(2009)Russell and Norvig]{aima3e}
Russell, S. and Norvig, P.
\newblock \emph{Artificial Intelligence: A Modern Approach}.
\newblock Prentice Hall, Upper Saddle River, NJ, third edition, 2009.
\newblock ISBN 9780136042594.

\bibitem[Rust(1994)]{rust1994}
Rust, J.
\newblock Structural estimation of {Markov} decision processes.
\newblock In Engle, R.~F. and McFadden, D.~L. (eds.), \emph{Handbook of
  Econometrics}, volume~4, pp.\  3081--3143. Elsevier, 1994.
\newblock \doi{10.1016/S1573-4412(05)80020-0}.

\bibitem[Shah et~al.(2019)Shah, Gundotra, Abbeel, and Dragan]{shah2019}
Shah, R., Gundotra, N., Abbeel, P., and Dragan, A.
\newblock On the feasibility of learning, rather than assuming, human biases
  for reward inference.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\
   5670--5679, Long Beach, California, USA, June 2019. PMLR.

\bibitem[Singh et~al.(2019)Singh, Yang, Hartikainen, Finn, and
  Levine]{singh2019}
Singh, A., Yang, L., Hartikainen, K., Finn, C., and Levine, S.
\newblock End-to-end robotic reinforcement learning without reward engineering.
\newblock In \emph{Proceedings of Robotics: Science and Systems}, Freiburg im
  Breisgau, Germany, June 2019.
\newblock \doi{10.15607/RSS.2019.XV.073}.

\bibitem[Srisuma(2015)]{srisuma2015}
Srisuma, S.
\newblock Identification in discrete {M}arkov decision models.
\newblock \emph{Econometric Theory}, 31\penalty0 (3):\penalty0 521--538, 2015.
\newblock \doi{10.1017/S0266466614000437}.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss,
  Radford, Amodei, and Christiano]{stiennon2020}
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D.~M., Lowe, R., Voss, C., Radford,
  A., Amodei, D., and Christiano, P.~F.
\newblock Learning to summarize from human feedback.
\newblock In \emph{Proceedings of the 34th Conference on Neural Information
  Processing Systems}, volume~33, pp.\  3008--3021, Virtual, 2020. Curran
  Associates, Inc., Red Hook, NY, USA.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock MIT Press, second edition, 2018.
\newblock ISBN 9780262352703.

\bibitem[Tamer(2010)]{tamer2010}
Tamer, E.
\newblock Partial identification in econometrics.
\newblock \emph{Annual Review of Economics}, 2:\penalty0 167--195, 2010.
\newblock \doi{10.1146/annurev.economics.050708.143401}.

\bibitem[Tung et~al.(2018)Tung, Harley, Huang, and Fragkiadaki]{tung2018}
Tung, H.-Y., Harley, A.~W., Huang, L.-K., and Fragkiadaki, K.
\newblock Reward learning from narrated demonstrations.
\newblock In \emph{Proceedings: 2018 IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pp.\  7004--7013, Salt Lake City, Utah, USA,
  June 2018. IEEE Computer Society, Los Alamitos, CA, USA.
\newblock \doi{10.1109/CVPR.2018.00732}.

\bibitem[von Neumann \& Morgenstern(1947)von Neumann and Morgenstern]{vnm2}
von Neumann, J. and Morgenstern, O.
\newblock \emph{Theory of Games and Economic Behavior}.
\newblock Princeton University Press, second revised edition, 1947.

\bibitem[Wiewiora et~al.(2003)Wiewiora, Cottrell, and Elkan]{wiewiora2003}
Wiewiora, E., Cottrell, G.~W., and Elkan, C.
\newblock Principled methods for advising reinforcement learning agents.
\newblock In \emph{Proceedings of the Twentieth International Conference on
  Machine Learning}, pp.\  792--799, Washington, D.C., USA, August 2003. AAAI
  Press.

\bibitem[Wirth et~al.(2017)Wirth, Akrour, Neumann, and
  F{\"u}rnkranz]{wirth2017}
Wirth, C., Akrour, R., Neumann, G., and F{\"u}rnkranz, J.
\newblock A survey of preference-based reinforcement learning methods.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 4945--4990, January 2017.

\bibitem[Ziebart(2010)]{ziebart2010thesis}
Ziebart, B.~D.
\newblock \emph{Modeling Purposeful Adaptive Behavior with the Principle of
  Maximum Causal Entropy}.
\newblock PhD thesis, Carnegie Mellon University, 2010.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and Dey]{ziebart2008}
Ziebart, B.~D., Maas, A.~L., Bagnell, J.~A., and Dey, A.~K.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{23rd AAAI Conference on Artificial Intelligence}, volume~8,
  pp.\  1433--1438, 2008.

\bibitem[Ziebart et~al.(2010)Ziebart, Bagnell, and Dey]{ziebart2010paper}
Ziebart, B.~D., Bagnell, J.~A., and Dey, A.~K.
\newblock Modeling interaction via the principle of maximum causal entropy.
\newblock In \emph{Proceedings of the 27th International Conference on Machine
  Learning}, pp.\  1255--1262, Haifa, Israel, June 2010. Omnipress, Madison,
  Wisconsin, USA.

\end{thebibliography}
