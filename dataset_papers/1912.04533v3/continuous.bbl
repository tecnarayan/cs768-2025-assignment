\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alaoui and Mahoney(2015)]{ridge-leverage-scores}
Ahmed~El Alaoui and Michael~W. Mahoney.
\newblock Fast randomized kernel ridge regression with statistical guarantees.
\newblock In \emph{Proceedings of the 28th International Conference on Neural
  Information Processing Systems}, pages 775--783, Montreal, Canada, December
  2015.

\bibitem[Arora et~al.(2019)Arora, Cohen, Hu, and Luo]{ACHL19}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d~Alch\'{e}-Buc,
  E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural Information
  Processing Systems 32}, pages 7411--7422. Curran Associates, Inc., 2019.

\bibitem[Bai et~al.(1993)Bai, Yin, et~al.]{bai1993limit}
ZD~Bai, YQ~Yin, et~al.
\newblock Limit of the smallest eigenvalue of a large dimensional sample
  covariance matrix.
\newblock \emph{The Annals of Probability}, 21\penalty0 (3):\penalty0
  1275--1294, 1993.

\bibitem[Bai et~al.(1998)Bai, Silverstein, et~al.]{bai1998no}
Zhi-Dong Bai, Jack~W Silverstein, et~al.
\newblock No eigenvalues outside the support of the limiting spectral
  distribution of large-dimensional sample covariance matrices.
\newblock \emph{The Annals of Probability}, 26\penalty0 (1):\penalty0 316--345,
  1998.

\bibitem[Bartlett et~al.(2019)Bartlett, Long, Lugosi, and Tsigler]{BLLT19_TR}
P.~L. Bartlett, P.~M. Long, G.~Lugosi, and A.~Tsigler.
\newblock Benign overfitting in linear regression.
\newblock Technical Report Preprint: arXiv:1906.11300, 2019.

\bibitem[Belkin et~al.(2018{\natexlab{a}})Belkin, Ma, and Mandal]{BMM18_TR}
M.~Belkin, S.~Ma, and S.~Mandal.
\newblock To understand deep learning we need to understand kernel learning.
\newblock In \emph{Proceedings of the 35st International Conference on Machine
  Learning}, volume~80 of \emph{Proceedings of Machine Learning Research},
  Stockholm, Sweden, 2018{\natexlab{a}}. PMLR.

\bibitem[Belkin et~al.(2019{\natexlab{a}})Belkin, Hsu, Ma, and Mandal]{BHMM19}
M.~Belkin, D.~Hsu, S.~Ma, and S.~Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  biasâ€“variance trade-off.
\newblock \emph{Proc. Natl. Acad. Sci. USA}, 116:\penalty0 15849--15854,
  2019{\natexlab{a}}.

\bibitem[Belkin et~al.(2019{\natexlab{b}})Belkin, Rakhlin, and
  Tsybakov]{BRT18_TR}
M.~Belkin, A.~Rakhlin, and A.~B. Tsybakov.
\newblock Does data interpolation contradict statistical optimality?
\newblock In \emph{Proceedings of the 22nd International Conference on
  Artificial Intelligence and Statistics}, volume~89 of \emph{Proceedings of
  Machine Learning Research}, Naha, Okinawa, Japan, 2019{\natexlab{b}}. PMLR.

\bibitem[Belkin et~al.(2018{\natexlab{b}})Belkin, Hsu, and Mitra]{BHM18_TR}
Mikhail Belkin, Daniel~J Hsu, and Partha Mitra.
\newblock Overfitting or perfect fitting? {R}isk bounds for classification and
  regression rules that interpolate.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems 31}, pages 2300--2311. Curran Associates, Inc., 2018{\natexlab{b}}.

\bibitem[Belkin et~al.(2019{\natexlab{c}})Belkin, Hsu, and Xu]{belkin2019two}
Mikhail Belkin, Daniel Hsu, and Ji~Xu.
\newblock Two models of double descent for weak features.
\newblock \emph{arXiv preprint arXiv:1903.07571}, 2019{\natexlab{c}}.

\bibitem[Bernstein(2011)]{matrix-mathematics}
Dennis~S. Bernstein.
\newblock \emph{Matrix Mathematics: Theory, Facts, and Formulas}.
\newblock Princeton University Press, second edition, 2011.

\bibitem[Chikuse(1990)]{chikuse1990matrix}
Yasuko Chikuse.
\newblock The matrix angular central gaussian distribution.
\newblock \emph{Journal of Multivariate Analysis}, 33\penalty0 (2):\penalty0
  265--274, 1990.

\bibitem[Chikuse(1991)]{CHIKUSE1991145}
Yasuko Chikuse.
\newblock High dimensional limit theorems and matrix decompositions on the
  stiefel manifold.
\newblock \emph{Journal of Multivariate Analysis}, 36\penalty0 (2):\penalty0
  145 -- 162, 1991.

\bibitem[Chikuse(1998)]{CHIKUSE1998188}
Yasuko Chikuse.
\newblock Density estimation on the stiefel manifold.
\newblock \emph{Journal of Multivariate Analysis}, 66\penalty0 (2):\penalty0
  188 -- 206, 1998.

\bibitem[Cook and Forzani(2011)]{cook2011}
R.~Dennis Cook and Liliana Forzani.
\newblock On the mean and variance of the generalized inverse of a singular
  wishart matrix.
\newblock \emph{Electron. J. Statist.}, 5:\penalty0 146--158, 2011.

\bibitem[Derezi{\'n}ski(2019)]{dpp-intermediate}
Micha{\l} Derezi{\'n}ski.
\newblock Fast determinantal point processes via distortion-free intermediate
  sampling.
\newblock In Alina Beygelzimer and Daniel Hsu, editors, \emph{Proceedings of
  the Thirty-Second Conference on Learning Theory}, volume~99 of
  \emph{Proceedings of Machine Learning Research}, pages 1029--1049, Phoenix,
  USA, 25--28 Jun 2019.

\bibitem[Derezi\'{n}ski and Mahoney(2019)]{determinantal-averaging}
Micha{\l} Derezi\'{n}ski and Michael~W Mahoney.
\newblock Distributed estimation of the inverse {H}essian by determinantal
  averaging.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d~Alch\'{e}-Buc,
  E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural Information
  Processing Systems 32}, pages 11401--11411. Curran Associates, Inc., 2019.

\bibitem[Derezi\'{n}ski and Warmuth(2017)]{unbiased-estimates}
Micha{\l} Derezi\'{n}ski and Manfred~K. Warmuth.
\newblock Unbiased estimates for linear regression via volume sampling.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pages
  3087--3096, Long Beach, CA, USA, 2017.

\bibitem[Derezi\'{n}ski and Warmuth(2018)]{regularized-volume-sampling}
Micha{\l} Derezi\'{n}ski and Manfred~K. Warmuth.
\newblock Subsampling for ridge regression via regularized volume sampling.
\newblock In Amos Storkey and Fernando Perez-Cruz, editors, \emph{Proceedings
  of the Twenty-First International Conference on Artificial Intelligence and
  Statistics}, pages 716--725, Playa Blanca, Lanzarote, Canary Islands, April
  2018.

\bibitem[Derezi\'{n}ski et~al.(2018)Derezi\'{n}ski, Warmuth, and
  Hsu]{leveraged-volume-sampling}
Micha{\l} Derezi\'{n}ski, Manfred~K. Warmuth, and Daniel Hsu.
\newblock Leveraged volume sampling for linear regression.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems 31}, pages 2510--2519. Curran Associates, Inc., 2018.

\bibitem[Derezi{\'n}ski et~al.(2019)Derezi{\'n}ski, Clarkson, Mahoney, and
  Warmuth]{minimax-experimental-design}
Micha{\l} Derezi{\'n}ski, Kenneth~L. Clarkson, Michael~W. Mahoney, and
  Manfred~K. Warmuth.
\newblock Minimax experimental design: Bridging the gap between statistical and
  worst-case approaches to least squares regression.
\newblock In Alina Beygelzimer and Daniel Hsu, editors, \emph{Proceedings of
  the Thirty-Second Conference on Learning Theory}, volume~99 of
  \emph{Proceedings of Machine Learning Research}, pages 1050--1069, Phoenix,
  USA, 25--28 Jun 2019.

\bibitem[{Derezi{\'n}ski} et~al.(2019){Derezi{\'n}ski}, {Liang}, and
  {Mahoney}]{bayesian-experimental-design}
Micha{\l} {Derezi{\'n}ski}, Feynman {Liang}, and Michael~W. {Mahoney}.
\newblock {Bayesian experimental design using regularized determinantal point
  processes}.
\newblock \emph{arXiv e-prints}, art. arXiv:1906.04133, Jun 2019.

\bibitem[Derezi\'{n}ski et~al.(2019)Derezi\'{n}ski, Warmuth, and
  Hsu]{correcting-bias}
Micha{\l} Derezi\'{n}ski, Manfred~K. Warmuth, and Daniel Hsu.
\newblock Correcting the bias in least squares regression with volume-rescaled
  sampling.
\newblock In Kamalika Chaudhuri and Masashi Sugiyama, editors,
  \emph{Proceedings of the 22nd International Conference on Artificial
  Intelligence and Statistics}, volume~89 of \emph{Proceedings of Machine
  Learning Research}, pages 944--953. PMLR, 16--18 Apr 2019.

\bibitem[Derezi{\'n}ski et~al.(2019)Derezi{\'n}ski, Warmuth, and
  Hsu]{correcting-bias-journal}
Micha{\l} Derezi{\'n}ski, Manfred~K. Warmuth, and Daniel Hsu.
\newblock {Unbiased estimators for random design regression}.
\newblock \emph{arXiv e-prints}, art. arXiv:1907.03411, Jul 2019.

\bibitem[Drineas and Mahoney(2016)]{DM16_CACM}
Petros Drineas and Michael~W. Mahoney.
\newblock {RandNLA}: Randomized numerical linear algebra.
\newblock \emph{Communications of the ACM}, 59:\penalty0 80--90, 2016.

\bibitem[Drineas and Mahoney(2017)]{RandNLA_PCMIchapter_TR}
Petros Drineas and Michael~W. Mahoney.
\newblock Lectures on randomized numerical linear algebra.
\newblock Technical report, 2017.
\newblock Preprint: arXiv:1712.08880; To appear in: \emph{Lectures of the 2016
  PCMI Summer School on Mathematics of Data}.

\bibitem[Friedman et~al.(2001)Friedman, Hastie, and Tibshirani]{HFT09}
Jerome Friedman, Trevor Hastie, and Robert Tibshirani.
\newblock \emph{The elements of statistical learning}, volume~1.
\newblock Springer series in statistics New York, 2001.

\bibitem[Geiger et~al.(2019)Geiger, Jacot, Spigler, Gabriel, Sagun, d'Ascoli,
  Biroli, Hongler, and Wyart]{GJSx19_TR}
M.~Geiger, A.~Jacot, S.~Spigler, F.~Gabriel, L.~Sagun, S.~d'Ascoli, G.~Biroli,
  C.~Hongler, and M.~Wyart.
\newblock Scaling description of generalization with number of parameters in
  deep learning.
\newblock Technical Report Preprint: arXiv:1901.01608, 2019.

\bibitem[Gleich and Mahoney(2014)]{GM14_ICML}
D.~F. Gleich and M.~W. Mahoney.
\newblock Anti-differentiating approximation algorithms: A case study with
  min-cuts, spectral, and flow.
\newblock In \emph{Proceedings of the 31st International Conference on Machine
  Learning}, pages 1018--1025, 2014.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{GWBNx17}
Suriya Gunasekar, Blake~E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur,
  and Nati Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems 30}, pages 6151--6159. Curran Associates,
  Inc., 2017.

\bibitem[Hachem et~al.(2013)Hachem, Loubaton, Najim, and
  Vallet]{hachem2013bilinear}
Walid Hachem, Philippe Loubaton, Jamal Najim, and Pascal Vallet.
\newblock On bilinear forms based on the resolvent of large random matrices.
\newblock \emph{Annales de l'IHP Probabilit{\'e}s et statistiques}, 49\penalty0
  (1):\penalty0 36--63, 2013.

\bibitem[Hastie et~al.(2019)Hastie, Montanari, Rosset, and
  Tibshirani]{HMRT19_TR}
T.~Hastie, A.~Montanari, S.~Rosset, and R.~J. Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock Technical Report Preprint: arXiv:1903.08560, 2019.

\bibitem[Kobak et~al.(2018)Kobak, Lomond, and Sanchez]{KLS18_TR}
D.~Kobak, J.~Lomond, and B.~Sanchez.
\newblock Optimal ridge penalty for real-world high-dimensional data can be
  zero or negative due to the implicit ridge regularization.
\newblock Technical report, 2018.
\newblock Preprint: arXiv:1805.10939.

\bibitem[Kubo et~al.(2019)Kubo, Banno, Manabe, and Minoji]{KBMM19_TR}
M.~Kubo, R.~Banno, H.~Manabe, and M.~Minoji.
\newblock Implicit regularization in over-parameterized neural networks.
\newblock Technical Report Preprint: arXiv:1903.01997, 2019.

\bibitem[Kulesza and Taskar(2012)]{dpp-ml}
Alex Kulesza and Ben Taskar.
\newblock \emph{Determinantal Point Processes for Machine Learning}.
\newblock Now Publishers Inc., Hanover, MA, USA, 2012.

\bibitem[Ledoit and P{\'e}ch{\'e}(2011)]{ledoit2011eigenvectors}
Olivier Ledoit and Sandrine P{\'e}ch{\'e}.
\newblock Eigenvectors of some large sample covariance matrix ensembles.
\newblock \emph{Probability Theory and Related Fields}, 151\penalty0
  (1-2):\penalty0 233--264, 2011.

\bibitem[LeJeune et~al.(2019)LeJeune, Javadi, and Baraniuk]{LJB19_TR}
D.~LeJeune, H.~Javadi, and R.~G. Baraniuk.
\newblock The implicit regularization of ordinary least squares ensembles.
\newblock Technical report, 2019.
\newblock Preprint: arXiv:1910.04743.

\bibitem[Liang and Rakhlin(2019)]{LR18_TR}
T.~Liang and A.~Rakhlin.
\newblock Just interpolate: Kernel ``ridgeless'' regression can generalize.
\newblock \emph{The Annals of Statistics, to appear}, 2019.

\bibitem[Lopes et~al.(2019)Lopes, Erichson, and
  Mahoney]{lopes2019bootstrapping}
Miles~E Lopes, N~Benjamin Erichson, and Michael~W Mahoney.
\newblock Bootstrapping the operator norm in high dimensions: Error estimation
  for covariance matrices and sketching.
\newblock \emph{arXiv preprint arXiv:1909.06120}, 2019.

\bibitem[Ma et~al.(2015)Ma, Mahoney, and Yu]{MMY15}
P.~Ma, M.~W. Mahoney, and B.~Yu.
\newblock A statistical perspective on algorithmic leveraging.
\newblock \emph{Journal of Machine Learning Research}, 16:\penalty0 861--911,
  2015.

\bibitem[Mahoney(2012)]{Mah12}
M.~W. Mahoney.
\newblock Approximate computation and implicit regularization for very
  large-scale data analysis.
\newblock In \emph{Proceedings of the 31st ACM Symposium on Principles of
  Database Systems}, pages 143--154, 2012.

\bibitem[Mahoney and Orecchia(2011)]{MO11-implementing}
M.~W. Mahoney and L.~Orecchia.
\newblock Implementing regularization implicitly via approximate eigenvector
  computation.
\newblock In \emph{Proceedings of the 28th International Conference on Machine
  Learning}, pages 121--128, 2011.

\bibitem[Martin and Mahoney(2018)]{MM18_TR}
C.~H. Martin and M.~W. Mahoney.
\newblock Implicit self-regularization in deep neural networks: Evidence from
  random matrix theory and implications for learning.
\newblock Technical Report Preprint: arXiv:1810.01075, 2018.

\bibitem[Martin and Mahoney(2019)]{MM19_HTSR_ICML}
C.~H. Martin and M.~W. Mahoney.
\newblock Traditional and heavy-tailed self regularization in neural network
  models.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, pages 4284--4293, 2019.

\bibitem[Mei and Montanari(2019)]{MM19_TR}
S.~Mei and A.~Montanari.
\newblock The generalization error of random features regression: Precise
  asymptotics and double descent curve.
\newblock Technical Report Preprint: arXiv:1908.05355, 2019.

\bibitem[Mitra(2019)]{Mit19_TR}
P.~P. Mitra.
\newblock Understanding overfitting peaks in generalization error: Analytical
  risk curves for l2 and l1 penalized interpolation.
\newblock Technical Report Preprint: arXiv:1906.03667, 2019.

\bibitem[Muthukumar et~al.(2019)Muthukumar, Vodrahalli, Subramanian, and
  Sahai]{MVSS19_TR}
V.~Muthukumar, K.~Vodrahalli, V.~Subramanian, and A.~Sahai.
\newblock Harmless interpolation of noisy data in regression.
\newblock Technical Report Preprint: arXiv:1903.09139, 2019.

\bibitem[Mutn{\'y} et~al.(2019)Mutn{\'y}, Derezi{\'n}ski, and Krause]{MDK19_TR}
M.~Mutn{\'y}, M.~Derezi{\'n}ski, and A.~Krause.
\newblock Convergence analysis of the randomized {N}ewton method with
  determinantal sampling.
\newblock Technical report, 2019.
\newblock Preprint: arXiv:1910.11561.

\bibitem[Neyshabur(2017)]{Ney17_TR}
B.~Neyshabur.
\newblock Implicit regularization in deep learning.
\newblock Technical report, 2017.
\newblock Preprint: arXiv:1709.01953.

\bibitem[Neyshabur et~al.(2014)Neyshabur, Tomioka, and Srebro]{NTS14_TR}
B.~Neyshabur, R.~Tomioka, and N.~Srebro.
\newblock In search of the real inductive bias: on the role of implicit
  regularization in deep learning.
\newblock Technical Report Preprint: arXiv:1412.6614, 2014.

\bibitem[Perry and Mahoney(2011)]{PM11}
P.~O. Perry and M.~W. Mahoney.
\newblock Regularized {L}aplacian estimation and fast eigenvector
  approximation.
\newblock In \emph{Annual Advances in Neural Information Processing Systems 24:
  Proceedings of the 2011 Conference}, 2011.

\bibitem[Raskutti and Mahoney(2016)]{GarveshMahoney_JMLR}
G.~Raskutti and M.~W. Mahoney.
\newblock A statistical perspective on randomized sketching for ordinary
  least-squares.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (214):\penalty0 1--31, 2016.

\bibitem[Silverstein and Bai(1995)]{silverstein1995empirical}
Jack~W Silverstein and ZD~Bai.
\newblock On the empirical distribution of eigenvalues of a class of large
  dimensional random matrices.
\newblock \emph{Journal of Multivariate analysis}, 54\penalty0 (2):\penalty0
  175--192, 1995.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{SHNx17_TR}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Srivastava(2003)]{srivastava2003}
M.S. Srivastava.
\newblock Singular wishart and multivariate beta distributions.
\newblock \emph{Ann. Statist.}, 31\penalty0 (5):\penalty0 1537--1560, 10 2003.

\bibitem[van~der Vaart(1965)]{expected-generalized-variance}
H.~Robert van~der Vaart.
\newblock A note on {W}ilks' internal scatter.
\newblock \emph{Ann. Math. Statist.}, 36\penalty0 (4):\penalty0 1308--1312, 08
  1965.

\end{thebibliography}
