@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@article{karpathy2020convolutional,
  title={Convolutional neural networks for visual recognition},
  author={Karpathy, Andrej and others},
  journal={Course notes hosted on GitHub. Retrieved from: http://cs231n. github. io},
  year={2020}
}

@inproceedings{shankar2020neural,
  title={Neural kernels without tangents},
  author={Shankar, Vaishaal and Fang, Alex and Guo, Wenshuo and Fridovich-Keil, Sara and Ragan-Kelley, Jonathan and Schmidt, Ludwig and Recht, Benjamin},
  booktitle={International Conference on Machine Learning},
  pages={8614--8623},
  year={2020},
  organization={PMLR}
}

@inproceedings{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}

@article{sagun2017empirical,
  title={Empirical analysis of the hessian of over-parametrized neural networks},
  author={Sagun, Levent and Evci, Utku and Guney, V Ugur and Dauphin, Yann and Bottou, Leon},
  journal={arXiv preprint arXiv:1706.04454},
  year={2017}
}

@article{sagun2016eigenvalues,
  title={Eigenvalues of the hessian in deep learning: Singularity and beyond},
  author={Sagun, Levent and Bottou, Leon and LeCun, Yann},
  journal={arXiv preprint arXiv:1611.07476},
  year={2016}
}

@article{mishkin2015all,
  title={All you need is a good init},
  author={Mishkin, Dmytro and Matas, Jiri},
  journal={arXiv preprint arXiv:1511.06422},
  year={2015}
}

@article{janocha2017loss,
  title={On loss functions for deep neural networks in classification},
  author={Janocha, Katarzyna and Czarnecki, Wojciech Marian},
  journal={arXiv preprint arXiv:1702.05659},
  year={2017}
}


@article{rosasco2004loss,
  title={Are loss functions all the same?},
  author={Rosasco, Lorenzo and Vito, Ernesto De and Caponnetto, Andrea and Piana, Michele and Verri, Alessandro},
  journal={Neural Computation},
  volume={16},
  number={5},
  pages={1063--1076},
  year={2004},
  publisher={MIT Press}
}

@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={41--48},
  year={2009}
}


@incollection{mccloskey1989catastrophic,
  title={Catastrophic interference in connectionist networks: The sequential learning problem},
  author={McCloskey, Michael and Cohen, Neal J},
  booktitle={Psychology of learning and motivation},
  volume={24},
  pages={109--165},
  year={1989},
  publisher={Elsevier}
}

@article{ratcliff1990connectionist,
  title={Connectionist models of recognition memory: constraints imposed by learning and forgetting functions.},
  author={Ratcliff, Roger},
  journal={Psychological review},
  volume={97},
  number={2},
  pages={285},
  year={1990},
  publisher={American Psychological Association}
}

@article{goodfellow2013empirical,
  title={An empirical investigation of catastrophic forgetting in gradient-based neural networks},
  author={Goodfellow, Ian J and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1312.6211},
  year={2013}
}

@article{wales1999global,
  title={Global optimization of clusters, crystals, and biomolecules},
  author={Wales, David J and Scheraga, Harold A},
  journal={Science},
  volume={285},
  number={5432},
  pages={1368--1372},
  year={1999},
  publisher={American Association for the Advancement of Science}
}

@article{stillinger1988nonlinear,
  title={Nonlinear optimization simplified by hypersurface deformation},
  author={Stillinger, FH and Weber, TA},
  journal={Journal of statistical physics},
  volume={52},
  number={5-6},
  pages={1429--1445},
  year={1988},
  publisher={Springer}
}

@article{wawak1998diffusion,
  title={Diffusion equation and distance scaling methods of global optimization: Applications to crystal structure prediction},
  author={Wawak, Ryszard J and Pillardy, Jaroslaw and Liwo, Adam and Gibson, Kenneth D and Scheraga, Harold A},
  journal={The Journal of Physical Chemistry A},
  volume={102},
  number={17},
  pages={2904--2918},
  year={1998},
  publisher={ACS Publications}
}

@inproceedings{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  booktitle={Advances in neural information processing systems},
  pages={8572--8583},
  year={2019}
}

@article{ghorbani2019investigation,
  title={An investigation into neural net optimization via hessian eigenvalue density},
  author={Ghorbani, Behrooz and Krishnan, Shankar and Xiao, Ying},
  journal={arXiv preprint arXiv:1901.10159},
  year={2019}
}

@misc{gilmer2020,
  author = {  Gilmer, Justin},
  title = {Large Scale Spectral Density Estimation for Deep Neural Networks},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/google/spectral-density}}
}

@article{ruiz2019tuning,
  title={Tuning and jamming reduced to their minima},
  author={Ruiz-Garc{\'\i}a, Miguel and Liu, Andrea J and Katifori, Eleni},
  journal={Physical Review E},
  volume={100},
  number={5},
  pages={052608},
  year={2019},
  publisher={APS}
}

 @InProceedings{pmlr-v9-glorot10a, title = {Understanding the difficulty of training deep feedforward neural networks}, author = {Xavier Glorot and Yoshua Bengio}, booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics}, pages = {249--256}, year = {2010}, editor = {Yee Whye Teh and Mike Titterington}, volume = {9}, series = {Proceedings of Machine Learning Research}, address = {Chia Laguna Resort, Sardinia, Italy}, month = {13--15 May}, publisher = {JMLR Workshop and Conference Proceedings}, pdf = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf}, url = {http://proceedings.mlr.press/v9/glorot10a.html}, abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.} } 
 
  @InProceedings{pmlr-v80-xiao18a, title = {Dynamical Isometry and a Mean Field Theory of {CNN}s: How to Train 10,000-Layer Vanilla Convolutional Neural Networks}, author = {Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha and Schoenholz, Samuel and Pennington, Jeffrey}, booktitle = {Proceedings of the 35th International Conference on Machine Learning}, pages = {5393--5402}, year = {2018}, editor = {Jennifer Dy and Andreas Krause}, volume = {80}, series = {Proceedings of Machine Learning Research}, address = {Stockholmsmässan, Stockholm Sweden}, month = {10--15 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v80/xiao18a/xiao18a.pdf}, url = {http://proceedings.mlr.press/v80/xiao18a.html}, abstract = {In recent years, state-of-the-art methods in computer vision have utilized increasingly deep convolutional neural network architectures (CNNs), with some of the most successful models employing hundreds or even thousands of layers. A variety of pathologies such as vanishing/exploding gradients make training such deep networks challenging. While residual connections and batch normalization do enable training at these depths, it has remained unclear whether such specialized architecture designs are truly necessary to train deep CNNs. In this work, we demonstrate that it is possible to train vanilla CNNs with ten thousand layers or more simply by using an appropriate initialization scheme. We derive this initialization scheme theoretically by developing a mean field theory for signal propagation and by characterizing the conditions for dynamical isometry, the equilibration of singular values of the input-output Jacobian matrix. These conditions require that the convolution operator be an orthogonal transformation in the sense that it is norm-preserving. We present an algorithm for generating such random initial orthogonal convolution kernels and demonstrate empirically that they enable efficient training of extremely deep architectures.} } 
  
  @INPROCEEDINGS{he2016,

  author={K. {He} and X. {Zhang} and S. {Ren} and J. {Sun}},

  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 

  title={Deep Residual Learning for Image Recognition}, 

  year={2016},

  volume={},

  number={},

  pages={770-778},

  doi={10.1109/CVPR.2016.90}}

@article{zoph2016neural,
  title={Neural architecture search with reinforcement learning},
  author={Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1611.01578},
  year={2016}
}

@article{cubuk2018autoaugment,
  title={Autoaugment: Learning augmentation policies from data},
  author={Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
  journal={arXiv preprint arXiv:1805.09501},
  year={2018}
}

@inproceedings{cubuk2019,
title	= {AutoAugment: Learning Augmentation Policies from Data},
author= {Ekin Dogus Cubuk and Barret Zoph and Dandelion Mane and Vijay Vasudevan and Quoc V. Le},
journal={arXiv preprint arXiv:1805.09501},
year	= {2019},
URL	= {https://arxiv.org/pdf/1805.09501.pdf}
}

@article{Kingma2015AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2015},
  volume={abs/1412.6980}
}

@article{kornblith2020s,
  title={What's in a Loss Function for Image Classification?},
  author={Kornblith, Simon and Lee, Honglak and Chen, Ting and Norouzi, Mohammad},
  journal={arXiv preprint arXiv:2010.16402},
  year={2020}
}

 @InProceedings{pmlr-v48-amodei16, title = {Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin}, author = {Dario Amodei and Sundaram Ananthanarayanan and Rishita Anubhai and Jingliang Bai and Eric Battenberg and Carl Case and Jared Casper and Bryan Catanzaro and Qiang Cheng and Guoliang Chen and Jie Chen and Jingdong Chen and Zhijie Chen and Mike Chrzanowski and Adam Coates and Greg Diamos and Ke Ding and Niandong Du and Erich Elsen and Jesse Engel and Weiwei Fang and Linxi Fan and Christopher Fougner and Liang Gao and Caixia Gong and Awni Hannun and Tony Han and Lappi Johannes and Bing Jiang and Cai Ju and Billy Jun and Patrick LeGresley and Libby Lin and Junjie Liu and Yang Liu and Weigao Li and Xiangang Li and Dongpeng Ma and Sharan Narang and Andrew Ng and Sherjil Ozair and Yiping Peng and Ryan Prenger and Sheng Qian and Zongfeng Quan and Jonathan Raiman and Vinay Rao and Sanjeev Satheesh and David Seetapun and Shubho Sengupta and Kavya Srinet and Anuroop Sriram and Haiyuan Tang and Liliang Tang and Chong Wang and Jidong Wang and Kaifu Wang and Yi Wang and Zhijian Wang and Zhiqian Wang and Shuang Wu and Likai Wei and Bo Xiao and Wen Xie and Yan Xie and Dani Yogatama and Bin Yuan and Jun Zhan and Zhenyao Zhu}, booktitle = {Proceedings of The 33rd International Conference on Machine Learning}, pages = {173--182}, year = {2016}, editor = {Maria Florina Balcan and Kilian Q. Weinberger}, volume = {48}, series = {Proceedings of Machine Learning Research}, address = {New York, New York, USA}, month = {20--22 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v48/amodei16.pdf}, url = {http://proceedings.mlr.press/v48/amodei16.html}, abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech–two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, enabling experiments that previously took weeks to now run in days. This allows us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.} } 
 
 @article{graves2016hybrid,
  title={Hybrid computing using a neural network with dynamic external memory},
  author={Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'n}ska, Agnieszka and Colmenarejo, Sergio G{\'o}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and others},
  journal={Nature},
  volume={538},
  number={7626},
  pages={471--476},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
} 

@article{gur2018gradient,
  title={Gradient descent happens in a tiny subspace},
  author={Gur-Ari, Guy and Roberts, Daniel A and Dyer, Ethan},
  journal={arXiv preprint arXiv:1812.04754},
  year={2018}
}

@article{lewkowycz2020large,
  title={The large learning rate phase of deep learning: the catapult mechanism},
  author={Lewkowycz, Aitor and Bahri, Yasaman and Dyer, Ethan and Sohl-Dickstein, Jascha and Gur-Ari, Guy},
  journal={arXiv preprint arXiv:2003.02218},
  year={2020}
}

@article{verpoort2020archetypal,
  title={Archetypal landscapes for deep neural networks},
  author={Verpoort, Philipp C and Wales, David J and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={36},
  pages={21857--21864},
  year={2020},
  publisher={National Acad Sciences}
}

@article{ballard2017energy,
  title={Energy landscapes for machine learning},
  author={Ballard, Andrew J and Das, Ritankar and Martiniani, Stefano and Mehta, Dhagash and Sagun, Levent and Stevenson, Jacob D and Wales, David J},
  journal={Physical Chemistry Chemical Physics},
  volume={19},
  number={20},
  pages={12585--12603},
  year={2017},
  publisher={Royal Society of Chemistry}
}

@article{geiger2019jamming,
  title={Jamming transition as a paradigm to understand the loss landscape of deep neural networks},
  author={Geiger, Mario and Spigler, Stefano and d'Ascoli, St{\'e}phane and Sagun, Levent and Baity-Jesi, Marco and Biroli, Giulio and Wyart, Matthieu},
  journal={Physical Review E},
  volume={100},
  number={1},
  pages={012115},
  year={2019},
  publisher={APS}
}

@article{geiger2020scaling,
  title={Scaling description of generalization with number of parameters in deep learning},
  author={Geiger, Mario and Jacot, Arthur and Spigler, Stefano and Gabriel, Franck and Sagun, Levent and d’Ascoli, St{\'e}phane and Biroli, Giulio and Hongler, Cl{\'e}ment and Wyart, Matthieu},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2020},
  number={2},
  pages={023401},
  year={2020},
  publisher={IOP Publishing}
}

@article{hexner2020periodic,
  title={Periodic training of creeping solids},
  author={Hexner, Daniel and Liu, Andrea J and Nagel, Sidney R},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={50},
  pages={31690--31695},
  year={2020},
  publisher={National Acad Sciences}
}

@article{keim2014mechanical,
  title={Mechanical and microscopic properties of the reversible plastic regime in a 2D jammed material},
  author={Keim, Nathan C and Arratia, Paulo E},
  journal={Physical review letters},
  volume={112},
  number={2},
  pages={028302},
  year={2014},
  publisher={APS}
}

@article{keim2011generic,
  title={Generic transient memory formation in disordered systems with noise},
  author={Keim, Nathan C and Nagel, Sidney R},
  journal={Physical review letters},
  volume={107},
  number={1},
  pages={010603},
  year={2011},
  publisher={APS}
}

@article{pine2005chaos,
  title={Chaos and threshold for irreversibility in sheared suspensions},
  author={Pine, David J and Gollub, Jerry P and Brady, John F and Leshansky, Alexander M},
  journal={Nature},
  volume={438},
  number={7070},
  pages={997--1000},
  year={2005},
  publisher={Nature Publishing Group}
}

@article{towns2014xsede,
  title={XSEDE: accelerating scientific discovery},
  author={Towns, John and Cockerill, Timothy and Dahan, Maytal and Foster, Ian and Gaither, Kelly and Grimshaw, Andrew and Hazlewood, Victor and Lathrop, Scott and Lifka, Dave and Peterson, Gregory D and others},
  journal={Computing in science \& engineering},
  volume={16},
  number={5},
  pages={62--74},
  year={2014},
  publisher={IEEE}
}

@article{mannelli2019afraid,
  title={Who is Afraid of Big Bad Minima? Analysis of Gradient-Flow in a Spiked Matrix-Tensor Model},
  author={Mannelli, Stefano Sarao and Biroli, Giulio and Cammarota, Chiara and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={arXiv preprint arXiv:1907.08226},
  year={2019}
}

@article{arous2019landscape,
  title={The landscape of the spiked tensor model},
  author={Arous, Gerard Ben and Mei, Song and Montanari, Andrea and Nica, Mihai},
  journal={Communications on Pure and Applied Mathematics},
  volume={72},
  number={11},
  pages={2282--2330},
  year={2019},
  publisher={Wiley Online Library}
}

@article{franz2016simplest,
  title={The simplest model of jamming},
  author={Franz, Silvio and Parisi, Giorgio},
  journal={Journal of Physics A: Mathematical and Theoretical},
  volume={49},
  number={14},
  pages={145001},
  year={2016},
  publisher={IOP Publishing}
}

@article{franz2019jamming,
  title={Jamming in multilayer supervised learning models},
  author={Franz, Silvio and Hwang, Sungmin and Urbani, Pierfrancesco},
  journal={Physical review letters},
  volume={123},
  number={16},
  pages={160602},
  year={2019},
  publisher={APS}
}

@article{franz2019critical,
  title={Critical jammed phase of the linear perceptron},
  author={Franz, Silvio and Sclocchi, Antonio and Urbani, Pierfrancesco},
  journal={Physical review letters},
  volume={123},
  number={11},
  pages={115702},
  year={2019},
  publisher={APS}
}

@article{sachdeva2020tuning,
  title={Tuning environmental timescales to evolve and maintain generalists},
  author={Sachdeva, Vedant and Husain, Kabir and Sheng, Jiming and Wang, Shenshen and Murugan, Arvind},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={23},
  pages={12693--12699},
  year={2020},
  publisher={National Acad Sciences}
}

@inproceedings{choromanska2015loss,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle={Artificial intelligence and statistics},
  pages={192--204},
  year={2015},
  organization={PMLR}
}

@article{soudry2016no,
  title={No bad local minima: Data independent training error guarantees for multilayer neural networks},
  author={Soudry, Daniel and Carmon, Yair},
  journal={arXiv preprint arXiv:1605.08361},
  year={2016}
}

@article{cooper2018loss,
  title={The loss landscape of overparameterized neural networks},
  author={Cooper, Yaim},
  journal={arXiv preprint arXiv:1804.10200},
  year={2018}
}

@article{geiger2020perspective,
  title={Perspective: A Phase Diagram for Deep Learning unifying Jamming, Feature Learning and Lazy Training},
  author={Geiger, Mario and Petrini, Leonardo and Wyart, Matthieu},
  journal={arXiv preprint arXiv:2012.15110},
  year={2020}
}

@article{wrn,
  author    = {Sergey Zagoruyko and
               Nikos Komodakis},
  title     = {Wide Residual Networks},
  journal   = {CoRR},
  volume    = {abs/1605.07146},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.07146},
  archivePrefix = {arXiv},
  eprint    = {1605.07146},
  timestamp = {Mon, 13 Aug 2018 16:46:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ZagoruykoK16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}