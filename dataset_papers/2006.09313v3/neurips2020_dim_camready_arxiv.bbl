\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{BJSG{\etalchar{+}}18}

\bibitem[AAV18]{asadi2018chaining}
Amir Asadi, Emmanuel Abbe, and Sergio Verd{\'u}.
\newblock Chaining mutual information and tightening generalization bounds.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7234--7243, 2018.

\bibitem[AB99]{au1999new}
Siu-Kui Au and James~L Beck.
\newblock A new adaptive importance sampling scheme for reliability
  calculations.
\newblock {\em Structural Safety}, 21(2):135--158, 1999.

\bibitem[AB09]{anthony2009neural}
Martin Anthony and Peter~L Bartlett.
\newblock {\em Neural network learning: Theoretical foundations}.
\newblock cambridge university press, 2009.

\bibitem[Ama98]{amari1998natural}
Shun-Ichi Amari.
\newblock Natural gradient works efficiently in learning.
\newblock {\em Neural computation}, 10(2):251--276, 1998.

\bibitem[Ass83]{assouad1983densite}
Patrick Assouad.
\newblock Densit{\'e} et dimension.
\newblock In {\em Annales de l'Institut Fourier}, volume~33, pages 233--282,
  1983.

\bibitem[AZL19]{allen2019can}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Can sgd learn recurrent neural networks with provable generalization?
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10331--10341, 2019.

\bibitem[AZLL19]{allen2019learning}
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In {\em Advances in neural information processing systems}, pages
  6155--6166, 2019.

\bibitem[BB18]{barakat2018convergence}
Anas Barakat and Pascal Bianchi.
\newblock Convergence and dynamical behavior of the adam algorithm for non
  convex stochastic optimization.
\newblock {\em arXiv preprint:1810.02263}, 2018.

\bibitem[BE02]{bousquet2002stability}
Olivier Bousquet and Andr{\'e} Elisseeff.
\newblock Stability and generalization.
\newblock {\em JMLR}, 2(Mar), 2002.

\bibitem[BG60]{blumenthal1960some}
Robert~M Blumenthal and Ronald~K Getoor.
\newblock Some theorems on stable processes.
\newblock {\em Transactions of the American Mathematical Society},
  95(2):263--273, 1960.

\bibitem[BJSG{\etalchar{+}}18]{pmlr-v80-baity-jesi18a}
Marco Baity-Jesi, Levent Sagun, Mario Geiger, Stefano Spigler, Gerard~Ben
  Arous, Chiara Cammarota, Yann LeCun, Matthieu Wyart, and Giulio Biroli.
\newblock Comparing dynamics: Deep neural networks versus glassy systems.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning}, volume~80 of {\em Proceedings of Machine Learning Research}, pages
  314--323, 10--15 Jul 2018.

\bibitem[Bog07]{bogachev2007measure}
Vladimir~I Bogachev.
\newblock {\em Measure theory}, volume~1.
\newblock Springer, 2007.

\bibitem[BP17]{bishop2017fractals}
Christopher~J Bishop and Yuval Peres.
\newblock {\em Fractals in probability and analysis}.
\newblock Cambridge University Press, 2017.

\bibitem[Bra83]{bradley1983}
Richard~C Bradley.
\newblock On the $\psi$-mixing condition for stationary random sequences.
\newblock {\em Transactions of the American Mathematical Society},
  276(1):55--66, 1983.

\bibitem[BSW13]{bottcher2013levy}
B~B\"{o}ttcher, R~Schilling, and J~Wang.
\newblock L{\'e}vy matters. iii. l{\'e}vy-type processes: construction,
  approximation and sample path properties.
\newblock {\em Lecture Notes in Mathematics}, 2099, 2013.

\bibitem[Cou65]{courrege1965forme}
Philippe Courrege.
\newblock Sur la forme int{\'e}gro-diff{\'e}rentielle des op{\'e}rateurs de $
  c^\infty_k$ dans $ c $ satisfaisant au principe du maximum.
\newblock {\em S{\'e}minaire Brelot-Choquet-Deny. Th{\'e}orie du Potentiel},
  10(1):1--38, 1965.

\bibitem[CS18]{chaudhari2018stochastic}
P.~Chaudhari and S.~Soatto.
\newblock Stochastic gradient descent performs variational inference, converges
  to limit cycles for deep networks.
\newblock In {\em ICLR}, 2018.

\bibitem[Dal17]{dalalyan2017theoretical}
Arnak~S Dalalyan.
\newblock Theoretical guarantees for approximate sampling from smooth and
  log-concave densities.
\newblock {\em Journal of the Royal Statistical Society}, 79(3):651--676, 2017.

\bibitem[DDB19]{dieuleveut2017bridging}
Aymeric Dieuleveut, Alain Durmus, and Francis Bach.
\newblock Bridging the gap between constant step size stochastic gradient
  descent and markov chains.
\newblock {\em The Annals of Statistics (to appear)}, 2019.

\bibitem[DR17]{dziugaite2017computing}
Gintare~Karolina Dziugaite and Daniel~M Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock {\em arXiv preprint arXiv:1703.11008}, 2017.

\bibitem[DSD19]{dym2019expression}
Nadav Dym, Barak Sober, and Ingrid Daubechies.
\newblock Expression of fractals through neural network functions.
\newblock {\em arXiv preprint:1905.11345}, 2019.

\bibitem[EH20]{erdogdu2020convergence}
Murat~A Erdogdu and Rasa Hosseinzadeh.
\newblock On the convergence of langevin monte carlo: The interplay between
  tail growth and smoothness.
\newblock {\em arXiv preprint arXiv:2005.13097}, 2020.

\bibitem[EM15]{erdogdu2015convergence}
Murat~A Erdogdu and Andrea Montanari.
\newblock Convergence rates of sub-sampled newton methods.
\newblock {\em arXiv preprint:1508.02810}, 2015.

\bibitem[EMG90]{edgar1990undergraduate}
GA~Edgar, Topology Measure, and Fractal Geometry.
\newblock Undergraduate texts in mathematics, 1990.

\bibitem[EMS18]{erdogdu2018global}
Murat~A Erdogdu, Lester Mackey, and Ohad Shamir.
\newblock Global non-convex optimization with discretized diffusions.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9671--9680, 2018.

\bibitem[Fal04]{falconer2004fractal}
Kenneth Falconer.
\newblock {\em Fractal geometry: mathematical foundations and applications}.
\newblock John Wiley \& Sons, 2004.

\bibitem[FFP20]{favaro2020stable}
Stefano Favaro, Sandra Fortini, and Stefano Peluchetti.
\newblock Stable behaviour of infinitely wide deep neural networks.
\newblock In {\em AISTATS}, 2020.

\bibitem[GGZ18]{gao2018global}
Xuefeng Gao, Mert G{\"u}rb{\"u}zbalaban, and Lingjiong Zhu.
\newblock Global convergence of stochastic gradient hamiltonian monte carlo for
  non-convex stochastic optimization: Non-asymptotic performance bounds and
  momentum-based acceleration.
\newblock {\em arXiv preprint:1809.04618}, 2018.

\bibitem[GSZ20]{gurbuzbalaban2020heavy}
Mert Gurbuzbalaban, Umut Simsekli, and Lingjiong Zhu.
\newblock The heavy-tail phenomenon in sgd.
\newblock {\em arXiv preprint arXiv:2006.04740}, 2020.

\bibitem[HDS18]{huang2018homogenization}
Qiao Huang, Jinqiao Duan, and Renming Song.
\newblock Homogenization of stable-like feller processes.
\newblock {\em arXiv preprint:1812.11624}, 2018.

\bibitem[Hen73]{hendricks1973dimension}
WJ~Hendricks.
\newblock A dimension theorem for sample functions of processes with stable
  components.
\newblock {\em The Annals of Probability}, pages 849--853, 1973.

\bibitem[HLLL17]{hu2017diffusion}
W.~Hu, C.~J. Li, L.~Li, and J.-G. Liu.
\newblock On the diffusion approximation of nonconvex stochastic gradient
  descent.
\newblock {\em arXiv preprint:1705.07562}, 2017.

\bibitem[HM20]{hodgkinson2020multiplicative}
Liam Hodgkinson and Michael~W Mahoney.
\newblock Multiplicative noise and heavy tails in stochastic optimization.
\newblock {\em arXiv preprint arXiv:2006.06293}, 2020.

\bibitem[HS97]{hochreiter1997flat}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Flat minima.
\newblock {\em Neural Computation}, 9(1):1--42, 1997.

\bibitem[IP06]{imkeller2006first}
Peter Imkeller and Ilya Pavlyukevich.
\newblock First exit times of sdes driven by stable l{\'e}vy processes.
\newblock {\em Stochastic Processes and their Applications}, 116(4):611--642,
  2006.

\bibitem[Jac02]{jacob2002pseudo}
Niels Jacob.
\newblock {\em Pseudo Differential Operators And Markov Processes: Volume II:
  Generators and Their Potential Theory}.
\newblock World Scientific, 2002.

\bibitem[JKA{\etalchar{+}}17]{jastrzkebski2017three}
S.~Jastrzebski, Z.~Kenton, D.~Arpit, N.~Ballas, A.~Fischer, Y.~Bengio, and
  A.~Storkey.
\newblock Three factors influencing minima in {SGD}.
\newblock {\em arXiv preprint:1711.04623}, 2017.

\bibitem[KH09]{cifar10}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kho09]{khoshnevisan2009fractals}
Davar Khoshnevisan.
\newblock From fractals and probability to l{\'e}vy processes and stochastic
  pdes.
\newblock In {\em Fractal Geometry and Stochastics IV}, pages 111--141.
  Springer, 2009.

\bibitem[KL17]{kuzborskij2017data}
Ilja Kuzborskij and Christoph~H Lampert.
\newblock Data-dependent stability of stochastic gradient descent.
\newblock {\em arXiv preprint arXiv:1703.01678}, 2017.

\bibitem[KX17]{khoshnevisan2017macroscopic}
Davar Khoshnevisan and Yimin Xiao.
\newblock On the macroscopic fractal geometry of some random sets.
\newblock In {\em Stochastic Analysis and Related Topics}, pages 179--206.
  Springer, 2017.

\bibitem[L{\'e}v37]{paul1937theorie}
P.~L{\'e}vy.
\newblock Th{\'e}orie de l'addition des variables al{\'e}atoires.
\newblock {\em Gauthiers-Villars, Paris}, 1937.

\bibitem[LG19]{le2019hausdorff}
Ronan Le~Gu{\'e}vel.
\newblock The hausdorff dimension of the range of the l{\'e}vy multistable
  processes.
\newblock {\em Journal of Theoretical Probability}, 32(2):765--780, 2019.

\bibitem[Lon17]{london2017pac}
Ben London.
\newblock A pac-bayesian analysis of randomized learning with application to
  stochastic gradient descent.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2931--2940, 2017.

\bibitem[LPH{\etalchar{+}}17]{pmlr-v54-lu17b}
Xiaoyu Lu, Valerio Perrone, Leonard Hasenclever, Yee~Whye Teh, and Sebastian
  Vollmer.
\newblock {Relativistic Monte Carlo}.
\newblock In {\em AISTATS}, 2017.

\bibitem[LY19]{lHorinczi2019multifractal}
J{\'o}zsef L{\H{o}}rinczi and Xiaochuan Yang.
\newblock Multifractal properties of sample paths of ground state-transformed
  jump processes.
\newblock {\em Chaos, Solitons \& Fractals}, 120:83--94, 2019.

\bibitem[Mat99]{mattila1999geometry}
Pertti Mattila.
\newblock {\em Geometry of sets and measures in Euclidean spaces: fractals and
  rectifiability}.
\newblock Cambridge university press, 1999.

\bibitem[MBM16]{mei2016landscape}
Song Mei, Yu~Bai, and Andrea Montanari.
\newblock The landscape of empirical risk for non-convex losses.
\newblock {\em arXiv preprint arXiv:1607.06534}, 2016.

\bibitem[MHB16]{mandt2016variational}
S.~Mandt, M.~Hoffman, and D.~Blei.
\newblock A variational analysis of stochastic gradient algorithms.
\newblock In {\em ICML}, 2016.

\bibitem[MM19]{martin2019traditional}
Charles~H Martin and Michael~W Mahoney.
\newblock Traditional and heavy-tailed self regularization in neural network
  models.
\newblock In {\em ICML}, 2019.

\bibitem[MMO15]{bgindex}
Mohammad Mohammadi, Adel Mohammadpour, and Hiroaki Ogata.
\newblock {On estimating the tail index and the spectral measure of
  multivariate $\alpha$-stable distributions}.
\newblock {\em Metrika}, 78(5):549--561, 2015.

\bibitem[MSS19]{malach2019deeper}
Eran Malach and Shai Shalev-Shwartz.
\newblock Is deeper better only when shallow is good?
\newblock In {\em NeurIPS}, 2019.

\bibitem[MWZZ17]{mou2017generalization}
Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng.
\newblock Generalization bounds of sgld for non-convex learning: Two
  theoretical viewpoints.
\newblock {\em arXiv preprint arXiv:1707.05947}, 2017.

\bibitem[MX05]{meerschaert2005dimension}
Mark~M Meerschaert and Yimin Xiao.
\newblock Dimension results for sample paths of operator stable {L}{\'e}vy
  processes.
\newblock {\em Stochastic processes and their applications}, 115(1):55--75,
  2005.

\bibitem[NBMS17]{neyshabur2017exploring}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro.
\newblock Exploring generalization in deep learning.
\newblock In {\em NeurIPS}, 2017.

\bibitem[NHD{\etalchar{+}}19]{negrea2019information}
Jeffrey Negrea, Mahdi Haghifam, Gintare~Karolina Dziugaite, Ashish Khisti, and
  Daniel~M Roy.
\newblock Information-theoretic generalization bounds for sgld via
  data-dependent estimates.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  11015--11025, 2019.

\bibitem[N{\c{S}}GR19]{nguyen2019first}
Thanh~Huy Nguyen, Umut {\c{S}}im{\c{s}}ekli, Mert G{\"u}rb{\"u}zbalaban, and
  Ga{\"e}l Richard.
\newblock First exit time analysis of stochastic gradient descent under
  heavy-tailed gradient noise.
\newblock In {\em NeurIPS}, 2019.

\bibitem[NTS15]{neyshabur2015norm}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock Norm-based capacity control in neural networks.
\newblock In {\em Conference on Learning Theory}, pages 1376--1401, 2015.

\bibitem[OKL19]{orvieto2019role}
Antonio Orvieto, Jonas Kohler, and Aurelien Lucchi.
\newblock The role of memory in stochastic optimization.
\newblock {\em arXiv preprint:1907.01678}, 2019.

\bibitem[Pav07]{pavlyukevich2007cooling}
Ilya Pavlyukevich.
\newblock Cooling down l{\'e}vy flights.
\newblock {\em Journal of Physics A: Mathematical and Theoretical},
  40(41):12299, 2007.

\bibitem[PBL19]{poggio2019theoretical}
Tomaso Poggio, Andrzej Banburski, and Qianli Liao.
\newblock Theoretical issues in deep networks: Approximation, optimization and
  generalization.
\newblock {\em arXiv preprint arXiv:1908.09375}, 2019.

\bibitem[Pol64]{polyak1964some}
Boris~T Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock {\em USSR Computational Mathematics and Mathematical Physics},
  4(5):1--17, 1964.

\bibitem[PSGN19]{panigrahi2019non}
Abhishek Panigrahi, Raghav Somani, Navin Goyal, and Praneeth Netrapalli.
\newblock Non-{G}aussianity of stochastic gradient noise.
\newblock {\em arXiv preprint:1910.09626}, 2019.

\bibitem[RRT17]{raginsky2017non}
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky.
\newblock Non-convex learning via stochastic gradient langevin dynamics: a
  nonasymptotic analysis.
\newblock {\em arXiv preprint:1702.03849}, 2017.

\bibitem[RZ19]{russo2019much}
Daniel Russo and James Zou.
\newblock How much does your data exploration overfit? controlling bias via
  information usage.
\newblock {\em Transactions on Information Theory}, 66(1):302--323, 2019.

\bibitem[Sat99]{sato1999levy}
Ken-iti Sato.
\newblock {\em L{\'e}vy processes and infinitely divisible distributions}.
\newblock Cambridge university press, 1999.

\bibitem[Sch98]{schilling1998feller}
Ren{\'e}~L Schilling.
\newblock Feller processes generated by pseudo-differential operators: On the
  hausdorff dimension of their sample paths.
\newblock {\em Journal of Theoretical Probability}, 11(2):303--330, 1998.

\bibitem[Sch16]{schilling2016introduction}
René~L. Schilling.
\newblock An introduction to lévy and feller processes.
\newblock In D.~Khoshnevisan and R.~Schilling, editors, {\em L\'evy-type
  processes to parabolic SPDEs}. Birkhäuser, Cham, 2016.

\bibitem[{\c{S}}GN{\etalchar{+}}19]{csimcsekli2019heavy}
Umut {\c{S}}im{\c{s}}ekli, Mert G{\"u}rb{\"u}zbalaban, Thanh~Huy Nguyen,
  Ga{\"e}l Richard, and Levent Sagun.
\newblock On the heavy-tailed theory of stochastic gradient descent for deep
  neural networks.
\newblock {\em arXiv preprint arXiv:1912.00018}, 2019.

\bibitem[SHTY13]{sugiyama2013learning}
Mahito Sugiyama, Eiju Hirowatari, Hideki Tsuiki, and Akihiro Yamamoto.
\newblock Learning figures with the hausdorff metric by fractals—towards
  computable binary classification.
\newblock {\em Machine learning}, 90(1):91--126, 2013.

\bibitem[SSBD14]{shalev2014understanding}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock {\em Understanding machine learning: From theory to algorithms}.
\newblock Cambridge University Press, 2014.

\bibitem[SSG19]{pmlr-v97-simsekli19a}
Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban.
\newblock A tail-index analysis of stochastic gradient noise in deep neural
  networks.
\newblock In {\em ICML}, 2019.

\bibitem[ST94]{samorodnitsky1994stable}
G.~Samorodnitsky and M.~S. Taqqu.
\newblock {\em Stable non-{G}aussian random processes: stochastic models with
  infinite variance}, volume~1.
\newblock CRC press, 1994.

\bibitem[SZ15]{VGG}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In Yoshua Bengio and Yann LeCun, editors, {\em ICLR}, 2015.

\bibitem[{\c{S}}ZTG20]{csimcsekli2020fractional}
Umut {\c{S}}im{\c{s}}ekli, Lingjiong Zhu, Yee~Whye Teh, and Mert
  G{\"u}rb{\"u}zbalaban.
\newblock Fractional underdamped langevin dynamics: Retargeting sgd with
  momentum under heavy-tailed gradient noise.
\newblock {\em arXiv preprint:2002.05685}, 2020.

\bibitem[Ver19]{vershynin2019high}
Roman Vershynin.
\newblock High-dimensional probability, 2019.

\bibitem[Xia03]{xiao2003random}
Yimin Xiao.
\newblock Random fractals and markov processes.
\newblock {\em Mathematics Preprint Archive}, 2003(6):830--907, 2003.

\bibitem[XR17]{xu2017information}
Aolin Xu and Maxim Raginsky.
\newblock Information-theoretic analysis of generalization capability of
  learning algorithms.
\newblock In {\em NeurIPS}, 2017.

\bibitem[XZ20]{xie2020ergodicity}
Longjie Xie and Xicheng Zhang.
\newblock Ergodicity of stochastic differential equations with jumps and
  singular coefficients.
\newblock In {\em Annales de l'Institut Henri Poincar{\'e}, Probabilit{\'e}s et
  Statistiques}, volume~56, pages 175--229. Institut Henri Poincar{\'e}, 2020.

\bibitem[Yan18]{yang2018multifractality}
Xiaochuan Yang.
\newblock Multifractality of jump diffusion processes.
\newblock In {\em Annales de l'Institut Henri Poincar{\'e}, Probabilit{\'e}s et
  Statistiques}, volume~54, pages 2042--2074. Institut Henri Poincar{\'e},
  2018.

\bibitem[ZKV{\etalchar{+}}19]{zhang2019adam}
Jingzhao Zhang, Sai~Praneeth Karimireddy, Andreas Veit, Seungyeon Kim,
  Sashank~J Reddi, Sanjiv Kumar, and Suvrit Sra.
\newblock Why {ADAM} beats {SGD} for attention models.
\newblock {\em arXiv preprint:1912.03194}, 2019.

\bibitem[ZLZ19]{zhou2019understanding}
Yi~Zhou, Yingbin Liang, and Huishuai Zhang.
\newblock Understanding generalization error of sgd in nonconvex optimization.
\newblock {\em stat}, 1050:7, 2019.

\bibitem[ZWY{\etalchar{+}}19]{zhu2019anisotropic}
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma.
\newblock The anisotropic noise in stochastic gradient descent: Its behavior of
  escaping from sharp minima and regularization effects.
\newblock In {\em ICML}, 2019.

\end{thebibliography}
