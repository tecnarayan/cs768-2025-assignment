@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}
@article{xiao2021early,
  title={Early convolutions help transformers see better},
  author={Xiao, Tete and Singh, Mannat and Mintun, Eric and Darrell, Trevor and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={arXiv preprint arXiv:2106.14881},
  year={2021}
}
@article{berman2019multigrain,
  title={Multigrain: a unified image embedding for classes and instances},
  author={Berman, Maxim and J{\'e}gou, Herv{\'e} and Vedaldi, Andrea and Kokkinos, Iasonas and Douze, Matthijs},
  journal={arXiv preprint arXiv:1902.05509},
  year={2019}
}
@article{luo2021stable,
  title={Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding},
  author={Luo, Shengjie and Li, Shanda and Cai, Tianle and He, Di and Peng, Dinglan and Zheng, Shuxin and Ke, Guolin and Wang, Liwei and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2106.12566},
  year={2021}
}
@article{wu2021rethinking,
  title={Rethinking and Improving Relative Position Encoding for Vision Transformer},
  author={Wu, Kan and Peng, Houwen and Chen, Minghao and Fu, Jianlong and Chao, Hongyang},
  journal={arXiv preprint arXiv:2107.14222},
  year={2021}
}
@article{permuteformer,
  title={PermuteFormer: Efficient Relative Position Encoding for Long Sequences},
  author={Chen, Peng},
  journal={arXiv preprint arXiv:2109.02377},
  year={2021}
}
@inproceedings{
ape1,
title={How much Position Information Do Convolutional Neural Networks Encode?},
author={Md Amirul Islam and Sen Jia and Neil D. B. Bruce},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rJeB36NKvB}
}
@article{smca,
  title={Fast convergence of detr with spatially modulated co-attention},
  author={Gao, Peng and Zheng, Minghang and Wang, Xiaogang and Dai, Jifeng and Li, Hongsheng},
  journal={arXiv preprint arXiv:2101.07448},
  year={2021}
}
@inproceedings{xavier,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}
@inproceedings{coco,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}
@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@inproceedings{spe,
  title={Relative positional encoding for Transformers with linear complexity},
  author={Liutkus, Antoine and C\'{iÃÅ}fka, Ond{\v{r}}ej and Wu, Shih-Lun and Simsekli, Umut and Yang, Yi-Hsuan and Richard, Gael},
  booktitle={International Conference on Machine Learning},
  pages={7067--7079},
  year={2021},
  organization={PMLR}
}
@article{chu2021conditional,
  title={Conditional positional encodings for vision transformers},
  author={Chu, Xiangxiang and Tian, Zhi and Zhang, Bo and Wang, Xinlong and Wei, Xiaolin and Xia, Huaxia and Shen, Chunhua},
  journal={arXiv preprint arXiv:2102.10882},
  year={2021}
}
@article{rotary,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Wen, Bo and Liu, Yunfeng},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}
@inproceedings{shaw2018self,
  title={Self-Attention with Relative Position Representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  booktitle={NAACL-HLT (2)},
  year={2018}
}
@article{kasai2021finetuning,
  title={Finetuning Pretrained Transformers into RNNs},
  author={Kasai, Jungo and Peng, Hao and Zhang, Yizhe and Yogatama, Dani and Ilharco, Gabriel and Pappas, Nikolaos and Mao, Yi and Chen, Weizhu and Smith, Noah A},
  journal={arXiv preprint arXiv:2103.13076},
  year={2021}
}
@inproceedings{schlag2021linear,
  title={Linear Transformers are secretly fast weight programmers},
  author={Schlag, Imanol and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  booktitle={International Conference on Machine Learning},
  pages={9355--9366},
  year={2021},
  organization={PMLR}
}
@article{liu2021videoswin,
  title={Video swin transformer},
  author={Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
  journal={arXiv preprint arXiv:2106.13230},
  year={2021}
}
@inproceedings{timesformer,
    author  = {Gedas Bertasius and Heng Wang and Lorenzo Torresani},
    title = {Is Space-Time Attention All You Need for Video Understanding?},
    booktitle   = {Proceedings of the International Conference on Machine Learning (ICML)}, 
    month = {July},
    year = {2021}
}
@article{vtn,
  title={Video transformer network},
  author={Neimark, Daniel and Bar, Omri and Zohar, Maya and Asselmann, Dotan},
  journal={arXiv preprint arXiv:2102.00719},
  year={2021}
}
@article{vitae,
  title={ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias},
  author={Xu, Yufei and Zhang, Qiming and Zhang, Jing and Tao, Dacheng},
  journal={arXiv preprint arXiv:2106.03348},
  year={2021}
}
@article{focal,
  title={Focal Attention for Long-Range Interactions in Vision Transformers},
  author={Yang, Jianwei and Li, Chunyuan and Zhang, Pengchuan and Dai, Xiyang and Xiao, Bin and Yuan, Lu and Gao, Jianfeng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{pvtv2,
  title={Pvtv2: Improved baselines with pyramid vision transformer},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  journal={arXiv preprint arXiv:2106.13797},
  year={2021}
}
@article{pit,
  title={Rethinking spatial dimensions of vision transformers},
  author={Heo, Byeongho and Yun, Sangdoo and Han, Dongyoon and Chun, Sanghyuk and Choe, Junsuk and Oh, Seong Joon},
  journal={arXiv preprint arXiv:2103.16302},
  year={2021}
}
@article{cswin,
  title={Cswin transformer: A general vision transformer backbone with cross-shaped windows},
  author={Dong, Xiaoyi and Bao, Jianmin and Chen, Dongdong and Zhang, Weiming and Yu, Nenghai and Yuan, Lu and Chen, Dong and Guo, Baining},
  journal={arXiv preprint arXiv:2107.00652},
  year={2021}
}
@article{cond-detr,
  title={Conditional DETR for Fast Training Convergence},
  author={Meng, Depu and Chen, Xiaokang and Fan, Zejia and Zeng, Gang and Li, Houqiang and Yuan, Yuhui and Sun, Lei and Wang, Jingdong},
  journal={arXiv preprint arXiv:2108.06152},
  year={2021}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{cnn2,
  title={Backpropagation applied to handwritten zip code recognition},
  author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  journal={Neural computation},
  volume={1},
  number={4},
  pages={541--551},
  year={1989},
  publisher={MIT Press}
}
@article{nest,
  title={Aggregating nested transformers},
  author={Zhang, Zizhao and Zhang, Han and Zhao, Long and Chen, Ting and Pfister, Tomas},
  journal={arXiv preprint arXiv:2105.12723},
  year={2021}
}
@article{coat,
  title={Co-scale conv-attentional image transformers},
  author={Xu, Weijian and Xu, Yifan and Chang, Tyler and Tu, Zhuowen},
  journal={arXiv preprint arXiv:2104.06399},
  year={2021}
}
@incollection{cnn1,
  title={Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition},
  author={Fukushima, Kunihiko and Miyake, Sei},
  booktitle={Competition and cooperation in neural nets},
  pages={267--285},
  year={1982},
  publisher={Springer}
}
@article{attn-pooling-1,
  title={Attentive pooling networks},
  author={Santos, Cicero dos and Tan, Ming and Xiang, Bing and Zhou, Bowen},
  journal={arXiv preprint arXiv:1602.03609},
  year={2016}
}
@article{attn-pooling-2,
  title={Escaping the Big Data Paradigm with Compact Transformers},
  author={Hassani, Ali and Walton, Steven and Shah, Nikhil and Abuduweili, Abulikemu and Li, Jiachen and Shi, Humphrey},
  journal={arXiv preprint arXiv:2104.05704},
  year={2021}
}
@inproceedings{dai-etal-2019-transformer,
    title = "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context",
    author = "Dai, Zihang  and
      Yang, Zhilin  and
      Yang, Yiming  and
      Carbonell, Jaime  and
      Le, Quoc  and
      Salakhutdinov, Ruslan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1285",
    pages = "2978--2988",
}
@article{geman1992neural,
  title={Neural networks and the bias/variance dilemma},
  author={Geman, Stuart and Bienenstock, Elie and Doursat, Ren{\'e}},
  journal={Neural computation},
  volume={4},
  number={1},
  pages={1--58},
  year={1992},
  publisher={MIT Press}
}
@article{inductive-bias,
  title={Relational inductive biases, deep learning, and graph networks},
  author={Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and others},
  journal={arXiv preprint arXiv:1806.01261},
  year={2018}
}
@article{cnn3,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  pages={1097--1105},
  year={2012}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}
@inproceedings{
baevski2018adaptive,
title={Adaptive Input Representations for Neural Language Modeling},
author={Alexei Baevski and Michael Auli},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=ByxZX20qFQ},
}
@article{cnn-1,
  title={Backpropagation applied to handwritten zip code recognition},
  author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  journal={Neural computation},
  volume={1},
  number={4},
  pages={541--551},
  year={1989},
  publisher={MIT Press}
}
@article{cnn-inductive-bias,
  title={Natural image statistics and neural representation},
  author={Simoncelli, Eero P and Olshausen, Bruno A},
  journal={Annual review of neuroscience},
  volume={24},
  number={1},
  pages={1193--1216},
  year={2001},
  publisher={Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA}
}
@inproceedings{
deformable-detr,
title={Deformable {\{}DETR{\}}: Deformable Transformers for End-to-End Object Detection},
author={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=gZ9hCDWe6ke}
}
@article{performer,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}
@article{linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}
@inproceedings{
reformer,
title={Reformer: The Efficient Transformer},
author={Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgNKkHtvB}
}
@article{axial,
  title={Axial attention in multidimensional transformers},
  author={Ho, Jonathan and Kalchbrenner, Nal and Weissenborn, Dirk and Salimans, Tim},
  journal={arXiv preprint arXiv:1912.12180},
  year={2019}
}
@article{sparse-transformer,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}
@article{arnab2021vivit,
  title={Vivit: A video vision transformer},
  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\v{c}}i{\'c}, Mario and Schmid, Cordelia},
  journal={arXiv preprint arXiv:2103.15691},
  year={2021}
}
@article{obj-det-1,
  title={End-to-End Object Detection with Adaptive Clustering Transformer},
  author={Zheng, Minghang and Gao, Peng and Wang, Xiaogang and Li, Hongsheng and Dong, Hao},
  journal={arXiv preprint arXiv:2011.09315},
  year={2020}
}
@article{instance-segmentation,
  title={End-to-End Video Instance Segmentation with Transformers},
  author={Wang, Yuqing and Xu, Zhaoliang and Wang, Xinlong and Shen, Chunhua and Cheng, Baoshan and Shen, Hao and Xia, Huaxia},
  journal={arXiv preprint arXiv:2011.14503},
  year={2020}
}
@article{segmenter,
  title={Segmenter: Transformer for Semantic Segmentation},
  author={Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},
  journal={arXiv preprint arXiv:2105.05633},
  year={2021}
}
@article{IPT,
  title={Pre-trained image processing transformer},
  author={Chen, Hanting and Wang, Yunhe and Guo, Tianyu and Xu, Chang and Deng, Yiping and Liu, Zhenhua and Ma, Siwei and Xu, Chunjing and Xu, Chao and Gao, Wen},
  journal={arXiv preprint arXiv:2012.00364},
  year={2020}
}
@inproceedings{detr,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European Conference on Computer Vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}
@inproceedings{igpt,
  title={Image transformer},
  author={Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  booktitle={International Conference on Machine Learning},
  pages={4055--4064},
  year={2018},
  organization={PMLR}
}
@inproceedings{senet,
  title={Squeeze-and-excitation networks},
  author={Hu, Jie and Shen, Li and Sun, Gang},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7132--7141},
  year={2018}
}
@article{genet,
  title={Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks},
  author={Hu, Jie and Shen, Li and Albanie, Samuel and Sun, Gang and Vedaldi, Andrea},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  pages={9401--9411},
  year={2018}
}
@inproceedings{non-local,
  title={Non-local neural networks},
  author={Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7794--7803},
  year={2018}
}
@article{viola2001robust,
  title={Robust real-time object detection},
  author={Viola, Paul and Jones, Michael and others},
  journal={International journal of computer vision},
  volume={4},
  number={34-47},
  pages={4},
  year={2001}
}
@inproceedings{crow1984summed,
  title={Summed-area tables for texture mapping},
  author={Crow, Franklin C},
  booktitle={Proceedings of the 11th annual conference on Computer graphics and interactive techniques},
  pages={207--212},
  year={1984}
}
@inproceedings{repeat-augment,
  title={Augment Your Batch: Improving Generalization Through Instance Repetition},
  author={Hoffer, Elad and Ben-Nun, Tal and Hubara, Itay and Giladi, Niv and Hoefler, Torsten and Soudry, Daniel},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8129--8138},
  year={2020}
}
@inproceedings{cutmix,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6023--6032},
  year={2019}
}
@inproceedings{adamw,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@article{mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}
@inproceedings{stochastic-depth,
  title={Deep networks with stochastic depth},
  author={Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q},
  booktitle={European conference on computer vision},
  pages={646--661},
  year={2016},
  organization={Springer}
}
@inproceedings{random-augment,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={702--703},
  year={2020}
}
@inproceedings{random-erasing,
  title={Random erasing data augmentation},
  author={Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={07},
  pages={13001--13008},
  year={2020}
}
@article{cos-lr,
  title={Sgdr: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}
@misc{timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}
@article{pytorch,
  title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={8026--8037},
  year={2019}
}
@article{efficient-attn-2,
  title={Efficient transformers: A survey},
  author={Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal={arXiv preprint arXiv:2009.06732},
  year={2020}
}
@article{efficient-attn-1,
  title={Long Range Arena: A Benchmark for Efficient Transformers},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  journal={arXiv preprint arXiv:2011.04006},
  year={2020}
}
@article{pvt,
  title={Pyramid vision transformer: A versatile backbone for dense prediction without convolutions},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  journal={arXiv preprint arXiv:2102.12122},
  year={2021}
}

@article{tnt,
  title={Transformer in transformer},
  author={Han, Kai and Xiao, An and Wu, Enhua and Guo, Jianyuan and Xu, Chunjing and Wang, Yunhe},
  journal={arXiv preprint arXiv:2103.00112},
  year={2021}
}
@article{t2tvit,
  title={Tokens-to-token vit: Training vision transformers from scratch on imagenet},
  author={Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Jiang, Zihang and Tay, Francis EH and Feng, Jiashi and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2101.11986},
  year={2021}
}
@article{cifar,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}
@inproceedings{imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}
@article{crossvit,
  title={Crossvit: Cross-attention multi-scale vision transformer for image classification},
  author={Chen, Chun-Fu and Fan, Quanfu and Panda, Rameswar},
  journal={arXiv preprint arXiv:2103.14899},
  year={2021}
}
@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}
@article{rfa,
  title={Random feature attention},
  author={Peng, Hao and Pappas, Nikolaos and Yogatama, Dani and Schwartz, Roy and Smith, Noah A and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2103.02143},
  year={2021}
}
@article{local-self-attn-1,
  title={Scaling local self-attention for parameter efficient visual backbones},
  author={Vaswani, Ashish and Ramachandran, Prajit and Srinivas, Aravind and Parmar, Niki and Hechtman, Blake and Shlens, Jonathon},
  journal={arXiv preprint arXiv:2103.12731},
  year={2021}
}
@article{contnet,
  title={ConTNet: Why not use convolution and transformer at the same time?},
  author={Yan, Haotian and Li, Zhe and Li, Weijian and Wang, Changhu and Wu, Ming and Zhang, Chuang},
  journal={arXiv preprint arXiv:2104.13497},
  year={2021}
}
@article{ceit,
  title={Incorporating Convolution Designs into Visual Transformers},
  author={Yuan, Kun and Guo, Shaopeng and Liu, Ziwei and Zhou, Aojun and Yu, Fengwei and Wu, Wei},
  journal={arXiv preprint arXiv:2103.11816},
  year={2021}
}
@article{cvt,
  title={Cvt: Introducing convolutions to vision transformers},
  author={Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
  journal={arXiv preprint arXiv:2103.15808},
  year={2021}
}
@article{localvit,
  title={LocalViT: Bringing Locality to Vision Transformers},
  author={Li, Yawei and Zhang, Kai and Cao, Jiezhang and Timofte, Radu and Van Gool, Luc},
  journal={arXiv preprint arXiv:2104.05707},
  year={2021}
}
@article{convit,
  title={Convit: Improving vision transformers with soft convolutional inductive biases},
  author={d'Ascoli, St{\'e}phane and Touvron, Hugo and Leavitt, Matthew and Morcos, Ari and Biroli, Giulio and Sagun, Levent},
  journal={arXiv preprint arXiv:2103.10697},
  year={2021}
}
@article{swin-transformer,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  journal={arXiv preprint arXiv:2103.14030},
  year={2021}
}
@article{deit,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint arXiv:2012.12877},
  year={2020}
}
@article{vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
@article{mc-gradient-estimation-survey,
  title={Monte Carlo Gradient Estimation in Machine Learning},
  author={S. Mohamed and Mihaela Rosca and Michael Figurnov and A. Mnih},
  journal={ArXiv},
  year={2020},
  volume={abs/1906.10652}
}
@InProceedings{bbvi, 
title = {{Black Box Variational Inference}}, 
author = {Rajesh Ranganath and Sean Gerrish and David Blei}, 
booktitle = {Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics}, 
pages = {814--822}, 
year = {2014}, 
editor = {Samuel Kaski and Jukka Corander}, 
volume = {33}, 
series = {Proceedings of Machine Learning Research}, 
address = {Reykjavik, Iceland}, 
month = {22--25 Apr}, 
publisher = {PMLR}
}
@book{law2000simulation,
  title={Simulation modeling and analysis},
  author={Law, Averill M and Kelton, W David and Kelton, W David},
  volume={3},
  year={2000},
  publisher={McGraw-Hill New York}
}
@article{crn1,
  title={Some guidelines and guarantees for common random numbers},
  author={Glasserman, Paul and Yao, David D},
  journal={Management Science},
  volume={38},
  number={6},
  pages={884--908},
  year={1992},
  publisher={INFORMS}
}
@inproceedings{
dis-ars,
title={Coupled Gradient Estimators for Discrete Latent Variables},
author={Zhe Dong and Andriy Mnih and George Tucker},
booktitle={Third Symposium on Advances in Approximate Bayesian Inference},
year={2021},
}
@inproceedings{
gumbel-rao,
title={Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator},
author={Max B Paulus and Chris J. Maddison and Andreas Krause},
booktitle={International Conference on Learning Representations},
year={2021},
}
@InProceedings{sb-ref1, 
title = {{Reliable and Scalable Variational Inference for the Hierarchical Dirichlet Process}}, 
author = {Michael Hughes and Dae Il Kim and Erik Sudderth}, 
booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics}, 
year = {2015}, 
month = {09--12 May}, 
 url = { http://proceedings.mlr.press/v38/hughes15.html }
 }
@inproceedings{hazan2010direct,
 author = {Hazan, Tamir and Keshet, Joseph and McAllester, David},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 pages = {1594--1602},
 publisher = {Curran Associates, Inc.},
 title = {Direct Loss Minimization for Structured Prediction},
 volume = {23},
 year = {2010}
}
@inproceedings{lorberbom2019argmax,
 author = {Lorberbom, Guy and Gane, Andreea and Jaakkola, Tommi and Hazan, Tamir},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {6203--6214},
 publisher = {Curran Associates, Inc.},
 title = {Direct Optimization through \textbackslash arg \textbackslash max for Discrete Variational Auto-Encoder},
 volume = {32},
 year = {2019}
}
@article{Richter2020VarGradAL,
  title={VarGrad: A Low-Variance Gradient Estimator for Variational Inference},
  author={L. Richter and Ayman Boustati and N. N{\"u}sken and Francisco J. R. Ruiz and {\"O}mer Deniz Akyildiz},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.10436}
}
@inproceedings{vimco,
  title={Variational Inference for Monte Carlo Objectives},
  author={A. Mnih and Danilo Jimenez Rezende},
  booktitle={ICML},
  year={2016}
}
@article{sst,
  title={Gradient Estimation with Stochastic Softmax Tricks},
  author={Max B. Paulus and Dami Choi and Daniel Tarlow and A. Krause and Chris J. Maddison},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.08063}
}
@article{st,
title= {Neural Networks for Machine Learning Coursera Video Lectures},
journal= {coursera},
author= {Geoffrey Hinton},
year= {2012},
}
@article{kahn1955use,
  title={Use of different Monte Carlo sampling techniques},
  author={Kahn, Herman},
  year={1955},
  publisher={Rand Corporation}
}
@inproceedings{tokui2017evaluating,
  title={Evaluating the variance of likelihood-ratio gradient estimators},
  author={Tokui, Seiya and Sato, Issei},
  booktitle={International Conference on Machine Learning},
  pages={3414--3423},
  year={2017}
}
@book{sutton2018rl,
author = {Sutton, Richard S. and Barto, Andrew G.},
title = {Reinforcement Learning: An Introduction},
year = {2018},
isbn = {0262039249},
publisher = {A Bradford Book},
address = {Cambridge, MA, USA},
}
@article{adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{blei2017variational,
  title={Variational inference: A review for statisticians},
  author={Blei, David M and Kucukelbir, Alp and McAuliffe, Jon D},
  journal={Journal of the American statistical Association},
  volume={112},
  number={518},
  pages={859--877},
  year={2017},
  publisher={Taylor \& Francis}
}
@article{kool2019buy,
  title={Buy 4 REINFORCE Samples, Get a Baseline for Free!},
  author={Kool, Wouter and van Hoof, Herke and Welling, Max},
  booktitle={ICLR 2019 Deep Reinforcement Learning meets Structured Prediction Workshop},
  year={2019}
}
@ARTICLE{mohamed2019monte,
       author = {{Mohamed}, Shakir and {Rosca}, Mihaela and {Figurnov}, Michael and {Mnih}, Andriy},
        title = "{Monte Carlo Gradient Estimation in Machine Learning}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Mathematics - Optimization and Control},
         year = 2019,
        month = jun,
          eid = {arXiv:1906.10652},
        pages = {arXiv:1906.10652},
archivePrefix = {arXiv},
       eprint = {1906.10652},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190610652M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{richter2020vargrad,
  title={VarGrad: A Low-Variance Gradient Estimator for Variational Inference},
  author={Richter, Lorenz and Boustati, Ayman and N{\"u}sken, Nikolas and Ruiz, Francisco JR and Akyildiz, {\"O}mer Deniz},
  journal={arXiv preprint arXiv:2010.10436},
  year={2020}
}
@article{straight-through,
  title={Estimating or propagating gradients through stochastic neurons for conditional computation},
  author={Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  journal={arXiv preprint arXiv:1308.3432},
  year={2013}
}
@article{go-gradient,
  title={GO gradient for expectation-based objectives},
  author={Cong, Yulai and Zhao, Miaoyun and Bai, Ke and Carin, Lawrence},
  journal={arXiv preprint arXiv:1901.06020},
  year={2019}
}
@inproceedings{
Luo2020SUMO,
title={SUMO: Unbiased Estimation of Log Marginal Probability for Latent Variable Models},
author={Yucen Luo and Alex Beatson and Mohammad Norouzi and Jun Zhu and David Duvenaud and Ryan P. Adams and Ricky T. Q. Chen},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SylkYeHtwr}
}
@article{arsm,
  title={ARSM: Augment-reinforce-swap-merge estimator for gradient backpropagation through categorical variables},
  author={Yin, Mingzhang and Yue, Yuguang and Zhou, Mingyuan},
  journal={arXiv preprint arXiv:1905.01413},
  year={2019}
}
@inproceedings{
    Qiu2020Unbiased,
    title={Unbiased Contrastive Divergence Algorithm for Training Energy-Based Latent Variable Models},
    author={Yixuan Qiu and Lingsong Zhang and Xiao Wang},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=r1eyceSYPr}
    }
    @article{beatson2019efficient,
    title={Efficient optimization of loops and limits with randomized telescoping sums},
    author={Beatson, Alex and Adams, Ryan P},
    journal={arXiv preprint arXiv:1905.07006},
    year={2019}
  }
  @article{burda2015importance,
  title={Importance weighted autoencoders},
  author={Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1509.00519},
  year={2015}
}
@book{mcbook,
   author = {Art B. Owen},
   year = 2013,
   title = {Monte Carlo theory, methods and examples}
}
@inproceedings{
Kool2020Estimating,
title={Estimating Gradients for Discrete Random Variables by Sampling without Replacement},
author={Wouter Kool and Herke van Hoof and Max Welling},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rklEj2EFvB}
}
@article{dong2020disarm,
  title={DisARM: An Antithetic Gradient Estimator for Binary Latent Variables},
  author={Dong, Zhe and Mnih, Andriy and Tucker, George},
  journal={arXiv preprint arXiv:2006.10680},
  year={2020}
}
@article{hinton2002training,
  title={Training products of experts by minimizing contrastive divergence},
  author={Hinton, Geoffrey E},
  journal={Neural computation},
  volume={14},
  number={8},
  pages={1771--1800},
  year={2002},
  publisher={MIT Press}
}
@inproceedings{arm,
  title={ARM: Augment-REINFORCE-merge gradient for stochastic binary networks},
  author={Yin, Mingzhang and Zhou, Mingyuan},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@article{gu2015muprop,
  title={Muprop: Unbiased backpropagation for stochastic neural networks},
  author={Gu, Shixiang and Levine, Sergey and Sutskever, Ilya and Mnih, Andriy},
  journal={arXiv preprint arXiv:1511.05176},
  year={2015}
}
@inproceedings{kool2019stochastic,
  title={Stochastic Beams and Where To Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement},
  author={Kool, Wouter and Van Hoof, Herke and Welling, Max},
  booktitle={International Conference on Machine Learning},
  pages={3499--3508},
  year={2019}
}
@article{joo2020generalized,
  title={Generalized Gumbel-Softmax Gradient Estimator for Various Discrete Random Variables},
  author={Joo, Weonyoung and Kim, Dongjun and Shin, Seungjae and Moon, Il-Chul},
  journal={arXiv preprint arXiv:2003.01847},
  year={2020}
}
@article{chen2021regionvit,
  title={RegionViT: Regional-to-Local Attention for Vision Transformers},
  author={Chen, Chun-Fu and Panda, Rameswar and Fan, Quanfu},
  journal={arXiv preprint arXiv:2106.02689},
  year={2021}
}
@article{liu2018rao,
  title={Rao-blackwellized stochastic gradients for discrete distributions},
  author={Liu, Runjing and Regier, Jeffrey and Tripuraneni, Nilesh and Jordan, Michael I and McAuliffe, Jon},
  journal={arXiv preprint arXiv:1810.04777},
  year={2018}
}
@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}
@inproceedings{xie2019reparameterizable,
  title={Reparameterizable subset sampling via continuous relaxations},
  author={Xie, Sang Michael and Ermon, Stefano},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2019}
}  
@inproceedings{rebar,
  title={Rebar: Low-variance, unbiased gradient estimates for discrete latent variable models},
  author={Tucker, George and Mnih, Andriy and Maddison, Chris J and Lawson, John and Sohl-Dickstein, Jascha},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2627--2636},
  year={2017}
}
@article{relax,
  title={Backpropagation through the void: Optimizing control variates for black-box gradient estimation},
  author={Grathwohl, Will and Choi, Dami and Wu, Yuhuai and Roeder, Geoffrey and Duvenaud, David},
  journal={arXiv preprint arXiv:1711.00123},
  year={2017}
}
@article{gumbel1,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}
@article{gumbel2,
  title={The concrete distribution: A continuous relaxation of discrete random variables},
  author={Maddison, Chris J and Mnih, Andriy and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1611.00712},
  year={2016}
}
@article{vae1,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}
@inproceedings{vae2,
  title={Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
  author={Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  booktitle={International Conference on Machine Learning},
  pages={1278--1286},
  year={2014}
}
@inproceedings{vae3,
  title={Doubly stochastic variational Bayes for non-conjugate inference},
  author={Titsias, Michalis and L{\'a}zaro-Gredilla, Miguel},
  booktitle={International conference on machine learning},
  pages={1971--1979},
  year={2014}
}
@inproceedings{
dehghani2018universal,
title={Universal Transformers},
author={Mostafa Dehghani and Stephan Gouws and Oriol Vinyals and Jakob Uszkoreit and Lukasz Kaiser},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HyzdRiR9Y7},
}

@book{wasserman2006all,
  title={All of nonparametric statistics},
  author={Wasserman, Larry},
  year={2006},
  publisher={Springer Science \& Business Media}
}
@inproceedings{lin2017focal,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2980--2988},
  year={2017}
}