\begin{thebibliography}{10}

\bibitem{uncertainty_time}
Marin Bilo\v{s}, Bertrand Charpentier, and Stephan G\"{u}nnemann.
\newblock Uncertainty on asynchronous time event prediction.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  12831--12840. 2019.

\bibitem{update_belief_propagation}
P.~G. Bissiri, C.~C. Holmes, and S.~G. Walker.
\newblock A general framework for updating belief distributions.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 2016.

\bibitem{uncertainty_on_weigths}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural networks.
\newblock In {\em International Conference on International Conference on
  Machine Learning}, page 1613–1622, 2015.

\bibitem{waic_robust_anomaly_detection}
Hyunsun Choi and Eric Jang.
\newblock Generative ensembles for robust anomaly detection, 2019.

\bibitem{kmnist}
Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki
  Yamamoto, and David Ha.
\newblock Deep learning for classical japanese literature, 2018.

\bibitem{uci_datasets}
Dheeru Dua and Casey Graff.
\newblock {UCI} machine learning repository, 2017.

\bibitem{power_certainty}
Dhivya Eswaran, Stephan G{\"{u}}nnemann, and Christos Faloutsos.
\newblock The power of certainty: {A} dirichlet-multinomial model for belief
  propagation.
\newblock In {\em International Conference on Data Mining}, pages 144--152,
  2017.

\bibitem{drop_out}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In {\em International Conference on Machine Learning}, pages
  1050--1059, 2016.

\bibitem{grathwohl2018scalable}
Will Grathwohl, Ricky T.~Q. Chen, Jesse Bettencourt, and David Duvenaud.
\newblock Scalable reversible generative models with free-form continuous
  dynamics.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{energy_based_classifier}
Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud,
  Mohammad Norouzi, and Kevin Swersky.
\newblock Your classifier is secretly an energy based model and you should
  treat it like one.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{calibration_network}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q. Weinberger.
\newblock On calibration of modern neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1321--1330, 2017.

\bibitem{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock {\em arXiv preprint arXiv:1512.03385}, 2015.

\bibitem{benchmarking_corruptions}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{neural_flow}
Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville.
\newblock Neural autoregressive flows.
\newblock In {\em International Conference on Machine Learning}, pages
  2078--2087, 2018.

\bibitem{NIPS2018_8224}
Durk~P Kingma and Prafulla Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10215--10224. 2018.

\bibitem{glow}
Durk~P Kingma and Prafulla Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10215--10224. 2018.

\bibitem{iaf_flow}
Durk~P Kingma, Tim Salimans, Rafal Jozefowicz, Xi~Chen, Ilya Sutskever, and Max
  Welling.
\newblock Improved variational inference with inverse autoregressive flow.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4743--4751. 2016.

\bibitem{nf_fail_ood}
Polina {Kirichenko}, Pavel {Izmailov}, and Andrew~Gordon {Wilson}.
\newblock {Why Normalizing Flows Fail to Detect Out-of-Distribution Data}.
\newblock In {\em Advances in Neural Information Processing Systems}. 2020.

\bibitem{cifar10}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem{alexnet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1097--1105. 2012.

\bibitem{ensemble_simple}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In {\em Advances in Neural Information Processing Systems 30}, pages
  6402--6413. 2017.

\bibitem{mnist}
Yann LeCun and Corinna Cortes.
\newblock {MNIST} handwritten digit database.
\newblock 2010.

\bibitem{simple_baseline_uncertainty}
Wesley~J Maddox, Pavel Izmailov, Timur Garipov, Dmitry~P Vetrov, and
  Andrew~Gordon Wilson.
\newblock A simple baseline for bayesian uncertainty in deep learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  13132--13143. 2019.

\bibitem{prior_net}
Andrey Malinin and Mark Gales.
\newblock Predictive uncertainty estimation via prior networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7047--7058. 2018.

\bibitem{rev_kl_prior_net}
Andrey Malinin and Mark Gales.
\newblock Reverse kl-divergence training of prior networks: Improved
  uncertainty and adversarial robustness.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  14520--14531. 2019.

\bibitem{distribution_distillation}
Andrey Malinin, Bruno Mlodozeniec, and Mark Gales.
\newblock Ensemble distribution distillation.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{typicality_OOD_generative}
Eric Nalisnick, Akihiro Matsukawa, Yee~Whye Teh, and Balaji Lakshminarayanan.
\newblock Detecting out-of-distribution inputs to deep generative models using
  typicality, 2020.

\bibitem{deep_generative_do_not_know}
Eric~T. Nalisnick, Akihiro Matsukawa, Yee~Whye Teh, Dilan G{\"{o}}r{\"{u}}r,
  and Balaji Lakshminarayanan.
\newblock Do deep generative models know what they don't know?
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{svhn}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y.
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In {\em NIPS Workshop on Deep Learning and Unsupervised Feature
  Learning}, 2011.

\bibitem{practical_deep_bayesian_principles}
Kazuki Osawa, Siddharth Swaroop, Mohammad Emtiyaz~E Khan, Anirudh Jain, Runa
  Eschenhagen, Richard~E Turner, and Rio Yokota.
\newblock Practical deep learning with bayesian principles.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4289--4301. 2019.

\bibitem{NIPS2017_6828}
George Papamakarios, Theo Pavlakou, and Iain Murray.
\newblock Masked autoregressive flow for density estimation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2338--2347. 2017.

\bibitem{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8024--8035. 2019.

\bibitem{GAN_batch_norm}
Alec Radford, Luke Metz, and Soumith Chintala.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock In {\em International Conference on Learning Representations}, 2016.

\bibitem{vi_flow}
Danilo Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In {\em International Conference on Machine Learning}, pages
  1530--1538, 2015.

\bibitem{evidential_uncertainty}
Murat Sensoy, Lance Kaplan, and Melih Kandemir.
\newblock Evidential deep learning to quantify classification uncertainty.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3179--3189. 2018.

\bibitem{PAC_bayesian_estimator}
John Shawe-Taylor and Robert~C. Williamson.
\newblock A pac analysis of a bayesian estimator.
\newblock In {\em Conference on Computational Learning Theory}, page 2–9, New
  York, NY, USA, 1997.

\bibitem{vgg}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In {\em International Conference on Learning Representations,}, 2015.

\bibitem{uncertainty_survey}
Jasper Snoek, Yaniv Ovadia, Emily Fertig, Balaji Lakshminarayanan, Sebastian
  Nowozin, D.~Sculley, Joshua Dillon, Jie Ren, and Zachary Nado.
\newblock Can you trust your model\' s uncertainty? evaluating predictive
  uncertainty under dataset shift.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  13969--13980. 2019.

\bibitem{pixel_cnn}
Aaron van~den Oord, Nal Kalchbrenner, Lasse Espeholt, koray kavukcuoglu, Oriol
  Vinyals, and Alex Graves.
\newblock Conditional image generation with pixelcnn decoders.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4790--4798. 2016.

\bibitem{fashionmnist}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms, 2017.

\bibitem{opt_info_processing_bayes}
Arnold Zellner.
\newblock Optimal information processing and bayes's theorem.
\newblock {\em The American Statistician}, page 278, 11 1988.

\end{thebibliography}
