\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbeel \& Ng(2004)Abbeel and Ng]{abbeel2004apprenticeship}
Abbeel, P. and Ng, A.~Y.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In \emph{Proceedings of the twenty-first international conference on
  Machine learning}, pp.\ ~1. ACM, 2004.

\bibitem[Achiam et~al.(2017)Achiam, Held, Tamar, and
  Abbeel]{achiam2017constrained}
Achiam, J., Held, D., Tamar, A., and Abbeel, P.
\newblock Constrained policy optimization.
\newblock \emph{arXiv preprint arXiv:1705.10528}, 2017.

\bibitem[Altman(1999)]{altman1999constrained}
Altman, E.
\newblock \emph{Constrained Markov decision processes}, volume~7.
\newblock CRC Press, 1999.

\bibitem[Berkenkamp et~al.(2017)Berkenkamp, Turchetta, Schoellig, and
  Krause]{berkenkamp2017safe}
Berkenkamp, F., Turchetta, M., Schoellig, A., and Krause, A.
\newblock Safe model-based reinforcement learning with stability guarantees.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  908--919, 2017.

\bibitem[Bertsekas et~al.(1995)Bertsekas, Bertsekas, Bertsekas, and
  Bertsekas]{bertsekas1995dynamic}
Bertsekas, D.~P., Bertsekas, D.~P., Bertsekas, D.~P., and Bertsekas, D.~P.
\newblock \emph{Dynamic programming and optimal control}, volume~1.
\newblock Athena scientific Belmont, MA, 1995.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Chow et~al.(2015)Chow, Tamar, Mannor, and Pavone]{chow2015risk}
Chow, Y., Tamar, A., Mannor, S., and Pavone, M.
\newblock Risk-sensitive and robust decision-making: a cvar optimization
  approach.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1522--1530, 2015.

\bibitem[Chow et~al.(2018)Chow, Nachum, Duenez-Guzman, and
  Ghavamzadeh]{chow2018lyapunov}
Chow, Y., Nachum, O., Duenez-Guzman, E., and Ghavamzadeh, M.
\newblock A lyapunov-based approach to safe reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1805.07708}, 2018.

\bibitem[Chow et~al.(2019)Chow, Nachum, Faust, Ghavamzadeh, and
  Duenez-Guzman]{chow2019lyapunov}
Chow, Y., Nachum, O., Faust, A., Ghavamzadeh, M., and Duenez-Guzman, E.
\newblock Lyapunov-based safe policy optimization for continuous control.
\newblock \emph{arXiv preprint arXiv:1901.10031}, 2019.

\bibitem[Chow et~al.(2020)Chow, Nachum, Faust, Duenez-Guzman, and
  Ghavamzadeh]{chow2020safe}
Chow, Y., Nachum, O., Faust, A., Duenez-Guzman, E., and Ghavamzadeh, M.
\newblock Safe policy learning for continuous control, 2020.
\newblock URL \url{https://openreview.net/forum?id=HkxeThNFPH}.

\bibitem[Dalal et~al.(2018)Dalal, Dvijotham, Vecerik, Hester, Paduraru, and
  Tassa]{dalal2018safe}
Dalal, G., Dvijotham, K., Vecerik, M., Hester, T., Paduraru, C., and Tassa, Y.
\newblock Safe exploration in continuous action spaces.
\newblock \emph{arXiv preprint arXiv:1801.08757}, 2018.

\bibitem[Dhariwal et~al.(2017)Dhariwal, Hesse, Klimov, Nichol, Plappert,
  Radford, Schulman, Sidor, Wu, and Zhokhov]{baselines}
Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A.,
  Schulman, J., Sidor, S., Wu, Y., and Zhokhov, P.
\newblock Openai baselines.
\newblock \url{https://github.com/openai/baselines}, 2017.

\bibitem[Duan et~al.(2016)Duan, Chen, Houthooft, Schulman, and
  Abbeel]{duan2016benchmarking}
Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1329--1338, 2016.

\bibitem[Eysenbach et~al.(2017)Eysenbach, Gu, Ibarz, and
  Levine]{eysenbach2017leave}
Eysenbach, B., Gu, S., Ibarz, J., and Levine, S.
\newblock Leave no trace: Learning to reset for safe and autonomous
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1711.06782}, 2017.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{kakade2002approximately}
Kakade, S. and Langford, J.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{ICML}, volume~2, pp.\  267--274, 2002.

\bibitem[Khalil(1996)]{khalil1996noninear}
Khalil, H.~K.
\newblock Nonlinear systems.
\newblock 1996.

\bibitem[Koller et~al.(2018)Koller, Berkenkamp, Turchetta, and
  Krause]{koller2018learning}
Koller, T., Berkenkamp, F., Turchetta, M., and Krause, A.
\newblock Learning-based model predictive control for safe exploration and
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1803.08287}, 2018.

\bibitem[Konda \& Tsitsiklis(2000)Konda and Tsitsiklis]{konda2000actor}
Konda, V.~R. and Tsitsiklis, J.~N.
\newblock Actor-critic algorithms.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1008--1014, 2000.

\bibitem[Kostrikov(2018)]{pytorchrl}
Kostrikov, I.
\newblock Pytorch implementations of reinforcement learning algorithms.
\newblock \url{https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail}, 2018.

\bibitem[Leike et~al.(2017)Leike, Martic, Krakovna, Ortega, Everitt, Lefrancq,
  Orseau, and Legg]{leike2017ai}
Leike, J., Martic, M., Krakovna, V., Ortega, P.~A., Everitt, T., Lefrancq, A.,
  Orseau, L., and Legg, S.
\newblock Ai safety gridworlds.
\newblock \emph{arXiv preprint arXiv:1711.09883}, 2017.

\bibitem[Misra et~al.(2019)Misra, Henaff, Krishnamurthy, and
  Langford]{misra2019kinematic}
Misra, D., Henaff, M., Krishnamurthy, A., and Langford, J.
\newblock Kinematic state abstraction and provably efficient rich-observation
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.05815}, 2019.

\bibitem[Mitchell(2003)]{mitchell2003application}
Mitchell, I.~M.
\newblock Application of level set methods to control and reachability problems
  in continuous and hybrid systems.
\newblock 2003.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\
  1928--1937, 2016.

\bibitem[Moldovan \& Abbeel(2012)Moldovan and Abbeel]{moldovan2012safe}
Moldovan, T.~M. and Abbeel, P.
\newblock Safe exploration in markov decision processes.
\newblock \emph{arXiv preprint arXiv:1205.4810}, 2012.

\bibitem[Morimura et~al.(2010)Morimura, Uchibe, Yoshimoto, Peters, and
  Doya]{morimura2010}
Morimura, T., Uchibe, E., Yoshimoto, J., Peters, J., and Doya, K.
\newblock Derivatives of logarithmic stationary distributions for policy
  gradient reinforcement learning.
\newblock \emph{Neural Computation}, 22\penalty0 (2):\penalty0 342--376, 2010.
\newblock \doi{10.1162/neco.2009.12-08-922}.
\newblock URL \url{https://doi.org/10.1162/neco.2009.12-08-922}.
\newblock PMID: 19842990.

\bibitem[Neu et~al.(2017)Neu, Jonsson, and G{\'o}mez]{neu2017unified}
Neu, G., Jonsson, A., and G{\'o}mez, V.
\newblock A unified view of entropy-regularized markov decision processes.
\newblock \emph{arXiv preprint arXiv:1705.07798}, 2017.

\bibitem[Peng \& Williams(1994)Peng and Williams]{peng1994incremental}
Peng, J. and Williams, R.~J.
\newblock Incremental multi-step q-learning.
\newblock In \emph{Machine Learning Proceedings 1994}, pp.\  226--232.
  Elsevier, 1994.

\bibitem[Pineau(2018)]{rep-checklist}
Pineau, J.
\newblock The machine learning reproducibility checklist.
\newblock \url{https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf},
  2018.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Rummery \& Niranjan(1994)Rummery and Niranjan]{sarsa-ref}
Rummery, G.~A. and Niranjan, M.
\newblock \emph{On-line Q-learning using connectionist systems}, volume~37.
\newblock University of Cambridge, Department of Engineering Cambridge,
  England, 1994.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1889--1897, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Sutton(1988)]{sutton1988learning}
Sutton, R.~S.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine learning}, 3\penalty0 (1):\penalty0 9--44, 1988.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Tamar et~al.(2013)Tamar, Di~Castro, and Mannor]{tamar2013policy}
Tamar, A., Di~Castro, D., and Mannor, S.
\newblock Policy evaluation with variance related risk criteria in markov
  decision processes.
\newblock \emph{arXiv preprint arXiv:1301.0104}, 2013.

\bibitem[Tessler et~al.(2018)Tessler, Mankowitz, and Mannor]{tessler2018reward}
Tessler, C., Mankowitz, D.~J., and Mannor, S.
\newblock Reward constrained policy optimization.
\newblock \emph{arXiv preprint arXiv:1805.11074}, 2018.

\bibitem[Thomas et~al.(2019)Thomas, da~Silva, Barto, Giguere, Brun, and
  Brunskill]{thomas2019preventing}
Thomas, P.~S., da~Silva, B.~C., Barto, A.~G., Giguere, S., Brun, Y., and
  Brunskill, E.
\newblock Preventing undesirable behavior of intelligent machines.
\newblock \emph{Science}, 366\penalty0 (6468):\penalty0 999--1004, 2019.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pp.\  5026--5033. IEEE, 2012.

\bibitem[Turchetta et~al.(2016)Turchetta, Berkenkamp, and
  Krause]{turchetta2016safe}
Turchetta, M., Berkenkamp, F., and Krause, A.
\newblock Safe exploration in finite markov decision processes with gaussian
  processes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4312--4320, 2016.

\bibitem[Wachi et~al.(2018)Wachi, Sui, Yue, and Ono]{wachi2018safe}
Wachi, A., Sui, Y., Yue, Y., and Ono, M.
\newblock Safe exploration and optimization of constrained mdps using gaussian
  processes.
\newblock In \emph{AAAI Conference on Artificial Intelligence (AAAI)}, 2018.

\bibitem[Wu et~al.(2017)Wu, Mansimov, Grosse, Liao, and Ba]{wu2017scalable}
Wu, Y., Mansimov, E., Grosse, R.~B., Liao, S., and Ba, J.
\newblock Scalable trust-region method for deep reinforcement learning using
  kronecker-factored approximation.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5279--5288, 2017.

\end{thebibliography}
