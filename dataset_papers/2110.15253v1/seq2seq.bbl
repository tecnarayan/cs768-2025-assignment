\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aitken et~al.(2020)Aitken, Ramasesh, Garg, Cao, Sussillo, and
  Maheswaranathan]{aitken2020geometry}
Aitken, K., Ramasesh, V.~V., Garg, A., Cao, Y., Sussillo, D., and
  Maheswaranathan, N.
\newblock The geometry of integration in text classification rnns.
\newblock \emph{arXiv preprint arXiv:2010.15114}, 2020.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Bahdanau, D., Cho, K., and Bengio, Y.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem[Ba{\~n}{\'o}n et~al.(2020)Ba{\~n}{\'o}n, Chen, Haddow, Heafield,
  Hoang, Espl{\`a}-Gomis, Forcada, Kamran, Kirefu, Koehn, Ortiz~Rojas,
  Pla~Sempere, Ram{\'\i}rez-S{\'a}nchez, Sarr{\'\i}as, Strelec, Thompson,
  Waites, Wiggins, and Zaragoza]{banon-etal-2020-paracrawl}
Ba{\~n}{\'o}n, M., Chen, P., Haddow, B., Heafield, K., Hoang, H.,
  Espl{\`a}-Gomis, M., Forcada, M.~L., Kamran, A., Kirefu, F., Koehn, P.,
  Ortiz~Rojas, S., Pla~Sempere, L., Ram{\'\i}rez-S{\'a}nchez, G., Sarr{\'\i}as,
  E., Strelec, M., Thompson, B., Waites, W., Wiggins, D., and Zaragoza, J.
\newblock {P}ara{C}rawl: Web-scale acquisition of parallel corpora.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  4555--4567, Online, July 2020.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.417}.
\newblock URL \url{https://www.aclweb.org/anthology/2020.acl-main.417}.

\bibitem[Bastings \& Filippova(2020)Bastings and
  Filippova]{bastings2020elephant}
Bastings, J. and Filippova, K.
\newblock The elephant in the interpretability room: Why use attention as
  explanation when we have saliency methods?, 2020.

\bibitem[Chan et~al.(2015)Chan, Jaitly, Le, and Vinyals]{chan2015listen}
Chan, W., Jaitly, N., Le, Q.~V., and Vinyals, O.
\newblock Listen, attend and spell, 2015.

\bibitem[Chefer et~al.(2020)Chefer, Gur, and Wolf]{chefer2020transformer}
Chefer, H., Gur, S., and Wolf, L.
\newblock Transformer interpretability beyond attention visualization, 2020.

\bibitem[{Chiu} et~al.(2018){Chiu}, {Sainath}, {Wu}, {Prabhavalkar}, {Nguyen},
  {Chen}, {Kannan}, {Weiss}, {Rao}, {Gonina}, {Jaitly}, {Li}, {Chorowski}, and
  {Bacchiani}]{chiu2018state}
{Chiu}, C., {Sainath}, T.~N., {Wu}, Y., {Prabhavalkar}, R., {Nguyen}, P.,
  {Chen}, Z., {Kannan}, A., {Weiss}, R.~J., {Rao}, K., {Gonina}, E., {Jaitly},
  N., {Li}, B., {Chorowski}, J., and {Bacchiani}, M.
\newblock State-of-the-art speech recognition with sequence-to-sequence models.
\newblock In \emph{2018 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pp.\  4774--4778, 2018.
\newblock \doi{10.1109/ICASSP.2018.8462105}.

\bibitem[Cho et~al.(2014)Cho, van Merrienboer, G{\"{u}}l{\c{c}}ehre, Bougares,
  Schwenk, and Bengio]{GRU}
Cho, K., van Merrienboer, B., G{\"{u}}l{\c{c}}ehre, {\c{C}}., Bougares, F.,
  Schwenk, H., and Bengio, Y.
\newblock Learning phrase representations using {RNN} encoder-decoder for
  statistical machine translation.
\newblock \emph{CoRR}, abs/1406.1078, 2014.

\bibitem[Collins et~al.(2016)Collins, Sohl-Dickstein, and Sussillo]{UGRNN}
Collins, J., Sohl-Dickstein, J., and Sussillo, D.
\newblock Capacity and trainability in recurrent neural networks, 2016.

\bibitem[Ding et~al.(2019)Ding, Xu, and Koehn]{ding2019saliency}
Ding, S., Xu, H., and Koehn, P.
\newblock Saliency-driven word alignment interpretation for neural machine
  translation.
\newblock \emph{arXiv preprint arXiv:1906.10282}, 2019.

\bibitem[Ghader \& Monz(2017)Ghader and Monz]{ghader2017does}
Ghader, H. and Monz, C.
\newblock What does attention in neural machine translation pay attention to?
\newblock \emph{arXiv preprint arXiv:1710.03348}, 2017.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and Schmidhuber]{HochSchm97}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural Computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Jain \& Wallace(2019)Jain and Wallace]{jain2019attention}
Jain, S. and Wallace, B.~C.
\newblock Attention is not explanation, 2019.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{Adam}
Kingma, D. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{International Conference on Learning Representations}, 12 2014.

\bibitem[Lake \& Baroni(2018)Lake and Baroni]{scan}
Lake, B. and Baroni, M.
\newblock Generalization without systematicity: On the compositional skills of
  sequence-to-sequence recurrent networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2873--2882. PMLR, 2018.

\bibitem[Luong et~al.(2015)Luong, Pham, and Manning]{luong2015effective}
Luong, M.-T., Pham, H., and Manning, C.~D.
\newblock Effective approaches to attention-based neural machine translation.
\newblock \emph{arXiv preprint arXiv:1508.04025}, 2015.

\bibitem[Maheswaranathan \& Sussillo(2020)Maheswaranathan and
  Sussillo]{maheswaranathan2020recurrent}
Maheswaranathan, N. and Sussillo, D.
\newblock How recurrent networks implement contextual processing in sentiment
  analysis.
\newblock \emph{arXiv preprint arXiv:2004.08013}, 2020.

\bibitem[Maheswaranathan et~al.(2019)Maheswaranathan, Williams, Golub, Ganguli,
  and Sussillo]{Maheswaranathan2019}
Maheswaranathan, N., Williams, A., Golub, M., Ganguli, S., and Sussillo, D.
\newblock Reverse engineering recurrent networks for sentiment classification
  reveals line attractor dynamics.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  15696--15705. Curran Associates, Inc., 2019.

\bibitem[Prabhavalkar et~al.(2017)Prabhavalkar, Rao, Sainath, Li, Johnson, and
  Jaitly]{rohit2017comparison}
Prabhavalkar, R., Rao, K., Sainath, T., Li, B., Johnson, L., and Jaitly, N.
\newblock A comparison of sequence-to-sequence models for speech recognition.
\newblock 2017.
\newblock URL
  \url{http://www.isca-speech.org/archive/Interspeech_2017/pdfs/0233.PDF}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer, 2020.

\bibitem[Serrano \& Smith(2019)Serrano and Smith]{serrano2019attention}
Serrano, S. and Smith, N.~A.
\newblock Is attention interpretable?
\newblock \emph{arXiv preprint arXiv:1906.03731}, 2019.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and
  Le]{sutskever2014sequence}
Sutskever, I., Vinyals, O., and Le, Q.~V.
\newblock Sequence to sequence learning with neural networks.
\newblock \emph{arXiv preprint arXiv:1409.3215}, 2014.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5998--6008, 2017.

\bibitem[Wiegreffe \& Pinter(2019)Wiegreffe and Pinter]{wiegreffe2019attention}
Wiegreffe, S. and Pinter, Y.
\newblock Attention is not not explanation, 2019.

\bibitem[Wu et~al.(2016)Wu, Schuster, Chen, Le, Norouzi, Macherey, Krikun, Cao,
  Gao, and et~al.]{wu2016googles}
Wu, Y., Schuster, M., Chen, Z., Le, Q.~V., Norouzi, M., Macherey, W., Krikun,
  M., Cao, Y., Gao, Q., and et~al., K.~M.
\newblock Google's neural machine translation system: Bridging the gap between
  human and machine translation, 2016.

\end{thebibliography}
