@article{jiang2023latent,
  title={A latent space theory for emergent abilities in large language models},
  author={Jiang, Hui},
  journal={arXiv preprint arXiv:2304.09960},
  year={2023}
}

@misc{chatgpt35,
  author       = {OpenAI},
  title        = {Chat{GPT} 3.5},
  howpublished = {https://openai.com/chatgpt},
  year         = {2022},
}

@article{touvron2023llama,
  title={L{L}a{MA}: {O}pen and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{achiam2023gpt,
  title={{GPT}-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{chen2024parallel,
  title={Parallel Structures in Pre-training Data Yield In-Context Learning},
  author={Chen, Yanda and Zhao, Chen and Yu, Zhou and McKeown, Kathleen and He, He},
  journal={arXiv preprint arXiv:2402.12530},
  year={2024}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@inproceedings{wibisono2023bidirectional,
  title={Bidirectional attention as a mixture of continuous word experts},
  author={Wibisono, Kevin C and Wang, Yixin},
  booktitle={Uncertainty in Artificial Intelligence},
  year={2023}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Neural Information Processing Systems},
  year={2017}
}

@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle={Neural Information Processing Systems},
  year={2020}
}

@inproceedings{garg2022what,
  title={What can transformers learn in-context? {A} case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  booktitle={Neural Information Processing Systems},
  year={2022}
}

@inproceedings{oswald2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Joao and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  year={2023}
}

@inproceedings{akyurek2022learning,
  title={What learning algorithm is in-context learning? {I}nvestigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{dai2023why,
  title={Why can {GPT} learn in-context? {L}anguage models secretly perform gradient descent as meta optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Sui, Zhifang and Wei, Furu},
  booktitle={Association for Computational Linguistics},
  year={2023}
}

@inproceedings{bai2023transformers,
title={Transformers as statisticians: {P}rovable in-context learning with in-context algorithm selection},
author={Yu Bai and Fan Chen and Huan Wang and Caiming Xiong and Song Mei},
booktitle={Neural Information Processing Systems},
year={2023}
}

@article{zhang2024trained,
  title={Trained Transformers Learn Linear Models In-Context},
  author={Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L},
  journal={Journal of Machine Learning Research},
  year={2024}
}

@inproceedings{ahn2024transformers,
  title={Transformers learn to implement preconditioned gradient descent for in-context learning},
  author={Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit},
  booktitle={Neural Information Processing Systems},
  year={2024}
}

@article{jacobs1991adaptive,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural Computation},
  year={1991}
}

@inproceedings{kingma2015adam,
  title={Adam: {A} Method for Stochastic Optimization},
  author={Kingma, Diederik and Ba, Jimmy},
  booktitle={International Conference on Learning Representations},
  year={2015}
}

@article{eaton1983multivariate,
  title={Multivariate statistics: {A} vector space approach},
  author={Eaton, Morris L},
  year={1983}
}

@inproceedings{wu2023how,
  title={How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?},
  author={Wu, Jingfeng and Zou, Difan and Chen, Zixiang and Braverman, Vladimir and Gu, Quanquan and Bartlett, Peter},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@inproceedings{fu2023transformers,
title={Transformers Learn Higher-Order Optimization Methods for In-Context Learning: {A} Study with Linear Models},
author={Fu, Deqing and Chen, Tian-Qi and Jia, Robin and Sharan, Vatsal},
booktitle={Workshop on Mathematics of Modern Machine Learning at NeurIPS},
year={2023}
}

@inproceedings{li2023transformers,
author = {Li, Yingcong and Ildiz, M. Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
title = {Transformers as Algorithms: {G}eneralization and Stability in in-Context Learning},
year = {2023},
booktitle = {International Conference on Machine Learning}
}

@article{han2023explaining,
  title={Explaining Emergent In-Context Learning as Kernel Regression},
  author={Han, Chi and Wang, Ziqi and Zhao, Han and Ji, Heng},
  journal={arXiv preprint arXiv:2305.12766},
  year={2023}
}

@inproceedings{huang2023in,
title={In-Context Convergence of Transformers},
author={Huang, Yu and Cheng, Yuan and Liang, Yingbin},
booktitle={Workshop on Mathematics of Modern Machine Learning at NeurIPS},
year={2023}
}

@inproceedings{ding2023causallm,
title={Causal{LM} is not optimal for in-context learning},
author={Nan Ding and Tomer Levinboim and Jialin Wu and Sebastian Goodman and Radu Soricut},
booktitle={International Conference on Learning Representations},
year={2024}
}


@inproceedings{bietti2023birth,
title={Birth of a Transformer: {A} Memory Viewpoint},
author={Alberto Bietti and Vivien Cabannes and Diane Bouchacourt and Herve Jegou and Leon Bottou},
booktitle={Neural Information Processing Systems},
year={2023}
}

@inproceedings{raventos2023pretraining,
title={Pretraining task diversity and the emergence of non-{B}ayesian in-context learning for regression},
author={Allan Ravent√≥s and Mansheej Paul and Feng Chen and Surya Ganguli},
booktitle={Neural Information Processing Systems},
year={2023}
}

@inproceedings{kossen2023in,
title={In-Context Learning Learns Label Relationships but Is Not Conventional Learning},
author={Jannik Kossen and Yarin Gal and Tom Rainforth},
booktitle={International Conference on Learning Representations},
year={2024}
}


@article{yadlowsky2023pretraining,
  title={Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models},
  author={Yadlowsky, Steve and Doshi, Lyric and Tripuraneni, Nilesh},
  journal={arXiv preprint arXiv:2311.00871},
  year={2023}
}

@inproceedings{guo2023how,
  title={How Do Transformers Learn In-Context Beyond Simple Functions? {A} Case Study on Learning with Representations},
  author={Guo, Tianyu and Hu, Wei and Mei, Song and Wang, Huan and Xiong, Caiming and Savarese, Silvio and Bai, Yu},
  booktitle={International Conference on Learning Representations},
  year={2023}
}


@article{lu2023emergent,
  title={Are Emergent Abilities in Large Language Models just In-Context Learning?},
  author={Lu, Sheng and Bigoulaeva, Irina and Sachdeva, Rachneet and Madabushi, Harish Tayyar and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2309.01809},
  year={2023}
}

@inproceedings{mahankali2023one,
  title={One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention},
  author={Mahankali, Arvind V and Hashimoto, Tatsunori and Ma, Tengyu},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@inproceedings{min2022rethinking,
  title={Rethinking the Role of Demonstrations: {W}hat Makes In-Context Learning Work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  booktitle={Empirical Methods in Natural Language Processing},
  year={2022}
}

@inproceedings{xie2021an,
  title={An Explanation of In-context Learning as Implicit {B}ayesian Inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{olsson2022in,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread}
}

@inproceedings{chan2022data,
  title={Data Distributional Properties Drive Emergent In-Context Learning in Transformers},
  author={Chan, Stephanie CY and Santoro, Adam and Lampinen, Andrew Kyle and Wang, Jane X and Singh, Aaditya K and Richemond, Pierre Harvey and McClelland, James and Hill, Felix},
  booktitle={Neural Information Processing Systems},
  year={2022}
}

@article{ren2023in,
  title={In-context Learning with Transformer Is Really Equivalent to a Contrastive Learning Pattern},
  author={Ren, Ruifeng and Liu, Yong},
  journal={arXiv preprint arXiv:2310.13220},
  year={2023}
}

@article{li2023the,
  title={The closeness of in-context learning and weight shifting for softmax regression},
  author={Li, Shuai and Song, Zhao and Xia, Yu and Yu, Tong and Zhou, Tianyi},
  journal={arXiv preprint arXiv:2304.13276},
  year={2023}
}

@inproceedings{wies2023the,
title={The Learnability of In-Context Learning},
author={Noam Wies and Yoav Levine and Amnon Shashua},
booktitle={Neural Information Processing Systems},
year={2023}
}

@article{hahn202a,
  title={A theory of emergent in-context learning as implicit structure induction},
  author={Hahn, Michael and Goyal, Navin},
  journal={arXiv preprint arXiv:2303.07971},
  year={2023}
}

@inproceedings{wang2023large,
title={Large Language Models Are Latent Variable Models: {E}xplaining and Finding Good Demonstrations for In-Context Learning},
author={Xinyi Wang and Wanrong Zhu and Michael Saxon and Mark Steyvers and William Yang Wang},
booktitle={Neural Information Processing Systems},
year={2023}
}

@inproceedings{ahuja2023in,
  title={In-Context Learning through the {B}ayesian Prism},
  author={Panwar, Madhur and Ahuja, Kabir and Goyal, Navin},
  booktitle={International Conference on Learning Representations},
  year={2023}
}


@article{zhang2023what,
  title={What and How does In-Context Learning Learn? {B}ayesian Model Averaging, Parameterization, and Generalization},
  author={Zhang, Yufeng and Zhang, Fengzhuo and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2305.19420},
  year={2023}
}

@article{shen2023do,
  title={Do pretrained Transformers Really Learn In-context by Gradient Descent?},
  author={Shen, Lingfeng and Mishra, Aayush and Khashabi, Daniel},
  journal={arXiv preprint arXiv:2310.08540},
  year={2023}
}


@inproceedings{li2023finding,
  title={Finding Support Examples for In-Context Learning},
  author={Li, Xiaonan and Qiu, Xipeng},
  booktitle={Empirical Methods in Natural Language Processing},
  year={2023}
}



@article{qin2023in,
  title={In-context learning with iterative demonstration selection},
  author={Qin, Chengwei and Zhang, Aston and Dagar, Anirudh and Ye, Wenming},
  journal={arXiv preprint arXiv:2310.09881},
  year={2023}
}



@inproceedings{bhattamishra2023understanding,
  title={Understanding In-Context Learning in Transformers and {LLM}s by Learning to Learn Discrete Functions},
  author={Bhattamishra, Satwik and Patel, Arkil and Blunsom, Phil and Kanade, Varun},
  booktitle={International Conference on Learning Representations},
  year={2023}
}




@inproceedings{han2023understanding,
  title={Understanding In-Context Learning via Supportive Pretraining Data},
  author={Han, Xiaochuang and Simig, Daniel and Mihaylov, Todor and Tsvetkov, Yulia and Celikyilmaz, Asli and Wang, Tianlu},
  booktitle={Association for Computational Linguistics},
  year={2023}
}


@inproceedings{singh2023the,
  title={The transient nature of emergent in-context learning in transformers},
  author={Singh, Aaditya and Chan, Stephanie and Moskovitz, Ted and Grant, Erin and Saxe, Andrew and Hill, Felix},
  booktitle={Neural Information Processing Systems},
  year={2023}
}


@inproceedings{ahuja2023a,
  title={A Closer Look at In-Context Learning under Distribution Shifts},
  author={Ahuja, Kartik and Lopez-Paz, David},
  booktitle={Workshop on Efficient Systems for Foundation Models at ICML},
  year={2023}
}


@inproceedings{yan2023understanding,
  title={Understanding In-Context Learning from Repetitions},
  author={Yan, Jianhao and Xu, Jin and Song, Chiyu and Wu, Chenming and Li, Yafu and Zhang, Yue},
  booktitle={International Conference on Learning Representations},
  year={2023}
}









@article{peng2024revisiting,
  title={Revisiting Demonstration Selection Strategies in In-Context Learning},
  author={Peng, Keqin and Ding, Liang and Yuan, Yancheng and Liu, Xuebo and Zhang, Min and Ouyang, Yuanxin and Tao, Dacheng},
  journal={arXiv preprint arXiv:2401.12087},
  year={2024}
}


@inproceedings{abernethy2024a,
  title={A mechanism for sample-efficient in-context learning for sparse retrieval tasks},
  author={Abernethy, Jacob and Agarwal, Alekh and Marinov, Teodor Vanislavov and Warmuth, Manfred K},
  booktitle={Algorithmic Learning Theory},
  year={2024}
}


@article{akyurek2024in,
  title={In-Context Language Learning: {A}rchitectures and Algorithms},
  author={Aky{\"u}rek, Ekin and Wang, Bailin and Kim, Yoon and Andreas, Jacob},
  journal={arXiv preprint arXiv:2401.12973},
  year={2024}
}


@article{collins2024in,
  title={In-Context Learning with Transformers: {S}oftmax Attention Adapts to Function {L}ipschitzness},
  author={Collins, Liam and Parulekar, Advait and Mokhtari, Aryan and Sanghavi, Sujay and Shakkottai, Sanjay},
  journal={arXiv preprint arXiv:2402.11639},
  year={2024}
}


@article{lin2024dual,
  title={Dual Operating Modes of In-Context Learning},
  author={Lin, Ziqian and Lee, Kangwook},
  journal={arXiv preprint arXiv:2402.18819},
  year={2024}
}

@article{xing2024benefits,
  title={Benefits of Transformer: {I}n-Context Learning in Linear Regression Tasks with Unstructured Data},
  author={Xing, Yue and Lin, Xiaofeng and Suh, Namjoon and Song, Qifan and Cheng, Guang},
  journal={arXiv preprint arXiv:2402.00743},
  year={2024}
}



@article{chiang2024understanding,
  title={Understanding In-Context Learning with a Pelican Soup Framework},
  author={Chiang, Ting-Rui and Yogatama, Dani},
  journal={arXiv preprint arXiv:2402.10424},
  year={2024}
}


@article{van2024in,
  title={In-Context Learning Demonstration Selection via Influence Analysis},
  author={Van, Minh-Hao and Wu, Xintao and others},
  journal={arXiv preprint arXiv:2402.11750},
  year={2024}
}


@article{zhao2024noisyicl,
  title={Noisy{ICL}: {A} Little Noise in Model Parameters Calibrates In-context Learning},
  author={Zhao, Yufeng and Sakai, Yoshihiro and Inoue, Naoya},
  journal={arXiv preprint arXiv:2402.05515},
  year={2024}
}



@inproceedings{abbas2024enhancing,
  title={Enhancing In-context Learning via Linear Probe Calibration},
  author={Abbas, Momin and Zhou, Yi and Ram, Parikshit and Baracaldo, Nathalie and Samulowitz, Horst and Salonidis, Theodoros and Chen, Tianyi},
  booktitle={Artificial Intelligence and Statistics},
  year={2024}
}


@article{chen2024training,
  title={Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: {E}mergence, Convergence, and Optimality},
  author={Chen, Siyu and Sheen, Heejune and Wang, Tianhao and Yang, Zhuoran},
  journal={arXiv preprint arXiv:2402.19442},
  year={2024}
}




@article{sander2024how,
  title={How do Transformers perform In-Context Autoregressive Learning?},
  author={Sander, Michael E and Giryes, Raja and Suzuki, Taiji and Blondel, Mathieu and Peyr{\'e}, Gabriel},
  journal={arXiv preprint arXiv:2402.05787},
  year={2024}
}





@article{yu2024how,
  title={How do Large Language Models Learn In-Context? {Q}uery and Key Matrices of In-Context Heads are Two Towers for Metric Learning},
  author={Yu, Zeping and Ananiadou, Sophia},
  journal={arXiv preprint arXiv:2402.02872},
  year={2024}
}






@article{cui2024superiority,
  title={Superiority of Multi-Head Attention in In-Context Linear Regression},
  author={Cui, Yingqian and Ren, Jie and He, Pengfei and Tang, Jiliang and Xing, Yue},
  journal={arXiv preprint arXiv:2401.17426},
  year={2024}
}



@article{vladymyrov2024linear,
  title={Linear Transformers are Versatile In-Context Learners},
  author={Vladymyrov, Max and von Oswald, Johannes and Sandler, Mark and Ge, Rong},
  journal={arXiv preprint arXiv:2402.14180},
  year={2024}
}





@article{dalal2024the,
  title={The Matrix: A {B}ayesian learning model for {LLM}s},
  author={Dalal, Siddhartha and Misra, Vishal},
  journal={arXiv preprint arXiv:2402.03175},
  year={2024}
}



@article{jeon2024an,
  title={An Information-Theoretic Analysis of In-Context Learning},
  author={Jeon, Hong Jun and Lee, Jason D and Lei, Qi and Van Roy, Benjamin},
  journal={arXiv preprint arXiv:2401.15530},
  year={2024}
}



@article{ren2024identifying,
  title={Identifying semantic induction heads to understand in-context learning},
  author={Ren, Jie and Guo, Qipeng and Yan, Hang and Liu, Dongrui and Qiu, Xipeng and Lin, Dahua},
  journal={arXiv preprint arXiv:2402.13055},
  year={2024}
}


@inproceedings{wibisono2023on,
  title={On the Role of Unstructured Training Data in Transformers' In-Context Learning Capabilities},
  author={Wibisono, Kevin Christian and Wang, Yixin},
  booktitle={Workshop on Mathematics of Modern Machine Learning at NeurIPS},
  year={2023}
}

@inproceedings{haviv2022transformer,
  title={Transformer Language Models without Positional Encodings Still Learn Positional Information},
  author={Haviv, Adi and Ram, Ori and Press, Ofir and Izsak, Peter and Levy, Omer},
  booktitle={Empirical Methods in Natural Language Processing},
  year={2022}
}

@inproceedings{swaminathan2023schema,
  title={Schema-learning and rebinding as mechanisms of in-context learning and emergence},
  author={Swaminathan, Sivaramakrishnan and Dedieu, Antoine and Vasudeva Raju, Rajkumar and Shanahan, Murray and Lazaro-Gredilla, Miguel and George, Dileep},
  booktitle={Neural Information Processing Systems},
  year={2023}
}

@article{su2024roformer,
  title={Roformer: {E}nhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  year={2024}
}

@inproceedings{todd2024function,
  title={Function Vectors in Large Language Models},
  author={Todd, Eric and Li, Millicent and Sharma, Arnab and Mueller, Aaron and Wallace, Byron C and Bau, David},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@inproceedings{xu2024large,
  title={Do large language models have compositional ability? {A}n investigation into limitations and scalability},
  author={Xu, Zhuoyan and Shi, Zhenmei and Liang, Yingyu},
  booktitle={Workshop on Mathematical and Empirical Understanding of Foundation Models at ICLR},
  year={2024}
}