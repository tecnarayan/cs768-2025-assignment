\begin{thebibliography}{90}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbas et~al.(2022)Abbas, Xiao, Chen, Chen, and Chen]{abbas2022sharp}
Momin Abbas, Quan Xiao, Lisha Chen, Pin-Yu Chen, and Tianyi Chen.
\newblock {Sharp-MAML}: Sharpness-aware model-agnostic meta learning.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, pages 10--32. PMLR,
  2022.

\bibitem[Agarwala and Dauphin(2023)]{agarwala2023}
Atish Agarwala and Yann Dauphin.
\newblock {SAM} operates far from home: eigenvalue regularization as a
  dynamical phenomenon.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, pages 152--168. PMLR,
  2023.

\bibitem[Ahn et~al.(2023)Ahn, Bubeck, Chewi, Lee, Suarez, and
  Zhang]{ahn2024learning}
Kwangjun Ahn, S{\'e}bastien Bubeck, Sinho Chewi, Yin~Tat Lee, Felipe Suarez,
  and Yi~Zhang.
\newblock Learning threshold neurons via edge of stability.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~36,
  2023.

\bibitem[Andriushchenko and Flammarion(2022)]{maksym2022}
Maksym Andriushchenko and Nicolas Flammarion.
\newblock Towards understanding sharpness-aware minimization.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, pages 639--668. PMLR,
  2022.

\bibitem[Arora et~al.(2018)Arora, Cohen, and Hazan]{arora2018}
Sanjeev Arora, Nadav Cohen, and Elad Hazan.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, pages 244--253. PMLR,
  2018.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Cohen, Golowich, and
  Hu]{arora2018convergence}
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu.
\newblock A convergence analysis of gradient descent for deep linear neural
  networks.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Cohen, Hu, and
  Luo]{arora2019implicit}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~32,
  2019{\natexlab{b}}.

\bibitem[Arora et~al.(2019{\natexlab{c}})Arora, Du, Hu, Li, and
  Wang]{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, pages 322--332. PMLR,
  2019{\natexlab{c}}.

\bibitem[Arora et~al.(2022)Arora, Li, and Panigrahi]{arora2022eos}
Sanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi.
\newblock Understanding gradient descent on the edge of stability in deep
  learning.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, pages 948--1024. PMLR,
  2022.

\bibitem[Bahri et~al.(2022)Bahri, Mobahi, and Tay]{bahri2021}
Dara Bahri, Hossein Mobahi, and Yi~Tay.
\newblock Sharpness-aware minimization improves language model generalization.
\newblock In \emph{Proc. Conf. Assoc. Comput. Linguist. Meet.}, pages
  7360--7371, 2022.

\bibitem[Barrett and Dherin(2021)]{barrett2020implicit}
David Barrett and Benoit Dherin.
\newblock Implicit gradient regularization.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2021.

\bibitem[Bartlett et~al.(2018)Bartlett, Helmbold, and Long]{bartlett2018id}
Peter Bartlett, Dave Helmbold, and Philip Long.
\newblock Gradient descent with identity initialization efficiently learns
  positive definite linear transformations by deep residual networks.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, pages 521--530. PMLR,
  2018.

\bibitem[Bartlett et~al.(2023)Bartlett, Long, and
  Bousquet]{bartlett2022dynamics}
Peter Bartlett, Philip Long, and Olivier Bousquet.
\newblock The dynamics of sharpness-aware minimization: Bouncing across ravines
  and drifting towards wide minima.
\newblock \emph{J. Mach. Learn. Res.}, 24\penalty0 (316):\penalty0 1--36, 2023.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and Nocedal]{bottou2018}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{SIAM Review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Bowman et~al.(2015)Bowman, Angeli, Potts, and Manning]{snli}
Samuel Bowman, Gabor Angeli, Christopher Potts, and Christopher~D Manning.
\newblock A large annotated corpus for learning natural language inference.
\newblock In \emph{Proc. Conf. Empir. Methods Nat. Lang. Process.}, pages
  632--642, 2015.

\bibitem[Cer et~al.(2017)Cer, Diab, Agirre, Lopez-Gazpio, and Specia]{stsb}
Daniel Cer, Mona Diab, Eneko Agirre, I{\~n}igo Lopez-Gazpio, and Lucia Specia.
\newblock {SemEval}-2017 task 1: Semantic textual similarity-multilingual and
  cross-lingual focused evaluation.
\newblock In \emph{Proc. Int. Workshop Semant. Eval.}, pages 1--14. ACL, 2017.

\bibitem[Chaudhari et~al.(2017)Chaudhari, Choromanska, Soatto, LeCun, Baldassi,
  Borgs, Chayes, Sagun, and Zecchina]{pratik2017}
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi,
  Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina.
\newblock Entropy-{SGD}: {B}iasing gradient descent into wide valleys.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2017.

\bibitem[Chen et~al.(2022)Chen, Hsieh, and Gong]{sam4vit}
Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong.
\newblock When vision transformers outperform {ResNets} without pre-training or
  strong data augmentations.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2022.

\bibitem[Chen et~al.(2024)Chen, Qian, Tang, Lai, Liu, Han, and
  Jia]{chen2023longlora}
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and
  Jiaya Jia.
\newblock {Long-LoRA}: Efficient fine-tuning of long-context large language
  models.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2024.

\bibitem[Chen et~al.(2023)Chen, Zhang, Kou, Chen, Hsieh, and
  Gu]{chen2024why-sam-over-sgd}
Zixiang Chen, Junkai Zhang, Yiwen Kou, Xiangning Chen, Cho-Jui Hsieh, and
  Quanquan Gu.
\newblock Why does sharpness-aware minimization generalize better than {SGD}?
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~36,
  2023.

\bibitem[Dai et~al.(2023)Dai, Ahn, and Sra]{dai2024crucial}
Yan Dai, Kwangjun Ahn, and Suvrit Sra.
\newblock The crucial role of normalization in sharpness-aware minimization.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~36,
  2023.

\bibitem[De~Marneffe et~al.(2019)De~Marneffe, Simons, and Tonhauser]{cb}
Marie-Catherine De~Marneffe, Mandy Simons, and Judith Tonhauser.
\newblock The {CommitmentBank}: Investigating projection in naturally occurring
  discourse.
\newblock \emph{Proc. Sinn und Bedeutung}, 23\penalty0 (2):\penalty0 107--124,
  2019.

\bibitem[De~Sa et~al.(2015)De~Sa, Re, and Olukotun]{de2015}
Christopher De~Sa, Christopher Re, and Kunle Olukotun.
\newblock Global convergence of stochastic gradient descent for some non-convex
  matrix problems.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, pages 2332--2341. PMLR,
  2015.

\bibitem[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and
  Zettlemoyer]{dettmers2024qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Q{LoRA}: Efficient finetuning of quantized {LLM}s.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~36,
  2023.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh2017}
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, pages 1019--1028. PMLR,
  2017.

\bibitem[Dolan and Brockett(2005)]{mrpc}
Bill Dolan and Chris Brockett.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In \emph{Proc. Int. Workshop Paraphrasing}, 2005.

\bibitem[Du et~al.(2022{\natexlab{a}})Du, Yan, Feng, Zhou, Zhen, Goh, and
  Tan]{du2022}
Jiawei Du, Hanshu Yan, Jiashi Feng, Joey~Tianyi Zhou, Liangli Zhen, Rick
  Siow~Mong Goh, and Vincent Y.~F. Tan.
\newblock Efficient sharpness-aware minimization for improved training of
  neural networks.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2022{\natexlab{a}}.

\bibitem[Du et~al.(2022{\natexlab{b}})Du, Zhou, Feng, Tan, and Zhou]{du2022saf}
Jiawei Du, Daquan Zhou, Jiashi Feng, Vincent Y.~F. Tan, and Joey~Tianyi Zhou.
\newblock Sharpness-aware training for free.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems},
  2022{\natexlab{b}}.

\bibitem[Du et~al.(2018)Du, Hu, and Lee]{du2018}
Simon~S Du, Wei Hu, and Jason~D Lee.
\newblock Algorithmic regularization in learning deep homogeneous models:
  Layers are automatically balanced.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~31,
  2018.

\bibitem[Dziugaite and Roy(2017)]{dziugaite2017}
Gintare~Karolina Dziugaite and Daniel~M. Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock In \emph{Proc. Conf. Uncerntainty in Artif. Intel.}, 2017.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and Neyshabur]{foret2021}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2021.

\bibitem[Gardent et~al.(2017)Gardent, Shimorina, Narayan, and
  Perez-Beltrachini]{gardent2017webnlg}
Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura
  Perez-Beltrachini.
\newblock The {WebNLG} challenge: Generating text from {RDF} data.
\newblock In \emph{Proc. Int. Conf. Nat. Lang. Gener.}, pages 124--133. ACL,
  2017.

\bibitem[Ge et~al.(2017)Ge, Jin, and Zheng]{ge2017no}
Rong Ge, Chi Jin, and Yi~Zheng.
\newblock No spurious local minima in nonconvex low rank problems: A unified
  geometric analysis.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, pages 1233--1242. PMLR,
  2017.

\bibitem[Gidel et~al.(2019)Gidel, Bach, and Lacoste-Julien]{gidel2019implicit}
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien.
\newblock Implicit regularization of discrete gradient dynamics in linear
  neural networks.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~32,
  2019.

\bibitem[Gonon et~al.(2024)Gonon, Brisebarre, Riccietti, and
  Gribonval]{gonon2023path}
Antoine Gonon, Nicolas Brisebarre, Elisa Riccietti, and R{\'e}mi Gribonval.
\newblock A path-norm toolkit for modern networks: consequences, promises and
  challenges.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2024.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone,
  De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
  De~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for {NLP}.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, pages 2790--2799. PMLR,
  2019.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and
  Chen]{hu2021lora}
Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen.
\newblock Lo{RA}: Low-rank adaptation of large language models.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2022.

\bibitem[HuggingFace()]{gradientaccumulation}
HuggingFace.
\newblock Gradient accumulation.
\newblock URL
  \url{https://huggingface.co/docs/accelerate/en/usage_guides/gradient_accumulation}.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2018}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry~P. Vetrov, and
  Andrew~Gordon Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock In \emph{Proc. Conf. Uncerntainty in Artif. Intel.}, pages 876--885,
  2018.

\bibitem[Jastrz{\k{e}}bski et~al.(2017)Jastrz{\k{e}}bski, Kenton, Arpit,
  Ballas, Fischer, Bengio, and Storkey]{Jastrzebski2018}
Stanis{\l}aw Jastrz{\k{e}}bski, Zachary Kenton, Devansh Arpit, Nicolas Ballas,
  Asja Fischer, Yoshua Bengio, and Amos Storkey.
\newblock Three factors influencing minima in {SGD}.
\newblock \emph{arXiv:1711.04623}, 2017.

\bibitem[Ji and Telgarsky(2019)]{ji2018gradient}
Ziwei Ji and Matus Telgarsky.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2019.

\bibitem[Jiang et~al.(2023)Jiang, Yang, Zhang, and Kwok]{jiang2023}
Weisen Jiang, Hansi Yang, Yu~Zhang, and James Kwok.
\newblock An adaptive policy to employ sharpness-aware minimization.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2023.

\bibitem[Jiang et~al.(2020)Jiang, Neyshabur, Mobahi, Krishnan, and
  Bengio]{jiang2020}
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy
  Bengio.
\newblock Fantastic generalization measures and where to find them.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2020.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2016.

\bibitem[Kim et~al.(2022)Kim, Li, Hu, and Hospedales]{kim2022}
Minyoung Kim, Da~Li, Shell~Xu Hu, and Timothy~M. Hospedales.
\newblock Fisher {SAM}: Information geometry and sharpness aware minimisation.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, pages 11148--11161,
  2022.

\bibitem[Kopiczko et~al.(2024)Kopiczko, Blankevoort, and
  Asano]{kopiczko2023vera}
Dawid~Jan Kopiczko, Tijmen Blankevoort, and Yuki~M Asano.
\newblock Ve{RA}: Vector-based random matrix adaptation.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2024.

\bibitem[Kwon et~al.(2021)Kwon, Kim, Park, and Choi]{kwon2021}
Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In~Kwon Choi.
\newblock A{SAM}: Adaptive sharpness-aware minimization for scale-invariant
  learning of deep neural networks.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, pages 5905--5914. PMLR,
  2021.

\bibitem[Li and Giannakis(2023)]{li2023vasso}
Bingcong Li and Georgios~B Giannakis.
\newblock Enhancing sharpness-aware optimization through variance suppression.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~36,
  2023.

\bibitem[Li and Liang(2021)]{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In \emph{Proc. Conf. Assoc. Comput. Linguist. Meet.}, pages
  4582--4597, 2021.

\bibitem[Li et~al.(2022)Li, Wang, and Arora]{li2021what}
Zhiyuan Li, Tianhao Wang, and Sanjeev Arora.
\newblock What happens after {SGD} reaches zero loss? -- {A} mathematical
  framework.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2022.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{roberta2019}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock {RoBERTa}: A robustly optimized {BERT} pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Liu et~al.(2022)Liu, Mai, Chen, Hsieh, and You]{liu2022}
Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and Yang You.
\newblock Towards efficient and scalable sharpness-aware minimization.
\newblock In \emph{Proc. Conf. Computer Vision and Pattern Recognition}, pages
  12350--12360, 2022.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2017}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2019.

\bibitem[Lyu and Li(2020)]{lyu2019gradient}
Kaifeng Lyu and Jian Li.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2020.

\bibitem[Malladi et~al.(2023)Malladi, Gao, Nichani, Damian, Lee, Chen, and
  Arora]{malladi2023}
Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason~D. Lee, Danqi
  Chen, and Sanjeev Arora.
\newblock Fine-tuning language models with just forward passes.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~36,
  2023.

\bibitem[Mi et~al.(2022)Mi, Shen, Ren, Zhou, Sun, Ji, and Tao]{mi2022}
Peng Mi, Li~Shen, Tianhe Ren, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji, and
  Dacheng Tao.
\newblock Make sharpness-aware minimization stronger: A sparsified perturbation
  approach.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~35,
  2022.

\bibitem[Nesterov(2004)]{nesterov2004}
Yurii Nesterov.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2004.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Salakhutdinov, and
  Srebro]{neyshabur2015}
Behnam Neyshabur, Russ~R Salakhutdinov, and Nati Srebro.
\newblock {Path-SGD}: Path-normalized optimization in deep neural networks.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~28,
  2015.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, Mcallester, Srebro, and
  Srebro]{behnam2017}
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, Nathan Srebro, and
  Nati Srebro.
\newblock Exploring generalization in deep learning.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~30,
  pages 5947--5956, 2017.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Bhojanapalli, and
  Srebro]{neyshabur2017pac}
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.
\newblock A {PAC}-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2018.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and Liang]{squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock {SQuAD}: 100,000+ questions for machine comprehension of text.
\newblock In \emph{Proc. Conf. Empir. Methods Nat. Lang. Process.}, pages
  2383--2392, 2016.

\bibitem[Rajpurkar et~al.(2018)Rajpurkar, Jia, and Liang]{qnli}
Pranav Rajpurkar, Robin Jia, and Percy Liang.
\newblock Know what you don't know: Unanswerable questions for {SQuAD}.
\newblock In \emph{Proc. Conf. Assoc. Comput. Linguist. Meet.}, pages 784--789,
  2018.

\bibitem[Roemmele et~al.(2011)Roemmele, Bejan, and Gordon]{copa}
Melissa Roemmele, Cosmin~Adrian Bejan, and Andrew~S Gordon.
\newblock Choice of plausible alternatives: An evaluation of commonsense causal
  reasoning.
\newblock In \emph{AAAI Spring Symposium Series}, 2011.

\bibitem[Sheen et~al.(2024)Sheen, Chen, Wang, and Zhou]{sheen2024}
Heejune Sheen, Siyu Chen, Tianhao Wang, and Harrison~H Zhou.
\newblock Implicit regularization of gradient flow on one-layer softmax
  attention.
\newblock \emph{arXiv preprint arXiv:2403.08699}, 2024.

\bibitem[Sherborne et~al.(2023)Sherborne, Saphra, Dasigi, and
  Peng]{sherborne2023}
Tom Sherborne, Naomi Saphra, Pradeep Dasigi, and Hao Peng.
\newblock {TRAM}: Bridging trust regions and sharpness aware minimization.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2023.

\bibitem[Si and Yun(2023)]{si2024practical}
Dongkuk Si and Chulhee Yun.
\newblock Practical sharpness-aware minimization cannot converge all the way to
  optima.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~36,
  2023.

\bibitem[Singh and Hofmann(2024)]{singh2024closed}
Sidak~Pal Singh and Thomas Hofmann.
\newblock Closed form of the hessian spectrum for some neural networks.
\newblock In \emph{High-dimensional Learning Dynamics 2024: The Emergence of
  Structure and Reasoning}, 2024.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{sst2}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D Manning,
  Andrew~Y Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{Proc. Conf. Empir. Methods Nat. Lang. Process.}, pages
  1631--1642, 2013.

\bibitem[Tahmasebi et~al.(2024)Tahmasebi, Soleymani, Bahri, Jegelka, and
  Jaillet]{tahmasebi2024universal}
Behrooz Tahmasebi, Ashkan Soleymani, Dara Bahri, Stefanie Jegelka, and Patrick
  Jaillet.
\newblock A universal class of sharpness-aware minimization algorithms.
\newblock \emph{arXiv preprint arXiv:2406.03682}, 2024.

\bibitem[Tu et~al.(2016)Tu, Boczar, Simchowitz, Soltanolkotabi, and
  Recht]{tu2016low}
Stephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Ben Recht.
\newblock Low-rank solutions of linear matrix equations via procrustes flow.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, pages 964--973. PMLR,
  2016.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~30,
  2017.

\bibitem[Voorhees and Tice(2000)]{trec}
Ellen~M Voorhees and Dawn~M Tice.
\newblock Building a question answering test collection.
\newblock In \emph{Proc. Annu. Int. ACM SIGIR Conf. Res. Dev. Inf. Retr.},
  pages 200--207, 2000.

\bibitem[Wang et~al.(2019{\natexlab{a}})Wang, Pruksachatkun, Nangia, Singh,
  Michael, Hill, Levy, and Bowman]{wang2019superglue}
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
  Felix Hill, Omer Levy, and Samuel Bowman.
\newblock Super{GLUE}: A stickier benchmark for general-purpose language
  understanding systems.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~32,
  2019{\natexlab{a}}.

\bibitem[Wang et~al.(2019{\natexlab{b}})Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R
  Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2019{\natexlab{b}}.

\bibitem[Wang et~al.(2023)Wang, Zhang, Lei, and Zhang]{wang2023}
Pengfei Wang, Zhaoxiang Zhang, Zhen Lei, and Lei Zhang.
\newblock Sharpness-aware gradient matching for domain generalization.
\newblock In \emph{Proc. Conf. Computer Vision and Pattern Recognition}, pages
  3769--3778, 2023.

\bibitem[Wang and Mao(2022)]{wang2022}
Ziqiao Wang and Yongyi Mao.
\newblock On the generalization of models trained with {SGD}:
  Information-theoretic bounds and implications.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2022.

\bibitem[Warstadt et~al.(2019)Warstadt, Singh, and Bowman]{cola}
Alex Warstadt, Amanpreet Singh, and Samuel~R Bowman.
\newblock Neural network acceptability judgments.
\newblock \emph{Trans. Assoc. Comput. Linguist.}, 7:\penalty0 625--641, 2019.

\bibitem[Wen et~al.(2023{\natexlab{a}})Wen, Ma, and hiyuan Li]{wen2023}
Kaiyue Wen, Tengyu Ma, and Z~hiyuan Li.
\newblock How does sharpness-aware minimization minimizes sharpness.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2023{\natexlab{a}}.

\bibitem[Wen et~al.(2023{\natexlab{b}})Wen, Ma, and Li]{wen2023more}
Kaiyue Wen, Tengyu Ma, and Zhiyuan Li.
\newblock Sharpness minimization algorithms do not only minimize sharpness to
  achieve better generalization.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~36,
  2023{\natexlab{b}}.

\bibitem[Williams et~al.(2018)Williams, Nangia, and Bowman]{mnli}
Adina Williams, Nikita Nangia, and Samuel~R Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \emph{Proc. Conf. North Am. Chapter Assoc. Comput. Linguist.},
  pages 1112--1122, 2018.

\bibitem[Woodworth et~al.(2020)Woodworth, Gunasekar, Lee, Moroshko, Savarese,
  Golan, Soudry, and Srebro]{woodworth2020}
Blake Woodworth, Suriya Gunasekar, Jason~D Lee, Edward Moroshko, Pedro
  Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In \emph{Proc. Annual Conf. Learning Theory}, pages 3635--3673. PMLR,
  2020.

\bibitem[Wu et~al.(2020)Wu, Xia, and Wang]{wu2020}
Dongxian Wu, Shu-Tao Xia, and Yisen Wang.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~33,
  pages 2958--2969, 2020.

\bibitem[Xia et~al.(2024)Xia, Qin, and Hazan]{xia2024chain}
Wenhan Xia, Chengwei Qin, and Elad Hazan.
\newblock Chain of {LoRA}: Efficient fine-tuning of language models via
  residual learning.
\newblock \emph{arXiv preprint arXiv:2401.04151}, 2024.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Chen, Bukharin, He, Cheng,
  Chen, and Zhao]{zhang2023adaptive}
Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu~Cheng, Weizhu
  Chen, and Tuo Zhao.
\newblock Adaptive budget allocation for parameter-efficient fine-tuning.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Fan, Yao, Zhang, and
  Wang]{zhang2023domain}
Ruipeng Zhang, Ziqing Fan, Jiangchao Yao, Ya~Zhang, and Yanfeng Wang.
\newblock Domain-inspired sharpness aware minimization under domain shifts.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2023{\natexlab{b}}.

\bibitem[Zhang et~al.(2018)Zhang, Liu, Liu, Gao, Duh, and Van~Durme]{record}
Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin
  Van~Durme.
\newblock {ReCoRD}: Bridging the gap between human and machine commonsense
  reading comprehension.
\newblock \emph{arXiv preprint arXiv:1810.12885}, 2018.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, et~al.]{opt2022}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock {OPT}: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem[Zhao et~al.(2022)Zhao, Zhang, and Hu]{yang2022}
Yang Zhao, Hao Zhang, and Xiuyuan Hu.
\newblock Penalizing gradient norm for efficiently improving generalization in
  deep learning.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, pages 26982--26992,
  2022.

\bibitem[Zhou et~al.(2022)Zhou, Liu, Zhang, and Chen]{zhou2021sharpness}
Wenxuan Zhou, Fangyu Liu, Huan Zhang, and Muhao Chen.
\newblock Sharpness-aware minimization with dynamic reweighting.
\newblock In \emph{Proc. Conf. Empir. Methods Nat. Lang. Process.}, pages
  5686--5699, 2022.

\bibitem[Zhuang et~al.(2022)Zhuang, Gong, Yuan, Cui, Adam, Dvornek, Tatikonda,
  Duncan, and Liu]{zhuang2022}
Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha
  Dvornek, Sekhar Tatikonda, James Duncan, and Ting Liu.
\newblock Surrogate gap minimization improves sharpness-aware training.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2022.

\end{thebibliography}
