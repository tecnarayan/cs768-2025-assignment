\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bach(2017{\natexlab{a}})]{JMLR:v18:14-546}
Bach, F.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (19):\penalty0 1--53, 2017{\natexlab{a}}.

\bibitem[Bach(2017{\natexlab{b}})]{bach2017equivalence}
Bach, F.
\newblock On the equivalence between kernel quadrature rules and random feature
  expansions.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (21):\penalty0 1--38, 2017{\natexlab{b}}.

\bibitem[Balduzzi et~al.(2017)Balduzzi, Frean, Leary, Lewis, Ma, and
  McWilliams]{pmlr-v70-balduzzi17b}
Balduzzi, D., Frean, M., Leary, L., Lewis, J.P., Ma, K.W., and McWilliams, B.
\newblock The shattered gradients problem: If resnets are the answer, then what
  is the question?
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, volume~70, pp.\  342--350, 2017.

\bibitem[Barker et~al.(2017)Barker, Marxer, Vincent, and
  Watanabe]{Barker2017csl}
Barker, J., Marxer, R., Vincent, E., and Watanabe, S.
\newblock The third ‘chime’ speech separation and recognition challenge:
  Analysis and outcomes.
\newblock \emph{Computer Speech and Language}, 46:\penalty0 605--626, 2017.

\bibitem[Bengio et~al.(1994)Bengio, Simard, and Frasconi]{bengio1994learning}
Bengio, Y., Simard, P., and Frasconi, P.
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock \emph{IEEE transactions on neural networks}, 5\penalty0 (2):\penalty0
  157--166, 1994.

\bibitem[Berthelot et~al.(2017)Berthelot, Schumm, and Metz]{berthelot2017began}
Berthelot, D., Schumm, T., and Metz, L.
\newblock Began: Boundary equilibrium generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1703.10717}, 2017.

\bibitem[Billingsley(1995)]{Billingsley}
Billingsley, P.
\newblock \emph{Probability and Measure}.
\newblock Wiley-Interscience, 3rd edition, 1995.
\newblock ISBN 0471007102.

\bibitem[Bryc(1995)]{bryc1995rotation}
Bryc, W.
\newblock Rotation invariant distributions.
\newblock In \emph{The Normal Distribution}, pp.\  51--69. Springer, 1995.

\bibitem[Burgess(1997)]{burgess1997estimating}
Burgess, A.N.
\newblock Estimating equivalent kernels for neural networks: A data
  perturbation approach.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  382--388, 1997.

\bibitem[Cho \& Saul(2009)Cho and Saul]{cho2009kernel}
Cho, Y. and Saul, L.K.
\newblock Kernel methods for deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  342--350, 2009.

\bibitem[Choromanska et~al.(2015)Choromanska, Henaff, Mathieu, Arous, and
  LeCun]{choromanska2015loss}
Choromanska, A., Henaff, M., Mathieu, M., Arous, G.B., and LeCun, Y.
\newblock The loss surfaces of multilayer networks.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  192--204,
  2015.

\bibitem[Clevert et~al.(2016)Clevert, Unterthiner, and
  Hochreiter]{clevert2015fast}
Clevert, D., Unterthiner, T., and Hochreiter, S.
\newblock Fast and accurate deep network learning by exponential linear units
  (elus).
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Daniely et~al.(2016)Daniely, Frostig, and Singer]{daniely2016toward}
Daniely, A., Frostig, R., and Singer, Y.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In \emph{Advances In Neural Information Processing Systems}, pp.\
  2253--2261, 2016.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, pp.\  249--256, 2010.

\bibitem[Haeffele \& Vidal(2015)Haeffele and Vidal]{haeffele2015global}
Haeffele, B.D. and Vidal, R.
\newblock Global optimality in tensor factorization, deep learning, and beyond.
\newblock \emph{arXiv preprint arXiv:1506.07540}, 2015.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  1026--1034, 2015.

\bibitem[Hochreiter(1991)]{hochreiter1991untersuchungen}
Hochreiter, S.
\newblock Untersuchungen zu dynamischen neuronalen netzen.
\newblock \emph{Diploma, Technische Universit{\"a}t M{\"u}nchen}, 91, 1991.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Bengio, and
  Frasconi]{hochreiter2001gradient}
Hochreiter, S., Bengio, Y., and Frasconi, P.
\newblock Gradient flow in recurrent nets: the difficulty of learning long-term
  dependencies.
\newblock In Kolen, J. and Kremer, S. (eds.), \emph{Field Guide to Dynamical
  Recurrent Networks}. IEEE Press, 2001.

\bibitem[Huang et~al.(2004)Huang, Zhu, and Siew]{huang2004extreme}
Huang, G., Zhu, Q., and Siew, C.
\newblock Extreme learning machine: a new learning scheme of feedforward neural
  networks.
\newblock In \emph{Neural Networks, 2004. Proceedings. 2004 IEEE International
  Joint Conference on}, volume~2, pp.\  985--990. IEEE, 2004.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  448--456, 2015.

\bibitem[Johnson(2013)]{johnson2013multivariate}
Johnson, M.E.
\newblock \emph{Multivariate statistical simulation: A guide to selecting and
  generating continuous multivariate distributions}.
\newblock John Wiley \& Sons, 2013.

\bibitem[Jones(1982)]{jones_1982}
Jones, D.S.
\newblock \emph{The Theory of Generalised Functions}, chapter~7, pp.\  263.
\newblock Cambridge University Press, 2nd edition, 1982.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{krizhevsky2009learning}
Krizhevsky, Alex and Hinton, Geoffrey.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lee et~al.(2017)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl-Dickstein]{lee2017gp}
Lee, J., Bahri, Y., Novak, R., Schoenholz, S.S., Pennington, J., and
  Sohl-Dickstein, J.
\newblock Deep neural networks as gaussian processes.
\newblock \emph{arXiv preprint arXiv:1611.01232}, 2017.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015faceattributes}
Liu, Z., Luo, P., Wang, X., and Tang, X.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of International Conference on Computer Vision
  (ICCV)}, December 2015.

\bibitem[Livni et~al.(2017)Livni, Carmon, and Globerson]{livni2017learning}
Livni, R., Carmon, D., and Globerson, A.
\newblock Learning infinite layer networks without the kernel trick.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2198--2207, 2017.

\bibitem[MacKay(1992)]{mackay1992practical}
MacKay, D.J.C.
\newblock A practical {B}ayesian framework for backpropagation networks.
\newblock \emph{Neural Computation}, 4\penalty0 (3):\penalty0 448--472, 1992.

\bibitem[Martin \& Mahoney(2017)Martin and Mahoney]{martin2017rethinking}
Martin, C.H. and Mahoney, M.W.
\newblock Rethinking generalization requires revisiting old ideas: statistical
  mechanics approaches and complex learning behavior.
\newblock \emph{arXiv preprint arXiv:1710.09553}, 2017.

\bibitem[Mishkin \& Matas(2016)Mishkin and Matas]{mishkin2015all}
Mishkin, D. and Matas, J.
\newblock All you need is a good init.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Neal(1994)]{neal1995bayesian}
Neal, R.M.
\newblock \emph{Bayesian Learning for Neural Networks}.
\newblock PhD thesis, University of Toronto, 1994.

\bibitem[Pandey \& Dukkipati(2014{\natexlab{a}})Pandey and
  Dukkipati]{pandey2014go}
Pandey, G. and Dukkipati, A.
\newblock To go deep or wide in learning?
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  724--732,
  2014{\natexlab{a}}.

\bibitem[Pandey \& Dukkipati(2014{\natexlab{b}})Pandey and
  Dukkipati]{pandey2014learning}
Pandey, G. and Dukkipati, A.
\newblock Learning by stretching deep networks.
\newblock In \emph{Proceedings of the 31st International Conference on Machine
  Learning (ICML-14)}, pp.\  1719--1727, 2014{\natexlab{b}}.

\bibitem[Pao et~al.(1994)Pao, Park, and Sobajic]{pao1994learning}
Pao, Y., Park, G., and Sobajic, D.J.
\newblock Learning and generalization characteristics of the random vector
  functional-link net.
\newblock \emph{Neurocomputing}, 6\penalty0 (2):\penalty0 163--180, 1994.

\bibitem[Poole et~al.(2016)Poole, Lahiri, Raghu, Sohl-Dickstein, and
  Ganguli]{poole2016exponential}
Poole, B., Lahiri, S., Raghu, M., Sohl-Dickstein, J., and Ganguli, S.
\newblock Exponential expressivity in deep neural networks through transient
  chaos.
\newblock In \emph{Advances In Neural Information Processing Systems}, pp.\
  3360--3368, 2016.

\bibitem[Raghu et~al.(2017)Raghu, Poole, Kleinberg, Ganguli, and
  Sohl-Dickstein]{pmlr-v70-raghu17a}
Raghu, M., Poole, B., Kleinberg, J., Ganguli, S., and Sohl-Dickstein, J.
\newblock On the expressive power of deep neural networks.
\newblock In Precup, D. and Teh, Y.W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pp.\  2847--2854, 2017.

\bibitem[Rahimi \& Recht(2008)Rahimi and Recht]{rahimi2008random}
Rahimi, A. and Recht, B.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1177--1184, 2008.

\bibitem[Roux \& Bengio(2007)Roux and Bengio]{le2007continuous}
Roux, N.~Le and Bengio, Y.
\newblock Continuous neural networks.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  404--411,
  2007.

\bibitem[Rudi \& Rosasco(2017)Rudi and Rosasco]{rudi2017generalization}
Rudi, Alessandro and Rosasco, Lorenzo.
\newblock Generalization properties of learning with random features.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3218--3228, 2017.

\bibitem[Schmidt et~al.(1992)Schmidt, Kraaijveld, and
  Duin]{schmidt1992feedforward}
Schmidt, W.F., Kraaijveld, M.A., and Duin, R.P.W.
\newblock Feedforward neural networks with random weights.
\newblock In \emph{Pattern Recognition, 1992. Vol. II. Conference B: Pattern
  Recognition Methodology and Systems, Proceedings., 11th IAPR International
  Conference on}, pp.\  1--4. IEEE, 1992.

\bibitem[Schoenholz et~al.(2017)Schoenholz, Gilmer, Ganguli, and
  Sohl-Dickstein]{schoenholz2016deep}
Schoenholz, S.S., Gilmer, J., Ganguli, S., and Sohl-Dickstein, J.
\newblock Deep information propagation.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Shwartz-Ziv \& Tishby(2017)Shwartz-Ziv and Tishby]{shwartz2017opening}
Shwartz-Ziv, R. and Tishby, N.
\newblock {Opening the Black Box of Deep Neural Networks via Information}.
\newblock \emph{arXiv preprint arXiv:1703.00810}, 2017.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, Chen, Lillicrap, Hui, Sifre, van~den
  Driessche, Graepel, and Hassabis]{silver2017mastering}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, T., Huang, A., Guez,
  A., Hubert, T., Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, T., Hui,
  F., Sifre, L., van~den Driessche, G., Graepel, T., and Hassabis, D.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{Nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Sinha \& Duchi(2016)Sinha and Duchi]{sinha2016learning}
Sinha, A. and Duchi, J.C.
\newblock Learning kernels with random features.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1298--1306, 2016.

\bibitem[Tang et~al.(2016)Tang, Deng, and Huang]{tang2016extreme}
Tang, J., Deng, C., and Huang, G.
\newblock Extreme learning machine for multilayer perceptron.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  27\penalty0 (4):\penalty0 809--821, 2016.

\bibitem[van~den Oord et~al.(2016)van~den Oord, Dieleman, Zen, Simonyan,
  Vinyals, Graves, Kalchbrenner, Senior, and Kavukcuoglu]{vanwavenet}
van~den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A.,
  Kalchbrenner, N., Senior, A., and Kavukcuoglu, K.
\newblock Wavenet: A generative model for raw audio.
\newblock \emph{arXiv preprint arXiv:1609.03499}, 2016.

\bibitem[Vincent et~al.(2017)Vincent, Watanabe, Nugraha, Barker, and
  Marxer]{Vincent2017csl}
Vincent, E., Watanabe, S., Nugraha, A., Barker, J., and Marxer, R.
\newblock An analysis of environment, microphone and data simulation mismatches
  in robust speech recognition.
\newblock \emph{Computer Speech and Language}, 46:\penalty0 535--557, 2017.

\bibitem[Williams(1997)]{williams1997computing}
Williams, C.K.I.
\newblock Computing with infinite networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  295--301, 1997.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016.

\end{thebibliography}
