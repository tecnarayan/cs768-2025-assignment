\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2017)Achiam, Held, Tamar, and Abbeel]{CPO}
Achiam, J., Held, D., Tamar, A., and Abbeel, P.
\newblock Constrained policy optimization.
\newblock \emph{CoRR}, abs/1705.10528, 2017.
\newblock URL \url{http://arxiv.org/abs/1705.10528}.

\bibitem[Altman(1998)]{altman1998}
Altman, E.
\newblock Constrained markov decision processes with total cost criteria:
  Lagrangian approach and dual linear program.
\newblock \emph{Mathematical methods of operations research}, 48\penalty0
  (3):\penalty0 387--417, 1998.

\bibitem[Altman(1999)]{altman1999}
Altman, E.
\newblock \emph{Constrained Markov decision processes}, volume~7.
\newblock CRC Press, 1999.

\bibitem[An et~al.(2018)An, Wang, Sun, Xu, Dai, and Zhang]{An2018PID}
An, W., Wang, H., Sun, Q., Xu, J., Dai, Q., and Zhang, L.
\newblock A pid controller approach for stochastic optimization of deep
  networks.
\newblock \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pp.\  8522--8531, 2018.

\bibitem[{\AA}str{\"o}m \& H{\"a}gglund(2006){\AA}str{\"o}m and
  H{\"a}gglund]{aastrom2006pid}
{\AA}str{\"o}m, K.~J. and H{\"a}gglund, T.
\newblock Pid control.
\newblock \emph{IEEE Control Systems Magazine}, 1066\penalty0 (033X/06), 2006.

\bibitem[Bertsekas(1976)]{bertsekas1976penalty}
Bertsekas, D.~P.
\newblock On penalty and multiplier methods for constrained minimization.
\newblock \emph{SIAM Journal on Control and Optimization}, 14\penalty0
  (2):\penalty0 216--235, 1976.

\bibitem[Bertsekas(2014)]{bertsekas2014constrained}
Bertsekas, D.~P.
\newblock \emph{Constrained optimization and Lagrange multiplier methods}.
\newblock Academic press, 2014.

\bibitem[Chow et~al.(2018)Chow, Nachum, Du{\'{e}}{\~{n}}ez{-}Guzm{\'{a}}n, and
  Ghavamzadeh]{Chow2018}
Chow, Y., Nachum, O., Du{\'{e}}{\~{n}}ez{-}Guzm{\'{a}}n, E.~A., and
  Ghavamzadeh, M.
\newblock A lyapunov-based approach to safe reinforcement learning.
\newblock \emph{CoRR}, abs/1805.07708, 2018.
\newblock URL \url{http://arxiv.org/abs/1805.07708}.

\bibitem[Chow et~al.(2019)Chow, Nachum, Faust, Ghavamzadeh, and
  Du{\'{e}}{\~{n}}ez{-}Guzm{\'{a}}n]{chow2019}
Chow, Y., Nachum, O., Faust, A., Ghavamzadeh, M., and
  Du{\'{e}}{\~{n}}ez{-}Guzm{\'{a}}n, E.~A.
\newblock Lyapunov-based safe policy optimization for continuous control.
\newblock \emph{CoRR}, abs/1901.10031, 2019.
\newblock URL \url{http://arxiv.org/abs/1901.10031}.

\bibitem[Dalal et~al.(2018)Dalal, Dvijotham, Vecer{\'{\i}}k, Hester, Paduraru,
  and Tassa]{safety-layer}
Dalal, G., Dvijotham, K., Vecer{\'{\i}}k, M., Hester, T., Paduraru, C., and
  Tassa, Y.
\newblock Safe exploration in continuous action spaces.
\newblock \emph{CoRR}, abs/1801.08757, 2018.
\newblock URL \url{http://arxiv.org/abs/1801.08757}.

\bibitem[Galbraith \& Vinter(2003)Galbraith and Vinter]{galbraith}
Galbraith, G.~N. and Vinter, R.~B.
\newblock Lipschitz continuity of optimal controls for state constrained
  problems.
\newblock \emph{SIAM Journal on Control and Optimization}, 42\penalty0
  (5):\penalty0 1727--1744, 2003.
\newblock \doi{10.1137/S0363012902404711}.
\newblock URL \url{https://doi.org/10.1137/S0363012902404711}.

\bibitem[Geibel \& Wysotzki(2011)Geibel and Wysotzki]{geibel_rl}
Geibel, P. and Wysotzki, F.
\newblock Risk-sensitive reinforcement learning applied to control under
  constraints.
\newblock \emph{CoRR}, abs/1109.2147, 2011.
\newblock URL \url{http://arxiv.org/abs/1109.2147}.

\bibitem[Gu et~al.(2016)Gu, Lillicrap, Sutskever, and Levine]{ddpg}
Gu, S., Lillicrap, T., Sutskever, I., and Levine, S.
\newblock Continuous deep q-learning with model-based acceleration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2829--2838, 2016.

\bibitem[Hestenes(1969)]{hestenes1969multiplier}
Hestenes, M.~R.
\newblock Multiplier and gradient methods.
\newblock \emph{Journal of optimization theory and applications}, 4\penalty0
  (5):\penalty0 303--320, 1969.

\bibitem[Hu \& Lessard(2017)Hu and Lessard]{hu_lessard}
Hu, B. and Lessard, L.
\newblock Control interpretations for first-order optimization methods.
\newblock \emph{CoRR}, abs/1703.01670, 2017.
\newblock URL \url{http://arxiv.org/abs/1703.01670}.

\bibitem[Isidori et~al.(1995)Isidori, Thoma, Sontag, Dickinson, Fettweis,
  Massey, and Modestino]{isidori}
Isidori, A., Thoma, M., Sontag, E.~D., Dickinson, B.~W., Fettweis, A., Massey,
  J.~L., and Modestino, J.~W.
\newblock \emph{Nonlinear Control Systems}.
\newblock Springer-Verlag, Berlin, Heidelberg, 3rd edition, 1995.
\newblock ISBN 3540199160.

\bibitem[Jaderberg et~al.(2019)Jaderberg, Czarnecki, Dunning, Marris, Lever,
  Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, et~al.]{jaderberg2019human}
Jaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda,
  A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., et~al.
\newblock Human-level performance in 3d multiplayer games with population-based
  reinforcement learning.
\newblock \emph{Science}, 364\penalty0 (6443):\penalty0 859--865, 2019.

\bibitem[Lessard et~al.(2014)Lessard, Recht, and Packard]{lessard2014analysis}
Lessard, L., Recht, B., and Packard, A.
\newblock Analysis and design of optimization algorithms via integral quadratic
  constraints, 2014.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{levine2016}
Levine, S., Finn, C., Darrell, T., and Abbeel, P.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1334--1373, 2016.

\bibitem[{Liang-Liang Xie} \& {Lei Guo}(2000){Liang-Liang Xie} and {Lei
  Guo}]{how_much_uncertainty}
{Liang-Liang Xie} and {Lei Guo}.
\newblock How much uncertainty can be dealt with by feedback?
\newblock \emph{IEEE Transactions on Automatic Control}, 45\penalty0
  (12):\penalty0 2203--2217, Dec 2000.
\newblock ISSN 2334-3303.
\newblock \doi{10.1109/9.895559}.

\bibitem[Liu \& Theodorou(2019)Liu and Theodorou]{liu2019deep}
Liu, G.-H. and Theodorou, E.~A.
\newblock Deep learning theory review: An optimal control and dynamical systems
  perspective, 2019.

\bibitem[Liu et~al.(2019)Liu, Ding, and Liu]{liu2019ipo}
Liu, Y., Ding, J., and Liu, X.
\newblock Ipo: Interior-point policy optimization under constraints.
\newblock \emph{arXiv preprint arXiv:1910.09615}, 2019.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{DQN}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., and Riedmiller, M.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Nishihara et~al.(2015)Nishihara, Lessard, Recht, Packard, and
  Jordan]{nishihara2015general}
Nishihara, R., Lessard, L., Recht, B., Packard, A., and Jordan, M.~I.
\newblock A general analysis of the convergence of admm, 2015.

\bibitem[Nocedal \& Wright(2006)Nocedal and Wright]{nocedal2006numerical}
Nocedal, J. and Wright, S.
\newblock \emph{Numerical optimization}.
\newblock Springer Science \& Business Media, 2006.

\bibitem[OpenAI(2018)]{OpenAI_dota}
OpenAI.
\newblock Openai five.
\newblock \url{https://blog.openai.com/openai-five/}, 2018.

\bibitem[Paternain et~al.(2019)Paternain, Chamon, Calvo-Fullana, and
  Ribeiro]{paternain_duality_gap}
Paternain, S., Chamon, L., Calvo-Fullana, M., and Ribeiro, A.
\newblock Constrained reinforcement learning has zero duality gap.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  7553--7563, 2019.

\bibitem[Platt \& Barr(1988)Platt and Barr]{platt1988constrained}
Platt, J.~C. and Barr, A.~H.
\newblock Constrained differential optimization.
\newblock In \emph{Neural Information Processing Systems}, pp.\  612--621,
  1988.

\bibitem[Powell(1969)]{powell1969method}
Powell, M.~J.
\newblock A method for nonlinear constraints in minimization problems.
\newblock \emph{Optimization}, pp.\  283--298, 1969.

\bibitem[Ray et~al.(2019)Ray, Achiam, and Amodei]{safetygym}
Ray, A., Achiam, J., and Amodei, D.
\newblock {Benchmarking Safe Exploration in Deep Reinforcement Learning}.
\newblock 2019.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{trpo}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1889--1897, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{PPO}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Skelton(1988)]{skelton1988dynamic}
Skelton, R.
\newblock \emph{Dynamic Systems Control: Linear Systems Analysis and
  Synthesis}.
\newblock Dynamic Systems Control. John Wiley \& Sons, 1988.
\newblock ISBN 9780471837794.
\newblock URL \url{https://books.google.com/books?id=egFRAAAAMAAJ}.

\bibitem[Song \& Leland(1998)Song and Leland]{Song1998}
Song, Q. and Leland, R.~P.
\newblock An optimal control model of neural networks for constrained
  optimization problems.
\newblock \emph{Optimal Control Applications and Methods}, 19\penalty0
  (5):\penalty0 371--376, 1998.
\newblock
  \doi{10.1002/(SICI)1099-1514(199809/10)19:5<371::AID-OCA636>3.0.CO;2-8}.
\newblock URL
  \url{https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291099-1514%28199809/10%2919%3A5%3C371%3A%3AAID-OCA636%3E3.0.CO%3B2-8}.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{suttonbarto}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Introduction to Reinforcement Learning}.
\newblock MIT Press, Cambridge, MA, USA, 1st edition, 1998.
\newblock ISBN 0262193981.

\bibitem[Tessler et~al.(2018)Tessler, Mankowitz, and Mannor]{rcpo}
Tessler, C., Mankowitz, D.~J., and Mannor, S.
\newblock Reward constrained policy optimization.
\newblock \emph{CoRR}, abs/1805.11074, 2018.
\newblock URL \url{http://arxiv.org/abs/1805.11074}.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pp.\  5026--5033. IEEE, 2012.

\bibitem[Wah et~al.(2000)Wah, Wang, Shang, and Wu]{wah2000improving}
Wah, B.~W., Wang, T., Shang, Y., and Wu, Z.
\newblock Improving the performance of weighted lagrange-multiplier methods for
  nonlinear constrained optimization.
\newblock \emph{Information Sciences}, 124\penalty0 (1-4):\penalty0 241--272,
  2000.

\bibitem[Yang et~al.(2020)Yang, Rosca, Narasimhan, and
  Ramadge]{Yang2020Projection-Based}
Yang, T.-Y., Rosca, J., Narasimhan, K., and Ramadge, P.~J.
\newblock Projection-based constrained policy optimization.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rke3TJrtPS}.

\end{thebibliography}
