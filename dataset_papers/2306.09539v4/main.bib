@Article{HochSchm97,
  author      = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  number      = {8},
  pages       = {1735--1780},
  volume      = {9},
  optabstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  optdoi      = {10.1162/neco.1997.9.8.1735},
  opteprint   = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
  opturl      = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
}

@article{journals/ijufks/Hochreiter98,
  added-at = {2018-11-14T00:00:00.000+0100},
  author = {Hochreiter, Sepp},
  biburl = {https://www.bibsonomy.org/bibtex/2089a43b595b108d564dcbe28efa7163a/dblp},
  ee = {https://www.wikidata.org/entity/Q56621169},
  interhash = {45ffe6eabca767f56a3984c4d58d706e},
  intrahash = {089a43b595b108d564dcbe28efa7163a},
  journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  keywords = {dblp},
  number = 2,
  pages = {107-116},
  timestamp = {2018-11-15T15:25:14.000+0100},
  title = {The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions.},
  url = {http://dblp.uni-trier.de/db/journals/ijufks/ijufks6.html#Hochreiter98},
  volume = 6,
  year = 1998
}


@article{DBLP:journals/corr/abs-1805-04623,
  author       = {Urvashi Khandelwal and
                  He He and
                  Peng Qi and
                  Dan Jurafsky},
  title        = {Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context},
  journal      = {CoRR},
  volume       = {abs/1805.04623},
  year         = {2018},
  url          = {http://arxiv.org/abs/1805.04623},
  eprinttype    = {arXiv},
  eprint       = {1805.04623},
  timestamp    = {Tue, 09 Feb 2021 15:29:33 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1805-04623.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{zhang2022opt,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{tay2020long,
      title={Long Range Arena: A Benchmark for Efficient Transformers}, 
      author={Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
      year={2020},
      eprint={2011.04006},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{CooleyTukey,
  added-at = {2008-03-11T21:08:33.000+0100},
  author = {Cooley, James and Tukey, John},
  biburl = {https://www.bibsonomy.org/bibtex/2fff11135afc0f5d727d1b72fd8b3b199/voland},
  interhash = {2187c52cd053e20d97b4552b1de66d01},
  intrahash = {fff11135afc0f5d727d1b72fd8b3b199},
  journal = {Mathematics of Computation},
  keywords = {Fourier algorithm fft transform},
  number = 90,
  pages = {297-301},
  timestamp = {2008-03-11T21:08:33.000+0100},
  title = {An Algorithm for the Machine Calculation of Complex Fourier Series},
  volume = 19,
  year = 1965
}

@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@misc{gu2020hippo,
      title={HiPPO: Recurrent Memory with Optimal Polynomial Projections}, 
      author={Albert Gu and Tri Dao and Stefano Ermon and Atri Rudra and Christopher Re},
      year={2020},
      eprint={2008.07669},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{fu2023hungry,
      title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models}, 
      author={Daniel Y. Fu and Tri Dao and Khaled K. Saab and Armin W. Thomas and Atri Rudra and Christopher Ré},
      year={2023},
      eprint={2212.14052},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{fu2023simple,
      title={Simple Hardware-Efficient Long Convolutions for Sequence Modeling}, 
      author={Daniel Y. Fu and Elliot L. Epstein and Eric Nguyen and Armin W. Thomas and Michael Zhang and Tri Dao and Atri Rudra and Christopher Ré},
      year={2023},
      eprint={2302.06646},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{poli2023hyena,
      title={Hyena Hierarchy: Towards Larger Convolutional Language Models}, 
      author={Michael Poli and Stefano Massaroli and Eric Nguyen and Daniel Y. Fu and Tri Dao and Stephen Baccus and Yoshua Bengio and Stefano Ermon and Christopher Ré},
      year={2023},
      eprint={2302.10866},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gu2022efficiently,
      title={Efficiently Modeling Long Sequences with Structured State Spaces}, 
      author={Albert Gu and Karan Goel and Christopher Ré},
      year={2022},
      eprint={2111.00396},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{smith2023simplified,
      title={Simplified State Space Layers for Sequence Modeling}, 
      author={Jimmy T. H. Smith and Andrew Warrington and Scott W. Linderman},
      year={2023},
      eprint={2208.04933},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gu2022parameterization,
      title={On the Parameterization and Initialization of Diagonal State Space Models}, 
      author={Albert Gu and Ankit Gupta and Karan Goel and Christopher Ré},
      year={2022},
      eprint={2206.11893},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gupta2022diagonal,
      title={Diagonal State Spaces are as Effective as Structured State Spaces}, 
      author={Ankit Gupta and Albert Gu and Jonathan Berant},
      year={2022},
      eprint={2203.14343},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{
hutchins2022block,
title={Block-Recurrent Transformers},
author={DeLesley Hutchins and Imanol Schlag and Yuhuai Wu and Ethan Dyer and Behnam Neyshabur},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=uloenYmLCAo}
}

@inproceedings{
mehta2023long,
title={Long Range Language Modeling via Gated State Spaces},
author={Harsh Mehta and Ankit Gupta and Ashok Cutkosky and Behnam Neyshabur},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=5MkYIYCbva}
}

@inproceedings{pg19_raecompressive2019,
 author = {Jack W. Rae and
Anna Potapenko and
Siddhant M. Jayakumar and
Chloe Hillier and
Timothy P. Lillicrap},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/RaePJHL20.bib},
 booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
 publisher = {OpenReview.net},
 timestamp = {Thu, 07 May 2020 01:00:00 +0200},
 title = {Compressive Transformers for Long-Range Sequence Modelling},
 url = {https://openreview.net/forum?id=SylKikSYDH},
 year = {2020}
}

@article{raffel2019exploring,
 author = {Colin Raffel and
Noam Shazeer and
Adam Roberts and
Katherine Lee and
Sharan Narang and
Michael Matena and
Yanqi Zhou and
Wei Li and
Peter J. Liu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/jmlr/RaffelSRLNMZLL20.bib},
 journal = {J. Mach. Learn. Res.},
 pages = {140:1--140:67},
 timestamp = {Fri, 05 Feb 2021 15:43:41 +0100},
 title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
Transformer},
 url = {http://jmlr.org/papers/v21/20-074.html},
 volume = {21},
 year = {2020}
}

@article{Hendrycks2016BridgingNA,
 author = {Dan Hendrycks and Kevin Gimpel},
 journal = {ArXiv preprint},
 title = {Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units},
 url = {https://arxiv.org/abs/1606.08415},
 volume = {abs/1606.08415},
 year = {2016}
}

@article{meena_adiwardana2020humanlike,
  title={Towards a Human-like Open-Domain Chatbot},
  author={Daniel Adiwardana and Minh-Thang Luong and David R. So and Jamie Hall and Noah Fiedel and Romal Thoppilan and Zi Yang and Apoorv Kulshreshtha and Gaurav Nemade and Yifeng Lu and Quoc V. Le},
  journal={ArXiv},
  year={2020},
  volume={abs/2001.09977}
}

@inproceedings{wu2022memorizing,
 author = {Yuhuai Wu and Markus Norman Rabe and DeLesley Hutchins and Christian Szegedy},
 booktitle = {International Conference on Learning Representations},
 title = {Memorizing Transformers},
 url = {https://openreview.net/forum?id=TrjbxzRcnf-},
 year = {2022}
}

@article{t5,
  author       = {Colin Raffel and
                  Noam Shazeer and
                  Adam Roberts and
                  Katherine Lee and
                  Sharan Narang and
                  Michael Matena and
                  Yanqi Zhou and
                  Wei Li and
                  Peter J. Liu},
  title        = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
                  Transformer},
  journal      = {CoRR},
  volume       = {abs/1910.10683},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.10683},
  eprinttype    = {arXiv},
  eprint       = {1910.10683},
  timestamp    = {Fri, 05 Feb 2021 15:43:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-10683.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@article{lamda,
  author       = {Romal Thoppilan and
                  Daniel De Freitas and
                  Jamie Hall and
                  Noam Shazeer and
                  Apoorv Kulshreshtha and
                  Heng{-}Tze Cheng and
                  Alicia Jin and
                  Taylor Bos and
                  Leslie Baker and
                  Yu Du and
                  YaGuang Li and
                  Hongrae Lee and
                  Huaixiu Steven Zheng and
                  Amin Ghafouri and
                  Marcelo Menegali and
                  Yanping Huang and
                  Maxim Krikun and
                  Dmitry Lepikhin and
                  James Qin and
                  Dehao Chen and
                  Yuanzhong Xu and
                  Zhifeng Chen and
                  Adam Roberts and
                  Maarten Bosma and
                  Yanqi Zhou and
                  Chung{-}Ching Chang and
                  Igor Krivokon and
                  Will Rusch and
                  Marc Pickett and
                  Kathleen S. Meier{-}Hellstern and
                  Meredith Ringel Morris and
                  Tulsee Doshi and
                  Renelito Delos Santos and
                  Toju Duke and
                  Johnny Soraker and
                  Ben Zevenbergen and
                  Vinodkumar Prabhakaran and
                  Mark Diaz and
                  Ben Hutchinson and
                  Kristen Olson and
                  Alejandra Molina and
                  Erin Hoffman{-}John and
                  Josh Lee and
                  Lora Aroyo and
                  Ravi Rajakumar and
                  Alena Butryna and
                  Matthew Lamm and
                  Viktoriya Kuzmina and
                  Joe Fenton and
                  Aaron Cohen and
                  Rachel Bernstein and
                  Ray Kurzweil and
                  Blaise Aguera{-}Arcas and
                  Claire Cui and
                  Marian Croak and
                  Ed H. Chi and
                  Quoc Le},
  title        = {LaMDA: Language Models for Dialog Applications},
  journal      = {CoRR},
  volume       = {abs/2201.08239},
  year         = {2022},
  url          = {https://arxiv.org/abs/2201.08239},
  eprinttype    = {arXiv},
  eprint       = {2201.08239},
  timestamp    = {Fri, 22 Apr 2022 16:06:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2201-08239.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{chowdhery2022palm,
      title={PaLM: Scaling Language Modeling with Pathways}, 
      author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
      year={2022},
      eprint={2204.02311},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{gpt3,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  title        = {Language Models are Few-Shot Learners},
  journal      = {CoRR},
  volume       = {abs/2005.14165},
  year         = {2020},
  url          = {https://arxiv.org/abs/2005.14165},
  eprinttype    = {arXiv},
  eprint       = {2005.14165},
  timestamp    = {Wed, 03 Jun 2020 11:36:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{sparse_trsf,
  author       = {Rewon Child and
                  Scott Gray and
                  Alec Radford and
                  Ilya Sutskever},
  title        = {Generating Long Sequences with Sparse Transformers},
  journal      = {CoRR},
  volume       = {abs/1904.10509},
  year         = {2019},
  url          = {http://arxiv.org/abs/1904.10509},
  eprinttype    = {arXiv},
  eprint       = {1904.10509},
  timestamp    = {Thu, 02 May 2019 15:13:44 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1904-10509.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{zaheer2020bigbird,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{linformer,
  author       = {Sinong Wang and
                  Belinda Z. Li and
                  Madian Khabsa and
                  Han Fang and
                  Hao Ma},
  title        = {Linformer: Self-Attention with Linear Complexity},
  journal      = {CoRR},
  volume       = {abs/2006.04768},
  year         = {2020},
  url          = {https://arxiv.org/abs/2006.04768},
  eprinttype    = {arXiv},
  eprint       = {2006.04768},
  timestamp    = {Mon, 06 Feb 2023 11:49:42 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2006-04768.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{sinkhorn_trsf,
  title = 	 {Sparse {S}inkhorn Attention},
  author =       {Tay, Yi and Bahri, Dara and Yang, Liu and Metzler, Donald and Juan, Da-Cheng},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {9438--9447},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/tay20a/tay20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/tay20a.html},
  abstract = 	 {We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers.}
}


@article{performer,
  author       = {Krzysztof Choromanski and
                  Valerii Likhosherstov and
                  David Dohan and
                  Xingyou Song and
                  Andreea Gane and
                  Tam{\'{a}}s Sarl{\'{o}}s and
                  Peter Hawkins and
                  Jared Davis and
                  Afroz Mohiuddin and
                  Lukasz Kaiser and
                  David Belanger and
                  Lucy J. Colwell and
                  Adrian Weller},
  title        = {Rethinking Attention with Performers},
  journal      = {CoRR},
  volume       = {abs/2009.14794},
  year         = {2020},
  url          = {https://arxiv.org/abs/2009.14794},
  eprinttype    = {arXiv},
  eprint       = {2009.14794},
  timestamp    = {Wed, 23 Jun 2021 10:58:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2009-14794.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{linear_trsf,
  title = 	 {Transformers are {RNN}s: Fast Autoregressive Transformers with Linear Attention},
  author =       {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {5156--5165},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/katharopoulos20a/katharopoulos20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/katharopoulos20a.html},
  abstract = 	 {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input’s length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\bigO{N^2}$ to $\bigO{N}$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our \emph{Linear Transformers} achieve similar performance to vanilla Transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.}
}

@software{flax2020github,
  author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
  title = {{F}lax: A neural network library and ecosystem for {JAX}},
  url = {http://github.com/google/flax},
  version = {0.6.9},
  year = {2023},
}

@inproceedings{2015-kingma,
  added-at = {2021-11-20T12:34:31.000+0100},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  biburl = {https://www.bibsonomy.org/bibtex/2cf6db5e77b957676e0cbbd221f16c0fc/jaymt},
  booktitle = {ICLR (Poster)},
  editor = {Bengio, Yoshua and LeCun, Yann},
  ee = {http://arxiv.org/abs/1412.6980},
  interhash = {c14f3bd32b4636eff1d0234f08025bd5},
  intrahash = {cf6db5e77b957676e0cbbd221f16c0fc},
  keywords = {final thema:attentionisallyouneed},
  timestamp = {2021-11-20T12:34:31.000+0100},
  title = {Adam: A Method for Stochastic Optimization.},
  url = {http://dblp.uni-trier.de/db/conf/iclr/iclr2015.html#KingmaB14},
  year = 2015
}

@book{chihara2011introduction,
  title={An Introduction to Orthogonal Polynomials},
  author={Chihara, T.S.},
  isbn={9780486479293},
  lccn={2010043412},
  series={Dover Books on Mathematics},
  url={https://books.google.ca/books?id=71CVAwAAQBAJ},
  year={2011},
  publisher={Dover Publications}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{bert,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  journal      = {CoRR},
  volume       = {abs/1810.04805},
  year         = {2018},
  url          = {http://arxiv.org/abs/1810.04805},
  eprinttype    = {arXiv},
  eprint       = {1810.04805},
  timestamp    = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{transformer,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}


@InProceedings{pmlr-v162-hua22a,
  title = 	 {Transformer Quality in Linear Time},
  author =       {Hua, Weizhe and Dai, Zihang and Liu, Hanxiao and Le, Quoc},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {9099--9117},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/hua22a/hua22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/hua22a.html},
  abstract = 	 {We revisit the design choices in Transformers, and propose methods to address their weaknesses in handling long sequences. First, we propose a simple layer named gated attention unit, which allows the use of a weaker single-head attention with minimal quality loss. We then propose a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality. The resulting model, named FLASH, matches the perplexity of improved Transformers over both short (512) and long (8K) context lengths, achieving training speedups of up to 4.9x on Wiki-40B and 12.1x on PG-19 for auto-regressive language modeling, and 4.8x on C4 for masked language modeling.}
}

@misc{ma2023mega,
      title={Mega: Moving Average Equipped Gated Attention}, 
      author={Xuezhe Ma and Chunting Zhou and Xiang Kong and Junxian He and Liangke Gui and Graham Neubig and Jonathan May and Luke Zettlemoyer},
      year={2023},
      eprint={2209.10655},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
li2022the,
title={The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training {GPT} Models},
author={Conglong Li and Minjia Zhang and Yuxiong He},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=JpZ5du_Kdh}
}

@article{DBLP:journals/corr/abs-2005-00581,
  author       = {Sandeep Subramanian and
                  Ronan Collobert and
                  Marc'Aurelio Ranzato and
                  Y{-}Lan Boureau},
  title        = {Multi-scale Transformer Language Models},
  journal      = {CoRR},
  volume       = {abs/2005.00581},
  year         = {2020},
  url          = {https://arxiv.org/abs/2005.00581},
  eprinttype    = {arXiv},
  eprint       = {2005.00581},
  timestamp    = {Fri, 08 May 2020 15:04:04 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2005-00581.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
tay2021long,
title={Long Range Arena : A Benchmark for Efficient Transformers },
author={Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=qVyeW-grC2k}
}

@inproceedings{
gu2022on,
title={On the Parameterization and Initialization of Diagonal State Space Models},
author={Albert Gu and Karan Goel and Ankit Gupta and Christopher R{\'e}},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=yJE7iQSAep}
}

@InProceedings{KingBa15,
  author    = {Kingma, Diederik and Ba, Jimmy},
  booktitle = {International Conference on Learning Representations (ICLR)},
  title     = {Adam: A Method for Stochastic Optimization},
  year      = {2015},
  address   = {San Diega, CA, USA},
  optmonth  = {12},
}