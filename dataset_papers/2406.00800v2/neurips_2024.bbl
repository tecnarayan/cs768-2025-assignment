\begin{thebibliography}{10}

\bibitem{adepu2024framequant}
Harshavardhan Adepu, Zhanpeng Zeng, Li~Zhang, and Vikas Singh.
\newblock Framequant: Flexible low-bit quantization for transformers.
\newblock {\em arXiv preprint arXiv:2403.06082}, 2024.

\bibitem{alizadeh2020gradient}
Milad Alizadeh, Arash Behboodi, Mart Van~Baalen, Christos Louizos, Tijmen Blankevoort, and Max Welling.
\newblock Gradient $\ell_1 $ regularization for quantization robustness.
\newblock {\em arXiv preprint arXiv:2002.07520}, 2020.

\bibitem{aminabadi2022deepspeed}
Reza~Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar~Ahmad Awan, Cheng Li, Du~Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, and Yuxiong He.
\newblock Deepspeed inference: Enabling efficient inference of transformer models at unprecedented scale, 2022.

\bibitem{behdin2023quantease}
Kayhan Behdin, Ayan Acharya, Aman Gupta, Sathiya Keerthi, and Rahul Mazumder.
\newblock Quantease: Optimization-based quantization for language models--an efficient and intuitive algorithm.
\newblock {\em arXiv preprint arXiv:2309.01885}, 2023.

\bibitem{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~34, pages 7432--7439, 2020.

\bibitem{boyd2004convex}
Stephen Boyd and Lieven Vandenberghe.
\newblock {\em Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem{cai2017deep}
Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos.
\newblock Deep learning with low precision by half-wave gaussian quantization.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 5918--5926, 2017.

\bibitem{chee2024quip}
Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher~M De~Sa.
\newblock Quip: 2-bit quantization of large language models with guarantees.
\newblock In {\em Advances in Neural Information Processing Systems}, 2023.

\bibitem{choi2018pact}
Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan.
\newblock Pact: Parameterized clipping activation for quantized neural networks.
\newblock {\em arXiv preprint arXiv:1805.06085}, 2018.

\bibitem{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock {\em arXiv preprint arXiv:1803.05457}, 2018.

\bibitem{condat2016fast}
Laurent Condat.
\newblock Fast projection onto the simplex and the $\ell_1$ ball.
\newblock {\em Mathematical Programming}, 158(1):575--585, 2016.

\bibitem{courbariaux2015binaryconnect}
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.
\newblock Binaryconnect: Training deep neural networks with binary weights during propagations.
\newblock {\em Advances in neural information processing systems}, 28, 2015.

\bibitem{czaja2024frame}
Wojciech Czaja and Sanghoon Na.
\newblock Frame quantization of neural networks.
\newblock {\em arXiv preprint arXiv:2404.08131}, 2024.

\bibitem{duchi2008efficient}
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.
\newblock Efficient projections onto the l 1-ball for learning in high dimensions.
\newblock In {\em Proceedings of the 25th international conference on Machine learning}, pages 272--279, 2008.

\bibitem{frantar2022optimal}
Elias Frantar and Dan Alistarh.
\newblock Optimal brain compression: A framework for accurate post-training quantization and pruning.
\newblock {\em Advances in Neural Information Processing Systems}, 35:4475--4488, 2022.

\bibitem{frantar2022optq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock Optq: Accurate quantization for generative pre-trained transformers.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2022.

\bibitem{eval-harness}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le~Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
\newblock A framework for few-shot language model evaluation, 2023.

\bibitem{hubara2018quantized}
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio.
\newblock Quantized neural networks: Training neural networks with low precision weights and activations.
\newblock {\em Journal of Machine Learning Research}, 18(187):1--30, 2018.

\bibitem{kim2023stack}
Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael~W. Mahoney, Yakun~Sophia Shao, and Amir Gholami.
\newblock Full stack optimization of transformer inference: a survey, 2023.

\bibitem{kundu2023r2}
Arnav Kundu, Chungkuk Yoo, Srijan Mishra, Minsik Cho, and Saurabh Adya.
\newblock R2 loss: Range restriction loss for model compression and quantization.
\newblock {\em arXiv preprint arXiv:2303.08253}, 2023.

\bibitem{li2016ternary}
Fengfu Li, Bin Liu, Xiaoxing Wang, Bo~Zhang, and Junchi Yan.
\newblock Ternary weight networks.
\newblock {\em arXiv preprint arXiv:1605.04711}, 2016.

\bibitem{li2021brecq}
Yuhang Li, Ruihao Gong, Xu~Tan, Yang Yang, Peng Hu, Qi~Zhang, Fengwei Yu, Wei Wang, and Shi Gu.
\newblock Brecq: Pushing the limit of post-training quantization by block reconstruction.
\newblock {\em arXiv preprint arXiv:2102.05426}, 2021.

\bibitem{li2023feature}
Zhijian Li, Biao Yang, Penghang Yin, Yingyong Qi, and Jack Xin.
\newblock Feature affinity assisted knowledge distillation and quantization of deep neural networks on label-free data.
\newblock {\em IEEE Access}, 2023.

\bibitem{lin2023bit}
Chen Lin, Bo~Peng, Zheyang Li, Wenming Tan, Ye~Ren, Jun Xiao, and Shiliang Pu.
\newblock Bit-shrinking: Limiting instantaneous sharpness for improving post-training quantization.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 16196--16205, 2023.

\bibitem{lin2023awq}
Ji~Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.
\newblock Awq: Activation-aware weight quantization for llm compression and acceleration.
\newblock {\em arXiv preprint arXiv:2306.00978}, 2023.

\bibitem{ma2024affinequant}
Yuexiao Ma, Huixia Li, Xiawu Zheng, Feng Ling, Xuefeng Xiao, Rui Wang, Shilei Wen, Fei Chao, and Rongrong Ji.
\newblock Affinequant: Affine transformation quantization for large language models.
\newblock {\em arXiv preprint arXiv:2403.12544}, 2024.

\bibitem{maly2023simple}
Johannes Maly and Rayan Saab.
\newblock A simple approach for quantizing neural networks.
\newblock {\em Applied and Computational Harmonic Analysis}, 66:138--150, 2023.

\bibitem{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock {\em arXiv preprint arXiv:1609.07843}, 2016.

\bibitem{moreau1962decomposition}
Jean~Jacques Moreau.
\newblock D{\'e}composition orthogonale d'un espace hilbertien selon deux c{\^o}nes mutuellement polaires.
\newblock {\em Comptes rendus hebdomadaires des s{\'e}ances de l'Acad{\'e}mie des sciences}, 255:238--240, 1962.

\bibitem{nagel2020up}
Markus Nagel, Rana~Ali Amjad, Mart Van~Baalen, Christos Louizos, and Tijmen Blankevoort.
\newblock Up or down? adaptive rounding for post-training quantization.
\newblock In {\em International Conference on Machine Learning}, pages 7197--7206. PMLR, 2020.

\bibitem{nagel2019data}
Markus Nagel, Mart~van Baalen, Tijmen Blankevoort, and Max Welling.
\newblock Data-free quantization through weight equalization and bias correction.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 1325--1334, 2019.

\bibitem{parikh2014proximal}
Neal Parikh, Stephen Boyd, et~al.
\newblock Proximal algorithms.
\newblock {\em Foundations and trends{\textregistered} in Optimization}, 1(3):127--239, 2014.

\bibitem{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock {\em Journal of machine learning research}, 21(140):1--67, 2020.

\bibitem{rastegari2016xnor}
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.
\newblock Xnor-net: Imagenet classification using binary convolutional neural networks.
\newblock In {\em European conference on computer vision}, pages 525--542. Springer, 2016.

\bibitem{sakaguchi2021winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock {\em Communications of the ACM}, 64(9):99--106, 2021.

\bibitem{shao2023omniquant}
Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu~Qiao, and Ping Luo.
\newblock Omniquant: Omnidirectionally calibrated quantization for large language models.
\newblock {\em arXiv preprint arXiv:2308.13137}, 2023.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{tseng2024quip}
Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De~Sa.
\newblock Quip\#: Even better llm quantization with hadamard incoherence and lattice codebooks.
\newblock {\em arXiv preprint arXiv:2402.04396}, 2024.

\bibitem{wang2024quest}
Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, and Yan Yan.
\newblock Quest: Low-bit diffusion model quantization via efficient selective finetuning.
\newblock {\em arXiv preprint arXiv:2402.03666}, 2024.

\bibitem{wang2022deep}
Naigang Wang, Chi-Chun~Charlie Liu, Swagath Venkataramani, Sanchari Sen, Chia-Yu Chen, Kaoutar El~Maghraoui, Vijayalakshmi~Viji Srinivasan, and Leland Chang.
\newblock Deep compression of pre-trained transformer models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:14140--14154, 2022.

\bibitem{wei2023outlier}
Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu.
\newblock Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling.
\newblock {\em arXiv preprint arXiv:2304.09145}, 2023.

\bibitem{wu2024ptq4dit}
Junyi Wu, Haoxuan Wang, Yuzhang Shang, Mubarak Shah, and Yan Yan.
\newblock Ptq4dit: Post-training quantization for diffusion transformers.
\newblock {\em arXiv preprint arXiv:2405.16005}, 2024.

\bibitem{xiao2023smoothquant}
Guangxuan Xiao, Ji~Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.
\newblock Smoothquant: Accurate and efficient post-training quantization for large language models.
\newblock In {\em International Conference on Machine Learning}, pages 38087--38099. PMLR, 2023.

\bibitem{yao2024exploring}
Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He.
\newblock Exploring post-training quantization in llms from comprehensive study to low rank compensation.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pages 19377--19385, 2024.

\bibitem{yin2019understanding}
Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack Xin.
\newblock Understanding straight-through estimator in training activation quantized neural nets.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{yin2018binaryrelax}
Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, and Jack Xin.
\newblock Binaryrelax: A relaxation approach for training deep neural networks with quantized weights.
\newblock {\em SIAM Journal on Imaging Sciences}, 11(4):2205--2223, 2018.

\bibitem{yin2019blended}
Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, and Jack Xin.
\newblock Blended coarse gradient descent for full quantization of deep neural networks.
\newblock {\em Research in the Mathematical Sciences}, 6:1--23, 2019.

\bibitem{yin2016quantization}
Penghang Yin, Shuai Zhang, Yingyong Qi, and Jack Xin.
\newblock Quantization and training of low bit-width convolutional neural networks for object detection.
\newblock {\em arXiv preprint arXiv:1612.06052}, 2016.

\bibitem{yin2008bregman}
Wotao Yin, Stanley Osher, Donald Goldfarb, and Jerome Darbon.
\newblock Bregman iterative algorithms for $\ell_1$-minimization with applications to compressed sensing.
\newblock {\em SIAM Journal on Imaging sciences}, 1(1):143--168, 2008.

\bibitem{zhang2024comq}
Aozhong Zhang, Zi~Yang, Naigang Wang, Yingyong Qin, Jack Xin, Xin Li, and Penghang Yin.
\newblock Comq: A backpropagation-free algorithm for post-training quantization.
\newblock {\em arXiv preprint arXiv:2403.07134}, 2024.

\bibitem{zhang2023spfq}
Jinjie Zhang and Rayan Saab.
\newblock Spfq: A stochastic algorithm and its error analysis for neural network quantization.
\newblock {\em arXiv preprint arXiv:2309.10975}, 2023.

\bibitem{zhang2023post}
Jinjie Zhang, Yixuan Zhou, and Rayan Saab.
\newblock Post-training quantization for neural networks with provable guarantees.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 5(2):373--399, 2023.

\bibitem{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock {\em arXiv preprint arXiv:2205.01068}, 2022.

\end{thebibliography}
