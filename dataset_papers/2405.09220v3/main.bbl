\begin{thebibliography}{10}

\bibitem{Achiam2023GPT4TR}
O.~J. Achiam, S.~Adler, and et~al.
\newblock {GPT-4} technical report.
\newblock 2023.

\bibitem{allen2023physics}
Z.~Allen-Zhu and Y.~Li.
\newblock Physics of language models: Part 1, context-free grammar.
\newblock {\em arXiv preprint arXiv:2305.13673}, 2023.

\bibitem{zhu2023physics}
Z.~Allen-Zhu and Y.~Li.
\newblock Physics of language models: Part 3.1, knowledge storage and extraction.
\newblock {\em arXiv preprint arXiv:2309.14316}, 2023.

\bibitem{allenzhu2023physics}
Z.~Allen-Zhu and Y.~Li.
\newblock Physics of language models: Part 3.2, knowledge manipulation.
\newblock {\em arXiv preprint arXiv:2309.14402}, 2023.

\bibitem{zhu2024physics}
Z.~Allen-Zhu and Y.~Li.
\newblock Physics of language models: Part 3.3, knowledge capacity scaling laws.
\newblock {\em arXiv preprint arXiv:2404.05405}, 2024.

\bibitem{chai2023graphllm}
Z.~Chai, T.~Zhang, L.~Wu, K.~Han, X.~Hu, X.~Huang, and Y.~Yang.
\newblock {GraphLLM}: Boosting graph reasoning ability of large language model.
\newblock {\em arXiv preprint arXiv:2310.05845}, 2023.

\bibitem{feng2024towards}
G.~Feng, B.~Zhang, Y.~Gu, H.~Ye, D.~He, and L.~Wang.
\newblock Towards revealing the mystery behind chain of thought: a theoretical perspective.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{giannou2023looped}
A.~Giannou, S.~Rajput, J.-y. Sohn, K.~Lee, J.~D. Lee, and D.~Papailiopoulos.
\newblock Looped transformers as programmable computers.
\newblock In {\em International Conference on Machine Learning}, pages 11398--11442. PMLR, 2023.

\bibitem{guo2023gpt4graph}
J.~Guo, L.~Du, and H.~Liu.
\newblock Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking.
\newblock {\em arXiv preprint arXiv:2305.15066}, 2023.

\bibitem{huang2022language}
W.~Huang, P.~Abbeel, D.~Pathak, and I.~Mordatch.
\newblock Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.
\newblock In {\em International Conference on Machine Learning}, pages 9118--9147. PMLR, 2022.

\bibitem{lee2023teaching}
N.~Lee, K.~Sreenivasan, J.~D. Lee, K.~Lee, and D.~Papailiopoulos.
\newblock Teaching arithmetic to small transformers.
\newblock {\em arXiv preprint arXiv:2307.03381}, 2023.

\bibitem{liu2024toolnet}
X.~Liu, Z.~Peng, X.~Yi, X.~Xie, L.~Xiang, Y.~Liu, and D.~Xu.
\newblock {ToolNet}: Connecting large language models with massive tools via tool graph.
\newblock {\em arXiv preprint arXiv:2403.00839}, 2024.

\bibitem{liu2023controlllm}
Z.~Liu, Z.~Lai, Z.~Gao, E.~Cui, Z.~Li, X.~Zhu, L.~Lu, Q.~Chen, Y.~Qiao, J.~Dai, et~al.
\newblock {ControlLLM}: Augment language models with tools by searching on graphs.
\newblock {\em arXiv preprint arXiv:2310.17796}, 2023.

\bibitem{luo2024graphinstruct}
Z.~Luo, X.~Song, H.~Huang, J.~Lian, C.~Zhang, J.~Jiang, X.~Xie, and H.~Jin.
\newblock Graphinstruct: Empowering large language models with graph understanding and reasoning capability.
\newblock {\em arXiv preprint arXiv:2403.04483}, 2024.

\bibitem{merrill2023parallelism}
W.~Merrill and A.~Sabharwal.
\newblock The parallelism tradeoff: Limitations of log-precision transformers.
\newblock {\em Transactions of the Association for Computational Linguistics}, 11:531--545, 2023.

\bibitem{momennejad2024evaluating}
I.~Momennejad, H.~Hasanbeig, F.~Vieira~Frujeri, H.~Sharma, N.~Jojic, H.~Palangi, R.~Ness, and J.~Larson.
\newblock Evaluating cognitive maps and planning in large language models with {CogEval}.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{radford2018improving}
A.~Radford, K.~Narasimhan, T.~Salimans, I.~Sutskever, et~al.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem{shen2024hugginggpt}
Y.~Shen, K.~Song, X.~Tan, D.~Li, W.~Lu, and Y.~Zhuang.
\newblock {HuggingGPT}: Solving {AI} tasks with {ChatGPT} and its friends in {Huggingface}.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{shinn2024reflexion}
N.~Shinn, F.~Cassano, A.~Gopinath, K.~Narasimhan, and S.~Yao.
\newblock Reflexion: Language agents with verbal reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{tang2023graphgpt}
J.~Tang, Y.~Yang, W.~Wei, L.~Shi, L.~Su, S.~Cheng, D.~Yin, and C.~Huang.
\newblock {GraphGPT}: Graph instruction tuning for large language models.
\newblock {\em arXiv preprint arXiv:2310.13023}, 2023.

\bibitem{trinh2024solving}
T.~H. Trinh, Y.~Wu, Q.~V. Le, H.~He, and T.~Luong.
\newblock Solving {Olympiad} geometry without human demonstrations.
\newblock {\em Nature}, 625(7995):476--482, 2024.

\bibitem{valmeekam2023planning}
K.~Valmeekam, M.~Marquez, S.~Sreedharan, and S.~Kambhampati.
\newblock On the planning abilities of large language models-a critical investigation.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{wang2023voyager}
G.~Wang, Y.~Xie, Y.~Jiang, A.~Mandlekar, C.~Xiao, Y.~Zhu, L.~Fan, and A.~Anandkumar.
\newblock Voyager: An open-ended embodied agent with large language models.
\newblock {\em arXiv preprint arXiv:2305.16291}, 2023.

\bibitem{wang2024can}
H.~Wang, S.~Feng, T.~He, Z.~Tan, X.~Han, and Y.~Tsvetkov.
\newblock Can language models solve graph problems in natural language?
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{wang2024survey}
L.~Wang, C.~Ma, X.~Feng, Z.~Zhang, H.~Yang, J.~Zhang, Z.~Chen, J.~Tang, X.~Chen, Y.~Lin, et~al.
\newblock A survey on large language model based autonomous agents.
\newblock {\em Frontiers of Computer Science}, 18(6):1--26, 2024.

\bibitem{whittington2022build}
J.~C. Whittington, D.~McCaffary, J.~J. Bakermans, and T.~E. Behrens.
\newblock How to build a cognitive map: insights from models of the hippocampal formation.
\newblock {\em arXiv preprint arXiv:2202.01682}, 2022.

\bibitem{yang2024efficient}
K.~Yang, J.~Ackermann, Z.~He, G.~Feng, B.~Zhang, Y.~Feng, Q.~Ye, D.~He, and L.~Wang.
\newblock Do efficient transformers really save computation?
\newblock {\em arXiv preprint arXiv:2402.13934}, 2024.

\bibitem{zhu2024physics21}
T.~Ye, Z.~Xu, Y.~Li, and Z.~Allen-Zhu.
\newblock Physics of language models: Part 2.1, grade-school math and the hidden reasoning process.
\newblock {\em arXiv preprint arXiv:2407.20311}, 2024.

\bibitem{zhu2024physics22}
T.~Ye, Z.~Xu, Y.~Li, and Z.~Allen-Zhu.
\newblock Physics of language models: Part 2.2, how to learn from mistakes on grade-school math problems.
\newblock {\em arXiv preprint arXiv:2408.16293}, 2024.

\bibitem{zhang2023trained}
R.~Zhang, S.~Frei, and P.~L. Bartlett.
\newblock Trained transformers learn linear models in-context.
\newblock {\em arXiv preprint arXiv:2306.09927}, 2023.

\bibitem{zhang2022unveiling}
Y.~Zhang, A.~Backurs, S.~Bubeck, R.~Eldan, S.~Gunasekar, and T.~Wagner.
\newblock Unveiling {Transformers} with {LEGO}: a synthetic reasoning task.
\newblock {\em arXiv preprint arXiv:2206.04301}, 2022.

\end{thebibliography}
