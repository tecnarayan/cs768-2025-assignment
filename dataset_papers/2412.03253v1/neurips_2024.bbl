\begin{thebibliography}{10}

\bibitem{stiennon2020learning}
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul~F Christiano.
\newblock Learning to summarize with human feedback.
\newblock {\em Advances in Neural Information Processing Systems}, 33:3008--3021, 2020.

\bibitem{huang2024acegpt}
Huang Huang, Fei Yu, Jianqing Zhu, Xuening Sun, Hao Cheng, Dingjie Song, Zhihong Chen, Abdulmohsen Alharthi, Bang An, Juncai He, Ziche Liu, Zhiyi Zhang, Junying Chen, Jianquan Li, Benyou Wang, Lian Zhang, Ruoyu Sun, Xiang Wan, Haizhou Li, and Jinchao Xu.
\newblock Acegpt, localizing large language models in arabic, 2024.

\bibitem{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock {\em Advances in neural information processing systems}, 35:27730--27744, 2022.

\bibitem{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock {\em arXiv preprint arXiv:2109.01652}, 2021.

\bibitem{zhou2024lima}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et~al.
\newblock Lima: Less is more for alignment.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{fan2024reformatted}
Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li, Shwai He, Ethan Chern, Jiewen Hu, and Pengfei Liu.
\newblock Reformatted alignment.
\newblock {\em arXiv preprint arXiv:2402.12219}, 2024.

\bibitem{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback.
\newblock {\em arXiv preprint arXiv:2204.05862}, 2022.

\bibitem{vaswani2017attention}
A~Vaswani.
\newblock Attention is all you need.
\newblock {\em Advances in Neural Information Processing Systems}, 2017.

\bibitem{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock {\em Journal of machine learning research}, 21(140):1--67, 2020.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{alpaca_eval}
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Alpacaeval: An automatic evaluator of instruction-following models.
\newblock \url{https://github.com/tatsu-lab/alpaca_eval}, 2023.

\bibitem{lee2023rlaif}
Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash.
\newblock Rlaif: Scaling reinforcement learning from human feedback with ai feedback, 2023.

\bibitem{wang2023aligning}
Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu.
\newblock Aligning large language models with human: A survey.
\newblock {\em arXiv preprint arXiv:2307.12966}, 2023.

\bibitem{rafailov2024direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano Ermon, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{meyer2012study}
Dutch~T Meyer and William~J Bolosky.
\newblock A study of practical deduplication.
\newblock {\em ACM Transactions on Storage (ToS)}, 7(4):1--20, 2012.

\bibitem{gehman2020realtoxicityprompts}
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah~A Smith.
\newblock Realtoxicityprompts: Evaluating neural toxic degeneration in language models.
\newblock {\em arXiv preprint arXiv:2009.11462}, 2020.

\bibitem{jain2024polyglotoxicityprompts}
Devansh Jain, Priyanshu Kumar, Samuel Gehman, Xuhui Zhou, Thomas Hartvigsen, and Maarten Sap.
\newblock Polyglotoxicityprompts: Multilingual evaluation of neural toxic degeneration in large language models.
\newblock {\em arXiv preprint arXiv:2405.09373}, 2024.

\bibitem{marion2023less}
Max Marion, Ahmet {\"U}st{\"u}n, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, and Sara Hooker.
\newblock When less is more: Investigating data pruning for pretraining llms at scale.
\newblock {\em arXiv preprint arXiv:2309.04564}, 2023.

\bibitem{llama3modelcard}
AI@Meta.
\newblock Llama 3 model card.
\newblock 2024.

\bibitem{farghaly2009arabic}
Ali Farghaly and Khaled Shaalan.
\newblock Arabic natural language processing: Challenges and solutions.
\newblock {\em ACM Transactions on Asian Language Information Processing (TALIP)}, 8(4):1--22, 2009.

\bibitem{koto2024arabicmmlu}
"Fajri Koto, Haonan Li, Sara Shatanawi, Jad Doughman, Abdelrahman~Boda Sadallah, Aisha Alraeesi, Khalid Almubarak, Zaid Alyafeai, Neha Sengupta, Shady Shehata, Nizar Habash, Preslav Nakov, and Timothy Baldwin".
\newblock "arabicmmlu: Assessing massive multitask language understanding in arabic", "2024".

\bibitem{hardalov-etal-2020-exams}
Momchil Hardalov, Todor Mihaylov, Dimitrina Zlatkova, Yoan Dinkov, Ivan Koychev, and Preslav Nakov.
\newblock {EXAMS}: A multi-subject high school examinations dataset for cross-lingual and multilingual question answering.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing}, EMNLP~'20, pages 5427--5444, Online, 2020. Association for Computational Linguistics.

\bibitem{alghamdi2024aratrust}
Emad~A Alghamdi, Reem~I Masoud, Deema Alnuhait, Afnan~Y Alomairi, Ahmed Ashraf, and Mohamed Zaytoon.
\newblock Aratrust: An evaluation of trustworthiness for llms in arabic.
\newblock {\em arXiv preprint arXiv:2403.09017}, 2024.

\bibitem{qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu.
\newblock Qwen technical report.
\newblock {\em arXiv preprint arXiv:2309.16609}, 2023.

\bibitem{sengupta2023jais}
Neha Sengupta, Sunil~Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama~Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain~Muhammad Mujahid, Massa Baali, Alham~Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Preslav Nakov, Timothy Baldwin, and Eric Xing.
\newblock Jais and jais-chat: Arabic-centric foundation and instruction-tuned open generative large language models, 2023.

\bibitem{cerebras2023slimpajama}
Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob~R Steeves, Joel Hestness, and Nolan Dey.
\newblock {SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}.
\newblock \url{https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama}, June 2023.

\bibitem{du2024chinese}
Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang Ma, Ruibin Yuan, Xingwei Qu, Jiaheng Liu, Tianyu Zheng, Xinchen Luo, Guorui Zhou, Binhang Yuan, Wenhu Chen, Jie Fu, and Ge~Zhang.
\newblock Chinese tiny llm: Pretraining a chinese-centric large language model, 2024.

\bibitem{wikidump}
Wikimedia Foundation.
\newblock Wikimedia downloads.

\bibitem{azerbayev2023llemma}
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco~Dos Santos, Stephen McAleer, Albert~Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck.
\newblock Llemma: An open language model for mathematics, 2023.

\bibitem{zhu2024second}
Jianqing Zhu, Huang Huang, Zhihang Lin, Juhao Liang, Zhengyang Tang, Khalid Almubarak, Mosen Alharthi, Bang An, Juncai He, Xiangbo Wu, Fei Yu, Junying Chen, Zhuoheng Ma, Yuhao Du, Yan Hu, He~Zhang, Emad~A. Alghamdi, Lian Zhang, Ruoyu Sun, Haizhou Li, Benyou Wang, and Jinchao Xu.
\newblock Second language (arabic) acquisition of llms via progressive vocabulary expansion.
\newblock 2024.

\bibitem{2023opencompass}
OpenCompass Contributors.
\newblock Opencompass: A universal evaluation platform for foundation models.
\newblock \url{https://github.com/open-compass/opencompass}, 2023.

\bibitem{beavertails}
Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce~Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, and Yaodong Yang.
\newblock Beavertails: Towards improved safety alignment of llm via a human-preference dataset.
\newblock {\em arXiv preprint arXiv:2307.04657}, 2023.

\bibitem{wang2023large}
Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi~Liu, Tianyu Liu, and Zhifang Sui.
\newblock Large language models are not fair evaluators.
\newblock {\em arXiv preprint arXiv:2305.17926}, 2023.

\bibitem{penedo2023refinedweb}
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.
\newblock The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.
\newblock {\em arXiv preprint arXiv:2306.01116}, 2023.

\bibitem{gunasekar2023textbooks}
Suriya Gunasekar, Yi~Zhang, Jyoti Aneja, Caio C{\'e}sar~Teodoro Mendes, Allie Del~Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de~Rosa, Olli Saarikivi, et~al.
\newblock Textbooks are all you need.
\newblock {\em arXiv preprint arXiv:2306.11644}, 2023.

\bibitem{li2023textbooks}
Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya Gunasekar, and Yin~Tat Lee.
\newblock Textbooks are all you need ii: phi-1.5 technical report.
\newblock {\em arXiv preprint arXiv:2309.05463}, 2023.

\bibitem{javaheripi2023phi}
Mojan Javaheripi, S{\'e}bastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio C{\'e}sar~Teodoro Mendes, Weizhu Chen, Allie Del~Giorno, Ronen Eldan, Sivakanth Gopi, et~al.
\newblock Phi-2: The surprising power of small language models.
\newblock {\em Microsoft Research Blog}, 2023.

\bibitem{kreutzer2022quality}
Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, et~al.
\newblock Quality at a glance: An audit of web-crawled multilingual datasets.
\newblock {\em Transactions of the Association for Computational Linguistics}, 10:50--72, 2022.

\bibitem{maini2024rephrasing}
Pratyush Maini, Skyler Seto, He~Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly.
\newblock Rephrasing the web: A recipe for compute and data-efficient language modeling.
\newblock {\em arXiv preprint arXiv:2401.16380}, 2024.

\bibitem{hegazi2021preprocessing}
Mohamed~Osman Hegazi, Yasser Al-Dossari, Abdullah Al-Yahy, Abdulaziz Al-Sumari, and Anwer Hilal.
\newblock Preprocessing arabic text on social media.
\newblock {\em Heliyon}, 7(2), 2021.

\bibitem{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock {\em arXiv preprint arXiv:2101.00027}, 2020.

\bibitem{wenzek2019ccnet}
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm{\'a}n, Armand Joulin, and Edouard Grave.
\newblock Ccnet: Extracting high quality monolingual datasets from web crawl data.
\newblock {\em arXiv preprint arXiv:1911.00359}, 2019.

\bibitem{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et~al.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock {\em arXiv preprint arXiv:2212.08073}, 2022.

\bibitem{ji2024aligner}
Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, and Yaodong Yang.
\newblock Aligner: Achieving efficient alignment through weak-to-strong correction.
\newblock {\em arXiv preprint arXiv:2402.02416}, 2024.

\bibitem{zheng2024llamafactory}
Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma.
\newblock Llamafactory: Unified efficient fine-tuning of 100+ language models.
\newblock In {\em Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)}, Bangkok, Thailand, 2024. Association for Computational Linguistics.

\bibitem{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In {\em International Conference on Learning Representations}, 2020.

\end{thebibliography}
