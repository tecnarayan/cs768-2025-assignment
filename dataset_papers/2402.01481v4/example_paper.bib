@article{lu2023highly,
  title={Highly Accurate Quantum Chemical Property Prediction with Uni-Mol+},
  author={Lu, Shuqi and Gao, Zhifeng and He, Di and Zhang, Linfeng and Ke, Guolin},
  journal={arXiv preprint arXiv:2303.16982},
  year={2023}
}

@article{you2022cross,
  title={Cross-modality and self-supervised protein embedding for compound--protein affinity and contact prediction},
  author={You, Yuning and Shen, Yang},
  journal={Bioinformatics},
  volume={38},
  number={Supplement\_2},
  pages={ii68--ii74},
  year={2022},
  publisher={Oxford University Press}
}

@inproceedings{chen2022hotprotein,
  title={HotProtein: A novel framework for protein thermostability prediction and editing},
  author={Chen, Tianlong and Gong, Chengyue and Diaz, Daniel Jesus and Chen, Xuxi and Wells, Jordan Tyler and Wang, Zhangyang and Ellington, Andrew and Dimakis, Alex and Klivans, Adam and others},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{gao2022pifold,
  title={PiFold: Toward effective and efficient protein inverse folding},
  author={Gao, Zhangyang and Tan, Cheng and Chac{\'o}n, Pablo and Li, Stan Z},
  journal={arXiv preprint arXiv:2209.12643},
  year={2022}
}

@article{jumper_highly_2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	copyright = {2021 The Author(s)},
	
	
	
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2023-05-22},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	note = {Number: 7873
Publisher: Nature Publishing Group},
	keywords = {Computational biophysics, Machine learning, Protein structure predictions, Structural biology},
	pages = {583--589},
	file = {41586_2021_3819_MOESM1_ESM.pdf:files/573/41586_2021_3819_MOESM1_ESM.pdf:application/pdf;Full Text PDF:files/574/Jumper 等 - 2021 - Highly accurate protein structure prediction with .pdf:application/pdf},
}


@inproceedings{zhang2022protein,
  title={Protein Representation Learning by Geometric Structure Pretraining},
  author={Zhang, Zuobai and Xu, Minghao and Jamasb, Arian and Vijil, Vijil and Lozano, Aurelie and Das, Payel and Tang, Jian},
  booktitle={International Conference on Machine Learning},
  year={2022}
}

@misc{zhang_equipocket_2023,
	title = {{EquiPocket}: an {E}(3)-{Equivariant} {Geometric} {Graph} {Neural} {Network} for {Ligand} {Binding} {Site} {Prediction}},
	shorttitle = {{EquiPocket}},
	
	
	abstract = {Predicting the binding sites of the target proteins plays a fundamental role in drug discovery. Most existing deep-learning methods consider a protein as a 3D image by spatially clustering its atoms into voxels and then feed the voxelized protein into a 3D CNN for prediction. However, the CNN-based methods encounter several critical issues: 1) defective in representing irregular protein structures; 2) sensitive to rotations; 3) insufficient to characterize the protein surface; 4) unaware of data distribution shift. To address the above issues, this work proposes EquiPocket, an E(3)-equivariant Graph Neural Network (GNN) for binding site prediction. In particular, EquiPocket consists of three modules: the first one to extract local geometric information for each surface atom, the second one to model both the chemical and spatial structure of the protein, and the last one to capture the geometry of the surface via equivariant message passing over the surface atoms. We further propose a dense attention output layer to better alleviate the data distribution shift effect incurred by the variable protein size. Extensive experiments on several representative benchmarks demonstrate the superiority of our framework to the state-of-the-art methods.},
	urldate = {2023-05-10},
	publisher = {arXiv},
	author = {Zhang, Yang and Huang, Wenbing and Wei, Zhewei and Yuan, Ye and Ding, Zhaohan},
	month = feb,
	year = {2023},
	note = {arXiv:2302.12177 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Biomolecules},
	file = {arXiv Fulltext PDF:files/608/Zhang 等 - 2023 - EquiPocket an E(3)-Equivariant Geometric Graph Ne.pdf:application/pdf;arXiv.org Snapshot:files/607/2302.html:text/html},
}

@article{liu2013dnabind,
  title={DNABind: A hybrid algorithm for structure-based prediction of DNA-binding residues by combining machine learning-and template-based approaches},
  author={Liu, Rong and Hu, Jianjun},
  journal={PROTEINS: structure, Function, and Bioinformatics},
  volume={81},
  number={11},
  pages={1885--1899},
  year={2013},
  publisher={Wiley Online Library}
}

@article{wu2018coach,
  title={COACH-D: improved protein--ligand binding sites prediction with refined ligand-binding poses through molecular docking},
  author={Wu, Qi and Peng, Zhenling and Zhang, Yang and Yang, Jianyi},
  journal={Nucleic acids research},
  volume={46},
  number={W1},
  pages={W438--W442},
  year={2018},
  publisher={Oxford University Press}
}

@article{miao2015large,
  title={A large-scale assessment of nucleic acids binding site prediction programs},
  author={Miao, Zhichao and Westhof, Eric},
  journal={PLoS computational biology},
  volume={11},
  number={12},
  pages={e1004639},
  year={2015},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{su2019improving,
  title={Improving the prediction of protein--nucleic acids binding residues via multiple sequence profiles and the consensus of complementary methods},
  author={Su, Hong and Liu, Mengchen and Sun, Saisai and Peng, Zhenling and Yang, Jianyi},
  journal={Bioinformatics},
  volume={35},
  number={6},
  pages={930--936},
  year={2019},
  publisher={Oxford University Press}
}

@article{lam2019deep,
  title={A deep learning framework to predict binding preference of RNA constituents on protein surface},
  author={Lam, Jordy Homing and Li, Yu and Zhu, Lizhe and Umarov, Ramzan and Jiang, Hanlun and H{\'e}liou, Am{\'e}lie and Sheong, Fu Kit and Liu, Tianyun and Long, Yongkang and Li, Yunfei and others},
  journal={Nature communications},
  volume={10},
  number={1},
  pages={4941},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@article{walia2014rnabindrplus,
  title={RNABindRPlus: a predictor that combines machine learning and sequence homology-based methods to improve the reliability of predicted RNA-binding residues in proteins},
  author={Walia, Rasna R and Xue, Li C and Wilkins, Katherine and El-Manzalawy, Yasser and Dobbs, Drena and Honavar, Vasant},
  journal={PloS one},
  volume={9},
  number={5},
  pages={e97725},
  year={2014},
  publisher={Public Library of Science San Francisco, USA}
}

@article{zhu2019dnapred,
  title={DNAPred: accurate identification of DNA-binding sites from protein sequence by ensembled hyperplane-distance-based support vector machines},
  author={Zhu, Yi-Heng and Hu, Jun and Song, Xiao-Ning and Yu, Dong-Jun},
  journal={Journal of chemical information and modeling},
  volume={59},
  number={6},
  pages={3057--3071},
  year={2019},
  publisher={ACS Publications}
}

@article{hu2016predicting,
  title={Predicting protein-DNA binding residues by weightedly combining sequence-based features and boosting multiple SVMs},
  author={Hu, Jun and Li, Yang and Zhang, Ming and Yang, Xibei and Shen, Hong-Bin and Yu, Dong-Jun},
  journal={IEEE/ACM transactions on computational biology and bioinformatics},
  volume={14},
  number={6},
  pages={1389--1398},
  year={2016},
  publisher={IEEE}
}

@article{krivak2018p2rank,
  title={P2Rank: machine learning based tool for rapid and accurate prediction of ligand binding sites from protein structure},
  author={Kriv{\'a}k, Radoslav and Hoksza, David},
  journal={Journal of cheminformatics},
  volume={10},
  pages={1--12},
  year={2018},
  publisher={Springer}
}

@article{jimenez2017deepsite,
  title={DeepSite: protein-binding site predictor using 3D-convolutional neural networks},
  author={Jim{\'e}nez, Jos{\'e} and Doerr, Stefan and Mart{\'\i}nez-Rosell, Gerard and Rose, Alexander S and De Fabritiis, Gianni},
  journal={Bioinformatics},
  volume={33},
  number={19},
  pages={3036--3042},
  year={2017},
  publisher={Oxford University Press}
}

@article{macari2019computational,
  title={Computational methods and tools for binding site recognition between proteins and small molecules: from classical geometrical approaches to modern machine learning strategies},
  author={Macari, Gabriele and Toti, Daniele and Polticelli, Fabio},
  journal={Journal of computer-aided molecular design},
  volume={33},
  pages={887--903},
  year={2019},
  publisher={Springer}
}

@article{hernandez2009sitehound,
  title={SITEHOUND-web: a server for ligand binding site identification in protein structures},
  author={Hernandez, Marylens and Ghersi, Dario and Sanchez, Roberto},
  journal={Nucleic acids research},
  volume={37},
  number={suppl\_2},
  pages={W413--W416},
  year={2009},
  publisher={Oxford University Press}
}

@article{le_guilloux_fpocket_2009,
	title = {Fpocket: An open source platform for ligand pocket detection},
	volume = {10},
	issn = {1471-2105},
	doi = {10.1186/1471-2105-10-168},
	abstract = {Virtual screening methods start to be well established as effective approaches to identify hits, candidates and leads for drug discovery research. Among those, structure based virtual screening ({SBVS}) approaches aim at docking collections of small compounds in the target structure to identify potent compounds. For {SBVS}, the identification of candidate pockets in protein structures is a key feature, and the recent years have seen increasing interest in developing methods for pocket and cavity detection on protein surfaces.},
	pages = {168},
        year = {2009},
	number = {1},
	journal = {Bioinformatics},
	shortjournal = {{BMC} Bioinformatics},
	author = {Le Guilloux, Vincent and Schmidtke, Peter and Tuffery, Pierre},
	date = {2009-06-02},
}

@misc{chen2018gradnorm,
      title={GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks}, 
      author={Zhao Chen and Vijay Badrinarayanan and Chen-Yu Lee and Andrew Rabinovich},
      year={2018},
      eprint={1711.02257},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{javaloy2022rotograd,
      title={RotoGrad: Gradient Homogenization in Multitask Learning}, 
      author={Adrián Javaloy and Isabel Valera},
      year={2022},
      eprint={2103.02631},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{wang2022multi,
  title={Multi-level Protein Structure Pre-training via Prompt Learning},
  author={Wang, Zeyuan and Zhang, Qiang and Shuang-Wei, HU and Yu, Haoran and Jin, Xurui and Gong, Zhichen and Chen, Huajun},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}


@article{mirdita_fast_2021,
	title = {Fast and sensitive taxonomic assignment to metagenomic contigs},
	volume = {37},
	
	
	
	abstract = {{MMseqs}2 taxonomy is a new tool to assign taxonomic labels to metagenomic contigs. It extracts all possible protein fragments from each contig, quickly retains those that can contribute to taxonomic annotation, assigns them with robust labels and determines the contig’s taxonomic identity by weighted voting. Its fragment extraction step is suitable for the analysis of all domains of life. {MMseqs}2 taxonomy is 2–18× faster than state-of-the-art tools and also contains new modules for creating and manipulating taxonomic reference databases as well as reporting and visualizing taxonomic assignments.{MMseqs}2 taxonomy is part of the {MMseqs}2 free open-source software package available for Linux, {macOS} and Windows at https://mmseqs.com.Supplementary data are available at Bioinformatics online.},
	pages = {3029--3031},
	number = {18},
        year = {2021},
	journaltitle = {Bioinformatics},
	journal = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Mirdita, M and Steinegger, M and Breitwieser, F and Söding, J and Levy Karin, E},
	urldate = {2024-01-27},
	date = {2021-09-29},
	file = {Full Text PDF:C\:\\Users\\10076\\Zotero\\storage\\Y38WLA2E\\Mirdita 等 - 2021 - Fast and sensitive taxonomic assignment to metagen.pdf:application/pdf;Snapshot:C\:\\Users\\10076\\Zotero\\storage\\H735P38B\\6178277.html:text/html},
}


@article{mitternacht_freesasa_2016,
	title = {{FreeSASA}: An open source C library for solvent accessible surface area calculations},
	volume = {5},
	
	year = {2016},
	
	shorttitle = {{FreeSASA}},
	abstract = {Calculating solvent accessible surface areas ({SASA}) is a run-of-the-mill calculation in structural biology. Although there are many programs available for this calculation, there are no free-standing, open-source tools designed for easy tool-chain integration. {FreeSASA} is an open source C library for {SASA} calculations that provides both command-line and Python interfaces in addition to its C {API}. The library implements both Lee and Richards’ and Shrake and Rupley’s approximations, and is highly configurable to allow the user to control molecular parameters, accuracy and output granularity. It only depends on standard C libraries and should therefore be easy to compile and install on any platform. The library is well-documented, stable and efficient. The command-line interface can easily replace closed source legacy programs, with comparable or better accuracy and speed, and with some added functionality.},
	pages = {189},
	journal = {F1000Research},
	shortjournal = {F1000Res},
	author = {Mitternacht, Simon},
	urldate = {2024-01-27},
	date = {2016-02-18},
	pmid = {26973785},
	pmcid = {PMC4776673},
	file = {PubMed Central Full Text PDF:C\:\\Users\\10076\\Zotero\\storage\\3W2T2VF4\\Mitternacht - 2016 - FreeSASA An open source C library for solvent acc.pdf:application/pdf},
}



@misc{chen_xtrimopglm_2023,
	title = {{xTrimoPGLM}: {Unified} {100B}-{Scale} {Pre}-trained {Transformer} for {Deciphering} the {Language} of {Protein}},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {{xTrimoPGLM}},
	
	
	abstract = {Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.},
	language = {en},
	urldate = {2023-08-08},
	publisher = {bioRxiv},
	author = {Chen, Bo and Cheng, Xingyi and Geng, Yangli-ao and Li, Shen and Zeng, Xin and Wang, Boyan and Gong, Jing and Liu, Chiming and Zeng, Aohan and Dong, Yuxiao and Tang, Jie and Song, Le},
	month = jul,
	year = {2023},
	note = {Pages: 2023.07.05.547496
Section: New Results},
	file = {Full Text PDF:files/883/Chen 等 - 2023 - xTrimoPGLM Unified 100B-Scale Pre-trained Transfo.pdf:application/pdf},
}

@article{min2021pre,
  title={Pre-training of deep bidirectional protein sequence representations with structural information},
  author={Min, Seonwoo and Park, Seunghyun and Kim, Siwon and Choi, Hyun-Soo and Lee, Byunghan and Yoon, Sungroh},
  journal={IEEE Access},
  volume={9},
  pages={123912--123926},
  year={2021},
  publisher={IEEE}
}


@article{gao_hierarchical_2023,
	title = {Hierarchical graph learning for protein–protein interaction},
	volume = {14},
	copyright = {2023 The Author(s)},
	
	
	
	abstract = {Protein-Protein Interactions (PPIs) are fundamental means of functions and signalings in biological systems. The massive growth in demand and cost associated with experimental PPI studies calls for computational tools for automated prediction and understanding of PPIs. Despite recent progress, in silico methods remain inadequate in modeling the natural PPI hierarchy. Here we present a double-viewed hierarchical graph learning model, HIGH-PPI, to predict PPIs and extrapolate the molecular details involved. In this model, we create a hierarchical graph, in which a node in the PPI network (top outside-of-protein view) is a protein graph (bottom inside-of-protein view). In the bottom view, a group of chemically relevant descriptors, instead of the protein sequences, are used to better capture the structure-function relationship of the protein. HIGH-PPI examines both outside-of-protein and inside-of-protein of the human interactome to establish a robust machine understanding of PPIs. This model demonstrates high accuracy and robustness in predicting PPIs. Moreover, HIGH-PPI can interpret the modes of action of PPIs by identifying important binding and catalytic sites precisely. Overall, “HIGH-PPI [https://github.com/zqgao22/HIGH-PPI]” is a domain-knowledge-driven and interpretable framework for PPI prediction studies.},
	language = {en},
	number = {1},
	urldate = {2023-11-14},
	journal = {Nature Communications},
	author = {Gao, Ziqi and Jiang, Chenran and Zhang, Jiawen and Jiang, Xiaosen and Li, Lanqing and Zhao, Peilin and Yang, Huanming and Huang, Yong and Li, Jia},
	month = feb,
	year = {2023},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Proteome informatics},
	pages = {1093},
	file = {Full Text PDF:files/1159/Gao 等 - 2023 - Hierarchical graph learning for protein–protein in.pdf:application/pdf},
}

@inproceedings{wang2023learning,
  title={Learning hierarchical protein representations via complete 3d graph networks},
  author={Wang, Limei and Liu, Haoran and Liu, Yi and Kurtin, Jerry and Ji, Shuiwang},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023}
}

@article{rao2019evaluating,
  title={Evaluating protein transfer learning with TAPE},
  author={Rao, Roshan and Bhattacharya, Nicholas and Thomas, Neil and Duan, Yan and Chen, Peter and Canny, John and Abbeel, Pieter and Song, Yun},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{hermosilla_contrastive_2021,
	title = {Contrastive {Representation} {Learning} for {3D} {Protein} {Structures}},
	
	abstract = {Learning from 3D protein structures has gained a lot of attention in the fields of protein modeling and structural bioinformatics. Unfortunately, the number of available structures is orders of magnitude lower than the number of available protein sequences. Moreover, this number is reduced even more when only annotated protein structures are considered. This makes the training of existing models difficult and prone to overfitting. To address this limitation, we introduce a new representation learning framework for 3D protein structures. Our framework uses unsupervised contrastive learning to learn meaningful representations of protein structures making use of annotated and un-annotated proteins from the Protein Data Bank. We show how these representations can be used to directly solve different tasks in the field of structural bioinformatics, such as protein function and protein structural similarity prediction. Moreover, we show how fine-tuned networks, pre-trained with our algorithm, lead to significantly improved task performance.},
	language = {en},
	urldate = {2023-11-21},
	author = {Hermosilla, Pedro and Ropinski, Timo},
	month = oct,
	year = {2021},
	file = {Full Text PDF:files/1174/Hermosilla 和 Ropinski - 2021 - Contrastive Representation Learning for 3D Protein.pdf:application/pdf},
}

@article{liu2024protein,
  title={Protein--DNA binding sites prediction based on pre-trained protein language model and contrastive learning},
  author={Liu, Yufan and Tian, Boxue},
  journal={Briefings in Bioinformatics},
  volume={25},
  number={1},
  pages={bbad488},
  year={2024},
  publisher={Oxford University Press}
}

@article{xia2021graphbind,
  title={GraphBind: protein structural context embedded rules learned by hierarchical graph neural networks for recognizing nucleic-acid-binding residues},
  author={Xia, Ying and Xia, Chun-Qiu and Pan, Xiaoyong and Shen, Hong-Bin},
  journal={Nucleic acids research},
  volume={49},
  number={9},
  pages={e51--e51},
  year={2021},
  publisher={Oxford University Press}
}
@article{lei2021deep,
  title={A deep-learning framework for multi-level peptide--protein interaction prediction},
  author={Lei, Yipin and Li, Shuya and Liu, Ziyi and Wan, Fangping and Tian, Tingzhong and Li, Shao and Zhao, Dan and Zeng, Jianyang},
  journal={Nature communications},
  volume={12},
  number={1},
  pages={5465},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{taherzadeh_structure-based_2017,
	title = {Structure-based prediction of protein-peptide binding regions using {Random} {Forest}},
	volume = {34},
	
	abstract = {Motivation: 
Protein-peptide interactions are one of the most important biological interactions and play crucial role in many diseases including cancer. Therefore, knowledge of these interactions provides invaluable insights into all cellular processes, functional mechanisms, and drug discovery. Protein-peptide interactions can be analyzed by studying the structures of protein-peptide complexes. However, only a small portion has known complex structures and experimental determination of protein-peptide interaction is costly and inefficient. Thus, predicting peptide-binding sites computationally will be useful to improve efficiency and cost effectiveness of experimental studies. Here, we established a machine learning method called SPRINT-Str (Structure-based prediction of protein-Peptide Residue-level Interaction) to use structural information for predicting protein-peptide binding residues. These predicted binding residues are then employed to infer the peptide-binding site by a clustering algorithm.

Results:
SPRINT-Str achieves robust and consistent results for prediction of protein-peptide binding regions in terms of residues and sites. Matthews' Correlation Coefficient (MCC) for 10-fold cross validation and independent test set are 0.27 and 0.293, respectively, as well as 0.775 and 0.782, respectively for Area Under the Curve (AUC). The prediction outperforms other state-of-the-art methods, including our previously developed sequence-based method. A further spatial neighbor clustering of predicted binding residues leads to prediction of binding sites at 20\%-116\% higher coverage than the next best method at all precision levels in the test set. The application of SPRINT-Str to protein binding with DNA, RNA, and carbohydrate confirms the method's capability of separating peptide-binding sites from other functional sites. More importantly, similar performance in prediction of binding residues and sites is obtained when experimentally determined structures are replaced by unbound structures or quality model structures built from homologs, indicating its wide applicability.

Availability:
http://sparks-lab.org/server/SPRINT-Str.

Contact:
yangyd25@mail.sysu.edu.cn.

Supplementary information:
Supplementary data are available at Bioinformatics online.},
	journal = {Bioinformatics (Oxford, England)},
	author = {Taherzadeh, Ghazaleh and Zhou, Yaoqi and Liew, Alan Wee-Chung and Yang, Yuedong},
	month = sep,
	year = {2017},
	file = {Full Text PDF:files/1190/Taherzadeh 等 - 2017 - Structure-based prediction of protein-peptide bind.pdf:application/pdf},
}

@article{su_improving_2018,
	title = {Improving the prediction of protein-nucleic acids binding residues via multiple sequence profiles and the consensus of complementary methods},
	volume = {35},
	
	abstract = {Motivation: 
The interactions between protein and nucleic acids play a key role in various biological processes. Accurate recognition of the residues that bind nucleic acids can facilitate the study of uncharacterized protein-nucleic acids interactions. The accuracy of existing nucleic acids-binding residues prediction methods is relatively low.

Results:
In this work, we introduce NucBind, a novel method for the prediction of nucleic acids-binding residues. NucBind combines the predictions from a support vector machine-based ab-initio method SVMnuc and a template-based method COACH-D. SVMnuc was trained with features from three complementary sequence profiles. COACH-D predicts the binding residues based on homologous templates identified from a nucleic acids-binding library. The proposed methods were assessed and compared with other peering methods on three benchmark datasets. Experimental results show that NucBind consistently outperforms other state-of-the-art methods. Though with higher accuracy, similar to many other ab-initio methods, cross prediction between DNA and RNA-binding residues was also observed in SVMnuc and NucBind. We attribute the success of NucBind to two folds. The first is the utilization of improved features extracted from three complementary sequence profiles in SVMnuc. The second is the combination of two complementary methods: the ab-initio method SVMnuc and the template-based method COACH-D.

Availability and implementation:
http://yanglab.nankai.edu.cn/NucBind.

Supplementary information:
Supplementary data are available at Bioinformatics online.},
	journal = {Bioinformatics (Oxford, England)},
	author = {Su, Hong and Liu, Mengchen and Sun, Saisai and Peng, Zhenling and Yang, Jianyi},
	month = aug,
	year = {2018},
	file = {Full Text PDF:files/1192/Su 等 - 2018 - Improving the prediction of protein-nucleic acids .pdf:application/pdf},
}

@article{wu_coach-d_2018,
	title = {{COACH}-{D}: {Improved} protein-ligand binding sites prediction with refined ligand-binding poses through molecular docking},
	volume = {46},
	shorttitle = {{COACH}-{D}},
	
	abstract = {The identification of protein-ligand binding sites is critical to protein function annotation and drug discovery. The consensus algorithm COACH developed by us represents one of the most efficient approaches to protein-ligand binding sites prediction. One of the most commonly seen issues with the COACH prediction are the low quality of the predicted ligand-binding poses, which usually have severe steric clashes to the protein structure. Here, we present COACH-D, an enhanced version of COACH by utilizing molecular docking to refine the ligand-binding poses. The input to the COACH-D server is the amino acid sequence or the three-dimensional structure of a query protein. In addition, the users can also submit their own ligand of interest. For each job submission, the COACH algorithm is first used to predict the protein-ligand binding sites. The ligands from the users or the templates are then docked into the predicted binding pockets to build their complex structures. Blind tests show that the algorithm significantly outperforms other ligand-binding sites prediction methods. Benchmark tests show that the steric clashes between the ligand and the protein structures in the COACH models are reduced by 85\% after molecular docking in COACH-D. The COACH-D server is freely available to all users at http://yanglab.nankai.edu.cn/COACH-D/.},
	journal = {Nucleic acids research},
	author = {Wu, Qi and Peng, Zhenling and Zhang, Yang and Yang, Jianyi},
	month = may,
	year = {2018},
	file = {全文:files/1194/Wu 等 - 2018 - COACH-D Improved protein-ligand binding sites pre.pdf:application/pdf},
}

@article{littmann_protein_2021,
	title = {Protein embeddings and deep learning predict binding residues for various ligand classes},
	volume = {11},
	copyright = {2021 The Author(s)},
	
	
	
	abstract = {One important aspect of protein function is the binding of proteins to ligands, including small molecules, metal ions, and macromolecules such as DNA or RNA. Despite decades of experimental progress many binding sites remain obscure. Here, we proposed bindEmbed21, a method predicting whether a protein residue binds to metal ions, nucleic acids, or small molecules. The Artificial Intelligence (AI)-based method exclusively uses embeddings from the Transformer-based protein Language Model (pLM) ProtT5 as input. Using only single sequences without creating multiple sequence alignments (MSAs), bindEmbed21DL outperformed MSA-based predictions. Combination with homology-based inference increased performance to F1 = 48 ± 3\% (95\% CI) and MCC = 0.46 ± 0.04 when merging all three ligand classes into one. All results were confirmed by three independent data sets. Focusing on very reliably predicted residues could complement experimental evidence: For the 25\% most strongly predicted binding residues, at least 73\% were correctly predicted even when ignoring the problem of missing experimental annotations. The new method bindEmbed21 is fast, simple, and broadly applicable—neither using structure nor MSAs. Thereby, it found binding residues in over 42\% of all human proteins not otherwise implied in binding and predicted about 6\% of all residues as binding to metal ions, nucleic acids, or small molecules.},
	language = {en},
	number = {1},
	urldate = {2023-11-22},
	journal = {Scientific Reports},
	author = {Littmann, Maria and Heinzinger, Michael and Dallago, Christian and Weissenow, Konstantin and Rost, Burkhard},
	month = dec,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Protein function predictions, Protein sequence analyses, Sequence annotation},
	pages = {23916},
	file = {Full Text PDF:files/1199/Littmann 等 - 2021 - Protein embeddings and deep learning predict bindi.pdf:application/pdf},
}

@article{kozlovskii_proteinpeptide_2021,
	title = {Protein–{Peptide} {Binding} {Site} {Detection} {Using} {3D} {Convolutional} {Neural} {Networks}},
	copyright = {© 2021 American Chemical Society},
	
	
	abstract = {Peptides and peptide-based molecules represent a promising therapeutic modality targeting intracellular protein–protein interactions, potentially combining the beneficial properties of biologics and small-molecule drugs. Protein–peptide complexes occupy a unique niche of interaction interfaces with respect to protein–protein and protein–small molecule complexes. Protein–peptide binding site identification resembles image object detection, a field that had been revolutionalized with computer vision techniques. We present a new protein–peptide binding site detection method called BiteNetPp by harnessing the power of 3D convolutional neural network. Our method employs a tensor-based representation of spatial protein structures, which is fed to 3D convolutional neural network, resulting in probability scores and coordinates of the binding “hot spots” in the input structures. We used the domain adaptation technique to fine-tune model trained on protein–small molecule complexes using a manually curated set of protein–peptide structures. BiteNetPp consistently outperforms existing state-of-the-art methods in the independent test benchmark. It takes less than a second to analyze a single-protein structure, making BiteNetPp suitable for the large-scale analysis of protein–peptide binding sites.},
	language = {en},
	urldate = {2023-11-23},
	journal = {Journal of Chemical Information and Modeling},
	author = {Kozlovskii, Igor and Popov, Petr},
	month = jul,
	year = {2021},
	note = {Publisher: American Chemical Society},
	file = {Kozlovskii 和 Popov - 2021 - Protein–Peptide Binding Site Detection Using 3D Co.pdf:files/1203/Kozlovskii 和 Popov - 2021 - Protein–Peptide Binding Site Detection Using 3D Co.pdf:application/pdf;Snapshot:files/1204/acs.jcim.html:text/html},
}

@article{kim_functional_2023,
	title = {Functional annotation of enzyme-encoding genes using deep learning with transformer layers},
	volume = {14},
	copyright = {2023 The Author(s)},
	
	
	
	abstract = {Functional annotation of open reading frames in microbial genomes remains substantially incomplete. Enzymes constitute the most prevalent functional gene class in microbial genomes and can be described by their specific catalytic functions using the Enzyme Commission (EC) number. Consequently, the ability to predict EC numbers could substantially reduce the number of un-annotated genes. Here we present a deep learning model, DeepECtransformer, which utilizes transformer layers as a neural network architecture to predict EC numbers. Using the extensively studied Escherichia coli K-12 MG1655 genome, DeepECtransformer predicted EC numbers for 464 un-annotated genes. We experimentally validated the enzymatic activities predicted for three proteins (YgfF, YciO, and YjdM). Further examination of the neural network’s reasoning process revealed that the trained neural network relies on functional motifs of enzymes to predict EC numbers. Thus, DeepECtransformer is a method that facilitates the functional annotation of uncharacterized genes.},
	language = {en},
	number = {1},
	urldate = {2023-12-06},
	journal = {Nature Communications},
	author = {Kim, Gi Bae and Kim, Ji Yeon and Lee, Jong An and Norsigian, Charles J. and Palsson, Bernhard O. and Lee, Sang Yup},
	month = nov,
	year = {2023},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Bacterial genomics, Enzymes, Machine learning, Protein function predictions},
	pages = {7370},
	file = {Full Text PDF:files/1234/Kim 等 - 2023 - Functional annotation of enzyme-encoding genes usi.pdf:application/pdf},
}

@misc{nguyen_multimodal_2023,
	title = {Multimodal {Pretraining} for {Unsupervised} {Protein} {Representation} {Learning}},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	
	
	abstract = {In this paper, we introduce a framework of symmetry-preserving multimodal pretraining to learn a unified representation on proteins in an unsupervised manner that can take into account primary and tertiary structures. For each structure, we propose the corresponding pretraining method on sequence, graph and 3D point clouds based on large language models and generative models. We present a novel way to combining representations from multiple sources of information into a single global representation for proteins. We carefully analyze the performance of our framework in the pretraining tasks. For the fine-tuning tasks, our experiments have shown that our new multimodal representation can achieve competitive results in protein-ligand binding affinity prediction, protein fold classification, enzyme identification and mutation stability prediction. We expect that this work will accelerate future research in proteins. Our source code in PyTorch deep learning framework is publicly available at https://github.com/HySonLab/Protein\_Pretrain},
	language = {en},
	urldate = {2023-12-06},
	publisher = {bioRxiv},
	author = {Nguyen, Viet Thanh Duy and Hy, Truong Son},
	month = dec,
	year = {2023},
	note = {Pages: 2023.11.29.569288
Section: New Results},
	file = {Full Text PDF:files/1235/Nguyen 和 Hy - 2023 - Multimodal Pretraining for Unsupervised Protein Re.pdf:application/pdf},
}

@article{tubiana_scannet_2022,
	title = {{ScanNet}: an interpretable geometric deep learning model for structure-based protein binding site prediction},
	volume = {19},
	copyright = {2022 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	
	shorttitle = {{ScanNet}},
	
	
	abstract = {Predicting the functional sites of a protein from its structure, such as the binding sites of small molecules, other proteins or antibodies, sheds light on its function in vivo. Currently, two classes of methods prevail: machine learning models built on top of handcrafted features and comparative modeling. They are, respectively, limited by the expressivity of the handcrafted features and the availability of similar proteins. Here, we introduce ScanNet, an end-to-end, interpretable geometric deep learning model that learns features directly from 3D structures. ScanNet builds representations of atoms and amino acids based on the spatio-chemical arrangement of their neighbors. We train ScanNet for detecting protein–protein and protein–antibody binding sites, demonstrate its accuracy—including for unseen protein folds—and interpret the filters learned. Finally, we predict epitopes of the SARS-CoV-2 spike protein, validating known antigenic regions and predicting previously uncharacterized ones. Overall, ScanNet is a versatile, powerful and interpretable model suitable for functional site prediction tasks. A webserver for ScanNet is available from http://bioinfo3d.cs.tau.ac.il/ScanNet/.},
	language = {en},
	number = {6},
	urldate = {2023-12-07},
	journal = {Nature Methods},
	author = {Tubiana, Jérôme and Schneidman-Duhovny, Dina and Wolfson, Haim J.},
	month = jun,
	year = {2022},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Protein function predictions, Software},
	pages = {730--739},
	file = {Full Text PDF:files/1245/Tubiana 等 - 2022 - ScanNet an interpretable geometric deep learning .pdf:application/pdf;Tubiana 等 - 2022 - ScanNet an interpretable geometric deep learning .pdf:files/1243/Tubiana 等 - 2022 - ScanNet an interpretable geometric deep learning .pdf:application/pdf},
}

@article{clark_predicting_2020,
	title = {Predicting binding sites from unbound versus bound protein structures},
	volume = {10},
	copyright = {2020 The Author(s)},
	
	
	
	abstract = {We present the application of seven binding-site prediction algorithms to a meticulously curated dataset of ligand-bound and ligand-free crystal structures for 304 unique protein sequences (2528 crystal structures). We probe the influence of starting protein structures on the results of binding-site prediction, so the dataset contains a minimum of two ligand-bound and two ligand-free structures for each protein. We use this dataset in a brief survey of five geometry-based, one energy-based, and one machine-learning-based methods: Surfnet, Ghecom, LIGSITEcsc, Fpocket, Depth, AutoSite, and Kalasanty. Distributions of the F scores and Matthew’s correlation coefficients for ligand-bound versus ligand-free structure performance show no statistically significant difference in structure type versus performance for most methods. Only Fpocket showed a statistically significant but low magnitude enhancement in performance for holo structures. Lastly, we found that most methods will succeed on some crystal structures and fail on others within the same protein family, despite all structures being relatively high-quality structures with low structural variation. We expected better consistency across varying protein conformations of the same sequence. Interestingly, the success or failure of a given structure cannot be predicted by quality metrics such as resolution, Cruickshank Diffraction Precision index, or unresolved residues. Cryptic sites were also examined.},
	language = {en},
	number = {1},
	urldate = {2023-12-07},
	journal = {Scientific Reports},
	author = {Clark, Jordan J. and Orban, Zachary J. and Carlson, Heather A.},
	month = sep,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational biophysics, Molecular biophysics},
	pages = {15856},
	file = {Full Text PDF:files/1244/Clark 等 - 2020 - Predicting binding sites from unbound versus bound.pdf:application/pdf},
}

@misc{lee_boosting_2023,
	title = {Boosting {Convolutional} {Neural} {Networks}' {Protein} {Binding} {Site} {Prediction} {Capacity} {Using} {SE}(3)-invariant transformers, {Transfer} {Learning} and {Homology}-based {Augmentation}},
	
	
	abstract = {Figuring out small molecule binding sites in target proteins, in the resolution of either pocket or residue, is critical in many virtual and real drug-discovery scenarios. Since it is not always easy to find such binding sites based on domain knowledge or traditional methods, different deep learning methods that predict binding sites out of protein structures have been developed in recent years. Here we present a new such deep learning algorithm, that significantly outperformed all state-of-the-art baselines in terms of the both resolutions\${\textbackslash}unicode\{x2013\}\$pocket and residue. This good performance was also demonstrated in a case study involving the protein human serum albumin and its binding sites. Our algorithm included new ideas both in the model architecture and in the training method. For the model architecture, it incorporated SE(3)-invariant geometric self-attention layers that operate on top of residue-level CNN outputs. This residue-level processing of the model allowed a transfer learning between the two resolutions, which turned out to significantly improve the binding pocket prediction. Moreover, we developed novel augmentation method based on protein homology, which prevented our model from over-fitting. Overall, we believe that our contribution to the literature is twofold. First, we provided a new computational method for binding site prediction that is relevant to real-world applications, as shown by the good performance on different benchmarks and case study. Second, the novel ideas in our method\${\textbackslash}unicode\{x2013\}\$the model architecture, transfer learning and the homology augmentation\${\textbackslash}unicode\{x2013\}\$would serve as useful components in future works.},
	urldate = {2023-12-07},
	publisher = {arXiv},
	author = {Lee, Daeseok and Byun, Jeunghyun and Shin, Bonggun},
	month = apr,
	year = {2023},
	note = {arXiv:2303.08818 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Biomolecules, Quantitative Biology - Quantitative Methods},
	file = {arXiv Fulltext PDF:files/1241/Lee 等 - 2023 - Boosting Convolutional Neural Networks' Protein Bi.pdf:application/pdf;arXiv.org Snapshot:files/1242/2303.html:text/html},
}

@article{chandra_pepcnn_2023,
	title = {{PepCNN} deep learning tool for predicting peptide binding residues in proteins using sequence, structural, and language model features},
	volume = {13},
	copyright = {2023 The Author(s)},
	
	
	
	abstract = {Protein–peptide interactions play a crucial role in various cellular processes and are implicated in abnormal cellular behaviors leading to diseases such as cancer. Therefore, understanding these interactions is vital for both functional genomics and drug discovery efforts. Despite a significant increase in the availability of protein–peptide complexes, experimental methods for studying these interactions remain laborious, time-consuming, and expensive. Computational methods offer a complementary approach but often fall short in terms of prediction accuracy. To address these challenges, we introduce PepCNN, a deep learning-based prediction model that incorporates structural and sequence-based information from primary protein sequences. By utilizing a combination of half-sphere exposure, position specific scoring matrices from multiple-sequence alignment tool, and embedding from a pre-trained protein language model, PepCNN outperforms state-of-the-art methods in terms of specificity, precision, and AUC. The PepCNN software and datasets are publicly available at https://github.com/abelavit/PepCNN.git.},
	language = {en},
	number = {1},
	urldate = {2023-12-09},
	journal = {Scientific Reports},
	author = {Chandra, Abel and Sharma, Alok and Dehzangi, Iman and Tsunoda, Tatsuhiko and Sattar, Abdul},
	month = nov,
	year = {2023},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Bioinformatics, Computational biology and bioinformatics, Computational models, Machine learning, Proteomics},
	pages = {20882},
	file = {Full Text PDF:files/1253/Chandra 等 - 2023 - PepCNN deep learning tool for predicting peptide b.pdf:application/pdf},
}

@article{aggarwal_deeppocket_2022,
	title = {{DeepPocket}: {Ligand} {Binding} {Site} {Detection} and {Segmentation} using {3D} {Convolutional} {Neural} {Networks}},
	volume = {62},
	
	shorttitle = {{DeepPocket}},
	
	
	abstract = {A structure-based drug design pipeline involves the development of potential drug molecules or ligands that form stable complexes with a given receptor at its binding site. A prerequisite to this is finding druggable and functionally relevant binding sites on the 3D structure of the protein. Although several methods for detecting binding sites have been developed beforehand, a majority of them surprisingly fail in the identification and ranking of binding sites accurately. The rapid adoption and success of deep learning algorithms in various sections of structural biology beckons the usage of such algorithms for accurate binding site detection. As a combination of geometry based software and deep learning, we report a novel framework, DeepPocket that utilizes 3D convolutional neural networks for the rescoring of pockets identified by Fpocket and further segments these identified cavities on the protein surface. Apart from this, we also propose another data set SC6K containing protein structures submitted in the Protein Data Bank (PDB) from January 1st, 2018, until February 28th, 2020, for ligand binding site (LBS) detection. DeepPocket’s results on various binding site data sets and SC6K highlight its better performance over current state-of-the-art methods and good generalization ability over novel structures.},
	number = {21},
	urldate = {2023-12-15},
	journal = {Journal of Chemical Information and Modeling},
	author = {Aggarwal, Rishal and Gupta, Akash and Chelur, Vineeth and Jawahar, C. V. and Priyakumar, U. Deva},
	month = nov,
	year = {2022},
	note = {Publisher: American Chemical Society},
	pages = {5069--5079},
	file = {Aggarwal 等 - 2022 - DeepPocket Ligand Binding Site Detection and Segm.pdf:files/1286/Aggarwal 等 - 2022 - DeepPocket Ligand Binding Site Detection and Segm.pdf:application/pdf},
}

@article{kozlovskii_spatiotemporal_2020,
	title = {Spatiotemporal identification of druggable binding sites using deep learning},
	volume = {3},
	copyright = {2020 The Author(s)},
	
	
	
	abstract = {Identification of novel protein binding sites expands druggable genome and opens new opportunities for drug discovery. Generally, presence or absence of a binding site depends on the three-dimensional conformation of a protein, making binding site identification resemble the object detection problem in computer vision. Here we introduce a computational approach for the large-scale detection of protein binding sites, that considers protein conformations as 3D-images, binding sites as objects on these images to detect, and conformational ensembles of proteins as 3D-videos to analyze. BiteNet is suitable for spatiotemporal detection of hard-to-spot allosteric binding sites, as we showed for conformation-specific binding site of the epidermal growth factor receptor, oligomer-specific binding site of the ion channel, and binding site in G protein-coupled receptor. BiteNet outperforms state-of-the-art methods both in terms of accuracy and speed, taking about 1.5 minutes to analyze 1000 conformations of a protein with {\textasciitilde}2000 atoms.},
	language = {en},
	number = {1},
	urldate = {2023-12-16},
	journal = {Communications Biology},
	author = {Kozlovskii, Igor and Popov, Petr},
	month = oct,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Target identification},
	pages = {1--12},
	file = {Full Text PDF:files/1289/Kozlovskii 和 Popov - 2020 - Spatiotemporal identification of druggable binding.pdf:application/pdf},
}

@misc{stark_equibind_2022,
	title = {{EquiBind}: {Geometric} {Deep} {Learning} for {Drug} {Binding} {Structure} {Prediction}},
	shorttitle = {{EquiBind}},
	
	
	abstract = {Predicting how a drug-like molecule binds to a specific protein target is a core problem in drug discovery. An extremely fast computational binding method would enable key applications such as fast virtual screening or drug engineering. Existing methods are computationally expensive as they rely on heavy candidate sampling coupled with scoring, ranking, and fine-tuning steps. We challenge this paradigm with EquiBind, an SE(3)-equivariant geometric deep learning model performing direct-shot prediction of both i) the receptor binding location (blind docking) and ii) the ligand's bound pose and orientation. EquiBind achieves significant speed-ups and better quality compared to traditional and recent baselines. Further, we show extra improvements when coupling it with existing fine-tuning techniques at the cost of increased running time. Finally, we propose a novel and fast fine-tuning model that adjusts torsion angles of a ligand's rotatable bonds based on closed-form global minima of the von Mises angular distance to a given input atomic point cloud, avoiding previous expensive differential evolution strategies for energy minimization.},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Stärk, Hannes and Ganea, Octavian-Eugen and Pattanaik, Lagnajit and Barzilay, Regina and Jaakkola, Tommi},
	month = jun,
	year = {2022},
	note = {arXiv:2202.05146 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Biomolecules},
	file = {arXiv Fulltext PDF:files/1293/Stärk 等 - 2022 - EquiBind Geometric Deep Learning for Drug Binding.pdf:application/pdf;arXiv.org Snapshot:files/1292/2202.html:text/html},
}

@article{wang_lm-gvp_2022,
	title = {{LM}-{GVP}: an extensible sequence and structure informed deep learning framework for protein property prediction},
	volume = {12},
	copyright = {2022 The Author(s)},
	
	shorttitle = {{LM}-{GVP}},
	
	
	abstract = {Proteins perform many essential functions in biological systems and can be successfully developed as bio-therapeutics. It is invaluable to be able to predict their properties based on a proposed sequence and structure. In this study, we developed a novel generalizable deep learning framework, LM-GVP, composed of a protein Language Model (LM) and Graph Neural Network (GNN) to leverage information from both 1D amino acid sequences and 3D structures of proteins. Our approach outperformed the state-of-the-art protein LMs on a variety of property prediction tasks including fluorescence, protease stability, and protein functions from Gene Ontology (GO). We also illustrated insights into how a GNN prediction head can inform the fine-tuning of protein LMs to better leverage structural information. We envision that our deep learning framework will be generalizable to many protein property prediction problems to greatly accelerate protein engineering and drug development.},
	language = {en},
	number = {1},
	urldate = {2023-12-20},
	journal = {Scientific Reports},
	author = {Wang, Zichen and Combs, Steven A. and Brand, Ryan and Calvo, Miguel Romero and Xu, Panpan and Price, George and Golovach, Nataliya and Salawu, Emmanuel O. and Wise, Colby J. and Ponnapalli, Sri Priya and Clark, Peter M.},
	month = apr,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Protein function predictions},
	pages = {6832},
	file = {Full Text PDF:files/1305/Wang 等 - 2022 - LM-GVP an extensible sequence and structure infor.pdf:application/pdf},
}
@inproceedings{pei2023fabind,
  title={FABind: Fast and Accurate Protein-Ligand Binding},
  author={Pei, Qizhi and Gao, Kaiyuan and Wu, Lijun and Zhu, Jinhua and Xia, Yingce and Xie, Shufang and Qin, Tao and He, Kun and Liu, Tie-Yan and Yan, Rui},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}

@misc{townshend_atom3d_2022,
	title = {{ATOM3D}: {Tasks} {On} {Molecules} in {Three} {Dimensions}},
	shorttitle = {{ATOM3D}},
	
	
	abstract = {Computational methods that operate on three-dimensional molecular structure have the potential to solve important questions in biology and chemistry. In particular, deep neural networks have gained significant attention, but their widespread adoption in the biomolecular domain has been limited by a lack of either systematic performance benchmarks or a unified toolkit for interacting with molecular data. To address this, we present ATOM3D, a collection of both novel and existing benchmark datasets spanning several key classes of biomolecules. We implement several classes of three-dimensional molecular learning methods for each of these tasks and show that they consistently improve performance relative to methods based on one- and two-dimensional representations. The specific choice of architecture proves to be critical for performance, with three-dimensional convolutional networks excelling at tasks involving complex geometries, graph networks performing well on systems requiring detailed positional information, and the more recently developed equivariant networks showing significant promise. Our results indicate that many molecular problems stand to gain from three-dimensional molecular learning, and that there is potential for improvement on many tasks which remain underexplored. To lower the barrier to entry and facilitate further developments in the field, we also provide a comprehensive suite of tools for dataset processing, model training, and evaluation in our open-source atom3d Python package. All datasets are available for download from https://www.atom3d.ai .},
	urldate = {2023-12-20},
	publisher = {arXiv},
	author = {Townshend, Raphael J. L. and Vögele, Martin and Suriana, Patricia and Derry, Alexander and Powers, Alexander and Laloudakis, Yianni and Balachandar, Sidhika and Jing, Bowen and Anderson, Brandon and Eismann, Stephan and Kondor, Risi and Altman, Russ B. and Dror, Ron O.},
	month = jan,
	year = {2022},
	note = {arXiv:2012.04035 [physics, q-bio]},
	keywords = {Computer Science - Machine Learning, Physics - Biological Physics, Physics - Computational Physics, Quantitative Biology - Biomolecules},
	file = {arXiv.org Snapshot:files/1310/2012.html:text/html;Townshend 等 - 2022 - ATOM3D Tasks On Molecules in Three Dimensions.pdf:files/1306/Townshend 等 - 2022 - ATOM3D Tasks On Molecules in Three Dimensions.pdf:application/pdf},
}

@inproceedings{zhang2021ontoprotein,
  title={OntoProtein: Protein Pretraining With Gene Ontology Embedding},
  author={Zhang, Ningyu and Bi, Zhen and Liang, Xiaozhuan and Cheng, Siyuan and Hong, Haosen and Deng, Shumin and Zhang, Qiang and Lian, Jiazhang and Chen, Huajun},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{melnyk2023reprogramming,
  title={Reprogramming pretrained language models for antibody sequence infilling},
  author={Melnyk, Igor and Chenthamarakshan, Vijil and Chen, Pin-Yu and Das, Payel and Dhurandhar, Amit and Padhi, Inkit and Das, Devleena},
  booktitle={International Conference on Machine Learning},
  pages={24398--24419},
  year={2023},
  organization={PMLR}
}

@inproceedings{zhang2023pre,
  title={Pre-Training Protein Encoder via Siamese Sequence-Structure Diffusion Trajectory Prediction},
  author={Zhang, Zuobai and Xu, Minghao and Lozano, Aurelie and Chenthamarakshan, Vijil and Das, Payel and Tang, Jian},
  booktitle={Annual Conference on Neural Information Processing Systems},
  year={2023}
}

@misc{nguyen_multimodal_2023-1,
	title = {Multimodal {Pretraining} for {Unsupervised} {Protein} {Representation} {Learning}},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	
	
	abstract = {In this paper, we introduce a framework of symmetry-preserving multimodal pretraining to learn a unified representation of proteins in an unsupervised manner, encompassing both primary and tertiary structures. Our approach involves proposing specific pretraining methods for sequences, graphs, and 3D point clouds associated with each protein structure, leveraging the power of large language models and generative models. We present a novel way to combining representations from multiple sources of information into a single global representation for proteins. We carefully analyze the performance of our framework in the pretraining tasks. For the fine-tuning tasks, our experiments have shown that our new multimodal representation can achieve competitive results in protein-ligand binding affinity prediction, protein fold classification, enzyme identification and mutation stability prediction. We expect that this work will accelerate future research in proteins. Our source code in PyTorch deep learning framework is publicly available at https://github.com/HySonLab/Protein\_Pretrain.},
	language = {en},
	urldate = {2023-12-20},
	publisher = {bioRxiv},
	author = {Nguyen, Viet Thanh Duy and Hy, Truong Son},
	month = dec,
	year = {2023},
	note = {Pages: 2023.11.29.569288
Section: New Results},
	file = {Full Text PDF:files/1324/Nguyen 和 Hy - 2023 - Multimodal Pretraining for Unsupervised Protein Re.pdf:application/pdf},
}

@misc{huang_data-efficient_2023,
	title = {Data-{Efficient} {Protein} {3D} {Geometric} {Pretraining} via {Refinement} of {Diffused} {Protein} {Structure} {Decoy}},
	
	
	abstract = {Learning meaningful protein representation is important for a variety of biological downstream tasks such as structure-based drug design. Having witnessed the success of protein sequence pretraining, pretraining for structural data which is more informative has become a promising research topic. However, there are three major challenges facing protein structure pretraining: insufficient sample diversity, physically unrealistic modeling, and the lack of protein-specific pretext tasks. To try to address these challenges, we present the 3D Geometric Pretraining. In this paper, we propose a unified framework for protein pretraining and a 3D geometric-based, data-efficient, and protein-specific pretext task: RefineDiff (Refine the Diffused Protein Structure Decoy). After pretraining our geometric-aware model with this task on limited data(less than 1\% of SOTA models), we obtained informative protein representations that can achieve comparable performance for various downstream tasks.},
	urldate = {2023-12-20},
	publisher = {arXiv},
	author = {Huang, Yufei and Wu, Lirong and Lin, Haitao and Zheng, Jiangbin and Wang, Ge and Li, Stan Z.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.10888 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Quantitative Biology - Biomolecules},
	file = {arXiv Fulltext PDF:files/1326/Huang 等 - 2023 - Data-Efficient Protein 3D Geometric Pretraining vi.pdf:application/pdf;arXiv.org Snapshot:files/1325/2302.html:text/html},
}

@misc{shi2022benchmarking,
  doi = {10.48550/ARXIV.2203.04810},

  url = {https://arxiv.org/abs/2203.04810},

  author = {Shi, Yu and Zheng, Shuxin and Ke, Guolin and Shen, Yifei and You, Jiacheng and He, Jiyan and Luo, Shengjie and Liu, Chang and He, Di and Liu, Tie-Yan},

  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Benchmarking Graphormer on Large-Scale Molecular Modeling Datasets},

  publisher = {arXiv},

  year = {2022},

  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{zhou2022protein,
  title={Protein representation learning via knowledge enhanced primary structure reasoning},
  author={Zhou, Hong-Yu and Fu, Yunxiang and Zhang, Zhicheng and Cheng, Bian and Yu, Yizhou},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}


@inproceedings{fan2022continuous,
  title={Continuous-discrete convolution for geometry-sequence modeling in proteins},
  author={Fan, Hehe and Wang, Zhangyang and Yang, Yi and Kankanhalli, Mohan},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@misc{liu_symmetry-informed_2023,
	title = {Symmetry-{Informed} {Geometric} {Representation} for {Molecules}, {Proteins}, and {Crystalline} {Materials}},
	
	
	abstract = {Artificial intelligence for scientific discovery has recently generated significant interest within the machine learning and scientific communities, particularly in the domains of chemistry, biology, and material discovery. For these scientific problems, molecules serve as the fundamental building blocks, and machine learning has emerged as a highly effective and powerful tool for modeling their geometric structures. Nevertheless, due to the rapidly evolving process of the field and the knowledge gap between science (e.g., physics, chemistry, \& biology) and machine learning communities, a benchmarking study on geometrical representation for such data has not been conducted. To address such an issue, in this paper, we first provide a unified view of the current symmetry-informed geometric methods, classifying them into three main categories: invariance, equivariance with spherical frame basis, and equivariance with vector frame basis. Then we propose a platform, coined Geom3D, which enables benchmarking the effectiveness of geometric strategies. Geom3D contains 16 advanced symmetry-informed geometric representation models and 14 geometric pretraining methods over 46 diverse datasets, including small molecules, proteins, and crystalline materials. We hope that Geom3D can, on the one hand, eliminate barriers for machine learning researchers interested in exploring scientific problems; and, on the other hand, provide valuable guidance for researchers in computational chemistry, structural biology, and materials science, aiding in the informed selection of representation techniques for specific applications.},
	urldate = {2023-12-23},
	publisher = {arXiv},
	author = {Liu, Shengchao and Du, Weitao and Li, Yanjing and Li, Zhuoxinran and Zheng, Zhiling and Duan, Chenru and Ma, Zhiming and Yaghi, Omar and Anandkumar, Anima and Borgs, Christian and Chayes, Jennifer and Guo, Hongyu and Tang, Jian},
	month = jun,
	year = {2023},
	note = {arXiv:2306.09375 [physics, q-bio]},
	keywords = {Computer Science - Machine Learning, Physics - Chemical Physics, Quantitative Biology - Quantitative Methods},
	file = {arXiv Fulltext PDF:files/1336/Liu 等 - 2023 - Symmetry-Informed Geometric Representation for Mol.pdf:application/pdf;arXiv.org Snapshot:files/1337/2306.html:text/html},
}

@inproceedings{zhang2023full,
  title={Full-Atom Protein Pocket Design via Iterative Refinement},
  author={ZHANG, ZAIXI and Lu, Zepu and Hao, Zhongkai and Zitnik, Marinka and Liu, Qi},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}

@article{abdin_pepnn_2022,
	title = {{PepNN}: a deep attention model for the identification of peptide binding sites},
	volume = {5},
	copyright = {2022 The Author(s)},
	
	shorttitle = {{PepNN}},
	
	
	abstract = {Protein-peptide interactions play a fundamental role in many cellular processes, but remain underexplored experimentally and difficult to model computationally. Here, we present PepNN-Struct and PepNN-Seq, structure and sequence-based approaches for the prediction of peptide binding sites on a protein. A main difficulty for the prediction of peptide-protein interactions is the flexibility of peptides and their tendency to undergo conformational changes upon binding. Motivated by this, we developed reciprocal attention to simultaneously update the encodings of peptide and protein residues while enforcing symmetry, allowing for information flow between the two inputs. PepNN integrates this module with modern graph neural network layers and a series of transfer learning steps are used during training to compensate for the scarcity of peptide-protein complex information. We show that PepNN-Struct achieves consistently high performance across different benchmark datasets. We also show that PepNN makes reasonable peptide-agnostic predictions, allowing for the identification of novel peptide binding proteins.},
	language = {en},
	number = {1},
	urldate = {2023-12-26},
	journal = {Communications Biology},
	author = {Abdin, Osama and Nim, Satra and Wen, Han and Kim, Philip M.},
	month = may,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Molecular modelling},
	pages = {1--10},
	file = {Full Text PDF:files/1345/Abdin 等 - 2022 - PepNN a deep attention model for the identificati.pdf:application/pdf},
}

@article{chandra_pepcnn_2023-1,
	title = {{PepCNN} deep learning tool for predicting peptide binding residues in proteins using sequence, structural, and language model features},
	volume = {13},
	copyright = {2023 The Author(s)},
	
	
	
	abstract = {Protein–peptide interactions play a crucial role in various cellular processes and are implicated in abnormal cellular behaviors leading to diseases such as cancer. Therefore, understanding these interactions is vital for both functional genomics and drug discovery efforts. Despite a significant increase in the availability of protein–peptide complexes, experimental methods for studying these interactions remain laborious, time-consuming, and expensive. Computational methods offer a complementary approach but often fall short in terms of prediction accuracy. To address these challenges, we introduce PepCNN, a deep learning-based prediction model that incorporates structural and sequence-based information from primary protein sequences. By utilizing a combination of half-sphere exposure, position specific scoring matrices from multiple-sequence alignment tool, and embedding from a pre-trained protein language model, PepCNN outperforms state-of-the-art methods in terms of specificity, precision, and AUC. The PepCNN software and datasets are publicly available at https://github.com/abelavit/PepCNN.git.},
	language = {en},
	number = {1},
	urldate = {2023-12-26},
	journal = {Scientific Reports},
	author = {Chandra, Abel and Sharma, Alok and Dehzangi, Iman and Tsunoda, Tatsuhiko and Sattar, Abdul},
	month = nov,
	year = {2023},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Bioinformatics, Computational biology and bioinformatics, Computational models, Machine learning, Proteomics},
	pages = {20882},
	file = {Full Text PDF:files/1346/Chandra 等 - 2023 - PepCNN deep learning tool for predicting peptide b.pdf:application/pdf},
}

@misc{jing_crossbind_2023,
	title = {{CrossBind}: {Collaborative} {Cross}-{Modal} {Identification} of {Protein} {Nucleic}-{Acid}-{Binding} {Residues}},
	shorttitle = {{CrossBind}},
	
	
	abstract = {Accurate identification of protein nucleic-acid-binding residues poses a significant challenge with important implications for various biological processes and drug design. Many typical computational methods for protein analysis rely on a single model that could ignore either the semantic context of the protein or the global 3D geometric information. Consequently, these approaches may result in incomplete or inaccurate protein analysis. To address the above issue, in this paper, we present CrossBind, a novel collaborative cross-modal approach for identifying binding residues by exploiting both protein geometric structure and its sequence prior knowledge extracted from a large-scale protein language model. Specifically, our multi-modal approach leverages a contrastive learning technique and atom-wise attention to capture the positional relationships between atoms and residues, thereby incorporating fine-grained local geometric knowledge, for better binding residue prediction. Extensive experimental results demonstrate that our approach outperforms the next best state-of-the-art methods, GraphSite and GraphBind, on DNA and RNA datasets by 10.8/17.3\% in terms of the harmonic mean of precision and recall (F1-Score) and 11.9/24.8\% in Matthews correlation coefficient (MCC), respectively. We release the code at https://github.com/BEAM-Labs/CrossBind.},
	urldate = {2023-12-27},
	publisher = {arXiv},
	author = {Jing, Linglin and Xu, Sheng and Wang, Yifan and Zhou, Yuzhe and Shen, Tao and Ji, Zhigang and Fang, Hui and Li, Zhen and Sun, Siqi},
	month = dec,
	year = {2023},
	note = {arXiv:2312.12094 [q-bio]},
	keywords = {Quantitative Biology - Biomolecules},
	file = {arXiv Fulltext PDF:files/1359/Jing 等 - 2023 - CrossBind Collaborative Cross-Modal Identificatio.pdf:application/pdf;arXiv.org Snapshot:files/1360/2312.html:text/html},
}

@article{mcdermott_structure-inducing_2023,
	title = {Structure-inducing pre-training},
	volume = {5},
	copyright = {2023 The Author(s)},
	
	
	
	abstract = {Language model pre-training and the derived general-purpose methods have reshaped machine learning research. However, there remains considerable uncertainty regarding why pre-training improves the performance of downstream tasks. This challenge is pronounced when using language model pre-training in domains outside of natural language. Here we investigate this problem by analysing how pre-training methods impose relational structure in induced per-sample latent spaces—that is, what constraints do pre-training methods impose on the distance or geometry between the pre-trained embeddings of samples. A comprehensive review of pre-training methods reveals that this question remains open, despite theoretical analyses showing the importance of understanding this form of induced structure. Based on this review, we introduce a pre-training framework that enables a granular and comprehensive understanding of how relational structure can be induced. We present a theoretical analysis of the framework from the first principles and establish a connection between the relational inductive bias of pre-training and fine-tuning performance. Empirical studies spanning three data modalities and ten fine-tuning tasks confirm theoretical analyses, inform the design of novel pre-training methods and establish consistent improvements over a compelling suite of methods.},
	language = {en},
	number = {6},
	urldate = {2024-01-01},
	journal = {Nature Machine Intelligence},
	author = {McDermott, Matthew B. A. and Yap, Brendan and Szolovits, Peter and Zitnik, Marinka},
	month = jun,
	year = {2023},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {Computational models, Computer science, Machine learning, Statistics},
	pages = {612--621},
	file = {Full Text PDF:files/1394/McDermott 等 - 2023 - Structure-inducing pre-training.pdf:application/pdf},
}




@misc{jing_equivariant_2021,
	title = {Equivariant {Graph} {Neural} {Networks} for {3D} {Macromolecular} {Structure}},
	
	
	abstract = {Representing and reasoning about 3D structures of macromolecules is emerging as a distinct challenge in machine learning. Here, we extend recent work on geometric vector perceptrons and apply equivariant graph neural networks to a wide range of tasks from structural biology. Our method outperforms all reference architectures on three out of eight tasks in the ATOM3D benchmark, is tied for first on two others, and is competitive with equivariant networks using higher-order representations and spherical harmonic convolutions. In addition, we demonstrate that transfer learning can further improve performance on certain downstream tasks. Code is available at https://github.com/drorlab/gvp-pytorch.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Jing, Bowen and Eismann, Stephan and Soni, Pratham N. and Dror, Ron O.},
	month = jul,
	year = {2021},
	note = {arXiv:2106.03843 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Biomolecules},
	file = {arXiv Fulltext PDF:files/1400/Jing 等 - 2021 - Equivariant Graph Neural Networks for 3D Macromole.pdf:application/pdf;arXiv.org Snapshot:files/1399/2106.html:text/html},
}


@inproceedings{noauthor_proteinexus_2023,
	title = {{ProteiNexus}: {Illuminating} {Protein} {Pathways} through {Structural} {Pre}-training},
	shorttitle = {{ProteiNexus}},
	
	abstract = {Protein representation learning has emerged as a powerful tool for various biological tasks. Language models derived from protein sequences represent the predominant trend in many current approaches. However, recent advances reveal that protein sequences alone cannot fully encapsulate the abundant information contained within protein structures, critical for understanding protein function and aiding innovative protein design. In this study, we present ProteiNexus, an innovative approach, effectively integrating protein structure learning with numerous downstream tasks. We propose a structural encoding mechanism adept at capturing fine-grained distance details and spatial positioning. By implementing a robust pre-training strategy and fine-tuning with lightweight decoders designed for specific downstream tasks, our model exhibits outstanding performance, establishing new benchmarks across a range of tasks. The code and models could be found at github repos.},
	language = {en},
	urldate = {2024-01-02},
	month = oct,
	year = {2023},
	file = {Full Text PDF:files/1407/2023 - ProteiNexus Illuminating Protein Pathways through.pdf:application/pdf},
}

@article{dai_protein_2021,
	title = {Protein interaction interface region prediction by geometric deep learning},
	volume = {37},
	
	
	
	abstract = {Protein–protein interactions drive wide-ranging molecular processes, and characterizing at the atomic level how proteins interact (beyond just the fact that they interact) can provide key insights into understanding and controlling this machinery. Unfortunately, experimental determination of three-dimensional protein complex structures remains difficult and does not scale to the increasingly large sets of proteins whose interactions are of interest. Computational methods are thus required to meet the demands of large-scale, high-throughput prediction of how proteins interact, but unfortunately, both physical modeling and machine learning methods suffer from poor precision and/or recall.In order to improve performance in predicting protein interaction interfaces, we leverage the best properties of both data- and physics-driven methods to develop a unified Geometric Deep Neural Network, ‘PInet’ (Protein Interface Network). PInet consumes pairs of point clouds encoding the structures of two partner proteins, in order to predict their structural regions mediating interaction. To make such predictions, PInet learns and utilizes models capturing both geometrical and physicochemical molecular surface complementarity. In application to a set of benchmarks, PInet simultaneously predicts the interface regions on both interacting proteins, achieving performance equivalent to or even much better than the state-of-the-art predictor for each dataset. Furthermore, since PInet is based on joint segmentation of a representation of a protein surfaces, its predictions are meaningful in terms of the underlying physical complementarity driving molecular recognition.PInet scripts and models are available at https://github.com/FTD007/PInet.Supplementary data are available at Bioinformatics online.},
	number = {17},
	urldate = {2024-01-07},
	journal = {Bioinformatics},
	author = {Dai, Bowen and Bailey-Kellogg, Chris},
	month = sep,
	year = {2021},
	pages = {2580--2588},
	file = {Full Text PDF:files/1423/Dai 和 Bailey-Kellogg - 2021 - Protein interaction interface region prediction by.pdf:application/pdf;Snapshot:files/1422/6162157.html:text/html},
}

@article{gligorijevic2021structure,
  title={Structure-based protein function prediction using graph convolutional networks},
  author={Gligorijevi{\'c}, Vladimir and Renfrew, P Douglas and Kosciolek, Tomasz and Leman, Julia Koehler and Berenberg, Daniel and Vatanen, Tommi and Chandler, Chris and Taylor, Bryn C and Fisk, Ian M and Vlamakis, Hera and others},
  journal={Nature communications},
  volume={12},
  number={1},
  pages={3168},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{ProtTrans,
  author       = {Ahmed Elnaggar and
                  Michael Heinzinger and
                  Christian Dallago and
                  Ghalia Rehawi and
                  Yu Wang and
                  Llion Jones and
                  Tom Gibbs and
                  Tamas Feher and
                  Christoph Angerer and
                  Martin Steinegger and
                  Debsindhu Bhowmik and
                  Burkhard Rost},
  title        = {ProtTrans: Towards Cracking the Language of Life's Code Through Self-Supervised
                  Deep Learning and High Performance Computing},
  journal      = {CoRR},
  volume       = {abs/2007.06225},
  year         = {2020},
  eprinttype    = {arXiv},
  eprint       = {2007.06225},
  timestamp    = {Wed, 28 Dec 2022 14:59:10 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2007-06225.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{lin2023evolutionary,
  title={Evolutionary-scale prediction of atomic-level protein structure with a language model},
  author={Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Smetanin, Nikita and Verkuil, Robert and Kabeli, Ori and Shmueli, Yaniv and others},
  journal={Science},
  volume={379},
  number={6637},
  pages={1123--1130},
  year={2023},
  publisher={American Association for the Advancement of Science}
}

@misc{hermosilla_intrinsic-extrinsic_2021,
  title={Intrinsic-Extrinsic Convolution and Pooling for Learning on 3D Protein Structures},
  author={Hermosilla, Pedro and Sch{\"a}fer, Marco and Lang, Matej and Fackelmann, Gloria and V{\'a}zquez, Pere-Pau and Kozlikova, Barbora and Krone, Michael and Ritschel, Tobias and Ropinski, Timo},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{mcdermott_structure-inducing_2023-1,
	title = {Structure-inducing pre-training},
	volume = {5},
	copyright = {2023 The Author(s)},
	
	
	
	abstract = {Language model pre-training and the derived general-purpose methods have reshaped machine learning research. However, there remains considerable uncertainty regarding why pre-training improves the performance of downstream tasks. This challenge is pronounced when using language model pre-training in domains outside of natural language. Here we investigate this problem by analysing how pre-training methods impose relational structure in induced per-sample latent spaces—that is, what constraints do pre-training methods impose on the distance or geometry between the pre-trained embeddings of samples. A comprehensive review of pre-training methods reveals that this question remains open, despite theoretical analyses showing the importance of understanding this form of induced structure. Based on this review, we introduce a pre-training framework that enables a granular and comprehensive understanding of how relational structure can be induced. We present a theoretical analysis of the framework from the first principles and establish a connection between the relational inductive bias of pre-training and fine-tuning performance. Empirical studies spanning three data modalities and ten fine-tuning tasks confirm theoretical analyses, inform the design of novel pre-training methods and establish consistent improvements over a compelling suite of methods.},
	language = {en},
	number = {6},
	urldate = {2024-01-16},
	journal = {Nature Machine Intelligence},
	author = {McDermott, Matthew B. A. and Yap, Brendan and Szolovits, Peter and Zitnik, Marinka},
	month = jun,
	year = {2023},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {Computational models, Computer science, Machine learning, Statistics},
	pages = {612--621},
	file = {Full Text PDF:files/1446/McDermott 等 - 2023 - Structure-inducing pre-training.pdf:application/pdf},
}

@article{xu_protst_2023,
	title = {{ProtST}: {Multi}-{Modality} {Learning} of {Protein} {Sequences} and {Biomedical} {Texts}},
	shorttitle = {{ProtST}},
	
	abstract = {Current protein language models (PLMs) learn protein representations mainly based on their sequences, thereby well capturing co-evolutionary information, but they are unable to explicitly acquire protein functions, which is the end goal of protein representation learning. Fortunately, for many proteins, their textual property descriptions are available, where their various functions are also described. Motivated by this fact, we first build the ProtDescribe dataset to augment protein sequences with text descriptions of their functions and other important properties. Based on this dataset, we propose the ProtST framework to enhance Protein Sequence pre-training and understanding by biomedical Texts. During pre-training, we design three types of tasks, i.e., unimodal mask prediction, multimodal representation alignment and multimodal mask prediction, to enhance a PLM with protein property information with different granularities and, at the same time, preserve the PLM's original representation power. On downstream tasks, ProtST enables both supervised learning and zero-shot prediction. We verify the superiority of ProtST-induced PLMs over previous ones on diverse representation learning benchmarks. Under the zero-shot setting, we show the effectiveness of ProtST on zero-shot protein classification, and ProtST also enables functional protein retrieval from a large-scale database without any function annotation.},
	language = {en},
	urldate = {2024-01-16},
	author = {Xu, Minghao and Yuan, Xinyu and Miret, Santiago and Tang, Jian},
	month = jun,
	year = {2023},
	file = {Full Text PDF:files/1448/Xu 等 - 2023 - ProtST Multi-Modality Learning of Protein Sequenc.pdf:application/pdf},
}

@inproceedings{noauthor_evaluating_2023,
	title = {Evaluating {Representation} {Learning} on the {Protein} {Structure} {Universe}},
	
	abstract = {Protein structure representation learning is the foundation for promising applications in drug discovery, protein design, and protein function prediction. However, there remains a need for a robust, standardised benchmark to track the progress of new and established methods with greater granularity and relevance to downstream applications. In this work, we introduce a comprehensive and open benchmark suite for evaluating protein structure representation learning methods. We provide several pre-training methods, downstream tasks and pre-training corpora comprised of both experimental and predicted structures, offering a balanced challenge to representation learning algorithms. These tasks enable the systematic evaluation of the quality of the learned embeddings, the structural and functional relationships captured, and their usefulness in downstream tasks. We benchmark state-of-the-art protein-specific and generic geometric Graph Neural Networks and the extent to which they benefit from different types of pre-training. We find that pre-training consistently improves the performance of both rotation-invariant and equivariant models, and that equivariant models seem to benefit even more from pre-training compared to invariant models. We aim to establish a common ground for the machine learning and computational biology communities to collaborate, compare, and advance protein structure representation learning. By providing a standardised and rigorous evaluation platform, we expect to accelerate the development of novel methodologies and improve our understanding of protein structures and their functions. The codebase incorporates several engineering contributions which considerably reduces the barrier to entry for pre-training and working with large structure-based datasets. Our benchmark is available at: https://anonymous.4open.science/r/ProteinWorkshop-B8F5/},
	language = {en},
	urldate = {2024-01-19},
	month = oct,
	year = {2023},
	file = {Full Text PDF:files/1452/2023 - Evaluating Representation Learning on the Protein .pdf:application/pdf},
}

@article{rose2016rcsb,
  title={The RCSB protein data bank: integrative view of protein, gene and 3D structural information},
  author={Rose, Peter W and Prli{\'c}, Andreas and Altunkaya, Ali and Bi, Chunxiao and Bradley, Anthony R and Christie, Cole H and Costanzo, Luigi Di and Duarte, Jose M and Dutta, Shuchismita and Feng, Zukang and others},
  journal={Nucleic acids research},
  pages={gkw1000},
  year={2016},
  publisher={Oxford University Press}
}

@inproceedings{khadhraoui_hierarchical_2023,
	title = {Hierarchical {Protein} {Representation} for {Interface} {Co}-design with {HICON}},
	
	abstract = {Protein-protein interactions (PPIs) are essential for many biological processes, but their design is challenging due to their complex and dynamic nature. We propose a new model called Hierarchical Interface CO-design Network (HICON) that can jointly generate the sequence and 3D structure of protein interfaces. HICON uses a novel hierarchical architecture that combines atomic and amino acid resolutions in an equivariant manner and leverages Large Protein Language Models for sequence initialization. We evaluate HICON on a variety of biological interfaces, including protein-protein, enzyme-ligand, and antibody paratope-epitope interfaces. Our results show that HICON outperforms state-of-the-art models on sequence prediction and paratope co-design on several computational metrics.},
	language = {en},
	urldate = {2024-01-19},
	author = {Khadhraoui, Aous and Gutierrez, Daniel Nakhaee-Zadeh and Kozlova, Elizaveta},
	month = oct,
	year = {2023},
	file = {Full Text PDF:files/1454/Khadhraoui 等 - 2023 - Hierarchical Protein Representation for Interface .pdf:application/pdf},
}

@article{noauthor_multi-scale_2023,
	title = {Multi-{Scale} {Protein} {Language} {Model} for {Unified} {Molecular} {Modeling}},
	
	abstract = {Protein language models have shown great potential in protein engineering. However, the current protein language models mainly work in the residue scale, which cannot offer information in the atom scale. The strong power of protein language models could not be fully exploited to benefit the applications that cross protein and small molecules. In this paper, we propose msESM(multi-scale ESM) to realize the multi-scale unified molecular modeling by pre-training on multi-scale code-switch protein sequence and describing relationships among residues and atoms with a multi-scale position encoding. Experimental results show that msESM outperforms previous methods in protein-molecule tasks and is on par with the state-of-the-art in protein-only and molecule-only tasks.},
	language = {en},
	urldate = {2024-01-19},
	month = oct,
	year = {2023},
}

@article{noauthor_proteinadapter_2023,
	title = {{ProteinAdapter}: {Adapting} {Pre}-trained {Large} {Protein} {Models} for {Efficient} {Protein} {Representation} {Learning}},
	shorttitle = {{ProteinAdapter}},
	
	abstract = {The study of proteins is crucial in various scientific disciplines, but understanding their intricate multi-level relationships remains challenging. Recent advancements in Large Protein Models (LPMs) have demonstrated their ability in sequence and structure understanding, suggesting the potential of directly using them for efficient protein representation learning. In this work, we introduce ProteinAdapter, to efficiently transfer the general reference from the multiple Large Protein Models (LPMs), e.g., ESM-1b, to the task-specific knowledge. ProteinAdapter could largely save labor-intensive analysis on the 3D position and the amino acid order. We observe that such a simple yet effective approach works well on multiple downstream tasks. Specifically, (1) with limited extra parameters, ProteinAdapter enables multi-level protein representation learning by integrating both sequence and geometric structure embeddings from LPMs. (2) Based on the learned embedding, we further scale the proposed ProteinAdapter to multiple conventional protein tasks. Considering different task priors, we propose a unified multi-scale predictor to fully take advantage of the learned embeddings via task-specific focus. Extensive experiments on over 20 tasks show that ProteinAdapter outperforms state-of-the-art methods under both single-task and multi-task settings. We hope that the proposed method could accelerate the study of protein analysis in the future.},
	language = {en},
	urldate = {2024-01-19},
	month = oct,
	year = {2023},
	file = {Full Text PDF:files/1458/2023 - ProteinAdapter Adapting Pre-trained Large Protein.pdf:application/pdf},
}


@InProceedings{pmlr-v162-hsu22a,
  title = 	 {Learning inverse folding from millions of predicted structures},
  author =       {Hsu, Chloe and Verkuil, Robert and Liu, Jason and Lin, Zeming and Hie, Brian and Sercu, Tom and Lerer, Adam and Rives, Alexander},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {8946--8970},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/hsu22a/hsu22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/hsu22a.html},
  abstract = 	 {We consider the problem of predicting a protein sequence from its backbone atom coordinates. Machine learning approaches to this problem to date have been limited by the number of available experimentally determined protein structures. We augment training data by nearly three orders of magnitude by predicting structures for 12M protein sequences using AlphaFold2. Trained with this additional data, a sequence-to-sequence transformer with invariant geometric input processing layers achieves 51% native sequence recovery on structurally held-out backbones with 72% recovery for buried residues, an overall improvement of almost 10 percentage points over existing methods. The model generalizes to a variety of more complex tasks including design of protein complexes, partially masked structures, binding interfaces, and multiple states.}
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@misc{krishna_generalized_2023,
	title = {Generalized {Biomolecular} {Modeling} and {Design} with {RoseTTAFold} {All}-{Atom}},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	
	
	abstract = {Although AlphaFold2 (AF2) and RoseTTAFold (RF) have transformed structural biology by enabling high-accuracy protein structure modeling, they are unable to model covalent modifications or interactions with small molecules and other non-protein molecules that can play key roles in biological function. Here, we describe RoseTTAFold All-Atom (RFAA), a deep network capable of modeling full biological assemblies containing proteins, nucleic acids, small molecules, metals, and covalent modifications given the sequences of the polymers and the atomic bonded geometry of the small molecules and covalent modifications. Following training on structures of full biological assemblies in the Protein Data Bank (PDB), RFAA has comparable protein structure prediction accuracy to AF2, excellent performance in CAMEO for flexible backbone small molecule docking, and reasonable prediction accuracy for protein covalent modifications and assemblies of proteins with multiple nucleic acid chains and small molecules which, to our knowledge, no existing method can model simultaneously. By fine-tuning on diffusive denoising tasks, we develop RFdiffusion All-Atom (RFdiffusionAA), which generates binding pockets by directly building protein structures around small molecules and other non-protein molecules. Starting from random distributions of amino acid residues surrounding target small molecules, we design and experimentally validate proteins that bind the cardiac disease therapeutic digoxigenin, the enzymatic cofactor heme, and optically active bilin molecules with potential for expanding the range of wavelengths captured by photosynthesis. We anticipate that RFAA and RFdiffusionAA will be widely useful for modeling and designing complex biomolecular systems.},
	language = {en},
	urldate = {2024-01-19},
	publisher = {bioRxiv},
	author = {Krishna, Rohith and Wang, Jue and Ahern, Woody and Sturmfels, Pascal and Venkatesh, Preetham and Kalvet, Indrek and Lee, Gyu Rie and Morey-Burrows, Felix S. and Anishchenko, Ivan and Humphreys, Ian R. and McHugh, Ryan and Vafeados, Dionne and Li, Xinting and Sutherland, George A. and Hitchcock, Andrew and Hunter, C. Neil and Baek, Minkyung and DiMaio, Frank and Baker, David},
	month = oct,
	year = {2023},
	note = {Pages: 2023.10.09.561603
Section: New Results},
	file = {Full Text PDF:files/1460/Krishna 等 - 2023 - Generalized Biomolecular Modeling and Design with .pdf:application/pdf},
}

@inproceedings{zhang_enhancing_2023,
  title={Enhancing Protein Language Model with Structure-based Encoder and Pre-training},
  author={Zhang, Zuobai and Xu, Minghao and Lozano, Aurelie and Chenthamarakshan, Vijil and Das, Payel and Tang, Jian},
  booktitle={ICLR 2023-Machine Learning for Drug Discovery workshop},
  year={2023}
}

@inproceedings{noauthor_pre-training_2023,
  title={Pre-training Sequence, Structure, and Surface Features for Comprehensive Protein Representation Learning},
  year = {2023},
  author={Lee, Youhan and Yu, Hasun and Lee, Jaemyung and Kim, Jaehoon and Brain, Kakao}
}
@inproceedings{zhou_uni-mol_2023,
	title = {Uni-Mol: A Universal 3D Molecular Representation Learning Framework},
	year = {2023},
	shorttitle = {Uni-Mol},
	abstract = {Molecular representation learning ({MRL}) has gained tremendous attention due to its critical role in learning from limited supervised data for applications like drug design. In most {MRL} methods, molecules are treated as 1D sequential tokens or 2D topology graphs, limiting their ability to incorporate 3D information for downstream tasks and, in particular, making it almost impossible for 3D geometry prediction/generation. In this paper, we propose a universal 3D {MRL} framework, called Uni-Mol, that significantly enlarges the representation ability and application scope of {MRL} schemes. Uni-Mol contains two pretrained models with the same {SE}(3) Transformer architecture: a molecular model pretrained by 209M molecular conformations; a pocket model pretrained by 3M candidate protein pocket data. Besides, Uni-Mol contains several finetuning strategies to apply the pretrained models to various downstream tasks. By properly incorporating 3D information, Uni-Mol outperforms {SOTA} in 14/15 molecular property prediction tasks. Moreover, Uni-Mol achieves superior performance in 3D spatial tasks, including protein-ligand binding pose prediction, molecular conformation generation, etc. The code, model, and data are made publicly available at https://github.com/dptech-corp/Uni-Mol.},
	booktitle = {The Eleventh International Conference on Learning Representations},
	author = {Zhou, Gengmo and Gao, Zhifeng and Ding, Qiankun and Zheng, Hang and Xu, Hongteng and Wei, Zhewei and Zhang, Linfeng and Ke, Guolin},
	urldate = {2023-05-17},
	date = {2023-02-01},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\10076\\Zotero\\storage\\3H3S67G2\\Zhou 等 - 2023 - Uni-Mol A Universal 3D Molecular Representation L.pdf:application/pdf},
}


@misc{yifan_input-level_2022,
  title={Input-level inductive biases for 3D reconstruction},
  author={Yifan, Wang and Doersch, Carl and Arandjelovi{\'c}, Relja and Carreira, Joao and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6176--6186},
  year={2022}
}

@misc{yu_pixelnerf_2021,
  title={pixelnerf: Neural radiance fields from one or few images},
  author={Yu, Alex and Ye, Vickie and Tancik, Matthew and Kanazawa, Angjoo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4578--4587},
  year={2021}
}

@article{tateno1997evolutionary,
  title={Evolutionary motif and its biological and structural significance},
  author={Tateno, Y and Ikeo, K and Imanishi, T and Watanabe, H and Endo, T and Yamaguchi, Y and Suzuki, Y and Takahashi, K and Tsunoyama, K and Kawai, M and others},
  journal={Journal of molecular evolution},
  volume={44},
  pages={S38--S43},
  year={1997},
  publisher={Springer}
}

@misc{corso_diffdock_2023,
	title = {{DiffDock}: {Diffusion} {Steps}, {Twists}, and {Turns} for {Molecular} {Docking}},
	shorttitle = {{DiffDock}},
	
	
	abstract = {Predicting the binding structure of a small molecule ligand to a protein -- a task known as molecular docking -- is critical to drug design. Recent deep learning methods that treat docking as a regression problem have decreased runtime compared to traditional search-based methods but have yet to offer substantial improvements in accuracy. We instead frame molecular docking as a generative modeling problem and develop DiffDock, a diffusion generative model over the non-Euclidean manifold of ligand poses. To do so, we map this manifold to the product space of the degrees of freedom (translational, rotational, and torsional) involved in docking and develop an efficient diffusion process on this space. Empirically, DiffDock obtains a 38\% top-1 success rate (RMSD{\textless}2A) on PDBBind, significantly outperforming the previous state-of-the-art of traditional docking (23\%) and deep learning (20\%) methods. Moreover, while previous methods are not able to dock on computationally folded structures (maximum accuracy 10.4\%), DiffDock maintains significantly higher precision (21.7\%). Finally, DiffDock has fast inference times and provides confidence estimates with high selective accuracy.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Corso, Gabriele and Stärk, Hannes and Jing, Bowen and Barzilay, Regina and Jaakkola, Tommi},
	month = feb,
	year = {2023},
	note = {arXiv:2210.01776 [physics, q-bio]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Biomolecules, Physics - Biological Physics},
	file = {arXiv Fulltext PDF:files/1403/Corso 等 - 2023 - DiffDock Diffusion Steps, Twists, and Turns for M.pdf:application/pdf;arXiv.org Snapshot:files/1402/2210.html:text/html},
}


@misc{krishna_generalized_2023-1,
	title = {Generalized {Biomolecular} {Modeling} and {Design} with {RoseTTAFold} {All}-{Atom}},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	
	
	abstract = {Although AlphaFold2 (AF2) and RoseTTAFold (RF) have transformed structural biology by enabling high-accuracy protein structure modeling, they are unable to model covalent modifications or interactions with small molecules and other non-protein molecules that can play key roles in biological function. Here, we describe RoseTTAFold All-Atom (RFAA), a deep network capable of modeling full biological assemblies containing proteins, nucleic acids, small molecules, metals, and covalent modifications given the sequences of the polymers and the atomic bonded geometry of the small molecules and covalent modifications. Following training on structures of full biological assemblies in the Protein Data Bank (PDB), RFAA has comparable protein structure prediction accuracy to AF2, excellent performance in CAMEO for flexible backbone small molecule docking, and reasonable prediction accuracy for protein covalent modifications and assemblies of proteins with multiple nucleic acid chains and small molecules which, to our knowledge, no existing method can model simultaneously. By fine-tuning on diffusive denoising tasks, we develop RFdiffusion All-Atom (RFdiffusionAA), which generates binding pockets by directly building protein structures around small molecules and other non-protein molecules. Starting from random distributions of amino acid residues surrounding target small molecules, we design and experimentally validate proteins that bind the cardiac disease therapeutic digoxigenin, the enzymatic cofactor heme, and optically active bilin molecules with potential for expanding the range of wavelengths captured by photosynthesis. We anticipate that RFAA and RFdiffusionAA will be widely useful for modeling and designing complex biomolecular systems.},
	language = {en},
	urldate = {2024-01-23},
	publisher = {bioRxiv},
	author = {Krishna, Rohith and Wang, Jue and Ahern, Woody and Sturmfels, Pascal and Venkatesh, Preetham and Kalvet, Indrek and Lee, Gyu Rie and Morey-Burrows, Felix S. and Anishchenko, Ivan and Humphreys, Ian R. and McHugh, Ryan and Vafeados, Dionne and Li, Xinting and Sutherland, George A. and Hitchcock, Andrew and Hunter, C. Neil and Baek, Minkyung and DiMaio, Frank and Baker, David},
	month = oct,
	year = {2023},
	note = {Pages: 2023.10.09.561603
Section: New Results},
	file = {Full Text PDF:files/1471/Krishna 等 - 2023 - Generalized Biomolecular Modeling and Design with .pdf:application/pdf},
}


@misc{rao_msa_2021,
  title={MSA transformer},
  author={Rao, Roshan M and Liu, Jason and Verkuil, Robert and Meier, Joshua and Canny, John and Abbeel, Pieter and Sercu, Tom and Rives, Alexander},
  booktitle={International Conference on Machine Learning},
  pages={8844--8856},
  year={2021},
  organization={PMLR}
}



@misc{chen_structure-aware_2023,
  title={Structure-aware protein self-supervised learning},
  author={Chen, Can and Zhou, Jingbo and Wang, Fan and Liu, Xue and Dou, Dejing},
  journal={Bioinformatics},
  volume={39},
  number={4},
  pages={btad189},
  year={2023},
  publisher={Oxford University Press}
}

@article{guo_self-supervised_2022,
	title = {Self-Supervised Pre-training for Protein Embeddings Using Tertiary Structures},
  author={Guo, Yuzhi and Wu, Jiaxiang and Ma, Hehuan and Huang, Junzhou},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={6},
  pages={6801--6809},
  year={2022}
}

@article{zhang_us-align_2022,
  title={US-align: universal structure alignments of proteins, nucleic acids, and macromolecular complexes},
  author={Zhang, Chengxin and Shine, Morgan and Pyle, Anna Marie and Zhang, Yang},
  journal={Nature methods},
  volume={19},
  number={9},
  pages={1109--1115},
  year={2022},
  publisher={Nature Publishing Group US New York}
}