@inproceedings{sutskever2011generating,
	title={Generating text with recurrent neural networks},
	author={Sutskever, Ilya and Martens, James and Hinton, Geoffrey E},
	booktitle={Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
	pages={1017--1024},
	year={2011}
}

@inproceedings{seq2seq,
	title={Sequence to sequence learning with neural networks},
	author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
	booktitle={Advances in neural information processing systems},
	pages={3104--3112},
	year={2014}
}

@inproceedings{amodei2016deep,
	title={Deep speech 2: End-to-end speech recognition in english and mandarin},
	author={Amodei, Dario and Ananthanarayanan, Sundaram and Anubhai, Rishita and Bai, Jingliang and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Cheng, Qiang and Chen, Guoliang and others},
	booktitle={International conference on machine learning},
	pages={173--182},
	year={2016}
}

@ARTICLE{bengio_frasconi_1994, 
	author={Y. {Bengio} and P. {Simard} and P. {Frasconi}}, 
	journal={IEEE Transactions on Neural Networks}, 
	title={Learning long-term dependencies with gradient descent is difficult}, 
	year={1994}, 
	volume={5}, 
	number={2}, 
	pages={157-166}, 
	keywords={recurrent neural nets;learning (artificial intelligence);numerical analysis;long-term dependencies;gradient descent;recognition;production problems;prediction problems;recurrent neural network training;temporal contingencies;input/output sequence mapping;efficient learning;Recurrent neural networks;Production;Delay effects;Intelligent networks;Neural networks;Discrete transforms;Computer networks;Cost function;Neurofeedback;Displays}, 
	doi={10.1109/72.279181}, 
	ISSN={1045-9227}, 
	month={March},}


@article{LSTM,
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	title = {Long Short-Term Memory},
	journal = {Neural Computation},
	volume = {9},
	number = {8},
	pages = {1735-1780},
	year = {1997},
	doi = {10.1162/neco.1997.9.8.1735},
	URL = { https://doi.org/10.1162/neco.1997.9.8.1735},
	eprint = { https://doi.org/10.1162/neco.1997.9.8.1735},
	abstract = { Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms. }
}


@article{bengio2007_depth,
	title={Scaling learning algorithms towards AI},
	author={Bengio, Yoshua and LeCun, Yann and others},
	journal={Large-scale kernel machines},
	volume={34},
	number={5},
	pages={1--41},
	year={2007}
}


@ARTICLE{bianchini_2014, 
	author={M. {Bianchini} and F. {Scarselli}}, 
	journal={IEEE Transactions on Neural Networks and Learning Systems}, 
	title={On the Complexity of Neural Network Classifiers: A Comparison Between Shallow and Deep Architectures}, 
	year={2014}, 
	volume={25}, 
	number={8}, 
	pages={1553-1565}, 
	keywords={computational complexity;feedforward neural nets;pattern classification;topology;neural network classifiers;shallow architecture;deep architecture;artificial neural network;hidden layers;vision;human language understanding;feedforward neural network;high complexity functions;topological concepts;function complexity evaluation;classification;sigmoidal activation function;deep network;Complexity theory;Neurons;Biological neural networks;Polynomials;Upper bound;Computer architecture;Betti numbers;deep neural networks;function approximation;topological complexity;Vapnik--Chervonenkis dimension (VC-dim).;Betti numbers;deep neural networks;function approximation;topological complexity;Vapnik–Chervonenkis dimension (VC-dim);Algorithms;Computer Simulation;Models, Theoretical;Nerve Net;Pattern Recognition, Automated}, 
	doi={10.1109/TNNLS.2013.2293637}, 
	ISSN={2162-237X}, 
	month={Aug},
}


@article{stacking_schmidhuber1992,
	title={Learning complex, extended sequences using the principle of history compression},
	author={Schmidhuber, J{\"u}rgen},
	journal={Neural Computation},
	volume={4},
	number={2},
	pages={234--242},
	year={1992},
	publisher={MIT Press}
}


@article{bengio_depth,
	url = {http://dx.doi.org/10.1561/2200000006},
	year = {2009},
	volume = {2},
	journal = {Foundations and Trends in Machine Learning},
	title = {Learning Deep Architectures for AI},
	doi = {10.1561/2200000006},
	issn = {1935-8237},
	number = {1},
	pages = {1-127},
	author = {Yoshua Bengio}
}


@inproceedings{pascanu_2014,
	title = "How to construct deep recurrent neural networks",
	author = "Razvan Pascanu and Caglar Gulcehre and Kyunghyun Cho and Yoshua Bengio",
	year = "2014",
	language = "English (US)",
	booktitle = "Proceedings of the Second International Conference on Learning Representations (ICLR 2014)",
}

@InProceedings{rhn,
  title = 	 {Recurrent Highway Networks},
  author = 	 {Julian Georg Zilly and Rupesh Kumar Srivastava and Jan Koutn\'{\i}k and J{\"u}rgen Schmidhuber},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {4189--4198},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/zilly17a/zilly17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/zilly17a.html},
  abstract = 	 {Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with “deep” transition functions remain difficult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Gersgorin’s circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which extend the LSTM architecture to allow step-to-step transition depths larger than one. Several language modeling experiments demonstrate that the proposed architecture results in powerful and efficient models. On the Penn Treebank corpus, solely increasing the transition depth from 1 to 10 improves word-level perplexity from 90.6 to 65.4 using the same number of parameters. On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.}
}

@INPROCEEDINGS{hmrnn,
	author = {Chung, Junyoung and Ahn, Sungjin and Bengio, Yoshua},
	keywords = {Deep Learning, natural language processing},
	title = {Hierarchical Multiscale Recurrent Neural Networks},
	year = {2017},
	url = {https://openreview.net/forum?id=S1di0sfgl},
	crossref = {ICLR2017-conf},
}

@inproceedings{skiprnn,
	title={Skip {RNN}: Learning to Skip State Updates in Recurrent Neural Networks},
	author={Víctor Campos and Brendan Jou and Xavier Giró-i-Nieto and Jordi Torres and Shih-Fu Chang},
	booktitle={International Conference on Learning Representations},
	year={2018},
	url={https://openreview.net/forum?id=HkwVAXyCW},
}

@InProceedings{clockworkRNN,
	title = 	 {A Clockwork RNN},
	author = 	 {Jan Koutnik and Klaus Greff and Faustino Gomez and Juergen Schmidhuber},
	booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
	pages = 	 {1863--1871},
	year = 	 {2014},
	editor = 	 {Eric P. Xing and Tony Jebara},
	volume = 	 {32},
	number =       {2},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Bejing, China},
	month = 	 {22--24 Jun},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v32/koutnik14.pdf},
	url = 	 {http://proceedings.mlr.press/v32/koutnik14.html},
	abstract = 	 {Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when long-term memory is  required.    This paper introduces a simple, yet powerful modification to the  simple RNN (SRN) architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate.    Rather than making the standard RNN models more complex, CW-RNN  reduces the number of SRN parameters, improves the performance  significantly in the tasks tested, and speeds up the network evaluation.    The network is demonstrated in preliminary experiments involving three tasks: audio signal generation, TIMIT spoken word classification,  where it outperforms both SRN and LSTM networks, and online handwriting recognition, where it outperforms SRNs.}
}

@InProceedings{conditionalRNN,
	title = 	 {Focused Hierarchical {RNN}s for Conditional Sequence Processing},
	author = 	 {Ke, Nan Rosemary and {\.Z}o{\l}na, Konrad and Sordoni, Alessandro and Lin, Zhouhan and Trischler, Adam and Bengio, Yoshua and Pineau, Joelle and Charlin, Laurent and Pal, Christopher},
	booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
	pages = 	 {2554--2563},
	year = 	 {2018},
	editor = 	 {Dy, Jennifer and Krause, Andreas},
	volume = 	 {80},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Stockholmsmässan, Stockholm Sweden},
	month = 	 {10--15 Jul},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v80/ke18a/ke18a.pdf},
	url = 	 {http://proceedings.mlr.press/v80/ke18a.html},
	abstract = 	 {Recurrent Neural Networks (RNNs) with attention mechanisms have obtained state-of-the-art results for many sequence processing tasks. Most of these models use a simple form of encoder with attention that looks over the entire sequence and assigns a weight to each token independently. We present a mechanism for focusing RNN encoders for sequence modelling tasks which allows them to attend to key parts of the input as needed. We formulate this using a multi-layer conditional hierarchical sequence encoder that reads in one token at a time and makes a discrete decision on whether the token is relevant to the context or question being asked. The discrete gating mechanism takes in the context embedding and the current hidden state as inputs and controls information flow into the layer above. We train it using policy gradient methods. We evaluate this method on several types of tasks with different attributes. First, we evaluate the method on synthetic tasks which allow us to evaluate the model for its generalization ability and probe the behavior of the gates in more controlled settings. We then evaluate this approach on large scale Question Answering tasks including the challenging MS MARCO and SearchQA tasks. Our models shows consistent improvements for both tasks over prior work and our baselines. It has also shown to generalize significantly better on synthetic tasks as compared to the baselines.}
}

@article{act,
	title={Adaptive computation time for recurrent neural networks},
	author={Graves, Alex},
	journal={arXiv preprint arXiv:1603.08983},
	year={2016}
}


@inproceedings{All2All,
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
	title = {Gated Feedback Recurrent Neural Networks},
	booktitle = {Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37},
	series = {ICML'15},
	year = {2015},
	location = {Lille, France},
	pages = {2067--2075},
	numpages = {9},
	url = {http://dl.acm.org/citation.cfm?id=3045118.3045338},
	acmid = {3045338},
	publisher = {JMLR.org},
} 


@ARTICLE{BiRNN, 
	author={M. {Schuster} and K. K. {Paliwal}}, 
	journal={IEEE Transactions on Signal Processing}, 
	title={Bidirectional recurrent neural networks}, 
	year={1997}, 
	volume={45}, 
	number={11}, 
	pages={2673-2681}, 
	keywords={speech recognition;pattern classification;statistical analysis;recurrent neural nets;learning by example;speech processing;bidirectional recurrent neural networks;training;regular recurrent neural network;negative time direction;positive time direction;classification experiments;regression experiments;artificial data;phonemes;TIMIT database;conditional posterior probability;complete symbol sequences;real data;speech recognition;learning from examples;Recurrent neural networks;Artificial neural networks;Training data;Databases;Probability;Shape;Parameter estimation;Speech recognition;Control systems;Telecommunication control}, 
	doi={10.1109/78.650093}, 
	ISSN={1053-587X}, 
	month={Nov},
}


@article{BiLSTM,
	title = "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
	journal = "Neural Networks",
	volume = "18",
	number = "5",
	pages = "602 - 610",
	year = "2005",
	note = "IJCNN 2005",
	issn = "0893-6080",
	doi = "https://doi.org/10.1016/j.neunet.2005.06.042",
	url = "http://www.sciencedirect.com/science/article/pii/S0893608005001206",
	author = "Alex Graves and Jürgen Schmidhuber",
	abstract = "In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it.11An abbreviated version of some portions of this article appeared in (Graves and Schmidhuber, 2005), as part of the IJCNN 2005 conference proceedings, published under the IEEE copyright."
}


@inproceedings{POSDualBiLSTM,
	title = "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss",
	author = "Plank, Barbara  and
	S{\o}gaard, Anders  and
	Goldberg, Yoav",
	booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
	month = aug,
	year = "2016",
	address = "Berlin, Germany",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/P16-2067",
	doi = "10.18653/v1/P16-2067",
	pages = "412--418",
}


@inproceedings{sentimentanalysis,
	title={Datastories at semeval-2017 task 4: Deep lstm with attention for message-level and topic-based sentiment analysis},
	author={Baziotis, Christos and Pelekis, Nikos and Doulkeridis, Christos},
	booktitle={Proceedings of the 11th international workshop on semantic evaluation (SemEval-2017)},
	pages={747--754},
	year={2017}
}


@InProceedings{IndRNN,
	author = {Li, Shuai and Li, Wanqing and Cook, Chris and Zhu, Ce and Gao, Yanbo},
	title = {Independently Recurrent Neural Network (IndRNN): Building a Longer and Deeper RNN},
	booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	year = {2018}
}



@incollection{tdRNN,
	title = {Architectural Complexity Measures of Recurrent Neural Networks},
	author = {Zhang, Saizheng and Wu, Yuhuai and Che, Tong and Lin, Zhouhan and Memisevic, Roland and Salakhutdinov, Ruslan R and Bengio, Yoshua},
	booktitle = {Advances in Neural Information Processing Systems 29},
	editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
	pages = {1822--1830},
	year = {2016},
	publisher = {Curran Associates, Inc.},
}


@article{stacking_graves,
	title={Generating Sequences With Recurrent Neural Networks},
	author={Alex Graves},
	journal={CoRR},
	year={2013},
	volume={abs/1308.0850}
}

@INPROCEEDINGS{TimeSeriesAnomalyDetection, 
	author={T. {Guo} and Z. {Xu} and X. {Yao} and H. {Chen} and K. {Aberer} and K. {Funaya}}, 
	booktitle={2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA)}, 
	title={Robust Online Time Series Prediction with Recurrent Neural Networks}, 
	year={2016}, 
	volume={}, 
	number={}, 
	pages={816-825}, 
	keywords={data analysis;learning (artificial intelligence);recurrent neural nets;time series;streaming time series forecasting;RNN;adaptive gradient learning method;online learning mode;streaming data;recurrent neural networks;robust online time series prediction;Time series analysis;Recurrent neural networks;Robustness;Data models;Learning systems;Predictive models;Forecasting}, 
	doi={10.1109/DSAA.2016.92}, 
	ISSN={}, 
	month={Oct},
}

@inproceedings{DeepVoice,
	author = {Arik, Sercan \"{O}. and Chrzanowski, Mike and Coates, Adam and Diamos, Gregory and Gibiansky, Andrew and Kang, Yongguo and Li, Xian and Miller, John and Ng, Andrew and Raiman, Jonathan and Sengupta, Shubho and Shoeybi, Mohammad},
	title = {Deep Voice: Real-time Neural Text-to-speech},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
	series = {ICML'17},
	year = {2017},
	location = {Sydney, NSW, Australia},
	pages = {195--204},
	numpages = {10},
	url = {http://dl.acm.org/citation.cfm?id=3305381.3305402},
	acmid = {3305402},
	publisher = {JMLR.org},
} 


@inproceedings{ADAM,
	author    = {Diederik P. Kingma and Jimmy Ba},
	title     = {Adam: {A} Method for Stochastic Optimization},
	booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
	San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
	year      = {2015},
	url       = {http://arxiv.org/abs/1412.6980},
	biburl    = {https://dblp.org/rec/bib/journals/corr/KingmaB14},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{PyTorch,
	title={Automatic differentiation in {PyTorch}},
	author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year={2017},
	booktitle={NIPS Workshop on the future of gradient-based machine learning software \& techniques}
}



@inproceedings{Pascanu_bengio2013,
	author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
	title = {On the Difficulty of Training Recurrent Neural Networks},
	booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
	series = {ICML'13},
	year = {2013},
	location = {Atlanta, GA, USA},
	pages = {III-1310--III-1318},
	url = {http://dl.acm.org/citation.cfm?id=3042817.3043083},
	acmid = {3043083},
	publisher = {JMLR.org},
} 


@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	urldate = {2020-01-29},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
}


@misc{text8,
	title = {Relationship of {Wikipedia} {Text} to {Clean} {Text}},
	url = {http://mattmahoney.net/dc/textdata.html},
	author = {Mahoney, Matt},
	month = {June},
	year = {2006},
}


@article{Mikolov_dataSplit,
	title = {Subword language modeling with neural networks}, 
	year = {2012},
	journal = {Preprint},
	author = {Mikolov, Tomas and Ilya Sutskever and Deoras, Anoop and Le, Hai-Son and Kombrink, Stefan},
	url = {http://www.fit.vutbr.cz/~imikolov/rnnlm/char.pdf},
}


@inproceedings{POSBiLSTM,
	title = "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation",
	author = "Ling, Wang  and
	Dyer, Chris  and
	Black, Alan W.  and
	Trancoso, Isabel  and
	Fermandez, Ramon  and
	Amir, Silvio  and
	Marujo, Luis  and
	Luis, Tiago",
	booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
	month = sep,
	year = "2015",
	address = "Lisbon, Portugal",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D15-1176",
	doi = "10.18653/v1/D15-1176",
	pages = "1520--1530",
}

@inproceedings{POSBiLSTM2,
	title = "Improved Transition-based Parsing by Modeling Characters instead of Words with {LSTM}s",
	author = "Ballesteros, Miguel  and
	Dyer, Chris  and
	Smith, Noah A.",
	booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
	month = sep,
	year = "2015",
	address = "Lisbon, Portugal",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D15-1041",
	doi = "10.18653/v1/D15-1041",
	pages = "349--359",
}


@inproceedings{UD,
	title = "Universal Dependencies v1: A Multilingual Treebank Collection",
	author = "Nivre, Joakim  and
	de Marneffe, Marie-Catherine  and
	Ginter, Filip  and
	Goldberg, Yoav  and
	Haji{\v{c}}, Jan  and
	Manning, Christopher D.  and
	McDonald, Ryan  and
	Petrov, Slav  and
	Pyysalo, Sampo  and
	Silveira, Natalia  and
	Tsarfaty, Reut  and
	Zeman, Daniel",
	booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
	month = may,
	year = "2016",
	address = "Portoro{\v{z}}, Slovenia",
	publisher = "European Language Resources Association (ELRA)",
	url = "https://www.aclweb.org/anthology/L16-1262",
	pages = "1659--1666",
	abstract = "Cross-linguistically consistent annotation is necessary for sound comparative evaluation and cross-lingual learning experiments. It is also useful for multilingual system development and comparative linguistic studies. Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework. In this paper, we describe v1 of the universal guidelines, the underlying design principles, and the currently available treebanks for 33 languages.",
}



@InProceedings{shortcut_raiko,
	title = 	 {Deep Learning Made Easier by Linear Transformations in Perceptrons},
	author = 	 {Tapani Raiko and Harri Valpola and Yann Lecun},
	booktitle = 	 {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
	pages = 	 {924--932},
	year = 	 {2012},
	editor = 	 {Neil D. Lawrence and Mark Girolami},
	volume = 	 {22},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {La Palma, Canary Islands},
	month = 	 {21--23 Apr},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v22/raiko12/raiko12.pdf},
	url = 	 {http://proceedings.mlr.press/v22/raiko12.html},
	abstract = 	 {We transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero activation and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. This transformation aims at separating the problems of learning the linear and nonlinear parts of the whole input-output mapping, which has many benefits. We study the theoretical properties of the transformation by noting that they make the Fisher information matrix closer to a diagonal matrix, and thus standard gradient closer to the natural gradient. We experimentally confirm the usefulness of the transformations by noting that they make basic stochastic gradient learning competitive with state-of-the-art learning algorithms in speed, and that they seem also to help find solutions that generalize better. The experiments include both classification of small images and learning a low-dimensional representation for images by using a deep unsupervised auto-encoder network. The transformations were beneficial in all cases, with and without regularization and with networks from two to five hidden layers.}
}


@InProceedings{polyglot,
	title = "{P}olyglot: Distributed Word Representations for Multilingual {NLP}",
	author = "Al-Rfou{'}, Rami  and
	Perozzi, Bryan  and
	Skiena, Steven",
	booktitle = "Proceedings of the Seventeenth Conference on Computational Natural Language Learning",
	month = aug,
	year = "2013",
	address = "Sofia, Bulgaria",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/W13-3520",
	pages = "183--192",
}



@inproceedings{UDEngEWT,
	title = "A Gold Standard Dependency Corpus for {E}nglish",
	author = "Silveira, Natalia  and
	Dozat, Timothy  and
	de Marneffe, Marie-Catherine  and
	Bowman, Samuel  and
	Connor, Miriam  and
	Bauer, John  and
	Manning, Chris",
	booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
	month = may,
	year = "2014",
	address = "Reykjavik, Iceland",
	publisher = "European Language Resources Association (ELRA)",
	url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/1089_Paper.pdf",
	pages = "2897--2904",
}


