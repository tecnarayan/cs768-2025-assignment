@inproceedings{koltchinskii2005exponential,
  title={Exponential convergence rates in classification},
  author={Koltchinskii, Vladimir and Beznosova, Olexandra},
  booktitle={International Conference on Computational Learning Theory},
  pages={295--307},
  year={2005}
}

@article{audibert2007fast,
  title={Fast learning rates for plug-in classifiers},
  author={Audibert, Jean-Yves and Tsybakov, Alexandre B},
  journal={The Annals of statistics},
  volume={35},
  number={2},
  pages={608--633},
  year={2007}
}

@article{laurent2000adaptive,
  title={Adaptive estimation of a quadratic functional by model selection},
  author={Laurent, Beatrice and Massart, Pascal},
  journal={The Annals of statistics},
  volume={28},
  number={5},
  pages={1302--1338},
  year={2000}
}

@article{bartlett2006convexity,
  title={Convexity, Classification, and Risk Bounds},
  author={Bartlett, Peter L and Jordan, Michael I and McAuliffe, Jon D},
  journal={Journal of the American Statistical Association},
  volume={101},
  number={473},
  year={2006},
  pages={138--156}  
}

@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={SIAM Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018}
}

@article{dieuleveut2017harder,
  title={Harder, better, faster, stronger convergence rates for least-squares regression},
  author={Dieuleveut, Aymeric and Flammarion, Nicolas and Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={3520--3570},
  year={2017}
}

@article{dieuleveut2016nonparametric,
  title={Nonparametric stochastic approximation with large step-sizes},
  author={Dieuleveut, Aymeric and Bach, Francis},
  journal={The Annals of Statistics},
  volume={44},
  number={4},
  pages={1363--1399},
  year={2016}
}

@inproceedings{pillaud2018exponential,
  title={Exponential Convergence of Testing Error for Stochastic Gradient Methods},
  author={Pillaud-Vivien, Loucas and Rudi, Alessandro and Bach, Francis},
  booktitle={Proceedings of Conference on Learning Theory 31},
  pages={1--47},
  year={2018}
}

@inproceedings{rakhlin2012making,
  title={Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization},
  author={Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  booktitle={Proceedings of International Conference on Machine Learning 29},
  pages={1571--1578},
  year={2012}
}

@article{jabir2019mean,
  title={Mean-field neural odes via relaxed optimal control},
  author={Jabir, Jean-Fran{\c{c}}ois and {\v{S}}i{\v{s}}ka, David and Szpruch, {\L}ukasz},
  journal={arXiv preprint arXiv:1912.05475},
  year={2019}
}

@article{lacoste2012simpler,
  title={A Simpler Approach to Obtaining an ${O}(1/t)$ Convergence Rate for the Projected Stochastic Subgradient Method},
  author={Lacoste-Julien, Simon and Schmidt, Mark and Bach, Francis},
  journal={arXiv preprint arXiv:1212.2002},
  year={2012}
}

@Article{zhang2004statistical,
 author    ={Zhang, Tong},
 title     ={Statistical Behavior and Consistency of Classification Methods Based on Convex Ris Minimization},
 journal   ={The Annals of Statistics},
 year      ={2004},
 volume    ={32},
 number    ={1},
 pages     ={56--134}
}

@article{pinelis1994optimum,
  title={Optimum bounds for the distributions of martingales in Banach spaces},
  author={Pinelis, Iosif},
  journal={The Annals of Probability},
  pages={1679--1706},
  year={1994}
}

@inproceedings{frostig2015competing,
  title={Competing with the empirical risk minimizer in a single pass},
  author={Frostig, Roy and Ge, Rong and Kakade, Sham M and Sidford, Aaron},
  booktitle={Proceedings of Conference on Learning Theory 28},
  pages={728--763},
  year={2015}
}

@inproceedings{rakhlin2017equivalence,
  title={On Equivalence of Martingale Tail Bounds and Deterministic Regret Inequalities},
  author={Rakhlin, Alexander and Sridharan, Karthik},
  booktitle={Proceedings of Conference on Learning Theory 30},
  pages={1704--1722},
  year={2017}
}

@inproceedings{hardt2016train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle={Proceedings of International Conference on Machine Learning 33},
  pages={1225--1234},
  year={2016}
}

@inproceedings{liu2017algorithmic,
  title={Algorithmic Stability and Hypothesis Complexity},
  author={Liu, Tongliang and Lugosi, G{\'a}bor and Neu, Gergely and Tao, Dacheng},
  booktitle={Proceedings of International Conference on Machine Learning 34},
  pages={2159--2167},
  year={2017}
}

@inproceedings{allen2019convergence,
  title={A Convergence Theory for Deep Learning via Over-Parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={Proceedings of International Conference on Machine Learning 36},
  pages={242--252},
  year={2019}
}

@book{nes2004,
 title={Introductory Lectures on Convex Optimization: A Basic Course},
 author={Nesterov, Yurii},
 publisher={Kluwer Academic Publishers},
 year={2004}
}

@book{mohri2012foundations,
  title={Foundations of Machine Learning},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2012},
  publisher={The MIT Press}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@article{robbins1951stochastic,
  author={Robbins, Herbert and Monro, Sutton},
  title={A Stochastic Approximation Method},
  journal={The Annals of Mathematical Statistics},
  volume={22},
  number={3},
  pages={400--407},
  year={1951}
}

@article{cao2019explicit,
  title={On explicit $L^{2}$-convergence rate estimate for underdamped Langevin dynamics},
  author={Cao, Yu and Lu, Jianfeng and Wang, Lihan},
  journal={arXiv preprint arXiv:1908.04746},
  year={2019}
}

@article{ghadimi2013stochastic,
  title={Stochastic First-and Zeroth-Order Methods for Nonconvex Stochastic Programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013}
}

@article{polyak1992acceleration,
  title={Acceleration of Stochastic Approximation by Averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM Journal on Control and Optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992}
}

@techreport{ruppert1988efficient,
  title={Efficient Estimations from a Slowly Convergent {R}obbins-{M}onro Process},
  author={Ruppert, David},
  year={1988},
  institution={Cornell University Operations Research and Industrial Engineering}
}

@article{nemirovski2009robust,
  title={Robust Stochastic Approximation Approach to Stochastic Programming},
  author={Nemirovski, Arkadii S and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on Optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009}
}


@article{bubeck2015convex,
  title={Convex optimization: Algorithms and complexity},
  author={Bubeck, S{\'e}bastien},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015}
}

@article{nesterov2009primal,
  title={Primal-dual subgradient methods for convex problems},
  author={Nesterov, Yurii},
  journal={Mathematical programming},
  volume={120},
  number={1},
  pages={221--259},
  year={2009}
}

@book{nemirovskii1983problem,
  title={Problem Complexity and Method Efficiency in Optimization},
  author={Nemirovskii, Arkadii and Yudin, David Borisovich},
  year={1983},
  publisher={John Wiley}
}

@article{rosasco2004loss,
  title={Are loss functions all the same?},
  author={Rosasco, Lorenzo and Vito, Ernesto De and Caponnetto, Andrea and Piana, Michele and Verri, Alessandro},
  journal={Neural Computation},
  volume={16},
  number={5},
  pages={1063--1076},
  year={2004},
  publisher={MIT Press}
}

@article{schmidt2017minimizing,
  title={Minimizing Finite Sums with the Stochastic Average Gradient},
  author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  journal={Mathematical Programming},
  volume={162},
  number={1-2},
  pages={83--112},
  year={2017}
}

@inproceedings{allen2017katyusha,
  title={Katyusha: The First Direct Acceleration of Stochastic Gradient Methods},
  author={Allen-Zhu, Zeyuan},
  booktitle={Proceedings of Annual ACM SIGACT Symposium on Theory of Computing 49},
  pages={1200--1205},
  year={2017},
  organization={ACM}
}

@article{tsybakov2004optimal,
  title={Optimal aggregation of classifiers in statistical learning},
  author={Tsybakov, Alexander B},
  journal={The Annals of Statistics},
  volume={32},
  number={1},
  pages={135--166},
  year={2004}
}

@article{caponnetto2007optimal,
  title={Optimal rates for the regularized least-squares algorithm},
  author={Caponnetto, Andrea and De Vito, Ernesto},
  journal={Foundations of Computational Mathematics},
  volume={7},
  number={3},
  pages={331--368},
  year={2007}
}

@article{smale2006online,
  title={Online learning algorithms},
  author={Smale, Steve and Yao, Yuan},
  journal={Foundations of computational mathematics},
  volume={6},
  number={2},
  pages={145--170},
  year={2006}
}

@article{ying2006online,
  title={Online regularized classification algorithms},
  author={Ying, Yiming and Zhou, D-X},
  journal={IEEE Transactions on Information Theory},
  volume={52},
  number={11},
  pages={4775--4788},
  year={2006}
}

@article{cesa2004generalization,
  title={On the generalization ability of on-line learning algorithms},
  author={Cesa-Bianchi, Nicolo and Conconi, Alex and Gentile, Claudio},
  journal={IEEE Transactions on Information Theory},
  volume={50},
  number={9},
  pages={2050--2057},
  year={2004}
}

@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={Journal of machine learning research},
  volume={2},
  number={Mar},
  pages={499--526},
  year={2002}
}

% NeurIPS
@inproceedings{bengio2006convex,
  title={Convex neural networks},
  author={Bengio, Yoshua and Le Roux, Nicolas and Vincent, Pascal and Delalleau, Olivier and Marcotte, Patrice},
  booktitle={Advances in neural information processing systems 18},
  year={2005}
}

@inproceedings{rahimi2007random,
  title={Random Features for Large-scale Kernel Machines},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={Advances in Neural Information Processing Systems 20},
  pages={1177--1184},
  year={2007}
}

@inproceedings{agarwal2009information,
  title={Information-Theoretic Lower Bounds on the Oracle Complexity of Convex Optimization},
  author={Agarwal, Alekh and Wainwright, Martin J and Bartlett, Peter L and Ravikumar, Pradeep K},
  booktitle={Advances in Neural Information Processing Systems 22},
  pages={1--9},
  year={2009}
}

@inproceedings{xiao2009dual,
  title={Dual averaging method for regularized stochastic learning and online optimization},
  author={Xiao, Lin},
  booktitle={Advances in Neural Information Processing Systems 22},
  pages={2116--2124},
  year={2009}
}

@inproceedings{kakade2009generalization,
  title={On the generalization ability of online strongly convex programming algorithms},
  author={Kakade, Sham M and Tewari, Ambuj},
  booktitle={Advances in Neural Information Processing Systems 22},
  pages={801--808},
  year={2009}
}

@inproceedings{bach2011non,
  title={Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning},
  author={Bach, Francis and Moulines, Eric},
  booktitle={Advances in Neural Information Processing Systems 24},
  pages={451--459},
  year={2011}
}

@inproceedings{johnson2013accelerating,
  title={Accelerating Stochastic Gradient Descent using Predictive Variance Reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Advances in Neural Information Processing Systems 26},
  pages={315--323},
  year={2013}
}

@inproceedings{bach2013non,
  title={Non-Strongly-Convex Smooth Stochastic Approximation with Convergence Rate ${O}(1/n)$},
  author={Bach, Francis and Moulines, Eric},
  booktitle={Advances in Neural Information Processing Systems 26},
  pages={773--781},
  year={2013}
}

@inproceedings{defazio2014saga,
  title={SAGA: A Fast Incremental Gradient Method with Support for Non-Strongly Convex Composite Objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={Advances in neural information processing systems 27},
  pages={1646--1654},
  year={2014}
}

@inproceedings{nitanda2014stochastic,
  title={Stochastic Proximal Gradient Descent with Acceleration Techniques},
  author={Nitanda, Atsushi},
  booktitle={Advances in Neural Information Processing Systems 27},
  pages={1574--1582},
  year={2014}
}

@inproceedings{liu2016stein,
  title={Stein variational gradient descent: A general purpose bayesian inference algorithm},
  author={Liu, Qiang and Wang, Dilin},
  booktitle={Advances in neural information processing systems 29},
  pages={2378--2386},
  year={2016}
}

@inproceedings{murata2017doubly,
  title={Doubly Accelerated Stochastic Variance Reduced Dual Averaging Method for Regularized Empirical Risk Minimization},
  author={Murata, Tomoya and Suzuki, Taiji},
  booktitle={Advances in Neural Information Processing Systems 30},
  pages={608--617},
  year={2017}
}

@inproceedings{rudi2017generalization,
  title={Generalization properties of learning with random features},
  author={Rudi, Alessandro and Rosasco, Lorenzo},
  booktitle={Advances in Neural Information Processing Systems 30},
  pages={3215--3225},
  year={2017}
}

@inproceedings{jacot2018neural,
  title={Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in Neural Information Processing Systems 31},
  pages = {8580--8589},
  year={2018}
}

@inproceedings{pillaud2018statistical,
  title={Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes},
  author={Pillaud-Vivien, Loucas and Rudi, Alessandro and Bach, Francis},
  booktitle={Advances in Neural Information Processing Systems 31},
  pages={8114--8124},
  year={2018}
}

@inproceedings{carratino2018learning,
  title={Learning with sgd and random features},
  author={Carratino, Luigi and Rudi, Alessandro and Rosasco, Lorenzo},
  booktitle={Advances in Neural Information Processing Systems 31},
  pages={10192--10203},
  year={2018}
}

@inproceedings{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Advances in Neural Information Processing Systems 31},
  pages={3040--3050},
  year={2018}
}

@inproceedings{erdogdu2018global,
  title={Global non-convex optimization with discretized diffusions},
  author={Erdogdu, Murat A and Mackey, Lester and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems 31},
  pages={9671--9680},
  year={2018}
}

@inproceedings{xu2018global,
  title={Global convergence of Langevin dynamics based algorithms for nonconvex optimization},
  author={Xu, Pan and Chen, Jinghui and Zou, Difan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems 31},
  pages={3122--3133},
  year={2018}
}

@inproceedings{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={8139--8148},
  year={2019}
}

@inproceedings{su2019learning,
  title={On Learning Over-parameterized Neural Networks: A Functional Approximation Perspective},
  author={Su, Lili and Yang, Pengkun},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={2637--2646},
  year={2019}
}

@inproceedings{zou2019improved,
  title={An improved analysis of training over-parameterized deep neural networks},
  author={Zou, Difan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={2053--2062},
  year={2019}
}

@inproceedings{cao2019generalization,
  title={Generalization bounds of stochastic gradient descent for wide and deep neural networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={10836--10846},
  year={2019}
}

@inproceedings{bietti2019inductive,
  title={On the inductive bias of neural tangent kernels},
  author={Bietti, Alberto and Mairal, Julien},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={12873--12884},
  year={2019}
}

@inproceedings{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  booktitle={Advances in neural information processing systems 32},
  pages={8570--8581},
  year={2019}
}

@inproceedings{ronen2019convergence,
  title={The convergence rate of neural networks for learned functions of different frequencies},
  author={Ronen, Basri and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={4763--4772},
  year={2019}
}

@inproceedings{mucke2019beating,
  title={Beating SGD Saturation with Tail-Averaging and Minibatching},
  author={M{\"u}cke, Nicole and Neu, Gergely and Rosasco, Lorenzo},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={12568--12577},
  year={2019}
}

@inproceedings{vempala2019rapid,
  title={Rapid convergence of the unadjusted langevin algorithm: Isoperimetry suffices},
  author={Vempala, Santosh and Wibisono, Andre},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={8094--8106},
  year={2019}
}

@inproceedings{ghorbani2019limitations,
  title={Limitations of lazy training of two-layers neural network},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={9111--9121},
  year={2019}
}

@inproceedings{allen2019can,
  title={What Can ResNet Learn Efficiently, Going Beyond Kernels?},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={9017--9028},
  year={2019}
}

@inproceedings{yehudai2019power,
  title={On the power and limitations of random features for understanding neural networks},
  author={Yehudai, Gilad and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={6598--6608},
  year={2019}
}

@inproceedings{wei2019regularization,
  title={Regularization matters: Generalization and optimization of neural nets vs their induced kernel},
  author={Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={9712--9724},
  year={2019}
}

@inproceedings{li2019stochastic,
  title={Stochastic runge-kutta accelerates langevin monte carlo and beyond},
  author={Li, Xuechen and Wu, Yi and Mackey, Lester and Erdogdu, Murat A},
  booktitle={Advances in Neural Information Processing Systems 32},
  pages={7748--7760},
  year={2019}
}

@inproceedings{suzuki2020generalization,
  title={Generalization bound of globally optimal non-convex neural network training: Transportation map estimation by infinite dimensional Langevin dynamics},
  author={Suzuki, Taiji},
  booktitle={Advances in Neural Information Processing Systems 33},    
  year={2020}
}

@inproceedings{suzuki2019deep,
  title={Deep learning is adaptive to intrinsic dimensionality of model smoothness in anisotropic Besov space},
  author={Suzuki, Taiji and Nitanda, Atsushi},
  booktitle={Advances in Neural Information Processing Systems 34},  
  year={2021}
}

% ICLR
@inproceedings{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  booktitle={Proceedings of the 7th International Conference on Learning Representations},  
  year={2019}
}

@inproceedings{suzuki2018adaptivity,
  title={Adaptivity of deep relu network for learning in besov and mixed smooth besov spaces: optimal rate and curse of dimensionality},
  author={Suzuki, Taiji},
  booktitle={Proceedings of the 7th International Conference on Learning Representations},    
  year={2019}
}

@inproceedings{ba2019generalization,
  title={Generalization of two-layer neural networks: An asymptotic viewpoint},
  author={Ba, Jimmy and Erdogdu, Murat and Suzuki, Taiji and Wu, Denny and Zhang, Tianzong},
  booktitle={Proceedings of the 8th International Conference on Learning Representations},    
  year={2020}
}

@inproceedings{nitanda2020optimal,
  title={Optimal rates for averaged stochastic gradient descent under neural tangent kernel regime},
  author={Nitanda, Atsushi and Suzuki, Taiji},
  booktitle={Proceedings of the 9th International Conference on Learning Representations},      
  year={2021}
}

@inproceedings{pham2021global,
  title={Global Convergence of Three-layer Neural Networks in the Mean Field Regime},
  author={Huy Tuan Pham and Phan-Minh Nguyen},
  booktitle={Proceedings of the 9th International Conference on Learning Representations},
  year={2021},
}

%
@article{zhang2018learning,
  title={Learning one-hidden-layer relu networks via gradient descent},
  author={Zhang, Xiao and Yu, Yaodong and Wang, Lingxiao and Gu, Quanquan},
  journal={arXiv preprint arXiv:1806.07808},
  year={2018}
}

@article{zhang2019fast,
  title={Fast Convergence of Natural Gradient Descent for Overparameterized Neural Networks},
  author={Zhang, Guodong and Martens, James and Grosse, Roger},
  journal={arXiv preprint arXiv:1905.10961},
  year={2019}
}

@article{wu2019global,
  title={Global convergence of adaptive gradient methods for an over-parameterized neural network},
  author={Wu, Xiaoxia and Du, Simon S and Ward, Rachel},
  journal={arXiv preprint arXiv:1902.07111},
  year={2019}
}

@article{ramachandran2017,
  title={Searching for activation functions},
  author={Ramachandran, Prajit and Barret Zoph and Quoc V. Le},
  journal={arXiv preprint arXiv:1710.05941},
  year={2017}
}

@inproceedings{nitanda2019stochastic,
  title={Stochastic Gradient Descent with Exponential Convergence Rates of Expected Classification Errors},
  author={Nitanda, Atsushi and Suzuki, Taiji},
  booktitle={Proceedings of International Conference on Artificial Intelligence and Statistics 22},
  pages={1417--1426},
  year={2019}
}

@book{nm1994,
 author    ={Newey, Whitney K and McFadden, Daniel},
 title     ={Large Sample Estimation and Hypothesis Testing},
 journal   ={Handbook of Econometrics},
 year      ={1994},
 volume    ={4},
 pages     ={2111--2245} 
}

@article{cucker2002mathematical,
  title={On the mathematical foundations of learning},
  author={Cucker, Felipe and Smale, Steve},
  journal={Bulletin of the American mathematical society},
  volume={39},
  number={1},
  pages={1--49},
  year={2002}
}

@article{bach2017equivalence,
  title={On the equivalence between kernel quadrature rules and random feature expansions},
  author={Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={714--751},
  year={2017}
}

@article{chizat2018note,
  title={A note on lazy training in supervised differentiable programming},
  author={Chizat, Lenaic and Bach, Francis},
  journal={arXiv preprint arXiv:1812.07956},
  year={2018}
}

@article{weinan2019comparative,
  title={A comparative analysis of optimization and generalization properties of two-layer neural network and random feature models under gradient descent dynamics},
  author={Weinan, E and Ma, Chao and Wu, Lei},
  journal={Science China Mathematics},
  pages={1--24},
  year={2019}
}


@article{yashima2019exponential,
  title={Exponential Convergence Rates of Classification Errors on Learning with SGD and Random Features},
  author={Yashima, Shingo and Nitanda, Atsushi and Suzuki, Taiji},
  journal={arXiv preprint arXiv:1911.05350},
  year={2019}
}


@article{cao2019generalization_,
  title={A Generalization Theory of Gradient Descent for Learning Over-parameterized Deep ReLU Networks},
  author={Cao, Yuan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1902.01384},
  year={2019}
}

@article{zou2018stochastic,
  title={Stochastic gradient descent optimizes over-parameterized deep relu networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:1811.08888},
  year={2018}
}

@article{zou2020gradient,
  title={Gradient descent optimizes over-parameterized deep ReLU networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={Machine Learning},
  volume={109},
  number={3},
  pages={467--492},
  year={2020}
}

@article{ji2019polylogarithmic,
  title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1909.12292},
  year={2019}
}

@article{cao2019towards,
  title={Towards Understanding the Spectral Bias of Deep Learning},
  author={Cao, Yuan and Fang, Zhiying and Wu, Yue and Zhou, Ding-Xuan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1912.01198},
  year={2019}
}

@article{rosasco2010learning,
  title={On learning with integral operators},
  author={Rosasco, Lorenzo and Belkin, Mikhail and Vito, Ernesto De},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Feb},
  pages={905--934},
  year={2010}
}

@article{smale2009geometry,
  title={Geometry on probability spaces},
  author={Smale, Steve and Zhou, Ding-Xuan},
  journal={Constructive Approximation},
  volume={30},
  number={3},
  pages={311},
  year={2009}
}

@inproceedings{rahamanspectral2019,
  title={On the Spectral Bias of Neural Networks},
  author={Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred A and Bengio, Yoshua and Courville, Aaron},
  booktitle={Proceedings of International Conference on Machine Learning 36},
  pages={5301--5310},
  year={2019}
}

@inproceedings{arora2019fine,
  title={Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={Proceedings of International Conference on Machine Learning 36},
  pages={322--332},
  year={2019}
}

@inproceedings{du2019gradient,
  title={Gradient Descent Finds Global Minima of Deep Neural Networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={Proceedings of International Conference on Machine Learning 36},
  pages={1675--1685},
  year={2019}
}

@inproceedings{steinwart2009optimal,
  title={Optimal Rates for Regularized Least Squares Regression.},
  author={Steinwart, Ingo and Hush, Don R and Scovel, Clint},
  booktitle={Proceedings of Conference on Learning Theory 22},
  pages={79--93},
  year={2009}
}

@article{bach2017breaking,
  title={Breaking the curse of dimensionality with convex neural networks},
  author={Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={629--681},
  year={2017}
}

@book{milstein2013stochastic,
  title={Stochastic numerics for mathematical physics},
  author={Milstein, Grigori Noah and Tretyakov, Michael V},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@book{atkinson2012spherical,
  title={Spherical harmonics and approximations on the unit sphere: an introduction},
  author={Atkinson, Kendall and Han, Weimin},
  year={2012},
  publisher={Springer}
}

@article{blanchard2018optimal,
  title={Optimal rates for regularization of statistical inverse learning problems},
  author={Blanchard, Gilles and M{\"u}cke, Nicole},
  journal={Foundations of Computational Mathematics},
  volume={18},
  number={4},
  pages={971--1013},
  year={2018}
}

@article{nitanda2017stochastic,
  title={Stochastic Particle Gradient Descent for Infinite Ensembles},
  author={Nitanda, Atsushi and Suzuki, Taiji},
  journal={arXiv preprint arXiv:1712.05438},
  year={2017}
}

@article{mei2018mean,
  title={A mean field view of the landscape of two-layer neural networks},
  author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={33},
  pages={E7665--E7671},
  year={2018}
}


@article{durmus2019analysis,
  title={Analysis of Langevin Monte Carlo via Convex Optimization},
  author={Durmus, Alain and Majewski, Szymon and Miasojedow, B{\l}a{\.z}ej},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={73},
  pages={1--46},
  year={2019}
}

@inproceedings{wibisono2018sampling,
  title={Sampling as optimization in the space of measures: The Langevin dynamics as a composite optimization problem},
  author={Wibisono, Andre},
  booktitle={Proceedings of Conference on Learning Theory 31},
  pages={2093--3027},
  year={2018}
}

@article{holley1987logarithmic,
  title={Logarithmic Sobolev inequalities and stochastic Ising models},
  author={Holley, Richard and Stroock, Daniel},  
  journal={Journal of statistical physics},
  volume={46},
  number={5-6},
  pages={1159--1194},
  year={1987}
}

@misc{bakry1985diffusions,
  title={Diffusions hypercontractives in Sem. Probab. XIX LNM 1123},
  author={Bakry, Dominique and {\'E}mery, Michel},
  year={1985},
  publisher={Springer, New York}
}

@article{otto2000generalization,
  title={Generalization of an inequality by Talagrand and links with the logarithmic Sobolev inequality},
  author={Otto, Felix and Villani, C{\'e}dric},
  journal={Journal of Functional Analysis},
  volume={173},
  number={2},
  pages={361--400},
  year={2000}
}

@article{erdogdu2020convergence,
  title={On the Convergence of Langevin Monte Carlo: The Interplay between Tail Growth and Smoothness},
  author={Erdogdu, Murat A and Hosseinzadeh, Rasa},
  journal={arXiv preprint arXiv:2005.13097},
  year={2020}
}

@article{nesterov2005smooth,
  title={Smooth minimization of non-smooth functions},
  author={Nesterov, Yu},
  journal={Mathematical programming},
  volume={103},
  number={1},
  pages={127--152},
  year={2005}
}

@article{javanmard2019analysis,
  title={Analysis of a two-layer neural network via displacement convexity},
  author={Javanmard, Adel and Mondelli, Marco and Montanari, Andrea},
  journal={arXiv preprint arXiv:1901.01375},
  year={2019}
}

@article{jordan199618,
  title={18. An Extended Variational},
  author={Jordan, Richard and Kinderlehrer, David},
  journal={Partial differential equations and applications: collected papers in honor of Carlo Pucci},
  volume={177},
  pages={187},
  year={1996}
}

@article{jordan1998variational,
  title={The variational formulation of the Fokker--Planck equation},
  author={Jordan, Richard and Kinderlehrer, David and Otto, Felix},
  journal={SIAM journal on mathematical analysis},
  volume={29},
  number={1},
  pages={1--17},
  year={1998}
}

@article{hu2019mean,
  title={Mean-field Langevin dynamics and energy landscape of neural networks},
  author={Hu, Kaitong and Ren, Zhenjie and Siska, David and Szpruch, Lukasz},
  journal={arXiv preprint arXiv:1905.07769},
  year={2019}
}

@article{eberle2019quantitative,
  title={Quantitative Harris-type theorems for diffusions and McKean--Vlasov processes},
  author={Eberle, Andreas and Guillin, Arnaud and Zimmer, Raphael},
  journal={Transactions of the American Mathematical Society},
  volume={371},
  number={10},
  pages={7135--7173},
  year={2019}
}

@article{sirignano2020mean,
  title={Mean field analysis of neural networks: A central limit theorem},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={Stochastic Processes and their Applications},
  volume={130},
  number={3},
  pages={1820--1852},
  year={2020}
}

@article{rotskoff2018trainability,
  title={Trainability and accuracy of neural networks: An interacting particle system approach},
  author={Rotskoff, Grant M and Vanden-Eijnden, Eric},
  journal={arXiv preprint arXiv:1805.00915},
  year={2018}
}

@inproceedings{xie2017diverse,
  title={Diverse neural network learns true target functions},
  author={Xie, Bo and Liang, Yingyu and Song, Le},
  booktitle={Proceedings of International Conference on Artificial Intelligence and Statistics 20},
  pages={1216--1224},
  year={2017}
}

@article{dalalyan2017further,
  title={Further and stronger analogy between sampling and optimization: Langevin Monte Carlo and gradient descent},
  author={Dalalyan, Arnak S},
  journal={arXiv preprint arXiv:1704.04752},
  year={2017}
}

@inproceedings{rotskoff2019global,
  title={Global convergence of neuron birth-death dynamics},
  author={Rotskoff, Grant M and Jelassi, Samy and Bruna, Joan and Vanden-Eijnden, Eric},
  booktitle={Proceedings of International Conference on Machine Learning 36},
  pages={9689--9698},
  year={2019}
}

@article{ghorbani2020neural,
  title={When Do Neural Networks Outperform Kernel Methods?},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={arXiv preprint arXiv:2006.13409},
  year={2020}
}

@article{bai2019beyond,
  title={Beyond linearization: On quadratic and higher-order approximation of wide neural networks},
  author={Bai, Yu and Lee, Jason D},
  journal={arXiv preprint arXiv:1910.01619},
  year={2019}
}

@inproceedings{li2020learning,
  title={Learning Over-Parametrized Two-Layer Neural Networks beyond NTK},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang R},
  booktitle={Proceedings of Conference on Learning Theory 33},  
  pages={2613--2682},
  year={2020}
}

@article{ge2019mildly,
  title={Mildly overparametrized neural nets can memorize training data efficiently},
  author={Ge, Rong and Wang, Runzhe and Zhao, Haoyu},
  journal={arXiv preprint arXiv:1909.11837},
  year={2019}
}

@article{du2018power,
  title={On the power of over-parametrization in neural networks with quadratic activation},
  author={Du, Simon S and Lee, Jason D},
  journal={arXiv preprint arXiv:1803.01206},
  year={2018}
}

@article{carrillo2001entropy,
  title={Entropy dissipation methods for degenerate parabolicproblems and generalized sobolev inequalities},
  author={Carrillo, Jos{\'e} A and J{\"u}ngel, Ansgar and Markowich, Peter A and Toscani, Giuseppe and Unterreiter, Andreas},
  journal={Monatshefte f{\"u}r Mathematik},
  volume={133},
  number={1},
  pages={1--82},
  year={2001}
}

@article{raginsky2017non,
  title={Non-convex learning via stochastic gradient Langevin dynamics: a nonasymptotic analysis},
  author={Raginsky, Maxim and Rakhlin, Alexander and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1702.03849},
  year={2017}
}

@inproceedings{dai2016provable,
  title={Provable bayesian inference via particle mirror descent},
  author={Dai, Bo and He, Niao and Dai, Hanjun and Song, Le},
  booktitle={Proceedings of International Conference on Artificial Intelligence and Statistics 19},  
  pages={985--994},
  year={2016}
}

@article{chizat2019sparse,
  title={Sparse optimization on measures with over-parameterized gradient descent},
  author={Chizat, Lenaic},
  journal={arXiv preprint arXiv:1907.10300},
  year={2019}
}

@article{roberts1996exponential,
  title={Exponential convergence of Langevin distributions and their discrete approximations},
  author={Roberts, Gareth O and Tweedie, Richard L},
  journal={Bernoulli},
  volume={2},
  number={4},
  pages={341--363},
  year={1996},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@article{mattingly2002ergodicity,
  title={Ergodicity for SDEs and approximations: locally Lipschitz vector fields and degenerate noise},
  author={Mattingly, Jonathan C and Stuart, Andrew M and Higham, Desmond J},
  journal={Stochastic processes and their applications},
  volume={101},
  number={2},
  pages={185--232},
  year={2002},
  publisher={Elsevier}
}

@article{cheng2017convergence,
  title={Convergence of Langevin MCMC in KL-divergence},
  author={Cheng, Xiang and Bartlett, Peter},
  journal={arXiv preprint arXiv:1705.09048},
  year={2017}
}

@article{dalalyan2014theoretical,
  title={Theoretical guarantees for approximate sampling from smooth and log-concave densities},
  author={Dalalyan, Arnak S},
  journal={arXiv preprint arXiv:1412.7392},
  year={2014}
}

@article{durmus2017nonasymptotic,
  title={Nonasymptotic convergence analysis for the unadjusted Langevin algorithm},
  author={Durmus, Alain and Moulines, Eric},
  journal={The Annals of Applied Probability},
  volume={27},
  number={3},
  pages={1551--1587},
  year={2017},
  publisher={Institute of Mathematical Statistics}
}

@article{lu2019scaling,
  title={Scaling limit of the Stein variational gradient descent: The mean field regime},
  author={Lu, Jianfeng and Lu, Yulong and Nolen, James},
  journal={SIAM Journal on Mathematical Analysis},
  volume={51},
  number={2},
  pages={648--671},
  year={2019},
  publisher={SIAM}
}

@article{ding2019ensemble,
  title={Ensemble Kalman sampling: Mean-field limit and convergence analysis},
  author={Ding, Zhiyan and Li, Qin},
  journal={arXiv preprint arXiv:1910.12923},
  year={2019}
}

@article{garbuno2020interacting,
  title={Interacting Langevin diffusions: Gradient structure and ensemble Kalman sampler},
  author={Garbuno-Inigo, Alfredo and Hoffmann, Franca and Li, Wuchen and Stuart, Andrew M},
  journal={SIAM Journal on Applied Dynamical Systems},
  volume={19},
  number={1},
  pages={412--441},
  year={2020},
  publisher={SIAM}
}

@article{allen2020backward,
  title={Backward feature correction: How deep learning performs deep learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2001.04413},
  year={2020}
}

@article{nitanda2019gradient,
  title={Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems},
  author={Nitanda, Atsushi and Chinot, Geoffrey and Suzuki, Taiji},
  journal={arXiv preprint arXiv:1905.09870},
  year={2019}
}

@article{lu2020mean,
  title={A mean-field analysis of deep resnet and beyond: Towards provable optimization via overparameterization from depth},
  author={Lu, Yiping and Ma, Chao and Lu, Yulong and Lu, Jianfeng and Ying, Lexing},
  journal={arXiv preprint arXiv:2003.05508},
  year={2020}
}

@article{nguyen2020rigorous,
  title={A rigorous framework for the mean field limit of multilayer neural networks},
  author={Nguyen, Phan-Minh and Pham, Huy Tuan},
  journal={arXiv preprint arXiv:2001.11443},
  year={2020}
}

@article{araujo2019mean,
  title={A mean-field limit for certain deep neural networks},
  author={Ara{\'u}jo, Dyego and Oliveira, Roberto I and Yukimura, Daniel},
  journal={arXiv preprint arXiv:1906.00193},
  year={2019}
}

@article{mei2019mean,
  title={Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={arXiv preprint arXiv:1902.06015},
  year={2019}
}

@article{ghorbani2019linearized,
  title={Linearized two-layers neural networks in high dimension},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={arXiv preprint arXiv:1904.12191},
  year={2019}
}

@article{chizat2020implicit,
  title={Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss},
  author={Chizat, Lenaic and Bach, Francis},
  journal={arXiv preprint arXiv:2002.04486},
  year={2020}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{chen2020generalized,
  title={A Generalized Neural Tangent Kernel Analysis for Two-layer Neural Networks},
  author={Chen, Zixiang and Cao, Yuan and Gu, Quanquan and Zhang, Tong},
  journal={arXiv preprint arXiv:2002.04026},
  year={2020}
}

@article{daniely2020learning,
  title={Learning Parities with Neural Networks},
  author={Daniely, Amit and Malach, Eran},
  journal={arXiv preprint arXiv:2002.07400},
  year={2020}
}

@article{yang2020feature,
  title={Feature Learning in Infinite-Width Neural Networks},
  author={Yang, Greg and Hu, Edward J},
  journal={arXiv preprint arXiv:2011.14522},
  year={2020}
}

@article{ma2019there,
  title={Is There an Analog of Nesterov Acceleration for MCMC?},
  author={Ma, Yi-An and Chatterji, Niladri and Cheng, Xiang and Flammarion, Nicolas and Bartlett, Peter and Jordan, Michael I},
  journal={arXiv preprint arXiv:1902.00996},
  year={2019}
}
  
  
@article{chen2020dynamical,
  title={A Dynamical Central Limit Theorem for Shallow Neural Networks},
  author={Chen, Zhengdao and Rotskoff, Grant M and Bruna, Joan and Vanden-Eijnden, Eric},
  journal={arXiv preprint arXiv:2008.09623},
  year={2020}
}

@article{bou2020convergence,
  title={Convergence of unadjusted Hamiltonian Monte Carlo for mean-field models},
  author={Bou-Rabee, Nawaf and Schuh, Katharina},
  journal={arXiv preprint arXiv:2009.08735},
  year={2020}
}

@article{bou2021mixing,
  title={Mixing Time Guarantees for Unadjusted Hamiltonian Monte Carlo},
  author={Bou-Rabee, Nawaf and Eberle, Andreas},
  journal={arXiv e-prints},
  pages={arXiv--2105},
  year={2021}
}

@article{guillin2020uniform,
  title={Uniform long-time and propagation of chaos estimates for mean field kinetic particles in non-convex landscapes},
  author={Guillin, Arnaud and Monmarch{\'e}, Pierre},
  journal={arXiv preprint arXiv:2003.00735},
  year={2020}
}

@article{guillin2021kinetic,
  title={The kinetic Fokker-Planck equation with mean field interaction},
  author={Guillin, Arnaud and Liu, Wei and Wu, Liming and Zhang, Chaoen},
  journal={Journal de Math{\'e}matiques Pures et Appliqu{\'e}es},
  volume={150},
  pages={1--23},
  year={2021},
  publisher={Elsevier}
}

@article{kraskov2004estimating,
  title={Estimating mutual information},
  author={Kraskov, Alexander and St{\"o}gbauer, Harald and Grassberger, Peter},
  journal={Physical review E},
  volume={69},
  number={6},
  pages={066138},
  year={2004},
  publisher={APS}
}

@inproceedings{cheng2018underdamped,
  title={Underdamped Langevin MCMC: A non-asymptotic analysis},
  author={Cheng, Xiang and Chatterji, Niladri S and Bartlett, Peter L and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={300--323},
  year={2018},
  organization={PMLR}
}

@inproceedings{dwivedi2018log,
  title={Log-concave sampling: Metropolis-Hastings algorithms are fast!},
  author={Dwivedi, Raaz and Chen, Yuansi and Wainwright, Martin J and Yu, Bin},
  booktitle={Conference on Learning Theory},
  pages={793--797},
  year={2018},
  organization={PMLR}
}

@article{dalalyan2020sampling,
  title={On sampling from a log-concave density using kinetic Langevin diffusions},
  author={Dalalyan, Arnak S and Riou-Durand, Lionel and others},
  journal={Bernoulli},
  volume={26},
  number={3},
  pages={1956--1988},
  year={2020},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@article{eberle2019couplings,
  title={Couplings and quantitative contraction rates for Langevin dynamics},
  author={Eberle, Andreas and Guillin, Arnaud and Zimmer, Raphael and others},
  journal={Annals of Probability},
  volume={47},
  number={4},
  pages={1982--2010},
  year={2019},
  publisher={Institute of Mathematical Statistics}
}

@article{menz2014poincare,
  title={Poincar{\'e} and logarithmic Sobolev inequalities by decomposition of the energy landscape},
  author={Menz, Georg and Schlichting, Andr{\'e}},
  journal={The Annals of Probability},
  volume={42},
  number={5},
  pages={1809--1884},
  year={2014},
  publisher={Institute of Mathematical Statistics}
}


@article{kozachenko1987sample,
  title={Sample estimate of the entropy of a random vector},
  author={Kozachenko, LF and Leonenko, Nikolai N},
  journal={Problemy Peredachi Informatsii},
  volume={23},
  number={2},
  pages={9--16},
  year={1987},
  publisher={Russian Academy of Sciences, Branch of Informatics, Computer Equipment and~â€¦}
}

@article{kent2021frank,
  title={Frank-Wolfe Methods in Probability Space},
  author={Kent, Carson and Blanchet, Jose and Glynn, Peter},
  journal={arXiv preprint arXiv:2105.05352},
  year={2021}
}

@article{chizat2021convergence,
  title={Convergence Rates of Gradient Methods for Convex Optimization in the Space of Measures},
  author={Chizat, L{\'e}na{\"\i}c},
  journal={arXiv preprint arXiv:2105.08368},
  year={2021}
}

@inproceedings{hsieh2019finding,
  title={Finding mixed nash equilibria of generative adversarial networks},
  author={Hsieh, Ya-Ping and Liu, Chen and Cevher, Volkan},
  booktitle={Proceedings of International Conference on Machine Learning 36},
  pages={2810--2819},
  year={2019}
}

@inproceedings{chu2019probability,
  title={Probability functional descent: A unifying perspective on GANs, variational inference, and reinforcement learning},
  author={Chu, Casey and Blanchet, Jose and Glynn, Peter},
  booktitle={Proceedings of International Conference on Machine Learning 36},
  pages={1213--1222},
  year={2019}
}

@article{ying2020mirror,
  title={Mirror descent algorithms for minimizing interacting free energy},
  author={Ying, Lexing},
  journal={Journal of Scientific Computing},
  volume={84},
  number={3},
  pages={1--14},
  year={2020}
}

@article{schmidt2020nonparametric,
  title={Nonparametric regression using deep neural networks with ReLU activation function},
  author={Schmidt-Hieber, Johannes},
  journal={The Annals of Statistics},
  volume={48},
  number={4},
  pages={1875--1897},
  year={2020},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{imaizumi2019deep,
  title={Deep neural networks learn non-smooth functions effectively},
  author={Imaizumi, Masaaki and Fukumizu, Kenji},
  booktitle={The 22nd international conference on artificial intelligence and statistics},
  pages={869--878},
  year={2019},
  organization={PMLR}
}

@article{imaizumi2020advantage,
  title={Advantage of deep neural networks for estimating functions with singularity on curves},
  author={Imaizumi, Masaaki and Fukumizu, Kenji},
  journal={arXiv preprint arXiv:2011.02256},
  year={2020}
}
