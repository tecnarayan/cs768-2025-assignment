@article{rettinger1957reverberation,
        author={Rettinger, Michael},
        journal={Journal of the Audio Engineering Society},
        title={Reverberation Chambers for Broadcasting and Recording Studios}, 					  year={1957},
        volume={5},
        number={1},
        pages={18-22},
        doi={},
        year={},
        month={January},}						
        
        
@ARTICLE{1166351,  author={Schroeder, M.R. and Logan, B.F.},  journal={IRE Transactions on Audio},   title={"Colorless" artificial reverberation},   year={1961},  volume={AU-9},  number={6},  pages={209-214},  doi={10.1109/TAU.1961.1166351}}

@article{anderegg2004implementation,
        author={Anderegg, Rolf and  Felber, Norbert and  Fichtner, Wolfgang and  Franke, Ulrich},
        journal={Journal of the Audio Engineering Society},
        title={Implementation of High-order Convolution Algorithms with Low Latency on Silicon Chips}, 
        year={2004},
        volume={},
        number={},
        pages={},
        doi={},
        year={},
        month={October},}						
        
        
@inproceedings{NIPS2014_5ca3e9b1,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Adversarial Nets},
 url = {https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
 volume = {27},
 year = {2014}
}


@inproceedings{ratnarajah2022fast,
  title={FAST-RIR: Fast neural diffuse room impulse response generator},
  author={Ratnarajah, Anton and Zhang, Shi-Xiong and Yu, Meng and Tang, Zhenyu and Manocha, Dinesh and Yu, Dong},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={571--575},
  year={2022},
  organization={IEEE}
}


@inproceedings{singh2021image2reverb,
  title={Image2reverb: Cross-modal reverb impulse response synthesis},
  author={Singh, Nikhil and Mentch, Jeff and Ng, Jerry and Beveridge, Matthew and Drori, Iddo},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={286--295},
  year={2021}
}


@article{kon2019estimation,
author={Kon, Homare and Koike, Hideki},
journal={Journal of the Audio Engineering Society},
title={Estimation of Late Reverberation Characteristics from a Single Two-Dimensional Environmental Image using Convolutional Neural Networks},
year={2019},
volume={67},
number={7/8},
pages={540-548},
doi={https://doi.org/10.17743/jaes.2018.0069},
month={July},}



@ARTICLE{7486010,  author={Eaton, James and Gaubitch, Nikolay D. and Moore, Alastair H. and Naylor, Patrick A.},  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},   title={Estimation of Room Acoustic Parameters: The ACE Challenge},   year={2016},  volume={24},  number={10},  pages={1681-1693},  doi={10.1109/TASLP.2016.2577502}}


@INPROCEEDINGS{8521241,  author={Gamper, Hannes and Tashev, Ivan J.},  booktitle={2018 16th International Workshop on Acoustic Signal Enhancement (IWAENC)},   title={Blind Reverberation Time Estimation Using a Convolutional Neural Network},   year={2018},  volume={},  number={},  pages={136-140},  doi={10.1109/IWAENC.2018.8521241}}


@article{Klein_Neidhardt_Seipel_2019, place={Ilmenau}, title={Real-time Estimation of Reverberation Time for Selection of suitable binaural room impulse responses}, url={https://www.db-thueringen.de/receive/dbt_mods_00039968}, DOI={10.22032/dbt.39968}, abstractNote={The aim of auditory augmented reality is to create a highly immersive and plausible auditory illusion combining virtual audio objects and scenarios with the real acoustic surrounding. For this use case it is necessary to estimate the acoustics of the current room. A mismatch between real and simulated acoustics will easily be detected by the listener and will probably lead to In-head localization or an unrealistic acoustic envelopment of the virtual sound sources. This publication investigates State-of-the-Art algorithms for blind reverberation time estimation which are commonly used for speech enhancement algorithms or speech dereverberation and applies them to binaural ear signals. The outcome of these algorithms can be used to select the most appropriate room out of a room database for example. A room database could include pre-measured or simulated binaural room impulse responses which could directly be used to realize a binaural reproduction. First results show promising results combined with low computational effort. Further strategies for enhancing the used method are proposed in order to create a more precise reverberation time estimation.}, journal={Audio for Virtual, Augmented and Mixed Realities: Proceedings of ICSA 2019; 5th International Conference on Spatial Audio; September 26th to 28th, 2019, Ilmenau, Germany}, author={Klein, Florian and Neidhardt, Annika and Seipel, Marius}, year={2019}, month={Nov}, pages={145–150} }


@inproceedings{DBLP:conf/interspeech/MackDH20,
  author    = {Wolfgang Mack and
               Shuwen Deng and
               Emanu{\"{e}}l A. P. Habets},
  editor    = {Helen Meng and
               Bo Xu and
               Thomas Fang Zheng},
  title     = {Single-Channel Blind Direct-to-Reverberation Ratio Estimation Using
               Masking},
  booktitle = {Interspeech 2020, 21st Annual Conference of the International Speech
               Communication Association, Virtual Event, Shanghai, China, 25-29 October
               2020},
  pages     = {5066--5070},
  publisher = {{ISCA}},
  year      = {2020},
  url       = {https://doi.org/10.21437/Interspeech.2020-2171},
  doi       = {10.21437/Interspeech.2020-2171},
  timestamp = {Thu, 04 Nov 2021 13:41:52 +0100},
  biburl    = {https://dblp.org/rec/conf/interspeech/MackDH20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{8506462,  author={Xiong, Feifei and Goetze, Stefan and Kollmeier, Birger and Meyer, Bernd T.},  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},   title={Joint Estimation of Reverberation Time and Early-To-Late Reverberation Ratio From Single-Channel Speech Signals},   year={2019},  volume={27},  number={2},  pages={255-267},  doi={10.1109/TASLP.2018.2877894}}


@article{remaggi2019reproducing,
    author={Remaggi, Luca and  Kim, Hansung and  Jackson, Philip J. B. and  Hilton, Adrian},
    journal={Journal of the Audio Engineering Society}, 					  title={Reproducing Real World Acoustics in Virtual Reality using Spherical Cameras},
    year={2019},
    volume={},
    number={},
    pages={},
    doi={},
    month={March},}				
    
    
    
@MISC{Holters2009IMPULSERM,
  title={IMPULSE RESPONSE MEASUREMENT TECHNIQUES AND THEIR APPLICABILITY IN THE REAL WORLD},
  author={Martin Holters and Tobias Corbach and Udo Z{\"o}lzer},
  year={2009}
}

@article{stan2002comparison,			  		  author={Stan, Guy-Bart and  Embrechts, Jean-Jacques and  Archambeau, Dominique},					  journal={Journal of the Audio Engineering Society}, 					  title={Comparison of Different Impulse Response Measurement Techniques}, 					  year={2002},					  volume={50},					  number={4},					  pages={249-262},					  doi={},					  month={April},}	


@article{imageMethod79,
author = {Allen, Jont and Berkley, David},
year = {1979},
month = {04},
pages = {943-950},
title = {Image method for efficiently simulating small-room acoustics},
volume = {65},
journal = {The Journal of the Acoustical Society of America},
doi = {10.1121/1.382599}
}


@inproceedings{chen2020soundspaces,
  title={Soundspaces: Audio-visual navigation in 3d environments},
  author={Chen, Changan and Jain, Unnat and Schissler, Carl and Gari, Sebastia Vicenc Amengual and Al-Halah, Ziad and Ithapu, Vamsi Krishna and Robinson, Philip and Grauman, Kristen},
  booktitle={European Conference on Computer Vision},
  pages={17--36},
  year={2020},
  organization={Springer}
}

@inproceedings{christensen2020batvision,
  title={Batvision: Learning to see 3d spatial layout with two ears},
  author={Christensen, Jesper Haahr and Hornauer, Sascha and Stella, X Yu},
  booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1581--1587},
  year={2020},
  organization={IEEE}
}


@ARTICLE{4117929,  author={Murphy, Damian and Kelloniemi, Antti and Mullen, Jack and Shelley, Simon},  journal={IEEE Signal Processing Magazine},   title={Acoustic Modeling Using the Digital Waveguide Mesh},   year={2007},  volume={24},  number={2},  pages={55-66},  doi={10.1109/MSP.2007.323264}}


@article{10.1145/2601097.2601184,
author = {Raghuvanshi, Nikunj and Snyder, John},
title = {Parametric Wave Field Coding for Precomputed Sound Propagation},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2601097.2601184},
doi = {10.1145/2601097.2601184},
abstract = {The acoustic wave field in a complex scene is a chaotic 7D function of time and the positions of source and listener, making it difficult to compress and interpolate. This hampers precomputed approaches which tabulate impulse responses (IRs) to allow immersive, real-time sound propagation in static scenes. We code the field of time-varying IRs in terms of a few perceptual parameters derived from the IR's energy decay. The resulting parameter fields are spatially smooth and compressed using a lossless scheme similar to PNG. We show that this encoding removes two of the seven dimensions, making it possible to handle large scenes such as entire game maps within 100MB of memory. Run-time decoding is fast, taking 100μs per source. We introduce an efficient and scalable method for convolutionally rendering acoustic parameters that generates artifact-free audio even for fast motion and sudden changes in reverberance. We demonstrate convincing spatially-varying effects in complex scenes including occlusion/obstruction and reverberation, in our system integrated with Unreal Engine 3™.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {38},
numpages = {11},
keywords = {room acoustics, early decay time, occlusion, DSP, parametric reverb, convolution, impulse response, scattering, obstruction, environmental effects, wave equation, exclusion, reverberation time, diffraction}
}


@article{murgai2017blind,
author={Murgai, Prateek and Rau, Mark and Jot, Jean-Marc},
journal={Journal of the Audio Engineering Society},
title={Blind Estimation of the Reverberation Fingerprint of Unknown Acoustic Environments},
year={2017},
volume={},
number={},
pages={},
doi={},
month={October},}


@article{10.1109/TVCG.2014.38,
author = {Mehra, Ravish and Antani, Lakulish and Kim, Sujeong and Manocha, Dinesh},
title = {Source and Listener Directivity for Interactive Wave-Based Sound Propagation},
year = {2014},
issue_date = {April 2014},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {20},
number = {4},
issn = {1077-2626},
url = {https://doi.org/10.1109/TVCG.2014.38},
doi = {10.1109/TVCG.2014.38},
abstract = {We present an approach to model dynamic, data-driven source and listener directivity for interactive wave-based sound propagation in virtual environments and computer games. Our directional source representation is expressed as a linear combination of elementary spherical harmonic (SH) sources. In the preprocessing stage, we precompute and encode the propagated sound fields due to each SH source. At runtime, we perform the SH decomposition of the varying source directivity interactively and compute the total sound field at the listener position as a weighted sum of precomputed SH sound fields. We propose a novel plane-wave decomposition approach based on higher-order derivatives of the sound field that enables dynamic HRTF-based listener directivity at runtime. We provide a generic framework to incorporate our source and listener directivity in any offline or online frequency-domain wave-based sound propagation algorithm. We have integrated our sound propagation system in Valve's Source game engine and use it to demonstrate realistic acoustic effects such as sound amplification, diffraction low-passing, scattering, localization, externalization, and spatial sound, generated by wave-based propagation of directional sources and listener in complex scenarios. We also present results from our preliminary user study.},
journal = {IEEE Transactions on Visualization and Computer Graphics},
month = {apr},
pages = {495–503},
numpages = {9},
keywords = {Sound propagation, Acoustics, Equations, plane-wave decomposition, spatial sound, Mathematical model, Computational modeling, Ear, helmholtz equation, directivity, Frequency-domain analysis, Runtime}
}




@article{10.1145/3197517.3201339,
author = {Raghuvanshi, Nikunj and Snyder, John},
title = {Parametric Directional Coding for Precomputed Sound Propagation},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3197517.3201339},
doi = {10.1145/3197517.3201339},
abstract = {Convincing audio for games and virtual reality requires modeling directional propagation effects. The initial sound's arrival direction is particularly salient and derives from multiply-diffracted paths in complex scenes. When source and listener straddle occluders, the initial sound and multiply-scattered reverberation stream through gaps and portals, helping the listener navigate. Geometry near the source and/or listener reveals its presence through anisotropic reflections. We propose the first precomputed wave technique to capture such directional effects in general scenes comprising millions of polygons. These effects are formally represented with the 9D directional response function of 3D source and listener location, time, and direction at the listener, making memory use the major concern. We propose a novel parametric encoder that compresses this function within a budget of ~100MB for large scenes, while capturing many salient acoustic effects indoors and outdoors. The encoder is complemented with a lightweight signal processing algorithm whose filtering cost is largely insensitive to the number of sound sources, resulting in an immediately practical system.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {108},
numpages = {14},
keywords = {directional impulse response, virtual acoustics, sound propagation, plane wave decomposition, flux density, wave equation, spatial audio, HRTF, vector intensity}
}


@article{10.1145/3386569.3392459,
author = {Chaitanya, Chakravarty R. Alla and Raghuvanshi, Nikunj and Godin, Keith W. and Zhang, Zechen and Nowrouzezahrai, Derek and Snyder, John M.},
title = {Directional Sources and Listeners in Interactive Sound Propagation Using Reciprocal Wave Field Coding},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3386569.3392459},
doi = {10.1145/3386569.3392459},
abstract = {Common acoustic sources, like voices or musical instruments, exhibit strong frequency and directional dependence. When transported through complex environments, their anisotropic radiated field undergoes scattering, diffraction, and occlusion before reaching a directionally-sensitive listener. We present the first wave-based interactive auralization system that encodes and renders a complete reciprocal description of acoustic wave fields in general scenes. Our method renders directional effects at freely moving and rotating sources and listeners and supports any tabulated source directivity function and head-related transfer function. We represent a static scene's global acoustic transfer as an 11-dimensional bidirectional impulse response (BIR) field, which we extract from a set of wave simulations. We parametrically encode the BIR as a pair of radiating and arriving directions for the perceptually-salient initial (direct) response, and a compact 6 \texttimes{} 6 reflections transfer matrix capturing indirect energy transfer with scene-dependent anisotropy. We render our encoded data with an efficient and scalable algorithm - integrated in the Unreal Engine™ - whose CPU performance is agnostic to scene complexity and angular source/listener resolutions. We demonstrate convincing effects that depend on detailed scene geometry, for a variety of environments and source types.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {44},
numpages = {14},
keywords = {head-related transfer function (HRTF), wave simulation, sound propagation, source directivity, bidirectional impulse response, equalization, spatial audio, virtual acoustics}
}


@article{luo2022learning,
  title={Learning Neural Acoustic Fields},
  author={Luo, Andrew and Du, Yilun and Tarr, Michael J and Tenenbaum, Joshua B and Torralba, Antonio and Gan, Chuang},
  journal={arXiv preprint arXiv:2204.00628},
  year={2022}
}

@inproceedings{mildenhall2020nerf,
  title={Nerf: Representing scenes as neural radiance fields for view synthesis},
  author={Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and Barron, Jonathan T and Ramamoorthi, Ravi and Ng, Ren},
  booktitle={European conference on computer vision},
  pages={405--421},
  year={2020},
  organization={Springer}
}

@article{sitzmann2020implicit,
  title={Implicit neural representations with periodic activation functions},
  author={Sitzmann, Vincent and Martel, Julien and Bergman, Alexander and Lindell, David and Wetzstein, Gordon},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7462--7473},
  year={2020}
}


@article{DBLP:journals/corr/abs-1812-04204,
  author    = {Ruohan Gao and
               Kristen Grauman},
  title     = {2.5D Visual Sound},
  journal   = {CoRR},
  volume    = {abs/1812.04204},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.04204},
  eprinttype = {arXiv},
  eprint    = {1812.04204},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-04204.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gao20192,
  title={2.5 d visual sound},
  author={Gao, Ruohan and Grauman, Kristen},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={324--333},
  year={2019}
}



@article{DBLP:journals/corr/abs-2007-09902,
  author    = {Hang Zhou and
               Xudong Xu and
               Dahua Lin and
               Xiaogang Wang and
               Ziwei Liu},
  title     = {Sep-Stereo: Visually Guided Stereophonic Audio Generation by Associating
               Source Separation},
  journal   = {CoRR},
  volume    = {abs/2007.09902},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.09902},
  eprinttype = {arXiv},
  eprint    = {2007.09902},
  timestamp = {Thu, 14 Oct 2021 09:15:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-09902.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{Xu_2021_CVPR,
    author    = {Xu, Xudong and Zhou, Hang and Liu, Ziwei and Dai, Bo and Wang, Xiaogang and Lin, Dahua},
    title     = {Visually Informed Binaural Audio Generation without Binaural Audios},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {15485-15494}
}

@InProceedings{Rachavarapu_2021_ICCV,
    author    = {Rachavarapu, Kranthi Kumar and Aakanksha and Sundaresha, Vignesh and Rajagopalan, A. N.},
    title     = {Localize to Binauralize: Audio Spatialization From Visual Sound Source Localization},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {1930-1939}
}


@INPROCEEDINGS{5995444,  author={Fathi, Alireza and Ren, Xiaofeng and Rehg, James M.},  booktitle={CVPR 2011},   title={Learning to recognize objects in egocentric activities},   year={2011},  volume={},  number={},  pages={3281-3288},  doi={10.1109/CVPR.2011.5995444}}


@InProceedings{10.1007/978-3-642-33718-5_23,
author="Fathi, Alireza
and Li, Yin
and Rehg, James M.",
editor="Fitzgibbon, Andrew
and Lazebnik, Svetlana
and Perona, Pietro
and Sato, Yoichi
and Schmid, Cordelia",
title="Learning to Recognize Daily Actions Using Gaze",
booktitle="Computer Vision -- ECCV 2012",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="314--327",
abstract="We present a probabilistic generative model for simultaneously recognizing daily actions and predicting gaze locations in videos recorded from an egocentric camera. We focus on activities requiring eye-hand coordination and model the spatio-temporal relationship between the gaze point, the scene objects, and the action label. Our model captures the fact that the distribution of both visual features and object occurrences in the vicinity of the gaze point is correlated with the verb-object pair describing the action. It explicitly incorporates known properties of gaze behavior from the psychology literature, such as the temporal delay between fixation and manipulation events. We present an inference method that can predict the best sequence of gaze locations and the associated action label from an input sequence of images. We demonstrate improvements in action recognition rates and gaze prediction accuracy relative to state-of-the-art methods, on two new datasets that contain egocentric videos of daily activities and gaze.",
isbn="978-3-642-33718-5"
}


@INPROCEEDINGS{6248010,  author={Pirsiavash, Hamed and Ramanan, Deva},  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition},   title={Detecting activities of daily living in first-person camera views},   year={2012},  volume={},  number={},  pages={2847-2854},  doi={10.1109/CVPR.2012.6248010}}


@article{li2021eye,
  title={In the eye of the beholder: Gaze and actions in first person video},
  author={Li, Yin and Liu, Miao and Rehg, Jame},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  publisher={IEEE}
}

@inproceedings{fouhey2018lifestyle,
  title={From lifestyle vlogs to everyday interactions},
  author={Fouhey, David F and Kuo, Wei-cheng and Efros, Alexei A and Malik, Jitendra},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4991--5000},
  year={2018}
}

@inproceedings{kazakos2019epic,
  title={Epic-fusion: Audio-visual temporal binding for egocentric action recognition},
  author={Kazakos, Evangelos and Nagrani, Arsha and Zisserman, Andrew and Damen, Dima},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={5492--5501},
  year={2019}
}

@inproceedings{munro2020multi,
  title={Multi-modal domain adaptation for fine-grained action recognition},
  author={Munro, Jonathan and Damen, Dima},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={122--132},
  year={2020}
}


@article{DBLP:journals/corr/abs-2110-07058,
  author    = {Kristen Grauman and
               Andrew Westbury and
               Eugene Byrne and
               Zachary Chavis and
               Antonino Furnari and
               Rohit Girdhar and
               Jackson Hamburger and
               Hao Jiang and
               Miao Liu and
               Xingyu Liu and
               Miguel Martin and
               Tushar Nagarajan and
               Ilija Radosavovic and
               Santhosh Kumar Ramakrishnan and
               Fiona Ryan and
               Jayant Sharma and
               Michael Wray and
               Mengmeng Xu and
               Eric Zhongcong Xu and
               Chen Zhao and
               Siddhant Bansal and
               Dhruv Batra and
               Vincent Cartillier and
               Sean Crane and
               Tien Do and
               Morrie Doulaty and
               Akshay Erapalli and
               Christoph Feichtenhofer and
               Adriano Fragomeni and
               Qichen Fu and
               Christian Fuegen and
               Abrham Gebreselasie and
               Cristina Gonzalez and
               James Hillis and
               Xuhua Huang and
               Yifei Huang and
               Wenqi Jia and
               Weslie Khoo and
               Jachym Kolar and
               Satwik Kottur and
               Anurag Kumar and
               Federico Landini and
               Chao Li and
               Yanghao Li and
               Zhenqiang Li and
               Karttikeya Mangalam and
               Raghava Modhugu and
               Jonathan Munro and
               Tullie Murrell and
               Takumi Nishiyasu and
               Will Price and
               Paola Ruiz Puentes and
               Merey Ramazanova and
               Leda Sari and
               Kiran Somasundaram and
               Audrey Southerland and
               Yusuke Sugano and
               Ruijie Tao and
               Minh Vo and
               Yuchen Wang and
               Xindi Wu and
               Takuma Yagi and
               Yunyi Zhu and
               Pablo Arbelaez and
               David Crandall and
               Dima Damen and
               Giovanni Maria Farinella and
               Bernard Ghanem and
               Vamsi Krishna Ithapu and
               C. V. Jawahar and
               Hanbyul Joo and
               Kris Kitani and
               Haizhou Li and
               Richard Newcombe and
               Aude Oliva and
               Hyun Soo Park and
               James M. Rehg and
               Yoichi Sato and
               Jianbo Shi and
               Mike Zheng Shou and
               Antonio Torralba and
               Lorenzo Torresani and
               Mingfei Yan and
               Jitendra Malik},
  title     = {Ego4D: Around the World in 3, 000 Hours of Egocentric Video},
  journal   = {CoRR},
  volume    = {abs/2110.07058},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.07058},
  eprinttype = {arXiv},
  eprint    = {2110.07058},
  timestamp = {Fri, 05 Nov 2021 13:22:27 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-07058.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

   @INPROCEEDINGS{Damen2018EPICKITCHENS,
   title={Scaling Egocentric Vision: The EPIC-KITCHENS Dataset},
   author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria  and Fidler, Sanja and 
           Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan 
           and Perrett, Toby and Price, Will and Wray, Michael},
   booktitle={European Conference on Computer Vision (ECCV)},
   year={2018}
} 


@ARTICLE{Damen2021RESCALING,
           title={Rescaling Egocentric Vision: Collection, Pipeline and Challenges for EPIC-KITCHENS-100},
           author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria  and and Furnari, Antonino 
           and Ma, Jian and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan 
           and Perrett, Toby and Price, Will and Wray, Michael},
           journal   = {International Journal of Computer Vision (IJCV)},
           year      = {2021},
           Url       = {https://doi.org/10.1007/s11263-021-01531-2}
} 

@InProceedings{Tsutsui_2021_WACV,
    author    = {Tsutsui, Satoshi and Fu, Yanwei and Crandall, David J.},
    title     = {Whose Hand Is This? Person Identification From Egocentric Hand Gestures},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2021},
    pages     = {3399-3408}
}

@INPROCEEDINGS{9157609,  author={Cai, Minjie and Lu, Feng and Sato, Yoichi},  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Generalizing Hand Segmentation in Egocentric Videos With Uncertainty-Guided Model Adaptation},   year={2020},  volume={},  number={},  pages={14380-14389},  doi={10.1109/CVPR42600.2020.01440}}

@INPROCEEDINGS{7410583,  author={Bambach, Sven and Lee, Stefan and Crandall, David J. and Yu, Chen},  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)},   title={Lending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions},   year={2015},  volume={},  number={},  pages={1949-1957},  doi={10.1109/ICCV.2015.226}}


@InProceedings{10.1007/978-3-319-10602-1_19,
author="Xiong, Bo
and Grauman, Kristen",
editor="Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne",
title="Detecting Snap Points in Egocentric Video with a Web Photo Prior",
booktitle="Computer Vision -- ECCV 2014",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="282--298",
abstract="Wearable cameras capture a first-person view of the world, and offer a hands-free way to record daily experiences or special events. Yet, not every frame is worthy of being captured and stored. We propose to automatically predict ``snap points'' in unedited egocentric video---that is, those frames that look like they could have been intentionally taken photos. We develop a generative model for snap points that relies on a Web photo prior together with domain-adapted features. Critically, our approach avoids strong assumptions about the particular content of snap points, focusing instead on their composition. Using 17 hours of egocentric video from both human and mobile robot camera wearers, we show that the approach accurately isolates those frames that human judges would believe to be intentionally snapped photos. In addition, we demonstrate the utility of snap point detection for improving object detection and keyframe selection in egocentric video.",
isbn="978-3-319-10602-1"
}


@article{DBLP:journals/corr/Garcia-Hernando17,
  author    = {Guillermo Garcia{-}Hernando and
               Shanxin Yuan and
               Seungryul Baek and
               Tae{-}Kyun Kim},
  title     = {First-Person Hand Action Benchmark with {RGB-D} Videos and 3D Hand
               Pose Annotations},
  journal   = {CoRR},
  volume    = {abs/1704.02463},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.02463},
  eprinttype = {arXiv},
  eprint    = {1704.02463},
  timestamp = {Mon, 13 Aug 2018 16:47:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Garcia-Hernando17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-1911-10967,
  author    = {Miao Liu and
               Siyu Tang and
               Yin Li and
               James M. Rehg},
  title     = {Forecasting Human Object Interaction: Joint Prediction of Motor Attention
               and Egocentric Activity},
  journal   = {CoRR},
  volume    = {abs/1911.10967},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.10967},
  eprinttype = {arXiv},
  eprint    = {1911.10967},
  timestamp = {Wed, 12 Jan 2022 09:08:13 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-10967.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2001-04583,
  author    = {Tushar Nagarajan and
               Yanghao Li and
               Christoph Feichtenhofer and
               Kristen Grauman},
  title     = {{EGO-TOPO:} Environment Affordances from Egocentric Video},
  journal   = {CoRR},
  volume    = {abs/2001.04583},
  year      = {2020},
  url       = {https://arxiv.org/abs/2001.04583},
  eprinttype = {arXiv},
  eprint    = {2001.04583},
  timestamp = {Fri, 17 Jan 2020 14:07:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2001-04583.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/RhinehartK16a,
  author    = {Nicholas Rhinehart and
               Kris M. Kitani},
  title     = {Online Semantic Activity Forecasting with {DARKO}},
  journal   = {CoRR},
  volume    = {abs/1612.07796},
  year      = {2016},
  url       = {http://arxiv.org/abs/1612.07796},
  eprinttype = {arXiv},
  eprint    = {1612.07796},
  timestamp = {Mon, 13 Aug 2018 16:46:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RhinehartK16a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-1803-09125,
  author    = {Yifei Huang and
               Minjie Cai and
               Zhenqiang Li and
               Yoichi Sato},
  title     = {Predicting Gaze in Egocentric Video by Learning Task-dependent Attention
               Transition},
  journal   = {CoRR},
  volume    = {abs/1803.09125},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.09125},
  eprinttype = {arXiv},
  eprint    = {1803.09125},
  timestamp = {Mon, 13 Aug 2018 16:47:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-09125.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{tavakoli2019digging,
  title={Digging deeper into egocentric gaze prediction},
  author={Tavakoli, Hamed Rezazadegan and Rahtu, Esa and Kannala, Juho and Borji, Ali},
  booktitle={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={273--282},
  year={2019},
  organization={IEEE}
}

@article{DBLP:journals/corr/SharghiLG17,
  author    = {Aidean Sharghi and
               Jacob S. Laurel and
               Boqing Gong},
  title     = {Query-Focused Video Summarization: Dataset, Evaluation, and {A} Memory
               Network Based Approach},
  journal   = {CoRR},
  volume    = {abs/1707.04960},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.04960},
  eprinttype = {arXiv},
  eprint    = {1707.04960},
  timestamp = {Mon, 13 Aug 2018 16:48:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SharghiLG17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{6619194,  author={Lu, Zheng and Grauman, Kristen},  booktitle={2013 IEEE Conference on Computer Vision and Pattern Recognition},   title={Story-Driven Summarization for Egocentric Video},   year={2013},  volume={},  number={},  pages={2714-2721},  doi={10.1109/CVPR.2013.350}}

@INPROCEEDINGS{6247820,  author={Lee, Yong Jae and Ghosh, Joydeep and Grauman, Kristen},  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition},   title={Discovering important people and objects for egocentric video summarization},   year={2012},  volume={},  number={},  pages={1346-1353},  doi={10.1109/CVPR.2012.6247820}}


@article{DBLP:journals/corr/abs-1904-09882,
  author    = {Evonne Ng and
               Donglai Xiang and
               Hanbyul Joo and
               Kristen Grauman},
  title     = {You2Me: Inferring Body Pose in Egocentric Video via First and Second
               Person Interactions},
  journal   = {CoRR},
  volume    = {abs/1904.09882},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.09882},
  eprinttype = {arXiv},
  eprint    = {1904.09882},
  timestamp = {Fri, 26 Apr 2019 13:18:53 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-09882.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-1906-03173,
  author    = {Ye Yuan and
               Kris Kitani},
  title     = {Ego-Pose Estimation and Forecasting as Real-Time {PD} Control},
  journal   = {CoRR},
  volume    = {abs/1906.03173},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.03173},
  eprinttype = {arXiv},
  eprint    = {1906.03173},
  timestamp = {Fri, 14 Jun 2019 09:38:24 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-03173.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-1907-10045,
  author    = {Denis Tom{\`{e}} and
               Patrick Peluse and
               Lourdes Agapito and
               Hern{\'{a}}n Badino},
  title     = {xR-EgoPose: Egocentric 3D Human Pose from an {HMD} Camera},
  journal   = {CoRR},
  volume    = {abs/1907.10045},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.10045},
  eprinttype = {arXiv},
  eprint    = {1907.10045},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-10045.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/JiangG16,
  author    = {Hao Jiang and
               Kristen Grauman},
  title     = {Seeing Invisible Poses: Estimating 3D Body Pose from Egocentric Video},
  journal   = {CoRR},
  volume    = {abs/1603.07763},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.07763},
  eprinttype = {arXiv},
  eprint    = {1603.07763},
  timestamp = {Tue, 25 Sep 2018 16:25:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/JiangG16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{7299061,
  author={Rogez, Grégory and Supančič, James S. and Ramanan, Deva},
  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={First-person pose recognition using egocentric workspaces}, 
  year={2015},
  volume={},
  number={},
  pages={4325-4333},
  doi={10.1109/CVPR.2015.7299061}}
  
  
@inproceedings{BMVC.28.30,
	title = {You-Do, I-Learn: Discovering Task Relevant Objects and their Modes of Interaction from Multi-User Egocentric Video},
	author = {Damen, Dima and Leelasawassuk, Teesid and Haines, Osian and Calway, Andrew and Mayol-Cuevas, Walterio},
	year = {2014},
	booktitle = {Proceedings of the British Machine Vision Conference},
	publisher = {BMVA Press},
	editors = {Valstar, Michel and French, Andrew and Pridmore, Tony},
	doi = { http://dx.doi.org/10.5244/C.28.30 }
}

@article{DBLP:journals/corr/GuptaDLSM17,
  author    = {Saurabh Gupta and
               James Davidson and
               Sergey Levine and
               Rahul Sukthankar and
               Jitendra Malik},
  title     = {Cognitive Mapping and Planning for Visual Navigation},
  journal   = {CoRR},
  volume    = {abs/1702.03920},
  year      = {2017},
  url       = {http://arxiv.org/abs/1702.03920},
  eprinttype = {arXiv},
  eprint    = {1702.03920},
  timestamp = {Mon, 04 Mar 2019 08:31:20 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/GuptaDLSM17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-1903-01959,
  author    = {Tao Chen and
               Saurabh Gupta and
               Abhinav Gupta},
  title     = {Learning Exploration Policies for Navigation},
  journal   = {CoRR},
  volume    = {abs/1903.01959},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.01959},
  eprinttype = {arXiv},
  eprint    = {1903.01959},
  timestamp = {Mon, 22 Jul 2019 07:24:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1903-01959.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1911-00357,
  author    = {Erik Wijmans and
               Abhishek Kadian and
               Ari Morcos and
               Stefan Lee and
               Irfan Essa and
               Devi Parikh and
               Manolis Savva and
               Dhruv Batra},
  title     = {Decentralized Distributed {PPO:} Solving PointGoal Navigation},
  journal   = {CoRR},
  volume    = {abs/1911.00357},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.00357},
  eprinttype = {arXiv},
  eprint    = {1911.00357},
  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-00357.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
Chaplot2020Learning,
title={Learning To Explore Using Active Neural SLAM},
author={Devendra Singh Chaplot and Dhiraj Gandhi and Saurabh Gupta and Abhinav Gupta and Ruslan Salakhutdinov},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HklXn1BKDH}
}

@article{DBLP:journals/corr/abs-2008-09622,
  author    = {Changan Chen and
               Sagnik Majumder and
               Ziad Al{-}Halah and
               Ruohan Gao and
               Santhosh Kumar Ramakrishnan and
               Kristen Grauman},
  title     = {Audio-Visual Waypoints for Navigation},
  journal   = {CoRR},
  volume    = {abs/2008.09622},
  year      = {2020},
  url       = {https://arxiv.org/abs/2008.09622},
  eprinttype = {arXiv},
  eprint    = {2008.09622},
  timestamp = {Fri, 28 Aug 2020 12:11:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2008-09622.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-2012-11583,
  author    = {Changan Chen and
               Ziad Al{-}Halah and
               Kristen Grauman},
  title     = {Semantic Audio-Visual Navigation},
  journal   = {CoRR},
  volume    = {abs/2012.11583},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.11583},
  eprinttype = {arXiv},
  eprint    = {2012.11583},
  timestamp = {Mon, 04 Jan 2021 16:33:46 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-11583.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
ramakrishnan2022environment,
title={Environment Predictive Coding for Visual Navigation},
author={Santhosh Kumar Ramakrishnan and Tushar Nagarajan and Ziad Al-Halah and Kristen Grauman},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=DBiQQYWykyy}
}

@inproceedings{majumder2021move2hear,
  title={Move2hear: Active audio-visual source separation},
  author={Majumder, Sagnik and Al-Halah, Ziad and Grauman, Kristen},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={275--285},
  year={2021}
}

@article{majumder2022active,
  title={Active Audio-Visual Separation of Dynamic Sound Sources},
  author={Majumder, Sagnik and Al-Halah, Ziad and Grauman, Kristen},
  journal={arXiv preprint arXiv:2202.00850},
  year={2022}
}



@article{DBLP:journals/corr/abs-2008-09241,
  author    = {Tushar Nagarajan and
               Kristen Grauman},
  title     = {Learning Affordance Landscapes forInteraction Exploration in 3D Environments},
  journal   = {CoRR},
  volume    = {abs/2008.09241},
  year      = {2020},
  url       = {https://arxiv.org/abs/2008.09241},
  eprinttype = {arXiv},
  eprint    = {2008.09241},
  timestamp = {Fri, 28 Aug 2020 12:11:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2008-09241.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/ZhuMKLGFF16,
  author    = {Yuke Zhu and
               Roozbeh Mottaghi and
               Eric Kolve and
               Joseph J. Lim and
               Abhinav Gupta and
               Li Fei{-}Fei and
               Ali Farhadi},
  title     = {Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement
               Learning},
  journal   = {CoRR},
  volume    = {abs/1609.05143},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.05143},
  eprinttype = {arXiv},
  eprint    = {1609.05143},
  timestamp = {Mon, 22 Jul 2019 14:55:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ZhuMKLGFF16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{afouras2018conversation,
  title={The conversation: Deep audio-visual speech enhancement},
  author={Afouras, Triantafyllos and Chung, Joon Son and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1804.04121},
  year={2018}
}


@inproceedings{afouras2020self,
  title={Self-supervised learning of audio-visual objects from video},
  author={Afouras, Triantafyllos and Owens, Andrew and Chung, Joon Son and Zisserman, Andrew},
  booktitle={European Conference on Computer Vision},
  pages={208--224},
  year={2020},
  organization={Springer}
}

@article{chen2021learning,
  title={Learning audio-visual dereverberation},
  author={Chen, Changan and Sun, Wei and Harwath, David and Grauman, Kristen},
  journal={arXiv preprint arXiv:2106.07732},
  year={2021}
}


@article{ephrat2018looking,
  title={Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation},
  author={Ephrat, Ariel and Mosseri, Inbar and Lang, Oran and Dekel, Tali and Wilson, Kevin and Hassidim, Avinatan and Freeman, William T and Rubinstein, Michael},
  journal={arXiv preprint arXiv:1804.03619},
  year={2018}
}


@article{hou2018audio,
  title={Audio-visual speech enhancement using multimodal deep convolutional neural networks},
  author={Hou, Jen-Cheng and Wang, Syu-Siang and Lai, Ying-Hui and Tsao, Yu and Chang, Hsiu-Wen and Wang, Hsin-Min},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume={2},
  number={2},
  pages={117--128},
  year={2018},
  publisher={IEEE}
}

@article{michelsanti2021overview,
  title={An overview of deep-learning-based audio-visual speech enhancement and separation},
  author={Michelsanti, Daniel and Tan, Zheng-Hua and Zhang, Shi-Xiong and Xu, Yong and Yu, Meng and Yu, Dong and Jensen, Jesper},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2021},
  publisher={IEEE}
}

@inproceedings{owens2018audio,
  title={Audio-visual scene analysis with self-supervised multisensory features},
  author={Owens, Andrew and Efros, Alexei A},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={631--648},
  year={2018}
}

@article{sadeghi2020audio,
  title={Audio-visual speech enhancement using conditional variational auto-encoders},
  author={Sadeghi, Mostafa and Leglaive, Simon and Alameda-Pineda, Xavier and Girin, Laurent and Horaud, Radu},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={28},
  pages={1788--1800},
  year={2020},
  publisher={IEEE}
}

@inproceedings{zhao2019sound,
  title={The sound of motions},
  author={Zhao, Hang and Gan, Chuang and Ma, Wei-Chiu and Torralba, Antonio},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1735--1744},
  year={2019}
}

@inproceedings{zhou2019vision,
  title={Vision-infused deep audio inpainting},
  author={Zhou, Hang and Liu, Ziwei and Xu, Xudong and Luo, Ping and Wang, Xiaogang},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={283--292},
  year={2019}
}


@article{hu2020discriminative,
  title={Discriminative sounding objects localization via self-supervised audiovisual matching},
  author={Hu, Di and Qian, Rui and Jiang, Minyue and Tan, Xiao and Wen, Shilei and Ding, Errui and Lin, Weiyao and Dou, Dejing},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={10077--10087},
  year={2020}
}

@article{jiang2022egocentric,
  title={Egocentric Deep Multi-Channel Audio-Visual Active Speaker Localization},
  author={Jiang, Hao and Murdock, Calvin and Ithapu, Vamsi Krishna},
  journal={arXiv preprint arXiv:2201.01928},
  year={2022}
}


@article{alwassel2020self,
  title={Self-supervised learning by cross-modal audio-video clustering},
  author={Alwassel, Humam and Mahajan, Dhruv and Korbar, Bruno and Torresani, Lorenzo and Ghanem, Bernard and Tran, Du},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9758--9770},
  year={2020}
}




@article{morgado2020learning,
  title={Learning representations from audio-visual spatial alignment},
  author={Morgado, Pedro and Li, Yi and Nvasconcelos, Nuno},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={4733--4744},
  year={2020}
}


@article{korbar2018cooperative,
  title={Cooperative learning of audio and video models from self-supervised synchronization},
  author={Korbar, Bruno and Tran, Du and Torresani, Lorenzo},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}


@inproceedings{gan2020look,
  title={Look, listen, and act: Towards audio-visual embodied navigation},
  author={Gan, Chuang and Zhang, Yiwei and Wu, Jiajun and Gong, Boqing and Tenenbaum, Joshua B},
  booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={9701--9707},
  year={2020},
  organization={IEEE}
}

@inproceedings{NEURIPS2020_ab6b331e,
 author = {Dean, Victoria and Tulsiani, Shubham and Gupta, Abhinav},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {14961--14972},
 publisher = {Curran Associates, Inc.},
 title = {See, Hear, Explore: Curiosity via Audio-Visual Association},
 url = {https://proceedings.neurips.cc/paper/2020/file/ab6b331e94c28169d15cca0cb3bbc73e-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{chen2022visual,
  title={Visual Acoustic Matching},
  author={Chen, Changan and Gao, Ruohan and Calamia, Paul and Grauman, Kristen},
  journal={arXiv preprint arXiv:2202.06875},
  year={2022}
}


@inproceedings{steinmetz2021filtered,
  title={Filtered Noise Shaping for Time Domain Room Impulse Response Estimation From Reverberant Speech},
  author={Steinmetz, Christian J and Ithapu, Vamsi Krishna and Calamia, Paul},
  booktitle={2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
  pages={221--225},
  year={2021},
  organization={IEEE}
}

@inproceedings{gao2020visualechoes,
  title = {VisualEchoes: Spatial Image Representation Learning through Echolocation},
  author = {Gao, Ruohan and Chen, Changan and Al-Halab, Ziad and Schissler, Carl and Grauman, Kristen},
  booktitle = {ECCV},
  year = {2020}
}

@inproceedings{purushwalkam2021audio,
  title={Audio-visual floorplan reconstruction},
  author={Purushwalkam, Senthil and Gari, Sebastia Vicenc Amengual and Ithapu, Vamsi Krishna and Schissler, Carl and Robinson, Philip and Gupta, Abhinav and Grauman, Kristen},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1183--1192},
  year={2021}
}


@article{10.1109/TASL.2013.2256897,
author = {Bilbao, Stefan},
title = {Modeling of Complex Geometries and Boundary Conditions in Finite Difference/Finite Volume Time Domain Room Acoustics Simulation},
year = {2013},
issue_date = {July 2013},
publisher = {IEEE Press},
volume = {21},
number = {7},
issn = {1558-7916},
url = {https://doi.org/10.1109/TASL.2013.2256897},
doi = {10.1109/TASL.2013.2256897},
abstract = {Due to recent increases in computing power, room acoustics simulation in 3D using time stepping schemes is becoming a viable alternative to standard methods based on ray tracing and the image source method. Finite Difference Time Domain (FDTD) methods, operating over regular grids, are perhaps the best known among such methods, which simulate the acoustic field in its entirety over the problem domain. In a realistic room acoustics setting, working over a regular grid is attractive from a computational standpoint, but is complicated by geometrical considerations, particularly when the geometry does not conform neatly to the grid, and those of boundary conditions which emulate the properties of real wall materials. Both such features may be dealt with through an appeal to methods operating over unstructured grids, such as finite volume methods, which reduce to FDTD when employed over regular grids. Through numerical energy analysis, such methods lead to direct stability conditions for complex problems, including convenient geometrical conditions at irregular boundaries. Simulation results are presented.},
journal = {Trans. Audio, Speech and Lang. Proc.},
month = {jul},
pages = {1524–1533},
numpages = {10}
}



@article{10.1145/2980179.2982431,
author = {Cao, Chunxiao and Ren, Zhong and Schissler, Carl and Manocha, Dinesh and Zhou, Kun},
title = {Interactive Sound Propagation with Bidirectional Path Tracing},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/2980179.2982431},
doi = {10.1145/2980179.2982431},
abstract = {We introduce Bidirectional Sound Transport (BST), a new algorithm that simulates sound propagation by bidirectional path tracing using multiple importance sampling. Our approach can handle multiple sources in large virtual environments with complex occlusion, and can produce plausible acoustic effects at an interactive rate on a desktop PC. We introduce a new metric based on the signal-to-noise ratio (SNR) of the energy response and use this metric to evaluate the performance of ray-tracing-based acoustic simulation methods. Our formulation exploits temporal coherence in terms of using the resulting sample distribution of the previous frame to guide the sample distribution of the current one. We show that our sample redistribution algorithm converges and better balances between early and late reflections. We evaluate our approach on different benchmarks and demonstrate significant speedup over prior geometric acoustic algorithms.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {180},
numpages = {11},
keywords = {sound propagation, bidirectional path tracing}
}

@MISC{Funkhouser03abeam,
    author = {Thomas Funkhouser and Nicolas Tsingos and Ingrid Carlbom and Gary Elko and Mohan Sondhi and James E. West and Gopal Pingali and Patrick Min and Addy Ngan},
    title = {A Beam Tracing Method for Interactive Architectural Acoustics},
    year = {2003}
}


@article{Savioja2015OverviewOG,
  title={Overview of geometrical room acoustic modeling techniques.},
  author={Lauri Savioja and U. Peter Svensson},
  journal={The Journal of the Acoustical Society of America},
  year={2015},
  volume={138 2},
  pages={
          708-30
        }
}


@article{chang2017matterport3d,
  title={Matterport3d: Learning from rgb-d data in indoor environments},
  author={Chang, Angel and Dai, Angela and Funkhouser, Thomas and Halber, Maciej and Niessner, Matthias and Savva, Manolis and Song, Shuran and Zeng, Andy and Zhang, Yinda},
  journal={arXiv preprint arXiv:1709.06158},
  year={2017},
  note={{Matterport3D license available at http://kaldir.vc.in.tum.de/matterport/MP\_TOS.pdf}}
}


@inproceedings{habitat19iccv,
  title     =     {Habitat: {A} {P}latform for {E}mbodied {AI} {R}esearch},
  author    =     {Manolis Savva and Abhishek Kadian and Oleksandr Maksymets and Yili Zhao and Erik Wijmans and Bhavana Jain and Julian Straub and Jia Liu and Vladlen Koltun and Jitendra Malik and Devi Parikh and Dhruv Batra},
  booktitle =     {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year      =     {2019}
}

@INPROCEEDINGS{9054701,  author={Su, Jiaqi and Jin, Zeyu and Finkelstein, Adam},  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   title={Acoustic Matching By Embedding Impulse Responses},   year={2020},  volume={},  number={},  pages={426-430},  doi={10.1109/ICASSP40776.2020.9054701}}


@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{sun2015deeply,
  title={Deeply learned face representations are sparse, selective, and robust},
  author={Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2892--2900},
  year={2015}
}

@inproceedings{DBLP:conf/icml/NairH10,
  author={Vinod Nair and Geoffrey E. Hinton},
  title={Rectified Linear Units Improve Restricted Boltzmann Machines},
  year={2010},
  cdate={1262304000000},
  pages={807-814},
  url={https://icml.cc/Conferences/2010/papers/432.pdf},
  booktitle={ICML},
}

@article{JMLR:v15:srivastava14a,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}



@InProceedings{pmlr-v37-ioffe15,
  title = 	 {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author = 	 {Ioffe, Sergey and Szegedy, Christian},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {448--456},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/ioffe15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/ioffe15.html},
  abstract = 	 {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.}
}



@article{ratnarajah2020ir,
  title={IR-GAN: Room impulse response generator for far-field speech recognition},
  author={Ratnarajah, Anton and Tang, Zhenyu and Manocha, Dinesh},
  journal={arXiv preprint arXiv:2010.13219},
  year={2020}
}

@article{ratnarajah2021ts,
  title={TS-RIR: Translated synthetic room impulse responses for speech augmentation},
  author={Ratnarajah, Anton and Tang, Zhenyu and Manocha, Dinesh},
  journal={arXiv preprint arXiv:2103.16804},
  year={2021}
}

@article{farina2000simultaneous,
author={Farina, Angelo},
journal={Journal of the Audio Engineering Society},
title={Simultaneous Measurement of Impulse Response and Distortion with a Swept-Sine Technique},
year={2000},
volume={},
number={},
pages={},
doi={},
month={February},}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@Article{app11031150,
AUTHOR = {Werner, Stephan and Klein, Florian and Neidhardt, Annika and Sloma, Ulrike and Schneiderwind, Christian and Brandenburg, Karlheinz},
TITLE = {Creation of Auditory Augmented Reality Using a Position-Dynamic Binaural Synthesis System—Technical Components, Psychoacoustic Needs, and Perceptual Evaluation},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {1150},
URL = {https://www.mdpi.com/2076-3417/11/3/1150},
ISSN = {2076-3417},
ABSTRACT = {For a spatial audio reproduction in the context of augmented reality, a position-dynamic binaural synthesis system can be used to synthesize the ear signals for a moving listener. The goal is the fusion of the auditory perception of the virtual audio objects with the real listening environment. Such a system has several components, each of which help to enable a plausible auditory simulation. For each possible position of the listener in the room, a set of binaural room impulse responses (BRIRs) congruent with the expected auditory environment is required to avoid room divergence effects. Adequate and efficient approaches are methods to synthesize new BRIRs using very few measurements of the listening room. The required spatial resolution of the BRIR positions can be estimated by spatial auditory perception thresholds. Retrieving and processing the tracking data of the listener&rsquo;s head-pose and position as well as convolving BRIRs with an audio signal needs to be done in real-time. This contribution presents work done by the authors including several technical components of such a system in detail. It shows how the single components are affected by psychoacoustics. Furthermore, the paper also discusses the perceptive effect by means of listening tests demonstrating the appropriateness of the approaches.},
DOI = {10.3390/app11031150}
}

@book{vigran2008building,
  title={Building Acoustics},
  author={Vigran, T.E.},
  isbn={9780203931318},
  lccn={2007039258},
  url={https://books.google.com/books?id=deBMmQEACAAJ},
  year={2008},
  publisher={Taylor \& Francis}
}


@article{rir_from_white_noise,
author = {Lebart, K and Boucher, J.-M and Denbigh, P},
year = {2001},
month = {05},
pages = {359-366},
title = {A new method based on spectral subtraction for speech dereverberation},
volume = {87},
journal = {Acta Acustica united with Acustica}
}

@article{carlo2021dechorate,
  title={dEchorate: a calibrated room impulse response dataset for echo-aware signal processing},
  author={Carlo, Diego Di and Tandeitnik, Pinchas and Foy, Cedri{\'c} and Bertin, Nancy and Deleforge, Antoine and Gannot, Sharon},
  journal={EURASIP Journal on Audio, Speech, and Music Processing},
  volume={2021},
  number={1},
  pages={1--15},
  year={2021},
  publisher={Springer}
}

@inproceedings{piczak2015dataset,
  title = {{ESC}: {Dataset} for {Environmental Sound Classification}},
  year = {2015},
  author = {Piczak, Karol J.},
  booktitle = {Proceedings of the 23rd {Annual ACM Conference} on {Multimedia}},
  date = {2015-10-13},
  url = {http://dl.acm.org/citation.cfm?doid=2733373.2806390},
  doi = {10.1145/2733373.2806390},
  location = {{Brisbane, Australia}},
  isbn = {978-1-4503-3459-4},
  publisher = {{ACM Press}},
  pages = {1015--1018}
}

@INPROCEEDINGS{7178964,  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   title={Librispeech: An ASR corpus based on public domain audio books},   year={2015},  volume={},  number={},  pages={5206-5210},  doi={10.1109/ICASSP.2015.7178964}}


@ARTICLE{8717722,  author={Szöke, Igor and Skácel, Miroslav and Mošner, Ladislav and Paliesek, Jakub and Černocký, Jan},  journal={IEEE Journal of Selected Topics in Signal Processing},   title={Building and evaluation of a real room impulse response dataset},   year={2019},  volume={13},  number={4},  pages={863-876},  doi={10.1109/JSTSP.2019.2917582}}

@inproceedings{62e5b0bcac3243f0b77291509411ed52,
title = "More Than 50 Years of Artificial Reverberation",
author = "Vesa V{\"a}lim{\"a}ki and Parker, {Julian D.} and Lauri Savioja and Smith, {Julius O.} and Abel, {Jonathan S.}",
year = "2016",
language = "English",
editor = "Stefan Goetze and Ann Spriet",
booktitle = "Proc. 60th International Conference of the Audio Engineering Society",
publisher = "Audio Engineering Society",
address = "United States",
note = "AES International Conference on Dereverberation and Reverberation of Audio, Music, and Speech, DREAMS ; Conference date: 03-02-2016 Through 05-02-2016",
}

@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={International Conference on Medical image computing and computer-assisted intervention},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@inproceedings{mosnet,
  author={Lo, Chen-Chou and Fu, Szu-Wei and Huang, Wen-Chin and Wang, Xin and Yamagishi, Junichi and Tsao, Yu and Wang, Hsin-Min},
  title={MOSNet: Deep Learning based Objective Assessment for Voice Conversion},
  year=2019,
  booktitle={Proc. Interspeech 2019},
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{griffin,
  title={Signal estimation from modified short-time Fourier transform},
  author={Griffin, Daniel and Lim, Jae},
  journal={IEEE Transactions on acoustics, speech, and signal processing},
  volume={32},
  number={2},
  pages={236--243},
  year={1984},
  publisher={IEEE}
}