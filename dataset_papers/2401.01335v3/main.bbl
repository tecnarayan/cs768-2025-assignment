\begin{thebibliography}{101}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen, et~al.]{anil2023palm}
Anil, R., Dai, A.~M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et~al.
\newblock Palm 2 technical report.
\newblock \emph{arXiv preprint arXiv:2305.10403}, 2023.

\bibitem[Anthony et~al.(2017)Anthony, Tian, and Barber]{anthony2017thinking}
Anthony, T., Tian, Z., and Barber, D.
\newblock Thinking fast and slow with deep learning and tree search.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and Bottou]{arjovsky2017wasserstein}
Arjovsky, M., Chintala, S., and Bottou, L.
\newblock Wasserstein generative adversarial networks.
\newblock In \emph{International conference on machine learning}, pp.\  214--223. PMLR, 2017.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, et~al.]{austin2021program}
Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et~al.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}, 2021.

\bibitem[Bai et~al.(2022{\natexlab{a}})Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training}
Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback.
\newblock \emph{arXiv preprint arXiv:2204.05862}, 2022{\natexlab{a}}.

\bibitem[Bai et~al.(2022{\natexlab{b}})Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional}
Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et~al.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock \emph{arXiv preprint arXiv:2212.08073}, 2022{\natexlab{b}}.

\bibitem[Bansal et~al.(2018)Bansal, Pachocki, Sidor, Sutskever, and Mordatch]{bansal2018emergent}
Bansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I.
\newblock Emergent complexity via multi-agent competition.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Beeching et~al.(2023)Beeching, Fourrier, Habib, Han, Lambert, Rajani, Sanseviero, Tunstall, and Wolf]{open-llm-leaderboard}
Beeching, E., Fourrier, C., Habib, N., Han, S., Lambert, N., Rajani, N., Sanseviero, O., Tunstall, L., and Wolf, T.
\newblock Open llm leaderboard, 2023.

\bibitem[bench authors(2023)]{srivastava2023beyond}
bench authors, B.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock \emph{Transactions on Machine Learning Research}, 2023.
\newblock ISSN 2835-8856.

\bibitem[Bengio et~al.(2009)Bengio, Louradour, Collobert, and Weston]{bengio2009curriculum}
Bengio, Y., Louradour, J., Collobert, R., and Weston, J.
\newblock Curriculum learning.
\newblock In \emph{Proceedings of the 26th annual international conference on machine learning}, pp.\  41--48, 2009.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks}
Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y.~T., Li, Y., Lundberg, S., et~al.
\newblock Sparks of artificial general intelligence: Early experiments with gpt-4.
\newblock \emph{arXiv preprint arXiv:2303.12712}, 2023.

\bibitem[Burns et~al.(2023)Burns, Izmailov, Kirchner, Baker, Gao, Aschenbrenner, Chen, Ecoffet, Joglekar, Leike, et~al.]{burnsweak}
Burns, C., Izmailov, P., Kirchner, J.~H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J., et~al.
\newblock Weak-to-strong generalization: Eliciting strong capabilities with weak supervision.
\newblock 2023.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, et~al.]{chen2021evaluating}
Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d.~O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Cheng et~al.(2023)Cheng, Yang, Li, Dai, and Du]{cheng2023adversarial}
Cheng, P., Yang, Y., Li, J., Dai, Y., and Du, N.
\newblock Adversarial preference optimization, 2023.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang, Zhuang, Gonzalez, Stoica, and Xing]{vicuna2023}
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J.~E., Stoica, I., and Xing, E.~P.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and Amodei]{christiano2017deep}
Christiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et~al.]{chung2022scaling}
Chung, H.~W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv:2210.11416}, 2022.

\bibitem[Cirik et~al.(2016)Cirik, Hovy, and Morency]{cirik2016visualizing}
Cirik, V., Hovy, E., and Morency, L.-P.
\newblock Visualizing and understanding curriculum learning for long short-term memory networks.
\newblock \emph{arXiv preprint arXiv:1611.06204}, 2016.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think}
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}, 2018.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training}
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Cui et~al.(2023)Cui, Yuan, Ding, Yao, Zhu, Ni, Xie, Liu, and Sun]{cui2023ultrafeedback}
Cui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G., Liu, Z., and Sun, M.
\newblock Ultrafeedback: Boosting language models with high-quality feedback, 2023.

\bibitem[Dao(2023)]{dao2023flashattention}
Dao, T.
\newblock Flashattention-2: Faster attention with better parallelism and work partitioning.
\newblock \emph{arXiv preprint arXiv:2307.08691}, 2023.

\bibitem[Deng et~al.(2023)Deng, Zhang, Chen, and Gu]{deng2023rephrase}
Deng, Y., Zhang, W., Chen, Z., and Gu, Q.
\newblock Rephrase and respond: Let large language models ask better questions for themselves.
\newblock \emph{arXiv preprint arXiv:2311.04205}, 2023.

\bibitem[Ding et~al.(2023)Ding, Chen, Xu, Qin, Zheng, Hu, Liu, Sun, and Zhou]{ding2023enhancing}
Ding, N., Chen, Y., Xu, B., Qin, Y., Zheng, Z., Hu, S., Liu, Z., Sun, M., and Zhou, B.
\newblock Enhancing chat language models by scaling high-quality instructional conversations.
\newblock \emph{arXiv preprint arXiv:2305.14233}, 2023.

\bibitem[Frei et~al.(2022)Frei, Zou, Chen, and Gu]{frei2022self}
Frei, S., Zou, D., Chen, Z., and Gu, Q.
\newblock Self-training converts weak learners to strong learners in mixture models.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  8003--8021. PMLR, 2022.

\bibitem[Freund(1995)]{freund1995boosting}
Freund, Y.
\newblock Boosting a weak learning algorithm by majority.
\newblock \emph{Information and computation}, 121\penalty0 (2):\penalty0 256--285, 1995.

\bibitem[Freund \& Schapire(1997)Freund and Schapire]{freund1997decision}
Freund, Y. and Schapire, R.~E.
\newblock A decision-theoretic generalization of on-line learning and an application to boosting.
\newblock \emph{Journal of computer and system sciences}, 55\penalty0 (1):\penalty0 119--139, 1997.

\bibitem[Gao et~al.(2023{\natexlab{a}})Gao, Schulman, and Hilton]{gao2023scaling}
Gao, L., Schulman, J., and Hilton, J.
\newblock Scaling laws for reward model overoptimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\  10835--10866. PMLR, 2023{\natexlab{a}}.

\bibitem[Gao et~al.(2023{\natexlab{b}})Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster, Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds, Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and Zou]{eval-harness}
Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le~Noac'h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A.
\newblock A framework for few-shot language model evaluation, 12 2023{\natexlab{b}}.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Grandvalet \& Bengio(2004)Grandvalet and Bengio]{grandvalet2004semi}
Grandvalet, Y. and Bengio, Y.
\newblock Semi-supervised learning by entropy minimization.
\newblock \emph{Advances in neural information processing systems}, 17, 2004.

\bibitem[Gugger et~al.(2022)Gugger, Debut, Wolf, Schmid, Mueller, Mangrulkar, Sun, and Bossan]{accelerate}
Gugger, S., Debut, L., Wolf, T., Schmid, P., Mueller, Z., Mangrulkar, S., Sun, M., and Bossan, B.
\newblock Accelerate: Training and inference at scale made simple, efficient and adaptable., 2022.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and Courville]{gulrajani2017improved}
Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A.~C.
\newblock Improved training of wasserstein gans.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring}
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Hernandez-Leal et~al.(2018)Hernandez-Leal, Kartal, and Taylor]{hernandez2018multiagent}
Hernandez-Leal, P., Kartal, B., and Taylor, M.~E.
\newblock Is multiagent deep reinforcement learning the answer or the question? a brief survey.
\newblock \emph{learning}, 21:\penalty0 22, 2018.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, and Swersky]{hinton2012neural}
Hinton, G., Srivastava, N., and Swersky, K.
\newblock Neural networks for machine learning lecture 6a overview of mini-batch gradient descent.
\newblock \emph{Cited on}, 14\penalty0 (8):\penalty0 2, 2012.

\bibitem[Ho \& Ermon(2016)Ho and Ermon]{ho2016generative}
Ho, J. and Ermon, S.
\newblock Generative adversarial imitation learning.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
Jiang, A.~Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.~S., Casas, D. d.~l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Jolicoeur-Martineau(2018)]{jolicoeur2018relativistic}
Jolicoeur-Martineau, A.
\newblock The relativistic discriminator: a key element missing from standard gan.
\newblock \emph{arXiv preprint arXiv:1807.00734}, 2018.

\bibitem[Josifoski et~al.(2023)Josifoski, Sakota, Peyrard, and West]{josifoski2023exploiting}
Josifoski, M., Sakota, M., Peyrard, M., and West, R.
\newblock Exploiting asymmetry for synthetic training data generation: Synthie and the case of information extraction.
\newblock \emph{arXiv preprint arXiv:2303.04132}, 2023.

\bibitem[Kearns \& Valiant(1994)Kearns and Valiant]{kearns1994cryptographic}
Kearns, M. and Valiant, L.
\newblock Cryptographic limitations on learning boolean formulae and finite automata.
\newblock \emph{Journal of the ACM (JACM)}, 41\penalty0 (1):\penalty0 67--95, 1994.

\bibitem[Kou et~al.(2022)Kou, Chen, Cao, and Gu]{kou2022does}
Kou, Y., Chen, Z., Cao, Y., and Gu, Q.
\newblock How does semi-supervised learning with pseudo-labelers work? a case study.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Kumar et~al.(2010)Kumar, Packer, and Koller]{kumar2010self}
Kumar, M., Packer, B., and Koller, D.
\newblock Self-paced learning for latent variable models.
\newblock \emph{Advances in neural information processing systems}, 23, 2010.

\bibitem[Lanctot et~al.(2017)Lanctot, Zambaldi, Gruslys, Lazaridou, Tuyls, P{\'e}rolat, Silver, and Graepel]{lanctot2017unified}
Lanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A., Tuyls, K., P{\'e}rolat, J., Silver, D., and Graepel, T.
\newblock A unified game-theoretic approach to multiagent reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Lee(2013)]{lee2013pseudolabel}
Lee, D.-H.
\newblock Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks.
\newblock In \emph{ICML Challenges in Representation Learning Workshop}, 2013.

\bibitem[Lee et~al.(2023)Lee, Phatale, Mansoor, Lu, Mesnard, Bishop, Carbune, and Rastogi]{lee2023rlaif}
Lee, H., Phatale, S., Mansoor, H., Lu, K., Mesnard, T., Bishop, C., Carbune, V., and Rastogi, A.
\newblock Rlaif: Scaling reinforcement learning from human feedback with ai feedback.
\newblock \emph{arXiv preprint arXiv:2309.00267}, 2023.

\bibitem[Lee \& Grauman(2011)Lee and Grauman]{lee2011learning}
Lee, Y.~J. and Grauman, K.
\newblock Learning the easy things first: Self-paced visual category discovery.
\newblock In \emph{CVPR 2011}, pp.\  1721--1728. IEEE, 2011.

\bibitem[Lewkowycz et~al.(2022)Lewkowycz, Andreassen, Dohan, Dyer, Michalewski, Ramasesh, Slone, Anil, Schlag, Gutman-Solo, et~al.]{lewkowycz2022solving}
Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., et~al.
\newblock Solving quantitative reasoning problems with language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 3843--3857, 2022.

\bibitem[Li et~al.(2022)Li, Choi, Chung, Kushman, Schrittwieser, Leblond, Eccles, Keeling, Gimeno, Dal~Lago, et~al.]{li2022competition}
Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal~Lago, A., et~al.
\newblock Competition-level code generation with alphacode.
\newblock \emph{Science}, 378\penalty0 (6624):\penalty0 1092--1097, 2022.

\bibitem[Li et~al.(2023)Li, Bubeck, Eldan, Giorno, Gunasekar, and Lee]{li2023textbooks}
Li, Y., Bubeck, S., Eldan, R., Giorno, A.~D., Gunasekar, S., and Lee, Y.~T.
\newblock Textbooks are all you need ii: phi-1.5 technical report, 2023.

\bibitem[Lin et~al.(2021)Lin, Hilton, and Evans]{lin2021truthfulqa}
Lin, S., Hilton, J., and Evans, O.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock \emph{arXiv preprint arXiv:2109.07958}, 2021.

\bibitem[Liu et~al.(2023)Liu, Bubeck, Eldan, Kulkarni, Li, Nguyen, Ward, and Zhang]{liu2023tinygsm}
Liu, B., Bubeck, S., Eldan, R., Kulkarni, J., Li, Y., Nguyen, A., Ward, R., and Zhang, Y.
\newblock Tinygsm: achieving> 80\% on gsm8k with small language models.
\newblock \emph{arXiv preprint arXiv:2312.09241}, 2023.

\bibitem[Liu et~al.(2018)Liu, He, Liu, Zhao, et~al.]{liu2018curriculum}
Liu, C., He, S., Liu, K., Zhao, J., et~al.
\newblock Curriculum learning for natural answer generation.
\newblock In \emph{IJCAI}, pp.\  4223--4229, 2018.

\bibitem[Liu et~al.(2021)Liu, Ge, and Wu]{liu2021competence}
Liu, F., Ge, S., and Wu, X.
\newblock Competence-based multimodal curriculum learning for medical report generation.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pp.\  3001--3012, 2021.

\bibitem[Luo et~al.(2023)Luo, Sun, Xu, Zhao, Lou, Tao, Geng, Lin, Chen, and Zhang]{luo2023wizardmath}
Luo, H., Sun, Q., Xu, C., Zhao, P., Lou, J., Tao, C., Geng, X., Lin, Q., Chen, S., and Zhang, D.
\newblock Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.
\newblock \emph{arXiv preprint arXiv:2308.09583}, 2023.

\bibitem[Mao et~al.(2017)Mao, Li, Xie, Lau, Wang, and Paul~Smolley]{mao2017least}
Mao, X., Li, Q., Xie, H., Lau, R.~Y., Wang, Z., and Paul~Smolley, S.
\newblock Least squares generative adversarial networks.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pp.\  2794--2802, 2017.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal]{mihaylov2018can}
Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A.
\newblock Can a suit of armor conduct electricity? a new dataset for open book question answering.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pp.\  2381--2391, 2018.

\bibitem[Mishra et~al.(2021)Mishra, Khashabi, Baral, and Hajishirzi]{mishra2021cross}
Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H.
\newblock Cross-task generalization via natural language crowdsourcing instructions.
\newblock \emph{arXiv preprint arXiv:2104.08773}, 2021.

\bibitem[Mroueh \& Sercu(2017)Mroueh and Sercu]{mroueh2017fisher}
Mroueh, Y. and Sercu, T.
\newblock Fisher gan.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[M{\"u}ller(1997)]{muller1997integral}
M{\"u}ller, A.
\newblock Integral probability metrics and their generating classes of functions.
\newblock \emph{Advances in applied probability}, 29\penalty0 (2):\penalty0 429--443, 1997.

\bibitem[Muller et~al.(2019)Muller, Omidshafiei, Rowland, Tuyls, Perolat, Liu, Hennes, Marris, Lanctot, Hughes, et~al.]{muller2019generalized}
Muller, P., Omidshafiei, S., Rowland, M., Tuyls, K., Perolat, J., Liu, S., Hennes, D., Marris, L., Lanctot, M., Hughes, E., et~al.
\newblock A generalized training approach for multiagent learning.
\newblock \emph{arXiv preprint arXiv:1909.12823}, 2019.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Prasad et~al.(2023)Prasad, Stengel-Eskin, and Bansal]{prasad2023rephrase}
Prasad, A., Stengel-Eskin, E., and Bansal, M.
\newblock Rephrase, augment, reason: Visual grounding of questions for vision-language models.
\newblock \emph{arXiv preprint arXiv:2310.05861}, 2023.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Ermon, Manning, and Finn]{rafailov2023direct}
Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C.~D., and Finn, C.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock \emph{arXiv preprint arXiv:2305.18290}, 2023.

\bibitem[Rajbhandari et~al.(2020)Rajbhandari, Rasley, Ruwase, and He]{rajbhandari2020zero}
Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y.
\newblock Zero: Memory optimizations toward training trillion parameter models.
\newblock In \emph{SC20: International Conference for High Performance Computing, Networking, Storage and Analysis}, pp.\  1--16. IEEE, 2020.

\bibitem[Roziere et~al.(2023)Roziere, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Remez, Rapin, et~al.]{roziere2023code}
Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X.~E., Adi, Y., Liu, J., Remez, T., Rapin, J., et~al.
\newblock Code llama: Open foundation models for code.
\newblock \emph{arXiv preprint arXiv:2308.12950}, 2023.

\bibitem[Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and Choi]{sakaguchi2021winogrande}
Sakaguchi, K., Bras, R.~L., Bhagavatula, C., and Choi, Y.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64\penalty0 (9):\penalty0 99--106, 2021.

\bibitem[Samuel(1959)]{samuel1959some}
Samuel, A.~L.
\newblock Some studies in machine learning using the game of checkers.
\newblock \emph{IBM Journal of research and development}, 3\penalty0 (3):\penalty0 210--229, 1959.

\bibitem[Samuel(2000)]{samuel2000some}
Samuel, A.~L.
\newblock Some studies in machine learning using the game of checkers.
\newblock \emph{IBM Journal of research and development}, 44\penalty0 (1.2):\penalty0 206--226, 2000.

\bibitem[Saunders et~al.(2022)Saunders, Yeh, Wu, Bills, Ouyang, Ward, and Leike]{saunders2022self}
Saunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J., and Leike, J.
\newblock Self-critiquing models for assisting human evaluators.
\newblock \emph{arXiv preprint arXiv:2206.05802}, 2022.

\bibitem[Schapire(1990)]{schapire1990strength}
Schapire, R.~E.
\newblock The strength of weak learnability.
\newblock \emph{Machine learning}, 5:\penalty0 197--227, 1990.

\bibitem[Silver et~al.(2017{\natexlab{a}})Silver, Hubert, Schrittwieser, Antonoglou, Lai, Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2017masteringchess}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al.
\newblock Mastering chess and shogi by self-play with a general reinforcement learning algorithm.
\newblock \emph{arXiv preprint arXiv:1712.01815}, 2017{\natexlab{a}}.

\bibitem[Silver et~al.(2017{\natexlab{b}})Silver, Schrittwieser, Simonyan, Antonoglou, Huang, Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017{\natexlab{b}}.

\bibitem[Singh et~al.(2023)Singh, Co-Reyes, Agarwal, Anand, Patil, Liu, Harrison, Lee, Xu, Parisi, et~al.]{singh2023beyond}
Singh, A., Co-Reyes, J.~D., Agarwal, R., Anand, A., Patil, P., Liu, P.~J., Harrison, J., Lee, J., Xu, K., Parisi, A., et~al.
\newblock Beyond human data: Scaling self-training for problem-solving with language models.
\newblock \emph{arXiv preprint arXiv:2312.06585}, 2023.

\bibitem[Soviany et~al.(2022)Soviany, Ionescu, Rota, and Sebe]{soviany2022curriculum}
Soviany, P., Ionescu, R.~T., Rota, P., and Sebe, N.
\newblock Curriculum learning: A survey.
\newblock \emph{International Journal of Computer Vision}, 130\penalty0 (6):\penalty0 1526--1565, 2022.

\bibitem[Spitkovsky et~al.(2009)Spitkovsky, Alshawi, and Jurafsky]{valentin2009babysteps}
Spitkovsky, V.~I., Alshawi, H., and Jurafsky, D.
\newblock Baby steps: How “less is more” in unsupervised dependency parsing.
\newblock In \emph{NIPS 2009 Workshop on Grammar Induction, Representation of Language and Language Learning}, 2009.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, and Christiano]{stiennon2020learning}
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P.~F.
\newblock Learning to summarize with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 3008--3021, 2020.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto]{alpaca}
Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T.~B.
\newblock Stanford alpaca: An instruction-following llama model, 2023.

\bibitem[Tesauro et~al.(1995)]{tesauro1995temporal}
Tesauro, G. et~al.
\newblock Temporal difference learning and td-gammon.
\newblock \emph{Communications of the ACM}, 38\penalty0 (3):\penalty0 58--68, 1995.

\bibitem[Thoppilan et~al.(2022)Thoppilan, De~Freitas, Hall, Shazeer, Kulshreshtha, Cheng, Jin, Bos, Baker, Du, et~al.]{thoppilan2022lamda}
Thoppilan, R., De~Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et~al.
\newblock Lamda: Language models for dialog applications.
\newblock \emph{arXiv preprint arXiv:2201.08239}, 2022.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Tunstall et~al.(2023{\natexlab{a}})Tunstall, Beeching, Lambert, Rajani, Rasul, Belkada, Huang, von Werra, Fourrier, Habib, et~al.]{tunstall2023zephyr}
Tunstall, L., Beeching, E., Lambert, N., Rajani, N., Rasul, K., Belkada, Y., Huang, S., von Werra, L., Fourrier, C., Habib, N., et~al.
\newblock Zephyr: Direct distillation of lm alignment.
\newblock \emph{arXiv preprint arXiv:2310.16944}, 2023{\natexlab{a}}.

\bibitem[Tunstall et~al.(2023{\natexlab{b}})Tunstall, Beeching, Lambert, Rajani, Rush, and Wolf]{alignment_handbook2023}
Tunstall, L., Beeching, E., Lambert, N., Rajani, N., Rush, A.~M., and Wolf, T.
\newblock The alignment handbook, 2023{\natexlab{b}}.

\bibitem[Vapnik(1999)]{vapnik1999nature}
Vapnik, V.
\newblock \emph{The nature of statistical learning theory}.
\newblock Springer science \& business media, 1999.

\bibitem[Victor et~al.(2022)Victor, Albert, Colin, Stephen, Lintang, Zaid, Antoine, Arnaud, Arun, Manan, et~al.]{victor2022multitask}
Victor, S., Albert, W., Colin, R., Stephen, B., Lintang, S., Zaid, A., Antoine, C., Arnaud, S., Arun, R., Manan, D., et~al.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Chung, Mathieu, Jaderberg, Czarnecki, Dudzik, Huang, Georgiev, Powell, Ewalds, Horgan, Kroiss, Danihelka, Agapiou, Oh, Dalibard, Choi, Sifre, Sulsky, Vezhnevets, Molloy, Cai, Budden, Paine, Gulcehre, Wang, Pfaff, Pohlen, Yogatama, Cohen, McKinney, Smith, Schaul, Lillicrap, Apps, Kavukcuoglu, Hassabis, and Silver]{alphastarblog}
Vinyals, O., Babuschkin, I., Chung, J., Mathieu, M., Jaderberg, M., Czarnecki, W., Dudzik, A., Huang, A., Georgiev, P., Powell, R., Ewalds, T., Horgan, D., Kroiss, M., Danihelka, I., Agapiou, J., Oh, J., Dalibard, V., Choi, D., Sifre, L., Sulsky, Y., Vezhnevets, S., Molloy, J., Cai, T., Budden, D., Paine, T., Gulcehre, C., Wang, Z., Pfaff, T., Pohlen, T., Yogatama, D., Cohen, J., McKinney, K., Smith, O., Schaul, T., Lillicrap, T., Apps, C., Kavukcuoglu, K., Hassabis, D., and Silver, D.
\newblock {AlphaStar: Mastering the Real-Time Strategy Game StarCraft II}, 2019.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.~V., Zhou, D., et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 24824--24837, 2022.

\bibitem[Wu et~al.(2022)Wu, Liang, Akbari, Wang, Yu, et~al.]{wu2022scaling}
Wu, J., Liang, Y., Akbari, H., Wang, Z., Yu, C., et~al.
\newblock Scaling multimodal pre-training via cross-modality gradient harmonization.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 36161--36173, 2022.

\bibitem[Xu et~al.(2023)Xu, Lee, Sukhbaatar, and Weston]{xu2023some}
Xu, J., Lee, A., Sukhbaatar, S., and Weston, J.
\newblock Some things are more cringe than others: Preference optimization with the pairwise cringe loss.
\newblock \emph{arXiv preprint arXiv:2312.16682}, 2023.

\bibitem[Yang et~al.(2023)Yang, Singh, Elhoushi, Mahmoud, Tirumala, Gloeckle, Rozi{\`e}re, Wu, Morcos, and Ardalani]{yang2023decoding}
Yang, Y., Singh, A.~K., Elhoushi, M., Mahmoud, A., Tirumala, K., Gloeckle, F., Rozi{\`e}re, B., Wu, C.-J., Morcos, A.~S., and Ardalani, N.
\newblock Decoding data quality via synthetic corruptions: Embedding-guided pruning of code data.
\newblock \emph{arXiv preprint arXiv:2312.02418}, 2023.

\bibitem[Yu et~al.(2023)Yu, Jiang, Shi, Yu, Liu, Zhang, Kwok, Li, Weller, and Liu]{yu2023metamath}
Yu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok, J.~T., Li, Z., Weller, A., and Liu, W.
\newblock Metamath: Bootstrap your own mathematical questions for large language models.
\newblock \emph{arXiv preprint arXiv:2309.12284}, 2023.

\bibitem[Yuan et~al.(2024)Yuan, Pang, Cho, Sukhbaatar, Xu, and Weston]{yuan2024self}
Yuan, W., Pang, R.~Y., Cho, K., Sukhbaatar, S., Xu, J., and Weston, J.
\newblock Self-rewarding language models.
\newblock \emph{arXiv preprint arXiv:2401.10020}, 2024.

\bibitem[Yuan et~al.(2023)Yuan, Yuan, Li, Dong, Tan, and Zhou]{yuan2023scaling}
Yuan, Z., Yuan, H., Li, C., Dong, G., Tan, C., and Zhou, C.
\newblock Scaling relationship on learning mathematical reasoning with large language models.
\newblock \emph{arXiv preprint arXiv:2308.01825}, 2023.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag}
Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock \emph{arXiv preprint arXiv:1905.07830}, 2019.

\bibitem[Zhang et~al.(2015)Zhang, Meng, Li, Jiang, Zhao, and Han]{zhang2015self}
Zhang, D., Meng, D., Li, C., Jiang, L., Zhao, Q., and Han, J.
\newblock A self-paced multiple-instance learning framework for co-saliency detection.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pp.\  594--602, 2015.

\bibitem[Zhang et~al.(2018)Zhang, Kumar, Khayrallah, Murray, Gwinnup, Martindale, McNamee, Duh, and Carpuat]{zhang2018empirical}
Zhang, X., Kumar, G., Khayrallah, H., Murray, K., Gwinnup, J., Martindale, M.~J., McNamee, P., Duh, K., and Carpuat, M.
\newblock An empirical exploration of curriculum learning for neural machine translation.
\newblock \emph{arXiv preprint arXiv:1811.00739}, 2018.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, et~al.]{zheng2023judging}
Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock \emph{arXiv preprint arXiv:2306.05685}, 2023.

\bibitem[Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving]{ziegler2019fine}
Ziegler, D.~M., Stiennon, N., Wu, J., Brown, T.~B., Radford, A., Amodei, D., Christiano, P., and Irving, G.
\newblock Fine-tuning language models from human preferences.
\newblock \emph{arXiv preprint arXiv:1909.08593}, 2019.

\end{thebibliography}
