\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal \& Jia(2017)Agrawal and Jia]{agrawal2017optimistic}
Agrawal, S. and Jia, R.
\newblock
  \href{https://papers.nips.cc/paper/2017/hash/3621f1454cacf995530ea53652ddf8fb-Abstract.html}{Optimistic
  posterior sampling for reinforcement learning: worst-case regret bounds}.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Aytar et~al.(2018)Aytar, Pfaff, Budden, Paine, Wang, and
  De~Freitas]{aytar2018playing}
Aytar, Y., Pfaff, T., Budden, D., Paine, T., Wang, Z., and De~Freitas, N.
\newblock
  \href{https://papers.nips.cc/paper/2018/hash/35309226eb45ec366ca86a4329a2b7c3-Abstract.html}{Playing
  hard exploration games by watching youtube}.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R.
\newblock \href{https://proceedings.mlr.press/v70/azar17a.html}{Minimax regret
  bounds for reinforcement learning}.
\newblock In \emph{International Conference on Machine Learning}, volume~70,
  pp.\  263--272. PMLR, 2017.

\bibitem[Azizzadenesheli et~al.(2018)Azizzadenesheli, Brunskill, and
  Anandkumar]{8503252}
Azizzadenesheli, K., Brunskill, E., and Anandkumar, A.
\newblock \href{https://ieeexplore.ieee.org/document/8503252}{Efficient
  Exploration Through Bayesian Deep Q-Networks}.
\newblock In \emph{2018 Information Theory and Applications Workshop (ITA)},
  pp.\  1--9, 2018.

\bibitem[Badia et~al.(2019)Badia, Sprechmann, Vitvitskyi, Guo, Piot,
  Kapturowski, Tieleman, Arjovsky, Pritzel, Bolt, et~al.]{badia2019never}
Badia, A.~P., Sprechmann, P., Vitvitskyi, A., Guo, D., Piot, B., Kapturowski,
  S., Tieleman, O., Arjovsky, M., Pritzel, A., Bolt, A., et~al.
\newblock \href{https://openreview.net/forum?id=Sye57xStvB}{Never Give Up:
  Learning Directed Exploration Strategies}.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Badia et~al.(2020)Badia, Piot, Kapturowski, Sprechmann, Vitvitskyi,
  Guo, and Blundell]{badia2020agent57}
Badia, A.~P., Piot, B., Kapturowski, S., Sprechmann, P., Vitvitskyi, A., Guo,
  Z.~D., and Blundell, C.
\newblock \href{https://proceedings.mlr.press/v119/badia20a.html}{Agent57:
  Outperforming the atari human benchmark}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  507--517. PMLR, 2020.

\bibitem[Bahdanau et~al.(2015)Bahdanau, Cho, and
  Bengio]{https://doi.org/10.48550/arxiv.1409.0473}
Bahdanau, D., Cho, K.~H., and Bengio, Y.
\newblock \href{http://arxiv.org/abs/1409.0473}{Neural machine translation by
  jointly learning to align and translate}.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Bai et~al.(2021)Bai, Wang, Han, Hao, Garg, Liu, and
  Wang]{bai2021principled}
Bai, C., Wang, L., Han, L., Hao, J., Garg, A., Liu, P., and Wang, Z.
\newblock \href{https://proceedings.mlr.press/v139/bai21d.html}{Principled
  exploration via optimistic bootstrapping and backward induction}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  577--587. PMLR, 2021.

\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,
  Saxton, and Munos]{bellemare2016unifying}
Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and
  Munos, R.
\newblock
  \href{https://papers.nips.cc/paper/2016/hash/afda332245e2af431fb7b672a68b659d-Abstract.html}{Unifying
  count-based exploration and intrinsic motivation}.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare2013arcade}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M.
\newblock \href{https://jair.org/index.php/jair/article/view/10819}{The arcade
  learning environment: An evaluation platform for general agents}.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, Debiak, Dennison,
  Farhi, Fischer, Hashme, Hesse, et~al.]{berner2019dota}
Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C.,
  Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et~al.
\newblock \href{https://arxiv.org/abs/1912.06680}{Dota 2 with large scale deep
  reinforcement learning}.
\newblock \emph{arXiv preprint}, 2019.

\bibitem[Botev et~al.(2013)Botev, Kroese, Rubinstein, and
  L’Ecuyer]{BOTEV201335}
Botev, Z.~I., Kroese, D.~P., Rubinstein, R.~Y., and L’Ecuyer, P.
\newblock
  \href{https://www.sciencedirect.com/science/article/abs/pii/B9780444538598000035}{Chapter
  3 - The Cross-Entropy Method for Optimization}.
\newblock In \emph{Handbook of Statistics}, volume~31, pp.\  35--59. Elsevier,
  2013.

\bibitem[Camacho \& Bordons(2007)Camacho and Bordons]{24286}
Camacho, E.~F. and Bordons, C.
\newblock
  \emph{\href{https://link.springer.com/book/10.1007/978-0-85729-398-5}{Model
  Predictive control}}.
\newblock Advanced Textbooks in Control and Signal Processing. Springer London,
  2007.

\bibitem[Chen et~al.(2017)Chen, Sidor, Abbeel, and Schulman]{chen2017ucb}
Chen, R.~Y., Sidor, S., Abbeel, P., and Schulman, J.
\newblock \href{https://arxiv.org/abs/1706.01502}{Ucb exploration via
  q-ensembles}.
\newblock \emph{arXiv preprint arXiv:1706.01502}, 2017.

\bibitem[Conserva \& Rauber(2022)Conserva and Rauber]{conserva2022hardness}
Conserva, M. and Rauber, P.
\newblock Hardness in markov decision processes: Theory and practice.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~35, 2022.

\bibitem[Ecoffet et~al.(2021)Ecoffet, Huizinga, Lehman, Stanley, and
  Clune]{ecoffet2021first}
Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K.~O., and Clune, J.
\newblock \href{https://www.nature.com/articles/s41586-020-03157-9}{First
  return, then explore}.
\newblock \emph{Nature}, 590\penalty0 (7847):\penalty0 580--586, 2021.

\bibitem[Engel et~al.(2003)Engel, Mannor, and Meir]{engel2003bayes}
Engel, Y., Mannor, S., and Meir, R.
\newblock \href{https://www.aaai.org/Library/ICML/2003/icml03-023.php}{Bayes
  meets Bellman: The Gaussian process approach to temporal difference
  learning}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  154--161. PMLR, 2003.

\bibitem[Engel et~al.(2005)Engel, Mannor, and Meir]{engel2005reinforcement}
Engel, Y., Mannor, S., and Meir, R.
\newblock
  \href{https://icml.cc/Conferences/2005/proceedings/papers/026_Reinforcement_EngelEtAl.pdf}{Reinforcement
  learning with Gaussian processes}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  201--208. PMLR, 2005.

\bibitem[Eysenbach et~al.(2018)Eysenbach, Gupta, Ibarz, and
  Levine]{eysenbach2018diversity}
Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S.
\newblock \href{https://openreview.net/forum?id=SJx63jRqFm}{Diversity is All
  You Need: Learning Skills without a Reward Function}.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Fan \& Ming(2021)Fan and Ming]{fan2021modelbased}
Fan, Y. and Ming, Y.
\newblock \href{https://proceedings.mlr.press/v139/fan21b.html}{Model-based
  Reinforcement Learning for Continuous Control with Posterior Sampling}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3078--3087. PMLR, 2021.

\bibitem[Flennerhag et~al.(2020)Flennerhag, Wang, Sprechmann, Visin, Galashov,
  Kapturowski, Borsa, Heess, Barreto, and Pascanu]{flennerhag2020temporal}
Flennerhag, S., Wang, J.~X., Sprechmann, P., Visin, F., Galashov, A.,
  Kapturowski, S., Borsa, D.~L., Heess, N., Barreto, A., and Pascanu, R.
\newblock \href{https://arxiv.org/abs/2010.02255}{Temporal difference
  uncertainties as a signal for exploration}.
\newblock \emph{arXiv preprint arXiv:2010.02255}, 2020.

\bibitem[Ha \& Schmidhuber(2018)Ha and Schmidhuber]{Ha2018-jd}
Ha, D. and Schmidhuber, J.
\newblock
  \href{https://papers.nips.cc/paper/2018/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html}{Recurrent
  world models facilitate policy evolution}.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Hafner et~al.(2019{\natexlab{a}})Hafner, Lillicrap, Ba, and
  Norouzi]{hafner2020dream}
Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M.
\newblock \href{https://openreview.net/forum?id=S1lOTC4tDS}{Dream to Control:
  Learning Behaviors by Latent Imagination}.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Hafner et~al.(2019{\natexlab{b}})Hafner, Lillicrap, Fischer, Villegas,
  Ha, Lee, and Davidson]{hafner2019learning}
Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and
  Davidson, J.
\newblock \href{https://proceedings.mlr.press/v97/hafner19a.html}{Learning
  latent dynamics for planning from pixels}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2555--2565. PMLR, 2019{\natexlab{b}}.

\bibitem[Hafner et~al.(2020)Hafner, Lillicrap, Norouzi, and Ba]{Hafner2020-ry}
Hafner, D., Lillicrap, T.~P., Norouzi, M., and Ba, J.
\newblock \href{https://openreview.net/forum?id=0oabwyZbOu}{Mastering Atari
  with Discrete World Models}.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Janner et~al.(2019)Janner, Fu, Zhang, and Levine]{janner2019trust}
Janner, M., Fu, J., Zhang, M., and Levine, S.
\newblock
  \href{https://papers.nips.cc/paper/2019/hash/5faf461eff3099671ad63c6f3f094f7f-Abstract.html}{When
  to trust your model: Model-based policy optimization}.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Janz et~al.(2019)Janz, Hron, Mazur, Hofmann, Hern{\'a}ndez-Lobato, and
  Tschiatschek]{janz2019successor}
Janz, D., Hron, J., Mazur, P., Hofmann, K., Hern{\'a}ndez-Lobato, J.~M., and
  Tschiatschek, S.
\newblock
  \href{https://proceedings.neurips.cc/paper/2019/hash/1b113258af3968aaf3969ca67e744ff8-Abstract.html}{Successor
  uncertainties: exploration and uncertainty in temporal difference learning}.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Kaiser et~al.(2019)Kaiser, Babaeizadeh, Mi{\l}os, Osi{\'n}ski,
  Campbell, Czechowski, Erhan, Finn, Kozakowski, Levine, et~al.]{Kaiser2019-ts}
Kaiser, {\L}., Babaeizadeh, M., Mi{\l}os, P., Osi{\'n}ski, B., Campbell, R.~H.,
  Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., et~al.
\newblock \href{https://openreview.net/forum?id=S1xCPJHtDB}{Model Based
  Reinforcement Learning for Atari}.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock \href{http://arxiv.org/abs/1412.6980}{Adam: A Method for Stochastic
  Optimization}.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Soh, Tucker, and
  Levine]{NEURIPS2019_c2073ffa}
Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S.
\newblock
  \href{https://proceedings.neurips.cc/paper/2019/hash/c2073ffa77b5357a498057413bb09d3a-Abstract.html}{Stabilizing
  Off-Policy Q-Learning via Bootstrapping Error Reduction}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Gupta, and Levine]{kumar2020discor}
Kumar, A., Gupta, A., and Levine, S.
\newblock
  \href{https://proceedings.neurips.cc/paper/2020/hash/d7f426ccbc6db7e235c57958c21c5dfa-Abstract.html}{Discor:
  Corrective feedback in reinforcement learning via distribution correction}.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Masci et~al.(2011)Masci, Meier, Cire{\c{s}}an, and
  Schmidhuber]{masci2011stacked}
Masci, J., Meier, U., Cire{\c{s}}an, D., and Schmidhuber, J.
\newblock
  \href{https://link.springer.com/chapter/10.1007/978-3-642-21735-7_7}{Stacked
  convolutional auto-encoders for hierarchical feature extraction}.
\newblock In \emph{International conference on artificial neural networks},
  pp.\  52--59. Springer, 2011.

\bibitem[M{\'e}nard et~al.(2021)M{\'e}nard, Domingues, Shang, and
  Valko]{menard2021ucb}
M{\'e}nard, P., Domingues, O.~D., Shang, X., and Valko, M.
\newblock \href{http://proceedings.mlr.press/v139/menard21b.html}{UCB Momentum
  Q-learning: Correcting the bias without forgetting}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7609--7618. PMLR, 2021.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{Mnih2015-dx}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock \href{https://www.nature.com/articles/nature14236}{Human-level
  control through deep reinforcement learning}.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Munos \& Szepesv{\'a}ri(2008)Munos and
  Szepesv{\'a}ri]{munos2008finite}
Munos, R. and Szepesv{\'a}ri, C.
\newblock \href{https://www.jmlr.org/papers/v9/munos08a.html}{Finite-Time
  Bounds for Fitted Value Iteration}.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0 (5):\penalty0
  815--857, 2008.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{munos2016safe}
Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M.
\newblock
  \href{https://papers.nips.cc/paper/2016/hash/c3992e9a68c5ae12bd18488bc579b30d-Abstract.html}{Safe
  and efficient off-policy reinforcement learning}.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{osband2013more}
Osband, I., Russo, D., and Van~Roy, B.
\newblock
  \href{https://papers.nips.cc/paper/2013/hash/6a5889bb0190d0211a991f47bb19a777-Abstract.html}{(More)
  efficient reinforcement learning via posterior sampling}.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Osband et~al.(2016{\natexlab{a}})Osband, Blundell, Pritzel, and
  Van~Roy]{osband2016deep}
Osband, I., Blundell, C., Pritzel, A., and Van~Roy, B.
\newblock
  \href{https://papers.nips.cc/paper/2016/hash/8d8818c8e140c64c743113f563cf750f-Abstract.html}{Deep
  exploration via bootstrapped DQN}.
\newblock \emph{Advances in Neural Information Processing Systems}, 29,
  2016{\natexlab{a}}.

\bibitem[Osband et~al.(2016{\natexlab{b}})Osband, Van~Roy, and
  Wen]{osband2016generalization}
Osband, I., Van~Roy, B., and Wen, Z.
\newblock \href{http://proceedings.mlr.press/v48/osband16.html}{Generalization
  and exploration via randomized value functions}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2377--2386. PMLR, 2016{\natexlab{b}}.

\bibitem[Osband et~al.(2018)Osband, Aslanides, and
  Cassirer]{osband2018randomized}
Osband, I., Aslanides, J., and Cassirer, A.
\newblock
  \href{https://papers.nips.cc/paper/2018/hash/5a7b238ba0f6502e5d6be14424b20ded-Abstract.html}{Randomized
  prior functions for deep reinforcement learning}.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Osband et~al.(2019)Osband, Van~Roy, Russo, and Wen]{osband2019deep}
Osband, I., Van~Roy, B., Russo, D.~J., and Wen, Z.
\newblock \href{https://jmlr.org/papers/v20/18-339.html}{Deep Exploration via
  Randomized Value Functions}.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (124):\penalty0 1--62, 2019.

\bibitem[O’Donoghue et~al.(2018)O’Donoghue, Osband, Munos, and
  Mnih]{o2018uncertainty}
O’Donoghue, B., Osband, I., Munos, R., and Mnih, V.
\newblock \href{https://proceedings.mlr.press/v80/odonoghue18a.html}{The
  uncertainty bellman equation and exploration}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3839--3848. PMLR, 2018.

\bibitem[Rashid et~al.(2020)Rashid, Peng, B{\"o}hmer, and
  Whiteson]{rashid2020optimistic}
Rashid, T., Peng, B., B{\"o}hmer, W., and Whiteson, S.
\newblock \href{https://openreview.net/forum?id=r1xGP6VYwH}{Optimistic
  Exploration even with a Pessimistic Initialisation}.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Rasmussen(2005)]{Rasmussen2004}
Rasmussen, C.~E.
\newblock
  \emph{\href{https://direct.mit.edu/books/book/2320/Gaussian-Processes-for-Machine-Learning}{Gaussian
  processes for machine learning}}.
\newblock MIT press, 2005.

\bibitem[Riquelme et~al.(2018)Riquelme, Tucker, and Snoek]{riquelme2018deep}
Riquelme, C., Tucker, G., and Snoek, J.
\newblock \href{https://openreview.net/forum?id=SyYe6k-CW}{Deep Bayesian
  Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for
  Thompson Sampling}.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Russo(2019)]{russo2019worst}
Russo, D.
\newblock
  \href{https://papers.nips.cc/paper/2019/hash/451ae86722d26a608c2e174b2b2773f1-Abstract.html}{Worst-case
  regret bounds for exploration via randomized value functions}.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Savinov et~al.(2018)Savinov, Raichuk, Vincent, Marinier, Pollefeys,
  Lillicrap, and Gelly]{savinov2018episodic}
Savinov, N., Raichuk, A., Vincent, D., Marinier, R., Pollefeys, M., Lillicrap,
  T., and Gelly, S.
\newblock \href{https://openreview.net/forum?id=SkeK3s0qKQ}{Episodic Curiosity
  through Reachability}.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and Silver]{schaul2016}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock \href{http://arxiv.org/abs/1511.05952}{Prioritized Experience
  Replay}.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Schrittwieser et~al.(2020)Schrittwieser, Antonoglou, Hubert, Simonyan,
  Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel,
  et~al.]{Schrittwieser2019-df}
Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L.,
  Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., et~al.
\newblock \href{https://www.nature.com/articles/s41586-020-03051-4}{Mastering
  atari, go, chess and shogi by planning with a learned model}.
\newblock \emph{Nature}, 588\penalty0 (7839):\penalty0 604--609, 2020.

\bibitem[Snoek et~al.(2015)Snoek, Rippel, Swersky, Kiros, Satish, Sundaram,
  Patwary, Prabhat, and Adams]{snoek2015scalable}
Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N.,
  Patwary, M., Prabhat, M., and Adams, R.
\newblock \href{http://proceedings.mlr.press/v37/snoek15.html}{Scalable
  bayesian optimization using deep neural networks}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2171--2180. PMLR, 2015.

\bibitem[Tiapkin et~al.(2022)Tiapkin, Belomestny, Moulines, Naumov, Samsonov,
  Tang, Valko, and Menard]{tiapkin2022dirichlet}
Tiapkin, D., Belomestny, D., Moulines, E., Naumov, A., Samsonov, S., Tang, Y.,
  Valko, M., and Menard, P.
\newblock From dirichlet to rubin: Optimistic exploration in rl without
  bonuses.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2022.

\bibitem[Toromanoff et~al.(2019)Toromanoff, Wirbel, and Moutarde]{recordmean}
Toromanoff, M., Wirbel, E., and Moutarde, F.
\newblock \href{https://arxiv.org/abs/1908.04683}{Is Deep Reinforcement
  Learning Really Superhuman on Atari? Leveling the playing field}.
\newblock \emph{arXiv preprint arXiv:1908.04683}, 2019.

\bibitem[Tziortziotis et~al.(2013)Tziortziotis, Dimitrakakis, and
  Blekas]{tziortziotis2013linear}
Tziortziotis, N., Dimitrakakis, C., and Blekas, K.
\newblock \href{https://www.ijcai.org/Proceedings/13/Papers/255.pdf}{Linear
  Bayesian reinforcement learning}.
\newblock In \emph{International joint conference on artificial intelligence},
  2013.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al.
\newblock \href{https://www.nature.com/articles/s41586-019-1724-z}{Grandmaster
  level in StarCraft II using multi-agent reinforcement learning}.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Zanette \& Brunskill(2019)Zanette and Brunskill]{zanette2019tighter}
Zanette, A. and Brunskill, E.
\newblock \href{https://proceedings.mlr.press/v97/zanette19a.html}{Tighter
  problem-dependent regret bounds in reinforcement learning without domain
  knowledge using value function bounds}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7304--7312. PMLR, 2019.

\end{thebibliography}
