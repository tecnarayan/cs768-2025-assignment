\begin{thebibliography}{88}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alvarez and Salzmann(2017)]{alvarez2017compression}
Jose~M Alvarez and Mathieu Salzmann.
\newblock Compression-aware training of deep networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  856--867, 2017.

\bibitem[Arora et~al.(2018)Arora, Ge, Neyshabur, and Zhang]{arora2018stronger}
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi~Zhang.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock In \emph{International Conference on Machine Learning}, pages
  254--263, 2018.

\bibitem[Baykal et~al.(2019{\natexlab{a}})Baykal, Liebenwein, Gilitschenski,
  Feldman, and Rus]{baykal2018datadependent}
Cenk Baykal, Lucas Liebenwein, Igor Gilitschenski, Dan Feldman, and Daniela
  Rus.
\newblock Data-dependent coresets for compressing neural networks with
  applications to generalization bounds.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=HJfwJ2A5KX}.

\bibitem[Baykal et~al.(2019{\natexlab{b}})Baykal, Liebenwein, Gilitschenski,
  Feldman, and Rus]{sipp2019}
Cenk Baykal, Lucas Liebenwein, Igor Gilitschenski, Dan Feldman, and Daniela
  Rus.
\newblock Sipping neural networks: Sensitivity-informed provable pruning of
  neural networks.
\newblock \emph{arXiv preprint arXiv:1910.05422}, 2019{\natexlab{b}}.

\bibitem[Chen et~al.(2020)Chen, Chen, and Pan]{chen2020storage}
Jianda Chen, Shangyu Chen, and Sinno~Jialin Pan.
\newblock Storage efficient and dynamic flexible runtime channel pruning via
  deep reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Chen et~al.(2017)Chen, Papandreou, Schroff, and
  Adam]{chen2017rethinking}
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam.
\newblock Rethinking atrous convolution for semantic image segmentation.
\newblock \emph{arXiv preprint arXiv:1706.05587}, 2017.

\bibitem[Chen et~al.(2018)Chen, Si, Li, Chelba, and Hsieh]{Chen2018}
Patrick~H. Chen, Si~Si, Yang Li, Ciprian Chelba, and Cho-jui Hsieh.
\newblock {GroupReduce: Block-Wise Low-Rank Approximation for Neural Language
  Model Shrinking}.
\newblock \emph{Advances in Neural Information Processing Systems},
  2018-December:\penalty0 10988--10998, jun 2018.
\newblock URL \url{http://arxiv.org/abs/1806.06950}.

\bibitem[Chen et~al.(2015{\natexlab{a}})Chen, Wilson, Tyree, Weinberger, and
  Chen]{Chen15Hash}
Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen.
\newblock Compressing neural networks with the hashing trick.
\newblock In \emph{International conference on machine learning}, pages
  2285--2294, 2015{\natexlab{a}}.

\bibitem[Chen et~al.(2015{\natexlab{b}})Chen, Wilson, Tyree, Weinberger, and
  Chen]{Chen15Fresh}
Wenlin Chen, James~T. Wilson, Stephen Tyree, Kilian~Q. Weinberger, and Yixin
  Chen.
\newblock Compressing convolutional neural networks.
\newblock \emph{CoRR}, abs/1506.04449, 2015{\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/1506.04449}.

\bibitem[Chin et~al.(2020)Chin, Ding, Zhang, and Marculescu]{chin2020towards}
Ting-Wu Chin, Ruizhou Ding, Cha Zhang, and Diana Marculescu.
\newblock Towards efficient model compression via learned global ranking.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 1518--1528, 2020.

\bibitem[Clarkson and Woodruff(2015)]{clarkson2015input}
Kenneth~L Clarkson and David~P Woodruff.
\newblock Input sparsity and hardness for robust subspace approximation.
\newblock In \emph{2015 IEEE 56th Annual Symposium on Foundations of Computer
  Science}, pages 310--329. IEEE, 2015.

\bibitem[Dempster et~al.(1977)Dempster, Laird, and Rubin]{dempster1977maximum}
Arthur~P Dempster, Nan~M Laird, and Donald~B Rubin.
\newblock Maximum likelihood from incomplete data via the em algorithm.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 39\penalty0 (1):\penalty0 1--22, 1977.

\bibitem[Denil et~al.(2013)Denil, Shakibi, Dinh, Ranzato, and
  de~Freitas]{Denil2013}
Misha Denil, Babak Shakibi, Laurent Dinh, Marc~Aurelio Ranzato, and Nando
  de~Freitas.
\newblock Predicting parameters in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems 26}, pages
  2148--2156, 2013.

\bibitem[Denton et~al.(2014)Denton, Zaremba, Bruna, LeCun, and
  Fergus]{Denton2014}
Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus.
\newblock {Exploiting Linear Structure Within Convolutional Networks for
  Efficient Evaluation}.
\newblock \emph{Advances in Neural Information Processing Systems}, 2\penalty0
  (January):\penalty0 1269--1277, apr 2014.
\newblock URL \url{http://arxiv.org/abs/1404.0736}.

\bibitem[Ding et~al.(2019)Ding, Ding, Guo, Han, and Yan]{ding2019approximated}
Xiaohan Ding, Guiguang Ding, Yuchen Guo, Jungong Han, and Chenggang Yan.
\newblock Approximated oracle filter pruning for destructive cnn width
  optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  1607--1616. PMLR, 2019.

\bibitem[Dong et~al.(2017)Dong, Huang, Yang, and Yan]{dong2017more}
Xuanyi Dong, Junshi Huang, Yi~Yang, and Shuicheng Yan.
\newblock More is less: A more complicated network with less inference
  complexity.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 5840--5848, 2017.

\bibitem[Everingham et~al.(2015)Everingham, Eslami, Van~Gool, Williams, Winn,
  and Zisserman]{everingham2015pascal}
Mark Everingham, SM~Ali Eslami, Luc Van~Gool, Christopher~KI Williams, John
  Winn, and Andrew Zisserman.
\newblock The pascal visual object classes challenge: A retrospective.
\newblock \emph{International journal of computer vision}, 111\penalty0
  (1):\penalty0 98--136, 2015.

\bibitem[Gao et~al.(2018)Gao, Zhao, Dudziak, Mullins, and Xu]{gao2018dynamic}
Xitong Gao, Yiren Zhao, {\L}ukasz Dudziak, Robert Mullins, and Cheng-zhong Xu.
\newblock Dynamic channel pruning: Feature boosting and suppression.
\newblock \emph{arXiv preprint arXiv:1810.05331}, 2018.

\bibitem[Garipov et~al.(2016)Garipov, Podoprikhin, Novikov, and
  Vetrov]{garipov2016ultimate}
Timur Garipov, Dmitry Podoprikhin, Alexander Novikov, and Dmitry Vetrov.
\newblock Ultimate tensorization: compressing convolutional and fc layers
  alike.
\newblock \emph{arXiv preprint arXiv:1611.03214}, 2016.

\bibitem[Golub and Van~Loan(2013)]{golub2013matrix}
Gene~H Golub and Charles~F Van~Loan.
\newblock \emph{Matrix computations}, volume~3.
\newblock JHU press, 2013.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Gusak et~al.(2019)Gusak, Kholiavchenko, Ponomarev, Markeeva,
  Blagoveschensky, Cichocki, and Oseledets]{gusak2019automated}
Julia Gusak, Maksym Kholiavchenko, Evgeny Ponomarev, Larisa Markeeva, Philip
  Blagoveschensky, Andrzej Cichocki, and Ivan Oseledets.
\newblock Automated multi-stage compression of neural networks.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision Workshops}, pages 0--0, 2019.

\bibitem[Han et~al.(2015)Han, Mao, and Dally]{Han15}
Song Han, Huizi Mao, and William~J. Dally.
\newblock Deep compression: Compressing deep neural network with pruning,
  trained quantization and huffman coding.
\newblock \emph{CoRR}, abs/1510.00149, 2015.
\newblock URL \url{http://arxiv.org/abs/1510.00149}.

\bibitem[Hariharan et~al.(2011)Hariharan, Arbel{\'a}ez, Bourdev, Maji, and
  Malik]{hariharan2011semantic}
Bharath Hariharan, Pablo Arbel{\'a}ez, Lubomir Bourdev, Subhransu Maji, and
  Jitendra Malik.
\newblock Semantic contours from inverse detectors.
\newblock In \emph{2011 International Conference on Computer Vision}, pages
  991--998. IEEE, 2011.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[He et~al.(2018)He, Kang, Dong, Fu, and Yang]{he2018soft}
Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi~Yang.
\newblock Soft filter pruning for accelerating deep convolutional neural
  networks.
\newblock In \emph{Proceedings of the 27th International Joint Conference on
  Artificial Intelligence}, pages 2234--2240. AAAI Press, 2018.

\bibitem[He et~al.(2019)He, Liu, Wang, Hu, and Yang]{he2019filter}
Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi~Yang.
\newblock Filter pruning via geometric median for deep convolutional neural
  networks acceleration.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4340--4349, 2019.

\bibitem[He et~al.(2017)He, Zhang, and Sun]{he2017channel}
Yihui He, Xiangyu Zhang, and Jian Sun.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1389--1397, 2017.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hua et~al.(2018)Hua, Zhou, De~Sa, Zhang, and Suh]{hua2018channel}
Weizhe Hua, Yuan Zhou, Christopher De~Sa, Zhiru Zhang, and G~Edward Suh.
\newblock Channel gating neural networks.
\newblock \emph{arXiv preprint arXiv:1805.12549}, 2018.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4700--4708, 2017.

\bibitem[Idelbayev and Carreira-Perpin{\'a}n(2020)]{idelbayev2020low}
Yerlan Idelbayev and Miguel~A Carreira-Perpin{\'a}n.
\newblock Low-rank compression of neural nets: Learning the rank of each layer.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 8049--8059, 2020.

\bibitem[Ioannou et~al.(2016)Ioannou, Robertson, Shotton, Cipolla, and
  Criminisi]{ioannou2016training}
Y~Ioannou, D~Robertson, J~Shotton, R~Cipolla, and A~Criminisi.
\newblock Training cnns with low-rank filters for efficient image
  classification.
\newblock In \emph{4th International Conference on Learning Representations,
  ICLR 2016-Conference Track Proceedings}, 2016.

\bibitem[Ioannou et~al.(2015)Ioannou, Robertson, Shotton, Cipolla, and
  Criminisi]{ioannou2015training}
Yani Ioannou, Duncan Robertson, Jamie Shotton, Roberto Cipolla, and Antonio
  Criminisi.
\newblock Training cnns with low-rank filters for efficient image
  classification.
\newblock \emph{arXiv preprint arXiv:1511.06744}, 2015.

\bibitem[Ioannou et~al.(2017)Ioannou, Robertson, Cipolla, and
  Criminisi]{ioannou2017deep}
Yani Ioannou, Duncan Robertson, Roberto Cipolla, and Antonio Criminisi.
\newblock Deep roots: Improving cnn efficiency with hierarchical filter groups.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1231--1240, 2017.

\bibitem[Jaderberg et~al.(2014)Jaderberg, Vedaldi, and
  Zisserman]{jaderberg2014speeding}
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman.
\newblock Speeding up convolutional neural networks with low rank expansions.
\newblock In \emph{Proceedings of the British Machine Vision Conference. BMVA
  Press}, 2014.

\bibitem[Kim et~al.(2019)Kim, Khan, and Kyung]{kim2019efficient}
Hyeji Kim, Muhammad Umar~Karim Khan, and Chong-Min Kyung.
\newblock Efficient neural network compression.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 12569--12577, 2019.

\bibitem[Kim et~al.(2015{\natexlab{a}})Kim, Park, Yoo, Choi, Yang, and
  Shin]{Kim2015}
Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu~Yang, and Dongjun
  Shin.
\newblock {Compression of Deep Convolutional Neural Networks for Fast and Low
  Power Mobile Applications}.
\newblock \emph{4th International Conference on Learning Representations, ICLR
  2016 - Conference Track Proceedings}, nov 2015{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1511.06530}.

\bibitem[Kim et~al.(2015{\natexlab{b}})Kim, Park, Yoo, Choi, Yang, and
  Shin]{kim2015compression}
Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu~Yang, and Dongjun
  Shin.
\newblock Compression of deep convolutional neural networks for fast and low
  power mobile applications.
\newblock \emph{arXiv preprint arXiv:1511.06530}, 2015{\natexlab{b}}.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{Alex2012}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In F.~Pereira, C.~J.~C. Burges, L.~Bottou, and K.~Q. Weinberger,
  editors, \emph{Advances in Neural Information Processing Systems 25}, pages
  1097--1105. Curran Associates, Inc., 2012.
\newblock URL
  \url{http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}.

\bibitem[Laparra et~al.(2015)Laparra, Malo, and
  Camps-Valls]{laparra2015dimensionality}
Valero Laparra, Jes{\'u}s Malo, and Gustau Camps-Valls.
\newblock Dimensionality reduction via regression in hyperspectral imagery.
\newblock \emph{IEEE Journal of Selected Topics in Signal Processing},
  9\penalty0 (6):\penalty0 1026--1036, 2015.

\bibitem[Lebedev et~al.(2015)Lebedev, Ganin, Rakhuba, Oseledets, and
  Lempitsky]{lebedev2014speeding}
Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan~V. Oseledets, and Victor~S.
  Lempitsky.
\newblock Speeding-up convolutional neural networks using fine-tuned
  cp-decomposition.
\newblock In \emph{ICLR (Poster)}, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.6553}.

\bibitem[Li and Shi(2018)]{li2018constrained}
Chong Li and CJ~Shi.
\newblock Constrained optimization based low-rank approximation of deep neural
  networks.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 732--747, 2018.

\bibitem[Li et~al.(2016)Li, Kadav, Durdanovic, Samet, and Graf]{li2016pruning}
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans~Peter Graf.
\newblock Pruning filters for efficient convnets.
\newblock \emph{arXiv preprint arXiv:1608.08710}, 2016.

\bibitem[Li et~al.(2019{\natexlab{a}})Li, Qi, Wang, Ge, Li, Yue, and
  Sun]{li2019oicsr}
Jiashi Li, Qi~Qi, Jingyu Wang, Ce~Ge, Yujian Li, Zhangzhang Yue, and Haifeng
  Sun.
\newblock Oicsr: Out-in-channel sparsity regularization for compact deep neural
  networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 7046--7055, 2019{\natexlab{a}}.

\bibitem[Li et~al.(2019{\natexlab{b}})Li, Gu, Gool, and
  Timofte]{li2019learning}
Yawei Li, Shuhang Gu, Luc~Van Gool, and Radu Timofte.
\newblock Learning filter basis for convolutional neural network compression.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pages 5623--5632, 2019{\natexlab{b}}.

\bibitem[Liebenwein et~al.(2020)Liebenwein, Baykal, Lang, Feldman, and
  Rus]{liebenwein2020provable}
Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, and Daniela Rus.
\newblock Provable filter pruning for efficient neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=BJxkOlSYDH}.

\bibitem[Liebenwein et~al.(2021{\natexlab{a}})Liebenwein, Baykal, Carter,
  Gifford, and Rus]{liebenwein2021lost}
Lucas Liebenwein, Cenk Baykal, Brandon Carter, David Gifford, and Daniela Rus.
\newblock Lost in pruning: The effects of pruning neural networks beyond test
  accuracy.
\newblock \emph{Proceedings of Machine Learning and Systems}, 3,
  2021{\natexlab{a}}.

\bibitem[Liebenwein et~al.(2021{\natexlab{b}})Liebenwein, Hasani, Amini, and
  Rus]{liebenwein2021sparse}
Lucas Liebenwein, Ramin Hasani, Alexander Amini, and Daniela Rus.
\newblock Sparse flows: Pruning continuous-depth models.
\newblock \emph{arXiv preprint arXiv:2106.12718}, 2021{\natexlab{b}}.

\bibitem[Lin et~al.(2020{\natexlab{a}})Lin, Ji, Wang, Zhang, Zhang, Tian, and
  Shao]{lin2020hrank}
Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong
  Tian, and Ling Shao.
\newblock Hrank: Filter pruning using high-rank feature map.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 1529--1538, 2020{\natexlab{a}}.

\bibitem[Lin et~al.(2020{\natexlab{b}})Lin, Stich, Barba, Dmitriev, and
  Jaggi]{Lin2020Dynamic}
Tao Lin, Sebastian~U. Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi.
\newblock Dynamic model pruning with feedback.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=SJem8lSFwB}.

\bibitem[Liu et~al.(2019{\natexlab{a}})Liu, Mu, Zhang, Guo, Yang, Cheng, and
  Sun]{liu2019metapruning}
Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting Cheng,
  and Jian Sun.
\newblock Metapruning: Meta learning for automatic neural network channel
  pruning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 3296--3305, 2019{\natexlab{a}}.

\bibitem[Liu et~al.(2017)Liu, Li, Shen, Huang, Yan, and Zhang]{liu2017learning}
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui
  Zhang.
\newblock Learning efficient convolutional networks through network slimming.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pages 2736--2744, 2017.

\bibitem[Liu et~al.(2019{\natexlab{b}})Liu, Sun, Zhou, Huang, and
  Darrell]{liu2018rethinking}
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell.
\newblock Rethinking the value of network pruning.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=rJlnB3C5Ym}.

\bibitem[Luo and Wu(2020)]{luo2020autopruner}
Jian-Hao Luo and Jianxin Wu.
\newblock Autopruner: An end-to-end trainable filter pruning method for
  efficient deep model inference.
\newblock \emph{Pattern Recognition}, 107:\penalty0 107461, 2020.

\bibitem[Maalouf et~al.(2021)Maalouf, Lang, Rus, and Feldman]{maalouf2021deep}
Alaa Maalouf, Harry Lang, Daniela Rus, and Dan Feldman.
\newblock Deep learning meets projective clustering.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=EQfpYwF3-b}.

\bibitem[Molchanov et~al.(2016)Molchanov, Tyree, Karras, Aila, and
  Kautz]{molchanov2016pruning}
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock \emph{arXiv preprint arXiv:1611.06440}, 2016.

\bibitem[Molchanov et~al.(2019)Molchanov, Mallya, Tyree, Frosio, and
  Kautz]{molchanov2019importance}
Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz.
\newblock Importance estimation for neural network pruning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 11264--11272, 2019.

\bibitem[Novikov et~al.(2015)Novikov, Podoprikhin, Osokin, and
  Vetrov]{novikov2015tensorizing}
Alexander Novikov, Dmitry Podoprikhin, Anton Osokin, and Dmitry Vetrov.
\newblock Tensorizing neural networks.
\newblock In \emph{Proceedings of the 28th International Conference on Neural
  Information Processing Systems-Volume 1}, pages 442--450, 2015.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock In \emph{NIPS-W}, 2017.

\bibitem[Peng et~al.(2015)Peng, Yi, and Tang]{peng2015robust}
Xi~Peng, Zhang Yi, and Huajin Tang.
\newblock Robust subspace clustering via thresholding ridge regression.
\newblock In \emph{Twenty-Ninth AAAI Conference on Artificial Intelligence},
  2015.

\bibitem[Renda et~al.(2020)Renda, Frankle, and Carbin]{renda2020comparing}
Alex Renda, Jonathan Frankle, and Michael Carbin.
\newblock Comparing fine-tuning and rewinding in neural network pruning.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=S1gSj0NKvB}.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{ILSVRC15}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
  Alexander~C. Berg, and Li~Fei-Fei.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 115\penalty0
  (3):\penalty0 211--252, 2015.
\newblock \doi{10.1007/s11263-015-0816-y}.

\bibitem[Sainath et~al.(2013)Sainath, Kingsbury, Sindhwani, Arisoy, and
  Ramabhadran]{sainath2013low}
Tara~N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana
  Ramabhadran.
\newblock Low-rank matrix factorization for deep neural network training with
  high-dimensional output targets.
\newblock In \emph{2013 IEEE international conference on acoustics, speech and
  signal processing}, pages 6655--6659. IEEE, 2013.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{sandler2018mobilenetv2}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4510--4520, 2018.

\bibitem[Shi et~al.(2009)Shi, Petterson, Dror, Langford, Smola, and
  Vishwanathan]{shi2009hash}
Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alex Smola, and SVN
  Vishwanathan.
\newblock Hash kernels for structured data.
\newblock \emph{Journal of Machine Learning Research}, 10\penalty0
  (Nov):\penalty0 2615--2637, 2009.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Simonyan and Zisserman(2015)]{Simonyan14}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Singh and Alistarh(2020)]{singh2020woodfisher}
Sidak~Pal Singh and Dan Alistarh.
\newblock Woodfisher: Efficient second-order approximations for model
  compression.
\newblock \emph{arXiv preprint arXiv:2004.14340}, 2020.

\bibitem[Tai et~al.(2015)Tai, Xiao, Zhang, Wang, et~al.]{tai2015convolutional}
Cheng Tai, Tong Xiao, Yi~Zhang, Xiaogang Wang, et~al.
\newblock Convolutional neural networks with low-rank regularization.
\newblock \emph{arXiv preprint arXiv:1511.06067}, 2015.

\bibitem[Tiwari et~al.(2021)Tiwari, Bamba, Chavan, and
  Gupta]{tiwari2021chipnet}
Rishabh Tiwari, Udbhav Bamba, Arnav Chavan, and Deepak Gupta.
\newblock Chipnet: Budget-aware pruning with heaviside continuous
  approximations.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=xCxXwTzx4L1}.

\bibitem[Torralba et~al.(2008)Torralba, Fergus, and Freeman]{torralba200880}
Antonio Torralba, Rob Fergus, and William~T Freeman.
\newblock 80 million tiny images: A large data set for nonparametric object and
  scene recognition.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 30\penalty0 (11):\penalty0 1958--1970, 2008.

\bibitem[Tukan et~al.(2020)Tukan, Maalouf, Weksler, and
  Feldman]{tukan2020compressed}
Murad Tukan, Alaa Maalouf, Matan Weksler, and Dan Feldman.
\newblock Compressed deep networks: Goodbye svd, hello robust low-rank
  approximation.
\newblock \emph{arXiv preprint arXiv:2009.05647}, 2020.

\bibitem[Ullrich et~al.(2017)Ullrich, Meeds, and Welling]{ullrich2017soft}
Karen Ullrich, Edward Meeds, and Max Welling.
\newblock Soft weight-sharing for neural network compression.
\newblock \emph{arXiv preprint arXiv:1702.04008}, 2017.

\bibitem[Wang et~al.(2021)Wang, Qin, Zhang, and Fu]{wang2021neural}
Huan Wang, Can Qin, Yulun Zhang, and Yun Fu.
\newblock Neural pruning via growing regularization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=o966_Is_nPA}.

\bibitem[Weinberger et~al.(2009)Weinberger, Dasgupta, Langford, Smola, and
  Attenberg]{Weinberger09}
Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh
  Attenberg.
\newblock Feature hashing for large scale multitask learning.
\newblock In \emph{Proceedings of the 26th annual international conference on
  machine learning}, pages 1113--1120, 2009.

\bibitem[Wen et~al.(2017)Wen, Xu, Wu, Wang, Chen, and Li]{Wen2017}
Wei Wen, Cong Xu, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock {Coordinating Filters for Faster Deep Neural Networks}.
\newblock \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, 2017-Octob:\penalty0 658--666, mar 2017.
\newblock URL \url{http://arxiv.org/abs/1703.09746}.

\bibitem[Wu et~al.(2016)Wu, Leng, Wang, Hu, and Cheng]{Wu2016}
Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng.
\newblock {Quantized Convolutional Neural Networks for Mobile Devices}.
\newblock In \emph{Proceedings of the International Conference on Computer
  Vision and Pattern Recognition (CVPR)}, 2016.

\bibitem[Xu et~al.(2020)Xu, Li, Zhang, Wen, Wang, Qi, Chen, Lin, and
  Xiong]{Xu2020}
Yuhui Xu, Yuxi Li, Shuai Zhang, Wei Wen, Botao Wang, Yingyong Qi, Yiran Chen,
  Weiyao Lin, and Hongkai Xiong.
\newblock {TRP: Trained Rank Pruning for Efficient Deep Neural Networks}.
\newblock \emph{IJCAI International Joint Conference on Artificial
  Intelligence}, 2021-Janua:\penalty0 977--983, apr 2020.
\newblock URL \url{http://arxiv.org/abs/2004.14566}.

\bibitem[Xue et~al.(2013)Xue, Li, and Gong]{xue2013restructuring}
Jian Xue, Jinyu Li, and Yifan Gong.
\newblock Restructuring of deep neural network acoustic models with singular
  value decomposition.
\newblock In \emph{Interspeech}, pages 2365--2369, 2013.

\bibitem[Ye et~al.(2018)Ye, Lu, Lin, and Wang]{ye2018rethinking}
Jianbo Ye, Xin Lu, Zhe Lin, and James~Z. Wang.
\newblock Rethinking the smaller-norm-less-informative assumption in channel
  pruning of convolution layers.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=HJ94fqApW}.

\bibitem[Yu et~al.(2018)Yu, Li, Chen, Lai, Morariu, Han, Gao, Lin, and
  Davis]{yu2018nisp}
Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad~I Morariu, Xintong Han,
  Mingfei Gao, Ching-Yung Lin, and Larry~S Davis.
\newblock Nisp: Pruning networks using neuron importance score propagation.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 9194--9203, 2018.

\bibitem[Yu et~al.(2017)Yu, Liu, Wang, and Tao]{yu2017compressing}
Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao.
\newblock On compressing deep models by low rank and sparse decomposition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 7370--7379, 2017.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zhang et~al.(2015{\natexlab{a}})Zhang, Zou, He, and Sun]{Zhang2015a}
Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun.
\newblock {Accelerating Very Deep Convolutional Networks for Classification and
  Detection}.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 38\penalty0 (10):\penalty0 1943--1955, may 2015{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1505.06798}.

\bibitem[Zhang et~al.(2015{\natexlab{b}})Zhang, Zou, He, and
  Sun]{zhang2015accelerating}
Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun.
\newblock Accelerating very deep convolutional networks for classification and
  detection.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 38\penalty0 (10):\penalty0 1943--1955, 2015{\natexlab{b}}.

\bibitem[Zhang et~al.(2015{\natexlab{c}})Zhang, Zou, Ming, He, and
  Sun]{zhang2015efficient}
Xiangyu Zhang, Jianhua Zou, Xiang Ming, Kaiming He, and Jian Sun.
\newblock Efficient and accurate approximations of nonlinear convolutional
  networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  pattern Recognition}, pages 1984--1992, 2015{\natexlab{c}}.

\bibitem[Zhuang et~al.(2018)Zhuang, Tan, Zhuang, Liu, Guo, Wu, Huang, and
  Zhu]{zhuang2018discrimination}
Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu,
  Junzhou Huang, and Jinhui Zhu.
\newblock Discrimination-aware channel pruning for deep neural networks.
\newblock \emph{arXiv preprint arXiv:1810.11809}, 2018.

\end{thebibliography}
