\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, Ring, Rutherford, Cabi, Han, Gong, Samangooei, Monteiro, Menick, Borgeaud, Brock, Nematzadeh, Sharifzadeh, Binkowski, Barreira, Vinyals, Zisserman, and Simonyan]{flamingo}
Alayrac, J., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J.~L., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., and Simonyan, K.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Barak et~al.(2022)Barak, Edelman, Goel, Kakade, Malach, and Zhang]{barak2022hidden}
Barak, B., Edelman, B.~L., Goel, S., Kakade, S., Malach, E., and Zhang, C.
\newblock Hidden progress in deep learning: Sgd learns parities near the computational limit.
\newblock \emph{arXiv preprint arXiv:2207.08799}, 2022.

\bibitem[Bender et~al.(2021)Bender, Gebru, McMillan-Major, and Shmitchell]{bender2021dangers}
Bender, E.~M., Gebru, T., McMillan-Major, A., and Shmitchell, S.
\newblock On the dangers of stochastic parrots: Can language models be too big?
\newblock In \emph{Proceedings of the 2021 ACM conference on fairness, accountability, and transparency}, pp.\  610--623, 2021.

\bibitem[Bolukbasi et~al.(2021)Bolukbasi, Pearce, Yuan, Coenen, Reif, Vi{\'{e}}gas, and Wattenberg]{rigorous}
Bolukbasi, T., Pearce, A., Yuan, A., Coenen, A., Reif, E., Vi{\'{e}}gas, F.~B., and Wattenberg, M.
\newblock An interpretability illusion for {BERT}.
\newblock \emph{CoRR}, abs/2104.07143, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{few-shot}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert{-}Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur{-}Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei, Meier{-}Hellstern, Eck, Dean, Petrov, and Fiedel]{palm}
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.~W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur{-}Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A.~M., Pillai, T.~S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier{-}Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{CoRR}, abs/2204.02311, 2022.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{gsm8k}
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{bert}
Devlin, J., Chang, M., Lee, K., and Toutanova, K.
\newblock {BERT:} pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{NAACL-HLT}, pp.\  4171--4186. Association for Computational Linguistics, 2019.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann, Askell, Bai, Chen, Conerly, DasSarma, Drain, Ganguli, Hatfield-Dodds, Hernandez, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan, McCandlish, and Olah]{elhage2021mathematical}
Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma, N., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 2021.
\newblock https://transformer-circuits.pub/2021/framework/index.html.

\bibitem[Finlayson et~al.(2021)Finlayson, Mueller, Gehrmann, Shieber, Linzen, and Belinkov]{CMA-subj}
Finlayson, M., Mueller, A., Gehrmann, S., Shieber, S.~M., Linzen, T., and Belinkov, Y.
\newblock Causal analysis of syntactic agreement mechanisms in neural language models.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021}, pp.\  1828--1843, 2021.

\bibitem[Frieder et~al.(2023)Frieder, Pinchetti, Griffiths, Salvatori, Lukasiewicz, Petersen, Chevalier, and Berner]{chatgpt-math}
Frieder, S., Pinchetti, L., Griffiths, R., Salvatori, T., Lukasiewicz, T., Petersen, P.~C., Chevalier, A., and Berner, J.
\newblock Mathematical capabilities of chatgpt.
\newblock \emph{CoRR}, abs/2301.13867, 2023.

\bibitem[Geiger et~al.(2021)Geiger, Lu, Icard, and Potts]{CMA-natural}
Geiger, A., Lu, H., Icard, T., and Potts, C.
\newblock Causal abstractions of neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\  9574--9586, 2021.

\bibitem[Geva et~al.(2022)Geva, Caciularu, Wang, and Goldberg]{trans_sub_space}
Geva, M., Caciularu, A., Wang, K.~R., and Goldberg, Y.
\newblock Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space.
\newblock In \emph{EMNLP}, pp.\  30--45, 2022.

\bibitem[Geva et~al.(2023)Geva, Bastings, Filippova, and Globerson]{CMA-factual}
Geva, M., Bastings, J., Filippova, K., and Globerson, A.
\newblock Dissecting recall of factual associations in auto-regressive language models.
\newblock \emph{CoRR}, abs/2304.14767, 2023.

\bibitem[Goldowsky{-}Dill et~al.(2023)Goldowsky{-}Dill, MacLeod, Sato, and Arora]{localizing}
Goldowsky{-}Dill, N., MacLeod, C., Sato, L., and Arora, A.
\newblock Localizing model behavior with path patching.
\newblock \emph{CoRR}, abs/2304.05969, 2023.

\bibitem[Hanna et~al.(2023)Hanna, Liu, and Variengien]{greater-than}
Hanna, M., Liu, O., and Variengien, A.
\newblock How does {GPT-2} compute greater-than?: Interpreting mathematical abilities in a pre-trained language model.
\newblock \emph{CoRR}, abs/2305.00586, 2023.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{MMLU}
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J.
\newblock Measuring massive multitask language understanding.
\newblock \emph{ArXiv}, abs/2009.03300, 2020.

\bibitem[Hernandez et~al.(2021)Hernandez, Schwettmann, Bau, Bagashvili, Torralba, and Andreas]{hernandez2021natural}
Hernandez, E., Schwettmann, S., Bau, D., Bagashvili, T., Torralba, A., and Andreas, J.
\newblock Natural language descriptions of deep visual features.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Hosseini et~al.(2014)Hosseini, Hajishirzi, Etzioni, and Kushman]{AddSub}
Hosseini, M.~J., Hajishirzi, H., Etzioni, O., and Kushman, N.
\newblock Learning to solve arithmetic word problems with verb categorization.
\newblock In \emph{EMNLP}, pp.\  523--533. {ACL}, 2014.

\bibitem[Huang et~al.(2016)Huang, Shi, Lin, Yin, and Ma]{math-interest-4}
Huang, D., Shi, S., Lin, C., Yin, J., and Ma, W.
\newblock How well do computers solve math word problems? large-scale dataset construction and evaluation.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, {ACL} 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers}, 2016.

\bibitem[Huang et~al.(2022)Huang, Gu, Hou, Wu, Wang, Yu, and Han]{huang2022large}
Huang, J., Gu, S.~S., Hou, L., Wu, Y., Wang, X., Yu, H., and Han, J.
\newblock Large language models can self-improve.
\newblock \emph{arXiv preprint arXiv:2210.11610}, 2022.

\bibitem[Imani et~al.(2023)Imani, Du, and Shrivastava]{MathPrompter}
Imani, S., Du, L., and Shrivastava, H.
\newblock Mathprompter: Mathematical reasoning using large language models.
\newblock In \emph{ACL}, pp.\  37--42, 2023.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, et~al.]{jiang2023mistral}
Jiang, A.~Q., Sablayrolles, A., Mensch, A., et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Jie et~al.(2022)Jie, Li, and Lu]{math2}
Jie, Z., Li, J., and Lu, W.
\newblock Learning to reason deductively: Math word problem solving as complex relation extraction.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland, May 22-27, 2022}, pp.\  5944--5955, 2022.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and Iwasawa]{cot-2}
Kojima, T., Gu, S.~S., Reid, M., Matsuo, Y., and Iwasawa, Y.
\newblock Large language models are zero-shot reasoners.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Koncel{-}Kedziorski et~al.(2015)Koncel{-}Kedziorski, Hajishirzi, Sabharwal, Etzioni, and Ang]{SingleEQ}
Koncel{-}Kedziorski, R., Hajishirzi, H., Sabharwal, A., Etzioni, O., and Ang, S.~D.
\newblock Parsing algebraic word problems into equations.
\newblock \emph{Trans. Assoc. Comput. Linguistics}, 3:\penalty0 585--597, 2015.

\bibitem[Kushman et~al.(2014)Kushman, Zettlemoyer, Barzilay, and Artzi]{math-interest-3}
Kushman, N., Zettlemoyer, L., Barzilay, R., and Artzi, Y.
\newblock Learning to automatically solve algebra word problems.
\newblock In \emph{Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, {ACL} 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 1: Long Papers}, pp.\  271--281, 2014.

\bibitem[Li et~al.(2023)Li, Lin, Zhang, Fu, Chen, Lou, and Chen]{li-etal-2023-making}
Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., and Chen, W.
\newblock Making language models better reasoners with step-aware verifier.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  5315--5333. Association for Computational Linguistics, July 2023.

\bibitem[Lieberum et~al.(2023)Lieberum, Rahtz, Kram{\'{a}}r, Nanda, Irving, Shah, and Mikulik]{pp-choice}
Lieberum, T., Rahtz, M., Kram{\'{a}}r, J., Nanda, N., Irving, G., Shah, R., and Mikulik, V.
\newblock Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla.
\newblock \emph{CoRR}, abs/2307.09458, 2023.

\bibitem[Madsen et~al.(2022)Madsen, Reddy, and Chandar]{madsen2022post}
Madsen, A., Reddy, S., and Chandar, S.
\newblock Post-hoc interpretability for neural nlp: A survey.
\newblock \emph{ACM Computing Surveys}, 55\penalty0 (8):\penalty0 1--42, 2022.

\bibitem[Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov]{CMA-locating}
Meng, K., Bau, D., Andonian, A., and Belinkov, Y.
\newblock Locating and editing factual associations in {GPT}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Mu \& Andreas(2020)Mu and Andreas]{mu2020compositionalExplanations}
Mu, J. and Andreas, J.
\newblock Compositional explanations of neurons.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  17153--17163, 2020.

\bibitem[Nanda \& Lieberum(2022)Nanda and Lieberum]{meca_interp_grokking}
Nanda, N. and Lieberum, T.
\newblock A mechanistic interpretability analysis of grokking, 2022.

\bibitem[Ni et~al.(2023)Ni, Inala, Wang, Polozov, Meek, Radev, and Gao]{ni2023learning}
Ni, A., Inala, J.~P., Wang, C., Polozov, A., Meek, C., Radev, D., and Gao, J.
\newblock Learning math reasoning from self-sampled correct and partially-correct solutions.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Nogueira et~al.(2021)Nogueira, Jiang, and Lin]{math-behavior-1}
Nogueira, R.~F., Jiang, Z., and Lin, J.
\newblock Investigating the limitations of the transformers with simple arithmetic tasks.
\newblock \emph{CoRR}, abs/2102.13019, 2021.

\bibitem[Obermeyer et~al.(2019)Obermeyer, Powers, Vogeli, and Mullainathan]{obermeyer2019dissecting}
Obermeyer, Z., Powers, B., Vogeli, C., and Mullainathan, S.
\newblock Dissecting racial bias in an algorithm used to manage the health of populations.
\newblock \emph{Science}, 366\penalty0 (6464):\penalty0 447--453, 2019.

\bibitem[Olah et~al.(2023)Olah, Cammarata, Schubert, Goh, Petrov, and Carter]{circuit}
Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., and Carter, S.
\newblock Zoom in: An introduction to circuits.
\newblock In \emph{Distill}, 2023.

\bibitem[Opedal et~al.(2024)Opedal, Stolfo, Shirakami, Jiao, Cotterell, Sch{\"{o}}lkopf, Saparov, and Sachan]{human-cog}
Opedal, A., Stolfo, A., Shirakami, H., Jiao, Y., Cotterell, R., Sch{\"{o}}lkopf, B., Saparov, A., and Sachan, M.
\newblock Do language models exhibit the same cognitive biases in problem solving as human learners?
\newblock \emph{CoRR}, abs/2401.18070, 2024.

\bibitem[Panigrahi et~al.(2023)Panigrahi, Saunshi, Zhao, and Arora]{graft}
Panigrahi, A., Saunshi, N., Zhao, H., and Arora, S.
\newblock Task-specific skill localization in fine-tuned language models.
\newblock In \emph{ICML}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  27011--27033, 2023.

\bibitem[Patel et~al.(2021)Patel, Bhattamishra, and Goyal]{SVAMP}
Patel, A., Bhattamishra, S., and Goyal, N.
\newblock Are {NLP} models really able to solve simple math word problems?
\newblock In \emph{NAACL-HLT}, pp.\  2080--2094. Association for Computational Linguistics, 2021.

\bibitem[Pearl(2001)]{CMA-theory}
Pearl, J.
\newblock Direct and indirect effects.
\newblock In \emph{{UAI} '01: Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence, University of Washington, Seattle, Washington, USA, August 2-5, 2001}, pp.\  411--420, 2001.

\bibitem[Pearl(2009)]{pearl2009causality}
Pearl, J.
\newblock \emph{Causality}.
\newblock Cambridge university press, 2009.

\bibitem[Qian et~al.(2023)Qian, Wang, Li, Li, and Yan]{math-behavior-2}
Qian, J., Wang, H., Li, Z., Li, S., and Yan, X.
\newblock Limitations of language models in arithmetic and symbolic induction.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2023, Toronto, Canada, July 9-14, 2023}, pp.\  9285--9298. Association for Computational Linguistics, 2023.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{gpt2}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rauker et~al.(2023)Rauker, Ho, Casper, and Hadfield-Menell]{rauker2023toward}
Rauker, T., Ho, A., Casper, S., and Hadfield-Menell, D.
\newblock Toward transparent ai: A survey on interpreting the inner structures of deep neural networks.
\newblock In \emph{2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)}, pp.\  464--483, 2023.

\bibitem[Romera{-}Paredes et~al.(2024)Romera{-}Paredes, Barekatain, Novikov, Balog, Kumar, Dupont, Ruiz, Ellenberg, Wang, Fawzi, Kohli, and Fawzi]{MathProgram}
Romera{-}Paredes, B., Barekatain, M., Novikov, A., Balog, M., Kumar, M.~P., Dupont, E., Ruiz, F. J.~R., Ellenberg, J.~S., Wang, P., Fawzi, O., Kohli, P., and Fawzi, A.
\newblock Mathematical discoveries from program search with large language models.
\newblock \emph{Nat.}, 625\penalty0 (7995):\penalty0 468--475, 2024.

\bibitem[Rudin(2019)]{rudin2019stop}
Rudin, C.
\newblock Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead.
\newblock \emph{Nature machine intelligence}, 1\penalty0 (5):\penalty0 206--215, 2019.

\bibitem[Saha et~al.(2018)Saha, Pahuja, Khapra, Sankaranarayanan, and Chandar]{CSQA}
Saha, A., Pahuja, V., Khapra, M.~M., Sankaranarayanan, K., and Chandar, S.
\newblock Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph.
\newblock In \emph{AAAI}, pp.\  705--713. {AAAI} Press, 2018.

\bibitem[Saxton et~al.(2019)Saxton, Grefenstette, Hill, and Kohli]{math-problem-2}
Saxton, D., Grefenstette, E., Hill, F., and Kohli, P.
\newblock Analysing mathematical reasoning abilities of neural models.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Stolfo et~al.(2023)Stolfo, Belinkov, and Sachan]{arithmetic}
Stolfo, A., Belinkov, Y., and Sachan, M.
\newblock Understanding arithmetic reasoning in language models using causal mediation analysis.
\newblock \emph{CoRR}, abs/2305.15054, 2023.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan, Goodfellow, and Fergus]{szegedy2014intriguing}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R.
\newblock Intriguing properties of neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto.]{Alpaca-model}
Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto., T.~B.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock 2023.

\bibitem[Thawani et~al.(2021)Thawani, Pujara, Ilievski, and Szekely]{ThawaniPIS21}
Thawani, A., Pujara, J., Ilievski, F., and Szekely, P.~A.
\newblock Representing numbers in {NLP:} a survey and a vision.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL-HLT} 2021, Online, June 6-11, 2021}, pp.\  644--656, 2021.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{llama2}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Uesato et~al.(2022)Uesato, Kushman, Kumar, Song, Siegel, Wang, Creswell, Irving, and Higgins]{uesato2022solving}
Uesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and Higgins, I.
\newblock Solving math word problems with process- and outcome-based feedback.
\newblock \emph{arXiv preprint arXiv:2211.14275}, 2022.

\bibitem[Vig et~al.(2020)Vig, Gehrmann, Belinkov, Qian, Nevo, Singer, and Shieber]{vig2020investigating}
Vig, J., Gehrmann, S., Belinkov, Y., Qian, S., Nevo, D., Singer, Y., and Shieber, S.
\newblock Investigating gender bias in language models using causal mediation analysis.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  12388--12401, 2020.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Variengien, Conmy, Shlegeris, and Steinhardt]{inter-IOI}
Wang, K.~R., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J.
\newblock Interpretability in the wild: a circuit for indirect object identification in {GPT-2} small.
\newblock In \emph{International Conference on Learning Representations}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou]{wang2023selfconsistency}
Wang, X., Wei, J., Schuurmans, D., Le, Q.~V., Chi, E.~H., Narang, S., Chowdhery, A., and Zhou, D.
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2017)Wang, Liu, and Shi]{WangLS17}
Wang, Y., Liu, X., and Shi, S.
\newblock Deep neural solver for math word problems.
\newblock In \emph{Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2017, Copenhagen, Denmark, September 9-11, 2017}, pp.\  845--854, 2017.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler, Chi, Hashimoto, Vinyals, Liang, Dean, and Fedus]{Wei2022EmergentAO}
Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., and Fedus, W.
\newblock Emergent abilities of large language models.
\newblock \emph{ArXiv}, abs/2206.07682, 2022{\natexlab{a}}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler, Chi, Hashimoto, Vinyals, Liang, Dean, and Fedus]{emergent}
Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E.~H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., and Fedus, W.
\newblock Emergent abilities of large language models.
\newblock \emph{Trans. Mach. Learn. Res.}, 2022, 2022{\natexlab{b}}.

\bibitem[Wei et~al.(2022{\natexlab{c}})Wei, Wang, Schuurmans, Bosma, Ichter, Xia, Chi, Le, and Zhou]{cot}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E.~H., Le, Q.~V., and Zhou, D.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022{\natexlab{c}}.

\bibitem[Wiegreffe \& Pinter(2019)Wiegreffe and Pinter]{rigorous-2}
Wiegreffe, S. and Pinter, Y.
\newblock Attention is not not explanation.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, {EMNLP-IJCNLP} 2019, Hong Kong, China, November 3-7, 2019}, pp.\  11--20. Association for Computational Linguistics, 2019.

\bibitem[Wu et~al.(2023)Wu, Geiger, Potts, and Goodman]{Alpaca-inter}
Wu, Z., Geiger, A., Potts, C., and Goodman, N.~D.
\newblock Interpretability at scale: Identifying causal mechanisms in alpaca.
\newblock \emph{CoRR}, abs/2305.08809, 2023.

\bibitem[Yu et~al.(2023)Yu, Bowen, Yu, Huang, and Li]{Yu2023LanguageMA}
Yu, L., Bowen, Y., Yu, H., Huang, F., and Li, Y.
\newblock Language models are super mario: Absorbing abilities from homologous models as a free lunch.
\newblock \emph{ArXiv}, abs/2311.03099, 2023.

\bibitem[Zelikman et~al.(2022)Zelikman, Wu, Mu, and Goodman]{zelikman2022star}
Zelikman, E., Wu, Y., Mu, J., and Goodman, N.
\newblock {ST}ar: Bootstrapping reasoning with reasoning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\end{thebibliography}
