@inproceedings{rane2024can,
  title={Can Generative Multimodal Models Count to Ten?},
  author={Rane, Sunayana and Ku, Alexander and Baldridge, Jason Michael and Tenney, Ian and Griffiths, Thomas L and Kim, Been},
  booktitle={ICLR 2024 Workshop on Reliable and Responsible Foundation Models}
}

@book{gelman1986child,
  title={The child’s understanding of number},
  author={Gelman, Rochel and Gallistel, Charles R},
  year={1986},
  publisher={Harvard University Press}
}

@article{betker2023improving,
  title={Improving image generation with better captions},
  author={Betker, James and Goh, Gabriel and Jing, Li and Brooks, Tim and Wang, Jianfeng and Li, Linjie and Ouyang, Long and Zhuang, Juntang and Lee, Joyce and Guo, Yufei and others},
  journal={Computer Science. https://cdn. openai. com/papers/dall-e-3.pdf},
  volume={2},
  number={3},
  pages={8},
  year={2023}
}

@article{tuo2023anytext,
  title={AnyText: Multilingual Visual Text Generation And Editing},
  author={Tuo, Yuxiang and Xiang, Wangmeng and He, Jun-Yan and Geng, Yifeng and Xie, Xuansong},
  journal={arXiv preprint arXiv:2311.03054},
  year={2023}
}
@article{ditz2016numerosity,
  title={Numerosity representations in crows obey the Weber--Fechner law},
  author={Ditz, Helen M and Nieder, Andreas},
  journal={Proceedings of the Royal Society B: Biological Sciences},
  volume={283},
  number={1827},
  pages={20160083},
  year={2016},
  publisher={The Royal Society}
}

@article{emmerton1997pigeons,
  title={Pigeons’ serial ordering of numerosity with visual arrays},
  author={Emmerton, Jacky and Lohmann, Annette and Niemann, Joachim},
  journal={Animal Learning \& Behavior},
  volume={25},
  number={2},
  pages={234--244},
  year={1997},
  publisher={Springer}
}

@article{singh2024tokenization,
  title={Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs},
  author={Singh, Aaditya K and Strouse, DJ},
  journal={arXiv preprint arXiv:2402.14903},
  year={2024}
}

@article{cho2024visual,
  title={Visual programming for step-by-step text-to-image generation and evaluation},
  author={Cho, Jaemin and Zala, Abhay and Bansal, Mohit},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  year={2023}
}
@article{huang2024t2i,
  title={T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation},
  author={Huang, Kaiyi and Sun, Kaiyue and Xie, Enze and Li, Zhenguo and Liu, Xihui},
  journal=NIPS,
  volume={36},
  year={2024}
}

@inproceedings{cho2023davidsonian,
  title={Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation},
  author={Cho, Jaemin and Hu, Yushi and Baldridge, Jason Michael and Garg, Roopal and Anderson, Peter and Krishna, Ranjay and Bansal, Mohit and Pont-Tuset, Jordi and Wang, Su},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}


@misc{paligemma,
  author={
Beyer*, Lucas and
Steiner*, Andreas and
Susano Pinto*, André and
Kolesnikov*, Alexander and
Wang*, Xiao and
Salz, Daniel and
Neumann, Maxim and
Alabdulmohsin, Ibrahim and
Tschannen, Michael and
Bugliarello, Emanuele and
Unterthiner, Thomas and
Keysers, Daniel and
Gritsenko, Alexey and
Chen, Xi and
Koppula, Skanda and
Grycner, Adam and
Bauer, Matthias and
Bošnjak, Matko and
Liu, Fangyu and
Houlsby, Neil and
Kumar, Manoj and
Rong, Keran and
Eisenschlos, Julian and
Minderer, Matthias and
Voigtlaender, Paul and
Bica, Ioana and
Balazevic, Ivana and
Puigcerver, Joan and
Papalampidi, Pinelopi and
Henaff, Olivier and
Xiong, Xi and
Soricut, Radu and
Harmsen, Jeremiah and
Zhai*, Xiaohua
},
  title = {{PaliGemma: A versatile 3B VLM for transfer}},
  year = {2024},
  note={To appear.}
}

@InProceedings{pmlr-v202-chang23b,
  title = 	 {Muse: Text-To-Image Generation via Masked Generative Transformers},
  author =       {Chang, Huiwen and Zhang, Han and Barber, Jarred and Maschinot, Aaron and Lezama, Jose and Jiang, Lu and Yang, Ming-Hsuan and Murphy, Kevin Patrick and Freeman, William T. and Rubinstein, Michael and Li, Yuanzhen and Krishnan, Dilip},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {4055--4075},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/chang23b/chang23b.pdf},
  url = 	 {https://proceedings.mlr.press/v202/chang23b.html},
  abstract = 	 {We present Muse, a text-to-image Transformermodel that achieves state-of-the-art image genera-tion performance while being significantly moreefficient than diffusion or autoregressive models.Muse is trained on a masked modeling task indiscrete token space: given the text embeddingextracted from a pre-trained large language model(LLM), Muse learns to predict randomly maskedimage tokens. Compared to pixel-space diffusionmodels, such as Imagen and DALL-E 2, Muse issignificantly more efficient due to the use of dis-crete tokens and requires fewer sampling itera-tions; compared to autoregressive models such asParti, Muse is more efficient due to the use of par-allel decoding. The use of a pre-trained LLM en-ables fine-grained language understanding, whichtranslates to high-fidelity image generation andthe understanding of visual concepts such as ob-jects, their spatial relationships, pose, cardinalityetc. Our 900M parameter model achieves a newSOTA on CC3M, with an FID score of 6.06. TheMuse 3B parameter model achieves an FID of7.88 on zero-shot COCO evaluation, along with aCLIP score of 0.32. Muse also directly enables anumber of image editing applications without theneed to fine-tune or invert the model: inpainting,outpainting, and mask-free editing. More resultsand videos demonstrating editing are available at https://muse-icml.github.io/}
}


@inproceedings{zhou2023solving,
  title={Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification},
  author={Zhou, Aojun and Wang, Ke and Lu, Zimu and Shi, Weikang and Luo, Sichun and Qin, Zipeng and Lu, Shaoqing and Jia, Anya and Song, Linqi and Zhan, Mingjie and others},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{wynn1992addition,
  title={Addition and subtraction by human infants},
  author={Wynn, Karen},
  journal={Nature},
  volume={358},
  number={6389},
  pages={749--750},
  year={1992},
  publisher={Nature Publishing Group UK London}
}

@article{woodruff1981primative,
  title={Primative mathematical concepts in the chimpanzee: proportionality and numerosity},
  author={Woodruff, Guy and Premack, David},
  journal={Nature},
  volume={293},
  number={5833},
  pages={568--570},
  year={1981},
  publisher={Nature Publishing Group UK London}
}

@article{SHIPLEY1990109,
title = {Countable entities: Developmental changes},
journal = {Cognition},
volume = {34},
number = {2},
pages = {109-136},
year = {1990},
issn = {0010-0277},
doi = {https://doi.org/10.1016/0010-0277(90)90041-H},
url = {https://www.sciencedirect.com/science/article/pii/001002779090041H},
author = {Elizabeth F. Shipley and Barbara Shepperson},
abstract = {The canonical countable entity for 3- and 4-year-old children is a discrete physical object. When children were asked to count labeled entities such as “forks”, they counted each detached part of a pork as a separate entity. When asked to count kinds (“How many kinds of animals?”) or properties (“How many colors?”), where each kind of property was exemplified by several separate objects, they included each discrete object in their count. Their counts of classes were more accurate in the absence of objects, or in the presence of a single member of each class, than in the presence of several members of each class. Young children are evidently predisposed to process discrete physical objects. Evidence is presented that, developmentally, this bias precedes learning to count. It is proposed that this discrete physical object bias facilities mastery of counting.}
}

@article{leivada2023dall,
  title={DALL{\textperiodcentered} E 2 fails to reliably capture common syntactic processes},
  author={Leivada, Evelina and Murphy, Elliot and Marcus, Gary},
  journal={Social Sciences \& Humanities Open},
  volume={8},
  number={1},
  pages={100648},
  year={2023},
  publisher={Elsevier}
}

@software{robyn_speer_2022_7199437,
  author       = {Robyn Speer},
  title        = {rspeer/wordfreq: v3.0},
  month        = sep,
  year         = 2022,
  publisher    = {Zenodo},
  version      = {v3.0.2},
  doi          = {10.5281/zenodo.7199437},
  url          = {https://doi.org/10.5281/zenodo.7199437}
}

@article{wellman1986thinking,
  title={Thinking about nothing: Development of concepts of zero},
  author={Wellman, Henry M and Miller, Kevin F},
  journal={British Journal of Developmental Psychology},
  volume={4},
  number={1},
  pages={31--42},
  year={1986},
  publisher={Wiley Online Library}
}

@article{izard2009newborn,
  title={Newborn infants perceive abstract numbers},
  author={Izard, V{\'e}ronique and Sann, Coralie and Spelke, Elizabeth S and Streri, Arlette},
  journal={Proceedings of the National Academy of Sciences},
  volume={106},
  number={25},
  pages={10382--10385},
  year={2009},
  publisher={National Acad Sciences}
}

@inproceedings{acharya2019tallyqa,
  title={TallyQA: Answering complex counting questions},
  author={Acharya, Manoj and Kafle, Kushal and Kanan, Christopher},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={8076--8084},
  year={2019}
}

@inproceedings{zhang2018learning,
  title={Learning to Count Objects in Natural Images for Visual Question Answering},
  author={Zhang, Yan and Hare, Jonathon and Pr{\"u}gel-Bennett, Adam},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{paiss2023teaching,
  title={Teaching clip to count to ten},
  author={Paiss, Roni and Ephrat, Ariel and Tov, Omer and Zada, Shiran and Mosseri, Inbar and Irani, Michal and Dekel, Tali},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3170--3180},
  year={2023}
}

@inproceedings{trott2018interpretable,
  title={Interpretable Counting for Visual Question Answering},
  author={Trott, Alexander and Xiong, Caiming and Socher, Richard},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{yu2022scaling,
  title={Scaling autoregressive models for content-rich text-to-image generation},
  author={Yu, Jiahui and Xu, Yuanzhong and Koh, Jing Yu and Luong, Thang and Baid, Gunjan and Wang, Zirui and Vasudevan, Vijay and Ku, Alexander and Yang, Yinfei and Ayan, Burcu Karagol and others},
  journal={arXiv preprint arXiv:2206.10789},
  volume={2},
  number={3},
  pages={5},
  year={2022}
}

@article{vasconcelos2024greedy,
  title={Greedy Growing Enables High-Resolution Pixel-Based Diffusion Models},
  author={Vasconcelos, Cristina N and Waters, Abdullah Rashwan Austin and Walker, Trevor and Xu, Keyang and Yan, Jimmy and Qian, Rui and Luo, Shixin and Parekh, Zarana and Bunner, Andrew and Fei, Hongliang and others},
  journal={arXiv preprint arXiv:2405.16759},
  year={2024}
}

@article{saharia2022photorealistic,
  title={Photorealistic text-to-image diffusion models with deep language understanding},
  author={Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily L and Ghasemipour, Kamyar and Gontijo Lopes, Raphael and Karagol Ayan, Burcu and Salimans, Tim and others},
  journal=NIPS,
  volume={35},
  pages={36479--36494},
  year={2022}
}
@article{lison2016opensubtitles2016,
  title={Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles},
  author={Lison, Pierre and Tiedemann, J{\"o}rg},
  year={2016},
  publisher={European Language Resources Association}
}

@article{chen2015microsoft,
  title={Microsoft coco captions: Data collection and evaluation server},
  author={Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  journal={arXiv preprint arXiv:1504.00325},
  year={2015}
}

@inproceedings{podell2024sdxl,
title={{SDXL}: Improving Latent Diffusion Models for High-Resolution Image Synthesis},
author={Dustin Podell and Zion English and Kyle Lacey and Andreas Blattmann and Tim Dockhorn and Jonas M{\"u}ller and Joe Penna and Robin Rombach},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=di52zR8xgf}
}

@article{gokhale2022benchmarking,
  title={Benchmarking spatial relationships in text-to-image generation},
  author={Gokhale, Tejas and Palangi, Hamid and Nushi, Besmira and Vineet, Vibhav and Horvitz, Eric and Kamar, Ece and Baral, Chitta and Yang, Yezhou},
  journal={arXiv preprint arXiv:2212.10015},
  year={2022}
}

@article{patil2022stable,
  author = {Patil, Suraj and Cuenca, Pedro and Lambert, Nathan and von Platen, Patrick},
  title = {Stable Diffusion with Diffusers},
  journal = {Hugging Face Blog},
  year = {2022},
  note = {https://huggingface.co/blog/rlhf},
}

@article{chen2022pali,
  title={Pali: A jointly-scaled multilingual language-image model},
  author={Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, AJ and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and others},
  journal={arXiv preprint arXiv:2209.06794},
  year={2022}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}

@inproceedings{changpinyo2021conceptual,
  title={Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts},
  author={Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3558--3568},
  year={2021}
}

@article{podell2023sdxl,
  title={Sdxl: Improving latent diffusion models for high-resolution image synthesis},
  author={Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and M{\"u}ller, Jonas and Penna, Joe and Rombach, Robin},
  journal={arXiv preprint arXiv:2307.01952},
  year={2023}
}

@InProceedings{Rombach2022CVPR,
    author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},
    title     = {High-Resolution Image Synthesis With Latent Diffusion Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {10684-10695}
}

@article{wiles2024revisiting,
  title={Revisiting Text-to-Image Evaluation with Gecko: On Metrics, Prompts, and Human Ratings},
  author={Wiles, Olivia and Zhang, Chuhan and Albuquerque, Isabela and Kaji{\'c}, Ivana and Wang, Su and Bugliarello, Emanuele and Onoe, Yasumasa and Knutsen, Chris and Rashtchian, Cyrus and Pont-Tuset, Jordi and others},
  journal={arXiv preprint arXiv:2404.16820},
  year={2024}
}

@inproceedings{hu2023tifa,
  title={Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering},
  author={Hu, Yushi and Liu, Benlin and Kasai, Jungo and Wang, Yizhong and Ostendorf, Mari and Krishna, Ranjay and Smith, Noah A},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={20406--20417},
  year={2023}
}

@article{yarom2024you,
  title={What you see is what you read? improving text-image alignment evaluation},
  author={Yarom, Michal and Bitton, Yonatan and Changpinyo, Soravit and Aharoni, Roee and Herzig, Jonathan and Lang, Oran and Ofek, Eran and Szpektor, Idan},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  year={2023}
}

@article{hessel2021clipscore,
  title={Clipscore: A reference-free evaluation metric for image captioning},
  author={Hessel, Jack and Holtzman, Ari and Forbes, Maxwell and Bras, Ronan Le and Choi, Yejin},
  journal={arXiv preprint arXiv:2104.08718},
  year={2021}
}

@article{lee2024holistic,
  title={Holistic evaluation of text-to-image models},
  author={Lee, Tony and Yasunaga, Michihiro and Meng, Chenlin and Mai, Yifan and Park, Joon Sung and Gupta, Agrim and Zhang, Yunzhi and Narayanan, Deepak and Teufel, Hannah and Bellagente, Marco and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{
trott2018howmanyqa,
title={Interpretable Counting for Visual Question Answering},
author={Alexander Trott and Caiming Xiong and Richard Socher},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=S1J2ZyZ0Z},
}

@article{zhai2023siglip,
  title={Sigmoid loss for language image pre-training},
  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  journal={International Conference on Computer Vision ({ICCV})},
  year={2023}
}

@article{gemma2024,
    title={Gemma},
    url={https://www.kaggle.com/m/3301},
    DOI={10.34740/KAGGLE/M/3301},
    publisher={Kaggle},
    author={Gemma Team, Thomas Mesnard and Cassidy Hardin and Robert Dadashi and Surya Bhupatiraju and Laurent Sifre and Morgane Rivière and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and Léonard Hussenot and et al.},
    year={2024}
}

@misc{chen2023pali3,
      title={PaLI-3 Vision Language Models: Smaller, Faster, Stronger}, 
      author={Xi Chen and Xiao Wang and Lucas Beyer and Alexander Kolesnikov and Jialin Wu and Paul Voigtlaender and Basil Mustafa and Sebastian Goodman and Ibrahim Alabdulmohsin and Piotr Padlewski and Daniel Salz and Xi Xiong and Daniel Vlasic and Filip Pavetic and Keran Rong and Tianli Yu and Daniel Keysers and Xiaohua Zhai and Radu Soricut},
      year={2023},
      eprint={2310.09199},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}