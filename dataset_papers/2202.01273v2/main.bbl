\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Auer et~al.(2007)Auer, Bizer, Kobilarov, Lehmann, Cyganiak, and
  Ives]{dbpedia}
Auer, S., Bizer, C., Kobilarov, G., Lehmann, J., Cyganiak, R., and Ives, Z.
\newblock Dbpedia: A nucleus for a web of open data.
\newblock In Aberer, K., Choi, K.-S., Noy, N., Allemang, D., Lee, K.-I., Nixon,
  L., Golbeck, J., Mika, P., Maynard, D., Mizoguchi, R., Schreiber, G., and
  Cudr{\'e}-Mauroux, P. (eds.), \emph{The Semantic Web}, pp.\  722--735,
  Berlin, Heidelberg, 2007. Springer Berlin Heidelberg.
\newblock ISBN 978-3-540-76298-0.

\bibitem[Awasthi et~al.(2021)Awasthi, Beutel, Kleindessner, Morgenstern, and
  Wang]{awasthi2021evaluating}
Awasthi, P., Beutel, A., Kleindessner, M., Morgenstern, J., and Wang, X.
\newblock Evaluating fairness of machine learning models under uncertain and
  incomplete information.
\newblock In \emph{Proceedings of the 2021 ACM Conference on Fairness,
  Accountability, and Transparency}, pp.\  206--214, 2021.

\bibitem[Battiti(1994)]{battiti1994using}
Battiti, R.
\newblock Using mutual information for selecting features in supervised neural
  net learning.
\newblock \emph{IEEE Transactions on neural networks}, 5\penalty0 (4):\penalty0
  537--550, 1994.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pp.\
  1597--1607. PMLR, 2020.

\bibitem[Cheng et~al.(2021{\natexlab{a}})Cheng, Zhu, Li, Gong, Sun, and
  Liu]{cheng2021learningsieve}
Cheng, H., Zhu, Z., Li, X., Gong, Y., Sun, X., and Liu, Y.
\newblock Learning with instance-dependent label noise: A sample sieve
  approach.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=2VXyy9mIyU3}.

\bibitem[Cheng et~al.(2021{\natexlab{b}})Cheng, Zhu, Sun, and
  Liu]{cheng2021demystifying}
Cheng, H., Zhu, Z., Sun, X., and Liu, Y.
\newblock Demystifying how self-supervised features improve training from noisy
  labels.
\newblock \emph{arXiv preprint arXiv:2110.09022}, 2021{\natexlab{b}}.

\bibitem[Csisz{\'a}r(1967)]{csiszar1967information}
Csisz{\'a}r, I.
\newblock Information-type measures of difference of probability distributions
  and indirect observation.
\newblock \emph{studia scientiarum Mathematicarum Hungarica}, 2:\penalty0
  229--318, 1967.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin-etal-2019-bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics}, pp.\  4171--4186,
  June 2019.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://aclanthology.org/N19-1423}.

\bibitem[Dua \& Graff(2017)Dua and Graff]{dua2017uci}
Dua, D. and Graff, C.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Est{\'e}vez et~al.(2009)Est{\'e}vez, Tesmer, Perez, and
  Zurada]{estevez2009normalized}
Est{\'e}vez, P.~A., Tesmer, M., Perez, C.~A., and Zurada, J.~M.
\newblock Normalized mutual information feature selection.
\newblock \emph{IEEE Transactions on neural networks}, 20\penalty0
  (2):\penalty0 189--201, 2009.

\bibitem[Han et~al.(2018)Han, Yao, Yu, Niu, Xu, Hu, Tsang, and
  Sugiyama]{han2018co}
Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I., and Sugiyama, M.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  8527--8537, 2018.

\bibitem[Han et~al.(2020)Han, Yao, Liu, Niu, Tsang, Kwok, and
  Sugiyama]{han2020survey}
Han, B., Yao, Q., Liu, T., Niu, G., Tsang, I.~W., Kwok, J.~T., and Sugiyama, M.
\newblock A survey of label-noise representation learning: Past, present and
  future.
\newblock \emph{arXiv preprint arXiv:2011.04406}, 2020.

\bibitem[Hu et~al.(2019)Hu, Han, Shan, and Chen]{hu2019weakly}
Hu, M., Han, H., Shan, S., and Chen, X.
\newblock Weakly supervised image classification through noise regularization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  11517--11525, 2019.

\bibitem[Jigsaw(2018)]{Jigsaw}
Jigsaw.
\newblock Jigsaw toxic comment classification challenge, 2018.
\newblock URL
  \url{https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge}.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Lamy et~al.(2019)Lamy, Zhong, Menon, and Verma]{lamy2019noise}
Lamy, A., Zhong, Z., Menon, A.~K., and Verma, N.
\newblock Noise-tolerant fair classification.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, Haffner,
  et~al.]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., et~al.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Li et~al.(2021)Li, Liu, Han, Niu, and Sugiyama]{li2021provably}
Li, X., Liu, T., Han, B., Niu, G., and Sugiyama, M.
\newblock Provably end-to-end label-noise learning without anchor points.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6403--6413. PMLR, 2021.

\bibitem[Liu et~al.(2012)Liu, Peng, and Ihler]{liu2012variational}
Liu, Q., Peng, J., and Ihler, A.
\newblock Variational inference for crowdsourcing.
\newblock In \emph{Proceedings of the 25th International Conference on Neural
  Information Processing Systems-Volume 1}, pp.\  692--700, 2012.

\bibitem[Liu \& Tao(2015)Liu and Tao]{liu2015classification}
Liu, T. and Tao, D.
\newblock Classification with noisy labels by importance reweighting.
\newblock \emph{IEEE Transactions on pattern analysis and machine
  intelligence}, 38\penalty0 (3):\penalty0 447--461, 2015.

\bibitem[Liu(2021)]{liu2021importance}
Liu, Y.
\newblock Understanding instance-level label noise: Disparate impacts and
  treatments.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6725--6735. PMLR, 2021.

\bibitem[Liu(2022)]{liu2022identifiability}
Liu, Y.
\newblock Identifiability of label noise transition matrix.
\newblock \emph{arXiv preprint arXiv:2202.02016}, 2022.

\bibitem[Liu \& Chen(2017)Liu and Chen]{liu2017machine}
Liu, Y. and Chen, Y.
\newblock Machine-learning aided peer prediction.
\newblock In \emph{Proceedings of the 2017 ACM Conference on Economics and
  Computation}, pp.\  63--80, 2017.

\bibitem[Liu \& Guo(2020)Liu and Guo]{liu2019peer}
Liu, Y. and Guo, H.
\newblock Peer loss functions: Learning from noisy labels without knowing noise
  rates.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6226--6236. PMLR, 2020.

\bibitem[Liu \& Liu(2015)Liu and Liu]{liu2015online}
Liu, Y. and Liu, M.
\newblock An online learning approach to improving the quality of
  crowd-sourcing.
\newblock \emph{ACM SIGMETRICS Performance Evaluation Review}, 43\penalty0
  (1):\penalty0 217--230, 2015.

\bibitem[Liu \& Wang(2021)Liu and Wang]{liu2021can}
Liu, Y. and Wang, J.
\newblock Can less be more? when increasing-to-balancing label noise rates
  considered beneficial.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Liu et~al.(2020)Liu, Wang, and Chen]{liu2020surrogate}
Liu, Y., Wang, J., and Chen, Y.
\newblock Surrogate scoring rules.
\newblock In \emph{Proceedings of the 21st ACM Conference on Economics and
  Computation}, pp.\  853--871, 2020.

\bibitem[Luca(2016)]{luca2016reviews}
Luca, M.
\newblock Reviews, reputation, and revenue: The case of yelp. com.
\newblock \emph{Com (March 15, 2016). Harvard Business School NOM Unit Working
  Paper}, \penalty0 (12-016), 2016.

\bibitem[Lukasik et~al.(2020)Lukasik, Bhojanapalli, Menon, and
  Kumar]{lukasik2020does}
Lukasik, M., Bhojanapalli, S., Menon, A., and Kumar, S.
\newblock Does label smoothing mitigate label noise?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6448--6458. PMLR, 2020.

\bibitem[Luo et~al.(2020)Luo, Li, Wang, and Liu]{luo2020research}
Luo, T., Li, X., Wang, H., and Liu, Y.
\newblock Research replication prediction using weakly supervised learning.
\newblock In \emph{In Proceedings of the 2020 Conference on Empirical Methods
  in Natural Language Processing: Findings}, 2020.

\bibitem[McCormick et~al.(2016)McCormick, Li, Calvert, Crampin, Kahn, and
  Clark]{mccormick2016probabilistic}
McCormick, T.~H., Li, Z.~R., Calvert, C., Crampin, A.~C., Kahn, K., and Clark,
  S.~J.
\newblock Probabilistic cause-of-death assignment using verbal autopsies.
\newblock \emph{Journal of the American Statistical Association}, 111\penalty0
  (515):\penalty0 1036--1049, 2016.

\bibitem[Menon et~al.(2015)Menon, Van~Rooyen, Ong, and
  Williamson]{menon2015learning}
Menon, A., Van~Rooyen, B., Ong, C.~S., and Williamson, B.
\newblock Learning from corrupted binary labels via class-probability
  estimation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  125--134, 2015.

\bibitem[Natarajan et~al.(2013)Natarajan, Dhillon, Ravikumar, and
  Tewari]{natarajan2013learning}
Natarajan, N., Dhillon, I.~S., Ravikumar, P.~K., and Tewari, A.
\newblock Learning with noisy labels.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1196--1204, 2013.

\bibitem[Northcutt et~al.(2021{\natexlab{a}})Northcutt, Jiang, and
  Chuang]{northcutt2021confident}
Northcutt, C., Jiang, L., and Chuang, I.
\newblock Confident learning: Estimating uncertainty in dataset labels.
\newblock \emph{Journal of Artificial Intelligence Research}, 70:\penalty0
  1373--1411, 2021{\natexlab{a}}.

\bibitem[Northcutt et~al.(2021{\natexlab{b}})Northcutt, Athalye, and
  Mueller]{northcutt2021pervasive}
Northcutt, C.~G., Athalye, A., and Mueller, J.
\newblock Pervasive label errors in test sets destabilize machine learning
  benchmarks.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 1)}, 2021{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=XccDXrDNLek}.

\bibitem[Nowozin et~al.(2016)Nowozin, Cseke, and Tomioka]{nowozin2016f}
Nowozin, S., Cseke, B., and Tomioka, R.
\newblock f-gan: Training generative neural samplers using variational
  divergence minimization.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, pp.\  271--279, 2016.

\bibitem[Patrini et~al.(2017)Patrini, Rozza, Krishna~Menon, Nock, and
  Qu]{patrini2017making}
Patrini, G., Rozza, A., Krishna~Menon, A., Nock, R., and Qu, L.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  1944--1952, 2017.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8748--8763. PMLR, 2021.

\bibitem[Scott(2015)]{scott2015rate}
Scott, C.
\newblock A rate of convergence for mixture proportion estimation, with
  application to learning from noisy labels.
\newblock In \emph{AISTATS}, 2015.

\bibitem[Sidorov et~al.(2014)Sidorov, Gelbukh, G{\'o}mez-Adorno, and
  Pinto]{Sidorov2014SoftSA}
Sidorov, G., Gelbukh, A., G{\'o}mez-Adorno, H., and Pinto, D.
\newblock Soft similarity and soft cosine measure: Similarity of features in
  vector space model.
\newblock \emph{Computacion y Sistemas}, pp.\  491--504, January 2014.
\newblock ISSN 1405-5546.
\newblock \doi{10.13053/CyS-18-3-2043}.

\bibitem[Vergara \& Est{\'e}vez(2014)Vergara and
  Est{\'e}vez]{vergara2014review}
Vergara, J.~R. and Est{\'e}vez, P.~A.
\newblock A review of feature selection methods based on mutual information.
\newblock \emph{Neural computing and applications}, 24\penalty0 (1):\penalty0
  175--186, 2014.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Xiao, Li, Feng, Niu, Chen, and
  Zhao]{wang2022pico}
Wang, H., Xiao, R., Li, Y., Feng, L., Niu, G., Chen, G., and Zhao, J.
\newblock Pi{CO}: Contrastive label disambiguation for partial label learning.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=EhYjZy6e1gJ}.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Guo, Zhu, and
  Liu]{wang2021policy}
Wang, J., Guo, H., Zhu, Z., and Liu, Y.
\newblock Policy learning using weak supervision.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{a}}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Liu, and Levy]{wang2021fair}
Wang, J., Liu, Y., and Levy, C.
\newblock Fair classification with group-dependent label noise.
\newblock In \emph{Proceedings of the 2021 ACM conference on fairness,
  accountability, and transparency}, pp.\  526--536, 2021{\natexlab{b}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Wang, and
  Liu]{wang2022understanding}
Wang, J., Wang, X.~E., and Liu, Y.
\newblock Understanding instance-level impact of fairness constraints.
\newblock In \emph{International Conference on Machine Learning}. PMLR,
  2022{\natexlab{b}}.

\bibitem[Wei et~al.(2020)Wei, Feng, Chen, and An]{wei2020combating}
Wei, H., Feng, L., Chen, X., and An, B.
\newblock Combating noisy labels by agreement: A joint training method with
  co-regularization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  13726--13735, 2020.

\bibitem[Wei et~al.(2021{\natexlab{a}})Wei, Tao, Xie, and An]{wei2021open}
Wei, H., Tao, L., Xie, R., and An, B.
\newblock Open-set label noise can improve robustness against inherent label
  noise.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{a}}.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Tao, Xie, Feng, and
  An]{wei2022open}
Wei, H., Tao, L., Xie, R., Feng, L., and An, B.
\newblock Open-sampling: Exploring out-of-distribution data for re-balancing
  long-tailed datasets.
\newblock In \emph{International Conference on Machine Learning (ICML)}. PMLR,
  2022{\natexlab{a}}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Xie, Cheng, Feng, An, and
  Li]{wei2022mitigating}
Wei, H., Xie, R., Cheng, H., Feng, L., An, B., and Li, Y.
\newblock Mitigating neural network overconfidence with logit normalization.
\newblock \emph{arXiv preprint arXiv:2205.09310}, 2022{\natexlab{b}}.

\bibitem[Wei et~al.(2022{\natexlab{c}})Wei, Xie, Feng, Han, and
  An]{wei2022deep}
Wei, H., Xie, R., Feng, L., Han, B., and An, B.
\newblock Deep learning from multiple noisy annotators as a union.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  2022{\natexlab{c}}.

\bibitem[Wei \& Liu(2020)Wei and Liu]{wei2020optimizing}
Wei, J. and Liu, Y.
\newblock When optimizing $ f $-divergence is robust with label noise.
\newblock \emph{arXiv preprint arXiv:2011.03687}, 2020.

\bibitem[Wei et~al.(2021{\natexlab{b}})Wei, Liu, Liu, Niu, Sugiyama, and
  Liu]{wei2021understanding}
Wei, J., Liu, H., Liu, T., Niu, G., Sugiyama, M., and Liu, Y.
\newblock To smooth or not? when label smoothing meets noisy labels,
  2021{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2106.04149}.

\bibitem[Wei et~al.(2022{\natexlab{d}})Wei, Zhu, Cheng, Liu, Niu, and
  Liu]{wei2022learning}
Wei, J., Zhu, Z., Cheng, H., Liu, T., Niu, G., and Liu, Y.
\newblock Learning with noisy labels revisited: A study using real-world human
  annotations.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{d}}.
\newblock URL \url{https://openreview.net/forum?id=TBWA6PLJZQm}.

\bibitem[Wei et~al.(2022{\natexlab{e}})Wei, Zhu, Luo, Amid, Kumar, and
  Liu]{wei2022aggregate}
Wei, J., Zhu, Z., Luo, T., Amid, E., Kumar, A., and Liu, Y.
\newblock To aggregate or not? {L}earning with separate noisy labels,
  2022{\natexlab{e}}.
\newblock URL \url{https://arxiv.org/abs/2206.07181}.

\bibitem[Wold et~al.(1987)Wold, Esbensen, and Geladi]{wold1987principal}
Wold, S., Esbensen, K., and Geladi, P.
\newblock Principal component analysis.
\newblock \emph{Chemometrics and intelligent laboratory systems}, 2\penalty0
  (1-3):\penalty0 37--52, 1987.

\bibitem[Xia et~al.(2019)Xia, Liu, Wang, Han, Gong, Niu, and
  Sugiyama]{xia2019anchor}
Xia, X., Liu, T., Wang, N., Han, B., Gong, C., Niu, G., and Sugiyama, M.
\newblock Are anchor points really indispensable in label-noise learning?
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6838--6849, 2019.

\bibitem[Yang et~al.(2021)Yang, Yang, Han, Liu, Xu, Niu, and
  Liu]{yang2021estimating}
Yang, S., Yang, E., Han, B., Liu, Y., Xu, M., Niu, G., and Liu, T.
\newblock Estimating instance-dependent label-noise transition matrix using
  dnns.
\newblock \emph{arXiv preprint arXiv:2105.13001}, 2021.

\bibitem[Yao et~al.(2020)Yao, Liu, Han, Gong, Deng, Niu, and
  Sugiyama]{yao2020dual}
Yao, Y., Liu, T., Han, B., Gong, M., Deng, J., Niu, G., and Sugiyama, M.
\newblock Dual t: Reducing estimation error for transition matrix in
  label-noise learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  7260--7271, 2020.

\bibitem[Yelp(2015)]{yelp}
Yelp.
\newblock Yelp dataset challenge, 2015.
\newblock URL \url{http://www.yelp.com/dataset_challenge.}

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Lee, and
  Agarwal]{zhang2021learning}
Zhang, M., Lee, J., and Agarwal, S.
\newblock Learning from noisy labels with no change to the training process.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12468--12478. PMLR, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2015)Zhang, Zhao, and LeCun]{NIPS2015_250cf8b5}
Zhang, X., Zhao, J., and LeCun, Y.
\newblock Character-level convolutional networks for text classification.
\newblock In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~28.
  Curran Associates, Inc., 2015.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf}.

\bibitem[Zhang et~al.(2014)Zhang, Chen, Zhou, and Jordan]{zhang2014spectral}
Zhang, Y., Chen, X., Zhou, D., and Jordan, M.~I.
\newblock Spectral methods meet em: A provably optimal algorithm for
  crowdsourcing.
\newblock \emph{Advances in neural information processing systems},
  27:\penalty0 1260--1268, 2014.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Li, Wei, Ma, Xu, and
  Zheng]{zhang2021alleviating}
Zhang, Z., Li, Y., Wei, H., Ma, K., Xu, T., and Zheng, Y.
\newblock Alleviating noisy-label effects in image classification via
  probability transition matrix.
\newblock \emph{arXiv preprint arXiv:2110.08866}, 2021{\natexlab{b}}.

\bibitem[Zhu et~al.(2021{\natexlab{a}})Zhu, Dong, and Liu]{zhu2022detect}
Zhu, Z., Dong, Z., and Liu, Y.
\newblock Detecting corrupted labels without training a model to predict,
  2021{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2110.06283}.

\bibitem[Zhu et~al.(2021{\natexlab{b}})Zhu, Liu, and Liu]{zhu2020second}
Zhu, Z., Liu, T., and Liu, Y.
\newblock A second-order approach to learning with instance-dependent label
  noise.
\newblock In \emph{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2021{\natexlab{b}}.

\bibitem[Zhu et~al.(2021{\natexlab{c}})Zhu, Song, and
  Liu]{zhu2021clusterability}
Zhu, Z., Song, Y., and Liu, Y.
\newblock Clusterability as an alternative to anchor points when learning with
  noisy labels.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, ICML '21, 2021{\natexlab{c}}.

\bibitem[Zhu et~al.(2022)Zhu, Luo, and Liu]{zhu2022the}
Zhu, Z., Luo, T., and Liu, Y.
\newblock The rich get richer: Disparate impact of semi-supervised learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=DXPftn5kjQK}.

\end{thebibliography}
