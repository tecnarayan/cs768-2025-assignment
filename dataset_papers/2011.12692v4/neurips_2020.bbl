\begin{thebibliography}{10}

\bibitem{bengio2009curriculum}
Y.~Bengio, J.~Louradour, R.~Collobert, and J.~Weston.
\newblock Curriculum learning.
\newblock In {\em Proceedings of the 26th annual international conference on
  machine learning}, pages 41--48, 2009.

\bibitem{berner2019dota}
C.~Berner, G.~Brockman, B.~Chan, V.~Cheung, P.~D{\k{e}}biak, C.~Dennison,
  D.~Farhi, Q.~Fischer, S.~Hashme, C.~Hesse, et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1912.06680}, 2019.

\bibitem{brown2018superhuman}
N.~Brown and T.~Sandholm.
\newblock Superhuman ai for heads-up no-limit poker: Libratus beats top
  professionals.
\newblock {\em Science}, 359(6374):418--424, 2018.

\bibitem{bu2008comprehensive}
L.~Bu, R.~Babu, B.~De~Schutter, et~al.
\newblock A comprehensive survey of multiagent reinforcement learning.
\newblock {\em IEEE Transactions on Systems, Man, and Cybernetics, Part C
  (Applications and Reviews)}, 38(2):156--172, 2008.

\bibitem{chen2018art}
Z.~Chen, T.-H.~D. Nguyen, Y.~Xu, C.~Amato, S.~Cooper, Y.~Sun, and M.~S.
  El-Nasr.
\newblock The art of drafting: a team-oriented hero recommendation system for
  multiplayer online battle arena games.
\newblock In {\em Proceedings of the 12th ACM Conference on Recommender
  Systems}, pages 200--208, 2018.

\bibitem{chen2017game}
Z.~Chen and D.~Yi.
\newblock The game imitation: Deep supervised convolutional networks for quick
  video game ai.
\newblock {\em arXiv preprint arXiv:1702.05663}, 2017.

\bibitem{coulom2006efficient}
R.~Coulom.
\newblock Efficient selectivity and backup operators in monte-carlo tree
  search.
\newblock In {\em International Conference on Computers and Games}, pages
  72--83. Springer, 2006.

\bibitem{coulom2008whole}
R.~Coulom.
\newblock Whole-history rating: A bayesian rating system for players of
  time-varying strength.
\newblock In {\em International Conference on Computers and Games}, pages
  113--124. Springer, 2008.

\bibitem{czarnecki2019distilling}
W.~M. Czarnecki, R.~Pascanu, S.~Osindero, S.~Jayakumar, G.~Swirszcz, and
  M.~Jaderberg.
\newblock Distilling policy distillation.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 1331--1340, 2019.

\bibitem{eisenach2018marginal}
C.~Eisenach, H.~Yang, J.~Liu, and H.~Liu.
\newblock Marginal policy gradients: A unified family of estimators for bounded
  action spaces with applications.
\newblock In {\em The Seventh International Conference on Learning
  Representations (ICLR 2019)}, 2019.

\bibitem{espeholt2019seed}
L.~Espeholt, R.~Marinier, P.~Stanczyk, K.~Wang, and M.~Michalski.
\newblock Seed rl: Scalable and efficient deep-rl with accelerated central
  inference.
\newblock {\em arXiv preprint arXiv:1910.06591}, 2019.

\bibitem{espeholt2018impala}
L.~Espeholt, H.~Soyer, R.~Munos, K.~Simonyan, V.~Mnih, T.~Ward, Y.~Doron,
  V.~Firoiu, T.~Harley, I.~Dunning, et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock {\em arXiv preprint arXiv:1802.01561}, 2018.

\bibitem{hernandez2017survey}
P.~Hernandez-Leal, M.~Kaisers, T.~Baarslag, and E.~M. de~Cote.
\newblock A survey of learning in multiagent environments: Dealing with
  non-stationarity.
\newblock {\em arXiv preprint arXiv:1707.09183}, 2017.

\bibitem{hochreiter1997long}
S.~Hochreiter and J.~Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem{jaderberg2019human}
M.~Jaderberg, W.~M. Czarnecki, I.~Dunning, L.~Marris, G.~Lever, A.~G.
  Casta{\~n}eda, C.~Beattie, N.~C. Rabinowitz, A.~S. Morcos, A.~Ruderman,
  et~al.
\newblock Human-level performance in 3d multiplayer games with population-based
  reinforcement learning.
\newblock {\em Science}, 364(6443):859--865, 2019.

\bibitem{jiang2018feedback}
D.~Jiang, E.~Ekwedike, and H.~Liu.
\newblock Feedback-based tree search for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  2289--2298, 2018.

\bibitem{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In Y.~Bengio and Y.~LeCun, editors, {\em 3rd International Conference
  on Learning Representations}, 2015.

\bibitem{knuth1975analysis}
D.~E. Knuth and R.~W. Moore.
\newblock An analysis of alpha-beta pruning.
\newblock {\em Artificial intelligence}, 6(4):293--326, 1975.

\bibitem{kocsis2006bandit}
L.~Kocsis and C.~Szepesv{\'a}ri.
\newblock Bandit based monte-carlo planning.
\newblock In {\em European conference on machine learning}, pages 282--293.
  Springer, 2006.

\bibitem{konda2000actor}
V.~R. Konda and J.~N. Tsitsiklis.
\newblock Actor-critic algorithms.
\newblock In {\em Advances in neural information processing systems}, pages
  1008--1014, 2000.

\bibitem{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529, 2015.

\bibitem{morisette1998exact}
J.~T. Morisette and S.~Khorram.
\newblock Exact binomial confidence interval for proportions.
\newblock {\em Photogrammetric engineering and remote sensing}, 64(4):281--282,
  1998.

\bibitem{ontanon2013survey}
S.~Ontan{\'o}n, G.~Synnaeve, A.~Uriarte, F.~Richoux, D.~Churchill, and
  M.~Preuss.
\newblock A survey of real-time strategy game ai research and competition in
  starcraft.
\newblock {\em IEEE Transactions on Computational Intelligence and AI in
  games}, 5(4):293--311, 2013.

\bibitem{OpenAI_dota}
OpenAI.
\newblock Openai five.
\newblock
  \url{https://openai.com/blog/openai-five-defeats-dota-2-world-champions/},
  2019.

\bibitem{robertson2014review}
G.~Robertson and I.~Watson.
\newblock A review of real-time strategy game ai.
\newblock {\em AI Magazine}, 35(4):75--104, 2014.

\bibitem{rusu2015policy}
A.~A. Rusu, S.~G. Colmenarejo, C.~Gulcehre, G.~Desjardins, J.~Kirkpatrick,
  R.~Pascanu, V.~Mnih, K.~Kavukcuoglu, and R.~Hadsell.
\newblock Policy distillation.
\newblock {\em arXiv preprint arXiv:1511.06295}, 2015.

\bibitem{schulman2015high}
J.~Schulman, P.~Moritz, S.~Levine, M.~Jordan, and P.~Abbeel.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock {\em arXiv preprint arXiv:1506.02438}, 2015.

\bibitem{schulman2017proximal}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv:1707.06347}, 2017.

\bibitem{silva2017moba}
V.~d.~N. Silva and L.~Chaimowicz.
\newblock Moba: a new arena for game ai.
\newblock {\em arXiv preprint arXiv:1705.10443}, 2017.

\bibitem{silver2016mastering}
D.~Silver, A.~Huang, C.~J. Maddison, A.~Guez, L.~Sifre, G.~Van Den~Driessche,
  J.~Schrittwieser, I.~Antonoglou, V.~Panneershelvam, M.~Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em nature}, 529(7587):484--489, 2016.

\bibitem{silver2017mastering}
D.~Silver, J.~Schrittwieser, K.~Simonyan, I.~Antonoglou, A.~Huang, A.~Guez,
  T.~Hubert, L.~Baker, M.~Lai, A.~Bolton, et~al.
\newblock Mastering the game of go without human knowledge.
\newblock {\em Nature}, 550(7676):354, 2017.

\bibitem{van2017hybrid}
H.~Van~Seijen, M.~Fatemi, J.~Romoff, R.~Laroche, T.~Barnes, and J.~Tsang.
\newblock Hybrid reward architecture for reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5392--5402, 2017.

\bibitem{vinyals2019grandmaster}
O.~Vinyals, I.~Babuschkin, W.~M. Czarnecki, M.~Mathieu, A.~Dudzik, J.~Chung,
  D.~H. Choi, R.~Powell, T.~Ewalds, P.~Georgiev, et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock {\em Nature}, 575(7782):350--354, 2019.

\bibitem{vinyals2017starcraft}
O.~Vinyals, T.~Ewalds, S.~Bartunov, P.~Georgiev, A.~S. Vezhnevets, M.~Yeo,
  A.~Makhzani, H.~K{\"u}ttler, J.~Agapiou, J.~Schrittwieser, et~al.
\newblock Starcraft ii: A new challenge for reinforcement learning.
\newblock {\em arXiv preprint arXiv:1708.04782}, 2017.

\bibitem{wang2018exponentially}
Q.~Wang, J.~Xiong, L.~Han, H.~Liu, and T.~Zhang.
\newblock Exponentially weighted imitation learning for batched historical
  data.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS
  2018)}, pages 6288--6297, 2018.

\bibitem{wu2019hierarchical}
B.~Wu.
\newblock Hierarchical macro strategy model for moba game ai.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 1206--1213, 2019.

\bibitem{ye2020supervised}
D.~Ye, G.~Chen, P.~Zhao, F.~Qiu, B.~Yuan, W.~Zhang, S.~Chen, M.~Sun, X.~Li,
  S.~Li, et~al.
\newblock Supervised learning achieves human-level performance in moba games: A
  case study of honor of kings.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  2020.

\bibitem{ye2020mastering}
D.~Ye, Z.~Liu, M.~Sun, B.~Shi, P.~Zhao, H.~Wu, H.~Yu, S.~Yang, X.~Wu, Q.~Guo,
  et~al.
\newblock Mastering complex control in moba games with deep reinforcement
  learning.
\newblock In {\em AAAI}, pages 6672--6679, 2020.

\end{thebibliography}
