\begin{thebibliography}{13}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2012)Agarwal, Bartlett, Ravikumar, and
  Wainwright]{agbarawa12}
Agarwal, A., Bartlett, P., Ravikumar, P., and Wainwright, M.
\newblock Information-theoretic lower bounds on the oracle complexity of
  stochastic convex optimization.
\newblock \emph{IEEE Transactions on Information Theory}, 58\penalty0
  (5):\penalty0 3235--3249, 2012.

\bibitem[Bach \& Moulines(2011)Bach and Moulines]{BachMoulines11}
Bach, F. and Moulines, E.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock In \emph{NIPS}, 2011.

\bibitem[Hazan \& Kale(2011)Hazan and Kale]{HazanKa11}
Hazan, E. and Kale, S.
\newblock Beyond the regret minimization barrier: An optimal algorithm for
  stochastic strongly-convex optimization.
\newblock In \emph{COLT}, 2011.

\bibitem[Hazan et~al.(2007)Hazan, Agarwal, and Kale]{HazAgKal07}
Hazan, E., Agarwal, A., and Kale, S.
\newblock Logarithmic regret algorithms for online convex optimization.
\newblock \emph{Machine Learning}, 69\penalty0 (2-3):\penalty0 169--192, 2007.

\bibitem[Kushner \& Yin(2003)Kushner and Yin]{KushnerYin03}
Kushner, H. and Yin, G.
\newblock \emph{Stochastic Approximation and Recursive Algorithms and
  Applications}.
\newblock Springer, 2nd edition, 2003.

\bibitem[Lacoste-Julien et~al.(2012)Lacoste-Julien, Schmidt, and
  Bach]{LaJuSchBach12}
Lacoste-Julien, S., Schmidt, M., and Bach, F.
\newblock A simpler approach to obtaining an o(1/t) convergence rate for
  projected stochastic subgradient descent.
\newblock \emph{CoRR}, abs/1212.2002, 2012.

\bibitem[Ouyang \& Gray(2012)Ouyang and Gray]{OuGr12}
Ouyang, H. and Gray, A.
\newblock Stochastic smoothing for nonsmooth minimizations: Accelerating sgd by
  exploiting structure.
\newblock In \emph{ICML}, 2012.

\bibitem[Rakhlin et~al.(2011)Rakhlin, Shamir, and Sridharan]{RakhShaSri12arxiv}
Rakhlin, A., Shamir, O., and Sridharan, K.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock \emph{CoRR}, abs/1109.5647, 2011.

\bibitem[Shalev-Shwartz et~al.(2009)Shalev-Shwartz, Shamir, Srebro, and
  Sridharan]{ShalShamSrebSri09b}
Shalev-Shwartz, S., Shamir, O., Srebro, N., and Sridharan, K.
\newblock Stochastic convex optimization.
\newblock In \emph{COLT}, 2009.

\bibitem[Shalev-Shwartz et~al.(2011)Shalev-Shwartz, Singer, Srebro, and
  Cotter]{ShaSiSreCo11}
Shalev-Shwartz, S., Singer, Y., Srebro, N., and Cotter, A.
\newblock Pegasos: primal estimated sub-gradient solver for svm.
\newblock \emph{Mathematical Programming}, 127\penalty0 (1):\penalty0 3--30,
  2011.

\bibitem[Shamir(2012)]{shamir12}
Shamir, O.
\newblock Is averaging needed for strongly convex stochastic gradient descent?
\newblock Open problem presented at COLT, 2012.

\bibitem[Zhang(2004)]{Zhang04}
Zhang, T.
\newblock Solving large scale linear prediction problems using stochastic
  gradient descent algorithms.
\newblock In \emph{ICML}, 2004.

\bibitem[Zinkevich(2003)]{Zin03}
Zinkevich, M.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{ICML}, 2003.

\end{thebibliography}
