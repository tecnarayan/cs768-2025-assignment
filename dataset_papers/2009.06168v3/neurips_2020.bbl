\begin{thebibliography}{10}

\bibitem{atlas1990training}
L.~E. Atlas, D.~A. Cohn, and R.~E. Ladner.
\newblock Training connectionist networks with queries and selective sampling.
\newblock In {\em Advances in neural information processing systems}, pages
  566--573, 1990.

\bibitem{berthelot2019mixmatch}
D.~Berthelot, N.~Carlini, I.~Goodfellow, N.~Papernot, A.~Oliver, and C.~A.
  Raffel.
\newblock Mixmatch: A holistic approach to semi-supervised learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5050--5060, 2019.

\bibitem{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em Computer Vision and Pattern Recognition}, 2009.

\bibitem{fergus2009semi}
R.~Fergus, Y.~Weiss, and A.~Torralba.
\newblock Semi-supervised learning in gigantic image collections.
\newblock In {\em Advances in neural information processing systems}, pages
  522--530, 2009.

\bibitem{freytag2014selecting}
A.~Freytag, E.~Rodner, and J.~Denzler.
\newblock Selecting influential examples: Active learning with expected model
  output changes.
\newblock In {\em European Conference on Computer Vision}, pages 562--577.
  Springer, 2014.

\bibitem{furlanello2018born}
T.~Furlanello, Z.~C. Lipton, M.~Tschannen, L.~Itti, and A.~Anandkumar.
\newblock Born again neural networks.
\newblock {\em arXiv preprint arXiv:1805.04770}, 2018.

\bibitem{gastaldi2017shake}
X.~Gastaldi.
\newblock Shake-shake regularization.
\newblock {\em arXiv preprint arXiv:1705.07485}, 2017.

\bibitem{grandvalet2005semi}
Y.~Grandvalet and Y.~Bengio.
\newblock Semi-supervised learning by entropy minimization.
\newblock In {\em Advances in neural information processing systems}, pages
  529--536, 2005.

\bibitem{guillaumin2010multimodal}
M.~Guillaumin, J.~Verbeek, and C.~Schmid.
\newblock Multimodal semi-supervised learning for image classification.
\newblock In {\em 2010 IEEE Computer society conference on computer vision and
  pattern recognition}, pages 902--909. IEEE, 2010.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hinton2015distilling}
G.~Hinton, O.~Vinyals, and J.~Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{hu2020creating}
H.~Hu, L.~Xie, R.~Hong, and Q.~Tian.
\newblock Creating something from nothing: Unsupervised knowledge distillation
  for cross-modal hashing.
\newblock {\em arXiv preprint arXiv:2004.00280}, 2020.

\bibitem{huang2017densely}
G.~Huang, Z.~Liu, L.~Van Der~Maaten, and K.~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4700--4708, 2017.

\bibitem{iscen2019label}
A.~Iscen, G.~Tolias, Y.~Avrithis, and O.~Chum.
\newblock Label propagation for deep semi-supervised learning.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 5070--5079, 2019.

\bibitem{kirsch2019batchbald}
A.~Kirsch, J.~van Amersfoort, and Y.~Gal.
\newblock Batchbald: Efficient and diverse batch acquisition for deep bayesian
  active learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7024--7035, 2019.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{laine2016temporal}
S.~Laine and T.~Aila.
\newblock Temporal ensembling for semi-supervised learning.
\newblock {\em arXiv preprint arXiv:1610.02242}, 2016.

\bibitem{lecun2015deep}
Y.~LeCun, Y.~Bengio, and G.~Hinton.
\newblock Deep learning.
\newblock {\em Nature}, 521(7553):436--444, 2015.

\bibitem{lee2013pseudo}
D.-H. Lee.
\newblock Pseudo-label: The simple and efficient semi-supervised learning
  method for deep neural networks.
\newblock In {\em Workshop on challenges in representation learning, ICML},
  volume~3, page~2, 2013.

\bibitem{lewis1994sequential}
D.~D. Lewis and W.~A. Gale.
\newblock A sequential algorithm for training text classifiers.
\newblock In {\em SIGIRâ€™94}, pages 3--12. Springer, 1994.

\bibitem{luo2013latent}
W.~Luo, A.~Schwing, and R.~Urtasun.
\newblock Latent structured active learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  728--736, 2013.

\bibitem{miyato2018virtual}
T.~Miyato, S.-i. Maeda, M.~Koyama, and S.~Ishii.
\newblock Virtual adversarial training: a regularization method for supervised
  and semi-supervised learning.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  41(8):1979--1993, 2018.

\bibitem{papadopoulos2016we}
D.~P. Papadopoulos, J.~R. Uijlings, F.~Keller, and V.~Ferrari.
\newblock We don't need no bounding-boxes: Training object class detectors
  using only human verification.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 854--863, 2016.

\bibitem{papadopoulos2017training}
D.~P. Papadopoulos, J.~R. Uijlings, F.~Keller, and V.~Ferrari.
\newblock Training object class detectors with click supervision.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2017.

\bibitem{pinsler2019bayesian}
R.~Pinsler, J.~Gordon, E.~Nalisnick, and J.~M. Hern{\'a}ndez-Lobato.
\newblock Bayesian batch active learning as sparse subset approximation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6356--6367, 2019.

\bibitem{qiao2018deep}
S.~Qiao, W.~Shen, Z.~Zhang, B.~Wang, and A.~Yuille.
\newblock Deep co-training for semi-supervised image recognition.
\newblock In {\em Proceedings of the european conference on computer vision
  (eccv)}, pages 135--152, 2018.

\bibitem{rasmus2015semi}
A.~Rasmus, M.~Berglund, M.~Honkala, H.~Valpola, and T.~Raiko.
\newblock Semi-supervised learning with ladder networks.
\newblock In {\em Advances in neural information processing systems}, pages
  3546--3554, 2015.

\bibitem{ravi2016optimization}
S.~Ravi and H.~Larochelle.
\newblock Optimization as a model for few-shot learning.
\newblock 2016.

\bibitem{romero2014fitnets}
A.~Romero, N.~Ballas, S.~E. Kahou, A.~Chassang, C.~Gatta, and Y.~Bengio.
\newblock Fitnets: Hints for thin deep nets.
\newblock {\em arXiv preprint arXiv:1412.6550}, 2014.

\bibitem{russakovsky2015imagenet}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International Journal of Computer Vision}, 115(3):211--252,
  2015.

\bibitem{sener2017active}
O.~Sener and S.~Savarese.
\newblock Active learning for convolutional neural networks: A core-set
  approach.
\newblock {\em arXiv preprint arXiv:1708.00489}, 2017.

\bibitem{shi2019integrating}
W.~Shi and Q.~Yu.
\newblock Integrating bayesian and discriminative sparse kernel machines for
  multi-class active learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2282--2291, 2019.

\bibitem{tarvainen2017mean}
A.~Tarvainen and H.~Valpola.
\newblock Mean teachers are better role models: Weight-averaged consistency
  targets improve semi-supervised deep learning results.
\newblock In {\em Advances in Neural Information Processing Systems}, 2017.

\bibitem{wang2016cost}
K.~Wang, D.~Zhang, Y.~Li, R.~Zhang, and L.~Lin.
\newblock Cost-effective active learning for deep image classification.
\newblock {\em IEEE Transactions on Circuits and Systems for Video Technology},
  27(12):2591--2600, 2016.

\bibitem{xu2016deep}
N.~Xu, B.~Price, S.~Cohen, J.~Yang, and T.~S. Huang.
\newblock Deep interactive object selection.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 373--381, 2016.

\bibitem{yang2018knowledge}
C.~Yang, L.~Xie, S.~Qiao, and A.~Yuille.
\newblock Knowledge distillation in generations: More tolerant teachers educate
  better students.
\newblock {\em arXiv preprint arXiv:1805.05551}, 2018.

\bibitem{yoo2019learning}
D.~Yoo and I.~S. Kweon.
\newblock Learning loss for active learning.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2019.

\bibitem{yu2019tangent}
B.~Yu, J.~Wu, J.~Ma, and Z.~Zhu.
\newblock Tangent-normal adversarial regularization for semi-supervised
  learning.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 10676--10684, 2019.

\bibitem{zagoruyko2016wide}
S.~Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock {\em arXiv preprint arXiv:1605.07146}, 2016.

\bibitem{zhang2020robust}
H.~Zhang, Z.~Zhang, M.~Zhao, Q.~Ye, M.~Zhang, and M.~Wang.
\newblock Robust triple-matrix-recovery-based auto-weighted label propagation
  for classification.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  2020.

\bibitem{zhu2017object}
Z.~Zhu, L.~Xie, and A.~L. Yuille.
\newblock Object recognition with and without objects.
\newblock In {\em International Joint Conference on Artificial Intelligence},
  2017.

\end{thebibliography}
