\begin{thebibliography}{10}

\bibitem{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  11895--11907, 2019.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{hyvarinen2005estimation}
Aapo Hyv{\"a}rinen.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock {\em Journal of Machine Learning Research}, 6(Apr):695--709, 2005.

\bibitem{vincent2011connection}
Pascal Vincent.
\newblock A connection between score matching and denoising autoencoders.
\newblock {\em Neural computation}, 23(7):1661--1674, 2011.

\bibitem{raphan2011least}
Martin Raphan and Eero~P Simoncelli.
\newblock Least squares estimation without priors or supervision.
\newblock {\em Neural computation}, 23(2):374--420, 2011.

\bibitem{roberts1996exponential}
Gareth~O Roberts, Richard~L Tweedie, et~al.
\newblock Exponential convergence of langevin distributions and their discrete
  approximations.
\newblock {\em Bernoulli}, 2(4):341--363, 1996.

\bibitem{welling2011bayesian}
Max Welling and Yee~W Teh.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In {\em Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 681--688, 2011.

\bibitem{song2019sliced}
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon.
\newblock Sliced score matching: {A} scalable approach to density and score
  estimation.
\newblock In {\em Proceedings of the Thirty-Fifth Conference on Uncertainty in
  Artificial Intelligence, {UAI} 2019, Tel Aviv, Israel, July 22-25, 2019},
  page 204, 2019.

\bibitem{jolicoeurpiche2020adversarial}
Alexia Jolicoeur-Martineau, Rémi Piché-Taillefer, Ioannis Mitliagkas, and
  Rémi Tachet~des Combes.
\newblock Adversarial score matching and improved sampling for image
  generation.
\newblock {\em arXiv preprint arXiv:2009.05475}, 2020.

\bibitem{saremi2019neural}
Saeed Saremi and Aapo Hyvarinen.
\newblock Neural empirical bayes.
\newblock {\em Journal of Machine Learning Research}, 20:1--23, 2019.

\bibitem{li2019learning}
Zengyi Li, Yubei Chen, and Friedrich~T Sommer.
\newblock Learning energy-based models in high-dimensional spaces with
  multi-scale denoising score matching.
\newblock {\em arXiv}, pages arXiv--1910, 2019.

\bibitem{kadkhodaie2020solving}
Zahra Kadkhodaie and Eero~P Simoncelli.
\newblock Solving linear inverse problems using the prior implicit in a
  denoiser.
\newblock {\em arXiv preprint arXiv:2007.13640}, 2020.

\bibitem{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock In {\em Advances in neural information processing systems}, pages
  6626--6637, 2017.

\bibitem{efron2011tweedie}
Bradley Efron.
\newblock Tweedie’s formula and selection bias.
\newblock {\em Journal of the American Statistical Association},
  106(496):1602--1614, 2011.

\bibitem{grafarend2006linear}
Erik~W Grafarend.
\newblock {\em Linear and nonlinear models: fixed effects, random effects, and
  mixed models}.
\newblock de Gruyter, 2006.

\bibitem{liu2015faceattributes}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock In {\em Proceedings of International Conference on Computer Vision
  (ICCV)}, December 2015.

\bibitem{van2016conditional}
Aaron Van~den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex
  Graves, et~al.
\newblock Conditional image generation with pixelcnn decoders.
\newblock In {\em Advances in neural information processing systems}, pages
  4790--4798, 2016.

\bibitem{du2019implicit}
Yilun Du and Igor Mordatch.
\newblock Implicit generation and generalization in energy-based models.
\newblock {\em arXiv preprint arXiv:1903.08689}, 2019.

\bibitem{gulrajani2017improved}
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron~C
  Courville.
\newblock Improved training of wasserstein gans.
\newblock In {\em Advances in neural information processing systems}, pages
  5767--5777, 2017.

\bibitem{miyato2018spectral}
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{barratt2018note}
Shane Barratt and Rishi Sharma.
\newblock A note on the inception score.
\newblock {\em arXiv preprint arXiv:1801.01973}, 2018.

\bibitem{sajjadi2018assessing}
Mehdi~SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain
  Gelly.
\newblock Assessing generative models via precision and recall.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5228--5237, 2018.

\bibitem{razavi2019generating}
Ali Razavi, Aaron van~den Oord, and Oriol Vinyals.
\newblock Generating diverse high-fidelity images with vq-vae-2.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  14837--14847, 2019.

\bibitem{lin2017refinenet}
Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid.
\newblock Refinenet: Multi-path refinement networks for high-resolution
  semantic segmentation.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1925--1934, 2017.

\bibitem{clevert2015fast}
Djork-Arn{\'e} Clevert, Thomas Unterthiner, and Sepp Hochreiter.
\newblock Fast and accurate deep network learning by exponential linear units
  (elus).
\newblock {\em arXiv preprint arXiv:1511.07289}, 2015.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{yu2015lsun}
Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong
  Xiao.
\newblock Lsun: Construction of a large-scale image dataset using deep learning
  with humans in the loop.
\newblock {\em arXiv preprint arXiv:1506.03365}, 2015.

\bibitem{karras2019style}
Tero Karras, Samuli Laine, and Timo Aila.
\newblock A style-based generator architecture for generative adversarial
  networks.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4401--4410, 2019.

\bibitem{zhou2019hype}
Sharon Zhou, Mitchell Gordon, Ranjay Krishna, Austin Narcomey, Li~F Fei-Fei,
  and Michael Bernstein.
\newblock Hype: A benchmark for human eye perceptual evaluation of generative
  models.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3444--3456, 2019.

\bibitem{song2019bridging}
Jiaming Song and Stefano Ermon.
\newblock Bridging the gap between $ f $-gans and wasserstein gans.
\newblock {\em arXiv preprint arXiv:1910.09779}, 2019.

\bibitem{reddi2019convergence}
Sashank~J Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock {\em arXiv preprint arXiv:1904.09237}, 2019.

\bibitem{karras2017progressive}
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
\newblock Progressive growing of gans for improved quality, stability, and
  variation.
\newblock {\em arXiv preprint arXiv:1710.10196}, 2017.

\bibitem{berthelot2017began}
David Berthelot, Thomas Schumm, and Luke Metz.
\newblock Began: Boundary equilibrium generative adversarial networks.
\newblock {\em arXiv preprint arXiv:1703.10717}, 2017.

\end{thebibliography}
