\begin{thebibliography}{75}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achard and Jutten(2005)]{achard2005identifiability}
S.~Achard and C.~Jutten.
\newblock Identifiability of post-nonlinear mixtures.
\newblock \emph{IEEE Signal Processing Letters}, 12\penalty0 (5):\penalty0
  423--426, 2005.

\bibitem[Allman et~al.(2009)Allman, Matias, and Rhodes]{allman2009}
E.~S. Allman, C.~Matias, and J.~A. Rhodes.
\newblock Identifiability of parameters in latent structure models with many
  observed variables.
\newblock \emph{Annals of Statistics}, pages 3099--3132, 2009.

\bibitem[Anandkumar et~al.(2013)Anandkumar, Hsu, Janzamin, and
  Kakade]{anandkumar2013overcomplete}
A.~Anandkumar, D.~J. Hsu, M.~Janzamin, and S.~M. Kakade.
\newblock When are overcomplete topic models identifiable? uniqueness of tensor
  tucker decompositions with structured sparsity.
\newblock \emph{Advances in neural information processing systems}, 26, 2013.

\bibitem[Aragam et~al.(2020)Aragam, Dan, Xing, and Ravikumar]{aragam2018npmix}
B.~Aragam, C.~Dan, E.~P. Xing, and P.~Ravikumar.
\newblock Identifiability of nonparametric mixture models and bayes optimal
  clustering.
\newblock \emph{Ann. Statist.}, 48\penalty0 (4):\penalty0 2277--2302, 2020.
\newblock ISSN 0090-5364.
\newblock \doi{10.1214/19-AOS1887}.
\newblock arXiv:1802.04397.

\bibitem[Arora et~al.(2012)Arora, Ge, and Moitra]{arora2012learning}
S.~Arora, R.~Ge, and A.~Moitra.
\newblock Learning topic models--going beyond svd.
\newblock In \emph{2012 IEEE 53rd annual symposium on foundations of computer
  science}, pages 1--10. IEEE, 2012.

\bibitem[Arora et~al.(2013)Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, and
  Zhu]{arora2013practical}
S.~Arora, R.~Ge, Y.~Halpern, D.~Mimno, A.~Moitra, D.~Sontag, Y.~Wu, and M.~Zhu.
\newblock A practical algorithm for topic modeling with provable guarantees.
\newblock In \emph{International Conference on Machine Learning}, pages
  280--288. PMLR, 2013.

\bibitem[Bansal et~al.(2021)Bansal, Nakkiran, and Barak]{bansal2021revisiting}
Y.~Bansal, P.~Nakkiran, and B.~Barak.
\newblock Revisiting model stitching to compare neural representations.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Barndorff-Nielsen(1965)]{barndorff1965}
O.~Barndorff-Nielsen.
\newblock Identifiability of mixtures of exponential families.
\newblock \emph{Journal of Mathematical Analysis and Applications}, 12\penalty0
  (1):\penalty0 115--121, 1965.

\bibitem[Bordes et~al.(2006)Bordes, Mottelet, and Vandekerkhove]{bordes2006}
L.~Bordes, S.~Mottelet, and P.~Vandekerkhove.
\newblock Semiparametric estimation of a two-component mixture model.
\newblock \emph{Annals of Statistics}, 34\penalty0 (3):\penalty0 1204--1232,
  2006.

\bibitem[Brehmer et~al.(2022)Brehmer, De~Haan, Lippe, and
  Cohen]{brehmer2022weakly}
J.~Brehmer, P.~De~Haan, P.~Lippe, and T.~Cohen.
\newblock Weakly supervised causal representation learning.
\newblock \emph{arXiv preprint arXiv:2203.16437}, 2022.

\bibitem[Comon(1994)]{comon1994}
P.~Comon.
\newblock Independent component analysis, a new concept?
\newblock \emph{Signal processing}, 36\penalty0 (3):\penalty0 287--314, 1994.

\bibitem[Csisz{\'a}rik et~al.(2021)Csisz{\'a}rik, K{\H{o}}r{\"o}si-Szab{\'o},
  Matszangosz, Papp, and Varga]{csiszarik2021similarity}
A.~Csisz{\'a}rik, P.~K{\H{o}}r{\"o}si-Szab{\'o}, {\'A}.~Matszangosz, G.~Papp,
  and D.~Varga.
\newblock Similarity and matching of neural network representations.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Dai et~al.(2020)Dai, Wang, and Wipf]{dai2020usual}
B.~Dai, Z.~Wang, and D.~Wipf.
\newblock The usual suspects? reassessing blame for vae posterior collapse.
\newblock In \emph{International Conference on Machine Learning}, pages
  2313--2322. PMLR, 2020.

\bibitem[D'Amour et~al.(2020)D'Amour, Heller, Moldovan, Adlam, Alipanahi,
  Beutel, Chen, Deaton, Eisenstein, Hoffman,
  et~al.]{damour2020underspecification}
A.~D'Amour, K.~Heller, D.~Moldovan, B.~Adlam, B.~Alipanahi, A.~Beutel, C.~Chen,
  J.~Deaton, J.~Eisenstein, M.~D. Hoffman, et~al.
\newblock Underspecification presents challenges for credibility in modern
  machine learning.
\newblock \emph{arXiv preprint arXiv:2011.03395}, 2020.

\bibitem[Darmois(1951)]{darmois1951analyse}
G.~Darmois.
\newblock Analyse des liaisons de probabilit{\'e}.
\newblock In \emph{Proc. Int. Stat. Conferences 1947}, page 231, 1951.

\bibitem[Dilokthanakul et~al.(2016)Dilokthanakul, Mediano, Garnelo, Lee,
  Salimbeni, Arulkumaran, and Shanahan]{dilokthanakul2016deep}
N.~Dilokthanakul, P.~A. Mediano, M.~Garnelo, M.~C. Lee, H.~Salimbeni,
  K.~Arulkumaran, and M.~Shanahan.
\newblock Deep unsupervised clustering with gaussian mixture variational
  autoencoders.
\newblock \emph{arXiv preprint arXiv:1611.02648}, 2016.

\bibitem[Evans(2016)]{evans2016graphs}
R.~J. Evans.
\newblock Graphs for margins of bayesian networks.
\newblock \emph{Scandinavian Journal of Statistics}, 43\penalty0 (3):\penalty0
  625--648, 2016.

\bibitem[Falck et~al.(2021)Falck, Zhang, Willetts, Nicholson, Yau, and
  Holmes]{falck2021multi}
F.~Falck, H.~Zhang, M.~Willetts, G.~Nicholson, C.~Yau, and C.~C. Holmes.
\newblock Multi-facet clustering variational autoencoders.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Gassiat et~al.(2016)Gassiat, Cleynen, and Robin]{gassiat2016inference}
{\'E}.~Gassiat, A.~Cleynen, and S.~Robin.
\newblock Inference in finite state space non parametric hidden markov models
  and applications.
\newblock \emph{Statistics and Computing}, 26\penalty0 (1):\penalty0 61--71,
  2016.

\bibitem[Gresele et~al.(2021)Gresele, Von~K{\"u}gelgen, Stimper, Sch{\"o}lkopf,
  and Besserve]{gresele2021independent}
L.~Gresele, J.~Von~K{\"u}gelgen, V.~Stimper, B.~Sch{\"o}lkopf, and M.~Besserve.
\newblock Independent mechanism analysis, a new concept?
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Hall and Zhou(2003)]{hall2003}
P.~Hall and X.-H. Zhou.
\newblock Nonparametric estimation of component distributions in a multivariate
  mixture.
\newblock \emph{Annals of Statistics}, pages 201--224, 2003.

\bibitem[H{\"a}lv{\"a} and Hyvarinen(2020)]{halva2020hidden}
H.~H{\"a}lv{\"a} and A.~Hyvarinen.
\newblock Hidden markov nonlinear ica: Unsupervised learning from nonstationary
  time series.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, pages
  939--948. PMLR, 2020.

\bibitem[H{\"a}lv{\"a} et~al.(2021)H{\"a}lv{\"a}, Corff, Leh{\'e}ricy, So, Zhu,
  Gassiat, and Hyvarinen]{halva2021disentangling}
H.~H{\"a}lv{\"a}, S.~L. Corff, L.~Leh{\'e}ricy, J.~So, Y.~Zhu, E.~Gassiat, and
  A.~Hyvarinen.
\newblock Disentangling identifiable features from noisy data with structured
  nonlinear ica.
\newblock \emph{arXiv preprint arXiv:2106.09620}, 2021.

\bibitem[He et~al.(2018)He, Spokoyny, Neubig, and
  Berg-Kirkpatrick]{he2018lagging}
J.~He, D.~Spokoyny, G.~Neubig, and T.~Berg-Kirkpatrick.
\newblock Lagging inference networks and posterior collapse in variational
  autoencoders.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Hunter et~al.(2007)Hunter, Wang, and Hettmansperger]{hunter2007}
D.~R. Hunter, S.~Wang, and T.~P. Hettmansperger.
\newblock Inference for mixtures of symmetric distributions.
\newblock \emph{Annals of Statistics}, pages 224--251, 2007.

\bibitem[Hyvarinen and Morioka(2016)]{hyvarinen2016unsupervised}
A.~Hyvarinen and H.~Morioka.
\newblock Unsupervised feature extraction by time-contrastive learning and
  nonlinear ica.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Hyvarinen and Morioka(2017)]{hyvarinen2017nonlinear}
A.~Hyvarinen and H.~Morioka.
\newblock Nonlinear ica of temporally dependent stationary sources.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 460--469.
  PMLR, 2017.

\bibitem[Hyv{\"a}rinen and Pajunen(1999)]{hyvarinen1999nonlinear}
A.~Hyv{\"a}rinen and P.~Pajunen.
\newblock Nonlinear independent component analysis: Existence and uniqueness
  results.
\newblock \emph{Neural networks}, 12\penalty0 (3):\penalty0 429--439, 1999.

\bibitem[Hyvarinen et~al.(2019)Hyvarinen, Sasaki, and
  Turner]{hyvarinen2019nonlinear}
A.~Hyvarinen, H.~Sasaki, and R.~Turner.
\newblock Nonlinear ica using auxiliary variables and generalized contrastive
  learning.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 859--868. PMLR, 2019.

\bibitem[Ishikawa et~al.(2022)Ishikawa, Teshima, Tojo, Oono, Ikeda, and
  Sugiyama]{ishikawa2022universal}
I.~Ishikawa, T.~Teshima, K.~Tojo, K.~Oono, M.~Ikeda, and M.~Sugiyama.
\newblock Universal approximation property of invertible neural networks.
\newblock \emph{arXiv preprint arXiv:2204.07415}, 2022.

\bibitem[Iwata et~al.(2013)Iwata, Duvenaud, and Ghahramani]{iwata2013warped}
T.~Iwata, D.~Duvenaud, and Z.~Ghahramani.
\newblock Warped mixtures for nonparametric cluster shapes.
\newblock In \emph{Proceedings of the Twenty-Ninth Conference on Uncertainty in
  Artificial Intelligence}, pages 311--320, 2013.

\bibitem[Jiang et~al.(2016)Jiang, Zheng, Tan, Tang, and
  Zhou]{jiang2016variational}
Z.~Jiang, Y.~Zheng, H.~Tan, B.~Tang, and H.~Zhou.
\newblock Variational deep embedding: An unsupervised and generative approach
  to clustering.
\newblock \emph{arXiv preprint arXiv:1611.05148}, 2016.

\bibitem[Johnson et~al.(2016)Johnson, Duvenaud, Wiltschko, Adams, and
  Datta]{johnson2016composing}
M.~J. Johnson, D.~K. Duvenaud, A.~Wiltschko, R.~P. Adams, and S.~R. Datta.
\newblock Composing graphical models with neural networks for structured
  representations and fast inference.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Jutten et~al.(2003)Jutten, Karhunen, et~al.]{jutten2003advances}
C.~Jutten, J.~Karhunen, et~al.
\newblock Advances in nonlinear blind source separation.
\newblock In \emph{Proc. of the 4th Int. Symp. on Independent Component
  Analysis and Blind Signal Separation (ICA2003)}, pages 245--256, 2003.

\bibitem[Khemakhem et~al.(2020{\natexlab{a}})Khemakhem, Kingma, Monti, and
  Hyvarinen]{khemakhem2020variational}
I.~Khemakhem, D.~Kingma, R.~Monti, and A.~Hyvarinen.
\newblock Variational autoencoders and nonlinear ica: A unifying framework.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2207--2217. PMLR, 2020{\natexlab{a}}.

\bibitem[Khemakhem et~al.(2020{\natexlab{b}})Khemakhem, Kingma, Monti, and
  Hyv{\"a}rinen]{khemakhem2020ice}
I.~Khemakhem, D.~P. Kingma, R.~P. Monti, and A.~Hyv{\"a}rinen.
\newblock Ice-beem: Identifiable conditional energy-based deep models.
\newblock \emph{NeurIPS2020}, 2020{\natexlab{b}}.

\bibitem[Kingma and Welling(2013)]{kingma2013auto}
D.~P. Kingma and M.~Welling.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Kivva et~al.(2021)Kivva, Rajendran, Ravikumar, and
  Aragam]{kivva2021learning}
B.~Kivva, G.~Rajendran, P.~Ravikumar, and B.~Aragam.
\newblock Learning latent causal graphs via mixture oracles.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Klindt et~al.(2020)Klindt, Schott, Sharma, Ustyuzhaninov, Brendel,
  Bethge, and Paiton]{klindt2020towards}
D.~A. Klindt, L.~Schott, Y.~Sharma, I.~Ustyuzhaninov, W.~Brendel, M.~Bethge,
  and D.~Paiton.
\newblock Towards nonlinear disentanglement in natural data with temporal
  sparse coding.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Klys et~al.(2018)Klys, Snell, and Zemel]{klys2018learning}
J.~Klys, J.~Snell, and R.~Zemel.
\newblock Learning latent subspaces in variational autoencoders.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Lee et~al.(2020)Lee, Min, Lee, and Hwang]{lee2020meta}
D.~B. Lee, D.~Min, S.~Lee, and S.~J. Hwang.
\newblock Meta-gmvae: Mixture of gaussian vae for unsupervised meta-learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Lenc and Vedaldi(2015)]{lenc2015understanding}
K.~Lenc and A.~Vedaldi.
\newblock Understanding image representations by measuring their equivariance
  and equivalence.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 991--999, 2015.

\bibitem[Li et~al.(2019)Li, Hooi, and Lee]{li2019identifying}
S.~Li, B.~Hooi, and G.~H. Lee.
\newblock Identifying through flows for recovering latent representations.
\newblock \emph{arXiv preprint arXiv:1909.12555}, 2019.

\bibitem[Li et~al.(2018)Li, Chen, Poon, and Zhang]{li2018learning}
X.~Li, Z.~Chen, L.~K. Poon, and N.~L. Zhang.
\newblock Learning latent superstructures in variational autoencoders for deep
  multidimensional clustering.
\newblock \emph{arXiv preprint arXiv:1803.05206}, 2018.

\bibitem[Locatello et~al.(2019)Locatello, Bauer, Lucic, Raetsch, Gelly,
  Sch{\"o}lkopf, and Bachem]{locatello2019challenging}
F.~Locatello, S.~Bauer, M.~Lucic, G.~Raetsch, S.~Gelly, B.~Sch{\"o}lkopf, and
  O.~Bachem.
\newblock Challenging common assumptions in the unsupervised learning of
  disentangled representations.
\newblock In \emph{international conference on machine learning}, pages
  4114--4124. PMLR, 2019.

\bibitem[Locatello et~al.(2020)Locatello, Poole, R{\"a}tsch, Sch{\"o}lkopf,
  Bachem, and Tschannen]{locatello2020weakly}
F.~Locatello, B.~Poole, G.~R{\"a}tsch, B.~Sch{\"o}lkopf, O.~Bachem, and
  M.~Tschannen.
\newblock Weakly-supervised disentanglement without compromises.
\newblock In \emph{International Conference on Machine Learning}, pages
  6348--6359. PMLR, 2020.

\bibitem[Lu and Lu(2020)]{lu2020universal}
Y.~Lu and J.~Lu.
\newblock A universal approximation theorem of deep neural networks for
  expressing probability distributions.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 3094--3105, 2020.

\bibitem[Luise et~al.(2020)Luise, Pontil, and
  Ciliberto]{luise2020generalization}
G.~Luise, M.~Pontil, and C.~Ciliberto.
\newblock Generalization properties of optimal transport gans with latent
  distribution learning.
\newblock \emph{arXiv preprint arXiv:2007.14641}, 2020.

\bibitem[Markham and Grosse-Wentrup(2020)]{markham2020measurement}
A.~Markham and M.~Grosse-Wentrup.
\newblock Measurement dependence inducing latent causal models.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, pages
  590--599. PMLR, 2020.

\bibitem[Mita et~al.(2021)Mita, Filippone, and Michiardi]{pmlr-v139-mita21a}
G.~Mita, M.~Filippone, and P.~Michiardi.
\newblock An identifiable double vae for disentangled representations.
\newblock In M.~Meila and T.~Zhang, editors, \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 7769--7779. PMLR,
  18--24 Jul 2021.

\bibitem[Moran et~al.(2021)Moran, Sridhar, Wang, and
  Blei]{moran2021identifiable}
G.~E. Moran, D.~Sridhar, Y.~Wang, and D.~M. Blei.
\newblock Identifiable variational autoencoders via sparse decoding.
\newblock \emph{arXiv preprint arXiv:2110.10804}, 2021.

\bibitem[Nalisnick et~al.(2016)Nalisnick, Hertel, and
  Smyth]{nalisnick2016approximate}
E.~Nalisnick, L.~Hertel, and P.~Smyth.
\newblock Approximate inference for deep latent gaussian mixtures.
\newblock In \emph{NIPS Workshop on Bayesian Deep Learning}, volume~2, page
  131, 2016.

\bibitem[Nguyen and McLachlan(2019)]{nguyen2019approximations}
H.~D. Nguyen and G.~McLachlan.
\newblock On approximations via convolution-defined mixture models.
\newblock \emph{Communications in Statistics-Theory and Methods}, 48\penalty0
  (16):\penalty0 3945--3955, 2019.

\bibitem[Pearl and Verma(1992)]{pearl1992statistical}
J.~Pearl and T.~S. Verma.
\newblock A statistical semantics for causation.
\newblock \emph{Statistics and Computing}, 2\penalty0 (2):\penalty0 91--95,
  1992.

\bibitem[Ran and Hu(2017)]{ran2017parameter}
Z.-Y. Ran and B.-G. Hu.
\newblock Parameter identifiability in statistical machine learning: a review.
\newblock \emph{Neural Computation}, 29\penalty0 (5):\penalty0 1151--1203,
  2017.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
D.~J. Rezende, S.~Mohamed, and D.~Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In E.~P. Xing and T.~Jebara, editors, \emph{Proceedings of the 31st
  International Conference on Machine Learning}, volume~32 of \emph{Proceedings
  of Machine Learning Research}, pages 1278--1286, Bejing, China, 22--24 Jun
  2014. PMLR.

\bibitem[Ritchie et~al.(2020)Ritchie, Vandermeulen, and
  Scott]{ritchie2020consistent}
A.~Ritchie, R.~A. Vandermeulen, and C.~Scott.
\newblock Consistent estimation of identifiable nonparametric mixture models
  from grouped observations.
\newblock \emph{arXiv preprint arXiv:2006.07459}, 2020.

\bibitem[Roeder et~al.(2021)Roeder, Metz, and Kingma]{pmlr-v139-roeder21a}
G.~Roeder, L.~Metz, and D.~Kingma.
\newblock On linear identifiability of learned representations.
\newblock In M.~Meila and T.~Zhang, editors, \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 9030--9039. PMLR,
  18--24 Jul 2021.

\bibitem[Sch{\"o}lkopf et~al.(2021)Sch{\"o}lkopf, Locatello, Bauer, Ke,
  Kalchbrenner, Goyal, and Bengio]{scholkopf2021toward}
B.~Sch{\"o}lkopf, F.~Locatello, S.~Bauer, N.~R. Ke, N.~Kalchbrenner, A.~Goyal,
  and Y.~Bengio.
\newblock Toward causal representation learning.
\newblock \emph{Proceedings of the IEEE}, 109\penalty0 (5):\penalty0 612--634,
  2021.

\bibitem[Schott et~al.(2021)Schott, von K{\"u}gelgen, Tr{\"a}uble, Gehler,
  Russell, Bethge, Sch{\"o}lkopf, Locatello, and Brendel]{schott2021visual}
L.~Schott, J.~von K{\"u}gelgen, F.~Tr{\"a}uble, P.~Gehler, C.~Russell,
  M.~Bethge, B.~Sch{\"o}lkopf, F.~Locatello, and W.~Brendel.
\newblock Visual representation learning does not generalize strongly within
  the same domain.
\newblock \emph{arXiv preprint arXiv:2107.08221}, 2021.

\bibitem[Sorrenson et~al.(2019)Sorrenson, Rother, and
  K{\"o}the]{sorrenson2019disentanglement}
P.~Sorrenson, C.~Rother, and U.~K{\"o}the.
\newblock Disentanglement by nonlinear ica with general incompressible-flow
  networks {(GIN)}.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Stock and Gribonval(2021)]{stock2021embedding}
P.~Stock and R.~Gribonval.
\newblock An embedding of relu networks and an analysis of their
  identifiability.
\newblock \emph{arXiv preprint arXiv:2107.09370}, 2021.

\bibitem[Teicher(1963)]{teicher1963identifiability}
H.~Teicher.
\newblock Identifiability of finite mixtures.
\newblock \emph{The annals of Mathematical statistics}, pages 1265--1269, 1963.

\bibitem[Teicher(1967)]{teicher1967}
H.~Teicher.
\newblock Identifiability of mixtures of product measures.
\newblock \emph{The Annals of Mathematical Statistics}, 38\penalty0
  (4):\penalty0 1300--1302, 1967.

\bibitem[Teshima et~al.(2020)Teshima, Ishikawa, Tojo, Oono, Ikeda, and
  Sugiyama]{teshima2020coupling}
T.~Teshima, I.~Ishikawa, K.~Tojo, K.~Oono, M.~Ikeda, and M.~Sugiyama.
\newblock Coupling-based invertible neural networks are universal
  diffeomorphism approximators.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3362--3373, 2020.

\bibitem[Tomczak and Welling(2018)]{tomczak2018vae}
J.~Tomczak and M.~Welling.
\newblock Vae with a vampprior.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1214--1223. PMLR, 2018.

\bibitem[Van Den~Oord et~al.(2017)Van Den~Oord, Vinyals, et~al.]{van2017neural}
A.~Van Den~Oord, O.~Vinyals, et~al.
\newblock Neural discrete representation learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Vandermeulen et~al.(2019)Vandermeulen, Scott,
  et~al.]{vandermeulen2019operator}
R.~A. Vandermeulen, C.~D. Scott, et~al.
\newblock An operator theoretic approach to nonparametric mixture models.
\newblock \emph{Annals of Statistics}, 47\penalty0 (5):\penalty0 2704--2733,
  2019.

\bibitem[Wang et~al.(2021)Wang, Blei, and Cunningham]{wang2021posterior}
Y.~Wang, D.~Blei, and J.~P. Cunningham.
\newblock Posterior collapse and latent variable non-identifiability.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Willetts and Paige(2021)]{willetts2021don}
M.~Willetts and B.~Paige.
\newblock I don't need $\mathbf{u}$: Identifiable non-linear ica without side
  information.
\newblock \emph{arXiv preprint arXiv:2106.05238}, 2021.

\bibitem[Willetts et~al.(2019)Willetts, Roberts, and
  Holmes]{willetts2019disentangling}
M.~Willetts, S.~Roberts, and C.~Holmes.
\newblock Disentangling to cluster: Gaussian mixture variational ladder
  autoencoders.
\newblock \emph{arXiv preprint arXiv:1909.11501}, 2019.

\bibitem[Yacoby et~al.(2020)Yacoby, Pan, and Doshi-Velez]{yacoby2020failure}
Y.~Yacoby, W.~Pan, and F.~Doshi-Velez.
\newblock Failure modes of variational autoencoders and their effects on
  downstream tasks.
\newblock In \emph{ICML Workshop on Uncertainty and Robustness in Deep Learning
  (UDL)}, 2020.

\bibitem[Yang et~al.(2021)Yang, Wang, Sun, Zhang, Zhang, Li, and
  Yan]{yang2021nonlinear}
X.~Yang, Y.~Wang, J.~Sun, X.~Zhang, S.~Zhang, Z.~Li, and J.~Yan.
\newblock Nonlinear ica using volume-preserving transformations.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Zhang and Chan(2008)]{zhang2008minimal}
K.~Zhang and L.~Chan.
\newblock Minimal nonlinear distortion principle for nonlinear independent
  component analysis.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0
  (Nov):\penalty0 2455--2487, 2008.

\bibitem[Zimmermann et~al.(2021)Zimmermann, Sharma, Schneider, Bethge, and
  Brendel]{zimmermann2021contrastive}
R.~S. Zimmermann, Y.~Sharma, S.~Schneider, M.~Bethge, and W.~Brendel.
\newblock Contrastive learning inverts the data generating process.
\newblock In \emph{International Conference on Machine Learning}, pages
  12979--12990. PMLR, 2021.

\end{thebibliography}
