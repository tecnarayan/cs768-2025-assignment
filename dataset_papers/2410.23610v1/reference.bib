@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}


@inproceedings{li2023transformers,
  title={How do transformers learn topic structure: Towards a mechanistic understanding},
  author={Li, Yuchen and Li, Yuanzhi and Risteski, Andrej},
  booktitle={International Conference on Machine Learning},
  pages={19689--19729},
  year={2023},
  organization={PMLR}
}


@inproceedings{akyurek2022learning,
  title={What learning algorithm is in-context learning? Investigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}


@article{huang2023context,
  title={In-context convergence of transformers},
  author={Huang, Yu and Cheng, Yuan and Liang, Yingbin},
  journal={arXiv preprint arXiv:2310.05249},
  year={2023}
}

@inproceedings{
kim2024transformers,
title={Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape},
author={Juno Kim and Taiji Suzuki},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=xm2lU7tteQ}
}

@misc{bordelon2024infinitelimitsmultiheadtransformer,
      title={Infinite Limits of Multi-head Transformer Dynamics}, 
      author={Blake Bordelon and Hamza Tahir Chaudhry and Cengiz Pehlevan},
      year={2024},
      eprint={2405.15712},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2405.15712}, 
}

@article{chen2018neural,
  title={Neural ordinary differential equations},
  author={Chen, Ricky TQ and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{cao2019generalizationsgd,
  title={Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@inproceedings{arora2019fine,
  title={Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019}
}


@inproceedings{allen2018learning,
  title={Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}



@article{liu2020toward,
  title={Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2003.00307},
  year={2020}
}


@inproceedings{chizat2018note,
  title={On Lazy Training in Differentiable Programming},
  author={Chizat, L\'{e}na\"{\i}c and Oyallon, Edouard and Bach, Francis},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}



@inproceedings{zou2019improved,
  title={An Improved Analysis of Training Over-parameterized Deep Neural Networks},
  author={Zou, Difan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@Article{zou2019gradient,
author="Zou, Difan
and Cao, Yuan
and Zhou, Dongruo
and Gu, Quanquan",
title="Gradient descent optimizes over-parameterized deep {ReLU} networks",
journal="Machine Learning",
year="2019",
month="Oct",
day="23"
}


@inproceedings{allen2018convergence,
  title={A Convergence Theory for Deep Learning via Over-Parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  pages={242--252},
  year={2019}
}


@inproceedings{du2018gradientdeep,
  title={Gradient Descent Finds Global Minima of Deep Neural Networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International Conference on Machine Learning},
  pages={1675--1685},
  year={2019}
}



@inproceedings{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}



@article{fang2019convexformulation,
  title={Convex formulation of overparameterized deep neural networks},
  author={Fang, Cong and Gu, Yihong and Zhang, Weizhong and Zhang, Tong},
  journal={IEEE Transactions on Information Theory},
  volume={68},
  number={8},
  pages={5340--5352},
  year={2022},
  publisher={IEEE}
}




@inproceedings{mei2019mean,
  title={Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Conference on Learning Theory},
  year={2019}
}

@article{mei2018mean,
author = {Song Mei  and Andrea Montanari  and Phan-Minh Nguyen },
title = {A mean field view of the landscape of two-layer neural networks},
journal = {Proceedings of the National Academy of Sciences},
volume = {115},
number = {33},
pages = {E7665-E7671},
year = {2018},
doi = {10.1073/pnas.1806579115},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1806579115},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1806579115}}

@inproceedings{chizat2018global,
author = {Chizat, L\'{e}na\"{\i}c and Bach, Francis}, title = {On the global convergence of gradient descent for over-parameterized models using optimal transport}, year = {2018}, publisher = {Curran Associates Inc.}, address = {Red Hook, NY, USA},  booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems}, pages = {3040â€“3050}, numpages = {11}, location = {Montr\'{e}al, Canada}, series = {NIPS'18} }




@article{weinan2017proposal,
  title={A proposal on machine learning via dynamical systems},
  author={Weinan, E},
  journal={Communications in Mathematics and Statistics},
  volume={1},
  number={5},
  pages={1--11},
  year={2017}
}


@inproceedings{li2018optimal,
  title={An optimal control approach to deep learning and applications to discrete-weight neural networks},
  author={Li, Qianxiao and Hao, Shuji},
  booktitle={International Conference on Machine Learning},
  pages={2985--2994},
  year={2018},
  organization={PMLR}
}

@inproceedings{zhang2023trained,
title={Trained Transformers Learn Linear Models In-Context},
author={Ruiqi Zhang and Spencer Frei and Peter Bartlett},
booktitle={R0-FoMo:Robustness of Few-shot and Zero-shot Learning in Large Foundation Models},
year={2023},
url={https://openreview.net/forum?id=MpDSo3Rglq}
}

@article{li2018maximum,
  title={Maximum principle based algorithms for deep learning},
  author={Li, Qianxiao and Chen, Long and Tai, Cheng and E, Weinan},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={165},
  pages={1--29},
  year={2018}
}

@article{han2019mean,
  title={A mean-field optimal control formulation of deep learning},
  author={E, Weinan and Han, Jiequn and Li, Qianxiao},
  journal={Research in the Mathematical Sciences},
  volume={6},
  number={1},
  pages={1--41},
  year={2019},
  publisher={Springer}
}

@article{li2022deep,
  title={Deep learning via dynamical systems: An approximation perspective},
  author={Li, Qianxiao and Lin, Ting and Shen, Zuowei},
  journal={Journal of the European Mathematical Society},
  volume={25},
  number={5},
  pages={1671--1709},
  year={2022}
}

@article{cheng2023interpolation,
  title={Interpolation, approximation and controllability of deep neural networks},
  author={Cheng, Jingpu and Li, Qianxiao and Lin, Ting and Shen, Zuowei},
  journal={arXiv preprint arXiv:2309.06015},
  year={2023}
}












@article{yun2019AreTU,
  title={Are Transformers universal approximators of sequence-to-sequence functions?},
  author={Chulhee Yun and Srinadh Bhojanapalli and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.10077},
  url={https://api.semanticscholar.org/CorpusID:209444410}
}

@misc{kajitsuka2024transformers,
      title={Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?}, 
      author={Tokio Kajitsuka and Issei Sato},
      year={2024},
      eprint={2307.14023},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{kolesnikov2021image,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{fu2023what,
title={What can a Single Attention Layer Learn? A Study Through the Random Features Lens},
author={Hengyu Fu and Tianyu Guo and Yu Bai and Song Mei},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=wX8GuzDSJR}
}

@inproceedings{kim2023provable,
title={Provable Memorization Capacity of Transformers},
author={Junghwan Kim and Michelle Kim and Barzan Mozafari},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=8JCg5xJCTPR}
}

@article{chen2020AGN,
  title={A Generalized Neural Tangent Kernel Analysis for Two-layer Neural Networks},
  author={Zixiang Chen and Yuan Cao and Quanquan Gu and Tong Zhang},
  journal={arXiv: Learning},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:222176800}
}


@InProceedings{takakura2023approximation,
  title = 	 {Approximation and Estimation Ability of Transformers for Sequence-to-Sequence Functions with Infinite Dimensional Input},
  author =       {Takakura, Shokichi and Suzuki, Taiji},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {33416--33447},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/takakura23a/takakura23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/takakura23a.html}
}

@book{villani2008kantorovich,
  title={Optimal Transport, Old and New},
  author={Villani, C.},
  volume={338},
  series={Grundlehren der Mathematischen Wissenschaften},
  publisher={Springer-Verlag},
  year={2008}
}

@inproceedings{ambrosio2005GradientFI,
  booktitle={Gradient Flows},
  title={In Metric Spaces and in the Space of Probability Measures},
  author={Luigi Ambrosio and Nicola Gigli and Giuseppe Savar{\'e}},
  year={2005},
  url={https://api.semanticscholar.org/CorpusID:62824717}
}

@article{richard2000variationalFPE,
author = {Jordan, Richard and Kinderlehrer, David and Otto, Felix},
year = {2000},
month = {04},
pages = {},
title = {The Variational Formulation of the Fokker-Planck Equation},
volume = {29},
journal = {SIAM Journal on Mathematical Analysis},
doi = {10.1137/S0036141096303359}
}

@article{nitanda2022particle,
doi = {10.1088/1742-5468/ac98a8},
url = {https://dx.doi.org/10.1088/1742-5468/ac98a8},
year = {2022},
month = {nov},
publisher = {IOP Publishing and SISSA},
volume = {2022},
number = {11},
pages = {114010},
author = {Atsushi Nitanda and Denny Wu and Taiji Suzuki},
title = {Particle dual averaging: optimization of mean field neural network with global convergence rate analysis*},
journal = {Journal of Statistical Mechanics: Theory and Experiment}
}

@article{otto2001geometry,
author = {Felix Otto},
title = {THE GEOMETRY OF DISSIPATIVE EVOLUTION EQUATIONS: THE POROUS MEDIUM EQUATION},
journal = {Communications in Partial Differential Equations},
volume = {26},
number = {1-2},
pages = {101--174},
year = {2001},
publisher = {Taylor \& Francis},
doi = {10.1081/PDE-100002243},
URL = {https://doi.org/10.1081/PDE-100002243},
eprint = {https://doi.org/10.1081/PDE-100002243}
}

@techreport{frigyik2008frechet,
  author      = {Bela A. Frigyik and Santosh Srivastava and Maya R. Gupta},
  title       = {Introduction to Functional Derivatives},
  institution = {University of Washington Department of Electrical Engineering},
  year        = {2008},
  type        = {UWEE Tech Report},
  number      = {2008-0001},
}


@article{bittner1963adjoint,
  title={The Mathematical Theory of Optimal Processes},
  author={L. S. Pontryagin and V. G. Boltyanskii and R. V. Gamkrelidze and E. F. Mishechenko},
  journal={Zamm-zeitschrift Fur Angewandte Mathematik Und Mechanik},
  year={1963},
  volume={43},
  pages={514-515},
  url={https://api.semanticscholar.org/CorpusID:122390952}
}

@article{santambrogio2016EuclideanMA,
  title={\{Euclidean, metric, and Wasserstein\} gradient flows: an overview},
  author={Filippo Santambrogio},
  journal={Bulletin of Mathematical Sciences},
  year={2016},
  volume={7},
  pages={87-154},
  url={https://api.semanticscholar.org/CorpusID:86851307}
}

@book{atkinson2008introduction,
  title={AN INTRODUCTION TO NUMERICAL ANALYSIS, 2ND ED},
  author={Atkinson, K.E.},
  isbn={9788126518500},
  url={https://books.google.com/books?id=lPV8Fv2XEosC},
  year={2008},
  publisher={Wiley India Pvt. Limited}
}

@incollection{sznitman1991nonlinear,
  title={Topics in propagation of chaos},
  author={Sznitman, Alain-Sol},
  booktitle={Ecole d'Et{\'e} de Probabilit{\'e}s de Saint-Flour XIXâ€”1989},
  pages={165--251},
  year={1991},
  publisher={Springer}
}

@book{risken1996fokker,
  title={The Fokker-Planck Equation: Methods of Solution and Applications},
  author={Risken, H.},
  pages={63--95},
  publisher={Springer},
  year={1996}
}

@misc{nitanda2017stochastic,
      title={Stochastic Particle Gradient Descent for Infinite Ensembles}, 
      author={Atsushi Nitanda and Taiji Suzuki},
      year={2017},
      eprint={1712.05438},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@InProceedings{lu2020meanfield,
  title = 	 {A Mean Field Analysis Of Deep {R}es{N}et And Beyond: Towards Provably Optimization Via Overparameterization From Depth},
  author =       {Lu, Yiping and Ma, Chao and Lu, Yulong and Lu, Jianfeng and Ying, Lexing},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6426--6436},
  year = 	 {2020},
  editor = 	 {III, Hal DaumÃ© and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/lu20b/lu20b.pdf},
  url = 	 {https://proceedings.mlr.press/v119/lu20b.html}
}

@inproceedings{luong2015effective,
    title = "Effective Approaches to Attention-based Neural Machine Translation",
    author = "Luong, Thang  and
      Pham, Hieu  and
      Manning, Christopher D.",
    editor = "M{\`a}rquez, Llu{\'\i}s  and
      Callison-Burch, Chris  and
      Su, Jian",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1166",
    doi = "10.18653/v1/D15-1166",
    pages = "1412--1421",
}

@inproceedings{bai2023transformers,
title={Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection},
author={Yu Bai and Fan Chen and Huan Wang and Caiming Xiong and Song Mei},
booktitle={Workshop on Efficient Systems for Foundation Models @ ICML2023},
year={2023},
url={https://openreview.net/forum?id=vlCG5HKEkI}
}

@inproceedings{barboni2022meanfield,
title={On global convergence of ResNets: From finite to infinite width using linear parameterization},
author={Rapha{\"e}l Barboni and Gabriel Peyr{\'e} and Fran{\c{c}}ois-Xavier Vialard},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=XVfOai2ytN1}
}

@misc{ding2021meanfield1,
      title={On the Global Convergence of Gradient Descent for multi-layer ResNets in the mean-field regime}, 
      author={Zhiyan Ding and Shi Chen and Qin Li and Stephen Wright},
      year={2021},
      eprint={2110.02926},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{ding2022meanfield2,
  title={Overparameterization of deep resnet: Zero loss and mean-field analysis},
  author={Zhiyan Ding and Shi Chen and Qin Li and Stephen Wright},
  journal={Journal of Machine Learning Research},
  volume={23},
  pages={48-1},
  year={2022}
}

@inproceedings{chen2024generalization,
title={Generalization of Scaled Deep ResNets in the Mean-Field Regime},
author={Yihang Chen and Fanghui Liu and Yiping Lu and Grigorios Chrysos and Volkan Cevher},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=tMzPZTvz2H}
}

@inproceedings{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{
mahdavi2024memorization,
title={Memorization Capacity of Multi-Head Attention in Transformers},
author={Sadegh Mahdavi and Renjie Liao and Christos Thrampoulidis},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=MrR3rMxqqv}
}

@inproceedings{tay2020synthesizer,
  title={Synthesizer: Rethinking Self-Attention for Transformer Models},
  author={Yi Tay and Dara Bahri and Donald Metzler and Da-Cheng Juan and Zhe Zhao and Che Zheng},
  booktitle={International Conference on Machine Learning},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:218487423}
}

@misc{bernhard2023alternatives,
      title={Alternatives to the Scaled Dot Product for Attention in the Transformer Neural Network Architecture}, 
      author={James Bernhard},
      year={2023},
      eprint={2311.09406},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{guo2024how,
title={How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations},
author={Tianyu Guo and Wei Hu and Song Mei and Huan Wang and Caiming Xiong and Silvio Savarese and Yu Bai},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=ikwEDva1JZ}
}

@inproceedings{lin2024transformers,
title={Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining},
author={Licong Lin and Yu Bai and Song Mei},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=yN4Wv17ss3}
}

@inproceedings{wei2018regularization,
  title={Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel},
  author={Colin Wei and J. Lee and Qiang Liu and Tengyu Ma},
  booktitle={Neural Information Processing Systems},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:170078599}
}

@article{weinan2019machine,
  title={Machine learning from a continuous viewpoint, I},
  author={E Weinan and Chao Ma and Lei Wu},
  journal={Science China Mathematics},
  year={2019},
  volume={63},
  pages={2233 - 2266},
  url={https://api.semanticscholar.org/CorpusID:209515941}
}

@misc{kim2021lipschitz,
title={The Lipschitz Constant of Self-Attention},
author={Hyunjik Kim and George Papamakarios and Andriy Mnih},
year={2021},
url={https://openreview.net/forum?id=DHSNrGhAY7W}
}


@InProceedings{george2021lipschitz,
  title = 	 {Lipschitz normalization for self-attention layers with application to graph neural networks},
  author =       {Dasoulas, George and Scaman, Kevin and Virmaux, Aladin},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2456--2466},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/dasoulas21a/dasoulas21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/dasoulas21a.html}
}

@article{vuckovic2020lipschitz,
  title={A Mathematical Theory of Attention},
  author={James Vuckovic and Aristide Baratin and R{\'e}mi Tachet des Combes},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.02876},
  url={https://api.semanticscholar.org/CorpusID:220363994}
}

@article{micchelli2006universal,
  title={Universal Kernels},
  author={Charles A. Micchelli and Yuesheng Xu and Haizhang Zhang},
  journal={J. Mach. Learn. Res.},
  year={2006},
  volume={7},
  pages={2651-2667},
  url={https://api.semanticscholar.org/CorpusID:17808083}
}

@misc{edelman2022inductive,
title={Inductive Biases and Variable Creation in Self-Attention Mechanisms},
author={Benjamin L. Edelman and Surbhi Goel and Sham M. Kakade and Cyril Zhang},
year={2022},
url={https://openreview.net/forum?id=UjynxfqnGWG}
}

@article{fang2019ntk,
  title={Over Parameterized Two-level Neural Networks Can Learn Near Optimal Feature Representations},
  author={Cong Fang and Hanze Dong and Tong Zhang},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.11508},
  url={https://api.semanticscholar.org/CorpusID:204915902}
}

@article{santambrogio2015optimal,
  title={Optimal transport for applied mathematicians},
  author={Santambrogio, Filippo},
  journal={Birk{\"a}user, NY},
  volume={55},
  number={58-63},
  pages={94},
  year={2015},
  publisher={Springer}
}

@article{veit2016residual,
  title={Residual networks behave like ensembles of relatively shallow networks},
  author={Veit, Andreas and Wilber, Michael J and Belongie, Serge},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{ying2021do,
title={Do Transformers Really Perform Badly for Graph Representation?},
author={Chengxuan Ying and Tianle Cai and Shengjie Luo and Shuxin Zheng and Guolin Ke and Di He and Yanming Shen and Tie-Yan Liu},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=OeWooOxFwDa}
}

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:52967399}
}
@article{achiam2023gpt4,
  added-at = {2023-10-23T16:08:31.000+0200},
  author = {OpenAI},
  biburl = {https://www.bibsonomy.org/bibtex/2b87062f1a9478148d2e5dd0006c9c455/tomvoelker},
  description = {Technical report detailing the development of GPT-4, a multimodal model capable of handling both image and text inputs. The model achieved human-level performance on various benchmarks, including scoring in the top 10% on a simulated bar exam. The study highlights the importance of the post-training alignment process for enhancing the model's accuracy and behavior.},
  interhash = {241e35649065841f159e6105eb87b1d3},
  intrahash = {b87062f1a9478148d2e5dd0006c9c455},
  journal = {ArXiv},
  keywords = {gpt-4 openai transformer multimodal bar_exam alignment_process paper_demo posted_with_chatgpt},
  timestamp = {2023-10-23T16:08:31.000+0200},
  title = {GPT-4 Technical Report},
  url = {https://arxiv.org/abs/2303.08774},
  volume = {abs/2303.08774},
  year = 2023
}

@inproceedings{brown2020icl,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth{\'e}e Lacroix and Baptiste Rozi{\`e}re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.13971},
  url={https://api.semanticscholar.org/CorpusID:257219404}
}

@inproceedings{jelassi2022vision,
 author = {Jelassi, Samy and Sander, Michael and Li, Yuanzhi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {37822--37836},
 publisher = {Curran Associates, Inc.},
 title = {Vision Transformers provably learn spatial structure},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/f69707de866eb0805683d3521756b73f-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}



@InProceedings{dehghani2023vision,
  title = 	 {Scaling Vision Transformers to 22 Billion Parameters},
  author =       {Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas Peter and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and Jenatton, Rodolphe and Beyer, Lucas and Tschannen, Michael and Arnab, Anurag and Wang, Xiao and Riquelme Ruiz, Carlos and Minderer, Matthias and Puigcerver, Joan and Evci, Utku and Kumar, Manoj and Steenkiste, Sjoerd Van and Elsayed, Gamaleldin Fathy and Mahendran, Aravindh and Yu, Fisher and Oliver, Avital and Huot, Fantine and Bastings, Jasmijn and Collier, Mark and Gritsenko, Alexey A. and Birodkar, Vighnesh and Vasconcelos, Cristina Nader and Tay, Yi and Mensink, Thomas and Kolesnikov, Alexander and Pavetic, Filip and Tran, Dustin and Kipf, Thomas and Lucic, Mario and Zhai, Xiaohua and Keysers, Daniel and Harmsen, Jeremiah J. and Houlsby, Neil},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {7480--7512},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR}
}

@article{funahashi1989OnTA,
  title={On the approximate realization of continuous mappings by neural networks},
  author={Ken-ichi Funahashi},
  journal={Neural Networks},
  year={1989},
  volume={2},
  pages={183-192},
  url={https://api.semanticscholar.org/CorpusID:10203109}
}

@article{cybenko1989ApproximationBS,
  title={Approximation by superpositions of a sigmoidal function},
  author={George V. Cybenko},
  journal={Mathematics of Control, Signals and Systems},
  year={1989},
  volume={2},
  pages={303-314},
  url={https://api.semanticscholar.org/CorpusID:3958369}
}

@article{pinkus1999ApproximationTO,
  title={Approximation theory of the MLP model in neural networks},
  author={Allan Pinkus},
  journal={Acta Numerica},
  year={1999},
  volume={8},
  pages={143 - 195},
  url={https://api.semanticscholar.org/CorpusID:16800260}
}