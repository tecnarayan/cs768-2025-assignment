\begin{thebibliography}{10}

\bibitem{alvarez2016learning}
Jose~M Alvarez and Mathieu Salzmann.
\newblock Learning the number of neurons in deep networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2270--2278, 2016.

\bibitem{alvarez2017compression}
Jose~M Alvarez and Mathieu Salzmann.
\newblock Compression-aware training of deep networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  856--867, 2017.

\bibitem{ba2014deep}
Jimmy Ba and Rich Caruana.
\newblock Do deep nets really need to be deep?
\newblock In {\em Advances in neural information processing systems}, pages
  2654--2662, 2014.

\bibitem{DBLP:conf/iclr/2015}
Yoshua Bengio and Yann LeCun, editors.
\newblock {\em 3rd International Conference on Learning Representations, {ICLR}
  2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings}, 2015.

\bibitem{DBLP:conf/iclr/2019}
Yoshua Bengio and Yann LeCun, editors.
\newblock {\em 7th International Conference on Learning Representations, {ICLR}
  2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.

\bibitem{boyd2011distributed}
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et~al.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock {\em Foundations and Trends{\textregistered} in Machine learning},
  3(1):1--122, 2011.

\bibitem{castellano1997iterative}
Giovanna Castellano, Anna~Maria Fanelli, and Marcello Pelillo.
\newblock An iterative pruning algorithm for feedforward neural networks.
\newblock {\em IEEE transactions on Neural networks}, 8(3):519--531, 1997.

\bibitem{courbariaux2016binarized}
Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua
  Bengio.
\newblock Binarized neural networks: Training deep neural networks with weights
  and activations constrained to+ 1 or-1.
\newblock {\em arXiv preprint arXiv:1602.02830}, 2016.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em Computer Vision and Pattern Recognition, 2009. CVPR 2009.
  IEEE Conference on}, pages 248--255. IEEE, 2009.

\bibitem{ding2019centripetal}
Xiaohan Ding, Guiguang Ding, Yuchen Guo, and Jungong Han.
\newblock Centripetal sgd for pruning very deep convolutional networks with
  complicated structure.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4943--4953, 2019.

\bibitem{ding2019approximated}
Xiaohan Ding, Guiguang Ding, Yuchen Guo, Jungong Han, and Chenggang Yan.
\newblock Approximated oracle filter pruning for destructive cnn width
  optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  1607--1616, 2019.

\bibitem{ding2018auto}
Xiaohan Ding, Guiguang Ding, Jungong Han, and Sheng Tang.
\newblock Auto-balanced filter pruning for efficient convolutional neural
  networks.
\newblock In {\em Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem{dong2017learning}
Xin Dong, Shangyu Chen, and Sinno Pan.
\newblock Learning to prune deep neural networks via layer-wise optimal brain
  surgeon.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4857--4867, 2017.

\bibitem{figurnov2016perforatedcnns}
Mikhail Figurnov, Aizhan Ibraimova, Dmitry~P Vetrov, and Pushmeet Kohli.
\newblock Perforatedcnns: Acceleration through elimination of redundant
  convolutions.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  947--955, 2016.

\bibitem{DBLP:conf/iclr/FrankleC19}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In Bengio and LeCun \cite{DBLP:conf/iclr/2019}.

\bibitem{goh2017why}
Gabriel Goh.
\newblock Why momentum really works.
\newblock {\em Distill}, 2017.

\bibitem{gordon2018morphnet}
Ariel Gordon, Elad Eban, Ofir Nachum, Bo~Chen, Hao Wu, Tien-Ju Yang, and Edward
  Choi.
\newblock Morphnet: Fast \& simple resource-constrained structure learning of
  deep networks.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1586--1595, 2018.

\bibitem{guo2016dynamic}
Yiwen Guo, Anbang Yao, and Yurong Chen.
\newblock Dynamic network surgery for efficient dnns.
\newblock In {\em Advances In Neural Information Processing Systems}, pages
  1379--1387, 2016.

\bibitem{gupta2015deep}
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
\newblock Deep learning with limited numerical precision.
\newblock In {\em International Conference on Machine Learning}, pages
  1737--1746, 2015.

\bibitem{han2015deep}
Song Han, Huizi Mao, and William~J. Dally.
\newblock Deep compression: Compressing deep neural network with pruning,
  trained quantization and huffman coding.
\newblock In Yoshua Bengio and Yann LeCun, editors, {\em 4th International
  Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico,
  May 2-4, 2016, Conference Track Proceedings}, 2016.

\bibitem{han2015learning}
Song Han, Jeff Pool, John Tran, and William Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1135--1143, 2015.

\bibitem{hassibi1993second}
Babak Hassibi and David~G Stork.
\newblock Second order derivatives for network pruning: Optimal brain surgeon.
\newblock In {\em Advances in neural information processing systems}, pages
  164--171, 1993.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{he2017channel}
Yihui He, Xiangyu Zhang, and Jian Sun.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In {\em International Conference on Computer Vision (ICCV)},
  volume~2, page~6, 2017.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{hu2016network}
Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang.
\newblock Network trimming: A data-driven neuron pruning approach towards
  efficient deep architectures.
\newblock {\em arXiv preprint arXiv:1607.03250}, 2016.

\bibitem{huang2017densely}
Gao Huang, Zhuang Liu, Laurens van~der Maaten, and Kilian~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em 2017 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017}, pages
  2261--2269. {IEEE} Computer Society, 2017.

\bibitem{kathuria2018}
Ayoosh Kathuria.
\newblock Intro to optimization in deep learning: Momentum, rmsprop and adam.
\newblock
  \url{https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/},
  2018.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{lecun1990optimal}
Yann LeCun, John~S Denker, and Sara~A Solla.
\newblock Optimal brain damage.
\newblock In {\em Advances in neural information processing systems}, pages
  598--605, 1990.

\bibitem{li2016pruning}
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans~Peter Graf.
\newblock Pruning filters for efficient convnets.
\newblock {\em arXiv preprint arXiv:1608.08710}, 2016.

\bibitem{lin2019towards}
Shaohui Lin, Rongrong Ji, Yuchao Li, Cheng Deng, and Xuelong Li.
\newblock Towards compact convnets via structure-sparsity regularized filter
  pruning.
\newblock {\em arXiv preprint arXiv:1901.07827}, 2019.

\bibitem{liu2015sparse}
Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky.
\newblock Sparse convolutional neural networks.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 806--814, 2015.

\bibitem{liu2019metapruning}
Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Tim Kwang-Ting
  Cheng, and Jian Sun.
\newblock Metapruning: Meta learning for automatic neural network channel
  pruning.
\newblock {\em arXiv preprint arXiv:1903.10258}, 2019.

\bibitem{liu2018bi}
Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng.
\newblock Bi-real net: Enhancing the performance of 1-bit cnns with improved
  representational capability and advanced training algorithm.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 722--737, 2018.

\bibitem{liu2017learning}
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui
  Zhang.
\newblock Learning efficient convolutional networks through network slimming.
\newblock In {\em 2017 IEEE International Conference on Computer Vision
  (ICCV)}, pages 2755--2763. IEEE, 2017.

\bibitem{liu2019rethinking}
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell.
\newblock Rethinking the value of network pruning.
\newblock In Bengio and LeCun \cite{DBLP:conf/iclr/2019}.

\bibitem{luo2017thinet}
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin.
\newblock Thinet: A filter level pruning method for deep neural network
  compression.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 5058--5066, 2017.

\bibitem{mathieu2013fast}
Micha{\"{e}}l Mathieu, Mikael Henaff, and Yann LeCun.
\newblock Fast training of convolutional networks through ffts.
\newblock In Yoshua Bengio and Yann LeCun, editors, {\em 2nd International
  Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April
  14-16, 2014, Conference Track Proceedings}, 2014.

\bibitem{molchanov2016pruning}
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock In {\em 5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.

\bibitem{polyak1964some}
Boris~T Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock {\em USSR Computational Mathematics and Mathematical Physics},
  4(5):1--17, 1964.

\bibitem{romero2014fitnets}
Adriana Romero, Nicolas Ballas, Samira~Ebrahimi Kahou, Antoine Chassang, Carlo
  Gatta, and Yoshua Bengio.
\newblock Fitnets: Hints for thin deep nets.
\newblock In Bengio and LeCun \cite{DBLP:conf/iclr/2015}.

\bibitem{roth2008group}
Volker Roth and Bernd Fischer.
\newblock The group-lasso for generalized linear models: uniqueness of
  solutions and efficient algorithms.
\newblock In {\em Proceedings of the 25th international conference on Machine
  learning}, pages 848--855. ACM, 2008.

\bibitem{rutishauser1959theory}
Heinz Rutishauser.
\newblock Theory of gradient methods.
\newblock In {\em Refined iterative methods for computation of the solution and
  the eigenvalues of self-adjoint boundary value problems}, pages 24--49.
  Springer, 1959.

\bibitem{sainath2013low}
Tara~N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana
  Ramabhadran.
\newblock Low-rank matrix factorization for deep neural network training with
  high-dimensional output targets.
\newblock In {\em Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE
  International Conference on}, pages 6655--6659. IEEE, 2013.

\bibitem{srinivas2017training}
Suraj Srinivas, Akshayvarun Subramanya, and R~Venkatesh~Babu.
\newblock Training sparse neural networks.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 138--145, 2017.

\bibitem{sutskever2013importance}
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In {\em International conference on machine learning}, pages
  1139--1147, 2013.

\bibitem{theis2018faster}
Lucas Theis, Iryna Korshunova, Alykhan Tejani, and Ferenc Husz{\'a}r.
\newblock Faster gaze prediction with dense networks and fisher pruning.
\newblock {\em arXiv preprint arXiv:1801.05787}, 2018.

\bibitem{vasilache2014fast}
Nicolas Vasilache, Jeff Johnson, Micha{\"{e}}l Mathieu, Soumith Chintala,
  Serkan Piantino, and Yann LeCun.
\newblock Fast convolutional nets with fbfft: {A} {GPU} performance evaluation.
\newblock In Bengio and LeCun \cite{DBLP:conf/iclr/2015}.

\bibitem{wang2018structured}
Huan Wang, Qiming Zhang, Yuehai Wang, and Haoji Hu.
\newblock Structured pruning for efficient convnets via incremental
  regularization.
\newblock {\em arXiv preprint arXiv:1811.08390}, 2018.

\bibitem{wang2017beyond}
Yunhe Wang, Chang Xu, Chao Xu, and Dacheng Tao.
\newblock Beyond filters: Compact feature map for portable deep model.
\newblock In {\em International Conference on Machine Learning}, pages
  3703--3711, 2017.

\bibitem{wang2016cnnpack}
Yunhe Wang, Chang Xu, Shan You, Dacheng Tao, and Chao Xu.
\newblock Cnnpack: Packing convolutional neural networks in the frequency
  domain.
\newblock In {\em Advances in neural information processing systems}, pages
  253--261, 2016.

\bibitem{wen2016learning}
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Learning structured sparsity in deep neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2074--2082, 2016.

\bibitem{yang2017designing}
Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze.
\newblock Designing energy-efficient convolutional neural networks using
  energy-aware pruning.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 5687--5695, 2017.

\bibitem{zhang2018systematic}
Tianyun Zhang, Shaokai Ye, Kaiqi Zhang, Jian Tang, Wujie Wen, Makan Fardad, and
  Yanzhi Wang.
\newblock A systematic dnn weight pruning framework using alternating direction
  method of multipliers.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 184--199, 2018.

\bibitem{zhang2016accelerating}
Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun.
\newblock Accelerating very deep convolutional networks for classification and
  detection.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  38(10):1943--1955, 2016.

\end{thebibliography}
