\begin{thebibliography}{10}

\bibitem{tensorflow2015}
Mart\'{\i}n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
  Craig Citro, Greg~S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,
  Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
  Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
  Levenberg, Dandelion Man\'{e}, Rajat Monga, Sherry Moore, Derek Murray, Chris
  Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal
  Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\'{e}gas,
  Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and
  Xiaoqiang Zheng.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock Software available from \url{https://www.tensorflow.org/}.

\bibitem{libsvmdataset}
Chih-Chung Chang and Chih-Jen Lin.
\newblock {LIBSVM : a library for support vector machines}.
\newblock {\em ACM Transactions on Intelligent Systems and Technology}, 2011.
\newblock Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}.

\bibitem{criteopressrelease}
{Criteo Labs}.
\newblock Criteo releases industryâ€™s largest-ever dataset for machine
  learning to academic community, 2015.
\newblock
  \url{https://www.criteo.com/news/press-releases/2015/07/criteo-releases-industrys-largest-ever-dataset/}.

\bibitem{criteo_criteo}
{Criteo Labs}.
\newblock {Learning Click-Through Rate at Scale with Tensorflow on Spark},
  2018.
\newblock \url{https://github.com/criteo/CriteoDisplayCTR-TFOnSpark}.

\bibitem{pyforj}
Barthelemy Dagenais.
\newblock Py4j: A bridge between python and java, 2018.
\newblock Software available at \url{https://www.py4j.org}.

\bibitem{CoCoAccel2017}
Celestine D{\"u}nner, Thomas Parnell, Kubilay Atasu, Manolis Sifalakis, and
  Haris Pozidis.
\newblock Understanding optimizing distributed machine learning applications on
  apache spark.
\newblock In {\em Proceedings of the IEEE International Conference on Big
  Data}, IEEEBigData'17, pages 99--100, Boston, MA, December 2017.

\bibitem{DuHL2017}
Celestine D{\"u}nner, Thomas Parnell, and Martin Jaggi.
\newblock Efficient use of limited memory accelerators for linear learning on
  heterogeneous systems.
\newblock In {\em Advances in Neural Information Processing Systems 30},
  NIPS'17, pages 4261--4270, Long Beach, CA, December 2017.

\bibitem{fan2008liblinear}
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
\newblock Liblinear: A library for large linear classification.
\newblock {\em Journal of machine learning research}, 9(Aug):1871--1874, 2008.

\bibitem{Marsaglia2003}
George Marsaglia.
\newblock Xorshift rngs.
\newblock {\em Journal of Statistical Software, Articles}, 8(14), 2003.

\bibitem{spark-ml}
Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkataraman,
  Davies Liu, Jeremy Freeman, DB~Tsai, Manish Amde, Sean Owen, Doris Xin,
  Reynold Xin, Michael~J. Franklin, Reza Zadeh, Matei Zaharia, and Ameet
  Talwalkar.
\newblock Mllib: Machine learning in apache spark.
\newblock {\em J. Mach. Learn. Res.}, 17(1):1235--1241, January 2016.

\bibitem{google_cloud_examples}
Gonzalo~Gasca Meza.
\newblock Samples for google cloud machine learning engine, 2017.
\newblock \url{https://github.com/GoogleCloudPlatform/cloudml-samples }.

\bibitem{NVTHRUST}
NVIDIA.
\newblock Thrust, 2018.
\newblock Software available at \url{https://developer.nvidia.com/thrust}.

\bibitem{tensorflow_batching_issue}
Stack Overflow.
\newblock Use large dataset in tensorflow, 2016.
\newblock
  \url{https://stackoverflow.com/questions/38087342/use-large-dataset-in-tensorflow}.

\bibitem{tpascd18}
Thomas Parnell, Celestine D{\"u}nner, Kubilay Atasu, Manolis Sifalakis, and
  Haralampous Pozidis.
\newblock Tera-scale coordinate descent on gpus.
\newblock {\em Future Generation Computer Systems}, 0(0):0, 2018.

\bibitem{TPASCD2017}
Thomas Parnell, Celestine D{\"u}nner, Kubilay Atasu, Manolis Sifalakis, and
  Haris Pozidis.
\newblock Large-scale stochastic learning using gpus.
\newblock In {\em Proceedings of the IEEE International Parallel and
  Distributed Processing Symposium Workshops}, IPDPS'17, pages 419--428,
  Orlando, FL, May 2017.

\bibitem{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock {\em Journal of Machine Learning Research}, 12:2825--2830, 2011.

\bibitem{NVCUB}
NVIDIA Research.
\newblock Cub, 2018.
\newblock \url{https://github.com/NVlabs/cub}.

\bibitem{cocoa}
Virginia Smith, Simone Forte, Chenxin Ma, Martin Tak{\'a}{\v c}, Michael~I
  Jordan, and Martin Jaggi.
\newblock {CoCoA: A General Framework for Communication-Efficient Distributed
  Optimization}.
\newblock {\em JMLR}, 18:1--49, 2018.

\bibitem{rambler1tbbenchmark}
Rambler~Digital Solutions.
\newblock criteo-1tb-benchmark, 2017.
\newblock
  \url{https://github.com/rambler-digital-solutions/criteo-1tb-benchmark}.

\bibitem{google_criteo_results}
Andreas Sterbenz.
\newblock Using google cloud machine learning to predict clicks at scale, 2017.
\newblock
  \url{https://cloud.google.com/blog/big-data/2017/02/using-google-cloud-machine-learning-to-predict-clicks-at-scale}.
  Online; Accessed: 2018-01-25.

\bibitem{yu2012large}
Hsiang-Fu Yu, Cho-Jui Hsieh, Kai-Wei Chang, and Chih-Jen Lin.
\newblock Large linear classification when data cannot fit in memory.
\newblock {\em ACM Transactions on Knowledge Discovery from Data (TKDD)},
  5(4):23, 2012.

\bibitem{Yu2011}
Hsiang-Fu Yu, Fang-Lan Huang, and Chih-Jen Lin.
\newblock Dual coordinate descent methods for logistic regression and maximum
  entropy models.
\newblock {\em Machine Learning}, 85(1):41--75, Oct 2011.

\bibitem{zhang2016fixing}
Huan Zhang and Cho-Jui Hsieh.
\newblock Fixing the convergence problems in parallel asynchronous dual
  coordinate descent.
\newblock In {\em Data Mining (ICDM), 2016 IEEE 16th International Conference
  on}, pages 619--628. IEEE, 2016.

\end{thebibliography}
