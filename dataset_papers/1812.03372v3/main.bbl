\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Azizzadenesheli et~al.(2019)Azizzadenesheli, Liu, Yang, and
  Anandkumar]{azizzadenesheli2019regularized}
Azizzadenesheli, K., Liu, A., Yang, F., and Anandkumar, A.
\newblock Regularized learning for domain adaptation under label shifts.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Bengio et~al.(2009)Bengio, Louradour, Collobert, and
  Weston]{bengio2009curriculum}
Bengio, Y., Louradour, J., Collobert, R., and Weston, J.
\newblock Curriculum learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2009.

\bibitem[Beygelzimer et~al.(2009)Beygelzimer, Dasgupta, and
  Langford]{Beygelzimer2009importance}
Beygelzimer, A., Dasgupta, S., and Langford, J.
\newblock Importance weighted active learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2009.

\bibitem[Burda et~al.(2015)Burda, Grosse, and
  Salakhutdinov]{burda2015importance}
Burda, Y., Grosse, R., and Salakhutdinov, R.
\newblock Importance weighted autoencoders.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M., Lee, K., and Toutanova, K.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{CoRR}, abs/1810.04805, 2018.

\bibitem[Dolan \& Brockett(2005)Dolan and Brockett]{dolan2005automatically}
Dolan, B. and Brockett, C.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In \emph{Third International Workshop on Paraphrasing (IWP2005)},
  2005.

\bibitem[Gretton et~al.(2009)Gretton, Smola, Huang, Schmittfull, Borgwardt, and
  Sch{\"o}lkopf]{gretton2009covariate}
Gretton, A., Smola, A.~J., Huang, J., Schmittfull, M., Borgwardt, K.~M., and
  Sch{\"o}lkopf, B.
\newblock Covariate shift by kernel mean matching.
\newblock \emph{Journal of Machine Learning Research}, 2009.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018implicit}
Gunasekar, S., Lee, J., Soudry, D., and Srebro, N.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock \emph{arXiv preprint arXiv:1806.00468}, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2016.

\bibitem[Horvitz \& Thompson(1952)Horvitz and
  Thompson]{horvitz1952generalization}
Horvitz, D.~G. and Thompson, D.~J.
\newblock A generalization of sampling without replacement from a finite
  universe.
\newblock \emph{Journal of the American statistical Association (JASA)}, 1952.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2015.

\bibitem[Jiang et~al.(2015)Jiang, Meng, Zhao, Shan, and
  Hauptmann]{jiang2015self}
Jiang, L., Meng, D., Zhao, Q., Shan, S., and Hauptmann, A.
\newblock Self-paced curriculum learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2015.

\bibitem[Joachims et~al.(2018)Joachims, Swaminathan, and
  Rijke]{joachims2018deep}
Joachims, T., Swaminathan, A., and Rijke, M.~d.
\newblock Deep learning with logged bandit feedback.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Kahn \& Marshall(1953)Kahn and Marshall]{kahn1953methods}
Kahn, H. and Marshall, A.~W.
\newblock Methods of reducing sample size in monte carlo computations.
\newblock \emph{Journal of the Operations Research Society of America}, 1953.

\bibitem[Khetan et~al.(2018)Khetan, Lipton, and Anandkumar]{khetan2017learning}
Khetan, A., Lipton, Z.~C., and Anandkumar, A.
\newblock Learning from noisy singly-labeled data.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2015adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem[Koller et~al.(2009)Koller, Friedman, and
  Bach]{koller2009probabilistic}
Koller, D., Friedman, N., and Bach, F.
\newblock \emph{Probabilistic graphical models: principles and techniques}.
\newblock MIT press, 2009.

\bibitem[Kostrikov et~al.(2019)Kostrikov, Agrawal, Levine, and
  Tompson]{kostrikov2018addressing}
Kostrikov, I., Agrawal, K.~K., Levine, S., and Tompson, J.
\newblock Addressing sample inefficiency and reward bias in inverse
  reinforcement learning.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019.

\bibitem[Lipton et~al.(2018)Lipton, Wang, and Smola]{lipton2018detecting}
Lipton, Z.~C., Wang, Y.-X., and Smola, A.
\newblock Detecting and correcting for label shift with black box predictors.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Mahmood et~al.(2014)Mahmood, van Hasselt, and
  Sutton]{mahmood2014weighted}
Mahmood, A.~R., van Hasselt, H.~P., and Sutton, R.~S.
\newblock Weighted importance sampling for off-policy learning with linear
  function approximation.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2014.

\bibitem[Matiisen et~al.(2017)Matiisen, Oliver, Cohen, and
  Schulman]{matiisen2017teacher}
Matiisen, T., Oliver, A., Cohen, T., and Schulman, J.
\newblock Teacher-student curriculum learning.
\newblock \emph{CoRR}, abs/1707.00183, 2017.

\bibitem[Murali et~al.(2016)Murali, Garg, Krishnan, Pokorny, Abbeel, Darrell,
  and Goldberg]{murali2016tsc}
Murali, A., Garg, A., Krishnan, S., Pokorny, F.~T., Abbeel, P., Darrell, T.,
  and Goldberg, K.
\newblock Tsc-dl: Unsupervised trajectory segmentation of multi-modal surgical
  demonstrations with deep learning.
\newblock In \emph{Robotics and Automation (ICRA)}, 2016.

\bibitem[Precup(2000)]{precup2000eligibility}
Precup, D.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock \emph{Computer Science Department Faculty Publication Series}, 2000.

\bibitem[Rubinstein \& Kroese(2016)Rubinstein and
  Kroese]{rubinstein2016simulation}
Rubinstein, R.~Y. and Kroese, D.~P.
\newblock \emph{Simulation and the Monte Carlo method}.
\newblock John Wiley \& Sons, 2016.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and
  Silver]{schaul2015prioritized}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[Settles(2010)]{settles2010active}
Settles, B.
\newblock Active learning literature survey.
\newblock Technical report, University of Wisconsin-Madison, 2010.

\bibitem[Shalit et~al.(2017)Shalit, Johansson, and
  Sontag]{shalit2017estimating}
Shalit, U., Johansson, F.~D., and Sontag, D.
\newblock Estimating individual treatment effect: generalization bounds and
  algorithms.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Shimodaira(2000)]{shimodaira2000improving}
Shimodaira, H.
\newblock Improving predictive inference under covariate shift by weighting the
  log-likelihood function.
\newblock \emph{Journal of statistical planning and inference}, 2000.

\bibitem[Soudry et~al.(2017)Soudry, Hoffer, and Srebro]{soudry2017implicit}
Soudry, D., Hoffer, E., and Srebro, N.
\newblock The implicit bias of gradient descent on separable data.
\newblock In \emph{Inernational Conference on Learning Representations (ICLR)},
  2017.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2015dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research}, 2014.

\bibitem[Swaminathan \& Joachims(2015)Swaminathan and
  Joachims]{swaminathan2015counterfactual}
Swaminathan, A. and Joachims, T.
\newblock Counterfactual risk minimization: Learning from logged bandit
  feedback.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2015.

\bibitem[Wolf \& Sanh(2018)Wolf and Sanh]{wolf2018pytorch}
Wolf, T. and Sanh, V.
\newblock Pytorch pretrained bert.
\newblock \url{https://github.com/huggingface/pytorch-pretrained-BERT}, 2018.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\end{thebibliography}
