@inproceedings{NIPS2012_16c222aa,
 author = {Wilson, Aaron and Fern, Alan and Tadepalli, Prasad},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Bayesian Approach for Policy Learning from Trajectory Preference Queries},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/16c222aa19898e5058938167c8ab6c57-Paper.pdf},
 volume = {25},
 year = {2012}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{DBLP:journals/corr/abs-1811-06521,
      title={Reward learning from human preferences and demonstrations in Atari}, 
      author={Borja Ibarz and Jan Leike and Tobias Pohlen and Geoffrey Irving and Shane Legg and Dario Amodei},
      year={2018},
      eprint={1811.06521},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{DBLP:journals/corr/abs-2106-05091,
      title={PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training}, 
      author={Kimin Lee and Laura Smith and Pieter Abbeel},
      year={2021},
      eprint={2106.05091},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{bradley_terry,
  title={Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons},
  author={Ralph Allan Bradley and Milton E. Terry},
  journal={Biometrika},
  year={1952},
  volume={39},
  pages={324},
  url={https://api.semanticscholar.org/CorpusID:125209808}
}



@article{andrychowicz2017hindsight,
  author       = {Marcin Andrychowicz and
                  Filip Wolski and
                  Alex Ray and
                  Jonas Schneider and
                  Rachel Fong and
                  Peter Welinder and
                  Bob McGrew and
                  Josh Tobin and
                  Pieter Abbeel and
                  Wojciech Zaremba},
  title        = {Hindsight Experience Replay},
  journal      = {CoRR},
  volume       = {abs/1707.01495},
  year         = {2017},
  url          = {http://arxiv.org/abs/1707.01495},
  eprinttype    = {arXiv},
  eprint       = {1707.01495},
  timestamp    = {Fri, 08 Nov 2019 12:51:04 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/AndrychowiczWRS17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@article{DBLP:journals/corr/abs-1801-01290,
  author       = {Tuomas Haarnoja and
                  Aurick Zhou and
                  Pieter Abbeel and
                  Sergey Levine},
  title        = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
                  with a Stochastic Actor},
  journal      = {CoRR},
  volume       = {abs/1801.01290},
  year         = {2018},
  url          = {http://arxiv.org/abs/1801.01290},
  eprinttype    = {arXiv},
  eprint       = {1801.01290},
  timestamp    = {Mon, 13 Aug 2018 16:48:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1801-01290.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{nachum2018data,
  title={Data-efficient hierarchical reinforcement learning},
  author={Nachum, Ofir and Gu, Shixiang Shane and Lee, Honglak and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{levy2018learning,
  title={Learning Multi-Level Hierarchies with Hindsight},
  author={Levy, Andrew and Konidaris, George and Platt, Robert and Saenko, Kate},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{chane2021goal,
  title={Goal-conditioned reinforcement learning with imagined subgoals},
  author={Chane-Sane, Elliot and Schmid, Cordelia and Laptev, Ivan},
  booktitle={International Conference on Machine Learning},
  pages={1430--1440},
  year={2021},
  organization={PMLR}
}



@article{DBLP:journals/corr/MnihKSGAWR13,
  author       = {Volodymyr Mnih and
                  Koray Kavukcuoglu and
                  David Silver and
                  Alex Graves and
                  Ioannis Antonoglou and
                  Daan Wierstra and
                  Martin A. Riedmiller},
  title        = {Playing Atari with Deep Reinforcement Learning},
  journal      = {CoRR},
  volume       = {abs/1312.5602},
  year         = {2013},
  url          = {http://arxiv.org/abs/1312.5602},
  eprinttype    = {arXiv},
  eprint       = {1312.5602},
  timestamp    = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{DBLP:journals/corr/abs-1709-10087,
  author       = {Aravind Rajeswaran and
                  Vikash Kumar and
                  Abhishek Gupta and
                  John Schulman and
                  Emanuel Todorov and
                  Sergey Levine},
  title        = {Learning Complex Dexterous Manipulation with Deep Reinforcement Learning
                  and Demonstrations},
  journal      = {CoRR},
  volume       = {abs/1709.10087},
  year         = {2017},
  url          = {http://arxiv.org/abs/1709.10087},
  eprinttype    = {arXiv},
  eprint       = {1709.10087},
  timestamp    = {Thu, 20 Dec 2018 16:30:14 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1709-10087.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1806-10293,
  author       = {Dmitry Kalashnikov and
                  Alex Irpan and
                  Peter Pastor and
                  Julian Ibarz and
                  Alexander Herzog and
                  Eric Jang and
                  Deirdre Quillen and
                  Ethan Holly and
                  Mrinal Kalakrishnan and
                  Vincent Vanhoucke and
                  Sergey Levine},
  title        = {QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic
                  Manipulation},
  journal      = {CoRR},
  volume       = {abs/1806.10293},
  year         = {2018},
  url          = {http://arxiv.org/abs/1806.10293},
  eprinttype    = {arXiv},
  eprint       = {1806.10293},
  timestamp    = {Mon, 13 Aug 2018 16:48:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1806-10293.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/GuHLL16,
  author       = {Shixiang Gu and
                  Ethan Holly and
                  Timothy P. Lillicrap and
                  Sergey Levine},
  title        = {Deep Reinforcement Learning for Robotic Manipulation},
  journal      = {CoRR},
  volume       = {abs/1610.00633},
  year         = {2016},
  url          = {http://arxiv.org/abs/1610.00633},
  eprinttype    = {arXiv},
  eprint       = {1610.00633},
  timestamp    = {Mon, 13 Aug 2018 16:49:14 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/GuHLL16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/LevineFDA15,
  author       = {Sergey Levine and
                  Chelsea Finn and
                  Trevor Darrell and
                  Pieter Abbeel},
  title        = {End-to-End Training of Deep Visuomotor Policies},
  journal      = {CoRR},
  volume       = {abs/1504.00702},
  year         = {2015},
  url          = {http://arxiv.org/abs/1504.00702},
  eprinttype    = {arXiv},
  eprint       = {1504.00702},
  timestamp    = {Mon, 13 Aug 2018 16:47:04 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/LevineFDA15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{dayan1992feudal,
  title={Feudal reinforcement learning},
  author={Dayan, Peter and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={5},
  year={1992}
}

@inproceedings{vezhnevets2017feudal,
  title={Feudal networks for hierarchical reinforcement learning},
  author={Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
  booktitle={International Conference on Machine Learning},
  pages={3540--3549},
  year={2017},
  organization={PMLR}
}

@inproceedings{nair2018overcoming,
  title={Overcoming exploration in reinforcement learning with demonstrations},
  author={Nair, Ashvin and McGrew, Bob and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle={2018 IEEE international conference on robotics and automation (ICRA)},
  pages={6292--6299},
  year={2018},
  organization={IEEE}
}

@article{sutton1999between,
  title={Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artificial intelligence},
  volume={112},
  number={1-2},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}

@article{klissarov2017learnings,
  title={Learnings options end-to-end for continuous action tasks},
  author={Klissarov, Martin and Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
  journal={arXiv preprint arXiv:1712.00004},
  year={2017}
}

@inproceedings{harb2018waiting,
  title={When waiting is not an option: Learning options with a deliberation cost},
  author={Harb, Jean and Bacon, Pierre-Luc and Klissarov, Martin and Precup, Doina},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{nachum2019does,
  title={Why does hierarchy (sometimes) work so well in reinforcement learning?},
  author={Nachum, Ofir and Tang, Haoran and Lu, Xingyu and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
  journal={arXiv preprint arXiv:1909.10618},
  year={2019}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{dalal2021accelerating,
  title={Accelerating robotic reinforcement learning via parameterized action primitives},
  author={Dalal, Murtaza and Pathak, Deepak and Salakhutdinov, Russ R},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={21847--21859},
  year={2021}
}

@article{kostrikov2018discriminator,
  title={Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial imitation learning},
  author={Kostrikov, Ilya and Agrawal, Kumar Krishna and Dwibedi, Debidatta and Levine, Sergey and Tompson, Jonathan},
  journal={arXiv preprint arXiv:1809.02925},
  year={2018}
}
@article{gupta2019relay,
  title={Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning},
  author={Gupta, Abhishek and Kumar, Vikash and Lynch, Corey and Levine, Sergey and Hausman, Karol},
  journal={arXiv preprint arXiv:1910.11956},
  year={2019}
}

@article{Barto03recentadvances,
  title={Recent Advances in Hierarchical Reinforcement Learning},
  author={Andrew G. Barto and Sridhar Mahadevan},
  journal={Discrete Event Dynamic Systems},
  year={2003},
  volume={13},
  pages={341-379}
}

@inproceedings{NIPS1997_5ca3e9b1,
 author = {Parr, Ronald and Russell, Stuart},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Jordan and M. Kearns and S. Solla},
 pages = {},
 publisher = {MIT Press},
 title = {Reinforcement Learning with Hierarchies of Machines},
 volume = {10},
 year = {1998}
}

@article{DBLP:journals/corr/cs-LG-9905014,
  author    = {Thomas G. Dietterich},
  title     = {Hierarchical Reinforcement Learning with the {MAXQ} Value Function
               Decomposition},
  journal   = {CoRR},
  volume    = {cs.LG/9905014},
  year      = {1999},
  url       = {https://arxiv.org/abs/cs/9905014},
  timestamp = {Fri, 10 Jan 2020 12:59:31 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/cs-LG-9905014.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2110-03655,
  author       = {Soroush Nasiriany and
                  Huihan Liu and
                  Yuke Zhu},
  title        = {Augmenting Reinforcement Learning with Behavior Primitives for Diverse
                  Manipulation Tasks},
  journal      = {CoRR},
  volume       = {abs/2110.03655},
  year         = {2021},
  url          = {https://arxiv.org/abs/2110.03655},
  eprinttype    = {arXiv},
  eprint       = {2110.03655},
  timestamp    = {Thu, 21 Oct 2021 16:20:08 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2110-03655.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{knox2009interactively,
  title={Interactively shaping agents via human reinforcement: The TAMER framework},
  author={Knox, W Bradley and Stone, Peter},
  booktitle={Proceedings of the fifth international conference on Knowledge capture},
  pages={9--16},
  year={2009}
}

@inproceedings{pilarski2011online,
  title={Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning},
  author={Pilarski, Patrick M and Dawson, Michael R and Degris, Thomas and Fahimi, Farbod and Carey, Jason P and Sutton, Richard S},
  booktitle={2011 IEEE international conference on rehabilitation robotics},
  pages={1--7},
  year={2011},
  organization={IEEE}
}

@article{wilson2012bayesian,
  title={A bayesian approach for policy learning from trajectory preference queries},
  author={Wilson, Aaron and Fern, Alan and Tadepalli, Prasad},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@article{daniel2015active,
  title={Active reward learning with a novel acquisition function},
  author={Daniel, Christian and Kroemer, Oliver and Viering, Malte and Metz, Jan and Peters, Jan},
  journal={Autonomous Robots},
  volume={39},
  pages={389--405},
  year={2015},
  publisher={Springer}
}

@inproceedings{warnell2018deep,
  title={Deep tamer: Interactive agent shaping in high-dimensional state spaces},
  author={Warnell, Garrett and Waytowich, Nicholas and Lawhern, Vernon and Stone, Peter},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}
@inproceedings{ng1999policy,
  title={Policy invariance under reward transformations: Theory and application to reward shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  booktitle={Icml},
  volume={99},
  pages={278--287},
  year={1999},
  organization={Citeseer}
}

@article{popov2017data,
  title={Data-efficient deep reinforcement learning for dexterous manipulation},
  author={Popov, Ivaylo and Heess, Nicolas and Lillicrap, Timothy and Hafner, Roland and Barth-Maron, Gabriel and Vecerik, Matej and Lampe, Thomas and Tassa, Yuval and Erez, Tom and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1704.03073},
  year={2017}
}
@article{cao2020human,
  title={Human preference scaling with demonstrations for deep reinforcement learning},
  author={Cao, Zehong and Wong, Kaichiu and Lin, Chin-Teng},
  journal={arXiv preprint arXiv:2007.12904},
  year={2020}
}
@article{nachum2018near,
  title={Near-optimal representation learning for hierarchical reinforcement learning},
  author={Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
  journal={arXiv preprint arXiv:1810.01257},
  year={2018}
}
@inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K and others},
  booktitle={Aaai},
  volume={8},
  pages={1433--1438},
  year={2008},
  organization={Chicago, IL, USA}
}


@article{levine2018reinforcement,
  title={Reinforcement learning and control as probabilistic inference: Tutorial and review},
  author={Levine, Sergey},
  journal={arXiv preprint arXiv:1805.00909},
  year={2018}
}