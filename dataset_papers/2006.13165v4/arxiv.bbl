\begin{thebibliography}{38}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

\bibitem[{Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l and
  Szepesv{\'a}ri}]{abbasi2011improved}
\textsc{Abbasi-Yadkori, Y.}, \textsc{P{\'a}l, D.} and \textsc{Szepesv{\'a}ri,
  C.} (2011).
\newblock Improved algorithms for linear stochastic bandits.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang and
  Yang}]{ayoub2020model}
\textsc{Ayoub, A.}, \textsc{Jia, Z.}, \textsc{Szepesvari, C.}, \textsc{Wang,
  M.} and \textsc{Yang, L.~F.} (2020).
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock \textit{arXiv preprint arXiv:2006.01107} .

\bibitem[{Azar et~al.(2013)Azar, Munos and Kappen}]{azar2013minimax}
\textsc{Azar, M.~G.}, \textsc{Munos, R.} and \textsc{Kappen, H.~J.} (2013).
\newblock Minimax pac bounds on the sample complexity of reinforcement learning
  with a generative model.
\newblock \textit{Machine learning} \textbf{91} 325--349.

\bibitem[{Azar et~al.(2017)Azar, Osband and Munos}]{azar2017minimax}
\textsc{Azar, M.~G.}, \textsc{Osband, I.} and \textsc{Munos, R.} (2017).
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \textit{Proceedings of the 34th International Conference on
  Machine Learning-Volume 70}. JMLR. org.

\bibitem[{Bertsekas(2018)}]{bertsekas2018feature}
\textsc{Bertsekas, D.~P.} (2018).
\newblock Feature-based aggregation and deep reinforcement learning: A survey
  and some new implementations.
\newblock \textit{IEEE/CAA Journal of Automatica Sinica} \textbf{6} 1--31.

\bibitem[{Besson and Kaufmann(2018)}]{besson2018doubling}
\textsc{Besson, L.} and \textsc{Kaufmann, E.} (2018).
\newblock What doubling tricks can and can't do for multi-armed bandits.
\newblock \textit{arXiv preprint arXiv:1803.06971} .

\bibitem[{Bhandari et~al.(2018)Bhandari, Russo and Singal}]{bhandari2018finite}
\textsc{Bhandari, J.}, \textsc{Russo, D.} and \textsc{Singal, R.} (2018).
\newblock A finite time analysis of temporal difference learning with linear
  function approximation.
\newblock \textit{arXiv preprint arXiv:1806.02450} .

\bibitem[{Boyd et~al.(2004)Boyd, Boyd and Vandenberghe}]{boyd2004convex}
\textsc{Boyd, S.}, \textsc{Boyd, S.~P.} and \textsc{Vandenberghe, L.} (2004).
\newblock \textit{Convex optimization}.
\newblock Cambridge university press.

\bibitem[{Dani et~al.(2008)Dani, Hayes and Kakade}]{dani2008stochastic}
\textsc{Dani, V.}, \textsc{Hayes, T.~P.} and \textsc{Kakade, S.~M.} (2008).
\newblock Stochastic linear optimization under bandit feedback .

\bibitem[{Dong et~al.(2019)Dong, Wang, Chen and Wang}]{dong2019q}
\textsc{Dong, K.}, \textsc{Wang, Y.}, \textsc{Chen, X.} and \textsc{Wang, L.}
  (2019).
\newblock Q-learning with ucb exploration is sample efficient for
  infinite-horizon mdp.
\newblock \textit{arXiv preprint arXiv:1901.09311} .

\bibitem[{Du et~al.(2019)Du, Kakade, Wang and Yang}]{du2019good}
\textsc{Du, S.~S.}, \textsc{Kakade, S.~M.}, \textsc{Wang, R.} and \textsc{Yang,
  L.~F.} (2019).
\newblock Is a good representation sufficient for sample efficient
  reinforcement learning?
\newblock \textit{arXiv preprint arXiv:1910.03016} .

\bibitem[{Jaksch et~al.(2010)Jaksch, Ortner and Auer}]{jaksch2010near}
\textsc{Jaksch, T.}, \textsc{Ortner, R.} and \textsc{Auer, P.} (2010).
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \textit{Journal of Machine Learning Research} \textbf{11} 1563--1600.

\bibitem[{Jia et~al.(2020)Jia, Yang, Szepesvari and Wang}]{jia2020model}
\textsc{Jia, Z.}, \textsc{Yang, L.}, \textsc{Szepesvari, C.} and \textsc{Wang,
  M.} (2020).
\newblock Model-based reinforcement learning with value-targeted regression .

\bibitem[{Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford and
  Schapire}]{jiang2017contextual}
\textsc{Jiang, N.}, \textsc{Krishnamurthy, A.}, \textsc{Agarwal, A.},
  \textsc{Langford, J.} and \textsc{Schapire, R.~E.} (2017).
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In \textit{Proceedings of the 34th International Conference on
  Machine Learning-Volume 70}. JMLR. org.

\bibitem[{Jin et~al.(2019)Jin, Yang, Wang and Jordan}]{jin2019provably}
\textsc{Jin, C.}, \textsc{Yang, Z.}, \textsc{Wang, Z.} and \textsc{Jordan,
  M.~I.} (2019).
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock \textit{arXiv preprint arXiv:1907.05388} .

\bibitem[{Kakade et~al.(2003)}]{kakade2003sample}
\textsc{Kakade, S.~M.} \textsc{et~al.} (2003).
\newblock \textit{On the sample complexity of reinforcement learning}.
\newblock Ph.D. thesis.

\bibitem[{Kearns and Singh(1999)}]{kearns1999finite}
\textsc{Kearns, M.~J.} and \textsc{Singh, S.~P.} (1999).
\newblock Finite-sample convergence rates for q-learning and indirect
  algorithms.
\newblock In \textit{Advances in neural information processing systems}.

\bibitem[{Lattimore and Hutter(2012)}]{lattimore2012pac}
\textsc{Lattimore, T.} and \textsc{Hutter, M.} (2012).
\newblock Pac bounds for discounted mdps.
\newblock In \textit{International Conference on Algorithmic Learning Theory}.
  Springer.

\bibitem[{Lattimore and Szepesv{\'a}ri(2018)}]{lattimore2018bandit}
\textsc{Lattimore, T.} and \textsc{Szepesv{\'a}ri, C.} (2018).
\newblock Bandit algorithms.
\newblock \textit{preprint}  28.

\bibitem[{Lattimore and Szepesvari(2019)}]{lattimore2019learning}
\textsc{Lattimore, T.} and \textsc{Szepesvari, C.} (2019).
\newblock Learning with good feature representations in bandits and in rl with
  a generative model.
\newblock \textit{arXiv preprint arXiv:1911.07676} .

\bibitem[{Liu and Su(2020)}]{liu2020regret}
\textsc{Liu, S.} and \textsc{Su, H.} (2020).
\newblock Regret bounds for discounted mdps.

\bibitem[{Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski et~al.}]{mnih2015human}
\textsc{Mnih, V.}, \textsc{Kavukcuoglu, K.}, \textsc{Silver, D.}, \textsc{Rusu,
  A.~A.}, \textsc{Veness, J.}, \textsc{Bellemare, M.~G.}, \textsc{Graves, A.},
  \textsc{Riedmiller, M.}, \textsc{Fidjeland, A.~K.}, \textsc{Ostrovski, G.}
  \textsc{et~al.} (2015).
\newblock Human-level control through deep reinforcement learning.
\newblock \textit{Nature} \textbf{518} 529--533.

\bibitem[{Modi et~al.(2019)Modi, Jiang, Tewari and Singh}]{modi2019sample}
\textsc{Modi, A.}, \textsc{Jiang, N.}, \textsc{Tewari, A.} and \textsc{Singh,
  S.} (2019).
\newblock Sample complexity of reinforcement learning using linearly combined
  model ensembles.
\newblock \textit{arXiv preprint arXiv:1910.10597} .

\bibitem[{Osband and Van~Roy(2016)}]{osband2016lower}
\textsc{Osband, I.} and \textsc{Van~Roy, B.} (2016).
\newblock On lower bounds for regret in reinforcement learning.
\newblock \textit{arXiv preprint arXiv:1608.02732} .

\bibitem[{Russo and Van~Roy(2013)}]{russo2013eluder}
\textsc{Russo, D.} and \textsc{Van~Roy, B.} (2013).
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock In \textit{NIPS}. Citeseer.

\bibitem[{Sidford et~al.(2018{\natexlab{a}})Sidford, Wang, Wu, Yang and
  Ye}]{sidford2018near}
\textsc{Sidford, A.}, \textsc{Wang, M.}, \textsc{Wu, X.}, \textsc{Yang, L.~F.}
  and \textsc{Ye, Y.} (2018{\natexlab{a}}).
\newblock Near-optimal time and sample complexities for for solving discounted
  markov decision process with a generative model.
\newblock \textit{arXiv preprint arXiv:1806.01492} .

\bibitem[{Sidford et~al.(2018{\natexlab{b}})Sidford, Wang, Wu and
  Ye}]{sidford2018variance}
\textsc{Sidford, A.}, \textsc{Wang, M.}, \textsc{Wu, X.} and \textsc{Ye, Y.}
  (2018{\natexlab{b}}).
\newblock Variance reduced value iteration and faster algorithms for solving
  markov decision processes.
\newblock In \textit{Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium
  on Discrete Algorithms}. SIAM.

\bibitem[{Singh et~al.(1995)Singh, Jaakkola and
  Jordan}]{singh1995reinforcement}
\textsc{Singh, S.~P.}, \textsc{Jaakkola, T.} and \textsc{Jordan, M.~I.} (1995).
\newblock Reinforcement learning with soft state aggregation.
\newblock In \textit{Advances in neural information processing systems}.

\bibitem[{Strehl et~al.(2006)Strehl, Li, Wiewiora, Langford and
  Littman}]{strehl2006pac}
\textsc{Strehl, A.~L.}, \textsc{Li, L.}, \textsc{Wiewiora, E.},
  \textsc{Langford, J.} and \textsc{Littman, M.~L.} (2006).
\newblock Pac model-free reinforcement learning.
\newblock In \textit{Proceedings of the 23rd international conference on
  Machine learning}. ACM.

\bibitem[{Szita and Szepesv{\'a}ri(2010)}]{szita2010model}
\textsc{Szita, I.} and \textsc{Szepesv{\'a}ri, C.} (2010).
\newblock Model-based reinforcement learning with nearly tight exploration
  complexity bounds .

\bibitem[{Van~Roy and Dong(2019)}]{van2019comments}
\textsc{Van~Roy, B.} and \textsc{Dong, S.} (2019).
\newblock Comments on the du-kakade-wang-yang lower bounds.
\newblock \textit{arXiv preprint arXiv:1911.07910} .

\bibitem[{Wang et~al.(2019)Wang, Wang, Du and Krishnamurthy}]{wang2019optimism}
\textsc{Wang, Y.}, \textsc{Wang, R.}, \textsc{Du, S.~S.} and
  \textsc{Krishnamurthy, A.} (2019).
\newblock Optimism in reinforcement learning with generalized linear function
  approximation.
\newblock \textit{arXiv preprint arXiv:1912.04136} .

\bibitem[{Watkins(1989)}]{watkins1989learning}
\textsc{Watkins, C. J. C.~H.} (1989).
\newblock Learning from delayed rewards .

\bibitem[{Yang and Wang(2019{\natexlab{a}})}]{yang2019sample}
\textsc{Yang, L.} and \textsc{Wang, M.} (2019{\natexlab{a}}).
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Yang and Wang(2019{\natexlab{b}})}]{yang2019reinforcement}
\textsc{Yang, L.~F.} and \textsc{Wang, M.} (2019{\natexlab{b}}).
\newblock Reinforcement leaning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock \textit{arXiv preprint arXiv:1905.10389} .

\bibitem[{Zanette et~al.(2020)Zanette, Lazaric, Kochenderfer and
  Brunskill}]{zanette2020learning}
\textsc{Zanette, A.}, \textsc{Lazaric, A.}, \textsc{Kochenderfer, M.} and
  \textsc{Brunskill, E.} (2020).
\newblock Learning near optimal policies with low inherent bellman error.
\newblock \textit{arXiv preprint arXiv:2003.00153} .

\bibitem[{Zheng et~al.(2018)Zheng, Zhang, Zheng, Xiang, Yuan, Xie and
  Li}]{zheng2018drn}
\textsc{Zheng, G.}, \textsc{Zhang, F.}, \textsc{Zheng, Z.}, \textsc{Xiang, Y.},
  \textsc{Yuan, N.~J.}, \textsc{Xie, X.} and \textsc{Li, Z.} (2018).
\newblock Drn: A deep reinforcement learning framework for news recommendation.
\newblock In \textit{Proceedings of the 2018 World Wide Web Conference}.

\bibitem[{Zou et~al.(2019)Zou, Xu and Liang}]{zou2019finite}
\textsc{Zou, S.}, \textsc{Xu, T.} and \textsc{Liang, Y.} (2019).
\newblock Finite-sample analysis for sarsa with linear function approximation.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\end{thebibliography}
