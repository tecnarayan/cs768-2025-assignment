\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von
  Arx, Bernstein, Bohg, Bosselut, Brunskill,
  et~al.]{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
  and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using
  model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Rasley et~al.(2020)Rasley, Rajbhandari, Ruwase, and
  He]{rasley2020deepspeed}
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.
\newblock Deepspeed: System optimizations enable training deep learning models
  with over 100 billion parameters.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 3505--3506, 2020.

\bibitem[Baines et~al.(2021)Baines, Bhosale, Caggiano, Goyal, Goyal, Ott,
  Lefaudeux, Liptchinsky, Rabbat, Sheiffer, et~al.]{baines2021fairscale}
Mandeep Baines, Shruti Bhosale, Vittorio Caggiano, Naman Goyal, Siddharth
  Goyal, Myle Ott, Benjamin Lefaudeux, Vitaliy Liptchinsky, Mike Rabbat, Sam
  Sheiffer, et~al.
\newblock Fairscale: A general purpose modular pytorch library for high
  performance and large scale training, 2021.

\bibitem[Xu et~al.(2021)Xu, Lee, Chen, Hechtman, Huang, Joshi, Krikun,
  Lepikhin, Ly, Maggioni, et~al.]{xu2021gspmd}
Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul
  Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et~al.
\newblock Gspmd: general and scalable parallelization for ml computation
  graphs.
\newblock \emph{arXiv preprint arXiv:2105.04663}, 2021.

\bibitem[Zheng et~al.(2022)Zheng, Li, Zhang, Zhuang, Chen, Huang, Wang, Xu,
  Zhuo, Gonzalez, et~al.]{zheng2022alpa}
Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping
  Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Joseph~E Gonzalez, et~al.
\newblock Alpa: Automating inter-and intra-operator parallelism for distributed
  deep learning.
\newblock \emph{arXiv preprint arXiv:2201.12023}, 2022.

\bibitem[Li et~al.(2021)Li, Zhuang, Guo, Zhuo, Zhang, Song, and
  Stoica]{li2021terapipe}
Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, Dawn Song, and
  Ion Stoica.
\newblock Terapipe: Token-level pipeline parallelism for training large-scale
  language models.
\newblock In \emph{International Conference on Machine Learning}, pages
  6543--6552. PMLR, 2021.

\bibitem[Rajbhandari et~al.(2020)Rajbhandari, Rasley, Ruwase, and
  He]{rajbhandari2020zero}
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.
\newblock Zero: Memory optimizations toward training trillion parameter models.
\newblock In \emph{SC20: International Conference for High Performance
  Computing, Networking, Storage and Analysis}, pages 1--16. IEEE, 2020.

\bibitem[Ren et~al.(2021)Ren, Rajbhandari, Aminabadi, Ruwase, Yang, Zhang, Li,
  and He]{ren2021zero}
Jie Ren, Samyam Rajbhandari, Reza~Yazdani Aminabadi, Olatunji Ruwase, Shuangyan
  Yang, Minjia Zhang, Dong Li, and Yuxiong He.
\newblock $\{$ZeRO-Offload$\}$: Democratizing $\{$Billion-Scale$\}$ model
  training.
\newblock In \emph{2021 USENIX Annual Technical Conference (USENIX ATC 21)},
  pages 551--564, 2021.

\bibitem[gpu()]{gpu_increase}
Q4â€™21 sees a nominal rise in gpu and pc shipments quarter-to-quarter.
\newblock
  \url{https://www.jonpeddie.com/press-releases/q421-sees-a-nominal-rise-in-gpu-and-pc-shipments-quarter-to-quarter}.

\bibitem[Shirts and Pande(2000)]{shirts2000screen}
Michael Shirts and Vijay~S Pande.
\newblock Screen savers of the world unite!
\newblock \emph{Science}, 290\penalty0 (5498):\penalty0 1903--1904, 2000.

\bibitem[fol(2022)]{folding_stats}
{OS Statistics}.
\newblock \url{https://stats.foldingathome.org/os}, 2022.
\newblock [Online; accessed 15-May-2022].

\bibitem[eco()]{economic_mining}
Gpu economics cost analysis.
\newblock
  \url{https://venturebeat.com/2018/02/25/the-real-cost-of-mining-ethereum/}.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017can}
Xiangru Lian, Ce~Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Koloskova et~al.(2019)Koloskova, Stich, and
  Jaggi]{koloskova2019decentralized}
Anastasia Koloskova, Sebastian Stich, and Martin Jaggi.
\newblock Decentralized stochastic optimization and gossip algorithms with
  compressed communication.
\newblock In \emph{International Conference on Machine Learning}, pages
  3478--3487. PMLR, 2019.

\bibitem[Koloskova et~al.(2020)Koloskova, Loizou, Boreiri, Jaggi, and
  Stich]{koloskova2020unified}
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian
  Stich.
\newblock A unified theory of decentralized sgd with changing topology and
  local updates.
\newblock In \emph{International Conference on Machine Learning}, pages
  5381--5393. PMLR, 2020.

\bibitem[Ryabinin and Gusev(2020)]{ryabinin2020towards}
Max Ryabinin and Anton Gusev.
\newblock Towards crowdsourced training of large neural networks using
  decentralized mixture-of-experts.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3659--3672, 2020.

\bibitem[Diskin et~al.(2021)Diskin, Bukhtiyarov, Ryabinin, Saulnier, Sinitsin,
  Popov, Pyrkin, Kashirin, Borzunov, Villanova~del Moral,
  et~al.]{diskin2021distributed}
Michael Diskin, Alexey Bukhtiyarov, Max Ryabinin, Lucile Saulnier, Anton
  Sinitsin, Dmitry Popov, Dmitry~V Pyrkin, Maxim Kashirin, Alexander Borzunov,
  Albert Villanova~del Moral, et~al.
\newblock Distributed deep learning in open collaborations.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 7879--7897, 2021.

\bibitem[Du et~al.(2021)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu,
  Firat, Zoph, Fedus, Bosma, Zhou, Wang, Wang, Webster, Pellat, Robinson,
  Meier{-}Hellstern, Duke, Dixon, Zhang, Le, Wu, Chen, and Cui]{glam}
Nan Du, Yanping Huang, Andrew~M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong
  Xu, Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, Barret Zoph, Liam
  Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu~Emma Wang, Kellie Webster,
  Marie Pellat, Kevin Robinson, Kathy Meier{-}Hellstern, Toju Duke, Lucas
  Dixon, Kun Zhang, Quoc~V. Le, Yonghui Wu, Zhifeng Chen, and Claire Cui.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock \emph{CoRR}, abs/2112.06905, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.06905}.

\bibitem[Narayanan et~al.(2019)Narayanan, Harlap, Phanishayee, Seshadri,
  Devanur, Ganger, Gibbons, and Zaharia]{narayanan2019pipedream}
Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil~R
  Devanur, Gregory~R Ganger, Phillip~B Gibbons, and Matei Zaharia.
\newblock Pipedream: generalized pipeline parallelism for dnn training.
\newblock In \emph{Proceedings of the 27th ACM Symposium on Operating Systems
  Principles}, pages 1--15, 2019.

\bibitem[Tarnawski et~al.(2020)Tarnawski, Phanishayee, Devanur, Mahajan, and
  Nina~Paravecino]{tarnawski2020efficient}
Jakub~M Tarnawski, Amar Phanishayee, Nikhil Devanur, Divya Mahajan, and Fanny
  Nina~Paravecino.
\newblock Efficient algorithms for device placement of dnn graph operators.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15451--15463, 2020.

\bibitem[Tarnawski et~al.(2021)Tarnawski, Narayanan, and
  Phanishayee]{tarnawski2021piper}
Jakub~M Tarnawski, Deepak Narayanan, and Amar Phanishayee.
\newblock Piper: Multidimensional planner for dnn parallelization.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Fan et~al.(2021)Fan, Rong, Meng, Cao, Wang, Zheng, Wu, Long, Yang,
  Xia, et~al.]{fan2021dapple}
Shiqing Fan, Yi~Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen Zheng, Chuan Wu,
  Guoping Long, Jun Yang, Lixue Xia, et~al.
\newblock Dapple: A pipelined data parallel approach for training large models.
\newblock In \emph{Proceedings of the 26th ACM SIGPLAN Symposium on Principles
  and Practice of Parallel Programming}, pages 431--445, 2021.

\bibitem[Park et~al.(2020)Park, Yun, Chang, Nguyen, Lee, Choi, Noh, and
  Choi]{park2020hetpipe}
Jay~H Park, Gyeongchan Yun, M~Yi Chang, Nguyen~T Nguyen, Seungmin Lee, Jaesik
  Choi, Sam~H Noh, and Young-ri Choi.
\newblock $\{$HetPipe$\}$: Enabling large $\{$DNN$\}$ training on (whimpy)
  heterogeneous $\{$GPU$\}$ clusters through integration of pipelined model
  parallelism and data parallelism.
\newblock In \emph{2020 USENIX Annual Technical Conference (USENIX ATC 20)},
  pages 307--321, 2020.

\bibitem[Papadimitriou(1977)]{papadimitriou1977euclidean}
Christos~H Papadimitriou.
\newblock The euclidean travelling salesman problem is np-complete.
\newblock \emph{Theoretical computer science}, 4\penalty0 (3):\penalty0
  237--244, 1977.

\bibitem[Kang and Moon(2000)]{kang2000hybrid}
So-Jin Kang and Byung-Ro Moon.
\newblock A hybrid genetic algorithm for multiway graph partitioning.
\newblock In \emph{Proceedings of the 2nd Annual Conference on Genetic and
  Evolutionary Computation}, pages 159--166. Citeseer, 2000.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Sergeev and Del~Balso(2018)]{sergeev2018horovod}
Alexander Sergeev and Mike Del~Balso.
\newblock Horovod: fast and easy distributed deep learning in tensorflow.
\newblock \emph{arXiv preprint arXiv:1802.05799}, 2018.

\bibitem[Jiang et~al.(2020)Jiang, Zhu, Lan, Yi, Cui, and Guo]{jiang2020unified}
Yimin Jiang, Yibo Zhu, Chang Lan, Bairen Yi, Yong Cui, and Chuanxiong Guo.
\newblock A unified architecture for accelerating distributed $\{$DNN$\}$
  training in heterogeneous $\{$GPU/CPU$\}$ clusters.
\newblock In \emph{14th USENIX Symposium on Operating Systems Design and
  Implementation (OSDI 20)}, pages 463--479, 2020.

\bibitem[Goemans(2009)]{goemans2009lecture}
Michel~X Goemans.
\newblock Lecture notes on bipartite matching.
\newblock \emph{Massachusetts Institute of Technology}, 2009.

\bibitem[Garey and Johnson(1979)]{garey1979computers}
Michael~R Garey and David~S Johnson.
\newblock \emph{Computers and intractability}, volume 174.
\newblock freeman San Francisco, 1979.

\bibitem[Andreev and Racke(2006)]{andreev2006balanced}
Konstantin Andreev and Harald Racke.
\newblock Balanced graph partitioning.
\newblock \emph{Theory of Computing Systems}, 39\penalty0 (6):\penalty0
  929--939, 2006.

\bibitem[Sanders and Schulz(2013)]{sanders2013think}
Peter Sanders and Christian Schulz.
\newblock Think locally, act globally: Highly balanced graph partitioning.
\newblock In \emph{International Symposium on Experimental Algorithms}, pages
  164--175. Springer, 2013.

\bibitem[Bulu{\c{c}} et~al.(2016)Bulu{\c{c}}, Meyerhenke, Safro, Sanders, and
  Schulz]{bulucc2016recent}
Ayd{\i}n Bulu{\c{c}}, Henning Meyerhenke, Ilya Safro, Peter Sanders, and
  Christian Schulz.
\newblock Recent advances in graph partitioning.
\newblock \emph{Algorithm engineering}, pages 117--158, 2016.

\bibitem[El-Mihoub et~al.(2006)El-Mihoub, Hopgood, Nolle, and
  Battersby]{el2006hybrid}
Tarek~A El-Mihoub, Adrian~A Hopgood, Lars Nolle, and Alan Battersby.
\newblock Hybrid genetic algorithms: A review.
\newblock \emph{Eng. Lett.}, 13\penalty0 (2):\penalty0 124--137, 2006.

\bibitem[Bui and Moon(1996)]{bui1996genetic}
Thang~Nguyen Bui and Byung~Ro Moon.
\newblock Genetic algorithm and graph partitioning.
\newblock \emph{IEEE Transactions on computers}, 45\penalty0 (7):\penalty0
  841--855, 1996.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning}, pages
  8748--8763. PMLR, 2021.

\bibitem[Li et~al.(2018)Li, Yu, Li, Avestimehr, Kim, and Schwing]{li2018pipe}
Youjie Li, Mingchao Yu, Songze Li, Salman Avestimehr, Nam~Sung Kim, and
  Alexander Schwing.
\newblock Pipe-sgd: a decentralized pipelined sgd framework for distributed
  deep net training.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 8056--8067, 2018.

\bibitem[Lian et~al.(2018)Lian, Zhang, Zhang, and Liu]{lian2018asynchronous}
Xiangru Lian, Wei Zhang, Ce~Zhang, and Ji~Liu.
\newblock Asynchronous decentralized parallel stochastic gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages
  3043--3052. PMLR, 2018.

\bibitem[Tang et~al.(2018{\natexlab{a}})Tang, Gan, Zhang, Zhang, and
  Liu]{tang2018communication}
Hanlin Tang, Shaoduo Gan, Ce~Zhang, Tong Zhang, and Ji~Liu.
\newblock Communication compression for decentralized training.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 7663--7673, 2018{\natexlab{a}}.

\bibitem[Tang et~al.(2018{\natexlab{b}})Tang, Lian, Yan, Zhang, and
  Liu]{tang2018d}
Hanlin Tang, Xiangru Lian, Ming Yan, Ce~Zhang, and Ji~Liu.
\newblock D2: Decentralized training over decentralized data.
\newblock In \emph{International Conference on Machine Learning}, pages
  4848--4856. PMLR, 2018{\natexlab{b}}.

\bibitem[Ryabinin et~al.(2023)Ryabinin, Dettmers, Diskin, and
  Borzunov]{ryabinin2021swarm}
Max Ryabinin, Tim Dettmers, Michael Diskin, and Alexander Borzunov.
\newblock Swarm parallelism: Training large models can be surprisingly
  communication-efficient.
\newblock 2023.

\bibitem[Athlur et~al.(2022)Athlur, Saran, Sivathanu, Ramjee, and
  Kwatra]{athlur2022varuna}
Sanjith Athlur, Nitika Saran, Muthian Sivathanu, Ramachandran Ramjee, and Nipun
  Kwatra.
\newblock Varuna: scalable, low-cost training of massive deep learning models.
\newblock In \emph{Proceedings of the Seventeenth European Conference on
  Computer Systems}, pages 472--487, 2022.

\bibitem[Anderson(2004)]{anderson2004boinc}
David~P Anderson.
\newblock Boinc: A system for public-resource computing and storage.
\newblock In \emph{Fifth IEEE/ACM international workshop on grid computing},
  pages 4--10. IEEE, 2004.

\bibitem[Shyalika et~al.(2020)Shyalika, Silva, and
  Karunananda]{shyalika2020reinforcement}
Chathurangi Shyalika, Thushari Silva, and Asoka Karunananda.
\newblock Reinforcement learning in dynamic task scheduling: A review.
\newblock \emph{SN Computer Science}, 1\penalty0 (6):\penalty0 1--17, 2020.

\bibitem[Smith et~al.(2022)Smith, Patwary, Norick, LeGresley, Rajbhandari,
  Casper, Liu, Prabhumoye, Zerveas, Korthikanti, et~al.]{smith2022using}
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam
  Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,
  Vijay Korthikanti, et~al.
\newblock Using deepspeed and megatron to train megatron-turing nlg 530b, a
  large-scale generative language model.
\newblock \emph{arXiv preprint arXiv:2201.11990}, 2022.

\bibitem[Huang et~al.(2019)Huang, Cheng, Bapna, Firat, Chen, Chen, Lee, Ngiam,
  Le, Wu, et~al.]{huang2019gpipe}
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen,
  HyoukJoong Lee, Jiquan Ngiam, Quoc~V Le, Yonghui Wu, et~al.
\newblock Gpipe: Efficient training of giant neural networks using pipeline
  parallelism.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Li et~al.(2020)Li, Zhao, Varma, Salpekar, Noordhuis, Li, Paszke,
  Smith, Vaughan, Damania, et~al.]{li2020pytorch}
Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li,
  Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et~al.
\newblock Pytorch distributed: experiences on accelerating data parallel
  training.
\newblock \emph{Proceedings of the VLDB Endowment}, 13\penalty0 (12):\penalty0
  3005--3018, 2020.

\bibitem[Chan et~al.(2007)Chan, Heimlich, Purkayastha, and Van
  De~Geijn]{chan2007collective}
Ernie Chan, Marcel Heimlich, Avi Purkayastha, and Robert Van De~Geijn.
\newblock Collective communication: theory, practice, and experience.
\newblock \emph{Concurrency and Computation: Practice and Experience},
  19\penalty0 (13):\penalty0 1749--1783, 2007.

\bibitem[ncc()]{nccl}
Nccl.
\newblock \url{https://developer.nvidia.com/nccl}.

\bibitem[Narayanan et~al.(2021)Narayanan, Phanishayee, Shi, Chen, and
  Zaharia]{narayanan2021memory}
Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia.
\newblock Memory-efficient pipeline-parallel dnn training.
\newblock In \emph{International Conference on Machine Learning}, pages
  7937--7947. PMLR, 2021.

\bibitem[str()]{strongswan}
Strongswan vpn.
\newblock \url{https://www.strongswan.org/}.

\bibitem[flu()]{fluidstak}
Fluidstack.
\newblock \url{https://www.fluidstack.io/}.

\end{thebibliography}
