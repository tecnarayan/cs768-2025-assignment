\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
  Ghemawat, S., Irving, G., Isard, M., et~al.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \emph{{USENIX}}, 2016.

\bibitem[Amsaleg et~al.(2015)Amsaleg, Chelly, Furon, Girard, Houle,
  Kawarabayashi, and Nett]{Amsaleg15}
Amsaleg, L., Chelly, O., Furon, T., Girard, S., Houle, M.~E., Kawarabayashi,
  K., and Nett, M.
\newblock Estimating local intrinsic dimensionality.
\newblock In \emph{{SIGKDD}}, 2015.

\bibitem[Amsaleg et~al.(2017)Amsaleg, Bailey, Barbe, Erfani, Houle, Nguyen, and
  Radovanovic]{Amsaleg17}
Amsaleg, L., Bailey, J., Barbe, D., Erfani, S.~M., Houle, M.~E., Nguyen, V.,
  and Radovanovic, M.
\newblock The vulnerability of learning to adversarial perturbation increases
  with intrinsic dimensionality.
\newblock In \emph{IEEE Workshop on Information Forensics and Security {WIFS}},
  2017.

\bibitem[Ansuini et~al.(2019)Ansuini, Laio, Macke, and Zoccolan]{Ansuini19}
Ansuini, A., Laio, A., Macke, J.~H., and Zoccolan, D.
\newblock Intrinsic dimension of data representations in deep neural networks.
\newblock In \emph{{NeurIPS}}, 2019.

\bibitem[Antoniou \& Storkey(2019)Antoniou and Storkey]{SCA}
Antoniou, A. and Storkey, A.~J.
\newblock Learning to learn by self-critique.
\newblock In \emph{{NeurIPS}}, 2019.

\bibitem[Arpit et~al.(2017)Arpit, Jastrzebski, Ballas, Krueger, Bengio, Kanwal,
  Maharaj, Fischer, Courville, Bengio, and Lacoste{-}Julien]{ArpitL17}
Arpit, D., Jastrzebski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal, M.~S.,
  Maharaj, T., Fischer, A., Courville, A.~C., Bengio, Y., and Lacoste{-}Julien,
  S.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{{ICML}}, 2017.

\bibitem[Boudiaf et~al.(2020)Boudiaf, Ziko, Rony, Dolz, Piantanida, and
  Ayed]{TIM}
Boudiaf, M., Ziko, I.~M., Rony, J., Dolz, J., Piantanida, P., and Ayed, I.~B.
\newblock Information maximization for few-shot learning.
\newblock In \emph{{NeurIPS}}, 2020.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{caron2021unsupervised}
Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock In \emph{{NeurIPS}}, 2020.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Swersky, Norouzi, and
  Hinton]{chen2020big}
Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G.
\newblock Big self-supervised models are strong semi-supervised learners.
\newblock In \emph{{NeurIPS}}, 2020.

\bibitem[Chen et~al.(2019)Chen, Liu, Kira, Wang, and Huang]{Chen19}
Chen, W., Liu, Y., Kira, Z., Wang, Y.~F., and Huang, J.
\newblock A closer look at few-shot classification.
\newblock In \emph{{ICLR}}, 2019.

\bibitem[Dhillon et~al.(2020)Dhillon, Chaudhari, Ravichandran, and
  Soatto]{transBaseline}
Dhillon, G.~S., Chaudhari, P., Ravichandran, A., and Soatto, S.
\newblock A baseline for few-shot image classification.
\newblock In \emph{{ICLR}}, 2020.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{MAML}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{{ICML}}, 2017.

\bibitem[Gidaris \& Komodakis(2019)Gidaris and Komodakis]{wDAE-GNN}
Gidaris, S. and Komodakis, N.
\newblock Generating classification weights with {GNN} denoising autoencoders
  for few-shot learning.
\newblock In \emph{{CVPR}}, 2019.

\bibitem[Gidaris et~al.(2018)Gidaris, Singh, and Komodakis]{Rotnet}
Gidaris, S., Singh, P., and Komodakis, N.
\newblock Unsupervised representation learning by predicting image rotations.
\newblock In \emph{{ICLR}}, 2018.

\bibitem[Gidaris et~al.(2019)Gidaris, Bursuc, Komodakis, Perez, and
  Cord]{Gidaris_2019_ICCV}
Gidaris, S., Bursuc, A., Komodakis, N., Perez, P., and Cord, M.
\newblock Boosting few-shot visual learning with self-supervision.
\newblock In \emph{{ICCV}}, 2019.

\bibitem[Gong et~al.(2019)Gong, Boddeti, and Jain]{Gong19}
Gong, S., Boddeti, V.~N., and Jain, A.~K.
\newblock On the intrinsic dimensionality of image representations.
\newblock In \emph{{CVPR}}, 2019.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond,
  Buchatskaya, Doersch, Pires, Guo, Azar, et~al.]{grill2020bootstrap}
Grill, J.-B., Strub, F., Altch{\'e}, F., Tallec, C., Richemond, P.~H.,
  Buchatskaya, E., Doersch, C., Pires, B.~A., Guo, Z.~D., Azar, M.~G., et~al.
\newblock Bootstrap your own latent: A new approach to self-supervised
  learning.
\newblock In \emph{{NeurIPS}}, 2020.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Lee, and Mazeika]{hendrycks2019using}
Hendrycks, D., Lee, K., and Mazeika, M.
\newblock Using pre-training can improve model robustness and uncertainty.
\newblock In \emph{{ICML}}, 2019.

\bibitem[Hou et~al.(2019)Hou, Chang, Ma, Shan, and Chen]{CAN}
Hou, R., Chang, H., Ma, B., Shan, S., and Chen, X.
\newblock Cross attention network for few-shot classification.
\newblock \emph{{NeurIPS}}, 2019.

\bibitem[Houle(2017{\natexlab{a}})]{Houle17a}
Houle, M.~E.
\newblock Local intrinsic dimensionality {I:} an extreme-value-theoretic
  foundation for similarity applications.
\newblock In \emph{{SISAP}}, 2017{\natexlab{a}}.

\bibitem[Houle(2017{\natexlab{b}})]{Houle17b}
Houle, M.~E.
\newblock Local intrinsic dimensionality {II:} multivariate analysis and
  distributional support.
\newblock In \emph{{SISAP}}, 2017{\natexlab{b}}.

\bibitem[Hu et~al.(2020)Hu, Moreno, Xiao, Shen, Obozinski, Lawrence, and
  Damianou]{SIB}
Hu, S.~X., Moreno, P.~G., Xiao, Y., Shen, X., Obozinski, G., Lawrence, N.~D.,
  and Damianou, A.~C.
\newblock Empirical bayes transductive meta-learning with synthetic gradients.
\newblock In \emph{{ICLR}}, 2020.

\bibitem[Jiang et~al.(2018)Jiang, Zhou, Leung, Li, and
  Fei-Fei]{jiang2018mentornet}
Jiang, L., Zhou, Z., Leung, T., Li, L.-J., and Fei-Fei, L.
\newblock Mentornet: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock In \emph{{ICML}}, 2018.

\bibitem[Kim et~al.(2021)Kim, Choo, Kwon, Joe, Min, and Gwon]{kim2021selfmatch}
Kim, B., Choo, J., Kwon, Y.-D., Joe, S., Min, S., and Gwon, Y.
\newblock Selfmatch: Combining contrastive self-supervision and consistency for
  semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:2101.06480}, 2021.

\bibitem[Kim et~al.(2019)Kim, Kim, Kim, and Yoo]{kim2019edge}
Kim, J., Kim, T., Kim, S., and Yoo, C.~D.
\newblock Edge-labeling graph neural network for few-shot learning.
\newblock In \emph{{CVPR}}, 2019.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{{ICLR}}, 2015.

\bibitem[Lampinen \& Ganguli(2019)Lampinen and Ganguli]{Lampinen19}
Lampinen, A.~K. and Ganguli, S.
\newblock An analytic theory of generalization dynamics and transfer learning
  in deep linear networks.
\newblock In \emph{{ICLR}}, 2019.

\bibitem[Li et~al.(2019)Li, Eigen, Dodge, Zeiler, and Wang]{CTM}
Li, H., Eigen, D., Dodge, S., Zeiler, M., and Wang, X.
\newblock Finding task-relevant features for few-shot learning by category
  traversal.
\newblock In \emph{{CVPR}}, 2019.

\bibitem[Lichtenstein et~al.(2020)Lichtenstein, Sattigeri, Feris, Giryes, and
  Karlinsky]{TAFSSL}
Lichtenstein, M., Sattigeri, P., Feris, R., Giryes, R., and Karlinsky, L.
\newblock {TAFSSL:} task-adaptive feature sub-space learning for few-shot
  classification.
\newblock In \emph{{ECCV}}, 2020.

\bibitem[Liu et~al.(2020)Liu, Song, and Qin]{BDCSPN}
Liu, J., Song, L., and Qin, Y.
\newblock Prototype rectification for few-shot learning.
\newblock In \emph{{ECCV}}, 2020.

\bibitem[Liu et~al.(2019)Liu, Lee, Park, Kim, Yang, Hwang, and Yang]{TPN}
Liu, Y., Lee, J., Park, M., Kim, S., Yang, E., Hwang, S.~J., and Yang, Y.
\newblock Learning to propagate labels: Transductive propagation network for
  few-shot learning.
\newblock In \emph{{ICLR}}, 2019.

\bibitem[Ma et~al.(2018a)Ma, Li, Wang, Erfani, Wijewickrema, Schoenebeck, Song,
  Houle, and Bailey]{Ma18a}
Ma, X., Li, B., Wang, Y., Erfani, S.~M., Wijewickrema, S. N.~R., Schoenebeck,
  G., Song, D., Houle, M.~E., and Bailey, J.
\newblock Characterizing adversarial subspaces using local intrinsic
  dimensionality.
\newblock In \emph{{ICLR}}, 2018a.

\bibitem[Ma et~al.(2018b)Ma, Wang, Houle, Zhou, Erfani, Xia, Wijewickrema, and
  Bailey]{Ma18b}
Ma, X., Wang, Y., Houle, M.~E., Zhou, S., Erfani, S.~M., Xia, S., Wijewickrema,
  S. N.~R., and Bailey, J.
\newblock Dimensionality-driven learning with noisy labels.
\newblock In \emph{{ICML}}, 2018b.

\bibitem[Miller et~al.(2000)Miller, Matsakis, and Viola]{miller2000learning}
Miller, E.~G., Matsakis, N.~E., and Viola, P.~A.
\newblock Learning from one example through shared densities on transforms.
\newblock In \emph{{CVPR}}, 2000.

\bibitem[Noroozi \& Favaro(2016)Noroozi and Favaro]{jigsaw}
Noroozi, M. and Favaro, P.
\newblock Unsupervised learning of visual representations by solving jigsaw
  puzzles.
\newblock In \emph{{ECCV}}, 2016.

\bibitem[Oreshkin et~al.(2018)Oreshkin, Rodriguez, and Lacoste]{tadam}
Oreshkin, B.~N., Rodriguez, P., and Lacoste, A.
\newblock Tadam: Task dependent adaptive metric for improved few-shot learning.
\newblock \emph{{NeurIPS}}, 2018.

\bibitem[Oymak et~al.(2019)Oymak, Fabian, Li, and
  Soltanolkotabi]{oymak2019generalization}
Oymak, S., Fabian, Z., Li, M., and Soltanolkotabi, M.
\newblock Generalization guarantees for neural networks via harnessing the
  low-rank structure of the jacobian.
\newblock \emph{arXiv preprint arXiv:1906.05392}, 2019.

\bibitem[Qiao et~al.(2019)Qiao, Shi, Li, Tian, Huang, and Wang]{Team}
Qiao, L., Shi, Y., Li, J., Tian, Y., Huang, T., and Wang, Y.
\newblock Transductive episodic-wise adaptive metric for few-shot learning.
\newblock In \emph{{ICCV}}, 2019.

\bibitem[Ravi \& Larochelle(2017)Ravi and Larochelle]{ravi2016optimization}
Ravi, S. and Larochelle, H.
\newblock Optimization as a model for few-shot learning.
\newblock In \emph{{ICLR}}, 2017.

\bibitem[Ren et~al.(2018)Ren, Ravi, Triantafillou, Snell, Swersky, Tenenbaum,
  Larochelle, and Zemel]{ren2018metalearning}
Ren, M., Ravi, S., Triantafillou, E., Snell, J., Swersky, K., Tenenbaum, J.~B.,
  Larochelle, H., and Zemel, R.~S.
\newblock Meta-learning for semi-supervised few-shot classification.
\newblock In \emph{{ICLR}}, 2018.

\bibitem[Rodr{\'\i}guez et~al.(2020)Rodr{\'\i}guez, Laradji, Drouin, and
  Lacoste]{epnet}
Rodr{\'\i}guez, P., Laradji, I., Drouin, A., and Lacoste, A.
\newblock Embedding propagation: Smoother manifold for few-shot classification.
\newblock In \emph{{ECCV}}, 2020.

\bibitem[Rusu et~al.(2019)Rusu, Rao, Sygnowski, Vinyals, Pascanu, Osindero, and
  Hadsell]{LEO}
Rusu, A.~A., Rao, D., Sygnowski, J., Vinyals, O., Pascanu, R., Osindero, S.,
  and Hadsell, R.
\newblock Meta-learning with latent embedding optimization.
\newblock In \emph{{ICLR}}, 2019.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{ProtoNet}
Snell, J., Swersky, K., and Zemel, R.~S.
\newblock Prototypical networks for few-shot learning.
\newblock In \emph{{NeurIPS}}, 2017.

\bibitem[Song et~al.(2020)Song, Kim, Park, and Lee]{song2020prestopping}
Song, H., Kim, M., Park, D., and Lee, J.-G.
\newblock How does early stopping help generalization against label noise?
\newblock \emph{arXiv preprint arXiv:1911.08059}, 2020.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (56):\penalty0 1929--1958, 2014.

\bibitem[Stephenson et~al.(2021)Stephenson, suchismita padhy, Ganesh, Hui,
  Tang, and Chung]{stephenson21}
Stephenson, C., suchismita padhy, Ganesh, A., Hui, Y., Tang, H., and Chung, S.
\newblock On the geometry of generalization and memorization in deep neural
  networks.
\newblock In \emph{{ICLR}}, 2021.

\bibitem[Su et~al.(2020)Su, Maji, and Hariharan]{self_eccv}
Su, J.-C., Maji, S., and Hariharan, B.
\newblock When does self-supervision improve few-shot learning?
\newblock In \emph{{ECCV}}, 2020.

\bibitem[Sugiyama(2018)]{sugiyama2018co}
Sugiyama, M.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Thrun \& Pratt(2012)Thrun and Pratt]{metalearning}
Thrun, S. and Pratt, L.
\newblock \emph{Learning to learn}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Tian et~al.(2020)Tian, Wang, Krishnan, Tenenbaum, and
  Isola]{tian2020rethinking}
Tian, Y., Wang, Y., Krishnan, D., Tenenbaum, J.~B., and Isola, P.
\newblock Rethinking few-shot image classification: a good embedding is all you
  need?
\newblock \emph{arXiv preprint arXiv:2003.11539}, 2020.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, Wierstra,
  et~al.]{vinyals2016matching}
Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et~al.
\newblock Matching networks for one shot learning.
\newblock \emph{{NeurIPS}}, 2016.

\bibitem[Wang et~al.(2019)Wang, Chao, Weinberger, and van~der
  Maaten]{SimpleShot}
Wang, Y., Chao, W., Weinberger, K.~Q., and van~der Maaten, L.
\newblock Simpleshot: Revisiting nearest-neighbor classification for few-shot
  learning.
\newblock \emph{arXiv preprint arXiv:1911.04623}, 2019.

\bibitem[Welinder et~al.(2010)Welinder, Branson, Mita, Wah, Schroff, Belongie,
  and Perona]{welinder2010caltech}
Welinder, P., Branson, S., Mita, T., Wah, C., Schroff, F., Belongie, S., and
  Perona, P.
\newblock Caltech-ucsd birds 200.
\newblock 2010.

\bibitem[Ye et~al.(2020)Ye, Hu, Zhan, and Sha]{FEAT}
Ye, H., Hu, H., Zhan, D., and Sha, F.
\newblock Few-shot learning via embedding adaptation with set-to-set functions.
\newblock In \emph{{CVPR}}, 2020.

\bibitem[Yu et~al.(2019)Yu, Han, Yao, Niu, Tsang, and Sugiyama]{yu2019does}
Yu, X., Han, B., Yao, J., Niu, G., Tsang, I., and Sugiyama, M.
\newblock How does disagreement help generalization against label corruption?
\newblock In \emph{{ICML}}, 2019.

\bibitem[Zhai et~al.(2019)Zhai, Oliver, Kolesnikov, and Beyer]{zhai2019s4l}
Zhai, X., Oliver, A., Kolesnikov, A., and Beyer, L.
\newblock S4l: Self-supervised semi-supervised learning.
\newblock In \emph{{ICCV}}, 2019.

\bibitem[Ziko et~al.(2020)Ziko, Dolz, Granger, and Ayed]{LaplacianShot}
Ziko, I.~M., Dolz, J., Granger, E., and Ayed, I.~B.
\newblock Laplacian regularized few-shot learning.
\newblock In \emph{{ICML}}, 2020.

\end{thebibliography}
