\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwala \& Fisher(2019)Agarwala and Fisher]{agarwala_adaptive_2019a}
Agarwala, A. and Fisher, D.~S.
\newblock Adaptive walks on high-dimensional fitness landscapes and seascapes
  with distance-dependent statistics.
\newblock \emph{Theoretical Population Biology}, 130:\penalty0 13--49, December
  2019.
\newblock ISSN 0040-5809.
\newblock \doi{10.1016/j.tpb.2019.09.011}.

\bibitem[Agarwala et~al.(2020)Agarwala, Pennington, Dauphin, and
  Schoenholz]{agarwala_temperature_2020}
Agarwala, A., Pennington, J., Dauphin, Y., and Schoenholz, S.
\newblock Temperature check: Theory and practice for training models with
  softmax-cross-entropy losses, October 2020.

\bibitem[Agarwala et~al.(2022)Agarwala, Pedregosa, and
  Pennington]{agarwala_secondorder_2022}
Agarwala, A., Pedregosa, F., and Pennington, J.
\newblock Second-order regression models exhibit progressive sharpening to the
  edge of stability, October 2022.

\bibitem[Andriushchenko \& Flammarion(2022)Andriushchenko and
  Flammarion]{andriushchenko_understanding_2022}
Andriushchenko, M. and Flammarion, N.
\newblock Towards {{Understanding Sharpness-Aware Minimization}}, June 2022.

\bibitem[Bahri et~al.(2022)Bahri, Mobahi, and Tay]{bahri_sharpnessaware_2022}
Bahri, D., Mobahi, H., and Tay, Y.
\newblock Sharpness-{{Aware Minimization Improves Language Model
  Generalization}}, March 2022.

\bibitem[Bartlett et~al.(2022)Bartlett, Long, and
  Bousquet]{bartlett_dynamics_2022}
Bartlett, P.~L., Long, P.~M., and Bousquet, O.
\newblock The {{Dynamics}} of {{Sharpness-Aware Minimization}}: {{Bouncing
  Across Ravines}} and {{Drifting Towards Wide Minima}}, October 2022.

\bibitem[Chaudhari et~al.(2019)Chaudhari, Choromanska, Soatto, LeCun, Baldassi,
  Borgs, Chayes, Sagun, and Zecchina]{chaudhari_entropysgd_2019}
Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y., Baldassi, C., Borgs, C.,
  Chayes, J., Sagun, L., and Zecchina, R.
\newblock Entropy-{{SGD}}: Biasing gradient descent into wide valleys.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2019\penalty0 (12):\penalty0 124018, December 2019.
\newblock ISSN 1742-5468.
\newblock \doi{10.1088/1742-5468/ab39d9}.

\bibitem[Cohen et~al.(2022{\natexlab{a}})Cohen, Kaur, Li, Kolter, and
  Talwalkar]{cohen_gradient_2022}
Cohen, J., Kaur, S., Li, Y., Kolter, J.~Z., and Talwalkar, A.
\newblock Gradient {{Descent}} on {{Neural Networks Typically Occurs}} at the
  {{Edge}} of {{Stability}}.
\newblock In \emph{International {{Conference}} on {{Learning
  Representations}}}, February 2022{\natexlab{a}}.

\bibitem[Cohen et~al.(2022{\natexlab{b}})Cohen, Ghorbani, Krishnan, Agarwal,
  Medapati, Badura, Suo, Cardoze, Nado, Dahl, and Gilmer]{cohen_adaptive_2022}
Cohen, J.~M., Ghorbani, B., Krishnan, S., Agarwal, N., Medapati, S., Badura,
  M., Suo, D., Cardoze, D., Nado, Z., Dahl, G.~E., and Gilmer, J.
\newblock Adaptive {{Gradient Methods}} at the {{Edge}} of {{Stability}}, July
  2022{\natexlab{b}}.

\bibitem[Damian et~al.(2022)Damian, Nichani, and
  Lee]{damian_selfstabilization_2022}
Damian, A., Nichani, E., and Lee, J.~D.
\newblock Self-{{Stabilization}}: {{The Implicit Bias}} of {{Gradient Descent}}
  at the {{Edge}} of {{Stability}}, September 2022.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh_sharp_2017}
Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{Proceedings of the 34th {{International Conference}} on
  {{Machine Learning}} - {{Volume}} 70}, {{ICML}}'17, pp.\  1019--1028,
  {Sydney, NSW, Australia}, August 2017. {JMLR.org}.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi_adaptive_2011}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive {{Subgradient Methods}} for {{Online Learning}} and
  {{Stochastic Optimization}}.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (61):\penalty0 2121--2159, 2011.
\newblock ISSN 1533-7928.

\bibitem[Foret et~al.(2022)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret_sharpnessaware_2022}
Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B.
\newblock Sharpness-aware {{Minimization}} for {{Efficiently Improving
  Generalization}}.
\newblock In \emph{International {{Conference}} on {{Learning
  Representations}}}, April 2022.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Krishnan, and
  Xiao]{ghorbani_investigation_2019}
Ghorbani, B., Krishnan, S., and Xiao, Y.
\newblock An {{Investigation}} into {{Neural Net Optimization}} via {{Hessian
  Eigenvalue Density}}.
\newblock In \emph{Proceedings of the 36th {{International Conference}} on
  {{Machine Learning}}}, pp.\  2232--2241. {PMLR}, May 2019.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter_flat_1997}
Hochreiter, S. and Schmidhuber, J.
\newblock Flat {{Minima}}.
\newblock \emph{Neural Computation}, 9\penalty0 (1):\penalty0 1--42, January
  1997.
\newblock ISSN 0899-7667.
\newblock \doi{10.1162/neco.1997.9.1.1}.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot_neural_2018}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural {{Tangent Kernel}}: {{Convergence}} and {{Generalization}} in
  {{Neural Networks}}.
\newblock In \emph{Advances in {{Neural Information Processing Systems}} 31},
  pp.\  8571--8580. {Curran Associates, Inc.}, 2018.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar_largebatch_2017}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On {{Large-Batch Training}} for {{Deep Learning}}: {{Generalization
  Gap}} and {{Sharp Minima}}, February 2017.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, {Sohl-Dickstein},
  and Pennington]{lee_wide_2019}
Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., {Sohl-Dickstein}, J.,
  and Pennington, J.
\newblock Wide {{Neural Networks}} of {{Any Depth Evolve}} as {{Linear Models
  Under Gradient Descent}}.
\newblock In \emph{Advances in {{Neural Information Processing Systems}} 32},
  pp.\  8570--8581. {Curran Associates, Inc.}, 2019.

\bibitem[Lee et~al.(2022)Lee, Cheng, Paquette, and
  Paquette]{lee_trajectory_2022}
Lee, K., Cheng, A.~N., Paquette, C., and Paquette, E.
\newblock Trajectory of {{Mini-Batch Momentum}}: {{Batch Size Saturation}} and
  {{Convergence}} in {{High Dimensions}}, June 2022.

\bibitem[Lewis \& Overton(2013)Lewis and Overton]{lewis_nonsmooth_2013}
Lewis, A.~S. and Overton, M.~L.
\newblock Nonsmooth optimization via quasi-{{Newton}} methods.
\newblock \emph{Mathematical Programming}, 141\penalty0 (1):\penalty0 135--163,
  October 2013.
\newblock ISSN 1436-4646.
\newblock \doi{10.1007/s10107-012-0514-2}.

\bibitem[Lewkowycz et~al.(2020)Lewkowycz, Bahri, Dyer, {Sohl-Dickstein}, and
  {Gur-Ari}]{lewkowycz_large_2020}
Lewkowycz, A., Bahri, Y., Dyer, E., {Sohl-Dickstein}, J., and {Gur-Ari}, G.
\newblock The large learning rate phase of deep learning: The catapult
  mechanism.
\newblock March 2020.

\bibitem[Loshchilov \& Hutter(2016)Loshchilov and Hutter]{loshchilov2016sgdr}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock \emph{arXiv preprint arXiv:1608.03983}, 2016.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, Mcallester, and
  Srebro]{neyshabur_exploring_2017}
Neyshabur, B., Bhojanapalli, S., Mcallester, D., and Srebro, N.
\newblock Exploring {{Generalization}} in {{Deep Learning}}.
\newblock In \emph{Advances in {{Neural Information Processing Systems}} 30},
  pp.\  5947--5956. {Curran Associates, Inc.}, 2017.

\bibitem[Nocedal(1980)]{nocedal_updating_1980}
Nocedal, J.
\newblock Updating quasi-{{Newton}} matrices with limited storage.
\newblock \emph{Mathematics of Computation}, 35\penalty0 (151):\penalty0
  773--782, 1980.
\newblock ISSN 0025-5718, 1088-6842.
\newblock \doi{10.1090/S0025-5718-1980-0572855-7}.

\bibitem[Nowak \& Krug(2015)Nowak and Krug]{nowak_analysis_2015}
Nowak, S. and Krug, J.
\newblock Analysis of adaptive walks on {{NK}} fitness landscapes with
  different interaction schemes.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2015\penalty0 (6):\penalty0 P06014, 2015.

\bibitem[Paquette et~al.(2021)Paquette, Lee, Pedregosa, and
  Paquette]{paquette_sgd_2021}
Paquette, C., Lee, K., Pedregosa, F., and Paquette, E.
\newblock {{SGD}} in the {{Large}}: {{Average-case Analysis}}, {{Asymptotics}},
  and {{Stepsize Criticality}}.
\newblock In \emph{Proceedings of {{Thirty Fourth Conference}} on {{Learning
  Theory}}}, pp.\  3548--3626. {PMLR}, July 2021.

\bibitem[Paquette et~al.(2022{\natexlab{a}})Paquette, Paquette, Adlam, and
  Pennington]{paquette_homogenization_2022}
Paquette, C., Paquette, E., Adlam, B., and Pennington, J.
\newblock Homogenization of {{SGD}} in high-dimensions: {{Exact}} dynamics and
  generalization properties, May 2022{\natexlab{a}}.

\bibitem[Paquette et~al.(2022{\natexlab{b}})Paquette, Paquette, Adlam, and
  Pennington]{paquette_implicit_2022}
Paquette, C., Paquette, E., Adlam, B., and Pennington, J.
\newblock Implicit {{Regularization}} or {{Implicit Conditioning}}? {{Exact
  Risk Trajectories}} of {{SGD}} in {{High Dimensions}}, June
  2022{\natexlab{b}}.

\bibitem[Park \& Krug(2016)Park and Krug]{park_dexceedance_2016}
Park, S.-C. and Krug, J.
\newblock {$\delta$}-exceedance records and random adaptive walks.
\newblock \emph{Journal of Physics A: Mathematical and Theoretical},
  49\penalty0 (31):\penalty0 315601, 2016.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{sutskever2013importance}
Sutskever, I., Martens, J., Dahl, G., and Hinton, G.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{International conference on machine learning}, pp.\
  1139--1147. PMLR, 2013.

\bibitem[Wen et~al.(2023)Wen, Ma, and Li]{wen_how_2023}
Wen, K., Ma, T., and Li, Z.
\newblock How {{Does Sharpness-Aware Minimization Minimize Sharpness}}?,
  January 2023.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zhuang et~al.(2022)Zhuang, Gong, Yuan, Cui, Adam, Dvornek, Tatikonda,
  Duncan, and Liu]{zhuang_surrogate_2022}
Zhuang, J., Gong, B., Yuan, L., Cui, Y., Adam, H., Dvornek, N., Tatikonda, S.,
  Duncan, J., and Liu, T.
\newblock Surrogate {{Gap Minimization Improves Sharpness-Aware Training}},
  March 2022.

\end{thebibliography}
