\begin{thebibliography}{93}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbe et~al.(2022)Abbe, Bengio, Cornacchia, Kleinberg, Lotfi, Raghu,
  and Zhang]{abbe2022learning}
Abbe, E., Bengio, S., Cornacchia, E., Kleinberg, J., Lotfi, A., Raghu, M., and
  Zhang, C.
\newblock Learning to reason with neural networks: Generalization, unseen data
  and boolean measures.
\newblock \emph{Advances in Neural Information Processing Systems}, 35, 2022.

\bibitem[Abbe et~al.(2023)Abbe, Bengio, Lotfi, and
  Rizk]{abbe2023generalization}
Abbe, E., Bengio, S., Lotfi, A., and Rizk, K.
\newblock Generalization on the unseen, logic reasoning and degree curriculum.
\newblock In \emph{International conference on machine learning}. PMLR, 2023.

\bibitem[Agarwal et~al.(2019{\natexlab{a}})Agarwal, Bullins, Hazan, Kakade, and
  Singh]{agarwal2019online}
Agarwal, N., Bullins, B., Hazan, E., Kakade, S., and Singh, K.
\newblock Online control with adversarial disturbances.
\newblock In \emph{International Conference on Machine Learning}. PMLR,
  2019{\natexlab{a}}.

\bibitem[Agarwal et~al.(2019{\natexlab{b}})Agarwal, Hazan, and
  Singh]{agarwal2019logarithmic}
Agarwal, N., Hazan, E., and Singh, K.
\newblock Logarithmic regret for online control.
\newblock \emph{Advances in Neural Information Processing Systems}, 32,
  2019{\natexlab{b}}.

\bibitem[Anderson \& Moore(2007)Anderson and Moore]{anderson2007optimal}
Anderson, B.~D. and Moore, J.~B.
\newblock \emph{Optimal control: linear quadratic methods}.
\newblock Courier Corporation, 2007.

\bibitem[Anderson et~al.(2010)Anderson, Guionnet, and
  Zeitouni]{anderson2010introduction}
Anderson, G.~W., Guionnet, A., and Zeitouni, O.
\newblock \emph{An introduction to random matrices}.
\newblock Number 118. Cambridge university press, 2010.

\bibitem[Andriushchenko et~al.(2023)Andriushchenko, Varre, Pillaud-Vivien, and
  Flammarion]{andriushchenko2023sgd}
Andriushchenko, M., Varre, A.~V., Pillaud-Vivien, L., and Flammarion, N.
\newblock Sgd with large step sizes learns sparse features.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2023.

\bibitem[Arora et~al.(2019)Arora, Cohen, Hu, and Luo]{arora2019implicit}
Arora, S., Cohen, N., Hu, W., and Luo, Y.
\newblock Implicit regularization in deep matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Azulay et~al.(2021)Azulay, Moroshko, Nacson, Woodworth, Srebro,
  Globerson, and Soudry]{azulay2021implicit}
Azulay, S., Moroshko, E., Nacson, M.~S., Woodworth, B.~E., Srebro, N.,
  Globerson, A., and Soudry, D.
\newblock On the implicit bias of initialization shape: Beyond infinitesimal
  mirror descent.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and
  Tsigler]{bartlett2020benign}
Bartlett, P.~L., Long, P.~M., Lugosi, G., and Tsigler, A.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (48):\penalty0 30063--30070, 2020.

\bibitem[Bhandari \& Russo(2019)Bhandari and Russo]{bhandari2019global}
Bhandari, J. and Russo, D.
\newblock Global optimality guarantees for policy gradient methods.
\newblock \emph{arXiv preprint arXiv:1906.01786}, 2019.

\bibitem[Bhounsule et~al.(2016)Bhounsule, Ameperosa, Miller, Seay, and
  Ulep]{bhounsule2016dead}
Bhounsule, P.~A., Ameperosa, E., Miller, S., Seay, K., and Ulep, R.
\newblock Dead-beat control of walking for a torso-actuated rimless wheel using
  an event-based, discrete, linear controller.
\newblock In \emph{International Design Engineering Technical Conferences and
  Computers and Information in Engineering Conference}, volume 50152, pp.\
  V05AT07A042. American Society of Mechanical Engineers, 2016.

\bibitem[Boursier et~al.(2022)Boursier, Pillaud-Vivien, and
  Flammarion]{boursier2022gradient}
Boursier, E., Pillaud-Vivien, L., and Flammarion, N.
\newblock Gradient flow dynamics of shallow relu networks for square loss and
  orthogonal inputs.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 20105--20118, 2022.

\bibitem[Bu et~al.(2019)Bu, Mesbahi, Fazel, and Mesbahi]{bu2019lqr}
Bu, J., Mesbahi, A., Fazel, M., and Mesbahi, M.
\newblock Lqr through the lens of first order methods: Discrete-time case.
\newblock \emph{arXiv preprint arXiv:1907.08921}, 2019.

\bibitem[Bu et~al.(2020)Bu, Mesbahi, and Mesbahi]{bu2020policy}
Bu, J., Mesbahi, A., and Mesbahi, M.
\newblock Policy gradient-based algorithms for continuous-time linear quadratic
  control.
\newblock \emph{arXiv preprint arXiv:2006.09178}, 2020.

\bibitem[Caron \& Traynor(2005)Caron and Traynor]{caron2005zero}
Caron, R. and Traynor, T.
\newblock The zero set of a polynomial.
\newblock \emph{WSMR Report}, pp.\  05--02, 2005.

\bibitem[Cassel \& Koren(2021)Cassel and Koren]{cassel2021online}
Cassel, A.~B. and Koren, T.
\newblock Online policy gradient for model free learning of linear quadratic
  regulators with $\sqrt{T}$ regret.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2021.

\bibitem[Chen et~al.(2023)Chen, Minasyan, Lee, and Hazan]{chen2023regret}
Chen, X., Minasyan, E., Lee, J.~D., and Hazan, E.
\newblock Regret guarantees for online deep control.
\newblock In \emph{Learning for Dynamics and Control Conference}. PMLR, 2023.

\bibitem[Chou et~al.(2023)Chou, Maly, and Rauhut]{chou2023more}
Chou, H.-H., Maly, J., and Rauhut, H.
\newblock More is less: inducing sparsity via overparameterization.
\newblock \emph{Information and Inference: A Journal of the IMA}, 12\penalty0
  (3), 2023.

\bibitem[Chou et~al.(2024)Chou, Gieshoff, Maly, and Rauhut]{chou2024gradient}
Chou, H.-H., Gieshoff, C., Maly, J., and Rauhut, H.
\newblock Gradient descent for deep matrix factorization: Dynamics and implicit
  bias towards low rank.
\newblock \emph{Applied and Computational Harmonic Analysis}, 68:\penalty0
  101595, 2024.

\bibitem[Clavera et~al.(2020)Clavera, Fu, and Abbeel]{clavera2020model}
Clavera, I., Fu, V., and Abbeel, P.
\newblock Model-augmented actor-critic: Backpropagating through paths.
\newblock \emph{International Conference on Learning Representations}, 2020.

\bibitem[Cohen et~al.(2018)Cohen, Hasidim, Koren, Lazic, Mansour, and
  Talwar]{cohen2018online}
Cohen, A., Hasidim, A., Koren, T., Lazic, N., Mansour, Y., and Talwar, K.
\newblock Online linear quadratic control.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2018.

\bibitem[Cohen-Karlik et~al.(2022)Cohen-Karlik, David, Cohen, and
  Globerson]{cohen2022implicit}
Cohen-Karlik, E., David, A.~B., Cohen, N., and Globerson, A.
\newblock On the implicit bias of gradient descent for temporal extrapolation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}. PMLR, 2022.

\bibitem[Cohen-Karlik et~al.(2023)Cohen-Karlik, Menuhin-Gruman, Cohen, Giryes,
  and Globerson]{cohen2023learning}
Cohen-Karlik, E., Menuhin-Gruman, I., Cohen, N., Giryes, R., and Globerson, A.
\newblock Learning low dimensional state spaces with overparameterized
  recurrent neural network.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[De~Don{\'a} \& Goodwin(1999)De~Don{\'a} and
  Goodwin]{de1999disturbance}
De~Don{\'a}, J.~A. and Goodwin, G.~C.
\newblock Disturbance sensitivity issues in predictive control.
\newblock \emph{International Journal of Adaptive Control and Signal
  Processing}, 13\penalty0 (6):\penalty0 507--519, 1999.

\bibitem[Dulac-Arnold et~al.(2021)Dulac-Arnold, Levine, Mankowitz, Li,
  Paduraru, Gowal, and Hester]{dulac2021challenges}
Dulac-Arnold, G., Levine, N., Mankowitz, D.~J., Li, J., Paduraru, C., Gowal,
  S., and Hester, T.
\newblock Challenges of real-world reinforcement learning: definitions,
  benchmarks and analysis.
\newblock \emph{Machine Learning}, 110\penalty0 (9):\penalty0 2419--2468, 2021.

\bibitem[Emami-Naeini \& Franklin(1982)Emami-Naeini and
  Franklin]{emami1982deadbeat}
Emami-Naeini, A. and Franklin, G.
\newblock Deadbeat control and tracking of discrete-time systems.
\newblock \emph{IEEE Transactions on Automatic Control}, 27\penalty0
  (1):\penalty0 176--181, 1982.

\bibitem[Fazel et~al.(2018)Fazel, Ge, Kakade, and Mesbahi]{fazel2018global}
Fazel, M., Ge, R., Kakade, S., and Mesbahi, M.
\newblock Global convergence of policy gradient methods for the linear
  quadratic regulator.
\newblock In \emph{International conference on machine learning}. PMLR, 2018.

\bibitem[Frei et~al.(2023{\natexlab{a}})Frei, Vardi, Bartlett, and
  Srebro]{frei2023benign}
Frei, S., Vardi, G., Bartlett, P., and Srebro, N.
\newblock Benign overfitting in linear classifiers and leaky relu networks from
  kkt conditions for margin maximization.
\newblock In \emph{The Thirty Sixth Annual Conference on Learning Theory}.
  PMLR, 2023{\natexlab{a}}.

\bibitem[Frei et~al.(2023{\natexlab{b}})Frei, Vardi, Bartlett, and
  Srebro]{frei2023double}
Frei, S., Vardi, G., Bartlett, P.~L., and Srebro, N.
\newblock The double-edged sword of implicit bias: Generalization vs.
  robustness in relu networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 36,
  2023{\natexlab{b}}.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019off}
Fujimoto, S., Meger, D., and Precup, D.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International conference on machine learning}. PMLR, 2019.

\bibitem[Gillen \& Byl(2022)Gillen and Byl]{gillen2022leveraging}
Gillen, S. and Byl, K.
\newblock Leveraging reward gradients for reinforcement learning in
  differentiable physics simulations.
\newblock \emph{arXiv preprint arXiv:2203.02857}, 2022.

\bibitem[Gravell et~al.(2020)Gravell, Esfahani, and
  Summers]{gravell2020learning}
Gravell, B., Esfahani, P.~M., and Summers, T.
\newblock Learning optimal controllers for linear systems with multiplicative
  noise via policy gradient.
\newblock \emph{IEEE Transactions on Automatic Control}, 66\penalty0 (11),
  2020.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar2017implicit}
Gunasekar, S., Woodworth, B.~E., Bhojanapalli, S., Neyshabur, B., and Srebro,
  N.
\newblock Implicit regularization in matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Hambly et~al.(2021)Hambly, Xu, and Yang]{hambly2021policy}
Hambly, B., Xu, R., and Yang, H.
\newblock Policy gradient methods for the noisy linear quadratic regulator over
  a finite horizon.
\newblock \emph{SIAM Journal on Control and Optimization}, 59\penalty0 (5),
  2021.

\bibitem[Hautus \& Silverman(1983)Hautus and Silverman]{hautus1983system}
Hautus, M.~L. and Silverman, L.~M.
\newblock System structure and singular control.
\newblock \emph{Linear algebra and its applications}, 50:\penalty0 369--402,
  1983.

\bibitem[Hazan \& Singh(2022)Hazan and Singh]{hazan2022introduction}
Hazan, E. and Singh, K.
\newblock Introduction to online nonstochastic control.
\newblock \emph{arXiv preprint arXiv:2211.09619}, 2022.

\bibitem[Howell et~al.(2022)Howell, Cleac'h, Br{\"u}digam, Kolter, Schwager,
  and Manchester]{howell2022dojo}
Howell, T.~A., Cleac'h, S.~L., Br{\"u}digam, J., Kolter, J.~Z., Schwager, M.,
  and Manchester, Z.
\newblock Dojo: A differentiable physics engine for robotics.
\newblock \emph{arXiv preprint arXiv:2203.00806}, 2022.

\bibitem[Hu et~al.(2023)Hu, Zhang, Li, Mesbahi, Fazel, and
  Ba{\c{s}}ar]{hu2023toward}
Hu, B., Zhang, K., Li, N., Mesbahi, M., Fazel, M., and Ba{\c{s}}ar, T.
\newblock Toward a theoretical foundation of policy optimization for learning
  control policies.
\newblock \emph{Annual Review of Control, Robotics, and Autonomous Systems}, 6,
  2023.

\bibitem[Hu et~al.(2019)Hu, Liu, Spielberg, Tenenbaum, Freeman, Wu, Rus, and
  Matusik]{hu2019chainqueen}
Hu, Y., Liu, J., Spielberg, A., Tenenbaum, J.~B., Freeman, W.~T., Wu, J., Rus,
  D., and Matusik, W.
\newblock Chainqueen: A real-time differentiable physical simulator for soft
  robotics.
\newblock In \emph{2019 International conference on robotics and automation
  (ICRA)}, pp.\  6265--6271. IEEE, 2019.

\bibitem[Hu et~al.(2022)Hu, Ji, and Telgarsky]{hu2022actor}
Hu, Y., Ji, Z., and Telgarsky, M.
\newblock Actor-critic is implicitly biased towards high entropy optimal
  policies.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Ji \& Telgarsky(2019{\natexlab{a}})Ji and Telgarsky]{ji2019gradient}
Ji, Z. and Telgarsky, M.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Ji \& Telgarsky(2019{\natexlab{b}})Ji and Telgarsky]{ji2019implicit}
Ji, Z. and Telgarsky, M.
\newblock The implicit bias of gradient descent on nonseparable data.
\newblock In \emph{Conference on Learning Theory}, 2019{\natexlab{b}}.

\bibitem[Jin et~al.(2020)Jin, Schmitt, and Wen]{jin2020analysis}
Jin, Z., Schmitt, J.~M., and Wen, Z.
\newblock On the analysis of model-free methods for the linear quadratic
  regulator.
\newblock \emph{arXiv preprint arXiv:2007.03861}, 2020.

\bibitem[Kemp(2013)]{kemp2013math}
Kemp, T.
\newblock Math 247a: Introduction to random matrix theory.
\newblock \emph{Lecture notes}, 2013.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2015adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Ku{\v{c}}era(1998)]{kuvcera1998deadbeat}
Ku{\v{c}}era, V.
\newblock Deadbeat control, pole placement, and lq regulation.
\newblock In \emph{Theory and Practice of Control and Systems}, pp.\  5--10.
  World Scientific, 1998.

\bibitem[Kumar et~al.(2021)Kumar, Agarwal, Ghosh, and
  Levine]{kumar2021implicit}
Kumar, A., Agarwal, R., Ghosh, D., and Levine, S.
\newblock Implicit under-parameterization inhibits data-efficient deep
  reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Kumar et~al.(2022)Kumar, Agarwal, Ma, Courville, Tucker, and
  Levine]{kumar2022dr3}
Kumar, A., Agarwal, R., Ma, T., Courville, A., Tucker, G., and Levine, S.
\newblock Dr3: Value-based deep reinforcement learning requires explicit
  regularization.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Li \& Todorov(2004)Li and Todorov]{li2004iterative}
Li, W. and Todorov, E.
\newblock Iterative linear quadratic regulator design for nonlinear biological
  movement systems.
\newblock In \emph{First International Conference on Informatics in Control,
  Automation and Robotics}, volume~2. SciTePress, 2004.

\bibitem[Lyu \& Li(2020)Lyu and Li]{lyu2020gradient}
Lyu, K. and Li, J.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock \emph{International Conference on Learning Representations}, 2020.

\bibitem[Lyu et~al.(2021)Lyu, Li, Wang, and Arora]{lyu2021gradient}
Lyu, K., Li, Z., Wang, R., and Arora, S.
\newblock Gradient descent on two-layer nets: Margin maximization and
  simplicity bias.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Malik et~al.(2019)Malik, Pananjady, Bhatia, Khamaru, Bartlett, and
  Wainwright]{malik2019derivative}
Malik, D., Pananjady, A., Bhatia, K., Khamaru, K., Bartlett, P., and
  Wainwright, M.
\newblock Derivative-free methods for policy optimization: Guarantees for
  linear quadratic systems.
\newblock In \emph{The 22nd international conference on artificial intelligence
  and statistics}. PMLR, 2019.

\bibitem[Marcotte et~al.(2023)Marcotte, Gribonval, and
  Peyr{\'e}]{marcotte2023abide}
Marcotte, S., Gribonval, R., and Peyr{\'e}, G.
\newblock Abide by the law and follow the flow: Conservation laws for gradient
  flows.
\newblock \emph{Advances in neural information processing systems}, 2023.

\bibitem[Marro et~al.(2002)Marro, Prattichizzo, and
  Zattoni]{marro2002geometric}
Marro, G., Prattichizzo, D., and Zattoni, E.
\newblock Geometric insight into discrete-time cheap and singular linear
  quadratic riccati (lqr) problems.
\newblock \emph{IEEE Transactions on Automatic Control}, 47\penalty0
  (1):\penalty0 102--107, 2002.

\bibitem[Mattavelli(2005)]{mattavelli2005improved}
Mattavelli, P.
\newblock An improved deadbeat control for ups using disturbance observers.
\newblock \emph{IEEE Transactions on Industrial Electronics}, 52\penalty0
  (1):\penalty0 206--212, 2005.

\bibitem[Metz et~al.(2021)Metz, Freeman, Schoenholz, and
  Kachman]{metz2021gradients}
Metz, L., Freeman, C.~D., Schoenholz, S.~S., and Kachman, T.
\newblock Gradients are not all you need.
\newblock \emph{arXiv preprint arXiv:2111.05803}, 2021.

\bibitem[Miller et~al.(2021)Miller, Taori, Raghunathan, Sagawa, Koh, Shankar,
  Liang, Carmon, and Schmidt]{miller2021accuracy}
Miller, J.~P., Taori, R., Raghunathan, A., Sagawa, S., Koh, P.~W., Shankar, V.,
  Liang, P., Carmon, Y., and Schmidt, L.
\newblock Accuracy on the line: on the strong correlation between
  out-of-distribution and in-distribution generalization.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2021.

\bibitem[Mohammadi et~al.(2019)Mohammadi, Zare, Soltanolkotabi, and
  Jovanovi{\'c}]{mohammadi2019global}
Mohammadi, H., Zare, A., Soltanolkotabi, M., and Jovanovi{\'c}, M.~R.
\newblock Global exponential convergence of gradient methods over the nonconvex
  landscape of the linear quadratic regulator.
\newblock In \emph{2019 IEEE 58th Conference on Decision and Control (CDC)}.
  IEEE, 2019.

\bibitem[Mohammadi et~al.(2021)Mohammadi, Zare, Soltanolkotabi, and
  Jovanovi{\'c}]{mohammadi2021convergence}
Mohammadi, H., Zare, A., Soltanolkotabi, M., and Jovanovi{\'c}, M.~R.
\newblock Convergence and sample complexity of gradient methods for the
  model-free linear--quadratic regulator problem.
\newblock \emph{IEEE Transactions on Automatic Control}, 67\penalty0 (5), 2021.

\bibitem[Mora et~al.(2021)Mora, Peychev, Ha, Vechev, and Coros]{mora2021pods}
Mora, M. A.~Z., Peychev, M., Ha, S., Vechev, M., and Coros, S.
\newblock Pods: Policy optimization via differentiable simulation.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2021.

\bibitem[Munkres(2018)]{munkres2018elements}
Munkres, J.~R.
\newblock \emph{Elements of algebraic topology}.
\newblock CRC press, 2018.

\bibitem[Neyshabur(2017)]{neyshabur2017implicit}
Neyshabur, B.
\newblock Implicit regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1709.01953}, 2017.

\bibitem[Neyshabur et~al.(2014)Neyshabur, Tomioka, and
  Srebro]{neyshabur2014search}
Neyshabur, B., Tomioka, R., and Srebro, N.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1412.6614}, 2014.

\bibitem[Panerati et~al.(2021)Panerati, Zheng, Zhou, Xu, Prorok, and
  Schoellig]{panerati2021learning}
Panerati, J., Zheng, H., Zhou, S., Xu, J., Prorok, A., and Schoellig, A.~P.
\newblock Learning to flyâ€”a gym environment with pybullet physics for
  reinforcement learning of multi-agent quadcopter control.
\newblock In \emph{2021 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}. IEEE, 2021.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Pesme et~al.(2021)Pesme, Pillaud-Vivien, and
  Flammarion]{pesme2021implicit}
Pesme, S., Pillaud-Vivien, L., and Flammarion, N.
\newblock Implicit bias of sgd for diagonal linear networks: a provable benefit
  of stochasticity.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Qiao et~al.(2020)Qiao, Liang, Koltun, and Lin]{qiao2020scalable}
Qiao, Y.-L., Liang, J., Koltun, V., and Lin, M.~C.
\newblock Scalable differentiable physics for learning and control.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2020.

\bibitem[Rajeswaran et~al.(2017)Rajeswaran, Lowrey, Todorov, and
  Kakade]{rajeswaran2017towards}
Rajeswaran, A., Lowrey, K., Todorov, E.~V., and Kakade, S.~M.
\newblock Towards generalization and simplicity in continuous control.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Razin \& Cohen(2020)Razin and Cohen]{razin2020implicit}
Razin, N. and Cohen, N.
\newblock Implicit regularization in deep learning may not be explainable by
  norms.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Razin et~al.(2021)Razin, Maman, and Cohen]{razin2021implicit}
Razin, N., Maman, A., and Cohen, N.
\newblock Implicit regularization in tensor factorization.
\newblock \emph{International Conference on Machine Learning}, 2021.

\bibitem[Razin et~al.(2022)Razin, Maman, and Cohen]{razin2022implicit}
Razin, N., Maman, A., and Cohen, N.
\newblock Implicit regularization in hierarchical tensor factorization and deep
  convolutional neural networks.
\newblock \emph{International Conference on Machine Learning}, 2022.

\bibitem[Redelmeier(2014)]{redelmeier2014real}
Redelmeier, C. E.~I.
\newblock Real second-order freeness and the asymptotic real second-order
  freeness of several real matrix models.
\newblock \emph{International Mathematics Research Notices}, 2014\penalty0
  (12):\penalty0 3353--3395, 2014.

\bibitem[Shamir(2022)]{shamir2022implicit}
Shamir, O.
\newblock The implicit bias of benign overfitting.
\newblock In \emph{Conference on Learning Theory}. PMLR, 2022.

\bibitem[Shen et~al.(2021)Shen, Liu, He, Zhang, Xu, Yu, and
  Cui]{shen2021towards}
Shen, Z., Liu, J., He, Y., Zhang, X., Xu, R., Yu, H., and Cui, P.
\newblock Towards out-of-distribution generalization: A survey.
\newblock \emph{arXiv preprint arXiv:2108.13624}, 2021.

\bibitem[Sontag(2013)]{sontag2013mathematical}
Sontag, E.~D.
\newblock \emph{Mathematical control theory: deterministic finite dimensional
  systems}, volume~6.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Soudry, D., Hoffer, E., Nacson, M.~S., Gunasekar, S., and Srebro, N.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0 (1),
  2018.

\bibitem[Vardi(2023)]{vardi2023implicit}
Vardi, G.
\newblock On the implicit bias in deep-learning algorithms.
\newblock \emph{Communications of the ACM}, 66\penalty0 (6), 2023.

\bibitem[Vershynin(2020)]{vershynin2020high}
Vershynin, R.
\newblock High-dimensional probability.
\newblock \emph{University of California, Irvine}, 2020.

\bibitem[Wiedemann et~al.(2023)Wiedemann, W{\"u}est, Loquercio, M{\"u}ller,
  Floreano, and Scaramuzza]{wiedemann2023training}
Wiedemann, N., W{\"u}est, V., Loquercio, A., M{\"u}ller, M., Floreano, D., and
  Scaramuzza, D.
\newblock Training efficient controllers via analytic policy gradient.
\newblock In \emph{2023 IEEE International Conference on Robotics and
  Automation (ICRA)}. IEEE, 2023.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8:\penalty0 229--256, 1992.

\bibitem[Witty et~al.(2021)Witty, Lee, Tosch, Atrey, Clary, Littman, and
  Jensen]{witty2021measuring}
Witty, S., Lee, J.~K., Tosch, E., Atrey, A., Clary, K., Littman, M.~L., and
  Jensen, D.
\newblock Measuring and characterizing generalization in deep reinforcement
  learning.
\newblock \emph{Applied AI Letters}, 2\penalty0 (4):\penalty0 e45, 2021.

\bibitem[Woodworth et~al.(2020)Woodworth, Gunasekar, Lee, Moroshko, Savarese,
  Golan, Soudry, and Srebro]{woodworth2020kernel}
Woodworth, B., Gunasekar, S., Lee, J.~D., Moroshko, E., Savarese, P., Golan,
  I., Soudry, D., and Srebro, N.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In \emph{Conference on Learning Theory}, 2020.

\bibitem[Xu et~al.(2022)Xu, Makoviychuk, Narang, Ramos, Matusik, Garg, and
  Macklin]{xu2022accelerated}
Xu, J., Makoviychuk, V., Narang, Y., Ramos, F., Matusik, W., Garg, A., and
  Macklin, M.
\newblock Accelerated policy learning with parallel differentiable simulation.
\newblock \emph{International Conference on Learning Representations}, 2022.

\bibitem[Xu et~al.(2021)Xu, Zhang, Li, Du, Kawarabayashi, and
  Jegelka]{xu2021neural}
Xu, K., Zhang, M., Li, J., Du, S.~S., Kawarabayashi, K.-i., and Jegelka, S.
\newblock How neural networks extrapolate: From feedforward to graph neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Zhang et~al.(2019)Zhang, Ballas, and Pineau]{zhang2019dissection}
Zhang, A., Ballas, N., and Pineau, J.
\newblock A dissection of overfitting and generalization in continuous
  reinforcement learning.
\newblock In \emph{International conference on machine learning}, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Zhang et~al.(2018)Zhang, Vinyals, Munos, and Bengio]{zhang2018study}
Zhang, C., Vinyals, O., Munos, R., and Bengio, S.
\newblock A study on overfitting in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1804.06893}, 2018.

\bibitem[Zhang et~al.(2020)Zhang, Hu, and Basar]{zhang2020policy}
Zhang, K., Hu, B., and Basar, T.
\newblock Policy optimization for $\mathcal{H}_2$ linear control with
  $\mathcal{H}_\infty$ robustness guarantee: Implicit regularization and global
  convergence.
\newblock In \emph{Learning for Dynamics and Control}. PMLR, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Zhang, Hu, and Basar]{zhang2021derivative}
Zhang, K., Zhang, X., Hu, B., and Basar, T.
\newblock Derivative-free policy optimization for linear risk-sensitive and
  robust control design: Implicit regularization and sample complexity.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Zhao et~al.(2023)Zhao, D{\"o}rfler, and You]{zhao2023data}
Zhao, F., D{\"o}rfler, F., and You, K.
\newblock Data-enabled policy optimization for the linear quadratic regulator.
\newblock \emph{arXiv preprint arXiv:2303.17958}, 2023.

\bibitem[Zhou et~al.(2023)Zhou, Bradley, Littwin, Razin, Saremi, Susskind,
  Bengio, and Nakkiran]{zhou2023algorithms}
Zhou, H., Bradley, A., Littwin, E., Razin, N., Saremi, O., Susskind, J.,
  Bengio, S., and Nakkiran, P.
\newblock What algorithms can transformers learn? a study in length
  generalization.
\newblock \emph{arXiv preprint arXiv:2310.16028}, 2023.

\bibitem[Zhu et~al.(2020)Zhu, Yu, Gupta, Shah, Hartikainen, Singh, Kumar, and
  Levine]{zhu2020ingredients}
Zhu, H., Yu, J., Gupta, A., Shah, D., Hartikainen, K., Singh, A., Kumar, V.,
  and Levine, S.
\newblock The ingredients of real-world robotic reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\end{thebibliography}
