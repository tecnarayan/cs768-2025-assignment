\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anastasiou et~al.(2019)Anastasiou, Balasubramanian, and
  Erdogdu]{anastasiou2019normal}
Anastasiou, A., Balasubramanian, K., and Erdogdu, M.~A.
\newblock Normal approximation for stochastic gradient descent via
  non-asymptotic rates of martingale {CLT}.
\newblock \emph{arXiv preprint arXiv:1904.02130}, 2019.

\bibitem[Chatzigeorgiou(2013)]{chatzigeorgiou2013bounds}
Chatzigeorgiou, I.
\newblock Bounds on the {L}ambert function and their application to the outage
  analysis of user cooperation.
\newblock \emph{IEEE Communications Letters}, 17\penalty0 (8):\penalty0
  1505--1508, 2013.

\bibitem[Chaudhari \& Soatto(2018)Chaudhari and
  Soatto]{chaudhari2018stochastic}
Chaudhari, P. and Soatto, S.
\newblock Stochastic gradient descent performs variational inference, converges
  to limit cycles for deep networks.
\newblock In \emph{2018 Information Theory and Applications Workshop (ITA)},
  pp.\  1--10. IEEE, 2018.

\bibitem[Chen et~al.(2016)Chen, Lee, Tong, and Zhang]{chen2016statistical}
Chen, X., Lee, J.~D., Tong, X.~T., and Zhang, Y.
\newblock Statistical inference for model parameters in stochastic gradient
  descent.
\newblock \emph{arXiv preprint arXiv:1610.08637}, 2016.

\bibitem[Cheng et~al.(2018)Cheng, Chatterji, Abbasi-Yadkori, Bartlett, and
  Jordan]{cheng2018sharp}
Cheng, X., Chatterji, N.~S., Abbasi-Yadkori, Y., Bartlett, P.~L., and Jordan,
  M.~I.
\newblock Sharp convergence rates for {L}angevin dynamics in the nonconvex
  setting.
\newblock \emph{arXiv preprint arXiv:1805.01648}, 2018.

\bibitem[Cheng et~al.(2019)Cheng, Bartlett, and Jordan]{cheng2019quantitative}
Cheng, X., Bartlett, P.~L., and Jordan, M.~I.
\newblock Quantitative central limit theorems for discrete stochastic
  processes.
\newblock \emph{arXiv preprint arXiv:1902.00832}, 2019.

\bibitem[Dalalyan(2017)]{dalalyan2017theoretical}
Dalalyan, A.~S.
\newblock Theoretical guarantees for approximate sampling from smooth and
  log-concave densities.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 79\penalty0 (3):\penalty0 651--676, 2017.

\bibitem[Durmus \& Moulines(2016)Durmus and Moulines]{durmus2016high}
Durmus, A. and Moulines, E.
\newblock High-dimensional {B}ayesian inference via the unadjusted langevin
  algorithm.
\newblock \emph{arXiv preprint arXiv:1605.01559}, 2016.

\bibitem[Eberle(2011)]{eberle2011reflection}
Eberle, A.
\newblock Reflection coupling and {W}asserstein contractivity without
  convexity.
\newblock \emph{Comptes Rendus Mathematique}, 349\penalty0 (19-20):\penalty0
  1101--1104, 2011.

\bibitem[Eberle(2016)]{eberle2016reflection}
Eberle, A.
\newblock Reflection couplings and contraction rates for diffusions.
\newblock \emph{Probability theory and related fields}, 166\penalty0
  (3-4):\penalty0 851--886, 2016.

\bibitem[Eldan et~al.(2018)Eldan, Mikulincer, and Zhai]{eldan2018clt}
Eldan, R., Mikulincer, D., and Zhai, A.
\newblock The {CLT} in high dimensions: quantitative bounds via martingale
  embedding.
\newblock \emph{arXiv preprint arXiv:1806.09087}, 2018.

\bibitem[Erdogdu et~al.(2018)Erdogdu, Mackey, and Shamir]{erdogdu2018global}
Erdogdu, M.~A., Mackey, L., and Shamir, O.
\newblock Global non-convex optimization with discretized diffusions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9671--9680, 2018.

\bibitem[Gorham et~al.(2016)Gorham, Duncan, Vollmer, and
  Mackey]{gorham2016measuring}
Gorham, J., Duncan, A.~B., Vollmer, S.~J., and Mackey, L.
\newblock Measuring sample quality with diffusions.
\newblock \emph{arXiv preprint arXiv:1611.06972}, 2016.

\bibitem[He et~al.(2019)He, Liu, and Tao]{he2019control}
He, F., Liu, T., and Tao, D.
\newblock Control batch size and learning rate to generalize well: Theoretical
  and empirical evidence.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1141--1150, 2019.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry]{hoffer2017train}
Hoffer, E., Hubara, I., and Soudry, D.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1731--1741, 2017.

\bibitem[Jastrz{\k{e}}bski et~al.(2017)Jastrz{\k{e}}bski, Kenton, Arpit,
  Ballas, Fischer, Bengio, and Storkey]{jastrzkebski2017three}
Jastrz{\k{e}}bski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio,
  Y., and Storkey, A.
\newblock Three factors influencing minima in {SGD}.
\newblock \emph{arXiv preprint arXiv:1711.04623}, 2017.

\bibitem[Karatzas \& Shreve(1998)Karatzas and Shreve]{karatzas1998brownian}
Karatzas, I. and Shreve, S.~E.
\newblock Brownian motion.
\newblock In \emph{Brownian Motion and Stochastic Calculus}, pp.\  47--127.
  Springer, 1998.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kleinberg et~al.(2018)Kleinberg, Li, and
  Yuan]{kleinberg2018alternative}
Kleinberg, R., Li, Y., and Yuan, Y.
\newblock An alternative view: When does {SGD} escape local minima?
\newblock \emph{arXiv preprint arXiv:1802.06175}, 2018.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Li et~al.(2018)Li, Liu, Kyrillidis, and Caramanis]{li2018statistical}
Li, T., Liu, L., Kyrillidis, A., and Caramanis, C.
\newblock Statistical inference using sgd.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Li et~al.(2019)Li, Wu, and Mackey]{li2019stochastic}
Li, X., Wu, Y., and Mackey, L.
\newblock Stochastic {R}unge-{K}utta accelerates {L}angevin {M}onte {C}arlo and
  beyond.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  7746--7758, 2019.

\bibitem[Ma et~al.(2018)Ma, Chen, Jin, Flammarion, and Jordan]{ma2018sampling}
Ma, Y.-A., Chen, Y., Jin, C., Flammarion, N., and Jordan, M.~I.
\newblock Sampling can be faster than optimization.
\newblock \emph{arXiv preprint arXiv:1811.08413}, 2018.

\bibitem[Mandt et~al.(2016)Mandt, Hoffman, and Blei]{mandt2016variational}
Mandt, S., Hoffman, M., and Blei, D.
\newblock A variational analysis of stochastic gradient algorithms.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  354--363, 2016.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Simsekli et~al.(2019)Simsekli, Sagun, and
  Gurbuzbalaban]{simsekli2019tail}
Simsekli, U., Sagun, L., and Gurbuzbalaban, M.
\newblock A tail-index analysis of stochastic gradient noise in deep neural
  networks.
\newblock \emph{arXiv preprint arXiv:1901.06053}, 2019.

\bibitem[Zhang et~al.(2019)Zhang, Karimireddy, Veit, Kim, Reddi, Kumar, and
  Sra]{zhang2019adam}
Zhang, J., Karimireddy, S.~P., Veit, A., Kim, S., Reddi, S.~J., Kumar, S., and
  Sra, S.
\newblock Why adam beats sgd for attention models.
\newblock \emph{arXiv preprint arXiv:1912.03194}, 2019.

\end{thebibliography}
