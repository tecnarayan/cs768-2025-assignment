\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Chang et~al.(2020)Chang, Flokas, and Lipson]{DBLP:conf/iclr/ChangFL20}
Chang, O., Flokas, L., and Lipson, H.
\newblock Principled weight initialization for hypernetworks.
\newblock In \emph{{ICLR}}. OpenReview.net, 2020.

\bibitem[Elhoushi et~al.(2019)Elhoushi, Tian, Chen, Shafiq, and
  Li]{DBLP:journals/corr/abs-1909-05675}
Elhoushi, M., Tian, Y.~H., Chen, Z., Shafiq, F., and Li, J.~Y.
\newblock Accelerating training using tensor decomposition.
\newblock \emph{CoRR}, abs/1909.05675, 2019.

\bibitem[Gao et~al.(2019)Gao, Cheng, He, Xie, Zhao, Lu, and
  Xiang]{DBLP:journals/corr/abs-1904-06194}
Gao, Z., Cheng, S., He, R., Xie, Z., Zhao, H., Lu, Z., and Xiang, T.
\newblock Compressing deep neural networks by matrix product operators.
\newblock \emph{CoRR}, abs/1904.06194, 2019.

\bibitem[Garipov et~al.(2016)Garipov, Podoprikhin, Novikov, and
  Vetrov]{DBLP:journals/corr/GaripovPNV16}
Garipov, T., Podoprikhin, D., Novikov, A., and Vetrov, D.~P.
\newblock Ultimate tensorization: compressing convolutional and {FC} layers
  alike.
\newblock \emph{CoRR}, abs/1611.03214, 2016.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{DBLP:journals/jmlr/GlorotB10}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{{AISTATS}}, volume~9 of \emph{{JMLR} Proceedings}, pp.\
  249--256. JMLR.org, 2010.

\bibitem[Hayashi et~al.(2019)Hayashi, Yamaguchi, Sugawara, and
  Maeda]{DBLP:conf/nips/HayashiYSM19}
Hayashi, K., Yamaguchi, T., Sugawara, Y., and Maeda, S.
\newblock Exploring unexplored tensor network decompositions for convolutional
  neural networks.
\newblock In \emph{NeurIPS}, pp.\  5553--5563, 2019.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{DBLP:conf/iccv/HeZRS15}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{{ICCV}}, pp.\  1026--1034. {IEEE} Computer Society, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{DBLP:conf/cvpr/HeZRS16}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{{CVPR}}, pp.\  770--778. {IEEE} Computer Society, 2016.

\bibitem[Idelbayev \& Carreira{-}Perpi{\~{n}}{\'{a}}n(2020)Idelbayev and
  Carreira{-}Perpi{\~{n}}{\'{a}}n]{DBLP:conf/cvpr/IdelbayevC20}
Idelbayev, Y. and Carreira{-}Perpi{\~{n}}{\'{a}}n, M.~{\'{A}}.
\newblock Low-rank compression of neural nets: Learning the rank of each layer.
\newblock In \emph{{CVPR}}, pp.\  8046--8056. {IEEE}, 2020.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{DBLP:conf/icml/IoffeS15}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{{ICML}}, volume~37 of \emph{{JMLR} Workshop and Conference
  Proceedings}, pp.\  448--456. JMLR.org, 2015.

\bibitem[Kim et~al.(2016)Kim, Park, Yoo, Choi, Yang, and
  Shin]{DBLP:journals/corr/KimPYCYS15}
Kim, Y., Park, E., Yoo, S., Choi, T., Yang, L., and Shin, D.
\newblock Compression of deep convolutional neural networks for fast and low
  power mobile applications.
\newblock In \emph{{ICLR} (Poster)}, 2016.

\bibitem[Kossaifi et~al.(2020)Kossaifi, Toisoul, Bulat, Panagakis, Hospedales,
  and Pantic]{DBLP:conf/cvpr/KossaifiTBPHP20}
Kossaifi, J., Toisoul, A., Bulat, A., Panagakis, Y., Hospedales, T.~M., and
  Pantic, M.
\newblock Factorized higher-order cnns with an application to spatio-temporal
  emotion estimation.
\newblock In \emph{{CVPR}}, pp.\  6059--6068. {IEEE}, 2020.

\bibitem[Lebedev et~al.(2015)Lebedev, Ganin, Rakhuba, Oseledets, and
  Lempitsky]{DBLP:journals/corr/LebedevGROL14}
Lebedev, V., Ganin, Y., Rakhuba, M., Oseledets, I.~V., and Lempitsky, V.~S.
\newblock Speeding-up convolutional neural networks using fine-tuned
  cp-decomposition.
\newblock In \emph{{ICLR} (Poster)}, 2015.

\bibitem[Li \& Sun(2020)Li and Sun]{DBLP:conf/icml/LiS20}
Li, C. and Sun, Z.
\newblock Evolutionary topology search for tensor network decomposition.
\newblock In \emph{{ICML}}, volume 119 of \emph{Proceedings of Machine Learning
  Research}, pp.\  5947--5957. {PMLR}, 2020.

\bibitem[Li et~al.(2021)Li, Pan, Chen, Ding, Zhao, and Xu]{li2021heuristic}
Li, N., Pan, Y., Chen, Y., Ding, Z., Zhao, D., and Xu, Z.
\newblock Heuristic rank selection with progressively searching tensor ring
  network.
\newblock \emph{Complex \& Intelligent Systems}, pp.\  1--15, 2021.

\bibitem[Liu et~al.(2021)Liu, Dai, So, and
  Le]{DBLP:journals/corr/abs-2105-08050}
Liu, H., Dai, Z., So, D.~R., and Le, Q.~V.
\newblock Pay attention to mlps.
\newblock \emph{CoRR}, abs/2105.08050, 2021.

\bibitem[Novikov et~al.(2015)Novikov, Podoprikhin, Osokin, and
  Vetrov]{DBLP:conf/nips/NovikovPOV15}
Novikov, A., Podoprikhin, D., Osokin, A., and Vetrov, D.~P.
\newblock Tensorizing neural networks.
\newblock In \emph{{NIPS}}, pp.\  442--450, 2015.

\bibitem[Pan et~al.(2019)Pan, Xu, Wang, Ye, Wang, Bai, and
  Xu]{DBLP:conf/aaai/PanXWYWBX19}
Pan, Y., Xu, J., Wang, M., Ye, J., Wang, F., Bai, K., and Xu, Z.
\newblock Compressing recurrent neural networks with tensor ring for action
  recognition.
\newblock In \emph{{AAAI}}, pp.\  4683--4690. {AAAI} Press, 2019.

\bibitem[Pan et~al.(2022)Pan, Wang, and Xu]{DBLP:journals/ijon/PanWX22}
Pan, Y., Wang, M., and Xu, Z.
\newblock Tednet: {A} pytorch toolkit for tensor decomposition networks.
\newblock \emph{Neurocomputing}, 469:\penalty0 234--238, 2022.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, K{\"{o}}pf, Yang, DeVito,
  Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and
  Chintala]{DBLP:conf/nips/PaszkeGMLBCKLGA19}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., K{\"{o}}pf, A., Yang,
  E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang,
  L., Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{NeurIPS}, pp.\  8024--8035, 2019.

\bibitem[Rigamonti et~al.(2013)Rigamonti, Sironi, Lepetit, and
  Fua]{DBLP:conf/cvpr/RigamontiSLF13}
Rigamonti, R., Sironi, A., Lepetit, V., and Fua, P.
\newblock Learning separable filters.
\newblock In \emph{{CVPR}}, pp.\  2754--2761. {IEEE} Computer Society, 2013.

\bibitem[Springenberg et~al.(2015)Springenberg, Dosovitskiy, Brox, and
  Riedmiller]{DBLP:journals/corr/SpringenbergDBR14}
Springenberg, J.~T., Dosovitskiy, A., Brox, T., and Riedmiller, M.~A.
\newblock Striving for simplicity: The all convolutional net.
\newblock In \emph{{ICLR} (Workshop)}, 2015.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{DBLP:conf/cvpr/SzegedyVISW16}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{{CVPR}}, pp.\  2818--2826. {IEEE} Computer Society, 2016.

\bibitem[Taki(2017)]{DBLP:journals/corr/abs-1709-02956}
Taki, M.
\newblock Deep residual networks and weight initialization.
\newblock \emph{CoRR}, abs/1709.02956, 2017.

\bibitem[Tolstikhin et~al.(2021)Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai,
  Unterthiner, Yung, Steiner, Keysers, Uszkoreit, Lucic, and
  Dosovitskiy]{DBLP:journals/corr/abs-2105-01601}
Tolstikhin, I.~O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X.,
  Unterthiner, T., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., Lucic,
  M., and Dosovitskiy, A.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock \emph{CoRR}, abs/2105.01601, 2021.

\bibitem[Wang et~al.(2020)Wang, Su, Luo, Pan, Zheng, and
  Xu]{DBLP:conf/iconip/WangSL0ZX20}
Wang, M., Su, Z., Luo, X., Pan, Y., Zheng, S., and Xu, Z.
\newblock Concatenated tensor networks for deep multi-task learning.
\newblock In \emph{{ICONIP} {(5)}}, volume 1333 of \emph{Communications in
  Computer and Information Science}, pp.\  517--525. Springer, 2020.

\bibitem[Wang et~al.(2018)Wang, Sun, Eriksson, Wang, and
  Aggarwal]{DBLP:conf/cvpr/WangSEWA18}
Wang, W., Sun, Y., Eriksson, B., Wang, W., and Aggarwal, V.
\newblock Wide compression: Tensor ring nets.
\newblock In \emph{{CVPR}}, pp.\  9329--9338. {IEEE} Computer Society, 2018.

\bibitem[Ye et~al.(2018)Ye, Wang, Li, Chen, Zhe, Chu, and
  Xu]{DBLP:conf/cvpr/YeWLCZCX18}
Ye, J., Wang, L., Li, G., Chen, D., Zhe, S., Chu, X., and Xu, Z.
\newblock Learning compact recurrent neural networks with block-term tensor
  decomposition.
\newblock In \emph{{CVPR}}, pp.\  9378--9387. {IEEE} Computer Society, 2018.

\bibitem[Ye et~al.(2020)Ye, Li, Chen, Yang, Zhe, and
  Xu]{DBLP:journals/nn/YeLCYZX20}
Ye, J., Li, G., Chen, D., Yang, H., Zhe, S., and Xu, Z.
\newblock Block-term tensor neural networks.
\newblock \emph{Neural Networks}, 130:\penalty0 11--21, 2020.

\bibitem[Yin et~al.(2021)Yin, Sui, Liao, and Yuan]{DBLP:conf/cvpr/YinSL021}
Yin, M., Sui, Y., Liao, S., and Yuan, B.
\newblock Towards efficient tensor decomposition-based {DNN} model compression
  with optimization framework.
\newblock In \emph{{CVPR}}, pp.\  10674--10683. Computer Vision Foundation /
  {IEEE}, 2021.

\end{thebibliography}
