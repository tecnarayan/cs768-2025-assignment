@article{zhang2020task,
  title={Task-agnostic exploration in reinforcement learning},
  author={Zhang, Xuezhou and Singla, Adish and others},
  journal={arXiv preprint arXiv:2006.09497},
  year={2020}
}

@article{zanette2020provably,
  title={Provably efficient reward-agnostic navigation with linear value iteration},
  author={Zanette, Andrea and Lazaric, Alessandro and Kochenderfer, Mykel J and Brunskill, Emma},
  journal={arXiv preprint arXiv:2008.07737},
  year={2020}
}

@article{brantley2020constrained,
  title={Constrained episodic reinforcement learning in concave-convex and knapsack settings},
  author={Brantley, Kiant{\'e} and Dudik, Miroslav and Lykouris, Thodoris and Miryoosefi, Sobhan and Simchowitz, Max and Slivkins, Aleksandrs and Sun, Wen},
  journal={arXiv preprint arXiv:2006.05051},
  year={2020}
}

@article{sun2019provably,
  title={Provably efficient imitation learning from observation alone},
  author={Sun, Wen and Vemula, Anirudh and Boots, Byron and Bagnell, J Andrew},
  journal={arXiv preprint arXiv:1905.10948},
  year={2019}
}

@inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K},
  booktitle={Aaai},
  volume={8},
  pages={1433--1438},
  year={2008},
  organization={Chicago, IL, USA}
}


@inproceedings{SyedSchapire2008,
author = {Syed, Umar and Schapire, Robert E.},
title = {A Game-Theoretic Approach to Apprenticeship Learning},
year = {2007},
isbn = {9781605603520},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 20th International Conference on Neural Information Processing Systems},
pages = {1449–1456},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS’07}
}

@inproceedings{jabbari2017fairness,
  title={Fairness in reinforcement learning},
  author={Jabbari, Shahin and Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie and Roth, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={1617--1626},
  year={2017},
  organization={PMLR}
}

@inproceedings{Mao2016RLSystems,
    author = {Mao, Hongzi and Alizadeh, Mohammad and Menache, Ishai and Kandula, Srikanth},
    title = {Resource Management with Deep Reinforcement Learning},
    year = {2016}, isbn = {9781450346610},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3005745.3005750},
    doi = {10.1145/3005745.3005750},
    booktitle = {Proceedings of the 15th ACM Workshop on Hot Topics in Networks},
    pages = {50–56},
    numpages = {7},
}

@article{DynPricing-ec12,
  author    = {Moshe Babaioff and
               Shaddin Dughmi and
               Robert D. Kleinberg and
               Aleksandrs Slivkins},
  title     = {Dynamic Pricing with Limited Supply},
  journal   = {TEAC},
  volume    = {3},
  number    = {1},
  pages     = {4},
  year      = {2015},
  note      = {Special issue for \emph{13th ACM EC}, 2012.}
}

@article{BZ09,
 author = {Besbes, Omar and Zeevi, Assaf},
 title = {Dynamic Pricing Without Knowing the Demand Function: Risk Bounds and Near-Optimal Algorithms},
 journal = {Operations Research},
 volume = {57},
 number = {6},
 year = {2009},
 pages = {1407--1420},
}

@article{vinyals2019grandmaster,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}


@article{Hoang2019BPLUC,
  author    = {Hoang Minh Le and
               Cameron Voloshin and
               Yisong Yue},
  title     = {Batch Policy Learning under Constraints},
  journal   = {CoRR},
  volume    = {abs/1903.08738},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.08738},
  archivePrefix = {arXiv},
  eprint    = {1903.08738},
  timestamp = {Thu, 06 Jun 2019 18:03:43 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1903-08738},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
tessler2018reward,
title={Reward Constrained Policy Optimization},
author={Chen Tessler and Daniel J. Mankowitz and Shie Mannor},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SkfrvsA9FX},
}

@inproceedings{agarwal2014taming,
  title={Taming the monster: A fast and simple algorithm for contextual bandits},
  author={Agarwal, Alekh and Hsu, Daniel and Kale, Satyen and Langford, John and Li, Lihong and Schapire, Robert},
  booktitle={International Conference on Machine Learning},
  pages={1638--1646},
  year={2014}
}
@article{bartlett2008high,
  title={High-probability regret bounds for bandit online linear optimization},
  author={Bartlett, Peter L and Dani, Varsha and Hayes, Thomas and Kakade, Sham and Rakhlin, Alexander and Tewari, Ambuj},
  year={2008}
}
@inproceedings{osband2014model,
  title={Model-based reinforcement learning and the eluder dimension},
  author={Osband, Ian and Van Roy, Benjamin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1466--1474},
  year={2014}
}
@inproceedings{russo2013eluder,
  title={Eluder dimension and the sample complexity of optimistic exploration},
  author={Russo, Daniel and Van Roy, Benjamin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2256--2264},
  year={2013}
}
@article{wang2020provably,
  title={Provably Efficient Reinforcement Learning with General Value Function Approximation},
  author={Wang, Ruosong and Salakhutdinov, Ruslan and Yang, Lin F},
  journal={arXiv preprint arXiv:2005.10804},
  year={2020}
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}
@article{wu2020accommodating,
  title={Accommodating Picky Customers: Regret Bound and Exploration Complexity for Multi-Objective Reinforcement Learning},
  author={Wu, Jingfeng and Braverman, Vladimir and Yang, Lin F},
  journal={arXiv preprint arXiv:2011.13034},
  year={2020}
}
@inproceedings{jin2020reward,
  title={Reward-free exploration for reinforcement learning},
  author={Jin, Chi and Krishnamurthy, Akshay and Simchowitz, Max and Yu, Tiancheng},
  booktitle={International Conference on Machine Learning},
  pages={4870--4879},
  year={2020},
  organization={PMLR}
}
@inproceedings{kaufmann2021adaptive,
  title={Adaptive reward-free exploration},
  author={Kaufmann, Emilie and M{\'e}nard, Pierre and Domingues, Omar Darwiche and Jonsson, Anders and Leurent, Edouard and Valko, Michal},
  booktitle={Algorithmic Learning Theory},
  pages={865--891},
  year={2021},
  organization={PMLR}
}

@article{wang2020reward,
  title={On reward-free reinforcement learning with linear function approximation},
  author={Wang, Ruosong and Du, Simon S and Yang, Lin F and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2006.11274},
  year={2020}
}

@inproceedings{abernethy2011blackwell,
  title={Blackwell approachability and no-regret learning are equivalent},
  author={Abernethy, Jacob and Bartlett, Peter L and Hazan, Elad},
  booktitle={Proceedings of the 24th Annual Conference on Learning Theory},
  pages={27--46},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}
@inproceedings{zinkevich2003online,
  title={Online convex programming and generalized infinitesimal gradient ascent},
  author={Zinkevich, Martin},
  booktitle={Proceedings of the 20th international conference on machine learning (icml-03)},
  pages={928--936},
  year={2003}
}
@book{rockafellar2015convex,
  title={Convex analysis},
  author={Rockafellar, Ralph Tyrell},
  year={2015},
  publisher={Princeton university press}
}
@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={263--272},
  year={2017},
  organization={PMLR}
}

@InProceedings{BaiChi20a, title = {Provable Self-Play Algorithms for Competitive Reinforcement Learning}, author = {Bai, Yu and Jin, Chi}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {551--560}, year = {2020}, editor = {Hal Daumé III and Aarti Singh}, volume = {119}, series = {Proceedings of Machine Learning Research}, month = {13--18 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v119/bai20a/bai20a.pdf}, url = { http://proceedings.mlr.press/v119/bai20a.html }, abstract = {Self-play, where the algorithm learns by playing against itself without requiring any direct supervision, has become the new weapon in modern Reinforcement Learning (RL) for achieving superhuman performance in practice. However, the majority of exisiting theory in reinforcement learning only applies to the setting where the agent plays against a fixed environment; it remains largely open whether self-play algorithms can be provably effective, especially when it is necessary to manage the exploration/exploitation tradeoff. We study self-play in competitive reinforcement learning under the setting of Markov games, a generalization of Markov decision processes to the two-player case. We introduce a self-play algorithm—Value Iteration with Upper/Lower Confidence Bound (VI-ULCB)—and show that it achieves regret $\mathcal{\tilde{O}}(\sqrt{T})$ after playing $T$ steps of the game, where the regret is measured by the agent’s performance against a fully adversarial opponent who can exploit the agent’s strategy at any step. We also introduce an explore-then-exploit style algorithm, which achieves a slightly worse regret of $\mathcal{\tilde{O}}(T^{2/3})$, but is guaranteed to run in polynomial time even in the worst case. To the best of our knowledge, our work presents the first line of provably sample-efficient self-play algorithms for competitive reinforcement learning.} }

@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2137--2143},
  year={2020},
  organization={PMLR}
}
@book{filar2012competitive,
  title={Competitive Markov decision processes},
  author={Filar, Jerzy and Vrieze, Koos},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{jin2019short,
  title={A short note on concentration inequalities for random vectors with subgaussian norm},
  author={Jin, Chi and Netrapalli, Praneeth and Ge, Rong and Kakade, Sham M and Jordan, Michael I},
  journal={arXiv preprint arXiv:1902.03736},
  year={2019}
}

@inproceedings{miryoosefi2019,
 author = {Miryoosefi, Sobhan and Brantley, Kiant\'{e} and Daume III, Hal and Dudik, Miro and Schapire, Robert E},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {14093--14102},
 publisher = {Curran Associates, Inc.},
 title = {Reinforcement Learning with Convex Constraints},
 url = {https://proceedings.neurips.cc/paper/2019/file/873be0705c80679f2c71fbf4d872df59-Paper.pdf},
 volume = {32},
 year = {2019}
}

@book{altman1999constrained,
  title={Constrained Markov decision processes},
  author={Altman, Eitan},
  volume={7},
  year={1999},
  publisher={CRC Press}
}
@article{blackwell1956analog,
  title={An analog of the minimax theorem for vector payoffs.},
  author={Blackwell, David and others},
  journal={Pacific Journal of Mathematics},
  volume={6},
  number={1},
  pages={1--8},
  year={1956},
  publisher={Pacific Journal of Mathematics}
}

@article{yu2021provably,
  title={Provably Efficient Algorithms for Multi-Objective Competitive RL},
  author={Yu, Tiancheng and Tian, Yi and Zhang, Jingzhao and Sra, Suvrit},
  journal={arXiv preprint arXiv:2102.03192},
  year={2021}
}

@article{freund1999adaptive,
  title={Adaptive game playing using multiplicative weights},
  author={Freund, Yoav and Schapire, Robert E},
  journal={Games and Economic Behavior},
  volume={29},
  number={1-2},
  pages={79--103},
  year={1999},
  publisher={Elsevier}
}

@article{liu2020sharp,
  title={A Sharp Analysis of Model-based Reinforcement Learning with Self-Play},
  author={Liu, Qinghua and Yu, Tiancheng and Bai, Yu and Jin, Chi},
  journal={arXiv preprint arXiv:2010.01604},
  year={2020}
}


@article{efroni2020exploration,
  title={Exploration-Exploitation in Constrained MDPs},
  author={Efroni, Yonathan and Mannor, Shie and Pirotta, Matteo},
  journal={arXiv preprint arXiv:2003.02189},
  year={2020}
}

@article{qiu2020upper,
  title={Upper confidence primal-dual optimization: Stochastically constrained markov decision processes with adversarial losses and unknown transitions},
  author={Qiu, Shuang and Wei, Xiaohan and Yang, Zhuoran and Ye, Jieping and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2003.00660},
  year={2020}
}


@inproceedings{miryoosefi2020,
 author = {Brantley, Kiant\'{e} and Dudik, Miro and Lykouris, Thodoris and Miryoosefi, Sobhan and Simchowitz, Max and Slivkins, Aleksandrs and Sun, Wen},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {16315--16326},
 publisher = {Curran Associates, Inc.},
 title = {Constrained episodic reinforcement learning in concave-convex and knapsack settings},
 url = {https://proceedings.neurips.cc/paper/2020/file/bc6d753857fe3dd4275dff707dedf329-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{singh2020learning,
  title={Learning in Markov Decision Processes under Constraints},
  author={Singh, Rahul and Gupta, Abhishek and Shroff, Ness B},
  journal={arXiv preprint arXiv:2002.12435},
  year={2020}
}

\

@InProceedings{ding2021provably,
  title = 	 { Provably Efficient Safe Exploration via Primal-Dual Policy Optimization },
  author =       {Ding, Dongsheng and Wei, Xiaohan and Yang, Zhuoran and Wang, Zhaoran and Jovanovic, Mihailo},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3304--3312},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/ding21d/ding21d.pdf},
  url = 	 {http://proceedings.mlr.press/v130/ding21d.html},
  abstract = 	 { We study the safe reinforcement learning problem using the constrained Markov decision processes in which an agent aims to maximize the expected total reward subject to a safety constraint on the expected total value of a utility function. We focus on an episodic setting with the function approximation where the Markov transition kernels have a linear structure but do not impose any additional assumptions on the sampling model. Designing safe reinforcement learning algorithms with provable computational and statistical efficiency is particularly challenging under this setting because of the need to incorporate both the safety constraint and the function approximation into the fundamental exploitation/exploration tradeoff. To this end, we present an \underline{O}ptimistic \underline{P}rimal-\underline{D}ual Proximal Policy \underline{OP}timization \mbox{(OPDOP)} algorithm where the value function is estimated by combining the least-squares policy evaluation and an additional bonus term for safe exploration. We prove that the proposed algorithm achieves an $\tilde{O}(d H^{2.5}\sqrt{T})$ regret and an $\tilde{O}(d H^{2.5}\sqrt{T})$ constraint violation, where $d$ is the dimension of the feature mapping, $H$ is the horizon of each episode, and $T$ is the total number of steps. These bounds hold when the reward/utility functions are fixed but the feedback after each episode is bandit. Our bounds depend on the capacity of the state-action space only through the dimension of the feature mapping and thus our results hold even when the number of states goes to infinity. To the best of our knowledge, we provide the first provably efficient online policy optimization algorithm for constrained Markov decision processes in the function approximation setting, with safe exploration. }
}

@article{neumann1928theorie,
  title={Zur theorie der gesellschaftsspiele},
  author={Neumann, J v},
  journal={Mathematische annalen},
  volume={100},
  number={1},
  pages={295--320},
  year={1928},
  publisher={Springer}
}

