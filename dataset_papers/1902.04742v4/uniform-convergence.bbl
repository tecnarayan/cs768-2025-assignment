\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen{-}Zhu et~al.(2018)Allen{-}Zhu, Li, and Liang]{zhu18beyond}
Zeyuan Allen{-}Zhu, Yuanzhi Li, and Yingyu Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock abs/1811.04918, 2018.
\newblock URL \url{http://arxiv.org/abs/1811.04918}.

\bibitem[Arora et~al.(2018)Arora, Ge, Neyshabur, and Zhang]{arora18compression}
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi~Zhang.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock In \emph{The 35th International Conference on Machine Learning,
  {ICML}}, 2018.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett17spectral}
Peter~L. Bartlett, Dylan~J. Foster, and Matus~J. Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017}, 2017.

\bibitem[Belkin et~al.(2018)Belkin, Ma, and Mandal]{belkin18kernel}
Mikhail Belkin, Siyuan Ma, and Soumik Mandal.
\newblock To understand deep learning we need to understand kernel learning.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018}, 2018.

\bibitem[Bousquet and Elisseeff(2002)]{bousquet02stability}
Olivier Bousquet and Andr{\'{e}} Elisseeff.
\newblock Stability and generalization.
\newblock \emph{Journal of Machine Learning Research}, 2, 2002.

\bibitem[Brutzkus et~al.(2018)Brutzkus, Globerson, Malach, and
  Shalev{-}Shwartz]{brutzkus18sgd}
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev{-}Shwartz.
\newblock {SGD} learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Dr{\"{a}}xler et~al.(2018)Dr{\"{a}}xler, Veschgini, Salmhofer, and
  Hamprecht]{felix18barriers}
Felix Dr{\"{a}}xler, Kambis Veschgini, Manfred Salmhofer, and Fred~A.
  Hamprecht.
\newblock Essentially no barriers in neural network energy landscape.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018}, 2018.

\bibitem[Dziugaite and Roy(2017)]{dziugaite17nonvacuous}
Gintare~Karolina Dziugaite and Daniel~M. Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock In \emph{Proceedings of the Thirty-Third Conference on Uncertainty in
  Artificial Intelligence, {UAI} 2017}, 2017.

\bibitem[Feldman and Vondr{\'{a}}k(2018)]{feldman18uniformly}
Vitaly Feldman and Jan Vondr{\'{a}}k.
\newblock Generalization bounds for uniformly stable algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018},
  2018.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{garipov18loss}
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry~P. Vetrov, and
  Andrew~G. Wilson.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock In \emph{Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018},
  2018.

\bibitem[Gilmer et~al.(2018)Gilmer, Metz, Faghri, Schoenholz, Raghu,
  Wattenberg, and Goodfellow]{gilmer18adversarial}
Justin Gilmer, Luke Metz, Fartash Faghri, Samuel~S. Schoenholz, Maithra Raghu,
  Martin Wattenberg, and Ian~J. Goodfellow.
\newblock Adversarial spheres.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018}, 2018.

\bibitem[Golowich et~al.(2018)Golowich, Rakhlin, and Shamir]{golowich17size}
Noah Golowich, Alexander Rakhlin, and Ohad Shamir.
\newblock Size-independent sample complexity of neural networks.
\newblock \emph{Computational Learning Theory, {COLT} 2018}, 2018.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt16stability}
Moritz Hardt, Ben Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In \emph{Proceedings of the 33nd International Conference on Machine
  Learning, {ICML}}, 2016.

\bibitem[Harvey et~al.(2017)Harvey, Liaw, and Mehrabian]{harvey17vc}
Nick Harvey, Christopher Liaw, and Abbas Mehrabian.
\newblock Nearly-tight vc-dimension bounds for piecewise linear neural
  networks.
\newblock In \emph{Proceedings of the 30th Conference on Learning Theory,
  {COLT} 2017}, 2017.

\bibitem[Hinton and van Camp(1993)]{hinton93mdl}
Geoffrey~E. Hinton and Drew van Camp.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In \emph{Proceedings of the Sixth Annual {ACM} Conference on
  Computational Learning Theory, {COLT}}, 1993.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter97flat}
Sepp Hochreiter and J{\"{u}}rgen Schmidhuber.
\newblock Flat minima.
\newblock \emph{Neural Computation}, 9\penalty0 (1), 1997.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry]{hoffer17longer}
Elad Hoffer, Itay Hubara, and Daniel Soudry.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock \emph{Advances in Neural Information Processing Systems ({\em to
  appear})}, 2017.

\bibitem[Jacot et~al.(2018)Jacot, Hongler, and Gabriel]{jacot18ntk}
Arthur Jacot, Cl{\'{e}}ment Hongler, and Franck Gabriel.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018},
  2018.

\bibitem[Jastrzebski et~al.(2018)Jastrzebski, Kenton, Arpit, Ballas, Fischer,
  Bengio, and Storkey]{jastrzebski18width}
Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja
  Fischer, Yoshua Bengio, and Amos~J. Storkey.
\newblock Width of minima reached by stochastic gradient descent is influenced
  by learning rate to batch size ratio.
\newblock In \emph{Artificial Neural Networks and Machine Learning - {ICANN}
  2018 - 27th International Conference on Artificial Neural Networks}, 2018.

\bibitem[Kawaguchi et~al.(2017)Kawaguchi, Kaelbling, and
  Bengio]{kawaguchi17generalization}
Kenji Kawaguchi, Leslie~Pack Kaelbling, and Yoshua Bengio.
\newblock Generalization in deep learning.
\newblock 2017.
\newblock URL \url{http://arxiv.org/abs/1710.05468}.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar17largebatch}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2017.

\bibitem[Langford and Caruana(2001)]{langford01not}
John Langford and Rich Caruana.
\newblock (not) bounding the true error.
\newblock In \emph{Advances in Neural Information Processing Systems 14 [Neural
  Information Processing Systems: Natural and Synthetic, {NIPS} 2001]}, 2001.

\bibitem[Langford and Shawe{-}Taylor(2002)]{langford02pacbayes}
John Langford and John Shawe{-}Taylor.
\newblock Pac-bayes {\&} margins.
\newblock In \emph{Advances in Neural Information Processing Systems 15 [Neural
  Information Processing Systems, {NIPS} 2002}, 2002.

\bibitem[Li and Liang(2018)]{li18learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \emph{Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018},
  2018.

\bibitem[McAllester(2003)]{mcallester03simplified}
David McAllester.
\newblock Simplified pac-bayesian margin bounds.
\newblock In \emph{Learning Theory and Kernel Machines}. Springer Berlin
  Heidelberg, 2003.

\bibitem[Mohri et~al.(2012)Mohri, Rostamizadeh, and
  Talwalkar]{mohri12foundations}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock \emph{Foundations of Machine Learning}.
\newblock Adaptive computation and machine learning. {MIT} Press, 2012.

\bibitem[Nagarajan and Kolter(2017)]{nagarajan17role}
Vaishnavh Nagarajan and J.~Zico Kolter.
\newblock Generalization in deep networks: The role of distance from
  initialization.
\newblock \emph{Deep Learning: Bridging Theory and Practice Workshop in
  Advances in Neural Information Processing Systems 30: Annual Conference on
  Neural Information Processing Systems 2017}, 2017.

\bibitem[Nagarajan and Kolter(2019)]{nagarajan2018deterministic}
Vaishnavh Nagarajan and Zico Kolter.
\newblock Deterministic {PAC}-bayesian generalization bounds for deep networks
  via generalizing noise-resilience.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Neyshabur et~al.(2015{\natexlab{a}})Neyshabur, Tomioka, and
  Srebro]{neyshabur15inductive}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock \emph{International Conference on Learning Representations Workshop
  Track}, 2015{\natexlab{a}}.

\bibitem[Neyshabur et~al.(2015{\natexlab{b}})Neyshabur, Tomioka, and
  Srebro]{neyshabur15norm}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock Norm-based capacity control in neural networks.
\newblock In \emph{Proceedings of The 28th Conference on Learning Theory,
  {COLT}}, 2015{\natexlab{b}}.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{neyshabur17exploring}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro.
\newblock Exploring generalization in deep learning.
\newblock \emph{Advances in Neural Information Processing Systems {\em to
  appear}}, 2017.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{neyshabur18pacbayes}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro.
\newblock A pac-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Neyshabur et~al.(2019)Neyshabur, Li, Bhojanapalli, LeCun, and
  Srebro]{neyshabur18unitwise}
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan
  Srebro.
\newblock The role of over-parametrization in generalization of neural
  networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Rogers and Wagner(1978)]{rogerss78finite}
W.~H. Rogers and T.~J. Wagner.
\newblock A finite sample distribution-free performance bound for local
  discrimination rules.
\newblock \emph{The Annals of Statistics}, 6\penalty0 (3), 1978.

\bibitem[Shalev{-}Shwartz et~al.(2010)Shalev{-}Shwartz, Shamir, Srebro, and
  Sridharan]{shwartz10learnability}
Shai Shalev{-}Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan.
\newblock Learnability, stability and uniform convergence.
\newblock \emph{Journal of Machine Learning Research}, 11, 2010.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, and Srebro]{soudry18sgd}
Daniel Soudry, Elad Hoffer, and Nathan Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Vapnik and Chervonenkis(1971)]{vapnik71uniform}
V.~N. Vapnik and A.~Ya. Chervonenkis.
\newblock \emph{On the Uniform Convergence of Relative Frequencies of Events to
  Their Probabilities}.
\newblock 1971.

\bibitem[Wainwright(2019)]{wainwright19high}
Martin~J. Wainwright.
\newblock \emph{High-Dimensional Statistics: A Non-Asymptotic Viewpoint}.
\newblock Cambridge Series in Statistical and Probabilistic Mathematics.
  Cambridge University Press, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang17generalization}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2017.

\bibitem[Zhou et~al.(2019)Zhou, Veitch, Austern, Adams, and
  Orbanz]{zhou2018nonvacuous}
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan~P. Adams, and Peter Orbanz.
\newblock Non-vacuous generalization bounds at the imagenet scale: a
  {PAC}-bayesian compression approach.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\end{thebibliography}
