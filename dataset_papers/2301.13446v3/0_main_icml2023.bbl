\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal \& Jia(2017)Agrawal and
  Jia]{paper:optimistic_posterior_sampling}
Agrawal, S. and Jia, R.
\newblock Optimistic posterior sampling for reinforcement learning: worst-case
  regret bounds.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Auer et~al.(2008)Auer, Jaksch, and Ortner]{paper:ucrl2}
Auer, P., Jaksch, T., and Ortner, R.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 21, 2008.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{paper:ucbvi}
Azar, M.~G., Osband, I., and Munos, R.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{ICML}, 2017.

\bibitem[Bai et~al.(2019)Bai, Xie, Jiang, and
  Wang]{paper:q_learning_low_switch_cost}
Bai, Y., Xie, T., Jiang, N., and Wang, Y.-X.
\newblock Provably efficient q-learning with low switching cost.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Bartlett \& Tewari(2012)Bartlett and Tewari]{paper:regal}
Bartlett, P.~L. and Tewari, A.
\newblock Regal: A regularization based algorithm for reinforcement learning in
  weakly communicating mdps.
\newblock \emph{arXiv preprint arXiv:1205.2661}, 2012.

\bibitem[Bertsekas(2012)]{book:dp_opt_control}
Bertsekas, D.
\newblock \emph{Dynamic programming and optimal control: Volume I}, volume~1.
\newblock Athena scientific, 2012.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Chen et~al.(2021)Chen, Jafarnia-Jahromi, Jain, and Luo]{paper:lcb_ssp}
Chen, L., Jafarnia-Jahromi, M., Jain, R., and Luo, H.
\newblock Implicit finite-horizon approximation and efficient optimal
  algorithms for stochastic shortest path.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{paper:ubev}
Dann, C., Lattimore, T., and Brunskill, E.
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock In \emph{NIPS}, 2017.

\bibitem[Dann et~al.(2019)Dann, Li, Wei, and Brunskill]{paper:orlc}
Dann, C., Li, L., Wei, W., and Brunskill, E.
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1507--1516. PMLR, 2019.

\bibitem[Domingues et~al.(2021)Domingues, M{\'e}nard, Kaufmann, and
  Valko]{paper:episodic_lower_bound}
Domingues, O.~D., M{\'e}nard, P., Kaufmann, E., and Valko, M.
\newblock Episodic reinforcement learning in finite mdps: Minimax lower bounds
  revisited.
\newblock In Feldman, V., Ligett, K., and Sabato, S. (eds.), \emph{Proceedings
  of the 32nd International Conference on Algorithmic Learning Theory}, volume
  132 of \emph{Proceedings of Machine Learning Research}, pp.\  578--598. PMLR,
  16--19 Mar 2021.
\newblock URL \url{https://proceedings.mlr.press/v132/domingues21a.html}.

\bibitem[Even-Dar et~al.(2006)Even-Dar, Mannor, Mansour, and
  Mahadevan]{paper:action_elimination}
Even-Dar, E., Mannor, S., Mansour, Y., and Mahadevan, S.
\newblock Action elimination and stopping conditions for the multi-armed bandit
  and reinforcement learning problems.
\newblock \emph{Journal of machine learning research}, 7\penalty0 (6), 2006.

\bibitem[Fruit et~al.(2018{\natexlab{a}})Fruit, Pirotta, and
  Lazaric]{fruit2018near}
Fruit, R., Pirotta, M., and Lazaric, A.
\newblock Near optimal exploration-exploitation in non-communicating markov
  decision processes.
\newblock \emph{Advances in Neural Information Processing Systems}, 31,
  2018{\natexlab{a}}.

\bibitem[Fruit et~al.(2018{\natexlab{b}})Fruit, Pirotta, Lazaric, and
  Ortner]{paper:scal}
Fruit, R., Pirotta, M., Lazaric, A., and Ortner, R.
\newblock Efficient bias-span-constrained exploration-exploitation in
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1578--1586. PMLR, 2018{\natexlab{b}}.

\bibitem[Garivier et~al.(2016)Garivier, Ménard, and
  Stoltz]{paper:regret_bandit}
Garivier, A., Ménard, P., and Stoltz, G.
\newblock Explore first, exploit next: The true shape of regret in bandit
  problems.
\newblock \emph{Mathematics of Operations Research}, 44, 02 2016.
\newblock \doi{10.1287/moor.2017.0928}.

\bibitem[Jiang \& Agarwal(2018)Jiang and Agarwal]{jiang2018open}
Jiang, N. and Agarwal, A.
\newblock Open problem: The dependence of sample complexity lower bounds on
  planning horizon.
\newblock In \emph{Conference On Learning Theory}, pp.\  3395--3398. PMLR,
  2018.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{paper:q_learning}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I.
\newblock Is q-learning provably efficient?
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Jin et~al.(2020)Jin, Krishnamurthy, Simchowitz, and
  Yu]{paper:reward_free_exploration}
Jin, C., Krishnamurthy, A., Simchowitz, M., and Yu, T.
\newblock Reward-free exploration for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4870--4879. PMLR, 2020.

\bibitem[Kim et~al.(2021)Kim, Yang, and Jun]{kim2021improved}
Kim, Y., Yang, I., and Jun, K.-S.
\newblock Improved regret analysis for variance-adaptive linear bandits and
  horizon-free linear mixture mdps.
\newblock \emph{arXiv preprint arXiv:2111.03289}, 2021.

\bibitem[Lattimore \& Szepesvári(2020)Lattimore and
  Szepesvári]{paper:bandit_algorithms}
Lattimore, T. and Szepesvári, C.
\newblock \emph{Bandit Algorithms}.
\newblock Cambridge University Press, 2020.
\newblock \doi{10.1017/9781108571401}.

\bibitem[Li et~al.(2021)Li, Shi, Chen, Gu, and Chi]{paper:model_free_breaking}
Li, G., Shi, L., Chen, Y., Gu, Y., and Chi, Y.
\newblock Breaking the sample complexity barrier to regret-optimal model-free
  reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 17762--17776, 2021.

\bibitem[Li \& Sun(2023)Li and Sun]{li2023variance}
Li, X. and Sun, Q.
\newblock Variance-aware robust reinforcement learning with linear function
  approximation with heavy-tailed rewards.
\newblock \emph{arXiv preprint arXiv:2303.05606}, 2023.

\bibitem[Maillard et~al.(2014)Maillard, Mann, and
  Mannor]{paper:distribution_norm}
Maillard, O.-A., Mann, T.~A., and Mannor, S.
\newblock How hard is my mdp?" the distribution-norm to the rescue".
\newblock \emph{Advances in Neural Information Processing Systems}, 27, 2014.

\bibitem[Maurer \& Pontil(2009)Maurer and Pontil]{paper:bennett}
Maurer, A. and Pontil, M.
\newblock Empirical bernstein bounds and sample-variance penalization.
\newblock In \emph{COLT}, 2009.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., and Riedmiller, M.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Neu \& Pike-Burke(2020)Neu and Pike-Burke]{paper:mdp_optimism}
Neu, G. and Pike-Burke, C.
\newblock A unifying view of optimism in episodic reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1392--1403, 2020.

\bibitem[Osband \& Van~Roy(2017)Osband and Van~Roy]{osband2017posterior}
Osband, I. and Van~Roy, B.
\newblock Why is posterior sampling better than optimism for reinforcement
  learning?
\newblock In \emph{International conference on machine learning}, pp.\
  2701--2710. PMLR, 2017.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{osband2013more}
Osband, I., Russo, D., and Van~Roy, B.
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Pacchiano et~al.(2020)Pacchiano, Ball, Parker-Holder, Choromanski, and
  Roberts]{paper:opt_mb_rl}
Pacchiano, A., Ball, P., Parker-Holder, J., Choromanski, K., and Roberts, S.
\newblock On optimism in model-based reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.11911}, 2020.

\bibitem[Russo(2019)]{russo2019worst}
Russo, D.
\newblock Worst-case regret bounds for exploration via randomized value
  functions.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Simchowitz \& Jamieson(2019)Simchowitz and
  Jamieson]{paper:non_asymptotic_gap}
Simchowitz, M. and Jamieson, K.~G.
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Talebi \& Maillard(2018)Talebi and
  Maillard]{paper:variance_aware_kl_ucrl}
Talebi, M.~S. and Maillard, O.-A.
\newblock Variance-aware regret bounds for undiscounted reinforcement learning
  in mdps.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  770--805. PMLR, 2018.

\bibitem[Tarbouriech et~al.(2021)Tarbouriech, Zhou, Du, Pirotta, Valko, and
  Lazaric]{paper:eb_ssp}
Tarbouriech, J., Zhou, R., Du, S.~S., Pirotta, M., Valko, M., and Lazaric, A.
\newblock Stochastic shortest path: Minimax, parameter-free and towards
  horizon-free regret.
\newblock In \emph{Neural Information Processing Systems}, 2021.

\bibitem[Wagenmaker et~al.(2022)Wagenmaker, Chen, Simchowitz, Du, and
  Jamieson]{paper:first_order_lfa}
Wagenmaker, A.~J., Chen, Y., Simchowitz, M., Du, S., and Jamieson, K.
\newblock First-order regret in reinforcement learning with linear function
  approximation: A robust estimation approach.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  22384--22429. PMLR, 2022.

\bibitem[Wang et~al.(2020)Wang, Du, Yang, and Kakade]{wang2020long}
Wang, R., Du, S.~S., Yang, L.~F., and Kakade, S.~M.
\newblock Is long horizon reinforcement learning more difficult than short
  horizon reinforcement learning?
\newblock \emph{arXiv preprint arXiv:2005.00527}, 2020.

\bibitem[Xiong et~al.(2021)Xiong, Shen, and Du]{paper:random_explore}
Xiong, Z., Shen, R., and Du, S.~S.
\newblock Randomized exploration is near-optimal for tabular mdp.
\newblock \emph{arXiv preprint arXiv:2102.09703}, 2021.

\bibitem[Xu et~al.(2021)Xu, Ma, and
  Du]{paper:gap_dependent_multi_step_bootstrap}
Xu, H., Ma, T., and Du, S.
\newblock Fine-grained gap-dependent bounds for tabular mdps via adaptive
  multi-step bootstrap.
\newblock In \emph{Conference on Learning Theory}, pp.\  4438--4472. PMLR,
  2021.

\bibitem[Yang et~al.(2021)Yang, Yang, and Du]{paper:q_learning_log_regret}
Yang, K., Yang, L., and Du, S.
\newblock Q-learning with logarithmic regret.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1576--1584. PMLR, 2021.

\bibitem[Zanette \& Brunskill(2019)Zanette and Brunskill]{paper:euler}
Zanette, A. and Brunskill, E.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \emph{ICML}, 2019.

\bibitem[Zhang \& Ji(2019)Zhang and Ji]{zhang2019regret}
Zhang, Z. and Ji, X.
\newblock Regret minimization for reinforcement learning by evaluating the
  optimal bias function.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Zhou, and Ji]{paper:ucbadv}
Zhang, Z., Zhou, Y., and Ji, X.
\newblock Almost optimal model-free reinforcement learning via
  reference-advantage decomposition.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15198--15207, 2020.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Ji, and Du]{paper:mvp}
Zhang, Z., Ji, X., and Du, S.~S.
\newblock Is reinforcement learning more difficult than bandits? a near-optimal
  algorithm escaping the curse of horizon.
\newblock In \emph{COLT}, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Yang, Ji, and
  Du]{zhang2021improved}
Zhang, Z., Yang, J., Ji, X., and Du, S.~S.
\newblock Improved variance-aware confidence sets for linear bandits and linear
  mixture mdp.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 4342--4355, 2021{\natexlab{b}}.

\bibitem[Zhang et~al.(2021{\natexlab{c}})Zhang, Zhou, and
  Ji]{paper:model_free_rl}
Zhang, Z., Zhou, Y., and Ji, X.
\newblock Model-free reinforcement learning: from clipped pseudo-regret to
  sample complexity.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, pp.\  12653--12662. PMLR, 2021{\natexlab{c}}.

\bibitem[Zhang et~al.(2022)Zhang, Ji, and Du]{paper:horizon_free}
Zhang, Z., Ji, X., and Du, S.~S.
\newblock Horizon-free reinforcement learning in polynomial time: the power of
  stationary policies.
\newblock In \emph{Annual Conference Computational Learning Theory}, 2022.

\bibitem[Zhao et~al.(2022)Zhao, Zhou, He, and Gu]{zhao2022bandit}
Zhao, H., Zhou, D., He, J., and Gu, Q.
\newblock Bandit learning with general function classes: Heteroscedastic noise
  and variance-dependent regret bounds.
\newblock \emph{arXiv preprint arXiv:2202.13603}, 2022.

\bibitem[Zhao et~al.(2023)Zhao, He, Zhou, Zhang, and Gu]{zhao2023variance}
Zhao, H., He, J., Zhou, D., Zhang, T., and Gu, Q.
\newblock Variance-dependent regret bounds for linear bandits and reinforcement
  learning: Adaptivity and computational efficiency.
\newblock \emph{arXiv preprint arXiv:2302.10371}, 2023.

\bibitem[Zhou et~al.(2021)Zhou, Gu, and Szepesvari]{zhou2021nearly}
Zhou, D., Gu, Q., and Szepesvari, C.
\newblock Nearly minimax optimal reinforcement learning for linear mixture
  markov decision processes.
\newblock In \emph{Conference on Learning Theory}, pp.\  4532--4576. PMLR,
  2021.

\bibitem[Zhou et~al.(2022)Zhou, Wang, and Du]{paper:horizon_free_lmdp}
Zhou, R., Wang, R., and Du, S.~S.
\newblock Horizon-free reinforcement learning for latent markov decision
  processes.
\newblock \emph{arXiv preprint arXiv:2210.11604}, 2022.

\end{thebibliography}
