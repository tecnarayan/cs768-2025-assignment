We propose PLLay, a novel topological layer for general deep learning models
based on persistence landscapes, in which we can efficiently exploit the
underlying topological features of the input data structure. In this work, we
show differentiability with respect to layer inputs, for a general persistent
homology with arbitrary filtration. Thus, our proposed layer can be placed
anywhere in the network and feed critical information on the topological
features of input data into subsequent layers to improve the learnability of
the networks toward a given task. A task-optimal structure of PLLay is learned
during training via backpropagation, without requiring any input featurization
or data preprocessing. We provide a novel adaptation for the DTM function-based
filtration, and show that the proposed layer is robust against noise and
outliers through a stability analysis. We demonstrate the effectiveness of our
approach by classification experiments on various datasets.