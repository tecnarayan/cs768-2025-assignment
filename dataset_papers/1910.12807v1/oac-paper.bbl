\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  J{\'{o}}zefowicz, Kaiser, Kudlur, Levenberg, Man{\'{e}}, Monga, Moore,
  Murray, Olah, Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker,
  Vanhoucke, Vasudevan, Vi{\'{e}}gas, Vinyals, Warden, Wattenberg, Wicke, Yu,
  and Zheng]{abadiTensorflow2016}
Mart{\'{\i}}n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
  Craig Citro, Gregory~S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,
  Sanjay Ghemawat, Ian~J. Goodfellow, Andrew Harp, Geoffrey Irving, Michael
  Isard, Yangqing Jia, Rafal J{\'{o}}zefowicz, Lukasz Kaiser, Manjunath Kudlur,
  Josh Levenberg, Dan Man{\'{e}}, Rajat Monga, Sherry Moore, Derek~Gordon
  Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya
  Sutskever, Kunal Talwar, Paul~A. Tucker, Vincent Vanhoucke, Vijay Vasudevan,
  Fernanda~B. Vi{\'{e}}gas, Oriol Vinyals, Pete Warden, Martin Wattenberg,
  Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.
\newblock Tensorflow: Large-scale machine learning on heterogeneous distributed
  systems.
\newblock \emph{CoRR}, abs/1603.04467, 2016.
\newblock URL \url{http://arxiv.org/abs/1603.04467}.

\bibitem[Abdolmaleki et~al.(2015)Abdolmaleki, Lioutikov, Peters, Lau,
  Pualo~Reis, and Neumann]{abdolmalekiModelBasedRelativeEntropy2015}
Abbas Abdolmaleki, Rudolf Lioutikov, Jan~R Peters, Nuno Lau, Luis Pualo~Reis,
  and Gerhard Neumann.
\newblock Model-{{Based Relative Entropy Stochastic Search}}.
\newblock In C.~Cortes, N.~D. Lawrence, D.~D. Lee, M.~Sugiyama, and R.~Garnett,
  editors, \emph{Advances in {{Neural Information Processing Systems}} 28},
  pages 3537--3545. {Curran Associates, Inc.}, 2015.

\bibitem[Abdolmaleki et~al.(2018)Abdolmaleki, Springenberg, Tassa, Munos,
  Heess, and Riedmiller]{abdolmalekiMaximumPosterioriPolicy2018a}
Abbas Abdolmaleki, Jost~Tobias Springenberg, Yuval Tassa, R{\'e}mi Munos,
  Nicolas Heess, and Martin~A. Riedmiller.
\newblock Maximum a {{Posteriori Policy Optimisation}}.
\newblock In \emph{6th {{International Conference}} on {{Learning
  Representations}}, {{ICLR}} 2018, {{Vancouver}}, {{BC}}, {{Canada}},
  {{April}} 30 - {{May}} 3, 2018, {{Conference Track Proceedings}}}.
  {OpenReview.net}, 2018.

\bibitem[Akrour et~al.(2016)Akrour, Neumann, Abdulsamad, and
  Abdolmaleki]{akrourModelFreeTrajectoryOptimization2016a}
Riad Akrour, Gerhard Neumann, Hany Abdulsamad, and Abbas Abdolmaleki.
\newblock Model-{{Free Trajectory Optimization}} for {{Reinforcement
  Learning}}.
\newblock In Maria-Florina Balcan and Kilian~Q. Weinberger, editors,
  \emph{Proceedings of the 33nd {{International Conference}} on {{Machine
  Learning}}, {{ICML}} 2016, {{New York City}}, {{NY}}, {{USA}}, {{June}}
  19-24, 2016}, volume~48 of \emph{{{JMLR Workshop}} and {{Conference
  Proceedings}}}, pages 2961--2970. {JMLR.org}, 2016.

\bibitem[Amari(1998)]{amariNaturalGradientWorks1998}
Shun-ichi Amari.
\newblock Natural {{Gradient Works Efficiently}} in {{Learning}}.
\newblock \emph{Neural Computation}, 10\penalty0 (2):\penalty0 251--276, 1998.
\newblock \doi{10.1162/089976698300017746}.

\bibitem[Baxter and Bartlett(2000)]{baxterDirectGradientbasedReinforcement2000}
J.~Baxter and P.~L. Bartlett.
\newblock Direct gradient-based reinforcement learning.
\newblock In \emph{{{IEEE International Symposium}} on {{Circuits}} and
  {{Systems}}, {{ISCAS}} 2000, {{Emerging Technologies}} for the 21st
  {{Century}}, {{Geneva}}, {{Switzerland}}, 28-31 {{May}} 2000,
  {{Proceedings}}}, pages 271--274. {IEEE}, 2000.
\newblock \doi{10.1109/ISCAS.2000.856049}.

\bibitem[Baxter and
  Bartlett(2001)]{baxterInfiniteHorizonPolicyGradientEstimation2001}
Jonathan Baxter and Peter~L. Bartlett.
\newblock Infinite-{{Horizon Policy}}-{{Gradient Estimation}}.
\newblock \emph{J. Artif. Intell. Res.}, 15:\penalty0 319--350, 2001.
\newblock \doi{10.1613/jair.806}.

\bibitem[Baxter et~al.(2001)Baxter, Bartlett, and
  Weaver]{baxterExperimentsInfiniteHorizonPolicyGradient2001}
Jonathan Baxter, Peter~L. Bartlett, and Lex Weaver.
\newblock Experiments with {{Infinite}}-{{Horizon}}, {{Policy}}-{{Gradient
  Estimation}}.
\newblock \emph{J. Artif. Intell. Res.}, 15:\penalty0 351--381, 2001.
\newblock \doi{10.1613/jair.807}.

\bibitem[Brafman and Tennenholtz(2002)]{brafmanRmaxaGeneralPolynomial2002}
Ronen~I. Brafman and Moshe Tennenholtz.
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Oct):\penalty0 213--231, 2002.

\bibitem[Callahan(2010)]{callahanAdvancedCalculusGeometric2010}
James~J. Callahan.
\newblock \emph{Advanced {{Calculus}}: {{A Geometric View}}}.
\newblock {Springer Science \& Business Media}, September 2010.
\newblock ISBN 978-1-4419-7332-0.

\bibitem[Chen et~al.(2017)Chen, Sidor, Abbeel, and Schulman]{chen2017ucb}
Richard~Y Chen, Szymon Sidor, Pieter Abbeel, and John Schulman.
\newblock Ucb exploration via q-ensembles.
\newblock \emph{arXiv preprint arXiv:1706.01502}, 2017.

\bibitem[Chua et~al.(2018)Chua, Calandra, McAllister, and
  Levine]{handful-trials}
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine.
\newblock Deep reinforcement learning in a handful of trials using
  probabilistic dynamics models.
\newblock In \emph{Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8
  December 2018, Montr{\'{e}}al, Canada.}, pages 4759--4770, 2018.
\newblock URL
  \url{http://papers.nips.cc/paper/7725-deep-reinforcement-learning-in-a-handful-of-trials-using-probabilistic-dynamics-models}.

\bibitem[Ciosek and
  Whiteson(2018{\natexlab{a}})]{ciosekExpectedPolicyGradients2018}
Kamil Ciosek and Shimon Whiteson.
\newblock Expected {{Policy Gradients}}.
\newblock In Sheila~A. McIlraith and Kilian~Q. Weinberger, editors,
  \emph{Proceedings of the {{Thirty}}-{{Second AAAI Conference}} on
  {{Artificial Intelligence}}, ({{AAAI}}-18), the 30th Innovative
  {{Applications}} of {{Artificial Intelligence}} ({{IAAI}}-18), and the 8th
  {{AAAI Symposium}} on {{Educational Advances}} in {{Artificial Intelligence}}
  ({{EAAI}}-18), {{New Orleans}}, {{Louisiana}}, {{USA}}, {{February}} 2-7,
  2018}, pages 2868--2875. {AAAI Press}, 2018{\natexlab{a}}.

\bibitem[Ciosek and
  Whiteson(2018{\natexlab{b}})]{ciosekExpectedPolicyGradients2018a}
Kamil Ciosek and Shimon Whiteson.
\newblock Expected {{Policy Gradients}} for {{Reinforcement Learning}}.
\newblock \emph{CoRR}, abs/1801.03326, 2018{\natexlab{b}}.

\bibitem[Depeweg et~al.(2016)Depeweg, Hern{\'a}ndez-Lobato, Doshi-Velez, and
  Udluft]{depeweg2016learning}
Stefan Depeweg, Jos{\'e}~Miguel Hern{\'a}ndez-Lobato, Finale Doshi-Velez, and
  Steffen Udluft.
\newblock Learning and policy search in stochastic dynamical systems with
  bayesian neural networks.
\newblock \emph{arXiv preprint arXiv:1605.07127}, 2016.

\bibitem[Efron and Tibshirani(1994)]{efronIntroductionBootstrap1994}
Bradley Efron and Robert~J. Tibshirani.
\newblock An {{Introduction}} to the {{Bootstrap}}.
\newblock \emph{SIAM Review}, 36\penalty0 (4):\penalty0 677--678, 1994.
\newblock \doi{10.1137/1036171}.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and
  Meger]{fujimotoAddressingFunctionApproximation2018a}
Scott Fujimoto, Herke Hoof, and David Meger.
\newblock Addressing {{Function Approximation Error}} in {{Actor}}-{{Critic
  Methods}}.
\newblock In \emph{International {{Conference}} on {{Machine Learning}}}, pages
  1582--1591, 2018.

\bibitem[Gal et~al.(2016)Gal, McAllister, and Rasmussen]{gal2016improving}
Yarin Gal, Rowan McAllister, and Carl~Edward Rasmussen.
\newblock Improving pilco with bayesian neural network dynamics models.
\newblock In \emph{Data-Efficient Machine Learning workshop, ICML}, volume~4,
  2016.

\bibitem[Ghavamzadeh and Engel(2007)]{bayesAC07}
Mohammad Ghavamzadeh and Yaakov Engel.
\newblock Bayesian actor-critic algorithms.
\newblock In \emph{Machine Learning, Proceedings of the Twenty-Fourth
  International Conference {(ICML} 2007), Corvallis, Oregon, USA, June 20-24,
  2007}, pages 297--304, 2007.
\newblock \doi{10.1145/1273496.1273534}.
\newblock URL \url{https://doi.org/10.1145/1273496.1273534}.

\bibitem[Ghavamzadeh et~al.(2015)Ghavamzadeh, Mannor, Pineau, and
  Tamar]{bayesRLasurvey}
Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar.
\newblock Bayesian reinforcement learning: {A} survey.
\newblock \emph{Foundations and Trends in Machine Learning}, 8\penalty0
  (5-6):\penalty0 359--483, 2015.
\newblock \doi{10.1561/2200000049}.
\newblock URL \url{https://doi.org/10.1561/2200000049}.

\bibitem[Ghavamzadeh et~al.(2016)Ghavamzadeh, Engel, and Valko]{bayesianPG2016}
Mohammad Ghavamzadeh, Yaakov Engel, and Michal Valko.
\newblock Bayesian policy gradient and actor-critic algorithms.
\newblock \emph{Journal of Machine Learning Research}, 17:\penalty0
  66:1--66:53, 2016.
\newblock URL \url{http://jmlr.org/papers/v17/10-245.html}.

\bibitem[Gu et~al.(2017{\natexlab{a}})Gu, Lillicrap, Turner, Ghahramani,
  Sch{\"o}lkopf, and Levine]{guInterpolatedPolicyGradient2017a}
Shixiang Gu, Tim Lillicrap, Richard~E. Turner, Zoubin Ghahramani, Bernhard
  Sch{\"o}lkopf, and Sergey Levine.
\newblock Interpolated {{Policy Gradient}}: {{Merging On}}-{{Policy}} and
  {{Off}}-{{Policy Gradient Estimation}} for {{Deep Reinforcement Learning}}.
\newblock In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna~M. Wallach,
  Rob Fergus, S.~V.~N. Vishwanathan, and Roman Garnett, editors, \emph{Advances
  in {{Neural Information Processing Systems}} 30: {{Annual Conference}} on
  {{Neural Information Processing Systems}} 2017, 4-9 {{December}} 2017, {{Long
  Beach}}, {{CA}}, {{USA}}}, pages 3849--3858, 2017{\natexlab{a}}.

\bibitem[Gu et~al.(2017{\natexlab{b}})Gu, Lillicrap, Ghahramani, Turner, and
  Levine]{guQPropSampleEfficientPolicy2017}
Shixiang Gu, Timothy~P. Lillicrap, Zoubin Ghahramani, Richard~E. Turner, and
  Sergey Levine.
\newblock Q-{{Prop}}: {{Sample}}-{{Efficient Policy Gradient}} with {{An
  Off}}-{{Policy Critic}}.
\newblock In \emph{5th {{International Conference}} on {{Learning
  Representations}}, {{ICLR}} 2017, {{Toulon}}, {{France}}, {{April}} 24-26,
  2017, {{Conference Track Proceedings}}}. {OpenReview.net},
  2017{\natexlab{b}}.

\bibitem[Haarnoja et~al.(2018{\natexlab{a}})Haarnoja, Zhou, Abbeel, and
  Levine]{haarnojaSoftActorCriticOffPolicy2018a}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft {{Actor}}-{{Critic}}: {{Off}}-{{Policy Maximum Entropy Deep
  Reinforcement Learning}} with a {{Stochastic Actor}}.
\newblock In \emph{International {{Conference}} on {{Machine Learning}}}, pages
  1856--1865, 2018{\natexlab{a}}.

\bibitem[Haarnoja et~al.(2018{\natexlab{b}})Haarnoja, Zhou, Hartikainen,
  Tucker, Ha, Tan, Kumar, Zhu, Gupta, Abbeel, and
  Levine]{haarnojaSoftActorCriticAlgorithms2018}
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha,
  Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey
  Levine.
\newblock Soft {{Actor}}-{{Critic Algorithms}} and {{Applications}}.
\newblock \emph{CoRR}, abs/1812.05905, 2018{\natexlab{b}}.

\bibitem[Hasselt(2010)]{hasseltDoubleQlearning2010}
Hado~V. Hasselt.
\newblock Double {{Q}}-learning.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  pages 2613--2621, 2010.

\bibitem[Heess et~al.(2015)Heess, Wayne, Silver, Lillicrap, Erez, and
  Tassa]{heessLearningContinuousControl2015}
Nicolas Heess, Gregory Wayne, David Silver, Timothy~P. Lillicrap, Tom Erez, and
  Yuval Tassa.
\newblock Learning {{Continuous Control Policies}} by {{Stochastic Value
  Gradients}}.
\newblock In Corinna Cortes, Neil~D. Lawrence, Daniel~D. Lee, Masashi Sugiyama,
  and Roman Garnett, editors, \emph{Advances in {{Neural Information Processing
  Systems}} 28: {{Annual Conference}} on {{Neural Information Processing
  Systems}} 2015, {{December}} 7-12, 2015, {{Montreal}}, {{Quebec}},
  {{Canada}}}, pages 2944--2952, 2015.

\bibitem[Jin et~al.(2018)Jin, {Allen-Zhu}, Bubeck, and
  Jordan]{jinQLearningProvablyEfficient2018}
Chi Jin, Zeyuan {Allen-Zhu}, S{\'e}bastien Bubeck, and Michael~I. Jordan.
\newblock Is {{Q}}-{{Learning Provably Efficient}}?
\newblock In Samy Bengio, Hanna~M. Wallach, Hugo Larochelle, Kristen Grauman,
  Nicol{\`o} {Cesa-Bianchi}, and Roman Garnett, editors, \emph{Advances in
  {{Neural Information Processing Systems}} 31: {{Annual Conference}} on
  {{Neural Information Processing Systems}} 2018, {{NeurIPS}} 2018, 3-8
  {{December}} 2018, {{Montr{\'e}al}}, {{Canada}}}, pages 4868--4878, 2018.

\bibitem[Kakade(2001)]{kakadeNaturalPolicyGradient2001}
Sham Kakade.
\newblock A {{Natural Policy Gradient}}.
\newblock In Thomas~G. Dietterich, Suzanna Becker, and Zoubin Ghahramani,
  editors, \emph{Advances in {{Neural Information Processing Systems}} 14
  [{{Neural Information Processing Systems}}: {{Natural}} and {{Synthetic}},
  {{NIPS}} 2001, {{December}} 3-8, 2001, {{Vancouver}}, {{British Columbia}},
  {{Canada}}]}, pages 1531--1538. {MIT Press}, 2001.

\bibitem[Kingma and Ba(2015)]{kingmaAdam2014}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{3rd International Conference on Learning Representations,
  {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
  Proceedings}, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.6980}.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrapContinuousControlDeep2016}
Timothy~P. Lillicrap, Jonathan~J. Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock In Yoshua Bengio and Yann LeCun, editors, \emph{4th {{International
  Conference}} on {{Learning Representations}}, {{ICLR}} 2016, {{San Juan}},
  {{Puerto Rico}}, {{May}} 2-4, 2016, {{Conference Track Proceedings}}}, 2016.

\bibitem[Lim et~al.(2018)Lim, Joseph, Le, Pan, and White]{actor-expert}
Sungsu Lim, Ajin Joseph, Lei Le, Yangchen Pan, and Martha White.
\newblock Actor-expert: {A} framework for using action-value methods in
  continuous action spaces.
\newblock \emph{CoRR}, abs/1810.09103, 2018.
\newblock URL \url{http://arxiv.org/abs/1810.09103}.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and
  Hassabis]{mnihHumanlevelControlDeep2015}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A. Rusu, Joel Veness,
  Marc~G. Bellemare, Alex Graves, Martin~A. Riedmiller, Andreas Fidjeland,
  Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis
  Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and
  Demis Hassabis.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.
\newblock \doi{10.1038/nature14236}.

\bibitem[O'Donoghue et~al.(2018)O'Donoghue, Osband, Munos, and
  Mnih]{odonoghue-ube}
Brendan O'Donoghue, Ian Osband, R{\'{e}}mi Munos, and Volodymyr Mnih.
\newblock The uncertainty bellman equation and exploration.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, pages 3836--3845, 2018.
\newblock URL \url{http://proceedings.mlr.press/v80/o-donoghue18a.html}.

\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and
  Roy]{osbandDeepExplorationBootstrapped2016}
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin~Van Roy.
\newblock Deep {{Exploration}} via {{Bootstrapped DQN}}.
\newblock In Daniel~D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle
  Guyon, and Roman Garnett, editors, \emph{Advances in {{Neural Information
  Processing Systems}} 29: {{Annual Conference}} on {{Neural Information
  Processing Systems}} 2016, {{December}} 5-10, 2016, {{Barcelona}},
  {{Spain}}}, pages 4026--4034, 2016.

\bibitem[Osband et~al.(2018)Osband, Aslanides, and Cassirer]{randomised-priors}
Ian Osband, John Aslanides, and Albin Cassirer.
\newblock Randomized prior functions for deep reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8
  December 2018, Montr{\'{e}}al, Canada.}, pages 8626--8638, 2018.
\newblock URL
  \url{http://papers.nips.cc/paper/8080-randomized-prior-functions-for-deep-reinforcement-learning}.

\bibitem[Peters and Schaal(2006)]{petersPolicyGradientMethods2006}
Jan Peters and Stefan Schaal.
\newblock Policy {{Gradient Methods}} for {{Robotics}}.
\newblock In \emph{2006 {{IEEE}}/{{RSJ International Conference}} on
  {{Intelligent Robots}} and {{Systems}}, {{IROS}} 2006, {{October}} 9-15,
  2006, {{Beijing}}, {{China}}}, pages 2219--2225. {IEEE}, 2006.
\newblock ISBN 978-1-4244-0258-8.
\newblock \doi{10.1109/IROS.2006.282564}.

\bibitem[Peters and Schaal(2008)]{petersNaturalActorCritic2008}
Jan Peters and Stefan Schaal.
\newblock Natural {{Actor}}-{{Critic}}.
\newblock \emph{Neurocomputing}, 71\penalty0 (7-9):\penalty0 1180--1190, 2008.
\newblock \doi{10.1016/j.neucom.2007.11.026}.

\bibitem[Puterman(2014)]{putermanMarkovDecisionProcesses2014}
Martin~L. Puterman.
\newblock \emph{Markov Decision Processes: Discrete Stochastic Dynamic
  Programming}.
\newblock {John Wiley \& Sons}, 2014.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{silverDeterministicPolicyGradient2014}
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and
  Martin~A. Riedmiller.
\newblock Deterministic {{Policy Gradient Algorithms}}.
\newblock In \emph{Proceedings of the 31th {{International Conference}} on
  {{Machine Learning}}, {{ICML}} 2014, {{Beijing}}, {{China}}, 21-26 {{June}}
  2014}, volume~32 of \emph{{{JMLR Workshop}} and {{Conference Proceedings}}},
  pages 387--395. {JMLR.org}, 2014.

\bibitem[Sutton(1995)]{suttonGeneralizationReinforcementLearning1995}
Richard~S. Sutton.
\newblock Generalization in {{Reinforcement Learning}}: {{Successful Examples
  Using Sparse Coarse Coding}}.
\newblock In David~S. Touretzky, Michael Mozer, and Michael~E. Hasselmo,
  editors, \emph{Advances in {{Neural Information Processing Systems}} 8,
  {{NIPS}}, {{Denver}}, {{CO}}, {{USA}}, {{November}} 27-30, 1995}, pages
  1038--1044. {MIT Press}, 1995.
\newblock ISBN 978-0-262-20107-0.

\bibitem[Sutton(1996)]{NIPS1995_1109}
Richard~S Sutton.
\newblock Generalization in {{Reinforcement Learning}}: {{Successful Examples
  Using Sparse Coarse Coding}}.
\newblock In D.~S. Touretzky, M.~C. Mozer, and M.~E. Hasselmo, editors,
  \emph{Advances in {{Neural Information Processing Systems}} 8}, pages
  1038--1044. {MIT Press}, 1996.

\bibitem[Sutton and Barto(2018)]{suttonReinforcementLearningIntroduction2018}
Richard~S. Sutton and Andrew~G. Barto.
\newblock \emph{Reinforcement {{Learning}}: {{An Introduction}}}.
\newblock {The MIT Press}, second edition, 2018.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and
  Mansour]{suttonPolicyGradientMethods1999}
Richard~S. Sutton, David~A. McAllester, Satinder~P. Singh, and Yishay Mansour.
\newblock Policy {{Gradient Methods}} for {{Reinforcement Learning}} with
  {{Function Approximation}}.
\newblock In Sara~A. Solla, Todd~K. Leen, and Klaus-Robert M{\"u}ller, editors,
  \emph{Advances in {{Neural Information Processing Systems}} 12, [{{NIPS
  Conference}}, {{Denver}}, {{Colorado}}, {{USA}}, {{November}} 29 -
  {{December}} 4, 1999]}, pages 1057--1063. {The MIT Press}, 1999.
\newblock ISBN 978-0-262-19450-1.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorovMujoco2012}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock {{MuJoCo}}: {{A}} physics engine for model-based control.
\newblock \emph{2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pages 5026--5033, 2012.

\bibitem[van Hasselt et~al.(2016)van Hasselt, Guez, and
  Silver]{hasseltDeepReinforcementLearning2016}
Hado van Hasselt, Arthur Guez, and David Silver.
\newblock Deep {{Reinforcement Learning}} with {{Double Q}}-{{Learning}}.
\newblock In Dale Schuurmans and Michael~P. Wellman, editors, \emph{Proceedings
  of the {{Thirtieth AAAI Conference}} on {{Artificial Intelligence}},
  {{February}} 12-17, 2016, {{Phoenix}}, {{Arizona}}, {{USA}}}, pages
  2094--2100. {AAAI Press}, 2016.
\newblock ISBN 978-1-57735-760-5.

\bibitem[van Hasselt et~al.(2018)van Hasselt, Doron, Strub, Hessel, Sonnerat,
  and Modayil]{hasseltDeepReinforcementLearning2018}
Hado van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat,
  and Joseph Modayil.
\newblock Deep {{Reinforcement Learning}} and the {{Deadly Triad}}.
\newblock \emph{CoRR}, abs/1812.02648, 2018.

\bibitem[van Seijen et~al.(2009)van Seijen, van Hasselt, Whiteson, and
  Wiering]{seijenTheoreticalEmpiricalAnalysis2009}
Harm van Seijen, Hado van Hasselt, Shimon Whiteson, and Marco~A. Wiering.
\newblock A theoretical and empirical analysis of {{Expected Sarsa}}.
\newblock In \emph{{{IEEE Symposium}} on {{Adaptive Dynamic Programming}} and
  {{Reinforcement Learning}}, {{ADPRL}} 2009, {{Nashville}}, {{TN}}, {{USA}},
  {{March}} 31 - {{April}} 1, 2009}, pages 177--184. {IEEE}, 2009.
\newblock ISBN 978-1-4244-2761-1.
\newblock \doi{10.1109/ADPRL.2009.4927542}.

\bibitem[Williams(1992)]{williamsSimpleStatisticalGradientFollowing1992}
Ronald~J. Williams.
\newblock Simple {{Statistical Gradient}}-{{Following Algorithms}} for
  {{Connectionist Reinforcement Learning}}.
\newblock \emph{Machine Learning}, 8:\penalty0 229--256, 1992.
\newblock \doi{10.1007/BF00992696}.

\bibitem[Ziebart(2010)]{ziebartModelingPurposefulAdaptive2010}
Brian~D. Ziebart.
\newblock \emph{Modeling {{Purposeful Adaptive Behavior}} with the
  {{Principle}} of {{Maximum Causal Entropy}}}.
\newblock {{PhD Thesis}}, Carnegie Mellon University, Pittsburgh, PA, USA,
  2010.

\end{thebibliography}
