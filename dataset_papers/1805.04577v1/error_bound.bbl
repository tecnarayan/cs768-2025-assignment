\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bach and Moulines(2013)]{DBLP:conf/nips/BachM13}
Francis~R. Bach and Eric Moulines.
\newblock Non-strongly-convex smooth stochastic approximation with convergence
  rate o(1/n).
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 773--781, 2013.

\bibitem[Bartlett and Mendelson(2006)]{Bartlett2006}
Peter~L. Bartlett and Shahar Mendelson.
\newblock Empirical minimization.
\newblock \emph{Probability Theory and Related Fields}, 2006.

\bibitem[Bartlett et~al.(2005)Bartlett, Bousquet, and Mendelson]{Local_RC}
Peter~L. Bartlett, Olivier Bousquet, and Shahar Mendelson.
\newblock Local rademacher complexities.
\newblock \emph{The Annals of Statistics}, 2005.

\bibitem[Bolte et~al.(2015)Bolte, Nguyen, Peypouquet, and
  Suter]{arxiv:1510.08234}
Jerome Bolte, Trong~Phong Nguyen, Juan Peypouquet, and Bruce Suter.
\newblock From error bounds to the complexity of first-order descent methods
  for convex functions.
\newblock \emph{CoRR}, abs/1510.08234, 2015.

\bibitem[Burke and Ferris.(1993)]{doi:10.1137/0331063}
James~V. Burke and Michael~C. Ferris.
\newblock Weak sharp minima in mathematical programming.
\newblock \emph{SIAM Journal on Control and Optimization}, 31\penalty0
  (5):\penalty0 1340--1359, 1993.
\newblock \doi{10.1137/0331063}.

\bibitem[Drusvyatskiy and Lewis(2016)]{Drusvyatskiy16a}
Dmitriy Drusvyatskiy and Adrian~S. Lewis.
\newblock Error bounds, quadratic growth, and linear convergence of proximal
  methods.
\newblock \emph{arXiv:1602.06661}, 2016.

\bibitem[Duchi and Singer(2009)]{duchi-2009-efficient}
John Duchi and Yoram Singer.
\newblock Efficient online and batch learning using forward backward splitting.
\newblock \emph{Journal of Machine Learning Research}, 10:\penalty0 2899--2934,
  2009.

\bibitem[Duchi et~al.(2010)Duchi, Shalev-Shwartz, Singer, and
  Tewari]{conf/colt/DuchiSST10}
John~C. Duchi, Shai Shalev-Shwartz, Yoram Singer, and Ambuj Tewari.
\newblock Composite objective mirror descent.
\newblock In \emph{COLT}, pages 14--26. Omnipress, 2010.

\bibitem[Feldman(2016)]{NIPS2016_ERM}
Vitaly Feldman.
\newblock Generalization of erm in stochastic convex optimization: The
  dimension strikes back.
\newblock In \emph{NIPS}. 2016.

\bibitem[Garber et~al.(2016)Garber, Hazan, Jin, Kakade, Musco, Netrapalli, and
  Sidford]{DBLP:conf/icml/GarberHJKMNS16}
Dan Garber, Elad Hazan, Chi Jin, Sham~M. Kakade, Cameron Musco, Praneeth
  Netrapalli, and Aaron Sidford.
\newblock Faster eigenvector computation via shift-and-invert preconditioning.
\newblock In \emph{ICML}, 2016.

\bibitem[Gonen and Shalev{-}Shwartz(2016)]{DBLP:journals/corr/abs/1601.04011}
Alon Gonen and Shai Shalev{-}Shwartz.
\newblock Average stability is invariant to data preconditioning. implications
  to exp-concave empirical risk minimization.
\newblock \emph{CoRR}, abs/1601.04011, 2016.

\bibitem[Gr{\"{u}}nwald and Mehta(2016)]{DBLP:journals/corr/GrunwaldM16}
Peter~D. Gr{\"{u}}nwald and Nishant~A. Mehta.
\newblock Fast rates with unbounded losses.
\newblock \emph{CoRR}, abs/1605.00252, 2016.

\bibitem[Hazan and Kale(2011)]{hazan-20110-beyond}
Elad Hazan and Satyen Kale.
\newblock Beyond the regret minimization barrier: an optimal algorithm for
  stochastic strongly-convex optimization.
\newblock In \emph{COLT}, 2011.

\bibitem[Hazan et~al.(2007)Hazan, Agarwal, and
  Kale]{DBLP:journals/ml/HazanAK07}
Elad Hazan, Amit Agarwal, and Satyen Kale.
\newblock Logarithmic regret algorithms for online convex optimization.
\newblock \emph{Machine Learning}, 2007.

\bibitem[Juditsky and Nesterov(2014)]{juditsky2014}
Anatoli Juditsky and Yuri Nesterov.
\newblock Deterministic and stochastic primal-dual subgradient algorithms for
  uniformly convex minimization.
\newblock \emph{Stoch. Syst.}, 2014.

\bibitem[Kakade and Tewari(2008)]{DBLP:conf/nips/KakadeT08}
Sham~M. Kakade and Ambuj Tewari.
\newblock On the generalization ability of online strongly convex programming
  algorithms.
\newblock In \emph{NIPS}, 2008.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and
  Schmidt]{DBLP:conf/pkdd/KarimiNS16}
Hamed Karimi, Julie Nutini, and Mark~W. Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In \emph{ECML-PKDD}, 2016.

\bibitem[Kim et~al.(2015)Kim, Pasupathy, and Henderson]{Kim2015}
Sujin Kim, Raghu Pasupathy, and Shane~G. Henderson.
\newblock \emph{A Guide to Sample Average Approximation}, pages 207--243.
\newblock Springer New York, New York, NY, 2015.

\bibitem[Koltchinskii(2006)]{Local:Vladimir}
Vladimir Koltchinskii.
\newblock Local rademacher complexities and oracle inequalities in risk
  minimization.
\newblock \emph{The Annals of Statistics}, 2006.

\bibitem[Koolen et~al.(2016)Koolen, Gr{\"{u}}nwald, and van
  Erven]{DBLP:conf/nips/KoolenGE16}
Wouter~M. Koolen, Peter Gr{\"{u}}nwald, and Tim van Erven.
\newblock Combining adversarial guarantees and stochastic fast rates in online
  learning.
\newblock In \emph{NIPS}, 2016.

\bibitem[Koren and Levy(2015)]{DBLP:conf/nips/KorenL15}
Tomer Koren and Kfir~Y. Levy.
\newblock Fast rates for exp-concave empirical risk minimization.
\newblock In \emph{NIPS}, 2015.

\bibitem[Lee et~al.(1998)Lee, Bartlett, and Williamson]{705577}
Wee~Sun Lee, P.~L. Bartlett, and R.~C. Williamson.
\newblock The importance of convexity in learning with squared loss.
\newblock \emph{IEEE Transactions on Information Theory}, 44\penalty0
  (5):\penalty0 1974--1980, 1998.

\bibitem[Li(2013)]{DBLP:journals/mp/Li13}
Guoyin Li.
\newblock Global error bounds for piecewise convex polynomials.
\newblock \emph{Math. Program.}, 2013.

\bibitem[Li and Pong(2016)]{guoyincalculus2016}
Guoyin Li and Ting~Kei Pong.
\newblock Calculus of the exponent of kurdyka- \l ojasiewicz inequality and its
  applications to linear convergence of first-order methods.
\newblock \emph{CoRR}, abs/1602.02915, 2016.

\bibitem[Mahdavi and Jin(2014)]{DBLP:journals/corr/MahdaviJ14}
Mehrdad Mahdavi and Rong Jin.
\newblock Excess risk bounds for exponentially concave losses.
\newblock \emph{CoRR}, abs/1401.4566, 2014.

\bibitem[Mehta(2017)]{arXiv:1605.01288}
Nishant~A. Mehta.
\newblock Fast rates with high probability in exp-concave statistical learning.
\newblock In \emph{The 20th International Conference on Artificial Intelligence
  and Statistics (AISTATS)}, pages~--, 2017.

\bibitem[Mehta and Williamson(2014)]{DBLP:conf/nips/MehtaW14}
Nishant~A. Mehta and Robert~C. Williamson.
\newblock From stochastic mixability to fast rates.
\newblock In \emph{NIPS}, 2014.

\bibitem[Necoara et~al.(2015)Necoara, Nesterov, and
  Glineur]{DBLP:journals/corr/nesterov16linearnon}
I.~Necoara, Yu. Nesterov, and F.~Glineur.
\newblock Linear convergence of first order methods for non-strongly convex
  optimization.
\newblock \emph{CoRR}, abs/1504.06298, v4, 2015.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{Nemirovski:2009:RSA:1654243.1654247}
Arkadi Nemirovski, Anatoli Juditsky, Lan, and Alexander Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on Optimization}, 2009.

\bibitem[Nesterov(2004)]{nesterov2004introductory}
Yurii Nesterov.
\newblock \emph{Introductory lectures on convex optimization: a basic course}.
\newblock 2004.

\bibitem[Pang(1997)]{DBLP:journals/mp/Pang97}
Jong{-}Shi Pang.
\newblock Error bounds in mathematical programming.
\newblock \emph{Math. Program.}, 1997.

\bibitem[Rakhlin et~al.(2012)Rakhlin, Shamir, and Sridharan]{ICML2012Rakhlin}
Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock In \emph{ICML}, 2012.

\bibitem[Ramdas and Singh(2013)]{DBLP:conf/icml/RamdasS13}
Aaditya Ramdas and Aarti Singh.
\newblock Optimal rates for stochastic convex optimization under tsybakov noise
  condition.
\newblock In \emph{ICML}, 2013.

\bibitem[Rockafellar(1970)]{rockafellar1970convex}
R.T. Rockafellar.
\newblock \emph{Convex Analysis}.
\newblock 1970.

\bibitem[Shalev{-}Shwartz and
  Tewari(2011)]{DBLP:journals/jmlr/Shalev-ShwartzT11}
Shai Shalev{-}Shwartz and Ambuj Tewari.
\newblock Stochastic methods for \emph{l}\({}_{\mbox{1}}\)-regularized loss
  minimization.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 1865--1892,
  2011.

\bibitem[Shalev-Shwartz et~al.(2007)Shalev-Shwartz, Singer, and
  Srebro]{shalev-shwartz-2007-pegasos}
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.
\newblock Pegasos: Primal estimated sub-gradient solver for svm.
\newblock In \emph{ICML}, 2007.

\bibitem[Shalev-Shwartz et~al.(2009)Shalev-Shwartz, Shamir, Srebro, and
  Sridharan]{COLT:Shalev:2009}
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan.
\newblock Stochastic convex optimization.
\newblock In \emph{COLT}, 2009.

\bibitem[Shamir and Zhang(2013)]{ICML2013Shamir:ICML}
Ohad Shamir and Tong Zhang.
\newblock Stochastic gradient descent for non-smooth optimization: Convergence
  results and optimal averaging schemes.
\newblock In \emph{ICML}, 2013.

\bibitem[Shapiro et~al.(2014)Shapiro, Dentcheva, and
  Ruszczynski]{Shapiro:2014:LSP:2678054}
Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski.
\newblock \emph{Lectures on Stochastic Programming: Modeling and Theory, Second
  Edition}.
\newblock Society for Industrial and Applied Mathematics, Philadelphia, PA,
  USA, 2014.
\newblock ISBN 1611973422, 9781611973426.

\bibitem[Smale and Zhou(2007)]{Smale:learning}
Steve Smale and Ding-Xuan Zhou.
\newblock Learning theory estimates via integral operators and their
  approximations.
\newblock \emph{Constructive Approximation}, 2007.

\bibitem[Srebro et~al.(2010{\natexlab{a}})Srebro, Sridharan, and
  Tewari]{DBLP:conf/nips/SrebroST10}
Nathan Srebro, Karthik Sridharan, and Ambuj Tewari.
\newblock Smoothness, low noise and fast rates.
\newblock In \emph{NIPS}, 2010{\natexlab{a}}.

\bibitem[Srebro et~al.(2010{\natexlab{b}})Srebro, Sridharan, and
  Tewari]{Smooth:Risk}
Nathan Srebro, Karthik Sridharan, and Ambuj Tewari.
\newblock Optimistic rates for learning with a smooth loss.
\newblock \emph{ArXiv e-prints}, arXiv:1009.3896, 2010{\natexlab{b}}.

\bibitem[Sridharan et~al.(2008)Sridharan, Shalev-Shwartz, and
  Srebro]{DBLP:conf/nips/SridharanSS08}
Karthik Sridharan, Shai Shalev-Shwartz, and Nathan Srebro.
\newblock Fast rates for regularized objectives.
\newblock In \emph{NIPS}, 2008.

\bibitem[van Erven and Koolen(2016)]{DBLP:conf/nips/ErvenK16}
Tim van Erven and Wouter~M. Koolen.
\newblock Metagrad: Multiple learning rates in online learning.
\newblock In \emph{NIPS}, 2016.

\bibitem[van Erven et~al.(2015)van Erven, Gr{\"{u}}nwald, Mehta, Reid, and
  Williamson]{DBLP:journals/jmlr/ErvenGMRW15}
Tim van Erven, Peter~D. Gr{\"{u}}nwald, Nishant~A. Mehta, Mark~D. Reid, and
  Robert~C. Williamson.
\newblock Fast rates in statistical and online learning.
\newblock \emph{JMLR}, 2015.

\bibitem[Vapnik(1998)]{Vapnik1998}
Vladimir~N. Vapnik.
\newblock \emph{Statistical Learning Theory}.
\newblock Wiley-Interscience, 1998.

\bibitem[Vovk(1990)]{Vovk:1990:AS:92571.92672}
Volodimir~G. Vovk.
\newblock Aggregating strategies.
\newblock In \emph{COLT}, 1990.

\bibitem[Xu et~al.(2016)Xu, Lin, and Yang]{DBLP:journals/corr/abs-1607-01027}
Yi~Xu, Qihang Lin, and Tianbao Yang.
\newblock Accelerate stochastic subgradient method by leveraging local error
  bound.
\newblock \emph{CoRR}, abs/1607.01027, 2016.

\bibitem[Xu et~al.(2017)Xu, Lin, and Yang]{ICMLASSG}
Yi~Xu, Qihang Lin, and Tianbao Yang.
\newblock Stochastic convex optimization: Faster local growth implies faster
  global convergence.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning (ICML)}, pages 3821--3830, 2017.

\bibitem[Yang and Lin(2016)]{DBLP:journals/corr/arXiv:1512.03107}
Tianbao Yang and Qihang Lin.
\newblock Rsg: Beating subgradient method without smoothness and strong
  convexity.
\newblock \emph{CoRR}, abs/1512.03107, 2016.

\bibitem[Yang(2009)]{doi:10.1137/070689838}
W.~H. Yang.
\newblock Error bounds for convex polynomials.
\newblock \emph{SIAM Journal on Optimization}, 2009.

\bibitem[Zhang(2016)]{DBLP:journals/corr/abs/1606.00269}
Hui Zhang.
\newblock New analysis of linear convergence of gradient-type methods via
  unifying error bound conditions.
\newblock \emph{CoRR}, abs/1606.00269, 2016.

\bibitem[Zhang et~al.(2017)Zhang, Yang, and Jin]{DBLP:journals/corr/0005YJ17}
Lijun Zhang, Tianbao Yang, and Rong Jin.
\newblock Empirical risk minimization for stochastic convex optimization:
  O(1/n)- and o(1/n\({}^{\mbox{2}}\))-type of risk bounds.
\newblock \emph{CoRR}, abs/1702.02030, 2017.

\bibitem[Zinkevich(2003)]{zinkevich2003online}
Martin Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{ICML}, pages 928--936, 2003.

\end{thebibliography}
