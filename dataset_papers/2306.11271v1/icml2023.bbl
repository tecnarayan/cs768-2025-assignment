\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2020optimality}
Agarwal, A., Kakade, S.~M., Lee, J.~D., and Mahajan, G.
\newblock Optimality and approximation with policy gradient methods in markov
  decision processes.
\newblock In \emph{Conference on Learning Theory}, pp.\  64--66. PMLR, 2020.

\bibitem[Ajalloeian \& Stich(2020)Ajalloeian and
  Stich]{ajalloeian2020convergence}
Ajalloeian, A. and Stich, S.~U.
\newblock On the convergence of {SGD} with biased gradients.
\newblock \emph{arXiv preprint arXiv:2008.00051}, 2020.

\bibitem[Bagnell et~al.(2003)Bagnell, Kakade, Schneider, and
  Ng]{bagnell2003policy}
Bagnell, J., Kakade, S.~M., Schneider, J., and Ng, A.
\newblock Policy search by dynamic programming.
\newblock \emph{Advances in neural information processing systems}, 16, 2003.

\bibitem[Bertsekas(2022{\natexlab{a}})]{bertsekas2022abstract}
Bertsekas, D.
\newblock \emph{Abstract Dynamic Programming}.
\newblock Athena Scientific, 2022{\natexlab{a}}.

\bibitem[Bertsekas(2022{\natexlab{b}})]{bertsekas2022lessons}
Bertsekas, D.
\newblock \emph{Lessons from Alphazero for Optimal, Model Predictive, and
  Adaptive Control}.
\newblock Athena Scientific, 2022{\natexlab{b}}.

\bibitem[Bhandari et~al.(2018)Bhandari, Russo, and Singal]{bhandari2018finite}
Bhandari, J., Russo, D., and Singal, R.
\newblock A finite time analysis of temporal difference learning with linear
  function approximation.
\newblock In \emph{Conference on learning theory}, pp.\  1691--1692. PMLR,
  2018.

\bibitem[Boyd et~al.(2004)Boyd, Boyd, and Vandenberghe]{boyd2004convex}
Boyd, S., Boyd, S.~P., and Vandenberghe, L.
\newblock \emph{Convex Optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Choi et~al.(2009)Choi, Laibson, Madrian, and
  Metrick]{choi2009reinforcement}
Choi, J.~J., Laibson, D., Madrian, B.~C., and Metrick, A.
\newblock Reinforcement learning and savings behavior.
\newblock \emph{The Journal of finance}, 64\penalty0 (6):\penalty0 2515--2534,
  2009.

\bibitem[Dalal et~al.(2018)Dalal, Thoppe, Sz{\"o}r{\'e}nyi, and
  Mannor]{dalal2018finite}
Dalal, G., Thoppe, G., Sz{\"o}r{\'e}nyi, B., and Mannor, S.
\newblock Finite sample analysis of two-timescale stochastic approximation with
  applications to reinforcement learning.
\newblock In \emph{Conference On Learning Theory}, pp.\  1199--1233. PMLR,
  2018.

\bibitem[Dalal et~al.(2020)Dalal, Szorenyi, and Thoppe]{dalal2020tale}
Dalal, G., Szorenyi, B., and Thoppe, G.
\newblock A tale of two-timescale reinforcement learning with the tightest
  finite-time bound.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pp.\  3701--3708, 2020.

\bibitem[Elfwing et~al.(2018)Elfwing, Uchibe, and Doya]{elfwing2018sigmoid}
Elfwing, S., Uchibe, E., and Doya, K.
\newblock Sigmoid-weighted linear units for neural network function
  approximation in reinforcement learning.
\newblock \emph{Neural Networks}, 107:\penalty0 3--11, 2018.

\bibitem[Fan et~al.(2021)Fan, Jiang, and Sun]{fan2021hoeffding}
Fan, J., Jiang, B., and Sun, Q.
\newblock Hoeffding's inequality for general markov chains and its applications
  to statistical learning.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (139):\penalty0 1--35, 2021.

\bibitem[Farahmand et~al.(2010)Farahmand, Szepesv{\'a}ri, and
  Munos]{farahmand2010error}
Farahmand, A.-m., Szepesv{\'a}ri, C., and Munos, R.
\newblock Error propagation for approximate policy and value iteration.
\newblock \emph{Advances in Neural Information Processing Systems}, 23, 2010.

\bibitem[Fu et~al.(2020)Fu, Yang, and Wang]{fu2020single}
Fu, Z., Yang, Z., and Wang, Z.
\newblock Single-timescale actor-critic provably finds globally optimal policy.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and
  Meger]{fujimoto2018addressing}
Fujimoto, S., Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International conference on machine learning}, pp.\
  1587--1596. PMLR, 2018.

\bibitem[Garc{\i}a \& Fern{\'a}ndez(2015)Garc{\i}a and
  Fern{\'a}ndez]{garcia2015comprehensive}
Garc{\i}a, J. and Fern{\'a}ndez, F.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 16\penalty0
  (1):\penalty0 1437--1480, 2015.

\bibitem[Gargiani et~al.(2022)Gargiani, Zanelli, Liao-McPherson, Summers, and
  Lygeros]{gargiani2022dynamic}
Gargiani, M., Zanelli, A., Liao-McPherson, D., Summers, T., and Lygeros, J.
\newblock Dynamic programming through the lens of semismooth newton-type
  methods.
\newblock \emph{IEEE Control Systems Letters}, 2022.

\bibitem[Gelly \& Silver(2007)Gelly and Silver]{gelly2007combining}
Gelly, S. and Silver, D.
\newblock Combining online and offline knowledge in {UCT}.
\newblock In \emph{Proceedings of the 24th international conference on Machine
  learning}, pp.\  273--280, 2007.

\bibitem[Grand-Cl{\'e}ment(2021)]{grand2021convex}
Grand-Cl{\'e}ment, J.
\newblock From convex optimization to {MDPs}: A review of first-order,
  second-order and quasi-newton methods for {MDPs}.
\newblock \emph{arXiv preprint arXiv:2104.10677}, 2021.

\bibitem[Grondman et~al.(2012)Grondman, Busoniu, Lopes, and
  Babuska]{grondman2012survey}
Grondman, I., Busoniu, L., Lopes, G.~A., and Babuska, R.
\newblock A survey of actor-critic reinforcement learning: Standard and natural
  policy gradients.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics, Part C
  (Applications and Reviews)}, 42\penalty0 (6):\penalty0 1291--1307, 2012.

\bibitem[Gupta et~al.(2020)Gupta, Kumar, Lynch, Levine, and
  Hausman]{gupta2020relay}
Gupta, A., Kumar, V., Lynch, C., Levine, S., and Hausman, K.
\newblock Relay policy learning: Solving long-horizon tasks via imitation and
  reinforcement learning.
\newblock In \emph{Conference on Robot Learning}, pp.\  1025--1037. PMLR, 2020.

\bibitem[Ijspeert et~al.(2002)Ijspeert, Nakanishi, and
  Schaal]{ijspeert2002learning}
Ijspeert, A., Nakanishi, J., and Schaal, S.
\newblock Learning attractor landscapes for learning motor primitives.
\newblock \emph{Advances in neural information processing systems}, 15, 2002.

\bibitem[Jiang et~al.(2018)Jiang, Sun, and Fan]{jiang2018bernstein}
Jiang, B., Sun, Q., and Fan, J.
\newblock Bernstein's inequality for general markov chains.
\newblock \emph{arXiv preprint arXiv:1805.10721}, 2018.

\bibitem[Kaelbling et~al.(1996)Kaelbling, Littman, and
  Moore]{kaelbling1996reinforcement}
Kaelbling, L.~P., Littman, M.~L., and Moore, A.~W.
\newblock Reinforcement learning: A survey.
\newblock \emph{Journal of artificial intelligence research}, 4:\penalty0
  237--285, 1996.

\bibitem[Khodadadian et~al.(2022)Khodadadian, Doan, Romberg, and
  Maguluri]{khodadadian2022finite}
Khodadadian, S., Doan, T.~T., Romberg, J., and Maguluri, S.~T.
\newblock Finite sample analysis of two-time-scale natural actor-critic
  algorithm.
\newblock \emph{IEEE Transactions on Automatic Control}, 2022.

\bibitem[Kim et~al.(2013)Kim, Farahmand, Pineau, and Precup]{kim2013learning}
Kim, B., Farahmand, A.-m., Pineau, J., and Precup, D.
\newblock Learning from limited demonstrations.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Konda \& Tsitsiklis(1999)Konda and Tsitsiklis]{konda1999Actor}
Konda, V. and Tsitsiklis, J.
\newblock Actor-critic algorithms.
\newblock \emph{Advances in neural information processing systems}, 12, 1999.

\bibitem[Kumar et~al.(2020)Kumar, Gupta, and Levine]{kumar2020discor}
Kumar, A., Gupta, A., and Levine, S.
\newblock Discor: Corrective feedback in reinforcement learning via
  distribution correction.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 18560--18572, 2020.

\bibitem[Kumar et~al.(2023)Kumar, Koppel, and Ribeiro]{kumar2023sample}
Kumar, H., Koppel, A., and Ribeiro, A.
\newblock On the sample complexity of actor-critic method for reinforcement
  learning with function approximation.
\newblock \emph{Machine Learning}, pp.\  1--35, 2023.

\bibitem[Lazaric et~al.(2010)Lazaric, Ghavamzadeh, and
  Munos]{lazaric2016analysis}
Lazaric, A., Ghavamzadeh, M., and Munos, R.
\newblock Analysis of a classification-based policy iteration algorithm.
\newblock In \emph{27th International Conference on Machine Learning}, pp.\
  607--614. Omnipress, 2010.

\bibitem[Lee et~al.(2022)Lee, Seo, Lee, Abbeel, and Shin]{lee2022offline}
Lee, S., Seo, Y., Lee, K., Abbeel, P., and Shin, J.
\newblock Offline-to-online reinforcement learning via balanced replay and
  pessimistic q-ensemble.
\newblock In \emph{Conference on Robot Learning}, pp.\  1702--1712. PMLR, 2022.

\bibitem[Levin \& Peres(2017)Levin and Peres]{levin2017markov}
Levin, D.~A. and Peres, Y.
\newblock \emph{Markov Chains and Mixing Times}, volume 107.
\newblock American Mathematical Soc., 2017.

\bibitem[Lezaud(1998)]{lezaud1998chernoff}
Lezaud, P.
\newblock Chernoff-type bound for finite markov chains.
\newblock \emph{Annals of Applied Probability}, pp.\  849--867, 1998.

\bibitem[Li(2017)]{li2017deep}
Li, Y.
\newblock Deep reinforcement learning: An overview.
\newblock \emph{arXiv preprint arXiv:1701.07274}, 2017.

\bibitem[Munos(2003)]{munos2003error}
Munos, R.
\newblock Error bounds for approximate policy iteration.
\newblock In \emph{ICML}, volume~3, pp.\  560--567. Citeseer, 2003.

\bibitem[Nair et~al.(2020)Nair, Dalal, Gupta, and Levine]{nair2020accelerating}
Nair, A., Dalal, M., Gupta, A., and Levine, S.
\newblock Accelerating online reinforcement learning with offline datasets.
\newblock \emph{arXiv preprint arXiv:2006.09359}, 2020.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Nesterov, Y.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Ortner(2020)]{ortner2020regret}
Ortner, R.
\newblock Regret bounds for reinforcement learning via markov chain
  concentration.
\newblock \emph{Journal of Artificial Intelligence Research}, 67:\penalty0
  115--128, 2020.

\bibitem[Peters \& Schaal(2008)Peters and Schaal]{peters2008natural}
Peters, J. and Schaal, S.
\newblock Natural actor-critic.
\newblock \emph{Neurocomputing}, 71\penalty0 (7-9):\penalty0 1180--1190, 2008.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Puterman \& Brumelle(1979)Puterman and
  Brumelle]{puterman1979convergence}
Puterman, M.~L. and Brumelle, S.~L.
\newblock On the convergence of policy iteration in stationary dynamic
  programming.
\newblock \emph{Mathematics of Operations Research}, 4\penalty0 (1):\penalty0
  60--69, 1979.

\bibitem[Santos \& Rust(2004)Santos and Rust]{santos2004convergence}
Santos, M.~S. and Rust, J.
\newblock Convergence properties of policy iteration.
\newblock \emph{SIAM Journal on Control and Optimization}, 42\penalty0
  (6):\penalty0 2094--2115, 2004.

\bibitem[Shalev-Shwartz et~al.(2016)Shalev-Shwartz, Shammah, and
  Shashua]{shalev2016safe}
Shalev-Shwartz, S., Shammah, S., and Shashua, A.
\newblock Safe, multi-agent, reinforcement learning for autonomous driving.
\newblock \emph{arXiv preprint arXiv:1610.03295}, 2016.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{silver2014deterministic}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{International conference on machine learning}, pp.\
  387--395. PMLR, 2014.

\bibitem[Silver et~al.(2017)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2017mastering}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
  Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al.
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.
\newblock \emph{arXiv preprint arXiv:1712.01815}, 2017.

\bibitem[Silver et~al.(2018)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2018general}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
  Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al.
\newblock A general reinforcement learning algorithm that masters chess, shogi,
  and go through self-play.
\newblock \emph{Science}, 362\penalty0 (6419):\penalty0 1140--1144, 2018.

\bibitem[Song et~al.(2022)Song, Zhou, Sekhari, Bagnell, Krishnamurthy, and
  Sun]{song2022hybrid}
Song, Y., Zhou, Y., Sekhari, A., Bagnell, J.~A., Krishnamurthy, A., and Sun, W.
\newblock Hybrid rl: Using both offline and online data can make rl efficient.
\newblock \emph{arXiv preprint arXiv:2210.06718}, 2022.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and
  Mansour]{sutton1999policy}
Sutton, R.~S., McAllester, D., Singh, S., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock \emph{Advances in neural information processing systems}, 12, 1999.

\bibitem[Thrun \& Schwartz(1993)Thrun and Schwartz]{thrun1993issues}
Thrun, S. and Schwartz, A.
\newblock Issues in using function approximation for reinforcement learning.
\newblock In \emph{Proceedings of the 1993 Connectionist Models Summer School
  Hillsdale, NJ. Lawrence Erlbaum}, volume~6, 1993.

\bibitem[Uchendu et~al.(2022)Uchendu, Xiao, Lu, Zhu, Yan, Simon, Bennice, Fu,
  Ma, Jiao, et~al.]{uchendu2022jump}
Uchendu, I., Xiao, T., Lu, Y., Zhu, B., Yan, M., Simon, J., Bennice, M., Fu,
  C., Ma, C., Jiao, J., et~al.
\newblock Jump-start reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2204.02372}, 2022.

\bibitem[Van~Hasselt et~al.(2016)Van~Hasselt, Guez, and Silver]{van2016deep}
Van~Hasselt, H., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~30, 2016.

\bibitem[Wagenmaker \& Pacchiano(2022)Wagenmaker and
  Pacchiano]{wagenmaker2022leveraging}
Wagenmaker, A. and Pacchiano, A.
\newblock Leveraging offline data in online reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2211.04974}, 2022.

\bibitem[Wu et~al.(2017)Wu, Kreidieh, Parvate, Vinitsky, and Bayen]{wu2017flow}
Wu, C., Kreidieh, A., Parvate, K., Vinitsky, E., and Bayen, A.~M.
\newblock Flow: Architecture and benchmarking for reinforcement learning in
  traffic control.
\newblock \emph{arXiv preprint arXiv:1710.05465}, 10, 2017.

\bibitem[Xie et~al.(2021)Xie, Jiang, Wang, Xiong, and Bai]{xie2021policy}
Xie, T., Jiang, N., Wang, H., Xiong, C., and Bai, Y.
\newblock Policy finetuning: Bridging sample-efficient offline and online
  reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 34, 2021.

\bibitem[Xu et~al.(2020{\natexlab{a}})Xu, Wang, and Liang]{xu2020improving}
Xu, T., Wang, Z., and Liang, Y.
\newblock Improving sample complexity bounds for (natural) actor-critic
  algorithms.
\newblock In \emph{Proceedings of the 34th International Conference on Neural
  Information Processing Systems}, 2020{\natexlab{a}}.

\bibitem[Xu et~al.(2020{\natexlab{b}})Xu, Wang, and Liang]{xu2020non}
Xu, T., Wang, Z., and Liang, Y.
\newblock Non-asymptotic convergence analysis of two time-scale (natural)
  actor-critic algorithms.
\newblock \emph{arXiv preprint arXiv:2005.03557}, 2020{\natexlab{b}}.

\bibitem[Yang et~al.(2019)Yang, Chen, Hong, and Wang]{yang2019provably}
Yang, Z., Chen, Y., Hong, M., and Wang, Z.
\newblock Provably global convergence of actor-critic: A case for linear
  quadratic regulator with ergodic cost.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Zou et~al.(2019)Zou, Xu, and Liang]{zou2019finite}
Zou, S., Xu, T., and Liang, Y.
\newblock Finite-sample analysis for {SARSA} with linear function
  approximation.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\end{thebibliography}
