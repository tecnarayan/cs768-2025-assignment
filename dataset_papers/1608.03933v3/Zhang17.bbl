\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abernethy et~al.(2008{\natexlab{a}})Abernethy, Bartlett, Rakhlin, and
  Tewari]{Minimax:Online}
Jacob Abernethy, Peter~L. Bartlett, Alexander Rakhlin, and Ambuj Tewari.
\newblock Optimal stragies and minimax lower bounds for online convex games.
\newblock In \emph{Proceedings of the 21st Annual Conference on Learning
  Theory}, 2008{\natexlab{a}}.

\bibitem[Abernethy et~al.(2008{\natexlab{b}})Abernethy, Hazan, and
  Rakhlin]{Competing:Dark}
Jacob Abernethy, Elad Hazan, and Alexander Rakhlin.
\newblock Competing in the dark: An efficient algorithm for bandit linear
  optimization.
\newblock In \emph{Proceedings of the 21st Annual Conference on Learning},
  pages 263--274, 2008{\natexlab{b}}.

\bibitem[Besbes et~al.(2015)Besbes, Gur, and Zeevi]{Non-Stationary}
Omar Besbes, Yonatan Gur, and Assaf Zeevi.
\newblock Non-stationary stochastic optimization.
\newblock \emph{Operations Research}, 63\penalty0 (5):\penalty0 1227--1244,
  2015.

\bibitem[Boyd and Vandenberghe(2004)]{Convex-Optimization}
Stephen Boyd and Lieven Vandenberghe.
\newblock \emph{Convex Optimization}.
\newblock Cambridge University Press, 2004.

\bibitem[Buchbinder et~al.(2012)Buchbinder, Chen, Naor, and
  Shamir]{Online:Competitive}
Niv Buchbinder, Shahar Chen, Joseph~(Seffi) Naor, and Ohad Shamir.
\newblock Unified algorithms for online learning and competitive analysis.
\newblock In \emph{Proceedings of the 25th Annual Conference on Learning
  Theory}, 2012.

\bibitem[Cesa-bianchi et~al.(2012)Cesa-bianchi, Gaillard, Lugosi, and
  Stoltz]{Fixed:Share:NIPS12}
Nicol\`{o} Cesa-bianchi, Pierre Gaillard, Gabor Lugosi, and Gilles Stoltz.
\newblock Mirror descent meets fixed share (and feels no regret).
\newblock In \emph{Advances in Neural Information Processing Systems 25}, pages
  980--988, 2012.

\bibitem[Chiang et~al.(2012)Chiang, Yang, Lee, Mahdavi, Lu, Jin, and
  Zhu]{Gradual:COLT:12}
Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong
  Jin, and Shenghuo Zhu.
\newblock Online optimization with gradual variations.
\newblock In \emph{Proceedings of the 25th Annual Conference on Learning
  Theory}, 2012.

\bibitem[Daniely et~al.(2015)Daniely, Gonen, and
  Shalev-Shwartz]{Adaptive:ICML:15}
Amit Daniely, Alon Gonen, and Shai Shalev-Shwartz.
\newblock Strongly adaptive online learning.
\newblock In \emph{Proceedings of The 32nd International Conference on Machine
  Learning}, 2015.

\bibitem[Gong and Ye(2014)]{Linear:Ye}
Pinghua Gong and Jieping Ye.
\newblock Linear convergence of variance-reduced stochastic gradient without
  strong convexity.
\newblock \emph{ArXiv e-prints}, arXiv:1406.1102, 2014.

\bibitem[Hall and Willett(2013)]{Dynamic:ICML:13}
Eric~C. Hall and Rebecca~M. Willett.
\newblock Dynamical models and tracking regret in online convex programming.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning}, pages 579--587, 2013.

\bibitem[Hazan and Kale(2011)]{COLT:Hazan:2011}
Elad Hazan and Satyen Kale.
\newblock Beyond the regret minimization barrier: an optimal algorithm for
  stochastic strongly-convex optimization.
\newblock In \emph{Proceedings of the 24th Annual Conference on Learning
  Theory}, pages 421--436, 2011.

\bibitem[Hazan and Seshadhri(2007)]{Adaptive:Hazan}
Elad Hazan and C.~Seshadhri.
\newblock Adaptive algorithms for online decision problems.
\newblock \emph{Electronic Colloquium on Computational Complexity}, 88, 2007.

\bibitem[Hazan et~al.(2007)Hazan, Agarwal, and Kale]{ML:Hazan:2007}
Elad Hazan, Amit Agarwal, and Satyen Kale.
\newblock Logarithmic regret algorithms for online convex optimization.
\newblock \emph{Machine Learning}, 69\penalty0 (2-3):\penalty0 169--192, 2007.

\bibitem[Herbster and Warmuth(1998)]{Herbster1998}
Mark Herbster and Manfred~K. Warmuth.
\newblock Tracking the best expert.
\newblock \emph{Machine Learning}, 32\penalty0 (2):\penalty0 151--178, 1998.

\bibitem[Jadbabaie et~al.(2015)Jadbabaie, Rakhlin, Shahrampour, and
  Sridharan]{Dynamic:AISTATS:15}
Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan.
\newblock Online optimization: Competing with dynamic comparators.
\newblock In \emph{Proceedings of the 18th International Conference on
  Artificial Intelligence and Statistics}, 2015.

\bibitem[Mokhtari et~al.(2016)Mokhtari, Shahrampour, Jadbabaie, and
  Ribeiro]{Dynamic:Strongly}
Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro.
\newblock Online optimization in dynamic environments: Improved regret rates
  for strongly convex problems.
\newblock \emph{ArXiv e-prints}, arXiv:1603.04954, 2016.

\bibitem[Necoara et~al.(2015)Necoara, Nesterov, and
  Glineur]{Linear:non-strongly:convex}
I.~Necoara, Yu. Nesterov, and F.~Glineur.
\newblock Linear convergence of first order methods for non-strongly convex
  optimization.
\newblock \emph{ArXiv e-prints}, arXiv:1504.06298, 2015.

\bibitem[Nemirovski(2004)]{Interior:Nemirovski}
Arkadi Nemirovski.
\newblock Interior point polynomial time methods in convex programming.
\newblock Lecture notes, Technion -- Israel Institute of Technology, 2004.

\bibitem[Nesterov(2004)]{nesterov2004introductory}
Yurii Nesterov.
\newblock \emph{Introductory lectures on convex optimization: a basic course},
  volume~87 of \emph{Applied optimization}.
\newblock Kluwer Academic Publishers, 2004.

\bibitem[Rakhlin and Sridharan(2013)]{Predictable:NIPS:2013}
Sasha Rakhlin and Karthik Sridharan.
\newblock Optimization, learning, and games with predictable sequences.
\newblock In \emph{Advances in Neural Information Processing Systems 26}, pages
  3066--3074, 2013.

\bibitem[Shalev-Shwartz(2011)]{Online:suvery}
Shai Shalev-Shwartz.
\newblock Online learning and online convex optimization.
\newblock \emph{Foundations and Trends in Machine Learning}, 4\penalty0
  (2):\penalty0 107--194, 2011.

\bibitem[Shalev-Shwartz et~al.(2007)Shalev-Shwartz, Singer, and
  Srebro]{ICML_Pegasos}
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.
\newblock Pegasos: primal estimated sub-gradient solver for {SVM}.
\newblock In \emph{Proceedings of the 24th International Conference on Machine
  Learning}, pages 807--814, 2007.

\bibitem[Wang and Lin(2014)]{Complexity:FDM}
Po-Wei Wang and Chih-Jen Lin.
\newblock Iteration complexity of feasible descent methods for convex
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 15:\penalty0 1523--1548,
  2014.

\bibitem[Yang et~al.(2016)Yang, Zhang, Jin, and Yi]{Dynamic:2016}
Tianbao Yang, Lijun Zhang, Rong Jin, and Jinfeng Yi.
\newblock Tracking slowly moving clairvoyant: Optimal dynamic regret of online
  learning with true and noisy gradient.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning}, 2016.

\bibitem[Zinkevich(2003)]{zinkevich-2003-online}
Martin Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{Proceedings of the 20th International Conference on Machine
  Learning}, pages 928--936, 2003.

\end{thebibliography}
