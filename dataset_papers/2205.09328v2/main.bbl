\begin{thebibliography}{10}

\bibitem{wang2017deep}
Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang.
\newblock Deep \& cross network for ad click predictions.
\newblock In {\em Proceedings of the ADKDD'17}, pages 1--7. 2017.

\bibitem{yoon2020vime}
Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van~der Schaar.
\newblock {VIME}: Extending the success of self-and semi-supervised learning to
  tabular domain.
\newblock {\em Advances in Neural Information Processing Systems},
  33:11033--11043, 2020.

\bibitem{zhang2020customer}
Yixuan Zhang, Jialiang Tong, Ziyi Wang, and Fengqiang Gao.
\newblock Customer transaction fraud detection using xgboost model.
\newblock In {\em International Conference on Computer Engineering and
  Application}, pages 554--558. IEEE, 2020.

\bibitem{wang2020data}
Zifeng Wang and Suzhen Li.
\newblock Data-driven risk assessment on urban pipeline network based on a
  cluster model.
\newblock {\em Reliability Engineering \& System Safety}, 196:106781, 2020.

\bibitem{huang2020tabtransformer}
Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin.
\newblock {TabTransformer}: Tabular data modeling using contextual embeddings.
\newblock {\em arXiv preprint arXiv:2012.06678}, 2020.

\bibitem{padhi2021tabular}
Inkit Padhi, Yair Schiff, Igor Melnyk, Mattia Rigotti, Youssef Mroueh, Pierre
  Dognin, Jerret Ross, Ravi Nair, and Erik Altman.
\newblock Tabular transformers for modeling multivariate time series.
\newblock In {\em IEEE International Conference on Acoustics, Speech and Signal
  Processing}, pages 3565--3569. IEEE, 2021.

\bibitem{cholakov2022gatedtabtransformer}
Radostin Cholakov and Todor Kolev.
\newblock The {GatedTabTransformer}. an enhanced deep learning architecture for
  tabular modeling.
\newblock {\em arXiv preprint arXiv:2201.00199}, 2022.

\bibitem{wang2021survtrace}
Zifeng Wang and Jimeng Sun.
\newblock {SurvTRACE}: Transformers for survival analysis with competing
  events.
\newblock {\em arXiv preprint arXiv:2110.00855}, 2021.

\bibitem{ucar2021subtab}
Talip Ucar, Ehsan Hajiramezanali, and Lindsay Edwards.
\newblock {SubTab}: Subsetting features of tabular data for self-supervised
  representation learning.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{somepalli2021saint}
Gowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C~Bayan Bruss, and Tom
  Goldstein.
\newblock {SAINT}: Improved neural networks for tabular data via row attention
  and contrastive pre-training.
\newblock {\em arXiv preprint arXiv:2106.01342}, 2021.

\bibitem{bahri2022scarf}
Dara Bahri, Heinrich Jiang, Yi~Tay, and Donald Metzler.
\newblock {SCARF}: Self-supervised contrastive learning using random feature
  corruption.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{nargesian2018table}
Fatemeh Nargesian, Erkang Zhu, Ken~Q Pu, and Ren{\'e}e~J Miller.
\newblock Table union search on open data.
\newblock {\em Proceedings of the VLDB Endowment}, 11(7):813--825, 2018.

\bibitem{zhu2019josie}
Erkang Zhu, Dong Deng, Fatemeh Nargesian, and Ren{\'e}e~J Miller.
\newblock Josie: Overlap set similarity search for finding joinable tables in
  data lakes.
\newblock In {\em International Conference on Management of Data}, pages
  847--864, 2019.

\bibitem{dong2021efficient}
Yuyang Dong, Kunihiro Takeoka, Chuan Xiao, and Masafumi Oyamada.
\newblock Efficient joinable table discovery in data lakes: A high-dimensional
  similarity-based approach.
\newblock In {\em IEEE International Conference on Data Engineering}, pages
  456--467. IEEE, 2021.

\bibitem{he2021masked}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross
  Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock {\em arXiv preprint arXiv:2111.06377}, 2021.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{bao2021beit}
Hangbo Bao, Li~Dong, and Furu Wei.
\newblock {BEiT}: Bert pre-training of image transformers.
\newblock {\em arXiv preprint arXiv:2106.08254}, 2021.

\bibitem{mikolov2013efficient}
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
\newblock Efficient estimation of word representations in vector space.
\newblock {\em arXiv preprint arXiv:1301.3781}, 2013.

\bibitem{schuster2012japanese}
Mike Schuster and Kaisuke Nakajima.
\newblock Japanese and korean voice search.
\newblock In {\em IEEE international conference on acoustics, speech and signal
  processing (ICASSP)}, pages 5149--5152. IEEE, 2012.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{lin2020birds}
Bill~Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang Ren.
\newblock Birds have four legs?! {NumerSense}: Probing numerical commonsense
  knowledge of pre-trained language models.
\newblock In {\em Proceedings of the EMNLP}, pages 6862--6868, 2020.

\bibitem{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem{darabi2021contrastive}
Sajad Darabi, Shayan Fazeli, Ali Pazoki, Sriram Sankararaman, and Majid
  Sarrafzadeh.
\newblock Contrastive mixup: Self-and semi-supervised learning for tabular
  domain.
\newblock {\em arXiv preprint arXiv:2108.12296}, 2021.

\bibitem{stonebraker2018c}
Mike Stonebraker, Daniel~J Abadi, Adam Batkin, Xuedong Chen, Mitch Cherniack,
  Miguel Ferreira, Edmond Lau, Amerson Lin, Sam Madden, Elizabeth O'Neil,
  et~al.
\newblock C-store: a column-oriented dbms.
\newblock In {\em Making Databases Work: the Pragmatic Wisdom of Michael
  Stonebraker}, pages 491--518. 2018.

\bibitem{khosla2020supervised}
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
  Isola, Aaron Maschinot, Ce~Liu, and Dilip Krishnan.
\newblock Supervised contrastive learning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:18661--18673, 2020.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{chen2016xgboost}
Tianqi Chen and Carlos Guestrin.
\newblock {Xgboost}: A scalable tree boosting system.
\newblock In {\em ACM SIGKDD International Conference on Knowledge Discovery
  and Data Mining}, pages 785--794, 2016.

\bibitem{klambauer2017self}
G{\"u}nter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter.
\newblock Self-normalizing neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem{arik2021tabnet}
Sercan~O Ar{\i}k and Tomas Pfister.
\newblock Tabnet: Attentive interpretable tabular learning.
\newblock In {\em AAAI}, volume~35, pages 6679--6687, 2021.

\bibitem{song2019autoint}
Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and
  Jian Tang.
\newblock {AutoInt}: Automatic feature interaction learning via self-attentive
  neural networks.
\newblock In {\em Proceedings of the 28th ACM International Conference on
  Information and Knowledge Management}, pages 1161--1170, 2019.

\bibitem{gorishniy2021revisiting}
Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.
\newblock Revisiting deep learning models for tabular data.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{romera2015embarrassingly}
Bernardino Romera-Paredes and Philip Torr.
\newblock An embarrassingly simple approach to zero-shot learning.
\newblock In {\em International conference on machine learning}, pages
  2152--2161. PMLR, 2015.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in Neural Information Processing Systems},
  33:1877--1901, 2020.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International Conference on Machine Learning}, pages
  8748--8763. PMLR, 2021.

\bibitem{ke2017lightgbm}
Guolin Ke, Qi~Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei
  Ye, and Tie-Yan Liu.
\newblock {LightGBM}: A highly efficient gradient boosting decision tree.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem{dorogush2018catboost}
Anna~Veronika Dorogush, Vasily Ershov, and Andrey Gulin.
\newblock {CatBoost}: gradient boosting with categorical features support.
\newblock {\em arXiv preprint arXiv:1810.11363}, 2018.

\bibitem{chen2021danets}
Jintai Chen, Kuanlun Liao, Yao Wan, Danny~Z Chen, and Jian Wu.
\newblock Danets: Deep abstract networks for tabular data classification and
  regression.
\newblock {\em arXiv preprint arXiv:2112.02962}, 2021.

\bibitem{borisov2021deep}
Vadim Borisov, Tobias Leemann, Kathrin Se{\ss}ler, Johannes Haug, Martin
  Pawelczyk, and Gjergji Kasneci.
\newblock Deep neural networks and tabular data: A survey.
\newblock {\em arXiv preprint arXiv:2110.01889}, 2021.

\bibitem{abutbul2020dnf}
Ami Abutbul, Gal Elidan, Liran Katzir, and Ran El-Yaniv.
\newblock Dnf-net: A neural architecture for tabular data.
\newblock {\em arXiv preprint arXiv:2006.06465}, 2020.

\bibitem{katzir2020net}
Liran Katzir, Gal Elidan, and Ran El-Yaniv.
\newblock Net-dnf: Effective deep modeling of tabular data.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{luo2020network}
Yuanfei Luo, Hao Zhou, Wei-Wei Tu, Yuqiang Chen, Wenyuan Dai, and Qiang Yang.
\newblock Network on network for tabular data classification in real-world
  applications.
\newblock In {\em Proceedings of the 43rd International ACM SIGIR Conference on
  Research and Development in Information Retrieval}, pages 2317--2326, 2020.

\bibitem{guo2021tabgnn}
Xiawei Guo, Yuhan Quan, Huan Zhao, Quanming Yao, Yong Li, and Weiwei Tu.
\newblock {TabGNN}: Multiplex graph neural network for tabular data prediction.
\newblock {\em arXiv preprint arXiv:2108.09127}, 2021.

\bibitem{luo2019autocross}
Yuanfei Luo, Mengshuo Wang, Hao Zhou, Quanming Yao, Wei-Wei Tu, Yuqiang Chen,
  Wenyuan Dai, and Qiang Yang.
\newblock Autocross: Automatic feature crossing for tabular data in real-world
  applications.
\newblock In {\em ACM SIGKDD International Conference on Knowledge Discovery \&
  Data Mining}, pages 1936--1945, 2019.

\bibitem{qin2021retrieval}
Jiarui Qin, Weinan Zhang, Rong Su, Zhirong Liu, Weiwen Liu, Ruiming Tang,
  Xiuqiang He, and Yong Yu.
\newblock Retrieval \& interaction machine for tabular data prediction.
\newblock In {\em ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages 1379--1389, 2021.

\bibitem{shwartz2022tabular}
Ravid Shwartz-Ziv and Amitai Armon.
\newblock Tabular data: Deep learning is not all you need.
\newblock {\em Information Fusion}, 81:84--90, 2022.

\bibitem{kadra2021well}
Arlind Kadra, Marius Lindauer, Frank Hutter, and Josif Grabocka.
\newblock Well-tuned simple nets excel on tabular datasets.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  pages 248--255. IEEE, 2009.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{yosinski2014transferable}
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
\newblock How transferable are features in deep neural networks?
\newblock {\em Advances in Neural Information Processing Systems}, 27, 2014.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  pages 770--778, 2016.

\bibitem{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  pages 4700--4708, 2017.

\bibitem{zoph2018learning}
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc~V Le.
\newblock Learning transferable architectures for scalable image recognition.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  pages 8697--8710, 2018.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em Journal of Machine Learning Research}, 21:1--67, 2020.

\bibitem{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ~R Salakhutdinov,
  and Quoc~V Le.
\newblock {XLNet}: Generalized autoregressive pretraining for language
  understanding.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{lewis2020bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.
\newblock {BART}: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock In {\em Annual Meeting of the Association for Computational
  Linguistics}, pages 7871--7880, 2020.

\bibitem{gao2021simcse}
Tianyu Gao, Xingcheng Yao, and Danqi Chen.
\newblock Simcse: Simple contrastive learning of sentence embeddings.
\newblock In {\em Conference on Empirical Methods in Natural Language
  Processing}, pages 6894--6910, 2021.

\bibitem{kenton2019bert}
Jacob Devlin Ming-Wei~Chang Kenton and Lee~Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of NAACL-HLT}, pages 4171--4186, 2019.

\bibitem{tsai2019multimodal}
Yao-Hung~Hubert Tsai, Shaojie Bai, Paul~Pu Liang, J~Zico Kolter, Louis-Philippe
  Morency, and Ruslan Salakhutdinov.
\newblock Multimodal transformer for unaligned multimodal language sequences.
\newblock In {\em Proceedings of the Annual Meeting of the Association for
  Computational Linguistics}, 2019.

\bibitem{akbari2021vatt}
Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin
  Cui, and Boqing Gong.
\newblock {VATT}: Transformers for multimodal self-supervised learning from raw
  video, audio and text.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{lepikhin2020gshard}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping
  Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
\newblock {GShard}: Scaling giant models with conditional computation and
  automatic sharding.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{fedus2021switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock {\em arXiv preprint arXiv:2101.03961}, 2021.

\bibitem{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In {\em International conference on machine learning}, pages
  1597--1607. PMLR, 2020.

\bibitem{yin2020tabert}
Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel.
\newblock {TaBERT}: Pretraining for joint understanding of textual and tabular
  data.
\newblock In {\em Annual Meeting of the Association for Computational
  Linguistics}, pages 8413--8426, 2020.

\bibitem{iida2021tabbie}
Hiroshi Iida, Dung Thai, Varun Manjunatha, and Mohit Iyyer.
\newblock {TABBIE}: Pretrained representations of tabular data.
\newblock In {\em Conference of the North American Chapter of the Association
  for Computational Linguistics}, 2021.

\bibitem{lin2020bridging}
Xi~Victoria Lin, Richard Socher, and Caiming Xiong.
\newblock Bridging textual and tabular data for cross-domain text-to-sql
  semantic parsing.
\newblock {\em arXiv preprint arXiv:2012.12627}, 2020.

\bibitem{deng2021turl}
Xiang Deng, Huan Sun, Alyssa Lees, You Wu, and Cong Yu.
\newblock {TURL}: Table understanding through representation learning.
\newblock {\em Proceedings of the VLDB Endowment}, 2021.

\bibitem{yang2022tableformer}
Jingfeng Yang, Aditya Gupta, Shyam Upadhyay, Luheng He, Rahul Goel, and Shachi
  Paul.
\newblock {TableFormer}: Robust transformer modeling for table-text encoding.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics}, pages 528--537, 2022.

\bibitem{wang2021retrieving}
Fei Wang, Kexuan Sun, Muhao Chen, Jay Pujara, and Pedro~A Szekely.
\newblock Retrieving complex tables with multi-granular graph representation
  learning.
\newblock In {\em SIGIR}, 2021.

\bibitem{wang2022robust}
Fei Wang, Zhewei Xu, Pedro Szekely, and Muhao Chen.
\newblock Robust (controlled) table-to-text generation with structure-aware
  equivariance learning.
\newblock {\em arXiv preprint arXiv:2205.03972}, 2022.

\bibitem{cutrona2019semantic}
Vincenzo Cutrona.
\newblock Semantic enrichment for large-scale data analytics.
\newblock 2019.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{van2018representation}
Aaron Van~den Oord, Yazhe Li, Oriol Vinyals, et~al.
\newblock Representation learning with contrastive predictive coding.
\newblock {\em arXiv preprint arXiv:1807.03748}, 2(3):4, 2018.

\end{thebibliography}
