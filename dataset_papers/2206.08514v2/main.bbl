\begin{thebibliography}{10}

\bibitem{azizi2021t}
Ahmadreza Azizi, Ibrahim~Asadullah Tahmid, Asim Waheed, Neal Mangaokar, Jiameng
  Pu, Mobin Javed, Chandan~K Reddy, and Bimal Viswanath.
\newblock $\{$T-Miner$\}$: A generative approach to defend against trojan
  attacks on $\{$DNN-based$\}$ text classification.
\newblock In {\em {USENIX} Security Symposium}, 2021.

\bibitem{bagdasaryan2021blind}
Eugene Bagdasaryan and Vitaly Shmatikov.
\newblock Blind backdoors in deep learning models.
\newblock In {\em {USENIX} Security Symposium}, 2021.

\bibitem{bommasani2021opportunities}
Rishi Bommasani, Drew~A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon,
  Niladri~S. Chatterji, Annie~S. Chen, Kathleen Creel, Jared~Quincy Davis,
  Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano
  Ermon, John Etchemendy, Kawin Ethayarajh, Li~Fei{-}Fei, Chelsea Finn, Trevor
  Gale, Lauren Gillespie, Karan Goel, Noah~D. Goodman, Shelby Grossman, Neel
  Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel~E. Ho, Jenny
  Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky,
  Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar
  Khattab, Pang~Wei Koh, Mark~S. Krass, Ranjay Krishna, Rohith Kuditipudi, and
  et~al.
\newblock On the opportunities and risks of foundation models.
\newblock {\em CoRR}, 2021.

\bibitem{Cer2018uni}
Daniel Cer, Yinfei Yang, Sheng{-}yi Kong, Nan Hua, Nicole Limtiaco, Rhomni~St.
  John, Noah Constant, Mario Guajardo{-}Cespedes, Steve Yuan, Chris Tar, Brian
  Strope, and Ray Kurzweil.
\newblock Universal sentence encoder for english.
\newblock In {\em Proceedings of EMNLP}, 2018.

\bibitem{chen2019detecting}
Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin
  Edwards, Taesung Lee, Ian~M. Molloy, and Biplav Srivastava.
\newblock Detecting backdoor attacks on deep neural networks by activation
  clustering.
\newblock In {\em Workshop on Artificial Intelligence Safety on AAAI}, 2019.

\bibitem{chen2021bki}
Chuanshuai Chen and Jiazhu Dai.
\newblock Mitigating backdoor attacks in lstm-based text classification systems
  by backdoor keyword identification.
\newblock {\em Neurocomputing}, 2021.

\bibitem{chen2021badnl}
Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni
  Shen, Zhonghai Wu, and Yang Zhang.
\newblock Badnl: Backdoor attacks against {NLP} models with semantic-preserving
  improvements.
\newblock In {\em Annual Computer Security Applications Conference, {ACSAC}},
  2021.

\bibitem{chen2017targeted}
Xinyun Chen, Chang Liu, Bo~Li, Kimberly Lu, and Dawn Song.
\newblock Targeted backdoor attacks on deep learning systems using data
  poisoning.
\newblock {\em CoRR}, 2017.

\bibitem{chen2021textual}
Yangyi Chen, Fanchao Qi, Zhiyuan Liu, and Maosong Sun.
\newblock Textual backdoor attacks can be more harmful via two simple tricks.
\newblock {\em CoRR}, 2021.

\bibitem{cui2022prototypical}
Ganqu Cui, Shengding Hu, Ning Ding, Longtao Huang, and Zhiyuan Liu.
\newblock Prototypical verbalizer for prompt-based few-shot tuning.
\newblock In {\em Proceedings of ACL}, 2022.

\bibitem{Dai2019insert}
Jiazhu Dai, Chuanshuai Chen, and Yufeng Li.
\newblock A backdoor attack against lstm-based text classification systems.
\newblock {\em {IEEE} Access}, 2019.

\bibitem{Davidson2017hsol}
Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber.
\newblock Automated hate speech detection and the problem of offensive
  language.
\newblock {\em Proceedings of AAAI on Web and Social Media}, 2017.

\bibitem{Devlin19bert}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of NAACL-HLT}, 2019.

\bibitem{ding2022delta}
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su,
  Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et~al.
\newblock Delta tuning: A comprehensive study of parameter efficient methods
  for pre-trained language models.
\newblock {\em arXiv preprint arXiv:2203.06904}, 2022.

\bibitem{gan2021triggerless}
Leilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yuxian Meng, Fei Wu, Yi~Yang,
  Shangwei Guo, and Chun Fan.
\newblock Triggerless backdoor attack for {NLP} tasks with clean labels.
\newblock In {\em Proceedings of NAACL}, 2022.

\bibitem{gao2021strip}
Yansong Gao, Yeonjae Kim, Bao~Gia Doan, Zhi Zhang, Gongxuan Zhang, Surya Nepal,
  Damith~Chinthana Ranasinghe, and Hyoungshick Kim.
\newblock Design and evaluation of a multi-domain trojan detection method on
  deep neural networks.
\newblock {\em CoRR}, 2019.

\bibitem{gu2017badnets}
Tianyu Gu, Brendan Dolan{-}Gavitt, and Siddharth Garg.
\newblock Badnets: Identifying vulnerabilities in the machine learning model
  supply chain.
\newblock {\em CoRR}, 2017.

\bibitem{han2021pre}
Xu~Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu,
  Liang Zhang, Wentao Han, Minlie Huang, Qin Jin, Yanyan Lan, Yang Liu, Zhiyuan
  Liu, Zhiwu Lu, Xipeng Qiu, Ruihua Song, Jie Tang, Ji{-}Rong Wen, Jinhui Yuan,
  Wayne~Xin Zhao, and Jun Zhu.
\newblock Pre-trained models: Past, present and future.
\newblock {\em AI Open}, 2021.

\bibitem{han2021ptr}
Xu~Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun.
\newblock Ptr: Prompt tuning with rules for text classification.
\newblock {\em arXiv preprint arXiv:2105.11259}, 2021.

\bibitem{houlsby2019parameter}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
  De~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In {\em Proceedings of ICML}, 2019.

\bibitem{hu2021knowledgeable}
Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Juanzi Li, and Maosong Sun.
\newblock Knowledgeable prompt-tuning: Incorporating knowledge into prompt
  verbalizer for text classification.
\newblock In {\em Proceedings of ACL}, 2022.

\bibitem{Jin2020is}
Di~Jin, Zhijing Jin, Joey~Tianyi Zhou, and Peter Szolovits.
\newblock Is {BERT} really robust? {A} strong baseline for natural language
  attack on text classification and entailment.
\newblock In {\em Proceedings of AAAI}, 2020.

\bibitem{kingma2014adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In {\em Proceedings of ICLR}, 2015.

\bibitem{Kumar20adv}
Ram Shankar~Siva Kumar, Magnus Nystr{\"{o}}m, John Lambert, Andrew Marshall,
  Mario Goertzel, Andi Comissoneru, Matt Swann, and Sharon Xia.
\newblock Adversarial machine learning-industry perspectives.
\newblock In {\em {IEEE} Security and Privacy Workshops, {SP} Workshops}, 2020.

\bibitem{Kurita2020weight}
Keita Kurita, Paul Michel, and Graham Neubig.
\newblock Weight poisoning attacks on pretrained models.
\newblock In {\em Proceedings of ACL}, 2020.

\bibitem{Li2020bert}
Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu.
\newblock {BERT}-{ATTACK}: Adversarial attack against {BERT} using {BERT}.
\newblock In {\em Proceedings of EMNLP}, 2020.

\bibitem{Li2021lwp}
Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, and Xipeng Qiu.
\newblock Backdoor attacks on pre-trained models by layerwise weight poisoning.
\newblock In {\em Proceedings of EMNLP}, 2021.

\bibitem{li2020invisible}
Shaofeng Li, Minhui Xue, Benjamin Zi~Hao Zhao, Haojin Zhu, and Xinpeng Zhang.
\newblock Invisible backdoor attacks on deep neural networks via steganography
  and regularization.
\newblock {\em IEEE Transactions on Dependable and Secure Computing}, 2021.

\bibitem{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In {\em Proceedings of ACL}, 2021.

\bibitem{li2020backdoor}
Yiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia.
\newblock Backdoor learning: A survey.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  2022.

\bibitem{li2022backdoorbox}
Yiming Li, Mengxi Ya, Yang Bai, Yong Jiang, and Shu-Tao Xia.
\newblock {BackdoorBox}: A python toolbox for backdoor learning.
\newblock 2022.

\bibitem{liu2018fine}
Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock Fine-pruning: Defending against backdooring attacks on deep neural
  networks.
\newblock In {\em International Symposium on Research in Attacks, Intrusions,
  and Defenses}, 2018.

\bibitem{liu2021pre}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
  Graham Neubig.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting
  methods in natural language processing.
\newblock {\em arXiv preprint arXiv:2107.13586}, 2021.

\bibitem{liu2018trojan}
Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen{-}Chuan Lee, Juan Zhai, Weihang Wang,
  and Xiangyu Zhang.
\newblock Trojaning attack on neural networks.
\newblock In {\em Annual Network and Distributed System Security Symposium,
  {NDSS}}, 2018.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: {A} robustly optimized {BERT} pretraining approach.
\newblock {\em CoRR}, 2019.

\bibitem{liu2020reflection}
Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu.
\newblock Reflection backdoor: A natural backdoor attack on deep neural
  networks.
\newblock In {\em Proceedings of ECCV}, 2020.

\bibitem{maas2011imdb}
Andrew~L. Maas, Raymond~E. Daly, Peter~T. Pham, Dan Huang, Andrew~Y. Ng, and
  Christopher Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In {\em Proceedings of NAACL-HLT}, 2011.

\bibitem{ryan2020n2d}
Ryan McConville, Raúl Santos-Rodríguez, Robert~J Piechocki, and Ian Craddock.
\newblock N2d: (not too) deep clustering via clustering the local manifold of
  an autoencoded embedding.
\newblock In {\em International Conference on Pattern Recognition (ICPR)},
  2021.

\bibitem{mcinnes2017hdbscn}
Leland McInnes and John Healy.
\newblock Accelerated hierarchical density based clustering.
\newblock In {\em IEEE International Conference on Data Mining Workshops
  (ICDMW)}, 2017.

\bibitem{Morris2020reeval}
John Morris, Eli Lifland, Jack Lanchantin, Yangfeng Ji, and Yanjun Qi.
\newblock Reevaluating adversarial examples in natural language.
\newblock In {\em Findings of EMNLP}, 2020.

\bibitem{pang:2022:eurosp}
Ren Pang, Zheng Zhang, Xiangshan Gao, Zhaohan Xi, Shouling Ji, Peng Cheng, and
  Ting Wang.
\newblock Trojanzoo: Towards unified, holistic, and practical evaluation of
  neural backdoors.
\newblock In {\em IEEE European Symposium on Security and Privacy (Euro S\&P)},
  2022.

\bibitem{Qi2021onion}
Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, and Maosong Sun.
\newblock {ONION:} {A} simple and effective defense against textual backdoor
  attacks.
\newblock In {\em Proceedings of EMNLP}, 2021.

\bibitem{Qi2021mind}
Fanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li, Zhiyuan Liu, and Maosong Sun.
\newblock Mind the style of text! adversarial and backdoor attacks based on
  text style transfer.
\newblock In {\em Proceedings of EMNLP}, 2021.

\bibitem{Qi2020hidden}
Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang,
  and Maosong Sun.
\newblock Hidden killer: Invisible textual backdoor attacks with syntactic
  trigger.
\newblock In {\em Proceedings of ACL-IJCNLP}, 2021.

\bibitem{Qi2020turn}
Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, and Maosong Sun.
\newblock Turn the combination lock: Learnable textual backdoor attacks via
  word substitution.
\newblock In {\em Proceedings of ACL-IJCNLP}, 2021.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 2019.

\bibitem{Reimers2019sentence}
Nils Reimers and Iryna Gurevych.
\newblock Sentence-{BERT}: Sentence embeddings using {S}iamese {BERT}-networks.
\newblock In {\em Proceedings of EMNLP-IJCNLP}, 2019.

\bibitem{saha2020hidden}
Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash.
\newblock Hidden trigger backdoor attacks.
\newblock In {\em Proceedings of AAAI}, 2020.

\bibitem{NBC2020umap}
Tim Sainburg, Leland McInnes, and Timothy~Q. Gentner.
\newblock Parametric {UMAP:} learning embeddings with deep neural networks for
  representation and semi-supervised learning.
\newblock {\em CoRR}, 2020.

\bibitem{Georgios2003lingspam}
Georgios Sakkis, Ion Androutsopoulos, Georgios Paliouras, Vangelis Karkaletsis,
  Constantine~D. Spyropoulos, and Panagiotis Stamatopoulos.
\newblock A memory-based approach to anti-spam filtering for mailing lists.
\newblock {\em Information Retrieval}, 2003.

\bibitem{schwarzschild2021just}
Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John~P. Dickerson, and Tom
  Goldstein.
\newblock Just how toxic is data poisoning? {A} unified benchmark for backdoor
  and data poisoning attacks.
\newblock In {\em Proceedings of ICML}, 2021.

\bibitem{shen2022rethink}
Lingfeng Shen, Haiyun Jiang, Lemao Liu, and Shuming Shi.
\newblock Rethink stealthy backdoor attacks in natural language processing.
\newblock {\em arXiv preprint arXiv:2201.02993}, 2022.

\bibitem{Shen2021backdoor}
Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi,
  Chengfang Fang, Jianwei Yin, and Ting Wang.
\newblock Backdoor pre-trained models can transfer to all.
\newblock In {\em {ACM} {SIGSAC} Conference on Computer and Communications
  Security, {CCS}}, 2021.

\bibitem{socher-etal-2013-recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D. Manning,
  Andrew Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In {\em Proceedings of EMNLP}, 2013.

\bibitem{tran2018spectral}
Brandon Tran, Jerry Li, and Aleksander Madry.
\newblock Spectral signatures in backdoor attacks.
\newblock In {\em Proceedings of NeurIPS}, 2018.

\bibitem{turner2019label}
Alexander Turner, Dimitris Tsipras, and Aleksander Madry.
\newblock Label-consistent backdoor attacks.
\newblock {\em CoRR}, 2019.

\bibitem{veldanda2021nnoculation}
Akshaj~Kumar Veldanda, Kang Liu, Benjamin Tan, Prashanth Krishnamurthy, Farshad
  Khorrami, Ramesh Karri, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock Nnoculation: Catching badnets in the wild.
\newblock In {\em Proceedings of the 14th ACM Workshop on Artificial
  Intelligence and Security}, 2021.

\bibitem{wang2020practical}
Ren Wang, Gaoyuan Zhang, Sijia Liu, Pin-Yu Chen, Jinjun Xiong, and Meng Wang.
\newblock Practical detection of trojan neural networks: Data-limited and
  data-free cases.
\newblock In {\em Proceedings of ECCV}, 2020.

\bibitem{wolf2020transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, R{\'{e}}mi Louf, Morgan Funtowicz,
  Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite,
  Julien Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin
  Lhoest, and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In {\em Proceedings of EMNLP}, 2020.

\bibitem{wu2022backdoorbench}
Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan,
  Chao Shen, and Hongyuan Zha.
\newblock Backdoorbench: {A} comprehensive benchmark of backdoor learning.
\newblock {\em CoRR}, 2022.

\bibitem{wu2022deep}
Lirong Wu, Lifan Yuan, Guojiang Zhao, Haitao Lin, and Stan~Z. Li.
\newblock Deep clustering and visualization for end-to-end high-dimensional
  data analysis.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  2022.

\bibitem{xu2022exploring}
Lei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao, and Zhiyuan Liu.
\newblock Exploring the universal vulnerability of prompt-based learning
  paradigm.
\newblock In {\em Findings of NAACL}, 2022.

\bibitem{xu2021detecting}
Xiaojun Xu, Qi~Wang, Huichen Li, Nikita Borisov, Carl~A Gunter, and Bo~Li.
\newblock Detecting ai trojans using meta neural analysis.
\newblock In {\em {IEEE} Symposium on Security and Privacy, {SP}}, 2021.

\bibitem{Yang2021ep}
Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu~Sun, and Bin He.
\newblock Be careful about poisoned word embeddings: Exploring the
  vulnerability of the embedding layers in {NLP} models.
\newblock In {\em Proceedings of NAACL-HLT}, 2021.

\bibitem{Yang2021rap}
Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu~Sun.
\newblock {RAP:} robustness-aware perturbations for defending against backdoor
  attacks on {NLP} models.
\newblock In {\em Proceedings of EMNLP}, 2021.

\bibitem{yang2021rethinking}
Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu~Sun.
\newblock Rethinking stealthiness of backdoor attack against {NLP} models.
\newblock In {\em Proceedings of ACL-IJCNLP}, 2021.

\bibitem{zampieri-etal-2019-semeval}
Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra,
  and Ritesh Kumar.
\newblock {S}em{E}val-2019 task 6: Identifying and categorizing offensive
  language in social media ({O}ffens{E}val).
\newblock In {\em Proceedings of the 13th International Workshop on Semantic
  Evaluation}, 2019.

\bibitem{Zang2020word}
Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, and
  Maosong Sun.
\newblock Word-level textual adversarial attacking as combinatorial
  optimization.
\newblock In {\em Proceedings of ACL}, 2020.

\bibitem{zhang2015agnews}
Xiang Zhang, Junbo Zhao, and Yann LeCun.
\newblock Character-level convolutional networks for text classification.
\newblock In {\em Proceedings of NeurIPS}, 2015.

\bibitem{zhang2021trojaning}
Xinyang Zhang, Zheng Zhang, Shouling Ji, and Ting Wang.
\newblock Trojaning language models for fun and profit.
\newblock In {\em {IEEE} European Symposium on Security and Privacy,
  EuroS{\&}P}, 2021.

\bibitem{zhang2022moefication}
Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou.
\newblock {M}o{E}fication: Transformer feed-forward layers are mixtures of
  experts.
\newblock In {\em Findings of ACL 2022}, 2022.

\bibitem{zhang2021red}
Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi, Zhiyuan Liu,
  Yasheng Wang, Xin Jiang, and Maosong Sun.
\newblock Red alarm for pre-trained models: Universal vulnerabilities by
  neuron-level backdoor attacks.
\newblock {\em CoRR}, 2021.

\end{thebibliography}
