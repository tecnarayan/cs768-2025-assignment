\begin{thebibliography}{98}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Advani et~al.(2020)Advani, Saxe, and Sompolinsky]{ADVANI2020428}
M.~S. Advani, A.~M. Saxe, and H.~Sompolinsky.
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock \emph{Neural Networks}, 132:\penalty0 428--446, 2020.
\newblock ISSN 0893-6080.
\newblock \doi{https://doi.org/10.1016/j.neunet.2020.08.022}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0893608020303117}.

\bibitem[Ahn et~al.(2022)Ahn, Zhang, and Sra]{sra1}
K.~Ahn, J.~Zhang, and S.~Sra.
\newblock Understanding the unstable convergence of gradient descent, 2022.
\newblock URL \url{https://arxiv.org/abs/2204.01050}.

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Liang]{allen-zhu}
Z.~Allen-Zhu, Y.~Li, and Y.~Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/62dad6e273d32235ae02b7d321578ee8-Paper.pdf}.

\bibitem[Arbabi and Mezic(2017)]{arbabi2017ergodic}
H.~Arbabi and I.~Mezic.
\newblock Ergodic theory, dynamic mode decomposition, and computation of
  spectral properties of the koopman operator.
\newblock \emph{SIAM Journal on Applied Dynamical Systems}, 16\penalty0
  (4):\penalty0 2096--2126, 2017.

\bibitem[Aron and Schwartz(1984)]{periodicDoubling2}
J.~L. Aron and I.~B. Schwartz.
\newblock Seasonality and period-doubling bifurcations in an epidemic model.
\newblock \emph{Journal of theoretical biology}, 110\penalty0 (4):\penalty0
  665--679, 1984.

\bibitem[Arora et~al.(2018)Arora, Ge, Neyshabur, and Zhang]{arora2018stronger}
S.~Arora, R.~Ge, B.~Neyshabur, and Y.~Zhang.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock In \emph{International Conference on Machine Learning}, pages
  254--263. PMLR, 2018.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, and
  Wang]{arora2019fine}
S.~Arora, S.~Du, W.~Hu, Z.~Li, and R.~Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  322--332. PMLR, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{aroraNTK}
S.~Arora, S.~S. Du, W.~Hu, Z.~Li, R.~R. Salakhutdinov, and R.~Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc.,
  2019{\natexlab{b}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/dbc4d84bfcfe2284ba11beffb853a8c4-Paper.pdf}.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett2017spectrally}
P.~L. Bartlett, D.~J. Foster, and M.~Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 6241--6250, 2017.

\bibitem[Bartlett et~al.(2019)Bartlett, Harvey, Liaw, and
  Mehrabian]{JMLR:v20:17-612}
P.~L. Bartlett, N.~Harvey, C.~Liaw, and A.~Mehrabian.
\newblock Nearly-tight vc-dimension and pseudodimension bounds for piecewise
  linear neural networks.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (63):\penalty0 1--17, 2019.
\newblock URL \url{http://jmlr.org/papers/v20/17-612.html}.

\bibitem[Bartlett et~al.(2021)Bartlett, Montanari, and
  Rakhlin]{bartlett2021deep}
P.~L. Bartlett, A.~Montanari, and A.~Rakhlin.
\newblock Deep learning: a statistical viewpoint.
\newblock \emph{Acta numerica}, 30:\penalty0 87--201, 2021.

\bibitem[Bousquet and Elisseeff(2002)]{bousquet2002stability}
O.~Bousquet and A.~Elisseeff.
\newblock Stability and generalization.
\newblock \emph{The Journal of Machine Learning Research}, 2:\penalty0
  499--526, 2002.

\bibitem[Bousquet et~al.(2020)Bousquet, Klochkov, and Zhivotovskiy]{algstab}
O.~Bousquet, Y.~Klochkov, and N.~Zhivotovskiy.
\newblock Sharper bounds for uniformly stable algorithms.
\newblock In J.~Abernethy and S.~Agarwal, editors, \emph{Proceedings of Thirty
  Third Conference on Learning Theory}, volume 125 of \emph{Proceedings of
  Machine Learning Research}, pages 610--626. PMLR, 09--12 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v125/bousquet20b.html}.

\bibitem[Budišić et~al.(2012)Budišić, Mohr, and Mezić]{mezic}
M.~Budišić, R.~Mohr, and I.~Mezić.
\newblock Applied koopmanism.
\newblock \emph{Chaos: An Interdisciplinary Journal of Nonlinear Science},
  22\penalty0 (4):\penalty0 047510, 2012.
\newblock \doi{10.1063/1.4772195}.

\bibitem[Chee and Toulis(2018)]{phaseTransition}
J.~Chee and P.~Toulis.
\newblock Convergence diagnostics for stochastic gradient descent with constant
  learning rate.
\newblock In A.~Storkey and F.~Perez-Cruz, editors, \emph{Proceedings of the
  Twenty-First International Conference on Artificial Intelligence and
  Statistics}, volume~84 of \emph{Proceedings of Machine Learning Research},
  pages 1476--1485. PMLR, 09--11 Apr 2018.

\bibitem[Chekroun et~al.(2014)Chekroun, Neelin, Kondrashov, McWilliams, and
  Ghil]{chekroun2014}
M.~D. Chekroun, J.~D. Neelin, D.~Kondrashov, J.~C. McWilliams, and M.~Ghil.
\newblock Rough parameter dependence in climate models and the role of
  ruelle-pollicott resonances.
\newblock \emph{Proceedings of the National Academy of Sciences}, 111\penalty0
  (5):\penalty0 1684--1690, 2014.

\bibitem[Chen et~al.(2020)Chen, Cao, Gu, and Zhang]{chen2020mean}
Z.~Chen, Y.~Cao, Q.~Gu, and T.~Zhang.
\newblock Mean-field analysis of two-layer neural networks: Non-asymptotic
  rates and generalization bounds.
\newblock \emph{arXiv preprint arXiv:2002.04026}, 2020.

\bibitem[Cohen et~al.(2021)Cohen, Kaur, Li, Kolter, and Talwalkar]{cohen}
J.~M. Cohen, S.~Kaur, Y.~Li, J.~Z. Kolter, and A.~Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of
  stability, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.00065}.

\bibitem[Crimmins and Froyland(2020)]{froyland}
H.~Crimmins and G.~Froyland.
\newblock Fourier approximation of the statistical properties of anosov maps on
  tori.
\newblock \emph{Nonlinearity}, 33\penalty0 (11):\penalty0 6244, 2020.

\bibitem[Dellnitz et~al.(2000)Dellnitz, Froyland, and
  Sertl]{dellnitz2000isolated}
M.~Dellnitz, G.~Froyland, and S.~Sertl.
\newblock On the isolated spectrum of the perron-frobenius operator.
\newblock \emph{Nonlinearity}, 13\penalty0 (4):\penalty0 1171, 2000.

\bibitem[Dieuleveut et~al.(2020)Dieuleveut, Durmus, and Bach]{bach}
A.~Dieuleveut, A.~Durmus, and F.~Bach.
\newblock {Bridging the gap between constant step size stochastic gradient
  descent and Markov chains}.
\newblock \emph{The Annals of Statistics}, 48\penalty0 (3):\penalty0 1348 --
  1382, 2020.
\newblock \doi{10.1214/19-AOS1850}.
\newblock URL \url{https://doi.org/10.1214/19-AOS1850}.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh2017sharp}
L.~Dinh, R.~Pascanu, S.~Bengio, and Y.~Bengio.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{International Conference on Machine Learning}, pages
  1019--1028. PMLR, 2017.

\bibitem[Dogra and Redman(2020)]{dogra2020optimizing}
A.~S. Dogra and W.~Redman.
\newblock Optimizing neural networks via koopman operator theory.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 2087--2097, 2020.

\bibitem[Dziugaite and Roy(2017)]{dziugaite2017computing}
G.~K. Dziugaite and D.~M. Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{arXiv preprint arXiv:1703.11008}, 2017.

\bibitem[Feldman and Vondrak(2018)]{feldmanVondrak}
V.~Feldman and J.~Vondrak.
\newblock Generalization bounds for uniformly stable algorithms.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/05a624166c8eb8273b8464e8d9cb5bd9-Paper.pdf}.

\bibitem[Fort and Pagès(1999)]{SGDconvergence1}
J.-C. Fort and G.~Pagès.
\newblock Asymptotic behavior of a markovian stochastic algorithm with constant
  step.
\newblock \emph{SIAM Journal on Control and Optimization}, 37\penalty0
  (5):\penalty0 1456--1482, 1999.

\bibitem[Freeman and Bruna(2016)]{freeman2016topology}
C.~D. Freeman and J.~Bruna.
\newblock Topology and geometry of half-rectified network optimization.
\newblock \emph{arXiv preprint arXiv:1611.01540}, 2016.

\bibitem[Frei et~al.(2019)Frei, Cao, and Gu]{frei}
S.~Frei, Y.~Cao, and Q.~Gu.
\newblock Algorithm-dependent generalization bounds for overparameterized deep
  residual networks.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/6e2290dbf1e11f39d246e7ce5ac50a1e-Paper.pdf}.

\bibitem[Gabri{\'e} et~al.(2018)Gabri{\'e}, Manoel, Luneau, Macris, Krzakala,
  Zdeborov{\'a}, et~al.]{gabrie2018entropy}
M.~Gabri{\'e}, A.~Manoel, C.~Luneau, N.~Macris, F.~Krzakala, L.~Zdeborov{\'a},
  et~al.
\newblock Entropy and mutual information in models of deep neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{baysian1}
T.~Garipov, P.~Izmailov, D.~Podoprikhin, D.~P. Vetrov, and A.~G. Wilson.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/be3087e74e9100d4bc4c6268cdbe8456-Paper.pdf}.

\bibitem[Ge et~al.(2015)Ge, Huang, Jin, and Yuan]{ge2015escaping}
R.~Ge, F.~Huang, C.~Jin, and Y.~Yuan.
\newblock Escaping from saddle points—online stochastic gradient for tensor
  decomposition.
\newblock In \emph{Conference on learning theory}, pages 797--842. PMLR, 2015.

\bibitem[Golowich et~al.(2018)Golowich, Rakhlin, and Shamir]{golowich2018size}
N.~Golowich, A.~Rakhlin, and O.~Shamir.
\newblock Size-independent sample complexity of neural networks.
\newblock In \emph{Conference On Learning Theory}, pages 297--299. PMLR, 2018.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and Srebro]{gunasekar}
S.~Gunasekar, J.~D. Lee, D.~Soudry, and N.~Srebro.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/0e98aeeb54acf612b9eb4e48a269814c-Paper.pdf}.

\bibitem[Haeffele and Vidal(2015)]{matrixFact}
B.~D. Haeffele and R.~Vidal.
\newblock Global optimality in tensor factorization, deep learning, and beyond,
  2015.
\newblock URL \url{https://arxiv.org/abs/1506.07540}.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardtRecht}
M.~Hardt, B.~Recht, and Y.~Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In M.~F. Balcan and K.~Q. Weinberger, editors, \emph{Proceedings of
  The 33rd International Conference on Machine Learning}, volume~48 of
  \emph{Proceedings of Machine Learning Research}, pages 1225--1234, New York,
  New York, USA, 20--22 Jun 2016. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v48/hardt16.html}.

\bibitem[He et~al.(2019)He, Huang, and Yuan]{he2019asymmetric}
H.~He, G.~Huang, and Y.~Yuan.
\newblock Asymmetric valleys: Beyond sharp and flat local minima.
\newblock \emph{arXiv preprint arXiv:1902.00744}, 2019.

\bibitem[Hochreiter and Schmidhuber(1997)]{10.1162/neco.1997.9.1.1}
S.~Hochreiter and J.~Schmidhuber.
\newblock {Flat Minima}.
\newblock \emph{Neural Computation}, 9\penalty0 (1):\penalty0 1--42, 01 1997.
\newblock ISSN 0899-7667.
\newblock \doi{10.1162/neco.1997.9.1.1}.
\newblock URL \url{https://doi.org/10.1162/neco.1997.9.1.1}.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry]{hoffer2017train}
E.~Hoffer, I.~Hubara, and D.~Soudry.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock In \emph{NIPS}, 2017.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot}
A.~Jacot, F.~Gabriel, and C.~Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf}.

\bibitem[Jastrz{\k{e}}bski et~al.(2017)Jastrz{\k{e}}bski, Kenton, Arpit,
  Ballas, Fischer, Bengio, and Storkey]{jastrzkebski2017three}
S.~Jastrz{\k{e}}bski, Z.~Kenton, D.~Arpit, N.~Ballas, A.~Fischer, Y.~Bengio,
  and A.~Storkey.
\newblock Three factors influencing minima in sgd.
\newblock \emph{arXiv preprint arXiv:1711.04623}, 2017.

\bibitem[Kato(2013)]{kato2013perturbation}
T.~Kato.
\newblock \emph{Perturbation theory for linear operators}, volume 132.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Katok and Hasselblatt(1997)]{katok}
A.~Katok and B.~Hasselblatt.
\newblock \emph{Introduction to the modern theory of dynamical systems}.
\newblock Number~54. Cambridge university press, 1997.

\bibitem[Keller and Liverani(1999)]{keller1999stability}
G.~Keller and C.~Liverani.
\newblock Stability of the spectrum for transfer operators.
\newblock \emph{Annali della Scuola Normale Superiore di Pisa-Classe di
  Scienze}, 28\penalty0 (1):\penalty0 141--152, 1999.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
N.~S. Keskar, D.~Mudigere, J.~Nocedal, M.~Smelyanskiy, and P.~T.~P. Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kleinberg et~al.(2018)Kleinberg, Li, and Yuan]{pmlr-v80-kleinberg18a}
B.~Kleinberg, Y.~Li, and Y.~Yuan.
\newblock An alternative view: When does {SGD} escape local minima?
\newblock In J.~Dy and A.~Krause, editors, \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pages 2698--2707. PMLR, 10--15 Jul 2018.
\newblock URL \url{https://proceedings.mlr.press/v80/kleinberg18a.html}.

\bibitem[Kondrashov et~al.(2015)Kondrashov, Chekroun, and
  Ghil]{KONDRASHOV201533}
D.~Kondrashov, M.~D. Chekroun, and M.~Ghil.
\newblock Data-driven non-markovian closure models.
\newblock \emph{Physica D: Nonlinear Phenomena}, 297:\penalty0 33--55, 2015.
\newblock ISSN 0167-2789.
\newblock \doi{https://doi.org/10.1016/j.physd.2014.12.005}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0167278914002413}.

\bibitem[Kong and Tao(2020)]{kongTao}
L.~Kong and M.~Tao.
\newblock Stochasticity of deterministic gradient descent: Large learning rate
  for multiscale objective function.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 2625--2638. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/1b9a80606d74d3da6db2f1274557e644-Paper.pdf}.

\bibitem[Koopman(1931)]{koopman}
B.~O. Koopman.
\newblock Hamiltonian systems and transformation in hilbert space.
\newblock \emph{Proceedings of the national academy of sciences of the united
  states of america}, 17\penalty0 (5):\penalty0 315, 1931.

\bibitem[Korda and Mezi{\'c}(2018)]{korda2018convergence}
M.~Korda and I.~Mezi{\'c}.
\newblock On convergence of extended dynamic mode decomposition to the koopman
  operator.
\newblock \emph{Journal of Nonlinear Science}, 28\penalty0 (2):\penalty0
  687--710, 2018.

\bibitem[Kuzborskij and Lampert(2018)]{kuzborskij2018data}
I.~Kuzborskij and C.~Lampert.
\newblock Data-dependent stability of stochastic gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages
  2815--2824. PMLR, 2018.

\bibitem[Lasota and Mackey(1998)]{lasota1998chaos}
A.~Lasota and M.~C. Mackey.
\newblock \emph{Chaos, fractals, and noise: stochastic aspects of dynamics},
  volume~97.
\newblock Springer Science \& Business Media, 1998.
\newblock \doi{10.1007/978-1-4612-4286-4}.

\bibitem[Li and Yuan(2017)]{li2017convergence}
Y.~Li and Y.~Yuan.
\newblock Convergence analysis of two-layer neural networks with relu
  activation.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Lin and Lu(2021)]{LIN2021109864}
K.~K. Lin and F.~Lu.
\newblock Data-driven model reduction, wiener projections, and the
  koopman-mori-zwanzig formalism.
\newblock \emph{Journal of Computational Physics}, 424:\penalty0 109864, 2021.
\newblock ISSN 0021-9991.
\newblock \doi{https://doi.org/10.1016/j.jcp.2020.109864}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0021999120306380}.

\bibitem[Liu et~al.(2022)Liu, Zhu, and Belkin]{liu2022loss}
C.~Liu, L.~Zhu, and M.~Belkin.
\newblock Loss landscapes and optimization in over-parameterized non-linear
  systems and neural networks.
\newblock \emph{Applied and Computational Harmonic Analysis}, 2022.

\bibitem[Liverani(2004)]{liverani2004invariant}
C.~Liverani.
\newblock Invariant measures and their properties. a functional analytic point
  of view.
\newblock \emph{Dynamical systems. Part II}, pages 185--237, 2004.

\bibitem[Lobacheva et~al.(2021)Lobacheva, Kodryan, Chirkova, Malinin, and
  Vetrov]{lobacheva2021periodic}
E.~Lobacheva, M.~Kodryan, N.~Chirkova, A.~Malinin, and D.~P. Vetrov.
\newblock On the periodic behavior of neural network training with batch
  normalization and weight decay.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Loukas et~al.(2021)Loukas, Poiitis, and Jegelka]{loukas2021what}
A.~Loukas, M.~Poiitis, and S.~Jegelka.
\newblock What training reveals about neural network complexity.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W. Vaughan, editors,
  \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=RcjW7p7z8aJ}.

\bibitem[Ma et~al.(2018)Ma, Bassily, and Belkin]{belkin1}
S.~Ma, R.~Bassily, and M.~Belkin.
\newblock The power of interpolation: Understanding the effectiveness of {SGD}
  in modern over-parametrized learning.
\newblock In J.~Dy and A.~Krause, editors, \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pages 3325--3334. PMLR, 10--15 Jul 2018.
\newblock URL \url{https://proceedings.mlr.press/v80/ma18a.html}.

\bibitem[Mattingly et~al.(2002)Mattingly, Stuart, and Higham]{MATTINGLY2002185}
J.~Mattingly, A.~Stuart, and D.~Higham.
\newblock Ergodicity for sdes and approximations: locally lipschitz vector
  fields and degenerate noise.
\newblock \emph{Stochastic Processes and their Applications}, 101\penalty0
  (2):\penalty0 185--232, 2002.
\newblock ISSN 0304-4149.
\newblock \doi{https://doi.org/10.1016/S0304-4149(02)00150-3}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0304414902001503}.

\bibitem[McAllester(1999)]{mcallester1999some}
D.~A. McAllester.
\newblock Some pac-bayesian theorems.
\newblock \emph{Machine Learning}, 37\penalty0 (3):\penalty0 355--363, 1999.

\bibitem[Mei et~al.(2019)Mei, Misiakiewicz, and Montanari]{mei2019mean}
S.~Mei, T.~Misiakiewicz, and A.~Montanari.
\newblock Mean-field theory of two-layers neural networks: dimension-free
  bounds and kernel limit.
\newblock In \emph{Conference on Learning Theory}, pages 2388--2464. PMLR,
  2019.

\bibitem[Mingard et~al.(2021)Mingard, Valle-P{\'e}rez, Skalse, and
  Louis]{mingard2021sgd}
C.~Mingard, G.~Valle-P{\'e}rez, J.~Skalse, and A.~A. Louis.
\newblock Is sgd a bayesian sampler? well, almost.
\newblock \emph{Journal of Machine Learning Research}, 22, 2021.

\bibitem[Mohri et~al.(2018)Mohri, Rostamizadeh, and Talwalkar]{mohri}
M.~Mohri, A.~Rostamizadeh, and A.~Talwalkar.
\newblock \emph{Foundations of machine learning}.
\newblock MIT press, 2018.

\bibitem[Montanari and Zhong(2020)]{montanari2020interpolation}
A.~Montanari and Y.~Zhong.
\newblock The interpolation phase transition in neural networks: Memorization
  and generalization under lazy training.
\newblock \emph{arXiv preprint arXiv:2007.12826}, 2020.

\bibitem[Moulines and Bach(2011)]{moulines}
E.~Moulines and F.~Bach.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock In J.~Shawe-Taylor, R.~Zemel, P.~Bartlett, F.~Pereira, and
  K.~Weinberger, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~24. Curran Associates, Inc., 2011.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2011/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf}.

\bibitem[Needell et~al.(2014)Needell, Ward, and Srebro]{ward}
D.~Needell, R.~Ward, and N.~Srebro.
\newblock Stochastic gradient descent, weighted sampling, and the randomized
  kaczmarz algorithm.
\newblock In Z.~Ghahramani, M.~Welling, C.~Cortes, N.~Lawrence, and
  K.~Weinberger, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~27. Curran Associates, Inc., 2014.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2014/file/f29c21d4897f78948b91f03172341b7b-Paper.pdf}.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Y.~Nesterov.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Bhojanapalli, and
  Srebro]{neyshabur2018pac}
B.~Neyshabur, S.~Bhojanapalli, and N.~Srebro.
\newblock A pac-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Nguyen et~al.(2018)Nguyen, Mukkamala, and Hein]{nguyen2018loss}
Q.~Nguyen, M.~C. Mukkamala, and M.~Hein.
\newblock On the loss landscape of a class of deep neural networks with no bad
  local valleys.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Pitas(2020)]{pitas2020dissecting}
K.~Pitas.
\newblock Dissecting non-vacuous generalization bounds based on the mean-field
  approximation.
\newblock In \emph{International Conference on Machine Learning}, pages
  7739--7749. PMLR, 2020.

\bibitem[Quail et~al.(2015)Quail, Shrier, and Glass]{periodiDoubling1}
T.~Quail, A.~Shrier, and L.~Glass.
\newblock Predicting the onset of period-doubling bifurcations in noisy cardiac
  systems.
\newblock \emph{Proceedings of the National Academy of Sciences}, 112\penalty0
  (30):\penalty0 9358--9363, 2015.
\newblock \doi{10.1073/pnas.1424320112}.
\newblock URL \url{https://www.pnas.org/doi/abs/10.1073/pnas.1424320112}.

\bibitem[Raginsky et~al.(2017)Raginsky, Rakhlin, and
  Telgarsky]{raginsky2017non}
M.~Raginsky, A.~Rakhlin, and M.~Telgarsky.
\newblock Non-convex learning via stochastic gradient langevin dynamics: a
  nonasymptotic analysis.
\newblock In \emph{Conference on Learning Theory}, pages 1674--1703. PMLR,
  2017.

\bibitem[Rakhlin(2006)]{rakhlin2006applications}
A.~Rakhlin.
\newblock \emph{Applications of empirical processes in learning theory:
  algorithmic stability and generalization bounds}.
\newblock PhD thesis, Massachusetts Institute of Technology, 2006.

\bibitem[Rudolf and Schweizer(2018)]{wasserstein}
D.~Rudolf and N.~Schweizer.
\newblock {Perturbation theory for Markov chains via Wasserstein distance}.
\newblock \emph{Bernoulli}, 24\penalty0 (4A):\penalty0 2610 -- 2639, 2018.
\newblock \doi{10.3150/17-BEJ938}.
\newblock URL \url{https://doi.org/10.3150/17-BEJ938}.

\bibitem[Safran and Shamir(2016)]{safran2016quality}
I.~Safran and O.~Shamir.
\newblock On the quality of the initial basin in overspecified neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  774--782. PMLR, 2016.

\bibitem[Sagun et~al.(2016)Sagun, Bottou, and LeCun]{sagun2016eigenvalues}
L.~Sagun, L.~Bottou, and Y.~LeCun.
\newblock Eigenvalues of the hessian in deep learning: Singularity and beyond.
\newblock \emph{arXiv preprint arXiv:1611.07476}, 2016.

\bibitem[Sagun et~al.(2017)Sagun, Evci, Guney, Dauphin, and
  Bottou]{sagun2017empirical}
L.~Sagun, U.~Evci, V.~U. Guney, Y.~Dauphin, and L.~Bottou.
\newblock Empirical analysis of the hessian of over-parametrized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1706.04454}, 2017.

\bibitem[Saxe et~al.(2013)Saxe, McClelland, and Ganguli]{saxe}
A.~M. Saxe, J.~L. McClelland, and S.~Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks, 2013.
\newblock URL \url{https://arxiv.org/abs/1312.6120}.

\bibitem[Sirignano and Spiliopoulos(2022)]{sirignano2022mean}
J.~Sirignano and K.~Spiliopoulos.
\newblock Mean field analysis of deep neural networks.
\newblock \emph{Mathematics of Operations Research}, 47\penalty0 (1):\penalty0
  120--152, 2022.

\bibitem[Smith and Le(2018)]{smith2018bayesian}
S.~L. Smith and Q.~V. Le.
\newblock A bayesian perspective on generalization and stochastic gradient
  descent.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Sokoli{\'c} et~al.(2017)Sokoli{\'c}, Giryes, Sapiro, and
  Rodrigues]{sokolic2017robust}
J.~Sokoli{\'c}, R.~Giryes, G.~Sapiro, and M.~R. Rodrigues.
\newblock Robust large margin deep neural networks.
\newblock \emph{IEEE Transactions on Signal Processing}, 65\penalty0
  (16):\penalty0 4265--4280, 2017.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
D.~Soudry, E.~Hoffer, M.~S. Nacson, S.~Gunasekar, and N.~Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Van~der Maaten and Hinton(2008)]{tsne}
L.~Van~der Maaten and G.~Hinton.
\newblock Visualizing data using t-sne.
\newblock \emph{Journal of machine learning research}, 9\penalty0 (11), 2008.

\bibitem[Vapnik(1999)]{vapnik1999overview}
V.~N. Vapnik.
\newblock An overview of statistical learning theory.
\newblock \emph{IEEE transactions on neural networks}, 10\penalty0
  (5):\penalty0 988--999, 1999.

\bibitem[Venturi et~al.(2019)Venturi, Bandeira, and Bruna]{venturi2019spurious}
L.~Venturi, A.~S. Bandeira, and J.~Bruna.
\newblock Spurious valleys in one-hidden-layer neural network optimization
  landscapes.
\newblock \emph{Journal of Machine Learning Research}, 20:\penalty0 133, 2019.

\bibitem[von Luxburg and Bousquet(2004)]{von2004distance}
U.~von Luxburg and O.~Bousquet.
\newblock Distance-based classification with lipschitz functions.
\newblock \emph{J. Mach. Learn. Res.}, 5:\penalty0 669--695, 2004.

\bibitem[Wang et~al.(2021)Wang, Chen, Zhao, and Tao]{WangTao}
Y.~Wang, M.~Chen, T.~Zhao, and M.~Tao.
\newblock Large learning rate tames homogeneity: Convergence and balancing
  effect.
\newblock 2021.
\newblock \doi{10.48550/ARXIV.2110.03677}.
\newblock URL \url{https://arxiv.org/abs/2110.03677}.

\bibitem[Wilkinson(2017)]{wilkinson2017lyapunov}
A.~Wilkinson.
\newblock What are lyapunov exponents, and why are they interesting?
\newblock \emph{Bulletin of the American Mathematical Society}, 54\penalty0
  (1):\penalty0 79--105, 2017.

\bibitem[Williams et~al.(2014)Williams, Rowley, and
  Kevrekidis]{williams2014kernel}
M.~O. Williams, C.~W. Rowley, and I.~G. Kevrekidis.
\newblock A kernel-based approach to data-driven koopman spectral analysis.
\newblock \emph{arXiv preprint arXiv:1411.2260}, 2014.

\bibitem[Wojtowytsch(2021)]{wojtowytsch}
S.~Wojtowytsch.
\newblock Stochastic gradient descent with noise of machine learning type. part
  i: Discrete time analysis, 2021.
\newblock URL \url{https://arxiv.org/abs/2105.01650}.

\bibitem[Xu and Mannor(2012)]{xu2012robustness}
H.~Xu and S.~Mannor.
\newblock Robustness and generalization.
\newblock \emph{Machine learning}, 86\penalty0 (3):\penalty0 391--423, 2012.

\bibitem[Zhang et~al.(2018)Zhang, Liao, Rakhlin, Miranda, Golowich, and
  Poggio]{zhang2018theory}
C.~Zhang, Q.~Liao, A.~Rakhlin, B.~Miranda, N.~Golowich, and T.~Poggio.
\newblock Theory of deep learning iib: Optimization properties of sgd.
\newblock \emph{arXiv preprint arXiv:1801.02254}, 2018.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2021understanding}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock \emph{Communications of the ACM}, 64\penalty0 (3):\penalty0 107--115,
  2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Li, Sra, and Jadbabaie]{suvrit}
J.~Zhang, H.~Li, S.~Sra, and A.~Jadbabaie.
\newblock On convergence of training loss without reaching stationary points,
  2021{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2110.06256}.

\bibitem[Zhang et~al.(2021{\natexlab{c}})Zhang, Zhang, Bald, Pingali, Chen, and
  Goswami]{zhang2021stability}
Y.~Zhang, W.~Zhang, S.~Bald, V.~Pingali, C.~Chen, and M.~Goswami.
\newblock Stability of sgd: Tightness analysis and improved bounds.
\newblock \emph{arXiv preprint arXiv:2102.05274}, 2021{\natexlab{c}}.

\bibitem[Zhao et~al.(2004)Zhao, Tang, Lin, and Zhao]{zhao2004observation}
L.~Zhao, D.~Tang, F.~Lin, and B.~Zhao.
\newblock Observation of period-doubling bifurcations in a femtosecond fiber
  soliton laser with dispersion management cavity.
\newblock \emph{Optics express}, 12\penalty0 (19):\penalty0 4573--4578, 2004.

\bibitem[Zhou et~al.(2019)Zhou, Veitch, Austern, Adams, and
  Orbanz]{zhou2018nonvacuous}
W.~Zhou, V.~Veitch, M.~Austern, R.~P. Adams, and P.~Orbanz.
\newblock Non-vacuous generalization bounds at the imagenet scale: a
  {PAC}-bayesian compression approach.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=BJgqqsAct7}.

\bibitem[Zwanzig(2001)]{zwanzig2001nonequilibrium}
R.~Zwanzig.
\newblock \emph{Nonequilibrium statistical mechanics}.
\newblock Oxford university press, 2001.

\end{thebibliography}
