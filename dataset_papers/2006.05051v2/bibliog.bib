@inproceedings{LykourisMiPa18,
title = {Stochastic bandits robust to adversarial corruptions},
author = {Lykouris, Thodoris and Mirrokni, Vahab and Paes Leme, Renato},
booktitle={Proceedings of the 50th ACM Annual Symposium on Theory of Computing (STOC)},
  year={2018}
}

@article{agarwal2019optimality,
  title={Optimality and approximation with policy gradient methods in markov decision processes},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={arXiv preprint arXiv:1908.00261},
  year={2019}
}

@article{even2009online,
  title={Online Markov decision processes},
  author={Even-Dar, Eyal and Kakade, Sham M and Mansour, Yishay},
  journal={Mathematics of Operations Research},
  volume={34},
  number={3},
  pages={726--736},
  year={2009},
  publisher={INFORMS}
}

@incollection{NIPS2013_4975,
title = {Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions},
author = {Abbasi, Yasin and Bartlett, Peter L and Kanade, Varun and Seldin, Yevgeny and Szepesvari, Csaba},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {2508--2516},
year = {2013},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4975-online-learning-in-markov-decision-processes-with-adversarially-chosen-transition-probability-distributions.pdf}
}
@inproceedings{jiang2017contextual,
  title={Contextual decision processes with low Bellman rank are PAC-learnable},
  author={Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1704--1713},
  year={2017},
  organization={JMLR. org}
}


@inproceedings{kalyanakrishnan2012pac,
  title={PAC Subset Selection in Stochastic Multi-armed Bandits.},
  author={Kalyanakrishnan, Shivaram and Tewari, Ambuj and Auer, Peter and Stone, Peter},
  year={2012}
}

@article{foster2018practical,
  title={Practical contextual bandits with regression oracles},
  author={Foster, Dylan J and Agarwal, Alekh and Dud{\'\i}k, Miroslav and Luo, Haipeng and Schapire, Robert E},
  journal={arXiv preprint arXiv:1803.01088},
  year={2018}
}

@article{gupta2019better,
  title={Better Algorithms for Stochastic Bandits with Adversarial Corruptions},
  author={Gupta, Anupam and Koren, Tomer and Talwar, Kunal},
  journal={arXiv preprint arXiv:1902.08647},
  year={2019}
}

@article{dudik2011efficient,
  title={Efficient optimal learning for contextual bandits},
  author={Dudik, Miroslav and Hsu, Daniel and Kale, Satyen and Karampatziakis, Nikos and Langford, John and Reyzin, Lev and Zhang, Tong},
  journal={arXiv preprint arXiv:1106.2369},
  year={2011}
}

@inproceedings{jin2018q,
  title={Is q-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4863--4873},
  year={2018}
}

@article{jin2019provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  journal={arXiv preprint arXiv:1907.05388},
  year={2019}
}

@inproceedings{neu2010online,
  title={Online Markov decision processes under bandit feedback},
  author={Neu, Gergely and Antos, Andras and Gy{\"o}rgy, Andr{\'a}s and Szepesv{\'a}ri, Csaba},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1804--1812},
  year={2010}
}

@inproceedings{rosenberg2019online,
  title={Online Convex Optimization in Adversarial Markov Decision Processes},
  author={Rosenberg, Aviv and Mansour, Yishay},
  booktitle={International Conference on Machine Learning},
  pages={5478--5486},
  year={2019}
}

@inproceedings{dann2015sample,
  title={Sample complexity of episodic fixed-horizon reinforcement learning},
  author={Dann, Christoph and Brunskill, Emma},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2818--2826},
  year={2015}
}

@inproceedings{karnin2013almost,
  title={Almost optimal exploration in multi-armed bandits},
  author={Karnin, Zohar and Koren, Tomer and Somekh, Oren},
  booktitle={International Conference on Machine Learning},
  pages={1238--1246},
  year={2013}
}

@inproceedings{jamieson2016non,
  title={Non-stochastic best arm identification and hyperparameter optimization},
  author={Jamieson, Kevin and Talwalkar, Ameet},
  booktitle={Artificial Intelligence and Statistics},
  pages={240--248},
  year={2016}
}

@article{jaksch2010near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Apr},
  pages={1563--1600},
  year={2010}
}

@article{auer2002finite,
  title={Finite-time regret bounds for the multi-armed bandit problems},
  author={Auer, Peter and Cesa-Bianchi, Nicol\`o and Fisher, Paul},
  journal={Machine Learning},
  volume={47},
  pages={2--3},
  year={2002}
}







@inproceedings{dann2017unifying,
  title={Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning},
  author={Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5713--5723},
  year={2017}
}

@InProceedings{pmlr-v99-gupta19a,
  title =    {Better Algorithms for Stochastic Bandits with Adversarial Corruptions},
  author =   {Gupta, Anupam and Koren, Tomer and Talwar, Kunal},
  booktitle =    {Proceedings of the Thirty-Second Conference on Learning Theory},
  pages =    {1562--1578},
  year =   {2019},
  editor =   {Beygelzimer, Alina and Hsu, Daniel},
  volume =   {99},
  series =   {Proceedings of Machine Learning Research},
  address =    {Phoenix, USA},
  month =    {25--28 Jun},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v99/gupta19a/gupta19a.pdf},
  url =    {http://proceedings.mlr.press/v99/gupta19a.html},
  abstract =   {We study the stochastic multi-armed bandits problem in the presence of adversarial corruption. We present a new algorithm for this problem whose regret is nearly optimal, substantially improving upon previous work. Our algorithm is agnostic to the level of adversarial contamination and can tolerate a significant amount of corruption with virtually no degradation in performance.}
}

@article{russo2019worst,
  title={Worst-Case Regret Bounds for Exploration via Randomized Value Functions},
  author={Russo, Daniel},
  journal={arXiv preprint arXiv:1906.02870},
  year={2019}
}

@article{dann2018policy,
  title={Policy certificates: Towards accountable reinforcement learning},
  author={Dann, Christoph and Li, Lihong and Wei, Wei and Brunskill, Emma},
  journal={arXiv preprint arXiv:1811.03056},
  year={2018}
}

@inproceedings{AzarOsMu17,
  title = 	 {Minimax Regret Bounds for Reinforcement Learning},
  author = 	 {Mohammad Gheshlaghi Azar and Ian Osband and R{\'e}mi Munos},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning (ICML)},
  year = 	 {2017}
}

@inproceedings{SimchowitzJamieson19,
  title={Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs},
  author={Simchowitz, Max and Jamieson, Kevin},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}

@article{Even-DarManMan06,
  author    = {Eyal Even{-}Dar and        Shie Mannor and       Yishay Mansour},
  title     = {Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems},
  journal   = {Journal of Machine Learning Research (JMLR)},
  volume    = {7},
  pages     = {1079--1105},
  year      = {2006}
}

@article{zanette2019tighter,
  title={Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds},
  author={Zanette, Andrea and Brunskill, Emma},
  journal={arXiv preprint arXiv:1901.00210},
  year={2019}
}

@inproceedings{KrishnamurthyWuSyrgkanis18,
title = "Semiparametric contextual bandits",
author = "Akshay Krishnamurthy and Steven Wu and Vasilis Syrgkanis",
year = "2018",
booktitle = "35th International Conference on Machine Learning (ICML)"
}

@inproceedings{JabbariJosKeaMorRot2017,
 author = {Jabbari, Shahin and Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie and Roth, Aaron},
 title = {Fairness in Reinforcement Learning},
 booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume (ICML)},
 year = {2017}
}
@phdthesis{kakade2003sample,
  title={On the sample complexity of reinforcement learning},
  author={Kakade, Sham Machandranath},
  year={2003}
}

@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John}
}

@article{munos2008finite,
  title={Finite-time bounds for fitted value iteration},
  author={Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={May},
  pages={815--857},
  year={2008}
}

@inproceedings{bagnell2004policy,
  title={Policy search by dynamic programming},
  author={Bagnell, J Andrew and Kakade, Sham M and Schneider, Jeff G and Ng, Andrew Y},
  booktitle={Advances in neural information processing systems},
  pages={831--838},
  year={2004}
}

@article{huang2017adversarial,
  title={Adversarial attacks on neural network policies},
  author={Huang, Sandy and Papernot, Nicolas and Goodfellow, Ian and Duan, Yan and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1702.02284},
  year={2017}
}

@article{ma2019policy,
  title={Policy Poisoning in Batch Reinforcement Learning and Control},
  author={Ma, Yuzhe and Zhang, Xuezhou and Sun, Wen and Zhu, Xiaojin},
  journal={NeurIPS},
  year={2019}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{kober2013reinforcement,
  title={Reinforcement learning in robotics: A survey},
  author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1238--1274},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{ZimmertSeldin19,
  author    = {Julian Zimmert and
               Yevgeny Seldin},
  title     = {An Optimal Algorithm for Stochastic and Adversarial Bandits},
  journal   = {CoRR},
  volume    = {abs/1807.07623},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.07623},
  archivePrefix = {arXiv},
  eprint    = {1807.07623},
  timestamp = {Mon, 13 Aug 2018 16:48:29 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1807-07623},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{LiLouShan2019,
  title={Stochastic Linear Optimization with Adversarial Corruption},
  author={Li, Yingkai and Lou, Edmund Y and Shan, Liren},
  journal={arXiv preprint arXiv:1909.02109},
  year={2019}
}

@article{ChenKrishWang2019,
  title={Robust Dynamic Assortment Optimization in the Presence of Outlier Customers},
  author={Chen, Xi and Krishnamurthy, Akshay and Wang, Yining},
  journal={arXiv preprint arXiv:1910.04183},
  year={2019}
}

@inproceedings{SeldinSlivkins2014,
  title={One Practical Algorithm for Both Stochastic and Adversarial Bandits.},
  author={Seldin, Yevgeny and Slivkins, Aleksandrs},
  booktitle={ICML},
  pages={1287--1295},
  year={2014}
}

@inproceedings{DBLP:conf/icml/ZimmertLW19,
  author    = {Julian Zimmert and
               Haipeng Luo and
               Chen{-}Yu Wei},
  title     = {Beating Stochastic and Adversarial Semi-bandits Optimally and Simultaneously},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  pages     = {7683--7692},
  year      = {2019},
}

@inproceedings{WeiLuo18,
  author    = {Chen{-}Yu Wei and
               Haipeng Luo},
  title     = {More Adaptive Algorithms for Adversarial Bandits},
  booktitle = {Conference On Learning Theory, {COLT} 2018, Stockholm, Sweden, 6-9
               July 2018.},
  pages     = {1263--1291},
  year      = {2018},
  crossref  = {DBLP:conf/colt/2018},
  url       = {http://proceedings.mlr.press/v75/wei18a.html},
  timestamp = {Wed, 03 Apr 2019 18:17:23 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/colt/WeiL18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DiakonikolasKKLMS16,
author={I. {Diakonikolas} and G. {Kamath} and D. M. {Kane} and J. {Li} and A. {Moitra} and A. {Stewart}},
booktitle={2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS)},
title={Robust Estimators in High Dimensions without the Computational Intractability},
year={2016},
volume={},
number={},
pages={655-664},
keywords={estimation theory;Gaussian distribution;learning (artificial intelligence);robust estimators;computational intractability;high-dimensional distribution learning;epsilon fraction;machine learning;lose dimension dependent factors;high-dimensional agnostic distribution learning;product distribution;Robustness;Algorithm design and analysis;Hidden Markov models;Digital TV;Approximation algorithms;Programming;Hypercubes;unsupervised learning;statistical learning;density estimation robust algorithm},
doi={10.1109/FOCS.2016.85},
ISSN={},
month={Oct},}


@inproceedings{CharikarSteinhardtValiant2017,
 author = {Charikar, Moses and Steinhardt, Jacob and Valiant, Gregory},
 title = {Learning from Untrusted Data},
 booktitle = {Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing},
 series = {STOC 2017},
 year = {2017},
 isbn = {978-1-4503-4528-6},
 location = {Montreal, Canada},
 pages = {47--60},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/3055399.3055491},
 doi = {10.1145/3055399.3055491},
 acmid = {3055491},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {high-dimensional statistics, outlier removal, robust learning},
}

@inproceedings{BubeckSli12,
  title = 	 {The Best of Both Worlds: Stochastic and Adversarial Bandits},
  author = 	 {Sébastien Bubeck and Aleksandrs Slivkins},
  booktitle = 	 {Proceedings of the 25th Annual Conference on Learning Theory},
  pages = 	 {42.1--42.23},
  year = 	 {2012},
  editor = 	 {Shie Mannor and Nathan Srebro and Robert C. Williamson},
  volume = 	 {23},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Edinburgh, Scotland},
  month = 	 {25--27 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v23/bubeck12b/bubeck12b.pdf},
  url = 	 {http://proceedings.mlr.press/v23/bubeck12b.html},
  abstract = 	 {We present a new bandit algorithm, SAO (Stochastic and Adversarial Optimal) whose regret is (essentially) optimal both for adversarial rewards and for stochastic rewards. Specifically, SAO combines the \emphO(√\emphn) worst-case regret of Exp3 (Auer et al., 2002b) and the (poly)logarithmic regret of UCB1 (Auer et al., 2002a) for stochastic rewards. Adversarial rewards and stochastic rewards are the two main settings in the literature on multi-armed bandits (MAB). Prior work on MAB treats them separately, and does not attempt to jointly optimize for both. This result falls into the general agenda to design algorithms that combine the optimal worst-case performance with improved guarantees for “nice” problem instances.}
}

@inproceedings{WeiL18,
  author    = {Chen{-}Yu Wei and
               Haipeng Luo},
  title     = {More Adaptive Algorithms for Adversarial Bandits},
  booktitle = {Conference On Learning Theory, {COLT} 2018, Stockholm, Sweden, 6-9
               July 2018.},
  pages     = {1263--1291},
  year      = {2018},
}

@article{Babaioff2015,
 author = {Babaioff, Moshe and Kleinberg, Robert D. and Slivkins, Aleksandrs},
 title = {Truthful Mechanisms with Implicit Payment Computation},
 journal = {J. ACM},
 issue_date = {May 2015},
 volume = {62},
 number = {2},
 month = may,
 year = {2015},
 issn = {0004-5411},
 pages = {10:1--10:37},
 articleno = {10},
 numpages = {37},
 url = {http://doi.acm.org/10.1145/2724705},
 doi = {10.1145/2724705},
 acmid = {2724705},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Algorithmic mechanism design, multi-armed bandits, multiparameter mechanisms, regret, single-parameter mechanisms},
}

@inproceedings{Esfandiari2015OnlineAW,
  title={Online Allocation with Traffic Spikes: Mixing Adversarial and Stochastic Models},
  author={Hossein Esfandiari and Nitish Korula and Vahab S. Mirrokni},
  booktitle={EC},
  year={2015}
}


@inproceedings{AgrawalDevanurEC14,
author = {Agrawal, Shipra and Devanur, Nikhil R.},
title = {Bandits with Concave Rewards and Convex Knapsacks},
year = {2014},
booktitle = {Proceedings of the 15th ACM Conference on Economics and Computatxion (EC)},
}

@inproceedings{Zinkevich03,
author = {Zinkevich, Martin},
title = {Online Convex Programming and Generalized Infinitesimal Gradient Ascent},
year = {2003},
booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning (ICML)}
}


@inproceedings{MiryoosefiBrDaDuSc19,
title = {Reinforcement Learning with Convex Constraints},
author = {Miryoosefi, Sobhan and Brantley, Kiant\'{e} and Daume III, Hal and Dudik, Miro and Schapire, Robert E},
year = {2019},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
}


@inproceedings{Cheung19,
  author    = {Wang Chi Cheung},
  title     = {Regret Minimization for Reinforcement Learning with Vectorial Feedback and Complex Objectives},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2019},
}

@inproceedings{
tessler2018reward,
title={Reward Constrained Policy Optimization},
author={Chen Tessler and Daniel J. Mankowitz and Shie Mannor},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SkfrvsA9FX},
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International Conference on Machine Learning},
  pages={1928--1937},
  year=2016
}

@article{leike2017ai,
  title={AI safety gridworlds},
  author={Leike, Jan and Martic, Miljan and Krakovna, Victoria and Ortega, Pedro A and Everitt, Tom and Lefrancq, Andrew and Orseau, Laurent and Legg, Shane},
  journal={arXiv preprint arXiv:1711.09883},
  year={2017}
}

@article{Kearns2002,
author="Kearns, Michael
and Singh, Satinder",
title="Near-Optimal Reinforcement Learning in Polynomial Time",
journal="Machine Learning",
year="2002",
month="Nov",
day="01",
volume="49",
number="2",
pages="209--232",
issn="1573-0565",
doi="10.1023/A:1017984413808",
url="https://doi.org/10.1023/A:1017984413808"
}

@article{Brafman2002,
author = {Brafman, Ronen I. and Tennenholtz, Moshe},
title = {R-Max - a General Polynomial Time Algorithm for near-Optimal Reinforcement Learning},
year = {2003},
issue_date = {March 2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
url = {https://doi.org/10.1162/153244303765208377},
doi = {10.1162/153244303765208377},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {213–231},
numpages = {19},
keywords = {provably efficient learning, reinforcement learning, Markov decision processes, learning in games, stochastic games}
}

@article{Tesauro1995,
  author = {Tesauro, G.},
  biburl = {https://www.bibsonomy.org/bibtex/214a4806622cac2fd49c92af68bf8a8f4/gromgull},
  interhash = {7c12a6018860af8b5fde3238d75844c5},
  intrahash = {14a4806622cac2fd49c92af68bf8a8f4},
  journal = {Communications of the ACM},
  keywords = {game-playing machine-learning reinforcement-learning},
  number = 3,
  pages = {58-68},
  timestamp = {2009-03-25T13:35:59.000+0100},
  title = {Temporal difference learning and {TD-Gammon}},
  volume = 38,
  year = 1995
}

@article{Silver2016,
  added-at = {2016-03-11T14:36:05.000+0100},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/29e987f58d895c490144693139cbc90c7/ytyoun},
  doi = {10.1038/nature16961},
  interhash = {48430c7891aaf9fe2582faa8f5d076c1},
  intrahash = {9e987f58d895c490144693139cbc90c7},
  journal = {Nature},
  keywords = {baduk go google},
  month = jan,
  number = 7587,
  pages = {484--489},
  publisher = {Nature Publishing Group},
  title = {Mastering the Game of {Go} with Deep Neural Networks and Tree Search},
  year = 2016
}

@article{DBLP:journals/ker/MannionDDH18,
  author    = {Patrick Mannion and
               Sam Devlin and
               Jim Duggan and
               Enda Howley},
  title     = {Reward shaping for knowledge-based multi-objective multi-agent reinforcement  learning},
  journal   = {Knowledge Eng. Review},
  volume    = {33},
  pages     = {e23},
  year      = {2018},
  url       = {https://doi.org/10.1017/S0269888918000292},
  doi       = {10.1017/S0269888918000292},
  timestamp = {Mon, 16 Sep 2019 14:52:14 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/ker/MannionDDH18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{TodorovErezTassa12,
author={E. {Todorov} and T. {Erez} and Y. {Tassa}},
booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
title={MuJoCo: A physics engine for model-based control},
year={2012},
volume={},
number={},
pages={5026-5033},
keywords={application program interfaces;C++ language;control engineering computing;data structures;finite difference methods;humanoid robots;optimal control;program compilers;shock absorbers;XML;MuJoCo;physics engine;model-based control;multijoint dynamics;recursive algorithms;velocity-stepping approach;spring-dampers;high-level C++ API;intuitive XML file format;built-in compiler transforms;optimized data structure;runtime computation;tendon wrapping;actuator activation states;optimal control applications;finite differencing;12-core machine;3D humanoid;active contacts;Engines;Optimization;Computational modeling;Heuristic algorithms;Dynamics;Mathematical model},
doi={10.1109/IROS.2012.6386109},
ISSN={2153-0858},
month={Oct},}

@inproceedings{BadanidiyuruKleSli13,
author={A. {Badanidiyuru} and R. {Kleinberg} and A. {Slivkins}},
booktitle={2013 IEEE 54th Annual Symposium on Foundations of Computer Science},
title={Bandits with Knapsacks},
year={2013},
volume={},
number={},
pages={207-216},
month={Oct},}

@inproceedings{AgrawalDevanur16,
author = {Agrawal, Shipra and Devanur, Nikhil R.},
title = {Linear Contextual Bandits with Knapsacks},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {3458–3467},
numpages = {10},
location = {Barcelona, Spain},
series = {NIPS’16}
}

@inproceedings{DBLP:conf/focs/ImmorlicaSSS19,
  author    = {Nicole Immorlica and
               Karthik Abinav Sankararaman and
               Robert E. Schapire and
               Aleksandrs Slivkins},
  editor    = {David Zuckerman},
  title     = {Adversarial Bandits with Knapsacks},
  booktitle = {60th {IEEE} Annual Symposium on Foundations of Computer Science, {FOCS}
               2019, Baltimore, Maryland, USA, November 9-12, 2019},
  pages     = {202--219},
  publisher = {{IEEE} Computer Society},
  year      = {2019},
  url       = {https://doi.org/10.1109/FOCS.2019.00022},
  doi       = {10.1109/FOCS.2019.00022},
  timestamp = {Tue, 07 Jan 2020 13:31:39 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/focs/ImmorlicaSSS19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{altman-constrainedMDP,
  added-at = {2007-07-05T16:17:35.000+0200},
  author = {Altman, E.},
  biburl = {https://www.bibsonomy.org/bibtex/2421fb1dafa61f1d028550297084c3cb8/jleny},
  description = {bandit problems},
  interhash = {84fb43430ab46ff2336d7e9926a37b45},
  intrahash = {421fb1dafa61f1d028550297084c3cb8},
  keywords = {imported},
  publisher = {Chapman and Hall},
  timestamp = {2007-07-05T16:17:35.000+0200},
  title = {Constrained Markov Decision Processes},
  year = 1999
}







@inproceedings{LeYue19,
  title = 	 {Batch Policy Learning under Constraints},
  author = 	 {Le, Hoang and Voloshin, Cameron and Yue, Yisong},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning (ICML)},
  year = 	 {2019},
}

@misc{lykouris2019corruption,
    title={Corruption Robust Exploration in Episodic Reinforcement Learning},
    author={Thodoris Lykouris and Max Simchowitz and Aleksandrs Slivkins and Wen Sun},
    year={2019},
    eprint={1911.08689},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@misc{jin2019learning,
    title={Learning Adversarial MDPs with Bandit Feedback and Unknown Transition},
    author={Chi Jin and Tiancheng Jin and Haipeng Luo and Suvrit Sra and Tiancheng Yu},
    year={2019},
    eprint={1912.01192},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@inproceedings{pmlr-v70-jiang17c,
  title = 	 {Contextual Decision Processes with low {B}ellman rank are {PAC}-Learnable},
  author = 	 {Nan Jiang and Akshay Krishnamurthy and Alekh Agarwal and John Langford and Robert E. Schapire},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1704--1713},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/jiang17c/jiang17c.pdf},
  url = 	 {http://proceedings.mlr.press/v70/jiang17c.html},
  abstract = 	 {This paper studies systematic exploration for reinforcement learning (RL) with rich observations and function approximation. We introduce contextual decision processes (CDPs), that unify most prior RL settings. Our first contribution is a complexity measure, the Bellman rank, that we show enables tractable learning of near-optimal behavior in CDPs and is naturally small for many well-studied RL models. Our second contribution is a new RL algorithm that does systematic exploration to learn near-optimal behavior in CDPs with low Bellman rank. The algorithm requires a number of samples that is polynomial in all relevant parameters but independent of the number of unique contexts. Our approach uses Bellman error minimization with optimistic exploration and provides new insights into efficient exploration for RL with function approximation.}
}

@inproceedings{DBLP:conf/colt/SunJKA019,
  author    = {Wen Sun and
               Nan Jiang and
               Akshay Krishnamurthy and
               Alekh Agarwal and
               John Langford},
  title     = {Model-based {RL} in Contextual Decision Processes: {PAC} bounds and
               Exponential Improvements over Model-free Approaches},
  booktitle = {Conference on Learning Theory, {COLT} 2019, 25-28 June 2019, Phoenix,
               AZ, {USA}},
  pages     = {2898--2933},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v99/sun19a.html},
  timestamp = {Mon, 08 Jul 2019 16:13:41 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/colt/SunJKA019},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{yang2019reinforcement,
  title={Reinforcement leaning in feature space: Matrix bandit, kernels, and regret bound},
  author={Yang, Lin F and Wang, Mengdi},
  journal={arXiv preprint arXiv:1905.10389},
  year={2019}
}

@article{dyna1991stutton,
  author = {Sutton, Richard S.},
  title = {Dyna, an Integrated Architecture for Learning, Planning, and Reacting},
  year = {1991},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {2},
  number = {4},
  issn = {0163-5719},
  doi = {10.1145/122344.122377},
  journal = {SIGART Bull.},
  pages = {160–163},
  numpages = {4}
}

@article{joshua2017cpo,
  author    = {Joshua Achiam and
               David Held and
               Aviv Tamar and
               Pieter Abbeel},
  title     = {Constrained Policy Optimization},
  journal   = {CoRR},
  volume    = {abs/1705.10528},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.10528},
  archivePrefix = {arXiv},
  eprint    = {1705.10528},
  timestamp = {Mon, 13 Aug 2018 16:47:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/AchiamHTA17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{Ray2019,
    author = {Ray, Alex and Achiam, Joshua and Amodei, Dario},
    title = {Benchmarking Safe Exploration in Deep Reinforcement Learning},
    note = {Accessed March 11, 2020},
    year = {2020},
    howpublished = {https://cdn.openai.com/safexp-short.pdf},
}

@inproceedings{SyedSchapire2008,
author = {Syed, Umar and Schapire, Robert E.},
title = {A Game-Theoretic Approach to Apprenticeship Learning},
year = {2007},
isbn = {9781605603520},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 20th International Conference on Neural Information Processing Systems},
pages = {1449–1456},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS’07}
}

@article{Schulman2015TRPO,
  author    = {John Schulman and
               Sergey Levine and
               Philipp Moritz and
               Michael I. Jordan and
               Pieter Abbeel},
  title     = {Trust Region Policy Optimization},
  journal   = {CoRR},
  volume    = {abs/1502.05477},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.05477},
  archivePrefix = {arXiv},
  eprint    = {1502.05477},
}

@article{Hoang2019BPLUC,
  author    = {Hoang Minh Le and
               Cameron Voloshin and
               Yisong Yue},
  title     = {Batch Policy Learning under Constraints},
  journal   = {CoRR},
  volume    = {abs/1903.08738},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.08738},
  archivePrefix = {arXiv},
  eprint    = {1903.08738},
  timestamp = {Thu, 06 Jun 2019 18:03:43 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1903-08738},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Mao2016RLSystems,
    author = {Mao, Hongzi and Alizadeh, Mohammad and Menache, Ishai and Kandula, Srikanth},
    title = {Resource Management with Deep Reinforcement Learning},
    year = {2016}, isbn = {9781450346610},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3005745.3005750},
    doi = {10.1145/3005745.3005750},
    booktitle = {Proceedings of the 15th ACM Workshop on Hot Topics in Networks},
    pages = {50–56},
    numpages = {7},
}

@article{Joshua2018CPO,
  author    = {Joshua Achiam and
               David Held and
               Aviv Tamar and
               Pieter Abbeel},
  title     = {Constrained Policy Optimization},
  journal   = {CoRR},
  volume    = {abs/1705.10528},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.10528},
  archivePrefix = {arXiv},
  eprint    = {1705.10528},
  timestamp = {Mon, 13 Aug 2018 16:47:33 +0200},
}


@article{burda2018exploration,
  title={Exploration by random network distillation},
  author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  journal={arXiv preprint arXiv:1810.12894},
  year={2018}
}

@inproceedings{bellemare2016unifying,
  title={Unifying count-based exploration and intrinsic motivation},
  author={Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  booktitle={Advances in neural information processing systems},
  pages={1471--1479},
  year={2016}
}


@inproceedings{abbasi2011improved,
  title={Improved algorithms for linear stochastic bandits},
  author={Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2312--2320},
  year={2011}
}

@book{cesa2006prediction,
  title={Prediction, learning, and games},
  author={Cesa-Bianchi, Nicolo and Lugosi, G{\'a}bor},
  year={2006},
  publisher={Cambridge university press}
}

@article{sun2019provably,
  title={Provably efficient imitation learning from observation alone},
  author={Sun, Wen and Vemula, Anirudh and Boots, Byron and Bagnell, J Andrew},
  journal={arXiv preprint arXiv:1905.10948},
  year={2019}
}

@inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K},
  booktitle={Aaai},
  volume={8},
  pages={1433--1438},
  year={2008},
  organization={Chicago, IL, USA}
}

@book{SuttonBa98,
  title={Reinforcement Learning: An Introduction},
  author = {Richard S. Sutton and Andrew G. Barto},
  year = {1998},
  publisher = {MIT Press},
  edition = {First},
}

@book{SuttonBa18,
  title={Reinforcement Learning: An Introduction},
  author = {Richard S. Sutton and Andrew G. Barto},
  year = {2018},
  publisher = {MIT Press},
  edition = {Second},
}


@article{singh2020learning,
  title={Learning in Markov Decision Processes under Constraints},
  author={Singh, Rahul and Gupta, Abhishek and Shroff, Ness B},
  journal={arXiv preprint arXiv:2002.12435},
  year={2020}
}

@article{efroni2020exploration,
  title={Exploration-Exploitation in Constrained MDPs},
  author={Efroni, Yonathan and Mannor, Shie and Pirotta, Matteo},
  journal={arXiv preprint arXiv:2003.02189},
  year={2020}
}

@article{qiu2020upper,
  title={Upper confidence primal-dual optimization: Stochastically constrained markov decision processes with adversarial losses and unknown transitions},
  author={Qiu, Shuang and Wei, Xiaohan and Yang, Zhuoran and Ye, Jieping and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2003.00660},
  year={2020}
}

@article{ding2020provably,
  title={Provably efficient safe exploration via primal-dual policy optimization},
  author={Ding, Dongsheng and Wei, Xiaohan and Yang, Zhuoran and Wang, Zhaoran and Jovanovi{\'c}, Mihailo R},
  journal={arXiv preprint arXiv:2003.00534},
  year={2020}
}

@ARTICLE{Bellman1957,
    author = "Richard Bellman",
     title = "A Markovian Decision Process",
   journal = "Indiana Univ. Math. J.",
  fjournal = "Indiana University Mathematics Journal",
    volume = 6,
      year = 1957,
     issue = 4,
     pages = "679--684",
      issn = "0022-2518",
     coden = "IUMJAB",
   mrclass = "",
}

@inproceedings{achiam2017constrained,
  title={Constrained policy optimization},
  author={Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={22--31},
  year={2017},
  organization={JMLR. org}
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@article{tarbouriech2019active,
  title={Active exploration in markov decision processes},
  author={Tarbouriech, Jean and Lazaric, Alessandro},
  journal={arXiv preprint arXiv:1902.11199},
  year={2019}
}

@article{zheng2020constrained,
  title={Constrained upper confidence reinforcement learning},
  author={Zheng, Liyuan and Ratliff, Lillian J},
  journal={arXiv preprint arXiv:2001.09377},
  year={2020}
}

@article{song2019efficient,
  title={Efficient model-free reinforcement learning in metric spaces},
  author={Song, Zhao and Sun, Wen},
  journal={arXiv preprint arXiv:1905.00475},
  year={2019}
}