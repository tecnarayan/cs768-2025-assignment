\begin{thebibliography}{18}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Banino et~al.(2021)Banino, Balaguer, and Blundell}]{pondernet}
Andrea Banino, Jan Balaguer, and Charles Blundell. 2021.
\newblock \href {https://openreview.net/forum?id=1EuxRTe0WN} {Pondernet:
  Learning to ponder}.
\newblock In \emph{8th ICML Workshop on Automated Machine Learning (AutoML)}.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova}]{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
\newblock \href {https://doi.org/10.18653/v1/N19-1423} {{BERT}: Pre-training of
  deep bidirectional transformers for language understanding}.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota. Association for Computational Linguistics.

\bibitem[{Graves(2016)}]{act}
Alex Graves. 2016.
\newblock \href {http://arxiv.org/abs/1603.08983} {Adaptive computation time
  for recurrent neural networks}.
\newblock Cite arxiv:1603.08983.

\bibitem[{Kingma and Ba(2015)}]{adam}
Diederik~P. Kingma and Jimmy Ba. 2015.
\newblock \href {http://arxiv.org/abs/1412.6980} {Adam: {A} method for
  stochastic optimization}.
\newblock In \emph{3rd International Conference on Learning Representations,
  {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
  Proceedings}.

\bibitem[{Kingma and Welling(2014)}]{vae}
Diederik~P. Kingma and Max Welling. 2014.
\newblock \href {http://arxiv.org/abs/http://arxiv.org/abs/1312.6114v10}
  {{Auto-Encoding Variational Bayes}}.
\newblock In \emph{2nd International Conference on Learning Representations,
  {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track
  Proceedings}.

\bibitem[{Lan et~al.(2020)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut}]{albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut. 2020.
\newblock \href {https://openreview.net/forum?id=H1eA7AEtvS} {Albert: A lite
  bert for self-supervised learning of language representations}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Liao et~al.(2021)Liao, Zhang, Ren, Su, Sun, and He}]{global}
Kaiyuan Liao, Yi~Zhang, Xuancheng Ren, Qi~Su, Xu~Sun, and Bin He. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.naacl-main.162} {A global
  past-future early exit method for accelerating inference of pre-trained
  language models}.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 2013--2023, Online. Association for Computational
  Linguistics.

\bibitem[{Liu et~al.(2020)Liu, Zhou, Wang, Zhao, Deng, and Ju}]{fastbert}
Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, and Qi~Ju. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.acl-main.537} {Fastbert: a
  self-distilling {BERT} with adaptive inference time}.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics, {ACL} 2020, Online, July 5-10, 2020}, pages
  6035--6044. Association for Computational Linguistics.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov}]{roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
\newblock \href {http://arxiv.org/abs/1907.11692} {Roberta: A robustly
  optimized bert pretraining approach}.
\newblock Cite arxiv:1907.11692.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever}]{gpt}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever. 2019.
\newblock Language models are unsupervised multitask learners.

\bibitem[{Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov}]{dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov. 2014.
\newblock \href {http://jmlr.org/papers/v15/srivastava14a.html} {Dropout: A
  simple way to prevent neural networks from overfitting}.
\newblock \emph{Journal of Machine Learning Research}, 15(56):1929--1958.

\bibitem[{Teerapittayanon et~al.(2016)Teerapittayanon, McDanel, and
  Kung}]{branchynet}
Surat Teerapittayanon, Bradley McDanel, and H.~T. Kung. 2016.
\newblock \href {https://doi.org/10.1109/ICPR.2016.7900006} {{BranchyNet: Fast
  inference via early exiting from deep neural networks}}.
\newblock In \emph{{Proceedings of the 23rd International Conference on Pattern
  Recognition}}, pages 2464--2469. {IEEE}.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5998--6008.

\bibitem[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman}]{glue}
Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R.
  Bowman. 2018.
\newblock \href {http://arxiv.org/abs/1804.07461} {Glue: A multi-task benchmark
  and analysis platform for natural language understanding}.
\newblock Cite arxiv:1804.07461Comment: https://gluebenchmark.com/.

\bibitem[{Xin et~al.(2020)Xin, Tang, Lee, Yu, and Lin}]{deebert}
Ji~Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. 2020.
\newblock \href {https://www.aclweb.org/anthology/2020.acl-main.204}
  {{D}ee{BERT}: Dynamic early exiting for accelerating {BERT} inference}.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 2246--2251, Online. Association for
  Computational Linguistics.

\bibitem[{Xin et~al.(2021)Xin, Tang, Yu, and Lin}]{berxit}
Ji~Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.eacl-main.8} {{BER}xi{T}:
  Early exiting for {BERT} with better fine-tuning and extension to
  regression}.
\newblock In \emph{Proceedings of the 16th Conference of the European Chapter
  of the Association for Computational Linguistics: Main Volume}, pages
  91--104, Online. Association for Computational Linguistics.

\bibitem[{Zhou et~al.(2020)Zhou, Xu, Ge, McAuley, Xu, and Wei}]{pabee}
Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke~Xu, and Furu Wei. 2020.
\newblock \href
  {https://proceedings.neurips.cc/paper/2020/file/d4dd111a4fd973394238aca5c05bebe3-Paper.pdf}
  {Bert loses patience: Fast and robust inference with early exit}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pages 18330--18341. Curran Associates, Inc.

\bibitem[{Zhu(2021)}]{leebert}
Wei Zhu. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.acl-long.231} {{L}ee{BERT}:
  Learned early exit for {BERT} with cross-level optimization}.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 2968--2980,
  Online. Association for Computational Linguistics.

\end{thebibliography}
