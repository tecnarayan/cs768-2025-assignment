\begin{thebibliography}{85}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and
  Szepesv{\'a}ri]{abbasi2011improved}
Abbasi-Yadkori, Y., P{\'a}l, D., and Szepesv{\'a}ri, C.
\newblock Improved algorithms for linear stochastic bandits.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Agarwal et~al.(2021)Agarwal, Chaudhuri, Jain, Nagaraj, and
  Netrapalli]{agarwal2021online}
Agarwal, N., Chaudhuri, S., Jain, P., Nagaraj, D., and Netrapalli, P.
\newblock Online target q-learning with reverse experience replay: Efficiently
  finding the optimal policy for linear mdps.
\newblock \emph{arXiv preprint arXiv:2110.08440}, 2021.

\bibitem[Agrawal \& Jia(2017)Agrawal and Jia]{agrawal2017optimistic}
Agrawal, S. and Jia, R.
\newblock Optimistic posterior sampling for reinforcement learning: worst-case
  regret bounds.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Antos et~al.(2008)Antos, Szepesv{\'a}ri, and Munos]{antos2008learning}
Antos, A., Szepesv{\'a}ri, C., and Munos, R.
\newblock Learning near-optimal policies with bellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock \emph{Machine Learning}, 71\penalty0 (1):\penalty0 89--129, 2008.

\bibitem[Auer et~al.(2008)Auer, Jaksch, and Ortner]{auer2008near}
Auer, P., Jaksch, T., and Ortner, R.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 21, 2008.

\bibitem[Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang, and
  Yang]{ayoub2020model}
Ayoub, A., Jia, Z., Szepesvari, C., Wang, M., and Yang, L.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  463--474. PMLR, 2020.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  263--272. PMLR, 2017.

\bibitem[Ball et~al.(2023)Ball, Smith, Kostrikov, and
  Levine]{ball2023efficient}
Ball, P.~J., Smith, L., Kostrikov, I., and Levine, S.
\newblock Efficient online reinforcement learning with offline data.
\newblock \emph{arXiv preprint arXiv:2302.02948}, 2023.

\bibitem[Brafman \& Tennenholtz(2002)Brafman and Tennenholtz]{brafman2002r}
Brafman, R.~I. and Tennenholtz, M.
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Oct):\penalty0 213--231, 2002.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Chang et~al.(2021)Chang, Uehara, Sreenivas, Kidambi, and
  Sun]{chang2021mitigating}
Chang, J., Uehara, M., Sreenivas, D., Kidambi, R., and Sun, W.
\newblock Mitigating covariate shift in imitation learning via offline data
  with partial coverage.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 965--979, 2021.

\bibitem[Chen \& Jiang(2019)Chen and Jiang]{chen2019information}
Chen, J. and Jiang, N.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1042--1051. PMLR, 2019.

\bibitem[Chen \& Jiang(2022)Chen and Jiang]{chen2022offline}
Chen, J. and Jiang, N.
\newblock Offline reinforcement learning under value and density-ratio
  realizability: the power of gaps.
\newblock \emph{arXiv preprint arXiv:2203.13935}, 2022.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Dann, C., Lattimore, T., and Brunskill, E.
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Dann et~al.(2021)Dann, Marinov, Mohri, and Zimmert]{dann2021beyond}
Dann, C., Marinov, T.~V., Mohri, M., and Zimmert, J.
\newblock Beyond value-function gaps: Improved instance-dependent regret bounds
  for episodic reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 1--12, 2021.

\bibitem[Du et~al.(2021)Du, Kakade, Lee, Lovett, Mahajan, Sun, and
  Wang]{du2021bilinear}
Du, S., Kakade, S., Lee, J., Lovett, S., Mahajan, G., Sun, W., and Wang, R.
\newblock Bilinear classes: A structural framework for provable generalization
  in rl.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2826--2836. PMLR, 2021.

\bibitem[Fiez et~al.(2019)Fiez, Jain, Jamieson, and
  Ratliff]{fiez2019sequential}
Fiez, T., Jain, L., Jamieson, K.~G., and Ratliff, L.
\newblock Sequential experimental design for transductive linear bandits.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Foster et~al.(2021)Foster, Kakade, Qian, and
  Rakhlin]{foster2021statistical}
Foster, D.~J., Kakade, S.~M., Qian, J., and Rakhlin, A.
\newblock The statistical complexity of interactive decision making.
\newblock \emph{arXiv preprint arXiv:2112.13487}, 2021.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019off}
Fujimoto, S., Meger, D., and Precup, D.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International conference on machine learning}, pp.\
  2052--2062. PMLR, 2019.

\bibitem[Hao et~al.(2021)Hao, Lattimore, Szepesv{\'a}ri, and
  Wang]{hao2021online}
Hao, B., Lattimore, T., Szepesv{\'a}ri, C., and Wang, M.
\newblock Online sparse reinforcement learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  316--324. PMLR, 2021.

\bibitem[He et~al.(2020)He, Zhou, and Gu]{he2020logarithmic}
He, J., Zhou, D., and Gu, Q.
\newblock Logarithmic regret for reinforcement learning with linear function
  approximation.
\newblock \emph{arXiv preprint arXiv:2011.11566}, 2020.

\bibitem[Hester et~al.(2018)Hester, Vecerik, Pietquin, Lanctot, Schaul, Piot,
  Horgan, Quan, Sendonaris, Osband, et~al.]{hester2018deep}
Hester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul, T., Piot, B.,
  Horgan, D., Quan, J., Sendonaris, A., Osband, I., et~al.
\newblock Deep q-learning from demonstrations.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Jiang \& Huang(2020)Jiang and Huang]{jiang2020minimax}
Jiang, N. and Huang, J.
\newblock Minimax value interval for off-policy evaluation and policy
  optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 2747--2758, 2020.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I.
\newblock Is q-learning provably efficient?
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Jin et~al.(2020)Jin, Yang, Wang, and Jordan]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pp.\  2137--2143. PMLR,
  2020.

\bibitem[Jin et~al.(2021{\natexlab{a}})Jin, Liu, and
  Miryoosefi]{jin2021bellman}
Jin, C., Liu, Q., and Miryoosefi, S.
\newblock Bellman eluder dimension: New rich classes of rl problems, and
  sample-efficient algorithms.
\newblock \emph{arXiv preprint arXiv:2102.00815}, 2021{\natexlab{a}}.

\bibitem[Jin et~al.(2021{\natexlab{b}})Jin, Yang, and Wang]{jin2021pessimism}
Jin, Y., Yang, Z., and Wang, Z.
\newblock Is pessimism provably efficient for offline rl?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5084--5096. PMLR, 2021{\natexlab{b}}.

\bibitem[Kakade(2003)]{kakade2003sample}
Kakade, S.~M.
\newblock \emph{On the sample complexity of reinforcement learning}.
\newblock University of London, University College London (United Kingdom),
  2003.

\bibitem[Katz-Samuels \& Jamieson(2020)Katz-Samuels and Jamieson]{katz2020true}
Katz-Samuels, J. and Jamieson, K.
\newblock The true sample complexity of identifying good arms.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1781--1791. PMLR, 2020.

\bibitem[Kaufmann et~al.(2016)Kaufmann, Capp{\'e}, and
  Garivier]{kaufmann2016complexity}
Kaufmann, E., Capp{\'e}, O., and Garivier, A.
\newblock On the complexity of best-arm identification in multi-armed bandit
  models.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1--42, 2016.

\bibitem[Kearns \& Singh(2002)Kearns and Singh]{kearns2002near}
Kearns, M. and Singh, S.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock \emph{Machine learning}, 49\penalty0 (2):\penalty0 209--232, 2002.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{kidambi2020morel}
Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T.
\newblock Morel: Model-based offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 21810--21823, 2020.

\bibitem[Kober et~al.(2013)Kober, Bagnell, and Peters]{kober2013reinforcement}
Kober, J., Bagnell, J.~A., and Peters, J.
\newblock Reinforcement learning in robotics: A survey.
\newblock \emph{The International Journal of Robotics Research}, 32\penalty0
  (11):\penalty0 1238--1274, 2013.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Soh, Tucker, and
  Levine]{kumar2019stabilizing}
Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1179--1191, 2020.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Liu et~al.(2020)Liu, Swaminathan, Agarwal, and
  Brunskill]{liu2020provably}
Liu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E.
\newblock Provably good batch off-policy reinforcement learning without great
  exploration.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1264--1274, 2020.

\bibitem[Munos \& Szepesv{\'a}ri(2008)Munos and
  Szepesv{\'a}ri]{munos2008finite}
Munos, R. and Szepesv{\'a}ri, C.
\newblock Finite-time bounds for fitted value iteration.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0 (5), 2008.

\bibitem[Nair et~al.(2018)Nair, McGrew, Andrychowicz, Zaremba, and
  Abbeel]{nair2018overcoming}
Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., and Abbeel, P.
\newblock Overcoming exploration in reinforcement learning with demonstrations.
\newblock In \emph{2018 IEEE international conference on robotics and
  automation (ICRA)}, pp.\  6292--6299. IEEE, 2018.

\bibitem[Nakamoto et~al.(2023)Nakamoto, Zhai, Singh, Mark, Ma, Finn, Kumar, and
  Levine]{nakamoto2023cal}
Nakamoto, M., Zhai, Y., Singh, A., Mark, M.~S., Ma, Y., Finn, C., Kumar, A.,
  and Levine, S.
\newblock Cal-ql: Calibrated offline rl pre-training for efficient online
  fine-tuning.
\newblock \emph{arXiv preprint arXiv:2303.05479}, 2023.

\bibitem[Ok et~al.(2018)Ok, Proutiere, and Tranos]{ok2018exploration}
Ok, J., Proutiere, A., and Tranos, D.
\newblock Exploration in structured reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Pacchiano et~al.(2021)Pacchiano, Ball, Parker-Holder, Choromanski, and
  Roberts]{pacchiano2021towards}
Pacchiano, A., Ball, P., Parker-Holder, J., Choromanski, K., and Roberts, S.
\newblock Towards tractable optimism in model-based reinforcement learning.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  1413--1423.
  PMLR, 2021.

\bibitem[Rajeswaran et~al.(2017)Rajeswaran, Kumar, Gupta, Vezzani, Schulman,
  Todorov, and Levine]{rajeswaran2017learning}
Rajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schulman, J., Todorov, E.,
  and Levine, S.
\newblock Learning complex dexterous manipulation with deep reinforcement
  learning and demonstrations.
\newblock \emph{arXiv preprint arXiv:1709.10087}, 2017.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and
  Sutskever]{ramesh2021zero}
Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and
  Sutskever, I.
\newblock Zero-shot text-to-image generation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8821--8831. PMLR, 2021.

\bibitem[Rashidinejad et~al.(2021)Rashidinejad, Zhu, Ma, Jiao, and
  Russell]{rashidinejad2021bridging}
Rashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell, S.
\newblock Bridging offline reinforcement learning and imitation learning: A
  tale of pessimism.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 11702--11716, 2021.

\bibitem[Ren et~al.(2022)Ren, Zhang, Szepesv{\'a}ri, and Dai]{ren2022free}
Ren, T., Zhang, T., Szepesv{\'a}ri, C., and Dai, B.
\newblock A free lunch from the noise: Provable and practical exploration for
  representation learning.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  1686--1696.
  PMLR, 2022.

\bibitem[Ross \& Bagnell(2012)Ross and Bagnell]{ross2012agnostic}
Ross, S. and Bagnell, J.~A.
\newblock Agnostic system identification for model-based reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1203.1007}, 2012.

\bibitem[Shamir(2013)]{shamir2013complexity}
Shamir, O.
\newblock On the complexity of bandit and derivative-free stochastic convex
  optimization.
\newblock In \emph{Conference on Learning Theory}, pp.\  3--24. PMLR, 2013.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Simchowitz \& Jamieson(2019)Simchowitz and
  Jamieson]{simchowitz2019non}
Simchowitz, M. and Jamieson, K.~G.
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Song et~al.(2022)Song, Zhou, Sekhari, Bagnell, Krishnamurthy, and
  Sun]{song2022hybrid}
Song, Y., Zhou, Y., Sekhari, A., Bagnell, J.~A., Krishnamurthy, A., and Sun, W.
\newblock Hybrid rl: Using both offline and online data can make rl efficient.
\newblock \emph{arXiv preprint arXiv:2210.06718}, 2022.

\bibitem[Tennenholtz et~al.(2021)Tennenholtz, Shalit, Mannor, and
  Efroni]{tennenholtz2021bandits}
Tennenholtz, G., Shalit, U., Mannor, S., and Efroni, Y.
\newblock Bandits with partially observable confounded data.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  430--439.
  PMLR, 2021.

\bibitem[Tewari \& Bartlett(2007)Tewari and Bartlett]{tewari2007optimistic}
Tewari, A. and Bartlett, P.
\newblock Optimistic linear programming gives logarithmic regret for
  irreducible mdps.
\newblock \emph{Advances in Neural Information Processing Systems}, 20, 2007.

\bibitem[Tirinzoni et~al.(2022)Tirinzoni, Al-Marjani, and
  Kaufmann]{tirinzoni2022optimistic}
Tirinzoni, A., Al-Marjani, A., and Kaufmann, E.
\newblock Optimistic pac reinforcement learning: the instance-dependent view.
\newblock \emph{arXiv preprint arXiv:2207.05852}, 2022.

\bibitem[Tsybakov(2009)]{tsybakov2009introduction}
Tsybakov, A.~B.
\newblock Introduction to nonparametric estimation., 2009.

\bibitem[Uehara \& Sun(2021)Uehara and Sun]{uehara2021pessimistic}
Uehara, M. and Sun, W.
\newblock Pessimistic model-based offline reinforcement learning under partial
  coverage.
\newblock \emph{arXiv preprint arXiv:2107.06226}, 2021.

\bibitem[Wagenmaker \& Foster(2023)Wagenmaker and
  Foster]{wagenmaker2023instance}
Wagenmaker, A. and Foster, D.~J.
\newblock Instance-optimality in interactive decision making: Toward a
  non-asymptotic theory.
\newblock \emph{arXiv preprint arXiv:2304.12466}, 2023.

\bibitem[Wagenmaker \& Jamieson(2022)Wagenmaker and
  Jamieson]{wagenmaker2022instance}
Wagenmaker, A. and Jamieson, K.
\newblock Instance-dependent near-optimal policy identification in linear mdps
  via online experiment design.
\newblock \emph{arXiv preprint arXiv:2207.02575}, 2022.

\bibitem[Wagenmaker et~al.(2022{\natexlab{a}})Wagenmaker, Chen, Simchowitz, Du,
  and Jamieson]{wagenmaker2022first}
Wagenmaker, A.~J., Chen, Y., Simchowitz, M., Du, S., and Jamieson, K.
\newblock First-order regret in reinforcement learning with linear function
  approximation: A robust estimation approach.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  22384--22429. PMLR, 2022{\natexlab{a}}.

\bibitem[Wagenmaker et~al.(2022{\natexlab{b}})Wagenmaker, Chen, Simchowitz, Du,
  and Jamieson]{wagenmaker2022reward}
Wagenmaker, A.~J., Chen, Y., Simchowitz, M., Du, S., and Jamieson, K.
\newblock Reward-free rl is no harder than reward-aware rl in linear markov
  decision processes.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  22430--22456. PMLR, 2022{\natexlab{b}}.

\bibitem[Wagenmaker et~al.(2022{\natexlab{c}})Wagenmaker, Simchowitz, and
  Jamieson]{wagenmaker2022beyond}
Wagenmaker, A.~J., Simchowitz, M., and Jamieson, K.
\newblock Beyond no regret: Instance-dependent pac reinforcement learning.
\newblock In \emph{Conference on Learning Theory}, pp.\  358--418. PMLR,
  2022{\natexlab{c}}.

\bibitem[Weisz et~al.(2021)Weisz, Amortila, and
  Szepesv{\'a}ri]{weisz2021exponential}
Weisz, G., Amortila, P., and Szepesv{\'a}ri, C.
\newblock Exponential lower bounds for planning in mdps with
  linearly-realizable optimal action-value functions.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  1237--1264. PMLR, 2021.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Wu, Y., Tucker, G., and Nachum, O.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Xie \& Jiang(2021)Xie and Jiang]{xie2021batch}
Xie, T. and Jiang, N.
\newblock Batch value-function approximation with only realizability.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11404--11413. PMLR, 2021.

\bibitem[Xie et~al.(2021{\natexlab{a}})Xie, Cheng, Jiang, Mineiro, and
  Agarwal]{xie2021bellman}
Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A.
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 6683--6694, 2021{\natexlab{a}}.

\bibitem[Xie et~al.(2021{\natexlab{b}})Xie, Jiang, Wang, Xiong, and
  Bai]{xie2021policy}
Xie, T., Jiang, N., Wang, H., Xiong, C., and Bai, Y.
\newblock Policy finetuning: Bridging sample-efficient offline and online
  reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 27395--27407, 2021{\natexlab{b}}.

\bibitem[Xie et~al.(2022)Xie, Foster, Bai, Jiang, and Kakade]{xie2022role}
Xie, T., Foster, D.~J., Bai, Y., Jiang, N., and Kakade, S.~M.
\newblock The role of coverage in online reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2210.04157}, 2022.

\bibitem[Xu et~al.(2021)Xu, Ma, and Du]{xu2021fine}
Xu, H., Ma, T., and Du, S.
\newblock Fine-grained gap-dependent bounds for tabular mdps via adaptive
  multi-step bootstrap.
\newblock In \emph{Conference on Learning Theory}, pp.\  4438--4472. PMLR,
  2021.

\bibitem[Yang et~al.(2021)Yang, Yang, and Du]{yang2021q}
Yang, K., Yang, L., and Du, S.
\newblock Q-learning with logarithmic regret.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1576--1584. PMLR, 2021.

\bibitem[Yang \& Wang(2020)Yang and Wang]{yang2020reinforcement}
Yang, L. and Wang, M.
\newblock Reinforcement learning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10746--10756. PMLR, 2020.

\bibitem[Yin et~al.(2021)Yin, Bai, and Wang]{yin2021near}
Yin, M., Bai, Y., and Wang, Y.-X.
\newblock Near-optimal offline reinforcement learning via double variance
  reduction.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 7677--7688, 2021.

\bibitem[Yin et~al.(2022)Yin, Duan, Wang, and Wang]{yin2022near}
Yin, M., Duan, Y., Wang, M., and Wang, Y.-X.
\newblock Near-optimal offline reinforcement learning with linear
  representation: Leveraging variance information with pessimism.
\newblock \emph{arXiv preprint arXiv:2203.05804}, 2022.

\bibitem[Yu et~al.(2021)Yu, Liu, Nemati, and Yin]{yu2021reinforcement}
Yu, C., Liu, J., Nemati, S., and Yin, G.
\newblock Reinforcement learning in healthcare: A survey.
\newblock \emph{ACM Computing Surveys (CSUR)}, 55\penalty0 (1):\penalty0 1--36,
  2021.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and
  Ma]{yu2020mopo}
Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J.~Y., Levine, S., Finn, C., and
  Ma, T.
\newblock Mopo: Model-based offline policy optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 14129--14142, 2020.

\bibitem[Zanette \& Brunskill(2019)Zanette and Brunskill]{zanette2019tighter}
Zanette, A. and Brunskill, E.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7304--7312. PMLR, 2019.

\bibitem[Zanette et~al.(2020{\natexlab{a}})Zanette, Brandfonbrener, Brunskill,
  Pirotta, and Lazaric]{zanette2020frequentist}
Zanette, A., Brandfonbrener, D., Brunskill, E., Pirotta, M., and Lazaric, A.
\newblock Frequentist regret bounds for randomized least-squares value
  iteration.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1954--1964. PMLR, 2020{\natexlab{a}}.

\bibitem[Zanette et~al.(2020{\natexlab{b}})Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020learning}
Zanette, A., Lazaric, A., Kochenderfer, M., and Brunskill, E.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10978--10989. PMLR, 2020{\natexlab{b}}.

\bibitem[Zanette et~al.(2020{\natexlab{c}})Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020provably}
Zanette, A., Lazaric, A., Kochenderfer, M.~J., and Brunskill, E.
\newblock Provably efficient reward-agnostic navigation with linear value
  iteration.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 11756--11766, 2020{\natexlab{c}}.

\bibitem[Zanette et~al.(2021)Zanette, Wainwright, and
  Brunskill]{zanette2021provable}
Zanette, A., Wainwright, M.~J., and Brunskill, E.
\newblock Provable benefits of actor-critic methods for offline reinforcement
  learning.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 13626--13640, 2021.

\bibitem[Zhan et~al.(2022)Zhan, Huang, Huang, Jiang, and Lee]{zhan2022offline}
Zhan, W., Huang, B., Huang, A., Jiang, N., and Lee, J.
\newblock Offline reinforcement learning with realizability and single-policy
  concentrability.
\newblock In \emph{Conference on Learning Theory}, pp.\  2730--2775. PMLR,
  2022.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Ren, Yang, Gonzalez,
  Schuurmans, and Dai]{zhang2022making}
Zhang, T., Ren, T., Yang, M., Gonzalez, J., Schuurmans, D., and Dai, B.
\newblock Making linear mdps practical via contrastive representation learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  26447--26466. PMLR, 2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Chen, Zhu, and
  Sun]{zhang2022corruption}
Zhang, X., Chen, Y., Zhu, X., and Sun, W.
\newblock Corruption-robust offline reinforcement learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  5757--5773. PMLR, 2022{\natexlab{b}}.

\bibitem[Zheng et~al.(2023)Zheng, Luo, Wei, Song, Li, and
  Jiang]{zheng2023adaptive}
Zheng, H., Luo, X., Wei, P., Song, X., Li, D., and Jiang, J.
\newblock Adaptive policy learning for offline-to-online reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2303.07693}, 2023.

\bibitem[Zhou et~al.(2020)Zhou, Gu, and Szepesvari]{zhou2020nearly}
Zhou, D., Gu, Q., and Szepesvari, C.
\newblock Nearly minimax optimal reinforcement learning for linear mixture
  markov decision processes.
\newblock \emph{arXiv preprint arXiv:2012.08507}, 2020.

\bibitem[Zhou et~al.(2021)Zhou, He, and Gu]{zhou2021provably}
Zhou, D., He, J., and Gu, Q.
\newblock Provably efficient reinforcement learning for discounted mdps with
  feature mapping.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12793--12802. PMLR, 2021.

\end{thebibliography}
