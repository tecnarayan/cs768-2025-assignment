\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2019)Agarwal, Schuurmans, and
  Norouzi]{agarwal2019striving}
Agarwal, R., Schuurmans, D., and Norouzi, M.
\newblock Striving for simplicity in off-policy deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1907.04543}, 2019.

\bibitem[Anschel et~al.(2017)Anschel, Baram, and Shimkin]{anschel2017averaged}
Anschel, O., Baram, N., and Shimkin, N.
\newblock Averaged-dqn: Variance reduction and stabilization for deep
  reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  176--185. JMLR. org, 2017.

\bibitem[Barth-Maron et~al.(2018)Barth-Maron, Hoffman, Budden, Dabney, Horgan,
  Muldal, Heess, and Lillicrap]{barth2018distributed}
Barth-Maron, G., Hoffman, M.~W., Budden, D., Dabney, W., Horgan, D., Muldal,
  A., Heess, N., and Lillicrap, T.
\newblock Distributed distributional deterministic policy gradients.
\newblock \emph{arXiv preprint arXiv:1804.08617}, 2018.

\bibitem[{Bellemare} et~al.(2017){Bellemare}, {Dabney}, and
  {Munos}]{bellemare2017distributional}
{Bellemare}, M.~G., {Dabney}, W., and {Munos}, R.
\newblock {A Distributional Perspective on Reinforcement Learning}.
\newblock \emph{arXiv e-prints}, art. arXiv:1707.06887, Jul 2017.

\bibitem[Bodnar et~al.(2019)Bodnar, Li, Hausman, Pastor, and
  Kalakrishnan]{bodnar2019quantile}
Bodnar, C., Li, A., Hausman, K., Pastor, P., and Kalakrishnan, M.
\newblock Quantile qt-opt for risk-aware vision-based robotic grasping.
\newblock \emph{arXiv preprint arXiv:1910.02787}, 2019.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Buckman et~al.(2018)Buckman, Hafner, Tucker, Brevdo, and
  Lee]{buckman2018sample}
Buckman, J., Hafner, D., Tucker, G., Brevdo, E., and Lee, H.
\newblock Sample-efficient reinforcement learning with stochastic ensemble
  value expansion.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8224--8234, 2018.

\bibitem[Choi et~al.(2019)Choi, Lee, and Oh]{choi2019distributional}
Choi, Y., Lee, K., and Oh, S.
\newblock Distributional deep reinforcement learning with a mixture of
  gaussians.
\newblock In \emph{2019 International Conference on Robotics and Automation
  (ICRA)}, pp.\  9791--9797. IEEE, 2019.

\bibitem[Dabney et~al.(2018{\natexlab{a}})Dabney, Ostrovski, Silver, and
  Munos]{dabney2018implicit}
Dabney, W., Ostrovski, G., Silver, D., and Munos, R.
\newblock Implicit quantile networks for distributional reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1806.06923}, 2018{\natexlab{a}}.

\bibitem[Dabney et~al.(2018{\natexlab{b}})Dabney, Rowland, Bellemare, and
  Munos]{dabney2018distributional}
Dabney, W., Rowland, M., Bellemare, M.~G., and Munos, R.
\newblock Distributional reinforcement learning with quantile regression.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018{\natexlab{b}}.

\bibitem[D'Eramo et~al.(2017)D'Eramo, Nuara, Pirotta, and
  Restelli]{d2017estimating}
D'Eramo, C., Nuara, A., Pirotta, M., and Restelli, M.
\newblock Estimating the maximum expected value in continuous reinforcement
  learning problems.
\newblock In \emph{Thirty-First AAAI Conference on Artificial Intelligence},
  2017.

\bibitem[D’Eramo et~al.(2016)D’Eramo, Restelli, and
  Nuara]{discrete2016estimating}
D’Eramo, C., Restelli, M., and Nuara, A.
\newblock Estimating maximum expected value through gaussian approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1032--1040, 2016.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and Meger]{fujimoto18a}
Fujimoto, S., van Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1587--1596, Stockholmsmässan, Stockholm
  Sweden, 10--15 Jul 2018. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v80/fujimoto18a.html}.

\bibitem[Ghavamzadeh et~al.(2011)Ghavamzadeh, Kappen, Azar, and
  Munos]{azar2011speedy}
Ghavamzadeh, M., Kappen, H.~J., Azar, M.~G., and Munos, R.
\newblock Speedy q-learning.
\newblock In Shawe-Taylor, J., Zemel, R.~S., Bartlett, P.~L., Pereira, F., and
  Weinberger, K.~Q. (eds.), \emph{Advances in Neural Information Processing
  Systems 24}, pp.\  2411--2419. Curran Associates, Inc., 2011.
\newblock URL \url{http://papers.nips.cc/paper/4251-speedy-q-learning.pdf}.

\bibitem[Haarnoja et~al.(2018{\natexlab{a}})Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock \emph{arXiv preprint arXiv:1801.01290}, 2018{\natexlab{a}}.

\bibitem[Haarnoja et~al.(2018{\natexlab{b}})Haarnoja, Zhou, Hartikainen,
  Tucker, Ha, Tan, Kumar, Zhu, Gupta, Abbeel, et~al.]{haarnoja2018applications}
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar,
  V., Zhu, H., Gupta, A., Abbeel, P., et~al.
\newblock Soft actor-critic algorithms and applications.
\newblock \emph{arXiv preprint arXiv:1812.05905}, 2018{\natexlab{b}}.

\bibitem[He \& Guo(2019)He and Guo]{he2019interleaved}
He, M. and Guo, H.
\newblock Interleaved q-learning with partially coupled training process.
\newblock In \emph{Proceedings of the 18th International Conference on
  Autonomous Agents and MultiAgent Systems}, pp.\  449--457. International
  Foundation for Autonomous Agents and Multiagent Systems, 2019.

\bibitem[Hessel et~al.(2018)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{hessel2018rainbow}
Hessel, M., Modayil, J., Van~Hasselt, H., Schaul, T., Ostrovski, G., Dabney,
  W., Horgan, D., Piot, B., Azar, M., and Silver, D.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Imagaw \& Kaneko(2017)Imagaw and Kaneko]{imagaw2017estimating}
Imagaw, T. and Kaneko, T.
\newblock Estimating the maximum expected value through upper confidence bound
  of likelihood.
\newblock In \emph{2017 Conference on Technologies and Applications of
  Artificial Intelligence (TAAI)}, pp.\  202--207. IEEE, 2017.

\bibitem[Ishwaei~D et~al.(1985)Ishwaei~D, Shabma, and
  Krishnamoorthy]{ishwaei1985non}
Ishwaei~D, B., Shabma, D., and Krishnamoorthy, K.
\newblock Non-existence of unbiased estimators of ordered parameters.
\newblock \emph{Statistics: A Journal of Theoretical and Applied Statistics},
  16\penalty0 (1):\penalty0 89--95, 1985.

\bibitem[Kaufmann et~al.(2018)Kaufmann, Koolen, and
  Garivier]{kaufmann2018sequential}
Kaufmann, E., Koolen, W.~M., and Garivier, A.
\newblock Sequential test for the lowest mean: From thompson to murphy
  sampling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6332--6342, 2018.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Soh, Tucker, and
  Levine]{kumar2019stabilizing}
Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  11761--11771, 2019.

\bibitem[Lan et~al.(2020)Lan, Pan, Fyshe, and White]{lan2020maxmin}
Lan, Q., Pan, Y., Fyshe, A., and White, M.
\newblock Maxmin q-learning: Controlling the estimation bias of q-learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=Bkg0u3Etwr}.

\bibitem[Lee \& Powell(2012)Lee and Powell]{lee2012intelligent}
Lee, D. and Powell, W.~B.
\newblock An intelligent battery controller using bias-corrected q-learning.
\newblock In \emph{Twenty-Sixth AAAI Conference on Artificial Intelligence},
  2012.

\bibitem[Li \& Hou(2019)Li and Hou]{li2019mixing}
Li, Z. and Hou, X.
\newblock Mixing update q-value for deep reinforcement learning.
\newblock In \emph{2019 International Joint Conference on Neural Networks
  (IJCNN)}, pp.\  1--6. IEEE, 2019.

\bibitem[{Lv} et~al.(2019){Lv}, {Wang}, {Cheng}, and
  {Duan}]{lv2019stochdoubledqn}
{Lv}, P., {Wang}, X., {Cheng}, Y., and {Duan}, Z.
\newblock Stochastic double deep q-network.
\newblock \emph{IEEE Access}, 7:\penalty0 79446--79454, 2019.
\newblock ISSN 2169-3536.
\newblock \doi{10.1109/ACCESS.2019.2922706}.

\bibitem[Mania et~al.(2018)Mania, Guy, and Recht]{mania2018simple}
Mania, H., Guy, A., and Recht, B.
\newblock Simple random search provides a competitive approach to reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1803.07055}, 2018.

\bibitem[Mavrin et~al.(2019)Mavrin, Yao, Kong, Wu, and
  Yu]{mavrin2019distributional}
Mavrin, B., Yao, H., Kong, L., Wu, K., and Yu, Y.
\newblock Distributional reinforcement learning for efficient exploration.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pp.\  4424--4434, Long
  Beach, California, USA, 09--15 Jun 2019. PMLR.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., and Riedmiller, M.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Patnaik \& Anwar(2008)Patnaik and Anwar]{patnaik2008q}
Patnaik, K. and Anwar, S.
\newblock Q learning in context of approximation spaces.
\newblock \emph{Contemporary Engineering Sciences}, 1\penalty0 (1):\penalty0
  41--49, 2008.

\bibitem[Smith \& Winkler(2006)Smith and Winkler]{smith2006optimizer}
Smith, J.~E. and Winkler, R.~L.
\newblock The optimizer’s curse: Skepticism and postdecision surprise in
  decision analysis.
\newblock \emph{Management Science}, 52\penalty0 (3):\penalty0 311--322, 2006.

\bibitem[Stone(1974)]{stone1974cross}
Stone, M.
\newblock Cross-validatory choice and assessment of statistical predictions.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 36\penalty0 (2):\penalty0 111--133, 1974.

\bibitem[Thaler(2012)]{thaler2012winner}
Thaler, R.
\newblock \emph{The winner's curse: Paradoxes and anomalies of economic life}.
\newblock Simon and Schuster, 2012.

\bibitem[Thrun \& Schwartz(1993)Thrun and Schwartz]{thrun1993issues}
Thrun, S. and Schwartz, A.
\newblock Issues in using function approximation for reinforcement learning.
\newblock In \emph{Proceedings of the 1993 Connectionist Models Summer School
  Hillsdale, NJ. Lawrence Erlbaum}, 1993.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pp.\  5026--5033. IEEE, 2012.

\bibitem[Van~Hasselt(2010)]{hasselt2010double}
Van~Hasselt, H.
\newblock Double q-learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2613--2621, 2010.

\bibitem[Van~Hasselt(2013)]{van2013estimating}
Van~Hasselt, H.
\newblock Estimating the maximum expected value: an analysis of (nested) cross
  validation and the maximum sample average.
\newblock \emph{arXiv preprint arXiv:1302.7175}, 2013.

\bibitem[{Van Hasselt} et~al.(2015){Van Hasselt}, {Guez}, and
  {Silver}]{hasselt2015double}
{Van Hasselt}, H., {Guez}, A., and {Silver}, D.
\newblock {Deep Reinforcement Learning with Double Q-learning}.
\newblock \emph{arXiv e-prints}, art. arXiv:1509.06461, Sep 2015.

\bibitem[White(1988)]{white1988mean}
White, D.
\newblock Mean, variance, and probabilistic criteria in finite markov decision
  processes: a review.
\newblock \emph{Journal of Optimization Theory and Applications}, 56\penalty0
  (1):\penalty0 1--29, 1988.

\bibitem[Yang et~al.(2019)Yang, Zhao, Lin, Qin, Bian, and Liu]{yang2019fully}
Yang, D., Zhao, L., Lin, Z., Qin, T., Bian, J., and Liu, T.-Y.
\newblock Fully parameterized quantile function for distributional
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6190--6199, 2019.

\bibitem[Zhang \& Yao(2019)Zhang and Yao]{zhang2019quota}
Zhang, S. and Yao, H.
\newblock Quota: The quantile option architecture for reinforcement learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pp.\  5797--5804, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Pan, and Kochenderfer]{zhang2017weighted}
Zhang, Z., Pan, Z., and Kochenderfer, M.~J.
\newblock Weighted double q-learning.
\newblock In \emph{IJCAI}, pp.\  3455--3461, 2017.

\end{thebibliography}
