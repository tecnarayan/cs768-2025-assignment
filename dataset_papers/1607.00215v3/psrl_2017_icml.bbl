\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Araya et~al.(2012)Araya, Buffet, and Thomas]{araya2012near}
Araya, Mauricio, Buffet, Olivier, and Thomas, Vincent.
\newblock Near-optimal brl using optimistic local transitions.
\newblock \emph{arXiv preprint arXiv:1206.4613}, 2012.

\bibitem[Asmuth et~al.(2009)Asmuth, Li, Littman, Nouri, and
  Wingate]{asmuth2009bayesian}
Asmuth, John, Li, Lihong, Littman, Michael~L, Nouri, Ali, and Wingate, David.
\newblock A {Bayesian} sampling approach to exploration in reinforcement
  learning.
\newblock In \emph{Proceedings of the Twenty-Fifth Conference on Uncertainty in
  Artificial Intelligence}, pp.\  19--26. AUAI Press, 2009.

\bibitem[Bartlett \& Tewari(2009)Bartlett and Tewari]{Bartlett2009}
Bartlett, Peter~L. and Tewari, Ambuj.
\newblock {REGAL}: A regularization based algorithm for reinforcement learning
  in weakly communicating {MDPs}.
\newblock In \emph{Proceedings of the 25th Conference on Uncertainty in
  Artificial Intelligence (UAI2009)}, pp.\  35--42, June 2009.

\bibitem[Bellman \& Kalaba(1959)Bellman and Kalaba]{bellman1959adaptive}
Bellman, Richard and Kalaba, Robert.
\newblock On adaptive control processes.
\newblock \emph{IRE Transactions on Automatic Control}, 4\penalty0
  (2):\penalty0 1--9, 1959.

\bibitem[Brafman \& Tennenholtz(2002)Brafman and Tennenholtz]{Brafman2002}
Brafman, Ronen~I. and Tennenholtz, Moshe.
\newblock R-max - a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 3:\penalty0 213--231,
  2002.

\bibitem[Buldygin \& Kozachenko(1980)Buldygin and Kozachenko]{buldygin1980sub}
Buldygin, Valerii~V and Kozachenko, Yu~V.
\newblock Sub-gaussian random variables.
\newblock \emph{Ukrainian Mathematical Journal}, 32\penalty0 (6):\penalty0
  483--489, 1980.

\bibitem[Burnetas \& Katehakis(1997)Burnetas and
  Katehakis]{burnetas1997optimal}
Burnetas, Apostolos~N and Katehakis, Michael~N.
\newblock Optimal adaptive policies for {Markov} decision processes.
\newblock \emph{Mathematics of Operations Research}, 22\penalty0 (1):\penalty0
  222--255, 1997.

\bibitem[Dani et~al.(2008)Dani, Hayes, and Kakade]{DaniHK2008}
Dani, Varsha, Hayes, Thomas~P., and Kakade, Sham~M.
\newblock Stochastic linear optimization under bandit feedback.
\newblock In \emph{COLT}, pp.\  355--366, 2008.

\bibitem[Dann \& Brunskill(2015)Dann and Brunskill]{dann2015sample}
Dann, Christoph and Brunskill, Emma.
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  TBA, 2015.

\bibitem[Filippi et~al.(2010)Filippi, Capp{\'e}, and
  Garivier]{filippi2010optimism}
Filippi, Sarah, Capp{\'e}, Olivier, and Garivier, Aur{\'e}lien.
\newblock Optimism in reinforcement learning and kullback-leibler divergence.
\newblock In \emph{Communication, Control, and Computing (Allerton), 2010 48th
  Annual Allerton Conference on}, pp.\  115--122. IEEE, 2010.

\bibitem[Fonteneau et~al.(2013)Fonteneau, Korda, and
  Munos]{fonteneau2013optimistic}
Fonteneau, Rapha{\"e}l, Korda, Nathan, and Munos, R{\'e}mi.
\newblock An optimistic posterior sampling strategy for {Bayesian}
  reinforcement learning.
\newblock In \emph{NIPS 2013 Workshop on Bayesian Optimization (BayesOpt2013)},
  2013.

\bibitem[Gopalan \& Mannor(2014)Gopalan and Mannor]{gopalan2014thompson}
Gopalan, Aditya and Mannor, Shie.
\newblock Thompson sampling for learning parameterized {Markov} decision
  processes.
\newblock \emph{arXiv preprint arXiv:1406.7498}, 2014.

\bibitem[Hadar \& Russell(1969)Hadar and Russell]{hadar1969rules}
Hadar, Josef and Russell, William~R.
\newblock Rules for ordering uncertain prospects.
\newblock \emph{The American Economic Review}, pp.\  25--34, 1969.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{Jaksch2010}
Jaksch, Thomas, Ortner, Ronald, and Auer, Peter.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11:\penalty0 1563--1600,
  2010.

\bibitem[Kakade(2003)]{Kakade2003}
Kakade, Sham.
\newblock \emph{On the Sample Complexity of Reinforcement Learning}.
\newblock PhD thesis, University College London, 2003.

\bibitem[Kearns \& Singh(2002)Kearns and Singh]{Kearns2002}
Kearns, Michael~J. and Singh, Satinder~P.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock \emph{Machine Learning}, 49\penalty0 (2-3):\penalty0 209--232, 2002.

\bibitem[Kolter \& Ng(2009)Kolter and Ng]{kolter2009near}
Kolter, J~Zico and Ng, Andrew~Y.
\newblock Near-{Bayesian} exploration in polynomial time.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning}, pp.\  513--520. ACM, 2009.

\bibitem[Lattimore \& Hutter(2012)Lattimore and Hutter]{lattimore2012pac}
Lattimore, Tor and Hutter, Marcus.
\newblock {PAC} bounds for discounted {MDP}s.
\newblock In \emph{Algorithmic learning theory}, pp.\  320--334. Springer,
  2012.

\bibitem[Munos(2014)]{munos2014bandits}
Munos, R{\'e}mi.
\newblock From bandits to monte-carlo tree search: The optimistic principle
  applied to optimization and planning.
\newblock 2014.

\bibitem[Osband(2016)]{osband2016thesis}
Osband, Ian.
\newblock \emph{Deep Exploration via Randomized Value Functions}.
\newblock PhD thesis, Stanford, 2016.

\bibitem[Osband \& Van~Roy(2014{\natexlab{a}})Osband and
  Van~Roy]{osband2014model}
Osband, Ian and Van~Roy, Benjamin.
\newblock Model-based reinforcement learning and the eluder dimension.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1466--1474, 2014{\natexlab{a}}.

\bibitem[Osband \& Van~Roy(2014{\natexlab{b}})Osband and
  Van~Roy]{osband2014near}
Osband, Ian and Van~Roy, Benjamin.
\newblock Near-optimal reinforcement learning in factored {MDP}s.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  604--612, 2014{\natexlab{b}}.

\bibitem[Osband \& Van~Roy(2016)Osband and Van~Roy]{osband2016lower}
Osband, Ian and Van~Roy, Benjamin.
\newblock On lower bounds for regret in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1608.02732}, 2016.

\bibitem[Osband \& Van~Roy(2017)Osband and Van~Roy]{osband2017gaussian}
Osband, Ian and Van~Roy, Benjamin.
\newblock Gaussian-dirichlet posterior dominance in sequential learning.
\newblock \emph{arXiv preprint arXiv:1702.04126}, 2017.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{Osband2013}
Osband, Ian, Russo, Daniel, and Van~Roy, Benjamin.
\newblock ({M}ore) efficient reinforcement learning via posterior sampling.
\newblock In \emph{NIPS}, pp.\  3003--3011. Curran Associates, Inc., 2013.

\bibitem[Osband et~al.(2014)Osband, Van~Roy, and Wen]{osband2014generalization}
Osband, Ian, Van~Roy, Benjamin, and Wen, Zheng.
\newblock Generalization and exploration via randomized value functions.
\newblock \emph{arXiv preprint arXiv:1402.0635}, 2014.

\bibitem[Russo \& Van~Roy(2014)Russo and Van~Roy]{Russo2013}
Russo, Daniel and Van~Roy, Benjamin.
\newblock Learning to optimize via posterior sampling.
\newblock \emph{Mathematics of Operations Research}, 39\penalty0 (4):\penalty0
  1221--1243, 2014.

\bibitem[Strehl \& Littman(2005)Strehl and Littman]{strehl2005theoretical}
Strehl, Alexander~L and Littman, Michael~L.
\newblock A theoretical analysis of model-based interval estimation.
\newblock In \emph{Proceedings of the 22nd international conference on Machine
  learning}, pp.\  856--863. ACM, 2005.

\bibitem[Strehl et~al.(2006)Strehl, Li, Wiewiora, Langford, and
  Littman]{Strehl2006}
Strehl, Alexander~L., Li, Lihong, Wiewiora, Eric, Langford, John, and Littman,
  Michael~L.
\newblock {PAC} model-free reinforcement learning.
\newblock In \emph{ICML}, pp.\  881--888, 2006.

\bibitem[Strens(2000)]{Strens00}
Strens, Malcolm J.~A.
\newblock A {Bayesian} framework for reinforcement learning.
\newblock In \emph{ICML}, pp.\  943--950, 2000.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{Sutton1998}
Sutton, Richard and Barto, Andrew.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock MIT Press, March 1998.

\bibitem[Szita \& Szepesv{\'a}ri(2010)Szita and Szepesv{\'a}ri]{szita2010model}
Szita, Istv{\'a}n and Szepesv{\'a}ri, Csaba.
\newblock Model-based reinforcement learning with nearly tight exploration
  complexity bounds.
\newblock In \emph{Proceedings of the 27th International Conference on Machine
  Learning (ICML-10)}, pp.\  1031--1038, 2010.

\bibitem[Thompson(1933)]{Thompson1933}
Thompson, W.R.
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock \emph{Biometrika}, 25\penalty0 (3/4):\penalty0 285--294, 1933.

\bibitem[Vlassis et~al.(2012)Vlassis, Ghavamzadeh, Mannor, and
  Poupart]{vlassis2012bayesian}
Vlassis, Nikos, Ghavamzadeh, Mohammad, Mannor, Shie, and Poupart, Pascal.
\newblock Bayesian reinforcement learning.
\newblock In \emph{Reinforcement Learning}, pp.\  359--386. Springer, 2012.

\end{thebibliography}
