@inproceedings{cho2014gru,
    title = {Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation},
    author = {Cho, Kyunghyun and
             van Merri{\"e}nboer, Bart and
             Gulcehre, Caglar and
             Bahdanau, Dzmitry and
             Bougares, Fethi and
             Schwenk, Holger and
             Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in
                Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}

@inproceedings{paszke2019torch,
 author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
 volume = {32},
 year = {2019}
}

@incollection{baddeley1974memory,
title = {Working Memory},
editor = {Gordon H. Bower},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {8},
pages = {47-89},
year = {1974},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60452-1},
author = {Alan D. Baddeley and Graham Hitch},
abstract = {Publisher Summary
This chapter presents a body of new experimental evidence, which provides a firm basis for the working memory hypothesis. The chapter presents a series of experiments on the role of memory in reasoning, language comprehension, and learning. An attempt is made to apply the comparable techniques in all three cases to allow a common pattern to emerge, if the same working memory system is operative in all three instances. The chapter makes a case for postulating the working memory-LTS system as a modification of the current STS-LTS view. Working memory represents a control system with limits on both its storage and processing capabilities, and has access to phonemically coded information (possibly by controlling a rehearsal buffer), that it is responsible for the limited memory span, but does not underlie the recency effect in free recall. The experiments presented in the chapter suggest that the phonemic rehearsal buffer plays a limited role in this process, but is by no means essential. These experiments also suggest that working memory plays a part in verbal reasoning and in prose comprehension. Understanding the detailed role of working memory in these tasks, however, must proceed hand-in-hand with an understanding of the tasks themselves.}
}

@article{hendrycks2016gelu,
  author       = {Dan Hendrycks and
                  Kevin Gimpel},
  title        = {Bridging Nonlinearities and Stochastic Regularizers with Gaussian
                  Error Linear Units},
  journal      = {CoRR},
  volume       = {abs/1606.08415},
  year         = {2016},
  url          = {http://arxiv.org/abs/1606.08415},
  eprinttype    = {arXiv},
  eprint       = {1606.08415},
  timestamp    = {Mon, 13 Aug 2018 16:46:20 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/HendrycksG16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@Article{graves2016neural,
author={Graves, Alex
and Wayne, Greg
and Reynolds, Malcolm
and Harley, Tim
and Danihelka, Ivo
and Grabska-Barwi{\'{n}}ska, Agnieszka
and Colmenarejo, Sergio G{\'o}mez
and Grefenstette, Edward
and Ramalho, Tiago
and Agapiou, John
and Badia, Adri{\`a} Puigdom{\`e}nech
and Hermann, Karl Moritz
and Zwols, Yori
and Ostrovski, Georg
and Cain, Adam
and King, Helen
and Summerfield, Christopher
and Blunsom, Phil
and Kavukcuoglu, Koray
and Hassabis, Demis},
title={Hybrid computing using a neural network with dynamic external memory},
journal={Nature},
year={2016},
month={Oct},
day={01},
volume={538},
number={7626},
pages={471-476},
abstract={Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external read--write memory.},
issn={1476-4687},
doi={10.1038/nature20101},
url={https://doi.org/10.1038/nature20101}
}



@article{graves2014turing,
  author       = {Alex Graves and
                  Greg Wayne and
                  Ivo Danihelka},
  title        = {Neural Turing Machines},
  journal      = {CoRR},
  volume       = {abs/1410.5401},
  year         = {2014},
  url          = {http://arxiv.org/abs/1410.5401},
  eprinttype    = {arXiv},
  eprint       = {1410.5401},
  timestamp    = {Mon, 13 Aug 2018 16:46:28 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/GravesWD14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{clevert2016elu,
      title={{Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)}}, 
      author={Djork-Arné Clevert and Thomas Unterthiner and Sepp Hochreiter},
      year={2016},
      eprint={1511.07289},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1511.07289},
}

@Article{gouk2021lipschitz,
author={Gouk, Henry and Frank, Eibe and Pfahringer, Bernhard and
        Cree, Michael J.},
title={{Regularisation of neural networks by enforcing Lipschitz continuity}},
journal={Machine Learning},
year={2021},
month={Feb},
day={01},
volume={110},
number={2},
pages={393-416},
issn={1573-0565},
doi={10.1007/s10994-020-05929-w},
url={https://doi.org/10.1007/s10994-020-05929-w}
}

@inproceedings{
miyato2018spectral,
title={{Spectral Normalization for Generative Adversarial Networks}},
author={Takeru Miyato and Toshiki Kataoka and Masanori Koyama and Yuichi Yoshida},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=B1QRgziT-},
}

@misc{yoshida2017spectral,
      title={{Spectral Norm Regularization for Improving the Generalizability of Deep Learning}}, 
      author={Yuichi Yoshida and Takeru Miyato},
      year={2017},
      eprint={1705.10941},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1705.10941},
      doi={10.48550/arXiv.1705.10941}
}

@article{schwarzschild2021dataset,
  author       = {Avi Schwarzschild and
                  Eitan Borgnia and
                  Arjun Gupta and
                  Arpit Bansal and
                  Zeyad Emam and
                  Furong Huang and
                  Micah Goldblum and
                  Tom Goldstein},
  title        = {{Datasets for Studying Generalization from Easy to Hard Examples}},
  journal      = {CoRR},
  volume       = {abs/2108.06011},
  year         = {2021},
  url          = {https://arxiv.org/abs/2108.06011},
  eprinttype    = {arXiv},
  eprint       = {2108.06011},
  timestamp    = {Wed, 18 Aug 2021 19:45:42 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2108-06011.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{lu2020dying,
   title={{Dying ReLU and Initialization: Theory and Numerical Examples}},
   volume={28},
   ISSN={1991-7120},
   url={http://dx.doi.org/10.4208/cicp.OA-2020-0165},
   DOI={10.4208/cicp.oa-2020-0165},
   number={5},
   journal={Communications in Computational Physics},
   publisher={Global Science Press},
   author={Lu Lu and Yeonjong Shin and Yanhui Su and George Em Karniadakis},
   year={2020},
   month=jun, pages={1671–1706}
}


@inproceedings{schwarzschild2021algorithm,
 author = {Schwarzschild, Avi and Borgnia, Eitan and Gupta, Arjun and Huang, Furong and Vishkin, Uzi and Goldblum, Micah and Goldstein, Tom},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {6695--6706},
 publisher = {Curran Associates, Inc.},
 title = {Can You Learn an Algorithm?  Generalizing from Easy to Hard Problems with Recurrent Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/3501672ebc68a5524629080e3ef60aef-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{sergey2015batchnorm,
author = {Ioffe, Sergey and Szegedy, Christian},
title = {Batch normalization: accelerating deep network training by reducing internal covariate shift},
year = {2015},
publisher = {JMLR.org},
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {448–456},
numpages = {9},
location = {Lille, France},
series = {ICML'15}
}

@InProceedings{kingma2015adam,
  author    = {Kingma, Diederik and Ba, Jimmy},
  booktitle = {International Conference on Learning Representations (ICLR)},
  title     = {Adam: A Method for Stochastic Optimization},
  year      = {2015},
  address   = {San Diega, CA, USA},
  optmonth  = {12},
}

@inproceedings{bansal2022endtoend,
 author = {Bansal, Arpit and Schwarzschild, Avi and Borgnia, Eitan and Emam, Zeyad and Huang, Furong and Goldblum, Micah and Goldstein, Tom},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {20232--20242},
 publisher = {Curran Associates, Inc.},
 title = {End-to-end Algorithm Synthesis with Recurrent Networks: Extrapolation without Overthinking},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/7f70331dbe58ad59d83941dfa7d975aa-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@article{woo2018cbam,
  author       = {Sanghyun Woo and
                  Jongchan Park and
                  Joon{-}Young Lee and
                  In So Kweon},
  title        = {{CBAM:} Convolutional Block Attention Module},
  journal      = {CoRR},
  volume       = {abs/1807.06521},
  year         = {2018},
  url          = {http://arxiv.org/abs/1807.06521},
  eprinttype    = {arXiv},
  eprint       = {1807.06521},
  timestamp    = {Mon, 13 Aug 2018 16:48:20 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1807-06521.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{chung2021turing,
 author = {Chung, Stephen and Siegelmann, Hava},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {28431--28441},
 publisher = {Curran Associates, Inc.},
 title = {{Turing Completeness of Bounded-Precision Recurrent Neural Networks}},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/ef452c63f81d0105dd4486f775adec81-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{hochreiter1997lstm,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {{Long Short-Term Memory}},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = {nov},
pages = {1735–1780},
numpages = {46}
}

@inproceedings{mena2018gumbel,
title={{Learning Latent Permutations with Gumbel-Sinkhorn Networks}},
author={Gonzalo Mena and David Belanger and Scott Linderman and Jasper Snoek},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=Byt3oJ-0W},
}

@article{trottier2016pelu,
  author       = {Ludovic Trottier and
                  Philippe Gigu{\`{e}}re and
                  Brahim Chaib{-}draa},
  title        = {{Parametric Exponential Linear Unit for Deep Convolutional Neural Networks}},
  journal      = {CoRR},
  volume       = {abs/1605.09332},
  year         = {2016},
  url          = {http://arxiv.org/abs/1605.09332},
  eprinttype    = {arXiv},
  eprint       = {1605.09332},
  timestamp    = {Mon, 13 Aug 2018 16:48:51 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/TrottierGC16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/Graves16,
  author       = {Alex Graves},
  title        = {Adaptive Computation Time for Recurrent Neural Networks},
  journal      = {CoRR},
  volume       = {abs/1603.08983},
  year         = {2016},
  url          = {http://arxiv.org/abs/1603.08983},
  eprinttype    = {arXiv},
  eprint       = {1603.08983},
  timestamp    = {Mon, 13 Aug 2018 16:47:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/Graves16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
jastrzebski2018residual,
title={Residual Connections Encourage Iterative Inference},
author={Stanisław Jastrzebski and Devansh Arpit and Nicolas Ballas and Vikas Verma and Tong Che and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=SJa9iHgAZ},
}


@article{10.15352/bjma/1240321550,
	author = {Krzysztof Ciesielski},
	doi = {10.15352/bjma/1240321550},
	journal = {Banach Journal of Mathematical Analysis},
	keywords = {Banach, Banach space, functional analysis, Lvov School of mathematics, Scottish Cafe},
	number = {1},
	pages = {1 -- 10},
	publisher = {Tusi Mathematical Research Group},
	title = {{On Stefan Banach and some of his results}},
	url = {https://doi.org/10.15352/bjma/1240321550},
	volume = {1},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.15352/bjma/1240321550}}

@article{banach,
	author = {Stefan Banach},
	journal = {Fundamenat Mathematicae},
	number = {3},
    volume = {1},
	pages = {133 -- 181},
	title = {{Sur les op\'erations dans les ensembles abstratits et leur applications aux \'equations int\'egrales}},
	year = {1922}
	}


@inproceedings{NEURIPS2020_4c5bcfec,
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {6840--6851},
	publisher = {Curran Associates, Inc.},
	title = {Denoising Diffusion Probabilistic Models},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf}}



@inproceedings{NEURIPS2019_6e79ed05,
	author = {Zhang, Yan and Hare, Jonathon and Prugel-Bennett, Adam},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Deep Set Prediction Networks},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/6e79ed05baec2754e25b4eac73a332d2-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/6e79ed05baec2754e25b4eac73a332d2-Paper.pdf}}
