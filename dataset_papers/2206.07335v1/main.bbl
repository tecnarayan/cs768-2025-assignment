\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anderson et~al.(1996)Anderson, Kevrekidis, and
  Rico-Martinez]{anderson1996comparison}
Anderson, J., Kevrekidis, I., and Rico-Martinez, R.
\newblock A comparison of recurrent training algorithms for time series
  analysis and system identification.
\newblock \emph{Computers \& chemical engineering}, 20:\penalty0 S751--S756,
  1996.

\bibitem[Arnold(2013)]{arnold2013mathematical}
Arnold, V.~I.
\newblock \emph{Mathematical methods of classical mechanics}, volume~60.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Arnold et~al.(2007)Arnold, Kozlov, and
  Neishtadt]{arnold2007mathematical}
Arnold, V.~I., Kozlov, V.~V., and Neishtadt, A.~I.
\newblock \emph{Mathematical aspects of classical and celestial mechanics},
  volume~3.
\newblock Springer Science \& Business Media, 2007.

\bibitem[Bertalan et~al.(2019)Bertalan, Dietrich, Mezi{\'c}, and
  Kevrekidis]{bertalan2019learning}
Bertalan, T., Dietrich, F., Mezi{\'c}, I., and Kevrekidis, I.~G.
\newblock On learning hamiltonian systems from data.
\newblock \emph{Chaos: An Interdisciplinary Journal of Nonlinear Science},
  29\penalty0 (12):\penalty0 121107, 2019.

\bibitem[Botev et~al.(2021)Botev, Jaegle, Wirnsberger, Hennes, and
  Higgins]{botev2021priors}
Botev, A., Jaegle, A., Wirnsberger, P., Hennes, D., and Higgins, I.
\newblock Which priors matter? benchmarking models for learning latent
  dynamics.
\newblock In \emph{35th Conference on Neural Information Processing Systems
  (NeurIPS 2021) Track on Datasets and Benchmarks}, 2021.

\bibitem[Burckel(1980)]{burckel1980introduction}
Burckel, R.~B.
\newblock \emph{An introduction to classical complex analysis}, volume~1.
\newblock Academic Press, 1980.

\bibitem[Chartier et~al.(2007)Chartier, Hairer, and
  Vilmart]{chartier2007numerical}
Chartier, P., Hairer, E., and Vilmart, G.
\newblock Numerical integrators based on modified differential equations.
\newblock \emph{Mathematics of computation}, 76\penalty0 (260):\penalty0
  1941--1953, 2007.

\bibitem[Chen \& Tao(2021)Chen and Tao]{chen2021data}
Chen, R. and Tao, M.
\newblock Data-driven prediction of general hamiltonian dynamics via learning
  exactly-symplectic maps.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning ({ICML} 2021)}, volume 139, pp.\  1717--1727. {PMLR}, 2021.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and
  Duvenaud]{chen2018neural}
Chen, T., Rubanova, Y., Bettencourt, J., and Duvenaud, D.
\newblock Neural ordinary differential equations.
\newblock In \emph{32nd Conference on Neural Information Processing Systems
  (NeurIPS 2018)}, pp.\  6572--6583, 2018.

\bibitem[Du et~al.(2021)Du, Gu, Yang, and Zhou]{du2021discovery}
Du, Q., Gu, Y., Yang, H., and Zhou, C.
\newblock The discovery of dynamics via linear multistep methods and deep
  learning: Error estimation.
\newblock \emph{arXiv preprint arXiv:2103.11488}, 2021.

\bibitem[Dupont et~al.(2019)Dupont, Doucet, and Teh]{dupont2019augmented}
Dupont, E., Doucet, A., and Teh, Y.~W.
\newblock Augmented neural odes.
\newblock In \emph{33rd Conference on Neural Information Processing Systems
  (NeurIPS 2019)}, pp.\  3134--3144, 2019.

\bibitem[E(2017)]{e2017proposal}
E, W.
\newblock A proposal on machine learning via dynamical systems.
\newblock \emph{Communications in Mathematics and Statistics}, 5\penalty0
  (1):\penalty0 1--11, 2017.

\bibitem[E et~al.(2019)E, Han, and Li]{e2019mean}
E, W., Han, J., and Li, Q.
\newblock A mean-field optimal control formulation of deep learning.
\newblock \emph{Research in the Mathematical Sciences}, 6\penalty0
  (1):\penalty0 1--41, 2019.

\bibitem[Eirola(1993)]{eirola1993aspects}
Eirola, T.
\newblock Aspects of backward error analysis of numerical odes.
\newblock \emph{Journal of Computational and Applied Mathematics}, 45\penalty0
  (1-2):\penalty0 65--73, 1993.

\bibitem[Feng(1984)]{feng1984on}
Feng, K.
\newblock On difference schemes and symplectic geometry.
\newblock In \emph{Proceedings of the 5th International Symposium on
  differential geometry and differential equations, August 1984 Beijing,
  {China}}, pp.\  42--58. Science Press, Beijing, 1984.

\bibitem[Feng(1986)]{feng1986difference}
Feng, K.
\newblock Difference schemes for {H}amiltonian formalism and symplectic
  geometry.
\newblock \emph{Journal of Computational Mathematics}, 4\penalty0 (3):\penalty0
  279--289, 1986.

\bibitem[Feng(1991)]{feng1991formal}
Feng, K.
\newblock Formal power series and numerical algorithms for dynamical systems.
\newblock In \emph{Proceedings of international conference on scientific
  computation, Hangzhou, China, Series on Appl. Math. Singapore: World
  Scientific}, volume~1, pp.\  28--35, 1991.

\bibitem[Feng(1993)]{feng1993formal}
Feng, K.
\newblock Formal dynamical systems and numerical algorithms.
\newblock \emph{SERIES ON APPLIED MATHEMATICS}, 4:\penalty0 1--10, 1993.

\bibitem[Fran{\c{c}}a et~al.(2021)Fran{\c{c}}a, Jordan, and Vidal]{fran2021on}
Fran{\c{c}}a, G., Jordan, M.~I., and Vidal, R.
\newblock On dissipative symplectic integration with applications to
  gradient-based optimization.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2021\penalty0 (4):\penalty0 043402, apr 2021.

\bibitem[Gonz{\'a}lez-Garc{\'\i}a et~al.(1998)Gonz{\'a}lez-Garc{\'\i}a,
  Rico-Mart{\`\i}nez, and Kevrekidis]{gonzalez1998identification}
Gonz{\'a}lez-Garc{\'\i}a, R., Rico-Mart{\`\i}nez, R., and Kevrekidis, I.~G.
\newblock Identification of distributed parameter systems: A neural net based
  approach.
\newblock \emph{Computers \& chemical engineering}, 22:\penalty0 S965--S968,
  1998.

\bibitem[Greydanus et~al.(2019)Greydanus, Dzamba, and
  Yosinski]{greydanus2019hamiltonian}
Greydanus, S., Dzamba, M., and Yosinski, J.
\newblock {H}amiltonian neural networks.
\newblock In \emph{33rd Conference on Neural Information Processing Systems
  (NeurIPS 2019)}, pp.\  15353--15363, 2019.

\bibitem[Gusak et~al.(2020)Gusak, Markeeva, Daulbaev, Katrutsa, Cichocki, and
  Oseledets]{gusak2020towards}
Gusak, J., Markeeva, L., Daulbaev, T., Katrutsa, A., Cichocki, A., and
  Oseledets, I.
\newblock Towards understanding normalization in neural {ODE}s.
\newblock In \emph{ICLR 2020 Workshop on Integration of Deep Neural Models and
  Differential Equations}, 2020.

\bibitem[Hairer \& Lubich(1997)Hairer and Lubich]{hairer1997life}
Hairer, E. and Lubich, C.
\newblock The life-span of backward error analysis for numerical integrators.
\newblock \emph{Numerische Mathematik}, 76\penalty0 (4):\penalty0 441--462,
  1997.

\bibitem[Hairer et~al.(2006)Hairer, Lubich, and Wanner]{hairer2006geometric}
Hairer, E., Lubich, C., and Wanner, G.
\newblock \emph{Geometric numerical integration: structure-preserving
  algorithms for ordinary differential equations}, volume~31.
\newblock Springer Science \& Business Media, 2006.

\bibitem[Howard(1998)]{howard1998gronwall}
Howard, R.
\newblock The gronwall inequality.
\newblock \emph{lecture notes}, 1998.

\bibitem[Huh et~al.(2020)Huh, Yang, Hwang, and Shin]{huh2020time}
Huh, I., Yang, E., Hwang, S.~J., and Shin, J.
\newblock Time-reversal symmetric {ODE} network.
\newblock In \emph{34th Conference on Neural Information Processing Systems
  (NeurIPS 2020)}, 2020.

\bibitem[Jia \& Benson(2019)Jia and Benson]{jia2019neural}
Jia, J. and Benson, A.~R.
\newblock Neural jump stochastic differential equations.
\newblock In \emph{33rd Conference on Neural Information Processing Systems
  (NeurIPS 2019)}, pp.\  9843--9854, 2019.

\bibitem[Jin et~al.(2020)Jin, Zhang, Zhu, Tang, and
  Karniadakis]{jin2020sympnets}
Jin, P., Zhang, Z., Zhu, A., Tang, Y., and Karniadakis, G.~E.
\newblock Symp{N}ets: Intrinsic structure-preserving symplectic networks for
  identifying {H}amiltonian systems.
\newblock \emph{Neural Networks}, 132:\penalty0 166--179, 2020.
\newblock ISSN 0893-608.

\bibitem[Keller \& Du(2021)Keller and Du]{keller2021discovery}
Keller, R.~T. and Du, Q.
\newblock Discovery of dynamics using linear multistep methods.
\newblock \emph{SIAM Journal on Numerical Analysis}, 59\penalty0 (1):\penalty0
  429--455, 2021.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{3rd International Conference on Learning Representations},
  2015.

\bibitem[Krishnapriyan et~al.(2022)Krishnapriyan, Queiruga, Erichson, and
  Mahoney]{krishnapriyan2022learning}
Krishnapriyan, A.~S., Queiruga, A.~F., Erichson, N.~B., and Mahoney, M.~W.
\newblock Learning continuous models for continuous physics.
\newblock \emph{arXiv preprint arXiv:2202.08494}, 2022.

\bibitem[Li et~al.(2017)Li, Chen, Tai, and E]{li2017maximum}
Li, Q., Chen, L., Tai, C., and E, W.
\newblock Maximum principle based algorithms for deep learning.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 5998--6026, 2017.

\bibitem[Lu et~al.(2018)Lu, Zhong, Li, and Dong]{lu2018beyond}
Lu, Y., Zhong, A., Li, Q., and Dong, B.
\newblock Beyond finite layer neural networks: Bridging deep architectures and
  numerical differential equations.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning ({ICML} 2018)}, volume~80, pp.\  3282--3291. {PMLR}, 2018.

\bibitem[Luo et~al.(2019)Luo, Ma, Xu, and Zhang]{luo2019theory}
Luo, T., Ma, Z., Xu, Z.~J., and Zhang, Y.
\newblock Theory of the frequency principle for general deep neural networks.
\newblock \emph{arXiv preprint arXiv:1906.09235}, 2019.

\bibitem[Massaroli et~al.(2020)Massaroli, Poli, Park, Yamashita, and
  Asama]{massaroli2020dissecting}
Massaroli, S., Poli, M., Park, J., Yamashita, A., and Asama, H.
\newblock Dissecting neural odes.
\newblock In \emph{34th Conference on Neural Information Processing Systems
  (NeurIPS 2020)}, 2020.

\bibitem[Norcliffe et~al.(2021)Norcliffe, Bodnar, Day, Moss, and
  Li{\`{o}}]{norcliffe2021neural}
Norcliffe, A., Bodnar, C., Day, B., Moss, J., and Li{\`{o}}, P.
\newblock Neural {ODE} processes.
\newblock In \emph{9th International Conference on Learning Representations
  ({ICLR} 2021)}, 2021.

\bibitem[Ott et~al.(2021)Ott, Katiyar, Hennig, and Tiemann]{ott2021resnet}
Ott, K., Katiyar, P., Hennig, P., and Tiemann, M.
\newblock Resnet after all: Neural {ODE}s and their numerical solution.
\newblock In \emph{9th International Conference on Learning Representations
  ({ICLR} 2021)}, 2021.

\bibitem[Pal et~al.(2021)Pal, Ma, Shah, and Rackauckas]{pal2021opening}
Pal, A., Ma, Y., Shah, V.~B., and Rackauckas, C.~V.
\newblock Opening the blackbox: Accelerating neural differential equations by
  regularizing internal solver heuristics.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning ({ICML} 2021)}, volume 139, pp.\  8325--8335. {PMLR}, 2021.

\bibitem[Poli et~al.(2020)Poli, Massaroli, Yamashita, Asama, and
  Park]{poli2020hypersolver}
Poli, M., Massaroli, S., Yamashita, A., Asama, H., and Park, J.
\newblock Hypersolvers: Toward fast continuous-depth models.
\newblock In \emph{34th Conference on Neural Information Processing Systems
  (NeurIPS 2020)}, 2020.

\bibitem[Queiruga et~al.(2020)Queiruga, Erichson, Taylor, and
  Mahoney]{queiruga2020continuous}
Queiruga, A.~F., Erichson, N.~B., Taylor, D., and Mahoney, M.~W.
\newblock Continuous-in-depth neural networks.
\newblock \emph{arXiv preprint arXiv:2008.02389}, 2020.

\bibitem[Raissi et~al.(2018)Raissi, Perdikaris, and
  Karniadakis]{raissi2018multistep}
Raissi, M., Perdikaris, P., and Karniadakis, G.~E.
\newblock Multistep neural networks for data-driven discovery of nonlinear
  dynamical systems.
\newblock \emph{arXiv preprint arXiv:1801.01236}, 2018.

\bibitem[Reich(1999)]{reich1999backward}
Reich, S.
\newblock Backward error analysis for numerical integrators.
\newblock \emph{SIAM Journal on Numerical Analysis}, 36\penalty0 (5):\penalty0
  1549--1570, 1999.

\bibitem[Rico-Martinez \& Kevrekidis(1993)Rico-Martinez and
  Kevrekidis]{rico1993continuous}
Rico-Martinez, R. and Kevrekidis, I.~G.
\newblock Continuous time modeling of nonlinear systems: A neural network-based
  approach.
\newblock In \emph{IEEE International Conference on Neural Networks}, pp.\
  1522--1525. IEEE, 1993.

\bibitem[Rico-Martinez et~al.(1994)Rico-Martinez, Anderson, and
  Kevrekidis]{rico1994continuous}
Rico-Martinez, R., Anderson, J., and Kevrekidis, I.
\newblock Continuous-time nonlinear signal processing: a neural network based
  approach for gray box identification.
\newblock In \emph{Proceedings of IEEE Workshop on Neural Networks for Signal
  Processing}, pp.\  596--605. IEEE, 1994.

\bibitem[Rubanova et~al.(2019)Rubanova, Chen, and Duvenaud]{rubanova2019latent}
Rubanova, Y., Chen, T.~Q., and Duvenaud, D.
\newblock Latent ordinary differential equations for irregularly-sampled time
  series.
\newblock In \emph{33rd Conference on Neural Information Processing Systems
  (NeurIPS 2019)}, pp.\  5321--5331, 2019.

\bibitem[Sanz-Serna(1992)]{sanz1992symplectic}
Sanz-Serna, J.~M.
\newblock Symplectic integrators for hamiltonian problems: an overview.
\newblock \emph{Acta numerica}, 1:\penalty0 243--286, 1992.

\bibitem[Scheidemann(2005)]{scheidemann2005introduction}
Scheidemann, V.
\newblock \emph{Introduction to complex analysis in several variables}.
\newblock Springer, 2005.

\bibitem[Sonoda \& Murata(2019)Sonoda and Murata]{sonoda2019transport}
Sonoda, S. and Murata, N.
\newblock Transport analysis of infinitely deep neural network.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (2):\penalty0 1--52, 2019.

\bibitem[Xia et~al.(2021)Xia, Suliafu, Ji, Nguyen, Bertozzi, Osher, and
  Wang]{xia2021heavy}
Xia, H., Suliafu, V., Ji, H., Nguyen, T.~M., Bertozzi, A.~L., Osher, S.~J., and
  Wang, B.
\newblock Heavy ball neural ordinary differential equations.
\newblock In \emph{35th Conference on Neural Information Processing Systems
  (NeurIPS 2021)}, 2021.

\bibitem[Xu et~al.(2019)Xu, Zhang, and Xiao]{xu2019training}
Xu, Z.~J., Zhang, Y., and Xiao, Y.
\newblock Training behavior of deep neural network in frequency domain.
\newblock In \emph{Neural Information Processing - 26th International
  Conference, {ICONIP}}, pp.\  264--274. Springer, 2019.

\bibitem[Yan et~al.(2020)Yan, Du, Tan, and Feng]{yan2020on}
Yan, H., Du, J., Tan, V., and Feng, J.
\newblock On robustness of neural ordinary differential equations.
\newblock In \emph{8th International Conference on Learning Representations,
  ({ICLR},2020)}, 2020.

\bibitem[Yildiz et~al.(2019)Yildiz, Heinonen, and
  L{\"{a}}hdesm{\"{a}}ki]{yildiz2019ode2vae}
Yildiz, C., Heinonen, M., and L{\"{a}}hdesm{\"{a}}ki, H.
\newblock {ODE2VAE:} deep generative second order odes with bayesian neural
  networks.
\newblock In \emph{33rd Conference on Neural Information Processing Systems
  (NeurIPS 2019)}, pp.\  13412--13421, 2019.

\bibitem[Yoshida(1993)]{yoshida1993recent}
Yoshida, H.
\newblock Recent progress in the theory and application of symplectic
  integrators.
\newblock \emph{Qualitative and Quantitative Behaviour of Planetary Systems},
  pp.\  27--43, 1993.

\bibitem[Yu et~al.(2021)Yu, Tian, E, and Li]{yu2021onsagernet}
Yu, H., Tian, X., E, W., and Li, Q.
\newblock Onsagernet: Learning stable and interpretable dynamics using a
  generalized onsager principle.
\newblock \emph{Physical Review Fluids}, 6\penalty0 (11):\penalty0 114402,
  2021.

\bibitem[Zhang et~al.(2020)Zhang, Gao, Unterman, and
  Arodz]{zhang2020approximation}
Zhang, H., Gao, X., Unterman, J., and Arodz, T.
\newblock Approximation capabilities of neural odes and invertible residual
  networks.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning ({ICML} 2020)}, volume 119, pp.\  11086--11095. {PMLR}, 2020.

\bibitem[Zhang et~al.(2019)Zhang, Yao, Gholami, Gonzalez, Keutzer, Mahoney, and
  Biros]{zhang2019anode}
Zhang, T., Yao, Z., Gholami, A., Gonzalez, J.~E., Keutzer, K., Mahoney, M.~W.,
  and Biros, G.
\newblock {ANODEV2:} {A} coupled neural {ODE} framework.
\newblock In \emph{33rd Conference on Neural Information Processing Systems
  (NeurIPS 2019)}, pp.\  5152--5162, 2019.

\bibitem[Zhuang et~al.(2020)Zhuang, Dvornek, Li, Tatikonda, Papademetris, and
  Duncan]{zhuang2020adaptive}
Zhuang, J., Dvornek, N.~C., Li, X., Tatikonda, S., Papademetris, X., and
  Duncan, J.~S.
\newblock Adaptive checkpoint adjoint method for gradient estimation in neural
  {ODE}.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning ({ICML} 2020)}, volume 119, pp.\  11639--11649. {PMLR}, 2020.

\end{thebibliography}
