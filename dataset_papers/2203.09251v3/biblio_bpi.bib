@misc{rlberry,
author = {Domingues, Omar Darwiche and Flet-Berliac, Yannis and Leurent, Edouard and M{\'e}nard, Pierre and Shang, Xuedong and Valko, Michal},
title = {{rlberry - A Reinforcement Learning Library for Research and Education}},
year = {2021},
publisher = {GitHub},
journal = {GitHub repository},
howpublished = {\url{https://github.com/rlberry-py/rlberry}},
}

@inproceedings{Yin2021TowardsIO,
  title={Towards Instance-Optimal Offline Reinforcement Learning with Pessimism},
  author={Ming Yin and Yu-Xiang Wang},
  booktitle={NeurIPS},
  year={2021}
}

@article{tirinzoni2021fully,
  title={A Fully Problem-Dependent Regret Lower Bound for Finite-Horizon MDPs},
  author={Tirinzoni, Andrea and Pirotta, Matteo and Lazaric, Alessandro},
  journal={arXiv preprint arXiv:2106.13013},
  year={2021}
}

@article{agarwal2020pc,
  title={Pc-pg: Policy cover directed exploration for provable policy gradient learning},
  author={Agarwal, Alekh and Henaff, Mikael and Kakade, Sham and Sun, Wen},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13399--13412},
  year={2020}
}

@article{efroni2020exploration,
  title={Exploration-exploitation in constrained mdps},
  author={Efroni, Yonathan and Mannor, Shie and Pirotta, Matteo},
  journal={arXiv preprint arXiv:2003.02189},
  year={2020}
}

@article{auer2008near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}

@inproceedings{degenne2019non,
  title={Non-asymptotic pure exploration by solving games},
  author={Degenne, R{\'e}my and Koolen, Wouter M and M{\'e}nard, Pierre},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14492--14501},
  year={2019}
}

@inproceedings{wang2021fast,
  title={Fast Pure Exploration via Frank-Wolfe},
  author={Wang, Po-An and Tzeng, Ruo-Chun and Proutiere, Alexandre},
  booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
  year={2021}
}

@article{tirinzoni2020asymptotically,
  title={An asymptotically optimal primal-dual incremental algorithm for contextual linear bandits},
  author={Tirinzoni, Andrea and Pirotta, Matteo and Restelli, Marcello and Lazaric, Alessandro},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1417--1427},
  year={2020}
}

@article{adlakha1991minimum,
  title={Minimum flows in (s, t) planar networks},
  author={Adlakha, Veena and Gladysz, Barbara and Kamburowski, Jerzy},
  journal={Networks},
  volume={21},
  number={7},
  pages={767--773},
  year={1991},
  publisher={Wiley Online Library}
}



@InProceedings{Grill16TrailBlazer,
  Title                    = {Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning},
  Author                   = {Grill, J.-B. and Valko, M. and Munos, R.},
  Booktitle                = {Neural Information Processing Systems (NeurIPS)},
  Year                     = {2016},

  Owner                    = {emilie},
  Timestamp                = {2016.10.17}
}

@article{nemhauser1978analysis,
  title={An analysis of approximations for maximizing submodular set functions—I},
  author={Nemhauser, George L and Wolsey, Laurence A and Fisher, Marshall L},
  journal={Mathematical programming},
  volume={14},
  number={1},
  pages={265--294},
  year={1978},
  publisher={Springer}
}

@inproceedings{bubeck2010olop,
  title={Open Loop Optimistic Planning},
  author={Bubeck, S and Munos, R},
  booktitle={Conference on Learning Theory},
  year={2010}
}

@article{krause2014submodular,
  title={Submodular function maximization.},
  author={Krause, Andreas and Golovin, Daniel},
  journal={Tractability},
  volume={3},
  pages={71--104},
  year={2014}
}

@article{adlakha1999alternate,
  title={An alternate linear algorithm for the minimum flow problem},
  author={Adlakha, Veena G},
  journal={Journal of the Operational Research Society},
  volume={50},
  number={2},
  pages={177--182},
  year={1999},
  publisher={Springer}
}

@article{voitishin1980algorithms,
  title={Algorithms for solving for the minimal flow in a network},
  author={Voitishin, Yu V},
  journal={Cybernetics},
  volume={16},
  number={1},
  pages={131--134},
  year={1980},
  publisher={Springer}
}

@article{garivier2021nonasymptotic,
  title={Nonasymptotic sequential tests for overlapping hypotheses applied to near-optimal arm identification in bandit models},
  author={Garivier, Aur{\'e}lien and Kaufmann, Emilie},
  journal={Sequential Analysis},
  volume={40},
  number={1},
  pages={61--96},
  year={2021},
  publisher={Taylor \& Francis}
}

@article{brandizi2012graph2tab,
  title={graph2tab, a library to convert experimental workflow graphs into tabular formats},
  author={Brandizi, Marco and Kurbatova, Natalja and Sarkans, Ugis and Rocca-Serra, Philippe},
  journal={Bioinformatics},
  volume={28},
  number={12},
  pages={1665--1667},
  year={2012},
  publisher={Oxford University Press}
}

@article{jonsson2020planning,
  title={Planning in markov decision processes with gap-dependent sample complexity},
  author={Jonsson, Anders and Kaufmann, Emilie and M{\'e}nard, Pierre and Darwiche Domingues, Omar and Leurent, Edouard and Valko, Michal},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1253--1263},
  year={2020}
}

@article{ciurea2004sequential,
  title={Sequential and parallel algorithms for minimum flows},
  author={Ciurea, Eleonor and Ciupala, Laura},
  journal={Journal of Applied Mathematics and Computing},
  volume={15},
  number={1},
  pages={53--75},
  year={2004},
  publisher={Springer}
}

@article{ciurea2001algorithms,
  title={Algorithms for minimum flows},
  author={Ciurea, Eleonor and Ciupal, Laura},
  journal={Computer Science},
  volume={9},
  number={3},
  pages={27},
  year={2001}
}

@article{wang2020long,
  title={Is Long Horizon Reinforcement Learning More Difficult Than Short Horizon Reinforcement Learning?},
  author={Wang, Ruosong and Du, Simon S and Yang, Lin F and Kakade, Sham M},
  journal={arXiv preprint arXiv:2005.00527},
  year={2020}
}

@inproceedings{dann2019policy,
  title={Policy certificates: Towards accountable reinforcement learning},
  author={Dann, Christoph and Li, Lihong and Wei, Wei and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={1507--1516},
  year={2019},
  organization={PMLR}
}

@article{hazan2018provably,
    title={Provably Efficient Maximum Entropy Exploration},
    author={Elad Hazan and Sham M. Kakade and Karan Singh and Abby Van Soest},
    year={2018},
    journal={arXiv preprint arXiv:1812.02690},
}

@article{dann21ReturnGap,
  author    = {Christoph Dann and
               Teodor V. Marinov and
               Mehryar Mohri and
               Julian Zimmert},
  title     = {Beyond Value-Function Gaps: Improved Instance-Dependent Regret Bounds
               for Episodic Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/2107.01264},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.01264},
  eprinttype = {arXiv},
  eprint    = {2107.01264},
}



@inproceedings{wagenmaker21IDPAC,
  author    = {Andrew Wagenmaker and
               Max Simchowitz and
               Kevin G. Jamieson},
  title     = {Beyond No Regret: Instance-Dependent {PAC} Reinforcement Learning},
  booktitle   = {Conference On Learning Theory (COLT)},
  year      = {2022}
}

a service of  Schloss Dagstuhl - Leibniz Center for Informatics

    home
    browse
    search
    about

@inproceedings{Tranos21,
  author    = {Damianos Tranos and
               Alexandre Prouti{\`{e}}re},
  title     = {Regret Analysis in Deterministic Reinforcement Learning},
  booktitle = {{CDC}},
  publisher = {{IEEE}},
  year      = {2021}
}

@inproceedings{DegenneK19,
  author    = {R{\'{e}}my Degenne and
               Wouter M. Koolen},
  title     = {Pure Exploration with Multiple Correct Answers},
  booktitle = {NeurIPS},
  year      = {2019}
}

@Article{MannorTsi04,
  Title                    = {{The Sample Complexity of Exploration in the Multi-Armed Bandit Problem}},
  Author                   = {Mannor, S. and Tsitsiklis, J.},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2004},
  Pages                    = {623--648}
}


@inproceedings{tang2017exploration,
  title={\# exploration: A study of count-based exploration for deep reinforcement learning},
  author={Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Chen, Xi and Duan, Yan and Schulman, John and DeTurck, Filip and Abbeel, Pieter},
  booktitle={Advances in neural information processing systems},
  pages={2753--2762},
  year={2017}
}

@article{strehl2009reinforcement,
  title={Reinforcement Learning in Finite MDPs: PAC Analysis.},
  author={Strehl, Alexander L and Li, Lihong and Littman, Michael L},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={11},
  year={2009}
}

@inproceedings{strehl2006pac,
  title={PAC model-free reinforcement learning},
  author={Strehl, Alexander L and Li, Lihong and Wiewiora, Eric and Langford, John and Littman, Michael L},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={881--888},
  year={2006}
}

@inproceedings{Du21OptimalRFE,
  author    = {Zihan Zhang and
               Simon Du and
               Xiangyang Ji},
  title     = {Near Optimal Reward-Free Reinforcement Learning},
  booktitle = {International Conference on Machine Learning,
               (ICML)},
year      = {2021}
}

@inproceedings{Omar21LB,
  author    = {Omar Darwiche Domingues and
               Pierre M{\'{e}}nard and
               Emilie Kaufmann and
               Michal Valko},
  title     = {Episodic Reinforcement Learning in Finite MDPs: Minimax Lower Bounds
               Revisited},
  booktitle = {Algorithmic Learning Theory (ALT)},
  year      = {2021}
}

@inproceedings{Kaufmann21RFE,
  author    = {Emilie Kaufmann and
               Pierre M{\'{e}}nard and
               Omar Darwiche Domingues and
               Anders Jonsson and
               Edouard Leurent and
               Michal Valko},
  title     = {Adaptive Reward-Free Exploration},
  booktitle = {Algorithmic Learning Theory (ALT)},
  year      = {2021}
}


@inproceedings{Menard21RFE,
  author    = {Pierre M{\'{e}}nard and
               Omar Darwiche Domingues and
               Anders Jonsson and Emilie Kaufmann and
               Edouard Leurent and
               Michal Valko},
  title     = {Fast active learning for pure exploration in reinforcement learning},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2021}
}


@inproceedings{Du21HorizonFreeRegret,
  author    = {Zihan Zhang and Xiangyang Ji and Simon Du},
  title     = {Is Reinforcement Learning More Difficult Than Bandits? A Near-optimal Algorithm Escaping the Curse of Horizon},
  booktitle = {International Conference on Learning Theory (COLT)},
  year      = {2021}
}

@inproceedings{ostrovski2017count,
  title={Count-based exploration with neural density models},
  author={Ostrovski, Georg and Bellemare, Marc G and van den Oord, A{\"a}ron and Munos, R{\'e}mi},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2721--2730},
  year={2017},
  organization={JMLR. org}
}


@misc{gajane2019autonomous,
    title={Autonomous exploration for navigating in non-stationary CMPs},
    author={Pratik Gajane and Ronald Ortner and Peter Auer and Csaba Szepesvari},
    year={2019},
    eprint={1910.08446},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@inproceedings{lim2012autonomous,
  title={Autonomous exploration for navigating in mdps},
  author={Lim, Shiau Hong and Auer, Peter},
  booktitle={Conference on Learning Theory},
  pages={40--1},
  year={2012}
}



@article{cohen2020near,
  title={Near-optimal Regret Bounds for Stochastic Shortest Path},
  author={Cohen, Alon and Kaplan, Haim and Mansour, Yishay and Rosenberg, Aviv},
  journal={arXiv preprint arXiv:2002.09869},
  year={2020}
}

@article{tarbouriech2019no,
  title={No-Regret Exploration in Goal-Oriented Reinforcement Learning},
  author={Tarbouriech, Jean and Garcelon, Evrard and Valko, Michal and Pirotta, Matteo and Lazaric, Alessandro},
  journal={arXiv preprint arXiv:1912.03517},
  year={2019}
}


@inproceedings{schmidhuber1991possibility,
  title={A possibility for implementing curiosity and boredom in model-building neural controllers},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={Proc. of the international conference on simulation of adaptive behavior: From animals to animats},
  pages={222--227},
  year={1991}
}


@inproceedings{hren2008optimistic,
	title = {{Optimistic planning of deterministic systems}},
	year = {2008},
	booktitle = {European Workshop on Reinforcement Learning},
	author = {Hren, Jean-Francois and Munos, Rémi}
}

@inproceedings{coquelin2007bandit,
	title = {{Bandit algorithms for tree search}},
	year = {2007},
	booktitle = {Uncertainty in Artificial Intelligence},
	author = {Coquelin, Pierre-Arnaud and Munos, Rémi},
	url = {https://arxiv.org/pdf/1408.2028.pdf}
}

@inproceedings{Zanette19Euler,
  author    = {Andrea Zanette and
               Emma Brunskill},
  title     = {Tighter Problem-Dependent Regret Bounds in Reinforcement Learning
               without Domain Knowledge using Value Function Bounds},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               (ICML)},
  year      = {2019}
}

@inproceedings{
hamrick2019combining,
title={Combining Q-Learning and Search with Amortized Value Estimates},
author={Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez-Gonzalez and Tobias Pfaff and Theophane Weber and Lars Buesing and Peter W. Battaglia},
booktitle={International Conference on Learning Representations},
year={2020}
}

@article{kearns02E3,
  author    = {Michael J. Kearns and
               Satinder P. Singh},
  title     = {Near-Optimal Reinforcement Learning in Polynomial Time},
  journal   = {Machine Learning},
  volume    = {49},
  number    = {2-3},
  pages     = {209--232},
  year      = {2002}
}


@book{cover2012elements,
  title={Elements of information theory},
  author={Cover, Thomas M and Thomas, Joy A},
  year={2012},
  publisher={John Wiley \& Sons}
}


@article{de2004self,
  title={Self-normalized processes: exponential inequalities, moment bounds and iterated logarithm laws},
  author={de la Pena, Victor H and Klass, Michael J and Lai, Tze Leung},
  journal={Annals of probability},
  pages={1902--1933},
  year={2004},
  publisher={JSTOR}
}

@inproceedings{garivier2011kl,
  title={The KL-UCB algorithm for bounded stochastic bandits and beyond},
  author={Garivier, Aur{\'e}lien and Capp{\'e}, Olivier},
  booktitle={Proceedings of the 24th annual conference on learning theory},
  pages={359--376},
  year={2011}
}

@Article{KLUCBJournal,
  Title                    = {{{K}ullback-{L}eibler upper confidence bounds for optimal sequential allocation}},
  Author                   = {Capp{\'e}, O. and Garivier, A. and Maillard, O-A. and Munos, R. and Stoltz, G.},
  Journal                  = {Annals of Statistics},
  Year                     = {2013},
  Pages                    = {1516--1541},
  Volume                   = {41(3)}
}

@inproceedings{dann2017unifying,
  title={Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning},
  author={Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5713--5723},
  year={2017}
}

@article{Jin20RewardFree,
  author    = {Chi Jin and
               Akshay Krishnamurthy and
               Max Simchowitz and
               Tiancheng Yu},
  title     = {Reward-Free Exploration for Reinforcement Learning},
  journal   = {arXiv:2002.02794},
  year      = {2020}
}

@inproceedings{Jin18OptQL,
  author    = {Chi Jin and
               Zeyuan Allen{-}Zhu and
               S{\'{e}}bastien Bubeck and
               Michael I. Jordan},
  title     = {Is Q-Learning Provably Efficient?},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2018}
}


@article{garivier2018kl,
  title={KL-UCB-switch: optimal regret bounds for stochastic bandits from both a distribution-dependent and a distribution-free viewpoints},
  author={Garivier, Aur{\'e}lien and Hadiji, H{\'e}di and Menard, Pierre and Stoltz, Gilles},
  journal={arXiv preprint arXiv:1805.05071},
  year={2018}
}

@Book{Puterman94MDP,
  Title                    = {{Markov Decision Processes. Discrete Stochastic. Dynamic Programming.}},
  Author                   = {Puterman, M.L.},
  Publisher                = {Wiley},
  Year                     = {1994}
}

@Book{SurveyRemiMCTS,
  Title                    = {From bandits to Monte-Carlo Tree Search: The optimistic principle applied to optimization and planning.},
  Author                   = {Munos, R.},
  Publisher                = {Foundations and Trends in Machine Learning},
  Year                     = {2014},
  Number                   = {1},
  Volume                   = {7}
}

@inproceedings{SmoothCruiser19,
  author    = {Jean{-}Bastien Grill and
               Omar Darwiche Domingues and
               Pierre M{\'{e}}nard and
               R{\'{e}}mi Munos and
               Michal Valko},
  title     = {Planning in entropy-regularized Markov decision processes and games},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2019}
}

@inproceedings{Huang17StructuredBAI,
  author    = {Ruitong Huang and
               Mohammad M. Ajallooeian and
               Csaba Szepesv{\'{a}}ri and
               Martin M{\"{u}}ller},
  title     = {Structured Best Arm Identification with Fixed Confidence},
  booktitle = {International Conference on Algorithmic Learning Theory (ALT)},
  year      = {2017}
}

@inproceedings{BAIMCTS17,
  author    = {Emilie Kaufmann and
               Wouter M. Koolen},
  title     = {{M}Onte-{C}Arlo Tree Search by Best Arm Identification},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2017},
}

@inproceedings{garivier2016optimal,
  title={Optimal best arm identification with fixed confidence},
  author={Garivier, Aur{\'e}lien and Kaufmann, Emilie},
  booktitle={Conference on Learning Theory},
  pages={998--1027},
  year={2016},
  organization={PMLR}
}

@InProceedings{Kocsis06UCT,
  Title                    = {Bandit Based Monte-carlo Planning},
  Author                   = {Kocsis, Levente and Szepesv\'{a}ri, Csaba},
  Booktitle                = {Proceedings of the 17th European Conference on Machine Learning (ECML)},
  Year                     = {2006}
}


@Article{EvenDaral06,
  Title                    = {{Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems}},
  Author                   = {Even-Dar, E. and Mannor, S. and Mansour, Y.},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2006},
  Pages                    = {1079--1105},
  Volume                   = {7}
}


@book{BanditBook,
author = {Lattimore, Tor and Szepesvari, Csaba},
publisher = {Cambridge University Press},
title = {{Bandit Algorithms}},
year = {2019}
}

@Article{Aueral02,
  Title                    = {{Finite-time analysis of the multiarmed bandit problem}},
  Author                   = {Auer, P. and Cesa-Bianchi, N. and Fischer, P.},
  Journal                  = {Machine Learning},
  Year                     = {2002},
  Number                   = {2},
  Pages                    = {235--256},
  Volume                   = {47},
  Publisher                = {Springer}
}

@article{UCRL10,
  author    = {Thomas Jaksch and
               Ronald Ortner and
               Peter Auer},
  title     = {Near-optimal Regret Bounds for Reinforcement Learning},
  journal   = {Journal of Machine Learning Research},
  volume    = {11},
  pages     = {1563--1600},
  year      = {2010}
}


@inproceedings{Tolpin12SRMCTS,
  author    = {David Tolpin and
               Solomon Eyal Shimony},
  title     = {{MCTS} Based on Simple Regret},
  booktitle = {Proceedings of the Twenty-Sixth {AAAI} Conference on Artificial Intelligence,
               July 22-26, 2012, Toronto, Ontario, Canada.},
  year      = {2012}
}

@inproceedings{Pepels14SimpleMCTS,
  author    = {Tom Pepels and
               Tristan Cazenave and
               Mark H. M. Winands and
               Marc Lanctot},
  title     = {Minimizing Simple and Cumulative Regret in Monte-Carlo Tree Search},
  booktitle = {Third Workshop on Computer Games (CGW)},
  pages     = {1--15},
  year      = {2014}
}

@article{Feldman14BRUE,
  author    = {Zohar Feldman and
               Carmel Domshlak},
  title     = {Simple Regret Optimization in Online Planning for Markov Decision
               Processes},
  journal   = {Journal of Artifial Intelligence Research},
  volume    = {51},
  pages     = {165--205},
  year      = {2014}
}

@inproceedings{Zhang20TAE,
  author    = {Xuezhou Zhang and
               Yuzhe Ma and
               Adish Singla},
  title     = {Task-agnostic Exploration in Reinforcement Learning},
  booktitle = {NeurIPS},
  year      = {2020}
}

@article{Kearns02SS,
  author    = {Michael J. Kearns and
               Yishay Mansour and
               Andrew Y. Ng},
  title     = {A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov
               Decision Processes},
  journal   = {Machine Learning},
  volume    = {49},
  number    = {2-3},
  pages     = {193--208},
  year      = {2002}
}

@InProceedings{STOP14,
  Title                    = {Optimistic Planning in Markov Decision Processes using a generative model},
  Author                   = {Szorenyi, B. and Kedenburg, G. and Munos, R.},
  Booktitle                = {Advances in Neural Information Processing Systems (NIPS)},
  Year                     = {2014}
}

@Book{SuttonBarto98,
  Title                    = {Reinforcement Learning: an Introduction},
  Author                   = {Sutton, R. and Barto, A.},
  Publisher                = {MIT press},
  Year                     = {1998},

  Owner                    = {emilie},
  Timestamp                = {2016.11.07}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{AlphaZero,
  author    = {David Silver and
               Thomas Hubert and
               Julian Schrittwieser and
               Ioannis Antonoglou and
               Matthew Lai and
               Arthur Guez and
               Marc Lanctot and
               Laurent Sifre and
               Dharshan Kumaran and
               Thore Graepel and
               Timothy P. Lillicrap and
               Karen Simonyan and
               Demis Hassabis},
  title     = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  journal   = {Science},
  volume    = {362},
  issue = {6419},
  page = {1140-1144},
  year      = {2018},
}

@inproceedings{dann15PAC,
  author    = {Christoph Dann and
               Emma Brunskill},
  title     = {Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2015}
}

@article{MDPGapE,
author = {Anders Jonsson and Emilie Kaufmann and Pierre M\'enard and Omar Darwiche-Domingues and Edouard Leurent and Michal Valko},
title = {Planning in Markov Decision Processes with Gap-Dependent Sample Complexity},
journal = {arXivXXXXX},
year = {2020}}

@article{MuZero,
  author    = {Julian Schrittwieser and
               Ioannis Antonoglou and
               Thomas Hubert and
               Karen Simonyan and
               Laurent Sifre and
               Simon Schmitt and
               Arthur Guez and
               Edward Lockhart and
               Demis Hassabis and
               Thore Graepel and
               Timothy P. Lillicrap and
               David Silver},
  title     = {Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model},
  journal   = {arXiv:1911.08265},
  year      = {2019}
}


@Article{SurveyMCTS12,
  Title                    = {A Survey of Monte Carlo Tree Search Methods},
  Author                   = {Browne, C. and Powley, E. and Whitehouse, D. and Lucas, S. and Cowling, P. and Rohlfshagen, P. and Tavener, S. and Perez, D. and Samothrakis, S. and Colton, S.},
  Journal                  = {IEEE Transactions on Computational Intelligence and AI in games,},
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {1-49},
  Volume                   = {4}
}


@InProceedings{TrailBlazer16,
  Title                    = {Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning},
  Author                   = {Grill, J.-B. and Valko, M. and Munos, R.},
  Booktitle                = {Neural Information Processing Systems (NIPS)},
  Year                     = {2016}
}



@inproceedings{gabillon2012best,
  title={Best arm identification: A unified approach to fixed budget and fixed confidence},
  author={Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3212--3220},
  year={2012}
}



@inproceedings{Azar17UCBVI,
  author    = {Mohammad Gheshlaghi Azar and
               Ian Osband and
               R{\'{e}}mi Munos},
  title     = {Minimax Regret Bounds for Reinforcement Learning},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning,
               (ICML) 2017},
  year      = {2017}
}


@inproceedings{Azar12SCGene,
  author    = {Mohammad Gheshlaghi Azar and
               R{\'{e}}mi Munos and
               Bert Kappen},
  title     = {On the Sample Complexity of Reinforcement Learning with a Generative
               Model},
  booktitle = {Proceedings of the 29th International Conference on Machine Learning (ICML)},
  year      = {2012}
}

@inproceedings{KearnsS98MBQVI,
  author    = {Michael J. Kearns and
               Satinder P. Singh},
  title     = {Finite-Sample Convergence Rates for Q-Learning and Indirect Algorithms},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  pages     = {996--1002},
  year      = {1998}
}


@inproceedings{Fiechter94,
  author    = {Claude{-}Nicolas Fiechter},
  title     = {Efficient Reinforcement Learning},
  booktitle = {Proceedings of the Seventh Conference on Computational
               Learning Theory (COLT)},
  year      = {1994}
}
@inproceedings{Fiechter97,
  author    = {Claude{-}Nicolas Fiechter},
  title     = {Expected Mistake Bound Model for On-Line Reinforcement Learning},
  booktitle = {Proceedings of the Fourteenth International Conference on Machine Learning (ICML)},
  year      = {1997}
}

@phdthesis{Kakade03PhD,
  author    = {Sham Kakade},
  title     = {On the Sample Complexity of Reinforcement Learning},
  school   = {University College London},
  year      = {2003}
}


@InProceedings{filippi2010optimism,
  Title                    = {{Optimism in Reinforcement Learning and {K}ullback-{L}eibler Divergence}},
  Author                   = {Filippi, S. and Capp{\'e}, O. and Garivier, A.},
  Booktitle                = {{Allerton Conference on Communication, Control, and Computing}},
  Year                     = {2010},
}

@article{Brafman02RMAX,
  author    = {Ronen I. Brafman and
               Moshe Tennenholtz},
  title     = {{R-MAX} - {A} General Polynomial Time Algorithm for Near-Optimal Reinforcement
               Learning},
  journal   = {Journal of Machine Learning Research},
  volume    = {3},
  pages     = {213--231},
  year      = {2002}
}

@inproceedings{Strehl06DelayedQL,
  author    = {Alexander L. Strehl and
               Lihong Li and
               Eric Wiewiora and
               John Langford and
               Michael L. Littman},
  title     = {{PAC} model-free reinforcement learning},
  booktitle = {Proceedings of the Twenty-Third International Conference on Machine Learning (ICML},
  year      = {2006}
}

@article{Strehl08MBIE,
  author    = {Alexander L. Strehl and
               Michael L. Littman},
  title     = {An analysis of model-based Interval Estimation for Markov Decision
               Processes},
  journal   = {Journal of Computer and System Sciences},
  volume    = {74},
  number    = {8},
  pages     = {1309--1331},
  year      = {2008}
}

@inproceedings{leurent2019practical,
	title={Practical Open-Loop Optimistic Planning},
	author={Edouard Leurent and Odalric-Ambrym Maillard},
	year={2019},
	booktitle={Proceedings of the 19th European Conference on Machine Learning and Principles and Practice (ECML-PKDD)}
}

@inproceedings{busoniu2012optimistic,
  title={Optimistic planning for Markov decision processes},
  author={Busoniu, Lucian and Munos, R{\'e}mi},
  booktitle={Artificial Intelligence and Statistics},
  pages={182--189},
  year={2012}
}

@incollection{NIPS2015_5668,
title = {Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning},
author = {Mohamed, Shakir and Jimenez Rezende, Danilo},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {2125--2133},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5668-variational-information-maximisation-for-intrinsically-motivated-reinforcement-learning.pdf}
}

@misc{montufar2016information,
    title={Information Theoretically Aided Reinforcement Learning for Embodied Agents},
    author={Guido Montufar and Keyan Ghazi-Zahedi and Nihat Ay},
    year={2016},
    eprint={1605.09735},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@article {PMID:22791268,
	Title = {An information-theoretic approach to curiosity-driven reinforcement learning},
	Author = {Still, Susanne and Precup, Doina},
	DOI = {10.1007/s12064-011-0142-z},
	Number = {3},
	Volume = {131},
	Month = {September},
	Year = {2012},
	Journal = {Theory in biosciences = Theorie in den Biowissenschaften},
	ISSN = {1431-7613},
	Pages = {139—148},
	URL = {https://doi.org/10.1007/s12064-011-0142-z},
}

@incollection{NIPS2004_2552,
title = {Intrinsically Motivated Reinforcement Learning},
author = {Nuttapong Chentanez and Andrew G. Barto and Satinder P. Singh},
booktitle = {Advances in Neural Information Processing Systems 17},
editor = {L. K. Saul and Y. Weiss and L. Bottou},
pages = {1281--1288},
year = {2005},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/2552-intrinsically-motivated-reinforcement-learning.pdf}
}



@inproceedings{al2021adaptive,
  title={Adaptive sampling for best policy identification in markov decision processes},
  author={Al Marjani, Aymen and Proutiere, Alexandre},
  booktitle={International Conference on Machine Learning},
  pages={7459--7468},
  year={2021},
  organization={PMLR}
}

@article{al2021navigating,
  title={Navigating to the best policy in markov decision processes},
  author={Al Marjani, Aymen and Garivier, Aur{\'e}lien and Proutiere, Alexandre},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{zanette2019tighter,
  title={Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds},
  author={Zanette, Andrea and Brunskill, Emma},
  journal={arXiv preprint arXiv:1901.00210},
  year={2019}
}

@book{puterman1994markov,
author = {Puterman, Martin L.}, 
title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
year = {1994}, 
isbn = {0471619779},
address = {USA},
edition = {1st}
}

@book{lattimore2020bandit,
	Author = {Lattimore, Tor and Szepesv{\'a}ri, Csaba},
	Publisher = {Cambridge University Press},
	Title = {Bandit algorithms},
	Year = {2020}}

@inproceedings{domingues2021episodic,
  title={Episodic reinforcement learning in finite MDPs: Minimax lower bounds revisited},
  author={Domingues, Omar Darwiche and M{\'e}nard, Pierre and Kaufmann, Emilie and Valko, Michal},
  booktitle={Algorithmic Learning Theory},
  pages={578--598},
  year={2021},
  organization={PMLR}
}

@inproceedings{Dann2019certificates,
  author    = {Christoph Dann and
               Lihong Li and
               Wei Wei and
               Emma Brunskill},
  title     = {Policy Certificates: Towards Accountable Reinforcement Learning},
  booktitle = {{ICML}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {1507--1516},
  publisher = {{PMLR}},
  year      = {2019}
}

@article{lai1985asymptotically,
  title={Asymptotically efficient adaptive allocation rules},
  author={Lai, Tze Leung and Robbins, Herbert},
  journal={Advances in applied mathematics},
  volume={6},
  number={1},
  pages={4--22},
  year={1985},
  publisher={Academic Press}
}

@article{ramponi2021online,
  title={Online Learning in Non-Cooperative Configurable Markov Decision Process},
  author={Ramponi, Giorgia and Metelli, Alberto Maria and Concetti, Alessandro and Restelli, Marcello},
  journal={AAAI-21 Workshop on Reinforcement Learning in Games},
  year={2021}
}

@inproceedings{wagenmaker2021experimental,
  title={Experimental design for regret minimization in linear bandits},
  author={Wagenmaker, Andrew and Katz-Samuels, Julian and Jamieson, Kevin},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3088--3096},
  year={2021},
  organization={PMLR}
}

@inproceedings{Ayoub2020vtr,
  author    = {Alex Ayoub and
               Zeyu Jia and
               Csaba Szepesv{\'{a}}ri and
               Mengdi Wang and
               Lin Yang},
  title     = {Model-Based Reinforcement Learning with Value-Targeted Regression},
  booktitle = {{ICML}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  pages     = {463--474},
  publisher = {{PMLR}},
  year      = {2020}
}

@inproceedings{Jin2020linear,
  author    = {Chi Jin and
               Zhuoran Yang and
               Zhaoran Wang and
               Michael I. Jordan},
  title     = {Provably efficient reinforcement learning with linear function approximation},
  booktitle = {{COLT}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {125},
  pages     = {2137--2143},
  publisher = {{PMLR}},
  year      = {2020}
}


@misc{marjani2021adaptive,
      title={Adaptive Sampling for Best Policy Identification in Markov Decision Processes}, 
      author={Aymen Al Marjani and Alexandre Proutiere},
      year={2021},
      eprint={2009.13405},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{hao2020adaptive,
	Author = {Botao Hao and Tor Lattimore and Csaba Szepesv{\'{a}}ri},
	Booktitle = {{AISTATS}},
	Pages = {3536--3545},
	Publisher = {{PMLR}},
	Series = {Proceedings of Machine Learning Research},
	Title = {Adaptive Exploration in Linear Contextual Bandit},
	Volume = {108},
	Year = {2020}}
	

	
@article{xu2021fine,
  title={Fine-Grained Gap-Dependent Bounds for Tabular MDPs via Adaptive Multi-Step Bootstrap},
  author={Xu, Haike and Ma, Tengyu and Du, Simon S},
  journal={arXiv preprint arXiv:2102.04692},
  year={2021}
}

@article{paulin2012concentration,
  title={Concentration inequalities for Markov chains by Marton couplings and spectral methods},
  author={Paulin, Daniel},
  journal={arXiv preprint arXiv:1212.2015},
  year={2012}
}

@article{heidergott2007series,
  title={Series expansions for finite-state Markov chains},
  author={Heidergott, Bernd and Hordijk, Arie and Van Uitert, Miranda},
  journal={Probability in the Engineering and Informational Sciences},
  volume={21},
  number={3},
  pages={381--400},
  year={2007},
  publisher={Cambridge University Press}
}

@article{mannor2005empirical,
  title={On the empirical state-action frequencies in Markov decision processes under general policies},
  author={Mannor, Shie and Tsitsiklis, John N},
  journal={Mathematics of Operations Research},
  volume={30},
  number={3},
  pages={545--561},
  year={2005},
  publisher={INFORMS}
}

@article{jun2020crush,
  title={Crush optimism with pessimism: Structured bandits beyond asymptotic optimality},
  author={Jun, Kwang-Sung and Zhang, Chicheng},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@inproceedings{zanette2019tighter,
  author    = {Andrea Zanette and
               Emma Brunskill},
  title     = {Tighter Problem-Dependent Regret Bounds in Reinforcement Learning
               without Domain Knowledge using Value Function Bounds},
  booktitle = {{ICML}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {7304--7312},
  publisher = {{PMLR}},
  year      = {2019}
}

@article{jaksch2010near,
  title={Near-optimal Regret Bounds for Reinforcement Learning},
  author={Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  journal={Journal of Machine Learning Research},
  volume={11},
  pages={1563--1600},
  year={2010}
}

@book{gallager2012discrete,
  title={Discrete stochastic processes},
  author={Gallager, Robert G},
  volume={321},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@inproceedings{simchowitz2019non,
  author    = {Max Simchowitz and
               Kevin G. Jamieson},
  title     = {Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs},
  booktitle = {NeurIPS},
  pages     = {1151--1160},
  year      = {2019}
}

@InProceedings{azar2017minimax,
  title = 	 {Minimax Regret Bounds for Reinforcement Learning},
  author =       {Mohammad Gheshlaghi Azar and Ian Osband and R{\'e}mi Munos},
  pages = 	 {263--272},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher =    {PMLR}
}

@inproceedings{ok2018exploration,
  title={Exploration in structured reinforcement learning},
  author={Ok, Jungseul and Proutiere, Alexandre and Tranos, Damianos},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8874--8882},
  year={2018}
}

@article{burnetas1997optimal,
  title={Optimal adaptive policies for Markov decision processes},
  author={Burnetas, Apostolos N and Katehakis, Michael N},
  journal={Mathematics of Operations Research},
  volume={22},
  number={1},
  pages={222--255},
  year={1997},
  publisher={INFORMS}
}

@article{garivier2019explore,
  title={Explore first, exploit next: The true shape of regret in bandit problems},
  author={Garivier, Aur{\'e}lien and M{\'e}nard, Pierre and Stoltz, Gilles},
  journal={Mathematics of Operations Research},
  volume={44},
  number={2},
  pages={377--399},
  year={2019},
  publisher={INFORMS}
}

@article{kaufmann2016complexity,
  title={On the complexity of best-arm identification in multi-armed bandit models},
  author={Kaufmann, Emilie and Capp{\'e}, Olivier and Garivier, Aur{\'e}lien},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1--42},
  year={2016},
  publisher={JMLR. org}
}

@book{altman1999constrained,
  title={Constrained Markov decision processes},
  author={Altman, Eitan},
  volume={7},
  year={1999},
  publisher={CRC Press}
}

@inproceedings{tewari2007opotimistic,
  author    = {Ambuj Tewari and
               Peter L. Bartlett},
  title     = {Optimistic Linear Programming gives Logarithmic Regret for Irreducible
               MDPs},
  booktitle = {{NIPS}},
  pages     = {1505--1512},
  publisher = {Curran Associates, Inc.},
  year      = {2007}
}

@article{he2020logarithmic,
  author    = {Jiafan He and
               Dongruo Zhou and
               Quanquan Gu},
  title     = {Logarithmic Regret for Reinforcement Learning with Linear Function
               Approximation},
  journal   = {CoRR},
  volume    = {abs/2011.11566},
  year      = {2020}
}

@inproceedings{Yang2021qlearnin,
  author    = {Kunhe Yang and
               Lin F. Yang and
               Simon S. Du},
  title     = {Q-learning with Logarithmic Regret},
  booktitle = {{AISTATS}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {130},
  pages     = {1576--1584},
  publisher = {{PMLR}},
  year      = {2021}
}

@inproceedings{zanette2019almost,
  author    = {Andrea Zanette and
               Mykel J. Kochenderfer and
               Emma Brunskill},
  title     = {Almost Horizon-Free Structure-Aware Best Policy Identification with
               a Generative Model},
  booktitle = {NeurIPS},
  pages     = {5626--5635},
  year      = {2019}
}

