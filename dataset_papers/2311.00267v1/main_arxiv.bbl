\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ajay et~al.(2020)Ajay, Kumar, Agrawal, Levine, and Nachum]{ajay2020opal}
Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum.
\newblock Opal: Offline primitive discovery for accelerating offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2010.13611}, 2020.

\bibitem[Aky{\"u}rek et~al.(2022)Aky{\"u}rek, Schuurmans, Andreas, Ma, and Zhou]{akyurek2022learning}
Ekin Aky{\"u}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
\newblock What learning algorithm is in-context learning? investigations with linear models.
\newblock \emph{arXiv preprint arXiv:2211.15661}, 2022.

\bibitem[Bacon et~al.(2017)Bacon, Harb, and Precup]{bacon2017option}
Pierre-Luc Bacon, Jean Harb, and Doina Precup.
\newblock The option-critic architecture.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~31, 2017.

\bibitem[Badrinath et~al.(2023)Badrinath, Flet-Berliac, Nie, and Brunskill]{badrinath2023waypoint}
Anirudhan Badrinath, Yannis Flet-Berliac, Allen Nie, and Emma Brunskill.
\newblock Waypoint transformer: Reinforcement learning via supervised learning with intermediate targets.
\newblock \emph{arXiv preprint arXiv:2306.14069}, 2023.

\bibitem[Bowling et~al.(2023)Bowling, Martin, Abel, and Dabney]{bowling2023settling}
Michael Bowling, John~D Martin, David Abel, and Will Dabney.
\newblock Settling the reward hypothesis.
\newblock In \emph{International Conference on Machine Learning}, pages 3003--3020. PMLR, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Chebotar et~al.(2023)Chebotar, Vuong, Irpan, Hausman, Xia, Lu, Kumar, Yu, Herzog, Pertsch, et~al.]{chebotar2023q}
Yevgen Chebotar, Quan Vuong, Alex Irpan, Karol Hausman, Fei Xia, Yao Lu, Aviral Kumar, Tianhe Yu, Alexander Herzog, Karl Pertsch, et~al.
\newblock Q-transformer: Scalable offline reinforcement learning via autoregressive q-functions.
\newblock \emph{arXiv preprint arXiv:2309.10150}, 2023.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel, Srinivas, and Mordatch]{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 15084--15097, 2021.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Emmons et~al.(2021)Emmons, Eysenbach, Kostrikov, and Levine]{emmons2021rvs}
Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine.
\newblock Rvs: What is essential for offline rl via supervised learning?
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Fedus et~al.(2022)Fedus, Zoph, and Shazeer]{fedus2022switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.
\newblock \emph{The Journal of Machine Learning Research}, 23\penalty0 (1):\penalty0 5232--5270, 2022.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[Fujimoto and Gu(2021)]{fujimoto2021minimalist}
Scott Fujimoto and Shixiang~Shane Gu.
\newblock A minimalist approach to offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 20132--20145, 2021.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and Meger]{fujimoto2018addressing}
Scott Fujimoto, Herke Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International conference on machine learning}, pages 1587--1596. PMLR, 2018.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International conference on machine learning}, pages 2052--2062. PMLR, 2019.

\bibitem[Gao et~al.(2020)Gao, Fisch, and Chen]{gao2020making}
Tianyu Gao, Adam Fisch, and Danqi Chen.
\newblock Making pre-trained language models better few-shot learners.
\newblock \emph{arXiv preprint arXiv:2012.15723}, 2020.

\bibitem[Garg et~al.(2022)Garg, Tsipras, Liang, and Valiant]{garg2022can}
Shivam Garg, Dimitris Tsipras, Percy~S Liang, and Gregory Valiant.
\newblock What can transformers learn in-context? a case study of simple function classes.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 30583--30598, 2022.

\bibitem[Jiang et~al.(2022)Jiang, Zhang, Janner, Li, Rockt{\"a}schel, Grefenstette, and Tian]{jiang2022efficient}
Zhengyao Jiang, Tianjun Zhang, Michael Janner, Yueying Li, Tim Rockt{\"a}schel, Edward Grefenstette, and Yuandong Tian.
\newblock Efficient planning in a compact latent action space.
\newblock \emph{arXiv preprint arXiv:2208.10291}, 2022.

\bibitem[Klissarov and Machado(2023)]{klissarov2023deep}
Martin Klissarov and Marlos~C Machado.
\newblock Deep laplacian-based options for temporally-extended exploration.
\newblock \emph{arXiv preprint arXiv:2301.11181}, 2023.

\bibitem[Kostrikov et~al.(2022)Kostrikov, Nair, and Levine]{kostrikov2022offline}
Ilya Kostrikov, Ashvin Nair, and Sergey Levine.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=68n2s9ZJWF8}.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and Levine]{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 1179--1191, 2020.

\bibitem[Laskin et~al.(2022)Laskin, Wang, Oh, Parisotto, Spencer, Steigerwald, Strouse, Hansen, Filos, Brooks, et~al.]{laskin2022context}
Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ~Strouse, Steven Hansen, Angelos Filos, Ethan Brooks, et~al.
\newblock In-context reinforcement learning with algorithm distillation.
\newblock \emph{arXiv preprint arXiv:2210.14215}, 2022.

\bibitem[Lee et~al.(2023)Lee, Xie, Pacchiano, Chandak, Finn, Nachum, and Brunskill]{lee2023supervised}
Jonathan~N Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, and Emma Brunskill.
\newblock Supervised pretraining can learn in-context reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2306.14892}, 2023.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock \emph{arXiv preprint arXiv:2104.08691}, 2021.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Liu and Abbeel(2023)]{liu2023emergent}
Hao Liu and Pieter Abbeel.
\newblock Emergent agentic transformer from chain of hindsight experience.
\newblock \emph{arXiv preprint arXiv:2305.16554}, 2023.

\bibitem[Liu et~al.(2023)Liu, Sferrazza, and Abbeel]{liu2023chain}
Hao Liu, Carmelo Sferrazza, and Pieter Abbeel.
\newblock Chain of hindsight aligns language models with feedback.
\newblock \emph{arXiv preprint arXiv:2302.02676}, 3, 2023.

\bibitem[Nachum et~al.(2018)Nachum, Gu, Lee, and Levine]{nachum2018data}
Ofir Nachum, Shixiang~Shane Gu, Honglak Lee, and Sergey Levine.
\newblock Data-efficient hierarchical reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Park et~al.(2023)Park, Ghosh, Eysenbach, and Levine]{park2023hiql}
Seohong Park, Dibya Ghosh, Benjamin Eysenbach, and Sergey Levine.
\newblock Hiql: Offline goal-conditioned rl with latent states as actions.
\newblock \emph{arXiv preprint arXiv:2307.11949}, 2023.

\bibitem[Peng et~al.(2019)Peng, Kumar, Zhang, and Levine]{peng2019advantage}
Xue~Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
\newblock Advantage-weighted regression: Simple and scalable off-policy reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.00177}, 2019.

\bibitem[Puterman(2014)]{puterman2014markov}
Martin~L Puterman.
\newblock \emph{Markov decision processes: discrete stochastic dynamic programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and Sutskever]{ramesh2021zero}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock In \emph{International Conference on Machine Learning}, pages 8821--8831. PMLR, 2021.

\bibitem[Reed et~al.(2022)Reed, Zolna, Parisotto, Colmenarejo, Novikov, Barth-Maron, Gimenez, Sulsky, Kay, Springenberg, et~al.]{reed2022generalist}
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio~Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost~Tobias Springenberg, et~al.
\newblock A generalist agent.
\newblock \emph{arXiv preprint arXiv:2205.06175}, 2022.

\bibitem[Schaul et~al.(2015)Schaul, Horgan, Gregor, and Silver]{schaul2015universal}
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver.
\newblock Universal value function approximators.
\newblock In \emph{International conference on machine learning}, pages 1312--1320. PMLR, 2015.

\bibitem[Silver et~al.(2021)Silver, Singh, Precup, and Sutton]{silver2021reward}
David Silver, Satinder Singh, Doina Precup, and Richard~S Sutton.
\newblock Reward is enough.
\newblock \emph{Artificial Intelligence}, 299:\penalty0 103535, 2021.

\bibitem[Singhal et~al.(2022)Singhal, Azizi, Tu, Mahdavi, Wei, Chung, Scales, Tanwani, Cole-Lewis, Pfohl, et~al.]{singhal2022large}
Karan Singhal, Shekoofeh Azizi, Tao Tu, S~Sara Mahdavi, Jason Wei, Hyung~Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et~al.
\newblock Large language models encode clinical knowledge.
\newblock \emph{arXiv preprint arXiv:2212.13138}, 2022.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{sutton1999between}
Richard~S Sutton, Doina Precup, and Satinder Singh.
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning.
\newblock \emph{Artificial intelligence}, 112\penalty0 (1-2):\penalty0 181--211, 1999.

\bibitem[Tarasov et~al.(2022)Tarasov, Nikulin, Akimov, Kurenkov, and Kolesnikov]{tarasov2022corl}
Denis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav Kurenkov, and Sergey Kolesnikov.
\newblock {CORL}: Research-oriented deep offline reinforcement learning library.
\newblock In \emph{3rd Offline RL Workshop: Offline RL as a ''Launchpad''}, 2022.
\newblock URL \url{https://openreview.net/forum?id=SyAS49bBcv}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou]{wang2022self}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock \emph{arXiv preprint arXiv:2203.11171}, 2022.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 24824--24837, 2022.

\bibitem[Wu et~al.(2023)Wu, Wang, and Hamaya]{wu2023elastic}
Yueh-Hua Wu, Xiaolong Wang, and Masashi Hamaya.
\newblock Elastic decision transformer.
\newblock \emph{arXiv preprint arXiv:2307.02484}, 2023.

\bibitem[Xiao et~al.(2023{\natexlab{a}})Xiao, Wang, Pan, White, and White]{xiao2023sample}
Chenjun Xiao, Han Wang, Yangchen Pan, Adam White, and Martha White.
\newblock The in-sample softmax for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2302.14372}, 2023{\natexlab{a}}.

\bibitem[Xiao et~al.(2023{\natexlab{b}})Xiao, Wang, Pan, White, and White]{xiaosample}
Chenjun Xiao, Han Wang, Yangchen Pan, Adam White, and Martha White.
\newblock The in-sample softmax for offline reinforcement learning.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{b}}.

\bibitem[Yamagata et~al.(2023)Yamagata, Khalil, and Santos-Rodriguez]{yamagata2023q}
Taku Yamagata, Ahmed Khalil, and Raul Santos-Rodriguez.
\newblock Q-learning decision transformer: Leveraging dynamic programming for conditional sequence modelling in offline rl.
\newblock In \emph{International Conference on Machine Learning}, pages 38989--39007. PMLR, 2023.

\bibitem[Yao et~al.(2023)Yao, Yu, Zhao, Shafran, Griffiths, Cao, and Narasimhan]{yao2023tree}
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas~L Griffiths, Yuan Cao, and Karthik Narasimhan.
\newblock Tree of thoughts: Deliberate problem solving with large language models.
\newblock \emph{arXiv preprint arXiv:2305.10601}, 2023.

\bibitem[Zhang et~al.(2022)Zhang, Zhang, Li, and Smola]{zhang2022automatic}
Zhuosheng Zhang, Aston Zhang, Mu~Li, and Alex Smola.
\newblock Automatic chain of thought prompting in large language models.
\newblock \emph{arXiv preprint arXiv:2210.03493}, 2022.

\end{thebibliography}
