\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abernethy et~al.(2023)Abernethy, Agarwal, Marinov, and
  Warmuth]{abernethy2023mechanism}
Jacob Abernethy, Alekh Agarwal, Teodor~V Marinov, and Manfred~K Warmuth.
\newblock A mechanism for sample-efficient in-context learning for sparse
  retrieval tasks.
\newblock \emph{arXiv preprint arXiv:2305.17040}, 2023.

\bibitem[Agarwala and Dauphin(2023)]{agarwala2023sam}
Atish Agarwala and Yann~N Dauphin.
\newblock Sam operates far from home: eigenvalue regularization as a dynamical
  phenomenon.
\newblock \emph{arXiv preprint arXiv:2302.08692}, 2023.

\bibitem[Ahn et~al.(2022)Ahn, Zhang, and Sra]{ahn2022understanding}
Kwangjun Ahn, Jingzhao Zhang, and Suvrit Sra.
\newblock Understanding the unstable convergence of gradient descent.
\newblock In \emph{Proceedings of the 39th International Conference on Machine
  Learning}, volume 162 of \emph{Proceedings of Machine Learning Research},
  pages 247--257. PMLR, 2022.

\bibitem[Ahn et~al.(2023{\natexlab{a}})Ahn, Bubeck, Chewi, Lee, Suarez, and
  Zhang]{ahn2022learning}
Kwangjun Ahn, S{\'e}bastien Bubeck, Sinho Chewi, Yin~Tat Lee, Felipe Suarez,
  and Yi~Zhang.
\newblock Learning threshold neurons via the ``edge of stability''.
\newblock \emph{NeurIPS 2023 (arXiv:2212.07469)}, 2023{\natexlab{a}}.

\bibitem[Ahn et~al.(2023{\natexlab{b}})Ahn, Cheng, Daneshmand, and
  Sra]{ahn2023transformers}
Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra.
\newblock Transformers learn to implement preconditioned gradient descent for
  in-context learning.
\newblock \emph{NeurIPS 2023 (arXiv:2306.00297)}, 2023{\natexlab{b}}.

\bibitem[Ahn et~al.(2023{\natexlab{c}})Ahn, Cheng, Song, Yun, Jadbabaie, and
  Sra]{ahn2023linear}
Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit
  Sra.
\newblock Linear attention is (maybe) all you need (to understand transformer
  optimization).
\newblock \emph{arXiv 2310.01082}, 2023{\natexlab{c}}.

\bibitem[Ahn et~al.(2023{\natexlab{d}})Ahn, Jadbabaie, and Sra]{ahn2023escape}
Kwangjun Ahn, Ali Jadbabaie, and Suvrit Sra.
\newblock How to escape sharp minima.
\newblock \emph{arXiv preprint arXiv:2305.15659}, 2023{\natexlab{d}}.

\bibitem[Allen-Zhu and Li(2023)]{allen2023physics}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Physics of language models: Part 1, context-free grammar.
\newblock \emph{arXiv preprint arXiv:2305.13673}, 2023.

\bibitem[Andriushchenko and Flammarion(2022)]{andriushchenko2022towards}
Maksym Andriushchenko and Nicolas Flammarion.
\newblock Towards understanding sharpness-aware minimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  639--668. PMLR, 2022.

\bibitem[Arora et~al.(2022)Arora, Li, and Panigrahi]{arora2022understanding}
Sanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi.
\newblock Understanding gradient descent on the edge of stability in deep
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  948--1024. PMLR, 2022.

\bibitem[Bahri et~al.(2022)Bahri, Mobahi, and Tay]{bahri2022sharpness}
Dara Bahri, Hossein Mobahi, and Yi~Tay.
\newblock Sharpness-aware minimization improves language model generalization.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 7360--7371,
  2022.

\bibitem[Bartlett et~al.(2022)Bartlett, Long, and
  Bousquet]{bartlett2022dynamics}
Peter~L Bartlett, Philip~M Long, and Olivier Bousquet.
\newblock The dynamics of sharpness-aware minimization: Bouncing across ravines
  and drifting towards wide minima.
\newblock \emph{arXiv preprint arXiv:2210.01513}, 2022.

\bibitem[Behdin and Mazumder(2023)]{behdin2023sharpness}
Kayhan Behdin and Rahul Mazumder.
\newblock Sharpness-aware minimization: An implicit regularization perspective.
\newblock \emph{arXiv preprint arXiv:2302.11836}, 2023.

\bibitem[Blanc et~al.(2020)Blanc, Gupta, Valiant, and
  Valiant]{blanc2020implicit}
Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant.
\newblock Implicit regularization for deep neural networks driven by an
  ornstein-uhlenbeck like process.
\newblock In \emph{Conference on learning theory}, pages 483--513. PMLR, 2020.

\bibitem[Cohen et~al.(2021)Cohen, Kaur, Li, Kolter, and Talwalkar]{Cohen2021}
Jeremy Cohen, Simran Kaur, Yuanzhi Li, J~Zico Kolter, and Ameet Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of
  stability.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Compagnoni et~al.(2023)Compagnoni, Orvieto, Biggio, Kersting, Proske,
  and Lucchi]{compagnoni2023sde}
Enea~Monzio Compagnoni, Antonio Orvieto, Luca Biggio, Hans Kersting,
  Frank~Norbert Proske, and Aurelien Lucchi.
\newblock An sde for modeling sam: Theory and insights.
\newblock \emph{arXiv preprint arXiv:2301.08203}, 2023.

\bibitem[Damian et~al.(2021)Damian, Ma, and Lee]{damian2021label}
Alex Damian, Tengyu Ma, and Jason~D. Lee.
\newblock Label noise {SGD} provably prefers flat global minimizers.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Damian et~al.(2023)Damian, Nichani, and Lee]{damian2022self}
Alex Damian, Eshaan Nichani, and Jason~D Lee.
\newblock Self-stabilization: The implicit bias of gradient descent at the edge
  of stability.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Du et~al.(2022)Du, Yan, Feng, Zhou, Zhen, Goh, and
  Tan]{du2022efficient}
Jiawei Du, Hanshu Yan, Jiashi Feng, Joey~Tianyi Zhou, Liangli Zhen, Rick
  Siow~Mong Goh, and Vincent Tan.
\newblock Efficient sharpness-aware minimization for improved training of
  neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Dziugaite and Roy(2017)]{dziugaite2017computing}
Gintare~Karolina Dziugaite and Daniel~M Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock In \emph{Proceedings of the Thirty-Third Conference on Uncertainty in
  Artificial Intelligence}, 2017.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2020sharpness}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Garg et~al.(2022)Garg, Tsipras, Liang, and Valiant]{garg2022can}
Shivam Garg, Dimitris Tsipras, Percy~S Liang, and Gregory Valiant.
\newblock What can transformers learn in-context? a case study of simple
  function classes.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 30583--30598, 2022.

\bibitem[Jastrzkebski et~al.(2017)Jastrzkebski, Kenton, Arpit, Ballas, Fischer,
  Bengio, and Storkey]{jastrzkebski2017three}
Stanislaw Jastrzkebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja
  Fischer, Yoshua Bengio, and Amos Storkey.
\newblock Three factors influencing minima in {SGD}.
\newblock \emph{arXiv preprint arXiv:1711.04623}, 2017.

\bibitem[Jiang et~al.(2020)Jiang, Neyshabur, Mobahi, Krishnan, and
  Bengio]{jiang2019fantastic}
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy
  Bengio.
\newblock Fantastic generalization measures and where to find them.
\newblock In \emph{8th International Conference on Learning Representations},
  2020.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{karimi2016linear}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases:
  European Conference}, pages 795--811. Springer, 2016.

\bibitem[Kaur et~al.(2022)Kaur, Cohen, and Lipton]{kaur2022maximum}
Simran Kaur, Jeremy Cohen, and Zachary~C Lipton.
\newblock On the maximum hessian eigenvalue and generalization.
\newblock \emph{arXiv preprint arXiv:2206.10654}, 2022.

\bibitem[Keskar et~al.(2017)Keskar, Nocedal, Tang, Mudigere, and
  Smelyanskiy]{keskar2017large}
Nitish~Shirish Keskar, Jorge Nocedal, Ping Tak~Peter Tang, Dheevatsa Mudigere,
  and Mikhail Smelyanskiy.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Kim et~al.(2023)Kim, Park, Choi, and Lee]{kim2023stability}
Hoki Kim, Jinseong Park, Yujin Choi, and Jaewook Lee.
\newblock Stability analysis of sharpness-aware minimization.
\newblock \emph{arXiv preprint arXiv:2301.06308}, 2023.

\bibitem[Kwon et~al.(2021)Kwon, Kim, Park, and Choi]{kwon2021asam}
Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In~Kwon Choi.
\newblock Asam: Adaptive sharpness-aware minimization for scale-invariant
  learning of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  5905--5914. PMLR, 2021.

\bibitem[Li et~al.(2018)Li, Ma, and Zhang]{li2018algorithmic}
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock In \emph{Conference On Learning Theory}, pages 2--47. PMLR, 2018.

\bibitem[Li et~al.(2023)Li, Li, and Risteski]{li2023transformers}
Yuchen Li, Yuanzhi Li, and Andrej Risteski.
\newblock How do transformers learn topic structure: Towards a mechanistic
  understanding.
\newblock \emph{International Conference on Machine Learning (ICML)
  (arXiv:2303.04245)}, 2023.

\bibitem[Liu et~al.(2023)Liu, Ash, Goel, Krishnamurthy, and
  Zhang]{liu2022transformers}
Bingbin Liu, Jordan~T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang.
\newblock Transformers learn shortcuts to automata.
\newblock \emph{ICLR (arXiv:2210.10749)}, 2023.

\bibitem[Liu et~al.(2020)Liu, Papailiopoulos, and Achlioptas]{liu2020bad}
Shengchao Liu, Dimitris Papailiopoulos, and Dimitris Achlioptas.
\newblock Bad global minima exist and sgd can reach them.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 8543--8552, 2020.

\bibitem[Liu et~al.(2022)Liu, Mai, Chen, Hsieh, and You]{liu2022towards}
Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and Yang You.
\newblock Towards efficient and scalable sharpness-aware minimization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 12360--12370, 2022.

\bibitem[Mi et~al.(2022)Mi, Shen, Ren, Zhou, Sun, Ji, and Tao]{mi2022make}
Peng Mi, Li~Shen, Tianhe Ren, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji, and
  Dacheng Tao.
\newblock Make sharpness-aware minimization stronger: A sparsified perturbation
  approach.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{neyshabur2017exploring}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro.
\newblock Exploring generalization in deep learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Si and Yun(2023)]{si2023practical}
Dongkuk Si and Chulhee Yun.
\newblock Practical sharpness-aware minimization cannot converge all the way to
  optima.
\newblock \emph{NeurIPS 2023 (arXiv:2306.09850)}, 2023.

\bibitem[von Oswald et~al.(2023)von Oswald, Niklasson, Randazzo, Sacramento,
  Mordvintsev, Zhmoginov, and Vladymyrov]{von2022transformers}
Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo{\~a}o Sacramento,
  Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages
  35151--35174. PMLR, 2023.

\bibitem[Wang et~al.(2022)Wang, Li, and Li]{wang2022analyzing}
Zixuan Wang, Zhouzi Li, and Jian Li.
\newblock Analyzing sharpness along gd trajectory: Progressive sharpening and
  edge of stability.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 9983--9994, 2022.

\bibitem[Wen et~al.(2023)Wen, Ma, and Li]{wen2022does}
Kaiyue Wen, Tengyu Ma, and Zhiyuan Li.
\newblock How does sharpness-aware minimization minimize sharpness?
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Wu et~al.(2020)Wu, Xia, and Wang]{wu2020adversarial}
Dongxian Wu, Shu-Tao Xia, and Yisen Wang.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 2958--2969, 2020.

\bibitem[Zhang et~al.(2022)Zhang, Backurs, Bubeck, Eldan, Gunasekar, and
  Wagner]{zhang2022unveiling}
Yi~Zhang, Arturs Backurs, S{\'e}bastien Bubeck, Ronen Eldan, Suriya Gunasekar,
  and Tal Wagner.
\newblock Unveiling transformers with lego: a synthetic reasoning task.
\newblock \emph{arXiv preprint arXiv:2206.04301}, 2022.

\bibitem[Zheng et~al.(2021)Zheng, Zhang, and Mao]{zheng2021regularizing}
Yaowei Zheng, Richong Zhang, and Yongyi Mao.
\newblock Regularizing neural networks via adversarial model perturbation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 8156--8165, 2021.

\bibitem[Zhong et~al.(2022)Zhong, Ding, Shen, Mi, Liu, Du, and
  Tao]{zhong2022improving}
Qihuang Zhong, Liang Ding, Li~Shen, Peng Mi, Juhua Liu, Bo~Du, and Dacheng Tao.
\newblock Improving sharpness-aware minimization with fisher mask for better
  generalization on language models.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2022}, pages 4064--4085, 2022.

\bibitem[Zhuang et~al.(2022)Zhuang, Gong, Yuan, Cui, Adam, Dvornek, s~Duncan,
  Liu, et~al.]{zhuang2022surrogate}
Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha~C
  Dvornek, James s~Duncan, Ting Liu, et~al.
\newblock Surrogate gap minimization improves sharpness-aware training.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\end{thebibliography}
