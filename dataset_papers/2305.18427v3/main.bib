
@article{fang2019curriculum,
  title={Curriculum-guided hindsight experience replay},
  author={Fang, Meng and Zhou, Tianyi and Du, Yali and Han, Lei and Zhang, Zhengyou},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{fang2018dher,
  title={DHER: Hindsight experience replay for dynamic goals},
  author={Fang, Meng and Zhou, Cheng and Shi, Bei and Gong, Boqing and Xu, Jia and Zhang, Tong},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{ke2018sparse,
  title={Sparse attentive backtracking: Temporal credit assignment through reminding},
  author={Ke, Nan Rosemary and ALIAS PARTH GOYAL, Anirudh Goyal and Bilaniuk, Olexa and Binas, Jonathan and Mozer, Michael C and Pal, Chris and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{han2022off,
  title={Off-policy reinforcement learning with delayed rewards},
  author={Han, Beining and Ren, Zhizhou and Wu, Zuofan and Zhou, Yuan and Peng, Jian},
  booktitle={International Conference on Machine Learning},
  pages={8280--8303},
  year={2022},
  organization={PMLR}
}

@article{ramsauer2020hopfield,
  title={Hopfield networks is all you need},
  author={Ramsauer, Hubert and Sch{\"a}fl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlovi{\'c}, Milena and Sandve, Geir Kjetil and others},
  journal={arXiv preprint arXiv:2008.02217},
  year={2020}
}

@inproceedings{widrich2021modern,
  title={Modern hopfield networks for return decomposition for delayed rewards},
  author={Widrich, Michael and Hofmarcher, Markus and Patil, Vihang Prakash and Bitto-Nemling, Angela and Hochreiter, Sepp},
  booktitle={Deep RL Workshop NeurIPS 2021},
  year={2021}
}

@inproceedings{yu2020meta,
  title={Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning},
  author={Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
  booktitle={Conference on robot learning},
  pages={1094--1100},
  year={2020},
  organization={PMLR}
}

@inproceedings{td3,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International conference on machine learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}

@inproceedings{ddpg,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={International conference on machine learning},
  pages={387--395},
  year={2014},
  organization={Pmlr}
}

@article{bishop1994mixture,
  title={Mixture density networks},
  author={Bishop, Christopher M},
  year={1994},
  publisher={Aston University}
}

@article{reynolds2009gaussian,
  title={Gaussian mixture models.},
  author={Reynolds, Douglas A and others},
  journal={Encyclopedia of biometrics},
  volume={741},
  number={659-663},
  year={2009},
  publisher={Berlin, Springer}
}


@book{murphy2002dynamic,
  title={Dynamic bayesian networks: representation, inference and learning},
  author={Murphy, Kevin Patrick},
  year={2002},
  publisher={University of California, Berkeley}
}


@inproceedings{zhang2019solar,
  title={Solar: Deep structured representations for model-based reinforcement learning},
  author={Zhang, Marvin and Vikram, Sharad and Smith, Laura and Abbeel, Pieter and Johnson, Matthew and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={7444--7453},
  year={2019},
  organization={PMLR}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{clavera2018model,
  title={Model-based reinforcement learning via meta-policy optimization},
  author={Clavera, Ignasi and Rothfuss, Jonas and Schulman, John and Fujita, Yasuhiro and Asfour, Tamim and Abbeel, Pieter},
  booktitle={Conference on Robot Learning},
  pages={617--629},
  year={2018},
  organization={PMLR}
}

@article{janner2019trust,
  title={When to trust your model: Model-based policy optimization},
  author={Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{eysenbach2020rewriting,
  title={Rewriting history with inverse rl: Hindsight inference for policy improvement},
  author={Eysenbach, Ben and Geng, Xinyang and Levine, Sergey and Salakhutdinov, Russ R},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={14783--14795},
  year={2020}
}

@article{glymour2019review,
  title={Review of causal discovery methods based on graphical models},
  author={Glymour, Clark and Zhang, Kun and Spirtes, Peter},
  journal={Frontiers in genetics},
  volume={10},
  pages={524},
  year={2019},
  publisher={Frontiers Media SA}
}

@article{ppo,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {CoRR},
  volume    = {abs/1707.06347},
  year      = {2017},
  eprint    = {1707.06347},
  timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SchulmanWDRK17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{A3C,
  author    = {Volodymyr Mnih and
               Adri{\`{a}} Puigdom{\`{e}}nech Badia and
               Mehdi Mirza and
               Alex Graves and
               Timothy P. Lillicrap and
               Tim Harley and
               David Silver and
               Koray Kavukcuoglu},
  title     = {Asynchronous Methods for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1602.01783},
  year      = {2016},
  eprint    = {1602.01783},
  timestamp = {Mon, 13 Aug 2018 16:47:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MnihBMGLHSK16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{du2019liir,
  title={LIIR: learning individual intrinsic reward in multi-agent reinforcement learning},
  author={Du, Yali and Han, Lei and Fang, Meng and Dai, Tianhong and Liu, Ji and Tao, Dacheng},
  booktitle={Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  pages={4403--4414},
  year={2019}
}

@inproceedings{han2019grid,
  title={Grid-wise control for multi-agent reinforcement learning in video game ai},
  author={Han, Lei and  Sun, Peng and Du, Yali and Xiong, Jiechao and Wang, Qing and Sun, Xinghai and Liu, Han and Zhang, Tong},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={2576--2585},
  year={2019},
  organization={PMLR}
}
@inproceedings{ng1999policy,
  title={Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart J},
  booktitle={Proceedings of the Sixteenth International Conference on Machine Learning},
  pages={278--287},
  year={1999}
}
@book{williams2006gaussian,
  title={Gaussian processes for machine learning},
  author={Williams, Christopher KI and Rasmussen, Carl Edward},
  volume={2},
  year={2006},
  publisher={MIT press Cambridge, MA}
}

@article{andrychowicz2017hindsight,
  title={Hindsight experience replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{fang2019dher,
  title={DHER: Hindsight experience replay for dynamic goals},
  author={Fang, Meng and Zhou, Cheng and Shi, Bei and Gong, Boqing and Xu, Jia and Zhang, Tong},
  booktitle={International Conference on Learning Representations},
  year={2019}
}




@inproceedings{causal_pg,
  title={Counterfactual Credit Assignment in Model-Free Reinforcement Learning},
  author={Mesnard, Thomas and Weber, Theophane and Viola, Fabio and Thakoor, Shantanu and Saade, Alaa and Harutyunyan, Anna and Dabney, Will and Stepleton, Thomas S and Heess, Nicolas and Guez, Arthur and others},
  booktitle={International Conference on Machine Learning},
  pages={7654--7664},
  year={2021},
  organization={PMLR}
}

@article{zhang2020causal,
  title={Causal imitation learning with unobserved confounders},
  author={Zhang, Junzhe and Kumor, Daniel and Bareinboim, Elias},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12263--12274},
  year={2020}
}

@techreport{zhang2016markov,
  title={Markov decision processes with unobserved confounders: A causal approach},
  author={Zhang, Junzhe and Bareinboim, Elias},
  year={2016},
  institution={Technical report, Technical Report R-23, Purdue AI Lab}
}



@inproceedings{jaques2019social,
  title={Social influence as intrinsic motivation for multi-agent deep reinforcement learning},
  author={Jaques, Natasha and Lazaridou, Angeliki and Hughes, Edward and Gulcehre, Caglar and Ortega, Pedro and Strouse, DJ and Leibo, Joel Z and De Freitas, Nando},
  booktitle={International Conference on Machine Learning},
  pages={3040--3049},
  year={2019},
  organization={PMLR}
}

@inproceedings{grimbly2021causal,
  title={Causal multi-agent reinforcement learning: Review and open problems},
  author={Grimbly, St John and Shock, Jonathan and Pretorius, Arnu},
  booktitle={Cooperative AI Workshop, Advances in Neural Information Processing Systems},
  year={2021}
}

@article{guestrin2001multiagent,
  title={Multiagent planning with factored MDPs},
  author={Guestrin, Carlos and Koller, Daphne and Parr, Ronald},
  journal={Advances in Neural Information Processing Systems},
  volume={14},
  year={2001}
}

@inproceedings{huang2022action,
  title={Action-sufficient state representation learning for control with structural constraints},
  author={Huang, Biwei and Lu, Chaochao and Leqi, Liu and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Glymour, Clark and Sch{\"o}lkopf, Bernhard and Zhang, Kun},
  booktitle={International Conference on Machine Learning},
  pages={9260--9279},
  year={2022},
  organization={PMLR}
}

@article{chen2020self,
  title={Self-imitation learning in sparse reward settings},
  author={Chen, Zhixin and Lin, Mengxiang},
  journal={CoRR, abs/2010.06962},
  year={2020}
}

@inproceedings{rajeswar2022haptics,
  title={Haptics-based curiosity for sparse-reward tasks},
  author={Rajeswar, Sai and Ibrahim, Cyril and Surya, Nitin and Golemo, Florian and Vazquez, David and Courville, Aaron and Pinheiro, Pedro O},
  booktitle={Conference on Robot Learning},
  pages={395--405},
  year={2022},
  organization={PMLR}
}

@article{packer2021hindsight,
  title={Hindsight task relabelling: Experience replay for sparse reward meta-RL},
  author={Packer, Charles and Abbeel, Pieter and Gonzalez, Joseph E},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2466--2477},
  year={2021}
}

@article{trott2019keeping,
  title={Keeping your distance: Solving sparse reward tasks using self-balancing shaped rewards},
  author={Trott, Alexander and Zheng, Stephan and Xiong, Caiming and Socher, Richard},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{choudhury2022scalable,
  title={Scalable online planning for multi-agent MDPs},
  author={Choudhury, Shushman and Gupta, Jayesh K and Morales, Peter and Kochenderfer, Mykel J},
  journal={Journal of Artificial Intelligence Research},
  volume={73},
  pages={821--846},
  year={2022}
}

@article{yu2021reinforcement,
  title={Reinforcement learning in healthcare: A survey},
  author={Yu, Chao and Liu, Jiming and Nemati, Shamim and Yin, Guosheng},
  journal={ACM Computing Surveys (CSUR)},
  volume={55},
  number={1},
  pages={1--36},
  year={2021},
  publisher={ACM New York, NY}
}

@inproceedings{yang2020deep,
  title={Deep reinforcement learning for automated stock trading: An ensemble strategy},
  author={Yang, Hongyang and Liu, Xiao-Yang and Zhong, Shan and Walid, Anwar},
  booktitle={Proceedings of the First ACM International Conference on AI in Finance},
  pages={1--8},
  year={2020}
}

@article{zhang2020deep,
  title={Deep reinforcement learning for trading},
  author={Zhang, Zihao and Zohren, Stefan and Roberts, Stephen},
  journal={The Journal of Financial Data Science},
  volume={2},
  number={2},
  pages={25--40},
  year={2020},
  publisher={Institutional Investor Journals Umbrella}
}

@inproceedings{li2020unsupervised,
  title={Unsupervised reinforcement learning of transferable meta-skills for embodied navigation},
  author={Li, Juncheng and Wang, Xin and Tang, Siliang and Shi, Haizhou and Wu, Fei and Zhuang, Yueting and Wang, William Yang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12123--12132},
  year={2020}
}

@article{nygaard2021real,
  title={Real-world embodied AI through a morphologically adaptive quadruped robot},
  author={Nygaard, T{\o}nnes F and Martin, Charles P and Torresen, Jim and Glette, Kyrre and Howard, David},
  journal={Nature Machine Intelligence},
  volume={3},
  number={5},
  pages={410--419},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{autonumous_driving1,
  title={Deep reinforcement learning for autonomous driving: A survey},
  author={Kiran, B Ravi and Sobh, Ibrahim and Talpaert, Victor and Mannion, Patrick and Al Sallab, Ahmad A and Yogamani, Senthil and P{\'e}rez, Patrick},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  year={2021},
  publisher={IEEE}
}

@inproceedings{autonumous_driving2,
  title={Model-free deep reinforcement learning for urban autonomous driving},
  author={Chen, Jianyu and Yuan, Bodi and Tomizuka, Masayoshi},
  booktitle={2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
  pages={2765--2771},
  year={2019},
  organization={IEEE}
}

@article{ghassami2018multi,
  title={Multi-domain causal structure learning in linear systems},
  author={Ghassami, AmirEmad and Kiyavash, Negar and Huang, Biwei and Zhang, Kun},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{zhang2011intervention,
  title={Intervention, determinism, and the causal minimality condition},
  author={Zhang, Jiji and Spirtes, Peter},
  journal={Synthese},
  volume={182},
  number={3},
  pages={335--347},
  year={2011},
  publisher={Springer}
}


@inproceedings{gumble,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  booktitle={International Conference on Learning Representations},
  year={2016}
}
@inproceedings{sac,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}

@inproceedings{todorov2012mujoco,
  title={MuJoCo: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE},
  doi={10.1109/IROS.2012.6386109}
}

@article{scholkopf2021toward,
  title={Toward causal representation learning},
  author={Sch{\"o}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  journal={Proceedings of the IEEE},
  volume={109},
  number={5},
  pages={612--634},
  year={2021},
  publisher={IEEE}
}

@article{huang2020causal,
  title={Causal Discovery from Heterogeneous/Nonstationary Data.},
  author={Huang, Biwei and Zhang, Kun and Zhang, Jiji and Ramsey, Joseph D and Sanchez-Romero, Ruben and Glymour, Clark and Sch{\"o}lkopf, Bernhard},
  journal={J. Mach. Learn. Res.},
  volume={21},
  number={89},
  pages={1--53},
  year={2020}
}

@inproceedings{li2019online,
  title={Online learning for markov decision processes in nonstationary environments: A dynamic regret analysis},
  author={Li, Yingying and Li, Na},
  booktitle={2019 American Control Conference (ACC)},
  pages={1232--1237},
  year={2019},
  organization={IEEE}
}
@article{even2009online,
  title={Online Markov decision processes},
  author={Even-Dar, Eyal and Kakade, Sham M and Mansour, Yishay},
  journal={Mathematics of Operations Research},
  volume={34},
  number={3},
  pages={726--736},
  year={2009},
  publisher={INFORMS}
}

@inproceedings{chane2021goal,
  title={Goal-conditioned reinforcement learning with imagined subgoals},
  author={Chane-Sane, Elliot and Schmid, Cordelia and Laptev, Ivan},
  booktitle={International Conference on Machine Learning},
  pages={1430--1440},
  year={2021},
  organization={PMLR}
}

@article{nasiriany2019planning,
  title={Planning with goal-conditioned policies},
  author={Nasiriany, Soroush and Pong, Vitchyr and Lin, Steven and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@inproceedings{rethinking-goal-condition,
  title={Rethinking Goal-Conditioned Supervised Learning and Its Connection to Offline RL},
  author={Yang, Rui and Lu, Yiming and Li, Wenzhe and Sun, Hao and Fang, Meng and Du, Yali and Li, Xiu and Han, Lei and Zhang, Chongjie},
  year = {2022},
  booktitle={International Conference on Learning Representations}
}


@book{Spirtes1993CausationPA,
  title={Causation, prediction, and search},
  author={Spirtes, Peter and Glymour, Clark N and Scheines, Richard and Heckerman, David},
  year={2000},
  publisher={MIT press}
}

@misc{d-separation, 
  title={CAUSALITY: MODELS, REASONING, AND INFERENCE}, 
  volume={19}, 
  number={4}, 
  publisher={Cambridge University Press},
  author={Judea Pearl}, 
  year={2000}, 
  pages={675–685}
}
@article{varing_causal_structure,
  title={MoCoDA: Model-based Counterfactual Data Augmentation},
  author={Pitis, Silviu and Creager, Elliot and Mandlekar, Ajay and Garg, Animesh},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}


@article{wang2022causal,
  title={Causal dynamics learning for task-independent state abstraction},
  author={Wang, Zizhao and Xiao, Xuesu and Xu, Zifan and Zhu, Yuke and Stone, Peter},
  journal={International Conference on Machine Learning},
  pages={23151-23180},
  year={2022}
}



@article{osband2014near,
  title={Near-optimal reinforcement learning in factored mdps},
  author={Osband, Ian and Van Roy, Benjamin},
  journal={Advances in Neural Information Processing Systems},
  volume={27},
  year={2014}
}


@inproceedings{kearns1999efficient,
  author    = {Michael J. Kearns and Daphne Koller},
  editor    = {Thomas Dean},
  title     = {Efficient Reinforcement Learning in Factored MDPs},
  booktitle = {Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence},
  pages     = {740--747},
  publisher = {Morgan Kaufmann},
  year      = {1999},
}

@inproceedings{wang2018nervenet,
  title={Nervenet: Learning structured policy with graph neural networks},
  author={Wang, Tingwu and Liao, Renjie and Ba, Jimmy and Fidler, Sanja},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{jonsson2006causal,
  title={Causal Graph Based Decomposition of Factored MDPs.},
  author={Jonsson, Anderson and Barto, Andrew},
  journal={Journal of Machine Learning Research},
  volume={7},
  number={11},
  year={2006}
}

@inproceedings{factored_mdp_1,
  title={Off-policy model-based learning under unknown factored dynamics},
  author={Hallak, Assaf and Schnitzler, Fran{\c{c}}ois and Mann, Timothy and Mannor, Shie},
  booktitle={International Conference on Machine Learning},
  pages={711--719},
  year={2015},
  organization={PMLR}
}

@article{attention_is_all_you_need,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}



@article{pengjian_arxiv,
  title={Sequence modeling of temporal credit assignment for episodic reinforcement learning},
  author={Liu, Yang and Luo, Yunan and Zhong, Yuanyi and Chen, Xi and Liu, Qiang and Peng, Jian},
  journal={arXiv preprint arXiv:1905.13420},
  year={2019}
}


@inproceedings{align_rudder,
  title     = {Align-RUDDER: Learning From Few Demonstrations by Reward Redistribution},
  author    = {Vihang P. Patil and Markus Hofmarcher and Marius-Constantin Dinu and Matthias Dorfer and Patrick M. Blies and Johannes Brandstetter and Jose A. Arjona-Medina and Sepp Hochreiter},
  booktitle = {International Conference on Machine Learning},
  year      = {2022},
  organization={PMLR}
}

@article{alphazero,
  author  = {David Silver  and Thomas Hubert  and Julian Schrittwieser  and Ioannis Antonoglou  and Matthew Lai  and Arthur Guez  and Marc Lanctot  and Laurent Sifre  and Dharshan Kumaran  and Thore Graepel  and Timothy Lillicrap  and Karen Simonyan  and Demis Hassabis },
  title   = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  journal = {Science},
  volume  = {362},
  number  = {6419},
  pages   = {1140-1144},
  year    = {2018},
  doi     = {10.1126/science.aar6404},
  url     = {https://www.science.org/doi/abs/10.1126/science.aar6404},
  eprint  = {https://www.science.org/doi/pdf/10.1126/science.aar6404}
}

@inproceedings{badcase4reward1,
  title     = {Learning to Drive a Bicycle Using Reinforcement Learning and Shaping.},
  author    = {Randl{\o}v, Jette and Alstr{\o}m, Preben},
  booktitle = {International Conference on Machine Learning},
  volume    = {98},
  pages     = {463--471},
  year      = {1998}
}

@article{badcase4reward2,
  title   = {Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising.},
  author  = {Bottou, L{\'e}on and Peters, Jonas and Qui{\~n}onero-Candela, Joaquin and Charles, Denis X and Chickering, D Max and Portugaly, Elon and Ray, Dipankar and Simard, Patrice and Snelson, Ed},
  journal = {Journal of Machine Learning Research},
  volume  = {14},
  number  = {11},
  year    = {2013}
}

@article{badcase4reward3,
  title   = {Hindsight experience replay},
  author  = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {30},
  year    = {2017}
}

@article{curiosity_episodic,
  title   = {Episodic multi-agent reinforcement learning with curiosity-driven exploration},
  author  = {Zheng, Lulu and Chen, Jiarui and Wang, Jianhao and He, Jiamin and Hu, Yujing and Chen, Yingfeng and Fan, Changjie and Gao, Yang and Zhang, Chongjie},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  pages   = {3757--3769},
  year    = {2021}
}

@inproceedings{curiosity_exploration,
  title        = {Curiosity-driven exploration by self-supervised prediction},
  author       = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle    = {International Conference on Machine Learning},
  pages        = {2778--2787},
  year         = {2017},
  organization = {PMLR}
}

@article{curiosity_language,
  title   = {Language as a cognitive tool to imagine goals in curiosity driven exploration},
  author  = {Colas, C{\'e}dric and Karch, Tristan and Lair, Nicolas and Dussoux, Jean-Michel and Moulin-Frier, Cl{\'e}ment and Dominey, Peter and Oudeyer, Pierre-Yves},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {3761--3774},
  year    = {2020}
}

@article{factored_adaption_huang2022,
  title   = {Factored Adaptation for Non-Stationary Reinforcement Learning},
  author  = {Feng, Fan and Huang, Biwei and Zhang, Kun and Magliacane, Sara},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2022}
}

@inproceedings{huang2021adarl,
  title     = {AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning},
  author    = {Huang, Biwei and Feng, Fan and Lu, Chaochao and Magliacane, Sara and Zhang, Kun},
  booktitle = {International Conference on Learning Representations},
  year      = {2021}
}

@article{mincraft,
  title   = {Hierarchical reinforcement learning for zero-shot generalization with subtask dependencies},
  author  = {Sohn, Sungryull and Oh, Junhyuk and Lee, Honglak},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {31},
  year    = {2018}
}

@article{muzero,
  title     = {Mastering atari, go, chess and shogi by planning with a learned model},
  author    = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal   = {Nature},
  volume    = {588},
  number    = {7839},
  pages     = {604--609},
  year      = {2020},
  publisher = {Nature Publishing Group}
}

@article{navigation_without_map,
  title   = {Learning to navigate in cities without a map},
  author  = {Mirowski, Piotr and Grimes, Matt and Malinowski, Mateusz and Hermann, Karl Moritz and Anderson, Keith and Teplyashin, Denis and Simonyan, Karen and Zisserman, Andrew and Hadsell, Raia and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {31},
  year    = {2018}
}

@article{object_navigation,
  title   = {Object goal navigation using goal-oriented semantic exploration},
  author  = {Chaplot, Devendra Singh and Gandhi, Dhiraj Prakashchand and Gupta, Abhinav and Salakhutdinov, Russ R},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {4247--4258},
  year    = {2020}
}

@inproceedings{randomized_return_decomposition,
  title     = {Learning Long-Term Reward Redistribution via Randomized Return Decomposition},
  author    = {Ren, Zhizhou and Guo, Ruihan and Zhou, Yuan and Peng, Jian},
  booktitle = {International Conference on Learning Representations},
  year      = {2022}
}


@article{rearrangement,
  title={Habitat 2.0: Training home assistants to rearrange their habitat},
  author={Szot, Andrew and Clegg, Alexander and Undersander, Eric and Wijmans, Erik and Zhao, Yili and Turner, John and Maestre, Noah and Mukadam, Mustafa and Chaplot, Devendra Singh and Maksymets, Oleksandr and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={251--266},
  year={2021}
}
@article{reward_shaping,
  title   = {Learning to utilize shaping rewards: A new approach of reward shaping},
  author  = {Hu, Yujing and Wang, Weixun and Jia, Hangtian and Wang, Yixiang and Chen, Yingfeng and Hao, Jianye and Wu, Feng and Fan, Changjie},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {15931--15941},
  year    = {2020}
}

@inproceedings{reward_shaping_neural_story_plot_generation,
  title     = {Controllable Neural Story Plot Generation via Reward Shaping.},
  author    = {Tambwekar, Pradyumna and Dhuliawala, Murtaza and Martin, Lara J and Mehta, Animesh and Harrison, Brent and Riedl, Mark O},
  booktitle = {Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence},
  pages     = {5982--5988},
  year      = {2019}
}

@book{rl_intro,
  title     = {Reinforcement learning: An introduction},
  author    = {Sutton, Richard S and Barto, Andrew G},
  year      = {2018},
  publisher = {MIT press}
}

@article{rudder,
  title   = {RUDDER: Return decomposition for delayed rewards},
  author  = {Arjona-Medina, Jose A and Gillhofer, Michael and Widrich, Michael and Unterthiner, Thomas and Brandstetter, Johannes and Hochreiter, Sepp},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {32},
  year    = {2019}
}

@article{uniform_return_redistribution,
  title   = {Learning guidance rewards with trajectory-space smoothing},
  author  = {Gangwani, Tanmay and Zhou, Yuan and Peng, Jian},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  year    = {2020}
}


@article{world_models,
  title={Recurrent world models facilitate policy evolution},
  author={Ha, David and Schmidhuber, J{\"u}rgen},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}