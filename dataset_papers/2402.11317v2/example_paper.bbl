\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[An et~al.(2021)An, Moon, Kim, and Song]{EDAC}
An, G., Moon, S., Kim, J.-H., and Song, H.~O.
\newblock Uncertainty-{B}ased {O}ffline {R}einforcement {L}earning with
  {D}iversified {Q}-{E}nsemble.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Chen et~al.(2018)Chen, Li, Grosse, and Duvenaud]{betaTCVAE}
Chen, R.~T., Li, X., Grosse, R.~B., and Duvenaud, D.~K.
\newblock Isolating {S}ources of {D}isentanglement in {V}ariational
  {A}utoencoders.
\newblock In \emph{Advances in {N}eural {I}nformation {P}rocessing {S}ystems},
  2018.

\bibitem[Cho et~al.(2014)Cho, van Merrienboer, G{\"{u}}l{\c{c}}ehre, Bahdanau,
  Bougares, Schwenk, and Bengio]{GRU}
Cho, K., van Merrienboer, B., G{\"{u}}l{\c{c}}ehre, {\c{C}}., Bahdanau, D.,
  Bougares, F., Schwenk, H., and Bengio, Y.
\newblock Learning {P}hrase {R}epresentations using {R}{N}{N}
  {E}ncoder-{D}ecoder for {S}tatistical {M}achine {T}ranslation.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, 2014.

\bibitem[Choi(2000)]{RL_in_nonstationary_environments}
Choi, P.-M.
\newblock \emph{Reinforcement learning in nonstationary environments}.
\newblock Hong Kong University of Science and Technology (Hong Kong), 2000.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{MAML}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-{A}gnostic {M}eta-{L}earning for {F}ast {A}daptation of {D}eep
  {N}etworks.
\newblock In \emph{{I}nternational {C}onference on {M}achine {L}earning}, 2017.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{BCQ}
Fujimoto, S., Meger, D., and Precup, D.
\newblock Off-{P}olicy {D}eep {R}einforcement {L}earning without {E}xploration.
\newblock In \emph{{I}nternational {C}onference on {M}achine {L}earning}, 2019.

\bibitem[Gao et~al.(2023)Gao, Zhang, Guo, Wu, Yi, Peng, Lan, Chen, Du, Hu,
  et~al.]{CSRO}
Gao, Y., Zhang, R., Guo, J., Wu, F., Yi, Q., Peng, S., Lan, S., Chen, R., Du,
  Z., Hu, X., et~al.
\newblock Context {S}hift {R}eduction for {O}ffline {M}eta-{R}einforcement
  {L}earning.
\newblock \emph{arXiv preprint arXiv:2311.03695}, 2023.

\bibitem[Garc{\i}a \& Fern{\'a}ndez(2015)Garc{\i}a and
  Fern{\'a}ndez]{SafeRL_survey}
Garc{\i}a, J. and Fern{\'a}ndez, F.
\newblock A {C}omprehensive {S}urvey on {S}afe {R}einforcement {L}earning.
\newblock \emph{Journal of Machine Learning Research}, 16:\penalty0 1437--1480,
  2015.

\bibitem[Ghosh et~al.(2022)Ghosh, Ajay, Agrawal, and Levine]{ghosh2022offline}
Ghosh, D., Ajay, A., Agrawal, P., and Levine, S.
\newblock Offline {RL} {P}olicies should be {T}rained to be {A}daptive.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{SAC}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft {A}ctor-{C}ritic: {O}ff-{P}olicy {M}aximum {E}ntropy {D}eep
  {R}einforcement {L}earning with a {S}tochastic {A}ctor.
\newblock In \emph{{I}nternational {C}onference on {M}achine {L}earning}, 2018.

\bibitem[Kaiser et~al.(2019)Kaiser, Babaeizadeh, Milos, Osinski, Campbell,
  Czechowski, Erhan, Finn, Kozakowski, Levine, et~al.]{mbAtari}
Kaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R.~H.,
  Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., et~al.
\newblock Model-{B}ased {R}einforcement {L}earning for {A}tari.
\newblock \emph{arXiv preprint arXiv:1903.00374}, 2019.

\bibitem[Kaplanis et~al.(2019)Kaplanis, Shanahan, and
  Clopath]{kaplanis2019policy}
Kaplanis, C., Shanahan, M., and Clopath, C.
\newblock Policy {C}onsolidation for {C}ontinual {R}einforcement {L}earning.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Khetarpal et~al.(2022)Khetarpal, Riemer, Rish, and
  Precup]{ContinualRL_survey}
Khetarpal, K., Riemer, M., Rish, I., and Precup, D.
\newblock Towards {C}ontinual {R}einforcement {L}earning: {A} {R}eview and
  {P}erspectives.
\newblock \emph{Journal of Artificial Intelligence Research}, 75:\penalty0
  1401--1476, 2022.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{Morel}
Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T.
\newblock M{O}{R}e{L} : {M}odel-{B}ased {O}ffline {R}einforcement {L}earning.
\newblock In \emph{Advances in {N}eural {I}nformation {P}rocessing {S}ystems},
  2020.

\bibitem[Kullback \& Leibler(1951)Kullback and Leibler]{kld}
Kullback, S. and Leibler, R.~A.
\newblock On information and sufficiency.
\newblock \emph{The annals of mathematical statistics}, 22\penalty0
  (1):\penalty0 79--86, 1951.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and Levine]{CQL}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock Conservative {Q}-{L}earning for {O}ffline {R}einforcement {L}earning.
\newblock In \emph{Advances in {N}eural {I}nformation {P}rocessing {S}ystems},
  2020.

\bibitem[Levine et~al.(2020{\natexlab{a}})Levine, Kumar, Tucker, and
  Fu]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline {R}einforcement {L}earning: {T}utorial, {R}eview, and
  {P}erspectives on {O}pen {P}roblems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020{\natexlab{a}}.

\bibitem[Levine et~al.(2020{\natexlab{b}})Levine, Kumar, Tucker, and
  Fu]{rl_tutorial}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2021)Li, Yang, and Luo]{FOCAL}
Li, L., Yang, R., and Luo, D.
\newblock F{O}{C}{A}{L}: {E}fficient {F}ully-{O}ffline {M}eta-{R}einforcement
  {L}earning via {D}istance {M}etric {L}earning and {B}ehavior
  {R}egularization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Lin et~al.(2022{\natexlab{a}})Lin, Li, Feng, Zhang, Fung, Zhang, Wang,
  Du, and Yang]{CMT}
Lin, R., Li, Y., Feng, X., Zhang, Z., Fung, X. H.~W., Zhang, H., Wang, J., Du,
  Y., and Yang, Y.
\newblock Contextual {T}ransformer for {O}ffline {M}eta {R}einforcement
  {L}earning.
\newblock \emph{arXiv preprint arXiv:2211.08016}, 2022{\natexlab{a}}.

\bibitem[Lin et~al.(2022{\natexlab{b}})Lin, Wan, Xu, Liang, and Zhang]{MerPO}
Lin, S., Wan, J., Xu, T., Liang, Y., and Zhang, J.
\newblock Model-{B}ased {O}ffline {M}eta-{R}einforcement {L}earning with
  {R}egularization.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{b}}.

\bibitem[Luo et~al.(2022)Luo, Jiang, Yu, Zhang, and Zhang]{ESCP}
Luo, F.-M., Jiang, S., Yu, Y., Zhang, Z., and Zhang, Y.-F.
\newblock Adapt to {E}nvironment {S}udden {C}hanges by {L}earning a {C}ontext
  {S}ensitive {P}olicy.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2022.

\bibitem[Mnih \& Teh(2012)Mnih and Teh]{mnih2012fast}
Mnih, A. and Teh, Y.~W.
\newblock A fast and simple algorithm for training neural probabilistic
  language models.
\newblock In \emph{International Conference on Machine Learning}, 2012.

\bibitem[Nagabandi et~al.(2019)Nagabandi, Clavera, Liu, Fearing, Abbeel,
  Levine, and Finn]{MetaRL_survey}
Nagabandi, A., Clavera, I., Liu, S., Fearing, R.~S., Abbeel, P., Levine, S.,
  and Finn, C.
\newblock Learning to {A}dapt in {D}ynamic, {R}eal-{W}orld {E}nvironments
  through {M}eta-{R}einforcement {L}earning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{InfoNCE}
Oord, A. v.~d., Li, Y., and Vinyals, O.
\newblock Representation {L}earning with {C}ontrastive {P}redictive {C}oding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Rakelly et~al.(2019)Rakelly, Zhou, Finn, Levine, and Quillen]{PEARL}
Rakelly, K., Zhou, A., Finn, C., Levine, S., and Quillen, D.
\newblock Efficient {O}ff-{P}olicy {M}eta-{R}einforcement {L}earning via
  {P}robabilistic {C}ontext {V}ariables.
\newblock In \emph{{I}nternational {C}onference on {M}achine {L}earning}, 2019.

\bibitem[Ran et~al.(2023)Ran, Li, Zhang, Zhang, and Yu]{prdc}
Ran, Y., Li, Y., Zhang, F., Zhang, Z., and Yu, Y.
\newblock Policy {R}egularization with {D}ataset {C}onstraint for offline
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Ross \& Bagnell(2010)Ross and Bagnell]{BC}
Ross, S. and Bagnell, D.
\newblock Efficient reductions for imitation learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2010.

\bibitem[Sohn et~al.(2015)Sohn, Lee, and Yan]{CVAE}
Sohn, K., Lee, H., and Yan, X.
\newblock Learning {S}tructured {O}utput {R}epresentation using {D}eep
  {C}onditional {G}enerative {M}odels.
\newblock In \emph{Advances in {N}eural {I}nformation {P}rocessing {S}ystems},
  2015.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{rlbook}
Sutton, R. and Barto, A.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Swazinna et~al.(2022)Swazinna, Udluft, and Runkler]{swazinna2022user}
Swazinna, P., Udluft, S., and Runkler, T.
\newblock User-{I}nteractive {O}ffline {R}einforcement {L}earning.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Tishby \& Zaslavsky(2015)Tishby and Zaslavsky]{DLandIB}
Tishby, N. and Zaslavsky, N.
\newblock Deep learning and the information bottleneck principle.
\newblock In \emph{IEEE Information Theory Workshop}, 2015.

\bibitem[Tishby et~al.(2000)Tishby, Pereira, and Bialek]{IB}
Tishby, N., Pereira, F.~C., and Bialek, W.
\newblock The information bottleneck method.
\newblock \emph{arXiv preprint arXiv: physics/0004057}, 2000.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: {A} {P}hysics {E}ngine for {M}odel-{B}ased {C}ontrol.
\newblock In \emph{International Conference on Intelligent Robots and Systems},
  pp.\  5026--5033, 2012.

\bibitem[Van~der Maaten \& Hinton(2008)Van~der Maaten and Hinton]{t-SNE}
Van~der Maaten, L. and Hinton, G.
\newblock Visualizing {D}ata using t-{S}{N}{E}.
\newblock \emph{Journal of machine learning research}, 9\penalty0 (11), 2008.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{BRAC}
Wu, Y., Tucker, G., and Nachum, O.
\newblock Behavior {R}egularized {O}ffline {R}einforcement {L}earning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Xu et~al.(2022)Xu, Shen, Zhang, Lu, Zhao, Tenenbaum, and
  Gan]{Prompt-DT}
Xu, M., Shen, Y., Zhang, S., Lu, Y., Zhao, D., Tenenbaum, J., and Gan, C.
\newblock Prompting {D}ecision {T}ransformer for {F}ew-{S}hot {P}olicy
  {G}eneralization.
\newblock In \emph{{I}nternational {C}onference on {M}achine {L}earning}, 2022.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and Ma]{MOPO}
Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J.~Y., Levine, S., Finn, C., and
  Ma, T.
\newblock M{O}{P}{O}: {M}odel-based {O}ffline {P}olicy {O}ptimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Yu et~al.(2021)Yu, Kumar, Rafailov, Rajeswaran, Levine, and
  Finn]{COMBO}
Yu, T., Kumar, A., Rafailov, R., Rajeswaran, A., Levine, S., and Finn, C.
\newblock C{O}{M}{B}{O}: {C}onservative {O}ffline {M}odel-{B}ased {P}olicy
  {O}ptimization.
\newblock In \emph{Advances in {N}eural {I}nformation {P}rocessing {S}ystems},
  2021.

\bibitem[Yuan \& Lu(2022)Yuan and Lu]{CORRO}
Yuan, H. and Lu, Z.
\newblock Robust {T}ask {R}epresentations for {O}ffline {M}eta-{R}einforcement
  {L}earning via {C}ontrastive {L}earning.
\newblock In \emph{{I}nternational {C}onference on {M}achine {L}earning}, 2022.

\bibitem[Zhou et~al.(2023)Zhou, Gao, Zhang, and Yu]{GENTLE}
Zhou, R., Gao, C.-X., Zhang, Z., and Yu, Y.
\newblock Generalizable {T}ask {R}epresentation {L}earning for {O}ffline
  {M}eta-{R}einforcement {L}earning with {D}ata {L}imitations.
\newblock \emph{arXiv preprint arXiv:2312.15909}, 2023.

\end{thebibliography}
