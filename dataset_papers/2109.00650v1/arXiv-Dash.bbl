\begin{thebibliography}{78}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pages
  242--252, 2019.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  322--332, 2019.

\bibitem[Bachman et~al.(2014)Bachman, Alsharif, and
  Precup]{bachman2014learning}
Philip Bachman, Ouais Alsharif, and Doina Precup.
\newblock Learning with pseudo-ensembles.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3365--3373, 2014.

\bibitem[Baird(1992)]{baird1992document}
Henry~S Baird.
\newblock Document image defect models.
\newblock In \emph{Structured Document Image Analysis}, pages 546--556.
  Springer, 1992.

\bibitem[Balcan and Blum(2005)]{balcan2005pac}
Maria-Florina Balcan and Avrim Blum.
\newblock A pac-style model for learning from labeled and unlabeled data.
\newblock In \emph{International Conference on Computational Learning Theory},
  pages 111--126. Springer, 2005.

\bibitem[Balsubramani and Freund(2015)]{balsubramani2015optimally}
Akshay Balsubramani and Yoav Freund.
\newblock Optimally combining classifiers using unlabeled data.
\newblock In \emph{Conference on Learning Theory}, pages 211--225, 2015.

\bibitem[Ben-David et~al.(2008)Ben-David, Lu, and P{\'a}l]{ben2008does}
Shai Ben-David, Tyler Lu, and D{\'a}vid P{\'a}l.
\newblock Does unlabeled data provably help? worst-case analysis of the sample
  complexity of semi-supervised learning.
\newblock In \emph{Conference on Learning Theory}, pages 33--44, 2008.

\bibitem[Berthelot et~al.(2019{\natexlab{a}})Berthelot, Carlini, Cubuk,
  Kurakin, Sohn, Zhang, and Raffel]{berthelot2019remixmatch}
David Berthelot, Nicholas Carlini, Ekin~D Cubuk, Alex Kurakin, Kihyuk Sohn, Han
  Zhang, and Colin Raffel.
\newblock Remixmatch: Semi-supervised learning with distribution matching and
  augmentation anchoring.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Berthelot et~al.(2019{\natexlab{b}})Berthelot, Carlini, Goodfellow,
  Papernot, Oliver, and Raffel]{berthelot2019mixmatch}
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital
  Oliver, and Colin~A Raffel.
\newblock Mixmatch: A holistic approach to semi-supervised learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5050--5060, 2019{\natexlab{b}}.

\bibitem[Blum and Mitchell(1998)]{blum1998combining}
Avrim Blum and Tom Mitchell.
\newblock Combining labeled and unlabeled data with co-training.
\newblock In \emph{Proceedings of Annual Conference on Computational Learning
  Theory}, pages 92--100, 1998.

\bibitem[Chapelle et~al.(2006)Chapelle, Scholkopf, and Zien]{chapelle2006semi}
Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien.
\newblock \emph{Semi-Supervised Learning (1st edition)}.
\newblock Cambridge: The MIT Press, 2006.

\bibitem[Charles and Papailiopoulos(2018)]{charles2018stability}
Zachary Charles and Dimitris Papailiopoulos.
\newblock Stability and generalization of learning algorithms that converge to
  global optima.
\newblock In \emph{International Conference on Machine Learning}, pages
  745--754, 2018.

\bibitem[Chen et~al.(2011)Chen, Weinberger, and Chen]{chen2011automatic}
Minmin Chen, Kilian~Q Weinberger, and Yixin Chen.
\newblock Automatic feature decomposition for single view co-training.
\newblock In \emph{International Conference on Machine Learning}, pages
  953--960, 2011.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2019lazy}
Lenaic Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2937--2947, 2019.

\bibitem[Chung and Lu(2006)]{chung2006concentration}
Fan Chung and Linyuan Lu.
\newblock Concentration inequalities and martingale inequalities: a survey.
\newblock \emph{Internet Mathematics}, 3\penalty0 (1):\penalty0 79--127, 2006.

\bibitem[Coates et~al.(2011)Coates, Ng, and Lee]{coates2011analysis}
Adam Coates, Andrew Ng, and Honglak Lee.
\newblock An analysis of single-layer networks in unsupervised feature
  learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 215--223, 2011.

\bibitem[Cozman et~al.(2003)Cozman, Cohen, and Cirelo]{cozman2003semi}
Fabio~G Cozman, Ira Cohen, and Marcelo~C Cirelo.
\newblock Semi-supervised learning of mixture models.
\newblock In \emph{International Conference on Machine Learning}, pages
  99--106, 2003.

\bibitem[Cubuk et~al.(2019)Cubuk, Zoph, Mane, Vasudevan, and
  Le]{cubuk2019autoaugment}
Ekin~D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V Le.
\newblock Autoaugment: Learning augmentation strategies from data.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 113--123, 2019.

\bibitem[Cubuk et~al.(2020)Cubuk, Zoph, Shlens, and Le]{cubuk2020randaugment}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 702--703, 2020.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  1675--1685, 2019.

\bibitem[Flach(2012)]{flach2012machine}
Peter Flach.
\newblock \emph{Machine learning: the art and science of algorithms that make
  sense of data}.
\newblock Cambridge University Press, 2012.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Ghadimi et~al.(2016)Ghadimi, Lan, and Zhang]{ghadimi2016mini}
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang.
\newblock Mini-batch stochastic approximation methods for nonconvex stochastic
  composite optimization.
\newblock \emph{Mathematical Programming}, 155\penalty0 (1-2):\penalty0
  267--305, 2016.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock \emph{Deep learning}.
\newblock MIT press, 2016.

\bibitem[Grandvalet and Bengio(2005)]{grandvalet2005semi}
Yves Grandvalet and Yoshua Bengio.
\newblock Semi-supervised learning by entropy minimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  529--536, 2005.

\bibitem[Guo et~al.(2020)Guo, Zhang, Jiang, Li, and Zhou]{guosafe2020}
Lan-Zhe Guo, Zhen-Yu Zhang, Yuan Jiang, Yu-Feng Li, and Zhi-Hua Zhou.
\newblock Safe deep semi-supervised learning for unseen-class unlabeled data.
\newblock In \emph{International Conference on Machine Learning}, pages
  3897--3906, 2020.

\bibitem[Hady and Schwenker(2013)]{hady2013semi}
Mohamed Farouk~Abdel Hady and Friedhelm Schwenker.
\newblock Semi-supervised learning.
\newblock In \emph{Handbook on Neural Information Processing}, pages 215--239.
  Springer, 2013.

\bibitem[Hastie et~al.(2019)Hastie, Montanari, Rosset, and
  Tibshirani]{hastie2019surprises}
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan~J Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock \emph{arXiv preprint arXiv:1903.08560}, 2019.

\bibitem[Hataya and Nakayama(2019)]{hataya2019unifying}
Ryuichiro Hataya and Hideki Nakayama.
\newblock Unifying semi-supervised and robust learning by mixup.
\newblock 2019.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{karimi2016linear}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 795--811. Springer, 2016.

\bibitem[Krijthe and Loog(2017)]{krijthe2017projected}
Jesse~H Krijthe and Marco Loog.
\newblock Projected estimators for robust semi-supervised classification.
\newblock \emph{Machine Learning}, 106\penalty0 (7):\penalty0 993--1008, 2017.

\bibitem[Krizhevsky and Hinton(2009)]{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Master's thesis, Technical report, University of Tronto}, 2009.

\bibitem[Laine and Aila(2017)]{laine2017temporal}
Samuli Laine and Timo Aila.
\newblock Temporal ensembling for semi-supervised learning.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Lee(2013)]{lee2013pseudo}
Dong-Hyun Lee.
\newblock Pseudo-label: The simple and efficient semi-supervised learning
  method for deep neural networks.
\newblock In \emph{In ICML Workshop on Challenges in Representation Learning},
  2013.

\bibitem[Li et~al.(2020)Li, Socher, and Hoi]{li2020dividemix}
Junnan Li, Richard Socher, and Steven~CH Hoi.
\newblock Dividemix: Learning with noisy labels as semi-supervised learning.
\newblock \emph{International Conference on Learning Representations}, 2020.

\bibitem[Li and Zhou(2011{\natexlab{a}})]{li2011improving}
Yu-Feng Li and Zhi-Hua Zhou.
\newblock Improving semi-supervised support vector machines through unlabeled
  instances selection.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pages 386--391, 2011{\natexlab{a}}.

\bibitem[Li and Zhou(2011{\natexlab{b}})]{li2011towards}
Yu-Feng Li and Zhi-Hua Zhou.
\newblock Towards making unlabeled data never hurt.
\newblock In \emph{International Conference on Machine Learning}, pages
  1081--1088, 2011{\natexlab{b}}.

\bibitem[Li and Zhou(2015)]{li2014towards}
Yu-Feng Li and Zhi-Hua Zhou.
\newblock Towards making unlabeled data never hurt.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 37\penalty0 (1):\penalty0 175--188, 2015.

\bibitem[Li et~al.(2017)Li, Zha, and Zhou]{li2017learning}
Yu-Feng Li, Han-Wen Zha, and Zhi-Hua Zhou.
\newblock Learning safe prediction for semi-supervised regression.
\newblock In \emph{Proceedings of the Thirty-First AAAI Conference on
  Artificial Intelligence}, pages 2217--2223, 2017.

\bibitem[Li et~al.(2021)Li, Guo, and Zhou]{li2019towards}
Yu-Feng Li, Lan-Zhe Guo, and Zhi-Hua Zhou.
\newblock Towards safe weakly supervised learning.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 43\penalty0 (1):\penalty0 334--346, 2021.

\bibitem[Li and Li(2018)]{li2018simple}
Zhize Li and Jian Li.
\newblock A simple proximal stochastic gradient method for nonsmooth nonconvex
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5564--5574, 2018.

\bibitem[Loog(2015)]{loog2015contrastive}
Marco Loog.
\newblock Contrastive pessimistic likelihood estimation for semi-supervised
  classification.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 38\penalty0 (3):\penalty0 462--475, 2015.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2016sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[Mammen et~al.(1999)Mammen, Tsybakov, et~al.]{mammen1999smooth}
Enno Mammen, Alexandre~B Tsybakov, et~al.
\newblock Smooth discrimination analysis.
\newblock \emph{The Annals of Statistics}, 27\penalty0 (6):\penalty0
  1808--1829, 1999.

\bibitem[McLachlan(1975)]{mclachlan1975iterative}
Geoffrey~J McLachlan.
\newblock Iterative reclassification procedure for constructing an
  asymptotically optimal rule of allocation in discriminant analysis.
\newblock \emph{Journal of the American Statistical Association}, 70\penalty0
  (350):\penalty0 365--369, 1975.

\bibitem[Mey and Loog(2019)]{mey2019improvability}
Alexander Mey and Marco Loog.
\newblock Improvability through semi-supervised learning: a survey of
  theoretical results.
\newblock \emph{arXiv preprint arXiv:1908.09574}, 2019.

\bibitem[Misra et~al.(2015)Misra, Shrivastava, and Hebert]{misra2015watch}
Ishan Misra, Abhinav Shrivastava, and Martial Hebert.
\newblock Watch and learn: Semi-supervised learning for object detectors from
  video.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3593--3602, 2015.

\bibitem[Miyato et~al.(2018)Miyato, Maeda, Koyama, and
  Ishii]{miyato2018virtual}
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii.
\newblock Virtual adversarial training: a regularization method for supervised
  and semi-supervised learning.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 41\penalty0 (8):\penalty0 1979--1993, 2018.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{Netzer201137648}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y.
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Oliver et~al.(2018)Oliver, Odena, Raffel, Cubuk, and
  Goodfellow]{oliver2018realistic}
Avital Oliver, Augustus Odena, Colin~A Raffel, Ekin~Dogus Cubuk, and Ian
  Goodfellow.
\newblock Realistic evaluation of deep semi-supervised learning algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3235--3246, 2018.

\bibitem[Polyak(1963)]{polyak1963gradient}
Boris~Teodorovich Polyak.
\newblock Gradient methods for minimizing functionals.
\newblock \emph{Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
  3\penalty0 (4):\penalty0 643--653, 1963.

\bibitem[Rasmus et~al.(2015)Rasmus, Valpola, Honkala, Berglund, and
  Raiko]{rasmus2015semi}
Antti Rasmus, Harri Valpola, Mikko Honkala, Mathias Berglund, and Tapani Raiko.
\newblock Semi-supervised learning with ladder networks.
\newblock In \emph{Neural Information Processing Systems}, pages 3546--3554,
  2015.

\bibitem[Ren et~al.(2020)Ren, Yeh, and Schwing]{ren2020not}
Zhongzheng Ren, Raymond Yeh, and Alexander Schwing.
\newblock Not all unlabeled data are equal: learning to weight data in
  semi-supervised learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Rosenberg et~al.(2005)Rosenberg, Hebert, and
  Schneiderman]{rosenberg2005semi}
Chuck Rosenberg, Martial Hebert, and Henry Schneiderman.
\newblock Semi-supervised self-training of object detection models.
\newblock In \emph{Proceedings of the 7th IEEE Workshops on Application of
  Computer Vision}, pages 29--36, 2005.

\bibitem[Sajjadi et~al.(2016)Sajjadi, Javanmardi, and
  Tasdizen]{sajjadi2016regularization}
Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen.
\newblock Regularization with stochastic transformations and perturbations for
  deep semi-supervised learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1163--1171, 2016.

\bibitem[Schmidhuber(2015)]{schmidhuber2015deep}
J{\"u}rgen Schmidhuber.
\newblock Deep learning in neural networks: An overview.
\newblock \emph{Neural networks}, 61:\penalty0 85--117, 2015.

\bibitem[Sindhwani and Rosenberg(2008)]{sindhwani2008rkhs}
Vikas Sindhwani and David~S Rosenberg.
\newblock An rkhs for multi-view learning and manifold co-regularization.
\newblock In \emph{International Conference on Machine Learning}, pages
  976--983, 2008.

\bibitem[Singh et~al.(2009)Singh, Nowak, and Zhu]{singh2009unlabeled}
Aarti Singh, Robert Nowak, and Jerry Zhu.
\newblock Unlabeled data: Now it helps, now it doesn't.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1513--1520, 2009.

\bibitem[Sohn et~al.(2020)Sohn, Berthelot, Li, Zhang, Carlini, Cubuk, Kurakin,
  Zhang, and Raffel]{sohn2020fixmatch}
Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini,
  Ekin~D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and
  confidence.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  596--608, 2020.

\bibitem[Tarvainen and Valpola(2017)]{tarvainen2017mean}
Antti Tarvainen and Harri Valpola.
\newblock Mean teachers are better role models: Weight-averaged consistency
  targets improve semi-supervised deep learning results.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1195--1204, 2017.

\bibitem[Tsybakov(2004)]{tsybakov2004optimal}
Alexander~B Tsybakov.
\newblock Optimal aggregation of classifiers in statistical learning.
\newblock \emph{The Annals of Statistics}, 32\penalty0 (1):\penalty0 135--166,
  2004.

\bibitem[Turian et~al.(2010)Turian, Ratinov, and Bengio]{turian2010word}
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
\newblock Word representations: a simple and general method for semi-supervised
  learning.
\newblock In \emph{Proceedings of Annual Meeting of the Association for
  Computational Linguistics}, pages 384--394, 2010.

\bibitem[Van~Engelen and Hoos(2020)]{van2020survey}
Jesper~E Van~Engelen and Holger~H Hoos.
\newblock A survey on semi-supervised learning.
\newblock \emph{Machine Learning}, 109\penalty0 (2):\penalty0 373--440, 2020.

\bibitem[Wang et~al.(2008)Wang, Luo, and Zeng]{wang2008random}
Jiao Wang, Si-wei Luo, and Xian-hua Zeng.
\newblock A random subspace method for co-training.
\newblock In \emph{Proceedings of IEEE International Joint Conference on Neural
  Networks}, pages 195--200, 2008.

\bibitem[Wang and Zhou(2010)]{wang2010new}
Wei Wang and Zhi-Hua Zhou.
\newblock A new analysis of co-training.
\newblock In \emph{International Conference on Machine Learning}, pages
  1135--1142, 2010.

\bibitem[Wang et~al.(2019)Wang, Ji, Zhou, Liang, and
  Tarokh]{wang2019spiderboost}
Zhe Wang, Kaiyi Ji, Yi~Zhou, Yingbin Liang, and Vahid Tarokh.
\newblock Spiderboost and momentum: Faster variance reduction algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2403--2413, 2019.

\bibitem[Xie et~al.(2020{\natexlab{a}})Xie, Dai, Hovy, Luong, and
  Le]{xie2020unsupervised}
Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le.
\newblock Unsupervised data augmentation for consistency training.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6256--6268, 2020{\natexlab{a}}.

\bibitem[Xie et~al.(2020{\natexlab{b}})Xie, Luong, Hovy, and Le]{xie2020self}
Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc~V Le.
\newblock Self-training with noisy student improves imagenet classification.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 10687--10698, 2020{\natexlab{b}}.

\bibitem[Yarowsky(1995)]{yarowsky1995unsupervised}
David Yarowsky.
\newblock Unsupervised word sense disambiguation rivaling supervised methods.
\newblock In \emph{Proceedings of Annual Meeting of the Association for
  Computational Linguistics}, pages 189--196, 1995.

\bibitem[Yu et~al.(2010)Yu, Varadarajan, Deng, and Acero]{yu2010active}
Dong Yu, Balakrishnan Varadarajan, Li~Deng, and Alex Acero.
\newblock Active learning and semi-supervised learning for speech recognition:
  A unified framework using the global entropy reduction maximization
  criterion.
\newblock \emph{Computer Speech \& Language}, 24\penalty0 (3):\penalty0
  433--444, 2010.

\bibitem[Yu et~al.(2008)Yu, Krishnapuram, Steck, Rao, and
  Rosales]{yu2008bayesian}
Shipeng Yu, Balaji Krishnapuram, Harald Steck, R~Bharat Rao, and R{\'o}mer
  Rosales.
\newblock Bayesian co-training.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1665--1672, 2008.

\bibitem[Yuan et~al.(2019)Yuan, Yan, Jin, and Yang]{yuan2019stagewise}
Zhuoning Yuan, Yan Yan, Rong Jin, and Tianbao Yang.
\newblock Stagewise training accelerates convergence of testing error over sgd.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2604--2614, 2019.

\bibitem[Yun et~al.(2019)Yun, Sra, and Jadbabaie]{yun2019small}
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie.
\newblock Small relu networks are powerful memorizers: a tight analysis of
  memorization capacity.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  15558--15569, 2019.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock \emph{British Machine Vision Conference}, 2016.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[Zhou and Li(2005)]{zhou2005tri}
Zhi-Hua Zhou and Ming Li.
\newblock Tri-training: Exploiting unlabeled data using three classifiers.
\newblock \emph{IEEE Transactions on knowledge and Data Engineering},
  17\penalty0 (11):\penalty0 1529--1541, 2005.

\bibitem[Zhu and Goldberg(2009)]{zhu2009introduction}
Xiaojin Zhu and Andrew~B Goldberg.
\newblock Introduction to semi-supervised learning.
\newblock \emph{Synthesis Lectures on Artificial Intelligence and Machine
  Learning}, 3\penalty0 (1):\penalty0 1--130, 2009.

\bibitem[Zhu(2005)]{zhu2005semi}
Xiaojin~Jerry Zhu.
\newblock Semi-supervised learning literature survey.
\newblock Technical report, Technical Report, University of Wisconsin-Madison
  Department of Computer Sciences, 2005.

\end{thebibliography}
