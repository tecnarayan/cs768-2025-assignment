\begin{thebibliography}{10}

\bibitem{allen2017neon2}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Neon2: Finding local minima via first-order oracles.
\newblock {\em arXiv preprint arXiv:1711.06673}, 2017.

\bibitem{das2020faster}
Rudrajit Das, Anish Acharya, Abolfazl Hashemi, Sujay Sanghavi, Inderjit~S
  Dhillon, and Ufuk Topcu.
\newblock Faster non-convex federated learning via global and local momentum.
\newblock {\em arXiv preprint arXiv:2012.04061}, 2020.

\bibitem{fang2018spider}
Cong Fang, Chris~Junchi Li, Zhouchen Lin, and Tong Zhang.
\newblock Spider: Near-optimal non-convex optimization via stochastic path
  integrated differential estimator.
\newblock {\em arXiv preprint arXiv:1807.01695}, 2018.

\bibitem{ge2015escaping}
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.
\newblock Escaping from saddle pointsâ€”online stochastic gradient for tensor
  decomposition.
\newblock In {\em Conference on learning theory}, pages 797--842. PMLR, 2015.

\bibitem{ge2019stabilized}
Rong Ge, Zhize Li, Weiyao Wang, and Xiang Wang.
\newblock Stabilized svrg: Simple variance reduction for nonconvex
  optimization.
\newblock In {\em Conference on learning theory}, pages 1394--1448. PMLR, 2019.

\bibitem{haddadpour2019local}
Farzin Haddadpour, Mohammad~Mahdi Kamani, Mehrdad Mahdavi, and Viveck~R
  Cadambe.
\newblock Local sgd with periodic averaging: Tighter analysis and adaptive
  synchronization.
\newblock {\em arXiv preprint arXiv:1910.13598}, 2019.

\bibitem{haddadpour2019convergence}
Farzin Haddadpour and Mehrdad Mahdavi.
\newblock On the convergence of local descent methods in federated learning.
\newblock {\em arXiv preprint arXiv:1910.14425}, 2019.

\bibitem{jin2017escape}
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham~M Kakade, and Michael~I Jordan.
\newblock How to escape saddle points efficiently.
\newblock In {\em International Conference on Machine Learning}, pages
  1724--1732. PMLR, 2017.

\bibitem{jin2019short}
Chi Jin, Praneeth Netrapalli, Rong Ge, Sham~M Kakade, and Michael~I Jordan.
\newblock A short note on concentration inequalities for random vectors with
  subgaussian norm.
\newblock {\em arXiv preprint arXiv:1902.03736}, 2019.

\bibitem{jin2021nonconvex}
Chi Jin, Praneeth Netrapalli, Rong Ge, Sham~M Kakade, and Michael~I Jordan.
\newblock On nonconvex optimization for machine learning: Gradients,
  stochasticity, and saddle points.
\newblock {\em Journal of the ACM (JACM)}, 68(2):1--29, 2021.

\bibitem{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock {\em Advances in neural information processing systems}, 26:315--323,
  2013.

\bibitem{karimireddy2020mime}
Sai~Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank~J
  Reddi, Sebastian~U Stich, and Ananda~Theertha Suresh.
\newblock Mime: Mimicking centralized stochastic algorithms in federated
  learning.
\newblock {\em arXiv preprint arXiv:2008.03606}, 2020.

\bibitem{karimireddy2020scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
  Stich, and Ananda~Theertha Suresh.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In {\em International Conference on Machine Learning}, pages
  5132--5143. PMLR, 2020.

\bibitem{khaled2020tighter}
Ahmed Khaled, Konstantin Mishchenko, and Peter Richt{\'a}rik.
\newblock Tighter theory for local sgd on identical and heterogeneous data.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 4519--4529. PMLR, 2020.

\bibitem{khanduriachieving}
Prashant Khanduri, Pranay Sharma, Haibo Yang, Mingyi Hong, Jia Liu, Ketan
  Rajawat, and Pramod~K Varshney.
\newblock Achieving optimal sample and communication complexities for non-iid
  federated learning.
\newblock 2021.

\bibitem{koloskova2020unified}
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian
  Stich.
\newblock A unified theory of decentralized sgd with changing topology and
  local updates.
\newblock In {\em International Conference on Machine Learning}, pages
  5381--5393. PMLR, 2020.

\bibitem{konevcny2015federated}
Jakub Kone{\v{c}}n{\`y}, Brendan McMahan, and Daniel Ramage.
\newblock Federated optimization: Distributed optimization beyond the
  datacenter.
\newblock {\em arXiv preprint arXiv:1511.03575}, 2015.

\bibitem{li2019ssrgd}
Zhize Li.
\newblock Ssrgd: Simple stochastic recursive gradient descent for escaping
  saddle points.
\newblock {\em arXiv preprint arXiv:1904.09265}, 2019.

\bibitem{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem{murata2021bias}
Tomoya Murata and Taiji Suzuki.
\newblock Bias-variance reduced local sgd for less heterogeneous federated
  learning.
\newblock In {\em International Conference on Machine Learning}, pages
  7872--7881. PMLR, 2021.

\bibitem{nguyen2017sarah}
Lam~M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak{\'a}{\v{c}}.
\newblock Sarah: A novel method for machine learning problems using stochastic
  recursive gradient.
\newblock In {\em International Conference on Machine Learning}, pages
  2613--2621. PMLR, 2017.

\bibitem{nguyen2019finite}
Lam~M Nguyen, Marten van Dijk, Dzung~T Phan, Phuong~Ha Nguyen, Tsui-Wei Weng,
  and Jayant~R Kalagnanam.
\newblock Finite-sum smooth optimization with sarah.
\newblock {\em arXiv preprint arXiv:1901.07648}, 2019.

\bibitem{reddi2016aide}
Sashank~J Reddi, Jakub Kone{\v{c}}n{\`y}, Peter Richt{\'a}rik, Barnab{\'a}s
  P{\'o}cz{\'o}s, and Alex Smola.
\newblock Aide: Fast and communication efficient distributed optimization.
\newblock {\em arXiv preprint arXiv:1608.06879}, 2016.

\bibitem{sharma2019parallel}
Pranay Sharma, Swatantra Kafle, Prashant Khanduri, Saikiran Bulusu, Ketan
  Rajawat, and Pramod~K Varshney.
\newblock Parallel restarted spider--communication efficient distributed
  nonconvex optimization with optimal computation complexity.
\newblock {\em arXiv preprint arXiv:1912.06036}, 2019.

\bibitem{shokri2015privacy}
Reza Shokri and Vitaly Shmatikov.
\newblock Privacy-preserving deep learning.
\newblock In {\em Proceedings of the 22nd ACM SIGSAC conference on computer and
  communications security}, pages 1310--1321, 2015.

\bibitem{stich2018local}
Sebastian~U Stich.
\newblock Local sgd converges fast and communicates little.
\newblock {\em arXiv preprint arXiv:1805.09767}, 2018.

\bibitem{vlaski2020second}
Stefan Vlaski, Elsa Rizk, and Ali~H Sayed.
\newblock Second-order guarantees in federated learning.
\newblock In {\em 2020 54th Asilomar Conference on Signals, Systems, and
  Computers}, pages 915--922. IEEE, 2020.

\bibitem{woodworth2020minibatch}
Blake Woodworth, Kumar~Kshitij Patel, and Nathan Srebro.
\newblock Minibatch vs local sgd for heterogeneous distributed learning.
\newblock {\em arXiv preprint arXiv:2006.04735}, 2020.

\bibitem{woodworth2020local}
Blake Woodworth, Kumar~Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins,
  Brendan Mcmahan, Ohad Shamir, and Nathan Srebro.
\newblock Is local sgd better than minibatch sgd?
\newblock In {\em International Conference on Machine Learning}, pages
  10334--10343. PMLR, 2020.

\bibitem{xu2017first}
Yi~Xu, Rong Jin, and Tianbao Yang.
\newblock First-order stochastic algorithms for escaping from saddle points in
  almost linear time.
\newblock {\em arXiv preprint arXiv:1711.01944}, 2017.

\bibitem{yu2019parallel}
Hao Yu, Sen Yang, and Shenghuo Zhu.
\newblock Parallel restarted sgd with faster convergence and less
  communication: Demystifying why model averaging works for deep learning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 5693--5700, 2019.

\end{thebibliography}
