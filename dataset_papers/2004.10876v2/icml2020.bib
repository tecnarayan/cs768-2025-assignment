@book{gopnik_scientistincrib,
  title={The Scientist In The Crib: Minds, Brains, And How Children Learn},
  author={Gopnik, A. and Meltzoff, A.N. and Kuhl, P.K.},
  isbn={9780061846915},
  publisher={HarperCollins},
  @url={https://books.google.com/books?id=ui6KAniUJfsC},
  year={2009},
}
@inproceedings{VegaBrown2018AsymptoticallyOP,
  title={Asymptotically optimal planning under piecewise-analytic constraints},
  author={William Vega-Brown and Nicholas Roy},
  year={2018}
}


@misc{ddpg,
    title={Continuous control with deep reinforcement learning},
    author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
    year={2015},
    eprint={1509.02971},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{Tang2017ExplorationAS,
  title={Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning},
  author={Haoran Tang and Rein Houthooft and Davis Foote and Adam Stooke and Xi Chen and Yan Duan and John Schulman and Filip De Turck and Pieter Abbeel},
  booktitle={NIPS},
  year={2017}
}


@INPROCEEDINGS{7346130, author={F. {Benureau} and P. {Oudeyer}}, booktitle={2015 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)}, title={Diversity-driven selection of exploration strategies in multi-armed bandits}, year={2015}, volume={}, number={}, pages={135-142},}


@Inbook{Baldassarre2013,
author="Baldassarre, Gianluca
and Mirolli, Marco",
editor="Baldassarre, Gianluca
and Mirolli, Marco",
title="Intrinsically Motivated Learning Systems: An Overview",
bookTitle="Intrinsically Motivated Learning in Natural and Artificial Systems",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--14",
abstract="This chapter introduces the field of intrinsically motivated learning systems and illustrates the content, objectives, and organisation of the book. The chapter first expands the concept of intrinsic motivations, then introduces a taxonomy of three classes of intrinsic-motivation mechanisms (based on predictors, on novelty detection, and on competence), and finally introduces and reviews the various contributions of the book. The contributions are organised in six parts. The contributions of the first part provide general overviews on the concept of intrinsic motivations, the possible mechanisms that may implement them, and the functions that they can play. The contributions of the second, third, and fourth part focus on the three classes of the aforementioned intrinsic-motivation mechanisms. The contributions of the fifth part discuss mechanisms that are complementary to intrinsic motivations. The contributions of the sixth part introduce tools and experimental paradigms that can be used to investigate intrinsic motivations.",
isbn="978-3-642-32375-1",
doi="10.1007/978-3-642-32375-1_1",
url="https://doi.org/10.1007/978-3-642-32375-1_1"
}





@InProceedings{pmlr-v80-riedmiller18a,
  title = 	 {Learning by Playing Solving Sparse Reward Tasks from Scratch},
  author = 	 {Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and van de Wiele, Tom and Mnih, Vlad and Heess, Nicolas and Springenberg, Jost Tobias},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4344--4353},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsm√§ssan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/riedmiller18a/riedmiller18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/riedmiller18a.html},
  abstract = 	 {We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors - from scratch - in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment - enabling it to excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach.}
}

@article{her,
  author    = {Marcin Andrychowicz and
               Filip Wolski and
               Alex Ray and
               Jonas Schneider and
               Rachel Fong and
               Peter Welinder and
               Bob McGrew and
               Josh Tobin and
               Pieter Abbeel and
               Wojciech Zaremba},
  title     = {Hindsight Experience Replay},
  journal   = {CoRR},
  volume    = {abs/1707.01495},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.01495},
  archivePrefix = {arXiv},
  eprint    = {1707.01495},
  timestamp = {Fri, 08 Nov 2019 12:51:04 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/AndrychowiczWRS17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{imgep,
  author    = {Adrien Laversanne{-}Finot and
               Alexandre P{\'{e}}r{\'{e}} and
               Pierre{-}Yves Oudeyer},
  title     = {Curiosity Driven Exploration of Learned Disentangled Goal Spaces},
  journal   = {CoRR},
  volume    = {abs/1807.01521},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.01521},
  archivePrefix = {arXiv},
  eprint    = {1807.01521},
  timestamp = {Mon, 13 Aug 2018 16:48:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1807-01521.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Chitnis2020IntrinsicMF,
  title={Intrinsic Motivation for Encouraging Synergistic Behavior},
  author={Rohan Chitnis and Shubham Tulsiani and Saurabh Gupta and Abhinav Gupta},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.05189}
}

@inproceedings{analytic,
  title={Asymptotically optimal planning under piecewise-analytic constraints},
  author={William Vega-Brown and Nicholas Roy},
  year={2018}
}

@article{factored,
  author    = {Caelan Reed Garrett and
               Tom{\'{a}}s Lozano{-}P{\'{e}}rez and
               Leslie Pack Kaelbling},
  title     = {Sampling-Based Methods for Factored Task and Motion Planning},
  journal   = {CoRR},
  volume    = {abs/1801.00680},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.00680},
  archivePrefix = {arXiv},
  eprint    = {1801.00680},
  timestamp = {Mon, 13 Aug 2018 16:46:43 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-00680},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{sparse1,
  author    = {Martin A. Riedmiller and
               Roland Hafner and
               Thomas Lampe and
               Michael Neunert and
               Jonas Degrave and
               Tom Van de Wiele and
               Volodymyr Mnih and
               Nicolas Heess and
               Jost Tobias Springenberg},
  title     = {Learning by Playing - Solving Sparse Reward Tasks from Scratch},
  journal   = {CoRR},
  volume    = {abs/1802.10567},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.10567},
  archivePrefix = {arXiv},
  eprint    = {1802.10567},
  timestamp = {Mon, 13 Aug 2018 16:47:06 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1802-10567},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Arulkumaran2017DeepRL,
  title={Deep Reinforcement Learning: A Brief Survey},
  author={Kai Arulkumaran and Marc Peter Deisenroth and Miles Brundage and Anil Anthony Bharath},
  journal={IEEE Signal Processing Magazine},
  year={2017},
  volume={34},
  pages={26-38}
}
@inproceedings{Dantam2016IncrementalTA,
  title={Incremental Task and Motion Planning: A Constraint-Based Approach},
  author={Neil T. Dantam and Zachary K. Kingston and Swarat Chaudhuri and Lydia E. Kavraki},
  booktitle={Robotics: Science and Systems},
  year={2016}
}

@article{strips,
  added-at = {2006-04-05T23:23:26.000+0200},
  author = {Fikes, Richard E. and Nilsson, Nils J.},
  biburl = {https://www.bibsonomy.org/bibtex/220cddeef6f20a944ac127af407fccea2/gromgull},
  date-added = {2006-02-06 14:10:53 +0000},
  date-modified = {2006-02-06 14:12:42 +0000},
  description = {My Main bibliography file},
  interhash = {e2ac2a2b100c680a77c9241e5d6a9c4f},
  intrahash = {20cddeef6f20a944ac127af407fccea2},
  journal = {Artificial Intelligence},
  keywords = {planning},
  pages = 189,
  timestamp = {2006-04-05T23:23:26.000+0200},
  title = {STRIPS: A new Approach to the Application of Theorem Proving to Problem Solving},
  volume = 2,
  year = 1971
}

@article{begus_infantslearnwhattheywant,
    author = {Begus, Katarina AND Gliga, Teodora AND Southgate, Victoria},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Infants Learn What They Want to Learn: Responding to Infant Pointing Leads to Superior Learning},
    year = {2014},
    month = {10},
    volume = {9},
    pages = {1-4},
    number = {10},
    doi = {10.1371/journal.pone.0108817},
    @url = {https://doi.org/10.1371/journal.pone.0108817},
}

@incollection{chentanez,
  added-at = {2008-03-11T14:52:34.000+0100},
  address = {Cambridge, MA},
  author = {Barto, A. G. and Singh, S. and Chentanez, N.},
  biburl = {https://www.bibsonomy.org/bibtex/2779d914461051caf9df2a5cdc8ec9111/idsia},
  booktitle = {Proceedings of International Conference on Developmental Learning (ICDL)},
  citeulike-article-id = {2380790},
  interhash = {47b5645fcf1c50c15882d5a6b4fa3d11},
  intrahash = {779d914461051caf9df2a5cdc8ec9111},
  keywords = {juergen},
  priority = {2},
  publisher = {MIT Press},
  timestamp = {2008-03-11T14:55:26.000+0100},
  title = {Intrinsically Motivated Learning of Hierarchical Collections of Skills},
  year = 2004
}



@inproceedings{kimAAAI2019,
    author={Beomjoon Kim and Leslie Pack Kaelbling and Tomas Lozano-Perez},
    title={Adversarial actor-critic method for task and motion planning problems using planning experience},
    booktitle={AAAI Conference on Artificial Intelligence (AAAI)},
    year={2019},
    url={http://lis.csail.mit.edu/pubs/kim-aaai19.pdf},
    keywords={Integrated Task and Motion Planning,Learning and Optimization}
}
@inproceedings{bullet,
 author = {Coumans, Erwin},
 title = {Bullet Physics Simulation},
 booktitle = {ACM SIGGRAPH 2015 Courses},
 series = {SIGGRAPH '15},
 year = {2015},
 isbn = {978-1-4503-3634-5},
 location = {Los Angeles, California},
 articleno = {7},
 url = {http://doi.acm.org/10.1145/2776880.2792704},
 doi = {10.1145/2776880.2792704},
 acmid = {2792704},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{sspybullet,
  Author = {Garrett, Caelan},
  Title = {PyBullet Planning},
  Year = {2018}
}


@article{ppo,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {CoRR},
  volume    = {abs/1707.06347},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.06347},
  archivePrefix = {arXiv},
  eprint    = {1707.06347},
  timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SchulmanWDRK17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{interaction,
  author    = {Peter W. Battaglia and
               Razvan Pascanu and
               Matthew Lai and
               Danilo Jimenez Rezende and
               Koray Kavukcuoglu},
  title     = {Interaction Networks for Learning about Objects, Relations and Physics},
  journal   = {CoRR},
  volume    = {abs/1612.00222},
  year      = {2016},
  url       = {http://arxiv.org/abs/1612.00222},
  archivePrefix = {arXiv},
  eprint    = {1612.00222},
  timestamp = {Mon, 13 Aug 2018 16:47:50 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BattagliaPLRK16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hrn,
  title={Flexible neural representation for physics prediction},
  author={Damian Mrowca and Chengxu Zhuang and Elias Wang and Nick Haber and Li Fei-Fei and Joshua B. Tenenbaum and Daniel L. K. Yamins},
  booktitle={NeurIPS},
  year={2018}
}

@incollection{imagination_recurrent,
  title = {Recurrent World Models Facilitate Policy Evolution},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  booktitle = {Advances in Neural Information Processing Systems 31},
  pages = {2451--2463},
  year = {2018},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper/7512-recurrent-world-models-facilitate-policy-evolution},
  note = "\url{https://worldmodels.github.io}",
}

@article{local,
  author    = {Sergey Levine and
               Nolan Wagener and
               Pieter Abbeel},
  title     = {Learning Contact-Rich Manipulation Skills with Guided Policy Search},
  journal   = {CoRR},
  volume    = {abs/1501.05611},
  year      = {2015},
  url       = {http://arxiv.org/abs/1501.05611},
  archivePrefix = {arXiv},
  eprint    = {1501.05611},
  timestamp = {Mon, 13 Aug 2018 16:46:15 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LevineWA15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{i2a,
  author    = {Theophane Weber and
               S{\'{e}}bastien Racani{\`{e}}re and
               David P. Reichert and
               Lars Buesing and
               Arthur Guez and
               Danilo Jimenez Rezende and
               Adri{\`{a}} Puigdom{\`{e}}nech Badia and
               Oriol Vinyals and
               Nicolas Heess and
               Yujia Li and
               Razvan Pascanu and
               Peter W. Battaglia and
               David Silver and
               Daan Wierstra},
  title     = {Imagination-Augmented Agents for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1707.06203},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.06203},
  archivePrefix = {arXiv},
  eprint    = {1707.06203},
  timestamp = {Wed, 24 Jul 2019 11:32:11 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/WeberRRBGRBVHLP17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mbatari,
  author    = {Lukasz Kaiser and
               Mohammad Babaeizadeh and
               Piotr Milos and
               Blazej Osinski and
               Roy H. Campbell and
               Konrad Czechowski and
               Dumitru Erhan and
               Chelsea Finn and
               Piotr Kozakowski and
               Sergey Levine and
               Ryan Sepassi and
               George Tucker and
               Henryk Michalewski},
  title     = {Model-Based Reinforcement Learning for Atari},
  journal   = {CoRR},
  volume    = {abs/1903.00374},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.00374},
  archivePrefix = {arXiv},
  eprint    = {1903.00374},
  timestamp = {Sat, 30 Mar 2019 19:27:21 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1903-00374},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{a2c,
  author    = {Volodymyr Mnih and
               Adri{\`{a}} Puigdom{\`{e}}nech Badia and
               Mehdi Mirza and
               Alex Graves and
               Timothy P. Lillicrap and
               Tim Harley and
               David Silver and
               Koray Kavukcuoglu},
  title     = {Asynchronous Methods for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1602.01783},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.01783},
  archivePrefix = {arXiv},
  eprint    = {1602.01783},
  timestamp = {Mon, 13 Aug 2018 16:47:40 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MnihBMGLHSK16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{RNEA,
    author = {Luh, J. Y. S. and Walker, M. W. and Paul, R. P. C.},
    title = "{On-Line Computational Scheme for Mechanical Manipulators}",
    journal = {Journal of Dynamic Systems, Measurement, and Control},
    volume = {102},
    number = {2},
    pages = {69-76},
    year = {1980},
    month = {06},
    abstract = "{Industrial robots are mechanical manipulators whose dynamic characteristics are highly nonlinear. To control a manipulator which carries a variable or unknown load and moves along a planned path, it is required to compute the forces and torques needed to drive all its joints accurately and frequently at an adequate sampling frequency (no less than 60 Hz for the arm considered). This paper presents a new approach of computation based on the method of Newton-Euler formulation which is independent of the type of manipulator-configuration. This method involves the successive transformation of velocities and accelerations from the base of the manipulator out to the gripper, link by link, using the relationships of moving coordinate systems. Forces are then transformed back from the gripper to the base to obtain the joint torques. Theoretically the mathematical model is ‚Äúexact‚Äù. A program has been written in floating point assembly language which has an average execution time of 4.5 milliseconds on a PDP 11/45 computer for a Stanford manipulator. This allows an on-line computation within control systems with a sampling frequency no lower than 60 Hz. A further advantage of using this method is that the amount of computation increases linearly with the number of links whereas the conventional method based on Lagrangian formulation increases as the quartic of the number of links.}",
    issn = {0022-0434},
    doi = {10.1115/1.3149599},
    url = {https://doi.org/10.1115/1.3149599},
    eprint = {https://asmedigitalcollection.asme.org/dynamicsystems/article-pdf/102/2/69/4662872/69\_1.pdf},
}





@inproceedings{rrtconnect,
author = {Kuffner, James and LaValle, Steven},
year = {2000},
month = {01},
pages = {995-1001},
title = {RRT-Connect: An Efficient Approach to Single-Query Path Planning.},
volume = {2},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2000.844730}
}

@article{aux,
  author    = {Max Jaderberg and
               Volodymyr Mnih and
               Wojciech Marian Czarnecki and
               Tom Schaul and
               Joel Z. Leibo and
               David Silver and
               Koray Kavukcuoglu},
  title     = {Reinforcement Learning with Unsupervised Auxiliary Tasks},
  journal   = {CoRR},
  volume    = {abs/1611.05397},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.05397},
  archivePrefix = {arXiv},
  eprint    = {1611.05397},
  timestamp = {Mon, 13 Aug 2018 16:48:00 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/JaderbergMCSLSK16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{interface,
title = "Combined task and motion planning through an extensible planner-independent interface layer",
abstract = "The need for combined task and motion planning in robotics is well understood. Solutions to this problem have typically relied on special purpose, integrated implementations of task planning and motion planning algorithms. We propose a new approach that uses off-the-shelf task planners and motion planners and makes no assumptions about their implementation. Doing so enables our approach to directly build on, and benefit from, the vast literature and latest advances in task planning and motion planning. It uses a novel representational abstraction and requires only that failures in computing a motion plan for a high-level action be identifiable and expressible in the form of logical predicates at the task level. We evaluate the approach and illustrate its robustness through a number of experiments using a state-of-the-art robotics simulator and a PR2 robot. These experiments show the system accomplishing a diverse set of challenging tasks such as taking advantage of a tray when laying out a table for dinner and picking objects from cluttered environments where other objects need to be re-arranged before the target object can be reached.",
author = "Siddharth Srivastava and Eugene Fang and Lorenzo Riano and Rohan Chitnis and Stuart Russell and Pieter Abbeel",
year = "2014",
month = "1",
day = "1",
doi = "10.1109/ICRA.2014.6906922",
language = "English (US)",
pages = "639--646",
journal = "Proceedings - IEEE International Conference on Robotics and Automation",
issn = "1050-4729",
publisher = "Institute of Electrical and Electronics Engineers Inc.",
}

@inproceedings{semanticattachments,
 author = {Hertle, Andreas and Dornhege, Christian and Keller, Thomas and Nebel, Bernhard},
 title = {Planning with Semantic Attachments: An Object-oriented View},
 booktitle = {Proceedings of the 20th European Conference on Artificial Intelligence},
 series = {ECAI'12},
 year = {2012},
 isbn = {978-1-61499-097-0},
 location = {Montpellier, France},
 pages = {402--407},
 numpages = {6},
 url = {https://doi.org/10.3233/978-1-61499-098-7-402},
 doi = {10.3233/978-1-61499-098-7-402},
 acmid = {3007410},
 publisher = {IOS Press},
 address = {Amsterdam, The Netherlands, The Netherlands},
} 


@conference {poseestimation,
	title = {A Self-supervised Learning System for Object Detection using Physics Simulation and Multi-view Pose Estimation},
	booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	year = {2017},
	month = {09/2017},
	address = {Vancouver, Canada},
	abstract = {Impressive progress has been achieved in object detection with the use of deep learning. Nevertheless, such tools typically require a large amount of training data and significant manual effort for labeling objects. This limits their applicability in robotics, where it is necessary to scale solutions to a large number of objects and a variety of conditions. The present work proposes a fully autonomous process to train a Convolutional Neural Network (CNNs) for object detection and pose estimation in robotic setups. The application involves detection of objects placed in a clutter and in tight environments, such as a shelf. In particular, given access to 3D object models, several aspects of the environment are simulated and the models are placed in physically realistic poses with respect to their environment to generate a labeled synthetic dataset. To further improve object detection, the network self-trains over real images that are labeled using a robust multi-view pose estimation process. The proposed training process is evaluated on several existing datasets and on a dataset that we collected with a Motoman robotic manipulator. Results show that the proposed process outperforms popular training processes relying on synthetic data generation and manual annotation.},
	url = {https://www.cs.rutgers.edu/~kb572/pubs/physics_object_detection.pdf},
	author = {Mitash, C. and Bekris, K. E. and Boularias, A.}
}

@article{checkpoints,
  author    = {Nicolas Heess and
               Dhruva TB and
               Srinivasan Sriram and
               Jay Lemmon and
               Josh Merel and
               Greg Wayne and
               Yuval Tassa and
               Tom Erez and
               Ziyu Wang and
               S. M. Ali Eslami and
               Martin A. Riedmiller and
               David Silver},
  title     = {Emergence of Locomotion Behaviours in Rich Environments},
  journal   = {CoRR},
  volume    = {abs/1707.02286},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.02286},
  archivePrefix = {arXiv},
  eprint    = {1707.02286},
  timestamp = {Mon, 22 Jul 2019 16:19:02 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HeessTSLMWTEWER17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{naturerl,
	Author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	Date = {2015/02/25/online},
	Date-Added = {2019-09-18 21:16:52 -0500},
	Date-Modified = {2019-09-18 21:16:52 -0500},
	Day = {25},
	Journal = {Nature},
	L3 = {10.1038/nature14236; https://www.nature.com/articles/nature14236#supplementary-information},
	Month = {02},
	Pages = {529 EP  -},
	Publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved. SN  -},
	Title = {Human-level control through deep reinforcement learning},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/nature14236},
	Volume = {518},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1038/nature14236}}


@article{rainbow,
  author    = {Matteo Hessel and
               Joseph Modayil and
               Hado van Hasselt and
               Tom Schaul and
               Georg Ostrovski and
               Will Dabney and
               Daniel Horgan and
               Bilal Piot and
               Mohammad Gheshlaghi Azar and
               David Silver},
  title     = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1710.02298},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.02298},
  archivePrefix = {arXiv},
  eprint    = {1710.02298},
  timestamp = {Mon, 13 Aug 2018 16:48:05 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1710-02298},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hpn,
 author = {Kaelbling, Leslie Pack and Lozano-P{\'e}rez, Tom\'{a}s},
 title = {Integrated Task and Motion Planning in Belief Space},
 journal = {Int. J. Rob. Res.},
 issue_date = {August-September 2013},
 volume = {32},
 number = {9-10},
 month = aug,
 year = {2013},
 issn = {0278-3649},
 pages = {1194--1227},
 numpages = {34},
 url = {http://dx.doi.org/10.1177/0278364913484072},
 doi = {10.1177/0278364913484072},
 acmid = {2528323},
 publisher = {Sage Publications, Inc.},
 address = {Thousand Oaks, CA, USA},
 keywords = {belief space, manipulation planning, mobile manipulation, planning under uncertainty, symbolic task planning},
}

@inproceedings{pathak18largescale,
  Author = {Burda, Yuri and
  Edwards, Harri and Pathak, Deepak and
  Storkey, Amos and Darrell, Trevor and
  Efros, Alexei A.},
  Title = {Large-Scale Study of
  Curiosity-Driven Learning},
  Booktitle = {ICLR},
  Year = {2019}
}
@inproceedings{haber2018learning,
  title={Learning to Play with Intrinsically-Motivated Self-Aware Agents},
  author={Haber, Nick and Mrowca, Damian and Fei-Fei, Li and Yamins, Daniel LK},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}

@inproceedings{
choi2018contingencyaware,
title={Contingency-Aware Exploration in Reinforcement Learning},
author={Jongwook Choi and Yijie Guo and Marcin Moczulski and Junhyuk Oh and Neal Wu and Mohammad Norouzi and Honglak Lee},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HyxGB2AcY7},
}

@inproceedings{kimAAAI2018,
title={Guiding Search in Continuous State-action Spaces by Learning an Action Sampler from Off-target Search Experience},
author={Kim, Beomjoon and Kaelbling, Leslie, and Lozano-Perez, Tom√°s},
booktitle={Proceedings of the 32th AAAI Conference on Artificial Intelligence (AAAI). To appear},
year={2018},
organization={AAAI Press},
keywords={Integrated Task and Motion Planning,Learning and Optimization},
url = {http://lis.csail.mit.edu/pubs/kim-aaai18.pdf},
}

@article{scorespace,
author = {Beomjoon Kim and Zi Wang and Leslie Pack Kaelbling and Tom√°s Lozano-P√©rez},
title ={Learning to guide task and motion planning using score-space representation},
journal = {The International Journal of Robotics Research},
volume = {38},
number = {7},
pages = {793-812},
year = {2019},
doi = {10.1177/0278364919848837},

URL = { 
        https://doi.org/10.1177/0278364919848837
    
},
eprint = { 
        https://doi.org/10.1177/0278364919848837
    
}
,
    abstract = { In this paper, we propose a learning algorithm that speeds up the search in task and motion planning problems. Our algorithm proposes solutions to three different challenges that arise in learning to improve planning efficiency: what to predict, how to represent a planning problem instance, and how to transfer knowledge from one problem instance to another. We propose a method that predicts constraints on the search space based on a generic representation of a planning problem instance, called score-space, where we represent a problem instance in terms of the performance of a set of solutions attempted so far. Using this representation, we transfer knowledge, in the form of constraints, from previous problems based on the similarity in score-space. We design a sequential algorithm that efficiently predicts these constraints, and evaluate it in three different challenging task and motion planning problems. Results indicate that our approach performs orders of magnitudes faster than an unguided planner. }
}


@inproceedings{learnedheuristics,
title = "Guided search for task and motion plans using learned heuristics",
abstract = "Tasks in mobile manipulation planning often require thousands of individual motions to complete. Such tasks require reasoning about complex goals as well as the feasibility of movements in configuration space. In discrete representations, planning complexity is exponential in the length of the plan. In mobile manipulation, parameters for an action often draw from a continuous space, so we must also cope with an infinite branching factor. Task and motion planning (TAMP) methods integrate logical search over high-level actions with geometric reasoning to address this challenge. We present an algorithm that searches the space of possible task and motion plans and uses statistical machine learning to guide the search process. Our contributions are as follows: 1) we present a complete algorithm for TAMP; 2) we present a randomized local search algorithm for plan refinement that is easily formulated as a Markov decision process (MDP); 3) we apply reinforcement learning (RL) to learn a policy for this MDP; 4) we learn from expert demonstrations to efficiently search the space of high-level task plans, given options that address different (potential) infeasibilities; and 5) we run experiments to evaluate our system in a variety of simulated domains. We show significant improvements in performance over prior work.",
author = "Rohan Chitnis and Dylan Hadfield-Menell and Abhishek Gupta and Siddharth Srivastava and Edward Groshev and Christopher Lin and Pieter Abbeel",
year = "2016",
month = "6",
day = "8",
doi = "10.1109/ICRA.2016.7487165",
language = "English (US)",
volume = "2016-June",
pages = "447--454",
booktitle = "2016 IEEE International Conference on Robotics and Automation, ICRA 2016",
publisher = "Institute of Electrical and Electronics Engineers Inc.",
}

@article{Srivastava2014CombinedTA,
  title={Combined task and motion planning through an extensible planner-independent interface layer},
  author={Siddharth Srivastava and Eugene Fang and Lorenzo Riano and Rohan Chitnis and Stuart J. Russell and Pieter Abbeel},
  journal={2014 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2014},
  pages={639-646}
}

@inproceedings{GarrettRSS17,
author={Garrett, Reed and Lozano-Perez, Tom√°s and Kaelbling, Leslie},
year={2017},
title={Sample-Based Methods for Factored Task and Motion Planning},
booktitle={Robotics: Science and Systems (RSS)},
url={http://lis.csail.mit.edu/pubs/garrett-rss17.pdf},
keywords={Integrated Task and Motion Planning}}

@article{prm,
author = {Kavraki, Lydia and Svestka, Petr and Latombe, J.C. and Overmars, M.H.},
year = {1996},
month = {09},
pages = {566 - 580},
title = {Probabilistic Roadmaps for Path Planning in High-Dimensional Configuration Spaces},
volume = {12},
journal = {Robotics and Automation, IEEE Transactions on},
doi = {10.1109/70.508439}
}
@inproceedings{rrt,
    author = {Steven M. Lavalle},
    title = {Rapidly-Exploring Random Trees: A New Tool for Path Planning},
    institution = {Iowa State University},
    year = {1998}
}

@article{constraints,
author = {Kingston, Zachary and Moll, Mark and Kavraki, Lydia E.},
title = {Sampling-Based Methods for Motion Planning with Constraints},
journal = {Annual Review of Control, Robotics, and Autonomous Systems},
volume = {1},
number = {1},
pages = {159-185},
year = {2018},
doi = {10.1146/annurev-control-060117-105226},

URL = { 
        https://doi.org/10.1146/annurev-control-060117-105226
    
},
eprint = { 
        https://doi.org/10.1146/annurev-control-060117-105226
    
}
,
    abstract = { Robots with many degrees of freedom (e.g., humanoid robots and mobile manipulators) have increasingly been employed to accomplish realistic tasks in domains such as disaster relief, spacecraft logistics, and home caretaking. Finding feasible motions for these robots autonomously is essential for their operation. Sampling-based motion planning algorithms are effective for these high-dimensional systems; however, incorporating task constraints (e.g., keeping a cup level or writing on a board) into the planning process introduces significant challenges. This survey describes the families of methods for sampling-based planning with constraints and places them on a spectrum delineated by their complexity. Constrained sampling-based methods are based on two core primitive operations: (a) sampling constraint-satisfying configurations and (b) generating constraint-satisfying continuous motion. Although this article presents the basics of sampling-based planning for contextual background, it focuses on the representation of constraints and sampling-based planners that incorporate constraints. }
}

@InProceedings{asymov,
author="Gravot, Fabien
and Cambon, Stephane
and Alami, Rachid",
editor="Dario, Paolo
and Chatila, Raja",
title="aSyMov: A Planner That Deals with Intricate Symbolic and Geometric Problems",
booktitle="Robotics Research. The Eleventh International Symposium",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="100--110",
abstract="We propose an original approach to integrate symbolic task planning, and geometric motion and manipulation planning. We focus more particularly on one key aspect: the relation between the symbolic positions and their geometric counterparts. Indeed, we have developed an instantiation process that is able to propagate incrementally task-dependent as well as 3D environment-dependent constraints and to guide efficiently the search until valid geometric configurations are found that satisfy the plan at both levels. The overall process is discussed and illustrated through an implemented example.",
isbn="978-3-540-31508-7"
}


@ARTICLE{Helmert06thefast,
    author = {Malte Helmert},
    title = {The Fast Downward Planning System},
    journal = {Journal of Artificial Intelligence Research},
    year = {2006},
    volume = {26},
    pages = {191--246}
}

@article{FF,
 author = {Hoffmann, J\"{o}rg and Nebel, Bernhard},
 title = {The FF Planning System: Fast Plan Generation Through Heuristic Search},
 journal = {J. Artif. Int. Res.},
 issue_date = {January 2001},
 volume = {14},
 number = {1},
 month = may,
 year = {2001},
 issn = {1076-9757},
 pages = {253--302},
 numpages = {50},
 url = {http://dl.acm.org/citation.cfm?id=1622394.1622404},
 acmid = {1622404},
 publisher = {AI Access Foundation},
 address = {USA},
} 


@article{TAMPwRL,
  author    = {Yuqian Jiang and
               Fangkai Yang and
               Shiqi Zhang and
               Peter Stone},
  title     = {Integrating Task-Motion Planning with Reinforcement Learning for Robust
               Decision Making in Mobile Robots},
  journal   = {CoRR},
  volume    = {abs/1811.08955},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.08955},
  archivePrefix = {arXiv},
  eprint    = {1811.08955},
  timestamp = {Fri, 30 Nov 2018 12:44:28 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1811-08955},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wangIROS2018,
    author={Zi Wang and Caelan Reed Garrett and Leslie Pack Kaelbling and Tomas Lozano-Perez},
    title={Active model learning and diverse action sampling for task and motion planning},
    booktitle={International Conference on Intelligent Robots and Systems (IROS)},
    year={2018},
    url={http://lis.csail.mit.edu/pubs/wang-iros18.pdf},
        keywords={Integrated Task and Motion Planning,Learning and Optimization}
}

@article{Kroemer2016LearningSP,
  title={Learning spatial preconditions of manipulation skills using random forests},
  author={Oliver Kroemer and Gaurav S. Sukhatme},
  journal={2016 IEEE-RAS 16th International Conference on Humanoid Robots (Humanoids)},
  year={2016},
  pages={676-683}
}

@misc{lillicrap2015continuous,
    title={Continuous control with deep reinforcement learning},
    author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
    year={2015},
    eprint={1509.02971},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{thrun1992efficient,
  title={Efficient exploration in reinforcement learning},
  author={Thrun, Sebastian B},
  year={1992},
  publisher={Citeseer}
}

@phdthesis{kakade2003sample,
  title={On the sample complexity of reinforcement learning},
  author={Kakade, Sham Machandranath},
  year={2003},
  school={University of London London, England}
}

@inproceedings{pathak2017curiosity,
  title={Curiosity-driven exploration by self-supervised prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  pages={16--17},
  year={2017}
}

@inproceedings{osband2016deep,
  title={Deep exploration via bootstrapped DQN},
  author={Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={4026--4034},
  year={2016}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{Silver2016MasteringTG,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={David Silver and Aja Huang and Chris J. Maddison and Arthur Guez and Laurent Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Vedavyas Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy P. Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
  journal={Nature},
  year={2016},
  volume={529},
  pages={484-489}
}

@inproceedings{Hessel2017RainbowCI,
  title={Rainbow: Combining Improvements in Deep Reinforcement Learning},
  author={Matteo Hessel and Joseph Modayil and Hado van Hasselt and Tom Schaul and Georg Ostrovski and Will Dabney and Dan Horgan and Bilal Piot and Mohammad Gheshlaghi Azar and David Silver},
  booktitle={AAAI},
  year={2017}
}

@article{Lillicrap2015ContinuousCW,
  title={Continuous control with deep reinforcement learning},
  author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Manfred Otto Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
  journal={CoRR},
  year={2015},
  volume={abs/1509.02971}
}

@article{Vinyals2019GrandmasterLI,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Oriol Vinyals and Igor Babuschkin and Wojciech Marian Czarnecki and Micha{\"e}l Mathieu and Andrew Joseph Dudzik and Junyoung Chung and Duck Hwan Choi and Richard W. Powell and Timo Ewalds and Petko Georgiev and Junhyuk Oh and Dan Horgan and Manuel Kroiss and Ivo Danihelka and Aja Huang and Laurent Sifre and Trevor Cai and John P. Agapiou and Max Jaderberg and Alexander Sasha Vezhnevets and R{\'e}mi Leblond and Tobias Pohlen and Valentin Dalibard and David Budden and Yury Sulsky and James Molloy and Tom Le Paine and Caglar Gulcehre and Ziyu Wang and Tobias Pfaff and Yuhuai Wu and Roman Ring and Dani Yogatama and Dario W{\"u}nsch and Katrina McKinney and Oliver Smith and Tom Schaul and Timothy P. Lillicrap and Koray Kavukcuoglu and Demis Hassabis and Chris Apps and David Silver},
  journal={Nature},
  year={2019},
  pages={1-5}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018}
}

@article{kaelbling1996reinforcement,
  title={Reinforcement learning: A survey},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  journal={Journal of artificial intelligence research},
  volume={4},
  pages={237--285},
  year={1996}
}

@article{kearns2002near,
  title={Near-optimal reinforcement learning in polynomial time},
  author={Kearns, Michael and Singh, Satinder},
  journal={Machine learning},
  volume={49},
  number={2-3},
  pages={209--232},
  year={2002},
  publisher={Springer}
}

@article{brafman2002r,
  title={R-max-a general polynomial time algorithm for near-optimal reinforcement learning},
  author={Brafman, Ronen I and Tennenholtz, Moshe},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Oct},
  pages={213--231},
  year={2002}
}

@article{strehl2009reinforcement,
  title={Reinforcement learning in finite MDPs: PAC analysis},
  author={Strehl, Alexander L and Li, Lihong and Littman, Michael L},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={Nov},
  pages={2413--2444},
  year={2009}
}


@inproceedings{hazan2019provably,
  title={Provably Efficient Maximum Entropy Exploration},
  author={Hazan, Elad and Kakade, Sham and Singh, Karan and Van Soest, Abby},
  booktitle={International Conference on Machine Learning},
  pages={2681--2691},
  year={2019}
}

@inproceedings{haarnoja2018soft,
  title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1856--1865},
  year={2018}
}

@inproceedings{chentanez2005intrinsically,
  title={Intrinsically motivated reinforcement learning},
  author={Singh, Satinder P and Barto, Andrew G and Chentanez, Nuttapong},
  booktitle={Advances in neural information processing systems},
  pages={1281--1288},
  year={2005}
}

@inproceedings{kulkarni2016hierarchical,
  title={Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation},
  author={Kulkarni, Tejas D and Narasimhan, Karthik and Saeedi, Ardavan and Tenenbaum, Josh},
  booktitle={Advances in neural information processing systems},
  pages={3675--3683},
  year={2016}
}

@inproceedings{bellemare2016unifying,
  title={Unifying count-based exploration and intrinsic motivation},
  author={Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1471--1479},
  year={2016}
}

@article{Achiam2017SurpriseBasedIM,
  title={Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning},
  author={Joshua Achiam and S. Shankar Sastry},
  journal={ArXiv},
  year={2017},
  volume={abs/1703.01732}
}

@article{haber2018emergence,
  title={Emergence of structured behaviors from curiosity-based intrinsic motivation},
  author={Haber, Nick and Mrowca, Damian and Fei-Fei, Li and Yamins, Daniel LK},
  journal={arXiv preprint arXiv:1802.07461},
  year={2018}
}

@inproceedings{precup1997planning,
  title={Planning with closed-loop macro actions},
  author={Precup, Doina and Sutton, Richard S and Singh, Satinder P},
  year={1997}
}

@article{sutton1999between,
  title={Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artificial intelligence},
  volume={112},
  number={1-2},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}

@inproceedings{da2012learning,
  title={Learning parameterized skills},
  author={Da Silva, Bruno Castro and Konidaris, George and Barto, Andrew G},
  booktitle={Proceedings of the 29th International Conference on Machine Learning},
  pages={1443--1450},
  year={2012},
  organization={Omnipress}
}

@inproceedings{bacon2017option,
  title={The option-critic architecture},
  author={Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
  booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
  year={2017}
}

@inproceedings{machado2017laplacian,
  title={A laplacian framework for option discovery in reinforcement learning},
  author={Machado, Marios C and Bellemare, Marc G and Bowling, Michael},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2295--2304},
  year={2017},
  organization={JMLR. org}
}

@Article{Bellman1957,
  author = {Bellman, R.},
  title = {A {M}arkovian decision process},
  journal = {Journal of Mathematics and Mechanics},
  volume = {6},
  issue = {5},
  pages = {679--684},
  year = {1957}
}

@book{Puterman94,
        author={Martin L. Puterman},
        title={Markov Decision Processes---Discrete Stochastic
		Dynamic Programming},
        publisher = "John Wiley \& Sons, Inc.",
        address={New York, NY},
        year={1994}
}

@article{Schmidhuber2010FormalTO,
  title={Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990‚Äì2010)},
  author={J{\"u}rgen Schmidhuber},
  journal={IEEE Transactions on Autonomous Mental Development},
  year={2010},
  volume={2},
  pages={230-247}
}

@Misc{diff_phys,
title = {Differentiable Physics and Stable Modes for Tool-Use and
Manipulation Planning -- Extended Abstract},
author  = {Marc Toussaint and Kelsey R Allen and Kevin A Smith and
Josh B Tenenbaum},
booktitle  = ijcai # { (IJCAI 2019)},
note = {Sister Conference Best Paper Track -- Extended abstract of
the R:SS'18 paper},
year = {2019}
}

@inproceedings{oer,
author = {Nair, Ashvin and McGrew, Bob and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
year = {2018},
month = {05},
pages = {6292-6299},
title = {Overcoming Exploration in Reinforcement Learning with Demonstrations},
doi = {10.1109/ICRA.2018.8463162}
}

@inproceedings{lowcostmanip,
author = {Deisenroth, Marc and Rasmussen, Carl and Fox, Dieter},
year = {2011},
month = {06},
pages = {},
title = {Learning to Control a Low-Cost Manipulator using Data-Efficient Reinforcement Learning},
doi = {10.15607/RSS.2011.VII.008}
}

@inproceedings{li19relationalrl,
  Author = {Li, Richard and
  Jabri, Allan and Darrell, Trevor and Agrawal, Pulkit},
  Title = {Towards Practical Multi-object Manipulation using Relational Reinforcement Learning},
  Booktitle = {arXiv preprint arXiv:1912.11032},
  Year = {2019}
}
