@inproceedings{li2024mediq,
  title={{MediQ}: Question-Asking {LLMs} for Adaptive and Reliable Clinical Reasoning},
  author={Li, Shuyue Stella and Balachandran, Vidhisha and Feng, Shangbin and Ilgen, Jonathan and Pierson, Emma and Koh, Pang Wei and Tsvetkov, Yulia},
  booktitle={Advances in Neural Information Processing Systems},
  year={2024}
}

@inproceedings{feng2024teaching,
  title={Teaching {LLMs} to Abstain across Languages via Multilingual Feedback},
  author={Feng, Shangbin and Shi, Weijia and Wang, Yike and Ding, Wenxuan and Ahia, Orevaoghene and Li, Shuyue Stella and Balachandran, Vidhisha and Sitaram, Sunayana and Tsvetkov, Yulia},
  booktitle={Empirical Methods in Natural Language Processing (EMNLP)},  
year={2024}
}

@misc{sun2024trustllm,
      title={TrustLLM: Trustworthiness in Large Language Models}, 
      author={Lichao Sun and Yue Huang and Haoran Wang and Siyuan Wu and Qihui Zhang and Yuan Li and Chujie Gao and Yixin Huang and Wenhan Lyu and Yixuan Zhang and Xiner Li and Zhengliang Liu and Yixin Liu and Yijue Wang and Zhikun Zhang and Bertie Vidgen and Bhavya Kailkhura and Caiming Xiong and Chaowei Xiao and Chunyuan Li and Eric Xing and Furong Huang and Hao Liu and Heng Ji and Hongyi Wang and Huan Zhang and Huaxiu Yao and Manolis Kellis and Marinka Zitnik and Meng Jiang and Mohit Bansal and James Zou and Jian Pei and Jian Liu and Jianfeng Gao and Jiawei Han and Jieyu Zhao and Jiliang Tang and Jindong Wang and Joaquin Vanschoren and John Mitchell and Kai Shu and Kaidi Xu and Kai-Wei Chang and Lifang He and Lifu Huang and Michael Backes and Neil Zhenqiang Gong and Philip S. Yu and Pin-Yu Chen and Quanquan Gu and Ran Xu and Rex Ying and Shuiwang Ji and Suman Jana and Tianlong Chen and Tianming Liu and Tianyi Zhou and William Wang and Xiang Li and Xiangliang Zhang and Xiao Wang and Xing Xie and Xun Chen and Xuyu Wang and Yan Liu and Yanfang Ye and Yinzhi Cao and Yong Chen and Yue Zhao},
      year={2024},
      eprint={2401.05561},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{NEURIPS2023_63cb9921,
 author = {Wang, Boxin and Chen, Weixin and Pei, Hengzhi and Xie, Chulin and Kang, Mintong and Zhang, Chenhui and Xu, Chejian and Xiong, Zidi and Dutta, Ritik and Schaeffer, Rylan and Truong, Sang and Arora, Simran and Mazeika, Mantas and Hendrycks, Dan and Lin, Zinan and Cheng, Yu and Koyejo, Sanmi and Song, Dawn and Li, Bo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {31232--31339},
 publisher = {Curran Associates, Inc.},
 title = {DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/63cb9921eecf51bfad27a99b2c53dd6d-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{wang-etal-2023-self-instruct,
    title = "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
    author = "Wang, Yizhong  and
      Kordi, Yeganeh  and
      Mishra, Swaroop  and
      Liu, Alisa  and
      Smith, Noah A.  and
      Khashabi, Daniel  and
      Hajishirzi, Hannaneh",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.754",
    doi = "10.18653/v1/2023.acl-long.754",
    pages = "13484--13508",
    abstract = "Large {``}instruction-tuned{''} language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33{\%} absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5{\%} absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.",
}

@inproceedings{röttger2024xstest,
    title = "{XST}est: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models",
    author = {R{\"o}ttger, Paul  and
      Kirk, Hannah  and
      Vidgen, Bertie  and
      Attanasio, Giuseppe  and
      Bianchi, Federico  and
      Hovy, Dirk},
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.301",
    doi = "10.18653/v1/2024.naacl-long.301",
    pages = "5377--5400",
    abstract = "Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This risk motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way. XSTest comprises 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. We describe XSTest{'}s creation and composition, and then use the test suite to highlight systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models.",
}


@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jiang2023mistral,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@misc{mazeika2024harmbench,
      title={HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal}, 
      author={Mantas Mazeika and Long Phan and Xuwang Yin and Andy Zou and Zifan Wang and Norman Mu and Elham Sakhaee and Nathaniel Li and Steven Basart and Bo Li and David Forsyth and Dan Hendrycks},
      year={2024},
      eprint={2402.04249},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{hartvigsen2022toxigen,
  title={Toxigen: Controlling language models to generate implied and adversarial toxicity},
  author={Hartvigsen, Thomas and Gabriel, Saadia and Palangi, Hamid and Sap, Maarten and Ray, Dipankar and Kamar, Ece},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  volume={1},
  year={2022}
}

@misc{ivison2023camels,
      title={Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2}, 
      author={Hamish Ivison and Yizhong Wang and Valentina Pyatkin and Nathan Lambert and Matthew Peters and Pradeep Dasigi and Joel Jang and David Wadden and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},
      year={2023},
      eprint={2311.10702},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{anwar2024safetychallenges,
      title={Foundational Challenges in Assuring Alignment and Safety of Large Language Models}, 
      author={Usman Anwar and Abulhair Saparov and Javier Rando and Daniel Paleka and Miles Turpin and Peter Hase and Ekdeep Singh Lubana and Erik Jenner and Stephen Casper and Oliver Sourbut and Benjamin L. Edelman and Zhaowei Zhang and Mario Günther and Anton Korinek and Jose Hernandez-Orallo and Lewis Hammond and Eric Bigelow and Alexander Pan and Lauro Langosco and Tomasz Korbak and Heidi Zhang and Ruiqi Zhong and Seán Ó hÉigeartaigh and Gabriel Recchia and Giulio Corsi and Alan Chan and Markus Anderljung and Lilian Edwards and Aleksandar Petrov and Christian Schroeder de Witt and Sumeet Ramesh Motwan and Yoshua Bengio and Danqi Chen and Philip H. S. Torr and Samuel Albanie and Tegan Maharaj and Jakob Foerster and Florian Tramer and He He and Atoosa Kasirzadeh and Yejin Choi and David Krueger},
      year={2024},
      eprint={2404.09932},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.09932}, 
}

@misc{alpaca_eval,
  author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}}
}

@inproceedings{suzgun2023challenging,
  title={Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc and Chi, Ed and Zhou, Denny and others},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={13003--13051},
  year={2023}
}

@article{chen2021evaluating,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Ponde de Oliveira Pinto, Henrique and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv e-prints},
  pages={arXiv--2107},
  year={2021}
}

@misc{cobbe2021training,
      title={Training Verifiers to Solve Math Word Problems}, 
      author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.14168}, 
}


@inproceedings{hendrycks2020measuring,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{clark2020tydi,
    title = "{T}y{D}i {QA}: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages",
    author = "Clark, Jonathan H.  and
      Choi, Eunsol  and
      Collins, Michael  and
      Garrette, Dan  and
      Kwiatkowski, Tom  and
      Nikolaev, Vitaly  and
      Palomaki, Jennimaria",
    editor = "Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.30",
    doi = "10.1162/tacl_a_00317",
    pages = "454--470",
    abstract = "Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA{---}a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology{---}the set of linguistic features each language expresses{---}such that we expect models performing well on this set to generalize across a large number of the world{'}s languages. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don{'}t know the answer yet, and the data is collected directly in each language without the use of translation.",
}

@inproceedings{lin2022truthfulqa,
  title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={3214--3252},
  year={2022}
}

@article{chatgpt2022,
  title={Introducing ChatGPT},
  author={OpenAI},
  year={2022},
  url={https://openai.com/blog/chatgpt}
}

@inproceedings{NEURIPS2023_a85b405e,
 author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {53728--53741},
 publisher = {Curran Associates, Inc.},
 title = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@article{openattack,
  title={Openattack: An open-source textual adversarial attack toolkit},
  author={Zeng, Guoyang and Qi, Fanchao and Zhou, Qianrui and Zhang, Tingji and Hou, Bairu and Zang, Yuan and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2009.09191},
  year={2020}
}

@article{perspectiveapi,
  title     = {A New Generation of Perspective API: Efficient Multilingual Character-level Transformers},
  author    = {Alyssa Lees and Vinh Q. Tran and Yi Tay and Jeffrey Scott Sorensen and Jai Gupta and Donald Metzler and Lucy Vasserman},
  journal   = {Knowledge Discovery And Data Mining},
  year      = {2022},
  doi       = {10.1145/3534678.3539147},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/7016eb4f34611f97fe8c99176246e314678e03f4}
}

@inproceedings{yao2020calm,
    title={Keep CALM and Explore: Language Models for Action Generation in Text-based Games},
    author={Yao, Shunyu and Rao, Rohan and Hausknecht, Matthew and Narasimhan, Karthik},
    booktitle={Empirical Methods in Natural Language Processing (EMNLP)},
    year={2020}
}

@inproceedings{bertscore,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{stereotype1,
    title = "Stereotyping {N}orwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets",
    author = "Blodgett, Su Lin  and
      Lopez, Gilsinia  and
      Olteanu, Alexandra  and
      Sim, Robert  and
      Wallach, Hanna",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.81",
    doi = "10.18653/v1/2021.acl-long.81",
    pages = "1004--1015",
    abstract = "Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system{'}s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens{---}originating from the social sciences{---}to inventory a range of pitfalls that threaten these benchmarks{'} validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.",
}

@misc{ganguli2023capacity,
      title={The Capacity for Moral Self-Correction in Large Language Models}, 
      author={Deep Ganguli and Amanda Askell and Nicholas Schiefer and Thomas I. Liao and Kamilė Lukošiūtė and Anna Chen and Anna Goldie and Azalia Mirhoseini and Catherine Olsson and Danny Hernandez and Dawn Drain and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jackson Kernion and Jamie Kerr and Jared Mueller and Joshua Landau and Kamal Ndousse and Karina Nguyen and Liane Lovitt and Michael Sellitto and Nelson Elhage and Noemi Mercado and Nova DasSarma and Oliver Rausch and Robert Lasenby and Robin Larson and Sam Ringer and Sandipan Kundu and Saurav Kadavath and Scott Johnston and Shauna Kravec and Sheer El Showk and Tamera Lanham and Timothy Telleen-Lawton and Tom Henighan and Tristan Hume and Yuntao Bai and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Christopher Olah and Jack Clark and Samuel R. Bowman and Jared Kaplan},
      year={2023},
      eprint={2302.07459},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{stereotype2,
title = "How stereotypes are shared through language: A review and introduction of the Social Categories and Stereotypes Communication (SCSC) Framework",
abstract = "Language use plays a crucial role in the consensualization of stereotypes within cultural groups. Based on an integrative review of the literature on stereotyping and biased language use, we propose the Social Categories and Stereotypes Communication (SCSC) framework. The framework integrates largely independent areas of literature, and explicates the linguistic processes through which social-category stereotypes are shared and maintained. We distinguish two groups of biases in language use that jointly feed and maintain three fundamental cognitive variables in (shared) social-category cognition: perceived category entitativity, stereotype content, and perceived essentialism of associated stereotypic characteristics. These are: (1) Biases in linguistic labels used to denote categories, within which we discuss biases in (a) label content and (b) linguistic form of labels; (2) Biases in describing behaviors and characteristics of categorized individuals, within which we discuss biases in (a) communication content (i.e., what information is communicated), and (b) linguistic form of descriptions (i.e., how is information formulated). Together, these biases create a self-perpetuating cycle in which social-category stereotypes are shared and maintained. The framework allows for a better understanding of stereotype maintaining biases in natural language. We discuss various opportunities for further research.",
keywords = "Communication, Discrimination, Entitativity, Essentialism, Language, Linguistic bias, Prejudice, Social categorization, Stereotypes",
author = "Beukeboom, {Camiel J.} and Christian Burgers",
year = "2019",
doi = "10.12840/issn.2255-4165.017",
language = "English",
volume = "7",
pages = "1--37",
journal = "Review of Communication Research",
issn = "2255-4165",
publisher = "Review of Communication Research",
}

@misc{bolukbasi2016man,
      title={Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings}, 
      author={Tolga Bolukbasi and Kai-Wei Chang and James Zou and Venkatesh Saligrama and Adam Kalai},
      year={2016},
      eprint={1607.06520},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{stereotype3,
    title = "Language (Technology) is Power: A Critical Survey of {``}Bias{''} in {NLP}",
    author = "Blodgett, Su Lin  and
      Barocas, Solon  and
      Daum{\'e} III, Hal  and
      Wallach, Hanna",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.485",
    doi = "10.18653/v1/2020.acl-main.485",
    pages = "5454--5476",
    abstract = "We survey 146 papers analyzing {``}bias{''} in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing {``}bias{''} is an inherently normative process. We further find that these papers{'} proposed quantitative techniques for measuring or mitigating {``}bias{''} are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing {``}bias{''} in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of {``}bias{''}{---}i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements{---}and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.",
}

@inproceedings{ood1,
    title = "Distributionally Robust Language Modeling",
    author = "Oren, Yonatan  and
      Sagawa, Shiori  and
      Hashimoto, Tatsunori B.  and
      Liang, Percy",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1432",
    doi = "10.18653/v1/D19-1432",
    pages = "4227--4237",
    abstract = "Language models are generally trained on data spanning a wide range of topics (e.g., news, reviews, fiction), but they might be applied to an a priori unknown target distribution (e.g., restaurant reviews). In this paper, we first show that training on text outside the test distribution can degrade test performance when using standard maximum likelihood (MLE) training. To remedy this without the knowledge of the test distribution, we propose an approach which trains a model that performs well over a wide range of potential test distributions. In particular, we derive a new distributionally robust optimization (DRO) procedure which minimizes the loss of the model over the worst-case mixture of topics with sufficient overlap with the training distribution. Our approach, called topic conditional value at risk (topic CVaR), obtains a 5.5 point perplexity reduction over MLE when the language models are trained on a mixture of Yelp reviews and news and tested only on reviews.",
}

@book{nltk,
  title={Natural language processing with Python: analyzing text with the natural language toolkit},
  author={Bird, Steven and Klein, Ewan and Loper, Edward},
  year={2009},
  publisher={" O'Reilly Media, Inc."}
}

@article{ood2,
  title     = {BREEDS: Benchmarks for Subpopulation Shift},
  author    = {Shibani Santurkar and Dimitris Tsipras and A. Madry},
  journal   = {International Conference On Learning Representations},
  year      = {2020},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/767c6702045f2290012a259744db9edb4d55bcb8}
}

@inproceedings{ood3,
  author    = {Pang Wei Koh and Shiori Sagawa and Henrik Marklund and Sang Michael Xie and Marvin Zhang and Akshay Balsubramani and Weihua Hu and Michihiro Yasunaga and Richard Lanas Phillips and Irena Gao and Tony Lee and Etienne David and Ian Stavness and Wei Guo and Berton Earnshaw and Imran S. Haque and Sara M. Beery and Jure Leskovec and Anshul Kundaje and Emma Pierson and Sergey Levine and Chelsea Finn and Percy Liang},
  editor    = {Marina Meila and Tong Zhang},
  title     = {{WILDS:} {A} Benchmark of in-the-Wild Distribution Shifts},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  pages     = {5637-5664},
  publisher = {{PMLR}},
  year      = {2021},
  url       = {http://proceedings.mlr.press/v139/koh21a.html},
  timestamp = {Tue, 13 Dec 2022 17:28:26 +0100},
  biburl    = {https://dblp.org/rec/conf/icml/KohSMXZBHYPGLDS21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{fairness1,
  title     = {Feature Noise Induces Loss Discrepancy Across Groups},
  author    = {Fereshte Khani and Percy Liang},
  journal   = {International Conference On Machine Learning},
  year      = {2019},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/6b1fedb830fc5608d7e5f51f7aa930f0cc67483b}
}

@article{fairness2,
  title={Big Data's Disparate Impact},
  author={Solon Barocas and Andrew D. Selbst},
  journal={California Law Review},
  year={2016},
  volume={104},
  pages={671}
}

@article{fairness3,
  title     = {Fairness Through Awareness},
  author    = {C. Dwork and Moritz Hardt and T. Pitassi and O. Reingold and R. Zemel},
  journal   = {Information Technology Convergence And Services},
  year      = {2011},
  doi       = {10.1145/2090236.2090255},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/adaa0523a5c9d5f92aa2009a51226391d8e62380}
}

@article{fairness4,
  title   = {Counterfactual fairness},
  author  = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}

@article{fairness5,
  title     = {Roles for Computing in Social Change},
  author    = {Rediet Abebe and Solon Barocas and J. Kleinberg and K. Levy and Manish Raghavan and D. G. Robinson},
  journal   = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  year      = {2019},
  doi       = {10.1145/3351095.3372871},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/e9dd8f892052fcdecb0852bc422dd79bce5ce7e8}
}

@article{hownet,
  title={OpenHowNet: An Open Sememe-based Lexical Knowledge Base},
  author={Fanchao Qi and Chenghao Yang and Zhiyuan Liu and Q. Dong and Maosong Sun and Zhendong Dong},
  journal={ArXiv},
  year={2019},
  volume={abs/1901.09957}
}

@inproceedings{fever,
  title={FEVER: a Large-scale Dataset for Fact Extraction and VERification},
  author={Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Mittal, Arpit},
  booktitle={NAACL-HLT},
  year={2018}
}

@inproceedings{muppet,
  title={Human vs. Muppet: A Conservative Estimate of Human Performance on the GLUE Benchmark},
  author={Nangia, Nikita and Bowman, Samuel},
  booktitle={ACL},
  year={2019}
}

@article{Yang2018CharacterizingAA,
  title={Characterizing Audio Adversarial Examples Using Temporal Dependency},
  author={Zhuolin Yang and Bo Li and Pin-Yu Chen and Dawn Xiaodong Song},
  journal={ArXiv},
  year={2018},
  volume={abs/1809.10875}
}

@article{Carlini2018AudioAE,
  title={Audio Adversarial Examples: Targeted Attacks on Speech-to-Text},
  author={Nicholas Carlini and David A. Wagner},
  journal={2018 IEEE Security and Privacy Workshops (SPW)},
  year={2018},
  pages={1-7}
}


@inproceedings{DBLP:conf/nips/MikolovSCCD13,
  author    = {Tomas Mikolov and
               Ilya Sutskever and
               Kai Chen and
               Gregory S. Corrado and
               Jeffrey Dean},
  editor    = {Christopher J. C. Burges and
               L{\'{e}}on Bottou and
               Zoubin Ghahramani and
               Kilian Q. Weinberger},
  title     = {Distributed Representations of Words and Phrases and their Compositionality},
  booktitle = {Advances in Neural Information Processing Systems 26: 27th Annual
               Conference on Neural Information Processing Systems 2013. Proceedings
               of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States},
  pages     = {3111--3119},
  year      = {2013}
}
@inproceedings{webson-pavlick-2022-prompt,
    title = "Do Prompt-Based Models Really Understand the Meaning of Their Prompts?",
    author = "Webson, Albert  and
      Pavlick, Ellie",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.167",
    doi = "10.18653/v1/2022.naacl-main.167",
    pages = "2300--2344",
    abstract = "Recently, a boom of papers has shown extraordinary progress in zero-shot and few-shot learning with various prompt-based models. It is commonly argued that prompts help models to learn faster in the same way that humans learn faster when provided with task instructions expressed in natural language. In this study, we experiment with over 30 prompts manually written for natural language inference (NLI). We find that models can learn just as fast with many prompts that are intentionally irrelevant or even pathologically misleading as they do with instructively {``}good{''} prompts. Further, such patterns hold even for models as large as 175 billion parameters (Brown et al., 2020) as well as the recently proposed instruction-tuned models which are trained on hundreds of prompts (Sanh et al., 2021). That is, instruction-tuned models often produce good predictions with irrelevant and misleading prompts even at zero shots. In sum, notwithstanding prompt-based models{'} impressive improvement, we find evidence of serious limitations that question the degree to which such improvement is derived from models understanding task instructions in ways analogous to humans{'} use of task instructions.",
}
@inproceedings{min-etal-2022-rethinking,
    title = "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
    author = "Min, Sewon  and
      Lyu, Xinxi  and
      Holtzman, Ari  and
      Artetxe, Mikel  and
      Lewis, Mike  and
      Hajishirzi, Hannaneh  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.759",
    pages = "11048--11064",
    abstract = "Large language models (LMs) are able to in-context learn{---}perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required{---}randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.",
}
@inproceedings{
xie2022an,
title={An Explanation of In-context Learning as Implicit Bayesian Inference},
author={Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=RdJVFCHjUMI}
}
@inproceedings{miller2021accuracy,
  title={Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization},
  author={Miller, John P and Taori, Rohan and Raghunathan, Aditi and Sagawa, Shiori and Koh, Pang Wei and Shankar, Vaishaal and Liang, Percy and Carmon, Yair and Schmidt, Ludwig},
  booktitle={International Conference on Machine Learning},
  pages={7721--7735},
  year={2021},
  organization={PMLR}
}
@inproceedings{DBLP:conf/emnlp/PenningtonSM14,
  author    = {Jeffrey Pennington and
               Richard Socher and
               Christopher D. Manning},
  editor    = {Alessandro Moschitti and
               Bo Pang and
               Walter Daelemans},
  title     = {Glove: Global Vectors for Word Representation},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2014, October 25-29, 2014, Doha, Qatar,
               {A} meeting of SIGDAT, a Special Interest Group of the {ACL}},
  pages     = {1532--1543},
  publisher = {{ACL}},
  year      = {2014}
}


@article{wall2021left,
  title={Left, Right, and Gender: Exploring Interaction Traces to Mitigate Human Biases},
  author={Wall, Emily and Narechania, Arpit and Coscia, Adam and Paden, Jamal and Endert, Alex},
  journal={arXiv preprint arXiv:2108.03536},
  year={2021}
}

@article{burghardt2020origins,
  title={Origins of Algorithmic Instabilities in Crowdsourced Ranking},
  author={Burghardt, Keith and Hogg, Tad and D'Souza, Raissa and Lerman, Kristina and Posfai, Marton},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={4},
  number={CSCW2},
  pages={1--20},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{dynabenchqa,
  title={Beat the AI: Investigating Adversarial Human Annotation for Reading Comprehension},
  author={Max Bartolo and A. Roberts and Johannes Welbl and Sebastian Riedel and Pontus Stenetorp},
  journal={Transactions of the Association for Computational Linguistics},
  year={2020},
  volume={8},
  pages={662-678}
}

@article{textflint,
  title={Textflint: Unified multilingual robustness evaluation toolkit for natural language processing},
  author={Gui, Tao and Wang, Xiao and Zhang, Qi and Liu, Qin and Zou, Yicheng and Zhou, Xin and Zheng, Rui and Zhang, Chong and Wu, Qinzhuo and Ye, Jiacheng and others},
  journal={arXiv preprint arXiv:2103.11441},
  year={2021}
}

@inproceedings{dynabench,
  title={Dynabench: Rethinking Benchmarking in NLP},
  author={Douwe Kiela and Max Bartolo and Yixin Nie and Divyansh Kaushik and Atticus Geiger and Zhengxuan Wu and Bertie Vidgen and G. Prasad and Amanpreet Singh and Pratik Ringshia and Zhiyi Ma and Tristan Thrush and Sebastian Riedel and Zeerak Waseem and Pontus Stenetorp and Robin Jia and M. Bansal and Christopher Potts and Adina Williams},
  booktitle={NAACL},
  year={2021}
}

@article{stresstest,
  title={Stress test evaluation for natural language inference},
  author={Naik, Aakanksha and Ravichander, Abhilasha and Sadeh, Norman and Rose, Carolyn and Neubig, Graham},
  journal={arXiv preprint arXiv:1806.00692},
  year={2018}
}

@article{robustnessgym,
  title={Robustness gym: Unifying the nlp evaluation landscape},
  author={Goel, Karan and Rajani, Nazneen and Vig, Jesse and Tan, Samson and Wu, Jason and Zheng, Stephan and Xiong, Caiming and Bansal, Mohit and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2101.04840},
  year={2021}
}

@article{textattack,
  title={Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp},
  author={Morris, John X and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},
  journal={arXiv preprint arXiv:2005.05909},
  year={2020}
}

@inproceedings{criteria,
  title={What Will it Take to Fix Benchmarking in Natural Language Understanding?},
  author={Samuel R. Bowman and George E. Dahl},
  booktitle={NAACL},
  year={2021}
}

@inproceedings {textshield,
author = {Jinfeng Li and Tianyu Du and Shouling Ji and Rong Zhang and Quan Lu and Min Yang and Ting Wang},
title = {TextShield: Robust Text Classification Based on Multimodal Embedding and Neural Machine Translation},
booktitle = {29th {USENIX} Security Symposium ({USENIX} Security 20)},
year = {2020},
publisher = {{USENIX} Association}
}


@InProceedings{P16-1123,
  author = 	"Wang, Linlin
		and Cao, Zhu
		and de Melo, Gerard
		and Liu, Zhiyuan",
  title = 	"Relation Classification via Multi-Level Attention CNNs",
  booktitle = 	"Proceedings of the 54th Annual Meeting of the Association for      Computational Linguistics (Volume 1: Long Papers)    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1298--1307",
  location = 	"Berlin, Germany",
  doi = 	"10.18653/v1/P16-1123",
  url = 	"http://aclweb.org/anthology/P16-1123"
}

@InProceedings{P16-1072,
  author = 	"Cai, Rui
		and Zhang, Xiaodong
		and Wang, Houfeng",
  title = 	"Bidirectional Recurrent Convolutional Neural Network for Relation      Classification    ",
  booktitle = 	"Proceedings of the 54th Annual Meeting of the Association for      Computational Linguistics (Volume 1: Long Papers)    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"756--765",
  location = 	"Berlin, Germany",
  doi = 	"10.18653/v1/P16-1072",
  url = 	"http://aclweb.org/anthology/P16-1072"
}

@InProceedings{P16-2034,
  author = 	"Zhou, Peng
		and Shi, Wei
		and Tian, Jun
		and Qi, Zhenyu
		and Li, Bingchen
		and Hao, Hongwei
		and Xu, Bo",
  title = 	"Attention-Based Bidirectional Long Short-Term Memory Networks for Relation      Classification    ",
  booktitle = 	"Proceedings of the 54th Annual Meeting of the Association for      Computational Linguistics (Volume 2: Short Papers)    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"207--212",
  location = 	"Berlin, Germany",
  doi = 	"10.18653/v1/P16-2034",
  url = 	"http://aclweb.org/anthology/P16-2034"
}

@InProceedings{K17-1034,
  author = 	"Levy, Omer
		and Seo, Minjoon
		and Choi, Eunsol
		and Zettlemoyer, Luke",
  title = 	"Zero-Shot Relation Extraction via Reading Comprehension",
  booktitle = 	"Proceedings of the 21st Conference on Computational Natural Language      Learning (CoNLL 2017)    ",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"333--342",
  location = 	"Vancouver, Canada",
  doi = 	"10.18653/v1/K17-1034",
  url = 	"http://aclweb.org/anthology/K17-1034"
}

@InProceedings{D17-1005,
  author = 	"Liu, Liyuan
		and Ren, Xiang
		and Zhu, Qi
		and Zhi, Shi
		and Gui, Huan
		and Ji, Heng
		and Han, Jiawei",
  title = 	"Heterogeneous Supervision for Relation Extraction: A Representation      Learning Approach    ",
  booktitle = 	"Proceedings of the 2017 Conference on Empirical Methods in Natural      Language Processing    ",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"46--56",
  location = 	"Copenhagen, Denmark",
  doi = 	"10.18653/v1/D17-1005",
  url = 	"http://aclweb.org/anthology/D17-1005"
}

@InProceedings{P18-1046,
  author = 	"Qin, Pengda
		and XU, Weiran
		and Wang, William Yang",
  title = 	"DSGAN: Generative Adversarial Training for Distant Supervision Relation Extraction",
  booktitle = 	"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"496--505",
  location = 	"Melbourne, Australia",
  url = 	"http://aclweb.org/anthology/P18-1046"
}
@inproceedings{
zhou2022metscov,
title={{METS}-CoV: A Dataset of Medical Entity and Targeted Sentiment on {COVID}-19 Related Tweets},
author={Peilin Zhou and Zeqiang Wang and Dading Chong and Zhijiang Guo and Yining Hua and Zichang Su and Zhiyang Teng and Jiageng Wu and Jie Yang},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=GP1Ncd8nTgn}
}

@InProceedings{P18-1201,
  author = 	"Huang, Lifu
		and Ji, Heng
		and Cho, Kyunghyun
		and Dagan, Ido
		and Riedel, Sebastian
		and Voss, Clare",
  title = 	"Zero-Shot Transfer Learning for Event Extraction",
  booktitle = 	"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"2160--2170",
  location = 	"Melbourne, Australia",
  url = 	"http://aclweb.org/anthology/P18-1201"
}

@InProceedings{D16-1038,
  author = 	"Peng, Haoruo
		and Song, Yangqiu
		and Roth, Dan",
  title = 	"Event Detection and Co-reference with Minimal Supervision",
  booktitle = 	"Proceedings of the 2016 Conference on Empirical Methods in Natural      Language Processing    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"392--402",
  location = 	"Austin, Texas",
  doi = 	"10.18653/v1/D16-1038",
  url = 	"http://aclweb.org/anthology/D16-1038"
}

@article{kasai2022realtime,
  title={RealTime QA: What's the Answer Right Now?},
  author={Kasai, Jungo and Sakaguchi, Keisuke and Takahashi, Yoichi and Bras, Ronan Le and Asai, Akari and Yu, Xinyan and Radev, Dragomir and Smith, Noah A and Choi, Yejin and Inui, Kentaro},
  journal={arXiv preprint arXiv:2207.13332},
  year={2022}
}

@inproceedings{fisch-etal-2019-mrqa,
    title = "{MRQA} 2019 Shared Task: Evaluating Generalization in Reading Comprehension",
    author = "Fisch, Adam  and
      Talmor, Alon  and
      Jia, Robin  and
      Seo, Minjoon  and
      Choi, Eunsol  and
      Chen, Danqi",
    booktitle = "Proceedings of the 2nd Workshop on Machine Reading for Question Answering",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5801",
    doi = "10.18653/v1/D19-5801",
    pages = "1--13",
    abstract = "We present the results of the Machine Reading for Question Answering (MRQA) 2019 shared task on evaluating the generalization capabilities of reading comprehension systems. In this task, we adapted and unified 18 distinct question answering datasets into the same format. Among them, six datasets were made available for training, six datasets were made available for development, and the rest were hidden for final evaluation. Ten teams submitted systems, which explored various ideas including data sampling, multi-task learning, adversarial training and ensembling. The best system achieved an average F1 score of 72.5 on the 12 held-out datasets, 10.7 absolute points higher than our initial baseline based on BERT.",
}
@article{schaeffer2023emergent,
  title={Are emergent abilities of Large Language Models a mirage?},
  author={Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  journal={arXiv preprint arXiv:2304.15004},
  year={2023}
}

@article{datasheet,
  title={Datasheets for datasets},
  author={Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daum{\'e} III, Hal and Crawford, Kate},
  journal={arXiv preprint arXiv:1803.09010},
  year={2018}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{maus2023adversarial,
  title={Adversarial prompting for black box foundation models},
  author={Maus, Natalie and Chao, Patrick and Wong, Eric and Gardner, Jacob},
  journal={arXiv preprint arXiv:2302.04237},
  year={2023}
}

@inproceedings{lee2022deduplicating,
  title={Deduplicating Training Data Makes Language Models Better},
  author={Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={8424--8445},
  year={2022}
}

@inproceedings{zhao2022provably,
  title={Provably Confidential Language Modelling},
  author={Zhao, Xuandong and Li, Lei and Wang, Yu-Xiang},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={943--955},
  year={2022}
}


@article{zhang2021counterfactual,
  title={Counterfactual memorization in neural language models},
  author={Zhang, Chiyuan and Ippolito, Daphne and Lee, Katherine and Jagielski, Matthew and Tram{\`e}r, Florian and Carlini, Nicholas},
  journal={arXiv preprint arXiv:2112.12938},
  year={2021}
}

@inproceedings{shi-etal-2022-just,
    title = "Just Fine-tune Twice: Selective Differential Privacy for Large Language Models",
    author = "Shi, Weiyan  and
      Shea, Ryan  and
      Chen, Si  and
      Zhang, Chiyuan  and
      Jia, Ruoxi  and
      Yu, Zhou",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.425",
    pages = "6327--6340",
    abstract = "Protecting large language models from privacy leakage is becoming increasingly crucial with their wide adoption in real-world products. Yet applying *differential privacy* (DP), a canonical notion with provable privacy guarantees for machine learning models, to those models remains challenging due to the trade-off between model utility and privacy loss. Utilizing the fact that sensitive information in language data tends to be sparse, Shi et al. (2021) formalized a DP notion extension called *Selective Differential Privacy* (SDP) to protect only the sensitive tokens defined by a policy function. However, their algorithm only works for RNN-based models. In this paper, we develop a novel framework, *Just Fine-tune Twice* (JFT), that achieves SDP for state-of-the-art large transformer-based models. Our method is easy to implement: it first fine-tunes the model with *redacted* in-domain data, and then fine-tunes it again with the *original* in-domain data using a private training mechanism. Furthermore, we study the scenario of imperfect implementation of policy functions that misses sensitive tokens and develop systematic methods to handle it. Experiments show that our method achieves strong utility compared to previous baselines. We also analyze the SDP privacy guarantee empirically with the canary insertion attack.",
}




@inproceedings{mattern2022differentially,
    title = "Differentially Private Language Models for Secure Data Sharing",
    author = "Mattern, Justus  and
      Jin, Zhijing  and
      Weggenmann, Benjamin  and
      Schoelkopf, Bernhard  and
      Sachan, Mrinmaya",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.323",
    pages = "4860--4873",
    abstract = "To protect the privacy of individuals whose data is being shared, it is of high importance to develop methods allowing researchers and companies to release textual data while providing formal privacy guarantees to its originators. In the field of NLP, substantial efforts have been directed at building mechanisms following the framework of local differential privacy, thereby anonymizing individual text samples before releasing them. In practice, these approaches are often dissatisfying in terms of the quality of their output language due to the strong noise required for local differential privacy. In this paper, we approach the problem at hand using global differential privacy, particularly by training a generative language model in a differentially private manner and consequently sampling data from it. Using natural language prompts and a new prompt-mismatch loss, we are able to create highly accurate and fluent textual datasets taking on specific desired attributes such as sentiment or topic and resembling statistical properties of the training data. We perform thorough experiments indicating that our synthetic datasets do not leak information from our original data and are of high language quality and highly suitable for training models for further analysis on real-world data. Notably, we also demonstrate that training classifiers on private synthetic data outperforms directly training classifiers with DP-SGD.",
}

@article{panda2023differentially,
  title={Differentially Private In-Context Learning},
  author={Panda, Ashwinee and Wu, Tong and Wang, Jiachen T and Mittal, Prateek},
  journal={arXiv preprint arXiv:2305.01639},
  year={2023}
}


@article{duan2023flocks,
  title={Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models},
  author={Duan, Haonan and Dziedzic, Adam and Papernot, Nicolas and Boenisch, Franziska},
  journal={arXiv preprint arXiv:2305.15594},
  year={2023}
}


@article{shao2023quantifying,
  title={Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage},
  author={Shao, Hanyin and Huang, Jie and Zheng, Shen and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2305.12707},
  year={2023}
}

@article{yue2022synthetic,
  title={Synthetic text generation with differential privacy: A simple and practical recipe},
  author={Yue, Xiang and Inan, Huseyin A and Li, Xuechen and Kumar, Girish and McAnallen, Julia and Sun, Huan and Levitan, David and Sim, Robert},
  journal={ACL},
  year={2023}
}

@inproceedings{yu2023crepe,
    title = "{CREPE}: Open-Domain Question Answering with False Presuppositions",
    author = "Yu, Xinyan  and
      Min, Sewon  and
      Zettlemoyer, Luke  and
      Hajishirzi, Hannaneh",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.583",
    doi = "10.18653/v1/2023.acl-long.583",
    pages = "10457--10480",
    abstract = "When asking about unfamiliar topics, information seeking users often pose questions with false presuppositions. Most existing question answering (QA) datasets, in contrast, assume all questions have well defined answers. We introduce CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums. We find that 25{\%} of questions contain false presuppositions, and provide annotations for these presuppositions and their corrections. Through extensive baseline experiments, we show that adaptations of existing open-domain QA models can find presuppositions moderately well, but struggle when predicting whether a presupposition is factually correct. This is in large part due to difficulty in retrieving relevant evidence passages from a large text corpus. CREPE provides a benchmark to study question answering in the wild, and our analyses provide avenues for future work in better modeling and further studying the task.",
}

@inproceedings{ravichander2019question,
  title={Question Answering for Privacy Policies: Combining Computational and Legal},
  author={Ravichander, Abhilasha},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural},
  pages={4947--4958},
  year={2019},
  organization={Association for Computational Linguistics}
}

@inproceedings{asai-choi-2021-challenges,
    title = "Challenges in Information-Seeking {QA}: Unanswerable Questions and Paragraph Retrieval",
    author = "Asai, Akari  and
      Choi, Eunsol",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.118",
    doi = "10.18653/v1/2021.acl-long.118",
    pages = "1492--1504",
    abstract = "Recent pretrained language models {``}solved{''} many reading comprehension benchmarks, where questions are written with access to the evidence document. However, datasets containing information-seeking queries where evidence documents are provided after the queries are written independently remain challenging. We analyze why answering information-seeking queries is more challenging and where their prevalent unanswerabilities arise, on Natural Questions and TyDi QA. Our controlled experiments suggest two headrooms {--} paragraph selection and answerability prediction, i.e. whether the paired evidence document contains the answer to the query or not. When provided with a gold paragraph and knowing when to abstain from answering, existing models easily outperform a human annotator. However, predicting answerability itself remains challenging. We manually annotate 800 unanswerable examples across six languages on what makes them challenging to answer. With this new data, we conduct per-category answerability prediction, revealing issues in the current dataset collection as well as task formulation. Together, our study points to avenues for future research in information-seeking question answering, both for dataset creation and model development. Our code and annotated data is publicly available at \url{https://github.com/AkariAsai/unanswerable_qa}.",
}


@article{kim20222,
  title={QA$^2$: Question Answering with Questionable Assumptions},
  author={Kim, Najoung and Htut, Phu Mon and Bowman, Samuel R and Petty, Jackson},
  journal={arXiv preprint arXiv:2212.10003},
  year={2022}
}

@article{lukas2023analyzing,
  title={Analyzing leakage of personally identifiable information in language models},
  author={Lukas, Nils and Salem, Ahmed and Sim, Robert and Tople, Shruti and Wutschitz, Lukas and Zanella-B{\'e}guelin, Santiago},
  journal={arXiv preprint arXiv:2302.00539},
  year={2023}
}

@article{pan2023risk,
  title={On the risk of misinformation pollution with large language models},
  author={Pan, Yikang and Pan, Liangming and Chen, Wenhu and Nakov, Preslav and Kan, Min-Yen and Wang, William Yang},
  journal={arXiv preprint arXiv:2305.13661},
  year={2023}
}

@article{salvi2024conversational,
  title={On the conversational persuasiveness of large language models: A randomized controlled trial},
  author={Salvi, Francesco and Ribeiro, Manoel Horta and Gallotti, Riccardo and West, Robert},
  journal={arXiv preprint arXiv:2403.14380},
  year={2024}
}

@article{li2023multi,
  title={Multi-step jailbreaking privacy attacks on ChatGPT},
  author={Li, Haoran and Guo, Dadi and Fan, Wei and Xu, Mingshi and Song, Yangqiu},
  journal={arXiv preprint arXiv:2304.05197},
  year={2023}
}


@article{dwork2014algorithmic,
  title={The algorithmic foundations of differential privacy},
  author={Dwork, Cynthia and Roth, Aaron and others},
  journal={Foundations and Trends{\textregistered} in Theoretical Computer Science},
  volume={9},
  number={3--4},
  pages={211--407},
  year={2014},
  publisher={Now Publishers, Inc.}
}

@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}


@article{li2021large,
  title={Large language models can be strong differentially private learners},
  author={Li, Xuechen and Tramer, Florian and Liang, Percy and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2110.05679},
  year={2021}
}



@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}
@inproceedings{
chen2021a,
title={A Dataset for Answering Time-Sensitive Questions},
author={Wenhu Chen and Xinyi Wang and William Yang Wang},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=9-LSfSU74n-}
}
@article{wang2023robustness,
  title={On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective},
  author={Wang, Jindong and Hu, Xixu and Hou, Wenxin and Chen, Hao and Zheng, Runkai and Wang, Yidong and Yang, Linyi and Huang, Haojun and Ye, Wei and Geng, Xiubo and others},
  journal={arXiv preprint arXiv:2302.12095},
  year={2023}
}
@inproceedings{
si2023prompting,
title={Prompting {GPT}-3 To Be Reliable},
author={Chenglei Si and Zhe Gan and Zhengyuan Yang and Shuohang Wang and Jianfeng Wang and Jordan Lee Boyd-Graber and Lijuan Wang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=98p5x51L5af}
}
@inproceedings{krishna-etal-2020-reformulating,
    title = "Reformulating Unsupervised Style Transfer as Paraphrase Generation",
    author = "Krishna, Kalpesh  and
      Wieting, John  and
      Iyyer, Mohit",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.55",
    doi = "10.18653/v1/2020.emnlp-main.55",
    pages = "737--762",
    abstract = "Modern NLP defines the task of style transfer as modifying the style of a given sentence without appreciably changing its semantics, which implies that the outputs of style transfer systems should be paraphrases of their inputs. However, many existing systems purportedly designed for style transfer inherently warp the input{'}s meaning through attribute transfer, which changes semantic properties such as sentiment. In this paper, we reformulate unsupervised style transfer as a paraphrase generation problem, and present a simple methodology based on fine-tuning pretrained language models on automatically generated paraphrase data. Despite its simplicity, our method significantly outperforms state-of-the-art style transfer systems on both human and automatic evaluations. We also survey 23 style transfer papers and discover that existing automatic metrics can be easily gamed and propose fixed variants. Finally, we pivot to a more real-world style transfer setting by collecting a large dataset of 15M sentences in 11 diverse styles, which we use for an in-depth analysis of our system.",
}
@ARTICLE{2014arXiv1412.6572G,
       author = {{Goodfellow}, Ian J. and {Shlens}, Jonathon and {Szegedy}, Christian},
        title = "{Explaining and Harnessing Adversarial Examples}",
      journal = {ArXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = 2014,
        month = Dec,
          eid = {arXiv:1412.6572},
        pages = {arXiv:1412.6572},
archivePrefix = {arXiv},
       eprint = {1412.6572},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/#abs/2014arXiv1412.6572G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016arXiv160804644C,
       author = {{Carlini}, Nicholas and {Wagner}, David},
        title = "{Towards Evaluating the Robustness of Neural Networks}",
      journal = {ArXiv e-prints},
     keywords = {Computer Science - Cryptography and Security, Computer Science -
        Computer Vision and Pattern Recognition},
         year = 2016,
        month = Aug,
          eid = {arXiv:1608.04644},
        pages = {arXiv:1608.04644},
archivePrefix = {arXiv},
       eprint = {1608.04644},
 primaryClass = {cs.CR},
       adsurl = {https://ui.adsabs.harvard.edu/#abs/2016arXiv160804644C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018arXiv180301128C,
       author = {{Cheng}, Minhao and {Yi}, Jinfeng and {Zhang}, Huan and {Chen}, Pin-Yu
        and {Hsieh}, Cho-Jui},
        title = "{Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with
        Adversarial Examples}",
      journal = {ArXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = 2018,
        month = Mar,
          eid = {arXiv:1803.01128},
        pages = {arXiv:1803.01128},
archivePrefix = {arXiv},
       eprint = {1803.01128},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/#abs/2018arXiv180301128C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016arXiv161101603S,
       author = {{Seo}, Minjoon and {Kembhavi}, Aniruddha and {Farhadi}, Ali and
        {Hajishirzi}, Hannaneh},
        title = "{Bidirectional Attention Flow for Machine Comprehension}",
      journal = {ArXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2016,
        month = Nov,
          eid = {arXiv:1611.01603},
        pages = {arXiv:1611.01603},
archivePrefix = {arXiv},
       eprint = {1611.01603},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/#abs/2016arXiv161101603S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018arXiv180901478M,
       author = {{Meng}, Yu and {Shen}, Jiaming and {Zhang}, Chao and {Han}, Jiawei},
        title = "{Weakly-Supervised Neural Text Classification}",
      journal = {ArXiv e-prints},
     keywords = {Computer Science - Information Retrieval, Computer Science - Computation
        and Language, Computer Science - Machine Learning, Statistics -
        Machine Learning},
         year = 2018,
        month = Sep,
          eid = {arXiv:1809.01478},
        pages = {arXiv:1809.01478},
archivePrefix = {arXiv},
       eprint = {1809.01478},
 primaryClass = {cs.IR},
       adsurl = {https://ui.adsabs.harvard.edu/#abs/2018arXiv180901478M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Cohen:2005:FSB:1642293.1642400,
 author = {Cohen, Shay and Ruppin, Eytan and Dror, Gideon},
 title = {Feature Selection Based on the Shapley Value},
 booktitle = {Proceedings of the 19th International Joint Conference on Artificial Intelligence},
 series = {IJCAI'05},
 year = {2005},
 location = {Edinburgh, Scotland},
 pages = {665--670},
 numpages = {6},
 url = {http://dl.acm.org/citation.cfm?id=1642293.1642400},
 acmid = {1642400},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 



@article{nfc512,
  title={A Structured Self-Attentive Sentence Embedding},
  author={Zhouhan Lin and Minwei Feng and C{\'i}cero Nogueira dos Santos and Mo Yu and Bing Xiang and Bowen Zhou and Yoshua Bengio},
  journal={ArXiv},
  year={2017},
  volume={abs/1703.03130}
}

@inproceedings{t3,
  title={T3: Tree-Autoencoder Constrained Adversarial Text Generation for Targeted Attack},
  author={Boxin Wang and Hengzhi Pei and Boyuan Pan and Qian Chen and Shuohang Wang and Bo Li},
  year={2020},
  booktitle={EMNLP}
}

@inproceedings{advstyle,
  title={AdvStyle: Adversarial Style Transfer for Adversarial Text Generation},
  author={Boxin Wang and Nora Hollenstein and Jiahao Yu and Qian Chen and Yu Cheng and Jingjing Liu and Ce Zhang and Bo Li},
  booktitle={Preprint}
}

@inproceedings{gpate,
  title={Scalable Differentially Private Data Generator via Private Aggregation of Teacher Discriminators},
  author={Yunhui Long* and Boxin Wang* and Zhuolin Yang and Kaizhao Liang and Shuang Yang and Bhavya Kailkhura and Carl Gunter and Bo Li},
  booktitle={Preprint}
}

@inproceedings{datalens,
  title={DataLens: Scalable Privacy Preserving Generative Model via Gradient Compression and Teacher Aggregation},
  author={Boxin Wang and Yunhui Long and Luka Rimanic and Qian Chen and Ce Zhang and Bo Li},
  booktitle={Preprint}
}


@article{advfever,
  author    = {James Thorne and
               Andreas Vlachos},
  title     = {Adversarial attacks against Fact Extraction and VERification},
  journal   = {CoRR},
  volume    = {abs/1903.05543},
  year      = {2019},
  archivePrefix = {arXiv},
  eprint    = {1903.05543}
}




@misc{yelpdataset,
  author = {Yelp Dataset Challenge},
  note = {data retrieved from Yelp Dataset Challenge, 
          \url{https://www.yelp.com/dataset/challenge}},
}

@inproceedings{dialogued,
  author    = {Saizheng Zhang and
               Emily Dinan and
               Jack Urbanek and
               Arthur Szlam and
               Douwe Kiela and
               Jason Weston},
  title     = {Personalizing Dialogue Agents: {I} have a dog, do you have pets too?},
  booktitle = {ACL},
  year      = {2018}
}



@inproceedings{DBLP:conf/nips/VaswaniSPUJGKP17,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention is All you Need},
  booktitle = {NeurIPS},
  pages     = {5998--6008},
  year      = {2017}
}

@article{worstcase,
  author    = {Sicheng Zhu and
               Xiao Zhang and
               David Evans},
  title     = {Learning Adversarially Robust Representations via Worst-Case Mutual
               Information Maximization},
  journal   = {CoRR},
  volume    = {abs/2002.11798},
  year      = {2020}
}

@article{DBLP:journals/computer/Linsker88,
  author    = {Ralph Linsker},
  title     = {Self-Organization in a Perceptual Network},
  journal   = {Computer},
  volume    = {21},
  number    = {3},
  pages     = {105--117},
  year      = {1988}
}

@article{infomin,
  author    = {Yonglong Tian and
               Chen Sun and
               Ben Poole and
               Dilip Krishnan and
               Cordelia Schmid and
               Phillip Isola},
  title     = {What makes for good views for contrastive learning},
  journal   = {CoRR},
  volume    = {abs/2005.10243},
  year      = {2020}
}

@inproceedings{DBLP:conf/icml/SaunshiPAKK19,
  author    = {Nikunj Saunshi and
               Orestis Plevrakis and
               Sanjeev Arora and
               Mikhail Khodak and
               Hrishikesh Khandeparkar},
  title     = {A Theoretical Analysis of Contrastive Unsupervised Representation
               Learning},
  booktitle = {ICML},
  year      = {2019}
}

@inproceedings{DBLP:conf/iclr/TschannenDRGL20,
  author    = {Michael Tschannen and
               Josip Djolonga and
               Paul K. Rubenstein and
               Sylvain Gelly and
               Mario Lucic},
  title     = {On Mutual Information Maximization for Representation Learning},
  booktitle = {ICLR},
  year      = {2020}
}

@inproceedings{DBLP:conf/icml/CohenRK19,
  author    = {Jeremy M. Cohen and
               Elan Rosenfeld and
               J. Zico Kolter},
  title     = {Certified Adversarial Robustness via Randomized Smoothing},
  booktitle = {ICML},
  year      = {2019}
}

@inproceedings{ibp1,
  author    = {Po{-}Sen Huang and
               Robert Stanforth and
               Johannes Welbl and
               Chris Dyer and
               Dani Yogatama and
               Sven Gowal and
               Krishnamurthy Dvijotham and
               Pushmeet Kohli},
  title     = {Achieving Verified Robustness to Symbol Substitutions via Interval
               Bound Propagation},
  booktitle = {EMNLP-IJCNLP},
  year      = {2019}
}

@inproceedings{ibp2,
  author    = {Robin Jia and
               Aditi Raghunathan and
               Kerem G{\"{o}}ksel and
               Percy Liang},
  title     = {Certified Robustness to Adversarial Word Substitutions},
  booktitle = {EMNLP-IJCNLP},
  year      = {2019}
}

@article{ibp,
  author    = {Krishnamurthy Dvijotham and
               Sven Gowal and
               Robert Stanforth and
               Relja Arandjelovic and
               Brendan O'Donoghue and
               Jonathan Uesato and
               Pushmeet Kohli},
  title     = {Training verified learners with learned verifiers},
  journal   = {CoRR},
  volume    = {abs/1805.10265},
  year      = {2018}
}

@inproceedings{safer,
  author    = {Mao Ye and
               Chengyue Gong and
               Qiang Liu},
  title     = {{SAFER:} {A} Structure-free Approach for Certified Robustness to Adversarial
               Word Substitutions},
  booktitle = {ACL},
  year      = {2020}
}

@inproceedings{DBLP:conf/naacl/ZhangBH19,
  author    = {Yuan Zhang and
               Jason Baldridge and
               Luheng He},
  title     = {{PAWS:} Paraphrase Adversaries from Word Scrambling},
  booktitle = {NAACL-HLT},
  year      = {2019}
}

@inproceedings{weight_attack,
  author    = {Shuhuai Ren and
               Yihe Deng and
               Kun He and
               Wanxiang Che},
  title     = {Generating Natural Language Adversarial Examples through Probability
               Weighted Word Saliency},
  booktitle = {ACL},
  year      = {2019}
}

@inproceedings{DBLP:conf/emnlp/AlzantotSEHSC18,
  author    = {Moustafa Alzantot and
               Yash Sharma and
               Ahmed Elgohary and
               Bo{-}Jhang Ho and
               Mani B. Srivastava and
               Kai{-}Wei Chang},
  title     = {Generating Natural Language Adversarial Examples},
  booktitle = {EMNLP},
  year      = {2018}
}

@inproceedings{scpn,
  author    = {Mohit Iyyer and
               John Wieting and
               Kevin Gimpel and
               Luke Zettlemoyer},
  title     = {Adversarial Example Generation with Syntactically Controlled Paraphrase
               Networks},
  booktitle = {NAACL-HLT},
  year      = {2018}
}

@inproceedings{hotflip,
  author    = {Javid Ebrahimi and
               Anyi Rao and
               Daniel Lowd and
               Dejing Dou},
  title     = {HotFlip: White-Box Adversarial Examples for Text Classification},
  booktitle = {ACL},
  year      = {2018}
}

@article{yang2022glue,
  title={GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective},
  author={Yang, Linyi and Zhang, Shuibai and Qin, Libo and Li, Yafu and Wang, Yidong and Liu, Hanmeng and Wang, Jindong and Xie, Xing and Zhang, Yue},
  journal={arXiv preprint arXiv:2211.08073},
  year={2022}
}

@article{weber2022certifying,
  title     = {Certifying Out-of-Domain Generalization for Blackbox Functions},
  author    = {Maurice Weber and Linyi Li and Boxin Wang and Zhikuan Zhao and Bo Li and Ce Zhang},
  journal   = {International Conference on Machine Learning},
  year      = {2022},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/663d2e54d8b20fd30282ab40521961fb4438805f}
}

@article{yang2022improving,   title   = {Improving certified robustness via statistical learning with logical reasoning},   author  = {Yang, Zhuolin and Zhao, Zhikuan and Wang, Boxin and Zhang, Jiawei and Li, Linyi and Pei, Hengzhi and Karla{\v{s}}, Bojan and Liu, Ji and Guo, Heng and Zhang, Ce and others},   journal = {Advances in Neural Information Processing Systems},   volume  = {35},   pages   = {34859-34873},   year    = {2022} }

@article{yuan2023revisiting,
  title={Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations},
  author={Yuan, Lifan and Chen, Yangyi and Cui, Ganqu and Gao, Hongcheng and Zou, Fangyuan and Cheng, Xingyi and Ji, Heng and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2306.04618},
  year={2023}
}
@inproceedings{hendrycks-etal-2020-pretrained,
    title = "Pretrained Transformers Improve Out-of-Distribution Robustness",
    author = "Hendrycks, Dan  and
      Liu, Xiaoyuan  and
      Wallace, Eric  and
      Dziedzic, Adam  and
      Krishnan, Rishabh  and
      Song, Dawn",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.244",
    doi = "10.18653/v1/2020.acl-main.244",
    pages = "2744--2751",
    abstract = "Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers{'} performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.",
}
@article{agarwal2022temporal,
  title={Temporal effects on pre-trained models for language processing tasks},
  author={Agarwal, Oshin and Nenkova, Ani},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={904--921},
  year={2022},
  publisher={MIT Press}
}
@inproceedings{wang-etal-2021-textflint,
    title = "{T}ext{F}lint: Unified Multilingual Robustness Evaluation Toolkit for Natural Language Processing",
    author = "Wang, Xiao  and
      Liu, Qin  and
      Gui, Tao  and
      Zhang, Qi  and
      Zou, Yicheng  and
      Zhou, Xin  and
      Ye, Jiacheng  and
      Zhang, Yongxin  and
      Zheng, Rui  and
      Pang, Zexiong  and
      Wu, Qinzhuo  and
      Li, Zhengyan  and
      Zhang, Chong  and
      Ma, Ruotian  and
      Fei, Zichu  and
      Cai, Ruijian  and
      Zhao, Jun  and
      Hu, Xingwu  and
      Yan, Zhiheng  and
      Tan, Yiding  and
      Hu, Yuan  and
      Bian, Qiyuan  and
      Liu, Zhihua  and
      Qin, Shan  and
      Zhu, Bolin  and
      Xing, Xiaoyu  and
      Fu, Jinlan  and
      Zhang, Yue  and
      Peng, Minlong  and
      Zheng, Xiaoqing  and
      Zhou, Yaqian  and
      Wei, Zhongyu  and
      Qiu, Xipeng  and
      Huang, Xuanjing",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-demo.41",
    doi = "10.18653/v1/2021.acl-demo.41",
    pages = "347--355",
    abstract = "TextFlint is a multilingual robustness evaluation toolkit for NLP tasks that incorporates universal text transformation, task-specific transformation, adversarial attack, subpopulation, and their combinations to provide comprehensive robustness analyses. This enables practitioners to automatically evaluate their models from various aspects or to customize their evaluations as desired with just a few lines of code. TextFlint also generates complete analytical reports as well as targeted augmented data to address the shortcomings of the model in terms of its robustness. To guarantee acceptability, all the text transformations are linguistically based and all the transformed data selected (up to 100,000 texts) scored highly under human evaluation. To validate the utility, we performed large-scale empirical evaluations (over 67,000) on state-of-the-art deep learning models, classic supervised methods, and real-world systems. The toolkit is already available at https://github.com/textflint with all the evaluation results demonstrated at textflint.io.",
}
@article{Goodfellow2015ExplainingAH,
  title={Explaining and Harnessing Adversarial Examples},
  author={Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
  journal={CoRR},
  year={2015},
  volume={abs/1412.6572}
}

@article{MoosaviDezfooli2016DeepFoolAS,
  title={DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks},
  author={Seyed-Mohsen Moosavi-Dezfooli and Alhussein Fawzi and Pascal Frossard},
  journal={CVPR},
  year={2016},
  pages={2574-2582}
}

@inproceedings{Eykholt2017RobustPA,
  title={Robust Physical-World Attacks on Deep Learning Models.},
  author={Kevin Eykholt and Ivan Evtimov and Earlence Fernandes and Bo Li and Amir Rahmati and Chaowei Xiao and Atul Prakash and Tadayoshi Kohno and Dawn Xiaodong Song},
  year={2017}
}
@inproceedings{arora-etal-2021-types,
    title = "Types of Out-of-Distribution Texts and How to Detect Them",
    author = "Arora, Udit  and
      Huang, William  and
      He, He",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.835",
    doi = "10.18653/v1/2021.emnlp-main.835",
    pages = "10687--10701",
    abstract = "Despite agreement on the importance of detecting out-of-distribution (OOD) examples, there is little consensus on the formal definition of the distribution shifts of OOD examples and how to best detect them. We categorize these examples as exhibiting a background shift or semantic shift, and find that the two major approaches to OOD detection, calibration and density estimation (language modeling for text), have distinct behavior on these types of OOD data. Across 14 pairs of in-distribution and OOD English natural language understanding datasets, we find that density estimation methods consistently beat calibration methods in background shift settings and perform worse in semantic shift settings. In addition, we find that both methods generally fail to detect examples from challenge data, indicating that these examples constitute a different type of OOD data. Overall, while the categorization we apply explains many of the differences between the two methods, our results call for a more explicit definition of OOD to create better benchmarks and build detectors that can target the type of OOD data expected at test time.",
}
@misc{dhole2021nlaugmenter,
      title={NL-Augmenter: A Framework for Task-Sensitive Natural Language Augmentation}, 
      author={Kaustubh D. Dhole and Varun Gangal and Sebastian Gehrmann and Aadesh Gupta and Zhenhao Li and Saad Mahamood and Abinaya Mahendiran and Simon Mille and Ashish Srivastava and Samson Tan and Tongshuang Wu and Jascha Sohl-Dickstein and Jinho D. Choi and Eduard Hovy and Ondrej Dusek and Sebastian Ruder and Sajant Anand and Nagender Aneja and Rabin Banjade and Lisa Barthe and Hanna Behnke and Ian Berlot-Attwell and Connor Boyle and Caroline Brun and Marco Antonio Sobrevilla Cabezudo and Samuel Cahyawijaya and Emile Chapuis and Wanxiang Che and Mukund Choudhary and Christian Clauss and Pierre Colombo and Filip Cornell and Gautier Dagan and Mayukh Das and Tanay Dixit and Thomas Dopierre and Paul-Alexis Dray and Suchitra Dubey and Tatiana Ekeinhor and Marco Di Giovanni and Rishabh Gupta and Rishabh Gupta and Louanes Hamla and Sang Han and Fabrice Harel-Canada and Antoine Honore and Ishan Jindal and Przemyslaw K. Joniak and Denis Kleyko and Venelin Kovatchev and Kalpesh Krishna and Ashutosh Kumar and Stefan Langer and Seungjae Ryan Lee and Corey James Levinson and Hualou Liang and Kaizhao Liang and Zhexiong Liu and Andrey Lukyanenko and Vukosi Marivate and Gerard de Melo and Simon Meoni and Maxime Meyer and Afnan Mir and Nafise Sadat Moosavi and Niklas Muennighoff and Timothy Sum Hon Mun and Kenton Murray and Marcin Namysl and Maria Obedkova and Priti Oli and Nivranshu Pasricha and Jan Pfister and Richard Plant and Vinay Prabhu and Vasile Pais and Libo Qin and Shahab Raji and Pawan Kumar Rajpoot and Vikas Raunak and Roy Rinberg and Nicolas Roberts and Juan Diego Rodriguez and Claude Roux and Vasconcellos P. H. S. and Ananya B. Sai and Robin M. Schmidt and Thomas Scialom and Tshephisho Sefara and Saqib N. Shamsi and Xudong Shen and Haoyue Shi and Yiwen Shi and Anna Shvets and Nick Siegel and Damien Sileo and Jamie Simon and Chandan Singh and Roman Sitelew and Priyank Soni and Taylor Sorensen and William Soto and Aman Srivastava and KV Aditya Srivatsa and Tony Sun and Mukund Varma T and A Tabassum and Fiona Anting Tan and Ryan Teehan and Mo Tiwari and Marie Tolkiehn and Athena Wang and Zijian Wang and Gloria Wang and Zijie J. Wang and Fuxuan Wei and Bryan Wilie and Genta Indra Winata and Xinyi Wu and Witold Wydmański and Tianbao Xie and Usama Yaseen and M. Yee and Jing Zhang and Yue Zhang},
      year={2021},
      eprint={2112.02721},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{
hendrycks2021measuring,
title={Measuring Massive Multitask Language Understanding},
author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=d7KBjmI3GmQ}
}
@article{Papernot2016DistillationAA,
  title={Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks},
  author={Nicolas Papernot and Patrick D. McDaniel and Xi Wu and Somesh Jha and Ananthram Swami},
  journal={2016 IEEE Symposium on Security and Privacy (SP)},
  year={2016},
  pages={582-597}
}


@article{simclr,
  author    = {Ting Chen and
               Simon Kornblith and
               Mohammad Norouzi and
               Geoffrey E. Hinton},
  title     = {A Simple Framework for Contrastive Learning of Visual Representations},
  journal   = {CoRR},
  volume    = {abs/2002.05709},
  year      = {2020}
}

@article{roberta,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019}
}

@inproceedings{DBLP:conf/uss/Carlini0EKS19,
  author    = {Nicholas Carlini and
               Chang Liu and
               {\'{U}}lfar Erlingsson and
               Jernej Kos and
               Dawn Song},
  title     = {The Secret Sharer: Evaluating and Testing Unintended Memorization
               in Neural Networks},
  booktitle = {28th {USENIX} Security Symposium, {USENIX} Security 2019},
  year      = {2019}
}

@inproceedings{mireshghallah2022empirical,
  title={An empirical analysis of memorization in fine-tuned autoregressive language models},
  author={Mireshghallah, Fatemehsadat and Uniyal, Archit and Wang, Tianhao and Evans, David K and Berg-Kirkpatrick, Taylor},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={1816--1826},
  year={2022}
}

@inproceedings{DBLP:conf/sp/ShokriSSS17,
  author    = {Reza Shokri and
               Marco Stronati and
               Congzheng Song and
               Vitaly Shmatikov},
  title     = {Membership Inference Attacks Against Machine Learning Models},
  booktitle = {2017 {IEEE} Symposium on Security and Privacy, {SP} 2017},
  year      = {2017},
}

@inproceedings{DBLP:conf/ccs/HitajAP17,
  author    = {Briland Hitaj and
               Giuseppe Ateniese and
               Fernando P{\'{e}}rez{-}Cruz},
  title     = {Deep Models Under the {GAN:} Information Leakage from Collaborative
               Deep Learning},
  booktitle = {{ACM} {SIGSAC}},
  year      = {2017},
}

@inproceedings{secretrevealer,
  author    = {Yuheng Zhang and
               Ruoxi Jia and
               Hengzhi Pei and
               Wenxiao Wang and
               Bo Li and
               Dawn Song},
  title     = {The Secret Revealer: Generative Model-Inversion Attacks Against Deep
               Neural Networks},
  booktitle = {{CVPR} },
  year      = {2020},
}


@inproceedings{DBLP:conf/ijcai/LiuDRSH19,
  author    = {Lydia T. Liu and
               Sarah Dean and
               Esther Rolf and
               Max Simchowitz and
               Moritz Hardt},
  title     = {Delayed Impact of Fair Machine Learning},
  booktitle = {{IJCAI}},

  year      = {2019},
}

@inproceedings{DBLP:conf/icml/HashimotoSNL18,
  author    = {Tatsunori B. Hashimoto and
               Megha Srivastava and
               Hongseok Namkoong and
               Percy Liang},
  title     = {Fairness Without Demographics in Repeated Loss Minimization},
  booktitle = {
               {ICML} },
  year      = {2018},
}

@article{DBLP:journals/corr/abs-1710-03184,
  author    = {Pratik Gajane},
  title     = {On formalizing fairness in prediction with machine learning},
  journal   = {CoRR},
  volume    = {abs/1710.03184},
  year      = {2017},
  archivePrefix = {arXiv},
  eprint    = {1710.03184},
}

@inproceedings{DBLP:conf/nips/KilbertusRPHJS17,
  author    = {Niki Kilbertus and
               Mateo Rojas{-}Carulla and
               Giambattista Parascandolo and
               Moritz Hardt and
               Dominik Janzing and
               Bernhard Sch{\"{o}}lkopf},
  editor    = {Isabelle Guyon and
               Ulrike von Luxburg and
               Samy Bengio and
               Hanna M. Wallach and
               Rob Fergus and
               S. V. N. Vishwanathan and
               Roman Garnett},
  title     = {Avoiding Discrimination through Causal Reasoning},
  booktitle = {NeurIPS},
  year      = {2017},
}

@inproceedings{dpdl,
  author    = {Mart{\'{\i}}n Abadi and
               Andy Chu and
               Ian J. Goodfellow and
               H. Brendan McMahan and
               Ilya Mironov and
               Kunal Talwar and
               Li Zhang},
  editor    = {Edgar R. Weippl and
               Stefan Katzenbeisser and
               Christopher Kruegel and
               Andrew C. Myers and
               Shai Halevi},
  title     = {Deep Learning with Differential Privacy},
  booktitle = {{ACM} {SIGSAC} },
  year      = {2016},
}

@inproceedings{pate,
  author    = {Nicolas Papernot and
               Mart{\'{\i}}n Abadi and
               {\'{U}}lfar Erlingsson and
               Ian J. Goodfellow and
               Kunal Talwar},
  title     = {Semi-supervised Knowledge Transfer for Deep Learning from Private
               Training Data},
  booktitle = { {ICLR} },
  year      = {2017},
}

@inproceedings{spate,
  author    = {Nicolas Papernot and
               Shuang Song and
               Ilya Mironov and
               Ananth Raghunathan and
               Kunal Talwar and
               {\'{U}}lfar Erlingsson},
  title     = {Scalable Private Learning with {PATE}},
  booktitle = {{ICLR}},
  year      = {2018},
}

@inproceedings{pategan,
  author    = {James Jordon and
               Jinsung Yoon and
               Mihaela van der Schaar},
  title     = {{PATE-GAN:} Generating Synthetic Data with Differential Privacy Guarantees},
  booktitle = { {ICLR}},
  year      = {2019},
}


@inproceedings{Yang2019XLNetGA,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author={Zhilin Yang and Zihang Dai and Yiming Yang and Jaime G. Carbonell and Ruslan Salakhutdinov and Quoc V. Le},
  booktitle={NeurIPS},
  year={2019}
}

@article{Lan2019ALBERTAL,
  title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author={Zhen-Zhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.11942}
}

@inproceedings{bert,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  editor    = {Jill Burstein and
               Christy Doran and
               Thamar Solorio},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  booktitle = {NAACL-HLT},
  year      = {2019}
}

@article{gpt3,
    title={Language Models are Few-Shot Learners},
    author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
    year={2020},
    eprint={2005.14165},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@inproceedings{klimt2004enron,
  title={The enron corpus: A new dataset for email classification research},
  author={Klimt, Bryan and Yang, Yiming},
  booktitle={Machine Learning: ECML 2004: 15th European Conference on Machine Learning, Pisa, Italy, September 20-24, 2004. Proceedings 15},
  pages={217--226},
  year={2004},
  organization={Springer}
}
@article{alum,
  author    = {Xiaodong Liu and
               Hao Cheng and
               Pengcheng He and
               Weizhu Chen and
               Yu Wang and
               Hoifung Poon and
               Jianfeng Gao},
  title     = {Adversarial Training for Large Neural Language Models},
  journal   = {CoRR},
  volume    = {abs/2004.08994},
  year      = {2020}
}


@inproceedings{smart,
  author    = {Haoming Jiang and
               Pengcheng He and
               Weizhu Chen and
               Xiaodong Liu and
               Jianfeng Gao and
               Tuo Zhao},
  editor    = {Dan Jurafsky and
               Joyce Chai and
               Natalie Schluter and
               Joel R. Tetreault},
  title     = {{SMART:} Robust and Efficient Fine-Tuning for Pre-trained Natural
               Language Models through Principled Regularized Optimization},
  booktitle = {ACL},
  year      = {2020}
}

@inproceedings{mnli,
  author    = {Adina Williams and
               Nikita Nangia and
               Samuel R. Bowman},
  editor    = {Marilyn A. Walker and
               Heng Ji and
               Amanda Stent},
  title     = {A Broad-Coverage Challenge Corpus for Sentence Understanding through
               Inference},
  booktitle = {NAACL-HLT},
  year      = {2018}
}

@inproceedings{snli,
  author    = {Samuel R. Bowman and
               Gabor Angeli and
               Christopher Potts and
               Christopher D. Manning},
  editor    = {Llu{\'{\i}}s M{\`{a}}rquez and
               Chris Callison{-}Burch and
               Jian Su and
               Daniele Pighin and
               Yuval Marton},
  title     = {A large annotated corpus for learning natural language inference},
  booktitle = {EMNLP},
  year      = {2015}
}

@inproceedings{squad,
  author    = {Pranav Rajpurkar and
               Jian Zhang and
               Konstantin Lopyrev and
               Percy Liang},
  editor    = {Jian Su and
               Xavier Carreras and
               Kevin Duh},
  title     = {SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  booktitle = {EMNLP},
  year      = {2016}
}

@inproceedings{advsquad,
  author    = {Robin Jia and
               Percy Liang},
  editor    = {Martha Palmer and
               Rebecca Hwa and
               Sebastian Riedel},
  title     = {Adversarial Examples for Evaluating Reading Comprehension Systems},
  booktitle = {EMNLP},
  year      = {2017}
}


@inproceedings{textfooler,
  author    = {Di Jin and
               Zhijing Jin and
               Joey Tianyi Zhou and
               Peter Szolovits},
  title     = {Is {BERT} Really Robust? {A} Strong Baseline for Natural Language
               Attack on Text Classification and Entailment},
  booktitle = {AAAI},
  year      = {2020}
}

@article{infonce,
  author    = {A{\"{a}}ron van den Oord and
               Yazhe Li and
               Oriol Vinyals},
  title     = {Representation Learning with Contrastive Predictive Coding},
  journal   = {CoRR},
  volume    = {abs/1807.03748},
  year      = {2018}
}

@inproceedings{freelb,
  author    = {Chen Zhu and
               Yu Cheng and
               Zhe Gan and
               Siqi Sun and
               Tom Goldstein and
               Jingjing Liu},
  title     = {FreeLB: Enhanced Adversarial Training for Natural Language Understanding},
  booktitle = {ICLR},
  year      = {2020}
}


@article{tishby2000information,
  title={The information bottleneck method},
  author={Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  journal={arXiv preprint physics/0004057},
  year={2000}
}

@article{Cheng2020CLUBAC,
  title={CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information},
  author={Pengyu Cheng and Weituo Hao and Shuyang Dai and Jiachang Liu and Zhe Gan and L. Carin},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.12013}
}

@INPROCEEDINGS{deepib,
  author={N. {Tishby} and N. {Zaslavsky}},
  booktitle={2015 IEEE Information Theory Workshop (ITW)}, 
  title={Deep learning and the information bottleneck principle}, 
  year={2015},
  volume={},
  number={},
  pages={1-5},}

@inproceedings{Barber2003TheIA,
  title={The IM Algorithm: A Variational Approach to Information Maximization},
  author={David Barber and Felix V. Agakov},
  booktitle={NeurIPS},
  year={2003}
}

@inproceedings{
hjelm2018learning,
title={Learning deep representations by mutual information estimation and maximization},
author={R Devon Hjelm and Alex Fedorov and Samuel Lavoie-Marchildon and Karan Grewal and Phil Bachman and Adam Trischler and Yoshua Bengio},
booktitle={ICLR},
year={2019}
}

@inproceedings{
Kong2020A,
title={A Mutual Information Maximization Perspective of Language Representation Learning},
author={Lingpeng Kong and Cyprien de Masson d'Autume and Lei Yu and Wang Ling and Zihang Dai and Dani Yogatama},
booktitle={ICLR},
year={2020}
}


@inproceedings{Ilyas2019AdversarialEA,
  title={Adversarial Examples Are Not Bugs, They Are Features},
  author={Andrew Ilyas and Shibani Santurkar and Dimitris Tsipras and Logan Engstrom and Brandon Tran and Aleksander Madry},
  booktitle={NeurIPS},
  year={2019}
}



@InProceedings{pmlr-v80-belghazi18a,
  title = 	 {Mutual Information Neural Estimation},
  author = 	 {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
  booktitle = 	 {ICML},
  year = 	 {2018},
}

@article{gan2020large,
  title={Large-Scale Adversarial Training for Vision-and-Language Representation Learning},
  author={Gan, Zhe and Chen, Yen-Chun and Li, Linjie and Zhu, Chen and Cheng, Yu and Liu, Jingjing},
  journal={arXiv preprint arXiv:2006.06195},
  year={2020}
}

@inproceedings{
vit,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{bae,
  title={BAE: BERT-based Adversarial Examples for Text Classification},
  author={Garg, Siddhant and Ramakrishnan, Goutham},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={6174--6181},
  year={2020}
}


@inproceedings{chen2020uniter,
  title={Uniter: Universal image-text representation learning},
  author={Chen, Yen-Chun and Li, Linjie and Yu, Licheng and Kholy, Ahmed El and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{villa,
  title={Large-Scale Adversarial Training for Vision-and-Language Representation Learning},
  author={Gan, Zhe and Chen, Yen-Chun and Li, Linjie and Zhu, Chen and Cheng, Yu and Liu, Jingjing},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{checklist,
    title = "Beyond Accuracy: Behavioral Testing of {NLP} Models with {C}heck{L}ist",
    author = "Ribeiro, Marco Tulio  and
      Wu, Tongshuang  and
      Guestrin, Carlos  and
      Singh, Sameer",
    booktitle = "ACL",
    month = jul,
    year = "2020",
    pages = "4902--4912",
}

@inproceedings{ng-etal-2020-ssmba,
    title = "{SSMBA}: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness",
    author = "Ng, Nathan  and
      Cho, Kyunghyun  and
      Ghassemi, Marzyeh",
    booktitle = "EMNLP",
    month = nov,
    year = "2020",
    pages = "1268--1283",
}

@inproceedings{xie2021innout,
  author = {Sang Michael Xie and Ananya Kumar and Robert Jones and Fereshte Khani and Tengyu Ma and Percy Liang},
  booktitle = {International Conference on Learning Representations (ICLR)},
  title = {In-{N}-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness},
  year = {2021},
}

@inproceedings{fairsent,
    title = "Towards Debiasing Sentence Representations",
    author = "Liang, Paul Pu  and
      Li, Irene Mengze  and
      Zheng, Emily  and
      Lim, Yao Chong  and
      Salakhutdinov, Ruslan  and
      Morency, Louis-Philippe",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.488",
    doi = "10.18653/v1/2020.acl-main.488",
    pages = "5502--5515",
}

@inproceedings{sun-etal-2019-mitigating,
    title = "Mitigating Gender Bias in Natural Language Processing: Literature Review",
    author = "Sun, Tony  and
      Gaut, Andrew  and
      Tang, Shirlyn  and
      Huang, Yuxin  and
      ElSherief, Mai  and
      Zhao, Jieyu  and
      Mirza, Diba  and
      Belding, Elizabeth  and
      Chang, Kai-Wei  and
      Wang, William Yang",
    booktitle = "ACL",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
}

@inproceedings{zhao-etal-2017-men,
    title = "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints",
    author = "Zhao, Jieyu  and
      Wang, Tianlu  and
      Yatskar, Mark  and
      Ordonez, Vicente  and
      Chang, Kai-Wei",
    booktitle = "EMNLP",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
}

@inproceedings{Bolukbasi2016ManIT,
  title={Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},
  author={Tolga Bolukbasi and Kai-Wei Chang and James Y. Zou and Venkatesh Saligrama and A. Kalai},
  booktitle={NIPS},
  year={2016}
}

@inproceedings{manzini-etal-2019-black,
    title = "Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings",
    author = "Manzini, Thomas  and
      Yao Chong, Lim  and
      Black, Alan W  and
      Tsvetkov, Yulia",
    booktitle = "NAACL",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
}

@InProceedings{pmlr-v97-pang19a, title = {Improving Adversarial Robustness via Promoting Ensemble Diversity}, author = {Pang, Tianyu and Xu, Kun and Du, Chao and Chen, Ning and Zhu, Jun}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {4970--4979}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}}


@inproceedings{
Pang2020Rethinking,
title={Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness},
author={Tianyu Pang and Kun Xu and Yinpeng Dong and Chao Du and Ning Chen and Jun Zhu},
booktitle={International Conference on Learning Representations},
year={2020},
}

@inproceedings{
wang2021infobert,
title={InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective},
author={Wang, Boxin and Wang, Shuohang and Cheng, Yu and Gan, Zhe and Jia, Ruoxi and Li, Bo and Liu, Jingjing},
booktitle={ICLR},
year={2021}}

@article{wieting2017paranmt,
  title={ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations},
  author={Wieting, John and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1711.05732},
  year={2017}
}

@article{he2020deberta,
  title={Deberta: Decoding-enhanced bert with disentangled attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2006.03654},
  year={2020}
}

@inproceedings{zhang2019ernie,
  title={ERNIE: Enhanced language representation with informative entities},
  author={Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  booktitle={ACL},
  year={2019}
}

@inproceedings{morris-etal-2020-reevaluating,
    title = "Reevaluating Adversarial Examples in Natural Language",
    author = "Morris, John  and
      Lifland, Eli  and
      Lanchantin, Jack  and
      Ji, Yangfeng  and
      Qi, Yanjun",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics"
}

@article{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={arXiv preprint arXiv:2003.10555},
  year={2020}
}

@inproceedings{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{wang2019superglue,
  title={Superglue: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  booktitle={NeurIPS},
  year={2019}
}

@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@article{williams2017broad,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1704.05426},
  year={2017}
}

@inproceedings{kaushik2019learning,
  title={Learning The Difference That Makes A Difference With Counterfactually-Augmented Data},
  author={Kaushik, Divyansh and Hovy, Eduard and Lipton, Zachary},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{qi-etal-2021-mind,
    title = "Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer",
    author = "Qi, Fanchao  and
      Chen, Yangyi  and
      Zhang, Xurui  and
      Li, Mukai  and
      Liu, Zhiyuan  and
      Sun, Maosong",
    booktitle = "EMNLP",
    year = "2021"
}

@inproceedings{qi-etal-2021-hidden,
    title = "Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger",
    author = "Qi, Fanchao  and
      Li, Mukai  and
      Chen, Yangyi  and
      Zhang, Zhengyan  and
      Liu, Zhiyuan  and
      Wang, Yasheng  and
      Sun, Maosong",
    booktitle = "ACL-IJCNLP",
    year = "2021"
}

@article{dai2019backdoor,
  title={A backdoor attack against lstm-based text classification systems},
  author={Dai, Jiazhu and Chen, Chuanshuai and Li, Yufeng},
  journal={IEEE Access},
  volume={7},
  pages={138872--138878},
  year={2019},
  publisher={IEEE}
}

@inproceedings{borgeaud2022improving,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle={ICML},
  year={2022}
}

@article{hnsw,
  title     = {Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs},
  author    = {Malkov, Yu A and Yashunin, Dmitry A},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  volume    = {42},
  number    = {4},
  pages     = {824-836},
  year      = {2018},
  publisher = {IEEE}
}

@ARTICLE{pq,
  author={Gray, R.M. and Neuhoff, D.L.},
  journal={IEEE Transactions on Information Theory}, 
  title={Quantization}, 
  year={1998},
  volume={44},
  number={6},
  pages={2325-2383},
  doi={10.1109/18.720541}}

@ARTICLE{opq,
  author={Ge, Tiezheng and He, Kaiming and Ke, Qifa and Sun, Jian},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Optimized Product Quantization}, 
  year={2014},
  volume={36},
  number={4},
  pages={744-755},
  doi={10.1109/TPAMI.2013.240}}


@article{lee2022factuality,
  title   = {Factuality Enhanced Language Models for Open-Ended Text Generation},
  author  = {Nayeon Lee and Wei Ping and Peng Xu and Mostofa Patwary and Pascale Fung and Mohammad Shoeybi and Bryan Catanzaro},
  year    = {2022},
  journal = {NeurIPS}
}

@article{nakano2021webgpt,
  title={Webgpt: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}

@software{lmharness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}

@article{lin2021truthfulqa,
  title     = {Truthful{QA}: Measuring How Models Mimic Human Falsehoods},
  author    = {Stephanie C. Lin and Jacob Hilton and Owain Evans},
  journal   = {ACL},
  year      = {2021},
}


@inproceedings{khandelwal2019generalization,
  title={Generalization through memorization: Nearest neighbor language models},
  author={Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  journal={ICLR},
  year={2020}
}

@inproceedings{karpukhin2020dense,
  title={Dense passage retrieval for open-domain question answering},
  author={Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  booktitle={EMNLP},
  year={2020}
}

@inproceedings{guu2020retrieval,
  title={{REALM}: Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={ICML},
  year={2020}
}

@inproceedings{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive {NLP} tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  booktitle={NeurIPS},
  year={2020}
}

@article{faiss,
  title={Billion-scale similarity search with {GPUs}},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  volume={7},
  number={3},
  pages={535--547},
  year={2019},
  publisher={IEEE}
}

@article{nucleus,
  title     = {The Curious Case of Neural Text Degeneration},
  author    = {Ari Holtzman and Jan Buys and Maxwell Forbes and Yejin Choi},
  journal   = {International Conference On Learning Representations},
  year      = {2019},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/cf4aa38ae31b43fd07abe13b4ffdb265babb7be1}
}

@article{izacard2022few,
  title={Few-shot learning with retrieval augmented language models},
  author={Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  journal={arXiv preprint arXiv:2208.03299},
  year={2022}
}

@unpublished{spacy2,
    AUTHOR = {Honnibal, Matthew and Montani, Ines},
    TITLE  = {{spaCy 2}: Natural language understanding with {B}loom embeddings, convolutional neural networks and incremental parsing},
    YEAR   = {2017},
    Note   = {To appear}
}


@article{piantadosi14zipfs,
  publisher = {Springer Science and Business Media LLC},
  author    = {Steven T. Piantadosi},
  title     = {Zipf’s word frequency law in natural language: A critical review and future directions},
  year      = {2014},
  doi       = {10.3758/s13423-014-0585-6},
  pages     = {1112-1130},
  volume    = {21},
  journal   = {Psychonomic Bulletin \& Review},
  pdf       = {https://link.springer.com/content/pdf/10.3758/s13423-014-0585-6.pdf},
  url       = {https://link.springer.com/article/10.3758/s13423-014-0585-6/fulltext.html}
}

@inproceedings{selfbleu,
author = {Zhu, Yaoming and Lu, Sidi and Zheng, Lei and Guo, Jiaxian and Zhang, Weinan and Wang, Jun and Yu, Yong},
title = {Texygen: A Benchmarking Platform for Text Generation Models},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210080},
doi = {10.1145/3209978.3210080},
abstract = {We introduce Texygen, a benchmarking platform to support research on open-domain text generation models. Texygen has not only implemented a majority of text generation models, but also covered a set of metrics that evaluate the diversity, the quality and the consistency of the generated texts. The Texygen platform could help standardize the research on text generation and improve the reproductivity and reliability of future research work in text generation.},
booktitle = {The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
pages = {1097–1100},
numpages = {4},
keywords = {evaluation metrics, benchmarking, text generation},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{hans,
    title = "Investigating the Effect of Natural Language Explanations on Out-of-Distribution Generalization in Few-shot {NLI}",
    author = "Zhou, Yangqiaoyu  and
      Tan, Chenhao",
    booktitle = "Proceedings of the Second Workshop on Insights from Negative Results in NLP",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.insights-1.17",
    doi = "10.18653/v1/2021.insights-1.17",
    pages = "117--124",
    abstract = "Although neural models have shown strong performance in datasets such as SNLI, they lack the ability to generalize out-of-distribution (OOD). In this work, we formulate a few-shot learning setup and examine the effects of natural language explanations on OOD generalization. We leverage the templates in the HANS dataset and construct templated natural language explanations for each template. Although generated explanations show competitive BLEU scores against ground truth explanations, they fail to improve prediction performance. We further show that generated explanations often hallucinate information and miss key elements that indicate the label.",
}

@article{izacardunsupervised,
  title={Unsupervised Dense Information Retrieval with Contrastive Learning},
  author={Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
  journal={Transactions on Machine Learning Research}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@misc{chatgpt,
  title={Chat{GPT}},
  author={OpenAI},
  year={2022},
  howpublished={\url{https://chat.openai.com}},
}

@misc{prompthack,
  title={Introduction to Prompt Hacking},
  author={{Learn Prompting}},
  year={2023},
  howpublished={\url{https://learnprompting.org/docs/prompt_hacking/intro}},
}

@misc{shakespearean,
  title={Shakespearean},
  howpublished={\url{https://lingojam.com/shakespearean}},
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{devlin2018bert,
  title={{BERT}: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer.},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  journal={Journal of Machine Learning Research},
  year={2020}
}

% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{adapter,
  title={Parameter-Efficient Transfer Learning for NLP},
  author={Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
  booktitle={ICML},
  year={2019}
}

@inproceedings{
wang2022semattack,
author = {Wang, Boxin and Xu, Chejian and Liu, Xiangyu and Cheng, Yu and Li, Bo},
title = {{S}em{A}ttack: Natural Textual Attacks via Different Semantic Spaces},
year = {2022},
booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}
}

@inproceedings{wang2020t3,
  title={T3: Tree-Autoencoder Regularized Adversarial Text Generation for Targeted Attack},
  author={Wang, Boxin and Pei, Hengzhi and Pan, Boyuan and Chen, Qian and Wang, Shuohang and Li, Bo},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={6134--6150},
  year={2020}
}


@inproceedings{anli,
  title={Adversarial NLI: A New Benchmark for Natural Language Understanding},
  author={Nie, Yixin and Williams, Adina and Dinan, Emily and Bansal, Mohit and Weston, Jason and Kiela, Douwe},
  booktitle={ACL},
  year={2020}
}

@inproceedings{booq,
  title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  booktitle={NAACL},
  year={2019}
}

@inproceedings{izacard2021leveraging,
  title={Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering},
  author={Izacard, Gautier and Grave, {\'E}douard},
  booktitle={Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  pages={874--880},
  year={2021}
}


@inproceedings{li-etal-2016-diversity,
    title = "A Diversity-Promoting Objective Function for Neural Conversation Models",
    author = "Li, Jiwei  and
      Galley, Michel  and
      Brockett, Chris  and
      Gao, Jianfeng  and
      Dolan, Bill",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1014",
    doi = "10.18653/v1/N16-1014",
    pages = "110--119",
}

@inproceedings{instuning,
  author    = {Jason Wei and Maarten Bosma and Vincent Y. Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
  title     = {Finetuned Language Models are Zero-Shot Learners},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022},
  publisher = {OpenReview.net},
  year      = {2022},
  url       = {https://openreview.net/forum?id=gEZrGCozdqR},
  timestamp = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/WeiBZGYLDDL22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{shinn2023reflexion,
  title   = {Reflexion: an autonomous agent with dynamic memory and self-reflection},
  author  = {Noah Shinn and Beck Labash and Ashwin Gopinath},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2303.11366}
}

@article{kojima2022large,
  title     = {Large Language Models are Zero-Shot Reasoners},
  author    = {Takeshi Kojima and S. Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
  journal   = {Neural Information Processing Systems},
  year      = {2022},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/e7ad08848d5d7c5c47673ffe0da06af443643bda}
}

@article{liu2023pre,
  title     = {Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author    = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal   = {ACM Computing Surveys},
  volume    = {55},
  number    = {9},
  pages     = {1-35},
  year      = {2023},
  publisher = {ACM New York, NY}
}

@article{solaiman2021process,
  title   = {Process for adapting language models to society (palms) with values-targeted datasets},
  author  = {Solaiman, Irene and Dennison, Christy},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  pages   = {5861-5873},
  year    = {2021}
}

@article{instuning2,
  title     = {Scaling Instruction-Finetuned Language Models},
  author    = {Hyung Won Chung and Le Hou and S. Longpre and Barret Zoph and Yi Tay and W. Fedus and Eric Li and Xuezhi Wang and M. Dehghani and Siddhartha Brahma and Albert Webson and S. Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Dasha Valter and Sharan Narang and Gaurav Mishra and A. Yu and Vincent Zhao and Yanping Huang and Andrew M. Dai and Hongkun Yu and Slav Petrov and E. Chi and J. Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
  journal   = {ARXIV.ORG},
  year      = {2022},
  doi       = {10.48550/arXiv.2210.11416},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/5484d228bfc50efbac6e86677bc2ec2ee4ede1a6}
}



@inproceedings{baheti-etal-2021-just,
    title = "Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts",
    author = "Baheti, Ashutosh  and
      Sap, Maarten  and
      Ritter, Alan  and
      Riedl, Mark",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.397",
    doi = "10.18653/v1/2021.emnlp-main.397",
    pages = "4846--4862",
    abstract = "Dialogue models trained on human conversations inadvertently learn to generate toxic responses. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with offensive language and stance. Our analysis reveals that 42{\%} of human responses agree with toxic comments, whereas only 13{\%} agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 F1 for offensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19{\%} reduction in agreement with offensive comments and produces 29{\%} fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer.",
}

@inproceedings{hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={ACL},
  year={2019}
}

@inproceedings{winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Le Bras, Ronan and Bhagavatula, Chandra and Choi, Yejin},
  booktitle={AAAI},
  year={2020}
}

@inproceedings{wic,
  title={WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations},
  author={Pilehvar, Mohammad Taher and Camacho-Collados, Jose},
  booktitle={NAACL},
  year={2019}
}

@inproceedings{lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Ngoc-Quan and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  booktitle={NAACL},
  year={2016}
}

@inproceedings{dhamala2021bold,
  title={Bold: Dataset and metrics for measuring biases in open-ended language generation},
  author={Dhamala, Jwala and Sun, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={862--872},
  year={2021}
}

@inproceedings{race,
  title={RACE: Large-scale ReAding Comprehension Dataset From Examinations},
  author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  booktitle={EMNLP},
  year={2017}
}

@inproceedings{piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={AAAI},
  year={2020}
}

@article{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2102.12092},
  year={2021}
}

@article{welbl2021challenges,
  title={Challenges in detoxifying language models},
  author={Welbl, Johannes and Glaese, Amelia and Uesato, Jonathan and Dathathri, Sumanth and Mellor, John and Hendricks, Lisa Anne and Anderson, Kirsty and Kohli, Pushmeet and Coppin, Ben and Huang, Po-Sen},
  journal={Findings of EMNLP},
  year={2021}
}


@inproceedings{dathathri2019plug,
  title={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},
  author={Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne},
  booktitle={ICLR},
  year={2019}
}

@article{selfdebiasing,
  title={Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp},
  author={Schick, Timo and Udupa, Sahana and Sch{\"u}tze, Hinrich},
  journal={TACL},
  year={2021}
}

@article{megatron,
  title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author={Mohammad Shoeybi and Mostofa Ali Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
  journal={ArXiv},
  year={2019}
}

@article{mcguffie2020radicalization,
  title={The radicalization risks of {GPT}-3 and advanced neural language models},
  author={McGuffie, Kris and Newhouse, Alex},
  journal={arXiv},
  year={2020}
}

@article{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{MegatronTuring,
  title={Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model},
  author={Kharya, Paresh and Alvi, Ali},
  year={2021},
}

@article{krause2020gedi,
  title={Ge{D}i: Generative discriminator guided sequence generation},
  author={Krause, Ben and Gotmare, Akhilesh Deepak and McCann, Bryan and Keskar, Nitish Shirish and Joty, Shafiq and Socher, Richard and Rajani, Nazneen Fatema},
  journal={arXiv},
  year={2020}
}

@inproceedings{holtzman2019curious,
  title={The Curious Case of Neural Text Degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{welleck2019neural,
  title={Neural text generation with unlikelihood training},
  author={Welleck, Sean and Kulikov, Ilia and Roller, Stephen and Dinan, Emily and Cho, Kyunghyun and Weston, Jason},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@misc{Gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@inproceedings{transformers,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NIPS},
  year={2017}
}

@article{gao2020pile,
  title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@misc{adam,
    title={Adam: A Method for Stochastic Optimization},
    author={Diederik P. Kingma and Jimmy Ba},
    year={2014},
    booktitle={ICLR},
}

@article{survey,
    title={Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
    author={Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},
    year={2021},
    journal={arXiv},
}
@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@article{t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@inproceedings{wang2022exploring,
  title     = {Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models},
  author    = {Boxin Wang and Wei Ping and Chaowei Xiao and Peng Xu and Mostofa Patwary and Mohammad Shoeybi and Bo Li and Anima Anandkumar and Bryan Catanzaro},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year      = {2022},
  url       = {https://openreview.net/forum?id=v_0F4IZJZw}
}

@article{prompt1,
  title={Autoprompt: Eliciting knowledge from language models with automatically generated prompts},
  author={Shin, Taylor and Razeghi, Yasaman and Logan IV, Robert L and Wallace, Eric and Singh, Sameer},
  journal={arXiv},
  year={2020}
}

@inproceedings{li2021prefix,
  title={Prefix-{T}uning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  booktitle={ACL},
  year={2021}
}


@article{prompt3,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv},
  year={2021}
}

@article{prompt4,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Tony Z and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  journal={arXiv},
  year={2021}
}

@article{prompt5,
  title={Exploiting Cloze Questions for Few-Shot Text Classification and Natural Language Inference},
  author={Timo Schick and Hinrich Schütze},
  journal={arXiv},
  year={2020}
}

@article{prompt6,
  title={It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners},
  author={Timo Schick and Hinrich Schütze},
  journal={arXiv},
  year={2020}
}

@inproceedings{liu2021dexperts,
  title={D{E}xperts: Decoding-time controlled text generation with experts and anti-experts},
  author={Liu, Alisa and Sap, Maarten and Lu, Ximing and Swayamdipta, Swabha and Bhagavatula, Chandra and Smith, Noah A and Choi, Yejin},
  booktitle={ACL},
  year={2021}
}

@article{basta2019evaluating,
  title={Evaluating the underlying gender bias in contextualized word embeddings},
  author={Basta, Christine and Costa-Juss{\`a}, Marta R and Casas, Noe},
  journal={arXiv preprint arXiv:1904.08783},
  year={2019}
}

@inproceedings{gehman2020realtoxicityprompts,
  title={Real{T}oxicity{P}rompts: Evaluating neural toxic degeneration in language models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  booktitle={Findings in EMNLP},
  year={2020}
}

@inproceedings{gururangan2020don,
  title={Don't stop pretraining: adapt language models to domains and tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  booktitle={ACL},
  year={2020}
}

@inproceedings{may2019measuring,
  title={On measuring social biases in sentence encoders},
  author={May, Chandler and Wang, Alex and Bordia, Shikha and Bowman, Samuel R and Rudinger, Rachel},
  booktitle={NAACL},
  year={2019}
}

@inproceedings{wallace2019universal,
  title={Universal adversarial triggers for attacking and analyzing NLP},
  author={Wallace, Eric and Feng, Shi and Kandpal, Nikhil and Gardner, Matt and Singh, Sameer},
  booktitle={EMNLP},
  year={2019}
}

@inproceedings{xu2021detoxifying,
  title={Detoxifying language models risks marginalizing minority voices},
  author={Xu, Albert and Pathak, Eshaan and Wallace, Eric and Gururangan, Suchin and Sap, Maarten and Klein, Dan},
  booktitle={NAACL},
  year={2021}
}

@inproceedings{zhao2019gender,
  title={Gender bias in contextualized word embeddings},
  author={Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Cotterell, Ryan and Ordonez, Vicente and Chang, Kai-Wei},
  booktitle={NAACL},
  year={2019}
}

@article{smith2022using,
  title={Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model},
  author={Shaden Smith and Mostofa Patwary and Brandon Norick and Patrick LeGresley and Samyam Rajbhandari and Jared Casper and  Zhun Liu and Shrimai Prabhumoye and George Zerveas and Vijay Korthikanti and Elton Zhang and Rewon Child and Reza Yazdani Aminabadi and Julie Bernauer and Xia Song and Mohammad Shoeybi and Yuxiong He and Michael Houston and Saurabh Tiwary and Bryan Catanzaro
},
  journal={arXiv},
  year={2022}
}

@article{fedus2021switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021}
}

@inproceedings{kim2016sequence,
  title={Sequence-level knowledge distillation},
  author={Kim, Yoon and Rush, Alexander M},
  booktitle={EMNLP},
  year={2016}
}

@inproceedings{bengio2015scheduled,
  title={Scheduled sampling for sequence prediction with recurrent neural networks},
  author={Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
  booktitle={NIPS},
  year={2015}
}

@article{hinton2002training,
  title={Training products of experts by minimizing contrastive divergence},
  author={Hinton, Geoffrey E},
  journal={Neural computation},
  year={2002},
}

@software{eval-harness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@article{perez2022red,
  title={Red teaming language models with language models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2202.03286},
  year={2022}
}

@inproceedings{petroni2019language,
  title={Language models as knowledge bases?},
  author={Petroni, Fabio and Rockt{\"a}schel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H and Riedel, Sebastian},
  booktitle={EMNLP},
  year={2019}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@inproceedings{meng2022locating,
  title={Locating and editing factual knowledge in {GPT}},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  booktitle={NeurIPS},
  year={2022}
}

@article{yogatama2021adaptive,
  title={Adaptive semiparametric language models},
  author={Yogatama, Dani and de Masson d’Autume, Cyprien and Kong, Lingpeng},
  journal={Transactions of the Association for Computational Linguistics},
  year={2021},
  publisher={MIT Press}
}

@inproceedings{bilotti2007structured,
  title={Structured retrieval for question answering},
  author={Bilotti, Matthew W and Ogilvie, Paul and Callan, Jamie and Nyberg, Eric},
  booktitle={Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval},
  year={2007}
}

@article{shuster2021retrieval,
  title={Retrieval augmentation reduces hallucination in conversation},
  author={Shuster, Kurt and Poff, Spencer and Chen, Moya and Kiela, Douwe and Weston, Jason},
  journal={arXiv preprint arXiv:2104.07567},
  year={2021}
}

@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}

@inproceedings{zhang2018guiding,
  title={Guiding neural machine translation with retrieved translation pieces},
  author={Zhang, Jingyi and Utiyama, Masao and Sumita, Eiichro and Neubig, Graham and Nakamura, Satoshi},
  booktitle={NAACL},
  year={2018}
}

@article{komeili2021internet,
  title={Internet-augmented dialogue generation},
  author={Komeili, Mojtaba and Shuster, Kurt and Weston, Jason},
  journal={arXiv preprint arXiv:2107.07566},
  year={2021}
}

@article{izacard2022unsupervised,
  title={Unsupervised dense information retrieval with contrastive learning},
  author={Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
   journal={Transactions on Machine Learning Research},
  year={2022}
}

@inproceedings{lewis2019bart,
  title={{BART}: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  booktitle={ACL},
  year={2020}
}

@misc{asuncion2007uci,
  title={UCI machine learning repository},
  author={Asuncion, Arthur and Newman, David},
  year={2007},
  publisher={Irvine, CA, USA}
}

@incollection{angwin2016machine,
  title={Machine bias},
  author={Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Lauren},
  booktitle={Ethics of Data and Analytics},
  pages={254--264},
  year={2016},
  publisher={Auerbach Publications}
}

@misc{healthdataset,
  title={Heritage Health Prize Kaggle},
  author={Kaggle Inc.},
  howpublished = {\url{https://www.kaggle.com/c/hhp}},
  url = {https://www.kaggle.com/c/hhp},
  urldate = {2022-05-17},
  year={2022}
}

@article{wightman1998lsac,
  title={LSAC National Longitudinal Bar Passage Study. LSAC Research Report Series.},
  author={Wightman, Linda F},
  year={1998},
  publisher={ERIC},
  journal={Law School Admission Council}
}

@inproceedings{warstadt-etal-2020-learning,
    title = "Learning Which Features Matter: {R}o{BERT}a Acquires a Preference for Linguistic Generalizations (Eventually)",
    author = "Warstadt, Alex  and
      Zhang, Yian  and
      Li, Xiaocheng  and
      Liu, Haokun  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.16",
    doi = "10.18653/v1/2020.emnlp-main.16",
    pages = "217--235",
    abstract = "One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding. However, we want pretrained models to learn not only to represent linguistic features, but also to use those features preferentially during fine-turning. With this goal in mind, we introduce a new English-language diagnostic set called MSGS (the Mixed Signals Generalization Set), which consists of 20 ambiguous binary classification tasks that we use to test whether a pretrained model prefers linguistic or surface generalizations during finetuning. We pretrain RoBERTa from scratch on quantities of data ranging from 1M to 1B words and compare their performance on MSGS to the publicly available RoBERTa{\_}BASE. We find that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones. Eventually, with about 30B words of pretraining data, RoBERTa{\_}BASE does consistently demonstrate a linguistic bias with some regularity. We conclude that while self-supervised pretraining is an effective way to learn helpful inductive biases, there is likely room to improve the rate at which models learn which features matter.",
}

@inproceedings{mccoy-etal-2019-right,
    title = "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
    author = "McCoy, Tom  and
      Pavlick, Ellie  and
      Linzen, Tal",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1334",
    doi = "10.18653/v1/P19-1334",
    pages = "3428--3448",
    abstract = "A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.",
}


@inproceedings{ethics,
  author       = {Dan Hendrycks and
                  Collin Burns and
                  Steven Basart and
                  Andrew Critch and
                  Jerry Li and
                  Dawn Song and
                  Jacob Steinhardt},
  title        = {Aligning {AI} With Shared Human Values},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021}
}

@inproceedings{jiminy,
  author       = {Dan Hendrycks and
                  Mantas Mazeika and
                  Andy Zou and
                  Sahil Patel and
                  Christine Zhu and
                  Jesus Navarro and
                  Dawn Song and
                  Bo Li and
                  Jacob Steinhardt},
  title        = {What Would Jiminy Cricket Do? Towards Agents That Behave Morally},
  booktitle    = {Proceedings of the Neural Information Processing Systems Track on
                  Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December
                  2021, virtual},
  year         = {2021}
}

@inproceedings{chen2021badnl,
  title={Badnl: Backdoor attacks against nlp models with semantic-preserving improvements},
  author={Chen, Xiaoyi and Salem, Ahmed and Chen, Dingfan and Backes, Michael and Ma, Shiqing and Shen, Qingni and Wu, Zhonghai and Zhang, Yang},
  booktitle={ACSAC},
  year={2021}
}

@inproceedings{DBLP:conf/nips/WangXWG0GA021,
  author       = {Boxin Wang and
                  Chejian Xu and
                  Shuohang Wang and
                  Zhe Gan and
                  Yu Cheng and
                  Jianfeng Gao and
                  Ahmed Hassan Awadallah and
                  Bo Li},
  editor       = {Joaquin Vanschoren and
                  Sai{-}Kit Yeung},
  title        = {Adversarial {GLUE:} {A} Multi-Task Benchmark for Robustness Evaluation
                  of Language Models},
  booktitle    = {Proceedings of the Neural Information Processing Systems Track on
                  Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December
                  2021, virtual},
  year         = {2021},
  url          = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/335f5352088d7d9bf74191e006d8e24c-Abstract-round2.html},
  timestamp    = {Thu, 05 May 2022 16:53:59 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/WangXWG0GA021.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{socher-etal-2013-recursive,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1170",
    pages = "1631--1642",
}


@inproceedings{brown2022does,
  title={What Does it Mean for a Language Model to Preserve Privacy?},
  author={Brown, Hannah and Lee, Katherine and Mireshghallah, Fatemehsadat and Shokri, Reza and Tram{\`e}r, Florian},
  booktitle={2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={2280--2292},
  year={2022}
}


@article{huang2022large,
  title={Are Large Pre-Trained Language Models Leaking Your Personal Information?},
  author={Huang, Jie and Shao, Hanyin and Chang, Kevin Chen-Chuan},
  journal={EMNLP Findings},
  year={2022}
}

@inproceedings{
carlini2023quantifying,
title={Quantifying Memorization Across Neural Language Models},
author={Nicholas Carlini and Daphne Ippolito and Matthew Jagielski and Katherine Lee and Florian Tramer and Chiyuan Zhang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=TatRHT_1cK}
}
@inproceedings{carlini2021extracting,
  title={Extracting Training Data from Large Language Models.},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom B and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={USENIX Security Symposium},
  volume={6},
  year={2021}
}

@inproceedings{carlini2023extracting,
  title={Extracting Training Data from Diffusion Models},
  author={Nicholas Carlini and Jamie Hayes and Milad Nasr and Matthew Jagielski and Vikash Sehwag and Florian Tramer and Borja Balle and Daphne Ippolito and  Eric Wallace},
  booktitle={arXiv:2301.13188v1},
  year={2023},
  url={https://www.usenix.org/system/files/sec21-carlini-extracting.pdf}
}


@inproceedings{reynolds2021prompt,
  title={Prompt programming for large language models: Beyond the few-shot paradigm},
  author={Laria Reynolds and Kyle McDonell},
  booktitle={In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  year={2021}
}

@article{cui2022unified,
  title={A unified evaluation of textual backdoor learning: Frameworks and benchmarks},
  author={Cui, Ganqu and Yuan, Lifan and He, Bingxiang and Chen, Yangyi and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2206.08514},
  year={2022}
}

@inproceedings{ouyangtraining,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Gray, Alex and others},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}


@InProceedings{pmlr-v28-zemel13,
  title = 	 {Learning Fair Representations},
  author = 	 {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {325--333},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/zemel13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/zemel13.html},
  abstract = 	 {We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a  whole), and individual fairness (similar individuals should be treated similarly).  We formulate fairness as an optimization problem of finding a  good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group.  We show positive results of our algorithm relative to other known techniques, on three datasets.  Moreover, we demonstrate several advantages to our approach.  First, our intermediate representation can be used for other classification tasks (i.e., transfer  learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.}
}

@inproceedings{NIPS2016_9d268236,
 author = {Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Equality of Opportunity in Supervised Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf},
 volume = {29},
 year = {2016}
}

@inproceedings{NEURIPS2019_b4189d9d,
 author = {Zhao, Han and Gordon, Geoff},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Inherent Tradeoffs in Learning Fair Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/b4189d9de0fb2b9cce090bd1a15e3420-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{kang2022certifying,
  title={Certifying some distributional fairness with subpopulation decomposition},
  author={Kang, Mintong and Li, Linyi and Weber, Maurice and Liu, Yang and Zhang, Ce and Li, Bo},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31045--31058},
  year={2022}
}

@article{ray2022fairness,
  title={Fairness in federated learning via core-stability},
  author={Ray Chaudhury, Bhaskar and Li, Linyi and Kang, Mintong and Li, Bo and Mehta, Ruta},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={5738--5750},
  year={2022}
}

@inproceedings{NEURIPS2020_55d491cf,
 author = {Ruoss, Anian and Balunovic, Mislav and Fischer, Marc and Vechev, Martin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {7584--7596},
 publisher = {Curran Associates, Inc.},
 title = {Learning Certified Individually Fair Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/55d491cf951b1b920900684d71419282-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{NEURIPS2021_07563a3f,
 author = {Roh, Yuji and Lee, Kangwook and Whang, Steven and Suh, Changho},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {815--827},
 publisher = {Curran Associates, Inc.},
 title = {Sample Selection for Fair and Robust Training},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{nangia-etal-2020-crows,
    title = "{C}row{S}-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
    author = "Nangia, Nikita  and
      Vania, Clara  and
      Bhalerao, Rasika  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.154",
    doi = "10.18653/v1/2020.emnlp-main.154",
    pages = "1953--1967",
    abstract = "Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.",
}


@inproceedings{nadeem-etal-2021-stereoset,
    title = "{S}tereo{S}et: Measuring stereotypical bias in pretrained language models",
    author = "Nadeem, Moin  and
      Bethke, Anna  and
      Reddy, Siva",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.416",
    doi = "10.18653/v1/2021.acl-long.416",
    pages = "5356--5371",
    abstract = "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu.",
}

@inproceedings{li-etal-2020-unqovering,
    title = "{UNQOVER}ing Stereotyping Biases via Underspecified Questions",
    author = "Li, Tao  and
      Khashabi, Daniel  and
      Khot, Tushar  and
      Sabharwal, Ashish  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.311",
    doi = "10.18653/v1/2020.findings-emnlp.311",
    pages = "3475--3489",
    abstract = "While language embeddings have been shown to have stereotyping biases, how these biases affect downstream question answering (QA) models remains unexplored. We present UNQOVER, a general framework to probe and quantify biases through underspecified questions. We show that a naive use of model scores can lead to incorrect bias estimates due to two forms of reasoning errors: positional dependence and question independence. We design a formalism that isolates the aforementioned errors. As case studies, we use this metric to analyze four important classes of stereotypes: gender, nationality, ethnicity, and religion. We probe five transformer-based QA models trained on two QA datasets, along with their underlying language models. Our broad study reveals that (1) all these models, with and without fine-tuning, have notable stereotyping biases in these classes; (2) larger models often have higher bias; and (3) the effect of fine-tuning on bias varies strongly with the dataset and the model size.",
}

@misc{abid2021persistent,
      title={Persistent Anti-Muslim Bias in Large Language Models}, 
      author={Abubakar Abid and Maheen Farooqi and James Zou},
      year={2021},
      eprint={2101.05783},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{parrish2022bbq,
      title={BBQ: A Hand-Built Bias Benchmark for Question Answering}, 
      author={Alicia Parrish and Angelica Chen and Nikita Nangia and Vishakh Padmakumar and Jason Phang and Jana Thompson and Phu Mon Htut and Samuel R. Bowman},
      year={2022},
      eprint={2110.08193},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gptdocumentation,
  title={{GPT} Documentation},
  author={OpenAI},
  year={2022},
  howpublished={\url{https://platform.openai.com/docs/guides/chat/introduction}},
}

@misc{parentingmyths,
  title={Five stereotypes about poor families and education},
  author={{Washington Post}},
  year={2013},
  howpublished={\url{https://www.washingtonpost.com/news/answer-sheet/wp/2013/10/28/five-stereotypes-about-poor-families-and-education/}},
}

@misc{slanteyestereotype,
title = {The Fox–Eye Trend Isn’t Cute—It’s Racist},
author = {{Teen Vogue}},
year = {2020},
howpublished = {\url{https://www.teenvogue.com/story/fox-eye-trend-cultural-appropriation-asian-features}}
}

@misc{greedmyths,
  title={Myth: Jews Are Greedy},
  author={{Anti-Defamation League}},
  year={},
  howpublished={\url{https://antisemitism.adl.org/greed/}},
}

@misc{hivmyths,
  title={Myths about HIV},
  author={{The Human Rights Campaign}},
  year={2023},
  howpublished={\url{https://www.hrc.org/resources/debunking-common-myths-about-hiv}},
}

@misc{terrormyths,
  title={Myths and Facts about Muslim People and Islam},
  author={{Anti-Defamation League}},
  year={2022},
  howpublished={\url{https://www.adl.org/resources/tools-and-strategies/myths-and-facts-about-muslim-people-and-islam}},
}

@misc{drugaddictmyths,
  title={A racist stereotype is shattered: Study finds white youth are more likely to abuse hard drugs than black youth},
  author={Salon},
  year={2016},
  howpublished={\url{https://www.salon.com/2016/04/06/this_racist_stereotype_is_shattered_study_finds_white_youth_are_more_likely_to_abuse_hard_drugs_than_black_youth_partner/}},
}

@misc{drivingmyths,
  title={Bad Drivers? No, Just Bad Stereotypes},
  author={{Association for Psychological Science}},
  year={2014},
  howpublished={\url{https://www.psychologicalscience.org/news/motr/bad-drivers-no-just-bad-stereotypes.html}},
}

@misc{jobsmyths,
  title={Do immigrants “steal” jobs from American workers?},
  author={{Brookings Institution}},
  year={2017},
  howpublished={\url{https://www.brookings.edu/blog/brookings-now/2017/08/24/do-immigrants-steal-jobs-from-american-workers/}},
}

@misc{leadershipmyths,
  title={Barriers \& Bias: The Status of Women in Leadership},
  author={{American Association of University Women}},
  year={},
  howpublished={\url{https://www.aauw.org/resources/research/barrier-bias/}},
}

@incollection{10.1093/oso/9780190465285.003.0011,
    author = {Keevak, Michael},
    isbn = {9780190465285},
    title = "{204How Did East Asians Become Yellow?}",
    booktitle = "{Reconsidering Race: Social Science Perspectives on Racial Categories in the Age of Genomics}",
    publisher = {Oxford University Press},
    year = {2018},
    month = {06},
    abstract = "{This chapter offers a brief historical intervention explaining the rise of the term yellow for racial thinking about Asians. Using his binomial nomenclature species-naming system, the Swedish taxonomist Carolus Linnaeus separated Homo sapiens into four continental types, with distinct colors assigned to each. Over two decades later the German anatomist Johann Friedrich Blumenbach also classified Asians as yellow in his five-race scheme. Although some early twentieth-century anthropologists claimed to have proven that Mongolians (Asians) were physically yellow in an attempt to place Asians lower than Europeans, the initial categorization of yellow had no visual or biological basis. As Asians continued to refuse to take part in Western systems (Christianity, international trade), Europeans' perceptions of Asians' skin color darkened. Moreover in the late eighteenth and early nineteenth century, the yellow idea began to spread to East Asian cultures themselves.}",
    doi = {10.1093/oso/9780190465285.003.0011},
    url = {https://doi.org/10.1093/oso/9780190465285.003.0011},
    eprint = {https://academic.oup.com/book/0/chapter/157327331/chapter-ag-pdf/44943658/book\_9965\_section\_157327331.ag.pdf},
}



@article{doi:10.1177/1043986207306870,
author = {Kelly Welch},
title ={Black Criminal Stereotypes and Racial Profiling},
journal = {Journal of Contemporary Criminal Justice},
volume = {23},
number = {3},
pages = {276-288},
year = {2007},
doi = {10.1177/1043986207306870},

URL = { 
        https://doi.org/10.1177/1043986207306870
    
},
eprint = { 
        https://doi.org/10.1177/1043986207306870
    
}
,
    abstract = { The racial stereotyping of criminals has been an enduring and unfortunate feature of American culture. However, following the civil rights movement, the linkage between Blacks and crime was galvanized. The stereotyping of Blacks as criminals is so pervasive throughout society that “criminal predator” is used as a euphemism for “young Black male.” This common stereotype has erroneously served as a subtle rationale for the unofficial policy and practice of racial profiling by criminal justice practitioners. This article details the theoretical elements contributing to the development of Black criminal typification to understand how this has been used to justify racial profiling. }
}

@article{drugdealingmyths,
author = {Steven W. Bender},
title = {Sight, Sound, and Stereotype: The War on Terrorism and Its Consequences for Latinas/os},
volume = {81},
journal = {Oregon Law Review},
year = {2002},
url = {https://digitalcommons.law.seattleu.edu/faculty/296}
}

@article{doi:10.1177/0361684317711412,
author = {Bettina J. Casad and Patricia Hale and Faye L. Wachs},
title ={Stereotype Threat Among Girls: Differences by Gender Identity and Math Education Context},
journal = {Psychology of Women Quarterly},
volume = {41},
number = {4},
pages = {513-529},
year = {2017},
doi = {10.1177/0361684317711412},

URL = { 
        https://doi.org/10.1177/0361684317711412
    
},
eprint = { 
        https://doi.org/10.1177/0361684317711412
    
}
,
    abstract = { Effects of stereotype threat on math performance have been well-documented among college women; however, the prevalence among adolescent girls is less well-known. Further, the moderating role of gender identity and effects of stereotype threat on high achieving girls in math is unknown. This study tested the effects of a stereotype threat condition (vs. control group) among middle school girls in standard and honors math classes and examined gender identity as a moderator. Students (N = 498) completed pre- and post-questionnaires and a math test as part of a stereotype threat experiment. Gender identity moderated effects of stereotype threat on math discounting, disengagement, attitudes, and performance, but whether gender identity was a protective or risk factor differed by math education context (honors math and standard math classes). Gender identity was protective for girls in honors math for attitudes, discounting, and disengagement but was a risk factor for math performance. Gender identity was a risk factor for disengagement and math attitudes among girls in standard math classes, but was a buffer for math performance. Results suggest the need to examine protective and risk properties of gender identity importance for adolescent girls and the need to examine stereotype threat within educational contexts. Stereotype threat can be reduced through interventions; thus, educators and practitioners can collaborate with social scientists to implement widespread interventions in K–12 schools. Additional online materials for this article are availableon PWQ’s website at http://journals.sagepub.com/doi/suppl/10.1177/0361684317711412.Online slides for instructors who want to use this article for teaching are available on PWQ's website at http://journals.sagepub.com/page/pwq/suppl/index }
}

@inproceedings{blodgett-etal-2021-stereotyping,
    title = "Stereotyping {N}orwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets",
    author = "Blodgett, Su Lin  and
      Lopez, Gilsinia  and
      Olteanu, Alexandra  and
      Sim, Robert  and
      Wallach, Hanna",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.81",
    doi = "10.18653/v1/2021.acl-long.81",
    pages = "1004--1015",
    abstract = "Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system{'}s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens{---}originating from the social sciences{---}to inventory a range of pitfalls that threaten these benchmarks{'} validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.",
}

@article{doi:10.1080/01419870.2017.1409900,
author = {Stephen Del Visco},
title = {Yellow peril, red scare: race and communism in National Review},
journal = {Ethnic and Racial Studies},
volume = {42},
number = {4},
pages = {626-644},
year  = {2019},
publisher = {Routledge},
doi = {10.1080/01419870.2017.1409900},

URL = { 
    
        https://doi.org/10.1080/01419870.2017.1409900
    
    

},
eprint = { 
    
        https://doi.org/10.1080/01419870.2017.1409900
    
    

}

}


@online{samsungprivacy2023,
  author = {Cybernews},
  title = {Lessons learned from ChatGPT’s Samsung leak},
  year = 2023,
  url = {https://cybernews.com/security/chatgpt-samsung-leak-explained-lessons/},
  urldate = {2023-05-28}
}


@online{microsoftprivacy2023,
  author = {CNN},
  title = {Microsoft is bringing ChatGPT technology to Word, Excel and Outlook},
  year = 2023,
  url = {https://www.cnn.com/2023/03/16/tech/openai-gpt-microsoft-365/index.html},
  urldate = {2023-06-04}
}




@article{xenophobiamyths,
author = {{Pew Research Center}},
title = {Majority of Latinos Say Skin Color Impacts Opportunity in America and Shapes Daily Life},
year = {2021},
url = {https://www.pewresearch.org/hispanic/2021/11/04/majority-of-latinos-say-skin-color-impacts-opportunity-in-america-and-shapes-daily-life/}
}


@article{https://doi.org/10.1111/j.1475-682x.2012.00437.x,
author = {Berg, Justin Allen},
title = {Opposition to Pro-Immigrant Public Policy: Symbolic Racism and Group Threat},
journal = {Sociological Inquiry},
volume = {83},
number = {1},
pages = {1-31},
doi = {https://doi.org/10.1111/j.1475-682x.2012.00437.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1475-682x.2012.00437.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1475-682x.2012.00437.x},
abstract = {This study examines the relationship between symbolic racism and native-born citizens’ policy opinions toward legal and undocumented immigration. With data from the 1994 General Social Survey and the NPR/Kaiser Foundation/Kennedy School of Government 2004 Immigration Survey, the results from logit regression models indicate that symbolic racism significantly predicts opposition to legal immigration, immigrant access to federal aid, and standard costs for college, citizenship for U.S.-born children, and work permits for undocumented immigrants. The effects are independent of group threat and other factors. Symbolic racism explained more variation in policy opinions toward government assistance, while group threat explained more variation toward immigration levels and citizenship status. Depending on the issue, native-born citizens likely derive their immigration policy opinions from moral ideologies in addition to intergroup competition.},
year = {2013}
}

@article{doi:10.1080/03601270903323976,
author = { Sean   Horton  and  Joseph   Baker  and  William   Pearce  and  Janice M.   Deakin },
title = {Immunity to Popular Stereotypes of Aging? Seniors and Stereotype Threat},
journal = {Educational Gerontology},
volume = {36},
number = {5},
pages = {353-371},
year  = {2010},
publisher = {Routledge},
doi = {10.1080/03601270903323976},
URL = {
        https://doi.org/10.1080/03601270903323976
},
eprint = {
        https://doi.org/10.1080/03601270903323976
},
}

@article{GENTILE201895,
title = {‘You play like a Woman!’ Effects of gender stereotype threat on Women's performance in physical and sport activities: A meta-analysis},
journal = {Psychology of Sport and Exercise},
volume = {39},
pages = {95-103},
year = {2018},
issn = {1469-0292},
doi = {https://doi.org/10.1016/j.psychsport.2018.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S1469029217305083},
author = {Ambra Gentile and Stefano Boca and Isabella Giammusso},
abstract = {Objectives
The purpose of this quantitative review was to provide an estimation of the effect of stereotype threat on women's performance in sport.
Design
This review employed a meta-analytic technique.
Method
a meta-analysis with random effects model was performed on 24 effects. Publication bias was tested through funnel plots and Egger's regression test.
Results
Findings show a symmetric distribution of effects, making it possible to conclude that no file-drawer problem affected the collected sample of effects. Aggregating the results of the reviewed studies, a medium effect of stereotype threat manipulation on women's sport performances emerged (d = 0.33). Collected studies were coded for stereotypicality of threatened exercise. The effect of stereotype threat was significantly higher for sports activities perceived as masculine.
Conclusions
This meta-analysis reveals that gender stereotype affects the sport activities of women and that this is particularly true for sports typically considered suited to males.}
}

@inproceedings{fan-etal-2018-hierarchical,
    title = "Hierarchical Neural Story Generation",
    author = "Fan, Angela  and
      Lewis, Mike  and
      Dauphin, Yann",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1082",
    doi = "10.18653/v1/P18-1082",
    pages = "889--898",
    abstract = "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
}

@inproceedings{welleckneural,
  title={Neural Text Generation With Unlikelihood Training},
  author={Welleck, Sean and Kulikov, Ilia and Roller, Stephen and Dinan, Emily and Cho, Kyunghyun and Weston, Jason},
  booktitle={International Conference on Learning Representations},
  year = "2020",
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@misc{StableVicuna2023,
  title = {{StableVicuna: An RLHF Fine-Tune of Vicuna-13B v0}},
  author = {StabilityAI},
  year = {2023},
  month = {4},
  url = {https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot},
  note = {DOI:10.57967/hf/0588},
  howpublished = {Available at \url{https://github.com/StabilityAI/StableVicuna}},
}


@article{instructgpt,
  title   = {Training language models to follow instructions with human feedback},
  author  = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {35},
  pages   = {27730-27744},
  year    = {2022}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title   = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author  = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2307.09288}
}

@article{wang2022self,
  title={Self-Instruct: Aligning Language Model with Self Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@inproceedings{DBLP:conf/ndss/LiJDLW19,
  author       = {Jinfeng Li and
                  Shouling Ji and
                  Tianyu Du and
                  Bo Li and
                  Ting Wang},
  title        = {TextBugger: Generating Adversarial Text Against Real-world Applications},
  booktitle    = {26th Annual Network and Distributed System Security Symposium, {NDSS}
                  2019, San Diego, California, USA, February 24-27, 2019},
  publisher    = {The Internet Society},
  year         = {2019},
  url          = {https://www.ndss-symposium.org/ndss-paper/textbugger-generating-adversarial-text-against-real-world-applications/},
  timestamp    = {Mon, 01 Feb 2021 08:42:25 +0100},
  biburl       = {https://dblp.org/rec/conf/ndss/LiJDLW19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/access/Kwon23,
  author       = {Hyun Kwon},
  title        = {Dual-Targeted Textfooler Attack on Text Classification Systems},
  journal      = {{IEEE} Access},
  volume       = {11},
  pages        = {15164--15173},
  year         = {2023},
  url          = {https://doi.org/10.1109/ACCESS.2021.3121366},
  doi          = {10.1109/ACCESS.2021.3121366},
  timestamp    = {Sat, 11 Mar 2023 00:12:15 +0100},
  biburl       = {https://dblp.org/rec/journals/access/Kwon23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/emnlp/LiMGXQ20,
  author       = {Linyang Li and
                  Ruotian Ma and
                  Qipeng Guo and
                  Xiangyang Xue and
                  Xipeng Qiu},
  editor       = {Bonnie Webber and
                  Trevor Cohn and
                  Yulan He and
                  Yang Liu},
  title        = {{BERT-ATTACK:} Adversarial Attack Against {BERT} Using {BERT}},
  booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages        = {6193--6202},
  publisher    = {Association for Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.emnlp-main.500},
  doi          = {10.18653/v1/2020.emnlp-main.500},
  timestamp    = {Wed, 23 Mar 2022 10:11:55 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/LiMGXQ20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/acl/ZangQYLZLS20,
  author       = {Yuan Zang and
                  Fanchao Qi and
                  Chenghao Yang and
                  Zhiyuan Liu and
                  Meng Zhang and
                  Qun Liu and
                  Maosong Sun},
  editor       = {Dan Jurafsky and
                  Joyce Chai and
                  Natalie Schluter and
                  Joel R. Tetreault},
  title        = {Word-level Textual Adversarial Attacking as Combinatorial Optimization},
  booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational
                  Linguistics, {ACL} 2020, Online, July 5-10, 2020},
  pages        = {6066--6080},
  publisher    = {Association for Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.acl-main.540},
  doi          = {10.18653/v1/2020.acl-main.540},
  timestamp    = {Wed, 01 Sep 2021 15:29:09 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/ZangQYLZLS20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/emnlp/WangPPCWL20,
  author       = {Boxin Wang and
                  Hengzhi Pei and
                  Boyuan Pan and
                  Qian Chen and
                  Shuohang Wang and
                  Bo Li},
  editor       = {Bonnie Webber and
                  Trevor Cohn and
                  Yulan He and
                  Yang Liu},
  title        = {{T3:} Tree-Autoencoder Constrained Adversarial Text Generation for
                  Targeted Attack},
  booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages        = {6134--6150},
  publisher    = {Association for Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.emnlp-main.495},
  doi          = {10.18653/v1/2020.emnlp-main.495},
  timestamp    = {Wed, 23 Mar 2022 10:11:55 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/WangPPCWL20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{wang2023instructretro,
    title   = {InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining},
    author  = {Boxin Wang and Wei Ping and Lawrence McAfee and Peng Xu and Bo Li and Mohammad Shoeybi and Bryan Catanzaro},
    year    = {2023},
    journal = {arXiv preprint arXiv: 2310.07713}
}

@inproceedings{
wang2023shall,
title={Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study},
author  = {Boxin Wang and Wei Ping and Peng Xu and Lawrence McAfee and Zihan Liu and Mohammad Shoeybi and Yi Dong and Oleksii Kuchaiev and Bo Li and Chaowei Xiao and Anima Anandkumar and Bryan Catanzaro},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
}

@article{DBLP:journals/corr/abs-1903-05543,
  author       = {James Thorne and
                  Andreas Vlachos},
  title        = {Adversarial attacks against Fact Extraction and VERification},
  journal      = {CoRR},
  volume       = {abs/1903.05543},
  year         = {2019},
  url          = {http://arxiv.org/abs/1903.05543},
  eprinttype    = {arXiv},
  eprint       = {1903.05543},
  timestamp    = {Tue, 03 Nov 2020 12:45:06 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1903-05543.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/naacl/IyyerWGZ18,
  author       = {Mohit Iyyer and
                  John Wieting and
                  Kevin Gimpel and
                  Luke Zettlemoyer},
  editor       = {Marilyn A. Walker and
                  Heng Ji and
                  Amanda Stent},
  title        = {Adversarial Example Generation with Syntactically Controlled Paraphrase
                  Networks},
  booktitle    = {Proceedings of the 2018 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  {NAACL-HLT} 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume
                  1 (Long Papers)},
  pages        = {1875--1885},
  publisher    = {Association for Computational Linguistics},
  year         = {2018},
  url          = {https://doi.org/10.18653/v1/n18-1170},
  doi          = {10.18653/v1/n18-1170},
  timestamp    = {Fri, 06 Aug 2021 00:41:31 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/IyyerWGZ18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/ijcai/RibeiroWG021,
  author       = {Marco T{\'{u}}lio Ribeiro and
                  Tongshuang Wu and
                  Carlos Guestrin and
                  Sameer Singh},
  editor       = {Zhi{-}Hua Zhou},
  title        = {Beyond Accuracy: Behavioral Testing of {NLP} Models with Checklist
                  (Extended Abstract)},
  booktitle    = {Proceedings of the Thirtieth International Joint Conference on Artificial
                  Intelligence, {IJCAI} 2021, Virtual Event / Montreal, Canada, 19-27
                  August 2021},
  pages        = {4824--4828},
  publisher    = {ijcai.org},
  year         = {2021},
  url          = {https://doi.org/10.24963/ijcai.2021/659},
  doi          = {10.24963/ijcai.2021/659},
  timestamp    = {Wed, 25 Aug 2021 17:11:16 +0200},
  biburl       = {https://dblp.org/rec/conf/ijcai/RibeiroWG021.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/coling/NaikRSRN18,
  author       = {Aakanksha Naik and
                  Abhilasha Ravichander and
                  Norman M. Sadeh and
                  Carolyn P. Ros{\'{e}} and
                  Graham Neubig},
  editor       = {Emily M. Bender and
                  Leon Derczynski and
                  Pierre Isabelle},
  title        = {Stress Test Evaluation for Natural Language Inference},
  booktitle    = {Proceedings of the 27th International Conference on Computational
                  Linguistics, {COLING} 2018, Santa Fe, New Mexico, USA, August 20-26,
                  2018},
  pages        = {2340--2353},
  publisher    = {Association for Computational Linguistics},
  year         = {2018},
  url          = {https://aclanthology.org/C18-1198/},
  timestamp    = {Wed, 21 Sep 2022 12:40:04 +0200},
  biburl       = {https://dblp.org/rec/conf/coling/NaikRSRN18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/acl/NieWDBWK20,
  author       = {Yixin Nie and
                  Adina Williams and
                  Emily Dinan and
                  Mohit Bansal and
                  Jason Weston and
                  Douwe Kiela},
  editor       = {Dan Jurafsky and
                  Joyce Chai and
                  Natalie Schluter and
                  Joel R. Tetreault},
  title        = {Adversarial {NLI:} {A} New Benchmark for Natural Language Understanding},
  booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational
                  Linguistics, {ACL} 2020, Online, July 5-10, 2020},
  pages        = {4885--4901},
  publisher    = {Association for Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.acl-main.441},
  doi          = {10.18653/v1/2020.acl-main.441},
  timestamp    = {Thu, 14 Oct 2021 09:46:09 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/NieWDBWK20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/emnlp/JiaL17,
  author       = {Robin Jia and
                  Percy Liang},
  editor       = {Martha Palmer and
                  Rebecca Hwa and
                  Sebastian Riedel},
  title        = {Adversarial Examples for Evaluating Reading Comprehension Systems},
  booktitle    = {Proceedings of the 2017 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2017, Copenhagen, Denmark, September
                  9-11, 2017},
  pages        = {2021--2031},
  publisher    = {Association for Computational Linguistics},
  year         = {2017},
  url          = {https://doi.org/10.18653/v1/d17-1215},
  doi          = {10.18653/v1/d17-1215},
  timestamp    = {Fri, 06 Aug 2021 00:40:40 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/JiaL17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2203-05314,
  author       = {Junjie Shen and
                  Ningfei Wang and
                  Ziwen Wan and
                  Yunpeng Luo and
                  Takami Sato and
                  Zhisheng Hu and
                  Xinyang Zhang and
                  Shengjian Guo and
                  Zhenyu Zhong and
                  Kang Li and
                  Ziming Zhao and
                  Chunming Qiao and
                  Qi Alfred Chen},
  title        = {SoK: On the Semantic {AI} Security in Autonomous Driving},
  journal      = {CoRR},
  volume       = {abs/2203.05314},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2203.05314},
  doi          = {10.48550/arXiv.2203.05314},
  eprinttype    = {arXiv},
  eprint       = {2203.05314},
  timestamp    = {Wed, 04 Jan 2023 08:38:50 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2203-05314.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{yoo-etal-2022-ground,
    title = "Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations",
    author = "Yoo, Kang Min  and
      Kim, Junyeob  and
      Kim, Hyuhng Joon  and
      Cho, Hyunsoo  and
      Jo, Hwiyeol  and
      Lee, Sang-Woo  and
      Lee, Sang-goo  and
      Kim, Taeuk",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.155",
    pages = "2422--2437",
}

@article{wei2023larger,
  title={Larger language models do in-context learning differently},
  author={Wei, Jerry and Wei, Jason and Tay, Yi and Tran, Dustin and Webson, Albert and Lu, Yifeng and Chen, Xinyun and Liu, Hanxiao and Huang, Da and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2303.03846},
  year={2023}
}

@inproceedings{lu-etal-2022-fantastically,
    title = "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
    author = "Lu, Yao  and
      Bartolo, Max  and
      Moore, Alastair  and
      Riedel, Sebastian  and
      Stenetorp, Pontus",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.556",
    doi = "10.18653/v1/2022.acl-long.556",
    pages = "8086--8098",
}

@inproceedings{mishra-etal-2022-cross,
    title = "Cross-Task Generalization via Natural Language Crowdsourcing Instructions",
    author = "Mishra, Swaroop  and
      Khashabi, Daniel  and
      Baral, Chitta  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.244",
    doi = "10.18653/v1/2022.acl-long.244",
    pages = "3470--3487",
    abstract = "Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19{\%} better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction.",
}

@inproceedings{lu1codexglue,
  title={CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation},
  author={Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin and Drain, Dawn and Jiang, Daxin and Tang, Duyu and others},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)}
}

@book{ross2002right,
  title={The right and the good},
  author={Ross, William David},
  year={2002},
  publisher={Oxford University Press}
}

@inproceedings{wang-etal-2022-super,
    title = "Super-{N}atural{I}nstructions: Generalization via Declarative Instructions on 1600+ {NLP} Tasks",
    author = "Wang, Yizhong  and
      Mishra, Swaroop  and
      Alipoormolabashi, Pegah  and
      Kordi, Yeganeh  and
      Mirzaei, Amirreza  and
      Naik, Atharva  and
      Ashok, Arjun  and
      Dhanasekaran, Arut Selvan  and
      Arunkumar, Anjana  and
      Stap, David  and
      Pathak, Eshaan  and
      Karamanolakis, Giannis  and
      Lai, Haizhi  and
      Purohit, Ishan  and
      Mondal, Ishani  and
      Anderson, Jacob  and
      Kuznia, Kirby  and
      Doshi, Krima  and
      Pal, Kuntal Kumar  and
      Patel, Maitreya  and
      Moradshahi, Mehrad  and
      Parmar, Mihir  and
      Purohit, Mirali  and
      Varshney, Neeraj  and
      Kaza, Phani Rohitha  and
      Verma, Pulkit  and
      Puri, Ravsehaj Singh  and
      Karia, Rushang  and
      Doshi, Savan  and
      Sampat, Shailaja Keyur  and
      Mishra, Siddhartha  and
      Reddy A, Sujan  and
      Patro, Sumanta  and
      Dixit, Tanay  and
      Shen, Xudong",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.340",
    pages = "5085--5109",
    abstract = "How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions{---}training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9{\%} on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.",
}

@inproceedings{kandpal2022deduplicating,
  title={Deduplicating training data mitigates privacy risks in language models},
  author={Kandpal, Nikhil and Wallace, Eric and Raffel, Colin},
  booktitle={International Conference on Machine Learning},
  pages={10697--10707},
  year={2022},
  organization={PMLR}
}

@article{wang2023chatcad,
  title={Chatcad: Interactive computer-aided diagnosis on medical image using large language models},
  author={Wang, Sheng and Zhao, Zihao and Ouyang, Xi and Wang, Qian and Shen, Dinggang},
  journal={arXiv preprint arXiv:2302.07257},
  year={2023}
}

@article{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@article{zhou2023navigating,
  title={Navigating the Grey Area: Expressions of Overconfidence and Uncertainty in Language Models},
  author={Zhou, Kaitlyn and Jurafsky, Dan and  Hashimoto, Tatsunori},
  journal={arXiv:2302.13439v1},
  year={2023}
}

@article{morris2022unsupervised,
  title={Unsupervised Text Deidentification},
  author={Morris, John X. and Chiu, Justin T. and Zabih, Ramin and Rush, Alexander M.},
  journal={arXiv:2210.11528v1},
  year={2022}
}

@article{tramer2022considerations,
  title={Considerations for Differentially Private Learning
with Large-Scale Public Pretraining},
  author={Tram`er, Florian and Gautam, Kamath and Carlini, Nicholas Carlini},
  journal={arXiv:2212.06470 },
  year={2022}
}


@inproceedings{yudifferentially,
  title={Differentially Private Fine-tuning of Language Models},
  author={Yu, Da and Naik, Saurabh and Backurs, Arturs and Gopi, Sivakanth and Inan, Huseyin A and Kamath, Gautam and Kulkarni, Janardhan and Lee, Yin Tat and Manoel, Andre and Wutschitz, Lukas and others},
  booktitle={International Conference on Learning Representations},
  year={2022}
}


@article{zhu2023promptbench,
  title={PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts},
  author={Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Gong, Neil Zhenqiang and Zhang, Yue and others},
  journal={arXiv preprint arXiv:2306.04528},
  year={2023}
}

@article{suzgun2022challenging,
  title={Challenging BIG-Bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}

@article{shi2022language,
  title={Language models are multilingual chain-of-thought reasoners},
  author={Shi, Freda and Suzgun, Mirac and Freitag, Markus and Wang, Xuezhi and Srivats, Suraj and Vosoughi, Soroush and Chung, Hyung Won and Tay, Yi and Ruder, Sebastian and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.03057},
  year={2022}
}

@article{kim2022ground,
  title={Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations},
  author={Kim, Junyeob and Kim, Hyuhng Joon and Cho, Hyunsoo and Jo, Hwiyeol and Lee, Sang-Woo and Lee, Sang-goo and Yoo, Kang Min and Kim, Taeuk},
  journal={arXiv preprint arXiv:2205.12685},
  year={2022}
}

@article{zhong2023can,
  title={Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert},
  author={Zhong, Qihuang and Ding, Liang and Liu, Juhua and Du, Bo and Tao, Dacheng},
  journal={arXiv preprint arXiv:2302.10198},
  year={2023}
}

@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-$3 $?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021}
}

@article{wang2023adversarial,
  title={Adversarial Demonstration Attacks on Large Language Models},
  author={Wang, Jiongxiao and Liu, Zichen and Park, Keun Hee and Chen, Muhao and Xiao, Chaowei},
  journal={arXiv preprint arXiv:2305.14950},
  year={2023}
}

@inproceedings{text_agent1,
  author       = {Marc{-}Alexandre C{\^{o}}t{\'{e}} and
                  {\'{A}}kos K{\'{a}}d{\'{a}}r and
                  Xingdi Yuan and
                  Ben Kybartas and
                  Tavian Barnes and
                  Emery Fine and
                  James Moore and
                  Matthew J. Hausknecht and
                  Layla El Asri and
                  Mahmoud Adada and
                  Wendy Tay and
                  Adam Trischler},
  title        = {TextWorld: {A} Learning Environment for Text-Based Games},
  booktitle    = {Computer Games - 7th Workshop, {CGW}, Held in Conjunction with
                  the 27th International Conference on Artificial Intelligence, {IJCAI}},
  series       = {Communications in Computer and Information Science},
  volume       = {1017},
  pages        = {41--75},
  publisher    = {Springer},
  year         = {2018}
}

@inproceedings{text_agent2,
  author       = {Mohit Shridhar and
                  Xingdi Yuan and
                  Marc{-}Alexandre C{\^{o}}t{\'{e}} and
                  Yonatan Bisk and
                  Adam Trischler and
                  Matthew J. Hausknecht},
  title        = {ALFWorld: Aligning Text and Embodied Environments for Interactive
                  Learning},
  booktitle    = {9th International Conference on Learning Representations, {ICLR}},
  year         = {2021}
}

@inproceedings{text_agent3,
  author       = {Matthew J. Hausknecht and
                  Prithviraj Ammanabrolu and
                  Marc{-}Alexandre C{\^{o}}t{\'{e}} and
                  Xingdi Yuan},
  title        = {Interactive Fiction Games: {A} Colossal Adventure},
  booktitle    = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}},
  pages        = {7903--7910},
  publisher    = {{AAAI} Press},
  year         = {2020}
}

@inproceedings{moral_story,
  author       = {Denis Emelin and
                  Ronan Le Bras and
                  Jena D. Hwang and
                  Maxwell Forbes and
                  Yejin Choi},
  title        = {Moral Stories: Situated Reasoning about Norms, Intents, Actions, and
                  their Consequences},
  booktitle    = {Proceedings of the 2021 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP}},
  pages        = {698--718},
  publisher    = {Association for Computational Linguistics},
  year         = {2021}
}

@inproceedings{moral_exception_qa,
  author       = {Zhijing Jin and
                  Sydney Levine and
                  Fernando Gonzalez Adauto and
                  Ojasv Kamal and
                  Maarten Sap and
                  Mrinmaya Sachan and
                  Rada Mihalcea and
                  Josh Tenenbaum and
                  Bernhard Sch{\"{o}}lkopf},
  title        = {When to Make Exceptions: Exploring Language Models as Accounts of
                  Human Moral Judgment},
  booktitle    = {NeurIPS},
  year         = {2022}
}

@article{ritual,
  author       = {Anurag Acharya and
                  Kartik Talamadupula and
                  Mark A. Finlayson},
  title        = {An Atlas of Cultural Commonsense for Machine Reasoning},
  journal      = {CoRR},
  volume       = {abs/2009.05664},
  year         = {2020}
}

@inproceedings{social_chemistry,
  author       = {Maxwell Forbes and
                  Jena D. Hwang and
                  Vered Shwartz and
                  Maarten Sap and
                  Yejin Choi},
  title        = {Social Chemistry 101: Learning to Reason about Social and Moral Norms},
  booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP}},
  pages        = {653--670},
  publisher    = {Association for Computational Linguistics},
  year         = {2020}
}

@article{MACHIAVELLI,
  author       = {Alexander Pan and
                  Jun Shern Chan and
                  Andy Zou and
                  Nathaniel Li and
                  Steven Basart and
                  Thomas Woodside and
                  Jonathan Ng and
                  Hanlin Zhang and
                  Scott Emmons and
                  Dan Hendrycks},
  title        = {Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards
                  and Ethical Behavior in the {MACHIAVELLI} Benchmark},
  journal      = {CoRR},
  volume       = {abs/2304.03279},
  year         = {2023}
}

@article{caton2020fairness,
  title={Fairness in machine learning: A survey},
  author={Caton, Simon and Haas, Christian},
  journal={arXiv preprint arXiv:2010.04053},
  year={2020}
}

@inproceedings{dwork2012fairness,
  title={Fairness through awareness},
  author={Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  booktitle={Proceedings of the 3rd innovations in theoretical computer science conference},
  pages={214--226},
  year={2012}
}

@article{mehrabi2021survey,
  title={A survey on bias and fairness in machine learning},
  author={Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={6},
  pages={1--35},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{zhou2023ethical,
  title={Ethical ChatGPT: Concerns, Challenges, and Commandments},
  author={Zhou, Jianlong and M{\"u}ller, Heimo and Holzinger, Andreas and Chen, Fang},
  journal={arXiv preprint arXiv:2305.10646},
  year={2023}
}

@article{zhuo2023exploring,
  title={Exploring ai ethics of chatgpt: A diagnostic analysis},
  author={Zhuo, Terry Yue and Huang, Yujin and Chen, Chunyang and Xing, Zhenchang},
  journal={arXiv preprint arXiv:2301.12867},
  year={2023}
}

@article{liu2023summary,
  title={Summary of chatgpt/gpt-4 research and perspective towards the future of large language models},
  author={Liu, Yiheng and Han, Tianle and Ma, Siyuan and Zhang, Jiayue and Yang, Yuanyuan and Tian, Jiaming and He, Hao and Li, Antong and He, Mengshen and Liu, Zhengliang and others},
  journal={arXiv preprint arXiv:2304.01852},
  year={2023}
}

@article{hariri2023unlocking,
  title={Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing},
  author={Hariri, Walid},
  journal={arXiv preprint arXiv:2304.02017},
  year={2023}
}

@article{nori2023capabilities,
  title={Capabilities of gpt-4 on medical challenge problems},
  author={Nori, Harsha and King, Nicholas and McKinney, Scott Mayer and Carignan, Dean and Horvitz, Eric},
  journal={arXiv preprint arXiv:2303.13375},
  year={2023}
}


@article{li2023fairness,
  title={Fairness of ChatGPT},
  author={Li, Yunqi and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2305.18569},
  year={2023}
}

@article{promptinject,
  author       = {F{\'{a}}bio Perez and
                  Ian Ribeiro},
  title        = {Ignore Previous Prompt: Attack Techniques For Language Models},
  journal      = {CoRR},
  volume       = {abs/2211.09527},
  year         = {2022}
}

@article{morethan,
  author       = {Kai Greshake and
                  Sahar Abdelnabi and
                  Shailesh Mishra and
                  Christoph Endres and
                  Thorsten Holz and
                  Mario Fritz},
  title        = {More than you've asked for: {A} Comprehensive Analysis of Novel Prompt
                  Injection Threats to Application-Integrated Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2302.12173},
  year         = {2023}
}

@misc{promptinject_app1,
   author = {Riley Goodside},
   title = {Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions},
   howpublished = {\url{https://web.archive.org/web/20220919192024/https://twitter.com/goodside/status/1569128808308957185}}
}

@misc{promptinject_app2,
   author = {Simon Willison},
   title = {Prompt injection attacks against GPT-3},
   howpublished = {\url{http://web.archive.org/web/20220928004736/https://simonwillison.net/2022/Sep/12/prompt-injection/}}
}

@misc{promptinject_app3,
   author = {Simon Willison},
   title = {I missed this one: Someone did get a prompt leak attack to work against the bot},
   howpublished = {\url{https://web.archive.org/web/20220924105826/https://twitter.com/simonw/status/1570933190289924096}}
}

@misc{dan,
   author = {Lavina Daryanani},
   title = {How to jailbreak chatgpt},
   howpublished = {\url{https://watcher.guru/news/how-to-jailbreak-chatgpt}}
}

@article{program_attack,
  author       = {Daniel Kang and
                  Xuechen Li and
                  Ion Stoica and
                  Carlos Guestrin and
                  Matei Zaharia and
                  Tatsunori Hashimoto},
  title        = {Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard
                  Security Attacks},
  journal      = {CoRR},
  volume       = {abs/2302.05733},
  year         = {2023}
}


@article{clever_han,
  author       = {Natalie Shapira and
                  Mosh Levy and
                  Seyed Hossein Alavi and
                  Xuhui Zhou and
                  Yejin Choi and
                  Yoav Goldberg and
                  Maarten Sap and
                  Vered Shwartz},
  title        = {Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning
                  in Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2305.14763},
  year         = {2023}
}

@misc{european2021aia,
  title={Laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain union legislative acts},
  author={European Commission},
  year={2021},
  publisher={Office for Official Publications of the European Communities Luxembourg},
  howpublished={\url{https://eur-lex.europa.eu/resource.html?uri=cellar:e0649735-a372-11eb-9585-01aa75ed71a1.0001.02/DOC_1\&format=PDF}}
}

@misc{ep2021aia,
  title={Amendments adopted by the European Parliament on 14 June 2023 on the proposal for a regulation of the European Parliament and of the Council on laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts },
  author={European Parliament},
  year={2023},
  howpublished={\url{https://www.europarl.europa.eu/doceo/document/TA-9-2023-0236_EN.pdf}}
}


@article{wh2022blueprint,
  title={Blueprint for an AI Bill of Rights},
  author={{White House Office of Science and Technology Policy}},
  year={2022}
}

@article{floridi2022capai,
  title={CapAI-A Procedure for Conducting Conformity Assessment of AI Systems in Line with the EU Artificial Intelligence Act},
  author={Floridi, Luciano and Holweg, Matthias and Taddeo, Mariarosaria and Amaya Silva, Javier and M{\"o}kander, Jakob and Wen, Yuni},
  journal={Available at SSRN 4064091},
  year={2022}
}

@misc{bommasani2023eu-ai-act, 
    author = {Rishi Bommasani and Kevin Klyman and Daniel Zhang and Percy Liang}, 
    title  = {Do Foundation Model Providers Comply with the EU AI Act?}, 
    url    = {https://crfm.stanford.edu/2023/06/15/eu-ai-act.html}, 
    year   = {2023}
}

@inproceedings{akyurek-etal-2022-measuring,
    title = "On Measuring Social Biases in Prompt-Based Multi-Task Learning",
    author = {Aky{\"u}rek, Afra Feyza  and
      Paik, Sejin  and
      Kocyigit, Muhammed  and
      Akbiyik, Seda  and
      Runyun, Serife Leman  and
      Wijaya, Derry},
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.42",
    doi = "10.18653/v1/2022.findings-naacl.42",
    pages = "551--564",
    abstract = "Large language models trained on a mixture of NLP tasks that are converted into a text-to-text format using prompts, can generalize into novel forms of language and handle novel tasks. A large body of work within prompt engineering attempts to understand the effects of input forms and prompts in achieving superior performance. We consider an alternative measure and inquire whether the way in which an input is encoded affects social biases promoted in outputs. In this paper, we study T0, a large-scale multi-task text-to-text language model trained using prompt-based learning. We consider two different forms of semantically equivalent inputs: question-answer format and premise-hypothesis format. We use an existing bias benchmark for the former BBQ and create the first bias benchmark in natural language inference BBNLI with hand-written hypotheses while also converting each benchmark into the other form. The results on two benchmarks suggest that given two different formulations of essentially the same input, T0 conspicuously acts more biased in question answering form, which is seen during training, compared to premise-hypothesis form which is unlike its training examples. Code and data are released under https://github.com/feyzaakyurek/bbnli.",
}

@inproceedings{bowman-etal-2015-large,
    title = "A large annotated corpus for learning natural language inference",
    author = "Bowman, Samuel R.  and
      Angeli, Gabor  and
      Potts, Christopher  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1075",
    doi = "10.18653/v1/D15-1075",
    pages = "632--642",
}

@article{DBLP:journals/corr/abs-2307-09288,
  author       = {Hugo Touvron and
                  Louis Martin and
                  Kevin Stone and
                  Peter Albert and
                  Amjad Almahairi and
                  Yasmine Babaei and
                  Nikolay Bashlykov and
                  Soumya Batra and
                  Prajjwal Bhargava and
                  Shruti Bhosale and
                  Dan Bikel and
                  Lukas Blecher and
                  Cristian Canton{-}Ferrer and
                  Moya Chen and
                  Guillem Cucurull and
                  David Esiobu and
                  Jude Fernandes and
                  Jeremy Fu and
                  Wenyin Fu and
                  Brian Fuller and
                  Cynthia Gao and
                  Vedanuj Goswami and
                  Naman Goyal and
                  Anthony Hartshorn and
                  Saghar Hosseini and
                  Rui Hou and
                  Hakan Inan and
                  Marcin Kardas and
                  Viktor Kerkez and
                  Madian Khabsa and
                  Isabel Kloumann and
                  Artem Korenev and
                  Punit Singh Koura and
                  Marie{-}Anne Lachaux and
                  Thibaut Lavril and
                  Jenya Lee and
                  Diana Liskovich and
                  Yinghai Lu and
                  Yuning Mao and
                  Xavier Martinet and
                  Todor Mihaylov and
                  Pushkar Mishra and
                  Igor Molybog and
                  Yixin Nie and
                  Andrew Poulton and
                  Jeremy Reizenstein and
                  Rashi Rungta and
                  Kalyan Saladi and
                  Alan Schelten and
                  Ruan Silva and
                  Eric Michael Smith and
                  Ranjan Subramanian and
                  Xiaoqing Ellen Tan and
                  Binh Tang and
                  Ross Taylor and
                  Adina Williams and
                  Jian Xiang Kuan and
                  Puxin Xu and
                  Zheng Yan and
                  Iliyan Zarov and
                  Yuchen Zhang and
                  Angela Fan and
                  Melanie Kambadur and
                  Sharan Narang and
                  Aur{\'{e}}lien Rodriguez and
                  Robert Stojnic and
                  Sergey Edunov and
                  Thomas Scialom},
  title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  journal      = {CoRR},
  volume       = {abs/2307.09288},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.09288},
  doi          = {10.48550/arXiv.2307.09288},
  eprinttype    = {arXiv},
  eprint       = {2307.09288},
  timestamp    = {Tue, 25 Jul 2023 16:04:08 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-09288.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@online{MosaicML2023Introducing,
    author    = {MosaicML NLP Team},
    title     = {Introducing MPT-7B: A New Standard for Open-Source,
    ly Usable LLMs},
    year      = {2023},
    url       = {www.mosaicml.com/blog/mpt-7b},
    note      = {Accessed: 2023-08-19},
    urldate   = {2023-08-19}
}

@article{falcon40b,
  title={{Falcon-40B}: an open large language model with state-of-the-art performance},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
  year={2023}
}

@software{together2023redpajama,
  author = {Together Computer},
  title = {RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset},
  month = April,
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}


@article{Qiu2023LatentJA,
  title={Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models},
  author={Huachuan Qiu and Shuai Zhang and Anqi Li and Hongliang He and Zhenzhong Lan},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.08487},
  url={https://api.semanticscholar.org/CorpusID:259937347}
}

@inproceedings{Liu2023TrustworthyLA,
  title={Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment},
  author={Yang Liu and Yuanshun Yao and Jean-Francois Ton and Xiaoying Zhang and Ruocheng Guo Hao Cheng and Yegor Klochkov and Muhammad Faaiz Taufiq and Hanguang Li},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:260775522}
}


@misc{jailbreakingprompts,
  title={Jailbreak Chat},
  howpublished={\url{https://www.jailbreakchat.com/}},
}

@inproceedings{abercrombie-etal-2023-mirages,
    title = "Mirages. On Anthropomorphism in Dialogue Systems",
    author = "Abercrombie, Gavin  and
      Cercas Curry, Amanda  and
      Dinkar, Tanvi  and
      Rieser, Verena  and
      Talat, Zeerak",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.290",
    doi = "10.18653/v1/2023.emnlp-main.290",
    pages = "4776--4790",
    abstract = "Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism is inevitable, conscious and unconscious design choices can guide users to personify them to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to transparency and trust issues, and high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have investigated the factors that induce personification and develop resources to mitigate such effects. However, these efforts are fragmented, and many aspects of anthropomorphism have yet to be explored. In this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise thereof, including reinforcing gender stereotypes and conceptions of acceptable language. We recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users.",
}

@article{claude3,
  title={Introducing the next generation of Claude},
  author={Anthropic},
  year={2024},
  url={https://www.anthropic.com/news/claude-3-family}
}

@misc{derczynski2023assessing,
      title={Assessing Language Model Deployment with Risk Cards}, 
      author={Leon Derczynski and Hannah Rose Kirk and Vidhisha Balachandran and Sachin Kumar and Yulia Tsvetkov and M. R. Leiser and Saif Mohammad},
      year={2023},
      eprint={2303.18190},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.18190}, 
}

@inproceedings{wang-etal-2024-answer,
    title = "Do-Not-Answer: Evaluating Safeguards in {LLM}s",
    author = "Wang, Yuxia  and
      Li, Haonan  and
      Han, Xudong  and
      Nakov, Preslav  and
      Baldwin, Timothy",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2024",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-eacl.61",
    pages = "896--911",
}

@inproceedings{weidingertaxonomy,
author = {Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese, Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa and Biles, Courtney and Brown, Sasha and Kenton, Zac and Hawkins, Will and Stepleton, Tom and Birhane, Abeba and Hendricks, Lisa Anne and Rimell, Laura and Isaac, William and Haas, Julia and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
title = {Taxonomy of Risks posed by Language Models},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533088},
doi = {10.1145/3531146.3533088},
abstract = {Responsible innovation on large-scale Language Models (LMs) requires foresight into and in-depth understanding of the risks these models may pose. This paper develops a comprehensive taxonomy of ethical and social risks associated with LMs. We identify twenty-one risks, drawing on expertise and literature from computer science, linguistics, and the social sciences. We situate these risks in our taxonomy of six risk areas: I. Discrimination, Hate speech and Exclusion, II. Information Hazards, III. Misinformation Harms, IV. Malicious Uses, V. Human-Computer Interaction Harms, and VI. Environmental and Socioeconomic harms. For risks that have already been observed in LMs, the causal mechanism leading to harm, evidence of the risk, and approaches to risk mitigation are discussed. We further describe and analyse risks that have not yet been observed but are anticipated based on assessments of other language technologies, and situate these in the same taxonomy. We underscore that it is the responsibility of organizations to engage with the mitigations we discuss throughout the paper. We close by highlighting challenges and directions for further research on risk evaluation and mitigation with the goal of ensuring that language models are developed responsibly.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {214–229},
numpages = {16},
keywords = {language models, responsible AI, responsible innovation, risk assessment, technology risks},
location = {<conf-loc>, <city>Seoul</city>, <country>Republic of Korea</country>, </conf-loc>},
series = {FAccT '22}
}

@misc{gemmateam2024gemma,
      title={Gemma: Open Models Based on Gemini Research and Technology}, 
      author={Gemma Team and Thomas Mesnard and Cassidy Hardin and Robert Dadashi and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivière and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and Léonard Hussenot and Pier Giuseppe Sessa and Aakanksha Chowdhery and Adam Roberts and Aditya Barua and Alex Botev and Alex Castro-Ros and Ambrose Slone and Amélie Héliou and Andrea Tacchetti and Anna Bulanova and Antonia Paterson and Beth Tsai and Bobak Shahriari and Charline Le Lan and Christopher A. Choquette-Choo and Clément Crepy and Daniel Cer and Daphne Ippolito and David Reid and Elena Buchatskaya and Eric Ni and Eric Noland and Geng Yan and George Tucker and George-Christian Muraru and Grigory Rozhdestvenskiy and Henryk Michalewski and Ian Tenney and Ivan Grishchenko and Jacob Austin and James Keeling and Jane Labanowski and Jean-Baptiste Lespiau and Jeff Stanway and Jenny Brennan and Jeremy Chen and Johan Ferret and Justin Chiu and Justin Mao-Jones and Katherine Lee and Kathy Yu and Katie Millican and Lars Lowe Sjoesund and Lisa Lee and Lucas Dixon and Machel Reid and Maciej Mikuła and Mateo Wirth and Michael Sharman and Nikolai Chinaev and Nithum Thain and Olivier Bachem and Oscar Chang and Oscar Wahltinez and Paige Bailey and Paul Michel and Petko Yotov and Rahma Chaabouni and Ramona Comanescu and Reena Jana and Rohan Anil and Ross McIlroy and Ruibo Liu and Ryan Mullins and Samuel L Smith and Sebastian Borgeaud and Sertan Girgin and Sholto Douglas and Shree Pandya and Siamak Shakeri and Soham De and Ted Klimenko and Tom Hennigan and Vlad Feinberg and Wojciech Stokowiec and Yu-hui Chen and Zafarali Ahmed and Zhitao Gong and Tris Warkentin and Ludovic Peran and Minh Giang and Clément Farabet and Oriol Vinyals and Jeff Dean and Koray Kavukcuoglu and Demis Hassabis and Zoubin Ghahramani and Douglas Eck and Joelle Barral and Fernando Pereira and Eli Collins and Armand Joulin and Noah Fiedel and Evan Senter and Alek Andreev and Kathleen Kenealy},
      year={2024},
      eprint={2403.08295},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{karamolegkou-etal-2023-copyright,
    title = "Copyright Violations and Large Language Models",
    author = "Karamolegkou, Antonia  and
      Li, Jiaang  and
      Zhou, Li  and
      S{\o}gaard, Anders",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.458",
    doi = "10.18653/v1/2023.emnlp-main.458",
    pages = "7403--7412",
    abstract = "Language models may memorize more than just facts, including entire chunks of texts seen during training. Fair use exemptions to copyright laws typically allow for limited use of copyrighted material without permission from the copyright holder, but typically for extraction of information from copyrighted materials, rather than \textit{verbatim} reproduction. This work explores the issue of copyright violations and large language models through the lens of verbatim memorization, focusing on possible redistribution of copyrighted text. We present experiments with a range of language models over a collection of popular books and coding problems, providing a conservative characterization of the extent to which language models can redistribute these materials. Overall, this research highlights the need for further examination and the potential impact on future developments in natural language processing to ensure adherence to copyright regulations. Code is at https://github.com/coastalcph/CopyrightLLMs.",
}

@misc{jiang2024mixtral,
      title={Mixtral of Experts}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Lélio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Théophile Gervet and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2024},
      eprint={2401.04088},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{modelspec, 
year=2024,
url={https://openai.com/index/introducing-the-model-spec}, journal={Introducing the model spec}, publisher={OpenAI}, author={OpenAI}} 


@inproceedings{zhang2021situatedqa,
    title = "{S}ituated{QA}: Incorporating Extra-Linguistic Contexts into {QA}",
    author = "Zhang, Michael  and
      Choi, Eunsol",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.586",
    doi = "10.18653/v1/2021.emnlp-main.586",
    pages = "7371--7387",
    abstract = "Answers to the same question may change depending on the extra-linguistic contexts (when and where the question was asked). To study this challenge, we introduce SituatedQA, an open-retrieval QA dataset where systems must produce the correct answer to a question given the temporal or geographical context. To construct SituatedQA, we first identify such questions in existing QA datasets. We find that a significant proportion of information seeking questions have context-dependent answers (e.g. roughly 16.5{\%} of NQ-Open). For such context-dependent questions, we then crowdsource alternative contexts and their corresponding answers. Our study shows that existing models struggle with producing answers that are frequently updated or from uncommon locations. We further quantify how existing models, which are trained on data collected in the past, fail to generalize to answering questions asked in the present, even when provided with an updated evidence corpus (a roughly 15 point drop in accuracy). Our analysis suggests that open-retrieval QA benchmarks should incorporate extra-linguistic context to stay relevant globally and in the future. Our data, code, and datasheet are available at \url{https://situatedqa.github.io/}.",
}

@inproceedings{
zhao2024wildchat,
title={WildChat: 1M Chat{GPT} Interaction Logs in the Wild},
author={Wenting Zhao and Xiang Ren and Jack Hessel and Claire Cardie and Yejin Choi and Yuntian Deng},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=Bl8u7ZRlbM}
}

@misc{hh-rlhf,
      title={Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned}, 
      author={Deep Ganguli and Liane Lovitt and Jackson Kernion and Amanda Askell and Yuntao Bai and Saurav Kadavath and Ben Mann and Ethan Perez and Nicholas Schiefer and Kamal Ndousse and Andy Jones and Sam Bowman and Anna Chen and Tom Conerly and Nova DasSarma and Dawn Drain and Nelson Elhage and Sheer El-Showk and Stanislav Fort and Zac Hatfield-Dodds and Tom Henighan and Danny Hernandez and Tristan Hume and Josh Jacobson and Scott Johnston and Shauna Kravec and Catherine Olsson and Sam Ringer and Eli Tran-Johnson and Dario Amodei and Tom Brown and Nicholas Joseph and Sam McCandlish and Chris Olah and Jared Kaplan and Jack Clark},
      year={2022},
      eprint={2209.07858},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
dai2023saferlhf,
title={Safe {RLHF}: Safe Reinforcement Learning from Human Feedback},
author={Josef Dai and Xuehai Pan and Ruiyang Sun and Jiaming Ji and Xinbo Xu and Mickel Liu and Yizhou Wang and Yaodong Yang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=TyFrPOKYXw}
}


@inproceedings{
bianchi2023safety,
title={Safety-Tuned {LL}a{MA}s: Lessons From Improving the Safety of Large Language Models that Follow Instructions},
author={Federico Bianchi and Mirac Suzgun and Giuseppe Attanasio and Paul Rottger and Dan Jurafsky and Tatsunori Hashimoto and James Zou},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=gT5hALch9z}
}

@misc{yang2023alignment,
      title={Alignment for Honesty}, 
      author={Yuqing Yang and Ethan Chern and Xipeng Qiu and Graham Neubig and Pengfei Liu},
      year={2024},
      eprint={2312.07000},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.07000}, 
}

@misc{wang2023far,
   title={How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources}, 
   author={Yizhong Wang and Hamish Ivison and Pradeep Dasigi and Jack Hessel and Tushar Khot and Khyathi Raghavi Chandu and David Wadden and Kelsey MacMillan and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},
   year={2023},
   eprint={2306.04751},
   archivePrefix={arXiv},
   primaryClass={cs.CL},
url={https://arxiv.org/abs/2306.04751}
}



@misc{dubois2024lengthcontrolled,
      title={Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators}, 
      author={Yann Dubois and Balázs Galambosi and Percy Liang and Tatsunori B. Hashimoto},
      year={2024},
      eprint={2404.04475},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{srivastava2023beyond,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={Transactions on Machine Learning Research},
  year={2023}
}

@misc{pair,
      title={Jailbreaking Black Box Large Language Models in Twenty Queries}, 
      author={Patrick Chao and Alexander Robey and Edgar Dobriban and Hamed Hassani and George J. Pappas and Eric Wong},
      year={2023},
      eprint={2310.08419},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{liu2023autodan,
      title={AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models}, 
      author={Xiaogeng Liu and Nan Xu and Muhao Chen and Chaowei Xiao},
      year={2023},
      eprint={2310.04451},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2024deepinception,
      title={DeepInception: Hypnotize Large Language Model to Be Jailbreaker}, 
      author={Xuan Li and Zhanke Zhou and Jianing Zhu and Jiangchao Yao and Tongliang Liu and Bo Han},
      year={2024},
      eprint={2311.03191},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{Markov_Zhang_Agarwal_Eloundou_Nekoul_Lee_Adler_Jiang_Weng_2023, title={A Holistic Approach to Undesired Content Detection in the Real World}, volume={37}, url={https://ojs.aaai.org/index.php/AAAI/article/view/26752}, DOI={10.1609/aaai.v37i12.26752}, number={12}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Markov, Todor and Zhang, Chong and Agarwal, Sandhini and Eloundou Nekoul, Florentine and Lee, Theodore and Adler, Steven and Jiang, Angela and Weng, Lilian}, year={2023}, month={Jun.}, pages={15009-15018} }

@inproceedings{banko-etal-2020-unified,
    title = "A Unified Taxonomy of Harmful Content",
    author = "Banko, Michele  and
      MacKeen, Brendon  and
      Ray, Laurie",
    editor = "Akiwowo, Seyi  and
      Vidgen, Bertie  and
      Prabhakaran, Vinodkumar  and
      Waseem, Zeerak",
    booktitle = "Proceedings of the Fourth Workshop on Online Abuse and Harms",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.alw-1.16",
    doi = "10.18653/v1/2020.alw-1.16",
    pages = "125--137",
}
@article{Salminen_Almerekhi_Milenković_Jung_An_Kwak_Jansen_2018, title={Anatomy of Online Hate: Developing a Taxonomy and Machine Learning Models for Identifying and Classifying Hate in Online News Media}, volume={12}, url={https://ojs.aaai.org/index.php/ICWSM/article/view/15028}, DOI={10.1609/icwsm.v12i1.15028}, number={1}, journal={Proceedings of the International AAAI Conference on Web and Social Media}, author={Salminen, Joni and Almerekhi, Hind and Milenković, Milica and Jung, Soon-gyo and An, Jisun and Kwak, Haewoon and Jansen, Bernard}, year={2018}, month={Jun.} }
@article{Zhuo2023ExploringAE,
  title={Exploring AI Ethics of ChatGPT: A Diagnostic Analysis},
  author={Terry Yue Zhuo and Yujin Huang and Chunyang Chen and Zhenchang Xing},
  journal={ArXiv},
  year={2023},
  volume={abs/2301.12867},
  url={https://api.semanticscholar.org/CorpusID:256390238}
}

@inproceedings{reimers-2020-multilingual-sentence-bert,
    title = "Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2020",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/2004.09813",
}

@article{ onoe2021creak,
  title={CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge},
  author={Onoe, Yasumasa and Zhang, Michael J.Q. and Choi, Eunsol and Durrett, Greg},
  journal={OpenReview},
  year={2021}
}

@inproceedings{gardner-etal-2020-evaluating,
    title = "Evaluating Models{'} Local Decision Boundaries via Contrast Sets",
    author = "Gardner, Matt  and
      Artzi, Yoav  and
      Basmov, Victoria  and
      Berant, Jonathan  and
      Bogin, Ben  and
      Chen, Sihao  and
      Dasigi, Pradeep  and
      Dua, Dheeru  and
      Elazar, Yanai  and
      Gottumukkala, Ananth  and
      Gupta, Nitish  and
      Hajishirzi, Hannaneh  and
      Ilharco, Gabriel  and
      Khashabi, Daniel  and
      Lin, Kevin  and
      Liu, Jiangming  and
      Liu, Nelson F.  and
      Mulcaire, Phoebe  and
      Ning, Qiang  and
      Singh, Sameer  and
      Smith, Noah A.  and
      Subramanian, Sanjay  and
      Tsarfaty, Reut  and
      Wallace, Eric  and
      Zhang, Ally  and
      Zhou, Ben",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.117",
    doi = "10.18653/v1/2020.findings-emnlp.117",
    pages = "1307--1323",
    abstract = "Standard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture the abilities a dataset is intended to test. We propose a more rigorous annotation paradigm for NLP that helps to close systematic gaps in the test data. In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets. Contrast sets provide a local view of a model{'}s decision boundary, which can be used to more accurately evaluate a model{'}s true linguistic capabilities. We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (e.g., DROP reading comprehension, UD parsing, and IMDb sentiment analysis). Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets{---}up to 25{\%} in some cases. We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.",
}

@inproceedings{ribeiro-etal-2018-semantically,
    title = "Semantically Equivalent Adversarial Rules for Debugging {NLP} models",
    author = "Ribeiro, Marco Tulio  and
      Singh, Sameer  and
      Guestrin, Carlos",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1079",
    doi = "10.18653/v1/P18-1079",
    pages = "856--865",
    abstract = "Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) {--} semantic-preserving perturbations that induce changes in the model{'}s predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) {--} simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.",
}

@article{
biderman2024lora,
title={Lo{RA} Learns Less and Forgets Less},
author={Dan Biderman and Jacob Portes and Jose Javier Gonzalez Ortiz and Mansheej Paul and Philip Greengard and Connor Jennings and Daniel King and Sam Havens and Vitaliy Chiley and Jonathan Frankle and Cody Blakeney and John Patrick Cunningham},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=aloEru2qCG},
note={Featured Certification}
}

%% Epistemology cites

@misc{ahdritz2024distinguishing,
      title={Distinguishing the Knowable from the Unknowable with Language Models}, 
      author={Gustaf Ahdritz and Tian Qin and Nikhil Vyas and Boaz Barak and Benjamin L. Edelman},
      year={2024},
      eprint={2402.03563},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.03563}, 
}

@article{hestness2017deep,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{kim2024epistemology,
  title={Epistemology of Language Models: Do Language Models Have Holistic Knowledge?},
  author={Kim, Minsu and Thorne, James},
  journal={arXiv preprint arXiv:2403.12862},
  year={2024}
}

@article{heersminkphenomenology,
  title={A phenomenology and epistemology of large language models: Transparency, trust, and trustworthiness},
  author={Heersmink, Richard and de Rooij, Barend and V{\'a}zquez, Mar{\'\i}a Jimena Clavel and Colombo, Matteo}
}

@article{grindrod2019computational,
  title={Computational beliefs},
  author={Grindrod, Jumbly},
  journal={Inquiry},
  pages={1--22},
  year={2019},
  publisher={Taylor \& Francis}
}

@article{kadavath2022language,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}

@inproceedings{varshney2022investigating,
    title = "Investigating Selective Prediction Approaches Across Several Tasks in {IID}, {OOD}, and Adversarial Settings",
    author = "Varshney, Neeraj  and
      Mishra, Swaroop  and
      Baral, Chitta",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.158",
    doi = "10.18653/v1/2022.findings-acl.158",
    pages = "1995--2002",
    abstract = "In order to equip NLP systems with {`}selective prediction{'} capability, several task-specific approaches have been proposed. However, which approaches work best across tasks or even if they consistently outperform the simplest baseline MaxProb remains to be explored. To this end, we systematically study selective prediction in a large-scale setup of 17 datasets across several NLP tasks. Through comprehensive experiments under in-domain (IID), out-of-domain (OOD), and adversarial (ADV) settings, we show that despite leveraging additional resources (held-out data/computation), none of the existing approaches consistently and considerably outperforms MaxProb in all three settings. Furthermore, their performance does not translate well across tasks. For instance, Monte-Carlo Dropout outperforms all other approaches on Duplicate Detection datasets but does not fare well on NLI datasets, especially in the OOD setting. Thus, we recommend that future selective prediction approaches should be evaluated across tasks and settings for reliable estimation of their capabilities.",
}



@inproceedings{desai2020calibration,
    title = "Calibration of Pre-trained Transformers",
    author = "Desai, Shrey  and
      Durrett, Greg",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.21",
    doi = "10.18653/v1/2020.emnlp-main.21",
    pages = "295--302",
    abstract = "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models{'} posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain.",
}

@article{jiang2021can,
  title={How can we know when language models know? on the calibration of language models for question answering},
  author={Jiang, Zhengbao and Araki, Jun and Ding, Haibo and Neubig, Graham},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={962--977},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{mielke2022reducing,
  title={Reducing conversational agents’ overconfidence through linguistic calibration},
  author={Mielke, Sabrina J and Szlam, Arthur and Dinan, Emily and Boureau, Y-Lan},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={857--872},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@book{code2020epistemic,
	address = {Hanover, N.H.},
	author = {Lorraine Code},
	editor = {},
	publisher = {Published for Brown University Press by University Press of New England},
	title = {Epistemic Responsibility},
	year = {1987}
}


@article{rottger2024safetyprompts,
  title={Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety},
  author={R{\"o}ttger, Paul and Pernisi, Fabio and Vidgen, Bertie and Hovy, Dirk},
  journal={arXiv preprint arXiv:2404.05399},
  year={2024}
}
@article{kirk2023personalisation,
  title={Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback},
  author={Kirk, Hannah Rose and Vidgen, Bertie and R{\"o}ttger, Paul and Hale, Scott A},
  journal={arXiv preprint arXiv:2303.05453},
  year={2023}
}
@inproceedings{kumar-etal-2023-language,
    title = "Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey",
    author = "Kumar, Sachin  and
      Balachandran, Vidhisha  and
      Njoo, Lucille  and
      Anastasopoulos, Antonios  and
      Tsvetkov, Yulia",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.241",
    doi = "10.18653/v1/2023.eacl-main.241",
    pages = "3299--3321",
}


@inproceedings{zhang2023safetybench,
    title = "{S}afety{B}ench: Evaluating the Safety of Large Language Models",
    author = "Zhang, Zhexin  and
      Lei, Leqi  and
      Wu, Lindong  and
      Sun, Rui  and
      Huang, Yongkang  and
      Long, Chong  and
      Liu, Xiao  and
      Lei, Xuanyu  and
      Tang, Jie  and
      Huang, Minlie",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.830",
    doi = "10.18653/v1/2024.acl-long.830",
    pages = "15537--15553",
    abstract = "With the rapid development of Large Language Models (LLMs), increasing attention has been paid to their safety concerns. Consequently, evaluating the safety of LLMs has become an essential task for facilitating the broad applications of LLMs. Nevertheless, the absence of comprehensive safety evaluation benchmarks poses a significant impediment to effectively assess and enhance the safety of LLMs. In this work, we present SafetyBench, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. Notably, SafetyBench also incorporates both Chinese and English data, facilitating the evaluation in both languages. Our extensive tests over 25 popular Chinese and English LLMs in both zero-shot and few-shot settings reveal a substantial performance advantage for GPT-4 over its counterparts, and there is still significant room for improving the safety of current LLMs. We also demonstrate that the measured safety understanding abilities in SafetyBench are correlated with safety generation abilities. Data and evaluation guidelines are available at https://github.com/thu-coai/SafetyBench. Submission entrance and leaderboard are available at https://llmbench.ai/safety.",
}

@inproceedings{zhong2024rose,
    title = "{ROSE} Doesn{'}t Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding",
    author = "Zhong, Qihuang  and
      Ding, Liang  and
      Liu, Juhua  and
      Du, Bo  and
      Tao, Dacheng",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand and virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.814",
    doi = "10.18653/v1/2024.findings-acl.814",
    pages = "13721--13736",
    abstract = "With the development of instruction-tuned large language models (LLMs), improving the safety of LLMs has become more critical. However, the current approaches for aligning the LLMs output with expected safety usually require substantial training efforts, e.g., high-quality safety data and expensive computational resources, which are costly and inefficient. To this end, we present reverse prompt contrastive decoding (ROSE), a simple-yet-effective method to directly boost the safety of existing instruction-tuned LLMs without any additional training. The principle of ROSE is to improve the probability of desired safe output via suppressing the undesired output induced by the carefully-designed reverse prompts. Experiments on 6 safety and 2 general-purpose tasks show that, our ROSE not only brings consistent and significant safety improvements (up to +13.8{\%} safety score) upon 5 types of instruction-tuned LLMs, but also benefits the general-purpose ability of LLMs. In-depth analyses explore the underlying mechanism of ROSE, and reveal when and where to use it.",
}



@article{lee2024learning,
  title={Learning diverse attacks on large language models for robust red-teaming and safety tuning},
  author={Lee, Seanie and Kim, Minsu and Cherif, Lynn and Dobre, David and Lee, Juho and Hwang, Sung Ju and Kawaguchi, Kenji and Gidel, Gauthier and Bengio, Yoshua and Malkin, Nikolay and others},
  journal={arXiv preprint arXiv:2405.18540},
  year={2024}
}
@article{liu2023trustworthy,
  title={Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment},
  author={Liu, Yang and Yao, Yuanshun and Ton, Jean-Francois and Zhang, Xiaoying and Cheng, Ruocheng Guo Hao and Klochkov, Yegor and Taufiq, Muhammad Faaiz and Li, Hang},
  journal={arXiv preprint arXiv:2308.05374},
  year={2023}
}

@misc{derner2023beyond,
      title={Beyond the Safeguards: Exploring the Security Risks of ChatGPT}, 
      author={Erik Derner and Kristina Batistič},
      year={2023},
      eprint={2305.08005},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2305.08005}, 
}

@article{Brown2020LanguageMA,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.14165},
  url={https://api.semanticscholar.org/CorpusID:218971783}
}
@article{Touvron2023LLaMAOA,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth{\'e}e Lacroix and Baptiste Rozi{\`e}re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.13971},
  url={https://api.semanticscholar.org/CorpusID:257219404}
}
@article{Bai2022TrainingAH,
  title={Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback},
  author={Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova Dassarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and John Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom B. Brown and Jack Clark and Sam McCandlish and Christopher Olah and Benjamin Mann and Jared Kaplan},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.05862},
  url={https://api.semanticscholar.org/CorpusID:248118878}
}

@article{Kirkpatrick_2017,
   title={Overcoming catastrophic forgetting in neural networks},
   volume={114},
   ISSN={1091-6490},
   url={http://dx.doi.org/10.1073/pnas.1611835114},
   DOI={10.1073/pnas.1611835114},
   number={13},
   journal={Proceedings of the National Academy of Sciences},
   publisher={Proceedings of the National Academy of Sciences},
   author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
   year={2017},
   month=mar, pages={3521–3526} }


@misc{goodfellow2015empirical,
      title={An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks}, 
      author={Ian J. Goodfellow and Mehdi Mirza and Da Xiao and Aaron Courville and Yoshua Bengio},
      year={2015},
      eprint={1312.6211},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{jacovi2021trust,
author = {Jacovi, Alon and Marasovi\'{c}, Ana and Miller, Tim and Goldberg, Yoav},
title = {Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445923},
doi = {10.1145/3442188.3445923},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {624–635},
numpages = {12},
keywords = {artificial intelligence, contractual trust, distrust, formalization, sociology, trust, trustworthy, warranted trust},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@unpublished{röttger2024safetyprompts, title={SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and Improving Large Language Model Safety}, author={Paul Röttger and Fabio Pernisi and Bertie Vidgen and Dirk Hovy}, year={2024}, eprint={2404.05399}, archivePrefix={arXiv}, primaryClass={cs.CL},url={https://safetyprompts.com/}}


@article{Markov_Zhang_Agarwal_EloundouNekoul_Lee_Adler_Jiang_Weng_2023, 
title={A Holistic Approach to Undesired Content Detection in the Real World}, volume={37}, 
url={https://ojs.aaai.org/index.php/AAAI/article/view/26752}, 
DOI={10.1609/aaai.v37i12.26752}, 
number={12}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Markov, Todor and Zhang, Chong and Agarwal, Sandhini and Eloundou Nekoul, Florentine and Lee, Theodore and Adler, Steven and Jiang, Angela and Weng, Lilian}, 
year={2023}, 
month={Jun.}, 
pages={15009-15018} 
}

@inproceedings{rajpurkar-etal-2018-know,
    title = "Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}",
    author = "Rajpurkar, Pranav  and
      Jia, Robin  and
      Liang, Percy",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-2124",
    doi = "10.18653/v1/P18-2124",
    pages = "784--789",
}


@inproceedings{min2020ambigqa,
  title={AmbigQA: Answering Ambiguous Open-domain Questions},
  author={Min, Sewon and Michael, Julian and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={5783--5797},
  year={2020}
}

@inproceedings{pyatkin2023clarifydelphi,
  title={ClarifyDelphi: Reinforced Clarification Questions with Defeasibility Rewards for Social and Moral Situations},
  author={Pyatkin, Valentina and Hwang, Jena D and Srikumar, Vivek and Lu, Ximing and Jiang, Liwei and Choi, Yejin and Bhagavatula, Chandra},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={11253--11271},
  year={2023}
}

@article{zhang2023clarify,
  title={Clarify When Necessary: Resolving Ambiguity Through Interaction with LMs},
  author={Zhang, Michael JQ and Choi, Eunsol},
  url={https://arxiv.org/abs/2311.09469},
  journal={arXiv preprint arXiv:2311.09469},
  year={2023}
}

@inproceedings{rao2018learning,
  title={Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information},
  author={Rao, Sudha and Daum{\'e} III, Hal},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2737--2746},
  year={2018}
}

@inproceedings{majumder2021ask,
  title={Ask what’s missing and what’s useful: Improving Clarification Question Generation using Global Knowledge},
  author={Majumder, Bodhisattwa Prasad and Rao, Sudha and Galley, Michel and McAuley, Julian},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4300--4312},
  year={2021}
}

@inproceedings{jia-liang-2017-adversarial,
    title = "Adversarial Examples for Evaluating Reading Comprehension Systems",
    author = "Jia, Robin  and
      Liang, Percy",
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1215",
    doi = "10.18653/v1/D17-1215",
    pages = "2021--2031",
}

@inproceedings{clark-gardner-2018-simple,
    title = "Simple and Effective Multi-Paragraph Reading Comprehension",
    author = "Clark, Christopher  and
      Gardner, Matt",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1078",
    doi = "10.18653/v1/P18-1078",
    pages = "845--855",
}

@inproceedings{levy-etal-2017-zero,
    title = "Zero-Shot Relation Extraction via Reading Comprehension",
    author = "Levy, Omer  and
      Seo, Minjoon  and
      Choi, Eunsol  and
      Zettlemoyer, Luke",
    editor = "Levy, Roger  and
      Specia, Lucia",
    booktitle = "Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K17-1034",
    doi = "10.18653/v1/K17-1034",
    pages = "333--342",
}

@unpublished{baan2023uncertainty,
  title={Uncertainty in natural language generation: From theory to applications},
  author={Baan, Joris and Daheim, Nico and Ilia, Evgenia and Ulmer, Dennis and Li, Haau-Sing and Fern{\'a}ndez, Raquel and Plank, Barbara and Sennrich, Rico and Zerva, Chrysoula and Aziz, Wilker},
  journal={arXiv preprint arXiv:2307.15703},
  year={2023},
url={https://arxiv.org/abs/2307.15703}
}

@inproceedings{kuhn2022clam,
  title={Clam: Selective clarification for ambiguous questions with generative language models},
  author={Kuhn, Lorenz and Gal, Yarin and Farquhar, Sebastian},
  booktitle={ICML Workshop on Deployable Generative AI},
  year={2023},
url={https://openreview.net/forum?id=VQWuqgSoVN#all}
}

@article{10.1145/3534965,
author = {Keyvan, Kimiya and Huang, Jimmy Xiangji},
title = {How to Approach Ambiguous Queries in Conversational Search: A Survey of Techniques, Approaches, Tools, and Challenges},
year = {2022},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3534965},
doi = {10.1145/3534965},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {129},
numpages = {40},
keywords = {Ambiguous query, query understanding, Conversational Search System, conversational question answering, conversational agents, dialogue system}
}

@inproceedings{xu-etal-2019-asking,
    title = "Asking Clarification Questions in Knowledge-Based Question Answering",
    author = "Xu, Jingjing  and
      Wang, Yuechen  and
      Tang, Duyu  and
      Duan, Nan  and
      Yang, Pengcheng  and
      Zeng, Qi  and
      Zhou, Ming  and
      Sun, Xu",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1172",
    doi = "10.18653/v1/D19-1172",
    pages = "1618--1629",
}

@misc{california_legislature_sb1001,
  author       = {California Legislature},
  title        = {Senate Bill No. 1001},
  howpublished = {\url{https://digitaldemocracy.calmatters.org/bills/ca_201720180sb1001}},
  note         = {Accessed: 2024-06-05}
}

@unpublished{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  note={arXiv preprint arXiv:2206.04615},
  year={2022},
url={https://arxiv.org/abs/2206.04615}
}

@unpublished{amayuelas2023knowledge,
  title={Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models},
  author={Amayuelas, Alfonso and Pan, Liangming and Chen, Wenhu and Wang, William},
  note={arXiv preprint arXiv:2305.13712},
  year={2023},
url={https://arxiv.org/abs/2305.13712}
}


@misc{cheng2024dated,
      title={Dated Data: Tracing Knowledge Cutoffs in Large Language Models}, 
      author={Jeffrey Cheng and Marc Marone and Orion Weller and Dawn Lawrie and Daniel Khashabi and Benjamin Van Durme},
      year={2024},
      eprint={2403.12958},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{feng2024don,
    title = "Don{'}t Hallucinate, Abstain: Identifying {LLM} Knowledge Gaps via Multi-{LLM} Collaboration",
    author = "Feng, Shangbin  and
      Shi, Weijia  and
      Wang, Yike  and
      Ding, Wenxuan  and
      Balachandran, Vidhisha  and
      Tsvetkov, Yulia",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.786",
    doi = "10.18653/v1/2024.acl-long.786",
    pages = "14664--14690",
    abstract = "Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps{---}missing or outdated information in LLMs{---}might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3{\%} improvements on abstain accuracy against the strongest baseline. Further analysis reveals that our abstention methods pinpoint failure cases in retrieval augmentation and knowledge gaps in multi-hop reasoning.",
}

@inproceedings{kamath-etal-2020-selective,
    title = "Selective Question Answering under Domain Shift",
    author = "Kamath, Amita  and
      Jia, Robin  and
      Liang, Percy",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.503",
    doi = "10.18653/v1/2020.acl-main.503",
    pages = "5684--5696",
}
@article{Whitehead2022ReliableVQ,
  title={Reliable Visual Question Answering: Abstain Rather Than Answer Incorrectly},
  author={Spencer Whitehead and Suzanne Petryk and Vedaad Shakib and Joseph E. Gonzalez and Trevor Darrell and Anna Rohrbach and Marcus Rohrbach},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.13631},
  url={https://api.semanticscholar.org/CorpusID:248426953}
}

@article{huang2023lorahub,
  title={Lorahub: Efficient cross-task generalization via dynamic lora composition},
  author={Huang, Chengsong and Liu, Qian and Lin, Bill Yuchen and Pang, Tianyu and Du, Chao and Lin, Min},
  journal={arXiv preprint arXiv:2307.13269},
  year={2023}
}



@inproceedings{jiang2024wildteamingscaleinthewildjailbreaks,
      title={WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models}, 
      author={Liwei Jiang and Kavel Rao and Seungju Han and Allyson Ettinger and Faeze Brahman and Sachin Kumar and Niloofar Mireshghallah and Ximing Lu and Maarten Sap and Yejin Choi and Nouha Dziri},
      year={2024},
      eprint={2406.18510},
    booktitle={Proceedings of the Neural Information Processing Systems},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.18510}, 
}

@article{zhou2024haicosystem,
  title={HAICOSYSTEM: An Ecosystem for Sandboxing Safety Risks in Human-AI Interactions},
  author={Zhou, Xuhui and Kim, Hyunwoo and Brahman, Faeze and Jiang, Liwei and Zhu, Hao and Lu, Ximing and Xu, Frank and Lin, Bill Yuchen and Choi, Yejin and Mireshghallah, Niloofar and Le Bras, Ronan and Sap, Maarten},
  journal={arXiv},
  year={2024},
  url={http://arxiv.org/abs/2409.16427}
}