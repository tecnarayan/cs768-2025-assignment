@article{mittal2021compositional,
  title={Compositional Attention: Disentangling Search and Retrieval},
  author={Mittal, Sarthak and Raparthy, Sharath Chandra and Rish, Irina and Bengio, Yoshua and Lajoie, Guillaume},
  journal={arXiv preprint arXiv:2110.09419},
  year={2021}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{bengio2017consciousness,
  title={The consciousness prior},
  author={Bengio, Yoshua},
  journal={arXiv preprint arXiv:1709.08568},
  year={2017}
}

@book{peters2017elements,
  title={Elements of causal inference: foundations and learning algorithms},
  author={Peters, Jonas and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
  year={2017},
  publisher={The MIT Press}
}

@article{goyal2020inductive,
  title={Inductive biases for deep learning of higher-level cognition},
  author={Goyal, Anirudh and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2011.15091},
  year={2020}
}


@article{kipf2018neural,
  title={Neural relational inference for interacting systems},
  author={Kipf, Thomas and Fetaya, Ethan and Wang, Kuan-Chieh and Welling, Max and Zemel, Richard},
  journal={arXiv preprint arXiv:1802.04687},
  year={2018}
}

@article{goyal2021neural,
  title={Neural Production Systems},
  author={Goyal, Anirudh and Didolkar, Aniket and Ke, Nan Rosemary and Blundell, Charles and Beaudoin, Philippe and Heess, Nicolas and Mozer, Michael and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2103.01937},
  year={2021}
}

@inproceedings{andreas2016neural,
  title={Neural module networks},
  author={Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={39--48},
  year={2016}
}

@article{higgins2016beta,
  title={beta-vae: Learning basic visual concepts with a constrained variational framework},
  author={Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  year={2016}
}

@inproceedings{kim2018disentangling,
  title={Disentangling by factorising},
  author={Kim, Hyunjik and Mnih, Andriy},
  booktitle={International Conference on Machine Learning},
  pages={2649--2658},
  year={2018},
  organization={PMLR}
}

@inproceedings{bengio2013deep,
  title={Deep learning of representations: Looking forward},
  author={Bengio, Yoshua},
  booktitle={International conference on statistical language and speech processing},
  pages={1--37},
  year={2013},
  organization={Springer}
}

@article{goyal2019recurrent,
  title={Recurrent independent mechanisms},
  author={Goyal, Anirudh and Lamb, Alex and Hoffmann, Jordan and Sodhani, Shagun and Levine, Sergey and Bengio, Yoshua and Sch{\"o}lkopf, Bernhard},
  journal={arXiv preprint arXiv:1909.10893},
  year={2019}
}

@inproceedings{hu2017learning,
  title={Learning to reason: End-to-end module networks for visual question answering},
  author={Hu, Ronghang and Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Saenko, Kate},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={804--813},
  year={2017}
}

@article{battaglia2018relational,
  title={Relational inductive biases, deep learning, and graph networks},
  author={Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and others},
  journal={arXiv preprint arXiv:1806.01261},
  year={2018}
}


@article{baars1997theatre,
  title={In the theatre of consciousness. Global workspace theory, a rigorous scientific theory of consciousness},
  author={Baars, Bernard J},
  journal={Journal of Consciousness Studies},
  volume={4},
  number={4},
  pages={292--309},
  year={1997},
  publisher={Imprint Academic}
}

@article{Dehaene-et-al-2017,
 title = {What is consciousness, and could machines have it?},
 author = {S. Dehaene and H. Lau and S. Kouider},
 journal = {Science},
 volume=358,
 number=6362, 
 pages={486--492},
 year=2017,
 }
 
@inproceedings{bengio2019meta,
  title={A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms},
  author={Bengio, Yoshua and Deleu, Tristan and Rahaman, Nasim and Ke, Nan Rosemary and Lachapelle, Sebastien and Bilaniuk, Olexa and Goyal, Anirudh and Pal, Christopher},
  booktitle={ICLR'2020, arXiv:1901.10912},
  year={2019}
}

@article{santoro2018relational,
  title={Relational recurrent neural networks},
  author={Santoro, Adam and Faulkner, Ryan and Raposo, David and Rae, Jack and Chrzanowski, Mike and Weber, Theophane and Wierstra, Daan and Vinyals, Oriol and Pascanu, Razvan and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:1806.01822},
  year={2018}
}

@inproceedings{mittal2020learning,
  title={Learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules},
  author={Mittal, Sarthak and Lamb, Alex and Goyal, Anirudh and Voleti, Vikram and Shanahan, Murray and Lajoie, Guillaume and Mozer, Michael and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={6972--6986},
  year={2020},
  organization={PMLR}
}

@article{ke2021systematic,
  title={Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning},
  author={Ke, Nan Rosemary and Didolkar, Aniket Rajiv and Mittal, Sarthak and Goyal, Anirudh and Lajoie, Guillaume and Bauer, Stefan and Rezende, Danilo Jimenez and Mozer, Michael Curtis and Bengio, Yoshua and Pal, Christopher},
  year={2021}
}

@article{locatello2020object,
  title={Object-centric learning with slot attention},
  author={Locatello, Francesco and Weissenborn, Dirk and Unterthiner, Thomas and Mahendran, Aravindh and Heigold, Georg and Uszkoreit, Jakob and Dosovitskiy, Alexey and Kipf, Thomas},
  journal={arXiv preprint arXiv:2006.15055},
  year={2020}
}

@article{graves2014neural,
  title={Neural turing machines},
  author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  journal={arXiv preprint arXiv:1410.5401},
  year={2014}
}

@article{Bahdanau2015NeuralMT,
  title={Neural Machine Translation by Jointly Learning to Align and Translate},
  author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  journal={ICLR'2015},
  year={2015},
  volume={arXiv:1409.0473}
}

@article{universal_tsf,
  author    = {Mostafa Dehghani and
               Stephan Gouws and
               Oriol Vinyals and
               Jakob Uszkoreit and
               Lukasz Kaiser},
  title     = {Universal Transformers},
  journal   = {CoRR},
  volume    = {abs/1807.03819},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.03819},
  archivePrefix = {arXiv},
  eprint    = {1807.03819},
  timestamp = {Mon, 13 Aug 2018 16:49:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1807-03819.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{xu2015show,
  title={Show, attend and tell: Neural image caption generation with visual attention},
  author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={2048--2057},
  year={2015},
  organization={PMLR}
}

@article{luong2015effective,
  title={Effective approaches to attention-based neural machine translation},
  author={Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D},
  journal={arXiv preprint arXiv:1508.04025},
  year={2015}
}

@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@article{madan2021fast,
  title={Fast and Slow Learning of Recurrent Independent Mechanisms},
  author={Madan, Kanika and Ke, Rosemary Nan and Goyal, Anirudh and Sch{\"o}lkopf, Bernhard Bernhard and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2105.08710},
  year={2021}
}

@article{goyal2020object,
  title={Object files and schemata: Factorizing declarative and procedural knowledge in dynamical systems},
  author={Goyal, Anirudh and Lamb, Alex and Gampa, Phanideep and Beaudoin, Philippe and Levine, Sergey and Blundell, Charles and Bengio, Yoshua and Mozer, Michael},
  journal={arXiv preprint arXiv:2006.16225},
  year={2020}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{ding2020object,
  title={Object-based attention for spatio-temporal reasoning: Outperforming neuro-symbolic models with flexible distributed architectures},
  author={Ding, David and Hill, Felix and Santoro, Adam and Botvinick, Matt},
  journal={arXiv preprint arXiv:2012.08508},
  year={2020}
}

@article{santoro2017simple,
  title={A simple neural network module for relational reasoning},
  author={Santoro, Adam and Raposo, David and Barrett, David GT and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:1706.01427},
  year={2017}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@article{nogueira2021investigating,
  title={Investigating the Limitations of Transformers with Simple Arithmetic Tasks},
  author={Nogueira, Rodrigo and Jiang, Zhiying and Lin, Jimmy},
  journal={arXiv preprint arXiv:2102.13019},
  year={2021}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@InProceedings{pmlr-v80-lake18a,
  title = 	 {Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks},
  author =       {Lake, Brenden and Baroni, Marco},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2873--2882},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/lake18a/lake18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/lake18a.html},
  abstract = 	 {Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks’ notorious training data thirst.}
}
@article{DBLP:journals/corr/abs-1802-06467,
  author    = {Adam Liska and
               Germ{\'{a}}n Kruszewski and
               Marco Baroni},
  title     = {Memorize or generalize? Searching for a compositional {RNN} in a haystack},
  journal   = {CoRR},
  volume    = {abs/1802.06467},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.06467},
  archivePrefix = {arXiv},
  eprint    = {1802.06467},
  timestamp = {Mon, 13 Aug 2018 16:47:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1802-06467.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{goyal2021coordination,
  title={Coordination among neural modules through a shared global workspace},
  author={Goyal, Anirudh and Didolkar, Aniket and Lamb, Alex and Badola, Kartikeya and Ke, Nan Rosemary and Rahaman, Nasim and Binas, Jonathan and Blundell, Charles and Mozer, Michael and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2103.01197},
  year={2021}
}

@misc{chen2021decision,
      title={Decision Transformer: Reinforcement Learning via Sequence Modeling}, 
      author={Lili Chen and Kevin Lu and Aravind Rajeswaran and Kimin Lee and Aditya Grover and Michael Laskin and Pieter Abbeel and Aravind Srinivas and Igor Mordatch},
      year={2021},
      eprint={2106.01345},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{lake2018generalization,
  title={Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks},
  author={Lake, Brenden and Baroni, Marco},
  booktitle={International conference on machine learning},
  pages={2873--2882},
  year={2018},
  organization={PMLR}
}

@article{newman2020eos,
  title={The EOS decision and length extrapolation},
  author={Newman, Benjamin and Hewitt, John and Liang, Percy and Manning, Christopher D},
  journal={arXiv preprint arXiv:2010.07174},
  year={2020}
}

@article{csordas2021devil,
  title={The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers},
  author={Csord{\'a}s, R{\'o}bert and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:2108.12284},
  year={2021}
}

@article{kerg2020untangling,
  title={Untangling tradeoffs between recurrence and self-attention in artificial neural networks},
  author={Kerg, Giancarlo and Kanuparthi, Bhargav and ALIAS PARTH GOYAL, Anirudh Goyal and Goyette, Kyle and Bengio, Yoshua and Lajoie, Guillaume},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{hudson2018compositional,
  title={Compositional attention networks for machine reasoning},
  author={Hudson, Drew A and Manning, Christopher D},
  journal={arXiv preprint arXiv:1803.03067},
  year={2018}
}

@article{selvakumar2018compositional,
  title={Compositional attention networks for interpretability in natural language question answering},
  author={Selvakumar, Muru and Ramamoorthy, Suriyadeepan and Archana, Vaidheeswaran and Sankarasubbu, Malaikannan},
  journal={arXiv preprint arXiv:1810.12698},
  year={2018}
}

@article{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@article{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@article{tay2020efficient,
  title={Efficient transformers: A survey},
  author={Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal={arXiv preprint arXiv:2009.06732},
  year={2020}
}

@article{li2019compositional,
  title={Compositional generalization for primitive substitutions},
  author={Li, Yuanpeng and Zhao, Liang and Wang, Jianyu and Hestness, Joel},
  journal={arXiv preprint arXiv:1910.02612},
  year={2019}
}

@article{keysers2019measuring,
  title={Measuring compositional generalization: A comprehensive method on realistic data},
  author={Keysers, Daniel and Sch{\"a}rli, Nathanael and Scales, Nathan and Buisman, Hylke and Furrer, Daniel and Kashubin, Sergii and Momchev, Nikola and Sinopalnikov, Danila and Stafiniak, Lukasz and Tihon, Tibor and others},
  journal={arXiv preprint arXiv:1912.09713},
  year={2019}
}

@article{chen2020compositional,
  title={Compositional generalization via neural-symbolic stack machines},
  author={Chen, Xinyun and Liang, Chen and Yu, Adams Wei and Song, Dawn and Zhou, Denny},
  journal={arXiv preprint arXiv:2008.06662},
  year={2020}
}

@article{hupkes2020compositionality,
  title={Compositionality decomposed: how do neural networks generalise?},
  author={Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  journal={Journal of Artificial Intelligence Research},
  volume={67},
  pages={757--795},
  year={2020}
}

@article{kim2020cogs,
  title={COGS: A compositional generalization challenge based on semantic interpretation},
  author={Kim, Najoung and Linzen, Tal},
  journal={arXiv preprint arXiv:2010.05465},
  year={2020}
}

@article{yuksel2012twenty,
  title={Twenty years of mixture of experts},
  author={Yuksel, Seniha Esen and Wilson, Joseph N and Gader, Paul D},
  journal={IEEE transactions on neural networks and learning systems},
  volume={23},
  number={8},
  pages={1177--1193},
  year={2012},
  publisher={IEEE}
}

@article{masoudnia2014mixture,
  title={Mixture of experts: a literature survey},
  author={Masoudnia, Saeed and Ebrahimpour, Reza},
  journal={Artificial Intelligence Review},
  volume={42},
  number={2},
  pages={275--293},
  year={2014},
  publisher={Springer}
}

@article{fedus2021switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021}
}

@article{du2021glam,
  title={GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  journal={arXiv preprint arXiv:2112.06905},
  year={2021}
}

@article{rahaman2021dynamic,
  title={Dynamic Inference with Neural Interpreters},
  author={Rahaman, Nasim and Gondal, Muhammad Waleed and Joshi, Shruti and Gehler, Peter and Bengio, Yoshua and Locatello, Francesco and Sch{\"o}lkopf, Bernhard},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{rosenbaum2019routing,
  title={Routing networks and the challenges of modular and compositional computation},
  author={Rosenbaum, Clemens and Cases, Ignacio and Riemer, Matthew and Klinger, Tim},
  journal={arXiv preprint arXiv:1904.12774},
  year={2019}
}

@article{maziarz2019flexible,
  title={Flexible multi-task networks by learning parameter allocation},
  author={Maziarz, Krzysztof and Kokiopoulou, Efi and Gesmundo, Andrea and Sbaiz, Luciano and Bartok, Gabor and Berent, Jesse},
  journal={arXiv preprint arXiv:1910.04915},
  year={2019}
}

@article{cui2020re,
  title={Re-examining Routing Networks for Multi-task Learning},
  author={Cui, Limeng and Jaech, Aaron},
  year={2020}
}

@article{kuhn1955hungarian,
  title={The Hungarian method for the assignment problem},
  author={Kuhn, Harold W},
  journal={Naval research logistics quarterly},
  volume={2},
  number={1-2},
  pages={83--97},
  year={1955},
  publisher={Wiley Online Library}
}

@article{csordas2020neural,
  title={Are neural nets modular? inspecting functional modularity through differentiable weight masks},
  author={Csord{\'a}s, R{\'o}bert and van Steenkiste, Sjoerd and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:2010.02066},
  year={2020}
}