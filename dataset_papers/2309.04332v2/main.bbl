\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alon \& Yahav(2021)Alon and Yahav]{alon2021bottleneck}
Alon, U. and Yahav, E.
\newblock On the bottleneck of graph neural networks and its practical implications, 2021.

\bibitem[Barabasi \& Albert(1999)Barabasi and Albert]{badist}
Barabasi, A.-L. and Albert, R.
\newblock Emergence of scaling in random networks.
\newblock \emph{Science}, 286\penalty0 (5439):\penalty0 509--512, 1999.
\newblock \doi{10.1126/science.286.5439.509}.
\newblock URL \url{http://www.sciencemag.org/cgi/content/abstract/286/5439/509}.

\bibitem[Brody et~al.(2022)Brody, Alon, and Yahav]{brody2022attentive}
Brody, S., Alon, U., and Yahav, E.
\newblock How attentive are graph attention networks?, 2022.

\bibitem[Chen et~al.(2020)Chen, Chen, and Bruna]{chen2020graph}
Chen, L., Chen, Z., and Bruna, J.
\newblock On graph neural networks versus graph-augmented mlps, 2020.

\bibitem[Dobson \& Doig(2003)Dobson and Doig]{proteins}
Dobson, P.~D. and Doig, A.~J.
\newblock Distinguishing enzyme structures from non-enzymes without alignments.
\newblock \emph{Journal of molecular biology}, 330 4:\penalty0 771--83, 2003.

\bibitem[Erd\"{o}s \& R\'{e}nyi(1959)Erd\"{o}s and R\'{e}nyi]{gnp}
Erd\"{o}s, P. and R\'{e}nyi, A.
\newblock On random graphs i.
\newblock \emph{Publicationes Mathematicae Debrecen}, 6:\penalty0 290, 1959.

\bibitem[Errica et~al.(2022)Errica, Podda, Bacciu, and Micheli]{errica2022fair}
Errica, F., Podda, M., Bacciu, D., and Micheli, A.
\newblock A fair comparison of graph neural networks for graph classification, 2022.

\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and Dahl]{mpgnn}
Gilmer, J., Schoenholz, S.~S., Riley, P.~F., Vinyals, O., and Dahl, G.~E.
\newblock Neural message passing for quantum chemistry, 2017.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and Srebro]{NEURIPS2018_0e98aeeb}
Gunasekar, S., Lee, J.~D., Soudry, D., and Srebro, N.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem[Hamilton et~al.(2017)Hamilton, Ying, and Leskovec]{graphsage}
Hamilton, W.~L., Ying, R., and Leskovec, J.
\newblock Inductive representation learning on large graphs, 2017.
\newblock URL \url{https://arxiv.org/abs/1706.02216}.

\bibitem[Hu et~al.(2020)Hu, Fey, Zitnik, Dong, Ren, Liu, Catasta, and Leskovec]{ogb}
Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M., and Leskovec, J.
\newblock Open graph benchmark: Datasets for machine learning on graphs, 2020.
\newblock URL \url{https://arxiv.org/abs/2005.00687}.

\bibitem[Kipf \& Welling(2017{\natexlab{a}})Kipf and Welling]{gcn}
Kipf, T.~N. and Welling, M.
\newblock Semi-supervised classification with graph convolutional networks, 2017{\natexlab{a}}.

\bibitem[Kipf \& Welling(2017{\natexlab{b}})Kipf and Welling]{kipf2017semisupervised}
Kipf, T.~N. and Welling, M.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock In \emph{International Conference on Learning Representations}, 2017{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=SJU4ayYgl}.

\bibitem[Liu et~al.(2020)Liu, Gao, and Ji]{10.1145/3394486.3403076}
Liu, M., Gao, H., and Ji, S.
\newblock Towards deeper graph neural networks.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, KDD '20, pp.\  338–348, New York, NY, USA, 2020. Association for Computing Machinery.
\newblock ISBN 9781450379984.
\newblock \doi{10.1145/3394486.3403076}.
\newblock URL \url{https://doi.org/10.1145/3394486.3403076}.

\bibitem[Lyu \& Li(2020)Lyu and Li]{lyu2020gradient}
Lyu, K. and Li, J.
\newblock Gradient descent maximizes the margin of homogeneous neural networks, 2020.

\bibitem[Morris et~al.(2020)Morris, Kriege, Bause, Kersting, Mutzel, and Neumann]{tudataset}
Morris, C., Kriege, N.~M., Bause, F., Kersting, K., Mutzel, P., and Neumann, M.
\newblock Tudataset: A collection of benchmark datasets for learning with graphs.
\newblock In \emph{ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020)}, 2020.
\newblock URL \url{www.graphlearning.io}.

\bibitem[Morris et~al.(2021)Morris, Ritzert, Fey, Hamilton, Lenssen, Rattan, and Grohe]{morris2021weisfeiler}
Morris, C., Ritzert, M., Fey, M., Hamilton, W.~L., Lenssen, J.~E., Rattan, G., and Grohe, M.
\newblock Weisfeiler and leman go neural: Higher-order graph neural networks, 2021.

\bibitem[Rong et~al.(2019)Rong, bing Huang, Xu, and Huang]{Rong2019DropEdgeTD}
Rong, Y., bing Huang, W., Xu, T., and Huang, J.
\newblock Dropedge: Towards deep graph convolutional networks on node classification.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:212859361}.

\bibitem[Seddik et~al.(2022)Seddik, Wu, Lutzeyer, and Vazirgiannis]{seddik2022node}
Seddik, M. E.~A., Wu, C., Lutzeyer, J.~F., and Vazirgiannis, M.
\newblock Node feature kernels increase graph convolutional network robustness, 2022.

\bibitem[Shervashidze et~al.(2011)Shervashidze, Schweitzer, van Leeuwen, Mehlhorn, and Borgwardt]{wl}
Shervashidze, N., Schweitzer, P., van Leeuwen, E.~J., Mehlhorn, K., and Borgwardt, K.~M.
\newblock Weisfeiler-lehman graph kernels.
\newblock \emph{J. Mach. Learn. Res.}, 12:\penalty0 2539--2561, 2011.
\newblock URL \url{http://dblp.uni-trier.de/db/journals/jmlr/jmlr12.html#ShervashidzeSLMB11}.

\bibitem[Shi et~al.(2021)Shi, Huang, Feng, Zhong, Wang, and Sun]{shi2021masked}
Shi, Y., Huang, Z., Feng, S., Zhong, H., Wang, W., and Sun, Y.
\newblock Masked label prediction: Unified message passing model for semi-supervised classification, 2021.

\bibitem[Soudry et~al.(2017)Soudry, Hoffer, Nacson, Gunasekar, and Srebro]{implcit_bias_separable_data}
Soudry, D., Hoffer, E., Nacson, M.~S., Gunasekar, S., and Srebro, N.
\newblock The implicit bias of gradient descent on separable data, 2017.
\newblock URL \url{https://arxiv.org/abs/1710.10345}.

\bibitem[Veli{\v{c}}ković et~al.(2018)Veli{\v{c}}ković, Cucurull, Casanova, Romero, Liò, and Bengio]{gat}
Veli{\v{c}}ković, P., Cucurull, G., Casanova, A., Romero, A., Liò, P., and Bengio, Y.
\newblock Graph attention networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Xu et~al.(2019)Xu, Hu, Leskovec, and Jegelka]{gin}
Xu, K., Hu, W., Leskovec, J., and Jegelka, S.
\newblock How powerful are graph neural networks?
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=ryGs6iA5Km}.

\bibitem[Yanardag \& Vishwanathan(2015)Yanardag and Vishwanathan]{dgk}
Yanardag, P. and Vishwanathan, S.
\newblock Deep graph kernels.
\newblock In \emph{Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, KDD '15, pp.\  1365–1374, New York, NY, USA, 2015. Association for Computing Machinery.
\newblock ISBN 9781450336642.
\newblock \doi{10.1145/2783258.2783417}.
\newblock URL \url{https://doi.org/10.1145/2783258.2783417}.

\bibitem[Yehudai et~al.(2020)Yehudai, Fetaya, Meirom, Chechik, and Maron]{size_generalization}
Yehudai, G., Fetaya, E., Meirom, E.~A., Chechik, G., and Maron, H.
\newblock On size generalization in graph neural networks.
\newblock \emph{CoRR}, abs/2010.08853, 2020.
\newblock URL \url{https://arxiv.org/abs/2010.08853}.

\bibitem[Zaheer et~al.(2018)Zaheer, Kottur, Ravanbakhsh, Poczos, Salakhutdinov, and Smola]{zaheer2018deep}
Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R., and Smola, A.
\newblock Deep sets, 2018.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and Vinyals]{zhang2017understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization, 2017.

\end{thebibliography}
