\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akrout et~al.(2019)Akrout, Wilson, Humphreys, Lillicrap, and
  Tweed]{akrout2019deep}
Akrout, M., Wilson, C., Humphreys, P., Lillicrap, T., and Tweed, D.~B.
\newblock Deep learning without weight transport.
\newblock 32, 2019.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019exact}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R., and Wang, R.
\newblock On exact computation with an infinitely wide neural net.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Arora et~al.(2020)Arora, Du, Li, Salakhutdinov, Wang, and
  Yu]{arora2020harnessing}
Arora, S., Du, S.~S., Li, Z., Salakhutdinov, R., Wang, R., and Yu, D.
\newblock Harnessing the power of infinitely wide deep nets on small-data
  tasks.
\newblock \emph{ICLR}, 2020.

\bibitem[Bartunov et~al.(2018)Bartunov, Santoro, Richards, Marris, Hinton, and
  Lillicrap]{bartunov2018assessing}
Bartunov, S., Santoro, A., Richards, B.~A., Marris, L., Hinton, G.~E., and
  Lillicrap, T.
\newblock Assessing the scalability of biologically-motivated deep learning
  algorithms and architectures.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Bellec et~al.(2020)Bellec, Scherr, Hajek, Salaj, Legenstein, and
  Maass]{bellec2020solution}
Bellec, G., Scherr, F., Hajek, E., Salaj, D., Legenstein, R., and Maass, W.
\newblock A solution to the learning dilemma for recurrent networks of spiking
  neurons.
\newblock \emph{Nature Communications}, 2020.

\bibitem[Bengio et~al.(2016)Bengio, Lee, Jorg~Bornschein, and
  Lin]{bengio2016stdpvae}
Bengio, Y., Lee, D.-H., Jorg~Bornschein, T.~M., and Lin, Z.
\newblock Towards biologically plausible deep learning.
\newblock \emph{arXiv preprint}, 2016.

\bibitem[Clanuwat et~al.(2018)Clanuwat, Bober{-}Irizar, Kitamoto, Lamb,
  Yamamoto, and Ha]{clanuwat2018kmnist}
Clanuwat, T., Bober{-}Irizar, M., Kitamoto, A., Lamb, A., Yamamoto, K., and Ha,
  D.
\newblock Deep learning for classical japanese literature.
\newblock \emph{NeurIPS Workshop on Machine Learning for Creativity and
  Design}, 2018.

\bibitem[de~G.~Matthews et~al.(2018)de~G.~Matthews, Rowland, Hron, Turner, and
  Ghahramani]{matthews2018gaussian}
de~G.~Matthews, A.~G., Rowland, M., Hron, J., Turner, R.~E., and Ghahramani, Z.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock \emph{ICLR}, 2018.

\bibitem[Fiete \& Seung(2006)Fiete and Seung]{Fiete06}
Fiete, I. and Seung, H.
\newblock Gradient learning in spiking neural networks by dynamic perturbation
  of conductances.
\newblock \emph{Phys Rev Lett}, 97\penalty0 (4):\penalty0 048104, 2006.
\newblock ISSN 0031-9007 (Print).

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018NTK}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: convergence and generalization in neural
  networks.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2015adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{ICLR}, 2015.

\bibitem[Krizhevsky(2009)]{krizhevsky2009cifar}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Launay et~al.(2020)Launay, Poli, Boniface, and
  Krzakala]{launay2020direct}
Launay, J., Poli, I., Boniface, F., and Krzakala, F.
\newblock Direct feedback alignment scales to modern deep learning tasks and
  architectures.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Lee et~al.(2015)Lee, Zhang, Fischer, and Bengio]{lee2015difference}
Lee, D.-H., Zhang, S., Fischer, A., and Bengio, Y.
\newblock Difference target propagation.
\newblock \emph{ECML/PKDD}, 2015.

\bibitem[Lee et~al.(2018)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl-Dickstein]{lee2018deep}
Lee, J., Bahri, Y., Novak, R., Schoenholz, S.~S., Pennington, J., and
  Sohl-Dickstein, J.
\newblock Deep neural networks as gaussian processes.
\newblock \emph{ICLR}, 2018.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019WideNN}
Lee, J., Xiao, L., Schoenholz, S.~S., Bahri, Y., Novak, R., Sohl-Dickstein, J.,
  and Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Lee et~al.(2020)Lee, Schoenholz, Pennington, Adlam, Xiao, Novak, and
  Sohl-Dickstein]{lee2020finite}
Lee, J., Schoenholz, S.~S., Pennington, J., Adlam, B., Xiao, L., Novak, R., and
  Sohl-Dickstein, J.
\newblock Finite versus infinite neural networks: an empirical study.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Liao et~al.(2016)Liao, Leibo, and Poggio]{liao2016important}
Liao, Q., Leibo, J.~Z., and Poggio, T.
\newblock How important is weight symmetry in backpropagation?
\newblock \emph{AAAI}, 2016.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Cownden, Tweed, and
  Akerman]{lillicrap2016random}
Lillicrap, T.~P., Cownden, D., Tweed, D.~B., and Akerman, C.~J.
\newblock Random synaptic feedback weights support error backpropagation for
  deep learning.
\newblock \emph{Nature Communications}, 2016.

\bibitem[M~Colonnier(1981)]{colonnier1981number}
M~Colonnier, J.~O.
\newblock Number of neurons and synapses in the visual cortex of different
  species.
\newblock \emph{Rev Can Biol.}, 1981.

\bibitem[{Maennel} et~al.(2020){Maennel}, {Alabdulmohsin}, {Tolstikhin},
  {Baldock}, {Bousquet}, {Gelly}, and {Keysers}]{maennel2020what}
{Maennel}, H., {Alabdulmohsin}, I., {Tolstikhin}, I., {Baldock}, R. J.~N.,
  {Bousquet}, O., {Gelly}, S., and {Keysers}, D.
\newblock {What do neural networks learn when trained with random labels?}
\newblock \emph{arXiv preprint}, 2020.

\bibitem[Manchev \& Spratling(2020)Manchev and Spratling]{manchev2020target}
Manchev, N. and Spratling, M.
\newblock Target propagation in recurrent neural networks.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (7):\penalty0 1--33, 2020.

\bibitem[Marschall et~al.(2020)Marschall, Cho, and Savin]{marschall2020unified}
Marschall, O., Cho, K., and Savin, C.
\newblock {A unified framework of online learning algorithms for training
  recurrent neural networks}.
\newblock \emph{JMLR}, 2020.

\bibitem[Meulemans et~al.(2020)Meulemans, Carzaniga, Suykens, Sacramento, and
  Grewe]{meulemans2020theoretical}
Meulemans, A., Carzaniga, F.~S., Suykens, J. A.~K., Sacramento, J., and Grewe,
  B.~F.
\newblock A theoretical framework for target propagation.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Millidge et~al.(2020)Millidge, Tschantz, and
  Buckley]{millidge2020predictive}
Millidge, B., Tschantz, A., and Buckley, C.~L.
\newblock Predictive coding approximates backprop along arbitrary computation
  graphs.
\newblock \emph{arXiv preprint}, 2020.

\bibitem[{N{\o}kland} \& {Hiller Eidnes}(2019){N{\o}kland} and {Hiller
  Eidnes}]{nokland2019training}
{N{\o}kland}, A. and {Hiller Eidnes}, L.
\newblock {Training neural networks with local error signals}.
\newblock \emph{ICML}, 2019.

\bibitem[Novak et~al.(2019)Novak, Xiao, Lee, Bahri, Yang, Hron, Abolafia,
  Pennington, and Sohl-Dickstein]{novak2019bayesian}
Novak, R., Xiao, L., Lee, J., Bahri, Y., Yang, G., Hron, J., Abolafia, D.~A.,
  Pennington, J., and Sohl-Dickstein, J.
\newblock Bayesian deep convolutional networks with many channels are gaussian
  processes.
\newblock \emph{ICLR}, 2019.

\bibitem[Nøkland(2016)]{nokland2016direct}
Nøkland, A.
\newblock Direct feedback alignment provides learning in deep neural networks.
\newblock \emph{NeurIPS}, 2016.

\bibitem[Ororbia \& Kifer(2022)Ororbia and Kifer]{ororbia2022neural}
Ororbia, A. and Kifer, D.
\newblock The neural coding framework for learning generative models.
\newblock \emph{arXiv preprint}, 2022.

\bibitem[Ororbia et~al.(2020{\natexlab{a}})Ororbia, Mali, Giles, and
  Kifer]{ororbia2020continual}
Ororbia, A., Mali, A., Giles, C.~L., and Kifer, D.
\newblock Continual learning of recurrent neural networks by locally aligning
  distributed representations.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  31\penalty0 (10):\penalty0 4267--4278, 2020{\natexlab{a}}.
\newblock \doi{10.1109/TNNLS.2019.2953622}.

\bibitem[Ororbia et~al.(2020{\natexlab{b}})Ororbia, Mali, Kifer, and
  Giles]{ororbia2020largescale}
Ororbia, A., Mali, A., Kifer, D., and Giles, C.~L.
\newblock Large-scale gradient-free deep learning with recursive local
  representation alignment.
\newblock \emph{arXiv preprint}, 2020{\natexlab{b}}.

\bibitem[Ororbia \& Mali(2019)Ororbia and Mali]{ororbia2019biologically}
Ororbia, A.~G. and Mali, A.
\newblock Biologically motivated algorithms for propagating local target
  representations.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  33\penalty0 (01):\penalty0 4651--4658, Jul. 2019.
\newblock \doi{10.1609/aaai.v33i01.33014651}.

\bibitem[Ororbia et~al.(2018)Ororbia, Mali, Kifer, and
  Giles]{ororbia2018conducting}
Ororbia, A.~G., Mali, A., Kifer, D., and Giles, C.~L.
\newblock Conducting credit assignment by aligning local representations.
\newblock \emph{arXiv preprint}, 2018.

\bibitem[Refinetti et~al.(2021)Refinetti, d'Ascoli, Ohana, and
  Goldt]{refinetti2021align}
Refinetti, M., d'Ascoli, S., Ohana, R., and Goldt, S.
\newblock Align, then memorise: the dynamics of learning with feedback
  alignment.
\newblock \emph{ICML}, 2021.

\bibitem[Richards et~al.(2019)Richards, Lillicrap, Beaudoin, Bengio, Bogacz,
  Christensen, Clopath, Costa, Berker, Ganguli, Gillon, Hafner, Kepecs,
  Kriegeskorte, Latham, Lindsay, Miller, Naud, Pack, and
  Kording]{richards2019deep}
Richards, B., Lillicrap, T., Beaudoin, P., Bengio, Y., Bogacz, R., Christensen,
  A., Clopath, C., Costa, R., Berker, A., Ganguli, S., Gillon, C., Hafner, D.,
  Kepecs, A., Kriegeskorte, N., Latham, P., Lindsay, G., Miller, K., Naud, R.,
  Pack, C., and Kording, K.
\newblock A deep learning framework for neuroscience.
\newblock \emph{Nature Neuroscience}, 22:\penalty0 1761--1770, 11 2019.
\newblock \doi{10.1038/s41593-019-0520-2}.

\bibitem[Roth et~al.(2019)Roth, Kanitscheider, and Fiete]{Roth19}
Roth, C., Kanitscheider, I., and Fiete, I.
\newblock Kernel rnn learning (kernl).
\newblock 2019.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and
  Fei-Fei]{russakovsky2015Imagenet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., Berg, A.~C., and Fei-Fei, L.
\newblock {ImageNet large scale visual recognition challenge}.
\newblock \emph{IJCV}, 2015.

\bibitem[Saxe et~al.(2019)Saxe, McClelland, and Ganguli]{saxe2019mathematical}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S.
\newblock A mathematical theory of semantic development in deep neural
  networks.
\newblock \emph{PNAS}, 2019.

\bibitem[Seung(2003)]{Seung03}
Seung, H.~S.
\newblock Learning in spiking neural networks by reinforcement of stochastic
  synaptic transmission.
\newblock \emph{Neuron}, 40\penalty0 (6):\penalty0 1063--1073, 2003.
\newblock ISSN 0896-6273 (Print).

\bibitem[Song et~al.(2021)Song, Xu, and Lafferty]{song2021convergence}
Song, G., Xu, R., and Lafferty, J.
\newblock Convergence and alignment of gradient descent with random
  backpropagation weights.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Williams \& Zipser(1989)Williams and Zipser]{williams1989learning}
Williams, R.~J. and Zipser, D.
\newblock A learning algorithm for continually running fully recurrent neural
  networks.
\newblock \emph{Neural Computation}, 1\penalty0 (2):\penalty0 270--280, 1989.
\newblock \doi{10.1162/neco.1989.1.2.270}.

\bibitem[Xiao et~al.(2018)Xiao, Chen, Liao, and
  Poggio]{xiao2018biologicallyplausible}
Xiao, W., Chen, H., Liao, Q., and Poggio, T.
\newblock Biologically-plausible learning algorithms can scale to large
  datasets.
\newblock \emph{arXiv preprint}, 2018.

\bibitem[Yang \& Hu(2020)Yang and Hu]{yang2020feature}
Yang, G. and Hu, E.~J.
\newblock Feature learning in infinite-width neural networks.
\newblock \emph{arXiv preprint}, 2020.

\bibitem[Yosinski et~al.(2014)Yosinski, Clune, Bengio, and
  Lipson]{yosinski2014transferable}
Yosinski, J., Clune, J., Bengio, Y., and Lipson, H.
\newblock How transferable are features in deep neural networks?
\newblock \emph{NeurIPS}, 2014.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{ICLR}, 2017.

\end{thebibliography}
