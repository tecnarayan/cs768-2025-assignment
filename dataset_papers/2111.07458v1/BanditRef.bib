%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Ali Tajer at 2020-06-05 01:47:00 -0400 


%% Saved with string encoding Unicode (UTF-8) 



@phdthesis{Bessler,
	Author = {S. Bessler},
	Date-Added = {2020-05-31 15:22:53 +0000},
	Date-Modified = {2020-06-05 05:40:59 +0000},
	Institution = {Department of Statistics, Stanford University},
	Month = {March},
	Title = {Theory and applications of the sequential design of experiments, $k$-actions and infinitely many experiments: Part {I} Theory},
	Year = {1960}}

@article{Wald1945,
	Ajournal = {Ann. Math. Statist.},
	Author = {Wald, A.},
	Date-Added = {2020-05-31 15:16:53 +0000},
	Date-Modified = {2020-06-05 05:47:00 +0000},
	Journal = {The Annals of Mathematical Statistics},
	Month = {June},
	Number = {2},
	Pages = {117--186},
	Publisher = {The Institute of Mathematical Statistics},
	Title = {Sequential Tests of Statistical Hypotheses},
	Volume = {16},
	Year = {1945}}
	

@article{Atia,
	Author = {Nitinawarat, S. and Atia, G. K. and Veeravalli, V. V.},
	Date-Added = {2020-05-31 15:14:22 +0000},
	Date-Modified = {2020-05-31 15:14:22 +0000},
	Journal = {IEEE Transactions on Automatic Control},
	Month = {May},
	Number = {10},
	Pages = {2451-2464},
	Title = {Controlled Sensing for Multihypothesis Testing},
	Volume = {58},
	Year = {2013}}

@article{Chernoff1959,
	Author = {Chernoff, H.},
	Date-Added = {2020-05-31 15:12:37 +0000},
	Date-Modified = {2020-06-05 05:42:07 +0000},
	Journal = {The Annals of Mathematical Statistics},
	Month = {September},
	Number = {3},
	Pages = {755--770},
	Publisher = {The Institute of Mathematical Statistics},
	Title = {Sequential Design of Experiments},
	Volume = {30},
	Year = {1959}}

@article{Heydari:TSP2019,
	Author = {J. Heydari and A. Tajer},
	Date-Added = {2020-05-31 15:12:37 +0000},
	Date-Modified = {2020-05-31 15:12:37 +0000},
	Journal = {IEEE Transactions on Signal Processing},
	Month = {February},
	Number = {3},
	Pages = {638-651},
	Title = {Quickest Search and Learning Over Correlated Sequences: Theory and Application},
	Volume = {67},
	Year = {2019}}

@inproceedings{NIPS2019_9251,
	Address = {Vancouver, Canada},
	Author = {Fiez, T. and Jain, L. and Jamieson, K. and Ratliff, L.},
	Booktitle = {Advances in Neural Information Processing Systems},
	Month = {December},
	Title = {Sequential Experimental Design for Transductive Linear Bandits},
	Year = {2019}}
	

@book{Shiryaev:1978,
	Address = {New York, NY},
	Author = {A. N. Shiryaev},
	Date-Added = {2020-05-31 15:12:37 +0000},
	Date-Modified = {2020-05-31 15:12:37 +0000},
	Publisher = {Springer,},
	Title = {Optimal Stopping Rules},
	Year = {1978}}

@inproceedings{LinGapE,
	Abstract = {We propose the first fully-adaptive algorithm for pure exploration in linear bandits---the task to find the arm with the largest expected reward, which depends on an unknown parameter linearly. While existing methods partially or entirely fix sequences of arm selections before observing rewards,  our method adaptively changes the arm selection strategy based on past observations at each round. We show our sample complexity matches the achievable lower bound up to a constant factor in an extreme case. Furthermore, we evaluate the performance of the methods by simulations based on both synthetic setting and real-world data, in which our method shows vast improvement over existing ones.},
	Author = {L. Xu and J. Honda and M. Sugiyama},
	Booktitle = {Proc. International Conference on Artificial Intelligence and Statistics},
	Title = {A fully adaptive algorithm for pure exploration in linear bandits},
	Address = {Lanzarote, Canary Islands},
	Month = {April},
	Year = {2018}}

@inproceedings{RAGE,
	Address = {Vancouver, Canada},
	Author = {Fiez, T. and Jain, L. and Jamieson, K. and Ratliff, L.},
	Booktitle = {Advances in Neural Information Processing Systems},
	month = {November},
	Title = {Sequential Experimental Design for Transductive Linear Bandits},
	Year = {2019}}

@inproceedings{Bubeck,
    Address = {Porto, Portugal},
	Author = {Bubeck, S. and Munos, R. and Stoltz, G.},
	Booktitle = {Proc. International Conference on Algorithmic Learning Theory},
	Date-Modified = {2020-06-05 05:41:31 +0000},
	Title = {Pure Exploration in Multi-Armed Bandits Problems},
	Year = {2009},
	Month = {October}}

@inproceedings{Soare,
	Author = {Soare, M. and Lazaric, A. and Munos, R.},
	Booktitle = {Proc. International Conference on Neural Information Processing Systems},
	Date-Modified = {2020-06-05 05:46:15 +0000},
	Address = {Montreal, Canada},
	Title = {Best-Arm Identification in Linear Bandits},
	Month = {December},
	Year = {2014}}

@inproceedings{Tao,
	Abstract = {We study the best arm identification problem in linear bandits, where the mean reward of each arm depends linearly on an unknown $d$-dimensional parameter vector $\theta$, and the goal is to identify the arm with the largest expected reward. We first design and analyze a novel randomized $\theta$ estimator based on the solution to the convex relaxation of an optimal $G$-allocation experiment design problem. Using this estimator, we describe an algorithm whose sample complexity depends linearly on the dimension $d$, as well as an algorithm with sample complexity dependent on the reward gaps of the best $d$ arms, matching the lower bound arising from the ordinary top-arm identification problem. We finally compare the empirical performance of our algorithms with other state-of-the-art algorithms in terms of both sample complexity and computational time.},
	Address = {Stockholmsm{\"a}ssan, Stockholm Sweden},
	Author = {Tao, Chao and Blanco, Sa{\'u}l and Zhou, Yuan},
	Booktitle = {International Conference on Machine Learning},
	Date-Modified = {2020-06-05 05:46:53 +0000},
	Month = {July},
	Pages = {4877--4886},
	Title = {Best Arm Identification in Linear Bandits with Linear Dimension Dependency},
	Year = {2018}}

@inproceedings{Bickel,
	Address = {Berkeley, CA},
	Author = {Bickel, Peter J. and Yahav, Joseph A.},
	Booktitle = {Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Statistics},
	Date-Modified = {2020-06-05 05:41:13 +0000},
	Pages = {401--413},
	Publisher = {University of California Press},
	Title = {Asymptotically pointwise optimal procedures in sequential analysis},
	Year = {1967},
	}


@inproceedings{Abbasi,
	Author = {Yasin Abbasi-yadkori and P\'{a}l, D\'{a}vid and Csaba Szepesv\'{a}ri},
	Booktitle = {Advances in Neural Information Processing Systems},
	Date-Modified = {2020-06-05 05:39:38 +0000},
	Editor = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
	Pages = {2312--2320},
	Title = {Improved Algorithms for Linear Stochastic Bandits},
	Year = {2011},
	}

@inproceedings{Kalyanakrishnan2012,
	Address = {Madison, WI},
	Author = {Kalyanakrishnan, S. and Tewari, A. and Auer, P. and Stone, P.},
	Booktitle = {Proc. International Conference on Machine Learning},
	Date-Modified = {2020-06-05 05:44:00 +0000},
	Month = {June},
	Title = {{PAC} Subset Selection in Stochastic Multi-Armed Bandits},
	Year = {2012}}

@inproceedings{Gabillon,
    Address = {Lake Tahoe, NV},
	Author = {Gabillon, V. and Ghavamzadeh, M. and Lazaric, A.},
	Booktitle = {Proc. Advances in Neural Information Processing Systems},
	Date-Modified = {2020-06-05 05:43:06 +0000},
	Title = {Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence},
	Month = {December},
	Year = {2012},
	}

@article{Kalyanakrishnan2013,
	Author = {Kaufmann, E. and Kalyanakrishnan, S.},
	Journal = {Journal of Machine Learning Research},
	Month = {January},
	pages = {228-251},
	Title = {Information complexity in bandit subset selection},
	volume = {30},
	Year = {2013}}

@inproceedings{Jamieson2014,
	Abstract = {The paper proposes a novel upper confidence bound (UCB) procedure for identifying the arm with the largest mean in a multi-armed bandit game in the fixed confidence setting using a small number of total samples.  The procedure cannot be improved in the sense that the number of samples required to identify the best arm is within a constant factor of a lower bound based on the law of the iterated logarithm (LIL). Inspired by the LIL, we construct our confidence bounds to explicitly account for the infinite time horizon of the algorithm. In addition, by using a novel stopping time for the algorithm we avoid a union bound over the arms that has been observed in other UCB-type algorithms. We prove that the algorithm is optimal up to constants and also show through simulations that it provides superior performance with respect to the state-of-the-art. },
	Author = {K. Jamieson and M. Malloy and R. Nowak and S. Bubeck},
	Booktitle = {Proc. Conference on Learning Theory},
	Date-Modified = {2020-06-05 05:43:35 +0000},
	Month = {June},
	Title = {lil' UCB : An Optimal Exploration Algorithm for Multi-Armed Bandits},
	Year = {2014}}

@article{Maron,
	Address = {Norwell, MA, USA},
	Author = {Maron, O. and Moore, A. W.},
	Date-Modified = {2020-06-05 05:44:54 +0000},
	Journal = {Artificial Intelligence Review},
	Number = {1--5},
	Pages = {193--225},
	Title = {The Racing Algorithm: Model Selection for Lazy Learners},
	Volume = {11},
	Year = {1997},
	Month = {February}
	}

@article{Even-Dar,
	Author = {Even-Dar, E. and Mannor, S. and Mansour, Y.},
	Date-Modified = {2020-06-05 05:42:41 +0000},
	Journal = {Journal of Machine Learning Research},
	Month = {December},
	Pages = {1079--1105},
	Title = {Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems},
	Volume = {7},
	Year = {2006}}

@inproceedings{Audibert,
    Address = {Haifa, Israel},
	Author = {Audibert, J. Y. and Bubeck, S. and Munos, R.},
	Booktitle = {Proc. Conference on Learning Theory},
	Date-Modified = {2020-06-05 05:40:01 +0000},
	Month = {November},
	Title = {Best Arm Identification in Multi-Armed Bandits},
	Year = {2010}}

@inproceedings{Chen2017b,
	Abstract = { In the classical best arm identification (Best-$1$-Arm) problem, we are given $n$ stochastic bandit arms, each associated with a reward distribution with an unknown mean. Upon each play of an arm, we can get a reward sampled i.i.d. from its reward distribution. We would like to identify the arm with the largest mean with probability at least $1-δ$, using as few samples as possible. The problem has a long history and understanding its sample complexity has attracted significant attention since the last decade. However, the optimal sample complexity of the problem is still unknown. Recently, Chen and Li (2016) made an interesting conjecture, called gap-entropy conjecture, concerning the instance optimal sample complexity of Best-$1$-Arm. Given a Best-$1$-Arm instance $I$ (i.e., a set of arms), let $\mu_[i]$ denote the $i$th largest mean and $\Delta_[i]=\mu_[1]-\mu_[i]$ denote the corresponding gap. $H(I)=\sum_i=2^n\Delta_[i]^-2$ denotes the complexity of the instance. The gap-entropy conjecture states that for any instance $I$, $Ω\left(H(I)⋅\left(\lnδ^-1 + \mathsf{Ent}(I)\right)\right)$ is an instance lower bound, where $\mathsf{Ent}(I)$ is an entropy-like term determined by the gaps, and there is a $δ$-correct algorithm for Best-$1$-Arm with sample complexity $O\left(H(I)⋅\left(\lnδ^-1 + \mathsf{Ent}(I)\right)+\Delta_[2]^-2\ln\ln\Delta_[2]^-1\right)$. We note that $Θ\left(\Delta_[2]^-2\ln\ln\Delta_[2]^-1\right)$ is necessary and sufficient to solve the two-arm instance with the best and second best arms. If the conjecture is true, we would have a complete understanding of the instance-wise sample complexity of Best-$1$-Arm (up to constant factors). In this paper, we make significant progress towards a complete resolution of the gap-entropy conjecture. For the upper bound, we provide a highly nontrivial algorithm which requires \[O\left(H(I)⋅\left(\lnδ^-1 + \mathsf{Ent}(I)\right)+\Delta_[2]^-2\ln\ln\Delta_[2]^-1\mathrmpolylog(n,δ^-1)\right)\]samples in expectation for any instance $I$. For the lower bound, we show that for any Gaussian Best-$1$-Arm instance with gaps of the form $2^-k$, any $δ$-correct monotone algorithm requires at least \[Ω\left(H(I)⋅\left(\lnδ^-1 + \mathsf{Ent}(I)\right)\right)\]samples in expectation. Here, a monotone algorithm is one which uses no more samples (in expectation) on $I'$ than on $I$, if $I'$ is a sub-instance of $I$ obtained by removing some sub-optimal arms. },
	Address = {Amsterdam, Netherlands},
	Author = {L. Chen and J. Li and M. Qiao},
	Booktitle = {Proc. Conference on Learning Theory},
	Date-Modified = {2020-06-05 05:41:59 +0000},
	Month = {July},
	Title = {Towards Instance Optimal Bounds for Best Arm Identification},
	Year = {2017}}

@article{Bechhofer:1954,
	Author = {Bechhofer, Robert E. and Sobel, Milton},
	Date-Modified = {2020-06-05 05:40:31 +0000},
	Journal = {Ann. Math. Statist.},
	Month = {06},
	Number = {2},
	Pages = {273--289},
	Title = {A Single-Sample Multiple Decision Procedure for Ranking Variances of Normal Populations},
	Volume = {25},
	Year = {1954}}

@book{bandit,
	Address = {Cambridge, UK},
	Author = {Lattimore, Tor and Szepesv{\'a}ri, Csaba},
	Date-Modified = {2020-06-05 05:40:16 +0000},
	Place = {Cambridge},
	Publisher = {Cambridge University Press},
	Title = {Bandit Algorithms},
	Year = {2020}}

@article{nguyen2019bayesian,
	Archiveprefix = {arXiv},
	Author = {Dang Nguyen and Sunil Gupta and Santu Rana and Alistair Shilton and Svetha Venkatesh},
	Date-Modified = {2020-06-05 05:44:37 +0000},
	Eprint = {1911.12473},
	Primaryclass = {cs.LG},
	Title = {Bayesian Optimization for Categorical and Category-Specific Continuous Inputs},
	Year = {2019}}
	
@article{paulson,
author = "Paulson, E.",
fjournal = "Annals of Mathematical Statistics",
journal = "Annals of Mathematical Statistics",
month = "March",
number = "1",
pages = "174--180",
publisher = "The Institute of Mathematical Statistics",
title = "A Sequential Procedure for Selecting the Population with the Largest Mean from $k$ Normal Populations",
volume = "35",
year = "1964"
}

@article{bechhofer,
author = {Robert E. Bechhofer},
journal = {Biometrics},
number = {3},
pages = {408--429},
publisher = {[Wiley, International Biometric Society]},
title = {A Sequential Multiple-Decision Procedure for Selecting the Best One of Several Normal Populations with a Common Unknown Variance, and Its Use with Various Experimental Designs},
volume = {14},
year = {1958}
}

@article{Huber,
	author = {P. J. Huber},
	title = {{Robust Estimation of a Location Parameter}},
	volume = {35},
	journal = {The Annals of Mathematical Statistics},
	number = {1},
	publisher = {Institute of Mathematical Statistics},
	pages = {73 -- 101},
	year = {1964}
}

@article{CBAI,
	author  = {J. Altschuler and V. E. Brunel and A. Malek},
	title   = {Best Arm Identification for Contaminated Bandits},
	journal = {Journal of Machine Learning Research},
	year    = {2019},
	volume  = {20},
	number  = {91},
	pages   = {1-39}
}
@article{JMLR:v7:evendar06a,
	author  = {E. Even-Dar and S. Mannor and Y. Mansour},
	title   = {Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems},
	journal = {Journal of Machine Learning Research},
	year    = {2006},
	volume  = {7},
	number  = {39},
	pages   = {1079-1105}
}

@InProceedings{pmlr-v49-garivier16a, title = {Optimal Best Arm Identification with Fixed Confidence}, author = {A. Garivier and E. Kaufmann}, booktitle = {Proc. Conference on Learning Theory}, year = {2016}, address = {New York, NY}, month = {June}, abstract = {We give a complete characterization of the complexity of best-arm identification in one-parameter bandit problems. We prove a new, tight lower bound on the sample complexity. We propose the ‘Track-and-Stop’ strategy, which we prove to be asymptotically optimal. It consists in a new sampling rule (which tracks the optimal proportions of arm draws highlighted by the lower bound) and in a stopping rule named after Chernoff, for which we give a new analysis.} }

@inproceedings{resilience,
	author        = {J. Steinhardt},
	title         = {{L}ecture {N}otes for {STAT240} ({R}obust and {N}onparametric {S}tatistics)},
	month         = {April},
	year          = {2021},
	publisher={University of California, Barkeley}
}

@article{LaiRobins,
	author = {Lai, T. L. and Robbins, H.},
	title = {Asymptotically Efficient Adaptive Allocation Rules},
	year = {1985},
	issue_date = {Mar., 1985},
	publisher = {Academic Press, Inc.},
	address = {USA},
	volume = {6},
	number = {1},
	journal = {Advanced Applied Mathematics},
	month = mar,
	pages = {4–22},
	numpages = {19}
}

@inproceedings{Lykouris,
	author = {Lykouris, T. and Mirrokni, V. and P. Leme, R.},
	title = {Stochastic Bandits Robust to Adversarial Corruptions},
	year = {2018},
	abstract = {We introduce a new model of stochastic bandits with adversarial corruptions which aims to capture settings where most of the input follows a stochastic pattern but some fraction of it can be adversarially changed to trick the algorithm, e.g., click fraud, fake reviews and email spam. The goal of this model is to encourage the design of bandit algorithms that (i) work well in mixed adversarial and stochastic models, and (ii) whose performance deteriorates gracefully as we move from fully stochastic to fully adversarial models. In our model, the rewards for all arms are initially drawn from a distribution and are then altered by an adaptive adversary. We provide a simple algorithm whose performance gracefully degrades with the total corruption the adversary injected in the data, measured by the sum across rounds of the biggest alteration the adversary made in the data in that round; this total corruption is denoted by C. Our algorithm provides a guarantee that retains the optimal guarantee (up to a logarithmic term) if the input is stochastic and whose performance degrades linearly to the amount of corruption C, while crucially being agnostic to it. We also provide a lower bound showing that this linear degradation is necessary if the algorithm achieves optimal performance in the stochastic setting (the lower bound works even for a known amount of corruption, a special case in which our algorithm achieves optimal performance without the extra logarithm).},
	booktitle = {Proc. ACM SIGACT Symposium on Theory of Computing},
	keywords = {bandits, online learning},
	Address = {Los Angeles, CA}
}

@article{zhong2021probabilistic,
	title={Probabilistic Sequential Shrinking: A Best Arm Identification Algorithm for Stochastic Bandits with Corruptions}, 
	author={Z. Zhong and W. C. Cheung and V. Y. F. Tan},
	year={2021},
	journal = {arXiv 2010.07904}
}

@InProceedings{gupta19a, title = {Better Algorithms for Stochastic Bandits with Adversarial Corruptions}, author = {Gupta, A. and Koren, T. and Talwar, K.}, booktitle = {Proc. Conference on Learning Theory}, year = {2019}, address = {Phoenix, AZ}, month = {June},   
abstract = {We study the stochastic multi-armed bandits problem in the presence of adversarial corruption. We present a new algorithm for this problem whose regret is nearly optimal, substantially improving upon previous work. Our algorithm is agnostic to the level of adversarial contamination and can tolerate a significant amount of corruption with virtually no degradation in performance.} }



@InProceedings{zimmert19a, title = {An Optimal Algorithm for Stochastic and Adversarial Bandits}, author = {Zimmert, J. and Seldin, Y.}, booktitle = {Proc. International Conference on Artificial Intelligence and Statistics}, Address = {Okinawa, Japan}, year = {2019}, month = {April}, abstract = {We derive an algorithm that achieves the optimal (up to constants) pseudo-regret in both adversarial and stochastic multi-armed bandits without prior knowledge of the regime and time horizon. The algorithm is based on online mirror descent with Tsallis entropy regularizer. We provide a complete characterization of such algorithms and show that Tsallis entropy with power $\alpha = 1/2$ achieves the goal. In addition, the proposed algorithm enjoys improved regret guarantees in two intermediate regimes: the moderately contaminated stochastic regime defined by Seldin and Slivkins [22] and the stochastically constrained adversary studied by Wei and Luo [26]. The algorithm also obtains adversarial and stochastic optimality in the utility-based dueling bandit setting. We provide empirical evaluation of the algorithm demonstrating that it outperforms Ucb1 and Exp3 in stochastic environments. In certain adversarial regimes the algorithm significantly outperforms Ucb1 and Thompson Sampling, which exhibit close to linear regret.} }


@inproceedings{regretadv1,
author = {Krishnamurthy, A. and Lykouris, T. and Podimata, C. and Schapire, R.},
title = {Contextual Search in the Presence of Irrational Agents},
year = {2021},
address = {New York, NY},
booktitle = {Proc. Annual ACM SIGACT Symposium on Theory of Computing}
}



@article{regretadv2,
	title={Contextual Search in the Presence of Irrational Agents}, 
	author={A. Krishnamurthy and T. Lykouris and C. Podimata and R. Schapire},
	journal = {arXiv 2002.11650},
	year={2020},
}


@article{diakonikolas2019recent,
	title={Recent Advances in Algorithmic High-Dimensional Robust Statistics}, 
	author={I. Diakonikolas and D. M. Kane},
	year={2019},
	journal = {arXiv 1911.05911}
}

@InProceedings{niss20a, title = { What You See May Not Be What You Get: {UCB} Bandit Algorithms Robust to $\varepsilon$-Contamination}, author = {Niss, Laura and Tewari, Ambuj}, booktitle = {Proc. Conference on Uncertainty in Artificial Intelligence}, year = {2020}, month = {August}, address = {Toronto, Canada}} 


@article{pkis2,
	title = {Progress towards a public chemogenomic set for protein kinases and a call for contributions.},
	journal = {PloS one},
	author={D. H. Drewry and C. I. Wells and D. M. Andrews and R. A. and H. Al-Ali and A. D.
	Axtman and S. J. Capuzzi and J. M. Elkins and P. Ettmayer and M. Frederiksen
},
	year = {2017},
	month = {August},
    Volume = {12(8)}
}

@inproceedings{mason2020finding,
  author    = {B. Mason and
               L. Jain and
               A. Tripathy and
               R. Nowak},
  title     = {Finding All {\textdollar}{\textbackslash}epsilon{\textdollar}-Good
               Arms in Stochastic Bandits},
  booktitle = {Proc. Advances in Neural Information Processing Systems},
  year      = {2020},
  month = {December},
  address = {Vancouver, Canada}
}


@article{clinical,
title = {Contamination in trials of educational interventions},
Author = {M. R. Keogh-Brown and M. O. Bachmann and L. Shepstone and C. Hewitt and A. Howe and C. R. Ramsay and F. Song and J. N. Miles and D. J. Torgerson and S. Miles and D. Elbourne and I. Harvey and M. J. Campbell},
journal = {Health Technol Assess},
year = {2007},
month = {October},
Volume = {11(43)}
}

@article{rangi2021secureucb,
      title={Secure-{UCB}: Saving Stochastic Bandits from Poisoning Attacks via Limited Data Verification}, 
      author={A. Rangi and L. Tran-Thanh and H. Xu and M. Franceschetti},
      year={2021},
      journal = {arXiv 2102.07711}
}