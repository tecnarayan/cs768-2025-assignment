\begin{thebibliography}{36}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

\bibitem[{Allen{-}Zhu and Li(2019)}]{allenzhu.kernel}
\textsc{Allen{-}Zhu, Z.} and \textsc{Li, Y.} (2019).
\newblock What can resnet learn efficiently, going beyond kernels?
\newblock In \textit{Conference on Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Allen-Zhu et~al.(2019)Allen-Zhu, Li and Liang}]{allenzhu.3layer}
\textsc{Allen-Zhu, Z.}, \textsc{Li, Y.} and \textsc{Liang, Y.} (2019).
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In \textit{Conference on Neural Information Processing Systems 32}.

\bibitem[{Auer et~al.(1995)Auer, Herbster and Warmuth}]{auer1995}
\textsc{Auer, P.}, \textsc{Herbster, M.} and \textsc{Warmuth, M.~K.} (1995).
\newblock Exponentially many local minima for single neurons.
\newblock In \textit{Conference on Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Awasthi et~al.(2017)Awasthi, Balcan and Long}]{awasthi}
\textsc{Awasthi, P.}, \textsc{Balcan, M.} and \textsc{Long, P.~M.} (2017).
\newblock The power of localization for efficiently learning linear separators
  with noise.
\newblock \textit{J. {ACM}} \textbf{63} 50:1--50:27.

\bibitem[{Beygelzimer et~al.(2011)Beygelzimer, Langford, Li, Reyzin and
  Schapire}]{beygelzimer}
\textsc{Beygelzimer, A.}, \textsc{Langford, J.}, \textsc{Li, L.},
  \textsc{Reyzin, L.} and \textsc{Schapire, R.~E.} (2011).
\newblock Contextual bandit algorithms with supervised learning guarantees.
\newblock In \textit{Conference on Artificial Intelligence and Statistics
  (AISTATS)}.

\bibitem[{{Brady} et~al.(1989){Brady}, {Raghavan} and {Slawny}}]{brady1989}
\textsc{{Brady}, M.~L.}, \textsc{{Raghavan}, R.} and \textsc{{Slawny}, J.}
  (1989).
\newblock Back propagation fails to separate where perceptrons succeed.
\newblock \textit{IEEE Transactions on Circuits and Systems} \textbf{36}
  665--674.

\bibitem[{Cao and Gu(2019)}]{cao2019cnn}
\textsc{Cao, Y.} and \textsc{Gu, Q.} (2019).
\newblock Tight sample complexity of learning one-hidden-layer convolutional
  neural networks.
\newblock In \textit{Conference on Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Diakonikolas et~al.(2020{\natexlab{a}})Diakonikolas, Goel, Karmalkar,
  Klivans and Soltanolkotabi}]{diakonikolas2020approximation}
\textsc{Diakonikolas, I.}, \textsc{Goel, S.}, \textsc{Karmalkar, S.},
  \textsc{Klivans, A.~R.} and \textsc{Soltanolkotabi, M.} (2020{\natexlab{a}}).
\newblock Approximation schemes for relu regression.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Diakonikolas et~al.(2020{\natexlab{b}})Diakonikolas, Kontonis, Tzamos
  and Zarifis}]{diakonikolas2020}
\textsc{Diakonikolas, I.}, \textsc{Kontonis, V.}, \textsc{Tzamos, C.} and
  \textsc{Zarifis, N.} (2020{\natexlab{b}}).
\newblock Learning halfspaces with massart noise under structured
  distributions.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Du et~al.(2018)Du, Lee and Tian}]{du2017}
\textsc{Du, S.~S.}, \textsc{Lee, J.~D.} and \textsc{Tian, Y.} (2018).
\newblock When is a convolutional filter easy to learn?
\newblock In \textit{International Conference on Learning Representations
  (ICLR)}.

\bibitem[{Foster et~al.(2018)Foster, Sekhari and Sridharan}]{foster2018}
\textsc{Foster, D.~J.}, \textsc{Sekhari, A.} and \textsc{Sridharan, K.} (2018).
\newblock Uniform convergence of gradients for non-convex learning and
  optimization.
\newblock In \textit{Conference on Neural Information Processing Systems}.

\bibitem[{Goel et~al.(2020)Goel, Gollakota and Klivans}]{goel2020sqlowerbounds}
\textsc{Goel, S.}, \textsc{Gollakota, A.} and \textsc{Klivans, A.} (2020).
\newblock Statistical-query lower bounds via functional gradients.
\newblock \textit{arXiv preprint arXiv:2006.15812} .

\bibitem[{Goel et~al.(2017)Goel, Kanade, Klivans and Thaler}]{goel2017relupoly}
\textsc{Goel, S.}, \textsc{Kanade, V.}, \textsc{Klivans, A.} and
  \textsc{Thaler, J.} (2017).
\newblock Reliably learning the relu in polynomial time.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Goel et~al.(2019)Goel, Karmalkar and Klivans}]{goel2019relugaussian}
\textsc{Goel, S.}, \textsc{Karmalkar, S.} and \textsc{Klivans, A.~R.} (2019).
\newblock Time/accuracy tradeoffs for learning a relu with respect to gaussian
  marginals.
\newblock In \textit{Conference on Neural Information Processing Systems 32}.

\bibitem[{Goel et~al.(2018)Goel, Klivans and Meka}]{goel.convotron}
\textsc{Goel, S.}, \textsc{Klivans, A.~R.} and \textsc{Meka, R.} (2018).
\newblock Learning one convolutional layer with overlapping patches.
\newblock In \textit{International Conference on Machine Learning} (J.~G. Dy
  and A.~Krause, eds.).

\bibitem[{Helmbold et~al.(1995)Helmbold, Kivinen and
  Warmuth}]{helmbold95worstcase}
\textsc{Helmbold, D.~P.}, \textsc{Kivinen, J.} and \textsc{Warmuth, M.~K.}
  (1995).
\newblock Worst-case loss bounds for single neurons.
\newblock In \textit{Conference on Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Helmbold et~al.(1999)Helmbold, Kivinen and
  Warmuth}]{helmbold99relativeloss}
\textsc{Helmbold, D.~P.}, \textsc{Kivinen, J.} and \textsc{Warmuth, M.~K.}
  (1999).
\newblock Relative loss bounds for single neurons.
\newblock \textit{IEEE Transactions on Neural Networks} .

\bibitem[{Ji and Telgarsky(2019)}]{jitelgarsky}
\textsc{Ji, Z.} and \textsc{Telgarsky, M.} (2019).
\newblock Polylogarithmic width suffices for gradient descent to achieve
  arbitrarily small test error with shallow relu networks.
\newblock \textit{arXiv} \textbf{abs/1909.12292}.

\bibitem[{Jin et~al.(2019)Jin, Netrapalli, Ge, Kakade and
  Jordan}]{jin20.normsubgaussian}
\textsc{Jin, C.}, \textsc{Netrapalli, P.}, \textsc{Ge, R.}, \textsc{Kakade,
  S.~M.} and \textsc{Jordan, M.~I.} (2019).
\newblock A short note on concentration inequalities for random vectors with
  subgaussian norm.
\newblock \textit{arXiv} \textbf{abs/1902.03736}.

\bibitem[{Kakade et~al.(2011)Kakade, Kalai, Kanade and Shamir}]{kakade2011}
\textsc{Kakade, S.~M.}, \textsc{Kalai, A.}, \textsc{Kanade, V.} and
  \textsc{Shamir, O.} (2011).
\newblock Efficient learning of generalized linear and single index models with
  isotonic regression.
\newblock In \textit{Conference on Neural Information Processing Systems}.

\bibitem[{Kalai and Sastry(2009)}]{kalai2009isotron}
\textsc{Kalai, A.~T.} and \textsc{Sastry, R.} (2009).
\newblock The isotron algorithm: High-dimensional isotonic regression.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Kearns and Schapire(1994)}]{kearns.probabilistic}
\textsc{Kearns, M.~J.} and \textsc{Schapire, R.~E.} (1994).
\newblock Efficient distribution-free learning of probabilistic concepts.
\newblock \textit{Journal of Computer and System Sciences} \textbf{48} 464 --
  497.

\bibitem[{Kearns et~al.(1994)Kearns, Schapire and Sellie}]{kearns.agnostic}
\textsc{Kearns, M.~J.}, \textsc{Schapire, R.~E.} and \textsc{Sellie, L.~M.}
  (1994).
\newblock Toward efficient agnostic learning.
\newblock \textit{Machine Learning} \textbf{17} 115--141.

\bibitem[{Lov\'{a}sz and Vempala(2007)}]{lovasz}
\textsc{Lov\'{a}sz, L.} and \textsc{Vempala, S.} (2007).
\newblock The geometry of logconcave functions and sampling algorithms.
\newblock \textit{Random Struct. Algorithms} \textbf{30} 307â€“358.

\bibitem[{Mei et~al.(2018)Mei, Bai and Montanari}]{mei2016landscape}
\textsc{Mei, S.}, \textsc{Bai, Y.} and \textsc{Montanari, A.} (2018).
\newblock The landscape of empirical risk for nonconvex losses.
\newblock \textit{Ann. Statist.} \textbf{46} 2747--2774.

\bibitem[{Mukherjee and Muthukumar(2020)}]{mukherjee}
\textsc{Mukherjee, A.} and \textsc{Muthukumar, R.} (2020).
\newblock A study of neural training with non-gradient and noise assisted
  gradient methods.
\newblock \textit{arXiv} \textbf{abs/2005.04211}.

\bibitem[{Pinelis and Sakhanenko(1986)}]{pinelis1986}
\textsc{Pinelis, I.~F.} and \textsc{Sakhanenko, A.~I.} (1986).
\newblock Remarks on inequalities for large deviation probabilities.
\newblock \textit{Theory of Probability \& Its Applications} \textbf{30}
  143--148.

\bibitem[{Shalev-Shwartz and Ben-David(2014)}]{shalevschwartz}
\textsc{Shalev-Shwartz, S.} and \textsc{Ben-David, S.} (2014).
\newblock \textit{Understanding Machine Learning: From Theory to Algorithms}.
\newblock Cambridge University Press, New York, NY, USA.

\bibitem[{Shamir(2015)}]{shamir15}
\textsc{Shamir, O.} (2015).
\newblock The sample complexity of learning linear predictors with the squared
  loss.
\newblock \textit{Journal of Machine Learning Research} \textbf{16} 3475--3486.

\bibitem[{Soltanolkotabi(2017)}]{soltanolkotabi2017relus}
\textsc{Soltanolkotabi, M.} (2017).
\newblock Learning relus via gradient descent.
\newblock In \textit{Conference on Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Soltanolkotabi et~al.(2019)Soltanolkotabi, Javanmard and
  Lee}]{soltanolkotabi2019theoretical}
\textsc{Soltanolkotabi, M.}, \textsc{Javanmard, A.} and \textsc{Lee, J.~D.}
  (2019).
\newblock Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks.
\newblock \textit{{IEEE} Trans. Inf. Theory} \textbf{65} 742--769.

\bibitem[{Srebro et~al.(2010)Srebro, Sridharan and Tewari}]{srebro.mirror}
\textsc{Srebro, N.}, \textsc{Sridharan, K.} and \textsc{Tewari, A.} (2010).
\newblock Smoothness, low noise and fast rates.
\newblock In \textit{Conference on Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Tian(2017)}]{tian2017relu}
\textsc{Tian, Y.} (2017).
\newblock Symmetry-breaking convergence analysis of certain two-layered neural
  networks with relu nonlinearity.
\newblock In \textit{International Conference on Learning Representations
  (ICLR)}.

\bibitem[{Vempala and Wilmes(2019)}]{vempala}
\textsc{Vempala, S.~S.} and \textsc{Wilmes, J.} (2019).
\newblock Gradient descent for one-hidden-layer neural networks: Polynomial
  convergence and {SQ} lower bounds.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Yehudai and Shamir(2020)}]{yehudai20}
\textsc{Yehudai, G.} and \textsc{Shamir, O.} (2020).
\newblock Learning a single neuron with gradient methods.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Zhang et~al.(2019)Zhang, Yu, Wang and Gu}]{zhanggu2019}
\textsc{Zhang, X.}, \textsc{Yu, Y.}, \textsc{Wang, L.} and \textsc{Gu, Q.}
  (2019).
\newblock Learning one-hidden-layer relu networks via gradient descent.
\newblock In \textit{Conference on Artificial Intelligence and Statistics
  (AISTATS)}.

\end{thebibliography}
