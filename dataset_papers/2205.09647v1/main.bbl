\begin{thebibliography}{}

\bibitem[Agarwal and Hazan, 2018]{agarwal2018lower}
Agarwal, N. and Hazan, E. (2018).
\newblock Lower bounds for higher-order convex optimization.
\newblock In {\em Conference On Learning Theory}, pages 774--792. PMLR.

\bibitem[Arjevani et~al., 2019]{arjevani2019oracle}
Arjevani, Y., Shamir, O., and Shiff, R. (2019).
\newblock Oracle complexity of second-order methods for smooth convex
  optimization.
\newblock {\em Mathematical Programming}, 178(1):327--360.

\bibitem[Bubeck et~al., 2019]{bubeck2019near}
Bubeck, S., Jiang, Q., Lee, Y.~T., Li, Y., and Sidford, A. (2019).
\newblock Near-optimal method for highly smooth convex optimization.
\newblock In {\em Conference on Learning Theory}, pages 492--507. PMLR.

\bibitem[Gasnikov et~al., 2019a]{gasnikov2019optimal}
Gasnikov, A., Dvurechensky, P., Gorbunov, E., Vorontsova, E., Selikhanovych,
  D., and Uribe, C.~A. (2019a).
\newblock Optimal tensor methods in smooth convex and uniformly
  convexoptimization.
\newblock In {\em Conference on Learning Theory}, pages 1374--1391. PMLR.

\bibitem[Gasnikov et~al., 2019b]{gasnikov2019near}
Gasnikov, A., Dvurechensky, P., Gorbunov, E., Vorontsova, E., Selikhanovych,
  D., Uribe, C.~A., Jiang, B., Wang, H., Zhang, S., Bubeck, S., et~al. (2019b).
\newblock Near optimal methods for minimizing convex functions with lipschitz $
  p $-th derivatives.
\newblock In {\em Conference on Learning Theory}, pages 1392--1393. PMLR.

\bibitem[Jiang et~al., 2019]{jiang2019optimal}
Jiang, B., Wang, H., and Zhang, S. (2019).
\newblock An optimal high-order tensor method for convex optimization.
\newblock In {\em Conference on Learning Theory}, pages 1799--1801. PMLR.

\bibitem[Korpelevich, 1976]{korpelevich1976extragradient}
Korpelevich, G.~M. (1976).
\newblock The extragradient method for finding saddle points and other
  problems.
\newblock {\em Matecon}, 12:747--756.

\bibitem[Monteiro and Svaiter, 2013]{monteiro2013accelerated}
Monteiro, R.~D. and Svaiter, B.~F. (2013).
\newblock An accelerated hybrid proximal extragradient method for convex
  optimization and its implications to second-order methods.
\newblock {\em SIAM Journal on Optimization}, 23(2):1092--1125.

\bibitem[Nemirovskij and Yudin, 1983]{nemirovskij1983problem}
Nemirovskij, A.~S. and Yudin, D.~B. (1983).
\newblock Problem complexity and method efficiency in optimization.

\bibitem[Nesterov, 2003]{nesterov2003introductory}
Nesterov, Y. (2003).
\newblock {\em Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media.

\bibitem[Nesterov, 2008]{nesterov2008accelerating}
Nesterov, Y. (2008).
\newblock Accelerating the cubic regularization of newtonâ€™s method on convex
  problems.
\newblock {\em Mathematical Programming}, 112(1):159--181.

\bibitem[Nesterov, 2021a]{nesterov2021implementable}
Nesterov, Y. (2021a).
\newblock Implementable tensor methods in unconstrained convex optimization.
\newblock {\em Mathematical Programming}, 186(1):157--183.

\bibitem[Nesterov, 2021b]{nesterov2021inexact}
Nesterov, Y. (2021b).
\newblock Inexact high-order proximal-point methods with auxiliary search
  procedure.
\newblock {\em SIAM Journal on Optimization}, 31(4):2807--2828.

\bibitem[Nesterov and Polyak, 2006]{nesterov2006cubic}
Nesterov, Y. and Polyak, B.~T. (2006).
\newblock Cubic regularization of newton method and its global performance.
\newblock {\em Mathematical Programming}, 108(1):177--205.

\bibitem[Nesterov, 1983]{nesterov1983method}
Nesterov, Y.~E. (1983).
\newblock A method for solving the convex programming problem with convergence
  rate o (1/k\^{} 2).
\newblock In {\em Dokl. akad. nauk Sssr}, volume 269, pages 543--547.

\end{thebibliography}
