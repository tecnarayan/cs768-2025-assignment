\begin{thebibliography}{}

\bibitem[Agarwal et~al., 2010]{dist3}
Agarwal, A., Wainwright, M.~J., and Duchi, J.~C. (2010).
\newblock Distributed dual averaging in networks.
\newblock In {\em Advances in Neural Information Processing Systems {(NIPS)}},
  pages 550--558.

\bibitem[Bagdasaryan et~al., 2018]{vitaly2018backdoor}
Bagdasaryan, E., Veit, A., Hua, Y., Estrin, D., and Shmatikov, V. (2018).
\newblock How to backdoor federated learning.
\newblock {\em arXiv preprint arXiv:1807.00459}.

\bibitem[Baydin et~al., 2017]{pytorch}
Baydin, A.~G., Pearlmutter, B.~A., Radul, A.~A., and Siskind, J.~M. (2017).
\newblock Automatic differentiation in machine learning: a survey.
\newblock {\em Journal of Machine Learning Research}, 18(153):1--153.

\bibitem[Biggio et~al., 2012]{Poisoning}
Biggio, B., Nelson, B., and Laskov, P. (2012).
\newblock Poisoning attacks against support vector machines.
\newblock In {\em Proceedings of the 29th International Coference on Machine
  Learning (ICML)}, pages 1467--1474. Omnipress.

\bibitem[Blanchard et~al., 2017]{krum}
Blanchard, P., Guerraoui, R., Stainer, J., et~al. (2017).
\newblock Machine learning with adversaries: Byzantine tolerant gradient
  descent.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)}.

\bibitem[Chen et~al., 2018]{backdoorDefense}
Chen, B., Carvalho, W., Baracaldo, N., Ludwig, H., Edwards, B., Lee, T.,
  Molloy, I., and Srivastava, B. (2018).
\newblock Detecting backdoor attacks on deep neural networks by activation
  clustering.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)}.

\bibitem[Chen et~al., 2017a]{backdoor3}
Chen, X., Liu, C., Li, B., Lu, K., and Song, D. (2017a).
\newblock Targeted backdoor attacks on deep learning systems using data
  poisoning.
\newblock {\em arXiv preprint}.

\bibitem[Chen et~al., 2017b]{iid2}
Chen, Y., Su, L., and Xu, J. (2017b).
\newblock Distributed statistical machine learning in adversarial settings:
  Byzantine gradient descent.
\newblock {\em Proceedings of the ACM on Measurement and Analysis of Computing
  Systems}, 1(2):44.

\bibitem[Dean et~al., 2012]{dist1}
Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Senior, A.,
  Tucker, P., Yang, K., Le, Q.~V., et~al. (2012).
\newblock Large scale distributed deep networks.
\newblock In {\em Advances in neural information processing systems {(NIPS)}},
  pages 1223--1231.

\bibitem[El~Mhamdi et~al., 2018]{Bulyan}
El~Mhamdi, E.~M., Guerraoui, R., and Rouault, S. (2018).
\newblock The hidden vulnerability of distributed learning in {B}yzantium.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, pages 3521--3530.

\bibitem[Fung et~al., 2018]{sybils}
Fung, C., Yoon, C.~J., and Beschastnikh, I. (2018).
\newblock Mitigating sybils in federated learning poisoning.
\newblock {\em arXiv preprint arXiv:1808.04866}.

\bibitem[Kleinberg et~al., 2018]{noise3}
Kleinberg, R.~D., Li, Y., and Yuan, Y. (2018).
\newblock An alternative view: When does sgd escape local minima?
\newblock In {\em the International Coference on Machine Learning (ICML)}.

\bibitem[Kone{\v{c}}n{\`y} et~al., 2016]{fed2}
Kone{\v{c}}n{\`y}, J., McMahan, H.~B., Yu, F.~X., Richt{\'a}rik, P., Suresh,
  A.~T., and Bacon, D. (2016).
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock {\em arXiv preprint arXiv:1610.05492}.

\bibitem[Krizhevsky and Hinton, 2009]{cifar10}
Krizhevsky, A. and Hinton, G. (2009).
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer.

\bibitem[LeCun, 1998]{MNIST}
LeCun, Y. (1998).
\newblock The {MNIST} database of handwritten digits.
\newblock {\em http://yann. lecun. com/exdb/mnist/}.

\bibitem[Li et~al., 2014a]{PS1}
Li, M., Andersen, D.~G., Park, J.~W., Smola, A.~J., Ahmed, A., Josifovski, V.,
  Long, J., Shekita, E.~J., and Su, B.-Y. (2014a).
\newblock Scaling distributed machine learning with the parameter server.
\newblock In {\em OSDI}, volume~14, pages 583--598.

\bibitem[Li et~al., 2014b]{PS2}
Li, M., Andersen, D.~G., Smola, A.~J., and Yu, K. (2014b).
\newblock Communication efficient distributed machine learning with the
  parameter server.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  19--27.

\bibitem[Liu et~al., 2018]{backdoor2}
Liu, Y., Ma, S., Aafer, Y., Lee, W.-C., Zhai, J., Wang, W., and Zhang, X.
  (2018).
\newblock Trojaning attack on neural networks.
\newblock In {\em 25nd Annual Network and Distributed System Security
  Symposium, {NDSS}}.

\bibitem[McMahan et~al., 2016]{fed1}
McMahan, H.~B., Moore, E., Ramage, D., Hampson, S., et~al. (2016).
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock {\em arXiv preprint arXiv:1602.05629}.

\bibitem[Neelakantan et~al., 2016]{noise}
Neelakantan, A., Vilnis, L., Le, Q.~V., Sutskever, I., Kaiser, L., Kurach, K.,
  and Martens, J. (2016).
\newblock Adding gradient noise improves learning for very deep networks.
\newblock {\em International Conference on Learning Representations Workshop
  (ICLR Workshop)}.

\bibitem[Qiao and Valiant, 2017]{backdoorDefense4}
Qiao, M. and Valiant, G. (2017).
\newblock Learning discrete distributions from untrusted batches.
\newblock {\em arXiv preprint arXiv:1711.08113}.

\bibitem[Recht et~al., 2011]{dist4}
Recht, B., Re, C., Wright, S., and Niu, F. (2011).
\newblock Hogwild: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In {\em Advances in neural information processing systems {(NIPS)}},
  pages 693--701.

\bibitem[Shen et~al., 2016]{Auror}
Shen, S., Tople, S., and Saxena, P. (2016).
\newblock A uror: defending against poisoning attacks in collaborative deep
  learning systems.
\newblock In {\em Proceedings of the 32nd Annual Conference on Computer
  Security Applications}, pages 508--519. ACM.

\bibitem[Shirish~Keskar et~al., 2017]{noise2}
Shirish~Keskar, N., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P.
  (2017).
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock {\em International Conference on Learning Representations (ICLR)
  Workshop}.

\bibitem[Steinhardt et~al., 2017]{backdoorDefense3}
Steinhardt, J., Koh, P. W.~W., and Liang, P.~S. (2017).
\newblock Certified defenses for data poisoning attacks.
\newblock In {\em Advances in Neural Information Processing Systems 30 (NIPS)}.

\bibitem[Tran et~al., 2018]{backdoorDefense2}
Tran, B., Li, J., and Madry, A. (2018).
\newblock Spectral signatures in backdoor attacks.
\newblock In {\em Advances in Neural Information Processing Systems 31 (NIPS)}.

\bibitem[Xie et~al., 2018]{MeanMed}
Xie, C., Koyejo, O., and Gupta, I. (2018).
\newblock Generalized {B}yzantine-tolerant {SGD}.
\newblock {\em arXiv preprint arXiv:1802.10116}.

\bibitem[Yin et~al., 2018]{TrimmedMean}
Yin, D., Chen, Y., Ramchandran, K., and Bartlett, P. (2018).
\newblock Byzantine-robust distributed learning: Towards optimal statistical
  rates.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning (ICML)}.

\bibitem[Zhang et~al., 2017]{dist2}
Zhang, H., Zheng, Z., Xu, S., Dai, W., Ho, Q., Liang, X., Hu, Z., Wei, J., Xie,
  P., and Xing, E.~P. (2017).
\newblock Poseidon: An efficient communication architecture for distributed
  deep learning on {GPU} clusters.
\newblock {\em arXiv preprint}.

\end{thebibliography}
