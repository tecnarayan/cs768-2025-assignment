\begin{thebibliography}{128}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[fro()]{frozenbilmwebpage}
Frozen{B}i{LM} project webpage.
\newblock \url{https://antoyang.github.io/frozenbilm.html}.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc,
  Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
  Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds,
  et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Amrani et~al.(2021)Amrani, Ben-Ari, Rotman, and
  Bronstein]{amrani2020noise}
Elad Amrani, Rami Ben-Ari, Daniel Rotman, and Alex Bronstein.
\newblock Noise estimation using density estimation for self-supervised
  multimodal learning.
\newblock In \emph{AAAI}, 2021.

\bibitem[Antol et~al.(2015)Antol, Agrawal, Lu, Mitchell, Batra,
  Lawrence~Zitnick, and Parikh]{antol2015vqa}
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
  C~Lawrence~Zitnick, and Devi Parikh.
\newblock {VQA}: Visual question answering.
\newblock In \emph{ICCV}, 2015.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bain et~al.(2021)Bain, Nagrani, Varol, and Zisserman]{bain2021frozen}
Max Bain, Arsha Nagrani, G{\"u}l Varol, and Andrew Zisserman.
\newblock Frozen in time: A joint video and image encoder for end-to-end
  retrieval.
\newblock In \emph{ICCV}, 2021.

\bibitem[Black et~al.(2021)Black, Leo, Wang, Leahy, and Biderman]{gpt-neo}
Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman.
\newblock {GPT-Neo: Large Scale Autoregressive Language Modeling with
  Mesh-Tensorflow}, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Caba~Heilbron et~al.(2015)Caba~Heilbron, Escorcia, Ghanem, and
  Carlos~Niebles]{caba2015activitynet}
Fabian Caba~Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos~Niebles.
\newblock Activitynet: {A} large-scale video benchmark for human activity
  understanding.
\newblock In \emph{CVPR}, 2015.

\bibitem[Castro et~al.(2020)Castro, Azab, Stroud, Noujaim, Wang, Deng, and
  Mihalcea]{castro2020lifeqa}
Santiago Castro, Mahmoud Azab, Jonathan Stroud, Cristina Noujaim, Ruoyao Wang,
  Jia Deng, and Rada Mihalcea.
\newblock {LifeQA}: A real-life dataset for video question answering.
\newblock In \emph{LREC}, 2020.

\bibitem[Chadha et~al.(2021)Chadha, Arora, and Kaloty]{chadha2020iperceive}
Aman Chadha, Gurneet Arora, and Navpreet Kaloty.
\newblock {iPerceive}: Applying common-sense reasoning to multi-modal dense
  video captioning and video question answering.
\newblock In \emph{WACV}, 2021.

\bibitem[Chen and Dolan(2011)]{chen2011collecting}
David~L Chen and William~B Dolan.
\newblock Collecting highly parallel data for paraphrase evaluation.
\newblock In \emph{ACL}, 2011.

\bibitem[Chen et~al.(2021)Chen, Guo, Yi, Li, and Elhoseiny]{chen2021visualgpt}
Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny.
\newblock {VisualGPT}: Data-efficient adaptation of pretrained language models
  for image captioning.
\newblock \emph{arXiv preprint arXiv:2102.10407}, 2021.

\bibitem[Chen et~al.(2015)Chen, Fang, Lin, Vedantam, Gupta, Doll{\'a}r, and
  Zitnick]{chen2015microsoft}
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr
  Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft {COCO} captions: Data collection and evaluation server.
\newblock \emph{arXiv preprint arXiv:1504.00325}, 2015.

\bibitem[Chen et~al.(2020)Chen, Li, Yu, Kholy, Ahmed, Gan, Cheng, and
  Liu]{chen2019uniter}
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed~El Kholy, Faisal Ahmed, Zhe Gan,
  Yu~Cheng, and Jingjing Liu.
\newblock {UNITER}: Universal image-text representation learning.
\newblock In \emph{ECCV}, 2020.

\bibitem[Choi et~al.(2021)Choi, On, Heo, Seo, Jang, Lee, Lee, and
  Zhang]{choi2020dramaqa}
Seongho Choi, Kyoung-Woon On, Yu-Jung Heo, Ahjeong Seo, Youwon Jang, Seungchan
  Lee, Minsu Lee, and Byoung-Tak Zhang.
\newblock {DramaQA}: Character-centered video story understanding with
  hierarchical qa.
\newblock In \emph{AAAI}, 2021.

\bibitem[Colas et~al.(2020)Colas, Kim, Dernoncourt, Gupte, Wang, and
  Kim]{colas2019tutorialvqa}
Anthony Colas, Seokhwan Kim, Franck Dernoncourt, Siddhesh Gupte, Daisy~Zhe
  Wang, and Doo~Soon Kim.
\newblock Tutorial{VQA}: Question answering dataset for tutorial videos.
\newblock In \emph{LREC}, 2020.

\bibitem[Dang et~al.(2021)Dang, Le, Le, and Tran]{dang2021object}
Long~Hoang Dang, Thao~Minh Le, Vuong Le, and Truyen Tran.
\newblock Object-centric representation learning for video question answering.
\newblock In \emph{IJCNN}, 2021.

\bibitem[De~Lange et~al.(2021)De~Lange, Aljundi, Masana, Parisot, Jia,
  Leonardis, Slabaugh, and Tuytelaars]{DeLange21}
Matthias De~Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu~Jia, Ale≈ù
  Leonardis, Gregory Slabaugh, and Tinne Tuytelaars.
\newblock A continual learning survey: Defying forgetting in classification
  tasks.
\newblock \emph{IEEE TPAMI}, 2021.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{bert18}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL-HLT}, 2019.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2021an}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{ICLR}, 2021.

\bibitem[Eichenberg et~al.(2021)Eichenberg, Black, Weinbach, Parcalabescu, and
  Frank]{eichenberg2021magma}
Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, and
  Anette Frank.
\newblock {MAGMA}--multimodal augmentation of generative models through
  adapter-based finetuning.
\newblock \emph{arXiv preprint arXiv:2112.05253}, 2021.

\bibitem[Fan et~al.(2019)Fan, Zhang, Zhang, Wang, Zhang, and
  Huang]{fan2019heterogeneous}
Chenyou Fan, Xiaofan Zhang, Shu Zhang, Wensheng Wang, Chi Zhang, and Heng
  Huang.
\newblock Heterogeneous memory enhanced multimodal attention model for video
  question answering.
\newblock In \emph{CVPR}, 2019.

\bibitem[Fu et~al.(2021)Fu, Li, Gan, Lin, Wang, Wang, and Liu]{fu2021violet}
Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William~Yang Wang, Lijuan Wang, and
  Zicheng Liu.
\newblock {VIOLET}: End-to-end video-language transformers with masked
  visual-token modeling.
\newblock \emph{arXiv preprint arXiv:2111.12681}, 2021.

\bibitem[Gan et~al.(2020)Gan, Chen, Li, Zhu, Cheng, and Liu]{gan2020large}
Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu~Cheng, and Jingjing Liu.
\newblock Large-scale adversarial training for vision-and-language
  representation learning.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Gao et~al.(2018)Gao, Ge, Chen, and Nevatia]{gao2018motion}
Jiyang Gao, Runzhou Ge, Kan Chen, and Ram Nevatia.
\newblock Motion-appearance co-memory networks for video question answering.
\newblock In \emph{CVPR}, 2018.

\bibitem[Garcia et~al.(2020)Garcia, Otani, Chu, and
  Nakashima]{garcia2020knowit}
Noa Garcia, Mayu Otani, Chenhui Chu, and Yuta Nakashima.
\newblock {KnowIT VQA}: Answering knowledge-based questions about videos.
\newblock In \emph{AAAI}, 2020.

\bibitem[He et~al.(2021)He, Liu, Gao, and Chen]{he2021deberta}
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.
\newblock De{BERT}a: Decoding-enhanced {BERT} with disentangled attention.
\newblock In \emph{ICLR}, 2021.

\bibitem[Hendricks et~al.(2021)Hendricks, Mellor, Schneider, Alayrac, and
  Nematzadeh]{hendricks2021decoupling}
Lisa~Anne Hendricks, John Mellor, Rosalia Schneider, Jean-Baptiste Alayrac, and
  Aida Nematzadeh.
\newblock Decoupling the role of data, attention, and losses in multimodal
  transformers.
\newblock In \emph{TACL}, 2021.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone,
  De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019parameter}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
  De~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In \emph{ICML}, 2019.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and
  Chen]{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen.
\newblock Lo{RA}: Low-rank adaptation of large language models.
\newblock In \emph{ICLR}, 2022.

\bibitem[Huang et~al.(2020{\natexlab{a}})Huang, Chen, Zeng, Du, Tan, and
  Gan]{huang2020location}
Deng Huang, Peihao Chen, Runhao Zeng, Qing Du, Mingkui Tan, and Chuang Gan.
\newblock Location-aware graph convolutional networks for video question
  answering.
\newblock In \emph{AAAI}, 2020{\natexlab{a}}.

\bibitem[Huang et~al.(2020{\natexlab{b}})Huang, Zeng, Liu, Fu, and
  Fu]{huang2020pixel}
Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu.
\newblock Pixel-{BERT}: Aligning image pixels with text by deep multi-modal
  transformers.
\newblock \emph{arXiv preprint arXiv:2004.00849}, 2020{\natexlab{b}}.

\bibitem[Jang et~al.(2017)Jang, Song, Yu, Kim, and Kim]{jang2017tgif}
Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim.
\newblock {TGIF-QA}: Toward spatio-temporal reasoning in visual question
  answering.
\newblock In \emph{CVPR}, 2017.

\bibitem[Jiang et~al.(2020{\natexlab{a}})Jiang, Chen, Lin, Zhao, and
  Gao]{jiang2020divide}
Jianwen Jiang, Ziqiang Chen, Haojie Lin, Xibin Zhao, and Yue Gao.
\newblock Divide and conquer: Question-guided spatio-temporal contextual
  attention for video question answering.
\newblock In \emph{AAAI}, 2020{\natexlab{a}}.

\bibitem[Jiang and Han(2020)]{jiang2020reasoning}
Pin Jiang and Yahong Han.
\newblock Reasoning with heterogeneous graph alignment for video question
  answering.
\newblock In \emph{AAAI}, 2020.

\bibitem[Jiang et~al.(2020{\natexlab{b}})Jiang, Anastasopoulos, Araki, Ding,
  and Neubig]{jiang2020x}
Zhengbao Jiang, Antonios Anastasopoulos, Jun Araki, Haibo Ding, and Graham
  Neubig.
\newblock X-factr: Multilingual factual knowledge retrieval from pretrained
  language models.
\newblock In \emph{EMNLP}, 2020{\natexlab{b}}.

\bibitem[Joshi et~al.(2020)Joshi, Chen, Liu, Weld, Zettlemoyer, and
  Levy]{joshi2020spanbert}
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel~S Weld, Luke Zettlemoyer, and Omer
  Levy.
\newblock Span{BERT}: Improving pre-training by representing and predicting
  spans.
\newblock In \emph{TACL}, 2020.

\bibitem[Kim et~al.(2020{\natexlab{a}})Kim, Tang, and Bansal]{kim2020dense}
Hyounghun Kim, Zineng Tang, and Mohit Bansal.
\newblock Dense-caption matching and frame-selection gating for temporal
  localization in {VideoQA}.
\newblock In \emph{ACL}, 2020{\natexlab{a}}.

\bibitem[Kim et~al.(2020{\natexlab{b}})Kim, Ma, Pham, Kim, and
  Yoo]{kim2020modality}
Junyeong Kim, Minuk Ma, Trung Pham, Kyungsu Kim, and Chang~D Yoo.
\newblock Modality shifting attention network for multi-modal video question
  answering.
\newblock In \emph{CVPR}, 2020{\natexlab{b}}.

\bibitem[Kim et~al.(2017)Kim, Heo, Choi, and Zhang]{kim2017deepstory}
Kyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, and Byoung-Tak Zhang.
\newblock Deepstory: Video story qa by deep embedded memory networks.
\newblock In \emph{IJCAI}, 2017.

\bibitem[Kim et~al.(2021{\natexlab{a}})Kim, Jeong, Kim, Kang, and
  Kwak]{kim2021self}
Seonhoon Kim, Seohyeong Jeong, Eunbyul Kim, Inho Kang, and Nojun Kwak.
\newblock Self-supervised pre-training and contrastive representation learning
  for multiple-choice video qa.
\newblock In \emph{AAAI}, 2021{\natexlab{a}}.

\bibitem[Kim et~al.(2021{\natexlab{b}})Kim, Son, and Kim]{kim2021vilt}
Wonjae Kim, Bokyung Son, and Ildoo Kim.
\newblock Vi{LT}: Vision-and-language transformer without convolution or region
  supervision.
\newblock In \emph{ICML}, 2021{\natexlab{b}}.

\bibitem[Kingma and Ba(2015)]{kingma15adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}, 2015.

\bibitem[Krishna et~al.(2016)Krishna, Zhu, Groth, Johnson, Hata, Kravitz, Chen,
  Kalantidis, Li, Shamma, Bernstein, and Fei-Fei]{visualgenome}
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
  Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David~A Shamma,
  Michael Bernstein, and Li~Fei-Fei.
\newblock Visual {G}enome: Connecting language and vision using crowdsourced
  dense image annotations.
\newblock \emph{IJCV}, 2016.

\bibitem[Kudo and Richardson(2018)]{kudo2018sentencepiece}
Taku Kudo and John Richardson.
\newblock Sentencepiece: A simple and language independent subword tokenizer
  and detokenizer for neural text processing.
\newblock In \emph{ACL}, 2018.

\bibitem[Lan et~al.(2020)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut]{lan2019albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut.
\newblock {ALBERT}: A lite {BERT} for self-supervised learning of language
  representations.
\newblock In \emph{ICLR}, 2020.

\bibitem[Le et~al.(2020{\natexlab{a}})Le, Le, Venkatesh, and
  Tran]{le2020hierarchical}
Thao~Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran.
\newblock Hierarchical conditional relation networks for video question
  answering.
\newblock In \emph{CVPR}, 2020{\natexlab{a}}.

\bibitem[Le et~al.(2020{\natexlab{b}})Le, Le, Venkatesh, and
  Tran]{le2020neural}
Thao~Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran.
\newblock Neural reasoning, fast and slow, for video question answering.
\newblock In \emph{IJCNN}, 2020{\natexlab{b}}.

\bibitem[Le et~al.(2021)Le, Le, Venkatesh, and Tran]{le2021hierarchical}
Thao~Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran.
\newblock Hierarchical conditional relation networks for multimodal video
  question answering.
\newblock In \emph{IJCV}, 2021.

\bibitem[Lei et~al.(2018)Lei, Yu, Bansal, and Berg]{lei2018tvqa}
Jie Lei, Licheng Yu, Mohit Bansal, and Tamara~L Berg.
\newblock {TVQA}: Localized, compositional video question answering.
\newblock In \emph{EMNLP}, 2018.

\bibitem[Lei et~al.(2020)Lei, Yu, Berg, and Bansal]{lei2019tvqa+}
Jie Lei, Licheng Yu, Tamara~L Berg, and Mohit Bansal.
\newblock {TVQA}+: Spatio-temporal grounding for video question answering.
\newblock In \emph{ACL}, 2020.

\bibitem[Lei et~al.(2021)Lei, Li, Zhou, Gan, Berg, Bansal, and
  Liu]{lei2021less}
Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara~L Berg, Mohit Bansal, and
  Jingjing Liu.
\newblock Less is more: {ClipBERT} for video-and-language learning via sparse
  sampling.
\newblock In \emph{CVPR}, 2021.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{EMNLP}, 2021.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Li, Li, Niebles, and
  Hoi]{li2021align2}
Dongxu Li, Junnan Li, Hongdong Li, Juan~Carlos Niebles, and Steven~CH Hoi.
\newblock Align and prompt: Video-and-language pre-training with entity
  prompts.
\newblock \emph{arXiv preprint arXiv:2112.09583}, 2021{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Duan, Fang, Gong, Jiang, and
  Zhou]{li2019unicodervl}
Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, and Ming Zhou.
\newblock Unicoder-{VL}: A universal encoder for vision and language by
  cross-modal pre-training.
\newblock In \emph{AAAI}, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Selvaraju, Gotmare, Joty, Xiong, and
  Hoi]{li2021align}
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong,
  and Steven Chu~Hong Hoi.
\newblock Align before fuse: Vision and language representation learning with
  momentum distillation.
\newblock In \emph{NeurIPS}, 2021{\natexlab{b}}.

\bibitem[Li et~al.(2022)Li, Li, Xiong, and Hoi]{li2022blip}
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
\newblock {BLIP}: Bootstrapping language-image pre-training for unified
  vision-language understanding and generation.
\newblock \emph{arXiv preprint arXiv:2201.12086}, 2022.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Chen, Cheng, Gan, Yu, and
  Liu]{li2020hero}
Linjie Li, Yen-Chun Chen, Yu~Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu.
\newblock {HERO}: Hierarchical encoder for video+language omni-representation
  pre-training.
\newblock In \emph{EMNLP}, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2021{\natexlab{c}})Li, Lei, Gan, Yu, Chen, Pillai, Cheng,
  Zhou, Wang, Wang, et~al.]{li2021value}
Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen, Rohit Pillai, Yu~Cheng,
  Luowei Zhou, Xin~Eric Wang, William~Yang Wang, et~al.
\newblock {VALUE}: A multi-task benchmark for video-and-language understanding
  evaluation.
\newblock In \emph{NeurIPS Track on Datasets and Benchmarks},
  2021{\natexlab{c}}.

\bibitem[Li et~al.(2019{\natexlab{a}})Li, Yatskar, Yin, Hsieh, and
  Chang]{li2019visualbert}
Liunian~Harold Li, Mark Yatskar, Da~Yin, Cho-Jui Hsieh, and Kai-Wei Chang.
\newblock Visual{BERT}: A simple and performant baseline for vision and
  language.
\newblock \emph{arXiv preprint arXiv:1908.03557}, 2019{\natexlab{a}}.

\bibitem[Li and Liang(2021)]{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In \emph{ACL}, 2021.

\bibitem[Li et~al.(2019{\natexlab{b}})Li, Song, Gao, Liu, Huang, He, and
  Gan]{li2019beyond}
Xiangpeng Li, Jingkuan Song, Lianli Gao, Xianglong Liu, Wenbing Huang, Xiangnan
  He, and Chuang Gan.
\newblock Beyond {RNN}s: Positional self-attention with co-attention for video
  question answering.
\newblock In \emph{AAAI}, 2019{\natexlab{b}}.

\bibitem[Li et~al.(2020{\natexlab{c}})Li, Yin, Li, Zhang, Hu, Zhang, Wang, Hu,
  Dong, Wei, et~al.]{li2020oscar}
Xiujun Li, Xi~Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan
  Wang, Houdong Hu, Li~Dong, Furu Wei, et~al.
\newblock Oscar: Object-semantics aligned pre-training for vision-language
  tasks.
\newblock In \emph{ECCV}, 2020{\natexlab{c}}.

\bibitem[Li et~al.(2016)Li, Song, Cao, Tetreault, Goldberg, Jaimes, and
  Luo]{tgif-cvpr2016}
Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault, Larry Goldberg,
  Alejandro Jaimes, and Jiebo Luo.
\newblock {TGIF: A New Dataset and Benchmark on Animated GIF Description}.
\newblock In \emph{CVPR}, 2016.

\bibitem[Lin et~al.(2021)Lin, Bertasius, Wang, Chang, Parikh, and
  Torresani]{lin2021vx2text}
Xudong Lin, Gedas Bertasius, Jue Wang, Shih-Fu Chang, Devi Parikh, and Lorenzo
  Torresani.
\newblock {VX2TEXT}: End-to-end learning of video-based text generation from
  multimodal inputs.
\newblock In \emph{CVPR}, 2021.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Ro{BERT}a: A robustly optimized {BERT} pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Lu et~al.(2019)Lu, Batra, Parikh, and Lee]{lu2019vilbert}
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
\newblock Vi{LBERT}: Pretraining task-agnostic visiolinguistic representations
  for vision-and-language tasks.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Lu et~al.(2020)Lu, Goswami, Rohrbach, Parikh, and Lee]{lu202012}
Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee.
\newblock 12-in-1: Multi-task vision and language representation learning.
\newblock In \emph{CVPR}, 2020.

\bibitem[Luo et~al.(2022)Luo, Xi, Zhang, and Ma]{luo2022vc}
Ziyang Luo, Yadong Xi, Rongsheng Zhang, and Jing Ma.
\newblock {VC-GPT}: Visual conditioned {GPT} for end-to-end generative
  vision-and-language pre-training.
\newblock \emph{arXiv preprint arXiv:2201.12723}, 2022.

\bibitem[Mahabadi et~al.(2022)Mahabadi, Zettlemoyer, Henderson, Saeidi,
  Mathias, Stoyanov, and Yazdani]{mahabadi2022perfect}
Rabeeh~Karimi Mahabadi, Luke Zettlemoyer, James Henderson, Marzieh Saeidi,
  Lambert Mathias, Veselin Stoyanov, and Majid Yazdani.
\newblock {PERFECT}: Prompt-free and efficient few-shot learning with language
  models.
\newblock In \emph{ACL}, 2022.

\bibitem[Maharaj et~al.(2017)Maharaj, Ballas, Rohrbach, Courville, and
  Pal]{maharaj2017dataset}
Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron Courville, and Christopher
  Pal.
\newblock A dataset and exploration of models for understanding video data
  through fill-in-the-blank question-answering.
\newblock In \emph{CVPR}, 2017.

\bibitem[Miech et~al.(2019)Miech, Zhukov, Alayrac, Tapaswi, Laptev, and
  Sivic]{miech19howto100m}
Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan
  Laptev, and Josef Sivic.
\newblock {HowTo100M}: Learning a text-video embedding by watching hundred
  million narrated video clips.
\newblock In \emph{ICCV}, 2019.

\bibitem[Mokady et~al.(2021)Mokady, Hertz, and Bermano]{mokady2021clipcap}
Ron Mokady, Amir Hertz, and Amit~H Bermano.
\newblock {ClipCap}: Clip prefix for image captioning.
\newblock \emph{arXiv preprint arXiv:2111.09734}, 2021.

\bibitem[Mun et~al.(2017)Mun, Hongsuck~Seo, Jung, and Han]{mun2017marioqa}
Jonghwan Mun, Paul Hongsuck~Seo, Ilchae Jung, and Bohyung Han.
\newblock {MarioQA}: Answering questions by watching gameplay videos.
\newblock In \emph{CVPR}, 2017.

\bibitem[Park et~al.(2021)Park, Lee, and Sohn]{park2021bridge}
Jungin Park, Jiyoung Lee, and Kwanghoon Sohn.
\newblock Bridge to answer: Structure-aware graph interaction network for video
  question answering.
\newblock In \emph{CVPR}, 2021.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock \emph{arXiv preprint arXiv:2103.00020}, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{JMLR}, 2020.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang]{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock {SQuAD}: 100,000+ questions for machine comprehension of text.
\newblock \emph{arXiv preprint arXiv:1606.05250}, 2016.

\bibitem[Rohrbach et~al.(2015)Rohrbach, Rohrbach, Tandon, and
  Schiele]{rohrbach15dataset}
Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele.
\newblock A dataset for movie description.
\newblock In \emph{CVPR}, 2015.

\bibitem[Rohrbach et~al.(2017)Rohrbach, Torabi, Rohrbach, Tandon, Pal,
  Larochelle, Courville, and Schiele]{rohrbach17movie}
Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Pal,
  Hugo Larochelle, Aaron Courville, and Bernt Schiele.
\newblock Movie description.
\newblock \emph{IJCV}, 2017.

\bibitem[Sadhu et~al.(2021)Sadhu, Chen, and Nevatia]{sadhu2021video}
Arka Sadhu, Kan Chen, and Ram Nevatia.
\newblock Video question answering with phrases via semantic roles.
\newblock In \emph{NAACL}, 2021.

\bibitem[Salazar et~al.(2020)Salazar, Liang, Nguyen, and
  Kirchhoff]{salazar2019masked}
Julian Salazar, Davis Liang, Toan~Q Nguyen, and Katrin Kirchhoff.
\newblock Masked language model scoring.
\newblock In \emph{ACL}, 2020.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distil{BERT}, a distilled version of {BERT}: smaller, faster, cheaper
  and lighter.
\newblock \emph{arXiv preprint arXiv:1910.01108}, 2019.

\bibitem[Schick and Sch{\"u}tze(2021{\natexlab{a}})]{schick2020exploiting}
Timo Schick and Hinrich Sch{\"u}tze.
\newblock Exploiting cloze questions for few shot text classification and
  natural language inference.
\newblock In \emph{EACL}, 2021{\natexlab{a}}.

\bibitem[Schick and Sch{\"u}tze(2021{\natexlab{b}})]{schick2020s}
Timo Schick and Hinrich Sch{\"u}tze.
\newblock It's not just size that matters: Small language models are also
  few-shot learners.
\newblock In \emph{NAACL}, 2021{\natexlab{b}}.

\bibitem[Seo et~al.(2021)Seo, Nagrani, and Schmid]{seo2020look}
Paul~Hongsuck Seo, Arsha Nagrani, and Cordelia Schmid.
\newblock Look before you speak: Visually contextualized utterances.
\newblock In \emph{CVPR}, 2021.

\bibitem[Seo et~al.(2022)Seo, Nagrani, Arnab, and Schmid]{seo2022end}
Paul~Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and Cordelia Schmid.
\newblock End-to-end generative pretraining for multimodal video captioning.
\newblock In \emph{CVPR}, 2022.

\bibitem[Shen et~al.(2021)Shen, Li, Tan, Bansal, Rohrbach, Chang, Yao, and
  Keutzer]{shen2021much}
Sheng Shen, Liunian~Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei
  Chang, Zhewei Yao, and Kurt Keutzer.
\newblock How much can clip benefit vision-and-language tasks?
\newblock \emph{arXiv preprint arXiv:2107.06383}, 2021.

\bibitem[Singh et~al.(2022)Singh, Hu, Goswami, Couairon, Galuba, Rohrbach, and
  Kiela]{singh2021flava}
Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech
  Galuba, Marcus Rohrbach, and Douwe Kiela.
\newblock Flava: A foundational language and vision alignment model.
\newblock In \emph{CVPR}, 2022.

\bibitem[So et~al.(2021)So, Ma{\'n}ke, Liu, Dai, Shazeer, and Le]{so2021primer}
David~R So, Wojciech Ma{\'n}ke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and
  Quoc~V Le.
\newblock Primer: Searching for efficient transformers for language modeling.
\newblock \emph{arXiv preprint arXiv:2109.08668}, 2021.

\bibitem[Song et~al.(2018)Song, Shi, Chen, and Han]{song2018explore}
Xiaomeng Song, Yucheng Shi, Xin Chen, and Yahong Han.
\newblock Explore multi-step reasoning in video question answering.
\newblock In \emph{ACM international conference on Multimedia}, 2018.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{JMLR}, 2014.

\bibitem[Strubell et~al.(2019)Strubell, Ganesh, and
  McCallum]{strubell2019energy}
Emma Strubell, Ananya Ganesh, and Andrew McCallum.
\newblock Energy and policy considerations for deep learning in nlp.
\newblock In \emph{ACL}, 2019.

\bibitem[Su et~al.(2019)Su, Zhu, Cao, Li, Lu, Wei, and Dai]{su2019vl}
Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai.
\newblock {VL-BERT}: Pre-training of generic visual-linguistic representations.
\newblock In \emph{ICLR}, 2019.

\bibitem[Sun et~al.(2019)Sun, Myers, Vondrick, Murphy, and
  Schmid]{sun2019videobert}
Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid.
\newblock {VideoBERT}: A joint model for video and language representation
  learning.
\newblock In \emph{ICCV}, 2019.

\bibitem[Tam et~al.(2021)Tam, Menon, Bansal, Srivastava, and
  Raffel]{tam2021improving}
Derek Tam, Rakesh~R Menon, Mohit Bansal, Shashank Srivastava, and Colin Raffel.
\newblock Improving and simplifying pattern exploiting training.
\newblock In \emph{EMNLP}, 2021.

\bibitem[Tan and Bansal(2019)]{tan2019lxmert}
Hao Tan and Mohit Bansal.
\newblock {LXMERT}: Learning cross-modality encoder representations from
  transformers.
\newblock In \emph{EMNLP}, 2019.

\bibitem[Tapaswi et~al.(2016)Tapaswi, Zhu, Stiefelhagen, Torralba, Urtasun, and
  Fidler]{tapaswi16movieqa}
Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel
  Urtasun, and Sanja Fidler.
\newblock {MovieQA}: Understanding stories in movies through
  question-answering.
\newblock In \emph{CVPR}, 2016.

\bibitem[Taylor(1953)]{taylor1953cloze}
Wilson~L Taylor.
\newblock ‚Äúcloze procedure‚Äù: A new tool for measuring readability.
\newblock \emph{Journalism quarterly}, 30\penalty0 (4):\penalty0 415--433,
  1953.

\bibitem[Tsimpoukelli et~al.(2021)Tsimpoukelli, Menick, Cabi, Eslami, Vinyals,
  and Hill]{tsimpoukelli2021multimodal}
Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM~Eslami, Oriol Vinyals, and
  Felix Hill.
\newblock Multimodal few-shot learning with frozen language models.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Ge, Yan, Ge, Lin, Cai, Wu, Shan,
  Qie, and Shou]{wang2022all}
Alex~Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Xudong Lin, Guanyu Cai,
  Jianping Wu, Ying Shan, Xiaohu Qie, and Mike~Zheng Shou.
\newblock All in one: Exploring unified video-language pre-training.
\newblock \emph{arXiv preprint arXiv:2203.07303}, 2022{\natexlab{a}}.

\bibitem[Wang and Komatsuzaki(2021)]{gpt-j}
Ben Wang and Aran Komatsuzaki.
\newblock {GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model},
  2021.

\bibitem[Wang et~al.(2021)Wang, Hu, Gan, Yang, Dai, Liu, Lu, and
  Wang]{wang2021ufo}
Jianfeng Wang, Xiaowei Hu, Zhe Gan, Zhengyuan Yang, Xiyang Dai, Zicheng Liu,
  Yumao Lu, and Lijuan Wang.
\newblock {UFO}: A unified transformer for vision-language representation
  learning.
\newblock \emph{arXiv preprint arXiv:2111.10023}, 2021.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Li, Xu, Zhou, Lei, Lin, Wang,
  Yang, Zhu, Hoiem, et~al.]{wang2022language}
Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin,
  Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, et~al.
\newblock Language models with image descriptors are strong few-shot
  video-language learners.
\newblock In \emph{NeurIPS}, 2022{\natexlab{b}}.

\bibitem[Xiao et~al.(2021)Xiao, Shang, Yao, and Chua]{xiao2021next}
Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua.
\newblock {NE}x{T}-{QA}: Next phase of question-answering to explaining
  temporal actions.
\newblock In \emph{CVPR}, 2021.

\bibitem[Xu et~al.(2017)Xu, Zhao, Xiao, Wu, Zhang, He, and Zhuang]{xu2017video}
Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting
  Zhuang.
\newblock Video question answering via gradually refined attention over
  appearance and motion.
\newblock In \emph{ACM international conference on Multimedia}, 2017.

\bibitem[Xu et~al.(2021)Xu, Ghosh, Huang, Okhonko, Aghajanyan, Metze,
  Zettlemoyer, and Feichtenhofer]{xu2021videoclip}
Hu~Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian
  Metze, Luke Zettlemoyer, and Christoph Feichtenhofer.
\newblock Videoclip: Contrastive pre-training for zero-shot video-text
  understanding.
\newblock In \emph{EMNLP}, 2021.

\bibitem[Xu et~al.(2016)Xu, Mei, Yao, and Rui]{xu16msrvtt}
Jun Xu, Tao Mei, Ting Yao, and Yong Rui.
\newblock {MSR-VTT}: A large video description dataset for bridging video and
  language.
\newblock In \emph{CVPR}, 2016.

\bibitem[Xue et~al.(2018)Xue, Chu, Zhao, and Cai]{xue2018better}
Hongyang Xue, Wenqing Chu, Zhou Zhao, and Deng Cai.
\newblock A better way to attend: Attention with trees for video question
  answering.
\newblock \emph{IEEE Transactions on Image Processing}, 2018.

\bibitem[Yang et~al.(2021{\natexlab{a}})Yang, Miech, Sivic, Laptev, and
  Schmid]{yang2021just}
Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid.
\newblock Just ask: Learning to answer questions from millions of narrated
  videos.
\newblock In \emph{ICCV}, 2021{\natexlab{a}}.

\bibitem[Yang et~al.(2022)Yang, Miech, Sivic, Laptev, and
  Schmid]{Yang2022LearningTA}
Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid.
\newblock Learning to answer visual questions from web videos.
\newblock \emph{IEEE TPAMI}, 2022.

\bibitem[Yang et~al.(2020)Yang, Garcia, Chu, Otani, Nakashima, and
  Takemura]{yang2020bert}
Zekun Yang, Noa Garcia, Chenhui Chu, Mayu Otani, Yuta Nakashima, and Haruo
  Takemura.
\newblock {BERT} representations for video question answering.
\newblock In \emph{WACV}, 2020.

\bibitem[Yang et~al.(2021{\natexlab{b}})Yang, Gan, Wang, Hu, Lu, Liu, and
  Wang]{yang2021empirical}
Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and
  Lijuan Wang.
\newblock An empirical study of {GPT-3} for few-shot knowledge-based {VQA}.
\newblock \emph{arXiv preprint arXiv:2109.05014}, 2021{\natexlab{b}}.

\bibitem[Ye et~al.(2017)Ye, Zhao, Li, Chen, Xiao, and Zhuang]{ye2017video}
Yunan Ye, Zhou Zhao, Yimeng Li, Long Chen, Jun Xiao, and Yueting Zhuang.
\newblock Video question answering via attribute-augmented attention network
  learning.
\newblock In \emph{ACM SIGIR}, 2017.

\bibitem[Yu et~al.(2020)Yu, Tang, Yin, Sun, Tian, Wu, and Wang]{yu2020ernie}
Fei Yu, Jiji Tang, Weichong Yin, Yu~Sun, Hao Tian, Hua Wu, and Haifeng Wang.
\newblock Ernie-vil: Knowledge enhanced vision-language representations through
  scene graph.
\newblock In \emph{AAAI}, 2020.

\bibitem[Yu et~al.(2021)Yu, Zheng, Li, Ji, Wu, Xiao, and Duan]{yu2021learning}
Weijiang Yu, Haoteng Zheng, Mengfei Li, Lei Ji, Lijun Wu, Nong Xiao, and Nan
  Duan.
\newblock Learning from inside: Self-driven siamese sampling and reasoning for
  video question answering.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Yu et~al.(2019)Yu, Xu, Yu, Yu, Zhao, Zhuang, and
  Tao]{yu2019activitynet}
Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng
  Tao.
\newblock {ActivityNet-QA}: A dataset for understanding complex web videos via
  question answering.
\newblock In \emph{AAAI}, 2019.

\bibitem[Zellers et~al.(2021)Zellers, Lu, Hessel, Yu, Park, Cao, Farhadi, and
  Choi]{zellers2021merlot}
Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae~Sung Park, Jize Cao,
  Ali Farhadi, and Yejin Choi.
\newblock {MERLOT}: Multimodal neural script knowledge models.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Zellers et~al.(2022)Zellers, Lu, Lu, Yu, Zhao, Salehi, Kusupati,
  Hessel, Farhadi, and Choi]{zellers2022merlot}
Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza
  Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi.
\newblock {MERLOT R}eserve: Neural script knowledge through vision and language
  and sound.
\newblock In \emph{CVPR}, 2022.

\bibitem[Zeng et~al.(2022)Zeng, Wong, Welker, Choromanski, Tombari, Purohit,
  Ryoo, Sindhwani, Lee, Vanhoucke, et~al.]{zeng2022socratic}
Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari,
  Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke,
  et~al.
\newblock Socratic models: Composing zero-shot multimodal reasoning with
  language.
\newblock \emph{arXiv preprint arXiv:2204.00598}, 2022.

\bibitem[Zha et~al.(2019)Zha, Liu, Yang, and Zhang]{zha2019spatiotemporal}
Zheng-Jun Zha, Jiawei Liu, Tianhao Yang, and Yongdong Zhang.
\newblock Spatiotemporal-textual co-attention network for video question
  answering.
\newblock \emph{ACM Transactions on Multimedia Computing, Communications, and
  Applications (TOMM)}, 2019.

\bibitem[Zhou et~al.(2021)Zhou, Yang, Loy, and Liu]{zhou2021learning}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Learning to prompt for vision-language models.
\newblock \emph{arXiv preprint arXiv:2109.01134}, 2021.

\bibitem[Zhou et~al.(2020)Zhou, Palangi, Zhang, Hu, Corso, and
  Gao]{zhou2020unified}
Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason~J Corso, and Jianfeng
  Gao.
\newblock Unified vision-language pre-training for image captioning and {VQA}.
\newblock In \emph{AAAI}, 2020.

\bibitem[Zhu and Yang(2020)]{zhu2020actbert}
Linchao Zhu and Yi~Yang.
\newblock Act{BERT}: Learning global-local video-text representations.
\newblock In \emph{CVPR}, 2020.

\bibitem[Zhuang et~al.(2020)Zhuang, Xu, Yan, Cheng, Zhao, Pu, and
  Xiao]{zhuang2020multichannel}
Yueting Zhuang, Dejing Xu, Xin Yan, Wenzhuo Cheng, Zhou Zhao, Shiliang Pu, and
  Jun Xiao.
\newblock Multichannel attention refinement for video question answering.
\newblock \emph{ACM Transactions on Multimedia Computing, Communications, and
  Applications (TOMM)}, 2020.

\end{thebibliography}
