\begin{thebibliography}{89}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Schuurmans, and
  Norouzi]{agarwal2020optimistic}
Agarwal, R., Schuurmans, D., and Norouzi, M.
\newblock An optimistic perspective on offline reinforcement learning.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  104--114. PMLR, 13--18
  Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/agarwal20c.html}.

\bibitem[Agarwal et~al.(2021)Agarwal, Schwarzer, Castro, Courville, and
  Bellemare]{agarwal2021deep}
Agarwal, R., Schwarzer, M., Castro, P.~S., Courville, A.~C., and Bellemare, M.
\newblock Deep reinforcement learning at the edge of the statistical precipice.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 29304--29320, 2021.

\bibitem[Agarwal et~al.(2022)Agarwal, Schwarzer, Castro, Courville, and
  Bellemare]{agarwal2022reincarnating}
Agarwal, R., Schwarzer, M., Castro, P.~S., Courville, A.~C., and Bellemare, M.
\newblock Reincarnating reinforcement learning: Reusing prior computation to
  accelerate progress.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and
  Oh, A. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  28955--28971. Curran Associates, Inc., 2022.

\bibitem[Arnob et~al.(2021)Arnob, Ohib, Plis, and Precup]{arnob2021single}
Arnob, S.~Y., Ohib, R., Plis, S., and Precup, D.
\newblock Single-shot pruning for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2112.15579}, 2021.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{Bellemare_2013}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, jun 2013.
\newblock \doi{10.1613/jair.3912}.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and Munos]{Bellemare2017ADP}
Bellemare, M.~G., Dabney, W., and Munos, R.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{ICML}, 2017.

\bibitem[Bellemare et~al.(2020)Bellemare, Candido, Castro, Gong, Machado,
  Moitra, Ponda, and Wang]{Bellemare2020AutonomousNO}
Bellemare, M.~G., Candido, S., Castro, P.~S., Gong, J., Machado, M.~C., Moitra,
  S., Ponda, S.~S., and Wang, Z.
\newblock Autonomous navigation of stratospheric balloons using reinforcement
  learning.
\newblock \emph{Nature}, 588:\penalty0 77 -- 82, 2020.

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, D{e}biak, Dennison,
  Farhi, Fischer, Hashme, Hesse, et~al.]{berner2019dota}
Berner, C., Brockman, G., Chan, B., Cheung, V., D{e}biak, P., Dennison, C.,
  Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.06680}, 2019.

\bibitem[Bjorck et~al.(2021)Bjorck, Gomes, and Weinberger]{bjorck2021towards}
Bjorck, N., Gomes, C.~P., and Weinberger, K.~Q.
\newblock Towards deeper deep reinforcement learning with spectral
  normalization.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 8242--8255, 2021.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, VanderPlas, Wanderman-Milne,
  et~al.]{bradbury2018jax}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., et~al.
\newblock Jax: composable transformations of python+ numpy programs.
\newblock 2018.

\bibitem[Castro et~al.(2018)Castro, Moitra, Gelada, Kumar, and
  Bellemare]{castro2018dopamine}
Castro, P.~S., Moitra, S., Gelada, C., Kumar, S., and Bellemare, M.~G.
\newblock Dopamine: A research framework for deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1812.06110}, 2018.

\bibitem[Ceron \& Castro(2021)Ceron and Castro]{ceron2021revisiting}
Ceron, J. S.~O. and Castro, P.~S.
\newblock Revisiting rainbow: Promoting more insightful and inclusive deep
  reinforcement learning research.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1373--1383. PMLR, 2021.

\bibitem[Ceron et~al.(2023)Ceron, Bellemare, and Castro]{ceron2023small}
Ceron, J. S.~O., Bellemare, M.~G., and Castro, P.~S.
\newblock Small batch deep reinforcement learning.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=wPqEvmwFEh}.

\bibitem[Ceron et~al.(2024)Ceron, Sokar, Willi, Lyle, Farebrother, Foerster,
  Dziugaite, Precup, and Castro]{obando2024mixtures}
Ceron, J. S.~O., Sokar, G., Willi, T., Lyle, C., Farebrother, J., Foerster,
  J.~N., Dziugaite, G.~K., Precup, D., and Castro, P.~S.
\newblock Mixtures of experts unlock parameter scaling for deep {RL}.
\newblock In \emph{Forty-first International Conference on Machine Learning},
  2024.
\newblock URL \url{https://openreview.net/forum?id=X9VMhfFxwn}.

\bibitem[Cetin et~al.(2022)Cetin, Ball, Roberts, and
  Celiktutan]{cetin2022stabilizing}
Cetin, E., Ball, P.~J., Roberts, S., and Celiktutan, O.
\newblock Stabilizing off-policy deep reinforcement learning from pixels.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2784--2810. PMLR, 2022.

\bibitem[Cobbe et~al.(2020)Cobbe, Hesse, Hilton, and
  Schulman]{cobbe2020leveraging}
Cobbe, K., Hesse, C., Hilton, J., and Schulman, J.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\
  2048--2056. PMLR, 2020.

\bibitem[Dabney et~al.(2018)Dabney, Ostrovski, Silver, and
  Munos]{dabney2018implicit}
Dabney, W., Ostrovski, G., Silver, D., and Munos, R.
\newblock Implicit quantile networks for distributional reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\
  1096--1105. PMLR, 2018.

\bibitem[D'Oro et~al.(2023)D'Oro, Schwarzer, Nikishin, Bacon, Bellemare, and
  Courville]{doro2023sampleefficient}
D'Oro, P., Schwarzer, M., Nikishin, E., Bacon, P.-L., Bellemare, M.~G., and
  Courville, A.
\newblock Sample-efficient reinforcement learning by breaking the replay ratio
  barrier.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=OpC-9aBBVJe}.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, et~al.]{espeholt2018impala}
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron,
  Y., Firoiu, V., Harley, T., Dunning, I., et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In \emph{International conference on machine learning}, pp.\
  1407--1416. PMLR, 2018.

\bibitem[Evci et~al.(2020)Evci, Gale, Menick, Castro, and
  Elsen]{evci2020rigging}
Evci, U., Gale, T., Menick, J., Castro, P.~S., and Elsen, E.
\newblock Rigging the lottery: Making all tickets winners.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2943--2952. PMLR, 2020.

\bibitem[Farebrother et~al.(2023)Farebrother, Greaves, Agarwal, Lan, Goroshin,
  Castro, and Bellemare]{farebrother2023protovalue}
Farebrother, J., Greaves, J., Agarwal, R., Lan, C.~L., Goroshin, R., Castro,
  P.~S., and Bellemare, M.~G.
\newblock Proto-value networks: Scaling representation learning with auxiliary
  tasks.
\newblock In \emph{Submitted to The Eleventh International Conference on
  Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=oGDKSt9JrZi}.
\newblock under review.

\bibitem[Farebrother et~al.(2024)Farebrother, Orbay, Vuong, Ta{\"i}ga,
  Chebotar, Xiao, Irpan, Levine, Castro, Faust, Kumar, and
  Agarwal]{farebrother24classification}
Farebrother, J., Orbay, J., Vuong, Q., Ta{\"i}ga, A.~A., Chebotar, Y., Xiao,
  T., Irpan, A., Levine, S., Castro, P.~S., Faust, A., Kumar, A., and Agarwal,
  R.
\newblock Stop regressing: Training value functions via classification for
  scalable deep rl.
\newblock In \emph{Forty-first International Conference on Machine Learning}.
  PMLR, 2024.

\bibitem[Fawzi et~al.(2022)Fawzi, Balog, Huang, Hubert, Romera-Paredes,
  Barekatain, Novikov, R~Ruiz, Schrittwieser, Swirszcz,
  et~al.]{fawzi2022discovering}
Fawzi, A., Balog, M., Huang, A., Hubert, T., Romera-Paredes, B., Barekatain,
  M., Novikov, A., R~Ruiz, F.~J., Schrittwieser, J., Swirszcz, G., et~al.
\newblock Discovering faster matrix multiplication algorithms with
  reinforcement learning.
\newblock \emph{Nature}, 610\penalty0 (7930):\penalty0 47--53, 2022.

\bibitem[Fedus et~al.(2020)Fedus, Ramachandran, Agarwal, Bengio, Larochelle,
  Rowland, and Dabney]{fedus2020revisiting}
Fedus, W., Ramachandran, P., Agarwal, R., Bengio, Y., Larochelle, H., Rowland,
  M., and Dabney, W.
\newblock Revisiting fundamentals of experience replay.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3061--3071. PMLR, 2020.

\bibitem[Fortunato et~al.(2018)Fortunato, Azar, Piot, Menick, Osband, Graves,
  Mnih, Munos, Hassabis, Pietquin, Blundell, and Legg]{fortunato18noisy}
Fortunato, M., Azar, M.~G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih,
  V., Munos, R., Hassabis, D., Pietquin, O., Blundell, C., and Legg, S.
\newblock Noisy networks for exploration.
\newblock 2018.

\bibitem[Frankle \& Carbin(2018)Frankle and Carbin]{frankle2018lottery}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{gal2016dropout}
Gal, Y. and Ghahramani, Z.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{international conference on machine learning}, pp.\
  1050--1059. PMLR, 2016.

\bibitem[Gale et~al.(2019)Gale, Elsen, and Hooker]{gale2019state}
Gale, T., Elsen, E., and Hooker, S.
\newblock The state of sparsity in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1902.09574}, 2019.

\bibitem[Graesser et~al.(2022)Graesser, Evci, Elsen, and
  Castro]{graesser2022state}
Graesser, L., Evci, U., Elsen, E., and Castro, P.~S.
\newblock The state of sparse training in deep reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7766--7792. PMLR, 2022.

\bibitem[Grooten et~al.(2023)Grooten, Sokar, Dohare, Mocanu, Taylor,
  Pechenizkiy, and Mocanu]{grooteautomatic23}
Grooten, B., Sokar, G., Dohare, S., Mocanu, E., Taylor, M.~E., Pechenizkiy, M.,
  and Mocanu, D.~C.
\newblock Automatic noise filtering with dynamic sparse training in deep
  reinforcement learning.
\newblock In \emph{Proceedings of the 2023 International Conference on
  Autonomous Agents and Multiagent Systems}, AAMAS '23, pp.\  1932–1941,
  Richland, SC, 2023. International Foundation for Autonomous Agents and
  Multiagent Systems.
\newblock ISBN 9781450394321.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pp.\
  1861--1870. PMLR, 2018.

\bibitem[Hafner et~al.(2023)Hafner, Pasukonis, Ba, and
  Lillicrap]{hafner2023dreamerv3}
Hafner, D., Pasukonis, J., Ba, J., and Lillicrap, T.
\newblock Mastering diverse domains through world models.
\newblock \emph{arXiv preprint arXiv:2301.04104}, 2023.

\bibitem[Han et~al.(2015)Han, Mao, and Dally]{han2015deep}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}, 2015.

\bibitem[Harris et~al.(2020)Harris, Millman, Van Der~Walt, Gommers, Virtanen,
  Cournapeau, Wieser, Taylor, Berg, Smith, et~al.]{harris2020array}
Harris, C.~R., Millman, K.~J., Van Der~Walt, S.~J., Gommers, R., Virtanen, P.,
  Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N.~J., et~al.
\newblock Array programming with numpy.
\newblock \emph{Nature}, 585\penalty0 (7825):\penalty0 357--362, 2020.

\bibitem[Hessel et~al.(2018)Hessel, Modayil, Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{Hessel2018RainbowCI}
Hessel, M., Modayil, J., Hasselt, H.~V., Schaul, T., Ostrovski, G., Dabney, W.,
  Horgan, D., Piot, B., Azar, M.~G., and Silver, D.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In \emph{AAAI}, 2018.

\bibitem[Hiraoka et~al.(2021)Hiraoka, Imagawa, Hashimoto, Onishi, and
  Tsuruoka]{hiraoka2021dropout}
Hiraoka, T., Imagawa, T., Hashimoto, T., Onishi, T., and Tsuruoka, Y.
\newblock Dropout q-functions for doubly efficient reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Hunter(2007)]{hunter2007matplotlib}
Hunter, J.~D.
\newblock Matplotlib: A 2d graphics environment.
\newblock \emph{Computing in science \& engineering}, 9\penalty0 (03):\penalty0
  90--95, 2007.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, pp.\
  448--456. pmlr, 2015.

\bibitem[Kaiser et~al.(2020)Kaiser, Babaeizadeh, Milos, Osinski, Campbell,
  Czechowski, Erhan, Finn, Kozakowski, Levine, et~al.]{Kaiser2020Model}
Kaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R.~H.,
  Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., et~al.
\newblock Model-based reinforcement learning for atari.
\newblock \emph{International Conference on Learning Representations}, 2020.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[{Kluyver} et~al.(2016){Kluyver}, {Ragan-Kelley}, {P{\'e}rez},
  {Granger}, {Bussonnier}, {Frederic}, {Kelley}, {Hamrick}, {Grout}, {Corlay},
  {Ivanov}, {Avila}, {Abdalla}, {Willing}, and {Jupyter Development
  Team}]{2016ppap}
{Kluyver}, T., {Ragan-Kelley}, B., {P{\'e}rez}, F., {Granger}, B.,
  {Bussonnier}, M., {Frederic}, J., {Kelley}, K., {Hamrick}, J., {Grout}, J.,
  {Corlay}, S., {Ivanov}, P., {Avila}, D., {Abdalla}, S., {Willing}, C., and
  {Jupyter Development Team}.
\newblock {Jupyter Notebooks{\textemdash}a publishing format for reproducible
  computational workflows}.
\newblock In \emph{IOS Press}, pp.\  87--90. 2016.
\newblock \doi{10.3233/978-1-61499-649-1-87}.

\bibitem[Kostrikov et~al.(2020)Kostrikov, Yarats, and
  Fergus]{kostrikov2020image}
Kostrikov, I., Yarats, D., and Fergus, R.
\newblock Image augmentation is all you need: Regularizing deep reinforcement
  learning from pixels.
\newblock \emph{arXiv preprint arXiv:2004.13649}, 2020.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1179--1191, 2020.

\bibitem[Kumar et~al.(2021{\natexlab{a}})Kumar, Agarwal, Ghosh, and
  Levine]{kumar2021implicit}
Kumar, A., Agarwal, R., Ghosh, D., and Levine, S.
\newblock Implicit under-parameterization inhibits data-efficient deep
  reinforcement learning.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=O9bnihsFfXU}.

\bibitem[Kumar et~al.(2021{\natexlab{b}})Kumar, Agarwal, Ma, Courville, Tucker,
  and Levine]{kumar2021dr3}
Kumar, A., Agarwal, R., Ma, T., Courville, A., Tucker, G., and Levine, S.
\newblock Dr3: Value-based deep reinforcement learning requires explicit
  regularization.
\newblock \emph{arXiv preprint arXiv:2112.04716}, 2021{\natexlab{b}}.

\bibitem[Kumar et~al.(2022)Kumar, Agarwal, Geng, Tucker, and
  Levine]{kumar2022offline}
Kumar, A., Agarwal, R., Geng, X., Tucker, G., and Levine, S.
\newblock Offline q-learning on diverse multi-task data both scales and
  generalizes.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2022.

\bibitem[Lee et~al.(2023)Lee, Cho, Kim, Gwak, Kim, Choo, Yun, and
  Yun]{lee2023plastic}
Lee, H., Cho, H., Kim, H., Gwak, D., Kim, J., Choo, J., Yun, S.-Y., and Yun, C.
\newblock Plastic: Improving input and label plasticity for sample efficient
  reinforcement learning.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.

\bibitem[Lee et~al.(2024)Lee, Park, Mitchell, Pilault, Ceron, Kim, Lee,
  Frantar, Long, Yazdanbakhsh, et~al.]{lee2024jaxpruner}
Lee, J.~H., Park, W., Mitchell, N.~E., Pilault, J., Ceron, J. S.~O., Kim,
  H.-B., Lee, N., Frantar, E., Long, Y., Yazdanbakhsh, A., et~al.
\newblock Jaxpruner: A concise library for sparsity research.
\newblock In \emph{Conference on Parsimony and Learning}, pp.\  515--528. PMLR,
  2024.

\bibitem[Lewandowski et~al.(2023)Lewandowski, Tanaka, Schuurmans, and
  Machado]{lewandowski2023curvature}
Lewandowski, A., Tanaka, H., Schuurmans, D., and Machado, M.~C.
\newblock Curvature explains loss of plasticity.
\newblock \emph{arXiv preprint arXiv:2312.00246}, 2023.

\bibitem[Liu et~al.(2020)Liu, Li, Kang, and Darrell]{liu2020regularization}
Liu, Z., Li, X., Kang, B., and Darrell, T.
\newblock Regularization matters in policy optimization-an empirical study on
  continuous control.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Livne \& Cohen(2020)Livne and Cohen]{Livne_2020}
Livne, D. and Cohen, K.
\newblock Pops: Policy pruning and shrinking for deep reinforcement learning.
\newblock \emph{IEEE Journal of Selected Topics in Signal Processing},
  14\penalty0 (4):\penalty0 789–801, May 2020.
\newblock ISSN 1941-0484.
\newblock \doi{10.1109/jstsp.2020.2967566}.
\newblock URL \url{http://dx.doi.org/10.1109/JSTSP.2020.2967566}.

\bibitem[Lyle et~al.(2022)Lyle, Rowland, and Dabney]{lyle2022understanding}
Lyle, C., Rowland, M., and Dabney, W.
\newblock Understanding and preventing capacity loss in reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=ZkC8wKoLbQ7}.

\bibitem[Lyle et~al.(2023)Lyle, Zheng, Nikishin, Pires, Pascanu, and
  Dabney]{clare2023understanding}
Lyle, C., Zheng, Z., Nikishin, E., Pires, B.~A., Pascanu, R., and Dabney, W.
\newblock Understanding plasticity in neural networks.
\newblock In \emph{Proceedings of the 40th International Conference on Machine
  Learning}, ICML'23. JMLR.org, 2023.

\bibitem[Lyle et~al.(2024)Lyle, Zheng, Khetarpal, van Hasselt, Pascanu,
  Martens, and Dabney]{lyle2024disentangling}
Lyle, C., Zheng, Z., Khetarpal, K., van Hasselt, H., Pascanu, R., Martens, J.,
  and Dabney, W.
\newblock Disentangling the causes of plasticity loss in neural networks.
\newblock \emph{arXiv preprint arXiv:2402.18762}, 2024.

\bibitem[McKinney(2013)]{McKinney2013Python}
McKinney, W.
\newblock \emph{Python for Data Analysis: Data Wrangling with Pandas, {NumPy},
  and {IPython}}.
\newblock O'Reilly Media, 1 edition, February 2013.
\newblock ISBN 9789351100065.
\newblock URL
  \url{http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/1449319793}.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{mnih2015humanlevel}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., Petersen,
  S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra,
  D., Legg, S., and Hassabis, D.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, February 2015.

\bibitem[Mocanu et~al.(2018)Mocanu, Mocanu, Stone, Nguyen, Gibescu, and
  Liotta]{mocanu2018scalable}
Mocanu, D.~C., Mocanu, E., Stone, P., Nguyen, P.~H., Gibescu, M., and Liotta,
  A.
\newblock Scalable training of artificial neural networks with adaptive sparse
  connectivity inspired by network science.
\newblock \emph{Nature communications}, 9\penalty0 (1):\penalty0 2383, 2018.

\bibitem[Nikishin et~al.(2022)Nikishin, Schwarzer, D'Oro, Bacon, and
  Courville]{nikishin22primacy}
Nikishin, E., Schwarzer, M., D'Oro, P., Bacon, P.-L., and Courville, A.
\newblock The primacy bias in deep reinforcement learning.
\newblock In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and
  Sabato, S. (eds.), \emph{Proceedings of the 39th International Conference on
  Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning
  Research}, pp.\  16828--16847. PMLR, 17--23 Jul 2022.

\bibitem[Nikishin et~al.(2023)Nikishin, Oh, Ostrovski, Lyle, Pascanu, Dabney,
  and Barreto]{nikishin2023deep}
Nikishin, E., Oh, J., Ostrovski, G., Lyle, C., Pascanu, R., Dabney, W., and
  Barreto, A.
\newblock Deep reinforcement learning with plasticity injection.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=jucDLW6G9l}.

\bibitem[Oliphant(2007)]{4160250}
Oliphant, T.~E.
\newblock Python for scientific computing.
\newblock \emph{Computing in Science \& Engineering}, 9\penalty0 (3):\penalty0
  10--20, 2007.
\newblock \doi{10.1109/MCSE.2007.58}.

\bibitem[Ostrovski et~al.(2021)Ostrovski, Castro, and
  Dabney]{ostrovski2021tandem}
Ostrovski, G., Castro, P.~S., and Dabney, W.
\newblock The difficulty of passive learning in deep reinforcement learning.
\newblock In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W.
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=nPHA8fGicZk}.

\bibitem[Ota et~al.(2021)Ota, Jha, and Kanezaki]{ota2021training}
Ota, K., Jha, D.~K., and Kanezaki, A.
\newblock Training larger networks for deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2102.07920}, 2021.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and
  Silver]{Schaul2016PrioritizedER}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock \emph{CoRR}, abs/1511.05952, 2016.

\bibitem[Schmidt \& Schmied(2021)Schmidt and Schmied]{schmidt2021fast}
Schmidt, D. and Schmied, T.
\newblock Fast and data-efficient training of rainbow: an experimental study on
  atari.
\newblock \emph{arXiv preprint arXiv:2111.10247}, 2021.

\bibitem[Schmitt et~al.(2018)Schmitt, Hudson, Zidek, Osindero, Doersch,
  Czarnecki, Leibo, Kuttler, Zisserman, Simonyan,
  et~al.]{schmitt2018kickstarting}
Schmitt, S., Hudson, J.~J., Zidek, A., Osindero, S., Doersch, C., Czarnecki,
  W.~M., Leibo, J.~Z., Kuttler, H., Zisserman, A., Simonyan, K., et~al.
\newblock Kickstarting deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1803.03835}, 2018.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Schwarzer et~al.(2023)Schwarzer, Ceron, Courville, Bellemare, Agarwal,
  and Castro]{schwarzer2023bigger}
Schwarzer, M., Ceron, J. S.~O., Courville, A., Bellemare, M.~G., Agarwal, R.,
  and Castro, P.~S.
\newblock Bigger, better, faster: Human-level atari with human-level
  efficiency.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  30365--30380. PMLR, 2023.

\bibitem[Sinha et~al.(2020)Sinha, Bharadhwaj, Srinivas, and
  Garg]{sinha2020d2rl}
Sinha, S., Bharadhwaj, H., Srinivas, A., and Garg, A.
\newblock D2rl: Deep dense architectures in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2010.09163}, 2020.

\bibitem[Sokar et~al.(2021)Sokar, Mocanu, Mocanu, Pechenizkiy, and
  Stone]{sokar2021dynamic}
Sokar, G., Mocanu, E., Mocanu, D.~C., Pechenizkiy, M., and Stone, P.
\newblock Dynamic sparse training for deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2106.04217}, 2021.

\bibitem[Sokar et~al.(2023)Sokar, Agarwal, Castro, and Evci]{sokar2023dormant}
Sokar, G., Agarwal, R., Castro, P.~S., and Evci, U.
\newblock The dormant neuron phenomenon in deep reinforcement learning.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,
  and Scarlett, J. (eds.), \emph{Proceedings of the 40th International
  Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine
  Learning Research}, pp.\  32145--32168. PMLR, 23--29 Jul 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/sokar23a.html}.

\bibitem[Song et~al.(2019)Song, Jiang, Tu, Du, and
  Neyshabur]{song2019observational}
Song, X., Jiang, Y., Tu, S., Du, Y., and Neyshabur, B.
\newblock Observational overfitting in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.02975}, 2019.

\bibitem[Sutton(1988)]{sutton88learning}
Sutton, R.~S.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine Learning}, 3\penalty0 (1):\penalty0 9–44, August
  1988.

\bibitem[Taiga et~al.(2023)Taiga, Agarwal, Farebrother, Courville, and
  Bellemare]{taiga2023investigating}
Taiga, A.~A., Agarwal, R., Farebrother, J., Courville, A., and Bellemare, M.~G.
\newblock Investigating multi-task pretraining and generalization in
  reinforcement learning.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Tan et~al.(2023)Tan, Hu, Pan, Huang, and Huang]{tan2023rlx}
Tan, Y., Hu, P., Pan, L., Huang, J., and Huang, L.
\newblock {RL}x2: Training a sparse deep reinforcement learning model from
  scratch.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=DJEEqoAq7to}.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ international conference on intelligent robots
  and systems}, pp.\  5026--5033. IEEE, 2012.

\bibitem[van Hasselt et~al.(2016)van Hasselt, Guez, and
  Silver]{hasselt2015doubledqn}
van Hasselt, H., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{Proceedings of the Thirthieth AAAI Conference On Artificial
  Intelligence (AAAI), 2016}, 2016.
\newblock cite arxiv:1509.06461Comment: AAAI 2016.

\bibitem[Van~Hasselt et~al.(2018)Van~Hasselt, Doron, Strub, Hessel, Sonnerat,
  and Modayil]{van2018deep}
Van~Hasselt, H., Doron, Y., Strub, F., Hessel, M., Sonnerat, N., and Modayil,
  J.
\newblock Deep reinforcement learning and the deadly triad.
\newblock \emph{arXiv preprint arXiv:1812.02648}, 2018.

\bibitem[Van~Hasselt et~al.(2019)Van~Hasselt, Hessel, and
  Aslanides]{van2019use}
Van~Hasselt, H.~P., Hessel, M., and Aslanides, J.
\newblock When to use parametric models in reinforcement learning?
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Van~Rossum \& Drake~Jr(1995)Van~Rossum and Drake~Jr]{van1995python}
Van~Rossum, G. and Drake~Jr, F.~L.
\newblock \emph{Python reference manual}.
\newblock Centrum voor Wiskunde en Informatica Amsterdam, 1995.

\bibitem[Vieillard et~al.(2020)Vieillard, Pietquin, and
  Geist]{vieillard2020munchausen}
Vieillard, N., Pietquin, O., and Geist, M.
\newblock Munchausen reinforcement learning.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  4235--4246. Curran Associates, Inc., 2020.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Vischer et~al.(2021)Vischer, Lange, and Sprekeler]{vischer2021lottery}
Vischer, M., Lange, R.~T., and Sprekeler, H.
\newblock On lottery tickets and minimal task representations in deep
  reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Wang et~al.(2016)Wang, Schaul, Hessel, Hasselt, Lanctot, and
  Freitas]{wang16dueling}
Wang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., and Freitas, N.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning}, volume~48, pp.\  1995--2003, 2016.

\bibitem[Yarats et~al.(2021)Yarats, Fergus, and Kostrikov]{yarats2021image}
Yarats, D., Fergus, R., and Kostrikov, I.
\newblock Image augmentation is all you need: Regularizing deep reinforcement
  learning from pixels.
\newblock In \emph{9th International Conference on Learning Representations,
  ICLR 2021}, 2021.

\bibitem[Yu et~al.(2019)Yu, Edunov, Tian, and Morcos]{yu2019playing}
Yu, H., Edunov, S., Tian, Y., and Morcos, A.~S.
\newblock Playing the lottery with rewards and multiple languages: lottery
  tickets in rl and nlp.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Zhang et~al.(2018)Zhang, Vinyals, Munos, and Bengio]{zhang2018study}
Zhang, C., Vinyals, O., Munos, R., and Bengio, S.
\newblock A study on overfitting in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1804.06893}, 2018.

\bibitem[Zhang et~al.(2019)Zhang, He, and Li]{zhang2019accelerating}
Zhang, H., He, Z., and Li, J.
\newblock Accelerating the deep reinforcement learning with neural network
  compression.
\newblock In \emph{2019 International Joint Conference on Neural Networks
  (IJCNN)}, pp.\  1--8. IEEE, 2019.

\bibitem[Zhu \& Gupta(2017)Zhu and Gupta]{zhu2017prune}
Zhu, M. and Gupta, S.
\newblock To prune, or not to prune: exploring the efficacy of pruning for
  model compression, 2017.

\end{thebibliography}
