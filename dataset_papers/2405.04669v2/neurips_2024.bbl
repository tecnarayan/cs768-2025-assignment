\begin{thebibliography}{65}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Reynolds and McDonell(2021)]{reynolds2021prompt}
Laria Reynolds and Kyle McDonell.
\newblock Prompt programming for large language models: Beyond the few-shot paradigm.
\newblock In \emph{Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems}, pages 1--7, 2021.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and Iwasawa]{kojima2022large}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 22199--22213, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 24824--24837, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou]{wang2022self}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock \emph{arXiv preprint arXiv:2203.11171}, 2022.

\bibitem[Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber, Dohan, Lewkowycz, Bosma, Luan, et~al.]{nye2021show}
Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et~al.
\newblock Show your work: Scratchpads for intermediate computation with language models.
\newblock \emph{arXiv preprint arXiv:2112.00114}, 2021.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Zelikman et~al.(2022)Zelikman, Wu, Mu, and Goodman]{zelikman2022star}
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman.
\newblock Star: Bootstrapping reasoning with reasoning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 15476--15488, 2022.

\bibitem[Allen-Zhu and Li(2023)]{allen2023physics}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Physics of language models: Part 3.2, knowledge manipulation.
\newblock \emph{arXiv preprint arXiv:2309.14402}, 2023.

\bibitem[Berglund et~al.(2023)Berglund, Tong, Kaufmann, Balesni, Stickland, Korbak, and Evans]{berglund2023reversal}
Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa~Cooper Stickland, Tomasz Korbak, and Owain Evans.
\newblock The reversal curse: Llms trained on" a is b" fail to learn" b is a".
\newblock \emph{arXiv preprint arXiv:2309.12288}, 2023.

\bibitem[Guo et~al.(2024)Guo, Wang, Guo, Tan, Bian, and Yang]{guo2024mitigating}
Qingyan Guo, Rui Wang, Junliang Guo, Xu~Tan, Jiang Bian, and Yujiu Yang.
\newblock Mitigating reversal curse via semantic-aware permutation training.
\newblock \emph{arXiv preprint arXiv:2403.00758}, 2024.

\bibitem[Golovneva et~al.(2024)Golovneva, Allen-Zhu, Weston, and Sukhbaatar]{golovneva2024reverse}
Olga Golovneva, Zeyuan Allen-Zhu, Jason Weston, and Sainbayar Sukhbaatar.
\newblock Reverse training to nurse the reversal curse.
\newblock \emph{arXiv preprint arXiv:2403.13799}, 2024.

\bibitem[Lv et~al.(2023)Lv, Zhang, Xie, Tu, Chen, Wen, and Yan]{lv2023we}
Ang Lv, Kaiyi Zhang, Shufang Xie, Quan Tu, Yuhan Chen, Ji-Rong Wen, and Rui Yan.
\newblock Are we falling in a middle-intelligence trap? an analysis and mitigation of the reversal curse.
\newblock \emph{arXiv preprint arXiv:2311.07468}, 2023.

\bibitem[Tian et~al.(2023{\natexlab{a}})Tian, Wang, Chen, and Du]{tian2023scan}
Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du.
\newblock Scan and snap: Understanding training dynamics and token composition in 1-layer transformer, 2023{\natexlab{a}}.

\bibitem[Feng et~al.(2024)Feng, Zhang, Gu, Ye, He, and Wang]{feng2024towards}
Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di~He, and Liwei Wang.
\newblock Towards revealing the mystery behind chain of thought: a theoretical perspective.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Huang et~al.(2022)Huang, Abbeel, Pathak, and Mordatch]{huang2022language}
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
\newblock Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.
\newblock In \emph{International Conference on Machine Learning}, pages 9118--9147. PMLR, 2022.

\bibitem[Jung et~al.(2022)Jung, Qin, Welleck, Brahman, Bhagavatula, Bras, and Choi]{jung2022maieutic}
Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan~Le Bras, and Yejin Choi.
\newblock Maieutic prompting: Logically consistent reasoning with recursive explanations.
\newblock \emph{arXiv preprint arXiv:2205.11822}, 2022.

\bibitem[Han et~al.(2024)Han, Ransom, Perfors, and Kemp]{han2024inductive}
Simon~Jerome Han, Keith~J Ransom, Andrew Perfors, and Charles Kemp.
\newblock Inductive reasoning in humans and large language models.
\newblock \emph{Cognitive Systems Research}, 83:\penalty0 101155, 2024.

\bibitem[Wang et~al.(2024)Wang, Amayuelas, Zhang, Pan, Chen, and Wang]{wang2024understanding}
Xinyi Wang, Alfonso Amayuelas, Kexun Zhang, Liangming Pan, Wenhu Chen, and William~Yang Wang.
\newblock Understanding the reasoning ability of language models from the perspective of reasoning paths aggregation.
\newblock \emph{arXiv preprint arXiv:2402.03268}, 2024.

\bibitem[Xie et~al.(2021)Xie, Raghunathan, Liang, and Ma]{xie2021explanation}
Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock \emph{arXiv preprint arXiv:2111.02080}, 2021.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan, Mann, Askell, Bai, Chen, et~al.]{olsson2022context}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et~al.
\newblock In-context learning and induction heads.
\newblock \emph{arXiv preprint arXiv:2209.11895}, 2022.

\bibitem[Nichani et~al.(2024)Nichani, Damian, and Lee]{nichani2024transformers}
Eshaan Nichani, Alex Damian, and Jason~D Lee.
\newblock How transformers learn causal structure with gradient descent.
\newblock \emph{arXiv preprint arXiv:2402.14735}, 2024.

\bibitem[Brinkmann et~al.(2024)Brinkmann, Sheshadri, Levoso, Swoboda, and Bartelt]{brinkmann2024mechanistic}
Jannik Brinkmann, Abhay Sheshadri, Victor Levoso, Paul Swoboda, and Christian Bartelt.
\newblock A mechanistic analysis of a transformer trained on a symbolic multi-step reasoning task.
\newblock \emph{arXiv preprint arXiv:2402.11917}, 2024.

\bibitem[Valmeekam et~al.(2022)Valmeekam, Olmo, Sreedharan, and Kambhampati]{valmeekam2022large}
Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati.
\newblock Large language models still can't plan (a benchmark for llms on planning and reasoning about change).
\newblock \emph{arXiv preprint arXiv:2206.10498}, 2022.

\bibitem[Chang et~al.(2023)Chang, Wang, Wang, Wu, Yang, Zhu, Chen, Yi, Wang, Wang, et~al.]{chang2023survey}
Yupeng Chang, Xu~Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et~al.
\newblock A survey on evaluation of large language models.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology}, 2023.

\bibitem[Zhang et~al.(2024)Zhang, Tigges, Zhang, Biderman, Raginsky, and Ringer]{zhang2024transformer}
Dylan Zhang, Curt Tigges, Zory Zhang, Stella Biderman, Maxim Raginsky, and Talia Ringer.
\newblock Transformer-based models are not yet perfect at learning to emulate structural recursion.
\newblock \emph{arXiv preprint arXiv:2401.12947}, 2024.

\bibitem[Qi et~al.(2023)Qi, Li, Hui, Wang, Li, Wu, and Laili]{qi2023investigation}
Chengwen Qi, Bowen Li, Binyuan Hui, Bailin Wang, Jinyang Li, Jinwang Wu, and Yuanjun Laili.
\newblock An investigation of llms' inefficacy in understanding converse relations.
\newblock \emph{arXiv preprint arXiv:2310.05163}, 2023.

\bibitem[Luo et~al.(2024)Luo, Gu, Li, Li, Lin, Li, and Yang]{luo2024chain}
Ruilin Luo, Tianle Gu, Haoling Li, Junzhe Li, Zicheng Lin, Jiayi Li, and Yujiu Yang.
\newblock Chain of history: Learning and forecasting with llms for temporal knowledge graph completion.
\newblock \emph{arXiv preprint arXiv:2401.06072}, 2024.

\bibitem[Yun et~al.(2019)Yun, Bhojanapalli, Rawat, Reddi, and Kumar]{yun2019transformers}
Chulhee Yun, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank~J Reddi, and Sanjiv Kumar.
\newblock Are transformers universal approximators of sequence-to-sequence functions?
\newblock \emph{arXiv preprint arXiv:1912.10077}, 2019.

\bibitem[Bhattamishra et~al.(2020{\natexlab{a}})Bhattamishra, Ahuja, and Goyal]{bhattamishra2020ability}
Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal.
\newblock On the ability and limitations of transformers to recognize formal languages.
\newblock \emph{arXiv preprint arXiv:2009.11264}, 2020{\natexlab{a}}.

\bibitem[Bhattamishra et~al.(2020{\natexlab{b}})Bhattamishra, Patel, and Goyal]{bhattamishra2020computational}
Satwik Bhattamishra, Arkil Patel, and Navin Goyal.
\newblock On the computational power of transformers and its implications in sequence modeling.
\newblock \emph{arXiv preprint arXiv:2006.09286}, 2020{\natexlab{b}}.

\bibitem[Dehghani et~al.(2018)Dehghani, Gouws, Vinyals, Uszkoreit, and Kaiser]{dehghani2018universal}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and {\L}ukasz Kaiser.
\newblock Universal transformers.
\newblock \emph{arXiv preprint arXiv:1807.03819}, 2018.

\bibitem[P{\'e}rez et~al.(2021)P{\'e}rez, Barcel{\'o}, and Marinkovic]{perez2021attention}
Jorge P{\'e}rez, Pablo Barcel{\'o}, and Javier Marinkovic.
\newblock Attention is turing-complete.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0 (75):\penalty0 1--35, 2021.

\bibitem[Edelman et~al.(2022)Edelman, Goel, Kakade, and Zhang]{edelman2022inductive}
Benjamin~L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang.
\newblock Inductive biases and variable creation in self-attention mechanisms.
\newblock In \emph{International Conference on Machine Learning}, pages 5793--5831. PMLR, 2022.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann, Askell, Bai, Chen, Conerly, et~al.]{elhage2021mathematical}
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et~al.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 1:\penalty0 1, 2021.

\bibitem[Likhosherstov et~al.(2021)Likhosherstov, Choromanski, and Weller]{likhosherstov2021expressive}
Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller.
\newblock On the expressive power of self-attention matrices.
\newblock \emph{arXiv preprint arXiv:2106.03764}, 2021.

\bibitem[Aky{\"u}rek et~al.(2022)Aky{\"u}rek, Schuurmans, Andreas, Ma, and Zhou]{akyurek2022learning}
Ekin Aky{\"u}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
\newblock What learning algorithm is in-context learning? investigations with linear models.
\newblock \emph{arXiv preprint arXiv:2211.15661}, 2022.

\bibitem[Zhao et~al.(2023)Zhao, Panigrahi, Ge, and Arora]{zhao2023transformers}
Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora.
\newblock Do transformers parse while predicting the masked word?
\newblock \emph{arXiv preprint arXiv:2303.08117}, 2023.

\bibitem[Yao et~al.(2021)Yao, Peng, Papadimitriou, and Narasimhan]{yao2021self}
Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan.
\newblock Self-attention networks can process bounded hierarchical languages.
\newblock \emph{arXiv preprint arXiv:2105.11115}, 2021.

\bibitem[Anil et~al.(2022)Anil, Wu, Andreassen, Lewkowycz, Misra, Ramasesh, Slone, Gur-Ari, Dyer, and Neyshabur]{anil2022exploring}
Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur.
\newblock Exploring length generalization in large language models.
\newblock \emph{arXiv preprint arXiv:2207.04901}, 2022.

\bibitem[Barak et~al.(2022)Barak, Edelman, Goel, Kakade, Malach, and Zhang]{barak2022hidden}
Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang.
\newblock Hidden progress in deep learning: Sgd learns parities near the computational limit.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 21750--21764, 2022.

\bibitem[Garg et~al.(2022)Garg, Tsipras, Liang, and Valiant]{garg2022can}
Shivam Garg, Dimitris Tsipras, Percy~S Liang, and Gregory Valiant.
\newblock What can transformers learn in-context? a case study of simple function classes.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 30583--30598, 2022.

\bibitem[Von~Oswald et~al.(2022)Von~Oswald, Niklasson, Randazzo, Sacramento, Mordvintsev, Zhmoginov, and Vladymyrov]{von2022transformers}
Johannes Von~Oswald, Eyvind Niklasson, Ettore Randazzo, Jo{\~a}o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock \emph{arXiv preprint arXiv:2212.07677}, 2022.

\bibitem[Bai et~al.(2023)Bai, Chen, Wang, Xiong, and Mei]{bai2023transformers}
Yu~Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei.
\newblock Transformers as statisticians: Provable in-context learning with in-context algorithm selection.
\newblock \emph{arXiv preprint arXiv:2306.04637}, 2023.

\bibitem[Li et~al.(2023)Li, Song, Xia, Yu, and Zhou]{li2023closeness}
Shuai Li, Zhao Song, Yu~Xia, Tong Yu, and Tianyi Zhou.
\newblock The closeness of in-context learning and weight shifting for softmax regression.
\newblock \emph{arXiv preprint arXiv:2304.13276}, 2023.

\bibitem[Von~Oswald et~al.(2023)Von~Oswald, Niklasson, Randazzo, Sacramento, Mordvintsev, Zhmoginov, and Vladymyrov]{von2023transformers}
Johannes Von~Oswald, Eyvind Niklasson, Ettore Randazzo, Jo{\~a}o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages 35151--35174. PMLR, 2023.

\bibitem[Liu et~al.(2022)Liu, Ash, Goel, Krishnamurthy, and Zhang]{liu2022transformers}
Bingbin Liu, Jordan~T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang.
\newblock Transformers learn shortcuts to automata.
\newblock \emph{arXiv preprint arXiv:2210.10749}, 2022.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Chen, and Ma]{wei2022statistically}
Colin Wei, Yining Chen, and Tengyu Ma.
\newblock Statistically meaningful approximation: a case study on approximating turing machines with transformers.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 12071--12083, 2022{\natexlab{b}}.

\bibitem[Mei and Wu(2023)]{mei2023deep}
Song Mei and Yuchen Wu.
\newblock Deep networks as denoising algorithms: Sample-efficient learning of diffusion models in high-dimensional graphical models.
\newblock \emph{arXiv preprint arXiv:2309.11420}, 2023.

\bibitem[Lin et~al.(2023)Lin, Bai, and Mei]{lin2023transformers}
Licong Lin, Yu~Bai, and Song Mei.
\newblock Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining.
\newblock \emph{arXiv preprint arXiv:2310.08566}, 2023.

\bibitem[Zhang et~al.(2020)Zhang, He, Sra, and Jadbabaie]{zhang2020why}
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie.
\newblock Why gradient clipping accelerates training: A theoretical justification for adaptivity.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Hron et~al.(2020)Hron, Bahri, Sohl-Dickstein, and Novak]{hron2020infinite}
Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak.
\newblock Infinite attention: Nngp and ntk for deep attention networks.
\newblock In \emph{International Conference on Machine Learning}, pages 4376--4386. PMLR, 2020.

\bibitem[Yang et~al.(2022)Yang, Hu, Babuschkin, Sidor, Liu, Farhi, Ryder, Pachocki, Chen, and Gao]{yang2022tensor}
Greg Yang, Edward~J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao.
\newblock Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer.
\newblock \emph{arXiv preprint arXiv:2203.03466}, 2022.

\bibitem[Boix-Adsera et~al.(2023)Boix-Adsera, Littwin, Abbe, Bengio, and Susskind]{boix2023transformers}
Enric Boix-Adsera, Etai Littwin, Emmanuel Abbe, Samy Bengio, and Joshua Susskind.
\newblock Transformers learn through gradual rank increase.
\newblock \emph{arXiv preprint arXiv:2306.07042}, 2023.

\bibitem[Bietti et~al.(2023)Bietti, Cabannes, Bouchacourt, Jegou, and Bottou]{bietti2023birth}
Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou.
\newblock Birth of a transformer: A memory viewpoint.
\newblock \emph{arXiv preprint arXiv:2306.00802}, 2023.

\bibitem[Jelassi et~al.(2022)Jelassi, Sander, and Li]{jelassi2022vision}
Samy Jelassi, Michael Sander, and Yuanzhi Li.
\newblock Vision transformers provably learn spatial structure.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 37822--37836, 2022.

\bibitem[Snell et~al.(2021)Snell, Zhong, Klein, and Steinhardt]{snell2021approximating}
Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt.
\newblock Approximating how single head attention learns.
\newblock \emph{arXiv preprint arXiv:2103.07601}, 2021.

\bibitem[Mahankali et~al.(2023)Mahankali, Hashimoto, and Ma]{mahankali2023one}
Arvind Mahankali, Tatsunori~B Hashimoto, and Tengyu Ma.
\newblock One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention.
\newblock \emph{arXiv preprint arXiv:2307.03576}, 2023.

\bibitem[Zhang et~al.(2023)Zhang, Frei, and Bartlett]{zhang2023trained}
Ruiqi Zhang, Spencer Frei, and Peter~L Bartlett.
\newblock Trained transformers learn linear models in-context.
\newblock \emph{arXiv preprint arXiv:2306.09927}, 2023.

\bibitem[Fu et~al.(2024)Fu, Guo, Bai, and Mei]{fu2024can}
Hengyu Fu, Tianyu Guo, Yu~Bai, and Song Mei.
\newblock What can a single attention layer learn? a study through the random features lens.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Huang et~al.(2023)Huang, Cheng, and Liang]{huang2023context}
Yu~Huang, Yuan Cheng, and Yingbin Liang.
\newblock In-context convergence of transformers.
\newblock \emph{arXiv preprint arXiv:2310.05249}, 2023.

\bibitem[Tian et~al.(2023{\natexlab{b}})Tian, Wang, Zhang, Chen, and Du]{tian2023joma}
Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du.
\newblock Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention.
\newblock \emph{arXiv preprint arXiv:2310.00535}, 2023{\natexlab{b}}.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu, Scao, Gugger, Drame, Lhoest, and Rush]{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pages 38--45, Online, October 2020. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/2020.emnlp-demos.6}.

\bibitem[Laurent and Massart(2000)]{laurent2000adaptive}
Beatrice Laurent and Pascal Massart.
\newblock Adaptive estimation of a quadratic functional by model selection.
\newblock \emph{Annals of Statistics}, pages 1302--1338, 2000.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{su2024roformer}
Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:\penalty0 127063, 2024.

\end{thebibliography}
