\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Asi \& Duchi(2019{\natexlab{a}})Asi and Duchi]{Asi201908018}
Asi, H. and Duchi, J.~C.
\newblock The importance of better models in stochastic optimization.
\newblock \emph{Proceedings of the National Academy of Sciences},
  2019{\natexlab{a}}.
\newblock ISSN 0027-8424.
\newblock \doi{10.1073/pnas.1908018116}.
\newblock URL \url{https://www.pnas.org/content/early/2019/10/29/1908018116}.

\bibitem[Asi \& Duchi(2019{\natexlab{b}})Asi and Duchi]{asi2019stochastic}
Asi, H. and Duchi, J.~C.
\newblock Stochastic (approximate) proximal point methods: Convergence,
  optimality, and adaptivity.
\newblock \emph{SIAM Journal on Optimization}, 29\penalty0 (3):\penalty0
  2257--2290, 2019{\natexlab{b}}.

\bibitem[Bergstra \& Bengio(2012)Bergstra and Bengio]{bergstra2012random}
Bergstra, J. and Bengio, Y.
\newblock Random search for hyper-parameter optimization.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (Feb):\penalty0 281--305, 2012.

\bibitem[Chen \& Gu(2018)Chen and Gu]{DBLP:journals/corr/abs-1806-06763}
Chen, J. and Gu, Q.
\newblock Closing the generalization gap of adaptive gradient methods in
  training deep neural networks.
\newblock \emph{CoRR}, abs/1806.06763, 2018.
\newblock URL \url{http://arxiv.org/abs/1806.06763}.

\bibitem[Choi et~al.(2019)Choi, Shallue, Nado, Lee, Maddison, and
  Dahl]{choi2019empirical}
Choi, D., Shallue, C.~J., Nado, Z., Lee, J., Maddison, C.~J., and Dahl, G.~E.
\newblock On empirical comparisons of optimizers for deep learning.
\newblock \emph{arXiv preprint arXiv:1910.05446}, 2019.

\bibitem[Dodge et~al.(2019)Dodge, Gururangan, Card, Schwartz, and
  Smith]{dodge2019show}
Dodge, J., Gururangan, S., Card, D., Schwartz, R., and Smith, N.~A.
\newblock Show your work: Improved reporting of experimental results.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pp.\  2185--2194, 2019.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Eggensperger et~al.(2019)Eggensperger, Lindauer, and
  Hutter]{DBLP:journals/jair/EggenspergerLH19}
Eggensperger, K., Lindauer, M., and Hutter, F.
\newblock Pitfalls and best practices in algorithm configuration.
\newblock \emph{Journal of Artificial Intelligence Research}, 64:\penalty0
  861--893, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Henderson et~al.(2018)Henderson, Islam, Bachman, Pineau, Precup, and
  Meger]{DBLP:conf/aaai/0002IBPPM18}
Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D.
\newblock Deep reinforcement learning that matters.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Hutter et~al.(2019{\natexlab{a}})Hutter, Kotthoff, and
  Vanschoren]{DBLP:books/sp/HKV2019}
Hutter, F., Kotthoff, L., and Vanschoren, J.
\newblock Automated machine learning-methods, systems, challenges,
  2019{\natexlab{a}}.

\bibitem[Hutter et~al.(2019{\natexlab{b}})Hutter, Kotthoff, and
  Vanschoren]{Feurer2019}
Hutter, F., Kotthoff, L., and Vanschoren, J. (eds.).
\newblock \emph{Hyperparameter Optimization}, pp.\  3--33.
\newblock Springer International Publishing, Cham, 2019{\natexlab{b}}.
\newblock ISBN 978-3-030-05318-5.
\newblock \doi{10.1007/978-3-030-05318-5_1}.
\newblock URL \url{https://doi.org/10.1007/978-3-030-05318-5_1}.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Liu et~al.(2015)Liu, Rabinovich, and Berg]{liu2016parsenet}
Liu, W., Rabinovich, A., and Berg, A.~C.
\newblock Parsenet: Looking wider to see better.
\newblock \emph{CoRR}, abs/1506.04579, 2015.
\newblock URL \url{http://arxiv.org/abs/1506.04579}.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and
  Hutter]{loshchilov2018decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[Lucic et~al.(2018)Lucic, Kurach, Michalski, Gelly, and
  Bousquet]{DBLP:conf/nips/LucicKMGB18}
Lucic, M., Kurach, K., Michalski, M., Gelly, S., and Bousquet, O.
\newblock Are gans created equal? a large-scale study.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  700--709, 2018.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and
  Potts]{maas2011learning}
Maas, A.~L., Daly, R.~E., Pham, P.~T., Huang, D., Ng, A.~Y., and Potts, C.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Proceedings of the 49th Annual Meeting of the Association
  for Computational Linguistics: Human Language Technologies}, pp.\  142--150,
  Portland, Oregon, USA, June 2011. Association for Computational Linguistics.
\newblock URL \url{http://www.aclweb.org/anthology/P11-1015}.

\bibitem[Mantovani et~al.(2018)Mantovani, Horv{\'a}th, Cerri, Junior,
  Vanschoren, de~Carvalho, and Ferreira]{mantovani2018empirical}
Mantovani, R.~G., Horv{\'a}th, T., Cerri, R., Junior, S.~B., Vanschoren, J.,
  de~Carvalho, A. C. P.~d., and Ferreira, L.
\newblock An empirical study on hyperparameter tuning of decision trees.
\newblock \emph{arXiv preprint arXiv:1812.02207}, 2018.

\bibitem[Melis et~al.(2018)Melis, Dyer, and Blunsom]{melis2018on}
Melis, G., Dyer, C., and Blunsom, P.
\newblock On the state of the art of evaluation in neural language models.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=ByJHuTgA-}.

\bibitem[Metz et~al.(2020)Metz, Maheswaranathan, Sun, Freeman, Poole, and
  Sohl-Dickstein]{metz2020using}
Metz, L., Maheswaranathan, N., Sun, R., Freeman, C.~D., Poole, B., and
  Sohl-Dickstein, J.
\newblock Using a thousand optimization tasks to learn hyperparameter search
  strategies.
\newblock \emph{arXiv preprint arXiv:2002.11887}, 2020.

\bibitem[Probst et~al.(2019)Probst, Boulesteix, and
  Bischl]{probstTunabilityImportanceHyperparameters}
Probst, P., Boulesteix, A.-L., and Bischl, B.
\newblock Tunability: Importance of hyperparameters of machine learning
  algorithms.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (53):\penalty0 1--32, 2019.
\newblock URL \url{http://jmlr.org/papers/v20/18-444.html}.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{robbins1951stochastic}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pp.\  400--407, 1951.

\bibitem[Schneider et~al.(2019)Schneider, Balles, and
  Hennig]{schneider2018deepobs}
Schneider, F., Balles, L., and Hennig, P.
\newblock Deep{OBS}: A deep learning optimizer benchmark suite.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJg6ssC5Y7}.

\bibitem[Schwartz et~al.(2019)Schwartz, Dodge, Smith, and
  Etzioni]{schwartz2019greenai}
Schwartz, R., Dodge, J., Smith, N.~A., and Etzioni, O.
\newblock Green {AI}.
\newblock \emph{CoRR}, abs/1907.10597, 2019.
\newblock URL \url{http://arxiv.org/abs/1907.10597}.

\bibitem[Sculley et~al.(2018)Sculley, Snoek, Wiltschko, and
  Rahimi]{sculley2018winner's}
Sculley, D., Snoek, J., Wiltschko, A.~B., and Rahimi, A.
\newblock Winner's curse? on pace, progress, and empirical rigor.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track
  Proceedings}, 2018.
\newblock URL \url{https://openreview.net/forum?id=rJWF0Fywf}.

\bibitem[Shah et~al.(2018)Shah, Kyrillidis, and Sanghavi]{shah2018minimum}
Shah, V., Kyrillidis, A., and Sanghavi, S.
\newblock Minimum norm solutions do not always generalize well for
  over-parameterized problems.
\newblock \emph{arXiv preprint arXiv:1811.07055}, 2018.

\bibitem[Shallue et~al.(2019)Shallue, Lee, Antognini, Sohl-Dickstein, Frostig,
  and Dahl]{shallue2019measuring}
Shallue, C.~J., Lee, J., Antognini, J., Sohl-Dickstein, J., Frostig, R., and
  Dahl, G.~E.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (112):\penalty0 1--49, 2019.
\newblock URL \url{http://jmlr.org/papers/v20/18-789.html}.

\bibitem[Smith et~al.(2018)Smith, Kindermans, and Le]{smith2018dont}
Smith, S.~L., Kindermans, P.-J., and Le, Q.~V.
\newblock Don't decay the learning rate, increase the batch size.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=B1Yy1BxCZ}.

\bibitem[Strubell et~al.(2019)Strubell, Ganesh, and
  McCallum]{strubell2019energy}
Strubell, E., Ganesh, A., and McCallum, A.
\newblock Energy and policy considerations for deep learning in {NLP}.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  3645--3650, Florence, Italy, July 2019.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P19-1355}.
\newblock URL \url{https://www.aclweb.org/anthology/P19-1355}.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{pmlr-v28-sutskever13}
Sutskever, I., Martens, J., Dahl, G., and Hinton, G.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In Dasgupta, S. and McAllester, D. (eds.), \emph{Proceedings of the
  30th International Conference on Machine Learning}, number~3 in Proceedings
  of Machine Learning Research, pp.\  1139--1147, Atlanta, Georgia, USA, 17--19
  Jun 2013. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v28/sutskever13.html}.

\bibitem[Tibshirani \& Efron(1993)Tibshirani and
  Efron]{tibshirani1993introduction}
Tibshirani, R.~J. and Efron, B.
\newblock An introduction to the bootstrap.
\newblock \emph{Monographs on statistics and applied probability}, 57:\penalty0
  1--436, 1993.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{Tieleman2012}
Tieleman, T. and Hinton, G.
\newblock {Lecture 6.5---RmsProp: Divide the gradient by a running average of
  its recent magnitude}.
\newblock COURSERA: Neural Networks for Machine Learning, 2012.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, and
  Recht]{wilson2017marginal}
Wilson, A.~C., Roelofs, R., Stern, M., Srebro, N., and Recht, B.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4148--4158, 2017.

\end{thebibliography}
