@book{BMP90,
  author    = {Albert Benveniste and
               Michel M{\'{e}}tivier and
               Pierre Priouret},
  title     = {Adaptive Algorithms and Stochastic Approximations},
  volume    = {22},
  publisher = {Springer},
  year      = {1990},
}

@inproceedings{mei2020global,
  title={On the global convergence rates of softmax policy gradient methods},
  author={Mei, Jincheng and Xiao, Chenjun and Szepesvari, Csaba and Schuurmans, Dale},
  booktitle={International Conference on Machine Learning},
  pages={6820--6829},
  year={2020},
  organization={PMLR}
}

@article{mei2020escaping,
  title={Escaping the Gravitational Pull of Softmax},
  author={Mei, Jincheng and Xiao, Chenjun and Dai, Bo and Li, Lihong and Szepesv{\'a}ri, Csaba and Schuurmans, Dale},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{li2021softmax,
  title={Softmax Policy Gradient Methods Can Take Exponential Time to Converge},
  author={Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
  journal={arXiv preprint arXiv:2102.11270},
  year={2021}
}

@article{khodadadian2021linear,
  title={On the Linear convergence of Natural Policy Gradient Algorithm},
  author={Khodadadian, Sajad and Jhunjhunwala, Prakirt Raj and Varma, Sushil Mahavir and Maguluri, Siva Theja},
  journal={arXiv preprint arXiv:2105.01424},
  year={2021}
}

@article{mei2021leveraging,
  title={Leveraging Non-uniformity in First-order Non-convex Optimization},
  author={Mei, Jincheng and Gao, Yue and Dai, Bo and Szepesvari, Csaba and Schuurmans, Dale},
  journal={arXiv preprint arXiv:2105.06072},
  year={2021}
}

@article{cen2020fast,
  title={Fast global convergence of natural policy gradient methods with entropy regularization},
  author={Cen, Shicong and Cheng, Chen and Chen, Yuxin and Wei, Yuting and Chi, Yuejie},
  journal={arXiv preprint arXiv:2007.06558},
  year={2020}
}

@article{lan2021policy,
  title={Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes},
  author={Lan, Guanghui},
  journal={arXiv preprint arXiv:2102.00135},
  year={2021}
}

@inproceedings{agarwal2020optimality,
  title={Optimality and approximation with policy gradient methods in markov decision processes},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  booktitle={Conference on Learning Theory},
  pages={64--66},
  year={2020},
  organization={PMLR}
}

@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
    booktitle={Advances in Neural Information Processing Systems},
  pages={1057--1063},
  year={2000}
}

@article{chung2020beyond,
  title={Beyond variance reduction: Understanding the true impact of baselines on policy optimization},
  author={Chung, Wesley and Thomas, Valentin and Machado, Marlos C and Roux, Nicolas Le},
  journal={arXiv preprint arXiv:2008.13773},
  year={2020}
}

@inproceedings{kakade2002natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1531--1538},
  year={2002}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International Conference on Machine Learning},
  pages={1889--1897},
  year={2015}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{tucker2018baseline,
  title={The mirage of action-dependent baselines in reinforcement learning},
  author={George Tucker and Surya Bhupatiraju and Shixiang Gu and Richard Turner and Zoubin Ghahramani and Sergey Levine},
  booktitle={International Conference on Machine Learning},
  pages={5015--5024},
  year={2018}
}

@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}

@inproceedings{abbasi2019politex,
  title={Politex: Regret bounds for policy iteration using expert prediction},
  author={Abbasi-Yadkori, Yasin and Bartlett, Peter and Bhatia, Kush and Lazic, Nevena and Szepesvari, Csaba and Weisz, Gell{\'e}rt},
  booktitle={International Conference on Machine Learning},
  pages={3692--3702},
  year={2019},
  organization={PMLR}
}

@article{andrychowicz2020learning,
  title={Learning dexterous in-hand manipulation},
  author={Andrychowicz, OpenAI: Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and others},
  journal={The International Journal of Robotics Research},
  volume={39},
  number={1},
  pages={3--20},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{zhang2021convergence,
  title={On the Convergence and Sample Efficiency of Variance-Reduced Policy Gradient Method},
  author={Zhang, Junyu and Ni, Chengzhuo and Yu, Zheng and Szepesvari, Csaba and Wang, Mengdi},
  journal={arXiv preprint arXiv:2102.08607},
  year={2021}
}

@article{zhang2020sample,
  title={Sample Efficient Reinforcement Learning with REINFORCE},
  author={Zhang, Junzi and Kim, Jongho and O'Donoghue, Brendan and Boyd, Stephen},
  journal={arXiv preprint arXiv:2010.11364},
  year={2020}
}

@article{ren2021nearly,
  title={Nearly Horizon-Free Offline Reinforcement Learning},
  author={Ren, Tongzheng and Li, Jialian and Dai, Bo and Du, Simon S and Sanghavi, Sujay},
  journal={arXiv preprint arXiv:2103.14077},
  year={2021}
}

@inproceedings{henderson2018deep,
  title={Deep Reinforcement Learning That Matters},
  author={Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{wiering2008ensemble,
  title={Ensemble algorithms in reinforcement learning},
  author={Wiering, Marco A and Van Hasselt, Hado},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  volume={38},
  number={4},
  pages={930--936},
  year={2008},
  publisher={IEEE}
}

@inproceedings{Jung2020Population-Guided,
  title={Population-Guided Parallel Policy Search for Reinforcement Learning},
  author={Whiyoung Jung and Giseung Park and Youngchul Sung},
  booktitle={International Conference on Learning Representations},
  year={2020},
}

@article{parker2020effective,
  title={Effective diversity in population-based reinforcement learning},
  author={Parker-Holder, Jack and Pacchiano, Aldo and Choromanski, Krzysztof and Roberts, Stephen},
  journal={arXiv preprint arXiv:2002.00632},
  year={2020}
}

@article{pourchot2018cem,
  title={CEM-RL: Combining evolutionary and gradient-based methods for policy search},
  author={Pourchot, Alo{\"\i}s and Sigaud, Olivier},
  journal={arXiv preprint arXiv:1810.01222},
  year={2018}
}

@article{khadka2018evolution,
  title={Evolution-guided policy gradient in reinforcement learning},
  author={Khadka, Shauharda and Tumer, Kagan},
  journal={arXiv preprint arXiv:1805.07917},
  year={2018}
}

@article{denisov2020regret,
  title={Regret Analysis of a Markov Policy Gradient Algorithm for Multi-arm Bandits},
  author={Denisov, Denis and Walton, Neil},
  journal={arXiv preprint arXiv:2007.10229},
  year={2020}
}

@article{lazic2021optimization,
  title={Optimization Issues in KL-Constrained Approximate Policy Iteration},
  author={Lazi{\'c}, Nevena and Hao, Botao and Abbasi-Yadkori, Yasin and Schuurmans, Dale and Szepesv{\'a}ri, Csaba},
  journal={arXiv preprint arXiv:2102.06234},
  year={2021}
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016},
  organization={PMLR}
}

@book{knopp1947theory,
  title={Theory and Application of Infinite Series},
  author={Knopp, Konrad},
  year={1947},
  publisher={Hafner Publishing Company, New York}
}

@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}

@article{zhang2020global,
  title={Global convergence of policy gradient methods to (almost) locally optimal policies},
  author={Zhang, Kaiqing and Koppel, Alec and Zhu, Hao and Basar, Tamer},
  journal={SIAM Journal on Control and Optimization},
  volume={58},
  number={6},
  pages={3586--3612},
  year={2020},
  publisher={SIAM}
}

@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={ICML},
  volume={2},
  pages={267--274},
  year={2002}
}

@article{bhandari2020note,
  title={A note on the linear convergence of policy gradient methods},
  author={Bhandari, Jalaj and Russo, Daniel},
  journal={arXiv preprint arXiv:2007.11120},
  year={2020}
}