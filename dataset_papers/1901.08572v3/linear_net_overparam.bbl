\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2018)Allen-Zhu, Li, and Song]{allen2018convergence}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock \emph{arXiv preprint arXiv:1811.03962}, 2018.

\bibitem[Arora et~al.(2018{\natexlab{a}})Arora, Cohen, Golowich, and
  Hu]{arora2018convergence}
Arora, S., Cohen, N., Golowich, N., and Hu, W.
\newblock A convergence analysis of gradient descent for deep linear neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02281}, 2018{\natexlab{a}}.

\bibitem[Arora et~al.(2018{\natexlab{b}})Arora, Cohen, and
  Hazan]{arora2018optimization}
Arora, S., Cohen, N., and Hazan, E.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock \emph{arXiv preprint arXiv:1802.06509}, 2018{\natexlab{b}}.

\bibitem[Bartlett et~al.(2018)Bartlett, Helmbold, and
  Long]{bartlett2018gradient}
Bartlett, P., Helmbold, D., and Long, P.
\newblock Gradient descent with identity initialization efficiently learns
  positive definite linear transformations.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  520--529, 2018.

\bibitem[Brutzkus \& Globerson(2017)Brutzkus and
  Globerson]{brutzkus2017globally}
Brutzkus, A. and Globerson, A.
\newblock Globally optimal gradient descent for a {ConvNet} with gaussian
  inputs.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  605--614, 2017.

\bibitem[Du \& Lee(2018)Du and Lee]{du2018power}
Du, S.~S. and Lee, J.~D.
\newblock On the power of over-parametrization in neural networks with
  quadratic activation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1329--1338, 2018.

\bibitem[Du et~al.(2018{\natexlab{a}})Du, Hu, and Lee]{du2018algorithmic}
Du, S.~S., Hu, W., and Lee, J.~D.
\newblock Algorithmic regularization in learning deep homogeneous models:
  Layers are automatically balanced.
\newblock \emph{arXiv preprint arXiv:1806.00900}, 2018{\natexlab{a}}.

\bibitem[Du et~al.(2018{\natexlab{b}})Du, Lee, Li, Wang, and Zhai]{du2018deep}
Du, S.~S., Lee, J.~D., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock \emph{arXiv preprint arXiv:1811.03804}, 2018{\natexlab{b}}.

\bibitem[Du et~al.(2018{\natexlab{c}})Du, Lee, and Tian]{du2018convolutional}
Du, S.~S., Lee, J.~D., and Tian, Y.
\newblock When is a convolutional filter easy to learn?
\newblock \emph{International Conference on Learning Representations},
  2018{\natexlab{c}}.

\bibitem[Du et~al.(2018{\natexlab{d}})Du, Lee, Tian, Singh, and
  Poczos]{du2018gradient}
Du, S.~S., Lee, J.~D., Tian, Y., Singh, A., and Poczos, B.
\newblock Gradient descent learns one-hidden-layer {CNN}: Don’t be afraid of
  spurious local minima.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1339--1348, 2018{\natexlab{d}}.

\bibitem[Du et~al.(2019)Du, Zhai, Poczos, and Singh]{du2018provably}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=S1eK3i09YQ}.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Freeman \& Bruna(2016)Freeman and Bruna]{freeman2016topology}
Freeman, C.~D. and Bruna, J.
\newblock Topology and geometry of half-rectified network optimization.
\newblock \emph{arXiv preprint arXiv:1611.01540}, 2016.

\bibitem[Ge et~al.(2015)Ge, Huang, Jin, and Yuan]{ge2015escaping}
Ge, R., Huang, F., Jin, C., and Yuan, Y.
\newblock Escaping from saddle points—online stochastic gradient for tensor
  decomposition.
\newblock In \emph{Conference on Learning Theory}, pp.\  797--842, 2015.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pp.\  249--256, 2010.

\bibitem[Haeffele \& Vidal(2017)Haeffele and Vidal]{haeffele2017global}
Haeffele, B. and Vidal, R.
\newblock Global optimality in neural network training.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  pp.\  4390--4398, 2017.

\bibitem[Hardt \& Ma(2016)Hardt and Ma]{hardt2016identity}
Hardt, M. and Ma, T.
\newblock Identity matters in deep learning.
\newblock \emph{International Conference on Learning Representations}, 2016.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Ji \& Telgarsky(2019)Ji and Telgarsky]{ji2018gradient}
Ji, Z. and Telgarsky, M.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HJflg30qKX}.

\bibitem[Jin et~al.(2017)Jin, Ge, Netrapalli, Kakade, and
  Jordan]{jin2017escape}
Jin, C., Ge, R., Netrapalli, P., Kakade, S.~M., and Jordan, M.~I.
\newblock How to escape saddle points efficiently.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1724--1732, 2017.

\bibitem[Kawaguchi(2016)]{kawaguchi2016deep}
Kawaguchi, K.
\newblock Deep learning without poor local minima.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  586--594, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Laurent \& Brecht(2018)Laurent and Brecht]{laurent2018deep}
Laurent, T. and Brecht, J.
\newblock Deep linear networks with arbitrary loss: All local minima are
  global.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2908--2913, 2018.

\bibitem[Li \& Liang(2018)Li and Liang]{li2018learning}
Li, Y. and Liang, Y.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8168--8177, 2018.

\bibitem[Li \& Yuan(2017)Li and Yuan]{li2017convergence}
Li, Y. and Yuan, Y.
\newblock Convergence analysis of two-layer neural networks with {ReLU}
  activation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  597--607, 2017.

\bibitem[Lu \& Kawaguchi(2017)Lu and Kawaguchi]{lu2017depth}
Lu, H. and Kawaguchi, K.
\newblock Depth creates no bad local minima.
\newblock \emph{arXiv preprint arXiv:1702.08580}, 2017.

\bibitem[Nguyen \& Hein(2017)Nguyen and Hein]{nguyen2017loss}
Nguyen, Q. and Hein, M.
\newblock The loss surface of deep and wide neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2603--2612, 2017.

\bibitem[Safran \& Shamir(2018)Safran and Shamir]{safran2018spurious}
Safran, I. and Shamir, O.
\newblock Spurious local minima are common in two-layer relu neural networks.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Saxe et~al.(2014)Saxe, McClelland, and Ganguli]{saxe2014exact}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{International Conference on Learning Representations}, 2014.

\bibitem[Shamir(2018)]{shamir2018exponential}
Shamir, O.
\newblock Exponential convergence time of gradient descent for one-dimensional
  deep linear neural networks.
\newblock \emph{arXiv preprint arXiv:1809.08587}, 2018.

\bibitem[Soltanolkotabi(2017)]{soltanolkotabi2017learning}
Soltanolkotabi, M.
\newblock Learning {ReLUs} via gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2007--2017, 2017.

\bibitem[Soltanolkotabi et~al.(2018)Soltanolkotabi, Javanmard, and
  Lee]{soltanolkotabi2018theoretical}
Soltanolkotabi, M., Javanmard, A., and Lee, J.~D.
\newblock Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks.
\newblock \emph{IEEE Transactions on Information Theory}, 2018.

\bibitem[Soudry \& Carmon(2016)Soudry and Carmon]{soudry2016no}
Soudry, D. and Carmon, Y.
\newblock No bad local minima: Data independent training error guarantees for
  multilayer neural networks.
\newblock \emph{arXiv preprint arXiv:1605.08361}, 2016.

\bibitem[Tian(2017)]{tian2017analytical}
Tian, Y.
\newblock An analytical formula of population gradient for two-layered {ReLU}
  network and its applications in convergence and critical point analysis.
\newblock \emph{arXiv preprint arXiv:1703.00560}, 2017.

\bibitem[Venturi et~al.(2018)Venturi, Bandeira, and Bruna]{venturi2018neural}
Venturi, L., Bandeira, A., and Bruna, J.
\newblock Neural networks with finite intrinsic dimension have no spurious
  valleys.
\newblock \emph{arXiv preprint arXiv:1802.06384}, 2018.

\bibitem[Yun et~al.(2017)Yun, Sra, and Jadbabaie]{yun2017global}
Yun, C., Sra, S., and Jadbabaie, A.
\newblock Global optimality conditions for deep neural networks.
\newblock \emph{arXiv preprint arXiv:1707.02444}, 2017.

\bibitem[Yun et~al.(2019)Yun, Sra, and Jadbabaie]{yun2018small}
Yun, C., Sra, S., and Jadbabaie, A.
\newblock Small nonlinearities in activation functions create bad local minima
  in neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rke_YiRct7}.

\bibitem[Zhang et~al.(2018)Zhang, Yu, Wang, and Gu]{zhang2018learning}
Zhang, X., Yu, Y., Wang, L., and Gu, Q.
\newblock Learning one-hidden-layer {ReLU} networks via gradient descent.
\newblock \emph{arXiv preprint arXiv:1806.07808}, 2018.

\bibitem[Zhong et~al.(2017)Zhong, Song, Jain, Bartlett, and
  Dhillon]{zhong2017recovery}
Zhong, K., Song, Z., Jain, P., Bartlett, P.~L., and Dhillon, I.~S.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4140--4149, 2017.

\bibitem[Zhou \& Liang(2018)Zhou and Liang]{zhou2018critical}
Zhou, Y. and Liang, Y.
\newblock Critical points of linear neural networks: Analytical forms and
  landscape properties.
\newblock 2018.

\bibitem[Zou et~al.(2018)Zou, Cao, Zhou, and Gu]{zou2018stochastic}
Zou, D., Cao, Y., Zhou, D., and Gu, Q.
\newblock Stochastic gradient descent optimizes over-parameterized deep relu
  networks.
\newblock \emph{arXiv preprint arXiv:1811.08888}, 2018.

\end{thebibliography}
