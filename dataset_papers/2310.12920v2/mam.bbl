\begin{thebibliography}{88}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Austin et~al.(2021)Austin, Johnson, Ho, Tarlow, and van~den
  Berg]{austin2021structured}
Austin, J., Johnson, D.~D., Ho, J., Tarlow, D., and van~den Berg, R.
\newblock Structured denoising diffusion models in discrete state-spaces.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 17981--17993, 2021.

\bibitem[Bengio et~al.(2021)Bengio, Jain, Korablyov, Precup, and
  Bengio]{bengio2021flow}
Bengio, E., Jain, M., Korablyov, M., Precup, D., and Bengio, Y.
\newblock Flow network based generative models for non-iterative diverse
  candidate generation.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 27381--27394, 2021.

\bibitem[Bengio \& Bengio(2000)Bengio and Bengio]{bengio2000taking}
Bengio, S. and Bengio, Y.
\newblock Taking on the curse of dimensionality in joint distributions using
  neural networks.
\newblock \emph{IEEE Transactions on Neural Networks}, 11\penalty0
  (3):\penalty0 550--557, 2000.

\bibitem[Bengio et~al.(2023)Bengio, Lahlou, Deleu, Hu, Tiwari, and
  Bengio]{bengio2023gflownet}
Bengio, Y., Lahlou, S., Deleu, T., Hu, E.~J., Tiwari, M., and Bengio, E.
\newblock Gflownet foundations.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0
  (210):\penalty0 1--55, 2023.

\bibitem[Berthelot et~al.(2023)Berthelot, Autef, Lin, Yap, Zhai, Hu, Zheng,
  Talbott, and Gu]{berthelot2023tract}
Berthelot, D., Autef, A., Lin, J., Yap, D.~A., Zhai, S., Hu, S., Zheng, D.,
  Talbott, W., and Gu, E.
\newblock Tract: Denoising diffusion models with transitive closure
  time-distillation.
\newblock \emph{arXiv preprint arXiv:2303.04248}, 2023.

\bibitem[Burda et~al.(2015)Burda, Grosse, and
  Salakhutdinov]{burda2015importance}
Burda, Y., Grosse, R., and Salakhutdinov, R.
\newblock Importance weighted autoencoders.
\newblock \emph{arXiv preprint arXiv:1509.00519}, 2015.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Child, R., Gray, S., Radford, A., and Sutskever, I.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Choi et~al.(2020)Choi, Vergari, and Van~den
  Broeck]{choi2020probabilistic}
Choi, Y., Vergari, A., and Van~den Broeck, G.
\newblock Probabilistic circuits: A unifying framework for tractable
  probabilistic models.
\newblock \emph{UCLA. URL: http://starai. cs. ucla. edu/papers/ProbCirc20.
  pdf}, 2020.

\bibitem[Chow \& Liu(1968)Chow and Liu]{chow1968approximating}
Chow, C. and Liu, C.
\newblock Approximating discrete probability distributions with dependence
  trees.
\newblock \emph{IEEE transactions on Information Theory}, 14\penalty0
  (3):\penalty0 462--467, 1968.

\bibitem[Chrabaszcz et~al.(2017)Chrabaszcz, Loshchilov, and
  Hutter]{chrabaszcz2017downsampled}
Chrabaszcz, P., Loshchilov, I., and Hutter, F.
\newblock A downsampled variant of imagenet as an alternative to the cifar
  datasets.
\newblock \emph{arXiv preprint arXiv:1707.08819}, 2017.

\bibitem[Cybenko(1989)]{cybenko1989approximation}
Cybenko, G.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \emph{Mathematics of control, signals and systems}, 2\penalty0
  (4):\penalty0 303--314, 1989.

\bibitem[Damewood et~al.(2022)Damewood, Schwalbe-Koda, and
  G{\'o}mez-Bombarelli]{damewood2022sampling}
Damewood, J., Schwalbe-Koda, D., and G{\'o}mez-Bombarelli, R.
\newblock Sampling lattices in semi-grand canonical ensemble with
  autoregressive machine learning.
\newblock \emph{npj Computational Materials}, 8\penalty0 (1):\penalty0 61,
  2022.

\bibitem[Darwiche(2003)]{darwiche2003differential}
Darwiche, A.
\newblock A differential approach to inference in {B}ayesian networks.
\newblock \emph{Journal of the ACM (JACM)}, 50\penalty0 (3):\penalty0 280--305,
  2003.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dinh et~al.(2014)Dinh, Krueger, and Bengio]{dinh2014nice}
Dinh, L., Krueger, D., and Bengio, Y.
\newblock {NICE}: Non-linear independent components estimation.
\newblock \emph{arXiv preprint arXiv:1410.8516}, 2014.

\bibitem[Dinh et~al.(2016)Dinh, Sohl-Dickstein, and Bengio]{dinh2016density}
Dinh, L., Sohl-Dickstein, J., and Bengio, S.
\newblock Density estimation using real {NVP}.
\newblock \emph{arXiv preprint arXiv:1605.08803}, 2016.

\bibitem[Douglas et~al.(2017)Douglas, Zarov, Gourgoulias, Lucas, Hart, Baker,
  Sahani, Perov, and Johri]{douglas2017universal}
Douglas, L., Zarov, I., Gourgoulias, K., Lucas, C., Hart, C., Baker, A.,
  Sahani, M., Perov, Y., and Johri, S.
\newblock A universal marginalizer for amortized inference in generative
  models.
\newblock \emph{Advances in Approximate Bayesian Inference, NIPS 2017
  Workshop}, 2017.

\bibitem[Flam-Shepherd et~al.(2022)Flam-Shepherd, Zhu, and
  Aspuru-Guzik]{flam2022language}
Flam-Shepherd, D., Zhu, K., and Aspuru-Guzik, A.
\newblock Language models can learn complex molecular distributions.
\newblock \emph{Nature Communications}, 13\penalty0 (1):\penalty0 3293, 2022.

\bibitem[Germain et~al.(2015)Germain, Gregor, Murray, and
  Larochelle]{germain2015made}
Germain, M., Gregor, K., Murray, I., and Larochelle, H.
\newblock Made: Masked autoencoder for distribution estimation.
\newblock In \emph{International conference on machine learning}, pp.\
  881--889. PMLR, 2015.

\bibitem[Grathwohl et~al.(2021)Grathwohl, Swersky, Hashemi, Duvenaud, and
  Maddison]{grathwohl2021oops}
Grathwohl, W., Swersky, K., Hashemi, M., Duvenaud, D., and Maddison, C.
\newblock Oops {I} took a gradient: Scalable sampling for discrete
  distributions.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3831--3841. PMLR, 2021.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Ho, J., Jain, A., and Abbeel, P.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6840--6851, 2020.

\bibitem[Hoogeboom et~al.(2019)Hoogeboom, Peters, Van Den~Berg, and
  Welling]{hoogeboom2019integer}
Hoogeboom, E., Peters, J., Van Den~Berg, R., and Welling, M.
\newblock Integer discrete flows and lossless compression.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Hoogeboom et~al.(2021{\natexlab{a}})Hoogeboom, Gritsenko, Bastings,
  Poole, Berg, and Salimans]{hoogeboom2021autoregressive}
Hoogeboom, E., Gritsenko, A.~A., Bastings, J., Poole, B., Berg, R. v.~d., and
  Salimans, T.
\newblock Autoregressive diffusion models.
\newblock \emph{arXiv preprint arXiv:2110.02037}, 2021{\natexlab{a}}.

\bibitem[Hoogeboom et~al.(2021{\natexlab{b}})Hoogeboom, Nielsen, Jaini,
  Forr{\'e}, and Welling]{hoogeboom2021argmax}
Hoogeboom, E., Nielsen, D., Jaini, P., Forr{\'e}, P., and Welling, M.
\newblock Argmax flows and multinomial diffusion: Learning categorical
  distributions.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 12454--12465, 2021{\natexlab{b}}.

\bibitem[Hornik(1991)]{hornik1991approximation}
Hornik, K.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock \emph{Neural networks}, 4\penalty0 (2):\penalty0 251--257, 1991.

\bibitem[Hornik et~al.(1989)Hornik, Stinchcombe, and
  White]{hornik1989multilayer}
Hornik, K., Stinchcombe, M., and White, H.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural networks}, 2\penalty0 (5):\penalty0 359--366, 1989.

\bibitem[{Ising}(1925)]{ising1925beitrag}
{Ising}, E.
\newblock {Beitrag zur Theorie des Ferromagnetismus}.
\newblock \emph{Zeitschrift fur Physik}, 31\penalty0 (1):\penalty0 253--258,
  February 1925.
\newblock \doi{10.1007/BF02980577}.

\bibitem[Ivanov et~al.(2018)Ivanov, Figurnov, and
  Vetrov]{ivanov2018variational}
Ivanov, O., Figurnov, M., and Vetrov, D.
\newblock Variational autoencoder with arbitrary conditioning.
\newblock \emph{arXiv preprint arXiv:1806.02382}, 2018.

\bibitem[Jin et~al.(2018)Jin, Barzilay, and Jaakkola]{jin2018junction}
Jin, W., Barzilay, R., and Jaakkola, T.
\newblock Junction tree variational autoencoder for molecular graph generation.
\newblock In \emph{International conference on machine learning}, pp.\
  2323--2332. PMLR, 2018.

\bibitem[Johnson et~al.(2021)Johnson, Austin, Berg, and
  Tarlow]{johnson2021beyond}
Johnson, D.~D., Austin, J., Berg, R. v.~d., and Tarlow, D.
\newblock Beyond in-place corruption: Insertion and deletion in denoising
  probabilistic models.
\newblock \emph{arXiv preprint arXiv:2107.07675}, 2021.

\bibitem[Jun et~al.(2020)Jun, Child, Chen, Schulman, Ramesh, Radford, and
  Sutskever]{jun2020distribution}
Jun, H., Child, R., Chen, M., Schulman, J., Ramesh, A., Radford, A., and
  Sutskever, I.
\newblock Distribution augmentation for generative modeling.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5006--5019. PMLR, 2020.

\bibitem[Kingma et~al.(2021)Kingma, Salimans, Poole, and
  Ho]{kingma2021variational}
Kingma, D., Salimans, T., Poole, B., and Ho, J.
\newblock Variational diffusion models.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 21696--21707, 2021.

\bibitem[Kingma et~al.(2016)Kingma, Salimans, Jozefowicz, Chen, Sutskever, and
  Welling]{kingma2016improved}
Kingma, D.~P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., and
  Welling, M.
\newblock Improved variational inference with inverse autoregressive flow.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[K{\"o}hler et~al.(2023)K{\"o}hler, Invernizzi, De~Haan, and
  No{\'e}]{kohler2023rigid}
K{\"o}hler, J., Invernizzi, M., De~Haan, P., and No{\'e}, F.
\newblock Rigid body flows for sampling molecular crystal structures.
\newblock \emph{arXiv preprint arXiv:2301.11355}, 2023.

\bibitem[Krenn et~al.(2020)Krenn, H{\"a}se, Nigam, Friederich, and
  Aspuru-Guzik]{krenn2020self}
Krenn, M., H{\"a}se, F., Nigam, A., Friederich, P., and Aspuru-Guzik, A.
\newblock Self-referencing embedded strings ({SELFIES}): A 100\% robust
  molecular string representation.
\newblock \emph{Machine Learning: Science and Technology}, 1\penalty0
  (4):\penalty0 045024, 2020.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Larochelle \& Murray(2011)Larochelle and Murray]{larochelle2011neural}
Larochelle, H. and Murray, I.
\newblock The neural autoregressive distribution estimator.
\newblock In \emph{Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pp.\  29--37. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[Li et~al.(2020)Li, Akbar, and Oliva]{li2020acflow}
Li, Y., Akbar, S., and Oliva, J.
\newblock Acflow: Flow models for arbitrary conditional likelihoods.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5831--5841. PMLR, 2020.

\bibitem[Lin et~al.(2023)Lin, Akin, Rao, Hie, Zhu, Lu, Smetanin, Verkuil,
  Kabeli, Shmueli, et~al.]{lin2023evolutionary}
Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., Smetanin, N., Verkuil,
  R., Kabeli, O., Shmueli, Y., et~al.
\newblock Evolutionary-scale prediction of atomic-level protein structure with
  a language model.
\newblock \emph{Science}, 379\penalty0 (6637):\penalty0 1123--1130, 2023.

\bibitem[Liu et~al.(2022)Liu, Zhang, and Broeck]{liu2022scaling}
Liu, A., Zhang, H., and Broeck, G. V.~d.
\newblock Scaling up probabilistic circuits by latent variable distillation.
\newblock \emph{arXiv preprint arXiv:2210.04398}, 2022.

\bibitem[Liu et~al.(2023)Liu, Liu, Van~den Broeck, and
  Liang]{liu2023understanding}
Liu, X., Liu, A., Van~den Broeck, G., and Liang, Y.
\newblock Understanding the distillation process from deep generative models to
  tractable probabilistic circuits.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  21825--21838. PMLR, 2023.

\bibitem[MacKay(2003)]{mackay2003information}
MacKay, D.~J.
\newblock \emph{Information Theory, Inference and Learning Algorithms}.
\newblock Cambridge University Press, 2003.

\bibitem[Madani et~al.(2023)Madani, Krause, Greene, Subramanian, Mohr, Holton,
  Olmos~Jr, Xiong, Sun, Socher, et~al.]{madani2023large}
Madani, A., Krause, B., Greene, E.~R., Subramanian, S., Mohr, B.~P., Holton,
  J.~M., Olmos~Jr, J.~L., Xiong, C., Sun, Z.~Z., Socher, R., et~al.
\newblock Large language models generate functional protein sequences across
  diverse families.
\newblock \emph{Nature Biotechnology}, pp.\  1--8, 2023.

\bibitem[Mahoney(2011)]{mahoney2011large}
Mahoney, M.
\newblock Large text compression benchmark, 2011.

\bibitem[Malkin et~al.(2022{\natexlab{a}})Malkin, Jain, Bengio, Sun, and
  Bengio]{malkin2022trajectory}
Malkin, N., Jain, M., Bengio, E., Sun, C., and Bengio, Y.
\newblock Trajectory balance: Improved credit assignment in gflownets.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 5955--5967, 2022{\natexlab{a}}.

\bibitem[Malkin et~al.(2022{\natexlab{b}})Malkin, Lahlou, Deleu, Ji, Hu,
  Everett, Zhang, and Bengio]{malkin2022gflownets}
Malkin, N., Lahlou, S., Deleu, T., Ji, X., Hu, E., Everett, K., Zhang, D., and
  Bengio, Y.
\newblock Gflownets and variational inference.
\newblock \emph{arXiv preprint arXiv:2210.00580}, 2022{\natexlab{b}}.

\bibitem[Mitchell et~al.(2023)Mitchell, Lee, Khazatsky, Manning, and
  Finn]{mitchell2023detectgpt}
Mitchell, E., Lee, Y., Khazatsky, A., Manning, C.~D., and Finn, C.
\newblock Detectgpt: Zero-shot machine-generated text detection using
  probability curvature.
\newblock \emph{arXiv preprint arXiv:2301.11305}, 2023.

\bibitem[No{\'e} et~al.(2019)No{\'e}, Olsson, K{\"o}hler, and
  Wu]{noe2019boltzmann}
No{\'e}, F., Olsson, S., K{\"o}hler, J., and Wu, H.
\newblock Boltzmann generators: Sampling equilibrium states of many-body
  systems with deep learning.
\newblock \emph{Science}, 365\penalty0 (6457), 2019.

\bibitem[{OpenAI}(2023)]{chatgpt}
{OpenAI}.
\newblock {ChatGPT}, 2023.
\newblock URL \url{https://openai.com}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang,
  C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 27730--27744, 2022.

\bibitem[Papamakarios et~al.(2017)Papamakarios, Pavlakou, and
  Murray]{papamakarios2017masked}
Papamakarios, G., Pavlakou, T., and Murray, I.
\newblock Masked autoregressive flow for density estimation.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Parmar et~al.(2018)Parmar, Vaswani, Uszkoreit, Kaiser, Shazeer, Ku,
  and Tran]{parmar2018image}
Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., and
  Tran, D.
\newblock Image transformer.
\newblock In \emph{International conference on machine learning}, pp.\
  4055--4064. PMLR, 2018.

\bibitem[Peharz et~al.(2020)Peharz, Lang, Vergari, Stelzner, Molina, Trapp,
  Van~den Broeck, Kersting, and Ghahramani]{peharz2020einsum}
Peharz, R., Lang, S., Vergari, A., Stelzner, K., Molina, A., Trapp, M., Van~den
  Broeck, G., Kersting, K., and Ghahramani, Z.
\newblock Einsum networks: Fast and scalable learning of tractable
  probabilistic circuits.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7563--7574. PMLR, 2020.

\bibitem[Polykovskiy et~al.(2020)Polykovskiy, Zhebrak, Sanchez-Lengeling,
  Golovanov, Tatanov, Belyaev, Kurbanov, Artamonov, Aladinskiy, Veselov,
  et~al.]{polykovskiy2020molecular}
Polykovskiy, D., Zhebrak, A., Sanchez-Lengeling, B., Golovanov, S., Tatanov,
  O., Belyaev, S., Kurbanov, R., Artamonov, A., Aladinskiy, V., Veselov, M.,
  et~al.
\newblock Molecular sets (moses): a benchmarking platform for molecular
  generation models.
\newblock \emph{Frontiers in Pharmacology}, 11:\penalty0 565644, 2020.

\bibitem[Poon \& Domingos(2011)Poon and Domingos]{poon2011sum}
Poon, H. and Domingos, P.~M.
\newblock Sum-product networks: {A} new deep architecture.
\newblock In \emph{Proceedings of the Twenty-Seventh Conference on Uncertainty
  in Artificial Intelligence}, 2011.

\bibitem[Prykhodko et~al.(2019)Prykhodko, Johansson, Kotsias, Ar{\'u}s-Pous,
  Bjerrum, Engkvist, and Chen]{prykhodko2019novo}
Prykhodko, O., Johansson, S.~V., Kotsias, P.-C., Ar{\'u}s-Pous, J., Bjerrum,
  E.~J., Engkvist, O., and Chen, H.
\newblock A de novo molecular generation method using latent vector based
  generative adversarial network.
\newblock \emph{Journal of Cheminformatics}, 11\penalty0 (1):\penalty0 1--13,
  2019.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Ren et~al.(2019)Ren, Liu, Fertig, Snoek, Poplin, Depristo, Dillon, and
  Lakshminarayanan]{ren2019likelihood}
Ren, J., Liu, P.~J., Fertig, E., Snoek, J., Poplin, R., Depristo, M., Dillon,
  J., and Lakshminarayanan, B.
\newblock Likelihood ratios for out-of-distribution detection.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Rezende \& Mohamed(2015)Rezende and Mohamed]{rezende2015variational}
Rezende, D. and Mohamed, S.
\newblock Variational inference with normalizing flows.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1530--1538. PMLR, 2015.

\bibitem[Rippel \& Adams(2013)Rippel and Adams]{rippel2013high}
Rippel, O. and Adams, R.~P.
\newblock High-dimensional probability estimation with deep density models.
\newblock \emph{arXiv preprint arXiv:1302.5125}, 2013.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{ronneberger2015u}
Ronneberger, O., Fischer, P., and Brox, T.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{Medical Image Computing and Computer-Assisted
  Intervention--MICCAI 2015: 18th International Conference, Munich, Germany,
  October 5-9, 2015, Proceedings, Part III 18}, pp.\  234--241. Springer, 2015.

\bibitem[Salakhutdinov \& Murray(2008)Salakhutdinov and
  Murray]{salakhutdinov2008quantitative}
Salakhutdinov, R. and Murray, I.
\newblock On the quantitative analysis of deep belief networks.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pp.\  872--879, 2008.

\bibitem[Salimans et~al.(2017)Salimans, Karpathy, Chen, and
  Kingma]{salimans2017pixelcnn++}
Salimans, T., Karpathy, A., Chen, X., and Kingma, D.~P.
\newblock Pixelcnn++: Improving the pixelcnn with discretized logistic mixture
  likelihood and other modifications.
\newblock \emph{arXiv preprint arXiv:1701.05517}, 2017.

\bibitem[Schneuing et~al.(2022)Schneuing, Du, Harris, Jamasb, Igashov, Du,
  Blundell, Li{\'o}, Gomes, Welling, et~al.]{schneuing2022structure}
Schneuing, A., Du, Y., Harris, C., Jamasb, A., Igashov, I., Du, W., Blundell,
  T., Li{\'o}, P., Gomes, C., Welling, M., et~al.
\newblock Structure-based drug design with equivariant diffusion models.
\newblock \emph{arXiv preprint arXiv:2210.13695}, 2022.

\bibitem[Segler et~al.(2018)Segler, Kogej, Tyrchan, and
  Waller]{segler2018generating}
Segler, M.~H., Kogej, T., Tyrchan, C., and Waller, M.~P.
\newblock Generating focused molecule libraries for drug discovery with
  recurrent neural networks.
\newblock \emph{ACS Central Science}, 4\penalty0 (1):\penalty0 120--131, 2018.

\bibitem[Shih et~al.(2022)Shih, Sadigh, and Ermon]{shih2022training}
Shih, A., Sadigh, D., and Ermon, S.
\newblock Training and inference on any-order autoregressive models the right
  way.
\newblock \emph{arXiv preprint arXiv:2205.13554}, 2022.

\bibitem[Shin et~al.(2021)Shin, Riesselman, Kollasch, McMahon, Simon, Sander,
  Manglik, Kruse, and Marks]{shin2021protein}
Shin, J.-E., Riesselman, A.~J., Kollasch, A.~W., McMahon, C., Simon, E.,
  Sander, C., Manglik, A., Kruse, A.~C., and Marks, D.~S.
\newblock Protein design and variant prediction using autoregressive generative
  models.
\newblock \emph{Nature Communications}, 12\penalty0 (1):\penalty0 2403, 2021.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and
  Ganguli]{sohl2015deep}
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2256--2265. PMLR, 2015.

\bibitem[Song \& Ermon(2019)Song and Ermon]{song2019generative}
Song, Y. and Ermon, S.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Song et~al.(2023)Song, Dhariwal, Chen, and
  Sutskever]{song2023consistency}
Song, Y., Dhariwal, P., Chen, M., and Sutskever, I.
\newblock Consistency models.
\newblock \emph{arXiv preprint arXiv:2303.01469}, 2023.

\bibitem[Sterling \& Irwin(2015)Sterling and Irwin]{sterling2015zinc}
Sterling, T. and Irwin, J.~J.
\newblock {ZINC} 15--ligand discovery for everyone.
\newblock \emph{Journal of Chemical Information and Modeling}, 55\penalty0
  (11):\penalty0 2324--2337, 2015.

\bibitem[Strauss \& Oliva(2021)Strauss and Oliva]{strauss2021arbitrary}
Strauss, R. and Oliva, J.~B.
\newblock Arbitrary conditional distributions with energy.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 752--763, 2021.

\bibitem[Subramanian et~al.(2023)Subramanian, Greenman, Gervaix, Yang, and
  G{\'o}mez-Bombarelli]{subramanian2023automated}
Subramanian, A., Greenman, K.~P., Gervaix, A., Yang, T., and
  G{\'o}mez-Bombarelli, R.
\newblock Automated patent extraction powers generative modeling in focused
  chemical spaces.
\newblock \emph{Digital Discovery}, 2\penalty0 (4):\penalty0 1006--1015, 2023.

\bibitem[Tabak \& Turner(2013)Tabak and Turner]{tabak2013family}
Tabak, E.~G. and Turner, C.~V.
\newblock A family of nonparametric density estimation algorithms.
\newblock \emph{Communications on Pure and Applied Mathematics}, 66\penalty0
  (2):\penalty0 145--164, 2013.

\bibitem[Tieleman(2008)]{tieleman2008training}
Tieleman, T.
\newblock Training restricted {B}oltzmann machines using approximations to the
  likelihood gradient.
\newblock In \emph{Proceedings of the 25th International Conference on Machine
  Learning}, pp.\  1064--1071, 2008.

\bibitem[Tran et~al.(2019)Tran, Vafa, Agrawal, Dinh, and
  Poole]{tran2019discrete}
Tran, D., Vafa, K., Agrawal, K., Dinh, L., and Poole, B.
\newblock Discrete flows: Invertible generative models of discrete data.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Uria et~al.(2014)Uria, Murray, and Larochelle]{uria2014deep}
Uria, B., Murray, I., and Larochelle, H.
\newblock A deep and tractable density estimator.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  467--475. PMLR, 2014.

\bibitem[Van Den~Oord et~al.(2016)Van Den~Oord, Kalchbrenner, and
  Kavukcuoglu]{van2016pixel}
Van Den~Oord, A., Kalchbrenner, N., and Kavukcuoglu, K.
\newblock Pixel recurrent neural networks.
\newblock In \emph{International conference on machine learning}, pp.\
  1747--1756. PMLR, 2016.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Wang et~al.(2022)Wang, Lisanza, Juergens, Tischer, Watson, Castro,
  Ragotte, Saragovi, Milles, Baek, et~al.]{wang2022scaffolding}
Wang, J., Lisanza, S., Juergens, D., Tischer, D., Watson, J.~L., Castro, K.~M.,
  Ragotte, R., Saragovi, A., Milles, L.~F., Baek, M., et~al.
\newblock Scaffolding protein functional sites using deep learning.
\newblock \emph{Science}, 377\penalty0 (6604):\penalty0 387--394, 2022.

\bibitem[Weininger et~al.(1989)Weininger, Weininger, and
  Weininger]{weininger1989smiles}
Weininger, D., Weininger, A., and Weininger, J.~L.
\newblock {SMILES}. 2. algorithm for generation of unique {SMILES} notation.
\newblock \emph{Journal of Chemical Information and Computer Sciences},
  29\penalty0 (2):\penalty0 97--101, 1989.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8:\penalty0 229--256, 1992.

\bibitem[Wu et~al.(2019)Wu, Wang, and Zhang]{wu2019solving}
Wu, D., Wang, L., and Zhang, P.
\newblock Solving statistical mechanics using variational autoregressive
  networks.
\newblock \emph{Physical review letters}, 122\penalty0 (8):\penalty0 080602,
  2019.

\bibitem[Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le]{yang2019xlnet}
Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.~R., and Le, Q.~V.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Yeh et~al.(2017)Yeh, Chen, Yian~Lim, Schwing, Hasegawa-Johnson, and
  Do]{yeh2017semantic}
Yeh, R.~A., Chen, C., Yian~Lim, T., Schwing, A.~G., Hasegawa-Johnson, M., and
  Do, M.~N.
\newblock Semantic image inpainting with deep generative models.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  5485--5493, 2017.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Chen, Malkin, and
  Bengio]{zhang2022unifying}
Zhang, D., Chen, R.~T., Malkin, N., and Bengio, Y.
\newblock Unifying generative models with gflownets.
\newblock \emph{arXiv preprint arXiv:2209.02606}, 2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Malkin, Liu, Volokhova,
  Courville, and Bengio]{zhang2022generative}
Zhang, D., Malkin, N., Liu, Z., Volokhova, A., Courville, A., and Bengio, Y.
\newblock Generative flow networks for discrete probabilistic modeling.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  26412--26428. PMLR, 2022{\natexlab{b}}.

\end{thebibliography}
