\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Krishnamurthy, and
  Sun]{agarwal2020flambe}
Agarwal, A., Kakade, S., Krishnamurthy, A., and Sun, W.
\newblock Flambe: Structural complexity and representation learning of low rank
  mdps.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 20095--20107, 2020.

\bibitem[Anahtarci et~al.(2023)Anahtarci, Kariksiz, and Saldi]{anahtarci2023q}
Anahtarci, B., Kariksiz, C.~D., and Saldi, N.
\newblock Q-learning in regularized mean-field games.
\newblock \emph{Dynamic Games and Applications}, 13\penalty0 (1):\penalty0
  89--117, 2023.

\bibitem[Auer et~al.(2008)Auer, Jaksch, and Ortner]{auer2008near}
Auer, P., Jaksch, T., and Ortner, R.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 21, 2008.

\bibitem[Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang, and
  Yang]{ayoub2020model}
Ayoub, A., Jia, Z., Szepesvari, C., Wang, M., and Yang, L.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  463--474. PMLR, 2020.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  263--272. PMLR, 2017.

\bibitem[Bai et~al.(2020)Bai, Jin, and Yu]{bai2020near}
Bai, Y., Jin, C., and Yu, T.
\newblock Near-optimal reinforcement learning with self-play.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 2159--2170, 2020.

\bibitem[Cardaliaguet \& Lehalle(2018)Cardaliaguet and
  Lehalle]{cardaliaguet2018mean}
Cardaliaguet, P. and Lehalle, C.-A.
\newblock Mean field game of controls and an application to trade crowding.
\newblock \emph{Mathematics and Financial Economics}, 12:\penalty0 335--363,
  2018.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, Li, Yuan, Gu, and
  Jordan]{chen2022general}
Chen, Z., Li, C.~J., Yuan, A., Gu, Q., and Jordan, M.~I.
\newblock A general framework for sample-efficient function approximation in
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2209.15634}, 2022{\natexlab{a}}.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Zhou, and Gu]{chen2022almost}
Chen, Z., Zhou, D., and Gu, Q.
\newblock Almost optimal algorithms for two-player zero-sum linear mixture
  markov games.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pp.\  227--261. PMLR, 2022{\natexlab{b}}.

\bibitem[Cui \& Koeppl(2021)Cui and Koeppl]{cui2021approximately}
Cui, K. and Koeppl, H.
\newblock Approximately solving mean field games via entropy-regularized deep
  reinforcement learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1909--1917. PMLR, 2021.

\bibitem[Cui et~al.(2023)Cui, Zhang, and Du]{cui2023breaking}
Cui, Q., Zhang, K., and Du, S.~S.
\newblock Breaking the curse of multiagents in a large state space: Rl in
  markov games with independent linear function approximation.
\newblock \emph{arXiv preprint arXiv:2302.03673}, 2023.

\bibitem[Daskalakis et~al.(2023)Daskalakis, Golowich, and
  Zhang]{daskalakis2023complexity}
Daskalakis, C., Golowich, N., and Zhang, K.
\newblock The complexity of markov equilibrium in stochastic games.
\newblock In \emph{The Thirty Sixth Annual Conference on Learning Theory}, pp.\
   4180--4234. PMLR, 2023.

\bibitem[Djehiche et~al.(2016)Djehiche, Tcheukam, and
  Tembine]{djehiche2016mean}
Djehiche, B., Tcheukam, A., and Tembine, H.
\newblock Mean-field-type games in engineering.
\newblock \emph{arXiv preprint arXiv:1605.03281}, 2016.

\bibitem[Du et~al.(2021)Du, Kakade, Lee, Lovett, Mahajan, Sun, and
  Wang]{du2021bilinear}
Du, S., Kakade, S., Lee, J., Lovett, S., Mahajan, G., Sun, W., and Wang, R.
\newblock Bilinear classes: A structural framework for provable generalization
  in rl.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2826--2836. PMLR, 2021.

\bibitem[Elie et~al.(2020)Elie, Perolat, Lauri{\`e}re, Geist, and
  Pietquin]{elie2020convergence}
Elie, R., Perolat, J., Lauri{\`e}re, M., Geist, M., and Pietquin, O.
\newblock On the convergence of model free learning in mean field games.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pp.\  7143--7150, 2020.

\bibitem[Foster et~al.(2023)Foster, Foster, Golowich, and
  Rakhlin]{foster2023complexity}
Foster, D., Foster, D.~J., Golowich, N., and Rakhlin, A.
\newblock On the complexity of multi-agent decision making: From learning in
  games to partial monitoring.
\newblock In \emph{The Thirty Sixth Annual Conference on Learning Theory}, pp.\
   2678--2792. PMLR, 2023.

\bibitem[Foster et~al.(2021)Foster, Kakade, Qian, and
  Rakhlin]{foster2021statistical}
Foster, D.~J., Kakade, S.~M., Qian, J., and Rakhlin, A.
\newblock The statistical complexity of interactive decision making.
\newblock \emph{arXiv preprint arXiv:2112.13487}, 2021.

\bibitem[Ghosh \& Aggarwal(2020)Ghosh and Aggarwal]{ghosh2020model}
Ghosh, A. and Aggarwal, V.
\newblock Model free reinforcement learning algorithm for stationary mean field
  equilibrium for multiple types of agents.
\newblock \emph{arXiv preprint arXiv:2012.15377}, 2020.

\bibitem[Gomes \& Pimentel()Gomes and Pimentel]{gomes2015economic}
Gomes, D.~A. and Pimentel, E.~A.
\newblock Economic models and mean-field games theory.

\bibitem[Guo et~al.(2019)Guo, Hu, Xu, and Zhang]{guo2019learning}
Guo, X., Hu, A., Xu, R., and Zhang, J.
\newblock Learning mean-field games.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Huang et~al.(2021)Huang, Lee, Wang, and Yang]{huang2021towards}
Huang, B., Lee, J.~D., Wang, Z., and Yang, Z.
\newblock Towards general function approximation in zero-sum markov games.
\newblock \emph{arXiv preprint arXiv:2107.14702}, 2021.

\bibitem[Huang et~al.(2022)Huang, Chen, Zhao, Qin, Jiang, and
  Liu]{huang2022towards}
Huang, J., Chen, J., Zhao, L., Qin, T., Jiang, N., and Liu, T.-Y.
\newblock Towards deployment-efficient reinforcement learning: Lower bound and
  optimality.
\newblock \emph{arXiv preprint arXiv:2202.06450}, 2022.

\bibitem[Huang et~al.(2024)Huang, Yardim, and He]{huang2023statistical}
Huang, J., Yardim, B., and He, N.
\newblock On the statistical efficiency of mean-field reinforcement learning
  with general function approximation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  289--297. PMLR, 2024.

\bibitem[Huang et~al.(2006)Huang, Malham{\'e}, and Caines]{huang2006large}
Huang, M., Malham{\'e}, R.~P., and Caines, P.~E.
\newblock Large population stochastic dynamic games: closed-loop mckean-vlasov
  systems and the nash certainty equivalence principle.
\newblock 2006.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang2017contextual}
Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R.~E.
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1704--1713. PMLR, 2017.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I.
\newblock Is q-learning provably efficient?
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Jin et~al.(2020)Jin, Yang, Wang, and Jordan]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pp.\  2137--2143. PMLR,
  2020.

\bibitem[Jin et~al.(2021{\natexlab{a}})Jin, Liu, and
  Miryoosefi]{jin2021bellman}
Jin, C., Liu, Q., and Miryoosefi, S.
\newblock Bellman eluder dimension: New rich classes of rl problems, and
  sample-efficient algorithms.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 13406--13418, 2021{\natexlab{a}}.

\bibitem[Jin et~al.(2021{\natexlab{b}})Jin, Liu, Wang, and Yu]{jin2021v}
Jin, C., Liu, Q., Wang, Y., and Yu, T.
\newblock V-learning--a simple, efficient, decentralized algorithm for
  multiagent rl.
\newblock \emph{arXiv preprint arXiv:2110.14555}, 2021{\natexlab{b}}.

\bibitem[Lasry \& Lions(2007)Lasry and Lions]{lasry2007mean}
Lasry, J.-M. and Lions, P.-L.
\newblock Mean field games.
\newblock \emph{Japanese journal of mathematics}, 2\penalty0 (1):\penalty0
  229--260, 2007.

\bibitem[Lauri{\`e}re et~al.(2022)Lauri{\`e}re, Perrin, Geist, and
  Pietquin]{lauriere2022learning}
Lauri{\`e}re, M., Perrin, S., Geist, M., and Pietquin, O.
\newblock Learning mean field games: A survey.
\newblock \emph{arXiv preprint arXiv:2205.12944}, 2022.

\bibitem[Levy et~al.(2022)Levy, Cassel, Cohen, and
  Mansour]{levy2022eluderbased}
Levy, O., Cassel, A., Cohen, A., and Mansour, Y.
\newblock Eluder-based regret for stochastic contextual mdps, 2022.

\bibitem[Mao et~al.(2022)Mao, Qiu, Wang, Franke, Kalbarczyk, Iyer, and
  Basar]{mao2022mean}
Mao, W., Qiu, H., Wang, C., Franke, H., Kalbarczyk, Z., Iyer, R., and Basar, T.
\newblock A mean-field game approach to cloud resource management with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Mishra et~al.(2020)Mishra, Vasal, and Vishwanath]{mishra2020model}
Mishra, R.~K., Vasal, D., and Vishwanath, S.
\newblock Model-free reinforcement learning for non-stationary mean field
  games.
\newblock In \emph{2020 59th IEEE Conference on Decision and Control (CDC)},
  pp.\  1032--1037. IEEE, 2020.

\bibitem[Modi et~al.(2021)Modi, Chen, Krishnamurthy, Jiang, and
  Agarwal]{modi2021model}
Modi, A., Chen, J., Krishnamurthy, A., Jiang, N., and Agarwal, A.
\newblock Model-free representation learning and exploration in low-rank mdps.
\newblock \emph{arXiv preprint arXiv:2102.07035}, 2021.

\bibitem[Moon \& Ba{\c{s}}ar(2018)Moon and Ba{\c{s}}ar]{moon2018linear}
Moon, J. and Ba{\c{s}}ar, T.
\newblock Linear quadratic mean field stackelberg differential games.
\newblock \emph{Automatica}, 97:\penalty0 200--213, 2018.

\bibitem[Ni et~al.(2022)Ni, Song, Zhang, Jin, and Wang]{ni2022representation}
Ni, C., Song, Y., Zhang, X., Jin, C., and Wang, M.
\newblock Representation learning for general-sum low-rank markov games.
\newblock \emph{arXiv preprint arXiv:2210.16976}, 2022.

\bibitem[Osband \& Van~Roy(2014)Osband and Van~Roy]{osband2014model}
Osband, I. and Van~Roy, B.
\newblock Model-based reinforcement learning and the eluder dimension.
\newblock \emph{Advances in Neural Information Processing Systems}, 27, 2014.

\bibitem[Pasztor et~al.(2021)Pasztor, Bogunovic, and
  Krause]{pasztor2021efficient}
Pasztor, B., Bogunovic, I., and Krause, A.
\newblock Efficient model-based multi-agent mean-field reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2107.04050}, 2021.

\bibitem[Perolat et~al.(2021)Perolat, Perrin, Elie, Lauri{\`e}re, Piliouras,
  Geist, Tuyls, and Pietquin]{perolat2021scaling}
Perolat, J., Perrin, S., Elie, R., Lauri{\`e}re, M., Piliouras, G., Geist, M.,
  Tuyls, K., and Pietquin, O.
\newblock Scaling up mean field games with online mirror descent.
\newblock \emph{arXiv preprint arXiv:2103.00623}, 2021.

\bibitem[Perrin et~al.(2020)Perrin, P{\'e}rolat, Lauri{\`e}re, Geist, Elie, and
  Pietquin]{perrin2020fictitious}
Perrin, S., P{\'e}rolat, J., Lauri{\`e}re, M., Geist, M., Elie, R., and
  Pietquin, O.
\newblock Fictitious play for mean field games: Continuous time analysis and
  applications.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 13199--13213, 2020.

\bibitem[Russo \& Van~Roy(2013)Russo and Van~Roy]{russo2013eluder}
Russo, D. and Van~Roy, B.
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Subramanian \& Mahajan(2019)Subramanian and
  Mahajan]{subramanian2019reinforcement}
Subramanian, J. and Mahajan, A.
\newblock Reinforcement learning in stationary mean-field games.
\newblock In \emph{Proceedings of the 18th International Conference on
  Autonomous Agents and Multi Agent Systems}, pp.\  251--259, 2019.

\bibitem[Subramanian et~al.(2020)Subramanian, Poupart, Taylor, and
  Hegde]{subramanian2020multi}
Subramanian, S.~G., Poupart, P., Taylor, M.~E., and Hegde, N.
\newblock Multi type mean field reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2002.02513}, 2020.

\bibitem[Sun et~al.(2019)Sun, Jiang, Krishnamurthy, Agarwal, and
  Langford]{sun2019model}
Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A., and Langford, J.
\newblock Model-based rl in contextual decision processes: Pac bounds and
  exponential improvements over model-free approaches.
\newblock In \emph{Conference on learning theory}, pp.\  2898--2933. PMLR,
  2019.

\bibitem[Uehara et~al.(2021)Uehara, Zhang, and Sun]{uehara2021representation}
Uehara, M., Zhang, X., and Sun, W.
\newblock Representation learning for online and offline rl in low-rank mdps.
\newblock \emph{arXiv preprint arXiv:2110.04652}, 2021.

\bibitem[uz~Zaman et~al.(2023)uz~Zaman, Miehling, and
  Ba{\c{s}}ar]{uz2023reinforcement}
uz~Zaman, M.~A., Miehling, E., and Ba{\c{s}}ar, T.
\newblock Reinforcement learning for non-stationary discrete-time
  linear--quadratic mean-field games in multiple populations.
\newblock \emph{Dynamic Games and Applications}, 13\penalty0 (1):\penalty0
  118--164, 2023.

\bibitem[Vasal \& Berry(2022)Vasal and Berry]{vasal2022master}
Vasal, D. and Berry, R.
\newblock Master equation for discrete-time stackelberg mean field games with a
  single leader.
\newblock In \emph{2022 IEEE 61st Conference on Decision and Control (CDC)},
  pp.\  5529--5535. IEEE, 2022.

\bibitem[Wang et~al.(2023)Wang, Liu, Bai, and Jin]{wang2023breaking}
Wang, Y., Liu, Q., Bai, Y., and Jin, C.
\newblock Breaking the curse of multiagency: Provably efficient decentralized
  multi-agent rl with function approximation.
\newblock \emph{arXiv preprint arXiv:2302.06606}, 2023.

\bibitem[Xie et~al.(2021)Xie, Yang, Wang, and Minca]{xie2021learning}
Xie, Q., Yang, Z., Wang, Z., and Minca, A.
\newblock Learning while playing in mean-field games: Convergence and
  optimality.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11436--11447. PMLR, 2021.

\bibitem[Xie et~al.(2022)Xie, Foster, Bai, Jiang, and Kakade]{xie2022role}
Xie, T., Foster, D.~J., Bai, Y., Jiang, N., and Kakade, S.~M.
\newblock The role of coverage in online reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2210.04157}, 2022.

\bibitem[Yardim et~al.(2022)Yardim, Cayci, Geist, and He]{yardim2022policy}
Yardim, B., Cayci, S., Geist, M., and He, N.
\newblock Policy mirror ascent for efficient and independent learning in mean
  field games.
\newblock \emph{arXiv preprint arXiv:2212.14449}, 2022.

\bibitem[Yardim et~al.(2024)Yardim, Goldman, and He]{yardim2024mean}
Yardim, B., Goldman, A., and He, N.
\newblock When is mean-field reinforcement learning tractable and relevant?
\newblock \emph{arXiv preprint arXiv:2402.05757}, 2024.

\bibitem[Zanette et~al.(2020)Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020learning}
Zanette, A., Lazaric, A., Kochenderfer, M., and Brunskill, E.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10978--10989. PMLR, 2020.

\bibitem[Zhang et~al.(2023)Zhang, Tan, Wang, and Yang]{zhang2023learning}
Zhang, F., Tan, V.~Y., Wang, Z., and Yang, Z.
\newblock Learning regularized monotone graphon mean-field games.
\newblock \emph{arXiv preprint arXiv:2310.08089}, 2023.

\bibitem[Zhang et~al.(2019)Zhang, Yang, and Basar]{zhang2019policy}
Zhang, K., Yang, Z., and Basar, T.
\newblock Policy optimization provably converges to nash equilibria in zero-sum
  linear quadratic games.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Zhang et~al.(2021)Zhang, Yang, and Ba{\c{s}}ar]{zhang2021multi}
Zhang, K., Yang, Z., and Ba{\c{s}}ar, T.
\newblock Multi-agent reinforcement learning: A selective overview of theories
  and algorithms.
\newblock \emph{Handbook of reinforcement learning and control}, pp.\
  321--384, 2021.

\bibitem[Zhong et~al.(2022)Zhong, Xiong, Zheng, Wang, Wang, Yang, and
  Zhang]{zhong2022posterior}
Zhong, H., Xiong, W., Zheng, S., Wang, L., Wang, Z., Yang, Z., and Zhang, T.
\newblock A posterior sampling framework for interactive decision making.
\newblock \emph{arXiv preprint arXiv:2211.01962}, 2022.

\end{thebibliography}
