\begin{thebibliography}{61}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{}\fi

\bibitem[{Agarwal et~al.(2020)Agarwal, Henaff, Kakade and Sun}]{agarwal2020pc}
\text{Agarwal, A.}, \text{Henaff, M.}, \text{Kakade, S.} and \text{Sun, W.} (2020).
\newblock Pc-pg: Policy cover directed exploration for provable policy gradient learning.
\newblock \textit{Advances in neural information processing systems}, \textbf{33} 13399--13412.

\bibitem[{Agarwal et~al.(2021)Agarwal, Kakade, Lee and Mahajan}]{agarwal2021theory}
\text{Agarwal, A.}, \text{Kakade, S.~M.}, \text{Lee, J.~D.} and \text{Mahajan, G.} (2021).
\newblock On the theory of policy gradient methods: Optimality, approximation, and distribution shift.
\newblock \textit{The Journal of Machine Learning Research}, \textbf{22} 4431--4506.

\bibitem[{Arora and Barak(2009)}]{arora2009computational}
\text{Arora, S.} and \text{Barak, B.} (2009).
\newblock \textit{Computational complexity: a modern approach}.
\newblock Cambridge University Press.

\bibitem[{Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang and Yang}]{ayoub2020model}
\text{Ayoub, A.}, \text{Jia, Z.}, \text{Szepesvari, C.}, \text{Wang, M.} and \text{Yang, L.} (2020).
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Azar et~al.(2017)Azar, Osband and Munos}]{azar2017minimax}
\text{Azar, M.~G.}, \text{Osband, I.} and \text{Munos, R.} (2017).
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Blondel and Tsitsiklis(2000)}]{blondel2000survey}
\text{Blondel, V.~D.} and \text{Tsitsiklis, J.~N.} (2000).
\newblock A survey of computational complexity results in systems and control.
\newblock \textit{Automatica}, \textbf{36} 1249--1274.

\bibitem[{Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider, Schulman, Tang and Zaremba}]{brockman2016openai}
\text{Brockman, G.}, \text{Cheung, V.}, \text{Pettersson, L.}, \text{Schneider, J.}, \text{Schulman, J.}, \text{Tang, J.} and \text{Zaremba, W.} (2016).
\newblock Openai gym.
\newblock \textit{arXiv preprint arXiv:1606.01540}.

\bibitem[{Cai et~al.(2020)Cai, Yang, Jin and Wang}]{cai2020provably}
\text{Cai, Q.}, \text{Yang, Z.}, \text{Jin, C.} and \text{Wang, Z.} (2020).
\newblock Provably efficient exploration in policy optimization.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Cen et~al.(2022)Cen, Cheng, Chen, Wei and Chi}]{cen2022fast}
\text{Cen, S.}, \text{Cheng, C.}, \text{Chen, Y.}, \text{Wei, Y.} and \text{Chi, Y.} (2022).
\newblock Fast global convergence of natural policy gradient methods with entropy regularization.
\newblock \textit{Operations Research}, \textbf{70} 2563--2578.

\bibitem[{Chen et~al.(2022)Chen, Li, Yuan, Gu and Jordan}]{chen2022general}
\text{Chen, Z.}, \text{Li, C.~J.}, \text{Yuan, A.}, \text{Gu, Q.} and \text{Jordan, M.~I.} (2022).
\newblock A general framework for sample-efficient function approximation in reinforcement learning.
\newblock \textit{arXiv preprint arXiv:2209.15634}.

\bibitem[{Chow and Tsitsiklis(1989)}]{chow1989complexity}
\text{Chow, C.-S.} and \text{Tsitsiklis, J.~N.} (1989).
\newblock The complexity of dynamic programming.
\newblock \textit{Journal of complexity}, \textbf{5} 466--488.

\bibitem[{Cook(1971)}]{cook71}
\text{Cook, S.~A.} (1971).
\newblock The complexity of theorem-proving procedures.
\newblock In \textit{Proceedings of the Third Annual ACM Symposium on Theory of Computing}. ACM, 1971.

\bibitem[{Dong et~al.(2020)Dong, Luo, Yu, Finn and Ma}]{dong2020expressivity}
\text{Dong, K.}, \text{Luo, Y.}, \text{Yu, T.}, \text{Finn, C.} and \text{Ma, T.} (2020).
\newblock On the expressivity of neural networks for deep reinforcement learning.
\newblock In \textit{International conference on machine learning}. PMLR.

\bibitem[{Du et~al.(2021)Du, Kakade, Lee, Lovett, Mahajan, Sun and Wang}]{du2021bilinear}
\text{Du, S.}, \text{Kakade, S.}, \text{Lee, J.}, \text{Lovett, S.}, \text{Mahajan, G.}, \text{Sun, W.} and \text{Wang, R.} (2021).
\newblock Bilinear classes: A structural framework for provable generalization in rl.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Du et~al.(2019)Du, Kakade, Wang and Yang}]{du2019good}
\text{Du, S.~S.}, \text{Kakade, S.~M.}, \text{Wang, R.} and \text{Yang, L.~F.} (2019).
\newblock Is a good representation sufficient for sample efficient reinforcement learning?
\newblock \textit{arXiv preprint arXiv:1910.03016}.

\bibitem[{Feng et~al.(2024)Feng, Zhang, Gu, Ye, He and Wang}]{feng2024towards}
\text{Feng, G.}, \text{Zhang, B.}, \text{Gu, Y.}, \text{Ye, H.}, \text{He, D.} and \text{Wang, L.} (2024).
\newblock Towards revealing the mystery behind chain of thought: a theoretical perspective.
\newblock \textit{Advances in Neural Information Processing Systems}, \textbf{36}.

\bibitem[{Foster et~al.(2021)Foster, Kakade, Qian and Rakhlin}]{foster2021statistical}
\text{Foster, D.~J.}, \text{Kakade, S.~M.}, \text{Qian, J.} and \text{Rakhlin, A.} (2021).
\newblock The statistical complexity of interactive decision making.
\newblock \textit{arXiv preprint arXiv:2112.13487}.

\bibitem[{Fujimoto et~al.(2018)Fujimoto, Hoof and Meger}]{fujimoto2018addressing}
\text{Fujimoto, S.}, \text{Hoof, H.} and \text{Meger, D.} (2018).
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \textit{International conference on machine learning}. PMLR.

\bibitem[{Jaksch et~al.(2010)Jaksch, Ortner and Auer}]{jaksch2010near}
\text{Jaksch, T.}, \text{Ortner, R.} and \text{Auer, P.} (2010).
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \textit{Journal of Machine Learning Research}, \textbf{11} 1563--1600.

\bibitem[{Janner et~al.(2019)Janner, Fu, Zhang and Levine}]{janner2019trust}
\text{Janner, M.}, \text{Fu, J.}, \text{Zhang, M.} and \text{Levine, S.} (2019).
\newblock When to trust your model: Model-based policy optimization.
\newblock \textit{Advances in neural information processing systems}, \textbf{32}.

\bibitem[{Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford and Schapire}]{jiang@2017}
\text{Jiang, N.}, \text{Krishnamurthy, A.}, \text{Agarwal, A.}, \text{Langford, J.} and \text{Schapire, R.~E.} (2017).
\newblock Contextual decision processes with low {B}ellman rank are {PAC}-learnable.
\newblock In \textit{Proceedings of the 34th International Conference on Machine Learning}, vol.~70 of \textit{Proceedings of Machine Learning Research}. PMLR.

\bibitem[{Jin et~al.(2018)Jin, Allen-Zhu, Bubeck and Jordan}]{jin2018q}
\text{Jin, C.}, \text{Allen-Zhu, Z.}, \text{Bubeck, S.} and \text{Jordan, M.~I.} (2018).
\newblock Is q-learning provably efficient?
\newblock \textit{Advances in neural information processing systems}, \textbf{31}.

\bibitem[{Jin et~al.(2021)Jin, Liu and Miryoosefi}]{jin2021bellman}
\text{Jin, C.}, \text{Liu, Q.} and \text{Miryoosefi, S.} (2021).
\newblock Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms.
\newblock \textit{Advances in neural information processing systems}, \textbf{34} 13406--13418.

\bibitem[{Jin et~al.(2020)Jin, Yang, Wang and Jordan}]{jin2020provably}
\text{Jin, C.}, \text{Yang, Z.}, \text{Wang, Z.} and \text{Jordan, M.~I.} (2020).
\newblock Provably efficient reinforcement learning with linear function approximation.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Jin et~al.(2022)Jin, Ren, Yang and Wang}]{jin2022policy}
\text{Jin, Y.}, \text{Ren, Z.}, \text{Yang, Z.} and \text{Wang, Z.} (2022).
\newblock Policy learning" without''overlap: Pessimism and generalized empirical bernstein's inequality.
\newblock \textit{arXiv preprint arXiv:2212.09900}.

\bibitem[{Kakade(2001)}]{kakade2001natural}
\text{Kakade, S.~M.} (2001).
\newblock A natural policy gradient.
\newblock \textit{Advances in neural information processing systems}, \textbf{14}.

\bibitem[{Kober et~al.(2013)Kober, Bagnell and Peters}]{kober2013reinforcement}
\text{Kober, J.}, \text{Bagnell, J.~A.} and \text{Peters, J.} (2013).
\newblock Reinforcement learning in robotics: A survey.
\newblock \textit{The International Journal of Robotics Research}, \textbf{32} 1238--1274.

\bibitem[{Ladner(1975)}]{ladner1975circuit}
\text{Ladner, R.~E.} (1975).
\newblock The circuit value problem is log space complete for p.
\newblock \textit{ACM Sigact News}, \textbf{7} 18--20.

\bibitem[{Lan(2023)}]{lan2023policy}
\text{Lan, G.} (2023).
\newblock Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes.
\newblock \textit{Mathematical programming}, \textbf{198} 1059--1106.

\bibitem[{LeCun et~al.(2015)LeCun, Bengio and Hinton}]{lecun2015deep}
\text{LeCun, Y.}, \text{Bengio, Y.} and \text{Hinton, G.} (2015).
\newblock Deep learning.
\newblock \textit{nature}, \textbf{521} 436--444.

\bibitem[{Levin(1973)}]{levin1973universal}
\text{Levin, L.~A.} (1973).
\newblock Universal sequential search problems.
\newblock \textit{Problemy peredachi informatsii}, \textbf{9} 115--116.

\bibitem[{Littman et~al.(1998)Littman, Goldsmith and Mundhenk}]{littman1998computational}
\text{Littman, M.~L.}, \text{Goldsmith, J.} and \text{Mundhenk, M.} (1998).
\newblock The computational complexity of probabilistic planning.
\newblock \textit{Journal of Artificial Intelligence Research}, \textbf{9} 1--36.

\bibitem[{Liu et~al.(2019)Liu, Cai, Yang and Wang}]{liu2019neural}
\text{Liu, B.}, \text{Cai, Q.}, \text{Yang, Z.} and \text{Wang, Z.} (2019).
\newblock Neural trust region/proximal policy optimization attains globally optimal policy.
\newblock \textit{Advances in neural information processing systems}, \textbf{32}.

\bibitem[{Liu et~al.(2023{\natexlab{a}})Liu, Weisz, Gy{\"o}rgy, Jin and Szepesv{\'a}ri}]{liu2023optimistic}
\text{Liu, Q.}, \text{Weisz, G.}, \text{Gy{\"o}rgy, A.}, \text{Jin, C.} and \text{Szepesv{\'a}ri, C.} (2023{\natexlab{a}}).
\newblock Optimistic natural policy gradient: a simple efficient policy optimization framework for online rl.
\newblock \textit{arXiv preprint arXiv:2305.11032}.

\bibitem[{Liu et~al.(2023{\natexlab{b}})Liu, Lu, Xiong, Zhong, Hu, Zhang, Zheng, Yang and Wang}]{liu2023one}
\text{Liu, Z.}, \text{Lu, M.}, \text{Xiong, W.}, \text{Zhong, H.}, \text{Hu, H.}, \text{Zhang, S.}, \text{Zheng, S.}, \text{Yang, Z.} and \text{Wang, Z.} (2023{\natexlab{b}}).
\newblock One objective to rule them all: A maximization objective fusing estimation and planning for exploration.
\newblock \textit{arXiv preprint arXiv:2305.18258}.

\bibitem[{Merrill and Sabharwal(2023)}]{merrill2023parallelism}
\text{Merrill, W.} and \text{Sabharwal, A.} (2023).
\newblock The parallelism tradeoff: Limitations of log-precision transformers.
\newblock \textit{Transactions of the Association for Computational Linguistics}, \textbf{11} 531--545.

\bibitem[{Papadimitriou and Tsitsiklis(1987)}]{papadimitriou1987complexity}
\text{Papadimitriou, C.~H.} and \text{Tsitsiklis, J.~N.} (1987).
\newblock The complexity of markov decision processes.
\newblock \textit{Mathematics of operations research}, \textbf{12} 441--450.

\bibitem[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford and Klimov}]{schulman2017proximal}
\text{Schulman, J.}, \text{Wolski, F.}, \text{Dhariwal, P.}, \text{Radford, A.} and \text{Klimov, O.} (2017).
\newblock Proximal policy optimization algorithms.
\newblock \textit{arXiv preprint arXiv:1707.06347}.

\bibitem[{Shani et~al.(2020)Shani, Efroni, Rosenberg and Mannor}]{shani2020optimistic}
\text{Shani, L.}, \text{Efroni, Y.}, \text{Rosenberg, A.} and \text{Mannor, S.} (2020).
\newblock Optimistic policy optimization with bandit feedback.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Sherman et~al.(2023)Sherman, Cohen, Koren and Mansour}]{sherman2023rate}
\text{Sherman, U.}, \text{Cohen, A.}, \text{Koren, T.} and \text{Mansour, Y.} (2023).
\newblock Rate-optimal policy optimization for linear markov decision processes.
\newblock \textit{arXiv preprint arXiv:2308.14642}.

\bibitem[{Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot et~al.}]{silver2016mastering}
\text{Silver, D.}, \text{Huang, A.}, \text{Maddison, C.~J.}, \text{Guez, A.}, \text{Sifre, L.}, \text{Van Den~Driessche, G.}, \text{Schrittwieser, J.}, \text{Antonoglou, I.}, \text{Panneershelvam, V.}, \text{Lanctot, M.} \text{et~al.} (2016).
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \textit{nature}, \textbf{529} 484--489.

\bibitem[{Sun et~al.(2019)Sun, Jiang, Krishnamurthy, Agarwal and Langford}]{sun2019model}
\text{Sun, W.}, \text{Jiang, N.}, \text{Krishnamurthy, A.}, \text{Agarwal, A.} and \text{Langford, J.} (2019).
\newblock Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches.
\newblock In \textit{Conference on learning theory}. PMLR.

\bibitem[{Sutton and Barto(2018)}]{sutton2018reinforcement}
\text{Sutton, R.~S.} and \text{Barto, A.~G.} (2018).
\newblock \textit{Reinforcement learning: An introduction}.
\newblock MIT press.

\bibitem[{Sutton et~al.(1999)Sutton, McAllester, Singh and Mansour}]{sutton1999policy}
\text{Sutton, R.~S.}, \text{McAllester, D.}, \text{Singh, S.} and \text{Mansour, Y.} (1999).
\newblock Policy gradient methods for reinforcement learning with function approximation.
\newblock \textit{Advances in neural information processing systems}, \textbf{12}.

\bibitem[{Tu and Recht(2019)}]{tu2019gap}
\text{Tu, S.} and \text{Recht, B.} (2019).
\newblock The gap between model-based and model-free methods on the linear quadratic regulator: An asymptotic viewpoint.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Uehara and Sun(2021)}]{uehara2021pessimistic}
\text{Uehara, M.} and \text{Sun, W.} (2021).
\newblock Pessimistic model-based offline reinforcement learning under partial coverage.
\newblock \textit{arXiv preprint arXiv:2107.06226}.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser and Polosukhin}]{vaswani2017attention}
\text{Vaswani, A.}, \text{Shazeer, N.}, \text{Parmar, N.}, \text{Uszkoreit, J.}, \text{Jones, L.}, \text{Gomez, A.~N.}, \text{Kaiser, {\L}.} and \text{Polosukhin, I.} (2017).
\newblock Attention is all you need.
\newblock \textit{Advances in neural information processing systems}, \textbf{30}.

\bibitem[{Wu et~al.(2022)Wu, Yang, Zhong, Wang, Du and Jiao}]{wu2022nearly}
\text{Wu, T.}, \text{Yang, Y.}, \text{Zhong, H.}, \text{Wang, L.}, \text{Du, S.} and \text{Jiao, J.} (2022).
\newblock Nearly optimal policy optimization with stable at any time guarantee.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Xiao(2022)}]{xiao2022convergence}
\text{Xiao, L.} (2022).
\newblock On the convergence rates of policy gradient methods.
\newblock \textit{The Journal of Machine Learning Research}, \textbf{23} 12887--12922.

\bibitem[{Xie et~al.(2021)Xie, Cheng, Jiang, Mineiro and Agarwal}]{xie2021bellman}
\text{Xie, T.}, \text{Cheng, C.-A.}, \text{Jiang, N.}, \text{Mineiro, P.} and \text{Agarwal, A.} (2021).
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock \textit{Advances in neural information processing systems}, \textbf{34} 6683--6694.

\bibitem[{Xu and Zeevi(2023)}]{xu2023bayesian}
\text{Xu, Y.} and \text{Zeevi, A.} (2023).
\newblock Bayesian design principles for frequentist sequential learning.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Yang and Wang(2019)}]{yang2019sample}
\text{Yang, L.} and \text{Wang, M.} (2019).
\newblock Sample-optimal parametric q-learning using linearly additive features.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn and Ma}]{yu2020mopo}
\text{Yu, T.}, \text{Thomas, G.}, \text{Yu, L.}, \text{Ermon, S.}, \text{Zou, J.~Y.}, \text{Levine, S.}, \text{Finn, C.} and \text{Ma, T.} (2020).
\newblock Mopo: Model-based offline policy optimization.
\newblock \textit{Advances in Neural Information Processing Systems}, \textbf{33} 14129--14142.

\bibitem[{Zanette and Brunskill(2019)}]{zanette2019tighter}
\text{Zanette, A.} and \text{Brunskill, E.} (2019).
\newblock Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Zhang et~al.(2023)Zhang, Chen, Lee and Du}]{zhang2023settling}
\text{Zhang, Z.}, \text{Chen, Y.}, \text{Lee, J.~D.} and \text{Du, S.~S.} (2023).
\newblock Settling the sample complexity of online reinforcement learning.
\newblock \textit{arXiv preprint arXiv:2307.13586}.

\bibitem[{Zhang et~al.(2021)Zhang, Ji and Du}]{zhang2021isreinforcement}
\text{Zhang, Z.}, \text{Ji, X.} and \text{Du, S.} (2021).
\newblock Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Zhong et~al.(2022)Zhong, Xiong, Zheng, Wang, Wang, Yang and Zhang}]{zhong2022gec}
\text{Zhong, H.}, \text{Xiong, W.}, \text{Zheng, S.}, \text{Wang, L.}, \text{Wang, Z.}, \text{Yang, Z.} and \text{Zhang, T.} (2022).
\newblock Gec: A unified framework for interactive decision making in mdp, pomdp, and beyond.
\newblock \textit{arXiv preprint arXiv:2211.01962}.

\bibitem[{Zhong et~al.(2021)Zhong, Yang, Wang and Szepesv{\'a}ri}]{zhong2021optimistic}
\text{Zhong, H.}, \text{Yang, Z.}, \text{Wang, Z.} and \text{Szepesv{\'a}ri, C.} (2021).
\newblock Optimistic policy optimization is provably efficient in non-stationary mdps.
\newblock \textit{arXiv preprint arXiv:2110.08984}.

\bibitem[{Zhong and Zhang(2023)}]{zhong2023theoretical}
\text{Zhong, H.} and \text{Zhang, T.} (2023).
\newblock A theoretical analysis of optimistic proximal policy optimization in linear markov decision processes.
\newblock \textit{arXiv preprint arXiv:2305.08841}.

\bibitem[{Zhou et~al.(2021)Zhou, Gu and Szepesvari}]{zhou2021nearly}
\text{Zhou, D.}, \text{Gu, Q.} and \text{Szepesvari, C.} (2021).
\newblock Nearly minimax optimal reinforcement learning for linear mixture markov decision processes.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Zhu et~al.(2023)Zhu, Huang and Russell}]{zhu2023representation}
\text{Zhu, H.}, \text{Huang, B.} and \text{Russell, S.} (2023).
\newblock On representation complexity of model-based and model-free reinforcement learning.
\newblock \textit{arXiv preprint arXiv:2310.01706}.

\end{thebibliography}
