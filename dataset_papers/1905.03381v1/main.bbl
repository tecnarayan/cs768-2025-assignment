\begin{thebibliography}{10}

\bibitem{alain2015variance}
Guillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron Courville, and Yoshua
  Bengio.
\newblock Variance reduction in sgd by distributed importance sampling.
\newblock {\em arXiv preprint arXiv:1511.06481}, 2015.

\bibitem{bengio2015scheduled}
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer.
\newblock Scheduled sampling for sequence prediction with recurrent neural
  networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1171--1179, 2015.

\bibitem{bengio2009curriculum}
Yoshua Bengio, J{\'e}r{\^o}me Louradour, Ronan Collobert, and Jason Weston.
\newblock Curriculum learning.
\newblock In {\em Proceedings of the 26th annual international conference on
  machine learning}, pages 41--48. ACM, 2009.

\bibitem{bottou2018optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock {\em SIAM Review}, 60(2):223--311, 2018.

\bibitem{CC01a}
Chih-Chung Chang and Chih-Jen Lin.
\newblock {LIBSVM}: A library for support vector machines.
\newblock {\em ACM Transactions on Intelligent Systems and Technology},
  2:27:1--27:27, 2011.
\newblock Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}.

\bibitem{chang2017active}
Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum.
\newblock Active bias: Training more accurate neural networks by emphasizing
  high variance samples.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1002--1012, 2017.

\bibitem{cohen2017emnist}
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr{\'e} van Schaik.
\newblock {EMNIST}: an extension of {MNIST} to handwritten letters.
\newblock {\em arXiv preprint arXiv:1702.05373}, 2017.

\bibitem{collobert2008unified}
Ronan Collobert and Jason Weston.
\newblock A unified architecture for natural language processing: Deep neural
  networks with multitask learning.
\newblock In {\em Proceedings of the 25th international conference on Machine
  learning}, pages 160--167. ACM, 2008.

\bibitem{elliott-EtAl:2016:VL16}
D.~{Elliott}, S.~{Frank}, K.~{Sima'an}, and L.~{Specia}.
\newblock Multi30k: Multilingual english-german image descriptions.
\newblock pages 70--74, 2016.

\bibitem{REF08a}
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
\newblock {LIBLINEAR}: A library for large linear classification.
\newblock {\em Journal of Machine Learning Research}, 9:1871--1874, 2008.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hsieh2008dual}
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S~Sathiya Keerthi, and
  Sellamanickam Sundararajan.
\newblock A dual coordinate descent method for large-scale linear svm.
\newblock In {\em Proceedings of the 25th international conference on Machine
  learning}, pages 408--415. ACM, 2008.

\bibitem{jiang2015self}
Lu~Jiang, Deyu Meng, Qian Zhao, Shiguang Shan, and Alexander~G Hauptmann.
\newblock Self-paced curriculum learning.
\newblock In {\em AAAI.}, page Vol. 2. No. 5.4., 2015.

\bibitem{jiang2017mentornet}
Lu~Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li~Fei-Fei.
\newblock Mentornet: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock {\em arXiv preprint arXiv:1712.05055}, 2017.

\bibitem{katharopoulos2017biased}
Angelos Katharopoulos and Fran{\c{c}}ois Fleuret.
\newblock Biased importance sampling for deep neural network training.
\newblock {\em arXiv preprint arXiv:1706.00043}, 2017.

\bibitem{katharopoulos2018not}
Angelos Katharopoulos and Fran{\c{c}}ois Fleuret.
\newblock Not all samples are created equal: Deep learning with importance
  sampling.
\newblock {\em arXiv preprint arXiv:1803.00942}, 2018.

\bibitem{kim2018screenernet}
Tae-Hoon Kim and Jonghyun Choi.
\newblock Screenernet: Learning curriculum for neural networks.
\newblock {\em arXiv preprint arXiv:1801.00904}, 2018.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem{kumar2010self}
M~Pawan Kumar, Benjamin Packer, and Daphne Koller.
\newblock Self-paced learning for latent variable models.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1189--1197, 2010.

\bibitem{langkvist2014review}
Martin L{\"a}ngkvist, Lars Karlsson, and Amy Loutfi.
\newblock A review of unsupervised feature learning and deep learning for
  time-series modeling.
\newblock {\em Pattern Recognition Letters}, 42:11--24, 2014.

\bibitem{lecun-mnisthandwrittendigit-2010}
Yann LeCun and Corinna Cortes.
\newblock {MNIST} handwritten digit database.
\newblock 2010.

\bibitem{li2017self}
Hao Li and Maoguo Gong.
\newblock Self-paced convolutional neural networks.
\newblock In {\em Proceedings of the International Joint Conference on
  Artificial Intelligence}, 2017.

\bibitem{needell2014stochastic}
Deanna Needell, Rachel Ward, and Nati Srebro.
\newblock Stochastic gradient descent, weighted sampling, and the randomized
  kaczmarz algorithm.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1017--1025, 2014.

\bibitem{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in {P}y{T}orch.
\newblock 2017.

\bibitem{schmidt2017minimizing}
Mark Schmidt, Nicolas Le~Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock {\em Mathematical Programming}, 162(1-2):83--112, 2017.

\bibitem{shalev2011pegasos}
Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter.
\newblock Pegasos: Primal estimated sub-gradient solver for svm.
\newblock {\em Mathematical programming}, 127(1):3--30, 2011.

\bibitem{tadic2017asymptotic}
Vladislav~B Tadi{\'c}, Arnaud Doucet, et~al.
\newblock Asymptotic bias of stochastic gradient search.
\newblock {\em The Annals of Applied Probability}, 27(6):3255--3304, 2017.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5998--6008, 2017.

\bibitem{xiao2017fashion}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-{MNIST}: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock {\em arXiv preprint arXiv:1708.07747}, 2017.

\bibitem{zhao2015stochastic}
Peilin Zhao and Tong Zhang.
\newblock Stochastic optimization with importance sampling for regularized loss
  minimization.
\newblock In {\em international conference on machine learning}, pages 1--9,
  2015.

\end{thebibliography}
