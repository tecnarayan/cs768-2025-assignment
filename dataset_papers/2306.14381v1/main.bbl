\begin{thebibliography}{22}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adil et~al.(2021)Adil, Bullins, and Sachdeva]{adil2021unifying}
Adil, D., Bullins, B., and Sachdeva, S.
\newblock Unifying width-reduced methods for quasi-self-concordant
  optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Axiotis \& Sviridenko(2021)Axiotis and Sviridenko]{axiotis2021sparse}
Axiotis, K. and Sviridenko, M.
\newblock Sparse convex optimization via adaptively regularized hard
  thresholding.
\newblock \emph{Journal of Machine Learning Research}, 22:\penalty0 1--47,
  2021.

\bibitem[Axiotis \& Sviridenko(2022)Axiotis and
  Sviridenko]{axiotis2022iterative}
Axiotis, K. and Sviridenko, M.
\newblock Iterative hard thresholding with adaptive regularization: Sparser
  solutions without sacrificing runtime.
\newblock \emph{arXiv preprint arXiv:2204.08274}, 2022.

\bibitem[Bach(2010)]{bach2010self}
Bach, F.
\newblock Self-concordant analysis for logistic regression.
\newblock \emph{Electronic Journal of Statistics}, 4:\penalty0 384--414, 2010.

\bibitem[Blumensath \& Davies(2009)Blumensath and Davies]{IHT}
Blumensath, T. and Davies, M.~E.
\newblock Iterative hard thresholding for compressed sensing.
\newblock \emph{Applied and computational harmonic analysis}, 27\penalty0
  (3):\penalty0 265--274, 2009.

\bibitem[Bunea(2008)]{bunea2008honest}
Bunea, F.
\newblock Honest variable selection in linear and logistic regression models
  via $\ell_1$ and $\ell_1+\ell_2$ penalization.
\newblock \emph{Electronic Journal of Statistics}, 2:\penalty0 1153--1194,
  2008.

\bibitem[Carmon et~al.(2020)Carmon, Jambulapati, Jiang, Jin, Lee, Sidford, and
  Tian]{carmon2020acceleration}
Carmon, Y., Jambulapati, A., Jiang, Q., Jin, Y., Lee, Y.~T., Sidford, A., and
  Tian, K.
\newblock Acceleration with a ball optimization oracle.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 19052--19063, 2020.

\bibitem[Cohen et~al.(2017)Cohen, Madry, Tsipras, and Vladu]{cohen2017matrix}
Cohen, M.~B., Madry, A., Tsipras, D., and Vladu, A.
\newblock Matrix scaling and balancing via box constrained newton's method and
  interior point methods.
\newblock In \emph{2017 IEEE 58th Annual Symposium on Foundations of Computer
  Science (FOCS)}, pp.\  902--913. IEEE, 2017.

\bibitem[Freund et~al.(2018)Freund, Grigas, and Mazumder]{freund2018condition}
Freund, R.~M., Grigas, P., and Mazumder, R.
\newblock Condition number analysis of logistic regression, and its
  implications for standard first-order solution methods.
\newblock \emph{arXiv preprint arXiv:1810.08727}, 2018.

\bibitem[Jain et~al.(2011)Jain, Tewari, and Dhillon]{JTD11}
Jain, P., Tewari, A., and Dhillon, I.~S.
\newblock Orthogonal matching pursuit with replacement.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1215--1223, 2011.

\bibitem[Jain et~al.(2014)Jain, Tewari, and Kar]{JTK14}
Jain, P., Tewari, A., and Kar, P.
\newblock On iterative hard thresholding methods for high-dimensional
  m-estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  685--693, 2014.

\bibitem[Ji \& Telgarsky(2018)Ji and Telgarsky]{ji2018risk}
Ji, Z. and Telgarsky, M.
\newblock Risk and parameter convergence of logistic regression.
\newblock \emph{arXiv preprint arXiv:1803.07300}, 2018.

\bibitem[Ji \& Telgarsky(2021)Ji and Telgarsky]{ji2021characterizing}
Ji, Z. and Telgarsky, M.
\newblock Characterizing the implicit bias via a primal-dual analysis.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  772--804. PMLR, 2021.

\bibitem[Karimireddy et~al.(2018)Karimireddy, Stich, and
  Jaggi]{karimireddy2018global}
Karimireddy, S.~P., Stich, S.~U., and Jaggi, M.
\newblock Global linear convergence of newton's method without strong-convexity
  or lipschitz gradients.
\newblock \emph{arXiv preprint arXiv:1806.00413}, 2018.

\bibitem[Nacson et~al.(2019)Nacson, Lee, Gunasekar, Savarese, Srebro, and
  Soudry]{nacson2019convergence}
Nacson, M.~S., Lee, J., Gunasekar, S., Savarese, P. H.~P., Srebro, N., and
  Soudry, D.
\newblock Convergence of gradient descent on separable data.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  3420--3428. PMLR, 2019.

\bibitem[Natarajan(1995)]{Natarajan95}
Natarajan, B.~K.
\newblock Sparse approximate solutions to linear systems.
\newblock \emph{SIAM journal on computing}, 24\penalty0 (2):\penalty0 227--234,
  1995.

\bibitem[R{\"a}tsch et~al.(2001)R{\"a}tsch, Mika, and
  Warmuth]{ratsch2001convergence}
R{\"a}tsch, G., Mika, S., and Warmuth, M.~K.
\newblock On the convergence of leveraging.
\newblock \emph{Advances in Neural Information Processing Systems}, 14, 2001.

\bibitem[Shalev-Shwartz et~al.(2010)Shalev-Shwartz, Srebro, and Zhang]{SSZ10}
Shalev-Shwartz, S., Srebro, N., and Zhang, T.
\newblock Trading accuracy for sparsity in optimization problems with sparsity
  constraints.
\newblock \emph{SIAM Journal on Optimization}, 20\penalty0 (6):\penalty0
  2807--2832, 2010.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Soudry, D., Hoffer, E., Nacson, M.~S., Gunasekar, S., and Srebro, N.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Telgarsky(2013)]{telgarsky2013margins}
Telgarsky, M.
\newblock Margins, shrinkage, and boosting.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  307--315. PMLR, 2013.

\bibitem[Telgarsky \& Singer(2012)Telgarsky and Singer]{telgarsky2012primal}
Telgarsky, M. and Singer, Y.
\newblock A primal-dual convergence analysis of boosting.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0 (3), 2012.

\bibitem[Van~de Geer(2008)]{van2008high}
Van~de Geer, S.~A.
\newblock High-dimensional generalized linear models and the lasso.
\newblock \emph{The Annals of Statistics}, 36\penalty0 (2):\penalty0 614--645,
  2008.

\end{thebibliography}
