\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Chen et~al.(2019)Chen, Sun, and Jin]{chen2019communication}
Chen, Y., Sun, X., and Jin, Y.
\newblock Communication-efficient federated deep learning with layerwise
  asynchronous model update and temporally weighted aggregation.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  2019.

\bibitem[Diao et~al.(2021)Diao, Ding, and Tarokh]{diao2020heterofl}
Diao, E., Ding, J., and Tarokh, V.
\newblock {HeteroFL}: Computation and communication efficient federated
  learning for heterogeneous clients.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2021.

\bibitem[Fallah et~al.(2020)Fallah, Mokhtari, and
  Ozdaglar]{fallah2020personalized}
Fallah, A., Mokhtari, A., and Ozdaglar, A.
\newblock Personalized federated learning: A meta-learning approach.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Gholami et~al.(2021)Gholami, Kim, Dong, Yao, Mahoney, and
  Keutzer]{gholami_survey_2021}
Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M.~W., and Keutzer, K.
\newblock A survey of quantization methods for efficient neural network
  inference.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision and Pattern Recognition (CVPR)}, 2021.

\bibitem[Haddadpour et~al.(2021)Haddadpour, Kamani, Mokhtari, and
  Mahdavi]{haddadpour_federated_2020}
Haddadpour, F., Kamani, M.~M., Mokhtari, A., and Mahdavi, M.
\newblock Federated learning with compression: Unified analysis and sharp
  guarantees.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics (AISTATS)}, 2021.

\bibitem[He et~al.(2020)He, Annavaram, and Avestimehr]{he2020group}
He, C., Annavaram, M., and Avestimehr, S.
\newblock Group knowledge transfer: Federated learning of large cnns at the
  edge.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision and Pattern Recognition (CVPR)}, 2016.

\bibitem[Hubara et~al.(2016)Hubara, Courbariaux, Soudry, El-Yaniv, and
  Bengio]{hubara_binarized_2016}
Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y.
\newblock Binarized {Neural} {Networks}.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2016.

\bibitem[Iandola et~al.(2014)Iandola, Moskewicz, Karayev, Girshick, Darrell,
  and Keutzer]{iandola2014densenet}
Iandola, F., Moskewicz, M., Karayev, S., Girshick, R., Darrell, T., and
  Keutzer, K.
\newblock Densenet: Implementing efficient convnet descriptor pyramids.
\newblock \emph{arXiv preprint arXiv:1404.1869}, 2014.

\bibitem[Jeong et~al.(2021)Jeong, Yoon, Yang, and Hwang]{jeong2020federated}
Jeong, W., Yoon, J., Yang, E., and Hwang, S.~J.
\newblock Federated semi-supervised learning with inter-client consistency and
  disjoint learning.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2021.

\bibitem[Jiang et~al.(2020)Jiang, Shan, and Zhang]{jiang2020federated}
Jiang, D., Shan, C., and Zhang, Z.
\newblock Federated learning algorithm based on knowledge distillation.
\newblock In \emph{2020 International Conference on Artificial Intelligence and
  Computer Engineering (ICAICE)}. IEEE, 2020.

\bibitem[Jiang et~al.(2019)Jiang, Kone{\v{c}}n{\`y}, Rush, and
  Kannan]{jiang2019improving}
Jiang, Y., Kone{\v{c}}n{\`y}, J., Rush, K., and Kannan, S.
\newblock Improving federated learning personalization via model agnostic meta
  learning.
\newblock \emph{arXiv preprint arXiv:1909.12488}, 2019.

\bibitem[Lee et~al.(2020)Lee, Kwon, Kim, Jeon, Park, and Yun]{lee_flexor_2020}
Lee, D., Kwon, S.~J., Kim, B., Jeon, Y., Park, B., and Yun, J.
\newblock {FleXOR}: Trainable fractional quantization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Li et~al.(2016)Li, Zhang, and Liu]{li_ternary_2016}
Li, F., Zhang, B., and Liu, B.
\newblock Ternary weight networks.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2016.

\bibitem[Li et~al.(2018)Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{li2018federated}
Li, T., Sahu, A.~K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V.
\newblock Federated optimization in heterogeneous networks.
\newblock \emph{arXiv preprint arXiv:1812.06127}, 2018.

\bibitem[Lin et~al.(2020)Lin, Kong, Stich, and Jaggi]{lin2020ensemble}
Lin, T., Kong, L., Stich, S.~U., and Jaggi, M.
\newblock Ensemble distillation for robust model fusion in federated learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{McMahan2017}
McMahan, H.~B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics (AISTATS)}, 2017.

\bibitem[Mei et~al.(2019)Mei, Guo, Liu, and Pan]{mei2019sgnn}
Mei, G., Guo, Z., Liu, S., and Pan, L.
\newblock Sgnn: A graph neural network based federated learning approach by
  hiding structure.
\newblock In \emph{2019 IEEE International Conference on Big Data (Big Data)}.
  IEEE, 2019.

\bibitem[Nielsen \& Winther(2020)Nielsen and Winther]{nielsen_closing_2020}
Nielsen, D. and Winther, O.
\newblock Closing the dequantization gap: {PixelCNN} as a single-layer flow.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Rastegari et~al.(2016)Rastegari, Ordonez, Redmon, and
  Farhadi]{rastegari_xnor-net_2016}
Rastegari, M., Ordonez, V., Redmon, J., and Farhadi, A.
\newblock {XNOR}-{Net}: {ImageNet} {Classification} {Using} {Binary}
  {Convolutional} {Neural} {Networks}.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, 2016.

\bibitem[Reisizadeh et~al.(2020)Reisizadeh, Mokhtari, Hassani, Jadbabaie, and
  Pedarsani]{reisizadeh_fedpaq_2020}
Reisizadeh, A., Mokhtari, A., Hassani, H., Jadbabaie, A., and Pedarsani, R.
\newblock {FedPAQ}: {A} {Communication}-{Efficient} {Federated} {Learning}
  {Method} with {Periodic} {Averaging} and {Quantization}.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics (AISTATS)}, 2020.

\bibitem[Salimans \& Kingma(2016)Salimans and Kingma]{salimans_weight_2016}
Salimans, T. and Kingma, D.~P.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2016.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{simonyan_very_2015}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2015.

\bibitem[Sun et~al.(2020)Sun, Wang, Chen, and Ni]{sun_ultra-low_2020}
Sun, X., Wang, N., Chen, C.-y., and Ni, J.-m.
\newblock Ultra-low precision 4-bit training of deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Wang et~al.(2020)Wang, Yurochkin, Sun, Papailiopoulos, and
  Khazaeni]{wang2020federated}
Wang, H., Yurochkin, M., Sun, Y., Papailiopoulos, D., and Khazaeni, Y.
\newblock Federated learning with matched averaging.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2020.

\bibitem[Wen et~al.(2017)Wen, Xu, Yan, Wu, Wang, Chen, and
  Li]{wen_terngrad_2017}
Wen, W., Xu, C., Yan, F., Wu, C., Wang, Y., Chen, Y., and Li, H.
\newblock {TernGrad}: {Ternary} {Gradients} to {Reduce} {Communication} in
  {Distributed} {Deep} {Learning}.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[Wu et~al.(2021)Wu, Wu, Cao, Huang, and Xie]{wu2021fedgnn}
Wu, C., Wu, F., Cao, Y., Huang, Y., and Xie, X.
\newblock Fedgnn: Federated graph neural network for privacy-preserving
  recommendation.
\newblock In \emph{Proceedings of the ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining (KDD)}, 2021.

\bibitem[Wu et~al.(2018)Wu, Li, Chen, and Shi]{wu_training_2018}
Wu, S., Li, G., Chen, F., and Shi, L.
\newblock Training and {Inference} with {Integers} in {Deep} {Neural}
  {Networks}.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2018.

\bibitem[Xing et~al.(2021)Xing, Qian, and Chen]{xing2021invertible}
Xing, Y., Qian, Z., and Chen, Q.
\newblock Invertible image signal processing.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision and Pattern Recognition (CVPR)}, 2021.

\bibitem[Yoon et~al.(2021)Yoon, Jeong, Lee, Yang, and Hwang]{yoon2021federated}
Yoon, J., Jeong, W., Lee, G., Yang, E., and Hwang, S.~J.
\newblock Federated continual learning with weighted inter-client transfer.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, 2021.

\bibitem[Zhang et~al.(2020)Zhang, Liu, Zhang, Liu, Huang, Zhou, Guo, Guo, Du,
  Zhi, and Chen]{zhang_fixed-point_2020}
Zhang, X., Liu, S., Zhang, R., Liu, C., Huang, D., Zhou, S., Guo, J., Guo, Q.,
  Du, Z., Zhi, T., and Chen, Y.
\newblock Fixed-{Point} {Back}-{Propagation} {Training}.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision and Pattern Recognition (CVPR)}, 2020.

\bibitem[Zhao et~al.(2021)Zhao, Huang, Pan, Li, Zhang, Gu, and
  Xu]{zhao_distribution_2021}
Zhao, K., Huang, S., Pan, P., Li, Y., Zhang, Y., Gu, Z., and Xu, Y.
\newblock Distribution {Adaptive} {INT8} {Quantization} for {Training} {CNNs}.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision and Pattern Recognition (CVPR)}, 2021.

\bibitem[Zhao et~al.(2018)Zhao, Li, Lai, Suda, Civin, and
  Chandra]{zhao2018federated}
Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., and Chandra, V.
\newblock Federated learning with non-iid data.
\newblock \emph{arXiv preprint arXiv:1806.00582}, 2018.

\bibitem[Zhou et~al.(2018)Zhou, Wu, Ni, Zhou, Wen, and
  Zou]{zhou_dorefa-net_2018}
Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., and Zou, Y.
\newblock {DoReFa}-{Net}: {Training} {Low} {Bitwidth} {Convolutional} {Neural}
  {Networks} with {Low} {Bitwidth} {Gradients}.
\newblock \emph{arXiv:1606.06160 [cs]}, 2018.

\bibitem[Zhu et~al.(2020)Zhu, Gong, Yu, Liu, Wang, Li, Yang, and
  Yan]{zhu_towards_2020}
Zhu, F., Gong, R., Yu, F., Liu, X., Wang, Y., Li, Z., Yang, X., and Yan, J.
\newblock Towards {Unified} {INT8} {Training} for {Convolutional} {Neural}
  {Network}.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision and Pattern Recognition (CVPR)}, 2020.

\end{thebibliography}
