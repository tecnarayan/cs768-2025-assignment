\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2017)Achiam, Held, Tamar, and
  Abbeel]{achiam2017constrained}
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel.
\newblock Constrained policy optimization.
\newblock In \emph{International conference on machine learning}, pages 22--31.
  PMLR, 2017.

\bibitem[Agarwal et~al.(2021)Agarwal, Schwarzer, Castro, Courville, and
  Bellemare]{agarwal2021deep}
Rishabh Agarwal, Max Schwarzer, Pablo~Samuel Castro, Aaron~C Courville, and
  Marc Bellemare.
\newblock Deep reinforcement learning at the edge of the statistical precipice.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Ahn(2019)]{Ahn-2019-117213}
Edward Ahn.
\newblock Towards safe reinforcement learning in the real world, July 2019.

\bibitem[Altman(1998)]{cmdpbook}
Eitan Altman.
\newblock \emph{Constrained {Markov} Decision Processes}, volume~1.
\newblock Taylor \& Francis, 1998.

\bibitem[Bertsekas(1996)]{copbook}
Dimitri~P. Bertsekas.
\newblock \emph{Constrained Optimization and Lagrange Multiplier Methods
  (Optimization and Neural Computation Series)}.
\newblock Athena Scientific, 1 edition, 1996.
\newblock ISBN 1886529043.

\bibitem[Bhatnagar(2010)]{articlecmdpbhatnagar}
Shalabh Bhatnagar.
\newblock An actor-critic algorithm with function approximation for discounted
  cost constrained markov decision processes.
\newblock \emph{Systems \& Control Letters}, 59:\penalty0 760--766, 12 2010.

\bibitem[Bhatnagar and Lakshmanan(2012)]{cmdp2}
Shalabh Bhatnagar and K~Lakshmanan.
\newblock An online actor--critic algorithm with function approximation for
  constrained {Markov} decision processes.
\newblock \emph{Journal of Optimization Theory and Applications}, 153\penalty0
  (3):\penalty0 688--708, 2012.

\bibitem[Bhatnagar et~al.(2009)Bhatnagar, Sutton, Ghavamzadeh, and Lee]{nac}
Shalabh Bhatnagar, Richard Sutton, Mohammad Ghavamzadeh, and Mark Lee.
\newblock {Natural Actor-Critic Algorithms}.
\newblock \emph{{Automatica}}, 45\penalty0 (11), July 2009.
\newblock \doi{10.1016/j.automatica.2009.07.008}.
\newblock URL \url{https://hal.inria.fr/hal-00840470}.

\bibitem[Bhatnagar et~al.(2013)Bhatnagar, Prasad, and Prashanth]{bhatnagaretal}
Shalabh Bhatnagar, H.L. Prasad, and L.A. Prashanth.
\newblock Stochastic recursive algorithms for optimization: Simultaneous
  perturbation methods.
\newblock In \emph{Stochastic Recursive Algorithms for Optimization:
  Simultaneous Perturbation Methods}, volume 434, page 320. Springer, 2013.

\bibitem[Borkar(2005)]{borkar-cmdp}
Vivek~S Borkar.
\newblock An actor-critic algorithm for constrained {Markov} decision
  processes.
\newblock \emph{Systems \& control letters}, 54\penalty0 (3):\penalty0
  207--213, 2005.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{gym}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock {OpenAI Gym}, 2016.
\newblock URL \url{https://arxiv.org/abs/1606.01540}.

\bibitem[Chow and Ghavamzadeh(2014)]{cvarmdp}
Yinlam Chow and Mohammad Ghavamzadeh.
\newblock Algorithms for {CVaR} optimization in {MDPs}, 2014.
\newblock URL \url{https://arxiv.org/abs/1406.3339}.

\bibitem[Chow et~al.(2018)Chow, Nachum, Duenez-Guzman, and
  Ghavamzadeh]{chow2018lyapunov}
Yinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh.
\newblock A {Lyapunov-based} approach to safe reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Chow et~al.(2019)Chow, Nachum, Faust, Duenez-Guzman, and
  Ghavamzadeh]{chow2019lyapunov}
Yinlam Chow, Ofir Nachum, Aleksandra Faust, Edgar Duenez-Guzman, and Mohammad
  Ghavamzadeh.
\newblock Lyapunov-based safe policy optimization for continuous control.
\newblock \emph{arXiv preprint arXiv:1901.10031}, 2019.

\bibitem[Chua et~al.(2018)Chua, Calandra, McAllister, and Levine]{petshandful}
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine.
\newblock Deep reinforcement learning in a handful of trials using
  probabilistic dynamics models.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Cowen-Rivers et~al.(2020)Cowen-Rivers, Palenicek, Moens, Abdullah,
  Sootla, Wang, and Ammar]{samba}
Alexander~I. Cowen-Rivers, Daniel Palenicek, Vincent Moens, Mohammed Abdullah,
  Aivar Sootla, Jun Wang, and Haitham Ammar.
\newblock Samba: Safe model-based \& active reinforcement learning, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.09436}.

\bibitem[Dalal et~al.(2018)Dalal, Dvijotham, Vecerik, Hester, Paduraru, and
  Tassa]{dalal2018safe}
Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin
  Paduraru, and Yuval Tassa.
\newblock Safe exploration in continuous action spaces.
\newblock \emph{arXiv preprint arXiv:1801.08757}, 2018.

\bibitem[de~Boer et~al.(2005)de~Boer, Kroese, Mannor, and
  Rubinstein]{crossentropy}
Pieter-Tjerk de~Boer, Dirk~P. Kroese, Shie Mannor, and Reuven~Y. Rubinstein.
\newblock A tutorial on the cross-entropy method.
\newblock \emph{Annals of Operations Research}, 134\penalty0 (1):\penalty0
  19--67, Feb 2005.
\newblock ISSN 1572-9338.
\newblock \doi{10.1007/s10479-005-5724-z}.
\newblock URL \url{https://doi.org/10.1007/s10479-005-5724-z}.

\bibitem[Deisenroth and Rasmussen(2011)]{PILCO}
Marc Deisenroth and Carl~E Rasmussen.
\newblock Pilco: A model-based and data-efficient approach to policy search.
\newblock In \emph{Proceedings of the 28th International Conference on machine
  learning (ICML-11)}, pages 465--472. Citeseer, 2011.

\bibitem[Diddigi et~al.(2022)Diddigi, Jain, Prabuchandran, and
  Bhatnagar]{diddigi}
Raghuram~Bharadwaj Diddigi, Prateek Jain, K.J. Prabuchandran, and Shalabh
  Bhatnagar.
\newblock Neural network compatible off-policy natural actor-critic algorithm.
\newblock In \emph{2022 International Joint Conference on Neural Networks
  (IJCNN)}, pages 1--10, 2022.
\newblock \doi{10.1109/IJCNN55064.2022.9892303}.

\bibitem[Garc{{\'i}}a et~al.(2015)Garc{{\'i}}a, Fern, and
  o~Fern{{\'a}}ndez]{surveysafe}
Javier Garc{{\'i}}a, Fern, and o~Fern{{\'a}}ndez.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 16\penalty0
  (42):\penalty0 1437--1480, 2015.
\newblock URL \url{http://jmlr.org/papers/v16/garcia15a.html}.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pages
  1861--1870. PMLR, 2018.

\bibitem[Han et~al.(2018)Han, Choi, Benz, and Loaiciga]{8367110}
Seung-Ho Han, Ho-Jin Choi, Philipp Benz, and Jorge Loaiciga.
\newblock Sensor-based mobile robot navigation via deep reinforcement learning.
\newblock In \emph{2018 IEEE International Conference on Big Data and Smart
  Computing (BigComp)}, pages 147--154, 2018.
\newblock \doi{10.1109/BigComp.2018.00030}.

\bibitem[Heess et~al.(2015)Heess, Wayne, Silver, Lillicrap, Tassa, and
  Erez]{svg}
Nicolas Heess, Greg Wayne, David Silver, Timothy Lillicrap, Yuval Tassa, and
  Tom Erez.
\newblock Learning continuous control policies by stochastic value gradients,
  2015.
\newblock URL \url{https://arxiv.org/abs/1510.09142}.

\bibitem[Janner et~al.(2019)Janner, Fu, Zhang, and Levine]{MBPO}
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.
\newblock When to trust your model: Model-based policy optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Kendall et~al.(2018)Kendall, Hawke, Janz, Mazur, Reda, Allen, Lam,
  Bewley, and Shah]{kendall2018learning}
Alex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur, Daniele Reda,
  John-Mark Allen, Vinh-Dieu Lam, Alex Bewley, and Amar Shah.
\newblock Learning to drive in a day, 2018.

\bibitem[Kurutach et~al.(2018)Kurutach, Clavera, Duan, Tamar, and
  Abbeel]{metrpo}
Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel.
\newblock Model-ensemble trust-region policy optimization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{uncertaintyprediction}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Liu et~al.(2022)Liu, Tateo, Ammar, and Peters]{manifoldliu}
Puze Liu, Davide Tateo, Haitham~Bou Ammar, and Jan Peters.
\newblock Robot reinforcement learning on the constraint manifold.
\newblock In \emph{Conference on Robot Learning}, pages 1357--1366. PMLR, 2022.

\bibitem[Liu et~al.(2020)Liu, Zhou, Chen, Zhong, Hebert, and Zhao]{liu2020safe}
Zuxin Liu, Hongyi Zhou, Baiming Chen, Sicheng Zhong, Martial Hebert, and Ding
  Zhao.
\newblock Safe model-based reinforcement learning with robust cross-entropy
  method, 2020.

\bibitem[Luo et~al.(2018)Luo, Xu, Li, Tian, Darrell, and Ma]{slbo}
Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu
  Ma.
\newblock Algorithmic framework for model-based deep reinforcement learning
  with theoretical guarantees, 2018.
\newblock URL \url{https://arxiv.org/abs/1807.03858}.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{atari}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing {Atari} with deep reinforcement learning, 2013.

\bibitem[Ray et~al.(2019)Ray, Achiam, and Amodei]{Achiam2019BenchmarkingSE}
Alex Ray, Joshua Achiam, and Dario Amodei.
\newblock Benchmarking safe exploration in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.01708}, 7:\penalty0 1, 2019.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{TRPO}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pages
  1889--1897. PMLR, 2015.

\bibitem[Schulman et~al.(2016)Schulman, Moritz, Levine, Jordan, and
  Abbeel]{schulman2018highdimensional}
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter
  Abbeel.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2016.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms, 2017.

\bibitem[Sikchi et~al.(2021{\natexlab{a}})Sikchi, Zhou, and Held]{LOOP}
Harshit Sikchi, Wenxuan Zhou, and David Held.
\newblock Learning off-policy with online planning.
\newblock \emph{CoRL}, 2021{\natexlab{a}}.

\bibitem[Sikchi et~al.(2021{\natexlab{b}})Sikchi, Zhou, and
  Held]{sikchi2021lyapunov}
Harshit Sikchi, Wenxuan Zhou, and David Held.
\newblock Lyapunov barrier policy optimization.
\newblock \emph{arXiv preprint arXiv:2103.09230}, 2021{\natexlab{b}}.

\bibitem[Stooke et~al.(2020)Stooke, Achiam, and Abbeel]{stooke2020responsive}
Adam Stooke, Joshua Achiam, and Pieter Abbeel.
\newblock Responsive safety in reinforcement learning by {PID Lagrangian}
  methods.
\newblock In \emph{International Conference on Machine Learning}, pages
  9133--9143. PMLR, 2020.

\bibitem[Tessler et~al.(2018)Tessler, Mankowitz, and Mannor]{tessler2018reward}
Chen Tessler, Daniel~J. Mankowitz, and Shie Mannor.
\newblock Reward constrained policy optimization, 2018.

\bibitem[Thomas et~al.(2022)Thomas, Luo, and Ma]{imaginingnips}
Garrett Thomas, Yuping Luo, and Tengyu Ma.
\newblock Safe reinforcement learning by imagining the near future, 2022.
\newblock URL \url{https://arxiv.org/abs/2202.07789}.

\bibitem[Uryasev and Rockafellar(2001)]{cvar}
Stanislav Uryasev and R~Tyrrell Rockafellar.
\newblock Conditional value-at-risk: optimization approach.
\newblock In \emph{Stochastic optimization: algorithms and applications}, pages
  411--435. Springer, 2001.

\bibitem[Wang et~al.(2019)Wang, Bao, Clavera, Hoang, Wen, Langlois, Zhang,
  Zhang, Abbeel, and Ba]{benchmodelbased}
Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric
  Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba.
\newblock Benchmarking model-based reinforcement learning, 2019.
\newblock URL \url{https://arxiv.org/abs/1907.02057}.

\bibitem[Wen and Topcu(2021)]{crossentropysafe}
Min Wen and Ufuk Topcu.
\newblock Constrained cross-entropy method for safe reinforcement learning.
\newblock \emph{IEEE Transactions on Automatic Control}, 66\penalty0
  (7):\penalty0 3123--3137, 2021.
\newblock \doi{10.1109/TAC.2020.3015931}.

\bibitem[Yu et~al.(2020)Yu, Liu, and Nemati]{yu2020reinforcement}
Chao Yu, Jiming Liu, and Shamim Nemati.
\newblock Reinforcement learning in healthcare: A survey, 2020.

\bibitem[Yu et~al.(2019)Yu, Yang, Kolar, and Wang]{yu2019convergent}
Ming Yu, Zhuoran Yang, Mladen Kolar, and Zhaoran Wang.
\newblock Convergent policy optimization for safe reinforcement learning, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Vuong, and Ross]{zhang2020order}
Yiming Zhang, Quan Vuong, and Keith Ross.
\newblock First order constrained optimization in policy space.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15338--15349, 2020.

\end{thebibliography}
