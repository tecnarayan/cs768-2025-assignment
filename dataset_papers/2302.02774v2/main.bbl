\begin{thebibliography}{68}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2019)Arora, Khandeparkar, Khodak, Plevrakis, and
  Saunshi]{arora_theoretical_2019}
Arora, S., Khandeparkar, H., Khodak, M., Plevrakis, O., and Saunshi, N.
\newblock A theoretical analysis of contrastive unsupervised representation
  learning.
\newblock In \emph{ICML}, 2019.

\bibitem[Bach(2017)]{bach2017breaking}
Bach, F.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 629--681, 2017.

\bibitem[Bach(2023)]{Bach2023}
Bach, F.
\newblock \emph{Learning Theory from First Principles}.
\newblock To appear at MIT press, 2023.

\bibitem[Balestriero \& LeCun(2022)Balestriero and
  LeCun]{balestriero_contrastive_2022}
Balestriero, R. and LeCun, Y.
\newblock Contrastive and non-contrastive self-supervised learning recover
  global and local spectral embedding methods.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Bardes et~al.(2022)Bardes, Ponce, and LeCun]{bardes_vicreg_2022}
Bardes, A., Ponce, J., and LeCun, Y.
\newblock Vicreg: Variance-invariance-covariance regularization for
  self-supervised learning.
\newblock In \emph{ICLR}, 2022.

\bibitem[Bartlett \& Mendelson(2002)Bartlett and
  Mendelson]{bartlett_rademacher_2002}
Bartlett, P. and Mendelson, S.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 2002.

\bibitem[Bartlett et~al.(2006)Bartlett, Jordan, and
  Mcauliffe]{bartlett_convexity_2006}
Bartlett, P., Jordan, M., and Mcauliffe, J.
\newblock Convexity, classification, and risk bounds.
\newblock \emph{Journal of the American Statistical Association}, 2006.

\bibitem[Bietti(2022)]{bietti2022approximation}
Bietti, A.
\newblock Approximation and learning with deep convolutional models: a kernel
  perspective.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Bietti \& Mairal(2019)Bietti and Mairal]{bietti2019inductive}
Bietti, A. and Mairal, J.
\newblock On the inductive bias of neural tangent kernels.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Bietti et~al.(2021)Bietti, Venturi, and Bruna]{bietti2021sample}
Bietti, A., Venturi, L., and Bruna, J.
\newblock On the sample complexity of learning under invariance and geometric
  stability.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in neural information processing systems}, 2020.

\bibitem[Bubeck(2015)]{Bubeck2015}
Bubeck, S.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Foundations and Trends in Machine Learning}, 2015.

\bibitem[Bun et~al.(2017)Bun, Bouchaud, and Potters]{Bun2017}
Bun, J., Bouchaud, J.-P., and Potters, M.
\newblock Cleaning large correlation matrices: Tools from random matrix theory.
\newblock \emph{Physics Reports}, 2017.

\bibitem[Cabannes et~al.(2021{\natexlab{a}})Cabannes, Pillaud-Vivien, Bach, and
  Rudi]{cabannes_overcoming_2021}
Cabannes, V., Pillaud-Vivien, L., Bach, F., and Rudi, A.
\newblock Overcoming the curse of dimensionality with {Laplacian}
  regularization in semi-supervised learning.
\newblock In \emph{NeurIPS}, 2021{\natexlab{a}}.

\bibitem[Cabannes et~al.(2021{\natexlab{b}})Cabannes, Rudi, and
  Bach]{Cabannes2022rates}
Cabannes, V., Rudi, A., and Bach, F.
\newblock Fast rates in structured prediction.
\newblock In \emph{Conference on Learning Theory}, 2021{\natexlab{b}}.

\bibitem[Cabannes et~al.(2023)Cabannes, Bietti, and
  Balestriero]{cabannes_minimal_2022}
Cabannes, V., Bietti, A., and Balestriero, R.
\newblock On minimal variations for unsupervised representation learning.
\newblock \emph{ICASSP}, 2023.

\bibitem[Caponnetto \& De~Vito(2007)Caponnetto and
  De~Vito]{caponnetto_optimal_2007}
Caponnetto, A. and De~Vito, E.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock \emph{Foundations of Computational Mathematics}, 2007.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{caron2020unsupervised}
Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J{\'e}gou, Mairal,
  Bojanowski, and Joulin]{caron2021emerging}
Caron, M., Touvron, H., Misra, I., J{\'e}gou, H., Mairal, J., Bojanowski, P.,
  and Joulin, A.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, 2021.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and
  Hinton]{chen_simple_2020}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{ICML}, 2020.

\bibitem[Chi et~al.(2019)Chi, Lu, and Chen]{chi_2019}
Chi, Y., Lu, Y., and Chen, Y.
\newblock Nonconvex optimization meets low-rank matrix factorization: An
  overview.
\newblock \emph{IEEE Transactions on Signal Processing}, 2019.

\bibitem[Coifman \& Lafon(2006)Coifman and Lafon]{Coifman2006}
Coifman, R. and Lafon, S.
\newblock Diffusion maps.
\newblock \emph{Applied and Computational Harmonic Analysis}, 2006.

\bibitem[Davis \& Kahan(1970)Davis and Kahan]{davis_rotation_1970}
Davis, C. and Kahan, W.
\newblock The rotation of eigenvectors by a perturbation.
\newblock \emph{SIAM Journal on Numerical Analysis}, 1970.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of NAACL-HLT}, 2019.

\bibitem[Efthimiou \& Frye(2014)Efthimiou and Frye]{efthimiou2014spherical}
Efthimiou, C. and Frye, C.
\newblock \emph{Spherical harmonics in p dimensions}.
\newblock World Scientific, 2014.

\bibitem[Favero et~al.(2021)Favero, Cagnetta, and Wyart]{favero2021locality}
Favero, A., Cagnetta, F., and Wyart, M.
\newblock Locality defeats the curse of dimensionality in convolutional
  teacher-student scenarios.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Garrido et~al.(2022)Garrido, Balestriero, Najman, and
  Lecun]{Garrido_rankme_2022}
Garrido, Q., Balestriero, R., Najman, L., and Lecun, Y.
\newblock Rankme: Assessing the downstream performance of pretrained
  self-supervised representations by their rank.
\newblock \emph{arXiv preprint arXiv:2210.02885}, 2022.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond,
  Buchatskaya, Doersch, Avila~Pires, Guo, Gheshlaghi~Azar,
  et~al.]{grill2020bootstrap}
Grill, J.-B., Strub, F., Altch{\'e}, F., Tallec, C., Richemond, P.,
  Buchatskaya, E., Doersch, C., Avila~Pires, B., Guo, Z., Gheshlaghi~Azar, M.,
  et~al.
\newblock Bootstrap your own latent-a new approach to self-supervised learning.
\newblock In \emph{Advances in neural information processing systems}, 2020.

\bibitem[HaoChen et~al.(2021)HaoChen, Wei, Gaidon, and
  Ma]{haochen_provable_2021}
HaoChen, J., Wei, C., Gaidon, A., and Ma, T.
\newblock Provable guarantees for self-supervised deep learning with spectral
  contrastive loss.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[HaoChen \& Ma(2022)HaoChen and Ma]{haochen2022theoretical}
HaoChen, J.~Z. and Ma, T.
\newblock A theoretical study of inductive biases in contrastive learning.
\newblock \emph{arXiv preprint arXiv:2211.14699}, 2022.

\bibitem[He \& Ozay(2020)He and Ozay]{he_exploring_2022}
He, B. and Ozay, M.
\newblock Exploring the gap between collapsed \& whitened features in
  self-supervised learning.
\newblock In \emph{ICML}, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot_ntk_2018}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Kato(1995)]{kato_perturbation_1995}
Kato, T.
\newblock \emph{Perturbation Theory for Linear Operators}.
\newblock Springer, 1995.

\bibitem[Kiani et~al.(2022)Kiani, Balestriero, Chen, Lloyd, and
  LeCun]{kiani2022joint}
Kiani, B.~T., Balestriero, R., Chen, Y., Lloyd, S., and LeCun, Y.
\newblock Joint embedding self-supervised learning in the kernel regime.
\newblock \emph{arXiv preprint arXiv:2209.14884}, 2022.

\bibitem[Kolmogorov \& Tikhomirov(1959)Kolmogorov and
  Tikhomirov]{Kolmogorov1959}
Kolmogorov, A. and Tikhomirov, V.
\newblock $\epsilon$-entropy and $\epsilon$-capacity of sets in functional
  spaces.
\newblock \emph{Uspekhi Matematicheskikh Nauk}, 1959.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019wide}
Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J.,
  and Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Lee et~al.(2021)Lee, Lei, Saunshi, and Zhuo]{Lee2021}
Lee, J., Lei, Q., Saunshi, N., and Zhuo, J.
\newblock Predicting what you already know helps: Provable self-supervised
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Lin et~al.(2020)Lin, Rudi, Rosasco, and Cevher]{Lin2020}
Lin, J., Rudi, A., Rosasco, L., and Cevher, V.
\newblock Optimal rates for spectral algorithms with least-squares regression
  over {Hilbert} spaces.
\newblock \emph{Applied and Computational Harmonic Analysis}, 2020.

\bibitem[Maurer(2016)]{Maurer2016}
Maurer, A.
\newblock A vector-contraction inequality for {R}ademacher complexities.
\newblock In \emph{Algorithmic Learning Theory}, 2016.

\bibitem[Mei et~al.(2021)Mei, Misiakiewicz, and Montanari]{mei2021learning}
Mei, S., Misiakiewicz, T., and Montanari, A.
\newblock Learning with invariances in random features and kernel models.
\newblock In \emph{Conference on Learning Theory}, 2021.

\bibitem[Meir \& Zhang(2003)Meir and Zhang]{meir_contraction_2003}
Meir, R. and Zhang, T.
\newblock Generalization error bounds for bayesian mixture algorithms.
\newblock \emph{Journal of Machine Learning Research}, 2003.

\bibitem[Micchelli et~al.(2006)Micchelli, Xu, and Zhang]{Micchelli2006}
Micchelli, C., Xu, Y., and Zhang, H.
\newblock Universal kernels.
\newblock \emph{Journal of Machine Learning Research}, 2006.

\bibitem[Misiakiewicz \& Mei(2022)Misiakiewicz and
  Mei]{misiakiewicz2021learning}
Misiakiewicz, T. and Mei, S.
\newblock Learning with convolution and pooling operations in kernel methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Mourtada \& Rosasco(2022)Mourtada and Rosasco]{mourtada2022exact}
Mourtada, J. and Rosasco, L.
\newblock An elementary analysis of ridge regression with random design.
\newblock \emph{Comptes Rendus. Math\'ematique}, 2022.

\bibitem[Mourtada et~al.(2022)Mourtada, skevi\v cius, and
  Zhivotovskiy]{mourtada2022}
Mourtada, J., skevi\v cius, T.~V., and Zhivotovskiy, N.
\newblock Distribution-free robust linear regression.
\newblock \emph{Mathematical Statistics and Learning}, 2022.

\bibitem[O'Donnell(2014)]{o2014analysis}
O'Donnell, R.
\newblock \emph{Analysis of boolean functions}.
\newblock Cambridge University Press, 2014.

\bibitem[Ostrovskii \& Bach(2018)Ostrovskii and Bach]{ostrovskii_finite_2018}
Ostrovskii, D. and Bach, F.
\newblock Finite-sample analysis of m-estimators using self-concordance.
\newblock \emph{Electronic Journal of Statistics}, 2018.

\bibitem[Pillaud-Vivien \& Bach(2023)Pillaud-Vivien and Bach]{Loucas2023}
Pillaud-Vivien, L. and Bach, F.
\newblock Kernelized diffusion maps.
\newblock \emph{ArXiv}, 2023.

\bibitem[Pinelis \& Sakhanenko(1986)Pinelis and Sakhanenko]{Pinelis1986}
Pinelis, I. and Sakhanenko, A.
\newblock Remarks on inequalities for large deviation probabilities.
\newblock \emph{Theory of Probability and Its Applications}, 1986.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Rigollet(2007)]{Rigollet2007}
Rigollet, P.
\newblock Generalization error bounds in semi-supervised classification under
  the cluster assumption.
\newblock \emph{Journal of Machine Learning Research}, 2007.

\bibitem[Saunshi et~al.(2022)Saunshi, Ash, Goel, Misra, Zhang, Arora, Kakade,
  and Krishnamurthy]{saunshi_understanding_2022}
Saunshi, N., Ash, J., Goel, S., Misra, D., Zhang, C., Arora, S., Kakade, S.,
  and Krishnamurthy, A.
\newblock Understanding contrastive learning requires incorporating inductive
  biases.
\newblock In \emph{ICML}, 2022.

\bibitem[Schiebinger et~al.(2015)Schiebinger, Wainwright, and
  Yu]{Schiebinger2015}
Schiebinger, G., Wainwright, M., and Yu, B.
\newblock The geometry of kernelized spectral clustering.
\newblock \emph{The annals of Statistics}, 2015.

\bibitem[Scholkopf \& Smola(2001)Scholkopf and Smola]{Scholkopf2001}
Scholkopf, B. and Smola, A.
\newblock \emph{Learning with kernels: support vector machines, regularization,
  optimization, and beyond}.
\newblock MIT press, 2001.

\bibitem[Simard et~al.(1991)Simard, Victorri, LeCun, and Denker]{Simard1991}
Simard, P., Victorri, B., LeCun, Y., and Denker, J.
\newblock Tangent prop - a formalism for specifying selected invariances in an
  adaptive network.
\newblock In \emph{NeuRIPS}, 1991.

\bibitem[Simon et~al.(2023)Simon, Knutins, Ziyin, Geisz, Fetterman, and
  Albrecht]{Simon2023}
Simon, J., Knutins, M., Ziyin, L., Geisz, D., Fetterman, A., and Albrecht, J.
\newblock On the stepwise nature of self-supervised learning.
\newblock \emph{ArXiv}, 2023.

\bibitem[Smale \& Zhou(2007)Smale and Zhou]{smale_learning_2007}
Smale, S. and Zhou, D.-X.
\newblock Learning theory estimates via integral operators and their
  approximations.
\newblock \emph{Constructive Approximation}, 2007.

\bibitem[Smola et~al.(2000)Smola, Ov{\'a}ri, and
  Williamson]{smola2000regularization}
Smola, A., Ov{\'a}ri, Z., and Williamson, R.~C.
\newblock Regularization with dot-product kernels.
\newblock In \emph{Advances in neural information processing systems}, 2000.

\bibitem[Sun \& Zhou(2008)Sun and Zhou]{Sun2008}
Sun, H.-W. and Zhou, D.-X.
\newblock Reproducing kernel hilbert spaces associated with analytic
  translation-invariant mercer kernels.
\newblock \emph{Journal of Fourier Analysis and Applications}, 2008.

\bibitem[Tian(2022)]{tian2022understanding}
Tian, Y.
\newblock Understanding the role of nonlinearity in training dynamics of
  contrastive learning.
\newblock \emph{arXiv preprint arXiv:2206.01342}, 2022.

\bibitem[Tian et~al.(2021)Tian, Yu, Chen, and Ganguli]{tian_understanding_2021}
Tian, Y., Yu, L., Chen, X., and Ganguli, S.
\newblock Understanding self-supervised learning with dual deep networks, 2021.

\bibitem[Tosh et~al.(2021{\natexlab{a}})Tosh, Krishnamurthy, and
  Hsu]{tosh2021contrastive}
Tosh, C., Krishnamurthy, A., and Hsu, D.
\newblock Contrastive learning, multi-view redundancy, and linear models.
\newblock In \emph{Algorithmic Learning Theory}, 2021{\natexlab{a}}.

\bibitem[Tosh et~al.(2021{\natexlab{b}})Tosh, Krishnamurthy, and
  Hsu]{tosh_contrastive_2021}
Tosh, C., Krishnamurthy, A., and Hsu, D.
\newblock Contrastive estimation reveals topic posterior information to linear
  models.
\newblock \emph{JMLR}, 2021{\natexlab{b}}.

\bibitem[Tropp(2015)]{tropp2015}
Tropp, J.
\newblock An introduction to matrix concentration inequalities.
\newblock \emph{Foundations and Trends in Machine Learning}, 2015.

\bibitem[van Engelen \& Hoos(2020)van Engelen and Hoos]{engelen_surver_2020}
van Engelen, J. and Hoos, H.
\newblock A survey of semi-supervised learning.
\newblock \emph{Machine Learning}, 2020.

\bibitem[Vitushkin(1954)]{Vitushkin1954}
Vitushkin, A.
\newblock On {H}ilbert's thirteenth problem.
\newblock \emph{Proceedings of the USSR Academy of Sciences}, 1954.

\bibitem[Wen \& Li(2021)Wen and Li]{wen2021toward}
Wen, Z. and Li, Y.
\newblock Toward understanding the feature learning process of self-supervised
  contrastive learning.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Yang \& Salman(2019)Yang and Salman]{yang2019fine}
Yang, G. and Salman, H.
\newblock A fine-grained spectral perspective on neural networks.
\newblock \emph{arXiv preprint arXiv:1907.10599}, 2019.

\end{thebibliography}
