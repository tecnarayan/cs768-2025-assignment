@misc{swamy2023inverse,
      title={Inverse Reinforcement Learning without Reinforcement Learning}, 
      author={Gokul Swamy and Sanjiban Choudhury and J. Andrew Bagnell and Zhiwei Steven Wu},
      year={2023},
      eprint={2303.14623},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{BEYER2000239,
title = {Evolutionary algorithms in noisy environments: theoretical issues and guidelines for practice},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {186},
number = {2},
pages = {239-267},
year = {2000},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(99)00386-2},
url = {https://www.sciencedirect.com/science/article/pii/S0045782599003862},
author = {Hans-Georg Beyer},
keywords = {Evolutionary algorithms (GA, ES, EP), Noisy fitness data, Convergence properties, Optimization under noise, Convergence improvement techniques, Self-adaptation},
abstract = {This paper is devoted to the effects of fitness noise in evolutionary algorithms (EAs). After a short introduction to the history of this research field, the performance of genetic algorithms (GAs) and evolution strategies (ESs) on the hyper-sphere test function is evaluated. It will be shown that the main effects of noise – the decrease of convergence velocity and the residual location error R∞ – are observed in both GAs and ESs. Different methods for improving the performance are presented and hypotheses on their working mechanisms are discussed. The method of rescaled mutations is analyzed in depth for the (1,λ)-ES on the sphere model. It is shown that this method needs advanced self-adaptation (SA) techniques in order to take advantage of the theoretically predicted performance gain. The troubles with current self-adaptation techniques are discussed and directions for further research will be worked out.}
}

@misc{salimans2017evolution,
      title={Evolution Strategies as a Scalable Alternative to Reinforcement Learning}, 
      author={Tim Salimans and Jonathan Ho and Xi Chen and Szymon Sidor and Ilya Sutskever},
      year={2017},
      eprint={1703.03864},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@article{Real_Aggarwal_Huang_Le_2019, title={Regularized Evolution for Image Classifier Architecture Search}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4405}, DOI={10.1609/aaai.v33i01.33014780}, abstractNote={&lt;p&gt;The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier— &lt;em&gt;AmoebaNet-A&lt;/em&gt;—that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-theart 83.9% top-1 / 96.6% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.}, year={2019}, month={Jul.}, pages={4780-4789} }

@misc{such2018deep,
      title={Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning}, 
      author={Felipe Petroski Such and Vashisht Madhavan and Edoardo Conti and Joel Lehman and Kenneth O. Stanley and Jeff Clune},
      year={2018},
      eprint={1712.06567},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@misc{ho2016generative,
      title={Generative Adversarial Imitation Learning}, 
      author={Jonathan Ho and Stefano Ermon},
      year={2016},
      eprint={1606.03476},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wulfmeier2016maximum,
      title={Maximum Entropy Deep Inverse Reinforcement Learning}, 
      author={Markus Wulfmeier and Peter Ondruska and Ingmar Posner},
      year={2016},
      eprint={1507.04888},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{finn2016guided,
      title={Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization}, 
      author={Chelsea Finn and Sergey Levine and Pieter Abbeel},
      year={2016},
      eprint={1603.00448},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{fu2018learning,
      title={Learning Robust Rewards with Adversarial Inverse Reinforcement Learning}, 
      author={Justin Fu and Katie Luo and Sergey Levine},
      year={2018},
      eprint={1710.11248},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{szot2023bcirl,
      title={BC-IRL: Learning Generalizable Reward Functions from Demonstrations}, 
      author={Andrew Szot and Amy Zhang and Dhruv Batra and Zsolt Kira and Franziska Meier},
      year={2023},
      eprint={2303.16194},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{MLP_Puterman,
author = {Puterman, Martin L.},
title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
year = {1994},
isbn = {0471619779},
publisher = {John Wiley \& Sons, Inc.},
address = {USA},
edition = {1st},
abstract = {From the Publisher:The past decade has seen considerable theoretical and applied research on Markov decision processes, as well as the growing use of these models in ecology, economics, communications engineering, and other fields where outcomes are uncertain and sequential decision-making processes are needed. A timely response to this increased activity, Martin L. Puterman's new work provides a uniquely up-to-date, unified, and rigorous treatment of the theoretical, computational, and applied research on Markov decision process models. It discusses all major research directions in the field, highlights many significant applications of Markov decision processes models, and explores numerous important topics that have previously been neglected or given cursory coverage in the literature. Markov Decision Processes focuses primarily on infinite horizon discrete time models and models with discrete time spaces while also examining models with arbitrary state spaces, finite horizon models, and continuous-time discrete state models. The book is organized around optimality criteria, using a common framework centered on the optimality (Bellman) equation for presenting results. The results are presented in a "theorem-proof" format and elaborated on through both discussion and examples, including results that are not available in any other book. A two-state Markov decision process model, presented in Chapter 3, is analyzed repeatedly throughout the book and demonstrates many results and algorithms. Markov Decision Processes covers recent research advances in such areas as countable state space models with average reward criterion, constrained models, and models with risk sensitive optimality criteria. It also explores several topics that have received little or no attention in other books, including modified policy iteration, multichain models with average reward criterion, and sensitive optimality. In addition, a Bibliographic Remarks section in each chapter comments on relevant historic}
}

@misc{liu2022theoretical,
      title={A Theoretical Understanding of Gradient Bias in Meta-Reinforcement Learning}, 
      author={Bo Liu and Xidong Feng and Jie Ren and Luo Mai and Rui Zhu and Haifeng Zhang and Jun Wang and Yaodong Yang},
      year={2022},
      eprint={2112.15400},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{LUCAS197619,
title = {Econometric policy evaluation: A critique},
journal = {Carnegie-Rochester Conference Series on Public Policy},
volume = {1},
pages = {19-46},
year = {1976},
issn = {0167-2231},
doi = {https://doi.org/10.1016/S0167-2231(76)80003-6},
url = {https://www.sciencedirect.com/science/article/pii/S0167223176800036},
author = {Robert E. Lucas}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@incollection{RUST19943081,
title = {Chapter 51 Structural estimation of markov decision processes},
series = {Handbook of Econometrics},
publisher = {Elsevier},
volume = {4},
pages = {3081-3143},
year = {1994},
issn = {1573-4412},
doi = {https://doi.org/10.1016/S1573-4412(05)80020-0},
url = {https://www.sciencedirect.com/science/article/pii/S1573441205800200},
author = {John Rust},
abstract = {Publisher Summary
This chapter summarizes the ability of the models to track the shift in departure rates induced by the 1982 window plan. All forecasts were based on the estimated utility function parameters using data prior to 1982. Using these parameters, predictions were generated from all four models after incorporating the extra bonus provisions of the window plan. The structural models were generally able to accurately predict the large increase in departure rates induced by the window plan, although once again none of the models was able to capture the peak in departure rates at age 65. On the other hand, the reduced-form probit model predicted that the window plan had essentially no effect on departure rates. Other reduced-form specifications greatly overpredicted departure rates under the window plan.}
}

@inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K and others},
  booktitle={Aaai},
  volume={8},
  pages={1433--1438},
  year={2008},
  organization={Chicago, IL, USA}
}

@inproceedings{kitani2012activity,
  title={Activity forecasting},
  author={Kitani, Kris M and Ziebart, Brian D and Bagnell, James Andrew and Hebert, Martial},
  booktitle={Computer Vision--ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part IV 12},
  pages={201--214},
  year={2012},
  organization={Springer}
}

@misc{gulrajani2017improved,
      title={Improved Training of Wasserstein GANs}, 
      author={Ishaan Gulrajani and Faruk Ahmed and Martin Arjovsky and Vincent Dumoulin and Aaron Courville},
      year={2017},
      eprint={1704.00028},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{lenton2021ivy,
  title={Ivy: Templated deep learning for inter-framework portability},
  author={Lenton, Daniel and Pardo, Fabio and Falck, Fabian and James, Stephen and Clark, Ronald},
  journal={arXiv preprint arXiv:2102.02886},
  year={2021}
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@software{gymnax2022github,
  author = {Robert Tjarko Lange},
  title = {{gymnax}: A {JAX}-based Reinforcement Learning Environment Library},
  url = {http://github.com/RobertTLange/gymnax},
  version = {0.0.4},
  year = {2022},
}

@inproceedings{codevilla2018end,
  title={End-to-end driving via conditional imitation learning},
  author={Codevilla, Felipe and M{\"u}ller, Matthias and L{\'o}pez, Antonio and Koltun, Vladlen and Dosovitskiy, Alexey},
  booktitle={2018 IEEE international conference on robotics and automation (ICRA)},
  pages={4693--4700},
  year={2018},
  organization={IEEE}
}

@misc{yu2018oneshot,
      title={One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning}, 
      author={Tianhe Yu and Chelsea Finn and Annie Xie and Sudeep Dasari and Tianhao Zhang and Pieter Abbeel and Sergey Levine},
      year={2018},
      eprint={1802.01557},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{swamy2021moments,
  title={Of moments and matching: A game-theoretic framework for closing the imitation gap},
  author={Swamy, Gokul and Choudhury, Sanjiban and Bagnell, J Andrew and Wu, Steven},
  booktitle={International Conference on Machine Learning},
  pages={10022--10032},
  year={2021},
  organization={PMLR}
}

@article{syed2007game,
  title={A game-theoretic approach to apprenticeship learning},
  author={Syed, Umar and Schapire, Robert E},
  journal={Advances in neural information processing systems},
  volume={20},
  year={2007}
}

@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@ARTICLE{cmaes,
  author={Hansen, Nikolaus and Ostermeier, Andreas},
  journal={Evolutionary Computation}, 
  title={Completely Derandomized Self-Adaptation in Evolution Strategies}, 
  year={2001},
  volume={9},
  number={2},
  pages={159-195},
  doi={10.1162/106365601750190398}}

@article{lu2022discovered,
  title={Discovered policy optimisation},
  author={Lu, Chris and Kuba, Jakub and Letcher, Alistair and Metz, Luke and Schroeder de Witt, Christian and Foerster, Jakob},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16455--16468},
  year={2022}
}

@inproceedings{lange2023evosax,
  title={evosax: Jax-based evolution strategies},
  author={Lange, Robert Tjarko},
  booktitle={Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
  pages={659--662},
  year={2023}
}

@inproceedings{nikishin2022primacy,
  title={The primacy bias in deep reinforcement learning},
  author={Nikishin, Evgenii and Schwarzer, Max and D’Oro, Pierluca and Bacon, Pierre-Luc and Courville, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={16828--16847},
  year={2022},
  organization={PMLR}
}

@article{lyle2022understanding,
  title={Understanding and preventing capacity loss in reinforcement learning},
  author={Lyle, Clare and Rowland, Mark and Dabney, Will},
  journal={arXiv preprint arXiv:2204.09560},
  year={2022}
}



@article{silver2010learning,
  title={Learning from demonstration for autonomous navigation in complex unstructured terrain},
  author={Silver, David and Bagnell, J Andrew and Stentz, Anthony},
  journal={The International Journal of Robotics Research},
  volume={29},
  number={12},
  pages={1565--1592},
  year={2010},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{ratliff2009learning,
  title={Learning to search: Functional gradient techniques for imitation learning},
  author={Ratliff, Nathan D and Silver, David and Bagnell, J Andrew},
  journal={Autonomous Robots},
  volume={27},
  number={1},
  pages={25--53},
  year={2009},
  publisher={Springer}
}

@article{zucker2011optimization,
  title={Optimization and learning for rough terrain legged locomotion},
  author={Zucker, Matt and Ratliff, Nathan and Stolle, Martin and Chestnutt, Joel and Bagnell, J Andrew and Atkeson, Christopher G and Kuffner, James},
  journal={The International Journal of Robotics Research},
  volume={30},
  number={2},
  pages={175--191},
  year={2011},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{kolter2008control,
  title={A control architecture for quadruped locomotion over rough terrain},
  author={Kolter, J Zico and Rodgers, Mike P and Ng, Andrew Y},
  booktitle={2008 IEEE International Conference on Robotics and Automation},
  pages={811--818},
  year={2008},
  organization={IEEE}
}

@incollection{ng2006autonomous,
  title={Autonomous inverted helicopter flight via reinforcement learning},
  author={Ng, Andrew Y and Coates, Adam and Diel, Mark and Ganapathi, Varun and Schulte, Jamie and Tse, Ben and Berger, Eric and Liang, Eric},
  booktitle={Experimental robotics IX},
  pages={363--372},
  year={2006},
  publisher={Springer}
}

@inproceedings{ziebart2012probabilistic,
  title={Probabilistic pointing target prediction via inverse optimal control},
  author={Ziebart, Brian and Dey, Anind and Bagnell, J Andrew},
  booktitle={Proceedings of the 2012 ACM international conference on Intelligent User Interfaces},
  pages={1--10},
  year={2012}
}

@inproceedings{ziebart2008navigate,
  title={Navigate like a cabbie: Probabilistic reasoning from observed context-aware behavior},
  author={Ziebart, Brian D and Maas, Andrew L and Dey, Anind K and Bagnell, J Andrew},
  booktitle={Proceedings of the 10th international conference on Ubiquitous computing},
  pages={322--331},
  year={2008}
}

@article{barde2020adversarial,
  title={Adversarial soft advantage fitting: Imitation learning without policy optimization},
  author={Barde, Paul and Roy, Julien and Jeon, Wonseok and Pineau, Joelle and Pal, Chris and Nowrouzezahrai, Derek},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12334--12344},
  year={2020}
}


@article{pomerleau1988alvinn,
  title={Alvinn: An autonomous land vehicle in a neural network},
  author={Pomerleau, Dean A},
  journal={Advances in neural information processing systems},
  volume={1},
  year={1988}
}

@inproceedings{ross2011reduction,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={627--635},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{hafner2023mastering,
  title={Mastering diverse domains through world models},
  author={Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:2301.04104},
  year={2023}
}

@article{schrittwieser2020mastering,
  title={Mastering atari, go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={Nature},
  volume={588},
  number={7839},
  pages={604--609},
  year={2020},
  publisher={Nature Publishing Group UK London}
}
@inproceedings{farahmand2017value,
  title={Value-aware loss function for model-based reinforcement learning},
  author={Farahmand, Amir-massoud and Barreto, Andre and Nikovski, Daniel},
  booktitle={Artificial Intelligence and Statistics},
  pages={1486--1494},
  year={2017},
  organization={PMLR}
}

@article{kearns2002near,
  title={Near-optimal reinforcement learning in polynomial time},
  author={Kearns, Michael and Singh, Satinder},
  journal={Machine learning},
  volume={49},
  pages={209--232},
  year={2002},
  publisher={Springer}
}

@article{lambert2020objective,
  title={Objective mismatch in model-based reinforcement learning},
  author={Lambert, Nathan and Amos, Brandon and Yadan, Omry and Calandra, Roberto},
  journal={arXiv preprint arXiv:2002.04523},
  year={2020}
}

@inproceedings{vemula2023virtues,
  title={The Virtues of Laziness in Model-based RL: A Unified Objective and Algorithms},
  author={Vemula, Anirudh and Song, Yuda and Singh, Aarti and Bagnell, Drew and Choudhury, Sanjiban},
  booktitle={International Conference on Machine Learning},
  pages={34978--35005},
  year={2023},
  organization={PMLR}
}
@article{waugh2013computational,
  title={Computational rationalization: The inverse equilibrium problem},
  author={Waugh, Kevin and Ziebart, Brian D and Bagnell, J Andrew},
  journal={arXiv preprint arXiv:1308.3506},
  year={2013}
}

@inproceedings{swamy2022causal,
  title={Causal imitation learning under temporally correlated noise},
  author={Swamy, Gokul and Choudhury, Sanjiban and Bagnell, Drew and Wu, Steven},
  booktitle={International Conference on Machine Learning},
  pages={20877--20890},
  year={2022},
  organization={PMLR}
}

@article{swamy2022sequence,
  title={Sequence model imitation learning with unobserved contexts},
  author={Swamy, Gokul and Choudhury, Sanjiban and Bagnell, J and Wu, Steven Z},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17665--17676},
  year={2022}
}

@article{zhang2020causal,
  title={Causal imitation learning with unobserved confounders},
  author={Zhang, Junzhe and Kumor, Daniel and Bareinboim, Elias},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12263--12274},
  year={2020}
}

@ARTICLE{58337,
  author={Werbos, P.J.},
  journal={Proceedings of the IEEE}, 
  title={Backpropagation through time: what it does and how to do it}, 
  year={1990},
  volume={78},
  number={10},
  pages={1550-1560},
  doi={10.1109/5.58337}}

@misc{metz2022gradients,
      title={Gradients are Not All You Need}, 
      author={Luke Metz and C. Daniel Freeman and Samuel S. Schoenholz and Tal Kachman},
      year={2022},
      eprint={2111.05803},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{51791361-8fe2-38d5-959f-ae8d048b490d,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 urldate = {2023-09-27},
 volume = {58},
 year = {1996}
}

@inproceedings{ng1999policy,
  title={Policy invariance under reward transformations: Theory and application to reward shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  booktitle={Icml},
  volume={99},
  pages={278--287},
  year={1999},
  organization={Citeseer}
}

@article{reddy2018you,
  title={Where do you think you're going?: Inferring beliefs about dynamics from behavior},
  author={Reddy, Sid and Dragan, Anca and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{herman2016inverse,
  title={Inverse reinforcement learning with simultaneous estimation of rewards and dynamics},
  author={Herman, Michael and Gindele, Tobias and Wagner, J{\"o}rg and Schmitt, Felix and Burgard, Wolfram},
  booktitle={Artificial intelligence and statistics},
  pages={102--110},
  year={2016},
  organization={PMLR}
}

@inproceedings{lu2023adversarial,
  title={Adversarial cheap talk},
  author={Lu, Chris and Willi, Timon and Letcher, Alistair and Foerster, Jakob Nicolaus},
  booktitle={International Conference on Machine Learning},
  pages={22917--22941},
  year={2023},
  organization={PMLR}
}

@article{les2022,
  title={Meta-learning black-box optimization via black-box optimization},
  author={Lange, Robert Tjarko and Schaul, Tom and Chen, Yutian and Zahavy, Tom and Dallibard, Valentin and Lu, Chris and Singh, Satinder and Flennerhag, Sebastian},
  journal={},
  year={2022},
}

@inproceedings{dudik2004performance,
  title={Performance guarantees for regularized maximum entropy density estimation},
  author={Dudik, Miroslav and Phillips, Steven J and Schapire, Robert E},
  booktitle={International Conference on Computational Learning Theory},
  pages={472--486},
  year={2004},
  organization={Springer}
}

@inproceedings{ng2000algorithms,
  title={Algorithms for inverse reinforcement learning.},
  author={Ng, Andrew Y and Russell, Stuart and others},
  booktitle={Icml},
  volume={1},
  pages={2},
  year={2000}
}

@misc{kingma2022autoencoding,
      title={Auto-Encoding Variational Bayes}, 
      author={Diederik P Kingma and Max Welling},
      year={2022},
      eprint={1312.6114},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{schulman2017proximal,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kumar2020balancing,
      title={Balancing a CartPole System with Reinforcement Learning -- A Tutorial}, 
      author={Swagat Kumar},
      year={2020},
      eprint={2006.04938},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@inproceedings{todorov2012mujoco,
  title={MuJoCo: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE},
  doi={10.1109/IROS.2012.6386109}
}

@article{niekum,
author = {Niekum, Scott and Barto, Andrew and Spector, Lee},
year = {2010},
month = {07},
pages = {83 - 90},
title = {Genetic Programming for Reward Function Search},
volume = {2},
journal = {Autonomous Mental Development, IEEE Transactions on},
doi = {10.1109/TAMD.2010.2051436}
}

@article{ELFWING20183,
title = {Sigmoid-weighted linear units for neural network function approximation in reinforcement learning},
journal = {Neural Networks},
volume = {107},
pages = {3-11},
year = {2018},
note = {Special issue on deep reinforcement learning},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2017.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608017302976},
author = {Stefan Elfwing and Eiji Uchibe and Kenji Doya},
keywords = {Reinforcement learning, Sigmoid-weighted linear unit, Function approximation, Tetris, Atari 2600, Deep learning},
abstract = {In recent years, neural networks have enjoyed a renaissance as function approximators in reinforcement learning. Two decades after Tesauro’s TD-Gammon achieved near top-level human performance in backgammon, the deep reinforcement learning algorithm DQN achieved human-level performance in many Atari 2600 games. The purpose of this study is twofold. First, we propose two activation functions for neural network function approximation in reinforcement learning: the sigmoid-weighted linear unit (SiLU) and its derivative function (dSiLU). The activation of the SiLU is computed by the sigmoid function multiplied by its input. Second, we suggest that the more traditional approach of using on-policy learning with eligibility traces, instead of experience replay, and softmax action selection can be competitive with DQN, without the need for a separate target network. We validate our proposed approach by, first, achieving new state-of-the-art results in both stochastic SZ-Tetris and Tetris with a small 10 × 10 board, using TD(λ) learning and shallow dSiLU network agents, and, then, by outperforming DQN in the Atari 2600 domain by using a deep Sarsa(λ) agent with SiLU and dSiLU hidden units.}
}

@misc{houthooft2018evolved,
      title={Evolved Policy Gradients}, 
      author={Rein Houthooft and Richard Y. Chen and Phillip Isola and Bradly C. Stadie and Filip Wolski and Jonathan Ho and Pieter Abbeel},
      year={2018},
      eprint={1802.04821},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{lu2022model,
  title={Model-free opponent shaping},
  author={Lu, Christopher and Willi, Timon and De Witt, Christian A Schroeder and Foerster, Jakob},
  booktitle={International Conference on Machine Learning},
  pages={14398--14411},
  year={2022},
  organization={PMLR}
}

@software{brax2021github,
  author = {C. Daniel Freeman and Erik Frey and Anton Raichuk and Sertan Girgin and Igor Mordatch and Olivier Bachem},
  title = {Brax - A Differentiable Physics Engine for Large Scale Rigid Body Simulation},
  url = {http://github.com/google/brax},
  version = {0.9.4},
  year = {2021},
}

@INPROCEEDINGS{mujoco,
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
  title={MuJoCo: A physics engine for model-based control}, 
  year={2012},
  volume={},
  number={},
  pages={5026-5033},
  keywords={Engines;Optimization;Computational modeling;Heuristic algorithms;Dynamics;Mathematical model},
  doi={10.1109/IROS.2012.6386109}}


@misc{wortsman2022model,
      title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time}, 
      author={Mitchell Wortsman and Gabriel Ilharco and Samir Yitzhak Gadre and Rebecca Roelofs and Raphael Gontijo-Lopes and Ari S. Morcos and Hongseok Namkoong and Ali Farhadi and Yair Carmon and Simon Kornblith and Ludwig Schmidt},
      year={2022},
      eprint={2203.05482},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zou2019reward,
      title={Reward Shaping via Meta-Learning}, 
      author={Haosheng Zou and Tongzheng Ren and Dong Yan and Hang Su and Jun Zhu},
      year={2019},
      eprint={1901.09330},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Jaderberg_2019,
   title={Human-level performance in 3D multiplayer games with population-based reinforcement learning},
   volume={364},
   ISSN={1095-9203},
   url={http://dx.doi.org/10.1126/science.aau6249},
   DOI={10.1126/science.aau6249},
   number={6443},
   journal={Science},
   publisher={American Association for the Advancement of Science (AAAS)},
   author={Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Castañeda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
   year={2019},
   month=may, pages={859–865} }

@inproceedings{Wu2017Doom,
  title={Training Agent for First-person Shooter Game with Actor-critic Curriculum Learning},
  author={Yuxin Wu and Yuandong Tian},
  journal={International Conference on Learning Representations (ICLR)},
  year={2017}
}

@inproceedings{NEURIPS2020_b7109157,
 author = {Hu, Yujing and Wang, Weixun and Jia, Hangtian and Wang, Yixiang and Chen, Yingfeng and Hao, Jianye and Wu, Feng and Fan, Changjie},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15931--15941},
 publisher = {Curran Associates, Inc.},
 title = {Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/b710915795b9e9c02cf10d6d2bdb688c-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{barnes2023massively,
  title={Massively Scalable Inverse Reinforcement Learning in Google Maps},
  author={Barnes, Matt and Abueg, Matthew and Lange, Oliver F and Deeds, Matt and Trader, Jason and Molitor, Denali and Wulfmeier, Markus and O'Banion, Shawn},
  journal={arXiv preprint arXiv:2305.11290},
  year={2023}
}

@article{freund1997decision,
  title={A decision-theoretic generalization of on-line learning and an application to boosting},
  author={Freund, Yoav and Schapire, Robert E},
  journal={Journal of computer and system sciences},
  volume={55},
  number={1},
  pages={119--139},
  year={1997},
  publisher={Elsevier}
}


@inproceedings{abbeel2004apprenticeship,
  title={Apprenticeship learning via inverse reinforcement learning},
  author={Abbeel, Pieter and Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={1},
  year={2004}
}

@article{likhachev2003ara,
  title={ARA*: Anytime A* with provable bounds on sub-optimality},
  author={Likhachev, Maxim and Gordon, Geoffrey J and Thrun, Sebastian},
  journal={Advances in neural information processing systems},
  volume={16},
  year={2003}
}

@inproceedings{likhachev2005anytime,
  title={Anytime dynamic A*: An anytime, replanning algorithm.},
  author={Likhachev, Maxim and Stentz, Anthony and Thrun, Sebastian},
  year={2005}
}

@article{laidlaw2023bridging,
  title={Bridging RL Theory and Practice with the Effective Horizon},
  author={Laidlaw, Cassidy and Russell, Stuart and Dragan, Anca},
  journal={arXiv preprint arXiv:2304.09853},
  year={2023}
}

@book{russell2010artificial,
  title={Artificial intelligence a modern approach},
  author={Russell, Stuart J and Norvig, Peter},
  year={2010}
}

@article{cooke2023toward,
  title={Toward Computationally Efficient Inverse Reinforcement Learning via Reward Shaping},
  author={Cooke, Lauren H and Klyne, Harvey and Zhang, Edwin and Laidlaw, Cassidy and Tambe, Milind and Doshi-Velez, Finale},
  journal={arXiv preprint arXiv:2312.09983},
  year={2023}
}

@article{swamy2022minimax,
  title={Minimax optimal online imitation learning via replay estimation},
  author={Swamy, Gokul and Rajaraman, Nived and Peng, Matt and Choudhury, Sanjiban and Bagnell, J and Wu, Steven Z and Jiao, Jiantao and Ramchandran, Kannan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7077--7088},
  year={2022}
}

@article{tiapkin2023regularized,
  title={Regularized RL},
  author={Tiapkin, Daniil and Belomestny, Denis and Calandriello, Daniele and Moulines, Eric and Naumov, Alexey and Perrault, Pierre and Valko, Michal and Menard, Pierre},
  journal={arXiv preprint arXiv:2310.17303},
  year={2023}
}

@article{ren2023hyrbid,
  title={Hybrid Inverse Reinforcement Learning},
  author={Juntao Ren and Gokul Swamy and Zhiwei Steven Wu and J. Andrew Bagnell and Sanjiban Choudhury},
  booktitle={6th Robot Learning Workshop at NeurIPS 2023},
  url={https://www.robot-learning.ml/2023/files/paper42.pdf},
  year={2023},
}


@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@inproceedings{flajolet2022fast,
  title={Fast population-based reinforcement learning on a single machine},
  author={Flajolet, Arthur and Monroc, Claire Bizon and Beguir, Karim and Pierrot, Thomas},
  booktitle={International Conference on Machine Learning},
  pages={6533--6547},
  year={2022},
  organization={PMLR}
}

@article{kidambi2020morel,
  title={Morel: Model-based offline reinforcement learning},
  author={Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21810--21823},
  year={2020}
}

@article{vinitsky2022nocturne,
  title={Nocturne: a scalable driving benchmark for bringing multi-agent learning one step closer to the real world},
  author={Vinitsky, Eugene and Lichtl{\'e}, Nathan and Yang, Xiaomeng and Amos, Brandon and Foerster, Jakob},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3962--3974},
  year={2022}
}

@article{gulino2024waymax,
  title={Waymax: An accelerated, data-driven simulator for large-scale autonomous driving research},
  author={Gulino, Cole and Fu, Justin and Luo, Wenjie and Tucker, George and Bronstein, Eli and Lu, Yiren and Harb, Jean and Pan, Xinlei and Wang, Yan and Chen, Xiangyu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{zinkevich2003online,
  title={Online convex programming and generalized infinitesimal gradient ascent},
  author={Zinkevich, Martin},
  booktitle={Proceedings of the 20th international conference on machine learning (icml-03)},
  pages={928--936},
  year={2003}
}

@inproceedings{mcmahan2011follow,
  title={Follow-the-regularized-leader and mirror descent: Equivalence theorems and l1 regularization},
  author={McMahan, Brendan},
  booktitle={Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages={525--533},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

@book{kakade2003sample,
  title={On the sample complexity of reinforcement learning},
  author={Kakade, Sham Machandranath},
  year={2003},
  publisher={University of London, University College London (United Kingdom)}
}

@article{rutherford2023jaxmarl,
  title={JaxMARL: Multi-Agent RL Environments in JAX},
  author={Rutherford, Alexander and Ellis, Benjamin and Gallici, Matteo and Cook, Jonathan and Lupu, Andrei and Ingvarsson, Gardar and Willi, Timon and Khan, Akbir and de Witt, Christian Schroeder and Souly, Alexandra and others},
  journal={arXiv preprint arXiv:2311.10090},
  year={2023}
}

@article{khan2023scaling,
  title={Scaling Opponent Shaping to High Dimensional Games},
  author={Khan, Akbir and Willi, Timon and Kwan, Newton and Tacchetti, Andrea and Lu, Chris and Grefenstette, Edward and Rockt{\"a}schel, Tim and Foerster, Jakob},
  journal={arXiv preprint arXiv:2312.12568},
  year={2023}
}

@inproceedings{jackson2023discovering,
  title={Discovering Temporally-Aware Reinforcement Learning Algorithms},
  author={Jackson, Matthew Thomas and Lu, Chris and Kirsch, Louis and Lange, Robert Tjarko and Whiteson, Shimon and Foerster, Jakob Nicolaus},
  booktitle={Second Agent Learning in Open-Endedness Workshop},
  year={2023}
}

@misc{finn2016connection,
      title={A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models}, 
      author={Chelsea Finn and Paul Christiano and Pieter Abbeel and Sergey Levine},
      year={2016},
      eprint={1611.03852},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
    lupu2024behaviour,
    title={Behaviour Distillation},
    author={Andrei Lupu and Chris Lu and Jarek Luca Liesen and Robert Tjarko Lange and Jakob Nicolaus Foerster},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=qup9xD8mW4}
}