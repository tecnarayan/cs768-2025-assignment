\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bansal et~al.(2017)Bansal, Pachocki, Sidor, Sutskever, and
  Mordatch]{bansal2017emergent}
Bansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I.
\newblock Emergent complexity via multi-agent competition.
\newblock \emph{arXiv preprint arXiv:1710.03748}, 2017.

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, Debiak, Dennison,
  Farhi, Fischer, Hashme, Hesse, et~al.]{dota}
Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C.,
  Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.06680}, 2019.

\bibitem[Bosansky et~al.(2014)Bosansky, Kiekintveld, Lisy, and
  Pechoucek]{bosansky2014exact}
Bosansky, B., Kiekintveld, C., Lisy, V., and Pechoucek, M.
\newblock An exact double-oracle algorithm for zero-sum extensive-form games
  with imperfect information.
\newblock \emph{Journal of Artificial Intelligence Research}, 51:\penalty0
  829--866, 2014.

\bibitem[Brown(1951)]{fp}
Brown, G.~W.
\newblock Iterative solution of games by fictitious play.
\newblock \emph{Activity analysis of production and allocation}, pp.\
  374--376, 1951.

\bibitem[Brown \& Sandholm(2015{\natexlab{a}})Brown and
  Sandholm]{brown2015regret}
Brown, N. and Sandholm, T.
\newblock Regret-based pruning in extensive-form games.
\newblock In \emph{NIPS}, pp.\  1972--1980, 2015{\natexlab{a}}.

\bibitem[Brown \& Sandholm(2015{\natexlab{b}})Brown and
  Sandholm]{brown2015simultaneous}
Brown, N. and Sandholm, T.
\newblock Simultaneous abstraction and equilibrium finding in games.
\newblock In \emph{Twenty-fourth international joint conference on artificial
  intelligence}, 2015{\natexlab{b}}.

\bibitem[Brown et~al.(2017)Brown, Kroer, and Sandholm]{brown2017dynamic}
Brown, N., Kroer, C., and Sandholm, T.
\newblock Dynamic thresholding and pruning for regret minimization.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~31, 2017.

\bibitem[Brown et~al.(2019)Brown, Lerer, Gross, and Sandholm]{deep_cfr}
Brown, N., Lerer, A., Gross, S., and Sandholm, T.
\newblock Deep counterfactual regret minimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  793--802, 2019.

\bibitem[{\v{C}}erm{\'a}k et~al.(2017){\v{C}}erm{\'a}k, Bo{\v{s}}ansky, and
  Lisy]{vcermak2017algorithm}
{\v{C}}erm{\'a}k, J., Bo{\v{s}}ansky, B., and Lisy, V.
\newblock An algorithm for constructing and solving imperfect recall
  abstractions of large extensive-form games.
\newblock In \emph{Proceedings of the 26th International Joint Conference on
  Artificial Intelligence}, pp.\  936--942, 2017.

\bibitem[Foerster et~al.(2018)Foerster, Farquhar, Afouras, Nardelli, and
  Whiteson]{foerster2018counterfactual}
Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.
\newblock Counterfactual multi-agent policy gradients.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Gruslys et~al.(2020)Gruslys, Lanctot, Munos, Timbers, Schmid, Perolat,
  Morrill, Zambaldi, Lespiau, Schultz, et~al.]{gruslys2020advantage}
Gruslys, A., Lanctot, M., Munos, R., Timbers, F., Schmid, M., Perolat, J.,
  Morrill, D., Zambaldi, V., Lespiau, J.-B., Schultz, J., et~al.
\newblock The advantage regret-matching actor-critic.
\newblock \emph{arXiv preprint arXiv:2008.12234}, 2020.

\bibitem[Hansen et~al.(2004)Hansen, Bernstein, and
  Zilberstein]{hansen2004dynamic}
Hansen, E.~A., Bernstein, D.~S., and Zilberstein, S.
\newblock Dynamic programming for partially observable stochastic games.
\newblock In \emph{AAAI}, volume~4, pp.\  709--715, 2004.

\bibitem[Hart \& Mas-Colell(2000)Hart and Mas-Colell]{rm}
Hart, S. and Mas-Colell, A.
\newblock A simple adaptive procedure leading to correlated equilibrium.
\newblock \emph{Econometrica}, 68\penalty0 (5):\penalty0 1127--1150, 2000.

\bibitem[Heinrich \& Silver(2016)Heinrich and Silver]{nfsp}
Heinrich, J. and Silver, D.
\newblock Deep reinforcement learning from self-play in imperfect-information
  games.
\newblock \emph{arXiv preprint arXiv:1603.01121}, 2016.

\bibitem[Heinrich et~al.(2015)Heinrich, Lanctot, and Silver]{xfp}
Heinrich, J., Lanctot, M., and Silver, D.
\newblock Fictitious self-play in extensive-form games.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  805--813, 2015.

\bibitem[Jaderberg et~al.(2019)Jaderberg, Czarnecki, Dunning, Marris, Lever,
  Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, et~al.]{pbt}
Jaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda,
  A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., et~al.
\newblock Human-level performance in 3d multiplayer games with population-based
  reinforcement learning.
\newblock \emph{Science}, 364\penalty0 (6443):\penalty0 859--865, 2019.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{DBLP:journals/corr/KingmaB14}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.6980}.

\bibitem[Kuhn \& Tucker(1953)Kuhn and Tucker]{kuhn1953contributions}
Kuhn, H.~W. and Tucker, A.~W.
\newblock \emph{Contributions to the Theory of Games}, volume~2.
\newblock Princeton University Press, 1953.

\bibitem[Lanctot et~al.(2009)Lanctot, Waugh, Zinkevich, and
  Bowling]{lanctot2009monte}
Lanctot, M., Waugh, K., Zinkevich, M., and Bowling, M.~H.
\newblock Monte carlo sampling for regret minimization in extensive games.
\newblock In \emph{NIPS}, pp.\  1078--1086, 2009.

\bibitem[Lanctot et~al.(2017)Lanctot, Zambaldi, Gruslys, Lazaridou, Tuyls,
  P{\'e}rolat, Silver, and Graepel]{psro}
Lanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A., Tuyls, K., P{\'e}rolat,
  J., Silver, D., and Graepel, T.
\newblock A unified game-theoretic approach to multiagent reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4190--4203, 2017.

\bibitem[Lanctot et~al.(2019{\natexlab{a}})Lanctot, Lockhart, Lespiau,
  Zambaldi, Upadhyay, P\'{e}rolat, Srinivasan, Timbers, Tuyls, Omidshafiei,
  Hennes, Morrill, Muller, Ewalds, Faulkner, Kram\'{a}r, Vylder, Saeta,
  Bradbury, Ding, Borgeaud, Lai, Schrittwieser, Anthony, Hughes, Danihelka, and
  Ryan-Davis]{LanctotEtAl2019OpenSpiel}
Lanctot, M., Lockhart, E., Lespiau, J.-B., Zambaldi, V., Upadhyay, S.,
  P\'{e}rolat, J., Srinivasan, S., Timbers, F., Tuyls, K., Omidshafiei, S.,
  Hennes, D., Morrill, D., Muller, P., Ewalds, T., Faulkner, R., Kram\'{a}r,
  J., Vylder, B.~D., Saeta, B., Bradbury, J., Ding, D., Borgeaud, S., Lai, M.,
  Schrittwieser, J., Anthony, T., Hughes, E., Danihelka, I., and Ryan-Davis, J.
\newblock {OpenSpiel}: A framework for reinforcement learning in games.
\newblock \emph{CoRR}, abs/1908.09453, 2019{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1908.09453}.

\bibitem[Lanctot et~al.(2019{\natexlab{b}})Lanctot, Lockhart, Lespiau,
  Zambaldi, Upadhyay, P{\'e}rolat, Srinivasan, Timbers, Tuyls, Omidshafiei,
  et~al.]{lanctot2019openspiel}
Lanctot, M., Lockhart, E., Lespiau, J.-B., Zambaldi, V., Upadhyay, S.,
  P{\'e}rolat, J., Srinivasan, S., Timbers, F., Tuyls, K., Omidshafiei, S.,
  et~al.
\newblock Openspiel: A framework for reinforcement learning in games.
\newblock \emph{arXiv preprint arXiv:1908.09453}, 2019{\natexlab{b}}.

\bibitem[Li et~al.(2018)Li, Hu, Ge, Jiang, Qi, and Song]{li2018double}
Li, H., Hu, K., Ge, Z., Jiang, T., Qi, Y., and Song, L.
\newblock Double neural counterfactual regret minimization.
\newblock \emph{arXiv preprint arXiv:1812.10607}, 2018.

\bibitem[Liang et~al.(2018)Liang, Liaw, Nishihara, Moritz, Fox, Goldberg,
  Gonzalez, Jordan, and Stoica]{pmlr-v80-liang18b}
Liang, E., Liaw, R., Nishihara, R., Moritz, P., Fox, R., Goldberg, K.,
  Gonzalez, J., Jordan, M., and Stoica, I.
\newblock {RL}lib: Abstractions for distributed reinforcement learning.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  3053--3062, Stockholmsm√§ssan, Stockholm
  Sweden, 10--15 Jul 2018. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v80/liang18b.html}.

\bibitem[Lowe et~al.(2017)Lowe, Wu, Tamar, Harb, Abbeel, and
  Mordatch]{lowe2017multi}
Lowe, R., Wu, Y.~I., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  6379--6390, 2017.

\bibitem[Majumdar et~al.(2020)Majumdar, Khadka, Miret, Mcaleer, and
  Tumer]{majumdar2020evolutionary}
Majumdar, S., Khadka, S., Miret, S., Mcaleer, S., and Tumer, K.
\newblock Evolutionary reinforcement learning for sample-efficient multiagent
  coordination.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6651--6660. PMLR, 2020.

\bibitem[McAleer et~al.(2020)McAleer, Lanier, Fox, and
  Baldi]{mcaleer2020pipeline}
McAleer, S., Lanier, J., Fox, R., and Baldi, P.
\newblock Pipeline psro: A scalable approach for finding approximate nash
  equilibria in large games.
\newblock 2020.

\bibitem[McMahan et~al.(2003)McMahan, Gordon, and Blum]{double_oracle}
McMahan, H.~B., Gordon, G.~J., and Blum, A.
\newblock Planning in the presence of cost functions controlled by an
  adversary.
\newblock In \emph{Proceedings of the 20th International Conference on Machine
  Learning (ICML-03)}, pp.\  536--543, 2003.

\bibitem[Rashid et~al.(2018)Rashid, Samvelyan, De~Witt, Farquhar, Foerster, and
  Whiteson]{rashid2018qmix}
Rashid, T., Samvelyan, M., De~Witt, C.~S., Farquhar, G., Foerster, J., and
  Whiteson, S.
\newblock Qmix: Monotonic value function factorisation for deep multi-agent
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1803.11485}, 2018.

\bibitem[Schmid et~al.(2014)Schmid, Moravcik, and Hladik]{schmid2014bounding}
Schmid, M., Moravcik, M., and Hladik, M.
\newblock Bounding the support size in extensive form games with imperfect
  information.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~28, 2014.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Steinberger(2019)]{steinberger2019single}
Steinberger, E.
\newblock Single deep counterfactual regret minimization.
\newblock \emph{arXiv preprint arXiv:1901.07621}, 2019.

\bibitem[Steinberger et~al.(2020)Steinberger, Lerer, and
  Brown]{steinberger2020dream}
Steinberger, E., Lerer, A., and Brown, N.
\newblock Dream: Deep regret minimization with advantage baselines and
  model-free learning.
\newblock \emph{arXiv preprint arXiv:2006.10410}, 2020.

\bibitem[Tammelin(2014)]{DBLP:journals/corr/Tammelin14}
Tammelin, O.
\newblock Solving large imperfect information games using {CFR+}.
\newblock \emph{CoRR}, abs/1407.5042, 2014.
\newblock URL \url{http://arxiv.org/abs/1407.5042}.

\bibitem[Van~Hasselt et~al.(2016)Van~Hasselt, Guez, and Silver]{van2016deep}
Van~Hasselt, H., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~30, 2016.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{alphastar}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Zinkevich et~al.(2008)Zinkevich, Johanson, Bowling, and Piccione]{cfr}
Zinkevich, M., Johanson, M., Bowling, M., and Piccione, C.
\newblock Regret minimization in games with incomplete information.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1729--1736, 2008.

\end{thebibliography}
