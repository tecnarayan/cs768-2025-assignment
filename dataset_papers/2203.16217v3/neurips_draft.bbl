\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bakry et~al.(2008)Bakry, Barthe, Cattiaux, and Guillin]{BBCG2008}
D.~Bakry, F.~Barthe, P.~Cattiaux, and A.~Guillin.
\newblock A simple proof of the {P}oincar{\'e} inequality for a large class of
  probability measures.
\newblock \emph{Electronic Communications in Probability}, 13:\penalty0 60--66,
  2008.

\bibitem[Balasubramanian et~al.(2022)Balasubramanian, Chewi, Erdogdu, Salim,
  and Zhang]{BCESZ2022}
K.~Balasubramanian, S.~Chewi, M.~A. Erdogdu, A.~Salim, and M.~Zhang.
\newblock Towards a theory of non-log-concave sampling: first-order
  stationarity guarantees for langevin monte carlo.
\newblock \emph{arXiv preprint arXiv:2202.05214}, 2022.

\bibitem[Borkar and Mitter(1999)]{BM1999}
V.~S. Borkar and S.~K. Mitter.
\newblock A strong approximation theorem for stochastic recursive algorithms.
\newblock \emph{Journal of Optimization Theory and Applications}, 100\penalty0
  (3):\penalty0 499--513, 1999.

\bibitem[Bovier and Den~Hollander(2016)]{BD2016}
A.~Bovier and F.~Den~Hollander.
\newblock \emph{Metastability: a Potential-Theoretic Approach}, volume 351.
\newblock Springer, 2016.

\bibitem[Cattiaux et~al.(2010)Cattiaux, Guillin, and Wu]{CGW2010}
P.~Cattiaux, A.~Guillin, and L.-M. Wu.
\newblock A note on {T}alagrand's transportation inequality and logarithmic
  {S}obolev inequality.
\newblock \emph{Probability Theory and Related Fields}, 148\penalty0
  (1):\penalty0 285--304, 2010.

\bibitem[Chatterji et~al.(2018)Chatterji, Flammarion, Ma, Bartlett, and
  Jordan]{CFMBJ2018}
N.~Chatterji, N.~Flammarion, Y.~Ma, P.~Bartlett, and M.~Jordan.
\newblock On the theory of variance reduction for stochastic gradient {M}onte
  {C}arlo.
\newblock In \emph{International Conference on Machine Learning}, pages
  764--773. PMLR, 2018.

\bibitem[Chen et~al.(2021)Chen, Lu, and Xu]{CLX2021}
P.~Chen, J.~Lu, and L.~Xu.
\newblock Approximation to stochastic variance reduced gradient {L}angevin
  dynamics by stochastic delay differential equations.
\newblock \emph{arXiv preprint arXiv:2106.04357}, 2021.

\bibitem[Chiang et~al.(1987)Chiang, Hwang, and Sheu]{CHS1987}
T.-S. Chiang, C.-R. Hwang, and S.~J. Sheu.
\newblock Diffusion for global optimization in $\mathbb{R}^n$.
\newblock \emph{SIAM Journal on Control and Optimization}, 25\penalty0
  (3):\penalty0 737--753, 1987.

\bibitem[Dalalyan(2017{\natexlab{a}})]{D2017b}
A.~Dalalyan.
\newblock Further and stronger analogy between sampling and optimization:
  {L}angevin {M}onte {C}arlo and gradient descent.
\newblock In \emph{Conference on Learning Theory}, pages 678--689. PMLR,
  2017{\natexlab{a}}.

\bibitem[Dalalyan(2017{\natexlab{b}})]{D2017a}
A.~S. Dalalyan.
\newblock Theoretical guarantees for approximate sampling from smooth and
  log-concave densities.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 79\penalty0 (3):\penalty0 651--676, 2017{\natexlab{b}}.

\bibitem[Dubey et~al.(2016)Dubey, J~Reddi, Williamson, Poczos, Smola, and
  Xing]{DJWPSX2016}
K.~A. Dubey, S.~J~Reddi, S.~A. Williamson, B.~Poczos, A.~J. Smola, and E.~P.
  Xing.
\newblock Variance reduction in stochastic gradient {L}angevin dynamics.
\newblock \emph{Advances in Neural Information Processing Systems},
  29:\penalty0 1154--1162, 2016.

\bibitem[Gelfand and Mitter(1991)]{GM1991}
S.~B. Gelfand and S.~K. Mitter.
\newblock {Recursive stochastic algorithms for global optimization in
  $\mathbb{R}^d$}.
\newblock \emph{SIAM Journal on Control and Optimization}, 29\penalty0
  (5):\penalty0 999--1018, 1991.

\bibitem[Holley and Stroock(1986)]{HS1986}
R.~Holley and D.~W. Stroock.
\newblock Logarithmic {S}obolev inequalities and stochastic {I}sing models.
\newblock 1986.

\bibitem[Huang and Becker(2021)]{HB2021}
Z.~Huang and S.~Becker.
\newblock Stochastic gradient {L}angevin dynamics with variance reduction.
\newblock In \emph{2021 International Joint Conference on Neural Networks},
  pages 1--8. IEEE, 2021.

\bibitem[Hwang(1980)]{H1980}
C.-R. Hwang.
\newblock {Laplace's method revisited: weak convergence of probability
  measures}.
\newblock \emph{The Annals of Probability}, 8\penalty0 (6):\penalty0
  1177--1182, 1980.

\bibitem[Johnson and Zhang(2013)]{JZ2013}
R.~Johnson and T.~Zhang.
\newblock {Accelerating stochastic gradient descent using predictive variance
  reduction}.
\newblock \emph{Advances in Neural Information Processing Systems},
  26:\penalty0 315--323, 2013.

\bibitem[Jordan et~al.(1998)Jordan, Kinderlehrer, and Otto]{JKO1998}
R.~Jordan, D.~Kinderlehrer, and F.~Otto.
\newblock {The variational formulation of the Fokker--Planck equation}.
\newblock \emph{SIAM Journal on Mathematical Analysis}, 29\penalty0
  (1):\penalty0 1--17, 1998.

\bibitem[Li and Erdogdu(2020)]{LE2020}
M.~B. Li and M.~A. Erdogdu.
\newblock {Riemannian langevin algorithm for solving semidefinite programs}.
\newblock \emph{arXiv preprint arXiv:2010.11176v4}, 2020.

\bibitem[Li(2019)]{L2019}
Z.~Li.
\newblock {SSRGD}: {S}imple stochastic recursive gradient descent for escaping
  saddle points.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Mattingly et~al.(2002)Mattingly, Stuart, and Higham]{MSH2002}
J.~C. Mattingly, A.~M. Stuart, and D.~J. Higham.
\newblock {Ergodicity for {SDE}s and approximations: locally Lipschitz vector
  fields and degenerate noise}.
\newblock \emph{Stochastic Processes and their Applications}, 101\penalty0
  (2):\penalty0 185--232, 2002.

\bibitem[Menz and Schlichting(2014)]{MS2014}
G.~Menz and A.~Schlichting.
\newblock {Poincar{\'e} and logarithmic Sobolev inequalities by decomposition
  of the energy landscape}.
\newblock \emph{The Annals of Probability}, 42\penalty0 (5):\penalty0
  1809--1884, 2014.

\bibitem[Nguyen et~al.(2017{\natexlab{a}})Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{NLST2017a}
L.~M. Nguyen, J.~Liu, K.~Scheinberg, and M.~Tak{\'a}{\v{c}}.
\newblock {SARAH: A novel method for machine learning problems using stochastic
  recursive gradient}.
\newblock In \emph{International Conference on Machine Learning}, pages
  2613--2621. PMLR, 2017{\natexlab{a}}.

\bibitem[Nguyen et~al.(2017{\natexlab{b}})Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{NLST2017b}
L.~M. Nguyen, J.~Liu, K.~Scheinberg, and M.~Tak{\'a}{\v{c}}.
\newblock {Stochastic recursive gradient algorithm for nonconvex optimization}.
\newblock \emph{arXiv preprint arXiv:1705.07261}, 2017{\natexlab{b}}.

\bibitem[Otto and Villani(2000)]{OV2000}
F.~Otto and C.~Villani.
\newblock {Generalization of an inequality by Talagrand and links with the
  logarithmic Sobolev inequality}.
\newblock \emph{Journal of Functional Analysis}, 173\penalty0 (2):\penalty0
  361--400, 2000.

\bibitem[P{\'e}rez-Cruz(2008)]{P2008}
F.~P{\'e}rez-Cruz.
\newblock Estimation of information theoretic measures for continuous random
  variables.
\newblock \emph{Advances in neural information processing systems}, 21, 2008.

\bibitem[Pham et~al.(2020)Pham, Nguyen, Phan, and Tran-Dinh]{PNPT2020}
N.~H. Pham, L.~M. Nguyen, D.~T. Phan, and Q.~Tran-Dinh.
\newblock {ProxSARAH: An efficient algorithmic framework for stochastic
  composite nonconvex optimization.}
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (110):\penalty0 1--48, 2020.

\bibitem[Raginsky et~al.(2017)Raginsky, Rakhlin, and Telgarsky]{RRT2017}
M.~Raginsky, A.~Rakhlin, and M.~Telgarsky.
\newblock {Non-convex learning via stochastic gradient langevin dynamics: a
  nonasymptotic analysis}.
\newblock In \emph{Conference on Learning Theory}, pages 1674--1703. PMLR,
  2017.

\bibitem[Vempala and Wibisono(2019)]{VW2019}
S.~Vempala and A.~Wibisono.
\newblock {Rapid convergence of the unadjusted Langevin algorithm: Isoperimetry
  suffices}.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 8094--8106, 2019.

\bibitem[Wainwright(2019)]{W2019}
M.~J. Wainwright.
\newblock \emph{{High-Dimensional Statistics: A Non-Asymptotic Viewpoint}},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Wang et~al.(2019)Wang, Ji, Zhou, Liang, and Tarokh]{WJZLT2019}
Z.~Wang, K.~Ji, Y.~Zhou, Y.~Liang, and V.~Tarokh.
\newblock {SpiderBoost and momentum: Faster variance reduction algorithms}.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Welling and Teh(2011)]{WT2011}
M.~Welling and Y.~W. Teh.
\newblock {Bayesian learning via stochastic gradient Langevin dynamics}.
\newblock In \emph{International Conference on Machine Learning}, pages
  681--688. Citeseer, 2011.

\bibitem[Wibisono(2018)]{W2018}
A.~Wibisono.
\newblock {Sampling as optimization in the space of measures: The Langevin
  dynamics as a composite optimization problem}.
\newblock In \emph{Conference on Learning Theory}, pages 2093--3027. PMLR,
  2018.

\bibitem[Xu et~al.(2018)Xu, Chen, Zou, and Gu]{XCZG2017}
P.~Xu, J.~Chen, D.~Zou, and Q.~Gu.
\newblock {Global convergence of Langevin dynamics based algorithms for
  nonconvex optimization}.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Zou et~al.(2018)Zou, Xu, and Gu]{ZXG2018}
D.~Zou, P.~Xu, and Q.~Gu.
\newblock {Subsampled stochastic variance-reduced gradient Langevin dynamics}.
\newblock In \emph{International Conference on Uncertainty in Artificial
  Intelligence}, 2018.

\bibitem[Zou et~al.(2019{\natexlab{a}})Zou, Xu, and Gu]{ZXG2019a}
D.~Zou, P.~Xu, and Q.~Gu.
\newblock {Sampling from non-log-concave distributions via variance-reduced
  gradient Langevin dynamics}.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2936--2945. PMLR, 2019{\natexlab{a}}.

\bibitem[Zou et~al.(2019{\natexlab{b}})Zou, Xu, and Gu]{ZXG2019b}
D.~Zou, P.~Xu, and Q.~Gu.
\newblock {Stochastic gradient Hamiltonian Monte Carlo methods with recursive
  variance reduction}.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 3835--3846, 2019{\natexlab{b}}.

\bibitem[Zou et~al.(2021)Zou, Xu, and Gu]{ZXG2020}
D.~Zou, P.~Xu, and Q.~Gu.
\newblock {Faster convergence of stochastic gradient Langevin Dynamics for
  non-log-concave sampling}.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 1152--1162.
  PMLR, 2021.

\end{thebibliography}
