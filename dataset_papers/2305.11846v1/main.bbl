\begin{thebibliography}{10}

\bibitem{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
  Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds,
  et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock {\em Advances in Neural Information Processing Systems},
  35:23716--23736, 2022.

\bibitem{latent_shift}
Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and
  Xi~Yin.
\newblock Latent-shift: Latent diffusion with temporal shift for efficient
  text-to-video generation.
\newblock {\em arXiv preprint arXiv:2304.08477}, 2023.

\bibitem{aytar2016soundnet}
Yusuf Aytar, Carl Vondrick, and Antonio Torralba.
\newblock Soundnet: Learning sound representations from unlabeled video.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{bain2021frozen}
Max Bain, Arsha Nagrani, G{\"u}l Varol, and Andrew Zisserman.
\newblock Frozen in time: A joint video and image encoder for end-to-end
  retrieval.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1728--1738, 2021.

\bibitem{blattmann2023align}
Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung~Wook Kim,
  Sanja Fidler, and Karsten Kreis.
\newblock Align your latents: High-resolution video synthesis with latent
  diffusion models.
\newblock {\em arXiv preprint arXiv:2304.08818}, 2023.

\bibitem{bubeck2023sparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
  Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg,
  et~al.
\newblock Sparks of artificial general intelligence: Early experiments with
  gpt-4.
\newblock {\em arXiv preprint arXiv:2303.12712}, 2023.

\bibitem{chen2023valor}
Sihan Chen, Xingjian He, Longteng Guo, Xinxin Zhu, Weining Wang, Jinhui Tang,
  and Jing Liu.
\newblock Valor: Vision-audio-language omni-perception pretraining model and
  dataset.
\newblock {\em arXiv preprint arXiv:2304.08345}, 2023.

\bibitem{cho2021unifying}
Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal.
\newblock Unifying vision-and-language tasks via text generation.
\newblock In {\em International Conference on Machine Learning}, pages
  1931--1942. PMLR, 2021.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{ding2021cogview}
Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da~Yin, Junyang
  Lin, Xu~Zou, Zhou Shao, Hongxia Yang, and Jie Tang.
\newblock Cogview: Mastering text-to-image generation via transformers.
\newblock {\em arXiv preprint arXiv:2105.13290}, 2021.

\bibitem{dosovitskiy2021image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10687--10696, 2021.

\bibitem{elizalde2022clap}
Benjamin Elizalde, Soham Deshmukh, Mahmoud~Al Ismail, and Huaming Wang.
\newblock Clap: Learning audio concepts from natural language supervision.
\newblock {\em arXiv preprint arXiv:2206.04769}, 2022.

\bibitem{gen-1}
Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and
  Anastasis Germanidis.
\newblock Structure and content-guided video synthesis with diffusion models.
\newblock {\em arXiv preprint arXiv:2302.03011}, 2023.

\bibitem{esser2023structure}
Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and
  Anastasis Germanidis.
\newblock Structure and content-guided video synthesis with diffusion models.
\newblock {\em arXiv preprint arXiv:2302.03011}, 2023.

\bibitem{gafni2022make}
Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv
  Taigman.
\newblock Make-a-scene: Scene-based text-to-image generation with human priors.
\newblock In {\em Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XV}, pages 89--106.
  Springer, 2022.

\bibitem{gemmeke2017audio}
Jort~F Gemmeke, Daniel~PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence,
  R~Channing Moore, Manoj Plakal, and Marvin Ritter.
\newblock Audio set: An ontology and human-labeled dataset for audio events.
\newblock In {\em 2017 IEEE international conference on acoustics, speech and
  signal processing (ICASSP)}, pages 776--780. IEEE, 2017.

\bibitem{gontier2021automated}
F{\'e}lix Gontier, Romain Serizel, and Christophe Cerisara.
\newblock Automated audio captioning by fine-tuning bart with audioset tags.
\newblock In {\em Detection and Classification of Acoustic Scenes and
  Events-DCASE 2021}, 2021.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{ho2022imagen}
Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey
  Gritsenko, Diederik~P Kingma, Ben Poole, Mohammad Norouzi, David~J Fleet,
  et~al.
\newblock Imagen video: High definition video generation with diffusion models.
\newblock {\em arXiv preprint arXiv:2210.02303}, 2022.

\bibitem{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6840--6851, 2020.

\bibitem{ho2022video}
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi,
  and David~J Fleet.
\newblock Video diffusion models.
\newblock {\em arXiv preprint arXiv:2204.03458}, 2022.

\bibitem{hong2022cogvideo}
Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang.
\newblock Cogvideo: Large-scale pretraining for text-to-video generation via
  transformers.
\newblock {\em arXiv preprint arXiv:2205.15868}, 2022.

\bibitem{huang2023make}
Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi~Ren, Luping Liu, Mingze Li,
  Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao.
\newblock Make-an-audio: Text-to-audio generation with prompt-enhanced
  diffusion models.
\newblock {\em arXiv preprint arXiv:2301.12661}, 2023.

\bibitem{kim2019audiocaps}
Chris~Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim.
\newblock Audiocaps: Generating captions for audios in the wild.
\newblock In {\em Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 119--132, 2019.

\bibitem{kim2022improving}
Eungbeom Kim, Jinhee Kim, Yoori Oh, Kyungsu Kim, Minju Park, Jaeheon Sim,
  Jinwoo Lee, and Kyogu Lee.
\newblock Improving audio-language learning with mixgen and multi-level
  test-time augmentation.
\newblock {\em arXiv preprint arXiv:2210.17143}, 2022.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kong2020hifi}
Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae.
\newblock Hifi-gan: Generative adversarial networks for efficient and high
  fidelity speech synthesis.
\newblock {\em Advances in Neural Information Processing Systems},
  33:17022--17033, 2020.

\bibitem{lee2021acav100m}
Sangho Lee, Jiwan Chung, Youngjae Yu, Gunhee Kim, Thomas Breuel, Gal Chechik,
  and Yale Song.
\newblock Acav100m: Automatic curation of large-scale datasets for audio-visual
  video representation learning.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 10274--10284, 2021.

\bibitem{li2020optimus}
Chunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiujun Li, Yizhe Zhang, and
  Jianfeng Gao.
\newblock Optimus: Organizing sentences via pre-trained modeling of a latent
  space.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 4678--4699, 2020.

\bibitem{li2023blip}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image
  encoders and large language models.
\newblock {\em arXiv preprint arXiv:2301.12597}, 2023.

\bibitem{li2020oscar}
Xiujun Li, Xi~Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan
  Wang, Houdong Hu, Li~Dong, Furu Wei, et~al.
\newblock Oscar: Object-semantics aligned pre-training for vision-language
  tasks.
\newblock In {\em Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part XXX 16}, pages 121--137.
  Springer, 2020.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em Computer Vision--ECCV 2014: 13th European Conference, Zurich,
  Switzerland, September 6-12, 2014, Proceedings, Part V 13}, pages 740--755.
  Springer, 2014.

\bibitem{liu2023audioldm}
Haohe Liu, Zehua Chen, Yi~Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu
  Wang, and Mark~D Plumbley.
\newblock Audioldm: Text-to-audio generation with latent diffusion models.
\newblock {\em arXiv preprint arXiv:2301.12503}, 2023.

\bibitem{luo2022semantic}
Jianjie Luo, Yehao Li, Yingwei Pan, Ting Yao, Jianlin Feng, Hongyang Chao, and
  Tao Mei.
\newblock Semantic-conditional diffusion networks for image captioning.
\newblock {\em arXiv preprint arXiv:2212.03099}, 2022.

\bibitem{mokady2021clipcap}
Ron Mokady, Amir Hertz, and Amit~H Bermano.
\newblock Clipcap: Clip prefix for image captioning.
\newblock {\em arXiv preprint arXiv:2111.09734}, 2021.

\bibitem{nichol2021glide}
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,
  Bob McGrew, Ilya Sutskever, and Mark Chen.
\newblock Glide: Towards photorealistic image generation and editing with
  text-guided diffusion models.
\newblock {\em arXiv preprint arXiv:2112.10741}, 2021.

\bibitem{pryzant2023automatic}
Reid Pryzant, Dan Iter, Jerry Li, Yin~Tat Lee, Chenguang Zhu, and Michael Zeng.
\newblock Automatic prompt optimization with" gradient descent" and beam
  search.
\newblock {\em arXiv preprint arXiv:2305.03495}, 2023.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International conference on machine learning}, pages
  8748--8763. PMLR, 2021.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{ramesh2022hierarchical}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock {\em arXiv preprint arXiv:2204.06125}, 2022.

\bibitem{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn
  Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10684--10695, 2022.

\bibitem{schuhmann2022laionb}
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade~W Gordon, Ross
  Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell
  Wortsman, Patrick Schramowski, Srivatsa~R Kundurthy, Katherine Crowson,
  Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.
\newblock {LAION}-5b: An open large-scale dataset for training next generation
  image-text models.
\newblock In {\em Thirty-sixth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}, 2022.

\bibitem{seo2022end}
Paul~Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and Cordelia Schmid.
\newblock End-to-end generative pretraining for multimodal video captioning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 17959--17968, 2022.

\bibitem{singer2022make}
Uriel Singer, Adam Polyak, Thomas Hayes, Xi~Yin, Jie An, Songyang Zhang, Qiyuan
  Hu, Harry Yang, Oron Ashual, Oran Gafni, et~al.
\newblock Make-a-video: Text-to-video generation without text-video data.
\newblock {\em arXiv preprint arXiv:2209.14792}, 2022.

\bibitem{sohl2015deep}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In {\em International Conference on Machine Learning}, pages
  2256--2265. PMLR, 2015.

\bibitem{song2021scorebased}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{tang2022tvlt}
Zineng Tang, Jaemin Cho, Yixin Nie, and Mohit Bansal.
\newblock {TVLT}: Textless vision-language transformer.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{wang2022git}
Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan,
  Zicheng Liu, Ce~Liu, and Lijuan Wang.
\newblock Git: A generative image-to-text transformer for vision and language.
\newblock {\em arXiv preprint arXiv:2205.14100}, 2022.

\bibitem{wang2022unifying}
Peng Wang, An~Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma,
  Chang Zhou, Jingren Zhou, and Hongxia Yang.
\newblock Unifying architectures, tasks, and modalities through a simple
  sequence-to-sequence learning framework.
\newblock {\em arXiv preprint arXiv:2202.03052}, 2022.

\bibitem{wu2021godiva}
Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo
  Sapiro, and Nan Duan.
\newblock Godiva: Generating open-domain videos from natural descriptions.
\newblock {\em arXiv preprint arXiv:2104.14806}, 2021.

\bibitem{wu2022nuwa}
Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan
  Duan.
\newblock N{\"u}wa: Visual synthesis pre-training for neural visual world
  creation.
\newblock In {\em Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XVI}, pages 720--736.
  Springer, 2022.

\bibitem{xu2023mplug}
Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang
  Li, Bin Bi, Qi~Qian, Wei Wang, et~al.
\newblock mplug-2: A modularized multi-modal foundation model across text,
  image and video.
\newblock {\em arXiv preprint arXiv:2302.00402}, 2023.

\bibitem{xu2022versatile}
Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, and Humphrey Shi.
\newblock Versatile diffusion: Text, images and variations all in one diffusion
  model.
\newblock {\em arXiv preprint arXiv:2211.08332}, 2022.

\bibitem{xue2022advancing}
Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang,
  Jianlong Fu, and Baining Guo.
\newblock Advancing high-resolution video-language representation with
  large-scale video transcriptions.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 5036--5045, 2022.

\bibitem{yang2022code}
Ziyi Yang, Yuwei Fang, Chenguang Zhu, Reid Pryzant, Dongdong Chen, Yu~Shi,
  Yichong Xu, Yao Qian, Mei Gao, Yi-Ling Chen, et~al.
\newblock i-code: An integrative and composable multimodal learning framework.
\newblock {\em arXiv preprint arXiv:2205.01818}, 2022.

\bibitem{zellers2022merlot}
Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza
  Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi.
\newblock Merlot reserve: Neural script knowledge through vision and language
  and sound.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 16375--16387, 2022.

\bibitem{zellers2021merlot}
Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae~Sung Park, Jize Cao,
  Ali Farhadi, and Yejin Choi.
\newblock Merlot: Multimodal neural script knowledge models.
\newblock {\em Advances in Neural Information Processing Systems},
  34:23634--23651, 2021.

\bibitem{zhang2020object}
Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin Wang, Weiming Hu, and
  Zheng-Jun Zha.
\newblock Object relational graph with teacher-recommended learning for video
  captioning.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 13278--13288, 2020.

\bibitem{zhu2022exploring}
Zixin Zhu, Yixuan Wei, Jianfeng Wang, Zhe Gan, Zheng Zhang, Le~Wang, Gang Hua,
  Lijuan Wang, Zicheng Liu, and Han Hu.
\newblock Exploring discrete diffusion models for image captioning.
\newblock {\em arXiv preprint arXiv:2211.11694}, 2022.

\end{thebibliography}
