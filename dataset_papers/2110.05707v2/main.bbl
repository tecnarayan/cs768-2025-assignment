\begin{thebibliography}{100}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2021)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2021theory}
Alekh Agarwal, Sham~M Kakade, Jason~D Lee, and Gaurav Mahajan.
\newblock On the theory of policy gradient methods: Optimality, approximation,
  and distribution shift.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (98):\penalty0 1--76, 2021.

\bibitem[Allen-Zhu \& Hazan(2016)Allen-Zhu and Hazan]{allen2016variance}
Zeyuan Allen-Zhu and Elad Hazan.
\newblock Variance reduction for faster non-convex optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  699--707. PMLR, 2016.

\bibitem[Arabneydi \& Mahajan(2015)Arabneydi and
  Mahajan]{arabneydi2015reinforcement}
Jalal Arabneydi and Aditya Mahajan.
\newblock Reinforcement learning in decentralized stochastic control systems
  with partial history sharing.
\newblock In \emph{American Control Conference}, pp.\  5449--5456. IEEE, 2015.

\bibitem[Arjevani et~al.(2019)Arjevani, Carmon, Duchi, Foster, Srebro, and
  Woodworth]{arjevani2019lower}
Yossi Arjevani, Yair Carmon, John~C Duchi, Dylan~J Foster, Nathan Srebro, and
  Blake Woodworth.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1912.02365}, 2019.

\bibitem[Arslan \& Y{\"u}ksel(2016)Arslan and
  Y{\"u}ksel]{arslan2016decentralized}
G{\"u}rdal Arslan and Serdar Y{\"u}ksel.
\newblock Decentralized {Q}-learning for stochastic teams and games.
\newblock \emph{IEEE Transactions on Automatic Control}, 62\penalty0
  (4):\penalty0 1545--1558, 2016.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, Freund, and
  Schapire]{auer2002nonstochastic}
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert~E Schapire.
\newblock The nonstochastic multiarmed bandit problem.
\newblock \emph{SIAM Journal on Computing}, 32\penalty0 (1):\penalty0 48--77,
  2002.

\bibitem[Avner \& Mannor(2014)Avner and Mannor]{avner2014concurrent}
Orly Avner and Shie Mannor.
\newblock Concurrent bandits and cognitive radio networks.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pp.\  66--81, 2014.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  263--272, 2017.

\bibitem[Bai \& Jin(2020)Bai and Jin]{bai2020provable}
Yu~Bai and Chi Jin.
\newblock Provable self-play algorithms for competitive reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  551--560, 2020.

\bibitem[Bai et~al.(2020)Bai, Jin, and Yu]{bai2020near}
Yu~Bai, Chi Jin, and Tiancheng Yu.
\newblock Near-optimal reinforcement learning with self-play.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Bernstein et~al.(2009)Bernstein, Amato, Hansen, and
  Zilberstein]{bernstein2009policy}
Daniel~S Bernstein, Christopher Amato, Eric~A Hansen, and Shlomo Zilberstein.
\newblock Policy iteration for decentralized control of {M}arkov decision
  processes.
\newblock \emph{Journal of Artificial Intelligence Research}, 34:\penalty0
  89--132, 2009.

\bibitem[Blum \& Mansour(2007)Blum and Mansour]{blum2007external}
Avrim Blum and Yishay Mansour.
\newblock From external to internal regret.
\newblock \emph{Journal of Machine Learning Research}, 8\penalty0 (6), 2007.

\bibitem[Boutilier(1996)]{boutilier1996planning}
Craig Boutilier.
\newblock Planning, learning and coordination in multiagent decision processes.
\newblock In \emph{Conference on Theoretical Aspects of Rationality and
  Knowledge}, pp.\  195--210, 1996.

\bibitem[Brafman \& Tennenholtz(2002)Brafman and Tennenholtz]{brafman2002r}
Ronen~I Brafman and Moshe Tennenholtz.
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Oct):\penalty0 213--231, 2002.

\bibitem[Brown \& Sandholm(2018)Brown and Sandholm]{brown2018superhuman}
Noam Brown and Tuomas Sandholm.
\newblock Superhuman {AI} for heads-up no-limit poker: {L}ibratus beats top
  professionals.
\newblock \emph{Science}, 359\penalty0 (6374):\penalty0 418--424, 2018.

\bibitem[Bubeck et~al.(2015)]{bubeck2015convex}
S{\'e}bastien Bubeck et~al.
\newblock Convex optimization: {A}lgorithms and complexity.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  8\penalty0 (3-4):\penalty0 231--357, 2015.

\bibitem[Cesa-Bianchi \& Lugosi(2006)Cesa-Bianchi and
  Lugosi]{cesa2006prediction}
Nicolo Cesa-Bianchi and G{\'a}bor Lugosi.
\newblock \emph{Prediction, Learning, and Games}.
\newblock Cambridge University Press, 2006.

\bibitem[Chang et~al.(2021)Chang, Jafarnia-Jahromi, and Jain]{chang2021online}
William Chang, Mehdi Jafarnia-Jahromi, and Rahul Jain.
\newblock Online learning for cooperative multi-player multi-armed bandits.
\newblock \emph{arXiv preprint arXiv:2109.03818}, 2021.

\bibitem[Claus \& Boutilier(1998)Claus and Boutilier]{claus1998dynamics}
Caroline Claus and Craig Boutilier.
\newblock The dynamics of reinforcement learning in cooperative multiagent
  systems.
\newblock \emph{AAAI Conference on Artificial Intelligence}, 1998\penalty0
  (746-752):\penalty0 2, 1998.

\bibitem[Cohen et~al.(2017)Cohen, H{\'e}liou, and
  Mertikopoulos]{cohen2017learning}
Johanne Cohen, Am{\'e}lie H{\'e}liou, and Panayotis Mertikopoulos.
\newblock Learning with bandit feedback in potential games.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pp.\  6372--6381, 2017.

\bibitem[Cutkosky \& Orabona(2019)Cutkosky and Orabona]{cutkosky2019momentum}
Ashok Cutkosky and Francesco Orabona.
\newblock Momentum-based variance reduction in non-convex {SGD}.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 15236--15245, 2019.

\bibitem[Daskalakis et~al.(2009)Daskalakis, Goldberg, and
  Papadimitriou]{daskalakis2009complexity}
Constantinos Daskalakis, Paul~W Goldberg, and Christos~H Papadimitriou.
\newblock The complexity of computing a {N}ash equilibrium.
\newblock \emph{SIAM Journal on Computing}, 39\penalty0 (1):\penalty0 195--259,
  2009.

\bibitem[Daskalakis et~al.(2020)Daskalakis, Foster, and
  Golowich]{daskalakis2020independent}
Constantinos Daskalakis, Dylan~J Foster, and Noah Golowich.
\newblock Independent policy gradient methods for competitive reinforcement
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Dubey \& Pentland(2021)Dubey and Pentland]{dubey2021provably}
Abhimanyu Dubey and Alex Pentland.
\newblock Provably efficient cooperative multi-agent reinforcement learning
  with function approximation.
\newblock \emph{arXiv preprint arXiv:2103.04972}, 2021.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018spider}
Cong Fang, Chris~Junchi Li, Zhouchen Lin, and Tong Zhang.
\newblock {SPIDER}: near-optimal non-convex optimization via stochastic path
  integrated differential estimator.
\newblock In \emph{International Conference on Neural Information Processing
  Systems}, pp.\  687--697, 2018.

\bibitem[Foerster et~al.(2016)Foerster, Assael, de~Freitas, and
  Whiteson]{foerster2016learning}
Jakob~N Foerster, Yannis~M Assael, Nando de~Freitas, and Shimon Whiteson.
\newblock Learning to communicate with deep multi-agent reinforcement learning.
\newblock In \emph{International Conference on Neural Information Processing
  Systems}, pp.\  2145--2153, 2016.

\bibitem[Foster et~al.(2016)Foster, Li, Lykouris, Sridharan, and
  Tardos]{foster2016learning}
Dylan~J Foster, Zhiyuan Li, Thodoris Lykouris, Karthik Sridharan, and {\'E}va
  Tardos.
\newblock Learning in games: {R}obustness of fast convergence.
\newblock In \emph{International Conference on Neural Information Processing
  Systems}, pp.\  4734--4742, 2016.

\bibitem[Fox et~al.(2021)Fox, McAleer, Overman, and
  Panageas]{fox2021independent}
Roy Fox, Stephen McAleer, Will Overman, and Ioannis Panageas.
\newblock Independent natural policy gradient always converges in {M}arkov
  potential games.
\newblock \emph{arXiv preprint arXiv:2110.10614}, 2021.

\bibitem[Freund \& Schapire(1999)Freund and Schapire]{freund1999adaptive}
Yoav Freund and Robert~E Schapire.
\newblock Adaptive game playing using multiplicative weights.
\newblock \emph{Games and Economic Behavior}, 29\penalty0 (1-2):\penalty0
  79--103, 1999.

\bibitem[Fudenberg et~al.(1998)Fudenberg, Drew, Levine, and
  Levine]{fudenberg1998theory}
Drew Fudenberg, Fudenberg Drew, David~K Levine, and David~K Levine.
\newblock \emph{The Theory of Learning in Games}, volume~2.
\newblock MIT press, 1998.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Guo et~al.(2021)Guo, Fu, Yang, and Wang]{guo2021decentralized}
Hongyi Guo, Zuyue Fu, Zhuoran Yang, and Zhaoran Wang.
\newblock Decentralized single-timescale actor-critic on zero-sum two-player
  stochastic games.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3899--3909. PMLR, 2021.

\bibitem[Hansen et~al.(2013)Hansen, Miltersen, and Zwick]{hansen2013strategy}
Thomas~Dueholm Hansen, Peter~Bro Miltersen, and Uri Zwick.
\newblock Strategy iteration is strongly polynomial for 2-player turn-based
  stochastic games with a constant discount factor.
\newblock \emph{Journal of the ACM}, 60\penalty0 (1):\penalty0 1--16, 2013.

\bibitem[Hart \& Mas-Colell(2000)Hart and Mas-Colell]{hart2000simple}
Sergiu Hart and Andreu Mas-Colell.
\newblock A simple adaptive procedure leading to correlated equilibrium.
\newblock \emph{Econometrica}, 68\penalty0 (5):\penalty0 1127--1150, 2000.

\bibitem[Hart \& Mas-Colell(2003)Hart and Mas-Colell]{hart2003uncoupled}
Sergiu Hart and Andreu Mas-Colell.
\newblock Uncoupled dynamics do not lead to {N}ash equilibrium.
\newblock \emph{American Economic Review}, 93\penalty0 (5):\penalty0
  1830--1836, 2003.

\bibitem[Ho(1980)]{ho1980team}
Yu-Chi Ho.
\newblock Team decision theory and information structures.
\newblock \emph{Proceedings of the IEEE}, 68\penalty0 (6):\penalty0 644--654,
  1980.

\bibitem[Hu \& Wellman(2003)Hu and Wellman]{hu2003nash}
Junling Hu and Michael~P Wellman.
\newblock Nash {Q}-learning for general-sum stochastic games.
\newblock \emph{Journal of Machine Learning Research}, 4\penalty0
  (Nov):\penalty0 1039--1069, 2003.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (4), 2010.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is {Q}-learning provably efficient?
\newblock In \emph{International Conference on Neural Information Processing
  Systems}, pp.\  4868--4878, 2018.

\bibitem[Jin et~al.(2021)Jin, Liu, Wang, and Yu]{jin2021v}
Chi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu.
\newblock V-learning--{A} simple, efficient, decentralized algorithm for
  multiagent {RL}.
\newblock \emph{arXiv preprint arXiv:2110.14555}, 2021.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock \emph{Advances in Neural Information Processing Systems},
  26:\penalty0 315--323, 2013.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{kakade2002approximately}
Sham Kakade and John Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{In Proc. 19th International Conference on Machine Learning}.
  Citeseer, 2002.

\bibitem[Kar et~al.(2013)Kar, Moura, and Poor]{kar13}
Soummya Kar, José M.~F. Moura, and H.~Vincent Poor.
\newblock Q{D}-learning: A collaborative distributed strategy for multi-agent
  reinforcement learning through consensus + innovations.
\newblock \emph{IEEE Transactions on Signal Processing}, 61\penalty0
  (7):\penalty0 1848--1862, 2013.

\bibitem[Kleinberg et~al.(2009)Kleinberg, Piliouras, and
  Tardos]{kleinberg2009multiplicative}
Robert Kleinberg, Georgios Piliouras, and {\'E}va Tardos.
\newblock Multiplicative updates outperform generic no-regret learning in
  congestion games.
\newblock In \emph{Proceedings of the Forty-First Annual ACM Symposium on
  Theory of Computing}, pp.\  533--542, 2009.

\bibitem[Kober et~al.(2013)Kober, Bagnell, and Peters]{kober2013reinforcement}
Jens Kober, J~Andrew Bagnell, and Jan Peters.
\newblock Reinforcement learning in robotics: {A} survey.
\newblock \emph{International Journal of Robotics Research}, 32\penalty0
  (11):\penalty0 1238--1274, 2013.

\bibitem[Lai et~al.(2008)Lai, Jiang, and Poor]{lai2008medium}
Lifeng Lai, Hai Jiang, and H~Vincent Poor.
\newblock Medium access in cognitive radio networks: {A} competitive
  multi-armed bandit framework.
\newblock In \emph{Asilomar Conference on Signals, Systems and Computers}, pp.\
   98--102. IEEE, 2008.

\bibitem[Lauer \& Riedmiller(2000)Lauer and Riedmiller]{lauer2000algorithm}
Martin Lauer and Martin Riedmiller.
\newblock An algorithm for distributed reinforcement learning in cooperative
  multi-agent systems.
\newblock In \emph{International Conference on Machine Learning}, 2000.

\bibitem[Leonardos et~al.(2021)Leonardos, Overman, Panageas, and
  Piliouras]{leonardos2021global}
Stefanos Leonardos, Will Overman, Ioannis Panageas, and Georgios Piliouras.
\newblock Global convergence of multi-agent policy gradient in {M}arkov
  potential games.
\newblock \emph{arXiv preprint arXiv:2106.01969}, 2021.

\bibitem[Leslie \& Collins(2005)Leslie and Collins]{leslie2005individual}
David~S Leslie and Edmund~J Collins.
\newblock Individual {Q}-learning in normal form games.
\newblock \emph{SIAM Journal on Control and Optimization}, 44\penalty0
  (2):\penalty0 495--514, 2005.

\bibitem[Li \& Orabona(2020)Li and Orabona]{li2020high}
Xiaoyu Li and Francesco Orabona.
\newblock A high probability analysis of adaptive {SGD} with momentum.
\newblock \emph{arXiv preprint arXiv:2007.14294}, 2020.

\bibitem[Littman(1994)]{littman1994markov}
Michael~L Littman.
\newblock Markov games as a framework for multi-agent reinforcement learning.
\newblock In \emph{Machine Learning}, pp.\  157--163. 1994.

\bibitem[Littman(2001)]{littman2001friend}
Michael~L Littman.
\newblock Friend-or-{F}oe {Q}-learning in general-sum games.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  322--328, 2001.

\bibitem[Liu et~al.(2021)Liu, Yu, Bai, and Jin]{liu2020sharp}
Qinghua Liu, Tiancheng Yu, Yu~Bai, and Chi Jin.
\newblock A sharp analysis of model-based reinforcement learning with
  self-play.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Lowe et~al.(2017)Lowe, Wu, Tamar, Harb, Abbeel, and
  Mordatch]{lowe2017multi}
Ryan Lowe, Yi~Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments.
\newblock \emph{Advances in Neural Information Processing Systems},
  30:\penalty0 6379--6390, 2017.

\bibitem[Macua et~al.(2018)Macua, Zazo, and Zazo]{macua2018learning}
Sergio~Valcarcel Macua, Javier Zazo, and Santiago Zazo.
\newblock Learning parametric closed-loop policies for {M}arkov potential
  games.
\newblock \emph{arXiv preprint arXiv:1802.00899}, 2018.

\bibitem[Mao \& Ba{\c{s}}ar(2022)Mao and Ba{\c{s}}ar]{mao2022provably}
Weichao Mao and Tamer Ba{\c{s}}ar.
\newblock Provably efficient reinforcement learning in decentralized
  general-sum {M}arkov games.
\newblock \emph{Dynamic Games and Applications}, pp.\  1--22, 2022.

\bibitem[Mao et~al.(2020{\natexlab{a}})Mao, Zhang, Miehling, and
  Ba{\c{s}}ar]{mao2020information}
Weichao Mao, Kaiqing Zhang, Erik Miehling, and Tamer Ba{\c{s}}ar.
\newblock Information state embedding in partially observable cooperative
  multi-agent reinforcement learning.
\newblock In \emph{IEEE Conference on Decision and Control}, pp.\  6124--6131.
  IEEE, 2020{\natexlab{a}}.

\bibitem[Mao et~al.(2020{\natexlab{b}})Mao, Zhang, Zhu, Simchi-Levi, and
  Ba{\c{s}}ar]{mao2020near}
Weichao Mao, Kaiqing Zhang, Ruihao Zhu, David Simchi-Levi, and Tamer
  Ba{\c{s}}ar.
\newblock Near-optimal regret bounds for model-free {RL} in non-stationary
  episodic {MDP}s.
\newblock In \emph{International Conference on Machine Learning},
  2020{\natexlab{b}}.

\bibitem[Marden et~al.(2009{\natexlab{a}})Marden, Arslan, and
  Shamma]{marden2009cooperative}
Jason~R Marden, G{\"u}rdal Arslan, and Jeff~S Shamma.
\newblock Cooperative control and potential games.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics, Part B
  (Cybernetics)}, 39\penalty0 (6):\penalty0 1393--1407, 2009{\natexlab{a}}.

\bibitem[Marden et~al.(2009{\natexlab{b}})Marden, Young, Arslan, and
  Shamma]{marden2009payoff}
Jason~R Marden, H~Peyton Young, G{\"u}rdal Arslan, and Jeff~S Shamma.
\newblock Payoff-based dynamics for multiplayer weakly acyclic games.
\newblock \emph{SIAM Journal on Control and Optimization}, 48\penalty0
  (1):\penalty0 373--396, 2009{\natexlab{b}}.

\bibitem[Menard et~al.(2021)Menard, Domingues, Shang, and Valko]{menard2021ucb}
Pierre Menard, Omar~Darwiche Domingues, Xuedong Shang, and Michal Valko.
\newblock {UCB} momentum {Q}-learning: {C}orrecting the bias without
  forgetting.
\newblock \emph{arXiv preprint arXiv:2103.01312}, 2021.

\bibitem[Mguni et~al.(2021)Mguni, Wu, Du, Yang, Wang, Li, Wen, Jennings, and
  Wang]{mguni2021learning}
David Mguni, Yutong Wu, Yali Du, Yaodong Yang, Ziyi Wang, Minne Li, Ying Wen,
  Joel Jennings, and Jun Wang.
\newblock Learning in nonzero-sum stochastic games with potentials.
\newblock \emph{arXiv preprint arXiv:2103.09284}, 2021.

\bibitem[Nayyar et~al.(2013{\natexlab{a}})Nayyar, Gupta, Langbort, and
  Ba{\c{s}}ar]{nayyar2013common}
Ashutosh Nayyar, Abhishek Gupta, Cedric Langbort, and Tamer Ba{\c{s}}ar.
\newblock Common information based {M}arkov perfect equilibria for stochastic
  games with asymmetric information: {F}inite games.
\newblock \emph{IEEE Transactions on Automatic Control}, 59\penalty0
  (3):\penalty0 555--570, 2013{\natexlab{a}}.

\bibitem[Nayyar et~al.(2013{\natexlab{b}})Nayyar, Mahajan, and
  Teneketzis]{nayyar2013decentralized}
Ashutosh Nayyar, Aditya Mahajan, and Demosthenis Teneketzis.
\newblock Decentralized stochastic control with partial history sharing: {A}
  common information approach.
\newblock \emph{IEEE Transactions on Automatic Control}, 58\penalty0
  (7):\penalty0 1644--1658, 2013{\natexlab{b}}.

\bibitem[Neu(2015)]{neu2015explore}
Gergely Neu.
\newblock Explore no more: {I}mproved high-probability regret bounds for
  non-stochastic bandits.
\newblock \emph{Advances in Neural Information Processing Systems},
  28:\penalty0 3168--3176, 2015.

\bibitem[Nisan et~al.(2007)Nisan, Roughgarden, Tardos, and
  Vazirani]{nisan2007algorithmic}
Noam Nisan, Tim Roughgarden, Eva Tardos, and Vijay~V Vazirani.
\newblock \emph{Algorithmic Game Theory}.
\newblock Cambridge University Press, 2007.

\bibitem[Oliehoek et~al.(2008)Oliehoek, Spaan, and
  Vlassis]{oliehoek2008optimal}
Frans~A Oliehoek, Matthijs~TJ Spaan, and Nikos Vlassis.
\newblock Optimal and approximate {Q}-value functions for decentralized
  {POMDP}s.
\newblock \emph{Journal of Artificial Intelligence Research}, 32:\penalty0
  289--353, 2008.

\bibitem[Pham et~al.(2018)Pham, La, Feil-Seifer, and
  Nefian]{pham2018cooperative}
Huy~Xuan Pham, Hung~Manh La, David Feil-Seifer, and Aria Nefian.
\newblock Cooperative and distributed reinforcement learning of drones for
  field coverage.
\newblock \emph{arXiv preprint arXiv:1803.07250}, 2018.

\bibitem[Radanovic et~al.(2019)Radanovic, Devidze, Parkes, and
  Singla]{radanovic2019learning}
Goran Radanovic, Rati Devidze, David Parkes, and Adish Singla.
\newblock Learning to collaborate in {M}arkov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5261--5270, 2019.

\bibitem[Rashid et~al.(2018)Rashid, Samvelyan, Schroeder, Farquhar, Foerster,
  and Whiteson]{rashid2018qmix}
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob
  Foerster, and Shimon Whiteson.
\newblock {QMIX}: {M}onotonic value function factorisation for deep multi-agent
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4295--4304. PMLR, 2018.

\bibitem[Reddi et~al.(2016)Reddi, Hefny, Sra, Poczos, and
  Smola]{reddi2016stochastic}
Sashank~J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  314--323. PMLR, 2016.

\bibitem[Roughgarden(2009)]{roughgarden2009intrinsic}
Tim Roughgarden.
\newblock Intrinsic robustness of the price of anarchy.
\newblock In \emph{ACM Symposium on Theory of Computing}, pp.\  513--522, 2009.

\bibitem[Rubinstein(2016)]{rubinstein2016settling}
Aviad Rubinstein.
\newblock Settling the complexity of computing approximate two-player {N}ash
  equilibria.
\newblock In \emph{2016 IEEE 57th Annual Symposium on Foundations of Computer
  Science (FOCS)}, pp.\  258--265. IEEE, 2016.

\bibitem[Sayin et~al.(2021)Sayin, Zhang, Leslie, Ba\c{s}ar, and
  Ozdaglar]{sayin2021decentralized}
Muhammed~O Sayin, Kaiqing Zhang, David~S Leslie, Tamer Ba\c{s}ar, and Asuman
  Ozdaglar.
\newblock Decentralized {Q}-learning in zero-sum {M}arkov games.
\newblock \emph{arXiv preprint arXiv:2106.02748}, 2021.

\bibitem[Seuken \& Zilberstein(2007)Seuken and Zilberstein]{seuken2007improved}
Sven Seuken and Shlomo Zilberstein.
\newblock Improved memory-bounded dynamic programming for decentralized
  {POMDP}s.
\newblock In \emph{Proceedings of the Twenty-Third Conference on Uncertainty in
  Artificial Intelligence}, pp.\  344--351, 2007.

\bibitem[Shalev-Shwartz et~al.(2016)Shalev-Shwartz, Shammah, and
  Shashua]{shalev2016safe}
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua.
\newblock Safe, multi-agent, reinforcement learning for autonomous driving.
\newblock \emph{arXiv preprint arXiv:1610.03295}, 2016.

\bibitem[Shapley(1953)]{shapley1953stochastic}
Lloyd~S Shapley.
\newblock Stochastic games.
\newblock \emph{Proceedings of the National Academy of Sciences}, 39\penalty0
  (10):\penalty0 1095--1100, 1953.

\bibitem[Sidford et~al.(2020)Sidford, Wang, Yang, and Ye]{sidford2020solving}
Aaron Sidford, Mengdi Wang, Lin Yang, and Yinyu Ye.
\newblock Solving discounted stochastic two-player games with near-optimal time
  and sample complexity.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  2992--3002. PMLR, 2020.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of {G}o with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Son et~al.(2019)Son, Kim, Kang, Hostallero, and Yi]{son2019qtran}
Kyunghwan Son, Daewoo Kim, Wan~Ju Kang, David~Earl Hostallero, and Yung Yi.
\newblock Qtran: {L}earning to factorize with transformation for cooperative
  multi-agent reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5887--5896. PMLR, 2019.

\bibitem[Song et~al.(2021)Song, Mei, and Bai]{song2021can}
Ziang Song, Song Mei, and Yu~Bai.
\newblock When can we learn general-sum {M}arkov games with a large number of
  players sample-efficiently?
\newblock \emph{arXiv preprint arXiv:2110.04184}, 2021.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{sutton2000policy}
Richard~S Sutton, David~A McAllester, Satinder~P Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1057--1063, 2000.

\bibitem[Syrgkanis \& Tardos(2013)Syrgkanis and
  Tardos]{syrgkanis2013composable}
Vasilis Syrgkanis and Eva Tardos.
\newblock Composable and efficient mechanisms.
\newblock In \emph{ACM Symposium on Theory of Computing}, pp.\  211--220, 2013.

\bibitem[Syrgkanis et~al.(2015)Syrgkanis, Agarwal, Luo, and
  Schapire]{syrgkanis2015fast}
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert~E Schapire.
\newblock Fast convergence of regularized learning in games.
\newblock In \emph{International Conference on Neural Information Processing
  Systems}, pp.\  2989--2997, 2015.

\bibitem[Tian et~al.(2021)Tian, Wang, Yu, and Sra]{tian2020provably}
Yi~Tian, Yuanhao Wang, Tiancheng Yu, and Suvrit Sra.
\newblock Online learning in unknown {M}arkov games.
\newblock \emph{International Conference on Machine Learning}, 2021.

\bibitem[Verbeeck et~al.(2002)Verbeeck, Now{\'e}, Lenaerts, and
  Parent]{verbeeck2002learning}
Katja Verbeeck, Ann Now{\'e}, Tom Lenaerts, and Johan Parent.
\newblock Learning to reach the {P}areto optimal {N}ash equilibrium as a team.
\newblock In \emph{Australian Joint Conference on Artificial Intelligence},
  pp.\  407--418. Springer, 2002.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Oriol Vinyals, Igor Babuschkin, Wojciech~M Czarnecki, Micha{\"e}l Mathieu,
  Andrew Dudzik, Junyoung Chung, David~H Choi, Richard Powell, Timo Ewalds,
  Petko Georgiev, et~al.
\newblock Grandmaster level in {S}tar{C}raft {II} using multi-agent
  reinforcement learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Viossat \& Zapechelnyuk(2013)Viossat and Zapechelnyuk]{viossat2013no}
Yannick Viossat and Andriy Zapechelnyuk.
\newblock No-regret dynamics and fictitious play.
\newblock \emph{Journal of Economic Theory}, 148\penalty0 (2):\penalty0
  825--842, 2013.

\bibitem[Wang \& Sandholm(2002)Wang and Sandholm]{wang2002reinforcement}
Xiaofeng Wang and Tuomas Sandholm.
\newblock Reinforcement learning to play an optimal {N}ash equilibrium in team
  {M}arkov games.
\newblock \emph{Advances in Neural Information Processing Systems},
  15:\penalty0 1603--1610, 2002.

\bibitem[Wei et~al.(2017)Wei, Hong, and Lu]{wei2017online}
Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu.
\newblock Online reinforcement learning in stochastic games.
\newblock In \emph{International Conference on Neural Information Processing
  Systems}, pp.\  4994--5004, 2017.

\bibitem[Wei et~al.(2021)Wei, Lee, Zhang, and Luo]{wei2021last}
Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo.
\newblock Last-iterate convergence of decentralized optimistic gradient
  descent/ascent in infinite-horizon competitive {M}arkov games.
\newblock \emph{Annual Conference on Learning Theory}, 2021.

\bibitem[Williams(1992)]{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3):\penalty0 229--256, 1992.

\bibitem[Xie et~al.(2020)Xie, Chen, Wang, and Yang]{xie2020learning}
Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang.
\newblock Learning zero-sum simultaneous-move {M}arkov games using function
  approximation and correlated equilibrium.
\newblock In \emph{Conference on Learning Theory}, pp.\  3674--3682, 2020.

\bibitem[Yongacoglu et~al.(2019)Yongacoglu, Arslan, and
  Y{\"u}ksel]{yongacoglu2019learning}
Bora Yongacoglu, G{\"u}rdal Arslan, and Serdar Y{\"u}ksel.
\newblock Learning team-optimality for decentralized stochastic control and
  dynamic games.
\newblock \emph{arXiv preprint arXiv:1903.05812}, 2019.

\bibitem[Zhang et~al.(2018)Zhang, Yang, Liu, Zhang, and
  Ba\c{s}ar]{zhang2018fully}
Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Ba\c{s}ar.
\newblock Fully decentralized multi-agent reinforcement learning with networked
  agents.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5872--5881, 2018.

\bibitem[Zhang et~al.(2019)Zhang, Miehling, and Ba{\c{s}}ar]{zhang2019online}
Kaiqing Zhang, Erik Miehling, and Tamer Ba{\c{s}}ar.
\newblock Online planning for decentralized stochastic control with partial
  history sharing.
\newblock In \emph{American Control Conference}, pp.\  3544--3550. IEEE, 2019.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Kakade, Ba\c{s}ar, and
  Yang]{zhang2020model}
Kaiqing Zhang, Sham Kakade, Tamer Ba\c{s}ar, and Lin Yang.
\newblock Model-based multi-agent {RL} in zero-sum {M}arkov games with
  near-optimal sample complexity.
\newblock \emph{Advances in Neural Information Processing Systems}, 33,
  2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2021)Zhang, Ren, and Li]{zhang2021gradient}
Runyu Zhang, Zhaolin Ren, and Na~Li.
\newblock Gradient play in multi-agent {M}arkov stochastic games: {S}tationary
  points and convergence.
\newblock \emph{arXiv preprint arXiv:2106.00198}, 2021.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Zhou, and Ji]{zhang2020almost}
Zihan Zhang, Yuan Zhou, and Xiangyang Ji.
\newblock Almost optimal model-free reinforcement learning via
  reference-advantage decomposition.
\newblock \emph{Advances in Neural Information Processing Systems}, 33,
  2020{\natexlab{b}}.

\bibitem[Zhao et~al.(2021)Zhao, Tian, Lee, and Du]{zhao2021provably}
Yulai Zhao, Yuandong Tian, Jason~D Lee, and Simon~S Du.
\newblock Provably efficient policy gradient methods for two-player zero-sum
  {M}arkov games.
\newblock \emph{arXiv preprint arXiv:2102.08903}, 2021.

\end{thebibliography}
