\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Hinton, Mnih, Leibo, and Ionescu]{Ba2016}
Ba, J., Hinton, G., Mnih, V., Leibo, J.~Z., and Ionescu, C.
\newblock Using fast weights to attend to the recent past.
\newblock \emph{NeurIPS}, 2016.

\bibitem[Bello et~al.(2021)Bello, Fedus, Du, Cubuk, Srinivas, Lin, Shlens, and
  Zoph]{revisitresnet}
Bello, I., Fedus, W., Du, X., Cubuk, E.~D., Srinivas, A., Lin, T.-Y., Shlens,
  J., and Zoph, B.
\newblock Revisiting {R}es{N}ets: {I}mproved training and scaling strategies.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Chen et~al.(2019)Chen, Chen, Shi, Hsieh, Liao, and Zhang]{BNdrop2}
Chen, G., Chen, P., Shi, Y., Hsieh, C., Liao, B., and Zhang, S.
\newblock Rethinking the usage of batch normalization and dropout in the
  training of deep neural networks.
\newblock \emph{arXiv:1905.05928}, 2019.

\bibitem[Constantinescu \& Sandu(2013)Constantinescu and
  Sandu]{Constantinescu2013}
Constantinescu, E. and Sandu, A.
\newblock Extrapolated multirate methods for differential equations with
  multiple time scales.
\newblock \emph{Journal of Scientific Computing}, 56\penalty0 (1):\penalty0
  28--44, 2013.

\bibitem[Dai \& Le(2015)Dai and Le]{DaiLe2015}
Dai, A. and Le, Q.
\newblock Semi-supervised sequence learning.
\newblock \emph{NIPS}, 2015.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{BERT}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: {P}re-training of deep bidirectional transformers for
  language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Engstler \& Lubich(1997)Engstler and Lubich]{Engstler1997}
Engstler, C. and Lubich, C.
\newblock Multirate extrapolation methods for differential equations with
  different time scales.
\newblock \emph{Computing}, 58:\penalty0 173--185, 1997.

\bibitem[Feldman(1982)]{feldmanfastweights}
Feldman, J.~A.
\newblock Dynamic connections in neural networks.
\newblock \emph{Biological Cybernetics}, 46\penalty0 (1):\penalty0 27--39,
  1982.

\bibitem[Gear \& Wells(1984)Gear and Wells]{GearWells1984}
Gear, C. and Wells, D.
\newblock Multirate linear multistep methods.
\newblock \emph{BIT}, 24:\penalty0 484--502, 1984.

\bibitem[Gear(1974)]{Gear1974}
Gear, C.~W.
\newblock Multirate methods for ordinary differential equations.
\newblock \emph{Technical Report, University of Illinois}, 1974.

\bibitem[G{\"u}nther \& Rentrop(1993)G{\"u}nther and Rentrop]{Gunther1993}
G{\"u}nther, M. and Rentrop, P.
\newblock Multirate row methods and latency of electric circuits.
\newblock \emph{Appl. Numer. Math.}, 13:\penalty0 83--102, 1993.

\bibitem[Hao et~al.(2019)Hao, Dong, Wei, and Xu]{Hao2019}
Hao, Y., Dong, L., Wei, F., and Xu, K.
\newblock Visualizing and understanding the effectiveness of {BERT}.
\newblock \emph{EMNLP}, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[He et~al.(2019)He, Girshick, and Dollár]{He2019}
He, K., Girshick, R., and Dollár, P.
\newblock Rethinking {I}mage{N}et pre-training.
\newblock \emph{ICCV}, 2019.

\bibitem[Hinton \& Plaut(1987)Hinton and Plaut]{HintonPlautfastweights}
Hinton, G.~E. and Plaut, D.~C.
\newblock Using fast weights to deblur old memories.
\newblock \emph{In Proceedings of the ninth annual conference of the Cognitive
  Science Society}, pp.\  177--186, 1987.

\bibitem[Howard \& Ruder(2018)Howard and Ruder]{HowardRuder2018}
Howard, J. and Ruder, S.
\newblock Universal language model fine-tuning for text classification.
\newblock \emph{ACL}, 2018.

\bibitem[Huh et~al.(2016)Huh, Agrawal, and Efros]{Huh2016}
Huh, M., Agrawal, P., and Efros, A.~A.
\newblock What makes {I}mage{N}et good for transfer learning?
\newblock \emph{arXiv:1608.08614}, 2016.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{batchnorm}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{ICML}, 2015.

\bibitem[Justus et~al.(2018)Justus, Brennan, Bonner, and McGough]{justus2018}
Justus, D., Brennan, J., Bonner, S., and McGough, A.
\newblock Predicting the computational cost of deep learning models.
\newblock \emph{CoRR}, 2018.

\bibitem[Kristiadi et~al.(2020)Kristiadi, Hein, and Hennig]{kristiadi2020}
Kristiadi, A., Hein, M., and Hennig, P.
\newblock Being {B}ayesian, even just a bit, fixes overconfidence in {R}e{LU}
  networks.
\newblock \emph{ICML}, 2020.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and Hinton]{cifar10}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Leimkuhler \& Reich(2001)Leimkuhler and Reich]{Leimkuhler2001}
Leimkuhler, B. and Reich, S.
\newblock A reversible averaging integrator for multiple time- scale dynamics.
\newblock \emph{Journal of Computational Physics}, 171\penalty0 (1):\penalty0
  95--114, 2001.

\bibitem[Leimkuhler et~al.(2019)Leimkuhler, Matthews, and Vlaar]{LMV}
Leimkuhler, B., Matthews, C., and Vlaar, T.
\newblock {P}artitioned integrators for thermodynamic parameterization of
  neural networks.
\newblock \emph{Foundations of Data Science}, 1\penalty0 (4):\penalty0
  457--489, 2019.

\bibitem[Li et~al.(2020)Li, Chaudhari, Yang, Lam, Ravichandran, Bhotika, and
  Soatto]{Li2020}
Li, H., Chaudhari, P., Yang, H., Lam, M., Ravichandran, A., Bhotika, R., and
  Soatto, S.
\newblock Rethinking the hyperparameters for fine-tuning.
\newblock \emph{ICLR}, 2020.

\bibitem[Li et~al.(2018)Li, Chen, Hu, and Yang]{BNdrop1}
Li, X., Chen, S., Hu, X., and Yang, J.
\newblock Understanding the disharmony between dropout and batch normalization
  by variance shift.
\newblock \emph{IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2018.

\bibitem[Li et~al.(2019)Li, Wei, and Ma]{Li2019}
Li, Y., Wei, C., and Ma, T.
\newblock Towards explaining the regularization effect of initial large
  learning rate in training neural networks.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L., and Stoyanov, V.
\newblock Ro{BERT}a: {A} robustly optimized {BERT} pretraining approach.
\newblock \emph{arXiv:1907.11692}, 2019.

\bibitem[Luo et~al.(2019)Luo, Wang, Shao, and Pen]{understandingBN}
Luo, P., Wang, X., Shao, W., and Pen, Z.
\newblock Towards understanding regularization in batch normalization.
\newblock \emph{ICLR}, 2019.

\bibitem[Marcus et~al.(1993)Marcus, Santorini, and Marcinkiewicz]{PennTreebank}
Marcus, M.~P., Santorini, B., and Marcinkiewicz, M.~A.
\newblock Building a large annotated corpus of {E}nglish: The {P}enn
  {T}reebank.
\newblock \emph{Computational Linguistics}, 19\penalty0 (2):\penalty0 313--330,
  1993.

\bibitem[Mattingly et~al.(2002)Mattingly, A.M.Stuart, and Higham]{MaStHi2002}
Mattingly, J., A.M.Stuart, and Higham, D.
\newblock Ergodicity for {SDEs} and approximations: locally {L}ipschitz vector
  fields and degenerate noise.
\newblock \emph{Stochastic Processes and their Applications}, 101\penalty0
  (2):\penalty0 185--232, 2002.

\bibitem[Murfet et~al.(2020)Murfet, Wei, Gong, Li, Gell-Redman, and
  Quella]{murfet2020}
Murfet, D., Wei, S., Gong, M., Li, H., Gell-Redman, J., and Quella, T.
\newblock Deep learning is singular, and that’s good.
\newblock \emph{arXiv:2010.11560}, 2020.

\bibitem[Neyshabur et~al.(2020)Neyshabur, Sedghi, and Zhang]{Neyshabur2020}
Neyshabur, B., Sedghi, H., and Zhang, C.
\newblock What is being transferred in transfer learning?
\newblock \emph{NeurIPS}, 2020.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{Pytorch}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in {P}y{T}orch.
\newblock 2017.

\bibitem[Polyak(1964)]{GDwithmom}
Polyak, B.~T.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  4\penalty0 (5):\penalty0 1--17, 1964.

\bibitem[Qi et~al.(2017)Qi, Sparks, and Talwalkar]{paleo}
Qi, H., Sparks, E.~R., and Talwalkar, A.
\newblock {PALEO}: A performance model for deep neural networks.
\newblock \emph{ICLR}, 2017.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{Radford2018}
Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I.
\newblock Improving language understanding by generative pre-training.
\newblock \emph{OpenAI}, 2018.

\bibitem[Raghu et~al.(2019)Raghu, Zhang, Kleinberg, and Bengio]{Raghu2019}
Raghu, M., Zhang, C., Kleinberg, J., and Bengio, S.
\newblock Transfusion: Understanding transfer learning for medical imaging.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Rice(1960)]{Rice1960}
Rice, J.~R.
\newblock Split runge-kutta methods for simultaneous equations.
\newblock \emph{Journal of Research of the National Institute of Standards and
  Technology}, 60, 1960.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{SST}
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C., Ng, A., and Potts,
  C.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock \emph{Conference on Empirical Methods in Natural Language
  Processing}, 2013.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{JMLR}, 15:\penalty0 1929--1958, 2014.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and Hinton]{sutskever}
Sutskever, I., Martens, J., Dahl, G., and Hinton, G.
\newblock On the importance of initialization and momentum in deep learning.
\newblock \emph{ICML}, 2013.

\bibitem[Tuckerman et~al.(1991)Tuckerman, Berne, and Martyna]{Tuckerman1991}
Tuckerman, M.~E., Berne, B.~J., and Martyna, G.~J.
\newblock Molecular dynamics algorithm for multiple time scales: Systems with
  long range forces.
\newblock \emph{The Journal of Chemical Physics}, 94\penalty0 (10):\penalty0
  6811--6815, 1991.

\bibitem[Tuckerman et~al.(1992)Tuckerman, Berne, and Martyna]{Tuckerman1992}
Tuckerman, M.~E., Berne, B.~J., and Martyna, G.~J.
\newblock Reversible multiple time scale molecular dynamics.
\newblock \emph{The Journal of Chemical Physics}, 97\penalty0 (3):\penalty0
  1990--2001, 1992.

\bibitem[Vlaar \& Frankle(2022)Vlaar and Frankle]{VF}
Vlaar, T. and Frankle, J.
\newblock What can linear interpolation of neural network loss landscapes tell
  us?
\newblock \emph{ICML}, 2022.

\bibitem[Wan et~al.(2013)Wan, Zeiler, Zhang, Cun, and Fergus]{dropconnect}
Wan, L., Zeiler, M., Zhang, S., Cun, Y.~L., and Fergus, R.
\newblock Regularization of neural networks using {D}rop{C}onnect.
\newblock \emph{ICML}, 2013.

\bibitem[Wang et~al.(2019)Wang, Singh, Michael, Hill, Levy, and Bowman]{glue}
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.~R.
\newblock {GLUE}: {A} multi-task benchmark and analysis platform for natural
  language understanding.
\newblock \emph{ICLR}, 2019.

\bibitem[Welling \& Teh(2011)Welling and Teh]{WeT11}
Welling, M. and Teh, Y.~W.
\newblock {B}ayesian learning via stochastic gradient {L}angevin dynamics.
\newblock \emph{ICML}, pp.\  681--688, 2011.

\bibitem[Wenzel et~al.(2020)Wenzel, Roth, Veeling, Swiatkowski, Tran, Mandt,
  Snoek, Salimans, Jenatton, and Nowozin]{coldposterior}
Wenzel, F., Roth, K., Veeling, B.~S., Swiatkowski, J., Tran, L., Mandt, S.,
  Snoek, J., Salimans, T., Jenatton, R., and Nowozin, S.
\newblock How good is the {B}ayes posterior in deep neural networks really?
\newblock \emph{arXiv:2002.02405}, 2020.

\bibitem[Yosinski et~al.(2014)Yosinski, Clune, Bengio, and
  Lipson]{Yosinski2014}
Yosinski, J., Clune, J., Bengio, Y., and Lipson, H.
\newblock How transferable are features in deep neural networks?
\newblock \emph{NIPS}, 2014.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{LARS}
You, Y., Gitman, I., and Ginsburg, B.
\newblock Large batch training of convolutional networks.
\newblock \emph{arXiv preprint arXiv:1708.03888}, 2017.

\bibitem[You et~al.(2020)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song,
  Demmel, Keutzer, and Hsieh]{LAMB}
You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X.,
  Demmel, J., Keutzer, K., and Hsieh, C.
\newblock Large batch optimization for deep learning: training {BERT} in 76
  minutes.
\newblock \emph{ICLR}, 2020.

\bibitem[Zhang et~al.(2019)Zhang, Bengio, and Singer]{Zhang2019}
Zhang, C., Bengio, S., and Singer, Y.
\newblock Are all layers created equal?
\newblock \emph{arXiv:1902.01996}, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Karimireddy, Veit, Kim, Reddi, Kumar, and
  Sra]{AdamvsSGD}
Zhang, J., Karimireddy, S., Veit, A., Kim, S., Reddi, S., Kumar, S., and Sra,
  S.
\newblock Why are adaptive methods good for attention models?
\newblock \emph{NeurIPS}, 2020.

\end{thebibliography}
