\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aronszajn(1950)]{aronszajn1950theory}
Aronszajn, N.
\newblock Theory of reproducing kernels.
\newblock \emph{Transactions of the American mathematical society}, 68\penalty0
  (3):\penalty0 337--404, 1950.

\bibitem[Arora et~al.(2020)Arora, Du, Li, Salakhutdinov, Wang, and
  Yu]{arora2020harnessing}
Arora, S., Du, S.~S., Li, Z., Salakhutdinov, R., Wang, R., and Yu, D.
\newblock Harnessing the power of infinitely wide deep nets on small-data
  tasks.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkl8sJBYvH}.

\bibitem[Baldi et~al.(2014)Baldi, Sadowski, and Whiteson]{HIGGS}
Baldi, P., Sadowski, P., and Whiteson, D.
\newblock Searching for exotic particles in high-energy physics with deep
  learning.
\newblock \emph{Nature communications}, 5\penalty0 (1):\penalty0 1--9, 2014.

\bibitem[Camoriano et~al.(2016)Camoriano, Angles, Rudi, and
  Rosasco]{camoriano2016nytro}
Camoriano, R., Angles, T., Rudi, A., and Rosasco, L.
\newblock Nytro: When subsampling meets early stopping.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1403--1411.
  PMLR, 2016.

\bibitem[Chen et~al.(2010)Chen, Welling, and Smola]{chen2012super}
Chen, Y., Welling, M., and Smola, A.
\newblock Super-samples from kernel herding.
\newblock \emph{UAI'10: Proceedings of the Twenty-Sixth Conference on
  Uncertainty in Artificial Intelligence, Pages 109â€“116}, 2010.

\bibitem[Chen et~al.(2022)Chen, Epperly, Tropp, and Webber]{chen2022randomly}
Chen, Y., Epperly, E.~N., Tropp, J.~A., and Webber, R.~J.
\newblock Randomly pivoted cholesky: Practical approximation of a kernel matrix
  with few entry evaluations.
\newblock \emph{arXiv preprint arXiv:2207.06503}, 2022.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Gardner et~al.(2018)Gardner, Pleiss, Wu, Weinberger, and
  Wilson]{gardner2018product}
Gardner, J., Pleiss, G., Wu, R., Weinberger, K., and Wilson, A.
\newblock Product kernel interpolation for scalable gaussian processes.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1407--1416. PMLR, 2018.

\bibitem[Geifman et~al.(2020)Geifman, Yadav, Kasten, Galun, Jacobs, and
  Ronen]{HGammaKernel}
Geifman, A., Yadav, A., Kasten, Y., Galun, M., Jacobs, D., and Ronen, B.
\newblock On the similarity between the laplace and neural tangent kernels.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, de~las Casas, Hendricks, Welbl, Clark, Hennigan, Noland,
  Millican, van~den Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Vinyals,
  Rae, and Sifre]{hoffmann2022training}
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford,
  E., de~las Casas, D., Hendricks, L.~A., Welbl, J., Clark, A., Hennigan, T.,
  Noland, E., Millican, K., van~den Driessche, G., Damoc, B., Guy, A.,
  Osindero, S., Simonyan, K., Elsen, E., Vinyals, O., Rae, J.~W., and Sifre, L.
\newblock An empirical analysis of compute-optimal large language model
  training.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=iBBcRUlOAPR}.

\bibitem[Hui \& Belkin(2021)Hui and Belkin]{hui2020evaluation}
Hui, L. and Belkin, M.
\newblock Evaluation of neural architectures trained with square loss vs
  cross-entropy in classification tasks.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=hsFN92eQEla}.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Jurafsky(2000)]{jurafsky2000speech}
Jurafsky, D.
\newblock \emph{Speech \& language processing}.
\newblock Pearson Education India, 2000.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R.,
  Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kimeldorf \& Wahba(1970)Kimeldorf and
  Wahba]{kimeldorf1970correspondence}
Kimeldorf, G.~S. and Wahba, G.
\newblock A correspondence between bayesian estimation on stochastic processes
  and smoothing by splines.
\newblock \emph{The Annals of Mathematical Statistics}, 41\penalty0
  (2):\penalty0 495--502, 1970.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Citeseer}, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{alexnet2012}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In Pereira, F., Burges, C., Bottou, L., and Weinberger, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, volume~25. Curran
  Associates, Inc., 2012.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf}.

\bibitem[LeCun(1998)]{lecun1998mnist}
LeCun, Y.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[Lee et~al.(2020)Lee, Schoenholz, Pennington, Adlam, Xiao, Novak, and
  Sohl-Dickstein]{lee2020finite}
Lee, J., Schoenholz, S., Pennington, J., Adlam, B., Xiao, L., Novak, R., and
  Sohl-Dickstein, J.
\newblock Finite versus infinite neural networks: an empirical study.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15156--15172, 2020.

\bibitem[Li et~al.(2017)Li, Wang, Li, Agustsson, and Van~Gool]{li2017webvision}
Li, W., Wang, L., Li, W., Agustsson, E., and Van~Gool, L.
\newblock Webvision database: Visual learning and understanding from web data.
\newblock \emph{arXiv preprint arXiv:1708.02862}, 2017.

\bibitem[Li et~al.(2019)Li, Wang, Yu, Du, Hu, Salakhutdinov, and
  Arora]{li2019enhanced}
Li, Z., Wang, R., Yu, D., Du, S.~S., Hu, W., Salakhutdinov, R., and Arora, S.
\newblock Enhanced convolutional neural tangent kernels.
\newblock \emph{arXiv preprint arXiv:1911.00809}, 2019.

\bibitem[Loosli et~al.(2007)Loosli, Canu, and Bottou]{loosli2007training}
Loosli, G., Canu, S., and Bottou, L.
\newblock Training invariant support vector machines using selective sampling.
\newblock \emph{Large scale kernel machines}, 2, 2007.

\bibitem[Ma \& Belkin(2017)Ma and Belkin]{ma2017diving}
Ma, S. and Belkin, M.
\newblock Diving into the shallows: a computational perspective on large-scale
  shallow learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Ma \& Belkin(2019)Ma and Belkin]{ma2019kernel}
Ma, S. and Belkin, M.
\newblock Kernel machines that adapt to gpus for effective large batch
  training.
\newblock \emph{Proceedings of Machine Learning and Systems}, 1:\penalty0
  360--373, 2019.

\bibitem[Ma et~al.(2018)Ma, Bassily, and Belkin]{ma2018power}
Ma, S., Bassily, R., and Belkin, M.
\newblock The power of interpolation: Understanding the effectiveness of sgd in
  modern over-parametrized learning.
\newblock \emph{International Conference on Machine Learning}, pp.\
  3325--3334, 2018.

\bibitem[Matthews et~al.(2017)Matthews, {van der Wilk}, Nickson, Fujii,
  {Boukouvalas}, {Le{\'o}n-Villagr{\'a}}, Ghahramani, and Hensman]{GPflow2017}
Matthews, A. G. d.~G., {van der Wilk}, M., Nickson, T., Fujii, K.,
  {Boukouvalas}, A., {Le{\'o}n-Villagr{\'a}}, P., Ghahramani, Z., and Hensman,
  J.
\newblock { {GP}flow: A {G}aussian process library using {T}ensor{F}low}.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (40):\penalty0 1--6, apr 2017.
\newblock URL \url{http://jmlr.org/papers/v18/16-537.html}.

\bibitem[Meanti et~al.(2020)Meanti, Carratino, Rosasco, and
  Rudi]{meanti2020kernel}
Meanti, G., Carratino, L., Rosasco, L., and Rudi, A.
\newblock Kernel methods through the roof: handling billions of points
  efficiently.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 14410--14422, 2020.

\bibitem[Nakkiran et~al.(2021)Nakkiran, Neyshabur, and
  Sedghi]{nakkiran2020deep}
Nakkiran, P., Neyshabur, B., and Sedghi, H.
\newblock The deep bootstrap framework: Good online learners are good offline
  generalizers.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Panayotov et~al.(2015)Panayotov, Chen, Povey, and
  Khudanpur]{panayotov2015librispeech}
Panayotov, V., Chen, G., Povey, D., and Khudanpur, S.
\newblock Librispeech: an asr corpus based on public domain audio books.
\newblock In \emph{2015 IEEE international conference on acoustics, speech and
  signal processing (ICASSP)}, pp.\  5206--5210. IEEE, 2015.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,
  Cournapeau, Brucher, Perrot, and Duchesnay]{scikit-learn}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,
  O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J.,
  Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2825--2830,
  2011.

\bibitem[Poggio \& Girosi(1990)Poggio and Girosi]{poggio1990networks}
Poggio, T. and Girosi, F.
\newblock Networks for approximation and learning.
\newblock \emph{Proceedings of the IEEE}, 78\penalty0 (9):\penalty0 1481--1497,
  1990.

\bibitem[Que \& Belkin(2016)Que and Belkin]{que2016back}
Que, Q. and Belkin, M.
\newblock Back to the future: Radial basis function networks revisited.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  1375--1383.
  PMLR, 2016.

\bibitem[Radhakrishnan et~al.(2022{\natexlab{a}})Radhakrishnan, Beaglehole,
  Pandit, and Belkin]{radhakrishnan2022feature}
Radhakrishnan, A., Beaglehole, D., Pandit, P., and Belkin, M.
\newblock Feature learning in neural networks and kernel machines that
  recursively learn features.
\newblock \emph{arXiv preprint arXiv:2212.13881}, 2022{\natexlab{a}}.

\bibitem[Radhakrishnan et~al.(2022{\natexlab{b}})Radhakrishnan, Stefanakis,
  Belkin, and Uhler]{radhakrishnan2022simple}
Radhakrishnan, A., Stefanakis, G., Belkin, M., and Uhler, C.
\newblock Simple, fast, and flexible framework for matrix completion with
  infinite width neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 119\penalty0
  (16):\penalty0 e2115064119, 2022{\natexlab{b}}.

\bibitem[Rahimi \& Recht(2007)Rahimi and Recht]{rahimi2007random}
Rahimi, A. and Recht, B.
\newblock Random features for large-scale kernel machines.
\newblock \emph{Advances in neural information processing systems}, 20, 2007.

\bibitem[Richardson(1911)]{richardson1911ix}
Richardson, L.~F.
\newblock Ix. the approximate arithmetical solution by finite differences of
  physical problems involving differential equations, with an application to
  the stresses in a masonry dam.
\newblock \emph{Philosophical Transactions of the Royal Society of London.
  Series A, Containing Papers of a Mathematical or Physical Character},
  210\penalty0 (459-470):\penalty0 307--357, 1911.

\bibitem[Rudi et~al.(2017)Rudi, Carratino, and Rosasco]{rudi2017falkon}
Rudi, A., Carratino, L., and Rosasco, L.
\newblock Falkon: An optimal large scale kernel method.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Sch{\"o}lkopf et~al.(2002)Sch{\"o}lkopf, Smola, Bach,
  et~al.]{scholkopf2002learning}
Sch{\"o}lkopf, B., Smola, A.~J., Bach, F., et~al.
\newblock \emph{Learning with kernels: support vector machines, regularization,
  optimization, and beyond}.
\newblock MIT press, 2002.

\bibitem[Shalev-Shwartz et~al.(2007)Shalev-Shwartz, Singer, and
  Srebro]{shalev2007pegasos}
Shalev-Shwartz, S., Singer, Y., and Srebro, N.
\newblock Pegasos: Primal estimated sub-gradient solver for svm.
\newblock In \emph{Proceedings of the 24th international conference on Machine
  learning}, pp.\  807--814, 2007.

\bibitem[Shankar et~al.(2020)Shankar, Fang, Guo, Fridovich-Keil, Ragan-Kelley,
  Schmidt, and Recht]{shankar2020neural}
Shankar, V., Fang, A., Guo, W., Fridovich-Keil, S., Ragan-Kelley, J., Schmidt,
  L., and Recht, B.
\newblock Neural kernels without tangents.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8614--8623. PMLR, 2020.

\bibitem[Titsias(2009)]{titsias2009variational}
Titsias, M.
\newblock Variational learning of inducing variables in sparse gaussian
  processes.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  567--574.
  PMLR, 2009.

\bibitem[Towns et~al.(2014)Towns, Cockerill, Dahan, Foster, Gaither, Grimshaw,
  Hazlewood, Lathrop, Lifka, Peterson, Roskies, Scott, and
  Wilkins-Diehr]{XSEDE}
Towns, J., Cockerill, T., Dahan, M., Foster, I., Gaither, K., Grimshaw, A.,
  Hazlewood, V., Lathrop, S., Lifka, D., Peterson, G.~D., Roskies, R., Scott,
  J., and Wilkins-Diehr, N.
\newblock Xsede: Accelerating scientific discovery.
\newblock \emph{Computing in Science \& Engineering}, 16\penalty0
  (05):\penalty0 62--74, sep 2014.
\newblock ISSN 1558-366X.
\newblock \doi{10.1109/MCSE.2014.80}.

\bibitem[Virtanen et~al.(2020)Virtanen, Gommers, Oliphant, Haberland, Reddy,
  Cournapeau, Burovski, Peterson, Weckesser, Bright, {van der Walt}, Brett,
  Wilson, Millman, Mayorov, Nelson, Jones, Kern, Larson, Carey, Polat, Feng,
  Moore, {VanderPlas}, Laxalde, Perktold, Cimrman, Henriksen, Quintero, Harris,
  Archibald, Ribeiro, Pedregosa, {van Mulbregt}, and {SciPy 1.0
  Contributors}]{2020SciPy-NMeth}
Virtanen, P., Gommers, R., Oliphant, T.~E., Haberland, M., Reddy, T.,
  Cournapeau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, J., {van
  der Walt}, S.~J., Brett, M., Wilson, J., Millman, K.~J., Mayorov, N., Nelson,
  A. R.~J., Jones, E., Kern, R., Larson, E., Carey, C.~J., Polat, {\.I}., Feng,
  Y., Moore, E.~W., {VanderPlas}, J., Laxalde, D., Perktold, J., Cimrman, R.,
  Henriksen, I., Quintero, E.~A., Harris, C.~R., Archibald, A.~M., Ribeiro,
  A.~H., Pedregosa, F., {van Mulbregt}, P., and {SciPy 1.0 Contributors}.
\newblock {{SciPy} 1.0: Fundamental Algorithms for Scientific Computing in
  Python}.
\newblock \emph{Nature Methods}, 17:\penalty0 261--272, 2020.
\newblock \doi{10.1038/s41592-019-0686-2}.

\bibitem[Watanabe et~al.(2018)Watanabe, Hori, Karita, Hayashi, Nishitoba, Unno,
  {Enrique Yalta Soplin}, Heymann, Wiesner, Chen, Renduchintala, and
  Ochiai]{watanabe2018espnet}
Watanabe, S., Hori, T., Karita, S., Hayashi, T., Nishitoba, J., Unno, Y.,
  {Enrique Yalta Soplin}, N., Heymann, J., Wiesner, M., Chen, N.,
  Renduchintala, A., and Ochiai, T.
\newblock {ESPnet}: End-to-end speech processing toolkit.
\newblock In \emph{Proceedings of Interspeech}, pp.\  2207--2211, 2018.
\newblock \doi{10.21437/Interspeech.2018-1456}.
\newblock URL \url{http://dx.doi.org/10.21437/Interspeech.2018-1456}.

\bibitem[Wightman(2019)]{rw2019timm}
Wightman, R.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem[Williams \& Seeger(2000)Williams and Seeger]{williams2000using}
Williams, C. and Seeger, M.
\newblock Using the nystr{\"o}m method to speed up kernel machines.
\newblock \emph{Advances in neural information processing systems}, 13, 2000.

\bibitem[Wilson \& Nickisch(2015)Wilson and Nickisch]{wilson2015kernel}
Wilson, A. and Nickisch, H.
\newblock Kernel interpolation for scalable structured gaussian processes
  (kiss-gp).
\newblock In \emph{International conference on machine learning}, pp.\
  1775--1784. PMLR, 2015.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Yang et~al.(2012)Yang, Li, Mahdavi, Jin, and Zhou]{yang2012nystrom}
Yang, T., Li, Y.-F., Mahdavi, M., Jin, R., and Zhou, Z.-H.
\newblock Nystr{\"o}m method vs random fourier features: A theoretical and
  empirical comparison.
\newblock \emph{Advances in neural information processing systems}, 25, 2012.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Zhu et~al.(2022)Zhu, Liu, and Belkin]{zhu2022transition}
Zhu, L., Liu, C., and Belkin, M.
\newblock Transition to linearity of general neural networks with directed
  acyclic graph architecture.
\newblock \emph{Advances in neural information processing systems}, 2022.

\end{thebibliography}
