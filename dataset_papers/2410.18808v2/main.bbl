\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[AI@Meta(2024)]{Llama3}
AI@Meta.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}, 2024.

\bibitem[Berglund et~al.(2023)Berglund, Tong, Kaufmann, Balesni, Stickland, Korbak, and Evans]{Reversal_curse}
Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa~Cooper Stickland, Tomasz Korbak, and Owain Evans.
\newblock The reversal curse: Llms trained on "a is b" fail to learn "b is a".
\newblock \emph{CoRR}, abs/2309.12288, 2023.
\newblock \doi{10.48550/ARXIV.2309.12288}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2309.12288}.

\bibitem[Bhakthavatsalam et~al.(2021)Bhakthavatsalam, Khashabi, Khot, Mishra, Richardson, Sabharwal, Schoenick, Tafjord, and Clark]{arc}
Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Bhavana~Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, and Peter Clark.
\newblock Think you have solved direct-answer question answering? try arc-da, the direct-answer {AI2} reasoning challenge.
\newblock \emph{CoRR}, abs/2102.03315, 2021.
\newblock URL \url{https://arxiv.org/abs/2102.03315}.

\bibitem[Bowman(2023)]{eight_things}
Sam Bowman.
\newblock Eight things to know about large language models.
\newblock \emph{ArXiv}, abs/2304.00612, 2023.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:257913333}.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg, Nori, Palangi, Ribeiro, and Zhang]{sparks_of_artificial}
S{\'{e}}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott~M. Lundberg, Harsha Nori, Hamid Palangi, Marco~T{\'{u}}lio Ribeiro, and Yi~Zhang.
\newblock Sparks of artificial general intelligence: Early experiments with {GPT-4}.
\newblock \emph{CoRR}, abs/2303.12712, 2023.
\newblock \doi{10.48550/ARXIV.2303.12712}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2303.12712}.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang, Zhuang, Gonzalez, et~al.]{Vicuna}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E Gonzalez, et~al.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality.
\newblock \emph{See https://vicuna. lmsys. org (accessed 14 April 2023)}, 2023.

\bibitem[Cossu et~al.(2022)Cossu, Tuytelaars, Carta, Passaro, Lomonaco, and Bacciu]{CPT2}
Andrea Cossu, Tinne Tuytelaars, Antonio Carta, Lucia~C. Passaro, Vincenzo Lomonaco, and Davide Bacciu.
\newblock Continual pre-training mitigates forgetting in language and vision.
\newblock \emph{CoRR}, abs/2205.09357, 2022.
\newblock \doi{10.48550/ARXIV.2205.09357}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2205.09357}.

\bibitem[Dai et~al.(2023)Dai, Liu, Liao, Huang, Cao, Wu, Zhao, Xu, Liu, Liu, et~al.]{auggpt}
Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Yihan Cao, Zihao Wu, Lin Zhao, Shaochen Xu, Wei Liu, Ninghao Liu, et~al.
\newblock Auggpt: Leveraging chatgpt for text data augmentation.
\newblock \emph{arXiv preprint arXiv:2302.13007}, 2023.

\bibitem[Dziri et~al.(2023)Dziri, Lu, Sclar, Li, Jian, Lin, West, Bhagavatula, Bras, Hwang, Sanyal, Welleck, Ren, Ettinger, Harchaoui, and Choi]{faith_and_fate}
Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang~Lorraine Li, Liwei Jian, Bill~Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan~Le Bras, Jena~D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger, Za{\"i}d Harchaoui, and Yejin Choi.
\newblock Faith and fate: Limits of transformers on compositionality.
\newblock \emph{ArXiv}, abs/2305.18654, 2023.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:258967391}.

\bibitem[Eldan and Li(2023)]{scaling-law2}
Ronen Eldan and Yuanzhi Li.
\newblock Tinystories: How small can language models be and still speak coherent english?
\newblock \emph{CoRR}, abs/2305.07759, 2023.
\newblock \doi{10.48550/ARXIV.2305.07759}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2305.07759}.

\bibitem[Geva et~al.(2021)Geva, Schuster, Berant, and Levy]{key-value_mem}
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.
\newblock Transformer feed-forward layers are key-value memories.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)}, 2021.

\bibitem[Grosse et~al.(2023)Grosse, Bae, Anil, Elhage, Tamkin, Tajdini, Steiner, Li, Durmus, Perez, Hubinger, Lukosiute, Nguyen, Joseph, McCandlish, Kaplan, and Bowman]{influence_function}
Roger~B. Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamile Lukosiute, Karina Nguyen, Nicholas Joseph, Sam McCandlish, Jared Kaplan, and Samuel~R. Bowman.
\newblock Studying large language model generalization with influence functions.
\newblock \emph{CoRR}, abs/2308.03296, 2023.
\newblock \doi{10.48550/ARXIV.2308.03296}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2308.03296}.

\bibitem[Gunasekar et~al.(2023)Gunasekar, Zhang, Aneja, Mendes, Giorno, Gopi, Javaheripi, Kauffmann, de~Rosa, Saarikivi, Salim, Shah, Behl, Wang, Bubeck, Eldan, Kalai, Lee, and Li]{scaling_law3}
Suriya Gunasekar, Yi~Zhang, Jyoti Aneja, Caio C{\'{e}}sar~Teodoro Mendes, Allie~Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de~Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat~Singh Behl, Xin Wang, S{\'{e}}bastien Bubeck, Ronen Eldan, Adam~Tauman Kalai, Yin~Tat Lee, and Yuanzhi Li.
\newblock Textbooks are all you need.
\newblock \emph{CoRR}, abs/2306.11644, 2023.
\newblock \doi{10.48550/ARXIV.2306.11644}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2306.11644}.

\bibitem[Guo et~al.(2020)Guo, Kim, and Rush]{Segmixup}
Demi Guo, Yoon Kim, and Alexander~M. Rush.
\newblock Sequence-level mixed sample data augmentation.
\newblock In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2020, Online, November 16-20, 2020}, pages 5547--5552. Association for Computational Linguistics, 2020.
\newblock \doi{10.18653/V1/2020.EMNLP-MAIN.447}.
\newblock URL \url{https://doi.org/10.18653/v1/2020.emnlp-main.447}.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{MMLU}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, and Chen]{LoRA}
J.~Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{ArXiv}, abs/2106.09685, 2021.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:235458009}.

\bibitem[Huang et~al.(2022)Huang, Gu, Hou, Wu, Wang, Yu, and Han]{train_with_CoT1}
Jiaxin Huang, Shixiang~Shane Gu, Le~Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han.
\newblock Large language models can self-improve.
\newblock \emph{arXiv preprint arXiv:2210.11610}, 2022.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, de~Las~Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock, Scao, Lavril, Wang, Lacroix, and Sayed]{Mistral}
Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~Las~Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L{\'{e}}lio~Renard Lavaud, Marie{-}Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timoth{\'{e}}e Lacroix, and William~El Sayed.
\newblock Mistral 7b.
\newblock \emph{CoRR}, abs/2310.06825, 2023.
\newblock \doi{10.48550/ARXIV.2310.06825}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2310.06825}.

\bibitem[Ke et~al.(2023)Ke, Shao, Lin, Konishi, Kim, and Liu]{CPT1}
Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu.
\newblock Continual pre-training of language models.
\newblock In \emph{The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}. OpenReview.net, 2023.
\newblock URL \url{https://openreview.net/pdf?id=m\_GDIItaI3o}.

\bibitem[Kingma and Ba(2014)]{adam_optimizer}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{CoRR}, abs/1412.6980, 2014.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:6628106}.

\bibitem[Lee et~al.(2022)Lee, Ippolito, Nystrom, Zhang, Eck, Callison{-}Burch, and Carlini]{deduplication1}
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison{-}Burch, and Nicholas Carlini.
\newblock Deduplicating training data makes language models better.
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland, May 22-27, 2022}, pages 8424--8445. Association for Computational Linguistics, 2022.
\newblock \doi{10.18653/V1/2022.ACL-LONG.577}.
\newblock URL \url{https://doi.org/10.18653/v1/2022.acl-long.577}.

\bibitem[Lee et~al.(2023)Lee, Atreya, Ye, and Choi]{crafting_IC}
Yoonsang Lee, Pranav Atreya, Xi~Ye, and Eunsol Choi.
\newblock Crafting in-context examples according to lms' parametric knowledge.
\newblock \emph{CoRR}, abs/2311.09579, 2023.
\newblock \doi{10.48550/ARXIV.2311.09579}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2311.09579}.

\bibitem[Li and Thompson(1976)]{human_language}
Charles~N. Li and Sandra~A. Thompson.
\newblock Subject and topic: A new typology of language.
\newblock 1976.

\bibitem[Li et~al.(2022)Li, Hopkins, Bau, Vi'egas, Pfister, and Wattenberg]{emergent_world}
Kenneth Li, Aspen~K. Hopkins, David Bau, Fernanda Vi'egas, Hanspeter Pfister, and Martin Wattenberg.
\newblock Emergent world representations: Exploring a sequence model trained on a synthetic task.
\newblock \emph{ArXiv}, abs/2210.13382, 2022.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:253098566}.

\bibitem[Li and Lewandowsky(1995)]{backward_recall}
Shu~Chen Li and Stephan Lewandowsky.
\newblock Forward and backward recall: Different retrieval processes.
\newblock \emph{Journal of Experimental Psychology Learning Memory and Cognition}, 21\penalty0 (4):\penalty0 837--847, 1995.

\bibitem[Lin(2004)]{Rouge}
Chin-Yew Lin.
\newblock Rouge: A package for automatic evaluation of summaries.
\newblock In \emph{Text summarization branches out}, pages 74--81, 2004.

\bibitem[Lin et~al.(2022)Lin, Hilton, and Evans]{truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland, May 22-27, 2022}, pages 3214--3252. Association for Computational Linguistics, 2022.
\newblock \doi{10.18653/V1/2022.ACL-LONG.229}.
\newblock URL \url{https://doi.org/10.18653/v1/2022.acl-long.229}.

\bibitem[Liu et~al.(2022)Liu, Kitouni, Nolte, Michaud, Tegmark, and Williams]{grokking_ML}
Ziming Liu, Ouail Kitouni, Niklas Nolte, Eric~J. Michaud, Max Tegmark, and Mike Williams.
\newblock Towards understanding grokking: An effective theory of representation learning.
\newblock \emph{ArXiv}, abs/2205.10343, 2022.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:248965387}.

\bibitem[Lv et~al.(2023)Lv, Zhang, Xie, Tu, Chen, Wen, and Yan]{BICO}
Ang Lv, Kaiyi Zhang, Shufang Xie, Quan Tu, Yuhan Chen, Ji{-}Rong Wen, and Rui Yan.
\newblock Are we falling in a middle-intelligence trap? an analysis and mitigation of the reversal curse.
\newblock \emph{CoRR}, abs/2311.07468, 2023.
\newblock \doi{10.48550/ARXIV.2311.07468}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2311.07468}.

\bibitem[Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov]{ROME}
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
\newblock Locating and editing factual associations in {GPT}.
\newblock In Sanmi Koyejo, S.~Mohamed, A.~Agarwal, Danielle Belgrave, K.~Cho, and A.~Oh, editors, \emph{Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022}, 2022.

\bibitem[Meng et~al.(2023)Meng, Sharma, Andonian, Belinkov, and Bau]{MEMIT}
Kevin Meng, Arnab~Sen Sharma, Alex~J. Andonian, Yonatan Belinkov, and David Bau.
\newblock Mass-editing memory in a transformer.
\newblock In \emph{The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}. OpenReview.net, 2023.

\bibitem[Mishra and Sachdeva(2020)]{deduplication2}
Swaroop Mishra and Bhavdeep~Singh Sachdeva.
\newblock Do we need to create big datasets to learn a task?
\newblock In Nafise~Sadat Moosavi, Angela Fan, Vered Shwartz, Goran Glavas, Shafiq~R. Joty, Alex Wang, and Thomas Wolf, editors, \emph{Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing, SustaiNLP@EMNLP 2020, Online, November 20, 2020}, pages 169--173. Association for Computational Linguistics, 2020.
\newblock \doi{10.18653/V1/2020.SUSTAINLP-1.23}.
\newblock URL \url{https://doi.org/10.18653/v1/2020.sustainlp-1.23}.

\bibitem[Mitchell and Krakauer(2022)]{understanding_debate}
Melanie Mitchell and David~C. Krakauer.
\newblock The debate over understanding in aiâ€™s large language models.
\newblock \emph{Proceedings of the National Academy of Sciences of the United States of America}, 120, 2022.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:253107905}.

\bibitem[Murty et~al.(2023)Murty, Sharma, Andreas, and Manning]{grokking_LM}
Shikhar Murty, Pratyusha Sharma, Jacob Andreas, and Christopher~D. Manning.
\newblock Grokking of hierarchical structure in vanilla transformers.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics}, 2023.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:258967837}.

\bibitem[Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber, Dohan, Lewkowycz, Bosma, Luan, Sutton, and Odena]{Scratchpad}
Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena.
\newblock Show your work: Scratchpads for intermediate computation with language models.
\newblock \emph{ArXiv}, abs/2112.00114, 2021.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:244773644}.

\bibitem[OpenAI(2023)]{GPT-4}
OpenAI.
\newblock {GPT-4} technical report.
\newblock \emph{CoRR}, abs/2303.08774, 2023.
\newblock \doi{10.48550/ARXIV.2303.08774}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2303.08774}.

\bibitem[Pan et~al.(2023)Pan, Luo, Wang, Chen, Wang, and Wu]{unifying_LLM_KG}
Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu.
\newblock Unifying large language models and knowledge graphs: {A} roadmap.
\newblock \emph{CoRR}, abs/2306.08302, 2023.
\newblock \doi{10.48550/ARXIV.2306.08302}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2306.08302}.

\bibitem[Peng et~al.(2023)Peng, Galley, He, Cheng, Xie, Hu, Huang, Liden, Yu, Chen, et~al.]{peng2023check}
Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu~Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et~al.
\newblock Check your facts and try again: Improving large language models with external knowledge and automated feedback.
\newblock \emph{arXiv preprint arXiv:2302.12813}, 2023.

\bibitem[Pfau et~al.(2024)Pfau, Merrill, and Bowman]{train_with_CoT2}
Jacob Pfau, William Merrill, and Samuel~R Bowman.
\newblock Let's think dot by dot: Hidden computation in transformer language models.
\newblock \emph{arXiv preprint arXiv:2404.15758}, 2024.

\bibitem[Simonyan et~al.(2014)Simonyan, Vedaldi, and Zisserman]{Saliency_technique}
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
\newblock Deep inside convolutional networks: Visualising image classification models and saliency maps.
\newblock In Yoshua Bengio and Yann LeCun, editors, \emph{2nd International Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings}, 2014.

\bibitem[Sorscher et~al.(2022)Sorscher, Geirhos, Shekhar, Ganguli, and Morcos]{scaling-law1}
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos.
\newblock Beyond neural scaling laws: beating power law scaling via data pruning.
\newblock In Sanmi Koyejo, S.~Mohamed, A.~Agarwal, Danielle Belgrave, K.~Cho, and A.~Oh, editors, \emph{Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022}, 2022.

\bibitem[Sun et~al.(2022)Sun, Wang, Tay, Yang, and Zhou]{Sun2022RecitationAugmentedLM}
Zhiqing Sun, Xuezhi Wang, Yi~Tay, Yiming Yang, and Denny Zhou.
\newblock Recitation-augmented language models.
\newblock \emph{ArXiv}, abs/2210.01296, 2022.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:252692968}.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Canton{-}Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{Llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton{-}Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie{-}Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aur{\'{e}}lien Rodriguez, Robert Stojnic, Sergey Edunov,
  and Thomas Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{CoRR}, abs/2307.09288, 2023.
\newblock \doi{10.48550/ARXIV.2307.09288}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2307.09288}.

\bibitem[Tuo and Yang(2023)]{review_of_ERE}
Meimei Tuo and Wenzhong Yang.
\newblock Review of entity relation extraction.
\newblock \emph{J. Intell. Fuzzy Syst.}, 44\penalty0 (5):\penalty0 7391--7405, 2023.
\newblock \doi{10.3233/JIFS-223915}.
\newblock URL \url{https://doi.org/10.3233/JIFS-223915}.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Li, Dai, Chen, Zhou, Meng, Zhou, and Sun]{anchors}
Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu~Sun.
\newblock Label words are anchors: An information flow perspective for understanding in-context learning.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023}, pages 9840--9855. Association for Computational Linguistics, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi]{self-instruct}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language models with self-generated instructions.
\newblock In Anna Rogers, Jordan~L. Boyd{-}Graber, and Naoaki Okazaki, editors, \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2023, Toronto, Canada, July 9-14, 2023}, pages 13484--13508. Association for Computational Linguistics, 2023{\natexlab{b}}.
\newblock \doi{10.18653/V1/2023.ACL-LONG.754}.
\newblock URL \url{https://doi.org/10.18653/v1/2023.acl-long.754}.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Ichter, Xia, Chi, Le, and Zhou]{CoT}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed~H. Chi, Quoc~V. Le, and Denny Zhou.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock In Sanmi Koyejo, S.~Mohamed, A.~Agarwal, Danielle Belgrave, K.~Cho, and A.~Oh, editors, \emph{Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022}, 2022.

\bibitem[Wei and Zou(2019)]{EDA}
Jason~W. Wei and Kai Zou.
\newblock {EDA:} easy data augmentation techniques for boosting performance on text classification tasks.
\newblock In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, {EMNLP-IJCNLP} 2019, Hong Kong, China, November 3-7, 2019}, pages 6381--6387. Association for Computational Linguistics, 2019.
\newblock \doi{10.18653/V1/D19-1670}.
\newblock URL \url{https://doi.org/10.18653/v1/D19-1670}.

\bibitem[Whitehouse et~al.(2023)Whitehouse, Choudhury, and Aji]{llm_powered_data_augmentation}
Chenxi Whitehouse, Monojit Choudhury, and Alham~Fikri Aji.
\newblock Llm-powered data augmentation for enhanced cross-lingual performance.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023}, pages 671--686. Association for Computational Linguistics, 2023.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.44}.

\bibitem[Xie et~al.(2023)Xie, Aggarwal, and Ahmad]{Xie2023EfficientCP}
Yong Xie, Karan Aggarwal, and Aitzaz Ahmad.
\newblock Efficient continual pre-training for building domain specific large language models.
\newblock \emph{ArXiv}, abs/2311.08545, 2023.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:265213147}.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura, Sridhar, Wang, and Zettlemoyer]{deduplication}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona~T. Diab, Xian Li, Xi~Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit~Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.
\newblock {OPT:} open pre-trained transformer language models.
\newblock \emph{CoRR}, abs/2205.01068, 2022.
\newblock \doi{10.48550/ARXIV.2205.01068}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2205.01068}.

\bibitem[Zhou et~al.(2023)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu, Zhang, Ghosh, Lewis, Zettlemoyer, and Levy]{LIMA}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy.
\newblock {LIMA:} less is more for alignment.
\newblock \emph{CoRR}, abs/2305.11206, 2023.
\newblock \doi{10.48550/ARXIV.2305.11206}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2305.11206}.

\bibitem[Zhu et~al.(2024)Zhu, Huang, Zhang, Jordan, Jiao, Tian, and Russell]{understanding_RC}
Hanlin Zhu, Baihe Huang, Shaolun Zhang, Michael Jordan, Jiantao Jiao, Yuandong Tian, and Stuart Russell.
\newblock Towards a theoretical understanding of the 'reversal curse' via training dynamics.
\newblock 2024.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:269626444}.

\bibitem[Zhu and Li(2023)]{Physics_of_LM_3.1}
Zeyuan~Allen Zhu and Yuanzhi Li.
\newblock Physics of language models: Part 3.1, knowledge storage and extraction.
\newblock \emph{CoRR}, abs/2309.14316, 2023.
\newblock \doi{10.48550/ARXIV.2309.14316}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2309.14316}.

\end{thebibliography}
