@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@article{tran2018spectral,
  title={Spectral signatures in backdoor attacks},
  author={Tran, Brandon and Li, Jerry and Madry, Aleksander},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{chen2018detecting,
  title={Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering},
  author={Chen, Bryant and Carvalho, Wilka and Baracaldo, Nathalie and Ludwig, Heiko and Edwards, Benjamin and Lee, Taesung and Molloy, Ian and Srivastava, Biplav},
  booktitle={SafeAI@ AAAI},
  year={2019}
}

@article{borgnia2021dp,
  title={DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations},
  author={Borgnia, Eitan and Geiping, Jonas and Cherepanova, Valeriia and Fowl, Liam and Gupta, Arjun and Ghiasi, Amin and Huang, Furong and Goldblum, Micah and Goldstein, Tom},
  journal={arXiv preprint arXiv:2103.02079},
  year={2021}
}

@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}

@article{hong2020effectiveness,
  title={On the effectiveness of mitigating data poisoning attacks with gradient shaping},
  author={Hong, Sanghyun and Chandrasekaran, Varun and Kaya, Yi{\u{g}}itcan and Dumitra{\c{s}}, Tudor and Papernot, Nicolas},
  journal={arXiv preprint arXiv:2002.11497},
  year={2020}
}


@inproceedings{saha2019hidden,
  title={Hidden trigger backdoor attacks},
  author={Saha, Aniruddha and Subramanya, Akshayvarun and Pirsiavash, Hamed},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={07},
  pages={11957--11965},
  year={2020}
}


@article{geiping2020witches,
  title={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching},
  author={Geiping, Jonas and Fowl, Liam and Huang, W Ronny and Czaja, Wojciech and Taylor, Gavin and Moeller, Michael and Goldstein, Tom},
  booktitle={International Conference on Learning Representations},
  journal={ICLR},
  year={2021}
}

@inproceedings{schwarzschild2020just,
  title={Just how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks},
  author={Schwarzschild, Avi and Goldblum, Micah and Gupta, Arjun and Dickerson, John P and Goldstein, Tom},
  booktitle={International Conference on Machine Learning},
  pages={9389--9398},
  year={2021},
  organization={PMLR}
}

@inproceedings{zhao2020dataset,
  title={Dataset Condensation with Gradient Matching},
  author={Zhao, Bo and Mopuri, Konda Reddy and Bilen, Hakan},
  booktitle={Ninth International Conference on Learning Representations 2021},
  year={2021}
}

@article{fowl2021preventing,
  title={Preventing Unauthorized Use of Proprietary Data: Poisoning for Secure Dataset Release},
  author={Fowl, Liam and Chiang, Ping-yeh and Goldblum, Micah and Geiping, Jonas and Bansal, Arpit and Czaja, Wojtek and Goldstein, Tom},
  journal={arXiv preprint arXiv:2103.02683},
  year={2021}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}


@article{gu2017badnets,
  title={Badnets: Identifying vulnerabilities in the machine learning model supply chain},
  author={Gu, Tianyu and Dolan-Gavitt, Brendan and Garg, Siddharth},
  journal={arXiv preprint arXiv:1708.06733},
  year={2017}
}

@article{chen2017targeted,
  title={Targeted backdoor attacks on deep learning systems using data poisoning},
  author={Chen, Xinyun and Liu, Chang and Li, Bo and Lu, Kimberly and Song, Dawn},
  journal={arXiv preprint arXiv:1712.05526},
  year={2017}
}

@article{li2019invisible,
  title={Invisible backdoor attacks against deep neural networks},
  author={Li, Shaofeng and Zhao, Benjamin Zi Hao and Yu, Jiahao and Xue, Minhui and Kaafar, Dali and Zhu, Haojin},
  journal={arXiv preprint arXiv:1909.02742},
  year={2019}
}

@article{li2020invisible,
  title={Invisible backdoor attacks on deep neural networks via steganography and regularization},
  author={Li, Shaofeng and Xue, Minhui and Zhao, Benjamin and Zhu, Haojin and Zhang, Xinpeng},
  journal={IEEE Transactions on Dependable and Secure Computing},
  year={2020},
  publisher={IEEE}
}

@inproceedings{bagdasaryan2020backdoor,
  title={How to backdoor federated learning},
  author={Bagdasaryan, Eugene and Veit, Andreas and Hua, Yiqing and Estrin, Deborah and Shmatikov, Vitaly},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2938--2948},
  year={2020},
  organization={PMLR}
}

@inproceedings{madry2017towards,
  title={Towards Deep Learning Models Resistant to Adversarial Attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{munoz-gonzalez_towards_2017,
  title = {Towards {{Poisoning}} of {{Deep Learning Algorithms}} with {{Back}}-Gradient {{Optimization}}},
  booktitle = {Proceedings of the 10th {{ACM Workshop}} on {{Artificial Intelligence}} and {{Security}}},
  author = {{Muñoz-González}, Luis and Biggio, Battista and Demontis, Ambra and Paudice, Andrea and Wongrassamee, Vasin and Lupu, Emil C. and Roli, Fabio},
  year = {2017},
  pages = {27--38},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3128572.3140451},
  abstract = {A number of online services nowadays rely upon machine learning to extract valuable information from data collected in the wild. This exposes learning algorithms to the threat of data poisoning, i.e., a coordinate attack in which a fraction of the training data is controlled by the attacker and manipulated to subvert the learning process. To date, these attacks have been devised only against a limited class of binary learning algorithms, due to the inherent complexity of the gradient-based procedure used to optimize the poisoning points (a.k.a. adversarial training examples). In this work, we first extend the definition of poisoning attacks to multiclass problems. We then propose a novel poisoning algorithm based on the idea of back-gradient optimization, i.e., to compute the gradient of interest through automatic differentiation, while also reversing the learning procedure to drastically reduce the attack complexity. Compared to current poisoning strategies, our approach is able to target a wider class of learning algorithms, trained with gradient-based procedures, including neural networks and deep learning architectures. We empirically evaluate its effectiveness on several application examples, including spam filtering, malware detection, and handwritten digit recognition. We finally show that, similarly to adversarial test examples, adversarial training examples can also be transferred across different learning algorithms.},
  isbn = {978-1-4503-5202-4},
  keywords = {adversarial examples,adversarial machine learning,deep learning,training data poisoning},
  series = {{{AISec}} '17}
}

@incollection{steinhardt_certified_2017,
  title = {Certified {{Defenses}} for {{Data Poisoning Attacks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Steinhardt, Jacob and Koh, Pang Wei W and Liang, Percy S},
  year = {2017},
  pages = {3517--3529},
  publisher = {{Curran Associates, Inc.}}
}

@article{shen2019tensorclog,
  title={TensorClog: An imperceptible poisoning attack on deep neural network applications},
  author={Shen, Juncheng and Zhu, Xiaolei and Ma, De},
  journal={IEEE Access},
  volume={7},
  pages={41498--41506},
  year={2019},
  publisher={IEEE}
}


@inproceedings{uchida2017embedding,
  title={Embedding watermarks into deep neural networks},
  author={Uchida, Yusuke and Nagai, Yuki and Sakazawa, Shigeyuki and Satoh, Shin'ichi},
  booktitle={Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval},
  pages={269--277},
  year={2017}
}


@inproceedings{huang2021unlearnable,
  title={Unlearnable Examples: Making Personal Data Unexploitable},
  author={Huang, Hanxun and Ma, Xingjun and Erfani, Sarah Monazam and Bailey, James and Wang, Yisen},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{feng2019learning,
  title={Learning to confuse: generating training time adversarial data with auto-encoder},
  author={Feng, Ji and Cai, Qi-Zhi and Zhou, Zhi-Hua},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{shafahi2018poison,
  title={Poison frogs! targeted clean-label poisoning attacks on neural networks},
  author={Shafahi, Ali and Huang, W Ronny and Najibi, Mahyar and Suciu, Octavian and Studer, Christoph and Dumitras, Tudor and Goldstein, Tom},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{zhu2019transferable,
  title={Transferable clean-label poisoning attacks on deep neural nets},
  author={Zhu, Chen and Huang, W Ronny and Li, Hengduo and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle={International Conference on Machine Learning},
  pages={7614--7623},
  year={2019},
  organization={PMLR}
}

@article{huang2020metapoison,
  title={Metapoison: Practical general-purpose clean-label data poisoning},
  author={Huang, W Ronny and Geiping, Jonas and Fowl, Liam and Taylor, Gavin and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12080--12091},
  year={2020}
}

@article{turner2019label,
  title={Label-consistent backdoor attacks},
  author={Turner, Alexander and Tsipras, Dimitris and Madry, Aleksander},
  journal={arXiv preprint arXiv:1912.02771},
  year={2019}
}

@article{goldblum2020data,
  title={Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses},
  author={Goldblum, Micah and Tsipras, Dimitris and Xie, Chulin and Chen, Xinyun and Schwarzschild, Avi and Song, Dawn and Madry, Aleksander and Li, Bo and Goldstein, Tom},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022},
  publisher={IEEE}
}

@inproceedings{zhang2017mixup,
  title={mixup: Beyond Empirical Risk Minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{sandler2018mobilenetv2,
  title={Mobilenetv2: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4510--4520},
  year={2018}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}


@inproceedings{biggio2012poisoning,
  title={Poisoning attacks against support vector machines},
  author={Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
  booktitle={Proceedings of the 29th International Coference on International Conference on Machine Learning},
  pages={1467--1474},
  year={2012}
}


@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@inproceedings{cherepanova2020lowkey,
  title={LowKey: Leveraging Adversarial Attacks to Protect Social Media Users from Facial Recognition},
  author={Cherepanova, Valeriia and Goldblum, Micah and Foley, Harrison and Duan, Shiyuan and Dickerson, John P and Taylor, Gavin and Goldstein, Tom},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{geiping2021doesn,
  title={What Doesn't Kill You Makes You Robust (er): Adversarial Training against Poisons and Backdoors},
  author={Geiping, Jonas and Fowl, Liam and Somepalli, Gowthami and Goldblum, Micah and Moeller, Michael and Goldstein, Tom},
  journal={arXiv preprint arXiv:2102.13624},
  year={2021}
}

@article{li2016data,
  title={Data poisoning attacks on factorization-based collaborative filtering},
  author={Li, Bo and Wang, Yining and Singh, Aarti and Vorobeychik, Yevgeniy},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{ma2018data,
  title={Data poisoning attacks in contextual bandits},
  author={Ma, Yuzhe and Jun, Kwang-Sung and Li, Lihong and Zhu, Xiaojin},
  booktitle={International Conference on Decision and Game Theory for Security},
  pages={186--204},
  year={2018},
  organization={Springer}
}


@inproceedings{gao2019strip,
  title={Strip: A defence against trojan attacks on deep neural networks},
  author={Gao, Yansong and Xu, Change and Wang, Derui and Chen, Shiping and Ranasinghe, Damith C and Nepal, Surya},
  booktitle={Proceedings of the 35th Annual Computer Security Applications Conference},
  pages={113--125},
  year={2019}
}


@inproceedings{wang2019neural,
  title={Neural cleanse: Identifying and mitigating backdoor attacks in neural networks},
  author={Wang, Bolun and Yao, Yuanshun and Shan, Shawn and Li, Huiying and Viswanath, Bimal and Zheng, Haitao and Zhao, Ben Y},
  booktitle={2019 IEEE Symposium on Security and Privacy (SP)},
  pages={707--723},
  year={2019},
  organization={IEEE}
}


@inproceedings{nguyen2021wanet,
  title={WaNet-Imperceptible Warping-based Backdoor Attack},
  author={Nguyen, Tuan Anh and Tran, Anh Tuan},
  booktitle={International Conference on Learning Representations},
  year={2020}
}


@article{liu2017trojaning,
  title={Trojaning attack on neural networks},
  author={Liu, Yingqi and Ma, Shiqing and Aafer, Yousra and Lee, Wen-Chuan and Zhai, Juan and Wang, Weihang and Zhang, Xiangyu},
  year={2017}
}



@inproceedings{liu2020reflection,
  title={Reflection backdoor: A natural backdoor attack on deep neural networks},
  author={Liu, Yunfei and Ma, Xingjun and Bailey, James and Lu, Feng},
  booktitle={European Conference on Computer Vision},
  pages={182--199},
  year={2020},
  organization={Springer}
}


@inproceedings{barni2019new,
  title={A new backdoor attack in CNNs by training set corruption without label poisoning},
  author={Barni, Mauro and Kallas, Kassem and Tondi, Benedetta},
  booktitle={2019 IEEE International Conference on Image Processing (ICIP)},
  pages={101--105},
  year={2019},
  organization={IEEE}
}


@article{li2020backdoor,
  title={Backdoor learning: A survey},
  author={Li, Yiming and Jiang, Yong and Li, Zhifeng and Xia, Shu-Tao},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}

@article{fowl2021adversarial,
  title={Adversarial Examples Make Strong Poisons},
  author={Fowl, Liam and Goldblum, Micah and Chiang, Ping-yeh and Geiping, Jonas and Czaja, Wojciech and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={30339--30351},
  year={2021}
}

@article{wu2021adversarial,
  title={Adversarial neuron pruning purifies backdoored deep models},
  author={Wu, Dongxian and Wang, Yisen},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16913--16925},
  year={2021}
}

@article{li2021anti,
  title={Anti-backdoor learning: Training clean models on poisoned data},
  author={Li, Yige and Lyu, Xixiang and Koren, Nodens and Lyu, Lingjuan and Li, Bo and Ma, Xingjun},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={14900--14912},
  year={2021}
}

@inproceedings{guo2020practical,
  title={Practical poisoning attacks on neural networks},
  author={Guo, Junfeng and Liu, Cong},
  booktitle={European Conference on Computer Vision},
  pages={142--158},
  year={2020},
  organization={Springer}
}

@inproceedings{li2021invisible,
  title={Invisible backdoor attack with sample-specific triggers},
  author={Li, Yuezun and Li, Yiming and Wu, Baoyuan and Li, Longkang and He, Ran and Lyu, Siwei},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={16463--16472},
  year={2021}
}


@inproceedings{wenger2021backdoor,
  title={Backdoor attacks against deep learning systems in the physical world},
  author={Wenger, Emily and Passananti, Josephine and Bhagoji, Arjun Nitin and Yao, Yuanshun and Zheng, Haitao and Zhao, Ben Y},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6206--6215},
  year={2021}
}