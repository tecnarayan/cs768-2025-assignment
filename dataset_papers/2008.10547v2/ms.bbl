\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2017)Agarwal, Bullins, and Hazan]{agarwal:2016:lissa}
N.~Agarwal, B.~Bullins, and E.~Hazan.
\newblock Second-order stochastic optimization in linear time.
\newblock \emph{Journal of Machine Learning Research}, 2017.

\bibitem[Bates et~al.(2015)Bates, M\"{a}chler, Bolker, and
  Walker]{bates:2015:lme4}
D.~Bates, M.~M\"{a}chler, B.~Bolker, and S.~Walker.
\newblock Fitting linear mixed-effects models using lme4.
\newblock \emph{Journal of Statistical Software}, 67, 2015.

\bibitem[Bathe and Wilson(1973)]{bathe:1973:subspaceIteration}
K.~J. Bathe and E.~L. Wilson.
\newblock Solution methods for eigenvalue problems in structural mechanics.
\newblock \emph{International Journal for Numerical Methods in Engineering},
  6:\penalty0 213--226, 1973.

\bibitem[Beirami et~al.(2017)Beirami, Razaviyayn, Shahrampour, and
  Tarokh]{beirami:2017:firstALOO}
A.~Beirami, M.~Razaviyayn, S.~Shahrampour, and V.~Tarokh.
\newblock On optimal generalizability in parametric learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 3458--3468, 2017.

\bibitem[Burman(1989)]{burman:1989:cvExperiments}
P.~Burman.
\newblock A comparative study of ordinary cross-validation, v-fold
  cross-validation and the repeated learning-testing methods.
\newblock \emph{Biometrika}, 76, September 1989.

\bibitem[Buza(2014)]{buza:2014:blogFeedback}
K.~Buza.
\newblock Feedback prediction for blogs.
\newblock In \emph{Data Analysis, Machine Learning and Knowledge Discovery},
  pages 145--152. Springer International Publishing, 2014.

\bibitem[Danziger et~al.(2006)Danziger, Swamidass, Zeng, Dearth, Lu, Chen,
  Cheng, Hoang, Saigo, Luo, Baldi, Brachmann, and Lathrop]{p53_3}
S.~A. Danziger, S.~J. Swamidass, J.~Zeng, L.~R. Dearth, Q.~Lu, J.~H. Chen,
  J.~Cheng, V.~P. Hoang, H.~Saigo, R.~Luo, P.~Baldi, R.~K. Brachmann, and R.~H.
  Lathrop.
\newblock Functional census of mutation sequence spaces: the example of p53
  cancer rescue mutants.
\newblock \emph{IEEE/ACM transactions on computational biology and
  bioinformatics}, 3, 2006.

\bibitem[Danziger et~al.(2007)Danziger, Zeng, Wang, Brachmann, and
  Lathrop]{p53_2}
S.~A. Danziger, J.~Zeng, Y.~Wang, R.~K. Brachmann, and R.~H. Lathrop.
\newblock Choosing where to look next in a mutation sequence space: active
  learning of informative p53 cancer rescue mutants.
\newblock \emph{Bioinformatics}, 23, 2007.

\bibitem[Danziger et~al.(2009)Danziger, Baronio, Ho, Hall, Salmon, Hatfield,
  Kaiser, and Lathrop]{p53_1}
S.~A. Danziger, R.~Baronio, L.~Ho, L.~Hall, K.~Salmon, G.~W. Hatfield,
  P.~Kaiser, and R.~H. Lathrop.
\newblock Predicting positive p53 cancer rescue regions using most informative
  positive ({MIP}) active learning.
\newblock \emph{PLOS computational biology}, 5, 2009.

\bibitem[Efron(1982)]{efron:1982:jackknife}
B.~Efron.
\newblock \emph{The Jackknife, the Bootstrap, and Other Resampling Plans},
  volume~38.
\newblock Society for Industrial and Applied Mathematics, 1982.

\bibitem[Geisser(1975)]{geisser:1975:earlyCV}
S.~Geisser.
\newblock The predictive sample reuse method with applications.
\newblock \emph{Journal of the American Statistical Association}, 70\penalty0
  (350):\penalty0 320--328, June 1975.

\bibitem[Giordano et~al.(2019)Giordano, Stephenson, Liu, Jordan, and
  Broderick]{giordano:2018:ourALOO}
R.~Giordano, W.~T. Stephenson, R.~Liu, M.~I. Jordan, and T.~Broderick.
\newblock A {S}wiss army infinitesimal jackknife.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, April 2019.

\bibitem[Hastie et~al.(2009)Hastie, Tibshirani, and Friedman]{hastie:2009:els}
T.~Hastie, R.~Tibshirani, and J.~Friedman.
\newblock \emph{The Elements of Statistical Learning}.
\newblock Springer-Verlag, 2009.

\bibitem[Jaeckel(1972)]{jaeckel:1972:infinitesimal}
L.~Jaeckel.
\newblock The infinitesimal jackknife, memorandum.
\newblock Technical report, MM 72-1215-11, Bell Lab. Murray Hill, NJ, 1972.

\bibitem[Koh and Liang(2017)]{koh:2017:influenceFunctions}
P.~W. Koh and P.~Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{International Conference in Machine Learning (ICML)}, 2017.

\bibitem[Koh et~al.(2019)Koh, Ang, Teo, and Liang]{koh:2019:IJNSBounds}
P.~W. Koh, K.~S. Ang, H.~Teo, and P.~Liang.
\newblock On the accuracy of influence functions for measuring group effects.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Lewis et~al.(2004)Lewis, Yang, Rose, and Li]{rcv1}
D.~D. Lewis, Y.~Yang, T.~G. Rose, and F.~Li.
\newblock {RCV1}: A new benchmark collection for text categorization research.
\newblock \emph{Journal of Machine Learning Research}, 5, 2004.

\bibitem[Lorraine et~al.(2020)Lorraine, Vicol, and
  Duvenaud]{lorraine:2020:hyperparamOpt}
J.~Lorraine, P.~Vicol, and D.~Duvenaud.
\newblock Optimizing millions of hyperparameters by implicit differentiation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2020.

\bibitem[Musgrave et~al.(2020)Musgrave, Belongie, and
  Lim]{musgrave:2020:realityCheck}
K.~Musgrave, S.~Belongie, and S.~N. Lim.
\newblock A machine learning reality check.
\newblock \emph{arXiv Preprint}, March 2020.

\bibitem[Obuchi and Kabashima(2016)]{obuchi:2016:linearALOO}
T.~Obuchi and Y.~Kabashima.
\newblock Cross validation in {LASSO} and its acceleration.
\newblock \emph{Journal of Statistical Mechanics}, May 2016.

\bibitem[Obuchi and Kabashima(2018)]{obuchi:2018:logisticALOO}
T.~Obuchi and Y.~Kabashima.
\newblock Accelerating cross-validation in multinomial logistic regression with
  l1-regularization.
\newblock \emph{Journal of Machine Learning Research}, September 2018.

\bibitem[Rad and Maleki(2020)]{rad:2018:detailedALOO}
K.~R. Rad and A.~Maleki.
\newblock {A scalable estimate of the extra-sample prediction error via
  approximate leave-one-out}.
\newblock \emph{arXiv Preprint}, January 2020.

\bibitem[Stephenson and Broderick(2020)]{stephenson:2020:sparseALOO}
W.~T. Stephenson and T.~Broderick.
\newblock Approximate cross-validation in high dimensions with guarantees.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, June 2020.

\bibitem[Stone(1974)]{stone:1974:earlyCV}
M.~Stone.
\newblock Cross-validatory choice and assessment of statistical predictions.
\newblock \emph{Journal of the American Statistical Association}, 36\penalty0
  (2):\penalty0 111--147, 1974.

\bibitem[Tropp et~al.(2017)Tropp, Yurtsever, Udell, and
  Cevher]{tropp:2017:psdRandomizedApproximation}
J.~Tropp, A.~Yurtsever, M.~Udell, and V.~Cevher.
\newblock Fixed-rank approximation of a positive-semidefinite matrix from
  streaming data.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2017.

\bibitem[Udell and Townsend(2019)]{udell:2019:bigDataLowRank}
M.~Udell and A.~Townsend.
\newblock Why are big data matrices approximately low rank?
\newblock \emph{SIAM Journal on Mathematics of Data Science (SIMODS)}, 2019.

\bibitem[Wilson et~al.(2020)Wilson, Kasy, and
  Mackey]{wilson:2020:modelSelectionALOO}
A.~Wilson, M.~Kasy, and L.~Mackey.
\newblock Approximate cross-validation: guarantees for model assessment and
  selection.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2020.

\end{thebibliography}
