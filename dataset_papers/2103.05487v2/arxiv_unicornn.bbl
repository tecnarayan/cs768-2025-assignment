\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Appleyard et~al.(2016)Appleyard, Kocisky, and Blunsom]{cudnn_lstm}
Appleyard, J., Kocisky, T., and Blunsom, P.
\newblock Optimizing performance of recurrent neural networks on gpus.
\newblock \emph{arXiv preprint arXiv:1604.01946}, 2016.

\bibitem[Arjovsky et~al.(2016)Arjovsky, Shah, and Bengio]{urnn}
Arjovsky, M., Shah, A., and Bengio, Y.
\newblock Unitary evolution recurrent neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1120--1128, 2016.

\bibitem[Arnold(1989)]{arn1}
Arnold, V.~I.
\newblock \emph{Mathematical methods of classical mechanics}.
\newblock Springer Verlag, New York, 1989.

\bibitem[Bagnall et~al.(2018)Bagnall, Dau, Lines, Flynn, Large, Bostrom,
  Southam, and Keogh]{eigenworms}
Bagnall, A., Dau, H.~A., Lines, J., Flynn, M., Large, J., Bostrom, A., Southam,
  P., and Keogh, E.
\newblock The uea multivariate time series classification archive, 2018.
\newblock \emph{arXiv preprint arXiv:1811.00075}, 2018.

\bibitem[Campos et~al.(2018)Campos, Jou, Gir{\'{o}}{-}i{-}Nieto, Torres, and
  Chang]{imdb_base}
Campos, V., Jou, B., Gir{\'{o}}{-}i{-}Nieto, X., Torres, J., and Chang, S.
\newblock Skip {RNN:} learning to skip state updates in recurrent neural
  networks.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}, 2018.

\bibitem[Casado(2019)]{dtriv}
Casado, M.~L.
\newblock Trivializations for gradient-based optimization on manifolds.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9154--9164, 2019.

\bibitem[Casado \& Mart{\'{\i}}nez{-}Rubio(2019)Casado and
  Mart{\'{\i}}nez{-}Rubio]{exprnn}
Casado, M.~L. and Mart{\'{\i}}nez{-}Rubio, D.
\newblock Cheap orthogonal constraints in neural networks: {A} simple
  parametrization of the orthogonal and unitary group.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019}, volume~97 of \emph{Proceedings of Machine Learning
  Research}, pp.\  3794--3803, 2019.

\bibitem[Chang et~al.(2018)Chang, Chen, Haber, and Chi]{anti}
Chang, B., Chen, M., Haber, E., and Chi, E.~H.
\newblock Antisymmetricrnn: A dynamical system view on recurrent neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Chang et~al.(2017)Chang, Zhang, Han, Yu, Guo, Tan, Cui, Witbrock,
  Hasegawa-Johnson, and Huang]{GRU_results}
Chang, S., Zhang, Y., Han, W., Yu, M., Guo, X., Tan, W., Cui, X., Witbrock, M.,
  Hasegawa-Johnson, M.~A., and Huang, T.~S.
\newblock Dilated recurrent neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  77--87, 2017.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and Duvenaud]{neuralODE}
Chen, R.~T., Rubanova, Y., Bettencourt, J., and Duvenaud, D.~K.
\newblock Neural ordinary differential equations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6571--6583, 2018.

\bibitem[Chen et~al.()Chen, Zhang, Arjovsky, and Bottou]{srnn}
Chen, Z., Zhang, J., Arjovsky, M., and Bottou, L.
\newblock Symplectic recurrent neural networks.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}.

\bibitem[Cho et~al.(2014)Cho, {van Merrienboer}, Gulcehre, Bougares, Schwenk,
  and Bengio]{gru}
Cho, K., {van Merrienboer}, B., Gulcehre, C., Bougares, F., Schwenk, H., and
  Bengio, Y.
\newblock Learning phrase representations using rnn encoder-decoder for
  statistical machine translation.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2014)}, 2014.

\bibitem[Dey \& Salemt(2017)Dey and Salemt]{imdb_gru}
Dey, R. and Salemt, F.~M.
\newblock Gate-variants of gated recurrent unit (gru) neural networks.
\newblock In \emph{2017 IEEE 60th International Midwest Symposium on Circuits
  and Systems (MWSCAS)}, pp.\  1597--1600. IEEE, 2017.

\bibitem[E(2017)]{E}
E, W.
\newblock A proposal on machine learning via dynamical systems.
\newblock \emph{Commun. Math. Stat}, 5:\penalty0 1--11, 2017.

\bibitem[Erichson et~al.(2021)Erichson, Azencot, Queiruga, and
  Mahoney]{lip_rnn}
Erichson, N.~B., Azencot, O., Queiruga, A., and Mahoney, M.~W.
\newblock Lipschitz recurrent neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{gal}
Gal, Y. and Ghahramani, Z.
\newblock A theoretically grounded application of dropout in recurrent neural
  networks.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 1019--1027, 2016.

\bibitem[Greydanus et~al.(2019)Greydanus, Dzamba, and Yosinski]{hnn}
Greydanus, S., Dzamba, M., and Yosinski, J.
\newblock Hamiltonian neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  15379--15389, 2019.

\bibitem[Guckenheimer \& Holmes(1990)Guckenheimer and Holmes]{GHbook}
Guckenheimer, J. and Holmes, P.
\newblock \emph{Nonlinear oscillations, dynamical systems, and bifurcations of
  vector fields}.
\newblock Springer Verlag, New York, 1990.

\bibitem[Hairer et~al.(2003)Hairer, Lubich, and Wanner]{HLW1}
Hairer, E., Lubich, C., and Wanner, G.
\newblock Geometric numerical integration illustrated by the st\"ormer-verlet
  method.
\newblock \emph{Acta Numerica}, 14:\penalty0 399--450, 2003.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{kaiming}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  1026--1034, 2015.

\bibitem[Helfrich et~al.(2018)Helfrich, Willmott, and Ye]{scornn}
Helfrich, K., Willmott, D., and Ye, Q.
\newblock Orthogonal recurrent neural networks with scaled cayley transform.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1969--1978. PMLR, 2018.

\bibitem[Henaff et~al.(2016)Henaff, Szlam, and LeCun]{orthornn}
Henaff, M., Szlam, A., and LeCun, Y.
\newblock Recurrent orthogonal networks and long-memory tasks.
\newblock In Balcan, M.~F. and Weinberger, K.~Q. (eds.), \emph{Proceedings of
  The 33rd International Conference on Machine Learning}, volume~48 of
  \emph{Proceedings of Machine Learning Research}, pp.\  2034--2042, 2016.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and Schmidhuber]{lstm}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Kag et~al.()Kag, Zhang, and Saligrama]{inc_rnn}
Kag, A., Zhang, Z., and Saligrama, V.
\newblock Rnns incrementally evolving on an equilibrium manifold: {A} panacea
  for vanishing and exploding gradients?
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}.

\bibitem[Kerg et~al.(2019)Kerg, Goyette, Touzel, Gidel, Vorontsov, Bengio, and
  Lajoie]{nnRNN}
Kerg, G., Goyette, K., Touzel, M.~P., Gidel, G., Vorontsov, E., Bengio, Y., and
  Lajoie, G.
\newblock Non-normal recurrent neural network (nnrnn): learning long time
  dependencies while improving expressivity with transient dynamics.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  13591--13601, 2019.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{cifar}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kusupati et~al.(2018)Kusupati, Singh, Bhatia, Kumar, Jain, and
  Varma]{fastrnn}
Kusupati, A., Singh, M., Bhatia, K., Kumar, A., Jain, P., and Varma, M.
\newblock Fastgrnn: A fast, accurate, stable and tiny kilobyte sized gated
  recurrent neural network.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9017--9028, 2018.

\bibitem[Laurent \& von Brecht(2017)Laurent and von Brecht]{chaotic_lstm}
Laurent, T. and von Brecht, J.
\newblock A recurrent neural network without chaos.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.

\bibitem[Le et~al.(2015)Le, Jaitly, and Hinton]{seq_mnist}
Le, Q.~V., Jaitly, N., and Hinton, G.~E.
\newblock A simple way to initialize recurrent networks of rectified linear
  units.
\newblock \emph{arXiv preprint arXiv:1504.00941}, 2015.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{mnist}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{DLnat}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{Nature}, 521:\penalty0 436--444, 2015.

\bibitem[Lei et~al.(2018)Lei, Zhang, Wang, Dai, and Artzi]{sru}
Lei, T., Zhang, Y., Wang, S.~I., Dai, H., and Artzi, Y.
\newblock Simple recurrent units for highly parallelizable recurrence.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)},
  2018.

\bibitem[Li et~al.(2018)Li, Li, Cook, Zhu, and Gao]{indrnn}
Li, S., Li, W., Cook, C., Zhu, C., and Gao, Y.
\newblock Independently recurrent neural network (indrnn): Building a longer
  and deeper rnn.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  5457--5466, 2018.

\bibitem[Li et~al.(2019)Li, Li, Cook, Gao, and Zhu]{deep_indrnn}
Li, S., Li, W., Cook, C., Gao, Y., and Zhu, C.
\newblock Deep independently recurrent neural network (indrnn).
\newblock \emph{arXiv preprint arXiv:1910.06251}, 2019.

\bibitem[Lorenz(1996)]{lorenz96}
Lorenz, E.~N.
\newblock Predictability: A problem partly solved.
\newblock In \emph{Proc. Seminar on Predictability}, volume~1, 1996.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and Potts]{imdb}
Maas, A.~L., Daly, R.~E., Pham, P.~T., Huang, D., Ng, A.~Y., and Potts, C.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Proceedings of the 49th Annual Meeting of the Association
  for Computational Linguistics: Human Language Technologies}, volume~1, pp.\
  142--150. Association for Computational Linguistics, 2011.

\bibitem[MacKay et~al.(2018)MacKay, Vicol, Ba, and Grosse]{inv_lstm}
MacKay, M., Vicol, P., Ba, J., and Grosse, R.~B.
\newblock Reversible recurrent neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9029--9040, 2018.

\bibitem[Morrill et~al.(2020)Morrill, Kidger, Salvi, Foster, and
  Lyons]{log_ode}
Morrill, J., Kidger, P., Salvi, C., Foster, J., and Lyons, T.
\newblock Neural cdes for long time series via the log-ode method.
\newblock \emph{arXiv preprint arXiv:2009.08295}, 2020.

\bibitem[Papamakarios et~al.(2019)Papamakarios, Nalisnick, Rezende, Mohamed,
  and Lakshminarayanan]{nf}
Papamakarios, G., Nalisnick, E., Rezende, D.~J., Mohamed, S., and
  Lakshminarayanan, B.
\newblock Normalizing flows for probabilistic modeling and inference.
\newblock \emph{arXiv preprint arXiv:1912.02762v1}, 2019.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and Bengio]{vanish_grad}
Pascanu, R., Mikolov, T., and Bengio, Y.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning}, volume~28 of \emph{ICML’13}, pp.\  III–1310–III–1318.
  JMLR.org, 2013.

\bibitem[Pennington et~al.(2014)Pennington, Socher, and Manning]{glove}
Pennington, J., Socher, R., and Manning, C.~D.
\newblock Glove: Global vectors for word representation.
\newblock In \emph{Proceedings of the 2014 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  1532--1543, 2014.

\bibitem[Pimentel et~al.(2016)Pimentel, Johnson, Charlton, Birrenkott,
  Watkinson, Tarassenko, and Clifton]{rr}
Pimentel, M.~A., Johnson, A.~E., Charlton, P.~H., Birrenkott, D., Watkinson,
  P.~J., Tarassenko, L., and Clifton, D.~A.
\newblock Toward a robust estimation of respiratory rate from pulse oximeters.
\newblock \emph{IEEE Transactions on Biomedical Engineering}, 64\penalty0
  (8):\penalty0 1914--1923, 2016.

\bibitem[Rusch \& Mishra(2021)Rusch and Mishra]{coRNN}
Rusch, T.~K. and Mishra, S.
\newblock Coupled oscillatory recurrent neural network (cornn): An accurate and
  (gradient) stable architecture for learning long time dependencies.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Sanz~Serna \& Calvo(1994)Sanz~Serna and Calvo]{ss1}
Sanz~Serna, J. and Calvo, M.
\newblock \emph{Numerical Hamiltonian problems}.
\newblock Chapman and Hall, London, 1994.

\bibitem[Strogatz(2015)]{stgz2}
Strogatz, S.
\newblock \emph{Nonlinear Dynamics and Chaos}.
\newblock Westview, Boulder CO, 2015.

\bibitem[Tan et~al.(2020)Tan, Bergmeir, Petitjean, and Webb]{ai_healthcare}
Tan, C.~W., Bergmeir, C., Petitjean, F., and Webb, G.~I.
\newblock Monash university, uea, ucr time series regression archive.
\newblock \emph{arXiv preprint arXiv:2006.10996}, 2020.

\bibitem[Werbos(1990)]{bptt}
Werbos, P.~J.
\newblock Backpropagation through time: what it does and how to do it.
\newblock \emph{Proceedings of the IEEE}, 78\penalty0 (10):\penalty0
  1550--1560, 1990.

\bibitem[Wisdom et~al.(2016)Wisdom, Powers, Hershey, Le~Roux, and Atlas]{eurnn}
Wisdom, S., Powers, T., Hershey, J., Le~Roux, J., and Atlas, L.
\newblock Full-capacity unitary recurrent neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4880--4888, 2016.

\end{thebibliography}
