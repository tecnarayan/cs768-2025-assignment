@article{cesa2004generalization,
	title={On the generalization ability of on-line learning algorithms},
	author={Cesa-Bianchi, Nicolo and Conconi, Alex and Gentile, Claudio},
	journal={IEEE Transactions on Information Theory},
	volume={50},
	number={9},
	pages={2050--2057},
	year={2004},
	publisher={IEEE}
}
@article{zou2021understanding,
  title={Understanding the generalization of adam in learning neural networks with proper regularization},
  author={Zou, Difan and Cao, Yuan and Li, Yuanzhi and Gu, Quanquan},
  journal={arXiv preprint arXiv:2108.11371},
  year={2021}
}
@inproceedings{
kunstner2023noise,
title={Noise Is Not the Main Factor Behind the Gap Between {SGD} and {Adam} on {Transformers}, But Sign Descent Might Be},
author={Frederik Kunstner and Jacques Chen and Jonathan Wilder Lavington and Mark Schmidt},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023}
}
@inproceedings{
xie2024implicit,
title={Implicit Bias of AdamW: $l_\infty$-Norm Constrained Optimization},
author={Shuo Xie and Zhiyuan Li},
booktitle={International Conference on Machine Learning},
year={2024}
}
@inproceedings{
zhang2024the,
title={The Implicit Bias of {Adam} on Separable Data},
author={Chenyang Zhang and Difan Zou and Yuan Cao},
booktitle={High-dimensional Learning Dynamics 2024: The Emergence of Structure and Reasoning},
year={2024},
url={https://openreview.net/forum?id=lwCXzNoiES}
}
@article{smale2006online,
	title={Online learning algorithms},
	author={Smale, Steve and Yao, Yuan},
	journal={Foundations of Computational Mathematics},
	volume={6},
	pages={145--170},
	year={2006},
	publisher={Springer}
}

@article{wang2022divergence,
	title={Divergence results and convergence of a variance reduced version of adam},
	author={Wang, Ruiqi and Klabjan, Diego},
	journal={arXiv preprint arXiv:2210.05607},
	year={2022}
}

@article{zhuang2020adabelief,
	title={Adabelief optimizer: Adapting stepsizes by the belief in observed gradients},
	author={Zhuang, Juntang and Tang, Tommy and Ding, Yifan and Tatikonda, Sekhar C and Dvornek, Nicha and Papademetris, Xenophon and Duncan, James},
	journal={Advances in neural information processing systems},
	volume={33},
	pages={18795--18806},
	year={2020}
}

@article{ying2006online,
	title={Online regularized classification algorithms},
	author={Ying, Yiming and Zhou, D-X},
	journal={IEEE Transactions on Information Theory},
	volume={52},
	number={11},
	pages={4775--4788},
	year={2006},
	publisher={IEEE}
}

@article{nemirovski2009robust,
	title={Robust stochastic approximation approach to stochastic programming},
	author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
	journal={SIAM Journal on Optimization},
	volume={19},
	number={4},
	pages={1574--1609},
	year={2009},
	publisher={SIAM}
}

@article{liu2022communication,
	title={A communication-efficient distributed gradient clipping algorithm for training deep neural networks},
	author={Liu, Mingrui and Zhuang, Zhenxun and Lei, Yunwen and Liao, Chunyang},
	journal={Advances in Neural Information Processing Systems},
	volume={35},
	pages={26204--26217},
	year={2022}
}

@inproceedings{lei2020fine,
	title={Fine-grained analysis of stability and generalization for stochastic gradient descent},
	author={Lei, Yunwen and Ying, Yiming},
	booktitle={International Conference on Machine Learning},
	pages={5809--5819},
	year={2020},
	organization={PMLR}
}

@article{lu2022adaptive,
	title={Adaptive gradient methods with local guarantees},
	author={Lu, Zhou and Xia, Wenhan and Arora, Sanjeev and Hazan, Elad},
	journal={arXiv preprint arXiv:2203.01400},
	year={2022}
}

@article{liu2019towards,
	title={Towards better understanding of adaptive gradient algorithms in generative adversarial nets},
	author={Liu, Mingrui and Mroueh, Youssef and Ross, Jerret and Zhang, Wei and Cui, Xiaodong and Das, Payel and Yang, Tianbao},
	journal={arXiv preprint arXiv:1912.11940},
	year={2019}
}

@article{chen2018closing,
	title={Closing the generalization gap of adaptive gradient methods in training deep neural networks},
	author={Chen, Jinghui and Zhou, Dongruo and Tang, Yiqi and Yang, Ziyan and Cao, Yuan and Gu, Quanquan},
	journal={arXiv preprint arXiv:1806.06763},
	year={2018}
}

@inproceedings{yang2018unified,
  title     = {A Unified Analysis of Stochastic Momentum Methods for Deep Learning},
  author    = {Yan Yan and Tianbao Yang and Zhe Li and Qihang Lin and Yi Yang},
  booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on
               Artificial Intelligence},
  year      = {2018}
}

@inproceedings{liu2020improved,
  title={An improved analysis of stochastic gradient descent with momentum},
  author={Liu, Yanli and Gao, Yuan and Yin, Wotao},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}
@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2017}
}
@article{bertsekas2000gradient,
  title={Gradient convergence in gradient methods with errors},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  journal={SIAM Journal on Optimization},
  volume={10},
  number={3},
  pages={627--642},
  year={2000},
  publisher={SIAM}
}
@inproceedings{crawshaw2022robustness,
  title={Robustness to unbounded smoothness of generalized {signSGD}},
  author={Crawshaw, Michael and Liu, Mingrui and Orabona, Francesco and Zhang, Wei and Zhuang, Zhenxun},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
} 
@article{reisizadeh2023variance,
  title={Variance-reduced clipping for non-convex optimization},
  author={Reisizadeh, Amirhossein and Li, Haochuan and Das, Subhro and Jadbabaie, Ali},
  journal={arXiv preprint arXiv:2303.00883},
  year={2023}
}
@article{zhao2021convergence,
  title={On the convergence and improvement of stochastic normalized gradient descent},
  author={Zhao, Shen-Yi and Xie, Yin-Peng and Li, Wu-Jun},
  journal={Science China Information Sciences},
  volume={64},
  pages={1--13},
  year={2021},
  publisher={Springer}
}
@inproceedings{qian2021understanding,
  title={Understanding gradient clipping in incremental gradient methods},
  author={Qian, Jiang and Wu, Yuren and Zhuang, Bojin and Wang, Shaojun and Xiao, Jing},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2021}
}
@inproceedings{
wang2023closing,
title={Closing the gap between the upper bound and lower bound of {Adam's} iteration complexity},
author={Bohan Wang and Jingwen Fu and Huishuai Zhang and Nanning Zheng and Wei Chen},
booktitle={Advances in Neural Information Processing Systems},
year={2023}
}
@inproceedings{xu2008robust,
  title={Robust regression and {Lasso}},
  author={Xu, Huan and Caramanis, Constantine and Mannor, Shie},
  booktitle={Advances in Neural Information Processing Systems},
  year={2008}
}
@inproceedings{khani2020feature,
  title={Feature noise induces loss discrepancy across groups},
  author={Khani, Fereshte and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={5209--5219},
  year={2020},
  organization={PMLR}
}
@book{fuller2009measurement,
  title={Measurement error models},
  author={Fuller, Wayne A},
  year={2009},
  publisher={John Wiley \& Sons}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{bottou2018optimization,
	title={Optimization methods for large-scale machine learning},
	author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
	journal={SIAM Review},
	volume={60},
	number={2},
	pages={223--311},
	year={2018},
	publisher={SIAM}
}

@article{ruder2016overview,
	title={An overview of gradient descent optimization algorithms},
	author={Ruder, Sebastian},
	journal={arXiv preprint arXiv:1609.04747},
	year={2016}
}

@inproceedings{agarwal2009information,
	title={Information-theoretic lower bounds on the oracle complexity of convex optimization},
	author={Agarwal, Alekh and Wainwright, Martin J and Bartlett, Peter and Ravikumar, Pradeep},
	booktitle={Advances in Neural Information Processing Systems},
	year={2009}
}

@inproceedings{guo2021novel,
	title={A novel convergence analysis for algorithms of the {Adam} family},
	author={Guo, Zhishuai and Xu, Yi and Yin, Wotao and Jin, Rong and Yang, Tianbao},
	booktitle  ={Annual Workshop on Optimization for Machine Learning},
	year={2021}
}


@article{streeter2010lesson,
	title={Less regret via online conditioning},
	author={Streeter, Matthew and McMahan, H Brendan},
	journal={arXiv preprint arXiv:1002.4862},
	year={2010}
}

@inproceedings{li2020high,
  title={A high probability analysis of adaptive {SGD} with momentum},
  author={Li, Xiaoyu and Orabona, Francesco},
  booktitle={Workshop on International Conference on Machine Learning},
  year={2020}
}
@inproceedings{levy2018online,
  title={Online adaptive methods, universality and acceleration},
  author={Levy, Kfir Y and Yurtsever, Alp and Cevher, Volkan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}
@article{ghadimi2016accelerated,
  title={Accelerated gradient methods for nonconvex nonlinear and stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={Mathematical Programming},
  volume={156},
  number={1},
  pages={59--99},
  year={2016},
  publisher={Springer}
}
@book{rockafellar1970convex,
  title={Convex Analysis},
  author={Rockafellar, R Tyrrell},
  volume={18},
  year={1970},
  publisher={Princeton University Press}
}
@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={7},
  pages={2121--2159},
  year={2011}
}
@inproceedings{kingma2014adam,
  author={Kingma, Diederik P and Ba, Jimmy},
  title={Adam: a method for stochastic optimization},
  booktitle={International Conference on Learning Representations},
  year={2015}
}
@inproceedings{
reddi2019convergence,
title={On the convergence of {Adam} and beyond},
author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
booktitle={International Conference on Learning Representations},
year={2018}
}
@inproceedings{zhou2023win,
title={Win: weight-decay-integrated {N}esterov acceleration for adaptive gradient algorithms},
author={Pan Zhou and Xingyu Xie and Shuicheng Yan},
booktitle={International Conference on Learning Representations },
year={2023}
}
@inproceedings{
alina2023on,
title={On the Convergence of AdaGrad(Norm) on $\mathbb{R}^d$: beyond convexity, non-asymptotic rate and acceleration},
author={Zijian Liu and Ta Duy Nguyen and Alina Ene and Huy Nguyen},
booktitle={International Conference on Learning Representations },
year={2023}
}
@inproceedings{zaheer2018adaptive,
  title={Adaptive methods for nonconvex optimization},
  author={Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}
@inproceedings{
chen2018convergence,
title={On the convergence of a class of {Adam}-type algorithms for non-convex optimization},
author={Xiangyi Chen and Sijia Liu and Ruoyu Sun and Mingyi Hong},
booktitle={International Conference on Learning Representations},
year={2019}
}
@inproceedings{zhou2018convergence,
  title={On the convergence of adaptive gradient methods for nonconvex optimization},
  author={Zhou, Dongruo and Chen, Jinghui and Cao, Yuan and Tang, Yiqi and Yang, Ziyan and Gu, Quanquan},
  booktitle={Annual Workshop on Optimization for Machine Learning},
  year={2020}
}
@article{
defossez2020simple,
title={A simple convergence proof of {Adam} and {Adagrad}},
author={Alexandre D{\'e}fossez and Leon Bottou and Francis Bach and Nicolas Usunier},
journal={Transactions on Machine Learning Research},
year={2022}
}
@inproceedings{harvey2019tight,
  title={Tight analyses for non-smooth stochastic gradient descent},
  author={Harvey, Nicholas JA and Liaw, Christopher and Plan, Yaniv and Randhawa, Sikander},
  booktitle={Conference on Learning Theory},
  year={2019},
}
@inproceedings{
kavis2022high,
title={High probability bounds for a class of nonconvex algorithms with {AdaGrad} stepsize},
author={Ali Kavis and Kfir Yehuda Levy and Volkan Cevher},
booktitle={International Conference on Learning Representations},
year={2022}
}
@article{davis2019stochastic,
  title={Stochastic model-based minimization of weakly convex functions},
  author={Davis, Damek and Drusvyatskiy, Dmitriy},
  journal={SIAM Journal on Optimization},
  volume={29},
  number={1},
  pages={207--239},
  year={2019},
  publisher={SIAM}
}
@inproceedings{li2019convergence,
  title={On the convergence of stochastic gradient descent with adaptive stepsizes},
  author={Li, Xiaoyu and Orabona, Francesco},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2019}
}
@article{ward2020adagrad,
  title={Adagrad stepsizes: sharp convergence over nonconvex landscapes},
  author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={9047--9076},
  year={2020},
  publisher={JMLRORG}
}
@article{ruszczynski1987linearization,
  title={A linearization method for nonsmooth stochastic programming problems},
  author={Ruszczy{\'n}ski, Andrzej},
  journal={Mathematics of Operations Research},
  volume={12},
  number={1},
  pages={32--49},
  year={1987},
  publisher={INFORMS}
}
@article{ermol1998stochastic,
  title={Stochastic generalized gradient method for nonconvex nonsmooth stochastic optimization},
  author={Ermol'ev, Yu M and Norkin, VI},
  journal={Cybernetics and Systems Analysis},
  volume={34},
  number={2},
  pages={196--215},
  year={1998},
  publisher={Springer US Boston}
}
@article{davis2019proximally,
  title={Proximally guided stochastic subgradient method for nonsmooth, nonconvex problems},
  author={Davis, Damek and Grimmer, Benjamin},
  journal={SIAM Journal on Optimization},
  volume={29},
  number={3},
  pages={1908--1930},
  year={2019},
  publisher={SIAM}
}
@article{drusvyatskiy2019efficiency,
  title={Efficiency of minimizing compositions of convex functions and smooth maps},
  author={Drusvyatskiy, Dmitriy and Paquette, Courtney},
  journal={Mathematical Programming},
  volume={178},
  pages={503--558},
  year={2019},
  publisher={Springer}
}
@article{davis2020nonsmooth,
  title={The nonsmooth landscape of phase retrieval},
  author={Davis, Damek and Drusvyatskiy, Dmitriy and Paquette, Courtney},
  journal={IMA Journal of Numerical Analysis},
  volume={40},
  number={4},
  pages={2652--2695},
  year={2020},
  publisher={Oxford University Press}
}
@article{duchi2019solving,
  title={Solving (most) of a set of quadratic equalities: Composite optimization for robust phase retrieval},
  author={Duchi, John C and Ruan, Feng},
  journal={Information and Inference: A Journal of the IMA},
  volume={8},
  number={3},
  pages={471--529},
  year={2019},
  publisher={Oxford University Press}
}
@article{ben1986expected,
  title={Expected utility, penalty functions, and duality in stochastic nonlinear programming},
  author={Ben-Tal, Aharon and Teboulle, Marc},
  journal={Management Science},
  volume={32},
  number={11},
  pages={1445--1466},
  year={1986},
  publisher={INFORMS}
}
@article{ben2007old,
  title={An old-new concept of convex risk measures: the optimized certainty equivalent},
  author={Ben-Tal, Aharon and Teboulle, Marc},
  journal={Mathematical Finance},
  volume={17},
  number={3},
  pages={449--476},
  year={2007},
  publisher={Wiley Online Library}
}
@article{abbe2014decoding,
  title={Decoding binary node labels from censored edge measurements: Phase transition and efficient recovery},
  author={Abbe, Emmanuel and Bandeira, Afonso S and Bracher, Annina and Singer, Amit},
  journal={IEEE Transactions on Network Science and Engineering},
  volume={1},
  number={1},
  pages={10--22},
  year={2014},
  publisher={IEEE}
}
@inproceedings{bandeira2016low,
  title={On the low-rank approach for semidefinite programs arising in synchronization and community detection},
  author={Bandeira, Afonso S and Boumal, Nicolas and Voroninski, Vladislav},
  booktitle={Conference on Learning Theory},
  year={2016}
}
@article{candes2011robust,
  title={Robust principal component analysis?},
  author={Cand{\`e}s, Emmanuel J and Li, Xiaodong and Ma, Yi and Wright, John},
  journal={Journal of the ACM},
  volume={58},
  number={3},
  pages={1--37},
  year={2011},
  publisher={ACM New York, NY, USA}
}
@inproceedings{deng2021minibatch,
  title={Minibatch and momentum model-based methods for stochastic weakly convex optimization},
  author={Deng, Qi and Gao, Wenzhi},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}
@inproceedings{mai2020convergence,
  title={Convergence of a stochastic gradient method with momentum for non-smooth non-convex optimization},
  author={Mai, Vien and Johansson, Mikael},
  booktitle={International Conference on Machine Learning},
  year={2020},
}
@inproceedings{alacaoglu2021convergence,
  title={Convergence of adaptive algorithms for constrained weakly convex optimization},
  author={Alacaoglu, Ahmet and Malitsky, Yura and Cevher, Volkan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}
@article{marquez2017imposing,
  title={Imposing hard constraints on deep networks: Promises and limitations},
  author={M{\'a}rquez-Neila, Pablo and Salzmann, Mathieu and Fua, Pascal},
  journal={arXiv preprint arXiv:1706.02025},
  year={2017}
}
@inproceedings{ilyas2018black,
  title={Black-box adversarial attacks with limited queries and information},
  author={Ilyas, Andrew and Engstrom, Logan and Athalye, Anish and Lin, Jessy},
  booktitle={International Conference on Machine Learning},
  year={2018},
}
@article{ma2019proximally,
  title={Proximally constrained methods for weakly convex optimization with weakly convex constraints},
  author={Ma, Runchao and Lin, Qihang and Yang, Tianbao},
  journal={arXiv preprint arXiv:1908.01871},
  year={2019}
}
@article{zeng2018nonconvex,
  title={On nonconvex decentralized gradient descent},
  author={Zeng, Jinshan and Yin, Wotao},
  journal={IEEE Transactions on Signal Processing},
  volume={66},
  number={11},
  pages={2834--2848},
  year={2018},
  publisher={IEEE}
}
@book{nesterov2018lectures,
  title={Lectures on Convex Optimization},
  author={Nesterov, Yurii and others},
  volume={137},
  year={2018},
  publisher={Springer}
}

@article{nazari2020adaptive,
	title={Adaptive first-and zeroth-order methods for weakly convex stochastic optimization problems},
	author={Nazari, Parvin and Tarzanagh, Davoud Ataee and Michailidis, George},
	journal={arXiv preprint arXiv:2005.09261},
	year={2020}
}
@inproceedings{zhang2020improved,
  title={Improved analysis of clipping algorithms for non-convex optimization},
  author={Zhang, Bohang and Jin, Jikai and Fang, Cong and Wang, Liwei},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}
@inproceedings{alina2023high,
  title={High probability convergence of stochastic gradient methods},
  author={Liu, Zijian and Nguyen, Ta Duy and Nguyen, Thien Hang and Ene, Alina and Nguyen, Huy},
  booktitle={International Conference on Machine Learning},
  year={2023}
}
@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={Annals of Mathematical Statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}
@inproceedings{
zhang2020why,
title={Why Gradient Clipping Accelerates Training: a Theoretical Justification for Adaptivity},
author={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},
booktitle={International Conference on Learning Representations},
year={2020}
}
@article{tieleman2012lecture,
  title={Lecture 6.5-{RMSProp}: Divide the gradient by a running average of its recent magnitude},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  journal={COURSERA: Neural Networks for Machine Learning},
  year={2012}
}
@inproceedings{attia2023sgd,
  title={{SGD} with {AdaGrad} stepsizes: full adaptivity with high probability to unknown parameters, unbounded gradients and affine variance},
  author={Attia, Amit and Koren, Tomer},
  booktitle={International Conference on Machine Learning},
  year={2023}
}
@inproceedings{faw2022power,
  title={The power of adaptivity in {SGD}: self-tuning step sizes with unbounded gradients and affine variance},
  author={Faw, Matthew and Tziotis, Isidoros and Caramanis, Constantine and Mokhtari, Aryan and Shakkottai, Sanjay and Ward, Rachel},
  booktitle={Conference on Learning Theory},
  year={2022},
}
@inproceedings{zou2019sufficient,
  title={A sufficient condition for convergences of {A}dam and {RMSP}rop},
  author={Zou, Fangyu and Shen, Li and Jie, Zequn and Zhang, Weizhong and Liu, Wei},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2019}
}
@article{shen2023unified,
  title={A Unified Analysis of {AdaGrad} With Weighted Aggregation and Momentum Acceleration},
  author={Shen, Li and Chen, Congliang and Zou, Fangyu and Jie, Zequn and Sun, Ju and Liu, Wei},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2023},
  publisher={IEEE}
}
@article{lan2012optimal,
  title={An optimal method for stochastic composite optimization},
  author={Lan, Guanghui},
  journal={Mathematical Programming},
  year={2012},
}
@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}
@article{polyak1964some,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={Ussr Computational Mathematics and Mathematical Physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}
@inproceedings{zhang2020adaptive,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
}
@article{nguyen2023high,
  title={High probability convergence of clipped-SGD under heavy-tailed noise},
  author={Nguyen, Ta Duy and Nguyen, Thien Hang and Ene, Alina and Nguyen, Huy Le},
  journal={arXiv preprint arXiv:2302.05437},
  year={2023}
}
@article{sadiev2023high,
  title={High-probability bounds for stochastic optimization and variational inequalities: the case of unbounded variance},
  author={Sadiev, Abdurakhmon and Danilova, Marina and Gorbunov, Eduard and Horv{\'a}th, Samuel and Gidel, Gauthier and Dvurechensky, Pavel and Gasnikov, Alexander and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2302.00999},
  year={2023}
}
@inproceedings{cutkosky2021high,
  title={High-probability bounds for non-convex stochastic optimization with heavy tails},
  author={Cutkosky, Ashok and Mehta, Harsh},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4883--4895},
  year={2021}
}
@inproceedings{rakhlinconvergence,
  title={Convergence of {Adam} under relaxed assumptions},
  author={Li, Haochuan and Jadbabaie, Ali and Rakhlin, Alexander},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}
@inproceedings{
    loshchilov2018decoupled,
    title={Decoupled weight decay regularization},
    author={Ilya Loshchilov and Frank Hutter},
    booktitle={International Conference on Learning Representations},
    year={2019}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2016}
}
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2009},
}
@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International Conference on Machine Learning},
  year={2016},
}
@article{de2018convergence,
  title={Convergence guarantees for {RMSP}rop and {Adam} in non-convex optimization and an empirical comparison to {N}esterov acceleration},
  author={De, Soham and Mukherjee, Anirbit and Ullah, Enayat},
  journal={arXiv preprint arXiv:1807.06766},
  year={2018}
}
@inproceedings{zhang2022adam,
  title={Adam can converge without any modification on update rules},
  author={Zhang, Yushun and Chen, Congliang and Shi, Naichen and Sun, Ruoyu and Luo, Zhi-Quan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}
@article{wang2022provable,
  title={Provable adaptivity in {A}dam},
  author={Wang, Bohan and Zhang, Yushun and Zhang, Huishuai and Meng, Qi and Ma, Zhi-Ming and Liu, Tie-Yan and Chen, Wei},
  journal={arXiv preprint arXiv:2208.09900},
  year={2022}
}
@inproceedings{wang2023convergence,
  title={Convergence of {AdaGrad} for non-convex objectives: simple proofs and relaxed assumptions},
  author={Wang, Bohan and Zhang, Huishuai and Ma, Zhiming and Chen, Wei},
  booktitle={Conference on Learning Theory},
  year={2023},
}
@inproceedings{shi2020rmsprop,
  title={{RMSProp} converges with proper hyper-parameter},
  author={Shi, Naichen and Li, Dawei and Hong, Mingyi and Sun, Ruoyu},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@inproceedings{liu2023on,
title={On the Convergence of AdaGrad(Norm) on $\mathbb{R}^d$: Beyond Convexity, Non-Asymptotic Rate and Acceleration},
author={Zijian Liu and Ta Duy Nguyen and Alina Ene and Huy Nguyen},
booktitle={International Conference on Learning Representations },
year={2023}
}
@inproceedings{faw2023beyond,
  title={Beyond uniform smoothness: a stopped analysis of adaptive {SGD}},
  author={Faw, Matthew and Rout, Litu and Caramanis, Constantine and Shakkottai, Sanjay},
  booktitle={Conference on Learning Theory},
  year={2023}
}
@inproceedings{ghadimi2015global,
  title={Global convergence of the heavy-ball method for convex optimization},
  author={Ghadimi, Euhanna and Feyzmahdavian, Hamid Reza and Johansson, Mikael},
  booktitle={European Control Conference},
  year={2015},
}
@inproceedings{yang2022nest,
  title={Nest your adaptive algorithm for parameter-agnostic nonconvex minimax optimization},
  author={Yang, Junchi and Li, Xiang and He, Niao},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}
@inproceedings{huang2021super,
  title={Super-{Adam}: faster and universal framework of adaptive gradients},
  author={Huang, Feihu and Li, Junyi and Huang, Heng},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{arjevani2023lower,
	title={Lower bounds for non-convex stochastic optimization},
	author={Arjevani, Yossi and Carmon, Yair and Duchi, John C and Foster, Dylan J and Srebro, Nathan and Woodworth, Blake},
	journal={Mathematical Programming},
	volume={199},
	number={1-2},
	pages={165--214},
	year={2023},
	publisher={Springer}
}
@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}
@inproceedings{hongrevisiting,
  title={Revisiting Convergence of AdaGrad with Relaxed Assumptions},
  author={Hong, Yusu and Lin, Junhong},
  booktitle={The 40th Conference on Uncertainty in Artificial Intelligence}
}
@article{yusu2023,
  title={High Probability Convergence of {Adam} Under Unbounded Gradients and Affine Variance Noise},
  author={Hong, Yusu and Lin, Junhong},
  journal={arXiv preprint arXiv:2311.02000},
  year={2023}
}
@inproceedings{gorbunov2020stochastic,
  title={Stochastic optimization with heavy-tailed noise via accelerated gradient clipping},
  author={Gorbunov, Eduard and Danilova, Marina and Gasnikov, Alexander},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}
@inproceedings{simsekli2019tail,
  title={A tail-index analysis of stochastic gradient noise in deep neural networks},
  author={Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  booktitle={International Conference on Machine Learning},
  year={2019}
}
@inproceedings{gurbuzbalaban2021heavy,
  title={The heavy-tail phenomenon in SGD},
  author={Gurbuzbalaban, Mert and Simsekli, Umut and Zhu, Lingjiong},
  booktitle={International Conference on Machine Learning},
  pages={3964--3975},
  year={2021},
  organization={PMLR}
}
@inproceedings{nguyen2019first,
  title={First exit time analysis of stochastic gradient descent under heavy-tailed gradient noise},
  author={Nguyen, Thanh Huy and Simsekli, Umut and Gurbuzbalaban, Mert and Richard, Ga{\"e}l},
  booktitle={Advances in neural information processing systems},
  year={2019}
}
@inproceedings{li2022high,
  title={High probability guarantees for nonconvex stochastic gradient descent with heavy tails},
  author={Li, Shaojie and Liu, Yong},
  booktitle={International Conference on Machine Learning},
  year={2022}
}
@misc{wang2024convergence,
      title={On the Convergence of Adam under Non-uniform Smoothness: Separability from SGDM and Beyond}, 
      author={Bohan Wang and Huishuai Zhang and Qi Meng and Ruoyu Sun and Zhi-Ming Ma and Wei Chen},
      year={2024},
      eprint={2403.15146},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}