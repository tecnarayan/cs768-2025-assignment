\begin{thebibliography}{}

\bibitem[Abbott et~al., 2016]{abbott2016building}
Abbott, L.~F., DePasquale, B., and Memmesheimer, R.-M. (2016).
\newblock Building functional networks of spiking model neurons.
\newblock {\em Nature neuroscience}, 19(3):350.

\bibitem[Akrout et~al., 2019]{akrout2019deep}
Akrout, M., Wilson, C., Humphreys, P., Lillicrap, T., and Tweed, D.~B. (2019).
\newblock Deep learning without weight transport.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  974--982.

\bibitem[{\AA}str{\"o}m and Murray, 2010]{aastrom2010feedback}
{\AA}str{\"o}m, K.~J. and Murray, R.~M. (2010).
\newblock {\em Feedback systems: an introduction for scientists and engineers}.
\newblock Princeton university press.

\bibitem[Bartunov et~al., 2018]{bartunov2018assessing}
Bartunov, S., Santoro, A., Richards, B., Marris, L., Hinton, G.~E., and
  Lillicrap, T. (2018).
\newblock Assessing the scalability of biologically-motivated deep learning
  algorithms and architectures.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9368--9378.

\bibitem[Bengio, 2014]{bengio2014auto}
Bengio, Y. (2014).
\newblock How auto-encoders could provide credit assignment in deep networks
  via target propagation.
\newblock {\em arXiv preprint arXiv:1407.7906}.

\bibitem[Bengio, 2020]{bengio2020deriving}
Bengio, Y. (2020).
\newblock Deriving differential target propagation from iterating approximate
  inverses.
\newblock {\em arXiv preprint arXiv:2007.15139}.

\bibitem[Boerlin et~al., 2013]{boerlin2013predictive}
Boerlin, M., Machens, C.~K., and Den{\`e}ve, S. (2013).
\newblock Predictive coding of dynamical variables in balanced spiking
  networks.
\newblock {\em PLoS computational biology}, 9(11).

\bibitem[Botev et~al., 2017]{botev2017practical}
Botev, A., Ritter, H., and Barber, D. (2017).
\newblock Practical gauss-newton optimisation for deep learning.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 557--565. JMLR. org.

\bibitem[Crick, 1989]{crick1989recent}
Crick, F. (1989).
\newblock The recent excitement about neural networks.
\newblock {\em Nature}, 337(6203):129--132.

\bibitem[Dauphin et~al., 2014]{dauphin2014identifying}
Dauphin, Y.~N., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., and Bengio, Y.
  (2014).
\newblock Identifying and attacking the saddle point problem in
  high-dimensional non-convex optimization.
\newblock In {\em Advances in neural information processing systems}, pages
  2933--2941.

\bibitem[Dayan and Abbott, 2001]{dayan2001theoretical}
Dayan, P. and Abbott, L.~F. (2001).
\newblock {\em Theoretical neuroscience: computational and mathematical
  modeling of neural systems}.
\newblock MIT press.

\bibitem[Desjardins et~al., 2015]{desjardins2015natural}
Desjardins, G., Simonyan, K., Pascanu, R., et~al. (2015).
\newblock Natural neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2071--2079.

\bibitem[Eliasmith and Anderson, 2004]{eliasmith2004neural}
Eliasmith, C. and Anderson, C.~H. (2004).
\newblock {\em Neural engineering: Computation, representation, and dynamics in
  neurobiological systems}.
\newblock MIT press.

\bibitem[Felleman and Van~Essen, 1991]{felleman1991distributed}
Felleman, D.~J. and Van~Essen, D. (1991).
\newblock Distributed hierarchical processing in the primate cerebral cortex.
\newblock {\em Cerebral cortex (New York, NY: 1991)}, 1(1):1--47.

\bibitem[Gilbert and Li, 2013]{gilbert2013top}
Gilbert, C.~D. and Li, W. (2013).
\newblock Top-down influences on visual processing.
\newblock {\em Nature Reviews Neuroscience}, 14(5):350--363.

\bibitem[Grossberg, 1987]{grossberg1987competitive}
Grossberg, S. (1987).
\newblock Competitive learning: From interactive activation to adaptive
  resonance.
\newblock {\em Cognitive science}, 11(1):23--63.

\bibitem[Guerguiev et~al., 2017]{guerguiev2017towards}
Guerguiev, J., Lillicrap, T.~P., and Richards, B.~A. (2017).
\newblock Towards deep learning with segregated dendrites.
\newblock {\em Elife}, 6:e22901.

\bibitem[Hennequin et~al., 2014]{hennequin2014optimal}
Hennequin, G., Vogels, T.~P., and Gerstner, W. (2014).
\newblock Optimal control of transient dynamics in balanced networks supports
  generation of complex movements.
\newblock {\em Neuron}, 82(6):1394--1406.

\bibitem[Hinton et~al., 1995]{hinton1995wake}
Hinton, G.~E., Dayan, P., Frey, B.~J., and Neal, R.~M. (1995).
\newblock The" wake-sleep" algorithm for unsupervised neural networks.
\newblock {\em Science}, 268(5214):1158--1161.

\bibitem[Hinton and Salakhutdinov, 2006]{hinton2006reducing}
Hinton, G.~E. and Salakhutdinov, R.~R. (2006).
\newblock Reducing the dimensionality of data with neural networks.
\newblock {\em science}, 313(5786):504--507.

\bibitem[Joglekar et~al., 2018]{joglekar2018inter}
Joglekar, M.~R., Mejias, J.~F., Yang, G.~R., and Wang, X.-J. (2018).
\newblock Inter-areal balanced amplification enhances signal propagation in a
  large-scale circuit model of the primate cortex.
\newblock {\em Neuron}, 98(1):222--234.

\bibitem[Jordan, 1996]{jordan1996computational}
Jordan, M.~I. (1996).
\newblock Computational aspects of motor control and motor learning.
\newblock In {\em Handbook of perception and action}, volume~2, pages 71--120.
  Elsevier.

\bibitem[Kawato, 1990]{kawato1990feedback}
Kawato, M. (1990).
\newblock Feedback-error-learning neural network for supervised motor learning.
\newblock In {\em Advanced neural computers}, pages 365--372. Elsevier.

\bibitem[Keck et~al., 2017]{keck2017integrating}
Keck, T., Toyoizumi, T., Chen, L., Doiron, B., Feldman, D.~E., Fox, K.,
  Gerstner, W., Haydon, P.~G., H{\"u}bener, M., Lee, H.-K., et~al. (2017).
\newblock Integrating hebbian and homeostatic plasticity: the current state of
  the field and future research directions.
\newblock {\em Philosophical Transactions of the Royal Society B: Biological
  Sciences}, 372(1715):20160158.

\bibitem[Kohan et~al., 2018]{kohan2018error}
Kohan, A.~A., Rietman, E.~A., and Siegelmann, H.~T. (2018).
\newblock Error forward-propagation: Reusing feedforward connections to
  propagate errors in deep learning.
\newblock {\em arXiv preprint arXiv:1808.03357}.

\bibitem[Kolen and Pollack, 1994]{kolen1994backpropagation}
Kolen, J.~F. and Pollack, J.~B. (1994).
\newblock Backpropagation without weight transport.
\newblock In {\em Proceedings of 1994 IEEE International Conference on Neural
  Networks (ICNN'94)}, volume~3, pages 1375--1380. IEEE.

\bibitem[Kunstner et~al., 2019]{kunstner2019limitations}
Kunstner, F., Balles, L., and Hennig, P. (2019).
\newblock Limitations of the empirical fisher approximation.
\newblock {\em arXiv preprint arXiv:1905.12558}.

\bibitem[Lansdell et~al., 2019]{lansdell2019learning}
Lansdell, B.~J., Prakash, P.~R., and Kording, K.~P. (2019).
\newblock Learning to solve the credit assignment problem.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[LeCun et~al., 2015]{lecun2015deep}
LeCun, Y., Bengio, Y., and Hinton, G. (2015).
\newblock Deep learning.
\newblock {\em nature}, 521(7553):436--444.

\bibitem[LeCun et~al., 1998]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998).
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324.

\bibitem[Lee et~al., 2015]{lee2015difference}
Lee, D.-H., Zhang, S., Fischer, A., and Bengio, Y. (2015).
\newblock Difference target propagation.
\newblock In {\em Joint european conference on machine learning and knowledge
  discovery in databases}, pages 498--515. Springer.

\bibitem[Lillicrap et~al., 2016]{lillicrap2016random}
Lillicrap, T.~P., Cownden, D., Tweed, D.~B., and Akerman, C.~J. (2016).
\newblock Random synaptic feedback weights support error backpropagation for
  deep learning.
\newblock {\em Nature communications}, 7(1):1--10.

\bibitem[Lillicrap et~al., 2020]{lillicrap2020backpropagation}
Lillicrap, T.~P., Santoro, A., Marris, L., Akerman, C.~J., and Hinton, G.
  (2020).
\newblock Backpropagation and the brain.
\newblock {\em Nature Reviews Neuroscience}, pages 1--12.

\bibitem[Martens, 2010]{martens2010deep}
Martens, J. (2010).
\newblock Deep learning via hessian-free optimization.
\newblock In {\em ICML}, volume~27, pages 735--742.

\bibitem[Martens, 2014]{martens2014new}
Martens, J. (2014).
\newblock New insights and perspectives on the natural gradient method.
\newblock {\em arXiv preprint arXiv:1412.1193}.

\bibitem[Meulemans et~al., 2020]{meulemans2020theoretical}
Meulemans, A., Carzaniga, F.~S., Suykens, J.~A., Sacramento, J., and Grewe,
  B.~F. (2020).
\newblock A theoretical framework for target propagation.
\newblock {\em arXiv preprint arXiv:2006.14331}.

\bibitem[Miller and Wang, 2006]{miller2006inhibitory}
Miller, P. and Wang, X.-J. (2006).
\newblock Inhibitory control by an integral feedback signal in prefrontal
  cortex: a model of discrimination between sequential stimuli.
\newblock {\em Proceedings of the National Academy of Sciences},
  103(1):201--206.

\bibitem[Moskovitz et~al., 2018]{moskovitz2018feedback}
Moskovitz, T.~H., Litwin-Kumar, A., and Abbott, L. (2018).
\newblock Feedback alignment in deep convolutional networks.
\newblock {\em arXiv preprint arXiv:1812.06488}.

\bibitem[Mueller and Siltanen, 2012]{mueller2012linear}
Mueller, J.~L. and Siltanen, S. (2012).
\newblock {\em Linear and nonlinear inverse problems with practical
  applications}.
\newblock SIAM.

\bibitem[Murray et~al., 2014]{murray2014hierarchy}
Murray, J.~D., Bernacchia, A., Freedman, D.~J., Romo, R., Wallis, J.~D., Cai,
  X., Padoa-Schioppa, C., Pasternak, T., Seo, H., Lee, D., et~al. (2014).
\newblock A hierarchy of intrinsic timescales across primate cortex.
\newblock {\em Nature neuroscience}, 17(12):1661.

\bibitem[N{\o}kland, 2016]{nokland2016direct}
N{\o}kland, A. (2016).
\newblock Direct feedback alignment provides learning in deep neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1037--1045.

\bibitem[O'Reilly, 1996]{o1996biologically}
O'Reilly, R.~C. (1996).
\newblock Biologically plausible error-driven learning using local activation
  differences: The generalized recirculation algorithm.
\newblock {\em Neural computation}, 8(5):895--938.

\bibitem[Payeur et~al., 2020]{payeur2020burst}
Payeur, A., Guerguiev, J., Zenke, F., Richards, B., and Naud, R. (2020).
\newblock Burst-dependent synaptic plasticity can coordinate learning in
  hierarchical circuits.
\newblock {\em bioRxiv}.

\bibitem[Pearlmutter, 1994]{pearlmutter1994fast}
Pearlmutter, B.~A. (1994).
\newblock Fast exact multiplication by the hessian.
\newblock {\em Neural computation}, 6(1):147--160.

\bibitem[Pizlo, 2001]{pizlo2001perception}
Pizlo, Z. (2001).
\newblock Perception viewed as an inverse problem.
\newblock {\em Vision research}, 41(24):3145--3161.

\bibitem[Rao and Ballard, 1999]{rao1999predictive}
Rao, R.~P. and Ballard, D.~H. (1999).
\newblock Predictive coding in the visual cortex: a functional interpretation
  of some extra-classical receptive-field effects.
\newblock {\em Nature neuroscience}, 2(1):79--87.

\bibitem[Roelfsema and Holtmaat, 2018]{roelfsema2018control}
Roelfsema, P.~R. and Holtmaat, A. (2018).
\newblock Control of synaptic plasticity in deep cortical networks.
\newblock {\em Nature Reviews Neuroscience}, 19(3):166.

\bibitem[Rumelhart et~al., 1986]{rumelhart1986learning}
Rumelhart, D.~E., Hinton, G.~E., and Williams, R.~J. (1986).
\newblock Learning representations by back-propagating errors.
\newblock {\em nature}, 323(6088):533--536.

\bibitem[Sacramento et~al., 2018]{sacramento2018dendritic}
Sacramento, J., Costa, R.~P., Bengio, Y., and Senn, W. (2018).
\newblock Dendritic cortical microcircuits approximate the backpropagation
  algorithm.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8721--8732.

\bibitem[Saxe et~al., 2013]{saxe2013exact}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S. (2013).
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock {\em arXiv preprint arXiv:1312.6120}.

\bibitem[Scellier and Bengio, 2017]{scellier2017equilibrium}
Scellier, B. and Bengio, Y. (2017).
\newblock Equilibrium propagation: Bridging the gap between energy-based models
  and backpropagation.
\newblock {\em Frontiers in computational neuroscience}, 11:24.

\bibitem[Schraudolph, 2002]{schraudolph2002fast}
Schraudolph, N.~N. (2002).
\newblock Fast curvature matrix-vector products for second-order gradient
  descent.
\newblock {\em Neural computation}, 14(7):1723--1738.

\bibitem[Slotine et~al., 1991]{slotine1991applied}
Slotine, J.-J.~E., Li, W., et~al. (1991).
\newblock {\em Applied nonlinear control}, volume 199.
\newblock Prentice hall Englewood Cliffs, NJ.

\bibitem[Somvanshi et~al., 2015]{somvanshi2015implementation}
Somvanshi, P.~R., Patel, A.~K., Bhartiya, S., and Venkatesh, K. (2015).
\newblock Implementation of integral feedback control in biological systems.
\newblock {\em Wiley Interdisciplinary Reviews: Systems Biology and Medicine},
  7(5):301--316.

\bibitem[Vanbiervliet et~al., 2009]{vanbiervliet2009smoothed}
Vanbiervliet, J., Vandereycken, B., Michiels, W., Vandewalle, S., and Diehl, M.
  (2009).
\newblock The smoothed spectral abscissa for robust stability optimization.
\newblock {\em SIAM Journal on Optimization}, 20(1):156--171.

\bibitem[V{\'e}rtes and Sahani, 2019]{vertes2019neurally}
V{\'e}rtes, E. and Sahani, M. (2019).
\newblock A neurally plausible model learns successor representations in
  partially observable environments.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  13692--13702.

\bibitem[Whittington and Bogacz, 2017]{whittington2017approximation}
Whittington, J.~C. and Bogacz, R. (2017).
\newblock An approximation of the error backpropagation algorithm in a
  predictive coding network with local hebbian synaptic plasticity.
\newblock {\em Neural computation}, 29(5):1229--1262.

\bibitem[Whittington and Bogacz, 2019]{whittington2019theories}
Whittington, J.~C. and Bogacz, R. (2019).
\newblock Theories of error back-propagation in the brain.
\newblock {\em Trends in cognitive sciences}.

\bibitem[Xiao et~al., 2018]{xiao2018biologically}
Xiao, W., Chen, H., Liao, Q., and Poggio, T. (2018).
\newblock Biologically-plausible learning algorithms can scale to large
  datasets.
\newblock {\em arXiv preprint arXiv:1811.03567}.

\bibitem[Zador, 2019]{zador2019critique}
Zador, A.~M. (2019).
\newblock A critique of pure learning and what artificial neural networks can
  learn from animal brains.
\newblock {\em Nature communications}, 10(1):1--7.

\end{thebibliography}
