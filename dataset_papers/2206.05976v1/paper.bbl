\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Asuncion \& Newman(2007)Asuncion and Newman]{asuncion2007uci}
Asuncion, A. and Newman, D.
\newblock {UCI} machine learning repository, 2007.

\bibitem[Attouch \& Bolte(2009)Attouch and Bolte]{attouch2009convergence}
Attouch, H. and Bolte, J.
\newblock On the convergence of the proximal algorithm for nonsmooth functions
  involving analytic features.
\newblock \emph{Mathematical Programming}, 116\penalty0 (1):\penalty0 5--16,
  2009.

\bibitem[Attouch et~al.(2010)Attouch, Bolte, Redont, and
  Soubeyran]{attouch2010proximal}
Attouch, H., Bolte, J., Redont, P., and Soubeyran, A.
\newblock Proximal alternating minimization and projection methods for
  nonconvex problems: An approach based on the kurdyka-{\l}ojasiewicz
  inequality.
\newblock \emph{Mathematics of Operations Research}, 35\penalty0 (2):\penalty0
  438--457, 2010.

\bibitem[Attouch et~al.(2013)Attouch, Bolte, and
  Svaiter]{attouch2013convergence}
Attouch, H., Bolte, J., and Svaiter, B.~F.
\newblock Convergence of descent methods for semi-algebraic and tame problems:
  proximal algorithms, forward--backward splitting, and regularized
  gauss--seidel methods.
\newblock \emph{Mathematical Programming}, 137\penalty0 (1):\penalty0 91--129,
  2013.

\bibitem[Bergstra et~al.(2013)Bergstra, Yamins, and Cox]{bergstra2013making}
Bergstra, J., Yamins, D., and Cox, D.
\newblock Making a science of model search: Hyperparameter optimization in
  hundreds of dimensions for vision architectures.
\newblock In \emph{ICML}, pp.\  115--123. PMLR, 2013.

\bibitem[Bertrand et~al.(2020)Bertrand, Klopfenstein, Blondel, Vaiter,
  Gramfort, and Salmon]{bertrand2020implicit}
Bertrand, Q., Klopfenstein, Q., Blondel, M., Vaiter, S., Gramfort, A., and
  Salmon, J.
\newblock Implicit differentiation of lasso-type models for hyperparameter
  optimization.
\newblock In \emph{ICML}, pp.\  810--821. PMLR, 2020.

\bibitem[Bertrand et~al.(2021)Bertrand, Klopfenstein, Massias, Blondel, Vaiter,
  Gramfort, and Salmon]{bertrand2021implicit}
Bertrand, Q., Klopfenstein, Q., Massias, M., Blondel, M., Vaiter, S., Gramfort,
  A., and Salmon, J.
\newblock Implicit differentiation for fast hyperparameter selection in
  non-smooth convex learning.
\newblock \emph{arXiv preprint arXiv:2105.01637}, 2021.

\bibitem[Bolte et~al.(2007{\natexlab{a}})Bolte, Daniilidis, and
  Lewis]{bolte2007lojasiewicz}
Bolte, J., Daniilidis, A., and Lewis, A.
\newblock The {\l}ojasiewicz inequality for nonsmooth subanalytic functions
  with applications to subgradient dynamical systems.
\newblock \emph{SIAM Journal on Optimization}, 17\penalty0 (4):\penalty0
  1205--1223, 2007{\natexlab{a}}.

\bibitem[Bolte et~al.(2007{\natexlab{b}})Bolte, Daniilidis, Lewis, and
  Shiota]{bolte2007clarke}
Bolte, J., Daniilidis, A., Lewis, A., and Shiota, M.
\newblock Clarke subgradients of stratifiable functions.
\newblock \emph{SIAM Journal on Optimization}, 18\penalty0 (2):\penalty0
  556--572, 2007{\natexlab{b}}.

\bibitem[Bolte et~al.(2010)Bolte, Daniilidis, Ley, and
  Mazet]{bolte2010characterizations}
Bolte, J., Daniilidis, A., Ley, O., and Mazet, L.
\newblock Characterizations of {\l}ojasiewicz inequalities: subgradient flows,
  talweg, convexity.
\newblock \emph{Transactions of the American Mathematical Society},
  362\penalty0 (6):\penalty0 3319--3363, 2010.

\bibitem[Bolte et~al.(2014)Bolte, Sabach, and Teboulle]{bolte2014proximal}
Bolte, J., Sabach, S., and Teboulle, M.
\newblock Proximal alternating linearized minimization for nonconvex and
  nonsmooth problems.
\newblock \emph{Mathematical Programming}, 146\penalty0 (1):\penalty0 459--494,
  2014.

\bibitem[Chang \& Lin(2011)Chang and Lin]{libsvm}
Chang, C.-C. and Lin, C.-J.
\newblock Libsvm: a library for support vector machines.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology (TIST)},
  2\penalty0 (3):\penalty0 1--27, 2011.

\bibitem[Clarke(1990)]{clarke1990optimization}
Clarke, F.~H.
\newblock \emph{Optimization and nonsmooth analysis}.
\newblock SIAM, 1990.

\bibitem[Cortes \& Vapnik(1995)Cortes and Vapnik]{cortes1995support}
Cortes, C. and Vapnik, V.
\newblock Support-vector networks.
\newblock \emph{Machine learning}, 20\penalty0 (3):\penalty0 273--297, 1995.

\bibitem[Duarte \& Hu(2004)Duarte and Hu]{duarte2004vehicle}
Duarte, M.~F. and Hu, Y.~H.
\newblock Vehicle classification in distributed sensor networks.
\newblock \emph{Journal of Parallel and Distributed Computing}, 64\penalty0
  (7):\penalty0 826--838, 2004.

\bibitem[Feng \& Simon(2018)Feng and Simon]{feng2018gradient}
Feng, J. and Simon, N.
\newblock Gradient-based regularization parameter selection for problems with
  nonsmooth penalty functions.
\newblock \emph{Journal of Computational and Graphical Statistics}, 27\penalty0
  (2):\penalty0 426--435, 2018.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{ICML}, pp.\  1126--1135. PMLR, 2017.

\bibitem[Franceschi et~al.(2017)Franceschi, Donini, Frasconi, and
  Pontil]{franceschi2017forward}
Franceschi, L., Donini, M., Frasconi, P., and Pontil, M.
\newblock Forward and reverse gradient-based hyperparameter optimization.
\newblock In \emph{ICML}, pp.\  1165--1173. PMLR, 2017.

\bibitem[Franceschi et~al.(2018)Franceschi, Frasconi, Salzo, Grazzi, and
  Pontil]{franceschi2018bilevel}
Franceschi, L., Frasconi, P., Salzo, S., Grazzi, R., and Pontil, M.
\newblock Bilevel programming for hyperparameter optimization and
  meta-learning.
\newblock In \emph{ICML}, pp.\  1563--1572. PMLR, 2018.

\bibitem[Grazzi et~al.(2020)Grazzi, Franceschi, Pontil, and
  Salzo]{grazzi2020iteration}
Grazzi, R., Franceschi, L., Pontil, M., and Salzo, S.
\newblock On the iteration complexity of hypergradient computation.
\newblock In \emph{ICML}, pp.\  3748--3758. PMLR, 2020.

\bibitem[Guyon et~al.(2004)Guyon, Gunn, Ben-Hur, and Dror]{guyon2004result}
Guyon, I., Gunn, S., Ben-Hur, A., and Dror, G.
\newblock Result analysis of the nips 2003 feature selection challenge.
\newblock \emph{NeurIPS}, 17, 2004.

\bibitem[Ji \& Liang(2021)Ji and Liang]{Ji2021}
Ji, K. and Liang, Y.
\newblock Lower bounds and accelerated algorithms for bilevel optimization.
\newblock \emph{arXiv preprint arXiv:2102.03926v2}, 2021.

\bibitem[Ji et~al.(2020{\natexlab{a}})Ji, Lee, Liang, and
  Poor]{ji2020convergence}
Ji, K., Lee, J.~D., Liang, Y., and Poor, H.~V.
\newblock Convergence of meta-learning with task-specific adaptation over
  partial parameters.
\newblock \emph{NeurIPS}, 33:\penalty0 11490--11500, 2020{\natexlab{a}}.

\bibitem[Ji et~al.(2020{\natexlab{b}})Ji, Yang, and Liang]{Ji2020}
Ji, K., Yang, J., and Liang, Y.
\newblock Bilevel optimization: Nonasymptotic analysis and faster algorithms.
\newblock \emph{arXiv preprint arXiv:2010.07962v2}, 2020{\natexlab{b}}.

\bibitem[Ji et~al.(2021)Ji, Yang, and Liang]{ji2021bilevel}
Ji, K., Yang, J., and Liang, Y.
\newblock Bilevel optimization: Convergence analysis and enhanced design.
\newblock In \emph{ICML}, pp.\  4882--4892. PMLR, 2021.

\bibitem[Ji et~al.(2022)Ji, Liu, Liang, and Ying]{ji2022will}
Ji, K., Liu, M., Liang, Y., and Ying, L.
\newblock Will bilevel optimizers benefit from loops.
\newblock \emph{arXiv preprint arXiv:2205.14224}, 2022.

\bibitem[Kunapuli(2008)]{kunapuli2008bilevel}
Kunapuli, G.
\newblock \emph{A bilevel optimization approach to machine learning}.
\newblock Rensselaer Polytechnic Institute, 2008.

\bibitem[Kunapuli et~al.(2008)Kunapuli, Bennett, Hu, and
  Pang]{kunapuli2008classification}
Kunapuli, G., Bennett, K.~P., Hu, J., and Pang, J.-S.
\newblock Classification model selection via bilevel programming.
\newblock \emph{Optimization Methods \& Software}, 23\penalty0 (4):\penalty0
  475--489, 2008.

\bibitem[Liu et~al.(2019{\natexlab{a}})Liu, Simonyan, and Yang]{liu2018darts}
Liu, H., Simonyan, K., and Yang, Y.
\newblock Darts: Differentiable architecture search.
\newblock In \emph{ICLR}, 2019{\natexlab{a}}.

\bibitem[Liu et~al.(2020)Liu, Mu, Yuan, Zeng, and Zhang]{liu2020generic}
Liu, R., Mu, P., Yuan, X., Zeng, S., and Zhang, J.
\newblock A generic first-order algorithmic framework for bi-level programming
  beyond lower-level singleton.
\newblock In \emph{ICML}, pp.\  6305--6315. PMLR, 2020.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Liu, Yuan, Zeng, and
  Zhang]{liu2021value}
Liu, R., Liu, X., Yuan, X., Zeng, S., and Zhang, J.
\newblock A value-function-based interior-point method for non-convex bi-level
  optimization.
\newblock In \emph{ICML}, pp.\  6882--6892. PMLR, 2021{\natexlab{a}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Liu, Zeng, and
  Zhang]{Liu2021TowardsGB}
Liu, R., Liu, Y., Zeng, S., and Zhang, J.
\newblock Towards gradient-based bilevel optimization with non-convex followers
  and beyond.
\newblock \emph{NeurIPS}, 34, 2021{\natexlab{b}}.

\bibitem[Liu et~al.(2022)Liu, Mu, Yuan, Zeng, and Zhang]{liu2022general}
Liu, R., Mu, P., Yuan, X., Zeng, S., and Zhang, J.
\newblock A general descent aggregation framework for gradient-based bi-level
  optimization.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2022.

\bibitem[Liu et~al.(2019{\natexlab{b}})Liu, Pong, and Takeda]{liu2019refined}
Liu, T., Pong, T.~K., and Takeda, A.
\newblock A refined convergence analysis of $\hbox {pDCA} _ {e} $ pdca e with
  applications to simultaneous sparse recovery and outlier detection.
\newblock \emph{Computational Optimization and Applications}, 73\penalty0
  (1):\penalty0 69--100, 2019{\natexlab{b}}.

\bibitem[Lorraine \& Duvenaud(2018)Lorraine and
  Duvenaud]{lorraine2018stochastic}
Lorraine, J. and Duvenaud, D.
\newblock Stochastic hyperparameter optimization through hypernetworks.
\newblock \emph{CoRR, abs/1802.09419}, 2018.

\bibitem[Lorraine et~al.(2020)Lorraine, Vicol, and
  Duvenaud]{lorraine2020optimizing}
Lorraine, J., Vicol, P., and Duvenaud, D.
\newblock Optimizing millions of hyperparameters by implicit differentiation.
\newblock In \emph{AISTATS}, pp.\  1540--1552. PMLR, 2020.

\bibitem[Nichol et~al.(2018)Nichol, Achiam, and Schulman]{nichol2018first}
Nichol, A., Achiam, J., and Schulman, J.
\newblock On first-order meta-learning algorithms.
\newblock \emph{CoRR, abs/1803.02999}, 2018.

\bibitem[Pedregosa(2016)]{pedregosa2016hyperparameter}
Pedregosa, F.
\newblock Hyperparameter optimization with approximate gradient.
\newblock In \emph{ICML}, pp.\  737--746. PMLR, 2016.

\bibitem[Platt(1999)]{Platt1999FastTO}
Platt, J.~C.
\newblock Fast training of support vector machines using sequential minimal
  optimization, advances in kernel methods.
\newblock 1999.

\bibitem[Rajeswaran et~al.(2019)Rajeswaran, Finn, Kakade, and
  Levine]{rajeswaran2019meta}
Rajeswaran, A., Finn, C., Kakade, S.~M., and Levine, S.
\newblock Meta-learning with implicit gradients.
\newblock \emph{NeurIPS}, 32, 2019.

\bibitem[Rockafellar(2015)]{rockafellar2015convex}
Rockafellar, R.~T.
\newblock \emph{Convex analysis}.
\newblock Princeton university press, 2015.

\bibitem[Shaban et~al.(2019)Shaban, Cheng, Hatch, and
  Boots]{shaban2019truncated}
Shaban, A., Cheng, C., Hatch, N., and Boots, B.
\newblock Truncated back-propagation for bilevel optimization.
\newblock In \emph{AISTATS}, 2019.

\bibitem[Simon et~al.(2013)Simon, Friedman, Hastie, and
  Tibshirani]{simon2013sparse}
Simon, N., Friedman, J., Hastie, T., and Tibshirani, R.
\newblock A sparse-group lasso.
\newblock \emph{Journal of Computational and Graphical Statistics}, 22\penalty0
  (2):\penalty0 231--245, 2013.

\bibitem[Sow et~al.(2022)Sow, Ji, Guan, and Liang]{sow2022constrained}
Sow, D., Ji, K., Guan, Z., and Liang, Y.
\newblock A constrained optimization approach to bilevel optimization with
  multiple inner minima.
\newblock \emph{arXiv preprint arXiv:2203.01123}, 2022.

\bibitem[West et~al.(2001)West, Blanchette, Dressman, Huang, Ishida, Spang,
  Zuzan, Olson, Marks, and Nevins]{west2001predicting}
West, M., Blanchette, C., Dressman, H., Huang, E., Ishida, S., Spang, R.,
  Zuzan, H., Olson, J.~A., Marks, J.~R., and Nevins, J.~R.
\newblock Predicting the clinical status of human breast cancer by using gene
  expression profiles.
\newblock \emph{Proceedings of the National Academy of Sciences}, 98\penalty0
  (20):\penalty0 11462--11467, 2001.

\bibitem[Ye et~al.(2021)Ye, Yuan, Zeng, and Zhang]{DCbilevel}
Ye, J.~J., Yuan, X., Zeng, S., and Zhang, J.
\newblock Difference of convex algorithms for bilevel programs with
  applications in hyperparameter selection.
\newblock \emph{arXiv preprint arXiv:2102.09006}, 2021.

\bibitem[Zou \& Hastie(2003)Zou and Hastie]{zou2003regression}
Zou, H. and Hastie, T.
\newblock Regression shrinkage and selection via the elastic net, with
  applications to microarrays.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 67:\penalty0 301--20, 2003.

\end{thebibliography}
