\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Antor{\'a}n et~al.(2022{\natexlab{a}})Antor{\'a}n, Barbano, Leuschner,
  Hern{\'a}ndez-Lobato, and Jin]{antoran2022probabilistic}
Javier Antor{\'a}n, Riccardo Barbano, Johannes Leuschner, Jos{\'e}~Miguel
  Hern{\'a}ndez-Lobato, and Bangti Jin.
\newblock A probabilistic deep image prior for computational tomography.
\newblock \emph{arXiv preprint arXiv:2203.00479}, 2022{\natexlab{a}}.

\bibitem[Antor{\'a}n et~al.(2022{\natexlab{b}})Antor{\'a}n, Janz, Allingham,
  Daxberger, Barbano, Nalisnick, and Hern{\'a}ndez-Lobato]{antoran2022adapting}
Javier Antor{\'a}n, David Janz, James~U Allingham, Erik Daxberger, Riccardo~Rb
  Barbano, Eric Nalisnick, and Jos{\'e}~Miguel Hern{\'a}ndez-Lobato.
\newblock Adapting the linearised laplace model evidence for modern deep
  learning.
\newblock In \emph{International Conference on Machine Learning},
  2022{\natexlab{b}}.

\bibitem[Batzner et~al.(2021)Batzner, Musaelian, Sun, Geiger, Mailoa,
  Kornbluth, Molinari, Smidt, and Kozinsky]{batzner2021se}
Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan~P Mailoa,
  Mordechai Kornbluth, Nicola Molinari, Tess~E Smidt, and Boris Kozinsky.
\newblock Se (3)-equivariant graph neural networks for data-efficient and
  accurate interatomic potentials.
\newblock \emph{arXiv preprint arXiv:2101.03164}, 2021.

\bibitem[Benton et~al.(2020)Benton, Finzi, Izmailov, and
  Wilson]{benton2020learning}
Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew~Gordon Wilson.
\newblock Learning invariances in neural networks.
\newblock \emph{arXiv preprint arXiv:2010.11882}, 2020.

\bibitem[Bishop(2006)]{bishop2006pattern}
Christopher~M Bishop.
\newblock \emph{Pattern recognition and machine learning}.
\newblock Information Science and Statistics. Springer, 2006.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell2015weight}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural networks.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning}, pages 1613--1622, 2015.

\bibitem[Botev et~al.(2017)Botev, Ritter, and Barber]{botev2017practical}
Aleksandar Botev, Hippolyt Ritter, and David Barber.
\newblock Practical {G}auss-{N}ewton optimisation for deep learning.
\newblock In \emph{International Conference on Machine Learning}, International
  Convention Centre, Sydney, Australia, 2017. PMLR.

\bibitem[Bottou(2010)]{bottou2010large}
L{\'e}on Bottou.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In \emph{Proceedings of COMPSTAT'2010}, pages 177--186. Springer,
  2010.

\bibitem[Brandstetter et~al.(2021)Brandstetter, Hesselink, van~der Pol,
  Bekkers, and Welling]{brandstetter2021geometric}
Johannes Brandstetter, Rob Hesselink, Elise van~der Pol, Erik Bekkers, and Max
  Welling.
\newblock Geometric and physical quantities improve e (3) equivariant message
  passing.
\newblock \emph{arXiv preprint arXiv:2110.02905}, 2021.

\bibitem[Cohen and Welling(2016)]{cohen2016group}
Taco Cohen and Max Welling.
\newblock Group equivariant convolutional networks.
\newblock In \emph{International conference on machine learning}, pages
  2990--2999. PMLR, 2016.

\bibitem[Cohen et~al.(2018)Cohen, Geiger, K{\"o}hler, and
  Welling]{cohen2018spherical}
Taco~S Cohen, Mario Geiger, Jonas K{\"o}hler, and Max Welling.
\newblock Spherical cnns.
\newblock \emph{arXiv preprint arXiv:1801.10130}, 2018.

\bibitem[Cubuk et~al.(2018)Cubuk, Zoph, Mane, Vasudevan, and
  Le]{cubuk2018autoaugment}
Ekin~D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V Le.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock \emph{arXiv preprint arXiv:1805.09501}, 2018.

\bibitem[Dangel et~al.(2019)Dangel, Kunstner, and Hennig]{dangel2019backpack}
Felix Dangel, Frederik Kunstner, and Philipp Hennig.
\newblock Backpack: Packing more into backprop.
\newblock In \emph{Proceedings of 7th International Conference on Learning
  Representations}, 2019.

\bibitem[Daxberger et~al.(2021)Daxberger, Kristiadi, Immer, Eschenhagen, Bauer,
  and Hennig]{daxberger2021laplace}
Erik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen,
  Matthias Bauer, and Philipp Hennig.
\newblock Laplace redux-effortless bayesian deep learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Dehaene(2019)]{dehaene2019deterministic}
Guillaume~P Dehaene.
\newblock A deterministic and computable bernstein-von mises theorem.
\newblock \emph{arXiv preprint arXiv:1904.02505}, 2019.

\bibitem[Fong and Holmes(2020)]{fong2020marginal}
Edwin Fong and CC~Holmes.
\newblock On the marginal likelihood and cross-validation.
\newblock \emph{Biometrika}, 107\penalty0 (2):\penalty0 489--496, 2020.

\bibitem[Foong et~al.(2019)Foong, Li, Hern{\'a}ndez-Lobato, and
  Turner]{foong2019between}
Andrew~YK Foong, Yingzhen Li, Jos{\'e}~Miguel Hern{\'a}ndez-Lobato, and
  Richard~E Turner.
\newblock 'in-between'uncertainty in bayesian neural networks.
\newblock \emph{arXiv preprint arXiv:1906.11537}, 2019.

\bibitem[Fukushima and Miyake(1982)]{fukushima1982neocognitron}
Kunihiko Fukushima and Sei Miyake.
\newblock Neocognitron: A self-organizing neural network model for a mechanism
  of visual pattern recognition.
\newblock In \emph{Competition and cooperation in neural nets}, pages 267--285.
  Springer, 1982.

\bibitem[Germain et~al.(2016)Germain, Bach, Lacoste, and
  Lacoste-Julien]{germain2016pac}
Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien.
\newblock Pac-bayesian theory meets bayesian inference.
\newblock In D.~Lee, M.~Sugiyama, U.~Luxburg, I.~Guyon, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016.

\bibitem[Ginsbourger et~al.(2016)Ginsbourger, Roustant, and
  Durrande]{ginsbourger2016}
David Ginsbourger, Olivier Roustant, and Nicolas Durrande.
\newblock On degeneracy and invariances of random fields paths with
  applications in gaussian process modelling.
\newblock \emph{Journal of Statistical Planning and Inference}, 170:\penalty0
  117--128, 2016.
\newblock ISSN 0378-3758.

\bibitem[Gr{\"u}nwald(2007)]{grunwald2007minimum}
Peter~D Gr{\"u}nwald.
\newblock \emph{The minimum description length principle}.
\newblock MIT press, 2007.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Immer et~al.(2021{\natexlab{a}})Immer, Bauer, Fortuin, R{\"a}tsch, and
  Khan]{immer2021scalable}
Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar R{\"a}tsch, and
  Mohammad~Emtiyaz Khan.
\newblock Scalable marginal likelihood estimation for model selection in deep
  learning.
\newblock \emph{arXiv preprint arXiv:2104.04975}, 2021{\natexlab{a}}.

\bibitem[Immer et~al.(2021{\natexlab{b}})Immer, Korzepa, and
  Bauer]{immer2020improving}
Alexander Immer, Maciej Korzepa, and Matthias Bauer.
\newblock Improving predictions of bayesian neural nets via local
  linearization.
\newblock In \emph{Proceedings of The 24th International Conference on
  Artificial Intelligence and Statistics}, pages 703--711, 2021{\natexlab{b}}.

\bibitem[Immer et~al.(2022)Immer, Torroba~Hennigen, Fortuin, and
  Cotterell]{immer2022probing}
Alexander Immer, Lucas Torroba~Hennigen, Vincent Fortuin, and Ryan Cotterell.
\newblock Probing as quantifying inductive bias.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics}, pages 1839--1851, 2022.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem[Jaderberg et~al.(2015)Jaderberg, Simonyan, Zisserman,
  et~al.]{jaderberg2015spatial}
Max Jaderberg, Karen Simonyan, Andrew Zisserman, et~al.
\newblock Spatial transformer networks.
\newblock \emph{Advances in neural information processing systems},
  28:\penalty0 2017--2025, 2015.

\bibitem[Khan et~al.(2019)Khan, Immer, Abedi, and Korzepa]{khan2019approximate}
Mohammad Emtiyaz~E Khan, Alexander Immer, Ehsan Abedi, and Maciej Korzepa.
\newblock Approximate inference turns deep networks into gaussian processes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3088--3098, 2019.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kingma and Welling(2013)]{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Kondor(2008)]{kondorthesis}
Imre~Risi Kondor.
\newblock Group theoretical methods in machine learning, 2008.

\bibitem[Kondor et~al.(2018)Kondor, Lin, and Trivedi]{kondor2018clebsch}
Risi Kondor, Zhen Lin, and Shubhendu Trivedi.
\newblock Clebsch--gordan nets: a fully fourier space spherical convolutional
  neural network.
\newblock \emph{Advances in Neural Information Processing Systems},
  31:\penalty0 10117--10126, 2018.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kunstner et~al.(2019)Kunstner, Hennig, and
  Balles]{kunstner2019limitations}
Frederik Kunstner, Philipp Hennig, and Lukas Balles.
\newblock Limitations of the empirical fisher approximation for natural
  gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4158--4169, 2019.

\bibitem[Laplace(1774)]{laplace1774memoire}
Pierre-Simon~de Laplace.
\newblock M{\'e}moire sur la probabilit{\'e} des causes par les
  {\'e}v{\'e}nements.
\newblock \emph{Mémoires de l’Académie royale des sciences de Paris
  (Savants étrangers)}, 6:\penalty0 621--656, 1774.

\bibitem[LeCun and Cortes(2010)]{lecun2010mnist}
Yann LeCun and Corinna Cortes.
\newblock {MNIST} handwritten digit database.
\newblock http://yann.lecun.com/exdb/mnist/, 2010.
\newblock URL \url{http://yann.lecun.com/exdb/mnist/}.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lorraine et~al.(2020)Lorraine, Vicol, and Duvenaud]{lorraine2020}
Jonathan Lorraine, Paul Vicol, and David Duvenaud.
\newblock Optimizing millions of hyperparameters by implicit differentiation.
\newblock In Silvia Chiappa and Roberto Calandra, editors, \emph{Proceedings of
  the Twenty Third International Conference on Artificial Intelligence and
  Statistics}, volume 108 of \emph{Proceedings of Machine Learning Research},
  pages 1540--1552. PMLR, 26--28 Aug 2020.

\bibitem[MacKay(1992)]{mackay1992practical}
David~JC MacKay.
\newblock A practical bayesian framework for backpropagation networks.
\newblock \emph{Neural computation}, 4\penalty0 (3):\penalty0 448--472, 1992.

\bibitem[MacKay(2003)]{mackay2003information}
David~JC MacKay.
\newblock \emph{Information theory, inference and learning algorithms}.
\newblock Cambridge university press, 2003.

\bibitem[Martens(2020)]{martens2014new}
James Martens.
\newblock New insights and perspectives on the natural gradient method.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (146):\penalty0 1--76, 2020.

\bibitem[Martens and Grosse(2015)]{martens2015optimizing}
James Martens and Roger Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{International conference on machine learning}, pages
  2408--2417, 2015.

\bibitem[Moler and Van~Loan(2003)]{moler2003nineteen}
Cleve Moler and Charles Van~Loan.
\newblock Nineteen dubious ways to compute the exponential of a matrix,
  twenty-five years later.
\newblock \emph{SIAM review}, 45\penalty0 (1):\penalty0 3--49, 2003.

\bibitem[Murphy(2012)]{murphy2012machine}
Kevin~P Murphy.
\newblock \emph{Machine learning: a probabilistic perspective}.
\newblock MIT press, 2012.

\bibitem[Nabarro et~al.(2021)Nabarro, Ganev, Garriga-Alonso, Fortuin, van~der
  Wilk, and Aitchison]{nabarro2021data}
Seth Nabarro, Stoil Ganev, Adri{\`a} Garriga-Alonso, Vincent Fortuin, Mark
  van~der Wilk, and Laurence Aitchison.
\newblock Data augmentation in bayesian neural networks and the cold posterior
  effect.
\newblock \emph{arXiv preprint arXiv:2106.05586}, 2021.

\bibitem[Ober and Aitchison(2021)]{ober2021global}
Sebastian~W Ober and Laurence Aitchison.
\newblock Global inducing point variational posteriors for bayesian neural
  networks and deep gaussian processes.
\newblock In \emph{International Conference on Machine Learning}, pages
  8248--8259. PMLR, 2021.

\bibitem[Ober et~al.(2021)Ober, Rasmussen, and van~der Wilk]{ober2021promises}
Sebastian~W. Ober, Carl~E. Rasmussen, and Mark van~der Wilk.
\newblock The promises and pitfalls of deep kernel learning.
\newblock In Cassio de~Campos and Marloes~H. Maathuis, editors,
  \emph{Proceedings of the Thirty-Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI)}, volume 161 of \emph{Proceedings of Machine
  Learning Research}, pages 1206--1216. PMLR, 27--30 Jul 2021.

\bibitem[Osawa et~al.(2019)Osawa, Swaroop, Khan, Jain, Eschenhagen, Turner, and
  Yokota]{osawa2019practical}
Kazuki Osawa, Siddharth Swaroop, Mohammad Emtiyaz~E Khan, Anirudh Jain, Runa
  Eschenhagen, Richard~E Turner, and Rio Yokota.
\newblock Practical deep learning with bayesian principles.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4289--4301, 2019.

\bibitem[Osawa et~al.(2020)Osawa, Tsuji, Ueno, Naruse, Foo, and
  Yokota]{osawa2020scalable}
Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Chuan-Sheng Foo, and
  Rio Yokota.
\newblock Scalable and practical natural gradient for large-scale deep
  learning.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2020.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem[Rasmussen and Ghahramani(2001)]{rasmussen2001occam}
Carl~Edward Rasmussen and Zoubin Ghahramani.
\newblock Occam's razor.
\newblock In \emph{Advances in neural information processing systems}, pages
  294--300, 2001.

\bibitem[Rasmussen and Williams(2006)]{rasmussen2006gaussian}
Carl~Edward Rasmussen and Christopher~KI Williams.
\newblock \emph{Gaussian processes for machine learning}.
\newblock MIT press Cambridge, MA, 2006.

\bibitem[Ritter et~al.(2018)Ritter, Botev, and Barber]{ritter2018scalable}
Hippolyt Ritter, Aleksandar Botev, and David Barber.
\newblock A scalable laplace approximation for neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Schwöbel et~al.(2022)Schwöbel, Jørgensen, Ober, and van~der
  Wilk]{schwoebel2022layer}
Pola Schwöbel, Martin Jørgensen, Sebastian~W. Ober, and Mark van~der Wilk.
\newblock Last layer marginal likelihood for invariance learning.
\newblock In \emph{Proceedings of the Twenty Fifth International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, 2022.

\bibitem[van Amersfoort et~al.(2021)van Amersfoort, Smith, Jesson, Key, and
  Gal]{amersfoort2021}
Joost van Amersfoort, Lewis Smith, Andrew Jesson, Oscar Key, and Yarin Gal.
\newblock Improving deterministic uncertainty estimation in deep learning for
  classification and regression.
\newblock \emph{CoRR}, abs/2102.11409, 2021.

\bibitem[van~der Ouderaa and van~der Wilk(2021)]{van2021learning}
Tycho~FA van~der Ouderaa and Mark van~der Wilk.
\newblock Learning invariant weights in neural networks.
\newblock In \emph{Workshop in Uncertainty \& Robustness in Deep Learning,
  ICML}, 2021.

\bibitem[van~der Pol et~al.(2020)van~der Pol, Worrall, van Hoof, Oliehoek, and
  Welling]{van2020mdp}
Elise van~der Pol, Daniel Worrall, Herke van Hoof, Frans Oliehoek, and Max
  Welling.
\newblock Mdp homomorphic networks: Group symmetries in reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[van~der Wilk et~al.(2018)van~der Wilk, Bauer, John, and
  Hensman]{van2018learning}
Mark van~der Wilk, Matthias Bauer, ST~John, and James Hensman.
\newblock Learning invariances using the marginal likelihood.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9938--9948, 2018.

\bibitem[Wenzel et~al.(2020)Wenzel, Roth, Veeling, {\'S}wiatkowski, Tran,
  Mandt, Snoek, Salimans, Jenatton, and Nowozin]{wenzel2020good}
Florian Wenzel, Kevin Roth, Bastiaan~S Veeling, Jakub {\'S}wiatkowski, Linh
  Tran, Stephan Mandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and
  Sebastian Nowozin.
\newblock How good is the bayes posterior in deep neural networks really?
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{FashionMNIST}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms, 2017.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoryuko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock In \emph{{BMVC}}. {BMVA} Press, 2016.

\bibitem[Zhang et~al.(2018)Zhang, Sun, Duvenaud, and Grosse]{zhang2018noisy}
Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger Grosse.
\newblock Noisy natural gradient as variational inference.
\newblock In \emph{International Conference on Machine Learning}, pages
  5852--5861, 2018.

\bibitem[Zhang et~al.(2019)Zhang, Dauphin, and Ma]{zhang2019fixup}
Hongyi Zhang, Yann~N Dauphin, and Tengyu Ma.
\newblock Fixup initialization: Residual learning without normalization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Zhou et~al.(2020)Zhou, Knowles, and Finn]{zhou2020meta}
Allan Zhou, Tom Knowles, and Chelsea Finn.
\newblock Meta-learning symmetries by reparameterization.
\newblock \emph{arXiv preprint arXiv:2007.02933}, 2020.

\end{thebibliography}
