\begin{thebibliography}{65}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[tgi()]{tgi}
https://huggingface.co/text-generation-inference.

\bibitem[trt()]{trtllm}
https://github.com/nvidia/tensorrt-llm.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, and Sutton]{austin2021program}
Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., and Sutton, C.
\newblock Program synthesis with large language models, 2021.

\bibitem[Besta et~al.(2023)Besta, Blach, Kubicek, Gerstenberger, Gianinazzi, Gajda, Lehmann, Podstawski, Niewiadomski, Nyczyk, and Hoefler]{besta2023got}
Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gianinazzi, L., Gajda, J., Lehmann, T., Podstawski, M., Niewiadomski, H., Nyczyk, P., and Hoefler, T.
\newblock Graph of thoughts: Solving elaborate problems with large language models, 2023.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Borgeaud, Irving, Lespiau, Sifre, and Jumper]{chen2023accelerating}
Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., and Jumper, J.
\newblock Accelerating large language model decoding with speculative sampling, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba]{chen2021codex}
Chen, M., Tworek, J., Jun, H., Yuan, Q., de~Oliveira~Pinto, H.~P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F.~P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W.~H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A.~N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W.
\newblock Evaluating large language models trained on code.
\newblock 2021.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Ma, Wang, and Cohen]{chen2023program}
Chen, W., Ma, X., Wang, X., and Cohen, W.~W.
\newblock Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks, 2023{\natexlab{b}}.

\bibitem[Dettmers et~al.(2023)Dettmers, Svirschevski, Egiazarian, Kuznedelev, Frantar, Ashkboos, Borzunov, Hoefler, and Alistarh]{dettmers2023spqr}
Dettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev, D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T., and Alistarh, D.
\newblock Spqr: A sparse-quantized representation for near-lossless llm weight compression, 2023.

\bibitem[Frantar \& Alistarh(2023)Frantar and Alistarh]{frantar2023sparsegpt}
Frantar, E. and Alistarh, D.
\newblock Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023.

\bibitem[Frantar et~al.(2022)Frantar, Ashkboos, Hoefler, and Alistarh]{frantar-gptq}
Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D.
\newblock {GPTQ}: Accurate post-training compression for generative pretrained transformers.
\newblock \emph{arXiv preprint arXiv:2210.17323}, 2022.

\bibitem[Gao et~al.(2022)Gao, Madaan, Zhou, Alon, Liu, Yang, Callan, and Neubig]{gao2022pal}
Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G.
\newblock Pal: Program-aided language models.
\newblock \emph{arXiv preprint arXiv:2211.10435}, 2022.

\bibitem[Hao et~al.(2023)Hao, Gu, Ma, Hong, Wang, Wang, and Hu]{hao2023reasoning}
Hao, S., Gu, Y., Ma, H., Hong, J.~J., Wang, Z., Wang, D.~Z., and Hu, Z.
\newblock Reasoning with language model is planning with world model, 2023.

\bibitem[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Basart, Kadavath, Mazeika, Arora, Guo, Burns, Puranik, He, Song, and Steinhardt]{hendrycksapps2021}
Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and Steinhardt, J.
\newblock Measuring coding challenge competence with apps.
\newblock \emph{NeurIPS}, 2021{\natexlab{a}}.

\bibitem[Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendryckstest2021}
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J.
\newblock Measuring massive multitask language understanding.
\newblock \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2021{\natexlab{b}}.

\bibitem[Hendrycks et~al.(2021{\natexlab{c}})Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt]{hendrycksmath2021}
Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J.
\newblock Measuring mathematical problem solving with the math dataset.
\newblock \emph{NeurIPS}, 2021{\natexlab{c}}.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone, De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{pmlr-v97-houlsby19a}
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De~Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In \emph{International conference on machine learning}, pp.\  2790--2799. PMLR, 2019.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2022lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock Lo{RA}: Low-rank adaptation of large language models.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Karpathy(2023)]{andrej}
Karpathy, A.
\newblock Intro to large language models, 2023.

\bibitem[Khot et~al.(2023)Khot, Trivedi, Finlayson, Fu, Richardson, Clark, and Sabharwal]{khot2023decomposed}
Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K., Clark, P., and Sabharwal, A.
\newblock Decomposed prompting: A modular approach for solving complex tasks.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Kim et~al.(2023)Kim, Hooper, Gholami, Dong, Li, Shen, Mahoney, and Keutzer]{kim2023squeezellm}
Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney, M.~W., and Keutzer, K.
\newblock Squeezellm: Dense-and-sparse quantization, 2023.

\bibitem[Kim et~al.(2024)Kim, Mangalam, Moon, Malik, Mahoney, Gholami, and Keutzer]{kim2023bild}
Kim, S., Mangalam, K., Moon, S., Malik, J., Mahoney, M.~W., Gholami, A., and Keutzer, K.
\newblock Speculative decoding with big little decoder, 2024.

\bibitem[Kojima et~al.(2023)Kojima, Gu, Reid, Matsuo, and Iwasawa]{kojima2023large}
Kojima, T., Gu, S.~S., Reid, M., Matsuo, Y., and Iwasawa, Y.
\newblock Large language models are zero-shot reasoners, 2023.

\bibitem[Kwon et~al.(2022)Kwon, Kim, Mahoney, Hassoun, Keutzer, and Gholami]{kwon2022fast}
Kwon, W., Kim, S., Mahoney, M.~W., Hassoun, J., Keutzer, K., and Gholami, A.
\newblock A fast post-training pruning framework for transformers, 2022.

\bibitem[Kwon et~al.(2023)Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang, and Stoica]{kwon2023vllm}
Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C.~H., Gonzalez, J.~E., Zhang, H., and Stoica, I.
\newblock Efficient memory management for large language model serving with pagedattention.
\newblock In \emph{Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles}, 2023.

\bibitem[Langchain()]{langchain}
Langchain.
\newblock https://github.com/langchain-ai/langchain.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
Lester, B., Al-Rfou, R., and Constant, N.
\newblock The power of scale for parameter-efficient prompt tuning, 2021.

\bibitem[Leviathan et~al.(2023)Leviathan, Kalman, and Matias]{leviathan2023fast}
Leviathan, Y., Kalman, M., and Matias, Y.
\newblock Fast inference from transformers via speculative decoding, 2023.

\bibitem[Liang et~al.(2023)Liang, Wu, Song, Wu, Xia, Liu, Ou, Lu, Ji, Mao, Wang, Shou, Gong, and Duan]{liang2023taskmatrixai}
Liang, Y., Wu, C., Song, T., Wu, W., Xia, Y., Liu, Y., Ou, Y., Lu, S., Ji, L., Mao, S., Wang, Y., Shou, L., Gong, M., and Duan, N.
\newblock Taskmatrix.ai: Completing tasks by connecting foundation models with millions of apis, 2023.

\bibitem[Lin et~al.(2023)Lin, Tang, Tang, Yang, Dang, Gan, and Han]{lin2023awq}
Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., Gan, C., and Han, S.
\newblock Awq: Activation-aware weight quantization for llm compression and acceleration, 2023.

\bibitem[Liu(2022)]{Liu_LlamaIndex_2022}
Liu, J.
\newblock {LlamaIndex}, 11 2022.
\newblock URL \url{https://github.com/jerryjliu/llama_index}.

\bibitem[Ma et~al.(2023)Ma, Zhang, Wang, Pan, and Yu]{ma2023laser}
Ma, K., Zhang, H., Wang, H., Pan, X., and Yu, D.
\newblock Laser: Llm agent with state-space exploration for web navigation, 2023.

\bibitem[Madaan et~al.(2023)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe, Alon, Dziri, Prabhumoye, Yang, Gupta, Majumder, Hermann, Welleck, Yazdanbakhsh, and Clark]{madaan2023selfrefine}
Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., Gupta, S., Majumder, B.~P., Hermann, K., Welleck, S., Yazdanbakhsh, A., and Clark, P.
\newblock Self-refine: Iterative refinement with self-feedback, 2023.

\bibitem[Ning et~al.(2023)Ning, Lin, Zhou, Wang, Yang, and Wang]{ning2023skeletonofthought}
Ning, X., Lin, Z., Zhou, Z., Wang, Z., Yang, H., and Wang, Y.
\newblock Skeleton-of-thought: Large language models can do parallel decoding, 2023.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[{OpenAI}(2023)]{openai_devday}
{OpenAI}.
\newblock New models and developer products announced at devday, 2023.

\bibitem[Packer et~al.(2023)Packer, Fang, Patil, Lin, Wooders, and Gonzalez]{packer2023memgpt}
Packer, C., Fang, V., Patil, S.~G., Lin, K., Wooders, S., and Gonzalez, J.~E.
\newblock Memgpt: Towards llms as operating systems, 2023.

\bibitem[Patel et~al.(2022)Patel, Mishra, Parmar, and Baral]{patel-etal-2022-question}
Patel, P., Mishra, S., Parmar, M., and Baral, C.
\newblock Is a question decomposition unit all we need?
\newblock 2022.

\bibitem[Patil et~al.(2023)Patil, Zhang, Wang, and Gonzalez]{patil2023gorilla}
Patil, S.~G., Zhang, T., Wang, X., and Gonzalez, J.~E.
\newblock Gorilla: Large language model connected with massive apis, 2023.

\bibitem[Press et~al.(2023)Press, Zhang, Min, Schmidt, Smith, and Lewis]{press2023measuring}
Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N.~A., and Lewis, M.
\newblock Measuring and narrowing the compositionality gap in language models, 2023.

\bibitem[Qin et~al.(2023)Qin, Liang, Ye, Zhu, Yan, Lu, Lin, Cong, Tang, Qian, et~al.]{qin2023toolllm}
Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y., Cong, X., Tang, X., Qian, B., et~al.
\newblock Toolllm: Facilitating large language models to master 16000+ real-world apis.
\newblock \emph{arXiv preprint arXiv:2307.16789}, 2023.

\bibitem[Ruan et~al.(2023{\natexlab{a}})Ruan, Chen, Zhang, Xu, Bao, Du, Shi, Mao, Zeng, and Zhao]{ruan2023tptu}
Ruan, J., Chen, Y., Zhang, B., Xu, Z., Bao, T., Du, G., Shi, S., Mao, H., Zeng, X., and Zhao, R.
\newblock Tptu: Task planning and tool usage of large language model-based ai agents.
\newblock \emph{arXiv preprint arXiv:2308.03427}, 2023{\natexlab{a}}.

\bibitem[Ruan et~al.(2023{\natexlab{b}})Ruan, Dong, Wang, Pitis, Zhou, Ba, Dubois, Maddison, and Hashimoto]{ruan2023toolemu}
Ruan, Y., Dong, H., Wang, A., Pitis, S., Zhou, Y., Ba, J., Dubois, Y., Maddison, C.~J., and Hashimoto, T.
\newblock Identifying the risks of lm agents with an lm-emulated sandbox.
\newblock \emph{arXiv preprint arXiv:2309.15817}, 2023{\natexlab{b}}.

\bibitem[Schick et~al.(2023)Schick, Dwivedi-Yu, Dess{\`\i}, Raileanu, Lomeli, Zettlemoyer, Cancedda, and Scialom]{schick2023toolformer}
Schick, T., Dwivedi-Yu, J., Dess{\`\i}, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T.
\newblock Toolformer: Language models can teach themselves to use tools.
\newblock \emph{arXiv preprint arXiv:2302.04761}, 2023.

\bibitem[Shen et~al.(2023)Shen, Song, Tan, Li, Lu, and Zhuang]{shen2023hugginggpt}
Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y.
\newblock Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face, 2023.

\bibitem[Shinn et~al.(2023)Shinn, Cassano, Berman, Gopinath, Narasimhan, and Yao]{shinn2023reflexion}
Shinn, N., Cassano, F., Berman, E., Gopinath, A., Narasimhan, K., and Yao, S.
\newblock Reflexion: Language agents with verbal reinforcement learning, 2023.

\bibitem[Song et~al.(2023)Song, Xiong, Zhu, Wu, Qian, Song, Huang, Li, Wang, Yao, Tian, and Li]{song2023restgpt}
Song, Y., Xiong, W., Zhu, D., Wu, W., Qian, H., Song, M., Huang, H., Li, C., Wang, K., Yao, R., Tian, Y., and Li, S.
\newblock Restgpt: Connecting large language models with real-world restful apis, 2023.

\bibitem[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2023beyond}
Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A.~M., Abid, A., Fisch, A., Brown, A.~R., Santoro, A., Gupta, A., Garriga-Alonso, A., et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock \emph{arXiv preprint arXiv:2206.04615}, 2022.

\bibitem[Sumers et~al.(2023)Sumers, Yao, Narasimhan, and Griffiths]{sumers2023cognitive}
Sumers, T.~R., Yao, S., Narasimhan, K., and Griffiths, T.~L.
\newblock Cognitive architectures for language agents, 2023.

\bibitem[Surís et~al.(2023)Surís, Menon, and Vondrick]{suris2023vipergpt}
Surís, D., Menon, S., and Vondrick, C.
\newblock Vipergpt: Visual inference via python execution for reasoning, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C.~C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P.~S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E.~M., Subramanian, R., Tan, X.~E., Tang, B., Taylor, R., Williams, A., Kuan, J.~X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T.
\newblock Llama 2: Open foundation and fine-tuned chat models, 2023.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Xu, Lan, Hu, Lan, Lee, and Lim]{wang2023plan}
Wang, L., Xu, W., Lan, Y., Hu, Z., Lan, Y., Lee, R. K.-W., and Lim, E.-P.
\newblock Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models.
\newblock \emph{arXiv preprint arXiv:2305.04091}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou]{wang2023selfconsistency}
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D.
\newblock Self-consistency improves chain of thought reasoning in language models, 2023{\natexlab{b}}.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022cot}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.~V., Zhou, D., et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock volume~35, pp.\  24824--24837, 2022.

\bibitem[Wolfson et~al.(2020)Wolfson, Geva, Gupta, Gardner, Goldberg, Deutch, and Berant]{Wolfson2020Break}
Wolfson, T., Geva, M., Gupta, A., Gardner, M., Goldberg, Y., Deutch, D., and Berant, J.
\newblock Break it down: A question understanding benchmark.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 2020.

\bibitem[Xu et~al.(2023)Xu, Peng, Lei, Mukherjee, Liu, and Xu]{xu2023rewoo}
Xu, B., Peng, Z., Lei, B., Mukherjee, S., Liu, Y., and Xu, D.
\newblock Rewoo: Decoupling reasoning from observations for efficient augmented language models, 2023.

\bibitem[Yang et~al.(2018)Yang, Qi, Zhang, Bengio, Cohen, Salakhutdinov, and Manning]{yang2018hotpotqa}
Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W.~W., Salakhutdinov, R., and Manning, C.~D.
\newblock Hotpotqa: A dataset for diverse, explainable multi-hop question answering.
\newblock \emph{arXiv preprint arXiv:1809.09600}, 2018.

\bibitem[Yang et~al.(2022)Yang, Dong, Du, Cheng, Cambria, Liu, Gao, and Wei]{yang2022language}
Yang, Z., Dong, L., Du, X., Cheng, H., Cambria, E., Liu, X., Gao, J., and Wei, F.
\newblock Language models as inductive reasoners, 2022.

\bibitem[Yao et~al.(2022)Yao, Zhao, Yu, Du, Shafran, Narasimhan, and Cao]{yao2022react}
Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y.
\newblock React: Synergizing reasoning and acting in language models.
\newblock \emph{arXiv preprint arXiv:2210.03629}, 2022.

\bibitem[Yao et~al.(2023{\natexlab{a}})Yao, Chen, Yang, and Narasimhan]{yao2022webshop}
Yao, S., Chen, H., Yang, J., and Narasimhan, K.
\newblock Webshop: Towards scalable real-world web interaction with grounded language agents, 2023{\natexlab{a}}.

\bibitem[Yao et~al.(2023{\natexlab{b}})Yao, Yu, Zhao, Shafran, Griffiths, Cao, and Narasimhan]{yao2023tree}
Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.~L., Cao, Y., and Narasimhan, K.
\newblock {Tree of Thoughts}: Deliberate problem solving with large language models, 2023{\natexlab{b}}.

\bibitem[Yu et~al.(2022)Yu, Jeong, Kim, Kim, and Chun]{yu2022orca}
Yu, G.-I., Jeong, J.~S., Kim, G.-W., Kim, S., and Chun, B.-G.
\newblock Orca: A distributed serving system for $\{$Transformer-Based$\}$ generative models.
\newblock In \emph{16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)}, pp.\  521--538, 2022.

\bibitem[Yu et~al.(2023)Yu, Jiang, Clark, and Sabharwal]{yu2023ifqa}
Yu, W., Jiang, M., Clark, P., and Sabharwal, A.
\newblock Ifqa: A dataset for open-domain question answering under counterfactual presuppositions, 2023.

\bibitem[Zheng et~al.(2023)Zheng, Mishra, Chen, Cheng, Chi, Le, and Zhou]{zheng2023step}
Zheng, H.~S., Mishra, S., Chen, X., Cheng, H.-T., Chi, E.~H., Le, Q.~V., and Zhou, D.
\newblock Take a step back: Evoking reasoning via abstraction in large language models, 2023.

\bibitem[Zhou et~al.(2023{\natexlab{a}})Zhou, Yan, Shlapentokh-Rothman, Wang, and Wang]{zhou2023language}
Zhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., and Wang, Y.-X.
\newblock Language agent tree search unifies reasoning acting and planning in language models, 2023{\natexlab{a}}.

\bibitem[Zhou et~al.(2023{\natexlab{b}})Zhou, Sch{\"a}rli, Hou, Wei, Scales, Wang, Schuurmans, Cui, Bousquet, Le, and Chi]{zhou2023leasttomost}
Zhou, D., Sch{\"a}rli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q.~V., and Chi, E.~H.
\newblock Least-to-most prompting enables complex reasoning in large language models.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{b}}.

\end{thebibliography}
