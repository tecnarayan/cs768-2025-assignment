\begin{thebibliography}{20}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Athalye et~al.(2018)Athalye, Carlini, and
  Wagner]{athalye2018obfuscated}
A.~Athalye, N.~Carlini, and D.~Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock \emph{arXiv preprint arXiv:1802.00420}, 2018.

\bibitem[Boyd et~al.(2004)Boyd, Boyd, and Vandenberghe]{boyd2004convex}
S.~Boyd, S.~P. Boyd, and L.~Vandenberghe.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Bubeck et~al.(2019)Bubeck, Lee, Price, and
  Razenshteyn]{bubeck2019adversarial}
S.~Bubeck, Y.~T. Lee, E.~Price, and I.~Razenshteyn.
\newblock Adversarial examples from computational constraints.
\newblock In \emph{International Conference on Machine Learning}, pages
  831--840, 2019.

\bibitem[Carlini and Wagner(2017)]{carlini2017adversarial}
N.~Carlini and D.~Wagner.
\newblock Adversarial examples are not easily detected: Bypassing ten detection
  methods.
\newblock In \emph{Proceedings of the 10th ACM Workshop on Artificial
  Intelligence and Security}, pages 3--14, 2017.

\bibitem[Carlini and Wagner(2018)]{carlini2018audio}
N.~Carlini and D.~Wagner.
\newblock Audio adversarial examples: Targeted attacks on speech-to-text.
\newblock In \emph{2018 IEEE Security and Privacy Workshops (SPW)}, pages 1--7.
  IEEE, 2018.

\bibitem[Fawzi et~al.(2018)Fawzi, Fawzi, and Fawzi]{fawzi2018adversarial}
A.~Fawzi, H.~Fawzi, and O.~Fawzi.
\newblock Adversarial vulnerability for any classifier.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1178--1187, 2018.

\bibitem[Feinman et~al.(2017)Feinman, Curtin, Shintre, and
  Gardner]{feinman2017detecting}
R.~Feinman, R.~R. Curtin, S.~Shintre, and A.~B. Gardner.
\newblock Detecting adversarial samples from artifacts.
\newblock \emph{arXiv preprint arXiv:1703.00410}, 2017.

\bibitem[Glorot and Bengio(2010)]{glorot2010understanding}
X.~Glorot and Y.~Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256, 2010.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
I.~J. Goodfellow, J.~Shlens, and C.~Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014.

\bibitem[Grosse et~al.(2017)Grosse, Manoharan, Papernot, Backes, and
  McDaniel]{grosse2017statistical}
K.~Grosse, P.~Manoharan, N.~Papernot, M.~Backes, and P.~McDaniel.
\newblock On the (statistical) detection of adversarial examples.
\newblock \emph{arXiv preprint arXiv:1702.06280}, 2017.

\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
A.~Madry, A.~Makelov, L.~Schmidt, D.~Tsipras, and A.~Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{arXiv preprint arXiv:1706.06083}, 2017.

\bibitem[Papernot et~al.(2016)Papernot, McDaniel, Wu, Jha, and
  Swami]{papernot2016distillation}
N.~Papernot, P.~McDaniel, X.~Wu, S.~Jha, and A.~Swami.
\newblock Distillation as a defense to adversarial perturbations against deep
  neural networks.
\newblock In \emph{2016 IEEE Symposium on Security and Privacy (SP)}, pages
  582--597. IEEE, 2016.

\bibitem[Papernot et~al.(2017)Papernot, McDaniel, Goodfellow, Jha, Celik, and
  Swami]{papernot2017practical}
N.~Papernot, P.~McDaniel, I.~Goodfellow, S.~Jha, Z.~B. Celik, and A.~Swami.
\newblock Practical black-box attacks against machine learning.
\newblock In \emph{Proceedings of the 2017 ACM on Asia conference on computer
  and communications security}, pages 506--519, 2017.

\bibitem[Schmidt et~al.(2018)Schmidt, Santurkar, Tsipras, Talwar, and
  Madry]{schmidt2018adversarially}
L.~Schmidt, S.~Santurkar, D.~Tsipras, K.~Talwar, and A.~Madry.
\newblock Adversarially robust generalization requires more data.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5014--5026, 2018.

\bibitem[Shafahi et~al.(2018)Shafahi, Huang, Studer, Feizi, and
  Goldstein]{shafahi2018adversarial}
A.~Shafahi, W.~R. Huang, C.~Studer, S.~Feizi, and T.~Goldstein.
\newblock Are adversarial examples inevitable?
\newblock \emph{arXiv preprint arXiv:1809.02104}, 2018.

\bibitem[Shamir et~al.(2019)Shamir, Safran, Ronen, and
  Dunkelman]{shamir2019simple}
A.~Shamir, I.~Safran, E.~Ronen, and O.~Dunkelman.
\newblock A simple explanation for the existence of adversarial examples with
  small hamming distance.
\newblock \emph{arXiv preprint arXiv:1901.10861}, 2019.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Estrach, Erhan,
  Goodfellow, and Fergus]{szegedy2014intriguing}
C.~Szegedy, W.~Zaremba, I.~Sutskever, J.~B. Estrach, D.~Erhan, I.~Goodfellow,
  and R.~Fergus.
\newblock Intriguing properties of neural networks.
\newblock In \emph{2nd International Conference on Learning Representations,
  ICLR 2014}, 2014.

\bibitem[van Handel(2014)]{van2014probability}
R.~van Handel.
\newblock Probability in high dimension.
\newblock Technical report, PRINCETON UNIV NJ, 2014.

\bibitem[Vershynin(2010)]{vershynin2010introduction}
R.~Vershynin.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock \emph{arXiv preprint arXiv:1011.3027}, 2010.

\bibitem[Wong and Kolter(2018)]{wong2018provable}
E.~Wong and Z.~Kolter.
\newblock Provable defenses against adversarial examples via the convex outer
  adversarial polytope.
\newblock In \emph{International Conference on Machine Learning}, pages
  5286--5295, 2018.

\end{thebibliography}
