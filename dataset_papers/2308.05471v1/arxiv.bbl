\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020{\natexlab{a}})Agarwal, Henaff, Kakade, and
  Sun]{DBLP:conf/nips/AgarwalHKS20}
Agarwal, A., Henaff, M., Kakade, S.~M., and Sun, W.
\newblock {PC-PG:} policy cover directed exploration for provable policy
  gradient learning.
\newblock In \emph{Advances in Neural Information Processing Systems 33},
  2020{\natexlab{a}}.

\bibitem[Agarwal et~al.(2020{\natexlab{b}})Agarwal, Kakade, Krishnamurthy, and
  Sun]{DBLP:conf/nips/AgarwalKKS20}
Agarwal, A., Kakade, S.~M., Krishnamurthy, A., and Sun, W.
\newblock {FLAMBE:} structural complexity and representation learning of low
  rank mdps.
\newblock In \emph{Advances in Neural Information Processing Systems 33},
  2020{\natexlab{b}}.

\bibitem[Agarwal et~al.(2022)Agarwal, Song, Sun, Wang, Wang, and
  Zhang]{agarwal2022provable}
Agarwal, A., Song, Y., Sun, W., Wang, K., Wang, M., and Zhang, X.
\newblock Provable benefits of representational transfer in reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2205.14571}, 2022.

\bibitem[Besbes et~al.(2015)Besbes, Gur, and
  Zeevi]{DBLP:journals/ior/BesbesGZ15}
Besbes, O., Gur, Y., and Zeevi, A.
\newblock Non-stationary stochastic optimization.
\newblock \emph{Oper. Res.}, 63\penalty0 (5):\penalty0 1227--1244, 2015.

\bibitem[Bojarski et~al.(2016)Bojarski, Del~Testa, Dworakowski, Firner, Flepp,
  Goyal, Jackel, Monfort, Muller, Zhang, et~al.]{bojarski2016end}
Bojarski, M., Del~Testa, D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P.,
  Jackel, L.~D., Monfort, M., Muller, U., Zhang, J., et~al.
\newblock End to end learning for self-driving cars.
\newblock \emph{arXiv preprint arXiv:1604.07316}, 2016.

\bibitem[Bubeck \& Cesa{-}Bianchi(2012)Bubeck and
  Cesa{-}Bianchi]{DBLP:journals/ftml/BubeckC12}
Bubeck, S. and Cesa{-}Bianchi, N.
\newblock Regret analysis of stochastic and nonstochastic multi-armed bandit
  problems.
\newblock \emph{Found. Trends Mach. Learn.}, 2012.

\bibitem[Cai et~al.(2020)Cai, Yang, Jin, and Wang]{DBLP:conf/icml/CaiYJW20}
Cai, Q., Yang, Z., Jin, C., and Wang, Z.
\newblock Provably efficient exploration in policy optimization.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, 2020.

\bibitem[Cheng et~al.(2022)Cheng, Feng, Yang, Zhang, and Liang]{chengprovable}
Cheng, Y., Feng, S., Yang, J., Zhang, H., and Liang, Y.
\newblock Provable benefit of multitask representation learning in
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Cheng et~al.(2023)Cheng, Huang, Yang, and Liang]{cheng2023improved}
Cheng, Y., Huang, R., Yang, J., and Liang, Y.
\newblock Improved sample complexity for reward-free reinforcement learning
  under low-rank mdps.
\newblock \emph{arXiv preprint arXiv:2303.10859}, 2023.

\bibitem[Cheung et~al.(2019)Cheung, Simchi{-}Levi, and
  Zhu]{DBLP:conf/aistats/CheungSZ19}
Cheung, W.~C., Simchi{-}Levi, D., and Zhu, R.
\newblock Learning to optimize under non-stationarity.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, volume~89 of \emph{Proceedings of Machine Learning
  Research}, pp.\  1079--1087. {PMLR}, 2019.

\bibitem[Cheung et~al.(2020)Cheung, Simchi{-}Levi, and
  Zhu]{DBLP:conf/icml/CheungSZ20}
Cheung, W.~C., Simchi{-}Levi, D., and Zhu, R.
\newblock Reinforcement learning for non-stationary markov decision processes:
  The blessing of (more) optimism.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, 2020.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and
  Brunskill]{DBLP:conf/nips/DannLB17}
Dann, C., Lattimore, T., and Brunskill, E.
\newblock Unifying {PAC} and regret: Uniform {PAC} bounds for episodic
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, 2017.

\bibitem[Fei et~al.(2020)Fei, Yang, Wang, and Xie]{DBLP:conf/nips/FeiYWX20}
Fei, Y., Yang, Z., Wang, Z., and Xie, Q.
\newblock Dynamic regret of policy optimization in non-stationary environments.
\newblock In \emph{Advances in Neural Information Processing Systems 33}, 2020.

\bibitem[Gajane et~al.(2018)Gajane, Ortner, and
  Auer]{DBLP:journals/corr/abs-1805-10066}
Gajane, P., Ortner, R., and Auer, P.
\newblock A sliding-window algorithm for markov decision processes with
  arbitrarily changing rewards and transitions.
\newblock \emph{CoRR}, abs/1805.10066, 2018.

\bibitem[Garivier \& Moulines(2011)Garivier and Moulines]{garivier2011upper}
Garivier, A. and Moulines, E.
\newblock On upper-confidence bound policies for switching bandit problems.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  2011.

\bibitem[Gu et~al.(2017)Gu, Holly, Lillicrap, and Levine]{gu2017deep}
Gu, S., Holly, E., Lillicrap, T., and Levine, S.
\newblock Deep reinforcement learning for robotic manipulation with
  asynchronous off-policy updates.
\newblock In \emph{IEEE international conference on robotics and automation},
  2017.

\bibitem[Hall \& Willett(2015)Hall and Willett]{DBLP:journals/jstsp/HallW15}
Hall, E.~C. and Willett, R.~M.
\newblock Online convex optimization in dynamic environments.
\newblock \emph{{IEEE} J. Sel. Top. Signal Process.}, 9\penalty0 (4):\penalty0
  647--662, 2015.

\bibitem[Hazan(2016)]{DBLP:journals/ftopt/Hazan16}
Hazan, E.
\newblock Introduction to online convex optimization.
\newblock \emph{Found. Trends Optim.}, 2\penalty0 (3-4):\penalty0 157--325,
  2016.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang2017contextual}
Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R.~E.
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{levine2016end}
Levine, S., Finn, C., Darrell, T., and Abbeel, P.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1334--1373, 2016.

\bibitem[Ma et~al.(2021)Ma, Li, Kochenderfer, Isele, and
  Fujimura]{DBLP:conf/icra/Ma0KIF21}
Ma, X., Li, J., Kochenderfer, M.~J., Isele, D., and Fujimura, K.
\newblock Reinforcement learning for autonomous driving with latent state
  inference and spatial-temporal relationships.
\newblock In \emph{{IEEE} International Conference on Robotics and Automation},
  2021.

\bibitem[Mao et~al.(2021)Mao, Zhang, Zhu, Simchi{-}Levi, and
  Basar]{DBLP:conf/icml/MaoZZSB21}
Mao, W., Zhang, K., Zhu, R., Simchi{-}Levi, D., and Basar, T.
\newblock Near-optimal model-free reinforcement learning in non-stationary
  episodic mdps.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, 2021.

\bibitem[Modi et~al.(2021)Modi, Chen, Krishnamurthy, Jiang, and
  Agarwal]{modi2021model}
Modi, A., Chen, J., Krishnamurthy, A., Jiang, N., and Agarwal, A.
\newblock Model-free representation learning and exploration in low-rank mdps.
\newblock \emph{arXiv preprint arXiv:2102.07035}, 2021.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{DBLP:journals/corr/SchulmanWDRK17}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{CoRR}, abs/1707.06347, 2017.

\bibitem[Shani et~al.(2020)Shani, Efroni, Rosenberg, and
  Mannor]{DBLP:conf/icml/ShaniE0M20}
Shani, L., Efroni, Y., Rosenberg, A., and Mannor, S.
\newblock Optimistic policy optimization with bandit feedback.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, 2020.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Silver et~al.(2017)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2017mastering}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
  Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al.
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.
\newblock \emph{arXiv preprint arXiv:1712.01815}, 2017.

\bibitem[Silver et~al.(2018)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2018general}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
  Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al.
\newblock A general reinforcement learning algorithm that masters chess, shogi,
  and go through self-play.
\newblock \emph{Science}, 362\penalty0 (6419):\penalty0 1140--1144, 2018.

\bibitem[Sun et~al.(2019)Sun, Jiang, Krishnamurthy, Agarwal, and
  Langford]{DBLP:conf/colt/SunJKA019}
Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A., and Langford, J.
\newblock Model-based {RL} in contextual decision processes: {PAC} bounds and
  exponential improvements over model-free approaches.
\newblock In \emph{Conference on Learning Theory}, 2019.

\bibitem[Touati \& Vincent(2020)Touati and
  Vincent]{DBLP:journals/corr/abs-2010-12870}
Touati, A. and Vincent, P.
\newblock Efficient learning in non-stationary linear markov decision
  processes.
\newblock \emph{CoRR}, abs/2010.12870, 2020.

\bibitem[Tsybakov(2009)]{DBLP:books/daglib/0035708}
Tsybakov, A.~B.
\newblock \emph{Introduction to Nonparametric Estimation}.
\newblock Springer series in statistics. Springer, 2009.

\bibitem[Uehara et~al.(2022)Uehara, Zhang, and Sun]{DBLP:conf/iclr/UeharaZS22}
Uehara, M., Zhang, X., and Sun, W.
\newblock Representation learning for online and offline {RL} in low-rank mdps.
\newblock In \emph{The Tenth International Conference on Learning
  Representations}, 2022.

\bibitem[Wei \& Luo(2021)Wei and Luo]{DBLP:conf/colt/WeiL21}
Wei, C. and Luo, H.
\newblock Non-stationary reinforcement learning without prior knowledge: an
  optimal black-box approach.
\newblock In \emph{Conference on Learning Theory}, 2021.

\bibitem[Xu et~al.(2021)Xu, Liang, and Lan]{DBLP:conf/icml/XuLL21}
Xu, T., Liang, Y., and Lan, G.
\newblock {CRPO:} {A} new approach for safe reinforcement learning with
  convergence guarantee.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, 2021.

\bibitem[Zanette et~al.(2021)Zanette, Cheng, and
  Agarwal]{zanette2021cautiously}
Zanette, A., Cheng, C.-A., and Agarwal, A.
\newblock Cautiously optimistic policy optimization and exploration with linear
  function approximation.
\newblock In \emph{Conference on Learning Theory}, 2021.

\bibitem[Zhao et~al.(2021)Zhao, Gu, Zhang, Yang, Liu, Tang, and
  Liu]{DBLP:conf/aaai/ZhaoGZYLTL21}
Zhao, X., Gu, C., Zhang, H., Yang, X., Liu, X., Tang, J., and Liu, H.
\newblock {DEAR:} deep reinforcement learning for online advertising impression
  in recommender systems.
\newblock In \emph{Thirty-Fifth {AAAI} Conference on Artificial Intelligence},
  2021.

\bibitem[Zhong et~al.(2021)Zhong, Yang, Wang, and
  Szepesv{\'{a}}ri]{DBLP:journals/corr/abs-2110-08984}
Zhong, H., Yang, Z., Wang, Z., and Szepesv{\'{a}}ri, C.
\newblock Optimistic policy optimization is provably efficient in
  non-stationary mdps.
\newblock \emph{CoRR}, abs/2110.08984, 2021.

\bibitem[Zhou et~al.(2020)Zhou, Chen, Varshney, and
  Jagmohan]{DBLP:journals/corr/abs-2010-04244}
Zhou, H., Chen, J., Varshney, L.~R., and Jagmohan, A.
\newblock Nonstationary reinforcement learning with linear function
  approximation.
\newblock \emph{CoRR}, 2020.

\end{thebibliography}
