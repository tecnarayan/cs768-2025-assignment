\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2017)Agarwal, Allen-Zhu, Bullins, Hazan, and
  Ma]{Zhu-16-apg}
Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma.
\newblock Finding approximate local minima for nonconvex optimization in linear
  time.
\newblock In \emph{ACM Symposium on the Theory of Computing (STOC)}, pages
  1195--1199, 2017.

\bibitem[Allen-Zhu and Li(2018)]{Allen-Zhu-2018-nc}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Neon2: Finding local minima via first-order oracles.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 3716--3726, 2018.

\bibitem[Cand\`{e}s et~al.(2015)Cand\`{e}s, Li, and
  Soltanolkotabi]{Candes-2015}
Emmanuel~J. Cand\`{e}s, Xiaodong Li, and Mahdi Soltanolkotabi.
\newblock Phase retrieval via wirtinger flow: Theory and algorithms.
\newblock \emph{IEEE Transactions on Information Theory}, 61\penalty0
  (4):\penalty0 1985--2007, 2015.

\bibitem[Carmon and Duchi(2018)]{Carmon-2018-nips}
Yair Carmon and John Duchi.
\newblock Analysis of krylov subspace solutions of regularized nonconvex
  quadratic problems.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 10728--10738, 2018.

\bibitem[Carmon and Duchi(2020)]{Carmon-siam-2020}
Yair Carmon and John Duchi.
\newblock First-order methods for nonconvex quadratic minimization.
\newblock \emph{SIAM Review}, 62\penalty0 (2):\penalty0 395--436, 2020.

\bibitem[Carmon et~al.(2017)Carmon, Duchi, Hinder, and
  Sidford]{carmon-2017-guilty}
Yair Carmon, John~C Duchi, Oliver Hinder, and Aaron Sidford.
\newblock Convex until proven guilty: Dimension-free acceleration of gradient
  descent on non-convex functions.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  654--663, 2017.

\bibitem[Carmon et~al.(2018)Carmon, Duchi, Hinder, and Sidford]{Carmon-2016}
Yair Carmon, John~C Duchi, Oliver Hinder, and Aaron Sidford.
\newblock Accelerated methods for nonconvex optimization.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (2):\penalty0
  1751--1772, 2018.

\bibitem[Carmon et~al.(2020)Carmon, Duchi, Hinder, and Sidford]{Carmon-mp-2020}
Yair Carmon, John Duchi, Oliver Hinder, and Aaron Sidford.
\newblock Lower bounds for finding stationary points {I}.
\newblock \emph{Mathematical Programming}, 184:\penalty0 71--120, 2020.

\bibitem[Carmon et~al.(2021)Carmon, Duchi, Hinder, and
  Sidford]{Carmon-mp2-2021}
Yair Carmon, John Duchi, Oliver Hinder, and Aaron Sidford.
\newblock Lower bounds for finding stationary points {II}: First-order methods.
\newblock \emph{Mathematical Programming}, 185:\penalty0 315--355, 2021.

\bibitem[Davenport et~al.(2014)Davenport, Plan, van~den Berg, and
  Wootters]{Davenport-2014}
Mark~A. Davenport, Yaniv Plan, Ewout van~den Berg, and Mary Wootters.
\newblock 1-bit matrix completion.
\newblock \emph{Information and Inference}, 3\penalty0 (3):\penalty0 189--223,
  2014.

\bibitem[Davis and Yin(2017)]{Davis-2017-admm}
Damek Davis and Wotao Yin.
\newblock Convergence rate analysis of several splitting schemes.
\newblock \emph{Part of the Scientific Computation book series (SCIENTCOMP)},
  2017.

\bibitem[Fang et~al.(2019)Fang, Lin, and Zhang]{congfang-19-sgd}
Cong Fang, Zhouchen Lin, and Tong Zhang.
\newblock Sharp analysis for nonconvex {SGD} escaping from saddle points.
\newblock In \emph{Conference On Learning Theory (COLT)}, pages 1192--1234,
  2019.

\bibitem[Ghadimi et~al.(2015)Ghadimi, Feyzmahdavian, and
  Johansson]{Ghadimi-2015-ecc}
Euhanna Ghadimi, Hamid~Reza Feyzmahdavian, and Mikael Johansson.
\newblock Global convergence of the heavy-ball method for convex optimization.
\newblock In \emph{European Control Conference (ECC)}, pages 310--315, 2015.

\bibitem[Ghadimi and Lan(2016)]{Saecd2013}
Saeed Ghadimi and Guanghui Lan.
\newblock Accelerated gradient methods for nonconvex nonlinear and stochastic
  programming.
\newblock \emph{Mathematical Programming}, 156:\penalty0 59--99, 2016.

\bibitem[Hardt(2014)]{Hardt-2014-focs}
Moritz Hardt.
\newblock Understanding alternating minimization for matrix completion.
\newblock In \emph{IEEE Annual Symposium on Foundations of Computer Science
  (FOCS)}, pages 651--660, 2014.

\bibitem[Harvey et~al.(2019)Harvey, Liaw, Plan, and Randhawa]{Harvey-19-sgd}
Nicholas J.~A. Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa.
\newblock Tight analyses for non-smooth stochastic gradient descent.
\newblock In \emph{Conference On Learning Theory (COLT)}, pages 1579--1613,
  2019.

\bibitem[Jin et~al.(2017)Jin, Ge, Netrapalli, Kakade, and
  Jordan]{jinchi-2017-icml}
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham~M. Kakade, and Michael~I. Jordan.
\newblock How to escape saddle points efficiently.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  1724--1732, 2017.

\bibitem[Jin et~al.(2018)Jin, Netrapalli, and Jordan]{jinchi-18-apg}
Chi Jin, Praneeth Netrapalli, and Michael~I. Jordan.
\newblock Accelerated gradient descent escapes saddle points faster than
  gradient descent.
\newblock In \emph{Conference On Learning Theory (COLT)}, pages 1042--1085,
  2018.

\bibitem[Le{C}un et~al.(2015)Le{C}un, Bengio, and Hinton]{Hinton-nature}
Yann Le{C}un, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock \emph{Nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Lee et~al.(2016)Lee, Simchowitz, Jordan, and Recht]{Jason-16-gd}
Jason~D. Lee, Max Simchowitz, Michael~I. Jordan, and Benjamin Recht.
\newblock Gradient descent only converges to minimizers.
\newblock In \emph{Conference On Learning Theory (COLT)}, pages 1246--1257,
  2016.

\bibitem[Li and Lin(2015)]{li-15-apg}
Huan Li and Zhouchen Lin.
\newblock Accelerated proximal gradient methods for nonconvex programming.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 379--387, 2015.

\bibitem[Li and Lin(2022)]{li-2022-icml}
Huan Li and Zhouchen Lin.
\newblock Restarted nonconvex accelerated gradient descent: No more
  polylogarithmic factor in the $o(\epsilon^{-7/4})$ complexity.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  12901--12916, 2022.

\bibitem[Li et~al.(2020)Li, Fang, and Lin]{li-pieee}
Huan Li, Cong Fang, and Zhouchen Lin.
\newblock Accelerated first-order optimization algorithms for machine learning.
\newblock \emph{Proceedings of the IEEE}, 108\penalty0 (11):\penalty0
  2067--2082, 2020.

\bibitem[Li et~al.(2017)Li, Zhou, Liang, and Varshney]{li-2017-icml}
Qunwei Li, Yi~Zhou, Yingbin Liang, and Pramod~K Varshney.
\newblock Convergence analysis of proximal gradient with momentum for nonconvex
  optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  2111--2119, 2017.

\bibitem[Liang et~al.(2016)Liang, Fadili, and Peyr\'{e}]{JingweiLiang-16-nips}
Jingwei Liang, Jalal~M. Fadili, and Gabriel Peyr\'{e}.
\newblock A multi-step inertial forwardâ€“backward splitting method for
  non-convex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 4035--4043, 2016.

\bibitem[Negahban and Wainwright(2012)]{Negahban-2012}
Sahand Negahban and Martin~J. Wainwright.
\newblock Resticted strong convexity and weighted matrix completion: optimal
  bounds with noise.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (53):\penalty0 1665--1697, 2012.

\bibitem[Nesterov(1983)]{Nesterov1983}
Yurii Nesterov.
\newblock A method for unconstrained convex minimization problem with the rate
  of convergence ${O}(1/k^2)$.
\newblock \emph{Soviet Mathematics Doklady}, 27\penalty0 (2):\penalty0
  372--376, 1983.

\bibitem[Nesterov(1988)]{Nesterov1988}
Yurii Nesterov.
\newblock On an approach to the construction of optimal methods of minimization
  of smooth convex functions.
\newblock \emph{Ekonomika I Mateaticheskie Metody}, 24\penalty0 (3):\penalty0
  509--517, 1988.

\bibitem[Nesterov(2004)]{Nesterov-2004}
Yurii Nesterov.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Springer Science+Business Media, 2004.

\bibitem[Nesterov(2005)]{Nesterov-smooth}
Yurii Nesterov.
\newblock Smooth minimization of non-smooth functions.
\newblock \emph{Mathematical Programming}, 103:\penalty0 127--152, 2005.

\bibitem[Nesterov and Polyak(2006)]{nesterov-cubic}
Yurii Nesterov and Boris~T. Polyak.
\newblock Cubic regularization of newton method and its global performance.
\newblock \emph{Mathematical Programming}, 108:\penalty0 177--205, 2006.

\bibitem[Netrapalli et~al.(2014)Netrapalli, Niranjan, Sanghavi, Anandkumar, and
  Jain]{Prateek-2014-pca}
Praneeth Netrapalli, U~N Niranjan, Sujay Sanghavi, Animashree Anandkumar, and
  Prateek Jain.
\newblock Non-convex robust {PCA}.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 1107--1115, 2014.

\bibitem[Ochs(2018)]{iPiano-2018}
Peter Ochs.
\newblock Local convergence of the heavy-ball method and ipiano for non-convex
  optimization.
\newblock \emph{Journal of Optimization Theory and Applications}, 177:\penalty0
  153--180, 2018.

\bibitem[Ochs et~al.(2014)Ochs, Chen, Brox, and Pock]{iPiano-2014}
Peter Ochs, Yunjin Chen, Thomas Brox, and Thomas Pock.
\newblock ipiano: Inertial proximal algorithm for nonconvex optimization.
\newblock \emph{SIAM Journal on Imaging Sciences}, 7\penalty0 (2):\penalty0
  1388--1419, 2014.

\bibitem[O'Donoghue and Cand\`{e}s(2015)]{Donoghue-2015-NesRestart}
Brendan O'Donoghue and Emmanuel Cand\`{e}s.
\newblock Adaptive restart for accelerated gradient schemes.
\newblock \emph{Foundations of Computational Mathematics}, 15\penalty0
  (3):\penalty0 715--732, 2015.

\bibitem[O'Neill and Wright(2019)]{Neill2019mp}
Michael O'Neill and Stephen~J. Wright.
\newblock Behavior of accelerated gradient methods near critical points of
  nonconvex functions.
\newblock \emph{Mathematical Programming}, 176:\penalty0 403--427, 2019.

\bibitem[Polak and Ribiere(1969)]{Polak-CG1969}
E.~Polak and G.~Ribiere.
\newblock Note sur la convergence de m\'{e}thodes de directions conjugu\'{e}es.
\newblock \emph{Revue fran\d{c}aise d'informatique et de recherche
  op\'{e}rationnelle. S\'{e}rie rouge}, 3\penalty0 (16):\penalty0 35--43, 1969.

\bibitem[Polyak(1964)]{Polyak-1964}
Boris~T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  4\penalty0 (5):\penalty0 791--803, 1964.

\bibitem[Royer and Wright(2018)]{Clement-siam-2018}
Clement~W. Royer and Stephen~J. Wright.
\newblock Complexity analysis of second-order line-search algorithms for smooth
  nonconvex optimization.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (2):\penalty0
  1448--1477, 2018.

\bibitem[Royer et~al.(2020)Royer, O'Neill, and Wright]{Royer20mp}
Clement~W. Royer, Michael O'Neill, and Stephen~J. Wright.
\newblock A {N}ewton-{CG} algorithm with complexity guarantees for smooth
  unconstrained optimization.
\newblock \emph{Mathematical Programming}, 180:\penalty0 451--488, 2020.

\bibitem[Shamir and Zhang(2013)]{Shamir-2013-icml}
Ohad Shamir and Tong Zhang.
\newblock Stochastic gradient descent for non-smooth optimization: Convergence
  results and optimal averaging schemes.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  71--79, 2013.

\bibitem[Simchowitz et~al.(2017)Simchowitz, Alaoui, and Recht]{Simchowitz-logd}
Max Simchowitz, Ahmed~El Alaoui, and Benjamin Recht.
\newblock On the gap between strict-saddles and true convexity: An
  {$\Omega(\log d)$} lower bound for eigenvector approximation.
\newblock \emph{Arxiv preprint: 1704.04548}, 2017.

\bibitem[Sun et~al.(2019)Sun, Li, Quan, Jiang, Li, and Dou]{hb-sun-2019}
Tao Sun, Dongsheng Li, Zhe Quan, Hao Jiang, Shengguo Li, and Yong Dou.
\newblock Heavy-ball algorithms always escape saddle points.
\newblock In \emph{International Joint Conference on Artificial Intelligence
  (IJCAI)}, pages 3520--3526, 2019.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{hb-dl-2013}
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  1139--1147, 2013.

\bibitem[Wang et~al.(2022)Wang, Lin, Wibisono, and Hu]{wang-icml22}
Jun-Kun Wang, Chi-Heng Lin, Andre Wibisono, and Bin Hu.
\newblock Provable acceleration of heaby ball beyond quadratics for a class of
  {P}olyak-{L}ojasiewicz functions when the non-convexity is averaged-out.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  22839--22864, 2022.

\bibitem[Xu et~al.(2018)Xu, Jin, and Yang]{yang-2018-nips}
Yi~Xu, Rong Jin, and Tianbao Yang.
\newblock First-order stochastic algorithms for escaping from saddle points in
  almost linear time.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 5535--5545, 2018.

\bibitem[Zavriev and Kostyuk(1993)]{hb-1993}
S.K. Zavriev and F.V. Kostyuk.
\newblock Heavy-ball method in nonconvex optimization problems.
\newblock \emph{Computational Mathematics and Modeling}, 4\penalty0
  (4):\penalty0 336--341, 1993.

\end{thebibliography}
