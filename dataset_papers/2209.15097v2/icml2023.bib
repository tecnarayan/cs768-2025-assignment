@article{BurerMonteiro2003,
	abstract = {In this paper, we present a nonlinear programming algorithm for solving semidefinite programs (SDPs) in standard form. The algorithm's distinguishing feature is a change of variables that replaces the symmetric, positive semidefinite variable X of the SDP with a rectangular variable R according to the factorization X=RRT. The rank of the factorization, i.e., the number of columns of R, is chosen minimally so as to enhance computational speed while maintaining equivalence with the SDP. Fundamental results concerning the convergence of the algorithm are derived, and encouraging computational results on some large-scale test problems are also presented.},
	author = {Burer, Samuel and Monteiro, Renato D. C.},
	date = {2003/02/01},
	date-added = {2023-05-04 09:46:49 -0500},
	date-modified = {2023-05-04 09:46:49 -0500},
	doi = {10.1007/s10107-002-0352-8},
	id = {Burer2003},
	isbn = {1436-4646},
	journal = {Mathematical Programming},
	number = {2},
	pages = {329--357},
	title = {A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization},
	url = {https://doi.org/10.1007/s10107-002-0352-8},
	volume = {95},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1007/s10107-002-0352-8}}

@InProceedings{pmlr-v97-xu19a,
  title = 	 {Power k-Means Clustering},
  author =       {Xu, Jason and Lange, Kenneth},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6921--6931},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/xu19a/xu19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/xu19a.html},
  abstract = 	 {Clustering is a fundamental task in unsupervised machine learning. Lloyd’s 1957 algorithm for k-means clustering remains one of the most widely used due to its speed and simplicity, but the greedy approach is sensitive to initialization and often falls short at a poor solution. This paper explores an alternative to Lloyd’s algorithm that retains its simplicity and mitigates its tendency to get trapped by local minima. Called power k-means, our method embeds the k-means problem in a continuous class of similar, better behaved problems with fewer local minima. Power k-means anneals its way toward the solution of ordinary k-means by way of majorization-minimization (MM), sharing the appealing descent property and low complexity of Lloyd’s algorithm. Further, our method complements widely used seeding strategies, reaping marked improvements when used together as demonstrated on a suite of simulated and real data examples.}
}

@misc{sahin2022inexact,
      title={An Inexact Augmented Lagrangian Framework for Nonconvex Optimization with Nonlinear Constraints}, 
      author={Mehmet Fatih Sahin and Armin Eftekhari and Ahmet Alacaoglu and Fabian Latorre and Volkan Cevher},
      year={2022},
      eprint={1906.11357},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@inbook{Berkhin2006,
	abstract = {Clustering is the division of data into groups of similar objects. In clustering, some details are disregarded in exchange for data simplification. Clustering can be viewed as a data modeling technique that provides for concise summaries of the data. Clustering is therefore related to many disciplines and plays an important role in a broad range of applications. The applications of clustering usually deal with large datasets and data with many attributes. Exploration of such data is a subject of data mining. This survey concentrates on clustering algorithms from a data mining perspective.},
	address = {Berlin, Heidelberg},
	author = {Berkhin, P.},
	booktitle = {Grouping Multidimensional Data: Recent Advances in Clustering},
	doi = {10.1007/3-540-28349-8_2},
	editor = {Kogan, Jacob and Nicholas, Charles and Teboulle, Marc},
	isbn = {978-3-540-28349-2},
	pages = {25--71},
	publisher = {Springer Berlin Heidelberg},
	title = {A Survey of Clustering Data Mining Techniques},
	url = {https://doi.org/10.1007/3-540-28349-8_2},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1007/3-540-28349-8_2}}

@inproceedings{10.5555/1283383.1283494,
author = {Arthur, David and Vassilvitskii, Sergei},
title = {K-Means++: The Advantages of Careful Seeding},
year = {2007},
isbn = {9780898716245},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is Θ(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
booktitle = {Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {1027–1035},
numpages = {9},
location = {New Orleans, Louisiana},
series = {SODA '07}
}

@misc{Dua:2019 ,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@misc{https://doi.org/10.48550/arxiv.1908.10935,
  doi = {10.48550/ARXIV.1908.10935},
  
  url = {https://arxiv.org/abs/1908.10935},
  
  author = {Wu, Yihong and Zhou, Harrison H.},
  
  keywords = {Statistics Theory (math.ST), Information Theory (cs.IT), Machine Learning (stat.ML), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Randomly initialized EM algorithm for two-component Gaussian mixture achieves near optimality in $O(\sqrt{n})$ iterations},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{10.1214/16-AOS1435,
	abstract = {The EM algorithm is a widely used tool in maximum-likelihood estimation in incomplete data problems. Existing theoretical work has focused on conditions under which the iterates or likelihood values converge, and the associated rates of convergence. Such guarantees do not distinguish whether the ultimate fixed point is a near global optimum or a bad local optimum of the sample likelihood, nor do they relate the obtained fixed point to the global optima of the idealized population likelihood (obtained in the limit of infinite data). This paper develops a theoretical framework for quantifying when and how quickly EM-type iterates converge to a small neighborhood of a given global optimum of the population likelihood. For correctly specified models, such a characterization yields rigorous guarantees on the performance of certain two-stage estimators in which a suitable initial pilot estimator is refined with iterations of the EM algorithm. Our analysis is divided into two parts: a treatment of the EM and first-order EM algorithms at the population level, followed by results that apply to these algorithms on a finite set of samples. Our conditions allow for a characterization of the region of convergence of EM-type iterates to a given population fixed point, that is, the region of the parameter space over which convergence is guaranteed to a point within a small neighborhood of the specified population fixed point. We verify our conditions and give tight characterizations of the region of convergence for three canonical problems of interest: symmetric mixture of two Gaussians, symmetric mixture of two regressions and linear regression with covariates missing completely at random.},
	author = {Sivaraman Balakrishnan and Martin J. Wainwright and Bin Yu},
	doi = {10.1214/16-AOS1435},
	journal = {The Annals of Statistics},
	keywords = {EM algorithm, first-order EM algorithm, maximum likelihood estimation, nonconvex optimization},
	number = {1},
	pages = {77 -- 120},
	publisher = {Institute of Mathematical Statistics},
	title = {{Statistical guarantees for the EM algorithm: From population to sample-based analysis}},
	url = {https://doi.org/10.1214/16-AOS1435},
	volume = {45},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1214/16-AOS1435}}


@inproceedings{ZhuangChenYang2022,
  author = {Zhuang, Yubo and Chen, Xiaohui and Yang, Yun},
  booktitle = {Proceedings of Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS)},
  date-added = {2022-09-14 20:28:06 -0500},
  date-modified = {2022-09-15 08:27:11 -0500},
  title = {Wasserstein $K$-means for clustering probability distributions},
  year = {2022},
}

@article{doi:10.1073/pnas.122653799,
	abstract = {A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well known---a collaboration network and a food web---and find that it detects significant and informative community divisions in both cases.},
	author = {M. Girvan and M. E. J. Newman},
	doi = {10.1073/pnas.122653799},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.122653799},
	journal = {Proceedings of the National Academy of Sciences},
	number = {12},
	pages = {7821-7826},
	title = {Community structure in social and biological networks},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.122653799},
	volume = {99},
	year = {2002},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.122653799},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.122653799}}

@article{10.1093/bib/bbz170,
	abstract = {{Clustering is central to many data-driven bioinformatics research and serves a powerful computational method. In particular, clustering helps at analyzing unstructured and high-dimensional data in the form of sequences, expressions, texts and images. Further, clustering is used to gain insights into biological processes in the genomics level, e.g. clustering of gene expressions provides insights on the natural structure inherent in the data, understanding gene functions, cellular processes, subtypes of cells and understanding gene regulations. Subsequently, clustering approaches, including hierarchical, centroid-based, distribution-based, density-based and self-organizing maps, have long been studied and used in classical machine learning settings. In contrast, deep learning (DL)-based representation and feature learning for clustering have not been reviewed and employed extensively. Since the quality of clustering is not only dependent on the distribution of data points but also on the learned representation, deep neural networks can be effective means to transform mappings from a high-dimensional data space into a lower-dimensional feature space, leading to improved clustering results. In this paper, we review state-of-the-art DL-based approaches for cluster analysis that are based on representation learning, which we hope to be useful, particularly for bioinformatics research. Further, we explore in detail the training procedures of DL-based clustering algorithms, point out different clustering quality metrics and evaluate several DL-based approaches on three bioinformatics use cases, including bioimaging, cancer genomics and biomedical text mining. We believe this review and the evaluation results will provide valuable insights and serve a starting point for researchers wanting to apply DL-based unsupervised methods to solve emerging bioinformatics research problems.}},
	author = {Karim, Md Rezaul and Beyan, Oya and Zappa, Achille and Costa, Ivan G and Rebholz-Schuhmann, Dietrich and Cochez, Michael and Decker, Stefan},
	doi = {10.1093/bib/bbz170},
	eprint = {https://academic.oup.com/bib/article-pdf/22/1/393/35934885/bbz170.pdf},
	issn = {1477-4054},
	journal = {Briefings in Bioinformatics},
	month = {02},
	number = {1},
	pages = {393-415},
	title = {{Deep learning-based clustering approaches for bioinformatics}},
	url = {https://doi.org/10.1093/bib/bbz170},
	volume = {22},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1093/bib/bbz170}}


@INPROCEEDINGS{5539868,
  author={Joulin, Armand and Bach, Francis and Ponce, Jean},
  booktitle={2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition}, 
  title={Discriminative clustering for image co-segmentation}, 
  year={2010},
  volume={},
  number={},
  pages={1943-1950},
  abstract={Purely bottom-up, unsupervised segmentation of a single image into foreground and background regions remains a challenging task for computer vision. Co-segmentation is the problem of simultaneously dividing multiple images into regions (segments) corresponding to different object classes. In this paper, we combine existing tools for bottom-up image segmentation such as normalized cuts, with kernel methods commonly used in object recognition. These two sets of techniques are used within a discriminative clustering framework: the goal is to assign foreground/background labels jointly to all images, so that a supervised classifier trained with these labels leads to maximal separation of the two classes. In practice, we obtain a combinatorial optimization problem which is relaxed to a continuous convex optimization problem, that can itself be solved efficiently for up to dozens of images. We illustrate the proposed method on images with very similar foreground objects, as well as on more challenging problems with objects with higher intra-class variations.},
  keywords={},
  doi={10.1109/CVPR.2010.5539868},
  ISSN={1063-6919},
  month={June},}
  
@ARTICLE{868688,
  author={Jianbo Shi and Malik, J.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Normalized cuts and image segmentation}, 
  year={2000},
  volume={22},
  number={8},
  pages={888-905},
  abstract={We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We applied this approach to segmenting static images, as well as motion sequences, and found the results to be very encouraging.},
  keywords={},
  doi={10.1109/34.868688},
  ISSN={1939-3539},
  month={Aug},}
  @inproceedings{pmlr-v151-zhuang22a,
  author = {Zhuang, Yubo and Chen, Xiaohui and Yang, Yun},
  booktitle = {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  date-added = {2022-08-29 09:39:43 -0500},
  date-modified = {2022-09-15 08:24:37 -0500},
  editor = {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  month = {28--30 Mar},
  pages = {9214--9246},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sketch-and-lift: scalable subsampled semidefinite program for K-means clustering},
  volume = {151},
  year = {2022},
}

@article{FraleyRaftery2002,
	abstract = { Cluster analysis is the automated search for groups of related observations in a dataset. Most clustering done in practice is based largely on heuristic but intuitively reasonable procedures, and most clustering methods available in commercial software are also of this type. However, there is little systematic guidance associated with these methods for solving important practical questions that arise in cluster analysis, such as how many clusters are there, which clustering method should be used, and how should outliers be handled. We review a general methodology for model-based clustering that provides a principled statistical approach to these issues. We also show that this can be useful for other problems in multivariate analysis, such as discriminant analysis and multivariate density estimation. We give examples from medical diagnosis, minefield detection, cluster recovery from noisy data, and spatial density estimation. Finally, we mention limitations of the methodology and discuss recent developments in model-based clustering for non-Gaussian data, high-dimensional datasets, large datasets, and Bayesian estimation. },
	author = {Chris Fraley and Adrian E Raftery},
	doi = {10.1198/016214502760047131},
	eprint = {https://doi.org/10.1198/016214502760047131},
	journal = {Journal of the American Statistical Association},
	number = {458},
	pages = {611-631},
	publisher = {Taylor & Francis},
	title = {Model-Based Clustering, Discriminant Analysis, and Density Estimation},
	url = {https://doi.org/10.1198/016214502760047131},
	volume = {97},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1198/016214502760047131}}

@inproceedings{JinZhangSilvaramanWainwrightJordan2016_EM,
author = {Jin, Chi and Zhang, Yuchen and Balakrishnan, Sivaraman and Wainwright, Martin J. and Jordan, Michael I.},
title = {Local Maxima in the Likelihood of Gaussian Mixture Models: Structural Results and Algorithmic Consequences},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We provide two fundamental results on the population (infinite-sample) likelihood function of Gaussian mixture models with M ≥ 3 components. Our first main result shows that the population likelihood function has bad local maxima even in the special case of equally-weighted mixtures of well-separated and spherical Gaussians. We prove that the log-likelihood value of these bad local maxima can be arbitrarily worse than that of any global optimum, thereby resolving an open question of Srebro [2007]. Our second main result shows that the EM algorithm (or a first-order variant of it) with random initialization will converge to bad critical points with probability at least 1 - e-Ω(M)). We further establish that a first-order variant of EM will not converge to strict saddle points almost surely, indicating that the poor performance of the first-order method can be attributed to the existence of bad local maxima rather than bad saddle points. Overall, our results highlight the necessity of careful initialization when using the EM algorithm in practice, even when applied in highly favorable settings.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {4123–4131},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}
@article{HunterLange2000_MM-algo,
author = { David R.   Hunter  and  Kenneth   Lange },
title = {Quantile Regression via an MM Algorithm},
journal = {Journal of Computational and Graphical Statistics},
volume = {9},
number = {1},
pages = {60-77},
year  = {2000},
publisher = {Taylor & Francis},
doi = {10.1080/10618600.2000.10474866},
abstract = { Abstract Quantile regression is an increasingly popular method for estimating the quantiles of a distribution conditional on the values of covariates. Regression quantiles are robust against the influence of outliers and, taken several at a time, they give a more complete picture of the conditional distribution than a single estimate of the center. This article first presents an iterative algorithm for finding sample quantiles without sorting and then explores a generalization of the algorithm to nonlinear quantile regression. Our quantile regression algorithm is termed an MM, or majorize—minimize, algorithm because it entails majorizing the objective function by a quadratic function followed by minimizing that quadratic. The algorithm is conceptually simple and easy to code, and our numerical tests suggest that it is computationally competitive with a recent interior point algorithm for most problems. }
}

@article{EM1977,
	abstract = {Summary A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
	author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
	doi = {https://doi.org/10.1111/j.2517-6161.1977.tb01600.x},
	eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1977.tb01600.x},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	keywords = {maximum likelihood, incomplete data, em algorithm, posterior mode},
	number = {1},
	pages = {1-22},
	title = {Maximum Likelihood from Incomplete Data Via the EM Algorithm},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1977.tb01600.x},
	volume = {39},
	year = {1977},
	bdsk-url-1 = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1977.tb01600.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.2517-6161.1977.tb01600.x}}

@article{Dasgupta2007,
	author = {Dasgupta, Sanjoy},
	date-added = {2019-09-28 03:36:22 +0000},
	date-modified = {2019-09-28 03:37:16 +0000},
	journal = {Technical Report CS2007-0890, University of California, San Diego},
	title = {The hardness of $k$-means clustering},
	year = {2007}}

@inproceedings{MahajanNimbhorkarVaradarajan2009,
	abstract = {In the k-means problem, we are given a finite set S of points in {\$}{\backslash}Re^m{\$}, and integer k{\thinspace}{\^a}¥{\thinspace}1, and we want to find k points (centers) so as to minimize the sum of the square of the Euclidean distance of each point in S to its nearest center. We show that this well-known problem is NP-hard even for instances in the plane, answering an open question posed by Dasgupta [6].},
	address = {Berlin, Heidelberg},
	author = {Mahajan, Meena and Nimbhorkar, Prajakta and Varadarajan, Kasturi},
	booktitle = {WALCOM: Algorithms and Computation},
	date-added = {2019-09-28 03:28:48 +0000},
	date-modified = {2019-09-28 03:29:11 +0000},
	editor = {Das, Sandip and Uehara, Ryuhei},
	isbn = {978-3-642-00202-1},
	pages = {274--285},
	publisher = {Springer Berlin Heidelberg},
	title = {The Planar k-Means Problem is NP-Hard},
	year = {2009}}


@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Xiaohui Chen at 2021-09-30 16:37:09 -0500 


%% Saved with string encoding Unicode (UTF-8) 


@inproceedings{kumar2010clustering,
  title={Clustering with spectral norm and the k-means algorithm},
  author={Kumar, Amit and Kannan, Ravindran},
  booktitle={2010 IEEE 51st Annual Symposium on Foundations of Computer Science},
  pages={299--308},
  year={2010},
  organization={IEEE}
}

@incollection{awasthi2012improved,
  title={Improved spectral-norm bounds for clustering},
  author={Awasthi, Pranjal and Sheffet, Or},
  booktitle={Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques},
  pages={37--49},
  year={2012},
  publisher={Springer}
}

@article{saxena2017review,
  title={A review of clustering techniques and developments},
  author={Saxena, Amit and Prasad, Mukesh and Gupta, Akshansh and Bharill, Neha and Patel, Om Prakash and Tiwari, Aruna and Er, Meng Joo and Ding, Weiping and Lin, Chin-Teng},
  journal={Neurocomputing},
  volume={267},
  pages={664--681},
  year={2017},
  publisher={Elsevier}
}

@article{Ndaoud2018,
	author = {Ndaoud, Mohamed},
	date-added = {2019-04-26 20:59:38 +0000},
	date-modified = {2019-04-26 21:01:49 +0000},
	journal = {arXiv:1812.08078},
	title = {Sharp optimal recovery in the Two Component Gaussian Mixture Model},
	year = {2018}}

@inproceedings{K-means++,
author = {Arthur, David and Vassilvitskii, Sergei},
title = {K-Means++: The Advantages of Careful Seeding},
year = {2007},
isbn = {9780898716245},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {The k-means method is a widely used clustering technique that seeks to minimize the
average squared distance between points in the same cluster. Although it offers no
accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting
k-means with a very simple, randomized seeding technique, we obtain an algorithm that
is Θ(logk)-competitive with the optimal clustering. Preliminary experiments show that
our augmentation improves both the speed and the accuracy of k-means, often quite
dramatically.},
booktitle = {Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {1027–1035},
numpages = {9},
location = {New Orleans, Louisiana},
series = {SODA '07}
}

@article{Alizadeh1995,
	author = {Alizadeh, Farid},
	journal = {SIAM J. Optim.},
	number = {1},
	pages = {13-51},
	title = {Interior Point Methods in Semidefinite Programming with Applications to Combinatorial Optimization},
	volume = {5},
	year = {1995}}
	
@misc{yurtsever2017sketchy,
      title={Sketchy Decisions: Convex Low-Rank Matrix Optimization with Optimal Storage}, 
      author={Alp Yurtsever and Madeleine Udell and Joel A. Tropp and Volkan Cevher},
      year={2017},
      eprint={1702.06838},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{BluhmFranca2019,
	abstract = {We show how to sketch semidefinite programs (SDPs) using positive maps in order to reduce their dimension. More precisely, we use Johnson--Lindenstrauss transforms to pro- duce a smaller SDP whose solution preserves feasibility or approximates the value of the original problem with high probability. These techniques allow to improve both complexity and storage space requirements. They apply to problems in which the Schatten 1-norm of the matrices specifying the SDP and also of a solution to the problem is constant in the problem size. Furthermore, we provide some results which clarify the limitations of positive, linear sketches in this setting.},
	author = {Andreas Bluhm and Daniel {Stilck Fran{\c c}a}},
	date-modified = {2021-10-07 21:32:27 -0500},
	issn = {0024-3795},
	journal = {Linear Algebra and its Applications},
	keywords = {Semidefinite programming, Sketching, Dimensionality reduction, Johnson--Lindenstrauss transforms},
	pages = {461-475},
	title = {Dimensionality reduction of SDPs through sketching},
	volume = {563},
	year = {2019},
	Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/S0024379518305354},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.laa.2018.11.012}}


@inproceedings{Awasthi2015_ITCS,
	acmid = {2688116},
	address = {New York, NY, USA},
	author = {Awasthi, Pranjal and Bandeira, Afonso S. and Charikar, Moses and Krishnaswamy, Ravishankar and Villar, Soledad and Ward, Rachel},
	booktitle = {Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
	date-added = {2021-09-30 16:37:06 -0500},
	date-modified = {2021-09-30 16:37:06 -0500},
	isbn = {978-1-4503-3333-7},
	location = {Rehovot, Israel},
	numpages = {10},
	pages = {191--200},
	publisher = {ACM},
	series = {ITCS '15},
	title = {Relax, No Need to Round: Integrality of Clustering Formulations},
	year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2688073.2688116},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/2688073.2688116}}

@article{LaurentMassart2000,
	author = {B. Laurent and P. Massart},
	date-added = {2021-09-29 22:41:07 -0500},
	date-modified = {2021-09-29 22:41:23 -0500},
	journal = {The Annals of Statistics},
	keywords = {$l_p$-bodies, adaptive estimation, Besov bodies, efficient estimation, Gaussian sequence model, Model selection, quadratic functionals},
	number = {5},
	pages = {1302 -- 1338},
	publisher = {Institute of Mathematical Statistics},
	title = {{Adaptive estimation of a quadratic functional by model selection}},
	volume = {28},
	year = {2000},
	Bdsk-Url-1 = {https://doi.org/10.1214/aos/1015957395}}

@misc{abdalla2021community,
	archiveprefix = {arXiv},
	author = {Pedro Abdalla and Afonso S. Bandeira},
	date-added = {2021-09-29 13:30:39 -0500},
	date-modified = {2021-09-29 13:30:39 -0500},
	eprint = {2102.01419},
	primaryclass = {math.OC},
	title = {Community Detection with a Subsampled Semidefinite Program},
	year = {2021}}

@misc{mixon2020sketching,
	archiveprefix = {arXiv},
	author = {Dustin G. Mixon and Kaiying Xie},
	date-added = {2021-09-29 13:28:33 -0500},
	date-modified = {2021-09-29 13:28:33 -0500},
	eprint = {2008.04270},
	primaryclass = {cs.IT},
	title = {Sketching semidefinite programs for faster clustering},
	year = {2020}}

@inproceedings{Jiang2020_FOCS,
	author = {Jiang, Haotian and Kathuria, Tarun and Lee, Yin Tat and Padmanabhan, Swati and Song, Zhao},
	booktitle = {2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS)},
	date-added = {2021-09-26 21:38:36 -0500},
	date-modified = {2021-09-26 21:38:57 -0500},
	pages = {910-918},
	title = {A Faster Interior Point Method for Semidefinite Programming},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1109/FOCS46700.2020.00089}}

@article{CHEN2021303,
	abstract = {We introduce the diffusion K-means clustering method on Riemannian submanifolds, which maximizes the within-cluster connectedness based on the diffusion distance. The diffusion K-means constructs a random walk on the similarity graph with vertices as data points randomly sampled on the manifolds and edges as similarities given by a kernel that captures the local geometry of manifolds. The diffusion K-means is a multi-scale clustering tool that is suitable for data with non-linear and non-Euclidean geometric features in mixed dimensions. Given the number of clusters, we propose a polynomial-time convex relaxation algorithm via the semidefinite programming (SDP) to solve the diffusion K-means. In addition, we also propose a nuclear norm regularized SDP that is adaptive to the number of clusters. In both cases, we show that exact recovery of the SDPs for diffusion K-means can be achieved under suitable between-cluster separability and within-cluster connectedness of the submanifolds, which together quantify the hardness of the manifold clustering problem. We further propose the localized diffusion K-means by using the local adaptive bandwidth estimated from the nearest neighbors. We show that exact recovery of the localized diffusion K-means is fully adaptive to the local probability density and geometric structures of the underlying submanifolds.},
	author = {Xiaohui Chen and Yun Yang},
	date-added = {2021-09-26 21:20:02 -0500},
	date-modified = {2021-09-26 21:20:02 -0500},
	issn = {1063-5203},
	journal = {Applied and Computational Harmonic Analysis},
	keywords = {Manifold clustering, -means, Riemannian submanifolds, Diffusion distance, Semidefinite programming, Random walk on random graphs, Laplace-Beltrami operator, Mixing times, Adaptivity},
	pages = {303-347},
	title = {Diffusion K-means clustering on manifolds: Provable exact recovery via semidefinite relaxations},
	volume = {52},
	year = {2021},
	Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/S106352032030021X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.acha.2020.03.002}}

@misc{drineas2017lectures,
	archiveprefix = {arXiv},
	author = {Petros Drineas and Michael W. Mahoney},
	date-added = {2021-09-22 11:35:06 -0500},
	date-modified = {2021-09-22 11:35:06 -0500},
	eprint = {1712.08880},
	primaryclass = {cs.DS},
	title = {Lectures on Randomized Numerical Linear Algebra},
	year = {2017}}

@article{chen2021cutoff,
	author = {Chen, Xiaohui and Yang, Yun},
	date-added = {2021-09-22 10:56:12 -0500},
	date-modified = {2021-09-22 10:56:12 -0500},
	journal = {IEEE Transactions on Information Theory},
	number = {6},
	pages = {4223--4238},
	title = {Cutoff for exact recovery of Gaussian mixture models},
	volume = {67},
	year = {2021}}

@article{BuneaGiraudRoyerVerzelen2016,
	author = {Bunea, Florentina and Giraud, Christophe and Royer, Martin and Verzelen, Nicolas},
	date-added = {2021-09-22 10:55:32 -0500},
	date-modified = {2021-09-22 10:55:32 -0500},
	journal = {arXiv:1606.05100},
	title = {{PECOK: a convex optimization approach to variable clustering}},
	year = {2016}}

@article{GiraudVerzelen2018,
	author = {Giraud, Christophe and Verzelen, Nicolas},
	date-added = {2021-09-22 10:55:13 -0500},
	date-modified = {2021-09-22 10:55:13 -0500},
	journal = {arXiv:1807.07547},
	title = {Partial recovery bounds for clustering with the relaxed $K$means},
	year = {2018}}

@incollection{Royer2017_NIPS,
	author = {Royer, Martin},
	booktitle = {Advances in Neural Information Processing Systems 30},
	date-added = {2021-09-22 10:54:42 -0500},
	date-modified = {2021-09-22 10:54:42 -0500},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {1795--1803},
	publisher = {Curran Associates, Inc.},
	title = {Adaptive Clustering through Semidefinite Programming},
	year = {2017},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/6776-adaptive-clustering-through-semidefinite-programming.pdf}}

@article{FeiChen2018,
	author = {Fei, Yingjie and Chen, Yudong},
	date-added = {2021-09-22 10:54:18 -0500},
	date-modified = {2021-09-22 10:54:18 -0500},
	journal = {arXiv:1803.06510},
	title = {Hidden integrality of SDP relaxation for sub-Gaussian mixture models},
	year = {2018}}

@article{LiLiLingStohmerWei2017,
	author = {Li, Xiaodong and Li, Yang and Ling, Shuyang and Stohmer, Thomas and Wei, Ke},
	date-added = {2021-09-22 10:53:51 -0500},
	date-modified = {2021-09-22 10:53:51 -0500},
	journal = {arXiv:1710.06008},
	title = {When do birds of a feather flock together? $k$-means, proximity, and conic programming},
	year = {2017}}

@article{MixonVillarWard2016,
	author = {Mixon, Dustin G and Villar, Soledad and Ward, Rachel},
	date-added = {2021-09-22 10:53:29 -0500},
	date-modified = {2021-09-22 10:53:29 -0500},
	journal = {arXiv:1602.06612v2},
	title = {Clustering subgaussian mixtures by semidefinite programming},
	year = {2016}}

@article{PengWei2007_SIAMJOPTIM,
	author = {Peng, Jiming and Wei, Yu},
	date-added = {2021-09-22 10:52:37 -0500},
	date-modified = {2021-09-22 10:52:37 -0500},
	journal = {SIAM J. OPTIM},
	number = {1},
	pages = {186-205},
	title = {Approximating $K$-means-type clustering via semidefinite programming},
	volume = {18},
	year = {2007}}

@article{vonLuxburgBelkinBousquet2008_AoS,
	author = {von Luxburg, Ulrike and Belkin, Mikhail and Bousquet, Olivier},
	date-added = {2021-09-22 10:41:42 -0500},
	date-modified = {2021-09-22 10:41:42 -0500},
	journal = {Annals of Statistics},
	number = {2},
	pages = {555-586},
	title = {Consistency of spectral clustering},
	volume = {36},
	year = {2008}}

@article{vanLuxburg2007_spectralclustering,
	author = {von Luxburg, Ulrike},
	date-added = {2021-09-22 10:41:21 -0500},
	date-modified = {2021-09-22 10:41:21 -0500},
	journal = {Statistics and Computing},
	number = {4},
	pages = {395-416},
	title = {A tutorial on spectral clustering},
	volume = {17},
	year = {2007}}

@inproceedings{Achlioptas2005McSherry,
	abstract = {We consider the problem of learning mixtures of distributions via spectral methods and derive a characterization of when such methods are useful. Specifically, given a mixture-sample, let {\$}{\backslash}bar{\backslash}mu{\_}{\{}i{\}}, {\{}{\backslash}bar C{\_}{\{}i{\}}{\}}, {\backslash}bar w{\_}{\{}i{\}}{\$}denote the empirical mean, covariance matrix, and mixing weight of the samples from the i-th component. We prove that a very simple algorithm, namely spectral projection followed by single-linkage clustering, properly classifies every point in the sample provided that each pair of means {\$}{\backslash}bar{\backslash}mu{\_}{\{}i{\}},{\backslash}bar{\backslash}mu{\_}{\{}j{\}}{\$}is well separated, in the sense that {\$}{\backslash}|{\backslash}bar{\backslash}mu{\_}{\{}i{\}} - {\backslash}bar{\backslash}mu{\_}{\{}j{\}}{\backslash}|^{\{}2{\}}{\$}is at least {\$}{\backslash}|{\{}{\backslash}bar C{\_}{\{}i{\}}{\backslash}|{\_}{\{}2{\}}(1/{\backslash}bar w{\_}{\{}i{\}}+1/{\backslash}bar w{\_}{\{}j{\}}){\}}{\$}plus a term that depends on the concentration properties of the distributions in the mixture. This second term is very small for many distributions, including Gaussians, Log-concave, and many others. As a result, we get the best known bounds for learning mixtures of arbitrary Gaussians in terms of the required mean separation. At the same time, we prove that there are many Gaussian mixtures {\{}($\mu$i,Ci,wi){\}} such that each pair of means is separated by ||Ci||2(1/wi{\thinspace}+{\thinspace}1/wj), yet upon spectral projection the mixture collapses completely, i.e., all means and covariance matrices in the projected mixture are identical.},
	address = {Berlin, Heidelberg},
	author = {Achlioptas, Dimitris and McSherry, Frank},
	booktitle = {Learning Theory},
	date-added = {2021-09-22 10:40:46 -0500},
	date-modified = {2021-09-22 10:40:46 -0500},
	editor = {Auer, Peter and Meir, Ron},
	isbn = {978-3-540-31892-7},
	pages = {458--469},
	publisher = {Springer Berlin Heidelberg},
	title = {On Spectral Learning of Mixtures of Distributions},
	year = {2005}}

@article{Vempala04aspectral,
	author = {Santosh Vempala and Grant Wang},
	date-added = {2021-09-22 10:40:20 -0500},
	date-modified = {2021-09-22 10:40:20 -0500},
	journal = {J. Comput. Syst. Sci},
	pages = {2004},
	title = {A spectral algorithm for learning mixture models},
	volume = {68},
	year = {2004}}

@inproceedings{NgJordanWeiss2001_NIPS,
	author = {Andrew Y. Ng and Michael I. Jordan and Yair Weiss},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2021-09-22 10:39:58 -0500},
	date-modified = {2021-09-22 10:39:58 -0500},
	pages = {849--856},
	publisher = {MIT Press},
	title = {On Spectral Clustering: Analysis and an algorithm},
	year = {2001}}

@inproceedings{Meila01learningsegmentation,
	author = {Marina Meila and Jianbo Shi},
	booktitle = {In Advances in Neural Information Processing Systems},
	date-added = {2021-09-22 10:39:36 -0500},
	date-modified = {2021-09-22 10:39:36 -0500},
	pages = {873--879},
	publisher = {MIT Press},
	title = {Learning Segmentation by Random Walks},
	year = {2001}}

@article{LuZhou2016,
	author = {Lu, Yu and Zhou, Harrison},
	date-added = {2021-09-22 10:38:42 -0500},
	date-modified = {2021-09-22 10:38:42 -0500},
	journal = {arXiv:1612.02099},
	title = {Statistical and Computational Guarantees of Lloyd's Algorithm and its Variants},
	year = {2016}}

@article{Lloyd1982_TIT,
	author = {Lloyd, Stuart},
	date-added = {2021-09-22 10:38:21 -0500},
	date-modified = {2021-09-22 10:38:21 -0500},
	journal = {IEEE Transactions on Information Theory},
	pages = {129-137},
	title = {Least squares quantization in PCM},
	volume = {28},
	year = {1982}}

@article{aloise2009np,
	author = {Aloise, Daniel and Deshpande, Amit and Hansen, Pierre and Popat, Preyas},
	date-added = {2021-09-22 10:37:17 -0500},
	date-modified = {2021-09-22 10:37:17 -0500},
	journal = {Machine learning},
	number = {2},
	pages = {245--248},
	title = {NP-hardness of Euclidean sum-of-squares clustering},
	volume = {75},
	year = {2009}}

@article{pollard1981,
	author = {Pollard, David},
	date-added = {2021-09-22 10:36:48 -0500},
	date-modified = {2021-09-22 10:36:48 -0500},
	fjournal = {The Annals of Statistics},
	journal = {Ann. Statist.},
	number = {1},
	pages = {135--140},
	publisher = {The Institute of Mathematical Statistics},
	title = {Strong Consistency of $K$-Means Clustering},
	volume = {9},
	year = {1981},
	Bdsk-Url-1 = {https://doi.org/10.1214/aos/1176345339}}

@article{MacQueen1967_kmeans,
	author = {MacQueen, J.B.},
	date-added = {2021-09-22 10:36:34 -0500},
	date-modified = {2021-09-22 10:36:34 -0500},
	journal = {Proc. Fifth Berkeley Sympos. Math. Statist. and Probability},
	pages = {281-297},
	title = {Some methods for classification and analysis of multivariate observations},
	year = {1967}}

@article{LEVINE2015184,
title = {Data-Driven Phenotypic Dissection of AML Reveals Progenitor-like Cells that Correlate with Prognosis},
journal = {Cell},
volume = {162},
number = {1},
pages = {184-197},
year = {2015},
issn = {0092-8674},
author = {Jacob H. Levine and Erin F. Simonds and Sean C. Bendall and Kara L. Davis and El-ad D. Amir and Michelle D. Tadmor and Oren Litvin and Harris G. Fienberg and Astraea Jager and Eli R. Zunder and Rachel Finck and Amanda L. Gedman and Ina Radtke and James R. Downing and Dana Pe’er and Garry P. Nolan},
abstract = {Summary
Acute myeloid leukemia (AML) manifests as phenotypically and functionally diverse cells, often within the same patient. Intratumor phenotypic and functional heterogeneity have been linked primarily by physical sorting experiments, which assume that functionally distinct subpopulations can be prospectively isolated by surface phenotypes. This assumption has proven problematic, and we therefore developed a data-driven approach. Using mass cytometry, we profiled surface and intracellular signaling proteins simultaneously in millions of healthy and leukemic cells. We developed PhenoGraph, which algorithmically defines phenotypes in high-dimensional single-cell data. PhenoGraph revealed that the surface phenotypes of leukemic blasts do not necessarily reflect their intracellular state. Using hematopoietic progenitors, we defined a signaling-based measure of cellular phenotype, which led to isolation of a gene expression signature that was predictive of survival in independent cohorts. This study presents new methods for large-scale analysis of single-cell heterogeneity and demonstrates their utility, yielding insights into AML pathophysiology.}
}

@article{UnbalanceSet,
    author = {M. Rezaei and P. Fr\"anti},
    title = {Set-matching methods for external cluster validity},
    journal = {IEEE Trans. on Knowledge and Data Engineering},
    year = {2016},
    volume = {28},
    number = {8},
    pages = {2173--2186}
}
