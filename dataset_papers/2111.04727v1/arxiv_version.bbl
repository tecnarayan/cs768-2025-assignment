\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{GKLW18}

\bibitem[ATV21]{awasthi2021efficient}
Pranjal Awasthi, Alex Tang, and Aravindan Vijayaraghavan.
\newblock Efficient algorithms for learning depth-2 neural networks with
  general relu activations.
\newblock {\em arXiv preprint arXiv:2107.10209}, 2021.

\bibitem[BJW19]{bakshi2019learning}
Ainesh Bakshi, Rajesh Jayaram, and David~P Woodruff.
\newblock Learning two layer rectified neural networks in polynomial time.
\newblock In {\em Conference on Learning Theory}, pages 195--268. PMLR, 2019.

\bibitem[CJM20]{CarliniJM20}
Nicholas Carlini, Matthew Jagielski, and Ilya Mironov.
\newblock Cryptanalytic extraction of neural network models.
\newblock In Daniele Micciancio and Thomas Ristenpart, editors, {\em Advances
  in Cryptology - CRYPTO 2020 - 40th Annual International Cryptology
  Conference, CRYPTO 2020, Santa Barbara, CA, USA, August 17-21, 2020,
  Proceedings, Part III}, volume 12172 of {\em Lecture Notes in Computer
  Science}, pages 189--218. Springer, 2020.

\bibitem[CKM20]{CKM}
Sitan Chen, Adam Klivans, and Raghu Meka.
\newblock Learning deep relu networks is fixed parameter tractable.
\newblock {\em arXiv preprint arXiv:2009.13512}, 2020.

\bibitem[CM20]{chen2020learning}
Sitan Chen and Raghu Meka.
\newblock Learning polynomials of few relevant dimensions.
\newblock {\em arXiv preprint arXiv:2004.13748}, 2020.

\bibitem[CS09]{ChoS09}
Youngmin Cho and Lawrence~K. Saul.
\newblock Kernel methods for deep learning.
\newblock In Yoshua Bengio, Dale Schuurmans, John~D. Lafferty, Christopher
  K.~I. Williams, and Aron Culotta, editors, {\em Advances in Neural
  Information Processing Systems 22: 23rd Annual Conference on Neural
  Information Processing Systems 2009. Proceedings of a meeting held 7-10
  December 2009, Vancouver, British Columbia, Canada}, pages 342--350. Curran
  Associates, Inc, 2009.

\bibitem[CW01]{CW01}
A.~Carbery and J.~Wright.
\newblock {Distributional and $L^q$ norm inequalities for polynomials over
  convex bodies in $R^n$}.
\newblock {\em Mathematical Research Letters}, 8(3):233--248, 2001.

\bibitem[DG21]{daniely2021exact}
Amit Daniely and Elad Granot.
\newblock An exact poly-time membership-queries algorithm for extraction a
  three-layer relu network.
\newblock {\em arXiv preprint arXiv:2105.09673}, 2021.

\bibitem[DKKZ20]{diakonikolas2020algorithms}
Ilias Diakonikolas, Daniel~M Kane, Vasilis Kontonis, and Nikos Zarifis.
\newblock Algorithms and sq lower bounds for pac learning one-hidden-layer relu
  networks.
\newblock In {\em Conference on Learning Theory}, pages 1514--1539, 2020.

\bibitem[DV20]{daniely2020hardness}
Amit Daniely and Gal Vardi.
\newblock Hardness of learning neural networks with natural weights.
\newblock {\em arXiv preprint arXiv:2006.03177}, 2020.

\bibitem[DV21]{danielyprg}
Amit Daniely and Gal Vardi.
\newblock From local pseudorandom generators to hardness of learning.
\newblock {\em CoRR}, abs/2101.08303, 2021.

\bibitem[GGJ{\etalchar{+}}20]{goel2020superpolynomial}
Surbhi Goel, Aravind Gollakota, Zhihan Jin, Sushrut Karmalkar, and Adam
  Klivans.
\newblock Superpolynomial lower bounds for learning one-layer neural networks
  using gradient descent.
\newblock {\em arXiv preprint arXiv:2006.12011}, 2020.

\bibitem[GKLW18]{ge2018learning2}
Rong Ge, Rohith Kuditipudi, Zhize Li, and Xiang Wang.
\newblock Learning two-layer neural networks with symmetric inputs.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem[GKM18]{convotron}
Surbhi Goel, Adam~R. Klivans, and Raghu Meka.
\newblock Learning one convolutional layer with overlapping patches.
\newblock In Jennifer~G. Dy and Andreas~Krause 0001, editors, {\em ICML},
  volume~80 of {\em Proceedings of Machine Learning Research}, pages
  1778--1786. PMLR, 2018.

\bibitem[GLM18]{ge2018learning}
Rong Ge, Jason~D Lee, and Tengyu Ma.
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock In {\em 6th International Conference on Learning Representations,
  ICLR 2018}, 2018.

\bibitem[Han09]{hanneke2009theoretical}
Steve Hanneke.
\newblock {\em Theoretical foundations of active learning}.
\newblock Carnegie Mellon University, 2009.

\bibitem[JCB{\etalchar{+}}20]{JagielskiCBKP20}
Matthew Jagielski, Nicholas Carlini, David Berthelot, Alex Kurakin, and Nicolas
  Papernot.
\newblock High accuracy and high fidelity extraction of neural networks.
\newblock In Srdjan Capkun and Franziska Roesner, editors, {\em 29th USENIX
  Security Symposium, USENIX Security 2020, August 12-14, 2020}, pages
  1345--1362. USENIX Association, 2020.

\bibitem[JSA15]{janzamin2015beating}
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar.
\newblock Beating the perils of non-convexity: Guaranteed training of neural
  networks using tensor methods.
\newblock {\em arXiv}, pages arXiv--1506, 2015.

\bibitem[JWZ20]{jayaram2020span}
Rajesh Jayaram, David~P. Woodruff, and Qiuyi Zhang.
\newblock Span recovery for deep neural networks with applications to input
  obfuscation, 2020.

\bibitem[LMZ20]{LiMaZhang}
Yuanzhi Li, Tengyu Ma, and Hongyang~R. Zhang.
\newblock Learning over-parametrized two-layer relu neural networks beyond ntk.
\newblock {\em CoRR}, abs/2007.04596, 2020.

\bibitem[MSDH19]{MilliSDH19}
Smitha Milli, Ludwig Schmidt, Anca~D. Dragan, and Moritz Hardt.
\newblock Model reconstruction from model explanations.
\newblock In {\em FAT}, pages 1--9. ACM, 2019.

\bibitem[PMG{\etalchar{+}}17]{Papernot}
Nicolas Papernot, Patrick~D. McDaniel, Ian~J. Goodfellow, Somesh Jha, Z.~Berkay
  Celik, and Ananthram Swami.
\newblock Practical black-box attacks against machine learning.
\newblock In Ramesh Karri, Ozgur Sinanoglu, Ahmad-Reza Sadeghi, and Xun Yi,
  editors, {\em Proceedings of the 2017 ACM on Asia Conference on Computer and
  Communications Security, AsiaCCS 2017, Abu Dhabi, United Arab Emirates, April
  2-6, 2017}, pages 506--519. ACM, 2017.

\bibitem[RK20]{RolnickK20}
David Rolnick and Konrad~P. Kording.
\newblock Reverse-engineering deep relu networks.
\newblock In {\em ICML}, volume 119 of {\em Proceedings of Machine Learning
  Research}, pages 8178--8187. PMLR, 2020.

\bibitem[TJ{\etalchar{+}}16]{TramerZJRR16}
Florian Tram{\`e}r, Fan~Zhang 0022, Ari Juels, Michael~K. Reiter, and Thomas
  Ristenpart.
\newblock Stealing machine learning models via prediction apis.
\newblock {\em CoRR}, abs/1609.02943, 2016.

\bibitem[Ver18]{vershynin2018high}
Roman Vershynin.
\newblock {\em High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[ZSJ{\etalchar{+}}17]{zhong2017recovery}
Kai Zhong, Zhao Song, Prateek Jain, Peter~L Bartlett, and Inderjit~S Dhillon.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 4140--4149, 2017.

\bibitem[ZYWG19]{zgu}
Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu.
\newblock Learning one-hidden-layer relu networks via gradient descent.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 1524--1534. PMLR, 2019.

\end{thebibliography}
