\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[OpenAI(2023)]{DBLP:journals/corr/abs-2303-08774}
OpenAI.
\newblock {GPT-4} technical report.
\newblock \emph{CoRR}, abs/2303.08774, 2023.
\newblock \doi{10.48550/arXiv.2303.08774}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2303.08774}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozi{\`{e}}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and
  Lample]{DBLP:journals/corr/abs-2302-13971}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie{-}Anne
  Lachaux, Timoth{\'{e}}e Lacroix, Baptiste Rozi{\`{e}}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, Aur{\'{e}}lien Rodriguez, Armand Joulin, Edouard Grave,
  and Guillaume Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{CoRR}, abs/2302.13971, 2023.
\newblock \doi{10.48550/arXiv.2302.13971}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2302.13971}.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin,
  Liang, and Hashimoto]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{NIPS2017_3f5ee243}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{DBLP:conf/nips/HoJA20}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
  Maria{-}Florina Balcan, and Hsuan{-}Tien Lin, editors, \emph{Advances in
  Neural Information Processing Systems 33: Annual Conference on Neural
  Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
  virtual}, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html}.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Thickstun, Gulrajani, Liang, and
  Hashimoto]{li2022diffusionlm}
Xiang~Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori
  Hashimoto.
\newblock Diffusion-{LM} improves controllable text generation.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=3s9IrEsjLyk}.

\bibitem[Gong et~al.(2022)Gong, Li, Feng, Wu, and
  Kong]{DBLP:journals/corr/abs-2210-08933}
Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and Lingpeng Kong.
\newblock Diffuseq: Sequence to sequence text generation with diffusion models.
\newblock \emph{CoRR}, abs/2210.08933, 2022.
\newblock \doi{10.48550/arXiv.2210.08933}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2210.08933}.

\bibitem[Dieleman et~al.(2022)Dieleman, Sartran, Roshannai, Savinov, Ganin,
  Richemond, Doucet, Strudel, Dyer, Durkan, et~al.]{dieleman2022continuous}
Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav
  Ganin, Pierre~H Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor
  Durkan, et~al.
\newblock Continuous diffusion for categorical data.
\newblock \emph{arXiv preprint arXiv:2211.15089}, 2022.

\bibitem[Yuan et~al.(2022)Yuan, Yuan, Tan, Huang, and
  Huang]{yuan2022seqdiffuseq}
Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, and Songfang Huang.
\newblock Seqdiffuseq: Text diffusion with encoder-decoder transformers, 2022.

\bibitem[Ye et~al.(2023)Ye, Zheng, Bao, Qian, and Wang]{ye2023dinoiser}
Jiasheng Ye, Zaixiang Zheng, Yu~Bao, Lihua Qian, and Mingxuan Wang.
\newblock Dinoiser: Diffused conditional sequence learning by manipulating
  noises, 2023.

\bibitem[Lewis et~al.(2020)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer]{DBLP:conf/acl/LewisLGGMLSZ20}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.
\newblock {BART:} denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel~R. Tetreault,
  editors, \emph{Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics, {ACL} 2020, Online, July 5-10, 2020}, pages
  7871--7880. Association for Computational Linguistics, 2020.
\newblock \doi{10.18653/v1/2020.acl-main.703}.
\newblock URL \url{https://doi.org/10.18653/v1/2020.acl-main.703}.

\bibitem[Qi et~al.(2020)Qi, Yan, Gong, Liu, Duan, Chen, Zhang, and
  Zhou]{qi-etal-2020-prophetnet}
Weizhen Qi, Yu~Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei
  Zhang, and Ming Zhou.
\newblock {P}rophet{N}et: Predicting future n-gram for
  sequence-to-{S}equence{P}re-training.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pages 2401--2410, Online, November 2020. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2020.findings-emnlp.217}.
\newblock URL \url{https://aclanthology.org/2020.findings-emnlp.217}.

\bibitem[Qi et~al.(2021)Qi, Gong, Jiao, Yan, Chen, Liu, Tang, Li, Chen, Zhang,
  et~al.]{qi2021bang}
Weizhen Qi, Yeyun Gong, Jian Jiao, Yu~Yan, Weizhu Chen, Dayiheng Liu, Kewen
  Tang, Houqiang Li, Jiusheng Chen, Ruofei Zhang, et~al.
\newblock Bang: Bridging autoregressive and non-autoregressive generation with
  large scale pretraining.
\newblock In \emph{International Conference on Machine Learning}, pages
  8630--8639. PMLR, 2021.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Tang, Zhao, Nie, and
  Wen]{DBLP:conf/emnlp/LiTZNW22}
Junyi Li, Tianyi Tang, Wayne~Xin Zhao, Jian{-}Yun Nie, and Ji{-}Rong Wen.
\newblock {ELMER:} {A} non-autoregressive pre-trained language model for
  efficient and effective text generation.
\newblock In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,
  \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural
  Language Processing, {EMNLP} 2022, Abu Dhabi, United Arab Emirates, December
  7-11, 2022}, pages 1044--1058. Association for Computational Linguistics,
  2022{\natexlab{b}}.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.68}.

\bibitem[Li et~al.(2022{\natexlab{c}})Li, Cui, Yin, and
  Zhang]{DBLP:conf/emnlp/LiCY022}
Yafu Li, Leyang Cui, Yongjing Yin, and Yue Zhang.
\newblock Multi-granularity optimization for non-autoregressive translation.
\newblock In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,
  \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural
  Language Processing, {EMNLP} 2022, Abu Dhabi, United Arab Emirates, December
  7-11, 2022}, pages 5073--5084. Association for Computational Linguistics,
  2022{\natexlab{c}}.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.339}.

\bibitem[Bao et~al.(2021)Bao, Huang, Xiao, Wang, Dai, and
  Chen]{bao-etal-2021-non}
Yu~Bao, Shujian Huang, Tong Xiao, Dongqi Wang, Xinyu Dai, and Jiajun Chen.
\newblock Non-autoregressive translation by learning target categorical codes.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 5749--5759, Online, June 2021. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.458}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.458}.

\bibitem[Lin et~al.(2023)Lin, Gong, Shen, Wu, Fan, Lin, Duan, and
  Chen]{lin2023text}
Zhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Nan Duan,
  and Weizhu Chen.
\newblock Text generation with diffusion language models: A pre-training
  approach with continuous paragraph denoise, 2023.

\bibitem[Gu et~al.(2017)Gu, Bradbury, Xiong, Li, and Socher]{gu2017non}
Jiatao Gu, James Bradbury, Caiming Xiong, Victor~OK Li, and Richard Socher.
\newblock Non-autoregressive neural machine translation.
\newblock \emph{arXiv preprint arXiv:1711.02281}, 2017.

\bibitem[Narayan et~al.(2018)Narayan, Cohen, and
  Lapata]{narayan-etal-2018-dont}
Shashi Narayan, Shay~B. Cohen, and Mirella Lapata.
\newblock Don{'}t give me the details, just the summary! topic-aware
  convolutional neural networks for extreme summarization.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 1797--1807, Brussels, Belgium,
  October-November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-1206}.
\newblock URL \url{https://aclanthology.org/D18-1206}.

\bibitem[Hermann et~al.(2015)Hermann, Kocisky, Grefenstette, Espeholt, Kay,
  Suleyman, and Blunsom]{NIPS2015_afdec700}
Karl~Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will
  Kay, Mustafa Suleyman, and Phil Blunsom.
\newblock Teaching machines to read and comprehend.
\newblock In C.~Cortes, N.~Lawrence, D.~Lee, M.~Sugiyama, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~28.
  Curran Associates, Inc., 2015.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2015/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf}.

\bibitem[Kudo and Richardson(2018)]{kudo-richardson-2018-sentencepiece}
Taku Kudo and John Richardson.
\newblock {S}entence{P}iece: A simple and language independent subword
  tokenizer and detokenizer for neural text processing.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 66--71, Brussels,
  Belgium, November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-2012}.
\newblock URL \url{https://aclanthology.org/D18-2012}.

\bibitem[Lee et~al.(2018)Lee, Mansimov, and Cho]{lee2018deterministic}
Jason Lee, Elman Mansimov, and Kyunghyun Cho.
\newblock Deterministic non-autoregressive neural sequence modeling by
  iterative refinement.
\newblock \emph{arXiv preprint arXiv:1802.06901}, 2018.

\bibitem[Ghazvininejad et~al.(2019)Ghazvininejad, Levy, Liu, and
  Zettlemoyer]{ghazvininejad-etal-2019-mask}
Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer.
\newblock Mask-predict: Parallel decoding of conditional masked language
  models.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 6112--6121, Hong Kong,
  China, November 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-1633}.
\newblock URL \url{https://aclanthology.org/D19-1633}.

\bibitem[Gu et~al.(2019)Gu, Wang, and Zhao]{gu2019levenshtein}
Jiatao Gu, Changhan Wang, and Junbo Zhao.
\newblock Levenshtein transformer.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  11181--11191, 2019.

\bibitem[Stern et~al.(2019)Stern, Chan, Kiros, and
  Uszkoreit]{stern2019insertion}
Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit.
\newblock Insertion transformer: Flexible sequence generation via insertion
  operations.
\newblock \emph{arXiv preprint arXiv:1902.03249}, 2019.

\bibitem[Gu et~al.(2016)Gu, Lu, Li, and Li]{GuLLL16}
Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.~K. Li.
\newblock Incorporating copying mechanism in sequence-to-sequence learning.
\newblock In \emph{{ACL} {(1)}}. The Association for Computer Linguistics,
  2016.

\bibitem[Greff et~al.(2017)Greff, Srivastava, Koutn{\'{\i}}k, Steunebrink, and
  Schmidhuber]{GreffSKSS17}
Klaus Greff, Rupesh~Kumar Srivastava, Jan Koutn{\'{\i}}k, Bas~R. Steunebrink,
  and J{\"{u}}rgen Schmidhuber.
\newblock {LSTM:} {A} search space odyssey.
\newblock \emph{{IEEE} Trans. Neural Networks Learn. Syst.}, 28\penalty0
  (10):\penalty0 2222--2232, 2017.

\bibitem[Lin et~al.(2020)Lin, Zhou, Shen, Zhou, Bhagavatula, Choi, and
  Ren]{lin-etal-2020-commongen}
Bill~Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula,
  Yejin Choi, and Xiang Ren.
\newblock {C}ommon{G}en: A constrained text generation challenge for generative
  commonsense reasoning.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pages 1823--1840, Online, November 2020. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2020.findings-emnlp.165}.
\newblock URL \url{https://aclanthology.org/2020.findings-emnlp.165}.

\bibitem[Kumar and Byrne(2004)]{kumar2004minimum}
Shankar Kumar and William Byrne.
\newblock Minimum bayes-risk decoding for statistical machine translation.
\newblock Technical report, JOHNS HOPKINS UNIV BALTIMORE MD CENTER FOR LANGUAGE
  AND SPEECH PROCESSING (CLSP), 2004.

\bibitem[Liu et~al.(2021)Liu, Yan, Gong, Qi, Zhang, Jiao, Chen, Fu, Shou, Gong,
  et~al.]{liu2021glge}
Dayiheng Liu, Yu~Yan, Yeyun Gong, Weizhen Qi, Hang Zhang, Jian Jiao, Weizhu
  Chen, Jie Fu, Linjun Shou, Ming Gong, et~al.
\newblock Glge: A new general language generation evaluation benchmark.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL-IJCNLP 2021}, pages 408--420, 2021.

\bibitem[Susanto et~al.(2020)Susanto, Chollampatt, and Tan]{SusantoCT20}
Raymond~Hendy Susanto, Shamil Chollampatt, and Liling Tan.
\newblock Lexically constrained neural machine translation with levenshtein
  transformer.
\newblock In \emph{{ACL}}, pages 3536--3543. Association for Computational
  Linguistics, 2020.

\bibitem[Zhu et~al.(2018)Zhu, Lu, Zheng, Guo, Zhang, Wang, and
  Yu]{zhu2018texygen}
Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong
  Yu.
\newblock Texygen: A benchmarking platform for text generation models.
\newblock In \emph{The 41st international ACM SIGIR conference on research \&
  development in information retrieval}, pages 1097--1100, 2018.

\bibitem[Xiao et~al.(2022)Xiao, Wu, Guo, Li, Zhang, Qin, and
  Liu]{journals/corr/abs-2204-09269}
Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and
  Tie{-}Yan Liu.
\newblock A survey on non-autoregressive generation for neural machine
  translation and beyond.
\newblock \emph{CoRR}, abs/2204.09269, 2022.

\bibitem[Vijayakumar et~al.(2016)Vijayakumar, Cogswell, Selvaraju, Sun, Lee,
  Crandall, and Batra]{journals/corr/VijayakumarCSSL16}
Ashwin~K. Vijayakumar, Michael Cogswell, Ramprasaath~R. Selvaraju, Qing Sun,
  Stefan Lee, David~J. Crandall, and Dhruv Batra.
\newblock Diverse beam search: Decoding diverse solutions from neural sequence
  models.
\newblock \emph{CoRR}, abs/1610.02424, 2016.

\bibitem[Meister et~al.(2022)Meister, Pimentel, Wiher, and
  Cotterell]{journals/corr/abs-2202-00666}
Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell.
\newblock Typical decoding for natural language generation.
\newblock \emph{CoRR}, abs/2202.00666, 2022.

\bibitem[Fan et~al.(2018)Fan, Lewis, and Dauphin]{LewisDF18}
Angela Fan, Mike Lewis, and Yann~N. Dauphin.
\newblock Hierarchical neural story generation.
\newblock In \emph{{ACL} {(1)}}, pages 889--898. Association for Computational
  Linguistics, 2018.

\bibitem[Holtzman et~al.(2020)Holtzman, Buys, Du, Forbes, and
  Choi]{HoltzmanBDFC20}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock In \emph{{ICLR}}. OpenReview.net, 2020.

\bibitem[Song et~al.(2021)Song, Meng, and Ermon]{DBLP:conf/iclr/SongME21}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.
\newblock URL \url{https://openreview.net/forum?id=St1giarCHLP}.

\bibitem[Dong et~al.(2023)Dong, Li, Gong, Chen, Li, Shen, and
  Yang]{DBLP:journals/csur/DongLGCLSY23}
Chenhe Dong, Yinghui Li, Haifan Gong, Miaoxin Chen, Junxin Li, Ying Shen, and
  Min Yang.
\newblock A survey of natural language generation.
\newblock \emph{{ACM} Comput. Surv.}, 55\penalty0 (8):\penalty0 173:1--173:38,
  2023.
\newblock \doi{10.1145/3554727}.
\newblock URL \url{https://doi.org/10.1145/3554727}.

\bibitem[Hoogeboom et~al.(2022)Hoogeboom, Gritsenko, Bastings, Poole, van~den
  Berg, and Salimans]{hoogeboom2022autoregressive}
Emiel Hoogeboom, Alexey~A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne
  van~den Berg, and Tim Salimans.
\newblock Autoregressive diffusion models.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=Lm8T39vLDTE}.

\bibitem[Rasul et~al.(2021)Rasul, Seward, Schuster, and
  Vollgraf]{pmlr-v139-rasul21a}
Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf.
\newblock Autoregressive denoising diffusion models for multivariate
  probabilistic time series forecasting.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 8857--8868. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/rasul21a.html}.

\bibitem[Luo(2022)]{luo2022understanding}
Calvin Luo.
\newblock Understanding diffusion models: A unified perspective.
\newblock \emph{arXiv preprint arXiv:2208.11970}, 2022.

\end{thebibliography}
