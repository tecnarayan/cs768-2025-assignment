\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[An et~al.(2020)An, Lyu, Wang, Li, Hu, Tan, Zhang, Hu, and
  Chen]{an2020repulsive}
An, B., Lyu, J., Wang, Z., Li, C., Hu, C., Tan, F., Zhang, R., Hu, Y., and
  Chen, C.
\newblock Repulsive attention: Rethinking multi-head attention as bayesian
  inference.
\newblock \emph{arXiv preprint arXiv:2009.09364}, 2020.

\bibitem[Bai et~al.(2022)Bai, Gautam, and Sojoudi]{bai2022efficient}
Bai, Y., Gautam, T., and Sojoudi, S.
\newblock Efficient global optimization of two-layer relu networks:
  Quadratic-time algorithms and adversarial training.
\newblock \emph{arXiv preprint arXiv:2201.01965}, 2022.

\bibitem[Burer \& Monteiro(2005)Burer and Monteiro]{burer2005local}
Burer, S. and Monteiro, R.~D.
\newblock Local minima and convergence in low-rank semidefinite programming.
\newblock \emph{Mathematical programming}, 103\penalty0 (3):\penalty0 427--444,
  2005.

\bibitem[Cand{\`e}s \& Tao(2010)Cand{\`e}s and Tao]{candes2010power}
Cand{\`e}s, E.~J. and Tao, T.
\newblock The power of convex relaxation: Near-optimal matrix completion.
\newblock \emph{IEEE Transactions on Information Theory}, 56\penalty0
  (5):\penalty0 2053--2080, 2010.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J{\'e}gou, Mairal,
  Bojanowski, and Joulin]{caron2021emerging}
Caron, M., Touvron, H., Misra, I., J{\'e}gou, H., Mairal, J., Bojanowski, P.,
  and Joulin, A.
\newblock Emerging properties in self-supervised vision transformers.
\newblock \emph{arXiv preprint arXiv:2104.14294}, 2021.

\bibitem[Chefer et~al.(2021)Chefer, Gur, and Wolf]{chefer2021transformer}
Chefer, H., Gur, S., and Wolf, L.
\newblock Transformer interpretability beyond attention visualization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  782--791, 2021.

\bibitem[Cordonnier et~al.(2019)Cordonnier, Loukas, and
  Jaggi]{cordonnier2019relationship}
Cordonnier, J.-B., Loukas, A., and Jaggi, M.
\newblock On the relationship between self-attention and convolutional layers.
\newblock \emph{arXiv preprint arXiv:1911.03584}, 2019.

\bibitem[Cubuk et~al.(2018)Cubuk, Zoph, Mane, Vasudevan, and
  Le]{cubuk2018autoaugment}
Cubuk, E.~D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q.~V.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock \emph{arXiv preprint arXiv:1805.09501}, 2018.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Ergen \& Pilanci(2020)Ergen and Pilanci]{ergen2020implicit}
Ergen, T. and Pilanci, M.
\newblock Implicit convex regularizers of cnn architectures: Convex
  optimization of two-and three-layer networks in polynomial time.
\newblock \emph{arXiv preprint arXiv:2006.14798}, 2020.

\bibitem[Ergen \& Pilanci(2021)Ergen and Pilanci]{ergen2021global}
Ergen, T. and Pilanci, M.
\newblock Global optimality beyond two layers: Training deep relu networks via
  convex programs.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2993--3003. PMLR, 2021.

\bibitem[Ergen et~al.(2021)Ergen, Sahiner, Ozturkler, Pauly, Mardani, and
  Pilanci]{ergen2021demystifying}
Ergen, T., Sahiner, A., Ozturkler, B., Pauly, J., Mardani, M., and Pilanci, M.
\newblock Demystifying batch normalization in relu networks: Equivalent convex
  optimization models and implicit regularization.
\newblock \emph{arXiv preprint arXiv:2103.01499}, 2021.

\bibitem[Fiat et~al.(2019)Fiat, Malach, and Shalev-Shwartz]{fiat2019decoupling}
Fiat, J., Malach, E., and Shalev-Shwartz, S.
\newblock Decoupling gating from linearity.
\newblock \emph{arXiv preprint arXiv:1906.05032}, 2019.

\bibitem[Geng et~al.(2021)Geng, Guo, Chen, Li, Wei, and Lin]{geng2021attention}
Geng, Z., Guo, M.-H., Chen, H., Li, X., Wei, K., and Lin, Z.
\newblock Is attention better than matrix decomposition?
\newblock \emph{arXiv preprint arXiv:2109.04553}, 2021.

\bibitem[Guibas et~al.(2021)Guibas, Mardani, Li, Tao, Anandkumar, and
  Catanzaro]{guibas2021adaptive}
Guibas, J., Mardani, M., Li, Z., Tao, A., Anandkumar, A., and Catanzaro, B.
\newblock Adaptive fourier neural operators: Efficient token mixers for
  transformers.
\newblock \emph{arXiv preprint arXiv:2111.13587}, 2021.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar2017implicit}
Gunasekar, S., Woodworth, B.~E., Bhojanapalli, S., Neyshabur, B., and Srebro,
  N.
\newblock Implicit regularization in matrix factorization.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016gaussian}
Hendrycks, D. and Gimpel, K.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Jaggi(2013)]{jaggi2013revisiting}
Jaggi, M.
\newblock Revisiting frank-wolfe: Projection-free sparse convex optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  427--435. PMLR, 2013.

\bibitem[Kim et~al.(2021)Kim, Papamakarios, and Mnih]{kim2021lipschitz}
Kim, H., Papamakarios, G., and Mnih, A.
\newblock The lipschitz constant of self-attention.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5562--5571. PMLR, 2021.

\bibitem[Kirichenko et~al.(2022)Kirichenko, Izmailov, and
  Wilson]{kirichenko2022last}
Kirichenko, P., Izmailov, P., and Wilson, A.~G.
\newblock Last layer re-training is sufficient for robustness to spurious
  correlations.
\newblock \emph{arXiv preprint arXiv:2204.02937}, 2022.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{kitaev2020reformer}
Kitaev, N., Kaiser, {\L}., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock \emph{arXiv preprint arXiv:2001.04451}, 2020.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lee-Thorp et~al.(2021)Lee-Thorp, Ainslie, Eckstein, and
  Ontanon]{lee2021fnet}
Lee-Thorp, J., Ainslie, J., Eckstein, I., and Ontanon, S.
\newblock Fnet: Mixing tokens with fourier transforms.
\newblock \emph{arXiv preprint arXiv:2105.03824}, 2021.

\bibitem[Li et~al.(2020)Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya,
  Stuart, and Anandkumar]{li2020fourier}
Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart,
  A., and Anandkumar, A.
\newblock Fourier neural operator for parametric partial differential
  equations.
\newblock \emph{arXiv preprint arXiv:2010.08895}, 2020.

\bibitem[Liu et~al.(2021)Liu, Dai, So, and Le]{liu2021pay}
Liu, H., Dai, Z., So, D.~R., and Le, Q.~V.
\newblock Pay attention to mlps, 2021.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and
  Hutter]{loshchilov2017decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Magnus \& Neudecker(2019)Magnus and Neudecker]{magnus2019matrix}
Magnus, J.~R. and Neudecker, H.
\newblock \emph{Matrix differential calculus with applications in statistics
  and econometrics}.
\newblock John Wiley \& Sons, 2019.

\bibitem[Mardani et~al.(2013)Mardani, Mateos, and
  Giannakis]{mardani2013decentralized}
Mardani, M., Mateos, G., and Giannakis, G.~B.
\newblock Decentralized sparsity-regularized rank minimization: Algorithms and
  applications.
\newblock \emph{IEEE Transactions on Signal Processing}, 61\penalty0
  (21):\penalty0 5374--5388, 2013.

\bibitem[Mardani et~al.(2015)Mardani, Mateos, and
  Giannakis]{mardani2015subspace}
Mardani, M., Mateos, G., and Giannakis, G.~B.
\newblock Subspace learning and imputation for streaming big data matrices and
  tensors.
\newblock \emph{IEEE Transactions on Signal Processing}, 63\penalty0
  (10):\penalty0 2663--2677, 2015.

\bibitem[Mishkin et~al.(2022)Mishkin, Sahiner, and Pilanci]{mishkin2022fast}
Mishkin, A., Sahiner, A., and Pilanci, M.
\newblock Fast convex optimization for two-layer relu networks: Equivalent
  model classes and cone decompositions.
\newblock \emph{arXiv preprint arXiv:2202.01331}, 2022.

\bibitem[Mosbach et~al.(2020)Mosbach, Andriushchenko, and
  Klakow]{mosbach2020stability}
Mosbach, M., Andriushchenko, M., and Klakow, D.
\newblock On the stability of fine-tuning bert: Misconceptions, explanations,
  and strong baselines.
\newblock \emph{arXiv preprint arXiv:2006.04884}, 2020.

\bibitem[Panahi et~al.(2021)Panahi, Saeedi, and Arodz]{panahi2021shapeshifter}
Panahi, A., Saeedi, S., and Arodz, T.
\newblock Shapeshifter: a parameter-efficient transformer using factorized
  reshaped matrices.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems},
  32:\penalty0 8026--8037, 2019.

\bibitem[Pilanci \& Ergen(2020)Pilanci and Ergen]{pilanci2020neural}
Pilanci, M. and Ergen, T.
\newblock Neural networks are convex regularizers: Exact polynomial-time convex
  optimization formulations for two-layer networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7695--7705. PMLR, 2020.

\bibitem[Raghu et~al.(2021)Raghu, Unterthiner, Kornblith, Zhang, and
  Dosovitskiy]{raghu2021vision}
Raghu, M., Unterthiner, T., Kornblith, S., Zhang, C., and Dosovitskiy, A.
\newblock Do vision transformers see like convolutional neural networks?
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Rao et~al.(2021)Rao, Zhao, Zhu, Lu, and Zhou]{rao2021global}
Rao, Y., Zhao, W., Zhu, Z., Lu, J., and Zhou, J.
\newblock Global filter networks for image classification.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Recht et~al.(2010)Recht, Fazel, and Parrilo]{recht2010guaranteed}
Recht, B., Fazel, M., and Parrilo, P.~A.
\newblock Guaranteed minimum-rank solutions of linear matrix equations via
  nuclear norm minimization.
\newblock \emph{SIAM review}, 52\penalty0 (3):\penalty0 471--501, 2010.

\bibitem[Sahiner et~al.(2020{\natexlab{a}})Sahiner, Ergen, Pauly, and
  Pilanci]{sahiner2020vector}
Sahiner, A., Ergen, T., Pauly, J., and Pilanci, M.
\newblock Vector-output relu neural network problems are copositive programs:
  Convex analysis of two layer networks and polynomial-time algorithms.
\newblock \emph{arXiv preprint arXiv:2012.13329}, 2020{\natexlab{a}}.

\bibitem[Sahiner et~al.(2020{\natexlab{b}})Sahiner, Mardani, Ozturkler,
  Pilanci, and Pauly]{sahiner2020convex}
Sahiner, A., Mardani, M., Ozturkler, B., Pilanci, M., and Pauly, J.
\newblock Convex regularization behind neural reconstruction.
\newblock \emph{arXiv preprint arXiv:2012.05169}, 2020{\natexlab{b}}.

\bibitem[Sahiner et~al.(2021)Sahiner, Ergen, Ozturkler, Bartan, Pauly, Mardani,
  and Pilanci]{sahiner2021hidden}
Sahiner, A., Ergen, T., Ozturkler, B., Bartan, B., Pauly, J., Mardani, M., and
  Pilanci, M.
\newblock Hidden convexity of wasserstein gans: Interpretable generative models
  with closed-form solutions.
\newblock \emph{arXiv preprint arXiv:2107.05680}, 2021.

\bibitem[Savarese et~al.(2019)Savarese, Evron, Soudry, and
  Srebro]{savarese2019infinite}
Savarese, P., Evron, I., Soudry, D., and Srebro, N.
\newblock How do infinite width bounded norm networks look in function space?
\newblock In \emph{Conference on Learning Theory}, pp.\  2667--2690. PMLR,
  2019.

\bibitem[Shapiro(2009)]{shapiro2009semi}
Shapiro, A.
\newblock Semi-infinite programming, duality, discretization and optimality
  conditions.
\newblock \emph{Optimization}, 58\penalty0 (2):\penalty0 133--161, 2009.

\bibitem[Shen et~al.(2021)Shen, Zhang, Zhao, Yi, and Li]{shen2021efficient}
Shen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H.
\newblock Efficient attention: Attention with linear complexities.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision}, pp.\  3531--3539, 2021.

\bibitem[Stanley et~al.(2004)]{stanley2004introduction}
Stanley, R.~P. et~al.
\newblock An introduction to hyperplane arrangements.
\newblock \emph{Geometric combinatorics}, 13\penalty0 (389-496):\penalty0 24,
  2004.

\bibitem[Tatsunami \& Taki(2021)Tatsunami and Taki]{tatsunami2021raftmlp}
Tatsunami, Y. and Taki, M.
\newblock Raftmlp: Do mlp-based models dream of winning over computer vision?
\newblock \emph{arXiv preprint arXiv:2108.04384}, 2021.

\bibitem[Toh \& Yun(2010)Toh and Yun]{toh2010accelerated}
Toh, K.-C. and Yun, S.
\newblock An accelerated proximal gradient algorithm for nuclear norm
  regularized linear least squares problems.
\newblock \emph{Pacific Journal of optimization}, 6\penalty0
  (615-640):\penalty0 15, 2010.

\bibitem[Tolstikhin et~al.(2021)Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai,
  Unterthiner, Yung, Keysers, Uszkoreit, Lucic, et~al.]{tolstikhin2021mlp}
Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner,
  T., Yung, J., Keysers, D., Uszkoreit, J., Lucic, M., et~al.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock \emph{arXiv preprint arXiv:2105.01601}, 2021.

\bibitem[Touvron et~al.(2021)Touvron, Bojanowski, Caron, Cord, El-Nouby, Grave,
  Izacard, Joulin, Synnaeve, Verbeek, et~al.]{touvron2021resmlp}
Touvron, H., Bojanowski, P., Caron, M., Cord, M., El-Nouby, A., Grave, E.,
  Izacard, G., Joulin, A., Synnaeve, G., Verbeek, J., et~al.
\newblock Resmlp: Feedforward networks for image classification with
  data-efficient training.
\newblock \emph{arXiv preprint arXiv:2105.03404}, 2021.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5998--6008, 2017.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
Wang, S., Li, B.~Z., Khabsa, M., Fang, H., and Ma, H.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Wightman(2019)]{rw2019timm}
Wightman, R.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem[Xiong et~al.(2021)Xiong, Zeng, Chakraborty, Tan, Fung, Li, and
  Singh]{xiong2021nystr}
Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., and Singh, V.
\newblock Nystr$\backslash$" omformer: A nystr$\backslash$" om-based algorithm
  for approximating self-attention.
\newblock \emph{arXiv preprint arXiv:2102.03902}, 2021.

\bibitem[Yorsh \& Kovalenko(2021)Yorsh and Kovalenko]{yorsh2021pureformer}
Yorsh, U. and Kovalenko, A.
\newblock Pureformer: Do we even need attention?
\newblock \emph{arXiv preprint arXiv:2111.15588}, 2021.

\bibitem[Yorsh et~al.(2021)Yorsh, Kord{\'\i}k, and
  Kovalenko]{yorsh2021simpletron}
Yorsh, U., Kord{\'\i}k, P., and Kovalenko, A.
\newblock Simpletron: Eliminating softmax from attention computation.
\newblock \emph{arXiv preprint arXiv:2111.15588}, 2021.

\bibitem[Yu et~al.(2021)Yu, Luo, Zhou, Si, Zhou, Wang, Feng, and
  Yan]{yu2021metaformer}
Yu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., Feng, J., and Yan, S.
\newblock Metaformer is actually what you need for vision.
\newblock \emph{arXiv preprint arXiv:2111.11418}, 2021.

\bibitem[Yun et~al.(2020)Yun, Chang, Bhojanapalli, Rawat, Reddi, and
  Kumar]{yun2020n}
Yun, C., Chang, Y.-W., Bhojanapalli, S., Rawat, A.~S., Reddi, S.~J., and Kumar,
  S.
\newblock $ o (n) $ connections are expressive enough: Universal
  approximability of sparse transformers.
\newblock \emph{arXiv preprint arXiv:2006.04862}, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Titov, and Sennrich]{zhang2021sparse}
Zhang, B., Titov, I., and Sennrich, R.
\newblock Sparse attention with linear units.
\newblock \emph{arXiv preprint arXiv:2104.07012}, 2021.

\end{thebibliography}
