\begin{thebibliography}{203}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abramson et~al.(2024)Abramson, Adler, Dunger, Evans, Green, Pritzel, Ronneberger, Willmore, Ballard, Bambrick, et~al.]{abramson2024accurate}
Abramson, J., Adler, J., Dunger, J., Evans, R., Green, T., Pritzel, A., Ronneberger, O., Willmore, L., Ballard, A.~J., Bambrick, J., et~al.
\newblock Accurate structure prediction of biomolecular interactions with alphafold 3.
\newblock \emph{Nature}, pp.\  1--3, 2024.

\bibitem[AbuOda et~al.(2020)AbuOda, De~Francisci~Morales, and Aboulnaga]{abuoda2020link}
AbuOda, G., De~Francisci~Morales, G., and Aboulnaga, A.
\newblock Link prediction via higher-order motif features.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2019, W{\"u}rzburg, Germany, September 16--20, 2019, Proceedings, Part I}, pp.\  412--429. Springer, 2020.

\bibitem[Adamic \& Adar(2003)Adamic and Adar]{adamic2003friends}
Adamic, L.~A. and Adar, E.
\newblock Friends and neighbors on the web.
\newblock \emph{Social networks}, 25\penalty0 (3):\penalty0 211--230, 2003.

\bibitem[Airoldi et~al.(2008)Airoldi, Blei, Fienberg, and Xing]{airoldi2008mixed}
Airoldi, E.~M., Blei, D., Fienberg, S., and Xing, E.
\newblock Mixed membership stochastic blockmodels.
\newblock \emph{Advances in neural information processing systems}, 21, 2008.

\bibitem[Albert \& Barab{\'a}si(2002)Albert and Barab{\'a}si]{albert2002statistical}
Albert, R. and Barab{\'a}si, A.-L.
\newblock Statistical mechanics of complex networks.
\newblock \emph{Reviews of modern physics}, 74\penalty0 (1):\penalty0 47, 2002.

\bibitem[Bai et~al.(2023)Bai, Geng, Mangalam, Bar, Yuille, Darrell, Malik, and Efros]{bai2023sequential}
Bai, Y., Geng, X., Mangalam, K., Bar, A., Yuille, A., Darrell, T., Malik, J., and Efros, A.~A.
\newblock Sequential modeling enables scalable learning for large vision models.
\newblock \emph{arXiv preprint arXiv:2312.00785}, 2023.

\bibitem[Barcelo et~al.(2022)Barcelo, Galkin, Morris, and Orth]{barcelo2022weisfeiler}
Barcelo, P., Galkin, M., Morris, C., and Orth, M.~R.
\newblock Weisfeiler and leman go relational.
\newblock In \emph{Learning on Graphs Conference}, pp.\  46--1. PMLR, 2022.

\bibitem[Barceló et~al.(2020)Barceló, Kostylev, Monet, Pérez, Reutter, and Silva]{barcelo2020}
Barceló, P., Kostylev, E.~V., Monet, M., Pérez, J., Reutter, J., and Silva, J.~P.
\newblock The logical expressiveness of graph neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=r1lZ7AEKvB}.

\bibitem[Batatia et~al.(2023)Batatia, Benner, Chiang, Elena, Kovács, Riebesell, Advincula, Asta, Baldwin, Bernstein, Bhowmik, Blau, Cărare, Darby, De, Pia, Deringer, Elijošius, El-Machachi, Fako, Ferrari, Genreith-Schriever, George, Goodall, Grey, Han, Handley, Heenen, Hermansson, Holm, Jaafar, Hofmann, Jakob, Jung, Kapil, Kaplan, Karimitari, Kroupa, Kullgren, Kuner, Kuryla, Liepuoniute, Margraf, Magdău, Michaelides, Moore, Naik, Niblett, Norwood, O'Neill, Ortner, Persson, Reuter, Rosen, Schaaf, Schran, Sivonxay, Stenczel, Svahn, Sutton, van~der Oord, Varga-Umbrich, Vegge, Vondrák, Wang, Witt, Zills, and Csányi]{batatia2023macemp}
Batatia, I., Benner, P., Chiang, Y., Elena, A.~M., Kovács, D.~P., Riebesell, J., Advincula, X.~R., Asta, M., Baldwin, W.~J., Bernstein, N., Bhowmik, A., Blau, S.~M., Cărare, V., Darby, J.~P., De, S., Pia, F.~D., Deringer, V.~L., Elijošius, R., El-Machachi, Z., Fako, E., Ferrari, A.~C., Genreith-Schriever, A., George, J., Goodall, R. E.~A., Grey, C.~P., Han, S., Handley, W., Heenen, H.~H., Hermansson, K., Holm, C., Jaafar, J., Hofmann, S., Jakob, K.~S., Jung, H., Kapil, V., Kaplan, A.~D., Karimitari, N., Kroupa, N., Kullgren, J., Kuner, M.~C., Kuryla, D., Liepuoniute, G., Margraf, J.~T., Magdău, I.-B., Michaelides, A., Moore, J.~H., Naik, A.~A., Niblett, S.~P., Norwood, S.~W., O'Neill, N., Ortner, C., Persson, K.~A., Reuter, K., Rosen, A.~S., Schaaf, L.~L., Schran, C., Sivonxay, E., Stenczel, T.~K., Svahn, V., Sutton, C., van~der Oord, C., Varga-Umbrich, E., Vegge, T., Vondrák, M., Wang, Y., Witt, W.~C., Zills, F., and Csányi, G.
\newblock A foundation model for atomistic materials chemistry, 2023.

\bibitem[Battiston et~al.(2020)Battiston, Cencetti, Iacopini, Latora, Lucas, Patania, Young, and Petri]{battiston2020networks}
Battiston, F., Cencetti, G., Iacopini, I., Latora, V., Lucas, M., Patania, A., Young, J.-G., and Petri, G.
\newblock Networks beyond pairwise interactions: Structure and dynamics.
\newblock \emph{Physics Reports}, 874:\penalty0 1--92, 2020.

\bibitem[Beaini et~al.(2023)Beaini, Huang, Cunha, Moisescu-Pareja, Dymov, Maddrell-Mander, McLean, Wenkel, M{\"u}ller, Mohamud, et~al.]{beaini2023towardsMoleculeFM}
Beaini, D., Huang, S., Cunha, J.~A., Moisescu-Pareja, G., Dymov, O., Maddrell-Mander, S., McLean, C., Wenkel, F., M{\"u}ller, L., Mohamud, J.~H., et~al.
\newblock Towards foundational models for molecular learning on large-scale multi-task datasets.
\newblock \emph{arXiv preprint arXiv:2310.04292}, 2023.

\bibitem[BehnamGhader et~al.(2024)BehnamGhader, Adlakha, Mosbach, Bahdanau, Chapados, and Reddy]{behnamghader2024llm2vec}
BehnamGhader, P., Adlakha, V., Mosbach, M., Bahdanau, D., Chapados, N., and Reddy, S.
\newblock Llm2vec: Large language models are secretly powerful text encoders.
\newblock \emph{arXiv preprint arXiv:2404.05961}, 2024.

\bibitem[Benson et~al.(2016)Benson, Gleich, and Leskovec]{benson2016higher}
Benson, A.~R., Gleich, D.~F., and Leskovec, J.
\newblock Higher-order organization of complex networks.
\newblock \emph{Science}, 353\penalty0 (6295):\penalty0 163--166, 2016.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von Arx, Bernstein, Bohg, Bosselut, Brunskill, et~al.]{bommasani2021opportunities}
Bommasani, R., Hudson, D.~A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.~S., Bohg, J., Bosselut, A., Brunskill, E., et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Brooks et~al.(2024)Brooks, Peebles, Holmes, DePue, Guo, Jing, Schnurr, Taylor, Luhman, Luhman, Ng, Wang, and Ramesh]{videoworldsimulators2024}
Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., Ng, C., Wang, R., and Ramesh, A.
\newblock Video generation models as world simulators.
\newblock 2024.
\newblock URL \url{https://openai.com/research/video-generation-models-as-world-simulators}.

\bibitem[Brugere et~al.(2018)Brugere, Gallagher, and Berger-Wolf]{brugere2018network}
Brugere, I., Gallagher, B., and Berger-Wolf, T.~Y.
\newblock Network structure inference, a survey: Motivations, methods, and applications.
\newblock \emph{ACM Computing Surveys (CSUR)}, 51\penalty0 (2):\penalty0 1--39, 2018.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg, et~al.]{gpt4}
Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y.~T., Li, Y., Lundberg, S., et~al.
\newblock Sparks of artificial general intelligence: Early experiments with gpt-4.
\newblock \emph{arXiv preprint arXiv:2303.12712}, 2023.

\bibitem[Cai \& Wang(2020)Cai and Wang]{cai2020note}
Cai, C. and Wang, Y.
\newblock A note on over-smoothing for graph neural networks.
\newblock \emph{arXiv preprint arXiv:2006.13318}, 2020.

\bibitem[Cao et~al.(2023)Cao, Xu, Yang, Wang, Zhang, Wang, Chen, and Yang]{cao2023preW2PGNN}
Cao, Y., Xu, J., Yang, C., Wang, J., Zhang, Y., Wang, C., Chen, L., and Yang, Y.
\newblock When to pre-train graph neural networks? an answer from data generation perspective!
\newblock \emph{arXiv preprint arXiv:2303.16458}, 2023.

\bibitem[Chai et~al.(2023)Chai, Zhang, Wu, Han, Hu, Huang, and Yang]{chai2023graphllm}
Chai, Z., Zhang, T., Wu, L., Han, K., Hu, X., Huang, X., and Yang, Y.
\newblock Graphllm: Boosting graph reasoning ability of large language model.
\newblock \emph{arXiv preprint arXiv:2310.05845}, 2023.

\bibitem[Chamberlain et~al.(2022)Chamberlain, Shirobokov, Rossi, Frasca, Markovich, Hammerla, Bronstein, and Hansmire]{BUDDY}
Chamberlain, B.~P., Shirobokov, S., Rossi, E., Frasca, F., Markovich, T., Hammerla, N., Bronstein, M.~M., and Hansmire, M.
\newblock Graph neural networks for link prediction with subgraph sketching.
\newblock \emph{arXiv preprint arXiv:2209.15486}, 2022.

\bibitem[Chawla \& Karakoulas(2005)Chawla and Karakoulas]{chawla2005learningLP}
Chawla, N.~V. and Karakoulas, G.
\newblock Learning from labeled and unlabeled data: An empirical study across techniques and domains.
\newblock \emph{Journal of Artificial Intelligence Research}, 23:\penalty0 331--366, 2005.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Zhu, Zhang, Du, Li, Liu, Wu, and Wang]{chen2023uncoveringMoleculeScaling}
Chen, D., Zhu, Y., Zhang, J., Du, Y., Li, Z., Liu, Q., Wu, S., and Wang, L.
\newblock Uncovering neural scaling laws in molecular representation learning.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2023{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=Ys8RmfF9w1}.

\bibitem[Chen et~al.(2024{\natexlab{a}})Chen, Li, Tang, and Li]{chen2024graphwiz}
Chen, N., Li, Y., Tang, J., and Li, J.
\newblock Graphwiz: An instruction-following language model for graph problems.
\newblock \emph{arXiv preprint arXiv:2402.16029}, 2024{\natexlab{a}}.

\bibitem[Chen et~al.(2024{\natexlab{b}})Chen, Zhao, Jaiswal, Shah, and Wang]{chen2024llaga}
Chen, R., Zhao, T., Jaiswal, A., Shah, N., and Wang, Z.
\newblock Llaga: Large language and graph assistant.
\newblock \emph{arXiv preprint arXiv:2402.08170}, 2024{\natexlab{b}}.

\bibitem[Chen et~al.(2022)Chen, Liu, Wang, and Yin]{chen2022representing}
Chen, Z., Liu, J., Wang, X., and Yin, W.
\newblock On representing linear programs by graph neural networks.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Mao, Li, Jin, Wen, Wei, Wang, Yin, Fan, Liu, and Tang]{Chen2023ExploringTP}
Chen, Z., Mao, H., Li, H., Jin, W., Wen, H., Wei, X., Wang, S., Yin, D., Fan, W., Liu, H., and Tang, J.
\newblock Exploring the potential of large language models (llms) in learning on graphs.
\newblock \emph{ArXiv}, abs/2307.03393, 2023{\natexlab{b}}.

\bibitem[Chien et~al.(2021)Chien, Peng, Li, and Milenkovic]{chien2021adaptiveGPRGNN}
Chien, E., Peng, J., Li, P., and Milenkovic, O.
\newblock Adaptive universal generalized pagerank graph neural network.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=n6jl7fLxrP}.

\bibitem[Dess{\'i} et~al.(2022)Dess{\'i}, Osborne, Recupero, Buscaldi, and Motta]{Dess2022CSKGAL}
Dess{\'i}, D., Osborne, F., Recupero, D.~R., Buscaldi, D., and Motta, E.
\newblock Cs-kg: A large-scale knowledge graph of research entities and claims in computer science.
\newblock In \emph{International Workshop on the Semantic Web}, 2022.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:253021556}.

\bibitem[Di~Giovanni et~al.(2023)Di~Giovanni, Rusch, Bronstein, Deac, Lackenby, Mishra, and Veli{\v{c}}kovi{\'c}]{di2023does}
Di~Giovanni, F., Rusch, T.~K., Bronstein, M.~M., Deac, A., Lackenby, M., Mishra, S., and Veli{\v{c}}kovi{\'c}, P.
\newblock How does over-squashing affect the power of gnns?
\newblock \emph{arXiv preprint arXiv:2306.03589}, 2023.

\bibitem[Dong et~al.(2024)Dong, Mao, Guo, and Chawla]{dong2024universal}
Dong, K., Mao, H., Guo, Z., and Chawla, N.~V.
\newblock Universal link predictor by in-context learning.
\newblock \emph{arXiv preprint arXiv:2402.07738}, 2024.

\bibitem[Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui]{icl}
Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., and Sui, Z.
\newblock A survey for in-context learning.
\newblock \emph{arXiv preprint arXiv:2301.00234}, 2022.

\bibitem[Dong et~al.(2017)Dong, Johnson, Xu, and Chawla]{dong2017structural}
Dong, Y., Johnson, R.~A., Xu, J., and Chawla, N.~V.
\newblock Structural diversity and homophily: A study across more than one hundred big networks.
\newblock In \emph{Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, pp.\  807--816, 2017.

\bibitem[Dziri et~al.(2023)Dziri, Lu, Sclar, Li, Jian, Lin, West, Bhagavatula, Bras, Hwang, et~al.]{dziri2023faith}
Dziri, N., Lu, X., Sclar, M., Li, X.~L., Jian, L., Lin, B.~Y., West, P., Bhagavatula, C., Bras, R.~L., Hwang, J.~D., et~al.
\newblock Faith and fate: Limits of transformers on compositionality.
\newblock \emph{arXiv preprint arXiv:2305.18654}, 2023.

\bibitem[Edmonds \& Johnson(1973)Edmonds and Johnson]{Edmonds1973MatchingET}
Edmonds, J. and Johnson, E.~L.
\newblock Matching, euler tours and the chinese postman.
\newblock \emph{Mathematical Programming}, 5:\penalty0 88--124, 1973.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:15249924}.

\bibitem[Fatemi et~al.(2023)Fatemi, Halcrow, and Perozzi]{fatemi2023talk}
Fatemi, B., Halcrow, J., and Perozzi, B.
\newblock Talk like a graph: Encoding graphs for large language models.
\newblock \emph{arXiv preprint arXiv:2310.04560}, 2023.

\bibitem[Fey \& Lenssen(2019)Fey and Lenssen]{fey2019fast}
Fey, M. and Lenssen, J.~E.
\newblock Fast graph representation learning with pytorch geometric.
\newblock \emph{arXiv preprint arXiv:1903.02428}, 2019.

\bibitem[Fey et~al.(2021)Fey, Lenssen, Weichert, and Leskovec]{fey2021gnnautoscale}
Fey, M., Lenssen, J.~E., Weichert, F., and Leskovec, J.
\newblock Gnnautoscale: Scalable and expressive graph neural networks via historical embeddings.
\newblock In \emph{International conference on machine learning}, pp.\  3294--3304. PMLR, 2021.

\bibitem[Freitas et~al.(2021)Freitas, Duggal, and Chau]{freitas2021malnet}
Freitas, S., Duggal, R., and Chau, D.~H.
\newblock Malnet: A large-scale image database of malicious software.
\newblock \emph{arXiv preprint arXiv:2102.01072}, 2021.

\bibitem[Galkin et~al.(2023)Galkin, Yuan, Mostafa, Tang, and Zhu]{galkin2023towardsULTRA}
Galkin, M., Yuan, X., Mostafa, H., Tang, J., and Zhu, Z.
\newblock Towards foundation models for knowledge graph reasoning.
\newblock \emph{arXiv preprint arXiv:2310.04562}, 2023.

\bibitem[Galkin et~al.(2024)Galkin, Zhou, Ribeiro, Tang, and Zhu]{galkin2024zero}
Galkin, M., Zhou, J., Ribeiro, B., Tang, J., and Zhu, Z.
\newblock Zero-shot logical query reasoning on any knowledge graph.
\newblock \emph{arXiv preprint arXiv:2404.07198}, 2024.

\bibitem[Gao et~al.(2023{\natexlab{a}})Gao, Zhou, and Ribeiro]{gao2023doubleEquivariant}
Gao, J., Zhou, Y., and Ribeiro, B.
\newblock Double permutation equivariance for knowledge graph completion.
\newblock \emph{arXiv preprint arXiv:2302.01313}, 2023{\natexlab{a}}.

\bibitem[Gao et~al.(2023{\natexlab{b}})Gao, Zhou, Zhou, and Ribeiro]{gao2023double}
Gao, J., Zhou, Y., Zhou, J., and Ribeiro, B.
\newblock Double equivariance for inductive link prediction for both new nodes and new relation types.
\newblock In \emph{NeurIPS 2023 Workshop: New Frontiers in Graph Learning}, 2023{\natexlab{b}}.

\bibitem[Granovetter(1973)]{granovetter1973strength}
Granovetter, M.~S.
\newblock The strength of weak ties.
\newblock \emph{American journal of sociology}, 78\penalty0 (6):\penalty0 1360--1380, 1973.

\bibitem[Gruver et~al.(2024{\natexlab{a}})Gruver, Finzi, Qiu, and Wilson]{gruver2024large}
Gruver, N., Finzi, M., Qiu, S., and Wilson, A.~G.
\newblock Large language models are zero-shot time series forecasters.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024{\natexlab{a}}.

\bibitem[Gruver et~al.(2024{\natexlab{b}})Gruver, Sriram, Madotto, Wilson, Zitnick, and Ulissi]{gruver2024fine}
Gruver, N., Sriram, A., Madotto, A., Wilson, A.~G., Zitnick, C.~L., and Ulissi, Z.
\newblock Fine-tuned language models generate stable inorganic materials as text.
\newblock \emph{arXiv preprint arXiv:2402.04379}, 2024{\natexlab{b}}.

\bibitem[Gupta et~al.(2023)Gupta, Manchanda, Ranu, and Bedathur]{gupta2023grafenne}
Gupta, S., Manchanda, S., Ranu, S., and Bedathur, S.~J.
\newblock Grafenne: learning on graphs with heterogeneous and dynamic feature sets.
\newblock In \emph{International Conference on Machine Learning}, pp.\  12165--12181. PMLR, 2023.

\bibitem[Han et~al.(2021)Han, Zhang, Ding, Gu, Liu, Huo, Qiu, Yao, Zhang, Zhang, et~al.]{han2021pre}
Han, X., Zhang, Z., Ding, N., Gu, Y., Liu, X., Huo, Y., Qiu, J., Yao, Y., Zhang, A., Zhang, L., et~al.
\newblock Pre-trained models: Past, present and future.
\newblock \emph{AI Open}, 2:\penalty0 225--250, 2021.

\bibitem[Hassani \& Khasahmadi(2020)Hassani and Khasahmadi]{icml2020_1971MVGRL}
Hassani, K. and Khasahmadi, A.~H.
\newblock Contrastive multi-view representation learning on graphs.
\newblock In \emph{Proceedings of International Conference on Machine Learning}, pp.\  3451--3461. 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  770--778, 2016.

\bibitem[He et~al.(2023)He, Bresson, Laurent, Perold, LeCun, and Hooi]{he2023harnessingTAPE}
He, X., Bresson, X., Laurent, T., Perold, A., LeCun, Y., and Hooi, B.
\newblock Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed graph representation learning, 2023.

\bibitem[He et~al.(2024)He, Tian, Sun, Chawla, Laurent, LeCun, Bresson, and Hooi]{he2024g}
He, X., Tian, Y., Sun, Y., Chawla, N.~V., Laurent, T., LeCun, Y., Bresson, X., and Hooi, B.
\newblock G-retriever: Retrieval-augmented generation for textual graph understanding and question answering.
\newblock \emph{arXiv preprint arXiv:2402.07630}, 2024.

\bibitem[Hibshman et~al.(2021)Hibshman, Gonzalez, Sikdar, and Weninger]{hibshman2021joint}
Hibshman, J.~I., Gonzalez, D., Sikdar, S., and Weninger, T.
\newblock Joint subgraph-to-subgraph transitions: Generalizing triadic closure for powerful and interpretable graph modeling.
\newblock In \emph{Proceedings of the 14th ACM International Conference on Web Search and Data Mining}, pp.\  815--823, 2021.

\bibitem[Ho{\v{c}}evar \& Dem{\v{s}}ar(2014)Ho{\v{c}}evar and Dem{\v{s}}ar]{hovcevar2014combinatorial}
Ho{\v{c}}evar, T. and Dem{\v{s}}ar, J.
\newblock A combinatorial approach to graphlet counting.
\newblock \emph{Bioinformatics}, 30\penalty0 (4):\penalty0 559--565, 2014.

\bibitem[Hou et~al.(2022)Hou, Liu, Cen, Dong, Yang, Wang, and Tang]{hou2022graphmae}
Hou, Z., Liu, X., Cen, Y., Dong, Y., Yang, H., Wang, C., and Tang, J.
\newblock Graphmae: Self-supervised masked graph autoencoders.
\newblock In \emph{Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pp.\  594--604, 2022.

\bibitem[Hu et~al.(2019)Hu, Liu, Gomes, Zitnik, Liang, Pande, and Leskovec]{hu2019strategiesAttrMask}
Hu, W., Liu, B., Gomes, J., Zitnik, M., Liang, P., Pande, V., and Leskovec, J.
\newblock Strategies for pre-training graph neural networks.
\newblock \emph{arXiv preprint arXiv:1905.12265}, 2019.

\bibitem[Hu et~al.(2020)Hu, Fey, Zitnik, Dong, Ren, Liu, Catasta, and Leskovec]{hu2020openOGB}
Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M., and Leskovec, J.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 22118--22133, 2020.

\bibitem[Huang et~al.(2015)Huang, Tang, Liu, Luo, and Fu]{huang2015triadic}
Huang, H., Tang, J., Liu, L., Luo, J., and Fu, X.
\newblock Triadic closure pattern analysis and prediction in social networks.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering}, 27\penalty0 (12):\penalty0 3374--3389, 2015.

\bibitem[Huang et~al.(2023{\natexlab{a}})Huang, Ren, Chen, Kr{\v{z}}manc, Zeng, Liang, and Leskovec]{huang2023prodigy}
Huang, Q., Ren, H., Chen, P., Kr{\v{z}}manc, G., Zeng, D., Liang, P., and Leskovec, J.
\newblock Prodigy: Enabling in-context learning over graphs.
\newblock \emph{arXiv preprint arXiv:2305.12600}, 2023{\natexlab{a}}.

\bibitem[Huang et~al.(2023{\natexlab{b}})Huang, Poursafaei, Danovitch, Fey, Hu, Rossi, Leskovec, Bronstein, Rabusseau, and Rabbany]{huang2023temporal}
Huang, S., Poursafaei, F., Danovitch, J., Fey, M., Hu, W., Rossi, E., Leskovec, J., Bronstein, M., Rabusseau, G., and Rabbany, R.
\newblock Temporal graph benchmark for machine learning on temporal graphs.
\newblock \emph{arXiv preprint arXiv:2307.01026}, 2023{\natexlab{b}}.

\bibitem[Huang et~al.(2023{\natexlab{c}})Huang, Orth, Ceylan, and Barcel{\'o}]{huang2023theoryLP}
Huang, X., Orth, M.~R., Ceylan, {\.I}.~{\.I}., and Barcel{\'o}, P.
\newblock A theory of link prediction via relational weisfeiler-leman.
\newblock \emph{arXiv preprint arXiv:2302.02209}, 2023{\natexlab{c}}.

\bibitem[Huang et~al.(2023{\natexlab{d}})Huang, Lu, Robinson, Yang, Zhang, Jegelka, and Li]{huang2023stability}
Huang, Y., Lu, W., Robinson, J., Yang, Y., Zhang, M., Jegelka, S., and Li, P.
\newblock On the stability of expressive positional encodings for graph neural networks.
\newblock \emph{arXiv preprint arXiv:2310.02579}, 2023{\natexlab{d}}.

\bibitem[Ibarz et~al.(2022)Ibarz, Kurin, Papamakarios, Nikiforou, Bennani, Csord{\'a}s, Dudzik, Bo{\v{s}}njak, Vitvitskyi, Rubanova, et~al.]{ibarz2022generalist}
Ibarz, B., Kurin, V., Papamakarios, G., Nikiforou, K., Bennani, M., Csord{\'a}s, R., Dudzik, A.~J., Bo{\v{s}}njak, M., Vitvitskyi, A., Rubanova, Y., et~al.
\newblock A generalist neural algorithmic learner.
\newblock In \emph{Learning on graphs conference}, pp.\  2--1. PMLR, 2022.

\bibitem[Jeh \& Widom(2002)Jeh and Widom]{jeh2002simrank}
Jeh, G. and Widom, J.
\newblock Simrank: a measure of structural-context similarity.
\newblock In \emph{Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining}, pp.\  538--543, 2002.

\bibitem[Jin et~al.(2023{\natexlab{a}})Jin, Liu, Han, Jiang, Ji, and Han]{jin2023largesurvey}
Jin, B., Liu, G., Han, C., Jiang, M., Ji, H., and Han, J.
\newblock Large language models on graphs: A comprehensive survey.
\newblock \emph{arXiv preprint arXiv:2312.02783}, 2023{\natexlab{a}}.

\bibitem[Jin et~al.(2023{\natexlab{b}})Jin, Zhang, Zhang, Meng, Zhang, Zhu, and Han]{jin2023patton}
Jin, B., Zhang, W., Zhang, Y., Meng, Y., Zhang, X., Zhu, Q., and Han, J.
\newblock Patton: Language model pretraining on text-rich networks.
\newblock \emph{arXiv preprint arXiv:2305.12268}, 2023{\natexlab{b}}.

\bibitem[Jin et~al.(2020{\natexlab{a}})Jin, Barzilay, and Jaakkola]{jin2020hierarchical}
Jin, W., Barzilay, R., and Jaakkola, T.
\newblock Hierarchical generation of molecular graphs using structural motifs.
\newblock In \emph{International conference on machine learning}, pp.\  4839--4848. PMLR, 2020{\natexlab{a}}.

\bibitem[Jin et~al.(2020{\natexlab{b}})Jin, Derr, Liu, Wang, Wang, Liu, and Tang]{jin2020self}
Jin, W., Derr, T., Liu, H., Wang, Y., Wang, S., Liu, Z., and Tang, J.
\newblock Self-supervised learning on graphs: Deep insights and new direction.
\newblock \emph{arXiv preprint arXiv:2006.10141}, 2020{\natexlab{b}}.

\bibitem[Jing et~al.(2023)Jing, Yuan, Ju, Yang, Wang, and Tao]{jing2023deepGR}
Jing, Y., Yuan, C., Ju, L., Yang, Y., Wang, X., and Tao, D.
\newblock Deep graph reprogramming.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  24345--24354, 2023.

\bibitem[Jo et~al.(2022)Jo, Lee, and Hwang]{jo2022score}
Jo, J., Lee, S., and Hwang, S.~J.
\newblock Score-based generative modeling of graphs via the system of stochastic differential equations.
\newblock In \emph{International Conference on Machine Learning}, pp.\  10362--10383. PMLR, 2022.

\bibitem[Ju et~al.(2023)Ju, Zhao, Wen, Yu, Shah, Ye, and Zhang]{ju2023multiParetoGNN}
Ju, M., Zhao, T., Wen, Q., Yu, W., Shah, N., Ye, Y., and Zhang, C.
\newblock Multi-task self-supervised graph neural networks enable stronger task generalization.
\newblock 2023.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scalingNLP}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Katz(1953)]{katz1953new}
Katz, L.
\newblock A new status index derived from sociometric analysis.
\newblock \emph{Psychometrika}, 18\penalty0 (1):\penalty0 39--43, 1953.

\bibitem[Khanam et~al.(2020)Khanam, Srivastava, and Mago]{khanam2020homophily}
Khanam, K.~Z., Srivastava, G., and Mago, V.
\newblock The homophily principle in social network analysis.
\newblock \emph{arXiv preprint arXiv:2008.10383}, 2020.

\bibitem[Kim et~al.(2022)Kim, Nguyen, Min, Cho, Lee, Lee, and Hong]{kim2022pureTOKENGT}
Kim, J., Nguyen, T.~D., Min, S., Cho, S., Lee, M., Lee, H., and Hong, S.
\newblock Pure transformers are powerful graph learners.
\newblock \emph{arXiv}, abs/2207.02505, 2022.
\newblock URL \url{https://arxiv.org/abs/2207.02505}.

\bibitem[Kipf \& Welling(2016)Kipf and Welling]{kipf2016variationalVGAE}
Kipf, T.~N. and Welling, M.
\newblock Variational graph auto-encoders.
\newblock \emph{arXiv preprint arXiv:1611.07308}, 2016.

\bibitem[Klicpera et~al.(2018)Klicpera, Bojchevski, and G{\"u}nnemann]{Klicpera2018PredictTP}
Klicpera, J., Bojchevski, A., and G{\"u}nnemann, S.
\newblock Predict then propagate: Graph neural networks meet personalized pagerank.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Kreuzer et~al.(2021)Kreuzer, Beaini, Hamilton, L{\'e}tourneau, and Tossou]{kreuzer2021rethinking}
Kreuzer, D., Beaini, D., Hamilton, W., L{\'e}tourneau, V., and Tossou, P.
\newblock Rethinking graph transformers with spectral attention.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 21618--21629, 2021.

\bibitem[Kriege et~al.(2020)Kriege, Johansson, and Morris]{kriege2020survey}
Kriege, N.~M., Johansson, F.~D., and Morris, C.
\newblock A survey on graph kernels.
\newblock \emph{Applied Network Science}, 5\penalty0 (1):\penalty0 1--42, 2020.

\bibitem[Leskovec \& Krevl(2014)Leskovec and Krevl]{snapnets}
Leskovec, J. and Krevl, A.
\newblock {SNAP Datasets}: {Stanford} large network dataset collection.
\newblock \url{http://snap.stanford.edu/data}, June 2014.

\bibitem[Leskovec et~al.(2010)Leskovec, Chakrabarti, Kleinberg, Faloutsos, and Ghahramani]{leskovec2010kronecker}
Leskovec, J., Chakrabarti, D., Kleinberg, J., Faloutsos, C., and Ghahramani, Z.
\newblock Kronecker graphs: an approach to modeling networks.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (2), 2010.

\bibitem[Li et~al.(2022)Li, Shomer, Ding, Wang, Ma, Shah, Tang, and Yin]{li2022graph}
Li, J., Shomer, H., Ding, J., Wang, Y., Ma, Y., Shah, N., Tang, J., and Yin, D.
\newblock Are graph neural networks really helpful for knowledge graph completion?
\newblock \emph{arXiv preprint arXiv:2205.10652}, 2022.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Shomer, Mao, Zeng, Ma, Shah, Tang, and Yin]{li2023evaluatingLP}
Li, J., Shomer, H., Mao, H., Zeng, S., Ma, Y., Shah, N., Tang, J., and Yin, D.
\newblock Evaluating graph neural networks for link prediction: Current pitfalls and new benchmarking.
\newblock \emph{arXiv preprint arXiv:2306.10453}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Li, Wang, Li, Sun, Cheng, and Yu]{li2023surveyGraphLLM}
Li, Y., Li, Z., Wang, P., Li, J., Sun, X., Cheng, H., and Yu, J.~X.
\newblock A survey of graph meets large language model: Progress and future directions.
\newblock \emph{arXiv preprint arXiv:2311.12399}, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2023{\natexlab{c}})Li, Xiong, and Hooi]{li2023graphcleaner}
Li, Y., Xiong, M., and Hooi, B.
\newblock Graphcleaner: Detecting mislabelled samples in popular graph learning benchmarks.
\newblock \emph{arXiv preprint arXiv:2306.00015}, 2023{\natexlab{c}}.

\bibitem[Li et~al.(2024)Li, Wang, Li, Yu, and Li]{li2024zerog}
Li, Y., Wang, P., Li, Z., Yu, J.~X., and Li, J.
\newblock Zerog: Investigating cross-dataset zero-shot transferability in graphs.
\newblock \emph{arXiv preprint arXiv:2402.11235}, 2024.

\bibitem[Lim et~al.(2021)Lim, Hohne, Li, Huang, Gupta, Bhalerao, and Lim]{lim2021largeLINKX}
Lim, D., Hohne, F., Li, X., Huang, S.~L., Gupta, V., Bhalerao, O., and Lim, S.~N.
\newblock Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 20887--20902, 2021.

\bibitem[Lim et~al.(2022)Lim, Robinson, Zhao, Smidt, Sra, Maron, and Jegelka]{lim2022sign}
Lim, D., Robinson, J.~D., Zhao, L., Smidt, T., Sra, S., Maron, H., and Jegelka, S.
\newblock Sign and basis invariant networks for spectral graph representation learning.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Ling et~al.(2021)Ling, Wu, Wang, Pan, Ma, Xu, Liu, Wu, and Ji]{ling2021deep}
Ling, X., Wu, L., Wang, S., Pan, G., Ma, T., Xu, F., Liu, A.~X., Wu, C., and Ji, S.
\newblock Deep graph matching and searching for semantic code retrieval.
\newblock \emph{ACM Transactions on Knowledge Discovery from Data (TKDD)}, 15\penalty0 (5):\penalty0 1--21, 2021.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Inae, Zhao, Xu, Luo, and Jiang]{liu2023data}
Liu, G., Inae, E., Zhao, T., Xu, J., Luo, T., and Jiang, M.
\newblock Data-centric learning from unlabeled graphs with diffusion model.
\newblock \emph{arXiv preprint arXiv:2303.10108}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Feng, Kong, Liang, Tao, Chen, and Zhang]{oneforall}
Liu, H., Feng, J., Kong, L., Liang, N., Tao, D., Chen, Y., and Zhang, M.
\newblock One for all: Towards training one graph model for all classification tasks.
\newblock \emph{arXiv preprint arXiv:2310.00149}, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2023{\natexlab{c}})Liu, Liao, and Luo]{liu2023simga}
Liu, H., Liao, N., and Luo, S.
\newblock Simga: A simple and effective heterophilous graph neural network with efficient global aggregation.
\newblock \emph{arXiv preprint arXiv:2305.09958}, 2023{\natexlab{c}}.

\bibitem[Liu et~al.(2023{\natexlab{d}})Liu, Yang, Lu, Chen, Li, Zhang, Bai, Fang, Sun, Yu, et~al.]{liu2023towardsGFMSurvey}
Liu, J., Yang, C., Lu, Z., Chen, J., Li, Y., Zhang, M., Bai, T., Fang, Y., Sun, L., Yu, P.~S., et~al.
\newblock Towards graph foundation models: A survey and beyond.
\newblock \emph{arXiv preprint arXiv:2310.11829}, 2023{\natexlab{d}}.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Mao, Chen, Zhao, Shah, and Tang]{liu2024neural}
Liu, J., Mao, H., Chen, Z., Zhao, T., Shah, N., and Tang, J.
\newblock Neural scaling laws on graphs.
\newblock 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2022)Liu, Wang, Bo, Shi, and Pei]{liu2022revisitingSGCL}
Liu, N., Wang, X., Bo, D., Shi, C., and Pei, J.
\newblock Revisiting graph contrastive learning from the perspective of graph spectrum.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 2972--2983, 2022.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Wang, Xu, Liu, Sun, Guo, and Ma]{liu2024source}
Liu, R., Wang, Y., Xu, H., Liu, B., Sun, J., Guo, Z., and Ma, W.
\newblock Source code vulnerability detection: Combining code language models and code property graphs.
\newblock \emph{arXiv preprint arXiv:2404.14719}, 2024{\natexlab{b}}.

\bibitem[Liu et~al.(2023{\natexlab{e}})Liu, Shi, Zhang, Zhang, Kawaguchi, Wang, and Chua]{liu2023rethinkingSIMSGT}
Liu, Z., Shi, Y., Zhang, A., Zhang, E., Kawaguchi, K., Wang, X., and Chua, T.-S.
\newblock Rethinking tokenizer and decoder in masked graph modeling for molecules.
\newblock In \emph{NeurIPS}, 2023{\natexlab{e}}.
\newblock URL \url{https://openreview.net/forum?id=fWLf8DV0fI}.

\bibitem[Liu et~al.(2023{\natexlab{f}})Liu, Yu, Fang, and Zhang]{liu2023graphpromptWWW}
Liu, Z., Yu, X., Fang, Y., and Zhang, X.
\newblock Graphprompt: Unifying pre-training and downstream tasks for graph neural networks.
\newblock In \emph{Proceedings of the ACM Web Conference 2023}, pp.\  417--428, 2023{\natexlab{f}}.

\bibitem[Lu et~al.(2023)Lu, Gao, He, Zhang, and Ke]{unimol}
Lu, S., Gao, Z., He, D., Zhang, L., and Ke, G.
\newblock Highly accurate quantum chemical property prediction with uni-mol+.
\newblock \emph{arXiv preprint arXiv:2303.16982}, 2023.

\bibitem[Luan et~al.(2021)Luan, Hua, Lu, Zhu, Zhao, Zhang, Chang, and Precup]{luan2021heterophily}
Luan, S., Hua, C., Lu, Q., Zhu, J., Zhao, M., Zhang, S., Chang, X.-W., and Precup, D.
\newblock Is heterophily a real nightmare for graph neural networks to do node classification?
\newblock \emph{arXiv preprint arXiv:2109.05641}, 2021.

\bibitem[Luan et~al.(2023)Luan, Hua, Xu, Lu, Zhu, Chang, Fu, Leskovec, and Precup]{luan2023graph}
Luan, S., Hua, C., Xu, M., Lu, Q., Zhu, J., Chang, X.-W., Fu, J., Leskovec, J., and Precup, D.
\newblock When do graph neural networks help with node classification: Investigating the homophily principle on node distinguishability.
\newblock \emph{arXiv preprint arXiv:2304.14274}, 2023.

\bibitem[Luo et~al.(2021)Luo, Yan, and Ji]{luo2021graphdf}
Luo, Y., Yan, K., and Ji, S.
\newblock Graphdf: A discrete flow model for molecular graph generation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  7192--7203. PMLR, 2021.

\bibitem[Luo et~al.(2024)Luo, Song, Huang, Lian, Zhang, Jiang, Xie, and Jin]{luo2024graphinstruct}
Luo, Z., Song, X., Huang, H., Lian, J., Zhang, C., Jiang, J., Xie, X., and Jin, H.
\newblock Graphinstruct: Empowering large language models with graph understanding and reasoning capability.
\newblock \emph{arXiv preprint arXiv:2403.04483}, 2024.

\bibitem[Ma et~al.(2021)Ma, Liu, Shah, and Tang]{ma2021homophily}
Ma, Y., Liu, X., Shah, N., and Tang, J.
\newblock Is homophily a necessity for graph neural networks?
\newblock \emph{arXiv preprint arXiv:2106.06134}, 2021.

\bibitem[Mao et~al.(2023{\natexlab{a}})Mao, Chen, Jin, Han, Ma, Zhao, Shah, and Tang]{mao2023demystifyingNC}
Mao, H., Chen, Z., Jin, W., Han, H., Ma, Y., Zhao, T., Shah, N., and Tang, J.
\newblock Demystifying structural disparity in graph neural networks: Can one size fit all?
\newblock \emph{arXiv preprint arXiv:2306.01323}, 2023{\natexlab{a}}.

\bibitem[Mao et~al.(2023{\natexlab{b}})Mao, Li, Shomer, Li, Fan, Ma, Zhao, Shah, and Tang]{mao2023revisitingLP}
Mao, H., Li, J., Shomer, H., Li, B., Fan, W., Ma, Y., Zhao, T., Shah, N., and Tang, J.
\newblock Revisiting link prediction: A data perspective.
\newblock \emph{arXiv preprint arXiv:2310.00793}, 2023{\natexlab{b}}.

\bibitem[Mao et~al.(2024)Mao, Liu, Ma, Wang, and Tang]{mao2024data}
Mao, H., Liu, G., Ma, Y., Wang, R., and Tang, J.
\newblock A data generation perspective to the mechanism of in-context learning.
\newblock \emph{arXiv preprint arXiv:2402.02212}, 2024.

\bibitem[Masters et~al.(2022)Masters, Dean, Klaser, Li, Maddrell-Mander, Sanders, Helal, Beker, Rampášek, and Beaini]{gps_plusplus}
Masters, D., Dean, J., Klaser, K., Li, Z., Maddrell-Mander, S., Sanders, A., Helal, H., Beker, D., Rampášek, L., and Beaini, D.
\newblock Gps++: An optimised hybrid mpnn/transformer for molecular property prediction.
\newblock \emph{arXiv preprint arXiv:2212.02229}, 2022.

\bibitem[McCoy et~al.(2023)McCoy, Yao, Friedman, Hardy, and Griffiths]{mccoy2023embers}
McCoy, R.~T., Yao, S., Friedman, D., Hardy, M., and Griffiths, T.~L.
\newblock Embers of autoregression: Understanding large language models through the problem they are trained to solve.
\newblock \emph{arXiv preprint arXiv:2309.13638}, 2023.

\bibitem[Menczer et~al.(2020)Menczer, Fortunato, and Davis]{Menczer_Fortunato_Davis_2020}
Menczer, F., Fortunato, S., and Davis, C.~A.
\newblock \emph{A First Course in Network Science}.
\newblock Cambridge University Press, 2020.

\bibitem[Milo et~al.(2002)Milo, Shen-Orr, Itzkovitz, Kashtan, Chklovskii, and Alon]{milo2002network}
Milo, R., Shen-Orr, S., Itzkovitz, S., Kashtan, N., Chklovskii, D., and Alon, U.
\newblock Network motifs: simple building blocks of complex networks.
\newblock \emph{Science}, 298\penalty0 (5594):\penalty0 824--827, 2002.

\bibitem[Mishra et~al.(2022)Mishra, Panda, Phoo, Chen, Karlinsky, Saenko, Saligrama, and Feris]{mishra2022task2sim}
Mishra, S., Panda, R., Phoo, C.~P., Chen, C.-F.~R., Karlinsky, L., Saenko, K., Saligrama, V., and Feris, R.~S.
\newblock Task2sim: Towards effective pre-training and transfer from synthetic data.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  9194--9204, 2022.

\bibitem[Morris et~al.(2019)Morris, Ritzert, Fey, Hamilton, Lenssen, Rattan, and Grohe]{morris2019weisfeiler}
Morris, C., Ritzert, M., Fey, M., Hamilton, W.~L., Lenssen, J.~E., Rattan, G., and Grohe, M.
\newblock Weisfeiler and leman go neural: Higher-order graph neural networks.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~33, pp.\  4602--4609, 2019.

\bibitem[Morris et~al.(2020)Morris, Kriege, Bause, Kersting, Mutzel, and Neumann]{Morris+2020}
Morris, C., Kriege, N.~M., Bause, F., Kersting, K., Mutzel, P., and Neumann, M.
\newblock Tudataset: A collection of benchmark datasets for learning with graphs.
\newblock In \emph{ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020)}, 2020.
\newblock URL \url{www.graphlearning.io}.

\bibitem[Morris et~al.(2023)Morris, Lipman, Maron, Rieck, Kriege, Grohe, Fey, and Borgwardt]{JMLR:v24:22-0240_WLStory}
Morris, C., Lipman, Y., Maron, H., Rieck, B., Kriege, N.~M., Grohe, M., Fey, M., and Borgwardt, K.
\newblock Weisfeiler and leman go machine learning: The story so far.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (333):\penalty0 1--59, 2023.
\newblock URL \url{http://jmlr.org/papers/v24/22-0240.html}.

\bibitem[M{\"u}ller et~al.(2023)M{\"u}ller, Galkin, Morris, and Ramp{\'a}{\v{s}}ek]{muller2023attending}
M{\"u}ller, L., Galkin, M., Morris, C., and Ramp{\'a}{\v{s}}ek, L.
\newblock Attending to graph transformers.
\newblock \emph{arXiv preprint arXiv:2302.04181}, 2023.

\bibitem[Murase et~al.(2019)Murase, Jo, T{\"o}r{\"o}k, Kert{\'e}sz, Kaski, et~al.]{murase2019structural}
Murase, Y., Jo, H.~H., T{\"o}r{\"o}k, J., Kert{\'e}sz, J., Kaski, K., et~al.
\newblock Structural transition in social networks.
\newblock 2019.

\bibitem[Oono \& Suzuki(2019)Oono and Suzuki]{oono2019graph}
Oono, K. and Suzuki, T.
\newblock Graph neural networks exponentially lose expressive power for node classification.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Panwar et~al.(2023)Panwar, Ahuja, and Goyal]{panwar2023context}
Panwar, M., Ahuja, K., and Goyal, N.
\newblock In-context learning through the bayesian prism.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[Perozzi et~al.(2024)Perozzi, Fatemi, Zelle, Tsitsulin, Kazemi, Al-Rfou, and Halcrow]{perozzi2024let}
Perozzi, B., Fatemi, B., Zelle, D., Tsitsulin, A., Kazemi, M., Al-Rfou, R., and Halcrow, J.
\newblock Let your graph do the talking: Encoding structured data for llms.
\newblock \emph{arXiv preprint arXiv:2402.05862}, 2024.

\bibitem[Project()]{recsys}
Project, U. C.~R.
\newblock Recommender systems and personalization datasets.
\newblock URL \url{https://cseweb.ucsd.edu/~jmcauley/datasets.html}.

\bibitem[Prystawski \& Goodman(2023)Prystawski and Goodman]{prystawski2023think}
Prystawski, B. and Goodman, N.~D.
\newblock Why think step-by-step? reasoning emerges from the locality of experience.
\newblock \emph{arXiv preprint arXiv:2304.03843}, 2023.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{CLIP}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International conference on machine learning}, pp.\  8748--8763. PMLR, 2021.

\bibitem[Rafi et~al.(2024)Rafi, Kim, Chen, Chen, and Wang]{rafi2024towards}
Rafi, M.~N., Kim, D.~J., Chen, A.~R., Chen, T.-H., and Wang, S.
\newblock Towards better graph neural neural network-based fault localization through enhanced code representation.
\newblock \emph{arXiv preprint arXiv:2404.04496}, 2024.

\bibitem[Ramsundar et~al.(2019)Ramsundar, Eastman, Walters, Pande, Leswing, and Wu]{MoleculeNet}
Ramsundar, B., Eastman, P., Walters, P., Pande, V., Leswing, K., and Wu, Z.
\newblock \emph{Deep Learning for the Life Sciences}.
\newblock O'Reilly Media, 2019.
\newblock \url{https://www.amazon.com/Deep-Learning-Life-Sciences-Microscopy/dp/1492039837}.

\bibitem[Ribeiro et~al.(2009)Ribeiro, Silva, and Kaiser]{ribeiro2009strategies}
Ribeiro, P., Silva, F., and Kaiser, M.
\newblock Strategies for network motifs discovery.
\newblock In \emph{2009 Fifth IEEE International Conference on e-Science}, pp.\  80--87. IEEE, 2009.

\bibitem[Ribeiro et~al.(2021)Ribeiro, Paredes, Silva, Aparicio, and Silva]{ribeiro2021survey}
Ribeiro, P., Paredes, P., Silva, M.~E., Aparicio, D., and Silva, F.
\newblock A survey on subgraph counting: concepts, algorithms, and applications to network motifs and graphlets.
\newblock \emph{ACM Computing Surveys (CSUR)}, 54\penalty0 (2):\penalty0 1--36, 2021.

\bibitem[Robins et~al.(2007)Robins, Pattison, Kalish, and Lusher]{robins2007introduction}
Robins, G., Pattison, P., Kalish, Y., and Lusher, D.
\newblock An introduction to exponential random graph (p*) models for social networks.
\newblock \emph{Social networks}, 29\penalty0 (2):\penalty0 173--191, 2007.

\bibitem[Rossi \& Ahmed(2015)Rossi and Ahmed]{nr}
Rossi, R.~A. and Ahmed, N.~K.
\newblock The network data repository with interactive graph analytics and visualization.
\newblock In \emph{AAAI}, 2015.
\newblock URL \url{http://networkrepository.com}.

\bibitem[Ruiz et~al.(2023)Ruiz, Chamon, and Ribeiro]{gnn_stability}
Ruiz, L., Chamon, L. F.~O., and Ribeiro, A.
\newblock Transferability properties of graph neural networks.
\newblock \emph{IEEE Transactions on Signal Processing}, 71:\penalty0 3474--3489, 2023.
\newblock \doi{10.1109/TSP.2023.3297848}.

\bibitem[Saparov \& He(2022)Saparov and He]{saparov2022language}
Saparov, A. and He, H.
\newblock Language models are greedy reasoners: A systematic formal analysis of chain-of-thought.
\newblock \emph{arXiv preprint arXiv:2210.01240}, 2022.

\bibitem[Schlichtkrull et~al.(2018)Schlichtkrull, Kipf, Bloem, Van Den~Berg, Titov, and Welling]{schlichtkrull2018modeling}
Schlichtkrull, M., Kipf, T.~N., Bloem, P., Van Den~Berg, R., Titov, I., and Welling, M.
\newblock Modeling relational data with graph convolutional networks.
\newblock In \emph{The Semantic Web: 15th International Conference, ESWC 2018, Heraklion, Crete, Greece, June 3--7, 2018, Proceedings 15}, pp.\  593--607. Springer, 2018.

\bibitem[Shi et~al.(2022)Shi, Ding, Cao, Liu, Li, et~al.]{shi2022learning}
Shi, H., Ding, J., Cao, Y., Liu, L., Li, Y., et~al.
\newblock Learning symbolic models for graph-structured physical mechanism.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Shoghi et~al.(2023)Shoghi, Kolluru, Kitchin, Ulissi, Zitnick, and Wood]{shoghi2023jmp}
Shoghi, N., Kolluru, A., Kitchin, J.~R., Ulissi, Z.~W., Zitnick, C.~L., and Wood, B.~M.
\newblock From molecules to materials: Pre-training large generalizable models for atomic property prediction, 2023.

\bibitem[Srinivasan \& Ribeiro(2019)Srinivasan and Ribeiro]{srinivasan2019equivalence}
Srinivasan, B. and Ribeiro, B.
\newblock On the equivalence between positional node embeddings and structural graph representations.
\newblock \emph{arXiv preprint arXiv:1910.00452}, 2019.

\bibitem[Stechly et~al.(2023)Stechly, Marquez, and Kambhampati]{stechly2023gpt}
Stechly, K., Marquez, M., and Kambhampati, S.
\newblock Gpt-4 doesn’t know it’s wrong: An analysis of iterative prompting for reasoning problems.
\newblock In \emph{NeurIPS 2023 Foundation Models for Decision Making Workshop}, 2023.

\bibitem[Sun et~al.(2019)Sun, Hoffmann, Verma, and Tang]{sun2019infograph}
Sun, F.-Y., Hoffmann, J., Verma, V., and Tang, J.
\newblock Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization.
\newblock \emph{arXiv preprint arXiv:1908.01000}, 2019.

\bibitem[Sun et~al.(2022)Sun, Zhou, He, Wang, and Wang]{GPPT}
Sun, M., Zhou, K., He, X., Wang, Y., and Wang, X.
\newblock Gppt: Graph pre-training and prompt tuning to generalize graph neural networks.
\newblock In \emph{Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, KDD '22, pp.\  1717–1727, New York, NY, USA, 2022. Association for Computing Machinery.
\newblock ISBN 9781450393850.
\newblock \doi{10.1145/3534678.3539249}.
\newblock URL \url{https://doi.org/10.1145/3534678.3539249}.

\bibitem[Sun et~al.(2023)Sun, Cheng, Li, Liu, and Guan]{sun2023all}
Sun, X., Cheng, H., Li, J., Liu, B., and Guan, J.
\newblock All in one: Multi-task prompting for graph neural networks.
\newblock 2023.

\bibitem[Taguchi et~al.(2021)Taguchi, Liu, and Murata]{taguchi2021graph}
Taguchi, H., Liu, X., and Murata, T.
\newblock Graph convolutional networks for graphs containing missing features.
\newblock \emph{Future Generation Computer Systems}, 117:\penalty0 155--168, 2021.

\bibitem[Tang(2016)]{aminer}
Tang, J.
\newblock Aminer: Toward understanding big scholar data.
\newblock In \emph{Proceedings of the Ninth ACM International Conference on Web Search and Data Mining}, WSDM '16, pp.\  467, New York, NY, USA, 2016. Association for Computing Machinery.
\newblock ISBN 9781450337168.
\newblock \doi{10.1145/2835776.2835849}.
\newblock URL \url{https://doi.org/10.1145/2835776.2835849}.

\bibitem[Tang et~al.(2023)Tang, Yang, Wei, Shi, Su, Cheng, Yin, and Huang]{tang2023graphgpt}
Tang, J., Yang, Y., Wei, W., Shi, L., Su, L., Cheng, S., Yin, D., and Huang, C.
\newblock Graphgpt: Graph instruction tuning for large language models.
\newblock \emph{arXiv preprint arXiv:2310.13023}, 2023.

\bibitem[Taylor et~al.(2022)Taylor, Kardas, Cucurull, Scialom, Hartshorn, Saravia, Poulton, Kerkez, and Stojnic]{taylor2022galactica}
Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E., Poulton, A., Kerkez, V., and Stojnic, R.
\newblock Galactica: A large language model for science.
\newblock \emph{arXiv preprint arXiv:2211.09085}, 2022.

\bibitem[Topping et~al.(2021)Topping, Di~Giovanni, Chamberlain, Dong, and Bronstein]{topping2021understanding}
Topping, J., Di~Giovanni, F., Chamberlain, B.~P., Dong, X., and Bronstein, M.~M.
\newblock Understanding over-squashing and bottlenecks on graphs via curvature.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Trinh et~al.(2024)Trinh, Wu, Le, He, and Luong]{trinh2024solving}
Trinh, T.~H., Wu, Y., Le, Q.~V., He, H., and Luong, T.
\newblock Solving olympiad geometry without human demonstrations.
\newblock \emph{Nature}, 625\penalty0 (7995):\penalty0 476--482, 2024.

\bibitem[Um et~al.(2023)Um, Park, Park, and young Choi]{um2023confidencebased}
Um, D., Park, J., Park, S., and young Choi, J.
\newblock Confidence-based feature imputation for graphs with partially known features.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=YPKBIILy-Kt}.

\bibitem[Van Den~Oord et~al.(2017)Van Den~Oord, Vinyals, et~al.]{van2017neural}
Van Den~Oord, A., Vinyals, O., et~al.
\newblock Neural discrete representation learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Vashishth et~al.(2019)Vashishth, Sanyal, Nitin, and Talukdar]{vashishth2019composition}
Vashishth, S., Sanyal, S., Nitin, V., and Talukdar, P.
\newblock Composition-based multi-relational graph convolutional networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Veli{\v{c}}kovi{\'c} et~al.(2017)Veli{\v{c}}kovi{\'c}, Cucurull, Casanova, Romero, Lio, and Bengio]{velivckovic2017graph}
Veli{\v{c}}kovi{\'c}, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y.
\newblock Graph attention networks.
\newblock \emph{arXiv preprint arXiv:1710.10903}, 2017.

\bibitem[Veli{\v{c}}kovi{\'c} et~al.(2018)Veli{\v{c}}kovi{\'c}, Fedus, Hamilton, Li{\`o}, Bengio, and Hjelm]{velivckovic2018deepDGI}
Veli{\v{c}}kovi{\'c}, P., Fedus, W., Hamilton, W.~L., Li{\`o}, P., Bengio, Y., and Hjelm, R.~D.
\newblock Deep graph infomax.
\newblock \emph{arXiv preprint arXiv:1809.10341}, 2018.

\bibitem[Veli{\v{c}}kovi{\'c} et~al.(2022)Veli{\v{c}}kovi{\'c}, Badia, Budden, Pascanu, Banino, Dashevskiy, Hadsell, and Blundell]{velivckovic2022clrs}
Veli{\v{c}}kovi{\'c}, P., Badia, A.~P., Budden, D., Pascanu, R., Banino, A., Dashevskiy, M., Hadsell, R., and Blundell, C.
\newblock The clrs algorithmic reasoning benchmark.
\newblock In \emph{International Conference on Machine Learning}, pp.\  22084--22102. PMLR, 2022.

\bibitem[Vignac et~al.(2023)Vignac, Krawczuk, Siraudin, Wang, Cevher, and Frossard]{vignac2023digress}
Vignac, C., Krawczuk, I., Siraudin, A., Wang, B., Cevher, V., and Frossard, P.
\newblock Digress: Discrete denoising diffusion for graph generation.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=UaAD-Nu86WX}.

\bibitem[Vishwanathan et~al.(2010)Vishwanathan, Schraudolph, Kondor, and Borgwardt]{vishwanathan2010graphkernel}
Vishwanathan, S. V.~N., Schraudolph, N.~N., Kondor, R., and Borgwardt, K.~M.
\newblock Graph kernels.
\newblock \emph{Journal of Machine Learning Research}, 11:\penalty0 1201--1242, 2010.

\bibitem[Wang et~al.(2021)Wang, Yin, Zhang, and Li]{wang2021equivariant}
Wang, H., Yin, H., Zhang, M., and Li, P.
\newblock Equivariant and stable positional encoding for more powerful graph neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Feng, He, Tan, Han, and Tsvetkov]{wang2023NLGRAPH}
Wang, H., Feng, S., He, T., Tan, Z., Han, X., and Tsvetkov, Y.
\newblock Can language models solve graph problems in natural language?
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=UDqHhbqYJV}.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Guo, Yang, and Wang]{wang2024understanding}
Wang, J., Guo, Y., Yang, L., and Wang, Y.
\newblock Understanding heterophily for graph neural networks.
\newblock \emph{arXiv preprint arXiv:2401.09125}, 2024{\natexlab{a}}.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Wu, Hou, Liu, Gao, and McAuley]{wang2024instructgraph}
Wang, J., Wu, J., Hou, Y., Liu, Y., Gao, M., and McAuley, J.
\newblock Instructgraph: Boosting large language models via graph-centric instruction tuning and preference alignment.
\newblock \emph{arXiv preprint arXiv:2402.08785}, 2024{\natexlab{b}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Yang, and Zhang]{wang2023neural}
Wang, X., Yang, H., and Zhang, M.
\newblock Neural common neighbor with completion for link prediction.
\newblock \emph{arXiv preprint arXiv:2302.00890}, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2023{\natexlab{c}})Wang, Elhag, Jaitly, Susskind, and Bautista]{wang2023generating}
Wang, Y., Elhag, A.~A., Jaitly, N., Susskind, J.~M., and Bautista, M.~A.
\newblock Generating molecular conformer fields.
\newblock \emph{arXiv preprint arXiv:2311.17932}, 2023{\natexlab{c}}.

\bibitem[Wang et~al.(2024{\natexlab{c}})Wang, Cui, and Kleinberg]{wang2024microstructures}
Wang, Y., Cui, H., and Kleinberg, J.
\newblock Microstructures and accuracy of graph recall by large language models.
\newblock \emph{arXiv preprint arXiv:2402.11821}, 2024{\natexlab{c}}.

\bibitem[Wu et~al.(2022)Wu, Zhao, Li, Wipf, and Yan]{nodeformer}
Wu, Q., Zhao, W., Li, Z., Wipf, D.~P., and Yan, J.
\newblock Nodeformer: A scalable graph structure learning transformer for node classification.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27387--27401, 2022.

\bibitem[Wu et~al.(2023)Wu, Ajorlou, Wu, and Jadbabaie]{wu2023demystifying}
Wu, X., Ajorlou, A., Wu, Z., and Jadbabaie, A.
\newblock Demystifying oversmoothing in attention-based graph neural networks.
\newblock \emph{arXiv preprint arXiv:2305.16102}, 2023.

\bibitem[Xia et~al.(2023)Xia, Zhao, Hu, Gao, Tan, Liu, Li, and Li]{xia2023molebert}
Xia, J., Zhao, C., Hu, B., Gao, Z., Tan, C., Liu, Y., Li, S., and Li, S.~Z.
\newblock Mole-{BERT}: Rethinking pre-training graph neural networks for molecules.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=jevY-DtiZTR}.

\bibitem[Xie et~al.(2023)Xie, Zheng, Ma, Zhang, Ioannidis, Song, Ping, Wang, Yang, Xu, et~al.]{xie2023graphGALM}
Xie, H., Zheng, D., Ma, J., Zhang, H., Ioannidis, V.~N., Song, X., Ping, Q., Wang, S., Yang, C., Xu, Y., et~al.
\newblock Graph-aware language model pre-training on a large graph corpus can help multiple graph applications.
\newblock \emph{arXiv preprint arXiv:2306.02592}, 2023.

\bibitem[Xu et~al.(2019)Xu, Hu, Leskovec, and Jegelka]{xu2018how}
Xu, K., Hu, W., Leskovec, J., and Jegelka, S.
\newblock How powerful are graph neural networks?
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=ryGs6iA5Km}.

\bibitem[Xu et~al.(2020)Xu, Li, Zhang, Du, Kawarabayashi, and Jegelka]{xu2020can}
Xu, K., Li, J., Zhang, M., Du, S.~S., Kawarabayashi, K.-i., and Jegelka, S.
\newblock What can neural networks reason about?
\newblock \emph{ICLR 2020}, 2020.

\bibitem[Yang et~al.(2023)Yang, Tian, Xu, Liu, Hong, Qu, Zhang, Cui, Zhang, and Leskovec]{yang2023vqgraph}
Yang, L., Tian, Y., Xu, M., Liu, Z., Hong, S., Qu, W., Zhang, W., Cui, B., Zhang, M., and Leskovec, J.
\newblock Vqgraph: Graph vector-quantization for bridging gnns and mlps.
\newblock \emph{arXiv preprint arXiv:2308.02117}, 2023.

\bibitem[Yang et~al.(2021)Yang, Liu, Wang, Zhou, Gan, Wei, Zhang, Huang, and Wipf]{yang2021graph}
Yang, Y., Liu, T., Wang, Y., Zhou, J., Gan, Q., Wei, Z., Zhang, Z., Huang, Z., and Wipf, D.
\newblock Graph neural networks inspired by classical iterative algorithms.
\newblock In \emph{International Conference on Machine Learning}, pp.\  11773--11783. PMLR, 2021.

\bibitem[Yao et~al.(2024)Yao, Wang, Zhang, Qin, Zhang, Chu, Yang, Zhu, and Mei]{yao2024exploring}
Yao, Y., Wang, X., Zhang, Z., Qin, Y., Zhang, Z., Chu, X., Yang, Y., Zhu, W., and Mei, H.
\newblock Exploring the potential of large language models in graph generation.
\newblock \emph{arXiv preprint arXiv:2403.14358}, 2024.

\bibitem[Yasunaga et~al.(2022{\natexlab{a}})Yasunaga, Bosselut, Ren, Zhang, Manning, Liang, and Leskovec]{yasunaga2022dragon}
Yasunaga, M., Bosselut, A., Ren, H., Zhang, X., Manning, C.~D., Liang, P., and Leskovec, J.
\newblock Deep bidirectional language-knowledge graph pretraining.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2022{\natexlab{a}}.

\bibitem[Yasunaga et~al.(2022{\natexlab{b}})Yasunaga, Leskovec, and Liang]{yasunaga2022linkbert}
Yasunaga, M., Leskovec, J., and Liang, P.
\newblock Linkbert: Pretraining language models with document links.
\newblock \emph{arXiv preprint arXiv:2203.15827}, 2022{\natexlab{b}}.

\bibitem[Ye et~al.(2022)Ye, Zhang, Chen, and Chen]{ye2022generative}
Ye, H., Zhang, N., Chen, H., and Chen, H.
\newblock Generative knowledge graph construction: A review.
\newblock \emph{arXiv preprint arXiv:2210.12714}, 2022.

\bibitem[Ye et~al.(2024)Ye, Zhang, Wang, Xu, and Zhang]{ye2024language}
Ye, R., Zhang, C., Wang, R., Xu, S., and Zhang, Y.
\newblock Language is all a graph needs.
\newblock In \emph{Findings of the Association for Computational Linguistics: EACL 2024}, pp.\  1955--1973, 2024.

\bibitem[Yin et~al.(2022)Yin, Zhang, Wang, Wang, and Li]{yin2022algorithm}
Yin, H., Zhang, M., Wang, Y., Wang, J., and Li, P.
\newblock Algorithm and system co-design for efficient subgraph-based graph representation learning.
\newblock \emph{arXiv preprint arXiv:2202.13538}, 2022.

\bibitem[Ying et~al.(2018)Ying, He, Chen, Eksombatchai, Hamilton, and Leskovec]{ying2018graphPINSAGE}
Ying, R., He, R., Chen, K., Eksombatchai, P., Hamilton, W.~L., and Leskovec, J.
\newblock Graph convolutional neural networks for web-scale recommender systems.
\newblock In \emph{Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery \& data mining}, pp.\  974--983, 2018.

\bibitem[You et~al.(2019)You, Ying, and Leskovec]{you2019position}
You, J., Ying, R., and Leskovec, J.
\newblock Position-aware graph neural networks.
\newblock In \emph{International conference on machine learning}, pp.\  7134--7143. PMLR, 2019.

\bibitem[You et~al.(2021)You, Gomes-Selman, Ying, and Leskovec]{you2021identity}
You, J., Gomes-Selman, J.~M., Ying, R., and Leskovec, J.
\newblock Identity-aware graph neural networks.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~35, pp.\  10737--10745, 2021.

\bibitem[You et~al.(2020)You, Chen, Sui, Chen, Wang, and Shen]{you2020graphGCL}
You, Y., Chen, T., Sui, Y., Chen, T., Wang, Z., and Shen, Y.
\newblock Graph contrastive learning with augmentations.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 5812--5823, 2020.

\bibitem[You et~al.(2023)You, Chen, Wang, and Shen]{you2023graphSpecReg}
You, Y., Chen, T., Wang, Z., and Shen, Y.
\newblock Graph domain adaptation via theory-grounded spectral regularization.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=OysfLgrk8mk}.

\bibitem[Yu et~al.(2023)Yu, Lezama, Gundavarapu, Versari, Sohn, Minnen, Cheng, Gupta, Gu, Hauptmann, et~al.]{yu2023language}
Yu, L., Lezama, J., Gundavarapu, N.~B., Versari, L., Sohn, K., Minnen, D., Cheng, Y., Gupta, A., Gu, X., Hauptmann, A.~G., et~al.
\newblock Language model beats diffusion--tokenizer is key to visual generation.
\newblock \emph{arXiv preprint arXiv:2310.05737}, 2023.

\bibitem[Yue et~al.(2023)Yue, Rabhi, Moreira, Wang, and Oldridge]{yue2023llamarec}
Yue, Z., Rabhi, S., Moreira, G. d. S.~P., Wang, D., and Oldridge, E.
\newblock Llamarec: Two-stage recommendation using large language models for ranking.
\newblock \emph{arXiv preprint arXiv:2311.02089}, 2023.

\bibitem[Zeng et~al.(2019)Zeng, Zhou, Srivastava, Kannan, and Prasanna]{zeng2019graphsaint}
Zeng, H., Zhou, H., Srivastava, A., Kannan, R., and Prasanna, V.
\newblock Graphsaint: Graph sampling based inductive learning method.
\newblock \emph{arXiv preprint arXiv:1907.04931}, 2019.

\bibitem[Zeng et~al.(2021)Zeng, Zhang, Xia, Srivastava, Malevich, Kannan, Prasanna, Jin, and Chen]{shaDow}
Zeng, H., Zhang, M., Xia, Y., Srivastava, A., Malevich, A., Kannan, R., Prasanna, V., Jin, L., and Chen, R.
\newblock Decoupling the depth and scope of graph neural networks.
\newblock In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W. (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=d0MtHWY0NZ}.

\bibitem[Zhai et~al.(2023)Zhai, {\"O}rnek, Wu, Di, Tombari, Navab, and Busam]{zhai2023commonscenes}
Zhai, G., {\"O}rnek, E.~P., Wu, S.-C., Di, Y., Tombari, F., Navab, N., and Busam, B.
\newblock Commonscenes: Generating commonsense 3d indoor scenes with scene graphs.
\newblock \emph{arXiv preprint arXiv:2305.16283}, 2023.

\bibitem[Zhai et~al.(2024)Zhai, {\"O}rnek, Wu, Di, Tombari, Navab, and Busam]{zhai2024commonscenes}
Zhai, G., {\"O}rnek, E.~P., Wu, S.-C., Di, Y., Tombari, F., Navab, N., and Busam, B.
\newblock Commonscenes: Generating commonsense 3d indoor scenes with scene graphs.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Zhang et~al.(2024)Zhang, Gai, Du, Ye, He, and Wang]{zhang2024beyond}
Zhang, B., Gai, J., Du, Y., Ye, Q., He, D., and Wang, L.
\newblock Beyond weisfeiler-lehman: A quantitative framework for gnn expressiveness.
\newblock \emph{arXiv preprint arXiv:2401.08514}, 2024.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Liu, Zhang, Zhang, Cai, Bi, Du, Qin, Huang, Li, Shan, Zeng, Zhang, Liu, Li, Chang, Wang, Zhou, Liu, Luo, Wang, Jiang, Wu, Yang, Yang, Yang, Gong, Zhang, Shi, Dai, York, Liu, Zhu, Zhong, Lv, Cheng, Jia, Chen, Ke, E, Zhang, and Wang]{zhang2023dpa2}
Zhang, D., Liu, X., Zhang, X., Zhang, C., Cai, C., Bi, H., Du, Y., Qin, X., Huang, J., Li, B., Shan, Y., Zeng, J., Zhang, Y., Liu, S., Li, Y., Chang, J., Wang, X., Zhou, S., Liu, J., Luo, X., Wang, Z., Jiang, W., Wu, J., Yang, Y., Yang, J., Yang, M., Gong, F.-Q., Zhang, L., Shi, M., Dai, F.-Z., York, D.~M., Liu, S., Zhu, T., Zhong, Z., Lv, J., Cheng, J., Jia, W., Chen, M., Ke, G., E, W., Zhang, L., and Wang, H.
\newblock Dpa-2: Towards a universal large atomic model for molecular and material simulation.
\newblock \emph{arXiv preprint arXiv:2312.15492}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Liu, Tang, Dong, Yao, Zhang, Gu, Wang, Kharlamov, Shao, Li, and Wang]{OAG}
Zhang, F., Liu, X., Tang, J., Dong, Y., Yao, P., Zhang, J., Gu, X., Wang, Y., Kharlamov, E., Shao, B., Li, R., and Wang, K.
\newblock Oag: Linking entities across large-scale heterogeneous knowledge graphs.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering}, 35\penalty0 (9):\penalty0 9225--9239, 2023{\natexlab{b}}.
\newblock \doi{10.1109/TKDE.2022.3222168}.

\bibitem[Zhang \& Chen(2018)Zhang and Chen]{zhang2018link}
Zhang, M. and Chen, Y.
\newblock Link prediction based on graph neural networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Zhang et~al.(2021)Zhang, Li, Xia, Wang, and Jin]{zhang2021labeling}
Zhang, M., Li, P., Xia, Y., Wang, K., and Jin, L.
\newblock Labeling trick: A theory of using graph neural networks for multi-node representation learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 9061--9073, 2021.

\bibitem[Zhang et~al.(2023{\natexlab{c}})Zhang, Zhang, Yang, and Wang]{zhang2023and}
Zhang, Y., Zhang, F., Yang, Z., and Wang, Z.
\newblock What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization.
\newblock \emph{arXiv preprint arXiv:2305.19420}, 2023{\natexlab{c}}.

\bibitem[Zhang et~al.(2023{\natexlab{d}})Zhang, Li, Zhang, Qin, Wang, and Zhu]{zhang2023graph}
Zhang, Z., Li, H., Zhang, Z., Qin, Y., Wang, X., and Zhu, W.
\newblock Graph meets llms: Towards large graph models.
\newblock In \emph{NeurIPS 2023 Workshop: New Frontiers in Graph Learning}, 2023{\natexlab{d}}.

\bibitem[Zhang et~al.(2023{\natexlab{e}})Zhang, Luo, Lu, and He]{zhang2023live}
Zhang, Z., Luo, B., Lu, S., and He, B.
\newblock Live graph lab: Towards open, dynamic and real transaction graphs with nft.
\newblock \emph{arXiv preprint arXiv:2310.11709}, 2023{\natexlab{e}}.

\bibitem[Zhao et~al.(2024)Zhao, Liu, Chang, Xu, Fu, Deng, Kong, and Liu]{zhao2024gimlet}
Zhao, H., Liu, S., Chang, M., Xu, H., Fu, J., Deng, Z., Kong, L., and Liu, Q.
\newblock Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Zhao et~al.(2023{\natexlab{a}})Zhao, Zhuo, Shen, Qu, Liu, Bronstein, Zhu, and Tang]{zhao2023graphtext}
Zhao, J., Zhuo, L., Shen, Y., Qu, M., Liu, K., Bronstein, M., Zhu, Z., and Tang, J.
\newblock Graphtext: Graph reasoning in text space.
\newblock \emph{arXiv preprint arXiv:2310.01089}, 2023{\natexlab{a}}.

\bibitem[Zhao et~al.(2023{\natexlab{b}})Zhao, Ren, Li, Xu, and Liu]{zhao2023graphgpt}
Zhao, Q., Ren, W., Li, T., Xu, X., and Liu, H.
\newblock Graphgpt: Graph learning with generative pre-trained transformers.
\newblock \emph{arXiv preprint arXiv:2401.00529}, 2023{\natexlab{b}}.

\bibitem[Zheng et~al.(2023{\natexlab{a}})Zheng, He, Liu, Shi, Lu, Feng, Ju, Wang, Zhu, Min, Zhang, Tang, Hao, Jin, Chen, Noé, Liu, and Liu]{zheng2023predicting}
Zheng, S., He, J., Liu, C., Shi, Y., Lu, Z., Feng, W., Ju, F., Wang, J., Zhu, J., Min, Y., Zhang, H., Tang, S., Hao, H., Jin, P., Chen, C., Noé, F., Liu, H., and Liu, T.-Y.
\newblock Towards predicting equilibrium distributions for molecular systems with deep learning.
\newblock \emph{arXiv preprint arXiv:2306.05445}, 2023{\natexlab{a}}.

\bibitem[Zheng et~al.(2023{\natexlab{b}})Zheng, Huang, Rao, Wang, and Subbian]{zheng2023you}
Zheng, W., Huang, E.~W., Rao, N., Wang, Z., and Subbian, K.
\newblock You only transfer what you share: Intersection-induced graph transfer learning for link prediction.
\newblock \emph{arXiv preprint arXiv:2302.14189}, 2023{\natexlab{b}}.

\bibitem[Zhong et~al.(2021)Zhong, Shi, Yang, Xu, and Li]{zhong2021learningSCENEGRAPH}
Zhong, Y., Shi, J., Yang, J., Xu, C., and Li, Y.
\newblock Learning to generate scene graph from natural language supervision.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  1823--1834, 2021.

\bibitem[Zhu et~al.(2021{\natexlab{a}})Zhu, Xu, Liu, and Wu]{zhu2021empirical}
Zhu, Y., Xu, Y., Liu, Q., and Wu, S.
\newblock An empirical study of graph contrastive learning.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)}, 2021{\natexlab{a}}.

\bibitem[Zhu et~al.(2021{\natexlab{b}})Zhu, Zhang, Xhonneux, and Tang]{zhu2021neuralNBF}
Zhu, Z., Zhang, Z., Xhonneux, L.-P., and Tang, J.
\newblock Neural bellman-ford networks: A general graph neural network framework for link prediction.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 29476--29490, 2021{\natexlab{b}}.

\end{thebibliography}
