\begin{thebibliography}{76}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akin and Kolyada(2003)]{akin2003li}
Ethan Akin and Sergii Kolyada.
\newblock Li--yorke sensitivity.
\newblock \emph{Nonlinearity}, 16\penalty0 (4):\penalty0 1421, 2003.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{a}})Allen-Zhu, Li, and
  Liang]{allen2019learning}
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In \emph{Advances in neural information processing systems}, pages
  6155--6166, 2019{\natexlab{a}}.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{b}})Allen-Zhu, Li, and
  Song]{allen2019convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pages
  242--252, 2019{\natexlab{b}}.

\bibitem[Alligood et~al.(1997)Alligood, Sauer, and Yorke]{alligood1997chaos}
Kathleen~T Alligood, Tim~D Sauer, and James~A Yorke.
\newblock Chaos: An introduction to dynamical systems. 1996, 1997.

\bibitem[Arora et~al.(2018)Arora, Ge, Neyshabur, and Zhang]{arora2018stronger}
Sanjeev Arora, R~Ge, B~Neyshabur, and Y~Zhang.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock In \emph{35th International Conference on Machine Learning, ICML
  2018}, 2018.

\bibitem[Aulbach and Kieninger(2001)]{aulbach2001three}
Bernd Aulbach and Bernd Kieninger.
\newblock On three definitions of chaos.
\newblock \emph{Nonlinear Dyn. Syst. Theory}, 1\penalty0 (1):\penalty0 23--37,
  2001.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett2017spectrally}
Peter~L Bartlett, Dylan~J Foster, and Matus~J Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6240--6249, 2017.

\bibitem[Block and Coppel(2006)]{block2006dynamics}
Louis~S Block and William~A Coppel.
\newblock \emph{Dynamics in one dimension}.
\newblock Springer, 2006.

\bibitem[Borkar and Mitter(1999)]{borkar1999strong}
Vivek~S Borkar and Sanjoy~K Mitter.
\newblock A strong approximation theorem for stochastic recursive algorithms.
\newblock \emph{Journal of optimization theory and applications}, 100\penalty0
  (3):\penalty0 499--513, 1999.

\bibitem[Cao and Gu(2020)]{cao2020generalization}
Yuan Cao and Quanquan Gu.
\newblock Generalization error bounds of gradient descent for learning
  overparameterized deep {ReLU} networks.
\newblock \emph{AAAI}, 2020.

\bibitem[Chae et~al.(2017)Chae, Walker, et~al.]{chae2017novel}
Minwoo Chae, Stephen~G Walker, et~al.
\newblock A novel approach to bayesian consistency.
\newblock \emph{Electronic Journal of Statistics}, 11\penalty0 (2):\penalty0
  4723--4745, 2017.

\bibitem[Cvitanovic(2017)]{cvitanovic2017universality}
Predrag Cvitanovic.
\newblock \emph{Universality in chaos}.
\newblock Routledge, 2017.

\bibitem[Devaney(2018)]{devaney2018introduction}
Robert Devaney.
\newblock \emph{An introduction to chaotic dynamical systems}.
\newblock CRC Press, 2018.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh2017sharp}
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1019--1028. JMLR. org, 2017.

\bibitem[Draxler et~al.(2018)Draxler, Veschgini, Salmhofer, and
  Hamprecht]{draxler2018essentially}
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred~A Hamprecht.
\newblock Essentially no barriers in neural network energy landscape.
\newblock \emph{arXiv preprint arXiv:1803.00885}, 2018.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang, and
  Zhai]{du2019gradient2}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  1675--1685, 2019{\natexlab{a}}.

\bibitem[Du et~al.(2018)Du, Hu, and Lee]{du2018algorithmic}
Simon~S Du, Wei Hu, and Jason~D Lee.
\newblock Algorithmic regularization in learning deep homogeneous models:
  Layers are automatically balanced.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  384--395, 2018.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Zhai, Poczos, and
  Singh]{du2019gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{ICLR}, 2019{\natexlab{b}}.

\bibitem[Dua and Graff(2017)]{Dua:2019}
Dheeru Dua and Casey Graff.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Dziugaite and Roy(2017)]{dziugaite2017computing}
Gintare~Karolina Dziugaite and Daniel~M Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{Uncertainty inArtificial Intelligence}, 2017.

\bibitem[E et~al.(2020)E, Ma, and Wu]{ma2020comparative}
Weinan E, Chao Ma, and Lei Wu.
\newblock A comparative analysis of the optimization and generalization
  property of two-layer neural network and random feature models under gradient
  descent dynamics.
\newblock \emph{Science China Mathematics}, 2020.

\bibitem[Eckmann and Ruelle(1985)]{eckmann1985ergodic}
J-P Eckmann and David Ruelle.
\newblock Ergodic theory of chaos and strange attractors.
\newblock In \emph{The theory of chaotic attractors}, pages 273--312. Springer,
  1985.

\bibitem[Falniowski et~al.(2015)Falniowski, Kulczycki, Kwietniak, and
  Li]{falniowski2015two}
Fryderyk Falniowski, Marcin Kulczycki, Dominik Kwietniak, and Jian Li.
\newblock Two results on entropy, chaos and independence in symbolic dynamics.
\newblock \emph{Discrete \& Continuous Dynamical Systems-B}, 20\penalty0
  (10):\penalty0 3487, 2015.

\bibitem[Franca et~al.(2018)Franca, Robinson, and Vidal]{franca2018admm}
Guilherme Franca, Daniel Robinson, and Rene Vidal.
\newblock Admm and accelerated admm as continuous dynamical systems.
\newblock In \emph{International Conference on Machine Learning}, pages
  1559--1567, 2018.

\bibitem[Golowich et~al.(2018)Golowich, Rakhlin, and Shamir]{golowich2018size}
Noah Golowich, Alexander Rakhlin, and Ohad Shamir.
\newblock Size-independent sample complexity of neural networks.
\newblock In \emph{Conference On Learning Theory}, pages 297--299, 2018.

\bibitem[Hairer et~al.(2006)Hairer, Lubich, and Wanner]{hairer2006geometric}
Ernst Hairer, Christian Lubich, and Gerhard Wanner.
\newblock \emph{Geometric numerical integration: structure-preserving
  algorithms for ordinary differential equations}, volume~31.
\newblock Springer Science \& Business Media, 2006.

\bibitem[Hennion and Herv{\'e}(2004)]{hennion2004central}
Hubert Hennion and Lo{\"\i}c Herv{\'e}.
\newblock Central limit theorems for iterated random lipschitz mappings.
\newblock \emph{The Annals of Probability}, 32\penalty0 (3):\penalty0
  1934--1984, 2004.

\bibitem[Iwanik(1991)]{iwanik1991independence}
Anzelm Iwanik.
\newblock Independence and scrambled sets for chaotic mappings.
\newblock In \emph{The mathematical heritage of CF Gauss}, pages 372--378.
  World Scientific, 1991.

\bibitem[Jastrz{\k{e}}bski et~al.(2017)Jastrz{\k{e}}bski, Kenton, Arpit,
  Ballas, Fischer, Bengio, and Storkey]{jastrzkebski2017three}
Stanis{\l}aw Jastrz{\k{e}}bski, Zachary Kenton, Devansh Arpit, Nicolas Ballas,
  Asja Fischer, Yoshua Bengio, and Amos Storkey.
\newblock Three factors influencing minima in sgd.
\newblock \emph{arXiv preprint arXiv:1711.04623}, 2017.

\bibitem[Jin et~al.(2017)Jin, Ge, Netrapalli, Kakade, and
  Jordan]{jin2017escape}
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham~M Kakade, and Michael~I Jordan.
\newblock How to escape saddle points efficiently.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1724--1732. JMLR. org, 2017.

\bibitem[Jin et~al.(2018)Jin, Liu, Ge, and Jordan]{jin2018local}
Chi Jin, Lydia~T Liu, Rong Ge, and Michael~I Jordan.
\newblock On the local minima of the empirical risk.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4896--4905, 2018.

\bibitem[Kovachki and Stuart(2019)]{kovachki2019analysis}
Nikola~B Kovachki and Andrew~M Stuart.
\newblock Analysis of momentum methods.
\newblock \emph{arXiv preprint arXiv:1906.04285}, 2019.

\bibitem[Lee et~al.(2016)Lee, Simchowitz, Jordan, and Recht]{lee2016gradient}
Jason~D Lee, Max Simchowitz, Michael~I Jordan, and Benjamin Recht.
\newblock Gradient descent only converges to minimizers.
\newblock In \emph{Conference on learning theory}, pages 1246--1257, 2016.

\bibitem[Lewkowycz et~al.(2020)Lewkowycz, Bahri, Dyer, Sohl-Dickstein, and
  Gur-Ari]{lewkowycz2020large}
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy
  Gur-Ari.
\newblock The large learning rate phase of deep learning: the catapult
  mechanism.
\newblock \emph{arXiv preprint arXiv:2003.02218}, 2020.

\bibitem[Li et~al.(2017)Li, Tai, and Weinan]{li2017stochastic}
Qianxiao Li, Cheng Tai, and E~Weinan.
\newblock Stochastic modified equations and adaptive stochastic gradient
  algorithms.
\newblock In \emph{International Conference on Machine Learning}, pages
  2101--2110, 2017.

\bibitem[Li et~al.(2019{\natexlab{a}})Li, Tai, and Weinan]{li2019stochastic}
Qianxiao Li, Cheng Tai, and E~Weinan.
\newblock Stochastic modified equations and dynamics of stochastic gradient
  algorithms i: Mathematical foundations.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (40):\penalty0 1--40, 2019{\natexlab{a}}.

\bibitem[Li(1993)]{li1993}
Shi~Hai Li.
\newblock $\omega$-chaos and topological entropy.
\newblock \emph{Transactions of the American Mathematical Society},
  339\penalty0 (1):\penalty0 243--249, 1993.

\bibitem[Li and Yorke(1975)]{li1975period}
Tien-Yien Li and James~A Yorke.
\newblock Period three implies chaos.
\newblock \emph{The American Mathematical Monthly}, 82\penalty0 (10):\penalty0
  985--992, 1975.

\bibitem[Li et~al.(2018)Li, Lu, Wang, Haupt, and Zhao]{li2018tighter}
Xingguo Li, Junwei Lu, Zhaoran Wang, Jarvis Haupt, and Tuo Zhao.
\newblock On tighter generalization bound for deep neural networks: Cnns,
  resnets, and beyond.
\newblock \emph{arXiv preprint arXiv:1806.05159}, 2018.

\bibitem[Li and Liang(2018)]{li2018learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8157--8166, 2018.

\bibitem[Li et~al.(2019{\natexlab{b}})Li, Wei, and Ma]{li2019towards}
Yuanzhi Li, Colin Wei, and Tengyu Ma.
\newblock Towards explaining the regularization effect of initial large
  learning rate in training neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  11669--11680, 2019{\natexlab{b}}.

\bibitem[Liu et~al.(2017)Liu, Shang, Cheng, Cheng, and
  Jiao]{liu2017accelerated}
Yuanyuan Liu, Fanhua Shang, James Cheng, Hong Cheng, and Licheng Jiao.
\newblock Accelerated first-order methods for geodesically convex optimization
  on {R}iemannian manifolds.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4868--4877, 2017.

\bibitem[Lyapunov(1992)]{lyapunov1992general}
Aleksandr~Mikhailovich Lyapunov.
\newblock The general problem of the stability of motion.
\newblock \emph{International journal of control}, 55\penalty0 (3):\penalty0
  531--534, 1992.

\bibitem[Ma et~al.(2019)Ma, Chatterji, Cheng, Flammarion, Bartlett, and
  Jordan]{ma2019there}
Yi-An Ma, Niladri Chatterji, Xiang Cheng, Nicolas Flammarion, Peter Bartlett,
  and Michael~I Jordan.
\newblock Is there an analog of {N}esterov acceleration for {MCMC}?
\newblock \emph{arXiv preprint arXiv:1902.00996}, 2019.

\bibitem[Mandt et~al.(2017)Mandt, Hoffman, and Blei]{mandt2017stochastic}
Stephan Mandt, Matthew~D Hoffman, and David~M Blei.
\newblock Stochastic gradient descent as approximate bayesian inference.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 4873--4907, 2017.

\bibitem[Mei et~al.(2018)Mei, Bai, and Montanari]{mei2018}
Song Mei, Yu~Bai, and Andrea Montanari.
\newblock The landscape of empirical risk for nonconvex losses.
\newblock \emph{Ann. Statist.}, 46\penalty0 (6A):\penalty0 2747--2774, 12 2018.
\newblock \doi{10.1214/17-AOS1637}.
\newblock URL \url{https://doi.org/10.1214/17-AOS1637}.

\bibitem[Misiurewicz(2010)]{misiurewicz2010horseshoes}
Micha{\l} Misiurewicz.
\newblock Horseshoes for continuous mappings of an interval.
\newblock In \emph{Dynamical systems}, pages 125--135. Springer, 2010.

\bibitem[Moser(1973)]{moser1973stable}
J{\"u}rgen Moser.
\newblock \emph{Stable and random motions in dynamical systems: With special
  emphasis on celestial mechanics}, volume~1.
\newblock Princeton University Press, 1973.

\bibitem[Moulines and Bach(2011)]{moulines2011non}
Eric Moulines and Francis~R Bach.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  451--459, 2011.

\bibitem[Nesterov(2013)]{nesterov2013introductory}
Yurii Nesterov.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Neyshabur and Li(2019)]{neyshabur2019towards}
Behnam Neyshabur and Zhiyuan Li.
\newblock Towards understanding the role of over-parametrization in
  generalization of neural networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and
  Srebro]{neyshabur2015norm}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock Norm-based capacity control in neural networks.
\newblock In \emph{Conference on Learning Theory}, pages 1376--1401, 2015.

\bibitem[Ornstein and Weiss(1973)]{ornstein1973geodesic}
Donald Ornstein and Benjamin Weiss.
\newblock Geodesic flows are bernoullian.
\newblock \emph{Israel Journal of Mathematics}, 14\penalty0 (2):\penalty0
  184--198, 1973.

\bibitem[Oseledec(1968)]{oseledec1968multiplicative}
Valery~Iustinovich Oseledec.
\newblock A multiplicative ergodic theorem. liapunov characteristic number for
  dynamical systems.
\newblock \emph{Trans. Moscow Math. Soc.}, 19:\penalty0 197--231, 1968.

\bibitem[Ott(2002)]{ott2002chaos}
Edward Ott.
\newblock \emph{Chaos in dynamical systems}.
\newblock Cambridge university press, 2002.

\bibitem[Pavliotis and Stuart(2008)]{pavliotis2008multiscale}
Grigoris Pavliotis and Andrew Stuart.
\newblock \emph{Multiscale methods: averaging and homogenization}, volume~53.
\newblock Springer, 2008.

\bibitem[Polyak(1964)]{polyak1964some}
Boris~T Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  4\penalty0 (5):\penalty0 1--17, 1964.

\bibitem[Polyak and Juditsky(1992)]{polyak1992acceleration}
Boris~T Polyak and Anatoli~B Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock \emph{SIAM journal on control and optimization}, 30\penalty0
  (4):\penalty0 838--855, 1992.

\bibitem[Robbins and Monro(1951)]{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, pages 400--407, 1951.

\bibitem[Roux et~al.(2012)Roux, Schmidt, and Bach]{roux2012stochastic}
Nicolas~L Roux, Mark Schmidt, and Francis~R Bach.
\newblock A stochastic gradient method with an exponential convergence \_rate
  for finite training sets.
\newblock In \emph{Advances in neural information processing systems}, pages
  2663--2671, 2012.

\bibitem[Sanders et~al.(2010)Sanders, Verhulst, and Murdock]{SaVeMu10}
J.~A. Sanders, F.~Verhulst, and J.~Murdock.
\newblock \emph{Averaging Methods in Nonlinear Dynamical Systems}.
\newblock Springer, 2010.

\bibitem[Sharkovski{\u\i}(Original 1962; Translated
  1995)]{sharkovskiui1995coexistence}
AN~Sharkovski{\u\i}.
\newblock Coexistence of cycles of a continuous map of the line into itself.
\newblock \emph{International Journal of Bifurcation and Chaos}, 5\penalty0
  (05):\penalty0 1263--1273, Original 1962; Translated 1995.

\bibitem[Shi et~al.(2018)Shi, Du, Jordan, and Su]{shi2018understanding}
Bin Shi, Simon~S Du, Michael~I Jordan, and Weijie~J Su.
\newblock Understanding the acceleration phenomenon via high-resolution
  differential equations.
\newblock \emph{arXiv preprint arXiv:1810.08907}, 2018.

\bibitem[Sinai(1970)]{sinai1970dynamical}
Yakov~G Sinai.
\newblock Dynamical systems with elastic reflections.
\newblock \emph{Russian Mathematical Surveys}, 25\penalty0 (2):\penalty0 137,
  1970.

\bibitem[Sitzmann et~al.(2020)Sitzmann, Martel, Bergman, Lindell, and
  Wetzstein]{sitzmann2020implicit}
Vincent Sitzmann, Julien~NP Martel, Alexander~W Bergman, David~B Lindell, and
  Gordon Wetzstein.
\newblock Implicit neural representations with periodic activation functions.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Smith(2017)]{smith2017cyclical}
Leslie~N Smith.
\newblock Cyclical learning rates for training neural networks.
\newblock In \emph{2017 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pages 464--472. IEEE, 2017.

\bibitem[Smith and Topin(2019)]{smith2019super}
Leslie~N Smith and Nicholay Topin.
\newblock Super-convergence: Very fast training of neural networks using large
  learning rates.
\newblock In \emph{Artificial Intelligence and Machine Learning for
  Multi-Domain Operations Applications}, volume 11006, page 1100612.
  International Society for Optics and Photonics, 2019.

\bibitem[Strogatz(2018)]{strogatz2018nonlinear}
Steven~H Strogatz.
\newblock \emph{Nonlinear dynamics and chaos: with applications to physics,
  biology, chemistry, and engineering}.
\newblock CRC Press, 2018.

\bibitem[Su et~al.(2014)Su, Boyd, and Candes]{su2014differential}
Weijie Su, Stephen Boyd, and Emmanuel Candes.
\newblock A differential equation for modeling {N}esterov’s accelerated
  gradient method: Theory and insights.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2510--2518, 2014.

\bibitem[Tao and Ohsawa(2020)]{tao2020Lie}
Molei Tao and Tomoki Ohsawa.
\newblock Variational optimization on {L}ie groups, with examples of leading
  (generalized) eigenvalue problems.
\newblock \emph{International Conference on Artificial Intelligence and
  Statistics}, 2020.

\bibitem[Tu et~al.(2015)Tu, Boczar, Simchowitz, Soltanolkotabi, and
  Recht]{tu2015low}
Stephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Benjamin
  Recht.
\newblock Low-rank solutions of linear matrix equations via procrustes flow.
\newblock \emph{arXiv preprint arXiv:1507.03566}, 2015.

\bibitem[Wei et~al.(2019)Wei, Lee, Liu, and Ma]{wei2019regularization}
Colin Wei, Jason~D Lee, Qiang Liu, and Tengyu Ma.
\newblock Regularization matters: Generalization and optimization of neural
  nets vs their induced kernel.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9709--9721, 2019.

\bibitem[Wibisono et~al.(2016)Wibisono, Wilson, and Jordan]{wibisonoe7351}
Andre Wibisono, Ashia~C. Wilson, and Michael~I. Jordan.
\newblock A variational perspective on accelerated methods in optimization.
\newblock \emph{Proceedings of the National Academy of Sciences}, 113\penalty0
  (47):\penalty0 E7351--E7358, 2016.
\newblock ISSN 0027-8424.

\bibitem[Yi et~al.(2016)Yi, Park, Chen, and Caramanis]{yi2016fast}
Xinyang Yi, Dohyung Park, Yudong Chen, and Constantine Caramanis.
\newblock Fast algorithms for robust pca via gradient descent.
\newblock In \emph{Advances in neural information processing systems}, pages
  4152--4160, 2016.

\bibitem[Young(1998)]{young1998statistical}
Lai-Sang Young.
\newblock Statistical properties of dynamical systems with some hyperbolicity.
\newblock \emph{Annals of Mathematics}, 147:\penalty0 585--650, 1998.

\bibitem[Zhang(2004)]{zhang2004solving}
Tong Zhang.
\newblock Solving large scale linear prediction problems using stochastic
  gradient descent algorithms.
\newblock In \emph{Proceedings of the twenty-first international conference on
  Machine learning}, page 116. ACM, 2004.

\end{thebibliography}
