\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[mis(2024)]{mistraldata}
Long-data collections, 2024.
\newblock URL
  \url{https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T}.

\bibitem[Amazon(2023)]{mistrallite}
Amazon.
\newblock Mistrallite, 2023.
\newblock URL \url{https://huggingface.co/amazon/MistralLite}.

\bibitem[Azerbayev et~al.(2022)Azerbayev, Ayers, and Piotrowski]{proof-pile}
Azerbayev, Z., Ayers, E., and Piotrowski, B.
\newblock Proof-pile, 2022.
\newblock URL \url{https://github.com/zhangir-azerbayev/ProofNet}.

\bibitem[Borgeaud et~al.(2022)Borgeaud, Mensch, Hoffmann, Cai, Rutherford,
  Millican, Van Den~Driessche, Lespiau, Damoc, Clark,
  et~al.]{borgeaud2022improving}
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K.,
  Van Den~Driessche, G.~B., Lespiau, J.-B., Damoc, B., Clark, A., et~al.
\newblock Improving language models by retrieving from trillions of tokens.
\newblock In \emph{International conference on machine learning}, pp.\
  2206--2240. PMLR, 2022.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Wong, Chen, and Tian]{pi}
Chen, S., Wong, S., Chen, L., and Tian, Y.
\newblock Extending context window of large language models via positional
  interpolation.
\newblock \emph{arXiv preprint arXiv:2306.15595}, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Qian, Tang, Lai, Liu, Han, and
  Jia]{longlora}
Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J.
\newblock Longlora: Efficient fine-tuning of long-context large language
  models.
\newblock \emph{arXiv:2309.12307}, 2023{\natexlab{b}}.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,
  and Tafjord]{allenai:arc}
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and
  Tafjord, O.
\newblock Think you have solved question answering? try arc, the ai2 reasoning
  challenge.
\newblock \emph{arXiv:1803.05457v1}, 2018.

\bibitem[Computer(2023)]{redpajama}
Computer, T.
\newblock Redpajama: An open source recipe to reproduce llama training dataset,
  2023.
\newblock URL \url{https://github.com/togethercomputer/RedPajama-Data}.

\bibitem[Dao(2023)]{dao2023flashattention2}
Dao, T.
\newblock Flash{A}ttention-2: Faster attention with better parallelism and work
  partitioning.
\newblock 2023.

\bibitem[Face(2024)]{huggingfaceboard}
Face, H.
\newblock Open llm leaderboard, 2024.
\newblock URL
  \url{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,
  He, Thite, Nabeshima, Presser, and Leahy]{pile}
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang,
  J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C.
\newblock The {P}ile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[Guo et~al.(2020)Guo, Zhang, Mu, Heng, Liu, Wei, and
  Sun]{guo2020single}
Guo, Z., Zhang, X., Mu, H., Heng, W., Liu, Z., Wei, Y., and Sun, J.
\newblock Single path one-shot neural architecture search with uniform
  sampling.
\newblock In \emph{Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part XVI 16}, pp.\  544--560.
  Springer, 2020.

\bibitem[Han et~al.(2023)Han, Wang, Xiong, Chen, Ji, and
  Wang]{han2023lminfinite}
Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang, S.
\newblock Lm-infinite: Simple on-the-fly length generalization for large
  language models.
\newblock \emph{arXiv preprint arXiv:2308.16137}, 2023.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt]{mmlu}
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and
  Steinhardt, J.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Huang et~al.(2023)Huang, Zhang, Cheng, Yang, and
  Yang]{Huang2023FewerIM}
Huang, X., Zhang, L.~L., Cheng, K.-T., Yang, F., and Yang, M.
\newblock Fewer is more: Boosting llm reasoning with reinforced context
  pruning.
\newblock 2023.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:266210460}.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot,
  de~las Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock,
  Scao, Lavril, Wang, Lacroix, and Sayed]{mistral}
Jiang, A.~Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.~S., de~las
  Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud,
  L.~R., Lachaux, M.-A., Stock, P., Scao, T.~L., Lavril, T., Wang, T., Lacroix,
  T., and Sayed, W.~E.
\newblock Mistral 7b, 2023.

\bibitem[Lin et~al.(2021)Lin, Hilton, and Evans]{lin2021truthfulqa}
Lin, S., Hilton, J., and Evans, O.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock \emph{arXiv preprint arXiv:2109.07958}, 2021.

\bibitem[Lin et~al.(2023)Lin, Miao, Liu, Shi, Zhang, Yang, Maleki, Zhu, Cao,
  Li, et~al.]{cube}
Lin, Z., Miao, Y., Liu, G., Shi, X., Zhang, Q., Yang, F., Maleki, S., Zhu, Y.,
  Cao, X., Li, C., et~al.
\newblock Superscaler: Supporting flexible dnn parallelization via a unified
  abstraction.
\newblock \emph{arXiv preprint arXiv:2301.08984}, 2023.

\bibitem[Liu et~al.(2023)Liu, Yan, Zhang, An, Qiu, and Lin]{liu2023scaling}
Liu, X., Yan, H., Zhang, S., An, C., Qiu, X., and Lin, D.
\newblock Scaling laws of rope-based extrapolation.
\newblock \emph{arXiv preprint arXiv:2310.05209}, 2023.

\bibitem[LocalLLaMA(2023{\natexlab{a}})]{dynamicntk}
LocalLLaMA.
\newblock Dynamically scaled rope further increases performance of long context
  llama with zero fine-tuning, 2023{\natexlab{a}}.
\newblock URL
  \url{{https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/}}.

\bibitem[LocalLLaMA(2023{\natexlab{b}})]{ntk}
LocalLLaMA.
\newblock Ntk-aware scaled rope allows llama models to have extended (8k+)
  context size without any fine-tuning and minimal perplexity degration,
  2023{\natexlab{b}}.
\newblock URL
  \url{{https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/}}.

\bibitem[Madaan et~al.(2023)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe,
  Alon, Dziri, Prabhumoye, Yang, Gupta, Majumder, Hermann, Welleck,
  Yazdanbakhsh, and Clark]{madaan2023selfrefine}
Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon,
  U., Dziri, N., Prabhumoye, S., Yang, Y., Gupta, S., Majumder, B.~P., Hermann,
  K., Welleck, S., Yazdanbakhsh, A., and Clark, P.
\newblock Self-refine: Iterative refinement with self-feedback, 2023.

\bibitem[Mohtashami \& Jaggi(2023)Mohtashami and Jaggi]{passkey}
Mohtashami, A. and Jaggi, M.
\newblock Landmark attention: Random-access infinite context length for
  transformers.
\newblock \emph{arXiv preprint arXiv:2305.16300}, 2023.

\bibitem[OpenAI et~al.(2023)OpenAI, :, Achiam, Adler, Agarwal, Ahmad, Akkaya,
  Aleman, Almeida, Altenschmidt, Altman, Anadkat, Avila, Babuschkin, Balaji,
  Balcom, Baltescu, Bao, Bavarian, Belgum, Bello, Berdine, Bernadett-Shapiro,
  Berner, Bogdonoff, Boiko, Boyd, Brakman, Brockman, Brooks, Brundage, Button,
  Cai, Campbell, Cann, Carey, Carlson, Carmichael, Chan, Chang, Chantzis, Chen,
  Chen, Chen, Chen, Chen, Chess, Cho, Chu, Chung, Cummings, Currier, Dai,
  Decareaux, Degry, Deutsch, Deville, Dhar, Dohan, Dowling, Dunning, Ecoffet,
  Eleti, Eloundou, Farhi, Fedus, Felix, Fishman, Forte, Fulford, Gao, Georges,
  Gibson, Goel, Gogineni, Goh, Gontijo-Lopes, Gordon, Grafstein, Gray, Greene,
  Gross, Gu, Guo, Hallacy, Han, Harris, He, Heaton, Heidecke, Hesse, Hickey,
  Hickey, Hoeschele, Houghton, Hsu, Hu, Hu, Huizinga, Jain, Jain, Jang, Jiang,
  Jiang, Jin, Jin, Jomoto, Jonn, Jun, Kaftan, Łukasz Kaiser, Kamali,
  Kanitscheider, Keskar, Khan, Kilpatrick, Kim, Kim, Kim, Kirchner, Kiros,
  Knight, Kokotajlo, Łukasz Kondraciuk, Kondrich, Konstantinidis, Kosic,
  Krueger, Kuo, Lampe, Lan, Lee, Leike, Leung, Levy, Li, Lim, Lin, Lin, Litwin,
  Lopez, Lowe, Lue, Makanju, Malfacini, Manning, Markov, Markovski, Martin,
  Mayer, Mayne, McGrew, McKinney, McLeavey, McMillan, McNeil, Medina, Mehta,
  Menick, Metz, Mishchenko, Mishkin, Monaco, Morikawa, Mossing, Mu, Murati,
  Murk, Mély, Nair, Nakano, Nayak, Neelakantan, Ngo, Noh, Ouyang, O'Keefe,
  Pachocki, Paino, Palermo, Pantuliano, Parascandolo, Parish, Parparita,
  Passos, Pavlov, Peng, Perelman, de~Avila Belbute~Peres, Petrov,
  de~Oliveira~Pinto, Michael, Pokorny, Pokrass, Pong, Powell, Power, Power,
  Proehl, Puri, Radford, Rae, Ramesh, Raymond, Real, Rimbach, Ross, Rotsted,
  Roussez, Ryder, Saltarelli, Sanders, Santurkar, Sastry, Schmidt, Schnurr,
  Schulman, Selsam, Sheppard, Sherbakov, Shieh, Shoker, Shyam, Sidor, Sigler,
  Simens, Sitkin, Slama, Sohl, Sokolowsky, Song, Staudacher, Such, Summers,
  Sutskever, Tang, Tezak, Thompson, Tillet, Tootoonchian, Tseng, Tuggle,
  Turley, Tworek, Uribe, Vallone, Vijayvergiya, Voss, Wainwright, Wang, Wang,
  Wang, Ward, Wei, Weinmann, Welihinda, Welinder, Weng, Weng, Wiethoff,
  Willner, Winter, Wolrich, Wong, Workman, Wu, Wu, Wu, Xiao, Xu, Yoo, Yu, Yuan,
  Zaremba, Zellers, Zhang, Zhang, Zhao, Zheng, Zhuang, Zhuk, and Zoph]{gpt4}
OpenAI, :, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman,
  F.~L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R.,
  Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M.,
  Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G., Berner, C.,
  Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.-L., Brockman, G., Brooks, T.,
  Brundage, M., Button, K., Cai, T., Campbell, R., Cann, A., Carey, B.,
  Carlson, C., Carmichael, R., Chan, B., Chang, C., Chantzis, F., Chen, D.,
  Chen, S., Chen, R., Chen, J., Chen, M., Chess, B., Cho, C., Chu, C., Chung,
  H.~W., Cummings, D., Currier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch,
  N., Deville, D., Dhar, A., Dohan, D., Dowling, S., Dunning, S., Ecoffet, A.,
  Eleti, A., Eloundou, T., Farhi, D., Fedus, L., Felix, N., Fishman, S.~P.,
  Forte, J., Fulford, I., Gao, L., Georges, E., Gibson, C., Goel, V., Gogineni,
  T., Goh, G., Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S., Greene,
  R., Gross, J., Gu, S.~S., Guo, Y., Hallacy, C., Han, J., Harris, J., He, Y.,
  Heaton, M., Heidecke, J., Hesse, C., Hickey, A., Hickey, W., Hoeschele, P.,
  Houghton, B., Hsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S.,
  Jang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S., Jonn, B., Jun,
  H., Kaftan, T., Łukasz Kaiser, Kamali, A., Kanitscheider, I., Keskar, N.~S.,
  Khan, T., Kilpatrick, L., Kim, J.~W., Kim, C., Kim, Y., Kirchner, H., Kiros,
  J., Knight, M., Kokotajlo, D., Łukasz Kondraciuk, Kondrich, A.,
  Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V., Lampe, M., Lan, I., Lee,
  T., Leike, J., Leung, J., Levy, D., Li, C.~M., Lim, R., Lin, M., Lin, S.,
  Litwin, M., Lopez, T., Lowe, R., Lue, P., Makanju, A., Malfacini, K.,
  Manning, S., Markov, T., Markovski, Y., Martin, B., Mayer, K., Mayne, A.,
  McGrew, B., McKinney, S.~M., McLeavey, C., McMillan, P., McNeil, J., Medina,
  D., Mehta, A., Menick, J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V.,
  Morikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O., Mély, D., Nair, A.,
  Nakano, R., Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Ouyang, L.,
  O'Keefe, C., Pachocki, J., Paino, A., Palermo, J., Pantuliano, A.,
  Parascandolo, G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng,
  A., Perelman, A., de~Avila Belbute~Peres, F., Petrov, M., de~Oliveira~Pinto,
  H.~P., Michael, Pokorny, Pokrass, M., Pong, V., Powell, T., Power, A., Power,
  B., Proehl, E., Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C.,
  Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H., Ryder, N.,
  Saltarelli, M., Sanders, T., Santurkar, S., Sastry, G., Schmidt, H., Schnurr,
  D., Schulman, J., Selsam, D., Sheppard, K., Sherbakov, T., Shieh, J., Shoker,
  S., Shyam, P., Sidor, S., Sigler, E., Simens, M., Sitkin, J., Slama, K.,
  Sohl, I., Sokolowsky, B., Song, Y., Staudacher, N., Such, F.~P., Summers, N.,
  Sutskever, I., Tang, J., Tezak, N., Thompson, M., Tillet, P., Tootoonchian,
  A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J. F.~C., Vallone,
  A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang, J.~J., Wang, A., Wang,
  B., Ward, J., Wei, J., Weinmann, C., Welihinda, A., Welinder, P., Weng, J.,
  Weng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich, S., Wong, H.,
  Workman, L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan,
  Q., Zaremba, W., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng, T.,
  Zhuang, J., Zhuk, W., and Zoph, B.
\newblock Gpt-4 technical report, 2023.

\bibitem[Park et~al.(2023)Park, O'Brien, Cai, Morris, Liang, and
  Bernstein]{park2023generative}
Park, J.~S., O'Brien, J., Cai, C.~J., Morris, M.~R., Liang, P., and Bernstein,
  M.~S.
\newblock Generative agents: Interactive simulacra of human behavior.
\newblock In \emph{Proceedings of the 36th Annual ACM Symposium on User
  Interface Software and Technology}, pp.\  1--22, 2023.

\bibitem[Peng et~al.(2023)Peng, Quesnelle, Fan, and Shippole]{yarn}
Peng, B., Quesnelle, J., Fan, H., and Shippole, E.
\newblock Yarn: Efficient context window extension of large language models.
\newblock \emph{arXiv preprint arXiv:2309.00071}, 2023.

\bibitem[Rae et~al.(2019)Rae, Potapenko, Jayakumar, Hillier, and
  Lillicrap]{pg19}
Rae, J.~W., Potapenko, A., Jayakumar, S.~M., Hillier, C., and Lillicrap, T.~P.
\newblock Compressive transformers for long-range sequence modelling.
\newblock \emph{arXiv preprint}, 2019.
\newblock URL \url{https://arxiv.org/abs/1911.05507}.

\bibitem[Ratner et~al.(2022)Ratner, Levine, Belinkov, Ram, Abend, Karpas,
  Shashua, Leyton-Brown, and Shoham]{parallelwindow}
Ratner, N., Levine, Y., Belinkov, Y., Ram, O., Abend, O., Karpas, E., Shashua,
  A., Leyton-Brown, K., and Shoham, Y.
\newblock Parallel context windows improve in-context learning of large
  language models.
\newblock \emph{arXiv preprint arXiv:2212.10947}, 2022.

\bibitem[Rozière et~al.(2023)Rozière, Gehring, Gloeckle, Sootla, Gat, Tan,
  Adi, Liu, Remez, Rapin, Kozhevnikov, Evtimov, Bitton, Bhatt, Ferrer,
  Grattafiori, Xiong, Défossez, Copet, Azhar, Touvron, Martin, Usunier,
  Scialom, and Synnaeve]{codellama}
Rozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X.~E., Adi,
  Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J.,
  Bhatt, M., Ferrer, C.~C., Grattafiori, A., Xiong, W., Défossez, A., Copet,
  J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and
  Synnaeve, G.
\newblock Code llama: Open foundation models for code, 2023.

\bibitem[Su et~al.(2021)Su, Lu, Pan, Murtadha, Wen, and Liu]{rope}
Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{arXiv preprint arXiv:2104.09864}, 2021.

\bibitem[Tancik et~al.(2020)Tancik, Srinivasan, Mildenhall, Fridovich-Keil,
  Raghavan, Singhal, Ramamoorthi, Barron, and Ng]{tancik2020fourier}
Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N.,
  Singhal, U., Ramamoorthi, R., Barron, J., and Ng, R.
\newblock Fourier features let networks learn high frequency functions in low
  dimensional domains.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7537--7547, 2020.

\bibitem[Together(2023)]{together}
Together, 2023.
\newblock URL \url{https://huggingface.co/togethercomputer/LLaMA-2-7B-32K}.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei,
  Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull,
  Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini,
  Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril,
  Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton,
  Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang,
  Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang,
  Rodriguez, Stojnic, Edunov, and Scialom]{llama2}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,
  Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L.,
  Ferrer, C.~C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu,
  W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S.,
  Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev,
  A., Koura, P.~S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,
  Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y.,
  Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva,
  R., Smith, E.~M., Subramanian, R., Tan, X.~E., Tang, B., Taylor, R.,
  Williams, A., Kuan, J.~X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A.,
  Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and
  Scialom, T.
\newblock Llama 2: Open foundation and fine-tuned chat models, 2023.

\bibitem[Tworkowski et~al.(2023)Tworkowski, Staniszewski, Pacek, Wu,
  Michalewski, and Miłoś]{longllama}
Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., and
  Miłoś, P.
\newblock Focused transformer: Contrastive training for context scaling.
\newblock 2023.

\bibitem[Wang et~al.(2023)Wang, Dong, Cheng, Liu, Yan, Gao, and
  Wei]{wang2023augmenting}
Wang, W., Dong, L., Cheng, H., Liu, X., Yan, X., Gao, J., and Wei, F.
\newblock Augmenting language models with long-term memory.
\newblock \emph{arXiv preprint arXiv:2306.07174}, 2023.

\bibitem[Xiao et~al.(2023)Xiao, Tian, Chen, Han, and Lewis]{streamingllm}
Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M.
\newblock Efficient streaming language models with attention sinks.
\newblock \emph{arXiv}, 2023.

\bibitem[Xiong et~al.(2023)Xiong, Liu, Molybog, Zhang, Bhargava, Hou, Martin,
  Rungta, Sankararaman, Oguz, Khabsa, Fang, Mehdad, Narang, Malik, Fan,
  Bhosale, Edunov, Lewis, Wang, and Ma]{xiong2023effective}
Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L.,
  Rungta, R., Sankararaman, K.~A., Oguz, B., Khabsa, M., Fang, H., Mehdad, Y.,
  Narang, S., Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis, M., Wang, S.,
  and Ma, H.
\newblock Effective long-context scaling of foundation models, 2023.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and
  Choi]{zellers2019hellaswag}
Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, 2019.

\bibitem[Zhang et~al.(2024)Zhang, Liu, Xiao, Shao, Ye, and
  Dou]{zhang2024soaring}
Zhang, P., Liu, Z., Xiao, S., Shao, N., Ye, Q., and Dou, Z.
\newblock Soaring from 4k to 400k: Extending llm's context with activation
  beacon.
\newblock \emph{arXiv preprint arXiv:2401.03462}, 2024.

\bibitem[Zhu et~al.(2023)Zhu, Yang, Wang, Song, Wu, Wei, and Li]{zhu2023pose}
Zhu, D., Yang, N., Wang, L., Song, Y., Wu, W., Wei, F., and Li, S.
\newblock Pose: Efficient context window extension of llms via positional
  skip-wise training, 2023.

\end{thebibliography}
