\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2021)Agarwal, Schwarzer, Castro, Courville, and
  Bellemare]{agarwal2021deep}
Agarwal, R., Schwarzer, M., Castro, P.~S., Courville, A., and Bellemare, M.~G.
\newblock Deep reinforcement learning at the edge of the statistical precipice.
\newblock \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Argenson \& Dulac-Arnold(2021)Argenson and
  Dulac-Arnold]{Argenson2021ModelBasedOP}
Argenson, A. and Dulac-Arnold, G.
\newblock Model-based offline planning.
\newblock \emph{ArXiv}, abs/2008.05556, 2021.

\bibitem[Bhardwaj et~al.(2020)Bhardwaj, Handa, Fox, and
  Boots]{Bhardwaj2020InformationTM}
Bhardwaj, M., Handa, A., Fox, D., and Boots, B.
\newblock Information theoretic model predictive q-learning.
\newblock \emph{ArXiv}, abs/2001.02153, 2020.

\bibitem[Bhardwaj et~al.(2021)Bhardwaj, Choudhury, and
  Boots]{Bhardwaj2021BlendingM}
Bhardwaj, M., Choudhury, S., and Boots, B.
\newblock Blending mpc \& value function approximation for efficient
  reinforcement learning.
\newblock \emph{ArXiv}, abs/2012.05909, 2021.

\bibitem[Buckman et~al.(2018)Buckman, Hafner, Tucker, Brevdo, and
  Lee]{Buckman2018SampleEfficientRL}
Buckman, J., Hafner, D., Tucker, G., Brevdo, E., and Lee, H.
\newblock Sample-efficient reinforcement learning with stochastic ensemble
  value expansion.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Chen \& He(2021)Chen and He]{Chen2021ExploringSS}
Chen, X. and He, K.
\newblock Exploring simple siamese representation learning.
\newblock \emph{2021 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  15745--15753, 2021.

\bibitem[Chua et~al.(2018)Chua, Calandra, McAllister, and
  Levine]{Chua2018DeepRL}
Chua, K., Calandra, R., McAllister, R., and Levine, S.
\newblock Deep reinforcement learning in a handful of trials using
  probabilistic dynamics models.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Clavera et~al.(2020)Clavera, Fu, and
  Abbeel]{Clavera2020ModelAugmentedAB}
Clavera, I., Fu, Y., and Abbeel, P.
\newblock Model-augmented actor-critic: Backpropagating through paths.
\newblock \emph{ArXiv}, abs/2005.08068, 2020.

\bibitem[Ebert et~al.(2018)Ebert, Finn, Dasari, Xie, Lee, and
  Levine]{Ebert2018VisualFM}
Ebert, F., Finn, C., Dasari, S., Xie, A., Lee, A.~X., and Levine, S.
\newblock Visual foresight: Model-based deep reinforcement learning for
  vision-based robotic control.
\newblock \emph{ArXiv}, abs/1812.00568, 2018.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, Legg, and Kavukcuoglu]{Espeholt2018IMPALASD}
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron,
  Y., Firoiu, V., Harley, T., Dunning, I., Legg, S., and Kavukcuoglu, K.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock \emph{ArXiv}, abs/1802.01561, 2018.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and
  Meger]{Fujimoto2018AddressingFA}
Fujimoto, S., Hoof, H.~V., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock \emph{ArXiv}, abs/1802.09477, 2018.

\bibitem[Ha \& Schmidhuber(2018)Ha and Schmidhuber]{ha2018worldmodels}
Ha, D. and Schmidhuber, J.
\newblock Recurrent world models facilitate policy evolution.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pp.\
  2451--2463. Curran Associates, Inc., 2018.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Hartikainen, Tucker, Ha, Tan,
  Kumar, Zhu, Gupta, Abbeel, and Levine]{Haarnoja2018SoftAA}
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar,
  V., Zhu, H., Gupta, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic algorithms and applications.
\newblock \emph{ArXiv}, abs/1812.05905, 2018.

\bibitem[Hafez et~al.(2019)Hafez, Weber, Kerzel, and
  Wermter]{Hafez2019CuriousMA}
Hafez, M.~B., Weber, C., Kerzel, M., and Wermter, S.
\newblock Curious meta-controller: Adaptive alternation between model-based and
  model-free control in deep reinforcement learning.
\newblock \emph{2019 International Joint Conference on Neural Networks
  (IJCNN)}, pp.\  1--8, 2019.

\bibitem[Hafner et~al.(2019)Hafner, Lillicrap, Fischer, Villegas, Ha, Lee, and
  Davidson]{hafner2019planet}
Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and
  Davidson, J.
\newblock Learning latent dynamics for planning from pixels.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2555--2565, 2019.

\bibitem[Hafner et~al.(2020{\natexlab{a}})Hafner, Lillicrap, Norouzi, and
  Ba]{hafner2020dreamerv2}
Hafner, D., Lillicrap, T., Norouzi, M., and Ba, J.
\newblock Mastering atari with discrete world models.
\newblock \emph{arXiv preprint arXiv:2010.02193}, 2020{\natexlab{a}}.

\bibitem[Hafner et~al.(2020{\natexlab{b}})Hafner, Lillicrap, Ba, and
  Norouzi]{Hafner2020DreamTC}
Hafner, D., Lillicrap, T.~P., Ba, J., and Norouzi, M.
\newblock Dream to control: Learning behaviors by latent imagination.
\newblock \emph{ArXiv}, abs/1912.01603, 2020{\natexlab{b}}.

\bibitem[Hansen \& Wang(2021)Hansen and Wang]{hansen2021softda}
Hansen, N. and Wang, X.
\newblock Generalization in reinforcement learning by soft data augmentation.
\newblock In \emph{International Conference on Robotics and Automation (ICRA)},
  2021.

\bibitem[Hansen et~al.(2021)Hansen, Jangir, Sun, Alenyà, Abbeel, Efros, Pinto,
  and Wang]{hansen2021deployment}
Hansen, N., Jangir, R., Sun, Y., Alenyà, G., Abbeel, P., Efros, A.~A., Pinto,
  L., and Wang, X.
\newblock Self-supervised policy adaptation during deployment.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Hasselt et~al.(2016)Hasselt, Guez, and Silver]{Hasselt2016DeepRL}
Hasselt, H.~V., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{Aaai}, 2016.

\bibitem[Hatch \& Boots(2021)Hatch and Boots]{Hatch2021TheVO}
Hatch, N. and Boots, B.
\newblock The value of planning for infinite-horizon model predictive control.
\newblock \emph{2021 IEEE International Conference on Robotics and Automation
  (ICRA)}, pp.\  7372--7378, 2021.

\bibitem[Janner et~al.(2019)Janner, Fu, Zhang, and Levine]{Janner2019WhenTT}
Janner, M., Fu, J., Zhang, M., and Levine, S.
\newblock When to trust your model: Model-based policy optimization.
\newblock \emph{ArXiv}, abs/1906.08253, 2019.

\bibitem[Kaiser et~al.(2020)Kaiser, Babaeizadeh, Milos, Osinski, Campbell,
  Czechowski, Erhan, Finn, Kozakowski, Levine, Sepassi, Tucker, and
  Michalewski]{Kaiser2020ModelBasedRL}
Kaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R.~H.,
  Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., Sepassi, R.,
  Tucker, G., and Michalewski, H.
\newblock Model-based reinforcement learning for atari.
\newblock \emph{ArXiv}, abs/1903.00374, 2020.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, and
  Levine]{Kalashnikov2018QTOptSD}
Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E.,
  Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., and Levine, S.
\newblock Qt-opt: Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock \emph{ArXiv}, abs/1806.10293, 2018.

\bibitem[Kalashnikov et~al.(2021)Kalashnikov, Varley, Chebotar, Swanson,
  Jonschkowski, Finn, Levine, and Hausman]{Kalashnikov2021MTOptCM}
Kalashnikov, D., Varley, J., Chebotar, Y., Swanson, B., Jonschkowski, R., Finn,
  C., Levine, S., and Hausman, K.
\newblock Mt-opt: Continuous multi-task robotic reinforcement learning at
  scale.
\newblock \emph{ArXiv}, abs/2104.08212, 2021.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{Kidambi2020MOReLM}
Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T.
\newblock Morel : Model-based offline reinforcement learning.
\newblock \emph{ArXiv}, abs/2005.05951, 2020.

\bibitem[Kostrikov et~al.(2020)Kostrikov, Yarats, and
  Fergus]{kostrikov2020image}
Kostrikov, I., Yarats, D., and Fergus, R.
\newblock Image augmentation is all you need: Regularizing deep reinforcement
  learning from pixels.
\newblock \emph{International Conference on Learning Representations}, 2020.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{Lillicrap2016ContinuousCW}
Lillicrap, T., Hunt, J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver,
  D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{CoRR}, abs/1509.02971, 2016.

\bibitem[Lowrey et~al.(2019)Lowrey, Rajeswaran, Kakade, Todorov, and
  Mordatch]{Lowrey2019PlanOL}
Lowrey, K., Rajeswaran, A., Kakade, S.~M., Todorov, E., and Mordatch, I.
\newblock Plan online, learn offline: Efficient learning and exploration via
  model-based control.
\newblock \emph{ArXiv}, abs/1811.01848, 2019.

\bibitem[Margolis et~al.(2021)Margolis, Chen, Paigwar, Fu, Kim, Kim, and
  Agrawal]{Margolis2021LearningTJ}
Margolis, G., Chen, T., Paigwar, K., Fu, X., Kim, D., Kim, S., and Agrawal, P.
\newblock Learning to jump from pixels.
\newblock In \emph{CoRL}, 2021.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., and Riedmiller, M.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{Mnih2016AsynchronousMF}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T.~P., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{ICML}, 2016.

\bibitem[Morgan et~al.(2021)Morgan, Nandha, Chalvatzaki, D'Eramo, Dollar, and
  Peters]{Morgan2021ModelPA}
Morgan, A.~S., Nandha, D., Chalvatzaki, G., D'Eramo, C., Dollar, A.~M., and
  Peters, J.
\newblock Model predictive actor-critic: Accelerating robot skill acquisition
  with deep reinforcement learning.
\newblock \emph{2021 IEEE International Conference on Robotics and Automation
  (ICRA)}, pp.\  6672--6678, 2021.

\bibitem[Nagabandi et~al.(2018)Nagabandi, Kahn, Fearing, and
  Levine]{Nagabandi2018NeuralND}
Nagabandi, A., Kahn, G., Fearing, R.~S., and Levine, S.
\newblock Neural network dynamics for model-based deep reinforcement learning
  with model-free fine-tuning.
\newblock \emph{2018 IEEE International Conference on Robotics and Automation
  (ICRA)}, pp.\  7559--7566, 2018.

\bibitem[Negenborn et~al.(2005)Negenborn, {De Schutter}, Wiering, and
  Hellendoorn]{negenborn2005}
Negenborn, R.~R., {De Schutter}, B., Wiering, M.~A., and Hellendoorn, H.
\newblock Learning-based model predictive control for markov decision
  processes.
\newblock \emph{IFAC Proceedings Volumes}, 38\penalty0 (1):\penalty0 354--359,
  2005.
\newblock 16th IFAC World Congress.

\bibitem[Nguyen et~al.(2021)Nguyen, Shu, Pham, Bui, and
  Ermon]{Nguyen2021TemporalPC}
Nguyen, T.~D., Shu, R., Pham, T., Bui, H.~H., and Ermon, S.
\newblock Temporal predictive coding for model-based planning in latent space.
\newblock In \emph{ICML}, 2021.

\bibitem[Pong et~al.(2018)Pong, Gu, Dalal, and Levine]{Pong2018TemporalDM}
Pong, V.~H., Gu, S.~S., Dalal, M., and Levine, S.
\newblock Temporal difference models: Model-free deep rl for model-based
  control.
\newblock \emph{ArXiv}, abs/1802.09081, 2018.

\bibitem[Pourchot \& Sigaud(2019)Pourchot and Sigaud]{Pourchot2019CEMRLCE}
Pourchot, A. and Sigaud, O.
\newblock Cem-rl: Combining evolutionary and gradient-based methods for policy
  search.
\newblock \emph{ArXiv}, abs/1810.01222, 2019.

\bibitem[Rubinstein(1997)]{Rubinstein1997OptimizationOC}
Rubinstein, R.~Y.
\newblock Optimization of computer simulation models with rare events.
\newblock \emph{European Journal of Operational Research}, 99:\penalty0
  89--112, 1997.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and
  Silver]{Schaul2016PrioritizedER}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock \emph{CoRR}, abs/1511.05952, 2016.

\bibitem[Schrittwieser et~al.(2020)Schrittwieser, Antonoglou, Hubert, Simonyan,
  Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel, Lillicrap, and
  Silver]{Schrittwieser2020MasteringAG}
Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L.,
  Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., Lillicrap,
  T.~P., and Silver, D.
\newblock Mastering atari, go, chess and shogi by planning with a learned
  model.
\newblock \emph{Nature}, 588 7839:\penalty0 604--609, 2020.

\bibitem[Sekar et~al.(2020)Sekar, Rybkin, Daniilidis, Abbeel, Hafner, and
  Pathak]{Sekar2020PlanningTE}
Sekar, R., Rybkin, O., Daniilidis, K., Abbeel, P., Hafner, D., and Pathak, D.
\newblock Planning to explore via self-supervised world models.
\newblock \emph{ArXiv}, abs/2005.05960, 2020.

\bibitem[Shao et~al.(2020)Shao, You, Yan, Sun, and Bohg]{shao2020grac}
Shao, L., You, Y., Yan, M., Sun, Q., and Bohg, J.
\newblock Grac: Self-guided and self-regularized actor-critic.
\newblock \emph{arXiv preprint arXiv:2009.08973}, 2020.

\bibitem[Sikchi et~al.(2022)Sikchi, Zhou, and Held]{Sikchi2020LearningOW}
Sikchi, H., Zhou, W., and Held, D.
\newblock Learning off-policy with online planning.
\newblock In \emph{Conference on Robot Learning}, pp.\  1622--1633. PMLR, 2022.

\bibitem[Srinivas et~al.(2020)Srinivas, Laskin, and Abbeel]{srinivas2020curl}
Srinivas, A., Laskin, M., and Abbeel, P.
\newblock Curl: Contrastive unsupervised representations for reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2004.04136}, 2020.

\bibitem[Sutton(1988)]{Sutton1988TD}
Sutton, R.
\newblock Learning to predict by the method of temporal differences.
\newblock \emph{Machine Learning}, 3:\penalty0 9--44, 08 1988.
\newblock \doi{10.1007/BF00115009}.

\bibitem[Sutton(2005)]{Sutton1988LearningTP}
Sutton, R.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine Learning}, 3:\penalty0 9--44, 2005.

\bibitem[Tassa et~al.(2012)Tassa, Erez, and Todorov]{Tassa2012SynthesisAS}
Tassa, Y., Erez, T., and Todorov, E.
\newblock Synthesis and stabilization of complex behaviors through online
  trajectory optimization.
\newblock \emph{2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pp.\  4906--4913, 2012.

\bibitem[Tassa et~al.(2018)Tassa, Doron, Muldal, Erez, Li, de~Las~Casas,
  Budden, Abdolmaleki, et~al.]{deepmindcontrolsuite2018}
Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., de~Las~Casas, D., Budden,
  D., Abdolmaleki, A., et~al.
\newblock Deepmind control suite.
\newblock Technical report, DeepMind, 2018.

\bibitem[Wang \& Ba(2020)Wang and Ba]{Wang2020ExploringMP}
Wang, T. and Ba, J.
\newblock Exploring model-based planning with policy networks.
\newblock \emph{ArXiv}, abs/1906.08649, 2020.

\bibitem[Williams et~al.(2015)Williams, Aldrich, and
  Theodorou]{Williams2015ModelPP}
Williams, G., Aldrich, A., and Theodorou, E.~A.
\newblock Model predictive path integral control using covariance variable
  importance sampling.
\newblock \emph{ArXiv}, abs/1509.01149, 2015.

\bibitem[Yarats \& Kostrikov(2020)Yarats and Kostrikov]{pytorch_sac}
Yarats, D. and Kostrikov, I.
\newblock Soft actor-critic (sac) implementation in pytorch.
\newblock \url{https://github.com/denisyarats/pytorch_sac}, 2020.

\bibitem[Yarats et~al.(2021)Yarats, Fergus, Lazaric, and
  Pinto]{yarats2021drqv2}
Yarats, D., Fergus, R., Lazaric, A., and Pinto, L.
\newblock Mastering visual continuous control: Improved data-augmented
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2107.09645}, 2021.

\bibitem[Ye et~al.(2021)Ye, Liu, Kurutach, Abbeel, and Gao]{Ye2021MasteringAG}
Ye, W., Liu, S., Kurutach, T., Abbeel, P., and Gao, Y.
\newblock Mastering atari games with limited data.
\newblock \emph{ArXiv}, abs/2111.00210, 2021.

\bibitem[Yu et~al.(2019)Yu, Quillen, He, Julian, Hausman, Finn, and
  Levine]{yu2019meta}
Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S.
\newblock Meta-world: A benchmark and evaluation for multi-task and meta
  reinforcement learning.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2019.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and
  Ma]{Yu2020MOPOMO}
Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J.~Y., Levine, S., Finn, C., and
  Ma, T.
\newblock Mopo: Model-based offline policy optimization.
\newblock \emph{ArXiv}, abs/2005.13239, 2020.

\bibitem[Zhang et~al.(2018)Zhang, Vikram, Smith, Abbeel, Johnson, and
  Levine]{Zhang2018SOLARDS}
Zhang, M., Vikram, S., Smith, L., Abbeel, P., Johnson, M.~J., and Levine, S.
\newblock Solar: Deep structured latent representations for model-based
  reinforcement learning.
\newblock \emph{ArXiv}, abs/1808.09105, 2018.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and
  Dey]{ziebart2008maximum}
Ziebart, B.~D., Maas, A., Bagnell, J.~A., and Dey, A.~K.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{Proceedings of the 23rd National Conference on Artificial
  Intelligence}, volume~3, 2008.

\end{thebibliography}
