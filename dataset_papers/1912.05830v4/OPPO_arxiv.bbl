\begin{thebibliography}{77}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{}\fi

\bibitem[{Abbasi-Yadkori et~al.(2019{\natexlab{a}})Abbasi-Yadkori, Bartlett,
  Bhatia, Lazic, Szepesv{\'a}ri and Weisz}]{abbasi2019politex}
\text{Abbasi-Yadkori, Y.}, \text{Bartlett, P.}, \text{Bhatia, K.}, \text{Lazic,
  N.}, \text{Szepesv{\'a}ri, C.} and \text{Weisz, G.} (2019{\natexlab{a}}).
\newblock {POLITEX}: Regret bounds for policy iteration using expert
  prediction.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Abbasi-Yadkori et~al.(2019{\natexlab{b}})Abbasi-Yadkori, Lazic,
  Szepesvari and Weisz}]{abbasi2019exploration}
\text{Abbasi-Yadkori, Y.}, \text{Lazic, N.}, \text{Szepesvari, C.} and
  \text{Weisz, G.} (2019{\natexlab{b}}).
\newblock Exploration-enhanced {POLITEX}.
\newblock \textit{arXiv preprint arXiv:1908.10479}.

\bibitem[{Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l and
  Szepesv{\'a}ri}]{abbasi2011improved}
\text{Abbasi-Yadkori, Y.}, \text{P{\'a}l, D.} and \text{Szepesv{\'a}ri, C.}
  (2011).
\newblock Improved algorithms for linear stochastic bandits.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Agarwal et~al.(2019)Agarwal, Kakade, Lee and
  Mahajan}]{agarwal2019optimality}
\text{Agarwal, A.}, \text{Kakade, S.~M.}, \text{Lee, J.~D.} and \text{Mahajan,
  G.} (2019).
\newblock Optimality and approximation with policy gradient methods in {M}arkov
  decision processes.
\newblock \textit{arXiv preprint arXiv:1908.00261}.

\bibitem[{Antos et~al.(2008)Antos, Szepesv{\'a}ri and Munos}]{antos2008fitted}
\text{Antos, A.}, \text{Szepesv{\'a}ri, C.} and \text{Munos, R.} (2008).
\newblock Fitted {Q}-iteration in continuous action-space mdps.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Auer et~al.(2002)Auer, Cesa-Bianchi and Fischer}]{auer2002finite}
\text{Auer, P.}, \text{Cesa-Bianchi, N.} and \text{Fischer, P.} (2002).
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock \textit{Machine Learning}, \textbf{47} 235--256.

\bibitem[{Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang and
  Yang}]{ayoub2020model}
\text{Ayoub, A.}, \text{Jia, Z.}, \text{Szepesvari, C.}, \text{Wang, M.} and
  \text{Yang, L.} (2020).
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock \textit{arXiv preprint arXiv:2006.01107}.

\bibitem[{Azar et~al.(2012{\natexlab{a}})Azar, G{\'o}mez and
  Kappen}]{azar2012dynamic}
\text{Azar, M.~G.}, \text{G{\'o}mez, V.} and \text{Kappen, H.~J.}
  (2012{\natexlab{a}}).
\newblock Dynamic policy programming.
\newblock \textit{Journal of Machine Learning Research}, \textbf{13}
  3207--3245.

\bibitem[{Azar et~al.(2011)Azar, Munos, Ghavamzadaeh and
  Kappen}]{azar2011speedy}
\text{Azar, M.~G.}, \text{Munos, R.}, \text{Ghavamzadaeh, M.} and \text{Kappen,
  H.~J.} (2011).
\newblock Speedy {Q}-learning.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Azar et~al.(2012{\natexlab{b}})Azar, Munos and
  Kappen}]{azar2012sample}
\text{Azar, M.~G.}, \text{Munos, R.} and \text{Kappen, B.}
  (2012{\natexlab{b}}).
\newblock On the sample complexity of reinforcement learning with a generative
  model.
\newblock \textit{arXiv preprint arXiv:1206.6461}.

\bibitem[{Azar et~al.(2017)Azar, Osband and Munos}]{azar2017minimax}
\text{Azar, M.~G.}, \text{Osband, I.} and \text{Munos, R.} (2017).
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Baxter and Bartlett(2000)}]{baxter2000direct}
\text{Baxter, J.} and \text{Bartlett, P.~L.} (2000).
\newblock Direct gradient-based reinforcement learning.
\newblock In \textit{International Symposium on Circuits and Systems}.

\bibitem[{Bhandari and Russo(2019)}]{bhandari2019global}
\text{Bhandari, J.} and \text{Russo, D.} (2019).
\newblock Global optimality guarantees for policy gradient methods.
\newblock \textit{arXiv preprint arXiv:1906.01786}.

\bibitem[{Boyan(2002)}]{boyan2002technical}
\text{Boyan, J.~A.} (2002).
\newblock Least-squares temporal difference learning.
\newblock \textit{Machine Learning}, \textbf{49} 233--246.

\bibitem[{Bradtke and Barto(1996)}]{bradtke1996linear}
\text{Bradtke, S.~J.} and \text{Barto, A.~G.} (1996).
\newblock Linear least-squares algorithms for temporal difference learning.
\newblock \textit{Machine Learning}, \textbf{22} 33--57.

\bibitem[{Bubeck and Cesa-Bianchi(2012)}]{bubeck2012regret}
\text{Bubeck, S.} and \text{Cesa-Bianchi, N.} (2012).
\newblock Regret analysis of stochastic and nonstochastic multi-armed bandit
  problems.
\newblock \textit{Foundations and Trends{\textregistered} in Machine Learning},
  \textbf{5} 1--122.

\bibitem[{Cesa-Bianchi and Lugosi(2006)}]{cesa2006prediction}
\text{Cesa-Bianchi, N.} and \text{Lugosi, G.} (2006).
\newblock \textit{Prediction, Learning, and Games}.
\newblock Cambridge.

\bibitem[{Chen and Jiang(2019)}]{chen2019information}
\text{Chen, J.} and \text{Jiang, N.} (2019).
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock \textit{arXiv preprint arXiv:1905.00360}.

\bibitem[{Chu et~al.(2011)Chu, Li, Reyzin and Schapire}]{chu2011contextual}
\text{Chu, W.}, \text{Li, L.}, \text{Reyzin, L.} and \text{Schapire, R.}
  (2011).
\newblock Contextual bandits with linear payoff functions.
\newblock In \textit{International Conference on Artificial Intelligence and
  Statistics}.

\bibitem[{Dani et~al.(2008)Dani, Hayes and Kakade}]{dani2008stochastic}
\text{Dani, V.}, \text{Hayes, T.~P.} and \text{Kakade, S.~M.} (2008).
\newblock Stochastic linear optimization under bandit feedback.
\newblock \textit{Conference on Learning Theory}.

\bibitem[{Dann et~al.(2017)Dann, Lattimore and Brunskill}]{dann2017unifying}
\text{Dann, C.}, \text{Lattimore, T.} and \text{Brunskill, E.} (2017).
\newblock Unifying {PAC} and regret: Uniform {PAC} bounds for episodic
  reinforcement learning.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Dong et~al.(2019)Dong, Peng, Wang and Zhou}]{dong2019sqrt}
\text{Dong, K.}, \text{Peng, J.}, \text{Wang, Y.} and \text{Zhou, Y.} (2019).
\newblock $\sqrt{n}$-regret for learning in {M}arkov decision processes with
  function approximation and low {B}ellman rank.
\newblock \textit{arXiv preprint arXiv:1909.02506}.

\bibitem[{Du et~al.(2019{\natexlab{a}})Du, Kakade, Wang and Yang}]{du2019good}
\text{Du, S.~S.}, \text{Kakade, S.~M.}, \text{Wang, R.} and \text{Yang, L.}
  (2019{\natexlab{a}}).
\newblock Is a good representation sufficient for sample efficient
  reinforcement learning?
\newblock \textit{arXiv preprint arXiv:1910.03016}.

\bibitem[{Du et~al.(2019{\natexlab{b}})Du, Luo, Wang and
  Zhang}]{du2019provably}
\text{Du, S.~S.}, \text{Luo, Y.}, \text{Wang, R.} and \text{Zhang, H.}
  (2019{\natexlab{b}}).
\newblock Provably efficient {Q}-learning with function approximation via
  distribution shift error checking oracle.
\newblock \textit{arXiv preprint arXiv:1906.06321}.

\bibitem[{Duan et~al.(2016)Duan, Chen, Houthooft, Schulman and
  Abbeel}]{duan2016benchmarking}
\text{Duan, Y.}, \text{Chen, X.}, \text{Houthooft, R.}, \text{Schulman, J.} and
  \text{Abbeel, P.} (2016).
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Even-Dar et~al.(2009)Even-Dar, Kakade and Mansour}]{even2009online}
\text{Even-Dar, E.}, \text{Kakade, S.~M.} and \text{Mansour, Y.} (2009).
\newblock Online {M}arkov decision processes.
\newblock \textit{Mathematics of Operations Research}, \textbf{34} 726--736.

\bibitem[{Farahmand et~al.(2010)Farahmand, Szepesv{\'a}ri and
  Munos}]{farahmand2010error}
\text{Farahmand, A.-m.}, \text{Szepesv{\'a}ri, C.} and \text{Munos, R.} (2010).
\newblock Error propagation for approximate policy and value iteration.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Fazel et~al.(2018)Fazel, Ge, Kakade and Mesbahi}]{fazel2018global}
\text{Fazel, M.}, \text{Ge, R.}, \text{Kakade, S.~M.} and \text{Mesbahi, M.}
  (2018).
\newblock Global convergence of policy gradient methods for the linear
  quadratic regulator.
\newblock \textit{arXiv preprint arXiv:1801.05039}.

\bibitem[{Geist et~al.(2019)Geist, Scherrer and Pietquin}]{geist2019theory}
\text{Geist, M.}, \text{Scherrer, B.} and \text{Pietquin, O.} (2019).
\newblock A theory of regularized {M}arkov decision processes.
\newblock \textit{arXiv preprint arXiv:1901.11275}.

\bibitem[{Jaksch et~al.(2010)Jaksch, Ortner and Auer}]{jaksch2010near}
\text{Jaksch, T.}, \text{Ortner, R.} and \text{Auer, P.} (2010).
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \textit{Journal of Machine Learning Research}, \textbf{11}
  1563--1600.

\bibitem[{Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford and
  Schapire}]{jiang2017contextual}
\text{Jiang, N.}, \text{Krishnamurthy, A.}, \text{Agarwal, A.}, \text{Langford,
  J.} and \text{Schapire, R.~E.} (2017).
\newblock Contextual decision processes with low {B}ellman rank are
  {PAC}-learnable.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Jin et~al.(2018)Jin, Allen-Zhu, Bubeck and Jordan}]{jin2018q}
\text{Jin, C.}, \text{Allen-Zhu, Z.}, \text{Bubeck, S.} and \text{Jordan,
  M.~I.} (2018).
\newblock Is {Q}-learning provably efficient?
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Jin et~al.(2019)Jin, Yang, Wang and Jordan}]{jin2019provably}
\text{Jin, C.}, \text{Yang, Z.}, \text{Wang, Z.} and \text{Jordan, M.~I.}
  (2019).
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock \textit{arXiv preprint arXiv:1907.05388}.

\bibitem[{Kakade(2002)}]{kakade2002natural}
\text{Kakade, S.~M.} (2002).
\newblock A natural policy gradient.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Kakade(2003)}]{kakade2003sample}
\text{Kakade, S.~M.} (2003).
\newblock \textit{On the Sample Complexity of Reinforcement Learning}.
\newblock Ph.D. thesis, University of London.

\bibitem[{Koenig and Simmons(1993)}]{koenig1993complexity}
\text{Koenig, S.} and \text{Simmons, R.~G.} (1993).
\newblock Complexity analysis of real-time reinforcement learning.
\newblock In \textit{Association for the Advancement of Artificial
  Intelligence}.

\bibitem[{Konda and Tsitsiklis(2000)}]{konda2000actor}
\text{Konda, V.~R.} and \text{Tsitsiklis, J.~N.} (2000).
\newblock Actor-critic algorithms.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Lattimore and Szepesvari(2019)}]{lattimore2019learning}
\text{Lattimore, T.} and \text{Szepesvari, C.} (2019).
\newblock Learning with good feature representations in bandits and in {RL}
  with a generative model.
\newblock \textit{arXiv preprint arXiv:1911.07676}.

\bibitem[{Leffler et~al.(2007)Leffler, Littman and
  Edmunds}]{leffler2007efficient}
\text{Leffler, B.~R.}, \text{Littman, M.~L.} and \text{Edmunds, T.} (2007).
\newblock Efficient reinforcement learning with relocatable action models.
\newblock In \textit{Association for the Advancement of Artificial
  Intelligence}.

\bibitem[{Liu et~al.(2019)Liu, Cai, Yang and Wang}]{liu2019neural}
\text{Liu, B.}, \text{Cai, Q.}, \text{Yang, Z.} and \text{Wang, Z.} (2019).
\newblock Neural proximal/trust region policy optimization attains globally
  optimal policy.
\newblock \textit{arXiv preprint arXiv:1906.10306}.

\bibitem[{Mania et~al.(2018)Mania, Guy and Recht}]{mania2018simple}
\text{Mania, H.}, \text{Guy, A.} and \text{Recht, B.} (2018).
\newblock Simple random search provides a competitive approach to reinforcement
  learning.
\newblock \textit{arXiv preprint arXiv:1803.07055}.

\bibitem[{Munos and Szepesv{\'a}ri(2008)}]{munos2008finite}
\text{Munos, R.} and \text{Szepesv{\'a}ri, C.} (2008).
\newblock Finite-time bounds for fitted value iteration.
\newblock \textit{Journal of Machine Learning Research}, \textbf{9} 815--857.

\bibitem[{Nemirovsky and Yudin(1983)}]{nemirovsky1983problem}
\text{Nemirovsky, A.~S.} and \text{Yudin, D.~B.} (1983).
\newblock \textit{Problem Complexity and Method Efficiency in Optimization.}
\newblock Wiley.

\bibitem[{Neu et~al.(2010{\natexlab{a}})Neu, Antos, Gy{\"o}rgy and
  Szepesv{\'a}ri}]{neu2010bandit}
\text{Neu, G.}, \text{Antos, A.}, \text{Gy{\"o}rgy, A.} and
  \text{Szepesv{\'a}ri, C.} (2010{\natexlab{a}}).
\newblock Online {M}arkov decision processes under bandit feedback.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Neu et~al.(2010{\natexlab{b}})Neu, Gy{\"o}rgy and
  Szepesv{\'a}ri}]{neu2010online}
\text{Neu, G.}, \text{Gy{\"o}rgy, A.} and \text{Szepesv{\'a}ri, C.}
  (2010{\natexlab{b}}).
\newblock The online loop-free stochastic shortest-path problem.
\newblock In \textit{Conference on Learning Theory}.

\bibitem[{Neu et~al.(2012)Neu, Gy{\"o}rgy and
  Szepesv{\'a}ri}]{neu2012adversarial}
\text{Neu, G.}, \text{Gy{\"o}rgy, A.} and \text{Szepesv{\'a}ri, C.} (2012).
\newblock The adversarial stochastic shortest path problem with unknown
  transition probabilities.
\newblock In \textit{International Conference on Artificial Intelligence and
  Statistics}.

\bibitem[{Neu et~al.(2017)Neu, Jonsson and G{\'o}mez}]{neu2017unified}
\text{Neu, G.}, \text{Jonsson, A.} and \text{G{\'o}mez, V.} (2017).
\newblock A unified view of entropy-regularized {M}arkov decision processes.
\newblock \textit{arXiv preprint arXiv:1705.07798}.

\bibitem[{OpenAI(2019)}]{openai2019dota}
\text{OpenAI} (2019).
\newblock Open{AI} {F}ive.
\newblock \url{https://openai.com/five/}.

\bibitem[{Osband and Van~Roy(2016)}]{osband2016lower}
\text{Osband, I.} and \text{Van~Roy, B.} (2016).
\newblock On lower bounds for regret in reinforcement learning.
\newblock \textit{arXiv preprint arXiv:1608.02732}.

\bibitem[{Osband et~al.(2014)Osband, Van~Roy and
  Wen}]{osband2014generalization}
\text{Osband, I.}, \text{Van~Roy, B.} and \text{Wen, Z.} (2014).
\newblock Generalization and exploration via randomized value functions.
\newblock \textit{arXiv preprint arXiv:1402.0635}.

\bibitem[{Rosenberg and Mansour(2019{\natexlab{a}})}]{rosenberg2019online}
\text{Rosenberg, A.} and \text{Mansour, Y.} (2019{\natexlab{a}}).
\newblock Online convex optimization in adversarial {M}arkov decision
  processes.
\newblock \textit{arXiv preprint arXiv:1905.07773}.

\bibitem[{Rosenberg and Mansour(2019{\natexlab{b}})}]{rosenberg2019bandit}
\text{Rosenberg, A.} and \text{Mansour, Y.} (2019{\natexlab{b}}).
\newblock Online stochastic shortest path with bandit feedback and unknown
  transition function.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Rusmevichientong and Tsitsiklis(2010)}]{rusmevichientong2010linearly}
\text{Rusmevichientong, P.} and \text{Tsitsiklis, J.~N.} (2010).
\newblock Linearly parameterized bandits.
\newblock \textit{Mathematics of Operations Research}, \textbf{35} 395--411.

\bibitem[{Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan and
  Moritz}]{schulman2015trust}
\text{Schulman, J.}, \text{Levine, S.}, \text{Abbeel, P.}, \text{Jordan, M.}
  and \text{Moritz, P.} (2015).
\newblock Trust region policy optimization.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford and
  Klimov}]{schulman2017proximal}
\text{Schulman, J.}, \text{Wolski, F.}, \text{Dhariwal, P.}, \text{Radford, A.}
  and \text{Klimov, O.} (2017).
\newblock Proximal policy optimization algorithms.
\newblock \textit{arXiv preprint arXiv:1707.06347}.

\bibitem[{Sidford et~al.(2018{\natexlab{a}})Sidford, Wang, Wu, Yang and
  Ye}]{sidford2018near}
\text{Sidford, A.}, \text{Wang, M.}, \text{Wu, X.}, \text{Yang, L.} and
  \text{Ye, Y.} (2018{\natexlab{a}}).
\newblock Near-optimal time and sample complexities for solving {M}arkov
  decision processes with a generative model.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Sidford et~al.(2018{\natexlab{b}})Sidford, Wang, Wu and
  Ye}]{sidford2018variance}
\text{Sidford, A.}, \text{Wang, M.}, \text{Wu, X.} and \text{Ye, Y.}
  (2018{\natexlab{b}}).
\newblock Variance reduced value iteration and faster algorithms for solving
  {M}arkov decision processes.
\newblock In \textit{Symposium on Discrete Algorithms}.

\bibitem[{Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot
  et~al.}]{silver2016mastering}
\text{Silver, D.}, \text{Huang, A.}, \text{Maddison, C.~J.}, \text{Guez, A.},
  \text{Sifre, L.}, \text{Van Den~Driessche, G.}, \text{Schrittwieser, J.},
  \text{Antonoglou, I.}, \text{Panneershelvam, V.}, \text{Lanctot, M.}
  \text{et~al.} (2016).
\newblock Mastering the game of {G}o with deep neural networks and tree search.
\newblock \textit{Nature}, \textbf{529} 484.

\bibitem[{Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou,
  Huang, Guez, Hubert, Baker, Lai, Bolton et~al.}]{silver2017mastering}
\text{Silver, D.}, \text{Schrittwieser, J.}, \text{Simonyan, K.},
  \text{Antonoglou, I.}, \text{Huang, A.}, \text{Guez, A.}, \text{Hubert, T.},
  \text{Baker, L.}, \text{Lai, M.}, \text{Bolton, A.} \text{et~al.} (2017).
\newblock Mastering the game of {G}o without human knowledge.
\newblock \textit{Nature}, \textbf{550} 354.

\bibitem[{Strehl et~al.(2006)Strehl, Li, Wiewiora, Langford and
  Littman}]{strehl2006pac}
\text{Strehl, A.~L.}, \text{Li, L.}, \text{Wiewiora, E.}, \text{Langford, J.}
  and \text{Littman, M.~L.} (2006).
\newblock {PAC} model-free reinforcement learning.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Sutton and Barto(2018)}]{sutton2018reinforcement}
\text{Sutton, R.~S.} and \text{Barto, A.~G.} (2018).
\newblock \textit{Reinforcement Learning: An Introduction}.
\newblock MIT.

\bibitem[{Sutton et~al.(2000)Sutton, McAllester, Singh and
  Mansour}]{sutton2000policy}
\text{Sutton, R.~S.}, \text{McAllester, D.~A.}, \text{Singh, S.~P.} and
  \text{Mansour, Y.} (2000).
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Tosatto et~al.(2017)Tosatto, Pirotta, D'Eramo and
  Restelli}]{tosatto2017boosted}
\text{Tosatto, S.}, \text{Pirotta, M.}, \text{D'Eramo, C.} and \text{Restelli,
  M.} (2017).
\newblock Boosted fitted {Q}-iteration.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Van~Roy and Dong(2019)}]{van2019comments}
\text{Van~Roy, B.} and \text{Dong, S.} (2019).
\newblock Comments on the {D}u-{K}akade-{W}ang-{Y}ang lower bounds.
\newblock \textit{arXiv preprint arXiv:1911.07910}.

\bibitem[{Wainwright(2019)}]{wainwright2019variance}
\text{Wainwright, M.~J.} (2019).
\newblock Variance-reduced {Q}-learning is minimax optimal.
\newblock \textit{arXiv preprint arXiv:1906.04697}.

\bibitem[{Wang et~al.(2019)Wang, Cai, Yang and Wang}]{wang2019neural}
\text{Wang, L.}, \text{Cai, Q.}, \text{Yang, Z.} and \text{Wang, Z.} (2019).
\newblock Neural policy gradient methods: Global optimality and rates of
  convergence.
\newblock \textit{arXiv preprint arXiv:1909.01150}.

\bibitem[{Wang et~al.(2018)Wang, Li and He}]{wang2018deep}
\text{Wang, W.~Y.}, \text{Li, J.} and \text{He, X.} (2018).
\newblock Deep reinforcement learning for {NLP}.
\newblock In \textit{Association for Computational Linguistics}.

\bibitem[{Wen and Van~Roy(2017)}]{wen2017efficient}
\text{Wen, Z.} and \text{Van~Roy, B.} (2017).
\newblock Efficient reinforcement learning in deterministic systems with value
  function generalization.
\newblock \textit{Mathematics of Operations Research}, \textbf{42} 762--782.

\bibitem[{Williams(1992)}]{williams1992simple}
\text{Williams, R.~J.} (1992).
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \textit{Machine Learning}, \textbf{8} 229--256.

\bibitem[{Xiao(2010)}]{xiao2010dual}
\text{Xiao, L.} (2010).
\newblock Dual averaging methods for regularized stochastic learning and online
  optimization.
\newblock \textit{Journal of Machine Learning Research}, \textbf{11}
  2543--2596.

\bibitem[{Yang and Wang(2019{\natexlab{a}})}]{yang2019reinforcement}
\text{Yang, L.} and \text{Wang, M.} (2019{\natexlab{a}}).
\newblock Reinforcement leaning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock \textit{arXiv preprint arXiv:1905.10389}.

\bibitem[{Yang and Wang(2019{\natexlab{b}})}]{yang2019sample}
\text{Yang, L.} and \text{Wang, M.} (2019{\natexlab{b}}).
\newblock Sample-optimal parametric {Q}-learning using linearly additive
  features.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Yang et~al.(2019{\natexlab{a}})Yang, Chen, Hong and
  Wang}]{yang2019global}
\text{Yang, Z.}, \text{Chen, Y.}, \text{Hong, M.} and \text{Wang, Z.}
  (2019{\natexlab{a}}).
\newblock On the global convergence of actor-critic: A case for linear
  quadratic regulator with ergodic cost.
\newblock \textit{arXiv preprint arXiv:1907.06246}.

\bibitem[{Yang et~al.(2019{\natexlab{b}})Yang, Xie and
  Wang}]{yang2019theoretical}
\text{Yang, Z.}, \text{Xie, Y.} and \text{Wang, Z.} (2019{\natexlab{b}}).
\newblock A theoretical analysis of deep {Q}-learning.
\newblock \textit{arXiv preprint arXiv:1901.00137}.

\bibitem[{Yu et~al.(2009)Yu, Mannor and Shimkin}]{yu2009markov}
\text{Yu, J.~Y.}, \text{Mannor, S.} and \text{Shimkin, N.} (2009).
\newblock {M}arkov decision processes with arbitrary reward processes.
\newblock \textit{Mathematics of Operations Research}, \textbf{34} 737--757.

\bibitem[{Zhou et~al.(2020)Zhou, He and Gu}]{zhou2020provably}
\text{Zhou, D.}, \text{He, J.} and \text{Gu, Q.} (2020).
\newblock Provably efficient reinforcement learning for discounted mdps with
  feature mapping.
\newblock \textit{arXiv preprint arXiv:2006.13165}.

\bibitem[{Zimin and Neu(2013)}]{zimin2013online}
\text{Zimin, A.} and \text{Neu, G.} (2013).
\newblock Online learning in episodic {M}arkovian decision processes by
  relative entropy policy search.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\end{thebibliography}
