\begin{thebibliography}{17}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 263--272. JMLR. org, 2017.

\bibitem[Bartlett and Tewari(2009)]{bartlett2009regal}
Peter~L Bartlett and Ambuj Tewari.
\newblock Regal: A regularization based algorithm for reinforcement learning in
  weakly communicating mdps.
\newblock In \emph{Proceedings of the Twenty-Fifth Conference on Uncertainty in
  Artificial Intelligence}, pages 35--42. AUAI Press, 2009.

\bibitem[Burnetas and Katehakis(1997)]{burnetas1997optimal}
Apostolos~N Burnetas and Michael~N Katehakis.
\newblock Optimal adaptive policies for markov decision processes.
\newblock \emph{Mathematics of Operations Research}, 22\penalty0 (1):\penalty0
  222--255, 1997.

\bibitem[Dann and Brunskill(2015)]{dann2015sample}
Christoph Dann and Emma Brunskill.
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2818--2826, 2015.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Christoph Dann, Tor Lattimore, and Emma Brunskill.
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5713--5723, 2017.

\bibitem[Dann et~al.(2018)Dann, Li, Wei, and Brunskill]{dann2018policy}
Christoph Dann, Lihong Li, Wei Wei, and Emma Brunskill.
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1811.03056}, 2018.

\bibitem[Garivier et~al.(2018)Garivier, M{\'e}nard, and
  Stoltz]{garivier2018explore}
Aur{\'e}lien Garivier, Pierre M{\'e}nard, and Gilles Stoltz.
\newblock Explore first, exploit next: The true shape of regret in bandit
  problems.
\newblock \emph{Mathematics of Operations Research}, 2018.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Apr):\penalty0 1563--1600, 2010.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is q-learning provably efficient?
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4868--4878, 2018.

\bibitem[Maurer and Pontil(2009)]{maurer2009empirical}
Andreas Maurer and Massimiliano Pontil.
\newblock Empirical bernstein bounds and sample variance penalization.
\newblock \emph{arXiv preprint arXiv:0907.3740}, 2009.

\bibitem[Ok et~al.(2018)Ok, Proutiere, and Tranos]{ok2018exploration}
Jungseul Ok, Alexandre Proutiere, and Damianos Tranos.
\newblock Exploration in structured reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8888--8896, 2018.

\bibitem[Osband and Van~Roy(2016)]{osband2016lower}
Ian Osband and Benjamin Van~Roy.
\newblock On lower bounds for regret in reinforcement learning.
\newblock \emph{stat}, 1050:\penalty0 9, 2016.

\bibitem[Simchowitz et~al.(2016)Simchowitz, Jamieson, and
  Recht]{simchowitz2016best}
Max Simchowitz, Kevin Jamieson, and Benjamin Recht.
\newblock Best-of-k-bandits.
\newblock In \emph{Conference on Learning Theory}, pages 1440--1489, 2016.

\bibitem[Song and Sun(2019)]{song2019efficient}
Zhao Song and Wen Sun.
\newblock Efficient model-free reinforcement learning in metric spaces.
\newblock \emph{arXiv preprint arXiv:1905.00475}, 2019.

\bibitem[Tewari(2007)]{tewari2007reinforcement}
Ambuj Tewari.
\newblock \emph{Reinforcement learning in large or unknown MDPs}.
\newblock University of California, Berkeley, 2007.

\bibitem[Tewari and Bartlett(2008)]{tewari2008optimistic}
Ambuj Tewari and Peter~L Bartlett.
\newblock Optimistic linear programming gives logarithmic regret for
  irreducible mdps.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1505--1512, 2008.

\bibitem[Zanette and Brunskill(2019)]{zanette2019tighter}
Andrea Zanette and Emma Brunskill.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock \emph{arXiv preprint arXiv:1901.00210}, 2019.

\end{thebibliography}
