\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and
  Szepesv{\'a}ri]{DBLP:conf/nips/Abbasi-YadkoriPS11}
Abbasi-Yadkori, Yasin, P{\'a}l, D{\'a}vid, and Szepesv{\'a}ri, Csaba.
\newblock {Improved Algorithms for Linear Stochastic Bandits}.
\newblock In \emph{NIPS}, pp.\  2312--2320, 2011.

\bibitem[{Abramowitz} \& {Stegun}(1964){Abramowitz} and
  {Stegun}]{abramowitz+stegun}
{Abramowitz}, Milton and {Stegun}, Irene~A.
\newblock \emph{{Handbook of Mathematical Functions with Formulas, Graphs, and
  Mathematical Tables}}.
\newblock Dover, New York, 1964.

\bibitem[Agrawal \& Goyal(2012)Agrawal and Goyal]{AgrawalG12}
Agrawal, Shipra and Goyal, Navin.
\newblock {Analysis of Thompson Sampling for the Multi-armed Bandit Problem}.
\newblock In \emph{COLT}, 2012.

\bibitem[Agrawal \& Goyal(2013{\natexlab{a}})Agrawal and
  Goyal]{AgrawalGoyalICML13}
Agrawal, Shipra and Goyal, Navin.
\newblock {Thompson Sampling for Contextual Bandits with Linear Payoffs}.
\newblock \emph{ICML}, 2013{\natexlab{a}}.

\bibitem[Agrawal \& Goyal(2013{\natexlab{b}})Agrawal and Goyal]{arxiv:basicMAB}
Agrawal, Shipra and Goyal, Navin.
\newblock {Further Optimal Regret Bounds for Thompson Sampling}.
\newblock \emph{AISTATS}, 2013{\natexlab{b}}.

\bibitem[Auer(2002)]{DBLP:journals/jmlr/Auer02}
Auer, Peter.
\newblock {Using Confidence Bounds for Exploitation-Exploration Trade-offs}.
\newblock \emph{Journal of Machine Learning Research}, 3:\penalty0 397--422,
  2002.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, Freund, and Schapire]{AuerCFS02}
Auer, Peter, Cesa-Bianchi, Nicol{\`o}, Freund, Yoav, and Schapire, Robert~E.
\newblock {The Nonstochastic Multiarmed Bandit Problem}.
\newblock \emph{SIAM J. Comput.}, 32\penalty0 (1):\penalty0 48--77, 2002.

\bibitem[Bubeck et~al.(2012)Bubeck, Cesa-Bianchi, and Kakade]{bubeck12}
Bubeck, S{\'e}bastien, Cesa-Bianchi, Nicol{\`o}, and Kakade, Sham~M.
\newblock {Towards minimax policies for online linear optimization with bandit
  feedback}.
\newblock \emph{Proceedings of the 25th Conference on Learning Theory (COLT)},
  pp.\  1--14, 2012.

\bibitem[Chapelle \& Li(2011)Chapelle and Li]{DBLP:conf/nips/ChapelleL11}
Chapelle, Olivier and Li, Lihong.
\newblock {An Empirical Evaluation of Thompson Sampling}.
\newblock In \emph{NIPS}, pp.\  2249--2257, 2011.

\bibitem[Chapelle \& Li(2012)Chapelle and Li]{ChapelleL12}
Chapelle, Olivier and Li, Lihong.
\newblock {Open Problem: Regret Bounds for Thompson Sampling}.
\newblock In \emph{COLT}, 2012.

\bibitem[Chu et~al.(2011)Chu, Li, Reyzin, and
  Schapire]{DBLP:journals/jmlr/ChuLRS11}
Chu, Wei, Li, Lihong, Reyzin, Lev, and Schapire, Robert~E.
\newblock {Contextual Bandits with Linear Payoff Functions}.
\newblock \emph{Journal of Machine Learning Research - Proceedings Track},
  15:\penalty0 208--214, 2011.

\bibitem[Dani et~al.(2008)Dani, Hayes, and Kakade]{DBLP:conf/colt/DaniHK08}
Dani, Varsha, Hayes, Thomas~P., and Kakade, Sham~M.
\newblock {Stochastic Linear Optimization under Bandit Feedback}.
\newblock In \emph{COLT}, pp.\  355--366, 2008.

\bibitem[Filippi et~al.(2010)Filippi, Capp{\'e}, Garivier, and
  Szepesv{\'a}ri]{DBLP:conf/nips/FilippiCGS10}
Filippi, Sarah, Capp{\'e}, Olivier, Garivier, Aur{\'e}lien, and Szepesv{\'a}ri,
  Csaba.
\newblock {Parametric Bandits: The Generalized Linear Case}.
\newblock In \emph{NIPS}, pp.\  586--594, 2010.

\bibitem[Graepel et~al.(2010)Graepel, Candela, Borchert, and
  Herbrich]{GraepelCBH10}
Graepel, Thore, Candela, Joaquin~Qui{\~n}onero, Borchert, Thomas, and Herbrich,
  Ralf.
\newblock {Web-Scale Bayesian Click-Through rate Prediction for Sponsored
  Search Advertising in Microsoft's Bing Search Engine}.
\newblock In \emph{ICML}, pp.\  13--20, 2010.

\bibitem[Granmo(2010)]{Granmo}
Granmo, O.-C.
\newblock {Solving Two-Armed Bernoulli Bandit Problems Using a Bayesian
  Learning Automaton}.
\newblock \emph{International Journal of Intelligent Computing and Cybernetics
  (IJICC)}, 3\penalty0 (2):\penalty0 207--234, 2010.

\bibitem[Kaelbling(1994)]{DBLP:journals/ml/Kaelbling94}
Kaelbling, Leslie~Pack.
\newblock {Associative Reinforcement Learning: Functions in k-DNF}.
\newblock \emph{Machine Learning}, 15\penalty0 (3):\penalty0 279--298, 1994.

\bibitem[Kaufmann et~al.(2012)Kaufmann, Korda, and Munos]{KaufmannMunos12}
Kaufmann, Emilie, Korda, Nathaniel, and Munos, R{\'e}mi.
\newblock {Thompson Sampling: An Optimal Finite Time Analysis}.
\newblock \emph{ALT}, 2012.

\bibitem[Lai \& Robbins(1985)Lai and Robbins]{LaiR}
Lai, T.~L. and Robbins, H.
\newblock {Asymptotically efficient adaptive allocation rules}.
\newblock \emph{Advances in Applied Mathematics}, 6:\penalty0 4--22, 1985.

\bibitem[Langford \& Zhang(2007)Langford and Zhang]{LangfordZ07}
Langford, John and Zhang, Tong.
\newblock {The Epoch-Greedy Algorithm for Multi-armed Bandits with Side
  Information}.
\newblock In \emph{NIPS}, 2007.

\bibitem[May \& Leslie(2011)May and Leslie]{MayL}
May, Benedict~C. and Leslie, David~S.
\newblock {Simulation studies in optimistic Bayesian sampling in
  contextual-bandit problems}.
\newblock Technical Report 11:02, Statistics Group, Department of Mathematics,
  University of Bristol, 2011.

\bibitem[May et~al.(2011)May, Korda, Lee, and Leslie]{MayKLL}
May, Benedict~C., Korda, Nathan, Lee, Anthony, and Leslie, David~S.
\newblock {Optimistic Bayesian sampling in contextual-bandit problems}.
\newblock Technical Report 11:01, Statistics Group, Department of Mathematics,
  University of Bristol, 2011.

\bibitem[Ortega \& Braun(2010)Ortega and Braun]{OrtegaB10}
Ortega, Pedro~A. and Braun, Daniel~A.
\newblock {Linearly Parametrized Bandits}.
\newblock \emph{Journal of Artificial Intelligence Research}, 38:\penalty0
  475--511, 2010.

\bibitem[Sarkar(1991)]{Sarkar91}
Sarkar, Jyotirmoy.
\newblock {One-armed badit problem with covariates}.
\newblock \emph{The Annals of Statistics}, 19(4):\penalty0 1978--2002, 1991.

\bibitem[Scott(2010)]{Scott}
Scott, S.
\newblock {A modern Bayesian look at the multi-armed bandit}.
\newblock \emph{Applied Stochastic Models in Business and Industry},
  26:\penalty0 639--658, 2010.

\bibitem[Strehl et~al.(2006)Strehl, Mesterharm, Littman, and
  Hirsh]{DBLP:conf/icml/StrehlMLH06}
Strehl, Alexander~L., Mesterharm, Chris, Littman, Michael~L., and Hirsh, Haym.
\newblock {Experience-efficient learning in associative bandit problems}.
\newblock In \emph{ICML}, pp.\  889--896, 2006.

\bibitem[Strens(2000)]{DBLP:conf/icml/Strens00}
Strens, Malcolm J.~A.
\newblock {A Bayesian Framework for Reinforcement Learning}.
\newblock In \emph{ICML}, pp.\  943--950, 2000.

\bibitem[Thompson(1933)]{Thompson}
Thompson, William~R.
\newblock {On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples}.
\newblock \emph{Biometrika}, 25\penalty0 (3-4):\penalty0 285--294, 1933.

\bibitem[Woodroofe(1979)]{Wood79}
Woodroofe, Michael.
\newblock {A one-armed bandit problem with a concomitant variable}.
\newblock \emph{Journal of the American Statistics Association},
  74(368):\penalty0 799--806, 1979.

\bibitem[Wyatt(1997)]{Wyatt97}
Wyatt, Jeremy.
\newblock \emph{{Exploration and Inference in Learning from Reinforcement}}.
\newblock PhD thesis, Department of Artificial Intelligence, University of
  Edinburgh, 1997.

\end{thebibliography}
