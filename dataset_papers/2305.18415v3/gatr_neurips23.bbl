\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anderson et~al.(2019)Anderson, Hy, and Kondor]{anderson2019cormorant}
Brandon Anderson, Truong~Son Hy, and Risi Kondor.
\newblock Cormorant: Covariant molecular neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Baevski and Auli(2018)]{baevski2018adaptive}
Alexei Baevski and Michael Auli.
\newblock Adaptive input representations for neural language modeling.
\newblock \emph{arXiv:1809.10853}, 2018.

\bibitem[Batatia et~al.(2022)Batatia, Kovacs, Simm, Ortner, and
  Csanyi]{Batatia2022-ab}
Ilyes Batatia, David~Peter Kovacs, Gregor N~C Simm, Christoph Ortner, and Gabor
  Csanyi.
\newblock {MACE}: Higher order equivariant message passing neural networks for
  fast and accurate force fields.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Batzner et~al.(2022)Batzner, Musaelian, Sun, Geiger, Mailoa,
  Kornbluth, Molinari, Smidt, and Kozinsky]{Batzner2022-zr}
Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan~P Mailoa,
  Mordechai Kornbluth, Nicola Molinari, Tess~E Smidt, and Boris Kozinsky.
\newblock {E(3)}-equivariant graph neural networks for data-efficient and
  accurate interatomic potentials.
\newblock \emph{Nat. Commun.}, 13\penalty0 (1):\penalty0 2453, May 2022.

\bibitem[Brandstetter et~al.(2022{\natexlab{a}})Brandstetter, Berg, Welling,
  and Gupta]{brandstetter2022clifford}
Johannes Brandstetter, Rianne van~den Berg, Max Welling, and Jayesh~K Gupta.
\newblock Clifford neural layers for {PDE} modeling.
\newblock \emph{arXiv:2209.04934}, 2022{\natexlab{a}}.

\bibitem[Brandstetter et~al.(2022{\natexlab{b}})Brandstetter, Hesselink,
  van~der Pol, Bekkers, and Welling]{Brandstetter2022-hw}
Johannes Brandstetter, Rob Hesselink, Elise van~der Pol, Erik~J Bekkers, and
  Max Welling.
\newblock Geometric and physical quantities improve {E(3)} equivariant message
  passing.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{b}}.

\bibitem[Brehmer et~al.(2023)Brehmer, Bose, De~Haan, and
  Cohen]{brehmer2023edgi}
Johann Brehmer, Joey Bose, Pim De~Haan, and Taco Cohen.
\newblock {EDGI}: Equivariant diffusion for planning with embodied agents.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~37, 2023.

\bibitem[Bronstein et~al.(2021)Bronstein, Bruna, Cohen, and Veli{\v
  c}kovi{\'c}]{bronstein2021geometric}
Michael~M Bronstein, Joan Bruna, Taco Cohen, and Petar Veli{\v c}kovi{\'c}.
\newblock Geometric deep learning: Grids, groups, graphs, geodesics, and
  gauges.
\newblock 2021.

\bibitem[Chen et~al.(2016)Chen, Xu, Zhang, and Guestrin]{chen2016training}
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
\newblock Training deep nets with sublinear memory cost.
\newblock \emph{arXiv:1604.06174}, 2016.

\bibitem[Clifford(1878)]{clifford1878applications}
William~Kingdon Clifford.
\newblock {Applications of Grassmann's Extensive Algebra}.
\newblock \emph{Amer. J. Math.}, 1\penalty0 (4):\penalty0 350--358, 1878.

\bibitem[Cohen(2021)]{cohen2021equivariant}
Taco Cohen.
\newblock \emph{Equivariant Convolutional Networks}.
\newblock PhD thesis, University of Amsterdam, 2021.

\bibitem[Cohen and Welling(2016)]{cohen2016group}
Taco Cohen and Max Welling.
\newblock Group equivariant convolutional networks.
\newblock In \emph{{International Conference on Machine Learning}}, pages
  2990--2999. PMLR, 2016.

\bibitem[Coumans and Bai(2016--2019)]{coumans2019}
Erwin Coumans and Yunfei Bai.
\newblock {PyBullet, a Python module for physics simulation for games, robotics
  and machine learning}.
\newblock \url{http://pybullet.org}, 2016--2019.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and
  R{\'e}]{dao2022flashattention}
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock {FlashAttention}: Fast and memory-efficient exact attention with
  {IO}-awareness.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 16344--16359, 2022.

\bibitem[De~Haan et~al.(2021)De~Haan, Weiler, Cohen, and Welling]{de2020gauge}
Pim De~Haan, Maurice Weiler, Taco Cohen, and Max Welling.
\newblock Gauge equivariant mesh {CNNs}: Anisotropic convolutions on geometric
  graphs.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Doran and Lasenby(2003)]{doran2003geometric}
C~Doran and A~Lasenby.
\newblock \emph{Geometric algebra for physicists}.
\newblock Cambridge University Press, 2003.

\bibitem[Dorst(2020)]{dorst2020guidedtour}
Leo Dorst.
\newblock A guided tour to the plane-based geometric algebra pga.
\newblock 2020.
\newblock URL \url{https://geometricalgebra.org/downloads/PGA4CS.pdf}.

\bibitem[Dorst et~al.(2007)Dorst, Fontijne, and Mann]{DorstFontijneMann07}
Leo Dorst, Daniel Fontijne, and Stephen Mann.
\newblock \emph{Geometric Algebra for Computer Science: An Object-oriented
  Approach to Geometry}.
\newblock Morgan Kaufmann Series in Computer Graphics. Morgan Kaufmann,
  Amsterdam, 2007.
\newblock ISBN 978-0-12-369465-2.

\bibitem[Frank et~al.(2022)Frank, Unke, and Muller]{Frank2022-fu}
Thorben Frank, Oliver~Thorsten Unke, and Klaus~Robert Muller.
\newblock So3krates: Equivariant attention for interactions on arbitrary
  length-scales in molecular systems.
\newblock October 2022.

\bibitem[Fuchs et~al.(2020)Fuchs, Worrall, Fischer, and Welling]{Fuchs2020-bw}
Fabian~B Fuchs, Daniel~E Worrall, Volker Fischer, and Max Welling.
\newblock {SE(3)-Transformers}: {3D} {Roto-Translation} equivariant attention
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{{International Conference on Machine Learning}}, pages
  2052--2062. PMLR, 2019.

\bibitem[Grassmann(1844)]{grassmann1844lineale}
Hermann Grassmann.
\newblock \emph{Die lineale Ausdehnungslehre}.
\newblock Otto Wigand, Leipzig, 1844.

\bibitem[Hendrycks and Gimpel(2016)]{hendrycks2016gaussian}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv:1606.08415}, 2016.

\bibitem[Ho et~al.(2019)Ho, Kalchbrenner, Weissenborn, and Salimans]{Ho2019-ci}
Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans.
\newblock Axial attention in multidimensional transformers.
\newblock \emph{arXiv:1912.12180 [cs]}, December 2019.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{Ho2020-mj}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock In \emph{Neural Information Processing Systems}, 2020.

\bibitem[Huang et~al.(2022)Huang, Wang, Walters, and Platt]{Huang2022-co}
Haojie Huang, Dian Wang, Robin Walters, and Robert Platt.
\newblock Equivariant transporter network.
\newblock In \emph{Proceedings of Robotics: Science and Systems}, 2022.

\bibitem[Janner et~al.(2022)Janner, Du, Tenenbaum, and
  Levine]{janner2022diffuser}
Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine.
\newblock Planning with diffusion for flexible behavior synthesis.
\newblock In \emph{{International Conference on Machine Learning}}, 2022.

\bibitem[Jumper et~al.(2021)Jumper, Evans, Pritzel, Green, Figurnov,
  Ronneberger, Tunyasuvunakool, Bates, {\v{Z}}{\'\i}dek, Potapenko,
  et~al.]{jumper2021highly}
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov,
  Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin
  {\v{Z}}{\'\i}dek, Anna Potapenko, et~al.
\newblock Highly accurate protein structure prediction with alphafold.
\newblock \emph{Nature}, 596\penalty0 (7873):\penalty0 583--589, 2021.

\bibitem[K{\"o}hler et~al.(2020)K{\"o}hler, Klein, and
  No{\'e}]{kohler2020equivariant}
Jonas K{\"o}hler, Leon Klein, and Frank No{\'e}.
\newblock Equivariant flows: exact likelihood generative learning for symmetric
  densities.
\newblock In \emph{{International Conference on Machine Learning}}, pages
  5361--5370. PMLR, 2020.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1179--1191, 2020.

\bibitem[Lafarge et~al.(2021)Lafarge, Bekkers, Pluim, Duits, and
  Veta]{Lafarge2021roto}
Maxime~W Lafarge, Erik~J Bekkers, Josien P~W Pluim, Remco Duits, and Mitko
  Veta.
\newblock Roto-translation equivariant convolutional networks: Application to
  histopathology image analysis.
\newblock \emph{Med. Image Anal.}, 68, 2021.

\bibitem[Lefaudeux et~al.(2022)Lefaudeux, Massa, Liskovich, Xiong, Caggiano,
  Naren, Xu, Hu, Tintore, Zhang, Labatut, and Haziza]{xFormers2022}
Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio
  Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick
  Labatut, and Daniel Haziza.
\newblock {xFormers}: A modular and hackable {Transformer} modelling library.
\newblock \url{https://github.com/facebookresearch/xformers}, 2022.

\bibitem[Liao and Smidt(2022)]{liao2022equiformer}
Yi-Lun Liao and Tess Smidt.
\newblock Equiformer: Equivariant graph attention transformer for 3d atomistic
  graphs.
\newblock \emph{arXiv:2206.11990}, 2022.

\bibitem[Lounesto(2001)]{Lounesto2001-vu}
Pertti Lounesto.
\newblock \emph{Clifford Algebras and Spinors}.
\newblock London Mathematical Society Lecture Note. Cambridge University Press,
  2001.

\bibitem[Milesi(2021)]{milesi2021}
Alexandre Milesi.
\newblock Accelerating {SE(3)-Transformers} training using an {NVIDIA}
  open-source model implementation.
\newblock
  \url{https://developer.nvidia.com/blog/accelerating-se3-transformers-training-using-an-nvidia-open-source-model-implementation/},
  2021.

\bibitem[Qi et~al.(2017)Qi, Yi, Su, and Guibas]{qi2017pointnet}
Charles~Ruizhongtai Qi, Li~Yi, Hao Su, and Leonidas~J Guibas.
\newblock Pointnet++: Deep hierarchical feature learning on point sets in a
  metric space.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Rabe and Staats(2021)]{rabe2021self}
Markus~N Rabe and Charles Staats.
\newblock Self-attention does not need {$O(n^2)$} memory.
\newblock \emph{arXiv:2112.05682}, 2021.

\bibitem[Roelfs and De~Keninck(2021)]{roelfs2021graded}
Martin Roelfs and Steven De~Keninck.
\newblock Graded symmetry groups: plane and simple.
\newblock \emph{arXiv:2107.03771}, 2021.

\bibitem[Romero et~al.(2020)Romero, Bekkers, Tomczak, and
  Hoogendoorn]{Romero2020-tm}
David~W Romero, Erik~J Bekkers, Jakub~M Tomczak, and Mark Hoogendoorn.
\newblock Attentive group equivariant convolutional networks.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, volume 119, pages 8188--8199. JMLR.org, July 2020.

\bibitem[Ruhe et~al.(2023{\natexlab{a}})Ruhe, Brandstetter, and
  Forr\'e]{ruhe2023clifford}
David Ruhe, Johannes Brandstetter, and Patrick Forr\'e.
\newblock Clifford group equivariant neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~37, 2023{\natexlab{a}}.

\bibitem[Ruhe et~al.(2023{\natexlab{b}})Ruhe, Gupta, de~Keninck, Welling, and
  Brandstetter]{ruhe2023geometric}
David Ruhe, Jayesh~K Gupta, Steven de~Keninck, Max Welling, and Johannes
  Brandstetter.
\newblock Geometric clifford algebra networks.
\newblock In \emph{{International Conference on Machine Learning}},
  2023{\natexlab{b}}.

\bibitem[Satorras et~al.(2021)Satorras, Hoogeboom, and
  Welling]{Satorras2021-hf}
V{\'\i}ctor~Garcia Satorras, Emiel Hoogeboom, and Max Welling.
\newblock {E(n)} equivariant graph neural networks.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 9323--9332. PMLR,
  2021.

\bibitem[Shazeer(2019)]{shazeer2019fast}
Noam Shazeer.
\newblock Fast transformer decoding: One write-head is all you need.
\newblock \emph{arXiv:1911.02150}, 2019.

\bibitem[Simeonov et~al.(2022)Simeonov, Du, Tagliasacchi, Tenenbaum, Rodriguez,
  Agrawal, and Sitzmann]{Simeonov2022-dn}
Anthony Simeonov, Yilun Du, Andrea Tagliasacchi, Joshua~B Tenenbaum, Alberto
  Rodriguez, Pulkit Agrawal, and Vincent Sitzmann.
\newblock Neural descriptor fields: {SE(3)-Equivariant} object representations
  for manipulation.
\newblock In \emph{{ICRA}}, 2022.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and
  Ganguli]{sohl2015deep}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In \emph{{International Conference on Machine Learning}}, pages
  2256--2265. PMLR, 2015.

\bibitem[Spellings(2021)]{spellings2021geometric}
Matthew Spellings.
\newblock Geometric algebra attention networks for small point clouds.
\newblock \emph{arXiv:2110.02393}, 2021.

\bibitem[Su et~al.(2021)Su, Lu, Pan, Wen, and Liu]{rope-paper}
Jianlin Su, Yu~Lu, Shengfeng Pan, Bo~Wen, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{arXiv:2104.09864}, 2021.

\bibitem[Suk et~al.(2022)Suk, de~Haan, Lippe, Brune, and
  Wolterink]{suk2022mesh}
Julian Suk, Pim de~Haan, Phillip Lippe, Christoph Brune, and Jelmer~M
  Wolterink.
\newblock Mesh neural networks for se (3)-equivariant hemodynamics estimation
  on the artery wall.
\newblock \emph{arXiv:2212.05023}, 2022.

\bibitem[Thomas et~al.(2018)Thomas, Smidt, Kearnes, Yang, Li, Kohlhoff, and
  Riley]{thomas2018tensor}
Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li~Li, Kai Kohlhoff,
  and Patrick Riley.
\newblock Tensor field networks: Rotation-and translation-equivariant neural
  networks for 3d point clouds.
\newblock \emph{arXiv:1802.08219}, 2018.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{NIPS2017_3f5ee243}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{{Advances in Neural Information Processing Systems}},
  volume~30, 2017.

\bibitem[Wang et~al.(2021)Wang, Walters, Zhu, and Platt]{Wang2021-vn}
Dian Wang, Robin Walters, Xupeng Zhu, and Robert Platt.
\newblock Equivariant {Q} learning in spatial action spaces, 2021.

\bibitem[Wang et~al.(2022)Wang, Jia, Zhu, Walters, and Platt]{Wang2022-ne}
Dian Wang, Mingxi Jia, Xupeng Zhu, Robin Walters, and Robert Platt.
\newblock {On-Robot} learning with equivariant models.
\newblock In \emph{{CoRL}}, 2022.

\bibitem[Winkels and Cohen(2019)]{Winkels2019pulmonary}
Marysia Winkels and Taco Cohen.
\newblock Pulmonary nodule detection in {CT} scans with equivariant {CNNs}.
\newblock \emph{MIA}, 2019.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan,
  Wang, and Liu]{xiong2020layer}
Ruibin Xiong, Yunchang Yang, Di~He, Kai Zheng, Shuxin Zheng, Chen Xing,
  Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu.
\newblock On layer normalization in the transformer architecture.
\newblock In \emph{{International Conference on Machine Learning}}, pages
  10524--10533. PMLR, 2020.

\bibitem[Zhu et~al.(2022)Zhu, Wang, Biza, Su, Walters, and Platt]{Zhu2022-wm}
Xupeng Zhu, Dian Wang, Ondrej Biza, Guanang Su, Robin Walters, and Robert
  Platt.
\newblock Sample efficient grasp learning using equivariant models.
\newblock In \emph{Robotics: Science and Systems {XVIII}}. Robotics: Science
  and Systems Foundation, June 2022.

\end{thebibliography}
