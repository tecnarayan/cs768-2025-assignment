\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arbel et~al.(2018)Arbel, Sutherland, Binkowski, and Gretton]{mmdgan}
M.~Arbel, D.~J. Sutherland, M.~Binkowski, and A.~Gretton.
\newblock On gradient regularizers for {MMD} {GAN}s.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and Bottou]{wgan}
M.~Arjovsky, S.~Chintala, and L.~Bottou.
\newblock {W}asserstein generative adversarial networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Bachman(2016)]{hierarchiVAE}
P.~Bachman.
\newblock An architecture for deep, hierarchical generative models.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Bishop(2006)]{bishop06patrec}
C.~Bishop.
\newblock \emph{Pattern recognition and machine learning}.
\newblock {Springer-Verlag}, 2006.

\bibitem[Brock et~al.(2019)Brock, Donahue, and Simonyan]{biggan}
A.~Brock, J.~Donahue, and K.~Simonyan.
\newblock Large scale {GAN} training for high fidelity natural image synthesis.
\newblock In \emph{ICLR}, 2019.

\bibitem[Chen et~al.(2018)Chen, Dai, Pu, Zhou, Li, Su, Chen, and Carin]{symVAE}
L.~Chen, S.~Dai, Y.~Pu, E.~Zhou, C.~Li, Q.~Su, C.~Chen, and L.~Carin.
\newblock Symmetric variational autoencoder and connections to adversarial
  learning.
\newblock In \emph{AISTATS}, 2018.

\bibitem[Chen et~al.(2017)Chen, Kingma, Salimans, Duan, Dhariwal, Schulman,
  Sutskever, and Abbeel]{chen17iclr}
X.~Chen, D.~Kingma, T.~Salimans, Y.~Duan, P.~Dhariwal, J.~Schulman,
  I.~Sutskever, and P.~Abbeel.
\newblock Variational lossy autoencoder.
\newblock In \emph{ICLR}, 2017.

\bibitem[De~Vries et~al.(2017)De~Vries, Strub, Mary, Larochelle, Pietquin, and
  Courville]{de2017modulating}
H.~De~Vries, F.~Strub, J.~Mary, H.~Larochelle, O.~Pietquin, and A.~Courville.
\newblock Modulating early visual processing by language.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Dinh et~al.(2017)Dinh, Sohl{-}Dickstein, and Bengio]{realnvp}
L.~Dinh, J.~Sohl{-}Dickstein, and S.~Bengio.
\newblock Density estimation using {R}eal {NVP}.
\newblock In \emph{ICLR}, 2017.

\bibitem[Donahue et~al.(2017)Donahue, Kr\"ahenb\"uhl, and
  Darrell]{donahue17iclr}
J.~Donahue, P.~Kr\"ahenb\"uhl, and T.~Darrell.
\newblock Adversarial feature learning.
\newblock In \emph{ICLR}, 2017.

\bibitem[Dorta et~al.(2018)Dorta, Vicente, Agapito, Campbell, and
  Simpson]{dorta18cvpr}
G.~Dorta, S.~Vicente, L.~Agapito, N.~Campbell, and I.~Simpson.
\newblock Structured uncertainty prediction networks.
\newblock In \emph{CVPR}, 2018.

\bibitem[Dumoulin et~al.(2017{\natexlab{a}})Dumoulin, Belghazi, Poole, Lamb,
  Arjovsky, Mastropietro, and Courville]{dumoulin17iclr}
V.~Dumoulin, I.~Belghazi, B.~Poole, A.~Lamb, M.~Arjovsky, O.~Mastropietro, and
  A.~Courville.
\newblock Adversarially learned inference.
\newblock In \emph{ICLR}, 2017{\natexlab{a}}.

\bibitem[Dumoulin et~al.(2017{\natexlab{b}})Dumoulin, Shlens, and
  Kudlur]{dumoulin2017learned}
V.~Dumoulin, J.~Shlens, and M.~Kudlur.
\newblock A learned representation for artistic style.
\newblock In \emph{ICLR}, 2017{\natexlab{b}}.

\bibitem[Elfeki et~al.(2018)Elfeki, Couprie, Riviere, and Elhoseiny]{gdpp}
M.~Elfeki, C.~Couprie, M.~Riviere, and M.~Elhoseiny.
\newblock {GDPP}: Learning diverse generations using determinantal point
  processes.
\newblock In \emph{arxiv.org/pdf/1812.00068}, 2018.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{gan}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{NeurIPS}, 2014.

\bibitem[Grover et~al.(2018)Grover, Dhar, and Ermon]{flowgan}
A.~Grover, M.~Dhar, and S.~Ermon.
\newblock {Flow-GAN}: Combining maximum likelihood and adversarial learning in
  generative models.
\newblock In \emph{AAAI Press}, 2018.

\bibitem[Gulrajani et~al.(2017{\natexlab{a}})Gulrajani, Ahmed, Arjovsky,
  Dumoulin, and Courville]{gulrajani17nips}
I.~Gulrajani, F.~Ahmed, M.~Arjovsky, V.~Dumoulin, and A.~Courville.
\newblock Improved training of {W}asserstein {GAN}s.
\newblock In \emph{NeurIPS}, 2017{\natexlab{a}}.

\bibitem[Gulrajani et~al.(2017{\natexlab{b}})Gulrajani, Kumar, Ahmed, Taiga,
  Visin, Vazquez, and Courville]{gulrajani17iclr}
I.~Gulrajani, K.~Kumar, F.~Ahmed, A.~A. Taiga, F.~Visin, D.~Vazquez, and
  A.~Courville.
\newblock {PixelVAE}: A latent variable model for natural images.
\newblock In \emph{ICLR}, 2017{\natexlab{b}}.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{fid}
M.~Heusel, H.~Ramsauer, T.~Unterthiner, B.~Nessler, and S.~Hochreiter.
\newblock {GAN}s trained by a two time-scale update rule converge to a local
  {N}ash equilibrium.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Ho et~al.(2019)Ho, Chen, Srinivas, Duan, and Abbeel]{Flow++}
J.~Ho, X.~Chen, A.~Srinivas, Y.~Duan, and P.~Abbeel.
\newblock Flow++: Improving flow-based generative models with variational
  dequantization and architecture design.
\newblock In \emph{ICML}, 2019.

\bibitem[Hou et~al.(2017)Hou, Shen, Sun, and Qiu]{featureConsistent}
X.~Hou, L.~Shen, K.~Sun, and G.~Qiu.
\newblock Deep feature consistent variational autoencoder.
\newblock In \emph{WACV}, 2017.

\bibitem[Karras et~al.(2018)Karras, Aila, Laine, and Lehtinen]{karras18iclr}
T.~Karras, T.~Aila, S.~Laine, and J.~Lehtinen.
\newblock Progressive growing of {GAN}s for improved quality, stability, and
  variation.
\newblock In \emph{ICLR}, 2018.

\bibitem[Kingma and Ba(2015)]{adam}
D.~Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}, 2015.

\bibitem[Kingma and Dhariwal(2018)]{Glow}
D.~Kingma and P.~Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Kingma and Welling(2014)]{vae}
D.~Kingma and M.~Welling.
\newblock Auto-encoding variational {B}ayes.
\newblock In \emph{ICLR}, 2014.

\bibitem[Kingma et~al.(2016)Kingma, Salimans, J{\'{o}}zefowicz, Chen,
  Sutskever, and Welling]{iaf}
D.~Kingma, T.~Salimans, R.~J{\'{o}}zefowicz, X.~Chen, I.~Sutskever, and
  M.~Welling.
\newblock Improving variational autoencoders with inverse autoregressive flow.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Larsen et~al.(2016)Larsen, S{\o}nderby, Larochelle, and
  Winther]{larsen16icml}
A.~Larsen, S.~S{\o}nderby, H.~Larochelle, and O.~Winther.
\newblock Autoencoding beyond pixels using a learned similarity metric.
\newblock In \emph{ICML}, 2016.

\bibitem[Li et~al.(2015)Li, Swersky, and Zemel]{mmd}
Y.~Li, K.~Swersky, and R.~Zemel.
\newblock Generative moment matching networks.
\newblock In \emph{ICML}, 2015.

\bibitem[Lin et~al.(2018)Lin, Khetan, Fanti, and Oh]{pacgan}
Z.~Lin, A.~Khetan, G.~Fanti, and S.~Oh.
\newblock {PacGAN}: The power of two samples in generative adversarial
  networks.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Litany et~al.(2018)Litany, Bronstein, Bronstein, and
  Makadia]{litany18cvpr}
O.~Litany, A.~Bronstein, M.~Bronstein, and A.~Makadia.
\newblock Deformable shape completion with graph convolutional autoencoders.
\newblock In \emph{CVPR}, 2018.

\bibitem[Lucas and Verbeek(2018)]{lucas18ecml}
T.~Lucas and J.~Verbeek.
\newblock Auxiliary guided autoregressive variational autoencoders.
\newblock In \emph{ECML}, 2018.

\bibitem[Lucas et~al.(2018)Lucas, Tallec, Ollivier, and Verbeek]{lucas18icml}
T.~Lucas, C.~Tallec, Y.~Ollivier, and J.~Verbeek.
\newblock Mixed batches and symmetric discriminators for {GAN} training.
\newblock In \emph{ICML}, 2018.

\bibitem[Makhzani et~al.(2016)Makhzani, Shlens, Jaitly, and
  Goodfellow]{makhzani16iclr}
A.~Makhzani, J.~Shlens, N.~Jaitly, and I.~Goodfellow.
\newblock Adversarial autoencoders.
\newblock In \emph{ICLR}, 2016.

\bibitem[Mathieu et~al.(2016)Mathieu, Couprie, and LeCun]{mathieu16iclr}
M.~Mathieu, C.~Couprie, and Y.~LeCun.
\newblock Deep multi-scale video prediction beyond mean square error.
\newblock In \emph{ICLR}, 2016.

\bibitem[Menick and Kalchbrenner(2019)]{highFidPixcnn}
J.~Menick and N.~Kalchbrenner.
\newblock Generating high fidelity images with subscale pixel networks and
  multidimensional upscaling.
\newblock In \emph{ICLR}, 2019.

\bibitem[Miyato and Koyama(2018)]{cgan}
T.~Miyato and M.~Koyama.
\newblock {cGANs} with projection discriminator.
\newblock In \emph{ICLR}, 2018.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and Yoshida]{sngan}
T.~Miyato, T.~Kataoka, M.~Koyama, and Y.~Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock In \emph{ICLR}, 2018.

\bibitem[Ramachandran et~al.(2017)Ramachandran, Paine, Khorrami, Babaeizadeh,
  Chang, Zhang, Hasegawa-Johnson, Campbell, and Huang]{fastpixcnn}
P.~Ramachandran, T.~Paine, P.~Khorrami, M.~Babaeizadeh, S.~Chang, Y.~Zhang,
  M.~Hasegawa-Johnson, R.~Campbell, and T.~Huang.
\newblock Fast generation for convolutional autoregressive models.
\newblock In \emph{ICLR workshop}, 2017.

\bibitem[Rezende and Mohamed(2015)]{rezende15icml}
D.~Rezende and S.~Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In \emph{ICML}, 2015.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and Wierstra]{rezende14icml}
D.~Rezende, S.~Mohamed, and D.~Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{ICML}, 2014.

\bibitem[Rosca et~al.(2017)Rosca, Lakshminarayanan, Warde-Farley, and
  Mohamed]{rosca2017variational}
M.~Rosca, B.~Lakshminarayanan, D.~Warde-Farley, and S.~Mohamed.
\newblock Variational approaches for auto-encoding generative adversarial
  networks.
\newblock \emph{arXiv preprint arXiv:1706.04987}, 2017.

\bibitem[Saatchi and Wilson(2017)]{bayesianGAN}
Y.~Saatchi and A.~Wilson.
\newblock Bayesian {GAN}.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Sajjadi et~al.(2018)Sajjadi, Bachem, Lucic, Bousquet, and
  Gelly]{prdEval}
M.~Sajjadi, O.~Bachem, M.~Lucic, O.~Bousquet, and S.~Gelly.
\newblock Assessing generative models via precision and recall.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Salimans and Kingma(2016)]{weightnorm}
T.~Salimans and D.~Kingma.
\newblock Weight normalization: {A} simple reparameterization to accelerate
  training of deep neural networks.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{salimans16nips}
T.~Salimans, I.~Goodfellow, W.~Zaremba, V.~Cheung, A.~Radford, and X.~Chen.
\newblock Improved techniques for training {GAN}s.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Salimans et~al.(2017)Salimans, Karpathy, Chen, and Kingma]{pixcnnpp}
T.~Salimans, A.~Karpathy, X.~Chen, and D.~Kingma.
\newblock {PixelCNN++}: Improving the {PixelCNN} with discretized logistic
  mixture likelihood and other modifications.
\newblock In \emph{ICLR}, 2017.

\bibitem[Salimans et~al.(2018)Salimans, Zhang, Radford, and Metaxas]{otgan}
T.~Salimans, H.~Zhang, A.~Radford, and D.~Metaxas.
\newblock Improving gans using optimal transport.
\newblock In \emph{ICLR}, 2018.

\bibitem[Shmelkov et~al.(2018)Shmelkov, Schmid, and Alahari]{Ktest}
K.~Shmelkov, C.~Schmid, and K.~Alahari.
\newblock How good is my {GAN}?
\newblock In \emph{ECCV}, 2018.

\bibitem[S{\o}nderby et~al.(2016)S{\o}nderby, Raiko, Maal{\o}e, S{\o}nderby,
  and Winther]{ladderVAE}
C.~S{\o}nderby, T.~Raiko, L.~Maal{\o}e, S.~S{\o}nderby, and O.~Winther.
\newblock Ladder variational autoencoders.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[S{\o}nderby et~al.(2017)S{\o}nderby, Caballero, Theis, Shi, and
  Husz{\'{a}}r]{amap}
C.~S{\o}nderby, J.~Caballero, L.~Theis, W.~Shi, and F.~Husz{\'{a}}r.
\newblock Amortised {MAP} inference for image super-resolution.
\newblock In \emph{ICLR}, 2017.

\bibitem[Thanh-Tung et~al.(2019)Thanh-Tung, Tran, and Venkatesh]{centeredwgp}
H.~Thanh-Tung, T.~Tran, and S.~Venkatesh.
\newblock Improving generalization and stability of generative adversarial
  networks.
\newblock In \emph{ICLR}, 2019.

\bibitem[Ulyanov et~al.(2018)Ulyanov, Vedaldi, and Lempitsky]{ulyanov18aaai}
D.~Ulyanov, A.~Vedaldi, and V.~Lempitsky.
\newblock It takes (only) two: Adversarial generator-encoder networks.
\newblock In \emph{AAAI}, 2018.

\bibitem[van~den Oord et~al.(2016)van~den Oord, Kalchbrenner, and
  Kavukcuoglu]{pixrnn}
A.~van~den Oord, N.~Kalchbrenner, and K.~Kavukcuoglu.
\newblock Pixel recurrent neural networks.
\newblock In \emph{ICML}, 2016.

\bibitem[Zhang et~al.(2019)Zhang, Goodfellow, Metaxas, and Odena]{sagan}
H.~Zhang, I.~Goodfellow, D.~Metaxas, and A.~Odena.
\newblock Self-attention generative adversarial networks.
\newblock In \emph{ICML}, 2019.

\end{thebibliography}
