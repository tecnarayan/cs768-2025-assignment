\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Chu, Goodfellow, McMahan, Mironov, Talwar,
  and Zhang]{abadi2016deep}
Abadi, M., Chu, A., Goodfellow, I., McMahan, H.~B., Mironov, I., Talwar, K.,
  and Zhang, L.
\newblock Deep learning with differential privacy.
\newblock In \emph{Proceedings of the 2016 ACM SIGSAC conference on computer
  and communications security}, pp.\  308--318, 2016.

\bibitem[Bao et~al.(2021)Bao, Dong, Piao, and Wei]{bao2021beit}
Bao, H., Dong, L., Piao, S., and Wei, F.
\newblock Beit: Bert pre-training of image transformers.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Bu et~al.({\natexlab{a}})Bu, Chiu, Liu, Wang, Zha, and
  Karypis]{bu2023zero}
Bu, Z., Chiu, J., Liu, R., Wang, Y.-X., Zha, S., and Karypis, G.
\newblock Zero redundancy distributed learning with differential privacy.
\newblock In \emph{ICLR 2023 Workshop on Pitfalls of limited data and
  computation for Trustworthy ML}, {\natexlab{a}}.

\bibitem[Bu et~al.({\natexlab{b}})Bu, Wang, Zha, and
  Karypis]{bu2022differentially}
Bu, Z., Wang, Y.-X., Zha, S., and Karypis, G.
\newblock Differentially private bias-term only fine-tuning of foundation
  models.
\newblock In \emph{Workshop on Trustworthy and Socially Responsible Machine
  Learning, NeurIPS 2022}, {\natexlab{b}}.

\bibitem[Bu et~al.(2020)Bu, Dong, Long, and Su]{bu2020deep}
Bu, Z., Dong, J., Long, Q., and Su, W.~J.
\newblock Deep learning with gaussian differential privacy.
\newblock \emph{Harvard data science review}, 2020\penalty0 (23), 2020.

\bibitem[Bu et~al.(2021{\natexlab{a}})Bu, Gopi, Kulkarni, Lee, Shen, and
  Tantipongpipat]{bu2021fast}
Bu, Z., Gopi, S., Kulkarni, J., Lee, Y.~T., Shen, H., and Tantipongpipat, U.
\newblock Fast and memory efficient differentially private-sgd via jl
  projections.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{a}}.

\bibitem[Bu et~al.(2021{\natexlab{b}})Bu, Wang, and Long]{bu2021convergence}
Bu, Z., Wang, H., and Long, Q.
\newblock On the convergence and calibration of deep learning with differential
  privacy.
\newblock \emph{arXiv preprint arXiv:2106.07830}, 2021{\natexlab{b}}.

\bibitem[Bu et~al.(2022{\natexlab{a}})Bu, Mao, and Xu]{bu2022scalable}
Bu, Z., Mao, J., and Xu, S.
\newblock Scalable and efficient training of large convolutional neural
  networks with differential privacy.
\newblock \emph{arXiv preprint arXiv:2205.10683}, 2022{\natexlab{a}}.

\bibitem[Bu et~al.(2022{\natexlab{b}})Bu, Wang, Zha, and
  Karypis]{bu2022automatic}
Bu, Z., Wang, Y.-X., Zha, S., and Karypis, G.
\newblock Automatic clipping: Differentially private deep learning made easier
  and stronger.
\newblock \emph{arXiv preprint arXiv:2206.07136}, 2022{\natexlab{b}}.

\bibitem[Carlini et~al.(2021)Carlini, Tramer, Wallace, Jagielski, Herbert-Voss,
  Lee, Roberts, Brown, Song, Erlingsson, et~al.]{carlini2021extracting}
Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K.,
  Roberts, A., Brown, T., Song, D., Erlingsson, U., et~al.
\newblock Extracting training data from large language models.
\newblock In \emph{30th USENIX Security Symposium (USENIX Security 21)}, pp.\
  2633--2650, 2021.

\bibitem[De et~al.(2022)De, Berrada, Hayes, Smith, and Balle]{de2022unlocking}
De, S., Berrada, L., Hayes, J., Smith, S.~L., and Balle, B.
\newblock Unlocking high-accuracy differentially private image classification
  through scale.
\newblock \emph{arXiv preprint arXiv:2204.13650}, 2022.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Dong et~al.(2019)Dong, Roth, and Su]{dong2019gaussian}
Dong, J., Roth, A., and Su, W.~J.
\newblock Gaussian differential privacy.
\newblock \emph{arXiv preprint arXiv:1905.02383}, 2019.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Dwork et~al.(2006)Dwork, McSherry, Nissim, and
  Smith]{dwork2006calibrating}
Dwork, C., McSherry, F., Nissim, K., and Smith, A.
\newblock Calibrating noise to sensitivity in private data analysis.
\newblock In \emph{Theory of cryptography conference}, pp.\  265--284.
  Springer, 2006.

\bibitem[Dwork et~al.(2014)Dwork, Roth, et~al.]{dwork2014algorithmic}
Dwork, C., Roth, A., et~al.
\newblock The algorithmic foundations of differential privacy.
\newblock \emph{Found. Trends Theor. Comput. Sci.}, 9\penalty0 (3-4):\penalty0
  211--407, 2014.

\bibitem[Goodfellow(2015)]{goodfellow2015efficient}
Goodfellow, I.
\newblock Efficient per-example gradient computations.
\newblock \emph{arXiv preprint arXiv:1510.01799}, 2015.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Goodfellow, I.~J., Shlens, J., and Szegedy, C.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014.

\bibitem[Gopi et~al.(2021)Gopi, Lee, and Wutschitz]{gopi2021numerical}
Gopi, S., Lee, Y.~T., and Wutschitz, L.
\newblock Numerical composition of differential privacy.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Haim et~al.(2022)Haim, Vardi, Yehudai, Shamir, and
  Irani]{haim2022reconstructing}
Haim, N., Vardi, G., Yehudai, G., Shamir, O., and Irani, M.
\newblock Reconstructing training data from trained neural networks.
\newblock \emph{arXiv preprint arXiv:2206.07758}, 2022.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone,
  De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019parameter}
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De~Laroussilhe, Q.,
  Gesmundo, A., Attariyan, M., and Gelly, S.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2790--2799. PMLR, 2019.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and
  Chen]{hu2021lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and
  Chen, W.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, pp.\
  448--456. PMLR, 2015.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and
  Zettlemoyer]{joshi2017triviaqa}
Joshi, M., Choi, E., Weld, D.~S., and Zettlemoyer, L.
\newblock Triviaqa: A large scale distantly supervised challenge dataset for
  reading comprehension.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  1601--1611,
  2017.

\bibitem[Koskela et~al.(2020)Koskela, J{\"a}lk{\"o}, and
  Honkela]{koskela2020computing}
Koskela, A., J{\"a}lk{\"o}, J., and Honkela, A.
\newblock Computing tight differential privacy guarantees using fft.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  2560--2569. PMLR, 2020.

\bibitem[Kurakin et~al.(2022)Kurakin, Chien, Song, Geambasu, Terzis, and
  Thakurta]{kurakin2022toward}
Kurakin, A., Chien, S., Song, S., Geambasu, R., Terzis, A., and Thakurta, A.
\newblock Toward training at imagenet scale with differential privacy.
\newblock \emph{arXiv preprint arXiv:2201.12328}, 2022.

\bibitem[Lee \& Kifer(2020)Lee and Kifer]{lee2020scaling}
Lee, J. and Kifer, D.
\newblock Scaling up differentially private deep learning with fast per-example
  gradient clipping.
\newblock \emph{arXiv preprint arXiv:2009.03106}, 2020.

\bibitem[Li et~al.(2021)Li, Tramer, Liang, and Hashimoto]{li2021large}
Li, X., Tramer, F., Liang, P., and Hashimoto, T.
\newblock Large language models can be strong differentially private learners.
\newblock \emph{arXiv preprint arXiv:2110.05679}, 2021.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L., and Stoyanov, V.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Mahabadi et~al.(2021)Mahabadi, Henderson, and
  Ruder]{mahabadi2021compacter}
Mahabadi, R.~K., Henderson, J., and Ruder, S.
\newblock Compacter: Efficient low-rank hypercomplex adapter layers.
\newblock \emph{arXiv preprint arXiv:2106.04647}, 2021.

\bibitem[Marcel \& Rodriguez(2010)Marcel and Rodriguez]{marcel2010torchvision}
Marcel, S. and Rodriguez, Y.
\newblock Torchvision the machine-vision package of torch.
\newblock In \emph{Proceedings of the 18th ACM international conference on
  Multimedia}, pp.\  1485--1488, 2010.

\bibitem[Mehta et~al.(2022)Mehta, Thakurta, Kurakin, and
  Cutkosky]{mehta2022large}
Mehta, H., Thakurta, A., Kurakin, A., and Cutkosky, A.
\newblock Large scale transfer learning for differentially private image
  classification.
\newblock \emph{arXiv preprint arXiv:2205.02973}, 2022.

\bibitem[Mironov(2017)]{mironov2017renyi}
Mironov, I.
\newblock R{\'e}nyi differential privacy.
\newblock In \emph{2017 IEEE 30th computer security foundations symposium
  (CSF)}, pp.\  263--275. IEEE, 2017.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rochette et~al.(2019)Rochette, Manoel, and
  Tramel]{rochette2019efficient}
Rochette, G., Manoel, A., and Tramel, E.~W.
\newblock Efficient per-example gradient computations in convolutional neural
  networks.
\newblock \emph{arXiv preprint arXiv:1912.06015}, 2019.

\bibitem[Shokri et~al.(2017)Shokri, Stronati, Song, and
  Shmatikov]{shokri2017membership}
Shokri, R., Stronati, M., Song, C., and Shmatikov, V.
\newblock Membership inference attacks against machine learning models.
\newblock In \emph{2017 IEEE symposium on security and privacy (SP)}, pp.\
  3--18. IEEE, 2017.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Subramani et~al.(2021)Subramani, Vadivelu, and
  Kamath]{subramani2021enabling}
Subramani, P., Vadivelu, N., and Kamath, G.
\newblock Enabling fast differentially private sgd via just-in-time compilation
  and vectorization.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Tensorflow()]{tfprivacy}
Tensorflow.
\newblock Tensorflow/privacy: Library for training machine learning models with
  privacy for training data.
\newblock URL \url{https://github.com/tensorflow/privacy}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
  T., Rozi{\`e}re, B., Goyal, N., Hambro, E., Azhar, F., et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Tramer \& Boneh(2020)Tramer and Boneh]{tramer2020differentially}
Tramer, F. and Boneh, D.
\newblock Differentially private learning needs better features (or much more
  data).
\newblock \emph{arXiv preprint arXiv:2011.11660}, 2020.

\bibitem[Wang et~al.(2019)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2019glue}
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.~R.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock 2019.
\newblock In the Proceedings of ICLR.

\bibitem[Welbl et~al.(2018)Welbl, Stenetorp, and Riedel]{welbl2018constructing}
Welbl, J., Stenetorp, P., and Riedel, S.
\newblock Constructing datasets for multi-hop reading comprehension across
  documents.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  6:\penalty0 287--302, 2018.

\bibitem[Wightman(2019)]{rw2019timm}
Wightman, R.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Scao, Gugger, Drame, Lhoest, and Rush]{wolf-etal-2020-transformers}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,
  Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen,
  P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T.~L., Gugger, S., Drame, M.,
  Lhoest, Q., and Rush, A.~M.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pp.\  38--45, Online,
  October 2020. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/2020.emnlp-demos.6}.

\bibitem[Yousefpour et~al.(2021)Yousefpour, Shilov, Sablayrolles, Testuggine,
  Prasad, Malek, Nguyen, Ghosh, Bharadwaj, Zhao, Cormode, and Mironov]{opacus}
Yousefpour, A., Shilov, I., Sablayrolles, A., Testuggine, D., Prasad, K.,
  Malek, M., Nguyen, J., Ghosh, S., Bharadwaj, A., Zhao, J., Cormode, G., and
  Mironov, I.
\newblock Opacus: {U}ser-friendly differential privacy library in {PyTorch}.
\newblock \emph{arXiv preprint arXiv:2109.12298}, 2021.

\bibitem[Yu et~al.(2021)Yu, Naik, Backurs, Gopi, Inan, Kamath, Kulkarni, Lee,
  Manoel, Wutschitz, et~al.]{yu2021differentially}
Yu, D., Naik, S., Backurs, A., Gopi, S., Inan, H.~A., Kamath, G., Kulkarni, J.,
  Lee, Y.~T., Manoel, A., Wutschitz, L., et~al.
\newblock Differentially private fine-tuning of language models.
\newblock \emph{arXiv preprint arXiv:2110.06500}, 2021.

\bibitem[Zhu et~al.(2021)Zhu, Dong, and Wang]{zhu2021optimal}
Zhu, Y., Dong, J., and Wang, Y.-X.
\newblock Optimal accounting of differential privacy via characteristic
  function.
\newblock \emph{arXiv preprint arXiv:2106.08567}, 2021.

\end{thebibliography}
