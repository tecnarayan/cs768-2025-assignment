\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ali et~al.(2022)Ali, Pinciroli, Yan, and Smirni]{ali2022optimizing}
Ali, A., Pinciroli, R., Yan, F., and Smirni, E.
\newblock Optimizing inference serving on serverless platforms.
\newblock \emph{Proceedings of the VLDB Endowment}, 15\penalty0 (10):\penalty0 2071--2084, 2022.

\bibitem[Athlur et~al.(2022)Athlur, Saran, Sivathanu, Ramjee, and Kwatra]{athlur2022varuna}
Athlur, S., Saran, N., Sivathanu, M., Ramjee, R., and Kwatra, N.
\newblock Varuna: scalable, low-cost training of massive deep learning models.
\newblock In \emph{Proceedings of the Seventeenth European Conference on Computer Systems}, pp.\  472--487, 2022.

\bibitem[Bhat et~al.(2023)Bhat, Chen, Cheng, Fang, Hebbar, Kannan, Rana, Sheng, Tyagi, Viswanath, et~al.]{bhat2023sakshi}
Bhat, S., Chen, C., Cheng, Z., Fang, Z., Hebbar, A., Kannan, S., Rana, R., Sheng, P., Tyagi, H., Viswanath, P., et~al.
\newblock Sakshi: Decentralized ai platforms.
\newblock \emph{arXiv preprint arXiv:2307.16562}, 2023.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von Arx, Bernstein, Bohg, Bosselut, Brunskill, et~al.]{bommasani2021opportunities}
Bommasani, R., Hudson, D.~A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.~S., Bohg, J., Bosselut, A., Brunskill, E., et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Borzunov et~al.(2022)Borzunov, Baranchuk, Dettmers, Ryabinin, Belkada, Chumachenko, Samygin, and Raffel]{borzunov2022petals}
Borzunov, A., Baranchuk, D., Dettmers, T., Ryabinin, M., Belkada, Y., Chumachenko, A., Samygin, P., and Raffel, C.
\newblock Petals: Collaborative inference and fine-tuning of large models.
\newblock \emph{arXiv preprint arXiv:2209.01188}, 2022.
\newblock URL \url{https://arxiv.org/abs/2209.01188}.

\bibitem[Borzunov et~al.(2023)Borzunov, Baranchuk, Dettmers, Riabinin, Belkada, Chumachenko, Samygin, and Raffel]{borzunov2023petals}
Borzunov, A., Baranchuk, D., Dettmers, T., Riabinin, M., Belkada, Y., Chumachenko, A., Samygin, P., and Raffel, C.
\newblock Petals: Collaborative inference and fine-tuning of large models.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)}, pp.\  558--568, 2023.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks}
Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y.~T., Li, Y., Lundberg, S., et~al.
\newblock Sparks of artificial general intelligence: Early experiments with gpt-4.
\newblock \emph{arXiv preprint arXiv:2303.12712}, 2023.

\bibitem[Cai et~al.(2023)Cai, Li, Geng, Peng, and Dao]{medusa}
Cai, T., Li, Y., Geng, Z., Peng, H., and Dao, T.
\newblock Medusa: Simple framework for accelerating llm generation with multiple decoding heads.
\newblock \url{https://github.com/FasterDecoding/Medusa}, 2023.

\bibitem[Cheatham et~al.(1996)Cheatham, Fahmy, Stefanescu, and Valiant]{cheatham1996bulk}
Cheatham, T., Fahmy, A., Stefanescu, D., and Valiant, L.
\newblock Bulk synchronous parallel computingâ€”a paradigm for transportable software.
\newblock \emph{Tools and Environments for Parallel and Distributed Systems}, pp.\  61--76, 1996.

\bibitem[Dao(2023)]{dao2023flashattention}
Dao, T.
\newblock Flashattention-2: Faster attention with better parallelism and work partitioning.
\newblock \emph{arXiv preprint arXiv:2307.08691}, 2023.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R{\'e}]{dao2022flashattention}
Dao, T., Fu, D., Ermon, S., Rudra, A., and R{\'e}, C.
\newblock Flashattention: Fast and memory-efficient exact attention with io-awareness.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 16344--16359, 2022.

\bibitem[Diskin et~al.(2021)Diskin, Bukhtiyarov, Ryabinin, Saulnier, Sinitsin, Popov, Pyrkin, Kashirin, Borzunov, Villanova~del Moral, et~al.]{diskin2021distributed}
Diskin, M., Bukhtiyarov, A., Ryabinin, M., Saulnier, L., Sinitsin, A., Popov, D., Pyrkin, D.~V., Kashirin, M., Borzunov, A., Villanova~del Moral, A., et~al.
\newblock Distributed deep learning in open collaborations.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 7879--7897, 2021.

\bibitem[Fang et~al.(2021)Fang, Yu, Zhao, and Zhou]{fang2021turbotransformers}
Fang, J., Yu, Y., Zhao, C., and Zhou, J.
\newblock Turbotransformers: an efficient gpu serving system for transformer models.
\newblock In \emph{Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}, pp.\  389--402, 2021.

\bibitem[Frantar \& Alistarh(2023)Frantar and Alistarh]{frantar2023sparsegpt}
Frantar, E. and Alistarh, D.
\newblock Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023.

\bibitem[Frantar et~al.(2022)Frantar, Ashkboos, Hoefler, and Alistarh]{frantar2022gptq}
Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D.
\newblock Gptq: Accurate post-training quantization for generative pre-trained transformers.
\newblock \emph{arXiv preprint arXiv:2210.17323}, 2022.

\bibitem[Guo et~al.(2022)Guo, Guo, Kim, Hildred, and Daudjee]{guo2022hydrozoa}
Guo, R., Guo, V., Kim, A., Hildred, J., and Daudjee, K.
\newblock Hydrozoa: Dynamic hybrid-parallel dnn training on serverless containers.
\newblock \emph{Proceedings of Machine Learning and Systems}, 4:\penalty0 779--794, 2022.

\bibitem[Huang et~al.(2019)Huang, Cheng, Bapna, Firat, Chen, Chen, Lee, Ngiam, Le, Wu, et~al.]{huang2019gpipe}
Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q.~V., Wu, Y., et~al.
\newblock Gpipe: Efficient training of giant neural networks using pipeline parallelism.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[HuggingFace(2022)]{huggingfaceAccelerate}
HuggingFace.
\newblock Hugging face accelerate.
\newblock \url{https://huggingface.co/docs/accelerate/index}, 2022.

\bibitem[HuggingFace(2023)]{huggingfaceTGI}
HuggingFace.
\newblock Text generation inference.
\newblock \url{https://huggingface.co/docs/text-generation-inference/index}, 2023.

\bibitem[Institute(2023)]{falcon180b}
Institute, T.~I.
\newblock Falcon 180b, 2023.
\newblock URL \url{https://falconllm.tii.ae/falcon-180b.html}.

\bibitem[Kwon et~al.(2022)Kwon, Kim, Bae, Yoo, Kim, Park, Kim, Ha, Sung, and Lee]{kwon2022alphatuning}
Kwon, S.~J., Kim, J., Bae, J., Yoo, K.~M., Kim, J.-H., Park, B., Kim, B., Ha, J.-W., Sung, N., and Lee, D.
\newblock Alphatuning: Quantization-aware parameter-efficient adaptation of large-scale pre-trained language models.
\newblock \emph{arXiv preprint arXiv:2210.03858}, 2022.

\bibitem[Kwon et~al.(2023)Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang, and Stoica]{kwon2023efficient}
Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C.~H., Gonzalez, J., Zhang, H., and Stoica, I.
\newblock Efficient memory management for large language model serving with pagedattention.
\newblock In \emph{Proceedings of the 29th Symposium on Operating Systems Principles}, pp.\  611--626, 2023.

\bibitem[Leviathan et~al.(2023)Leviathan, Kalman, and Matias]{leviathan2023fast}
Leviathan, Y., Kalman, M., and Matias, Y.
\newblock Fast inference from transformers via speculative decoding.
\newblock In \emph{International Conference on Machine Learning}, pp.\  19274--19286. PMLR, 2023.

\bibitem[Li et~al.(2023)Li, Zheng, Zhong, Liu, Sheng, Jin, Huang, Chen, Zhang, Gonzalez, et~al.]{li2023alpaserve}
Li, Z., Zheng, L., Zhong, Y., Liu, V., Sheng, Y., Jin, X., Huang, Y., Chen, Z., Zhang, H., Gonzalez, J.~E., et~al.
\newblock $\{$AlpaServe$\}$: Statistical multiplexing with model parallelism for deep learning serving.
\newblock In \emph{17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)}, pp.\  663--679, 2023.

\bibitem[LibP2P(2023)]{libp2p}
LibP2P.
\newblock A modular network stack, 2023.
\newblock URL \url{https://libp2p.io/}.

\bibitem[Lin et~al.(2023)Lin, Tang, Tang, Yang, Dang, and Han]{lin2023awq}
Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han, S.
\newblock Awq: Activation-aware weight quantization for llm compression and acceleration.
\newblock \emph{arXiv preprint arXiv:2306.00978}, 2023.

\bibitem[Liu et~al.(2023)Liu, Wang, Dao, Zhou, Yuan, Song, Shrivastava, Zhang, Tian, Re, et~al.]{liu2023deja}
Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z., Shrivastava, A., Zhang, C., Tian, Y., Re, C., et~al.
\newblock Deja vu: Contextual sparsity for efficient llms at inference time.
\newblock In \emph{International Conference on Machine Learning}, pp.\  22137--22176. PMLR, 2023.

\bibitem[Lmsys(2023)]{chatbotData}
Lmsys.
\newblock Chatbot arena conversations.
\newblock \url{https://huggingface.co/datasets/lmsys/chatbot_arena_conversations}, 2023.

\bibitem[Miao et~al.(2023{\natexlab{a}})Miao, Oliaro, Zhang, Cheng, Wang, Wong, Chen, Arfeen, Abhyankar, and Jia]{miao2023specinfer}
Miao, X., Oliaro, G., Zhang, Z., Cheng, X., Wang, Z., Wong, R. Y.~Y., Chen, Z., Arfeen, D., Abhyankar, R., and Jia, Z.
\newblock Specinfer: Accelerating generative llm serving with speculative inference and token tree verification.
\newblock \emph{arXiv preprint arXiv:2305.09781}, 2023{\natexlab{a}}.

\bibitem[Miao et~al.(2023{\natexlab{b}})Miao, Shi, Yang, Cui, and Jia]{miao2023sdpipe}
Miao, X., Shi, Y., Yang, Z., Cui, B., and Jia, Z.
\newblock Sdpipe: A semi-decentralized framework for heterogeneity-aware pipeline-parallel training.
\newblock \emph{Proceedings of the VLDB Endowment}, 16\penalty0 (9):\penalty0 2354--2363, 2023{\natexlab{b}}.

\bibitem[Narayanan et~al.(2019)Narayanan, Harlap, Phanishayee, Seshadri, Devanur, Ganger, Gibbons, and Zaharia]{narayanan2019pipedream}
Narayanan, D., Harlap, A., Phanishayee, A., Seshadri, V., Devanur, N.~R., Ganger, G.~R., Gibbons, P.~B., and Zaharia, M.
\newblock Pipedream: generalized pipeline parallelism for dnn training.
\newblock In \emph{Proceedings of the 27th ACM Symposium on Operating Systems Principles}, pp.\  1--15, 2019.

\bibitem[Narayanan et~al.(2021)Narayanan, Shoeybi, Casper, LeGresley, Patwary, Korthikanti, Vainbrand, Kashinkunti, Bernauer, Catanzaro, et~al.]{narayanan2021efficient}
Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B., et~al.
\newblock Efficient large-scale language model training on gpu clusters using megatron-lm.
\newblock In \emph{Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis}, pp.\  1--15, 2021.

\bibitem[NVIDIA(2022)]{fastertransformer}
NVIDIA.
\newblock Fastertransformer.
\newblock \url{https://github.com/NVIDIA/FasterTransformer}, 2022.

\bibitem[Ryabinin et~al.(2023)Ryabinin, Dettmers, Diskin, and Borzunov]{ryabinin2023swarm}
Ryabinin, M., Dettmers, T., Diskin, M., and Borzunov, A.
\newblock Swarm parallelism: Training large models can be surprisingly communication-efficient.
\newblock \emph{arXiv preprint arXiv:2301.11913}, 2023.

\bibitem[Spector \& Re(2023)Spector and Re]{spector2023accelerating}
Spector, B.~F. and Re, C.
\newblock Accelerating llm inference with staged speculative decoding.
\newblock In \emph{Workshop on Efficient Systems for Foundation Models@ ICML2023}, 2023.

\bibitem[Stoica \& Shenker(2021)Stoica and Shenker]{stoica2021cloud}
Stoica, I. and Shenker, S.
\newblock From cloud computing to sky computing.
\newblock In \emph{Proceedings of the Workshop on Hot Topics in Operating Systems}, pp.\  26--32, 2021.

\bibitem[Thorpe et~al.(2023)Thorpe, Zhao, Eyolfson, Qiao, Jia, Zhang, Netravali, and Xu]{thorpe2023bamboo}
Thorpe, J., Zhao, P., Eyolfson, J., Qiao, Y., Jia, Z., Zhang, M., Netravali, R., and Xu, G.~H.
\newblock Bamboo: Making preemptible instances resilient for affordable training of large $\{$DNNs$\}$.
\newblock In \emph{20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)}, pp.\  497--513, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Xiao et~al.(2022)Xiao, Lin, Seznec, Demouth, and Han]{xiao2022smoothquant}
Xiao, G., Lin, J., Seznec, M., Demouth, J., and Han, S.
\newblock Smoothquant: Accurate and efficient post-training quantization for large language models.
\newblock \emph{arXiv preprint arXiv:2211.10438}, 2022.

\bibitem[Yang et~al.(2023)Yang, Wu, Luo, Chiang, Bhardwaj, Kwon, Zhuang, Luan, Mittal, Shenker, et~al.]{yang2023skypilot}
Yang, Z., Wu, Z., Luo, M., Chiang, W.-L., Bhardwaj, R., Kwon, W., Zhuang, S., Luan, F.~S., Mittal, G., Shenker, S., et~al.
\newblock $\{$SkyPilot$\}$: An intercloud broker for sky computing.
\newblock In \emph{20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)}, pp.\  437--455, 2023.

\bibitem[Yao(2023)]{yao2023open}
Yao, X.
\newblock {Open Compute Framework: Peer-to-Peer Task Queue for Foundation Model Inference Serving}, September 2023.
\newblock URL \url{https://github.com/autoai-org/OpenComputeFramework}.

\bibitem[Yao et~al.(2022)Yao, Aminabadi, Zhang, Wu, Li, and He]{yao2022zeroquant}
Yao, Z., Aminabadi, R.~Y., Zhang, M., Wu, X., Li, C., and He, Y.
\newblock Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.
\newblock \emph{arXiv preprint arXiv:2206.01861}, 2022.

\bibitem[Yu et~al.(2022)Yu, Jeong, Kim, Kim, and Chun]{yu2022orca}
Yu, G.-I., Jeong, J.~S., Kim, G.-W., Kim, S., and Chun, B.-G.
\newblock Orca: A distributed serving system for $\{$Transformer-Based$\}$ generative models.
\newblock In \emph{16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)}, pp.\  521--538, 2022.

\bibitem[Yuan et~al.(2022)Yuan, He, Davis, Zhang, Dao, Chen, Liang, Re, and Zhang]{yuan2022decentralized}
Yuan, B., He, Y., Davis, J., Zhang, T., Dao, T., Chen, B., Liang, P.~S., Re, C., and Zhang, C.
\newblock Decentralized training of foundation models in heterogeneous environments.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 25464--25477, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Li, Zhao, Xu, Lu, Xiao, Han, Yang, and Du]{zhang2023efficient}
Zhang, Q., Li, J., Zhao, H., Xu, Q., Lu, W., Xiao, J., Han, F., Yang, C., and Du, X.
\newblock Efficient distributed transaction processing in heterogeneous networks.
\newblock \emph{Proceedings of the VLDB Endowment}, 16\penalty0 (6):\penalty0 1372--1385, 2023.

\end{thebibliography}
