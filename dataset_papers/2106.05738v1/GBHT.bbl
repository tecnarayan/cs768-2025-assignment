\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amarbayasgalan et~al.(2018)Amarbayasgalan, Jargalsaikhan, and
  Ryu]{amarbayasgalan2018unsupervised}
Amarbayasgalan, T., Jargalsaikhan, B., and Ryu, K.~H.
\newblock Unsupervised novelty detection using deep autoencoders with density
  based clustering.
\newblock \emph{Applied Sciences}, 8\penalty0 (9):\penalty0 1468, 2018.

\bibitem[Biau et~al.(2019)Biau, Cadre, and
  Rouv{\`\i}{\`e}re]{biau2014accelerated}
Biau, G., Cadre, B., and Rouv{\`\i}{\`e}re, L.
\newblock Accelerated gradient boosting.
\newblock \emph{Machine Learning}, 108\penalty0 (6):\penalty0 971--992, 2019.

\bibitem[Blanchard et~al.(2003)Blanchard, Lugosi, and
  Vayatis]{blanchard2003rate}
Blanchard, G., Lugosi, G., and Vayatis, N.
\newblock On the rate of convergence of regularized boosting classifiers.
\newblock \emph{The Journal of Machine Learning Research}, 4\penalty0
  (Oct):\penalty0 861--894, 2003.

\bibitem[Blaser \& Fryzlewicz(2016)Blaser and Fryzlewicz]{blaser2016random}
Blaser, R. and Fryzlewicz, P.
\newblock Random rotation ensembles.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 126--151, 2016.

\bibitem[Breiman(2000)]{breiman2000some}
Breiman, L.
\newblock Some infinity theory for predictor ensembles.
\newblock Technical report, Technical Report 579, Statistics Dept. UCB, 2000.

\bibitem[Breunig et~al.(2000)Breunig, Kriegel, Ng, and Sander]{Breunig2000LOF}
Breunig, M.~M., Kriegel, H.-P., Ng, R.~T., and Sander, J.
\newblock Lof: identifying density-based local outliers.
\newblock In \emph{ACM Sigmod Record}, volume~29, pp.\  93--104. ACM, 2000.

\bibitem[B{\"u}hlmann \& Yu(2003)B{\"u}hlmann and Yu]{buhlmann2003boosting}
B{\"u}hlmann, P. and Yu, B.
\newblock Boosting with the {L2} loss: regression and classification.
\newblock \emph{Journal of the American Statistical Association}, 98\penalty0
  (462):\penalty0 324--339, 2003.

\bibitem[Cai et~al.(2020)Cai, Hang, Yang, and Lin]{cai2020boosted}
Cai, Y., Hang, H., Yang, H., and Lin, Z.
\newblock Boosted histogram transform for regression.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1251--1261. PMLR, 2020.

\bibitem[Chen \& Guestrin(2016)Chen and Guestrin]{chen2016xgboost}
Chen, T. and Guestrin, C.
\newblock Xgboost: {A} scalable tree boosting system.
\newblock In \emph{Proceedings of the 22nd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pp.\  785--794, 2016.

\bibitem[Chen et~al.(2020)Chen, Hu, Fan, Shen, Zhang, Liu, Du, Li, Chen, and
  Li]{chen2020fast}
Chen, Y., Hu, X., Fan, W., Shen, L., Zhang, Z., Liu, X., Du, J., Li, H., Chen,
  Y., and Li, H.
\newblock Fast density peak clustering for large scale data based on knn.
\newblock \emph{Knowledge-Based Systems}, 187:\penalty0 104824, 2020.

\bibitem[Cortes et~al.(2019)Cortes, Mohri, and
  Storcheus]{cortes2019regularized}
Cortes, C., Mohri, M., and Storcheus, D.
\newblock Regularized gradient boosting.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 5449--5458, 2019.

\bibitem[Criminisi \& Shotton(2013)Criminisi and
  Shotton]{criminisi2013decision}
Criminisi, A. and Shotton, J.
\newblock \emph{Decision Forests for Computer Vision and Medical Image
  Analysis}.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Criminisi et~al.(2011)Criminisi, Shotton, and
  Konukoglu]{criminisi2011decision}
Criminisi, A., Shotton, J., and Konukoglu, E.
\newblock Decision forests for classification, regression, density estimation,
  manifold learning and semi-supervised learning.
\newblock \emph{Microsoft Research Technical Report 2011--114}, 2011.

\bibitem[Duan et~al.(2020)Duan, Anand, Ding, Thai, Basu, Ng, and
  Schuler]{duan2020ngboost}
Duan, T., Anand, A., Ding, D.~Y., Thai, K.~K., Basu, S., Ng, A., and Schuler,
  A.
\newblock Ngboost: Natural gradient boosting for probabilistic prediction.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2690--2700. PMLR, 2020.

\bibitem[Freund \& Schapire(1997)Freund and Schapire]{freund1997decision}
Freund, Y. and Schapire, R.~E.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock \emph{Journal of Computer and System Sciences}, 55\penalty0
  (1):\penalty0 119--139, 1997.

\bibitem[Friedman(2001)]{friedman2001greedy}
Friedman, J.~H.
\newblock Greedy function approximation: a gradient boosting machine.
\newblock \emph{The Annals of Statistics}, pp.\  1189--1232, 2001.

\bibitem[Ghaffari et~al.(2019)Ghaffari, Lattanzi, and
  Mitrovi{\'c}]{ghaffari2019improved}
Ghaffari, M., Lattanzi, S., and Mitrovi{\'c}, S.
\newblock Improved parallel algorithms for density-based network clustering.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2201--2210. PMLR, 2019.

\bibitem[Jang \& Jiang(2019)Jang and Jiang]{jang2019dbscanpp}
Jang, J. and Jiang, H.
\newblock {DBSCAN}++: Towards fast and scalable density clustering.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3019--3029. PMLR, 2019.

\bibitem[Jeffreys(1946)]{jeffreys1946invariant}
Jeffreys, H.
\newblock An invariant form for the prior probability in estimation problems.
\newblock \emph{Proceedings of the Royal Society of London. Series A.
  Mathematical and Physical Sciences}, 186\penalty0 (1007):\penalty0 453--461,
  1946.

\bibitem[Klemel\"{a}(2009)]{klemela2009multivariate}
Klemel\"{a}, J.
\newblock Multivariate histograms with data-dependent partitions.
\newblock \emph{Statistica Sinica}, 19\penalty0 (1):\penalty0 159--176, 2009.

\bibitem[Liu et~al.(2008)Liu, Ting, and Zhou]{liu2008isolation}
Liu, F.~T., Ting, K.~M., and Zhou, Z.-H.
\newblock Isolation forest.
\newblock In \emph{Proceedings of the IEEE International Conference on Data
  Mining}, pp.\  413--422, 2008.

\bibitem[Liu \& Wong(2014)Liu and Wong]{liu2014multivariate}
Liu, L. and Wong, W.~H.
\newblock Multivariate density estimation via adaptive partitioning {(I)}:
  sieve {MLE}.
\newblock \emph{arXiv preprint arXiv:1401.2597}, 2014.

\bibitem[L{\'o}pez-Rubio(2013)]{lopez2013histogram}
L{\'o}pez-Rubio, E.
\newblock A histogram transform for probability density function estimation.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 36\penalty0 (4):\penalty0 644--656, 2013.

\bibitem[Mathiasen et~al.(2019)Mathiasen, Larsen, and
  Gr{\o}nlund]{mathiasen2019optimal}
Mathiasen, A., Larsen, K.~G., and Gr{\o}nlund, A.
\newblock Optimal minimal margin maximization with boosting.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4392--4401. PMLR, 2019.

\bibitem[Nachman \& Shih(2020)Nachman and Shih]{nachman2020anomaly}
Nachman, B. and Shih, D.
\newblock Anomaly detection with density estimation.
\newblock \emph{Physical Review D}, 101\penalty0 (7):\penalty0 075042, 2020.

\bibitem[Parmar et~al.(2019)Parmar, Wang, Zhang, Tan, Miao, Jiang, and
  Zhou]{parmar2019redpc}
Parmar, M., Wang, D., Zhang, X., Tan, A.-H., Miao, C., Jiang, J., and Zhou, Y.
\newblock Redpc: A residual error-based density peak clustering algorithm.
\newblock \emph{Neurocomputing}, 348:\penalty0 82--96, 2019.

\bibitem[Parnell et~al.(2020)Parnell, Anghel, {\L}azuka, Ioannou, Kurella,
  Agarwal, Papandreou, and Pozidis]{parnell2020snapboost}
Parnell, T., Anghel, A., {\L}azuka, M., Ioannou, N., Kurella, S., Agarwal, P.,
  Papandreou, N., and Pozidis, H.
\newblock Snapboost: A heterogeneous boosting machine.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Ram \& Gray(2011)Ram and Gray]{ram2011density}
Ram, P. and Gray, A.~G.
\newblock Density estimation trees.
\newblock In \emph{Proceedings of the 17th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pp.\  627--635. ACM, 2011.

\bibitem[Ramaswamy et~al.(2000)Ramaswamy, Rastogi, and
  Shim]{ramaswamy2000efficient}
Ramaswamy, S., Rastogi, R., and Shim, K.
\newblock Efficient algorithms for mining outliers from large data sets.
\newblock In \emph{Proceedings of the ACM SIGMOD International Conference on
  Management of Data}, pp.\  427--438, 2000.

\bibitem[Ridgeway(2002)]{ridgeway2002looking}
Ridgeway, G.
\newblock Looking for lumps: Boosting and bagging for density estimation.
\newblock \emph{Computational Statistics \& Data Analysis}, 38\penalty0
  (4):\penalty0 379--392, 2002.

\bibitem[Rosset \& Segal(2003)Rosset and Segal]{rosset2003boosting}
Rosset, S. and Segal, E.
\newblock Boosting density estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  657--664, 2003.

\bibitem[Schapire \& Freund(1995)Schapire and Freund]{schapire1995decision}
Schapire, R. and Freund, Y.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock In \emph{Second European Conference on Computational Learning
  Theory}, pp.\  23--37, 1995.

\bibitem[Sch{\"o}lkopf et~al.(2001)Sch{\"o}lkopf, Platt, Shawe-Taylor, Smola,
  and Williamson]{scholkopf2001estimating}
Sch{\"o}lkopf, B., Platt, J.~C., Shawe-Taylor, J., Smola, A.~J., and
  Williamson, R.~C.
\newblock Estimating the support of a high-dimensional distribution.
\newblock \emph{Neural Computation}, 13\penalty0 (7):\penalty0 1443--1471,
  2001.

\bibitem[Scott(2015)]{scott2015multivariate}
Scott, D.~W.
\newblock \emph{Multivariate Density Estimation}.
\newblock John Wiley \& Sons, Inc., Hoboken, NJ, second edition, 2015.

\bibitem[Steinwart \& Christmann(2008)Steinwart and Christmann]{StCh08}
Steinwart, I. and Christmann, A.
\newblock \emph{Support Vector Machines}.
\newblock Information Science and Statistics. Springer, New York, 2008.

\bibitem[Sturges(1926)]{sturges1926choice}
Sturges, H.~A.
\newblock The choice of a class interval.
\newblock \emph{Journal of the American Statistical Association}, 21\penalty0
  (153):\penalty0 65--66, 1926.

\bibitem[Suggala et~al.(2020)Suggala, Liu, and
  Ravikumar]{suggala2020generalized}
Suggala, A., Liu, B., and Ravikumar, P.
\newblock Generalized boosting.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Van~der Vaart \& Wellner(1996)Van~der Vaart and Wellner]{van1996weak}
Van~der Vaart, A.~W. and Wellner, J.~A.
\newblock \emph{Weak Convergence and Empirical Processes}.
\newblock Springer Series in Statistics. Springer-Verlag, New York, 1996.

\bibitem[Vapnik \& Chervonenkis(2015)Vapnik and
  Chervonenkis]{vapnik2015uniform}
Vapnik, V.~N. and Chervonenkis, A.~Y.
\newblock On the uniform convergence of relative frequencies of events to their
  probabilities.
\newblock In \emph{Measures of Complexity}, pp.\  11--30. Springer, 2015.

\bibitem[Zhang et~al.(2018)Zhang, Lin, and Karim]{zhang2018adaptive}
Zhang, L., Lin, J., and Karim, R.
\newblock Adaptive kernel density-based anomaly detection for nonlinear
  systems.
\newblock \emph{Knowledge-Based Systems}, 139:\penalty0 50--63, 2018.

\end{thebibliography}
