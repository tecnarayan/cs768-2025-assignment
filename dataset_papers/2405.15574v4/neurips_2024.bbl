\begin{thebibliography}{100}

\bibitem{wei2022finetuned}
J.~Wei, M.~Bosma, V.~Zhao, K.~Guu, A.~W. Yu, B.~Lester, N.~Du, A.~M. Dai, and Q.~V. Le, ``Finetuned language models are zero-shot learners,'' in {\em International Conference on Learning Representations}, 2022.

\bibitem{chung2022scaling}
H.~W. Chung, L.~Hou, S.~Longpre, B.~Zoph, Y.~Tay, W.~Fedus, Y.~Li, X.~Wang, M.~Dehghani, S.~Brahma, {\em et~al.}, ``Scaling instruction-finetuned language models,'' {\em arXiv preprint arXiv:2210.11416}, 2022.

\bibitem{liu2023visual}
H.~Liu, C.~Li, Q.~Wu, and Y.~J. Lee, ``Visual instruction tuning,'' in {\em Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem{achiam2023gpt}
J.~Achiam, S.~Adler, S.~Agarwal, L.~Ahmad, I.~Akkaya, F.~L. Aleman, D.~Almeida, J.~Altenschmidt, S.~Altman, S.~Anadkat, {\em et~al.}, ``Gpt-4 technical report,'' {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{team2023gemini}
G.~Team, R.~Anil, S.~Borgeaud, Y.~Wu, J.-B. Alayrac, J.~Yu, R.~Soricut, J.~Schalkwyk, A.~M. Dai, A.~Hauth, {\em et~al.}, ``Gemini: a family of highly capable multimodal models,'' {\em arXiv preprint arXiv:2312.11805}, 2023.

\bibitem{bai2023qwen}
J.~Bai, S.~Bai, S.~Yang, S.~Wang, S.~Tan, P.~Wang, J.~Lin, C.~Zhou, and J.~Zhou, ``Qwen-vl: A frontier large vision-language model with versatile abilities,'' {\em arXiv preprint arXiv:2308.12966}, 2023.

\bibitem{chen2023sharegpt4v}
L.~Chen, J.~Li, X.~Dong, P.~Zhang, C.~He, J.~Wang, F.~Zhao, and D.~Lin, ``Sharegpt4v: Improving large multi-modal models with better captions,'' {\em arXiv preprint arXiv:2311.12793}, 2023.

\bibitem{liu2023improved}
H.~Liu, C.~Li, Y.~Li, and Y.~J. Lee, ``Improved baselines with visual instruction tuning,'' {\em arXiv preprint arXiv:2310.03744}, 2023.

\bibitem{dai2023instructblip}
W.~Dai, J.~Li, D.~Li, A.~Tiong, J.~Zhao, W.~Wang, B.~Li, P.~Fung, and S.~Hoi, ``Instruct{BLIP}: Towards general-purpose vision-language models with instruction tuning,'' in {\em Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem{li2024mini}
Y.~Li, Y.~Zhang, C.~Wang, Z.~Zhong, Y.~Chen, R.~Chu, S.~Liu, and J.~Jia, ``Mini-gemini: Mining the potential of multi-modality vision language models,'' {\em arXiv preprint arXiv:2403.18814}, 2024.

\bibitem{wang2023cogvlm}
W.~Wang, Q.~Lv, W.~Yu, W.~Hong, J.~Qi, Y.~Wang, J.~Ji, Z.~Yang, L.~Zhao, X.~Song, {\em et~al.}, ``Cogvlm: Visual expert for pretrained language models,'' {\em arXiv preprint arXiv:2311.03079}, 2023.

\bibitem{liu2024llavanext}
H.~Liu, C.~Li, Y.~Li, B.~Li, Y.~Zhang, S.~Shen, and Y.~J. Lee, ``Llava-next: Improved reasoning, ocr, and world knowledge,'' January 2024.

\bibitem{young2024yi}
A.~Young, B.~Chen, C.~Li, C.~Huang, G.~Zhang, G.~Zhang, H.~Li, J.~Zhu, J.~Chen, J.~Chang, {\em et~al.}, ``Yi: Open foundation models by 01. ai,'' {\em arXiv preprint arXiv:2403.04652}, 2024.

\bibitem{mckinzie2024mm1}
B.~McKinzie, Z.~Gan, J.-P. Fauconnier, S.~Dodge, B.~Zhang, P.~Dufter, D.~Shah, X.~Du, F.~Peng, F.~Weers, {\em et~al.}, ``Mm1: Methods, analysis \& insights from multimodal llm pre-training,'' {\em arXiv preprint arXiv:2403.09611}, 2024.

\bibitem{chen2023internvl}
Z.~Chen, J.~Wu, W.~Wang, W.~Su, G.~Chen, S.~Xing, Z.~Muyan, Q.~Zhang, X.~Zhu, L.~Lu, {\em et~al.}, ``Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks,'' {\em arXiv preprint arXiv:2312.14238}, 2023.

\bibitem{kaplan2020scaling}
J.~Kaplan, S.~McCandlish, T.~Henighan, T.~B. Brown, B.~Chess, R.~Child, S.~Gray, A.~Radford, J.~Wu, and D.~Amodei, ``Scaling laws for neural language models,'' {\em arXiv preprint arXiv:2001.08361}, 2020.

\bibitem{tay2021scale}
Y.~Tay, M.~Dehghani, J.~Rao, W.~Fedus, S.~Abnar, H.~W. Chung, S.~Narang, D.~Yogatama, A.~Vaswani, and D.~Metzler, ``Scale efficiently: Insights from pre-training and fine-tuning transformers,'' {\em arXiv preprint arXiv:2109.10686}, 2021.

\bibitem{li2023otterhd}
B.~Li, P.~Zhang, J.~Yang, Y.~Zhang, F.~Pu, and Z.~Liu, ``Otterhd: A high-resolution multi-modality model,'' {\em arXiv preprint arXiv:2311.04219}, 2023.

\bibitem{ye2023mplug2}
Q.~Ye, H.~Xu, J.~Ye, M.~Yan, H.~Liu, Q.~Qian, J.~Zhang, F.~Huang, and J.~Zhou, ``mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration,'' {\em arXiv preprint arXiv:2311.04257}, 2023.

\bibitem{hu2024mplug}
A.~Hu, H.~Xu, J.~Ye, M.~Yan, L.~Zhang, B.~Zhang, C.~Li, J.~Zhang, Q.~Jin, F.~Huang, {\em et~al.}, ``mplug-docowl 1.5: Unified structure learning for ocr-free document understanding,'' {\em arXiv preprint arXiv:2403.12895}, 2024.

\bibitem{xu2024llava}
R.~Xu, Y.~Yao, Z.~Guo, J.~Cui, Z.~Ni, C.~Ge, T.-S. Chua, Z.~Liu, M.~Sun, and G.~Huang, ``Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images,'' {\em arXiv preprint arXiv:2403.11703}, 2024.

\bibitem{kar2024brave}
O.~F. Kar, A.~Tonioni, P.~Poklukar, A.~Kulshrestha, A.~Zamir, and F.~Tombari, ``Brave: Broadening the visual encoding of vision-language models,'' {\em arXiv preprint arXiv:2404.07204}, 2024.

\bibitem{lu2024deepseek}
H.~Lu, W.~Liu, B.~Zhang, B.~Wang, K.~Dong, B.~Liu, J.~Sun, T.~Ren, Z.~Li, Y.~Sun, {\em et~al.}, ``Deepseek-vl: towards real-world vision-language understanding,'' {\em arXiv preprint arXiv:2403.05525}, 2024.

\bibitem{goncharova2024omnifusion}
E.~Goncharova, A.~Razzhigaev, M.~Mikhalchuk, M.~Kurkin, I.~Abdullaeva, M.~Skripkin, I.~Oseledets, D.~Dimitrov, and A.~Kuznetsov, ``Omnifusion technical report,'' {\em arXiv preprint arXiv:2404.06212}, 2024.

\bibitem{ranzinger2023radio}
M.~Ranzinger, G.~Heinrich, J.~Kautz, and P.~Molchanov, ``Am-radio: Agglomerative model--reduce all domains into one,'' {\em arXiv preprint arXiv:2312.06709}, 2023.

\bibitem{fang2023eva}
Y.~Fang, W.~Wang, B.~Xie, Q.~Sun, L.~Wu, X.~Wang, T.~Huang, X.~Wang, and Y.~Cao, ``Eva: Exploring the limits of masked visual representation learning at scale,'' in {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.~19358--19369, 2023.

\bibitem{oquab2023dinov2}
M.~Oquab, T.~Darcet, T.~Moutakanni, H.~Vo, M.~Szafraniec, V.~Khalidov, P.~Fernandez, D.~Haziza, F.~Massa, A.~El-Nouby, {\em et~al.}, ``Dinov2: Learning robust visual features without supervision,'' {\em arXiv preprint arXiv:2304.07193}, 2023.

\bibitem{kirillov2023segment}
A.~Kirillov, E.~Mintun, N.~Ravi, H.~Mao, C.~Rolland, L.~Gustafson, T.~Xiao, S.~Whitehead, A.~C. Berg, W.-Y. Lo, {\em et~al.}, ``Segment anything,'' in {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.~4015--4026, 2023.

\bibitem{zhai2023sigmoid}
X.~Zhai, B.~Mustafa, A.~Kolesnikov, and L.~Beyer, ``Sigmoid loss for language image pre-training,'' in {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.~11975--11986, 2023.

\bibitem{chen2024spatialvlm}
B.~Chen, Z.~Xu, S.~Kirmani, B.~Ichter, D.~Driess, P.~Florence, D.~Sadigh, L.~Guibas, and F.~Xia, ``Spatialvlm: Endowing vision-language models with spatial reasoning capabilities,'' {\em arXiv preprint arXiv:2401.12168}, 2024.

\bibitem{wang2024all}
W.~Wang, Y.~Ren, H.~Luo, T.~Li, C.~Yan, Z.~Chen, W.~Wang, Q.~Li, L.~Lu, X.~Zhu, {\em et~al.}, ``The all-seeing project v2: Towards general relation comprehension of the open world,'' {\em arXiv preprint arXiv:2402.19474}, 2024.

\bibitem{jiao2024enhancing}
Q.~Jiao, D.~Chen, Y.~Huang, Y.~Li, and Y.~Shen, ``Enhancing multimodal large language models with vision detection models: An empirical study,'' {\em arXiv preprint arXiv:2401.17981}, 2024.

\bibitem{lee2024collavo}
B.-K. Lee, B.~Park, C.~W. Kim, and Y.~M. Ro, ``Collavo: Crayon large language and vision model,'' {\em arXiv preprint arXiv:2402.11248}, 2024.

\bibitem{lee2024moai}
B.-K. Lee, B.~Park, C.~W. Kim, and Y.~M. Ro, ``Moai: Mixture of all intelligence for large language and vision models,'' {\em arXiv preprint arXiv:2403.07508}, 2024.

\bibitem{fu2023mme}
C.~Fu, P.~Chen, Y.~Shen, Y.~Qin, M.~Zhang, X.~Lin, J.~Yang, X.~Zheng, K.~Li, X.~Sun, {\em et~al.}, ``Mme: A comprehensive evaluation benchmark for multimodal large language models,'' {\em arXiv preprint arXiv:2306.13394}, 2023.

\bibitem{liu2023mmbench}
Y.~Liu, H.~Duan, Y.~Zhang, B.~Li, S.~Zhang, W.~Zhao, Y.~Yuan, J.~Wang, C.~He, Z.~Liu, {\em et~al.}, ``Mmbench: Is your multi-modal model an all-around player?,'' {\em arXiv preprint arXiv:2307.06281}, 2023.

\bibitem{lu2023mathvista}
P.~Lu, H.~Bansal, T.~Xia, J.~Liu, C.~Li, H.~Hajishirzi, H.~Cheng, K.-W. Chang, M.~Galley, and J.~Gao, ``Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts,'' {\em arXiv preprint arXiv:2310.02255}, 2023.

\bibitem{kembhavi2016diagram}
A.~Kembhavi, M.~Salvato, E.~Kolve, M.~Seo, H.~Hajishirzi, and A.~Farhadi, ``A diagram is worth a dozen images,'' in {\em Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV 14}, pp.~235--251, Springer, 2016.

\bibitem{gu2023mamba}
A.~Gu and T.~Dao, ``Mamba: Linear-time sequence modeling with selective state spaces,'' {\em arXiv preprint arXiv:2312.00752}, 2023.

\bibitem{gao2023g}
J.~Gao, R.~Pi, J.~Zhang, J.~Ye, W.~Zhong, Y.~Wang, L.~Hong, J.~Han, H.~Xu, Z.~Li, {\em et~al.}, ``G-llava: Solving geometric problem with multi-modal large language model,'' {\em arXiv preprint arXiv:2312.11370}, 2023.

\bibitem{wang2024measuring}
K.~Wang, J.~Pan, W.~Shi, Z.~Lu, M.~Zhan, and H.~Li, ``Measuring multimodal mathematical reasoning with math-vision dataset,'' {\em arXiv preprint arXiv:2402.14804}, 2024.

\bibitem{yue2023mammoth}
X.~Yue, X.~Qu, G.~Zhang, Y.~Fu, W.~Huang, H.~Sun, Y.~Su, and W.~Chen, ``Mammoth: Building math generalist models through hybrid instruction tuning,'' {\em arXiv preprint arXiv:2309.05653}, 2023.

\bibitem{yue2024mammoth2}
X.~Yue, T.~Zheng, G.~Zhang, and W.~Chen, ``Mammoth2: Scaling instructions from the web,'' 2024.

\bibitem{claude3series2024}
{Anthropic}, ``The claude 3 model family: Opus, sonnet, haiku.'' \url{https://www.anthropic.com}, 2024.

\bibitem{strout2019human}
J.~Strout, Y.~Zhang, and R.~Mooney, ``Do human rationales improve machine explanations?,'' in {\em Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP}, pp.~56--62, 2019.

\bibitem{lu2022rationale}
J.~Lu, L.~Yang, B.~Namee, and Y.~Zhang, ``A rationale-centric framework for human-in-the-loop machine learning,'' in {\em Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.~6986--6996, 2022.

\bibitem{hsieh2023distilling}
C.-Y. Hsieh, C.-L. Li, C.-K. Yeh, H.~Nakhost, Y.~Fujii, A.~Ratner, R.~Krishna, C.-Y. Lee, and T.~Pfister, ``Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes,'' {\em arXiv preprint arXiv:2305.02301}, 2023.

\bibitem{wang2023scott}
P.~Wang, Z.~Wang, Z.~Li, Y.~Gao, B.~Yin, and X.~Ren, ``Scott: Self-consistent chain-of-thought distillation,'' in {\em Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.~5546--5558, 2023.

\bibitem{xiong2023rationale}
W.~Xiong, Y.~Song, P.~Wang, and S.~Li, ``Rationale-enhanced language models are better continual relation learners,'' in {\em Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pp.~15489--15497, 2023.

\bibitem{li2023symbolic}
L.~H. Li, J.~Hessel, Y.~Yu, X.~Ren, K.-W. Chang, and Y.~Choi, ``Symbolic chain-of-thought distillation: Small models can also “think” step-by-step,'' in {\em Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.~2665--2679, 2023.

\bibitem{kang2024knowledge}
M.~Kang, S.~Lee, J.~Baek, K.~Kawaguchi, and S.~J. Hwang, ``Knowledge-augmented reasoning distillation for small language models in knowledge-intensive tasks,'' {\em Advances in Neural Information Processing Systems}, vol.~36, 2024.

\bibitem{wei2022chain}
J.~Wei, X.~Wang, D.~Schuurmans, M.~Bosma, F.~Xia, E.~Chi, Q.~V. Le, D.~Zhou, {\em et~al.}, ``Chain-of-thought prompting elicits reasoning in large language models,'' {\em Advances in neural information processing systems}, vol.~35, pp.~24824--24837, 2022.

\bibitem{zhang2022automatic}
Z.~Zhang, A.~Zhang, M.~Li, and A.~Smola, ``Automatic chain of thought prompting in large language models,'' {\em arXiv preprint arXiv:2210.03493}, 2022.

\bibitem{shum2023automatic}
K.~Shum, S.~Diao, and T.~Zhang, ``Automatic prompt augmentation and selection with chain-of-thought from labeled data,'' in {\em Findings of the Association for Computational Linguistics: EMNLP 2023}, pp.~12113--12139, 2023.

\bibitem{krishna2024post}
S.~Krishna, J.~Ma, D.~Slack, A.~Ghandeharioun, S.~Singh, and H.~Lakkaraju, ``Post hoc explanations of language models can improve language models,'' {\em Advances in Neural Information Processing Systems}, vol.~36, 2024.

\bibitem{chen2023shikra}
K.~Chen, Z.~Zhang, W.~Zeng, R.~Zhang, F.~Zhu, and R.~Zhao, ``Shikra: Unleashing multimodal llm's referential dialogue magic,'' {\em arXiv preprint arXiv:2306.15195}, 2023.

\bibitem{laurenccon2023obelisc}
H.~Lauren{\c{c}}on, L.~Saulnier, L.~Tronchon, S.~Bekman, A.~Singh, A.~Lozhkov, T.~Wang, S.~Karamcheti, A.~M. Rush, D.~Kiela, {\em et~al.}, ``Obelisc: An open web-scale filtered dataset of interleaved image-text documents,'' {\em arXiv preprint arXiv:2306.16527}, 2023.

\bibitem{zhu2023minigpt}
D.~Zhu, J.~Chen, X.~Shen, X.~Li, and M.~Elhoseiny, ``Minigpt-4: Enhancing vision-language understanding with advanced large language models,'' {\em arXiv preprint arXiv:2304.10592}, 2023.

\bibitem{li2023otter}
B.~Li, Y.~Zhang, L.~Chen, J.~Wang, J.~Yang, and Z.~Liu, ``Otter: A multi-modal model with in-context instruction tuning,'' {\em arXiv preprint arXiv:2305.03726}, 2023.

\bibitem{ye2023mplug}
Q.~Ye, H.~Xu, G.~Xu, J.~Ye, M.~Yan, Y.~Zhou, J.~Wang, A.~Hu, P.~Shi, Y.~Shi, {\em et~al.}, ``mplug-owl: Modularization empowers large language models with multimodality,'' {\em arXiv preprint arXiv:2304.14178}, 2023.

\bibitem{2023xtuner}
X.~Contributors, ``Xtuner: A toolkit for efficiently fine-tuning llm.'' \url{https://github.com/InternLM/xtuner}, 2023.

\bibitem{zhang2023internlm}
P.~Zhang, X.~D.~B. Wang, Y.~Cao, C.~Xu, L.~Ouyang, Z.~Zhao, S.~Ding, S.~Zhang, H.~Duan, H.~Yan, {\em et~al.}, ``Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition,'' {\em arXiv preprint arXiv:2309.15112}, 2023.

\bibitem{chen2024far}
Z.~Chen, W.~Wang, H.~Tian, S.~Ye, Z.~Gao, E.~Cui, W.~Tong, K.~Hu, J.~Luo, Z.~Ma, {\em et~al.}, ``How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites,'' {\em arXiv preprint arXiv:2404.16821}, 2024.

\bibitem{chen2024allava}
G.~H. Chen, S.~Chen, R.~Zhang, J.~Chen, X.~Wu, Z.~Zhang, Z.~Chen, J.~Li, X.~Wan, and B.~Wang, ``Allava: Harnessing gpt4v-synthesized data for a lite vision-language model,'' {\em arXiv preprint arXiv:2402.11684}, 2024.

\bibitem{zong2024mova}
Z.~Zong, B.~Ma, D.~Shen, G.~Song, H.~Shao, D.~Jiang, H.~Li, and Y.~Liu, ``Mova: Adapting mixture of vision experts to multimodal context,'' {\em arXiv preprint arXiv:2404.13046}, 2024.

\bibitem{clip}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry, A.~Askell, P.~Mishkin, J.~Clark, G.~Krueger, and I.~Sutskever, ``Learning transferable visual models from natural language supervision,'' in {\em Proceedings of the 38th International Conference on Machine Learning} (M.~Meila and T.~Zhang, eds.), vol.~139 of {\em Proceedings of Machine Learning Research}, pp.~8748--8763, PMLR, 18--24 Jul 2021.

\bibitem{hendrycks2016gaussian}
D.~Hendrycks and K.~Gimpel, ``Gaussian error linear units (gelus),'' {\em arXiv preprint arXiv:1606.08415}, 2016.

\bibitem{2023internlm}
I.~Team, ``Internlm: A multilingual language model with progressively enhanced capabilities.'' \url{https://github.com/InternLM/InternLM-techreport}, 2023.

\bibitem{cai2024internlm2}
Z.~Cai, M.~Cao, H.~Chen, K.~Chen, K.~Chen, X.~Chen, X.~Chen, Z.~Chen, Z.~Chen, P.~Chu, {\em et~al.}, ``Internlm2 technical report,'' {\em arXiv preprint arXiv:2403.17297}, 2024.

\bibitem{ouyang2022training}
L.~Ouyang, J.~Wu, X.~Jiang, D.~Almeida, C.~Wainwright, P.~Mishkin, C.~Zhang, S.~Agarwal, K.~Slama, A.~Ray, {\em et~al.}, ``Training language models to follow instructions with human feedback,'' {\em Advances in Neural Information Processing Systems}, vol.~35, pp.~27730--27744, 2022.

\bibitem{schuhmann2022laion}
C.~Schuhmann, R.~Beaumont, R.~Vencu, C.~Gordon, R.~Wightman, M.~Cherti, T.~Coombes, A.~Katta, C.~Mullis, M.~Wortsman, {\em et~al.}, ``Laion-5b: An open large-scale dataset for training next generation image-text models,'' {\em Advances in Neural Information Processing Systems}, vol.~35, pp.~25278--25294, 2022.

\bibitem{changpinyo2021conceptual}
S.~Changpinyo, P.~Sharma, N.~Ding, and R.~Soricut, ``Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts,'' in {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.~3558--3568, 2021.

\bibitem{saleh2015large}
B.~Saleh and A.~Elgammal, ``Large-scale classification of fine-art paintings: Learning the right metric on the right feature,'' {\em arXiv preprint arXiv:1505.00855}, 2015.

\bibitem{lin2014microsoft}
T.-Y. Lin, M.~Maire, S.~Belongie, J.~Hays, P.~Perona, D.~Ramanan, P.~Doll{\'a}r, and C.~L. Zitnick, ``Microsoft coco: Common objects in context,'' in {\em Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13}, pp.~740--755, Springer, 2014.

\bibitem{sidorov2020textcaps}
O.~Sidorov, R.~Hu, M.~Rohrbach, and A.~Singh, ``Textcaps: a dataset for image captioning with reading comprehension,'' in {\em Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part II 16}, pp.~742--758, Springer, 2020.

\bibitem{ordonez2011im2text}
V.~Ordonez, G.~Kulkarni, and T.~Berg, ``Im2text: Describing images using 1 million captioned photographs,'' {\em Advances in neural information processing systems}, vol.~24, 2011.

\bibitem{schuhmann2021laion}
C.~Schuhmann, R.~Vencu, R.~Beaumont, R.~Kaczmarczyk, C.~Mullis, A.~Katta, T.~Coombes, J.~Jitsev, and A.~Komatsuzaki, ``Laion-400m: Open dataset of clip-filtered 400 million image-text pairs,'' {\em arXiv preprint arXiv:2111.02114}, 2021.

\bibitem{sharma2018conceptual}
P.~Sharma, N.~Ding, S.~Goodman, and R.~Soricut, ``Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning,'' in {\em Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.~2556--2565, 2018.

\bibitem{mathew2021docvqa}
M.~Mathew, D.~Karatzas, and C.~Jawahar, ``Docvqa: A dataset for vqa on document images,'' in {\em Proceedings of the IEEE/CVF winter conference on applications of computer vision}, pp.~2200--2209, 2021.

\bibitem{masry2022chartqa}
A.~Masry, D.~X. Long, J.~Q. Tan, S.~Joty, and E.~Hoque, ``Chartqa: A benchmark for question answering about charts with visual and logical reasoning,'' {\em arXiv preprint arXiv:2203.10244}, 2022.

\bibitem{kafle2018dvqa}
K.~Kafle, B.~Price, S.~Cohen, and C.~Kanan, ``Dvqa: Understanding data visualizations via question answering,'' in {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.~5648--5656, 2018.

\bibitem{svetlichnaya2020deepform}
S.~Svetlichnaya, ``Deepform: Understand structured documents at scale,'' 2020.

\bibitem{mathew2022infographicvqa}
M.~Mathew, V.~Bagal, R.~Tito, D.~Karatzas, E.~Valveny, and C.~Jawahar, ``Infographicvqa,'' in {\em Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pp.~1697--1706, 2022.

\bibitem{stanislawek2021kleister}
T.~Stanis{\l}awek, F.~Grali{\'n}ski, A.~Wr{\'o}blewska, D.~Lipi{\'n}ski, A.~Kaliska, P.~Rosalska, B.~Topolski, and P.~Biecek, ``Kleister: key information extraction datasets involving long documents with complex layouts,'' in {\em International Conference on Document Analysis and Recognition}, pp.~564--579, Springer, 2021.

\bibitem{chen2019tabfact}
W.~Chen, H.~Wang, J.~Chen, Y.~Zhang, H.~Wang, S.~Li, X.~Zhou, and W.~Y. Wang, ``Tabfact: A large-scale dataset for table-based fact verification,'' {\em arXiv preprint arXiv:1909.02164}, 2019.

\bibitem{singh2019towards}
A.~Singh, V.~Natarajan, M.~Shah, Y.~Jiang, X.~Chen, D.~Batra, D.~Parikh, and M.~Rohrbach, ``Towards vqa models that can read,'' in {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.~8317--8326, 2019.

\bibitem{pasupat-liang-2015-compositional}
P.~Pasupat and P.~Liang, ``Compositional semantic parsing on semi-structured tables,'' in {\em Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)} (C.~Zong and M.~Strube, eds.), (Beijing, China), pp.~1470--1480, Association for Computational Linguistics, July 2015.

\bibitem{tanaka2021visualmrc}
R.~Tanaka, K.~Nishida, and S.~Yoshida, ``Visualmrc: Machine reading comprehension on document images,'' in {\em Proceedings of the AAAI Conference on Artificial Intelligence}, vol.~35, pp.~13878--13888, 2021.

\bibitem{yasunaga2022retrieval}
M.~Yasunaga, A.~Aghajanyan, W.~Shi, R.~James, J.~Leskovec, P.~Liang, M.~Lewis, L.~Zettlemoyer, and W.-t. Yih, ``Retrieval-augmented multimodal language modeling,'' {\em arXiv preprint arXiv:2211.12561}, 2022.

\bibitem{blip2}
J.~Li, D.~Li, S.~Savarese, and S.~Hoi, ``Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models,'' {\em arXiv preprint arXiv:2301.12597}, 2023.

\bibitem{li2023monkey}
Z.~Li, B.~Yang, Q.~Liu, Z.~Ma, S.~Zhang, J.~Yang, Y.~Sun, Y.~Liu, and X.~Bai, ``Monkey: Image resolution and text label are important things for large multi-modal models,'' {\em arXiv preprint arXiv:2311.06607}, 2023.

\bibitem{lin2023vila}
J.~Lin, H.~Yin, W.~Ping, Y.~Lu, P.~Molchanov, A.~Tao, H.~Mao, J.~Kautz, M.~Shoeybi, and S.~Han, ``Vila: On pre-training for visual language models,'' {\em arXiv preprint arXiv:2312.07533}, 2023.

\bibitem{lin2023sphinx}
Z.~Lin, C.~Liu, R.~Zhang, P.~Gao, L.~Qiu, H.~Xiao, H.~Qiu, C.~Lin, W.~Shao, K.~Chen, {\em et~al.}, ``Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models,'' {\em arXiv preprint arXiv:2311.07575}, 2023.

\bibitem{gao2024sphinx}
P.~Gao, R.~Zhang, C.~Liu, L.~Qiu, S.~Huang, W.~Lin, S.~Zhao, S.~Geng, Z.~Lin, P.~Jin, {\em et~al.}, ``Sphinx-x: Scaling data and parameters for a family of multi-modal large language models,'' {\em arXiv preprint arXiv:2402.05935}, 2024.

\bibitem{wu2023q}
H.~Wu, Z.~Zhang, E.~Zhang, C.~Chen, L.~Liao, A.~Wang, C.~Li, W.~Sun, Q.~Yan, G.~Zhai, {\em et~al.}, ``Q-bench: A benchmark for general-purpose foundation models on low-level vision,'' {\em arXiv preprint arXiv:2309.14181}, 2023.

\bibitem{lu2022learn}
P.~Lu, S.~Mishra, T.~Xia, L.~Qiu, K.-W. Chang, S.-C. Zhu, O.~Tafjord, P.~Clark, and A.~Kalyan, ``Learn to explain: Multimodal reasoning via thought chains for science question answering,'' {\em Advances in Neural Information Processing Systems}, vol.~35, pp.~2507--2521, 2022.

\bibitem{li2023seed}
B.~Li, R.~Wang, G.~Wang, Y.~Ge, Y.~Ge, and Y.~Shan, ``Seed-bench: Benchmarking multimodal llms with generative comprehension,'' {\em arXiv preprint arXiv:2307.16125}, 2023.

\bibitem{li2023evaluating}
Y.~Li, Y.~Du, K.~Zhou, J.~Wang, W.~X. Zhao, and J.-R. Wen, ``Evaluating object hallucination in large vision-language models,'' {\em arXiv preprint arXiv:2305.10355}, 2023.

\bibitem{liu2023hallusionbench}
F.~Liu, T.~Guan, Z.~Li, L.~Chen, Y.~Yacoob, D.~Manocha, and T.~Zhou, ``Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models,'' {\em arXiv preprint arXiv:2310.14566}, 2023.

\bibitem{yu2023mm}
W.~Yu, Z.~Yang, L.~Li, J.~Wang, K.~Lin, Z.~Liu, X.~Wang, and L.~Wang, ``Mm-vet: Evaluating large multimodal models for integrated capabilities,'' {\em arXiv preprint arXiv:2308.02490}, 2023.

\bibitem{chen2024we}
L.~Chen, J.~Li, X.~Dong, P.~Zhang, Y.~Zang, Z.~Chen, H.~Duan, J.~Wang, Y.~Qiao, D.~Lin, {\em et~al.}, ``Are we on the right way for evaluating large vision-language models?,'' {\em arXiv preprint arXiv:2403.20330}, 2024.

\bibitem{zhang2024mathverse}
R.~Zhang, D.~Jiang, Y.~Zhang, H.~Lin, Z.~Guo, P.~Qiu, A.~Zhou, P.~Lu, K.-W. Chang, P.~Gao, {\em et~al.}, ``Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?,'' {\em arXiv preprint arXiv:2403.14624}, 2024.

\bibitem{kudo2018sentencepiece}
T.~Kudo and J.~Richardson, ``Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,'' {\em arXiv preprint arXiv:1808.06226}, 2018.

\bibitem{kalamkar2019study}
D.~Kalamkar, D.~Mudigere, N.~Mellempudi, D.~Das, K.~Banerjee, S.~Avancha, D.~T. Vooturi, N.~Jammalamadaka, J.~Huang, H.~Yuen, {\em et~al.}, ``A study of bfloat16 for deep learning training,'' {\em arXiv preprint arXiv:1905.12322}, 2019.

\bibitem{dettmers2023qlora}
T.~Dettmers, A.~Pagnoni, A.~Holtzman, and L.~Zettlemoyer, ``Qlora: Efficient finetuning of quantized llms,'' {\em arXiv preprint arXiv:2305.14314}, 2023.

\bibitem{hu2021lora}
E.~J. Hu, Y.~Shen, P.~Wallis, Z.~Allen-Zhu, Y.~Li, S.~Wang, L.~Wang, and W.~Chen, ``Lora: Low-rank adaptation of large language models,'' {\em arXiv preprint arXiv:2106.09685}, 2021.

\bibitem{loshchilov2018decoupled}
I.~Loshchilov and F.~Hutter, ``Decoupled weight decay regularization,'' in {\em International Conference on Learning Representations}, 2019.

\bibitem{loshchilov2016sgdr}
I.~Loshchilov and F.~Hutter, ``Sgdr: Stochastic gradient descent with warm restarts,'' {\em arXiv preprint arXiv:1608.03983}, 2016.

\bibitem{sohoni2019low}
N.~S. Sohoni, C.~R. Aberger, M.~Leszczynski, J.~Zhang, and C.~R{\'e}, ``Low-memory neural network training: A technical report,'' {\em arXiv preprint arXiv:1904.10631}, 2019.

\bibitem{freitag-al-onaizan-2017-beam}
M.~Freitag and Y.~Al-Onaizan, ``Beam search strategies for neural machine translation,'' in {\em Proceedings of the First Workshop on Neural Machine Translation} (T.~Luong, A.~Birch, G.~Neubig, and A.~Finch, eds.), (Vancouver), pp.~56--60, Association for Computational Linguistics, Aug. 2017.

\bibitem{dao2022flashattention}
T.~Dao, D.~Fu, S.~Ermon, A.~Rudra, and C.~R{\'e}, ``Flashattention: Fast and memory-efficient exact attention with io-awareness,'' {\em Advances in Neural Information Processing Systems}, vol.~35, pp.~16344--16359, 2022.

\bibitem{dao2023flashattention}
T.~Dao, ``Flashattention-2: Faster attention with better parallelism and work partitioning,'' {\em arXiv preprint arXiv:2307.08691}, 2023.

\bibitem{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep bidirectional transformers for language understanding,'' {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever, {\em et~al.}, ``Language models are unsupervised multitask learners,''

\bibitem{yang2019xlnet}
Z.~Yang, Z.~Dai, Y.~Yang, J.~Carbonell, R.~R. Salakhutdinov, and Q.~V. Le, ``Xlnet: Generalized autoregressive pretraining for language understanding,'' {\em Advances in neural information processing systems}, vol.~32, 2019.

\bibitem{raposo2024mixture}
D.~Raposo, S.~Ritter, B.~Richards, T.~Lillicrap, P.~C. Humphreys, and A.~Santoro, ``Mixture-of-depths: Dynamically allocating compute in transformer-based language models,'' {\em arXiv preprint arXiv:2404.02258}, 2024.

\bibitem{lee2020towards}
B.-K. Lee, Y.~Yu, and Y.~M. Ro, ``Towards adversarial robustness of bayesian neural network through hierarchical variational inference,'' 2021.

\bibitem{kim2021distilling}
J.~Kim, B.-K. Lee, and Y.~M. Ro, ``Distilling robust and non-robust features in adversarial examples by information bottleneck,'' {\em Advances in Neural Information Processing Systems}, vol.~34, pp.~17148--17159, 2021.

\bibitem{lee2022masking}
B.-K. Lee, J.~Kim, and Y.~M. Ro, ``Masking adversarial damage: Finding adversarial saliency for robust and sparse network,'' in {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.~15126--15136, 2022.

\bibitem{kim2023demystifying}
J.~Kim, B.-K. Lee, and Y.~M. Ro, ``Demystifying causal features on adversarial examples and causal inoculation for robust network by adversarial instrumental variable regression,'' in {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.~12302--12312, 2023.

\bibitem{lee2023mitigating}
B.-K. Lee, J.~Kim, and Y.~M. Ro, ``Mitigating adversarial vulnerability through causal parameter estimation by adversarial double machine learning,'' in {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.~4499--4509, 2023.

\bibitem{kim2023causal}
J.~Kim, B.-K. Lee, and Y.~M. Ro, ``Causal unsupervised semantic segmentation,'' {\em arXiv preprint arXiv:2310.07379}, 2023.

\bibitem{kim2023mitigating}
Y.~Kim, J.~Kim, B.-K. Lee, S.~Shin, and Y.~M. Ro, ``Mitigating dataset bias in image captioning through clip confounder-free captioning network,'' in {\em 2023 IEEE International Conference on Image Processing (ICIP)}, pp.~1720--1724, IEEE, 2023.

\bibitem{llavahr}
G.~Luo, Y.~Zhou, Y.~Zhang, X.~Zheng, X.~Sun, and R.~Ji, ``Feast your eyes: Mixture-of-resolution adaptation for multimodal large language models,'' {\em arXiv preprint arXiv:2403.03003}, 2024.

\bibitem{zhao2024cobra}
H.~Zhao, M.~Zhang, W.~Zhao, P.~Ding, S.~Huang, and D.~Wang, ``Cobra: Extending mamba to multi-modal large language model for efficient inference,'' {\em arXiv preprint arXiv:2403.14520}, 2024.

\bibitem{qiao2024vlmamba}
Y.~Qiao, Z.~Yu, L.~Guo, S.~Chen, Z.~Zhao, M.~Sun, Q.~Wu, and J.~Liu, ``Vl-mamba: Exploring state space models for multimodal learning,'' {\em arXiv preprint arXiv:2403.13600}, 2024.

\bibitem{liu2024robomamba}
J.~Liu, M.~Liu, Z.~Wang, L.~Lee, K.~Zhou, P.~An, S.~Yang, R.~Zhang, Y.~Guo, and S.~Zhang, ``Robomamba: Multimodal state space model for efficient robot reasoning and manipulation,'' {\em arXiv preprint arXiv:2406.04339}, 2024.

\bibitem{huang2024mlmamba}
W.~Huang and J.~Hu, ``Ml-mamba: Efficient multi-modal large language model utilizing mamba-2,'' {\em arXiv preprint arXiv:2407.19832}, 2024.

\end{thebibliography}
