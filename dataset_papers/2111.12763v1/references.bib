
# Sparse Sinkhorn Attention
# https://arxiv.org/abs/2002.11296
@misc{tay2020sparse,
      title={Sparse Sinkhorn Attention}, 
      author={Yi Tay and Dara Bahri and Liu Yang and Donald Metzler and Da-Cheng Juan},
      year={2020},
      eprint={2002.11296},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

# Efficient Transformers: A Survey
# https://arxiv.org/pdf/2009.06732.pdf

@article{tay2020efficient,
  title={Efficient transformers: A survey},
  author={Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal={arXiv preprint arXiv:2009.06732},
  year={2020}
}

@article{kb2018discrete,
      title={Discrete Autoencoders for Sequence Models}, 
      author={\L{}ukasz Kaiser and Samy Bengio},
      year={2018},
      journal={arXiv preprint arXiv:1801.09797},
}

# performer go/performer

@article{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}

@article{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}

# Efficient Content-Based Sparse Attention with Routing Transformers
# Aurko Roy and Mohammad Saffar and Ashish Vaswani and David Grangier
# https://arxiv.org/pdf/2003.05997.pdf

@article{roy2020efficient,
  title={Efficient content-based sparse attention with routing transformers},
  author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal={arXiv preprint arXiv:2003.05997},
  year={2020}
}


@article{lepikhin2020gshard,
  title={Gshard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}

@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

@article{kaiser2017one,
  title={One model to learn them all},
  author={Kaiser, Lukasz and Gomez, Aidan N and Shazeer, Noam and Vaswani, Ashish and Parmar, Niki and Jones, Llion and Uszkoreit, Jakob},
  journal={arXiv preprint arXiv:1706.05137},
  year={2017}
}

@inproceedings{shazeer2018mesh,
  title={Mesh-tensorflow: Deep learning for supercomputers},
  author={Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10414--10423},
  year={2018}
}

@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@article{chen2015strategies,
  title={Strategies for training large vocabulary neural language models},
  author={Chen, Welin and Grangier, David and Auli, Michael},
  journal={arXiv preprint arXiv:1512.04906},
  year={2015}
}

@article{sru,
  author    = {Tao Lei and
               Yu Zhang and
               Yoav Artzi},
  title     = {Training RNNs as Fast as CNNs},
  journal   = {CoRR},
  volume    = {abs/1709.02755},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.02755},
  archivePrefix = {arXiv},
  eprint    = {1709.02755},
  timestamp = {Mon, 13 Aug 2018 16:46:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1709-02755.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{choi2020f,
  title={F\^{} 2-Softmax: Diversifying Neural Text Generation via Frequency Factorized Softmax},
  author={Choi, Byung-Ju and Hong, Jimin and Park, David Keetae and Lee, Sang Wan},
  journal={arXiv preprint arXiv:2009.09417},
  year={2020}
}

@article{kim2020fastformers,
  title={Fastformers: Highly efficient transformer models for natural language understanding},
  author={Kim, Young Jin and Awadalla, Hany Hassan},
  journal={arXiv preprint arXiv:2010.13382},
  year={2020}
}

@article{kaiser2018discrete,
  title={Discrete autoencoders for sequence models},
  author={Kaiser, {\L}ukasz and Bengio, Samy},
  journal={arXiv preprint arXiv:1801.09797},
  year={2018}
}

@article{tsai2020finding,
  title={Finding Fast Transformers: One-Shot Neural Architecture Search by Component Composition},
  author={Tsai, Henry and Ooi, Jayden and Ferng, Chun-Sung and Chung, Hyung Won and Riesa, Jason},
  journal={arXiv preprint arXiv:2008.06808},
  year={2020}
}

@article{li2019enhancing,
  title={Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting},
  author={Li, Shiyang and Jin, Xiaoyong and Xuan, Yao and Zhou, Xiyou and Chen, Wenhu and Wang, Yu-Xiang and Yan, Xifeng},
  journal={arXiv preprint arXiv:1907.00235},
  year={2019}
}

@article{gupta2020compression,
  title={Compression of Deep Learning Models for Text: A Survey},
  author={Gupta, Manish and Agrawal, Puneet},
  journal={arXiv preprint arXiv:2008.05221},
  year={2020}
}

@article{mehta2020delight,
  title={DeLighT: Very Deep and Light-weight Transformer},
  author={Mehta, Sachin and Ghazvininejad, Marjan and Iyer, Srinivasan and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2008.00623},
  year={2020}
}

@misc{xue2020mt5,
      title={mT5: A massively multilingual pre-trained text-to-text transformer}, 
      author={Linting Xue and Noah Constant and Adam Roberts and Mihir Kale and Rami Al-Rfou and Aditya Siddhant and Aditya Barua and Colin Raffel},
      year={2020},
      eprint={2010.11934},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{raffel2020exploring,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  pages={1--67},
  year={2020}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{shazeer2019fast,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}

@inproceedings{sun2020mobilebert,
  title={MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices},
  author={Sun, Zhiqing and Yu, Hongkun and Song, Xiaodan and Liu, Renjie and Yang, Yiming and Zhou, Denny},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={2158--2170},
  year={2020}
}

@article{iandola2020squeezebert,
  title={SqueezeBERT: What can computer vision teach NLP about efficient neural networks?},
  author={Iandola, Forrest N and Shaw, Albert E and Krishna, Ravi and Keutzer, Kurt W},
  journal={arXiv preprint arXiv:2006.11316},
  year={2020}
}

@article{maziarz2019gumbel,
  title={Gumbel-Matrix Routing for Flexible Multi-task Learning},
  author={Maziarz, Krzysztof and Kokiopoulou, Efi and Gesmundo, Andrea and Sbaiz, Luciano and Bartok, Gabor and Berent, Jesse},
  journal={arXiv preprint arXiv:1910.04915},
  year={2019}
}

@article{sun2019adashare,
  title={AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning},
  author={Sun, Ximeng and Panda, Rameswar and Feris, Rogerio},
  journal={arXiv preprint arXiv:1911.12423},
  year={2019}
}

@article{jang2016categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}

@article{brix2020successfully,
  title={Successfully applying the stabilized lottery ticket hypothesis to the transformer architecture},
  author={Brix, Christopher and Bahar, Parnia and Ney, Hermann},
  journal={arXiv preprint arXiv:2005.03454},
  year={2020}
}

@article{li2020efficient,
  title={Efficient Transformer-based Large Scale Language Representations using Hardware-friendly Block Structured Pruning},
  author={Li, Bingbing and Kong, Zhenglun and Zhang, Tianyun and Li, Ji and Li, Zhengang and Liu, Hang and Ding, Caiwen},
  journal={arXiv preprint arXiv:2009.08065},
  year={2020}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V},
  journal={arXiv preprint arXiv:1906.08237},
  year={2019}
}

@article{tsunoo2020streaming,
  title={Streaming Transformer ASR with Blockwise Synchronous Beam Search},
  author={Tsunoo, Emiru and Kashiwagi, Yosuke and Watanabe, Shinji},
  journal={arXiv preprint arXiv:2006.14941},
  year={2020}
}

@article{energyuse,
  title={Energy and Policy Considerations for Deep Learning in NLP},
  author={Emma Strubell and Ananya Ganesh and Andrew McCallum},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019}
}

@article{longseq,
  title={Long Range Arena: A Benchmark for Efficient Transformers},
  author={Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and
          Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
  journal={arXiv preprint arXiv:2011.04006},
  year={2020}
}

@article{switchtransformer,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={William Fedus and Barret Zoph and Noam Shazeer},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021}
}

@article{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeffrey Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019}
}

@inproceedings{moritz2020streaming,
  title={Streaming automatic speech recognition with the transformer model},
  author={Moritz, Niko and Hori, Takaaki and Le, Jonathan},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6074--6078},
  year={2020},
  organization={IEEE}
}

@inproceedings{
zhou2021learning,
title={Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch},
author={Aojun Zhou and Yukun Ma and Junnan Zhu and Jianbo Liu and Zhijie Zhang and Kun Yuan and Wenxiu Sun and Hongsheng Li},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=K9bw7vqp_s}
}

@misc{NvidiaAmpereArch,
author={},
title="{Nvidia Ampere Architecture}",
note={ \newline https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/},
}

@article{neuralgpu,
  title={Neural gpus learn algorithms},
  author={Kaiser, {\L}ukasz and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1511.08228},
  year={2015}
}

@article{tmods,
  title={Do Transformer Modifications Transfer Across Implementations and Applications?},
  author={Sharan Narang and Hyung Won Chung and Yi Tay and William Fedus and Thibault Fevry and Michael Matena and Karishma Malkan and Noah Fiedel and Noam Shazeer and Zhenzhong Lan and Yanqi Zhou and Wei Li and Nan Ding and Jake Marcus and Adam Roberts and Colin Raffel},
  journal={arXiv preprint arXiv:2102.11972},
  year={2021}
}

@article{albert,
  author    = {Zhenzhong Lan and
               Mingda Chen and
               Sebastian Goodman and
               Kevin Gimpel and
               Piyush Sharma and
               Radu Soricut},
  title     = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language
               Representations},
  journal   = {CoRR},
  volume    = {abs/1909.11942},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.11942},
  archivePrefix = {arXiv},
  eprint    = {1909.11942},
  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-11942.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{glue,
  author    = {Alex Wang and
               Amanpreet Singh and
               Julian Michael and
               Felix Hill and
               Omer Levy and
               Samuel R. Bowman},
  title     = {{GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural
               Language Understanding},
  journal   = {CoRR},
  volume    = {abs/1804.07461},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.07461},
  archivePrefix = {arXiv},
  eprint    = {1804.07461},
  timestamp = {Mon, 13 Aug 2018 16:46:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-07461.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{zhang2020pegasus,
      title={PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization}, 
      author={Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu},
      year={2020},
      eprint={1912.08777},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Cohan_2018,
   title={A Discourse-Aware Attention Model for Abstractive Summarization of
            Long Documents},
   url={http://dx.doi.org/10.18653/v1/n18-2097},
   DOI={10.18653/v1/n18-2097},
   journal={Proceedings of the 2018 Conference of the North American Chapter of
          the Association for Computational Linguistics: Human Language
          Technologies, Volume 2 (Short Papers)},
   publisher={Association for Computational Linguistics},
   author={Cohan, Arman and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Kim, Seokhwan and Chang, Walter and Goharian, Nazli},
   year={2018}
}

@article{sukhbaatar2019augmenting,
  title={Augmenting self-attention with persistent memory},
  author={Sukhbaatar, Sainbayar and Grave, Edouard and Lample, Guillaume and Jegou, Herve and Joulin, Armand},
  journal={arXiv preprint arXiv:1907.01470},
  year={2019}
}
@article{sukhbaatar2019adaptive,
  title={Adaptive attention span in transformers},
  author={Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand},
  journal={arXiv preprint arXiv:1905.07799},
  year={2019}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@article{dehghani2018universal,
  title={Universal transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  journal={arXiv preprint arXiv:1807.03819},
  year={2018}
}

@inproceedings{shen2020q,
  title={Q-bert: Hessian based ultra low precision quantization of bert},
  author={Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={8815--8821},
  year={2020}
}

@article{sun2019hybrid,
  title={Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks},
  author={Sun, Xiao and Choi, Jungwook and Chen, Chia-Yu and Wang, Naigang and Venkataramani, Swagath and Srinivasan, Vijayalakshmi Viji and Cui, Xiaodong and Zhang, Wei and Gopalakrishnan, Kailash},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={4900--4909},
  year={2019}
}

@inproceedings{li2020train,
  title={Train Big, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers},
  author={Li, Zhuohan and Wallace, Eric and Shen, Sheng and Lin, Kevin and Keutzer, Kurt and Klein, Dan and Gonzalez, Joey},
  booktitle={International Conference on Machine Learning},
  pages={5958--5968},
  year={2020},
  organization={PMLR}
}

@article{prato2019fully,
  title={Fully quantized transformer for machine translation},
  author={Prato, Gabriele and Charlaix, Ella and Rezagholizadeh, Mehdi},
  journal={arXiv preprint arXiv:1910.10485},
  year={2019}
}

@article{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019}
}

@article{mccarley2019structured,
  title={Structured pruning of a bert-based question answering model},
  author={McCarley, JS and Chakravarti, Rishav and Sil, Avirup},
  journal={arXiv preprint arXiv:1910.06360},
  year={2019}
}

@article{wang2020hat,
  title={Hat: Hardware-aware transformers for efficient natural language processing},
  author={Wang, Hanrui and Wu, Zhanghao and Liu, Zhijian and Cai, Han and Zhu, Ligeng and Gan, Chuang and Han, Song},
  journal={arXiv preprint arXiv:2005.14187},
  year={2020}
}

@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={arXiv preprint arXiv:2007.14062},
  year={2020}
}

@article{dancerPegasus,  author={A. {Gidiotis} and G. {Tsoumakas}},  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},   title={A Divide-and-Conquer Approach to the Summarization of Long Documents},   year={2020},  volume={28},  number={},  pages={3029-3040},  doi={10.1109/TASLP.2020.3037401}}

@misc{RougeScorer,
author={},
title="{Python ROUGE Implementation}",
note={ \newline https://github.com/google-research/google-research/tree/master/rouge},
}

@article{patterson2021carbon,
  title={Carbon Emissions and Large Neural Network Training},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021}
}

@inproceedings{irie2020much,
  title={How Much Self-Attention Do We Need∆í Trading Attention for Feed-Forward Layers},
  author={Irie, Kazuki and Gerstenberger, Alexander and Schl{\"u}ter, Ralf and Ney, Hermann},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6154--6158},
  year={2020},
  organization={IEEE}
}

@inproceedings{lwc18,
 author = {He, Tianyu and Tan, Xu and Xia, Yingce and He, Di and Qin, Tao and Chen, Zhibo and Liu, Tie-Yan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 title = {Layer-Wise Coordination between Encoder and Decoder for Neural Machine Translation},
 volume = {31},
 year = {2018}
}
