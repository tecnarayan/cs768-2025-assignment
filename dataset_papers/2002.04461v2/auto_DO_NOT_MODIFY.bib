
@article{benamou_computational_2000,
  title = {A Computational Fluid Mechanics Solution to the {{Monge}}-{{Kantorovich}} Mass Transfer Problem},
  author = {Benamou, Jean-David and Brenier, Yann},
  year = {2000},
  volume = {84},
  pages = {375--393},
  abstract = {The L2 Monge-Kantorovich mass transfer problem [31] is reset in a fluid mechanics framework and numerically solved by an augmented Lagrangian method.},
  file = {/Users/alextong/Zotero/storage/G3ITFCQ8/Benamou and Brenier - 2000 - A computational fluid mechanics solution to the Mo.pdf},
  journal = {Numerische Mathematik},
  language = {en},
  number = {3}
}

@article{benamou_numerical_2003,
  title = {Numerical Resolution of an ``Unbalanced'' Mass Transport Problem},
  author = {Benamou, Jean-David},
  year = {2003},
  volume = {37},
  pages = {851--868},
  abstract = {We introduce a modification of the Monge\textendash Kantorovitch problem of exponent 2 which accommodates non balanced initial and final densities. The augmented Lagrangian numerical method introduced in [6] is adapted to this ``unbalanced'' problem. We illustrate the usability of this method on an idealized error estimation problem in meteorology.},
  file = {/Users/alextong/Zotero/storage/HZWQQWWX/Benamou - 2003 - Numerical resolution of an “unbalanced” mass trans.pdf},
  journal = {ESAIM: Mathematical Modelling and Numerical Analysis},
  language = {en},
  number = {5}
}

@article{benamou_second-order_2019,
  title = {Second-{{Order Models}} for {{Optimal Transport}} and {{Cubic Splines}} on the {{Wasserstein Space}}},
  author = {Benamou, Jean-David and Gallou{\"e}t, Thomas O. and Vialard, Fran{\c c}ois-Xavier},
  year = {2019},
  volume = {19},
  pages = {1113--1143},
  abstract = {On the space of probability densities, we extend the Wasserstein geodesics to the case of higher-order interpolation such as cubic spline interpolation. After presenting the natural extension of cubic splines to the Wasserstein space, we propose a simpler approach based on the relaxation of the variational problem on the path space. We explore two different numerical approaches, one based on multimarginal optimal transport and entropic regularization and the other based on semi-discrete optimal transport.},
  file = {/Users/alextong/Zotero/storage/7DC8IIIK/Benamou et al. - 2019 - Second-Order Models for Optimal Transport and Cubi.pdf},
  journal = {Foundations of Computational Mathematics},
  language = {en},
  number = {5}
}

@article{bendall_single-cell_2014,
  title = {Single-{{Cell Trajectory Detection Uncovers Progression}} and {{Regulatory Coordination}} in {{Human B Cell Development}}},
  author = {Bendall, Sean C. and Davis, Kara L. and Amir, El-ad David and Tadmor, Michelle D. and Simonds, Erin F. and Chen, Tiffany J. and Shenfeld, Daniel K. and Nolan, Garry P. and Pe'er, Dana},
  year = {2014},
  volume = {157},
  pages = {714--725},
  abstract = {Tissue regeneration is an orchestrated progression of cells from an immature state to a mature one, conventionally represented as distinctive cell subsets. A continuum of transitional cell states exists between these discrete stages. We combine the depth of single-cell mass cytometry and an algorithm developed to leverage this continuum by aligning single cells of a given lineage onto a unified trajectory that accurately predicts the developmental path de novo. Applied to human B cell lymphopoiesis, the algorithm (termed Wanderlust) constructed trajectories spanning from hematopoietic stem cells through to naive B cells. This trajectory revealed nascent fractions of B cell progenitors and aligned them with developmentally cued regulatory signaling including IL-7/STAT5 and cellular events such as immunoglobulin rearrangement, highlighting checkpoints across which regulatory signals are rewired paralleling changes in cellular state. This study provides a comprehensive analysis of human B lymphopoiesis, laying a foundation to apply this approach to other tissues and ``corrupted'' developmental processes including cancer.},
  file = {/Users/alextong/Zotero/storage/63M4WVYB/Bendall et al. - 2014 - Single-Cell Trajectory Detection Uncovers Progress.pdf},
  journal = {Cell},
  language = {en},
  number = {3}
}

@article{bergen_generalizing_2019,
  title = {Generalizing {{RNA}} Velocity to Transient Cell States through Dynamical Modeling},
  author = {Bergen, Volker and Lange, Marius and Peidli, Stefan and Wolf, F. Alexander and Theis, Fabian J.},
  year = {2019},
  abstract = {The introduction of RNA velocity in single cells has opened up new ways of studying cellular differentiation. The originally proposed framework obtains velocities as the deviation of the observed ratio of spliced and unspliced mRNA from an inferred steady state. Errors in velocity estimates arise if the central assumptions of a common splicing rate and the observation of the full splicing dynamics with steady-state mRNA levels are violated. With scVelo (https://scvelo.org), we address these restrictions by solving the full transcriptional dynamics of splicing kinetics using a likelihood-based dynamical model. This generalizes RNA velocity to a wide variety of systems comprising transient cell states, which are common in development and in response to perturbations. We infer gene-specific rates of transcription, splicing and degradation, and recover the latent time of the underlying cellular processes. This latent time represents the cell's internal clock and is based only on its transcriptional dynamics. Moreover, scVelo allows us to identify regimes of regulatory changes such as stages of cell fate commitment and, therein, systematically detects putative driver genes. We demonstrate that scVelo enables disentangling heterogeneous subpopulation kinetics with unprecedented resolution in hippocampal dentate gyrus neurogenesis and pancreatic endocrinogenesis. We anticipate that scVelo will greatly facilitate the study of lineage decisions, gene regulation, and pathway activity identification.},
  file = {/Users/alextong/Zotero/storage/VIPNHJ9S/Bergen et al. - 2019 - Generalizing RNA velocity to transient cell states.pdf},
  journal = {BioRxiv 820936},
  language = {en}
}

@article{chen_measure-valued_2018,
  title = {Measure-{{Valued Spline Curves}}: {{An Optimal Transport Viewpoint}}},
  shorttitle = {Measure-{{Valued Spline Curves}}},
  author = {Chen, Yongxin and Conforti, Giovanni and Georgiou, Tryphon T.},
  year = {2018},
  volume = {50},
  pages = {5947--5968},
  abstract = {The aim of this article is to introduce and address the problem to smoothly interpolate (empirical) probability measures. To this end, we lift the concept of a spline curve from the setting of points in a Euclidean space to that of probability measures, using the framework of optimal transport.},
  file = {/Users/alextong/Zotero/storage/DWGKPNKV/Chen et al. - 2018 - Measure-Valued Spline Curves An Optimal Transport.pdf},
  journal = {SIAM Journal on Mathematical Analysis},
  language = {en},
  number = {6}
}

@inproceedings{chen_neural_2018,
  title = {Neural {{Ordinary Differential Equations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  year = {2018},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  archivePrefix = {arXiv},
  eprint = {1806.07366},
  eprinttype = {arxiv},
  file = {/Users/alextong/Zotero/storage/N7M3QE2I/Chen et al. - 2018 - Neural Ordinary Differential Equations.pdf;/Users/alextong/Zotero/storage/9ZVFXGBM/1806.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{chizat_unbalanced_2018,
  title = {Unbalanced Optimal Transport: {{Dynamic}} and {{Kantorovich}} Formulations},
  shorttitle = {Unbalanced Optimal Transport},
  author = {Chizat, L{\'e}na{\"i}c and Peyr{\'e}, Gabriel and Schmitzer, Bernhard and Vialard, Fran{\c c}ois-Xavier},
  year = {2018},
  volume = {274},
  pages = {3090--3123},
  abstract = {This article presents a new class of distances between arbitrary nonnegative Radon measures inspired by optimal transport. These distances are defined by two equivalent alternative formulations: (i) a dynamic formulation defining the distance as a geodesic distance over the space of measures (ii) a static ``Kantorovich'' formulation where the distance is the minimum of an optimization problem over pairs of couplings describing the transfer (transport, creation and destruction) of mass between two measures. Both formulations are convex optimization problems, and the ability to switch from one to the other depending on the targeted application is a crucial property of our models. Of particular interest is the Wasserstein\textendash Fisher\textendash Rao metric recently introduced independently by [7,15]. Defined initially through a dynamic formulation, it belongs to this class of metrics and hence automatically benefits from a static Kantorovich formulation.},
  file = {/Users/alextong/Zotero/storage/EIMV32L4/Chizat et al. - 2018 - Unbalanced optimal transport Dynamic and Kantorov.pdf},
  journal = {Journal of Functional Analysis},
  language = {en},
  number = {11}
}

@article{cotney_autism-associated_2015,
  title = {The Autism-Associated Chromatin Modifier {{CHD8}} Regulates Other Autism Risk Genes during Human Neurodevelopment},
  author = {Cotney, Justin and Muhle, Rebecca A. and Sanders, Stephan J. and Liu, Li and Willsey, A. Jeremy and Niu, Wei and Liu, Wenzhong and Klei, Lambertus and Lei, Jing and Yin, Jun and Reilly, Steven K. and Tebbenkamp, Andrew T. and Bichsel, Candace and Pletikos, Mihovil and Sestan, Nenad and Roeder, Kathryn and State, Matthew W. and Devlin, Bernie and Noonan, James P.},
  year = {2015},
  volume = {6},
  pages = {6404},
  file = {/Users/alextong/Zotero/storage/YXSXS6QY/Cotney et al. - 2015 - The autism-associated chromatin modifier CHD8 regu.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@inproceedings{cuturi_sinkhorn_2013,
  title = {Sinkhorn {{Distances}}: {{Lightspeed Computation}} of {{Optimal Transport}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  author = {Cuturi, Marco},
  year = {2013},
  pages = {2292--2300},
  abstract = {Optimal transport distances are a fundamental family of distances for probability measures and histograms of features. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost can quickly become prohibitive whenever the size of the support of these measures or the histograms' dimension exceeds a few hundred. We propose in this work a new family of optimal transport distances that look at transport problems from a maximumentropy perspective. We smooth the classic optimal transport problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transport solvers. We also show that this regularized distance improves upon classic optimal transport distances on the MNIST classification problem.},
  file = {/Users/alextong/Zotero/storage/HD4DPPJM/Cuturi - Sinkhorn Distances Lightspeed Computation of Opti.pdf},
  language = {en}
}

@article{dormand_family_1980,
  title = {A Family of Embedded {{Runge}}-{{Kutta}} Formulae},
  author = {Dormand, J.R. and Prince, P.J.},
  year = {1980},
  volume = {6},
  pages = {19--26},
  abstract = {A family of embedded Runge-Kutta formulae RK5 (4) are derived. From these are presented formulae which have (a) 'small' principal truncation terms in the fifth order and (b) extended regions of absolute stability.},
  file = {/Users/alextong/Zotero/storage/96HLDK92/Dormand and Prince - 1980 - A family of embedded Runge-Kutta formulae.pdf},
  journal = {Journal of Computational and Applied Mathematics},
  language = {en},
  number = {1}
}

@article{dudley_speed_1969,
  title = {The {{Speed}} of {{Mean Glivenko}}-{{Cantelli Convergence}}},
  author = {Dudley, R. M.},
  year = {1969},
  volume = {40},
  pages = {40--50},
  file = {/Users/alextong/Zotero/storage/H32UBA6A/euclid.aoms.1177697802.pdf},
  journal = {The Annals of Mathematical Statistics},
  number = {1}
}

@article{erhard_scslam-seq_2019,
  title = {{{scSLAM}}-Seq Reveals Core Features of Transcription Dynamics in Single Cells},
  author = {Erhard, Florian and Baptista, Marisa A. P. and Krammer, Tobias and Hennig, Thomas and Lange, Marius and Arampatzi, Panagiota and J{\"u}rges, Christopher S. and Theis, Fabian J. and Saliba, Antoine-Emmanuel and D{\"o}lken, Lars},
  year = {2019},
  volume = {571},
  pages = {419--423},
  file = {/Users/alextong/Zotero/storage/D7EPU2ZY/Erhard et al. - 2019 - scSLAM-seq reveals core features of transcription .pdf},
  journal = {Nature},
  language = {en},
  number = {7765}
}

@inproceedings{grathwohl_ffjord:_2019,
  title = {{{FFJORD}}: {{Free}}-Form {{Continuous Dynamics}} for {{Scalable Reversible Generative Models}}},
  shorttitle = {{{FFJORD}}},
  booktitle = {7th {{International Conference}} on {{Learning Representations}}},
  author = {Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
  year = {2019},
  abstract = {A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.},
  archivePrefix = {arXiv},
  eprint = {1810.01367},
  eprinttype = {arxiv},
  file = {/Users/alextong/Zotero/storage/CEC2RKX5/Grathwohl et al. - 2018 - FFJORD Free-form Continuous Dynamics for Scalable.pdf;/Users/alextong/Zotero/storage/W2CFSLZI/1810.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{haghverdi_diffusion_2016,
  title = {Diffusion Pseudotime Robustly Reconstructs Lineage Branching},
  author = {Haghverdi, Laleh and B{\"u}ttner, Maren and Wolf, F Alexander and Buettner, Florian and Theis, Fabian J},
  year = {2016},
  volume = {13},
  pages = {845--848},
  file = {/Users/alextong/Zotero/storage/I7WJDIZ3/Haghverdi et al. - 2016 - Diffusion pseudotime robustly reconstructs lineage.pdf},
  journal = {Nature Methods},
  language = {en},
  number = {10}
}

@inproceedings{hashimoto_learning_2016,
  title = {Learning {{Population}}-{{Level Diffusions}} with {{Generative Recurrent Networks}}},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Hashimoto, Tatsunori B and Gifford, David K and Jaakkola, Tommi S},
  year = {2016},
  pages = {2417--2426},
  abstract = {We estimate stochastic processes that govern the dynamics of evolving populations such as cell differentiation. The problem is challenging since longitudinal trajectory measurements of individuals in a population are rarely available due to experimental cost and/or privacy. We show that cross-sectional samples from an evolving population suffice for recovery within a class of processes even if samples are available only at a few distinct time points. We provide a stratified analysis of recoverability conditions, and establish that reversibility is sufficient for recoverability. For estimation, we derive a natural loss and regularization, and parameterize the processes as diffusive recurrent neural networks. We demonstrate the approach in the context of uncovering complex cellular dynamics known as the `epigenetic landscape' from existing biological assays.},
  file = {/Users/alextong/Zotero/storage/EUSILJ77/Hashimoto et al. - Learning Population-Level Diffusions with Generati.pdf},
  language = {en}
}

@article{hendriks_nasc-seq_2019,
  title = {{{NASC}}-Seq Monitors {{RNA}} Synthesis in Single Cells},
  author = {Hendriks, Gert-Jan and Jung, Lisa A. and Larsson, Anton J. M. and Lidschreiber, Michael and Andersson Forsman, Oscar and Lidschreiber, Katja and Cramer, Patrick and Sandberg, Rickard},
  year = {2019},
  volume = {10},
  pages = {3138},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-11028-9},
  file = {/Users/alextong/Zotero/storage/FRZKCDIE/Hendriks et al. - 2019 - NASC-seq monitors RNA synthesis in single cells.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@inproceedings{jambulapati_direct_2019,
  title = {A {{Direct}} \$\textbackslash tilde\{\vphantom\}{{O}}\vphantom\{\}(1/\textbackslash epsilon)\$ {{Iteration Parallel Algorithm}} for {{Optimal Transport}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Jambulapati, Arun and Sidford, Aaron and Tian, Kevin},
  year = {2019},
  pages = {11359--11370},
  abstract = {Optimal transportation, or computing the Wasserstein or ``earth mover's'' distance between two n-dimensional distributions, is a fundamental primitive which arises in many learning and statistical settings. We give an algorithm which solves the problem to additive accuracy with O\texttildelow (1/ ) parallel depth and O\texttildelow{} n2/ work. [BJKS18, Qua19] obtained this runtime through reductions to positive linear programming and matrix scaling. However, these reduction-based algorithms use subroutines which may be impractical due to requiring solvers for second-order iterations (matrix scaling) or non-parallelizability (positive LP). Our methods match the previous-best work bounds by [BJKS18, Qua19] while either improving parallelization or removing the need for linear system solves, and improve upon the previous best first-order methods running in time O\texttildelow (min(n2/ 2, n2.5/ )) [DGK18, LHJ19]. We obtain our results by a primal-dual extragradient method, motivated by recent theoretical improvements to maximum flow [She17].},
  file = {/Users/alextong/Zotero/storage/8TUXHAMB/Jambulapati et al. - A Direct O˜(1 ) Iteration Parallel Algorithm for .pdf},
  language = {en}
}

@article{kanton_organoid_2019,
  title = {Organoid Single-Cell Genomic Atlas Uncovers Human-Specific Features of Brain Development},
  author = {Kanton, Sabina and Boyle, Michael James and He, Zhisong and Santel, Malgorzata and Weigert, Anne and {Sanch{\'i}s-Calleja}, F{\'a}tima and Guijarro, Patricia and Sidow, Leila and Fleck, Jonas Simon and Han, Dingding and Qian, Zhengzong and Heide, Michael and Huttner, Wieland B. and Khaitovich, Philipp and P{\"a}{\"a}bo, Svante and Treutlein, Barbara and Camp, J. Gray},
  year = {2019},
  volume = {574},
  pages = {418--422},
  file = {/Users/alextong/Zotero/storage/GRFEPJXS/Kanton et al. - 2019 - Organoid single-cell genomic atlas uncovers human-.pdf},
  journal = {Nature},
  language = {en},
  number = {7778}
}

@article{kantorovich_translocation_1942,
  title = {On the {{Translocation}} of {{Masses}}},
  author = {Kantorovich, L V},
  year = {1942},
  file = {/Users/alextong/Zotero/storage/7R8KI9AA/Kantorovich - On the Translocation of Masses.pdf},
  journal = {Doklady Akademii Nauk},
  language = {en}
}

@article{katayama_chd8_2016,
  title = {{{CHD8}} Haploinsufficiency Results in Autistic-like Phenotypes in Mice},
  author = {Katayama, Yuta and Nishiyama, Masaaki and Shoji, Hirotaka and Ohkawa, Yasuyuki and Kawamura, Atsuki and Sato, Tetsuya and Suyama, Mikita and Takumi, Toru and Miyakawa, Tsuyoshi and Nakayama, Keiichi I.},
  year = {2016},
  volume = {537},
  pages = {675--679},
  file = {/Users/alextong/Zotero/storage/VYMACGMF/Katayama et al. - 2016 - CHD8 haploinsufficiency results in autistic-like p.pdf},
  journal = {Nature},
  language = {en},
  number = {7622}
}

@inproceedings{kingma_adam:_2015,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  booktitle = {3rd {{International Conference}} on {{Learning Representations}}},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2015},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archivePrefix = {arXiv},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  file = {/Users/alextong/Zotero/storage/KEBG4Q9F/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf;/Users/alextong/Zotero/storage/CEG7LAGP/1412.html},
  keywords = {Computer Science - Machine Learning}
}

@inproceedings{kingma_improving_2016,
  title = {Improving {{Variational Inference}} with {{Inverse Autoregressive Flow}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Kingma, Diederik P. and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  year = {2016},
  abstract = {The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.},
  archivePrefix = {arXiv},
  eprint = {1606.04934},
  eprinttype = {arxiv},
  file = {/Users/alextong/Zotero/storage/HW7WBDMR/Kingma et al. - 2016 - Improving Variational Inference with Inverse Autor.pdf;/Users/alextong/Zotero/storage/63MTQJTE/1606.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{kolouri_generalized_2019,
  title = {Generalized {{Sliced Wasserstein Distances}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Kolouri, Soheil and Nadjahi, Kimia and Simsekli, Umut and Badeau, Roland and Rohde, Gustavo},
  year = {2019},
  pages = {261--272},
  abstract = {The Wasserstein distance and its variations, e.g., the sliced-Wasserstein (SW) distance, have recently drawn attention from the machine learning community. The SW distance, specifically, was shown to have similar properties to the Wasserstein distance, while being much simpler to compute, and is therefore used in various applications including generative modeling and general supervised/unsupervised learning. In this paper, we first clarify the mathematical connection between the SW distance and the Radon transform. We then utilize the generalized Radon transform to define a new family of distances for probability measures, which we call generalized sliced-Wasserstein (GSW) distances. We further show that, similar to the SW distance, the GSW distance can be extended to a maximum GSW (max-GSW) distance. We then provide the conditions under which GSW and max-GSW distances are indeed proper metrics. Finally, we compare the numerical performance of the proposed distances on the generative modeling task of SW flows and report favorable results.},
  file = {/Users/alextong/Zotero/storage/N6IHDBHA/Kolouri et al. - Generalized Sliced Wasserstein Distances.pdf},
  language = {en}
}

@article{la_manno_rna_2018,
  title = {{{RNA}} Velocity of Single Cells},
  author = {La Manno, Gioele and Soldatov, Ruslan and Zeisel, Amit and Braun, Emelie and Hochgerner, Hannah and Petukhov, Viktor and Lidschreiber, Katja and Kastriti, Maria E. and L{\"o}nnerberg, Peter and Furlan, Alessandro and Fan, Jean and Borm, Lars E. and Liu, Zehua and {van Bruggen}, David and Guo, Jimin and He, Xiaoling and Barker, Roger and Sundstr{\"o}m, Erik and {Castelo-Branco}, Gon{\c c}alo and Cramer, Patrick and Adameyko, Igor and Linnarsson, Sten and Kharchenko, Peter V.},
  year = {2018},
  volume = {560},
  pages = {494--498},
  file = {/Users/alextong/Zotero/storage/8WLJPSLP/La Manno et al. - 2018 - RNA velocity of single cells.pdf},
  journal = {Nature},
  language = {en},
  number = {7719}
}

@article{lederer_emergence_2020,
  title = {The Emergence and Promise of Single-Cell Temporal-Omics Approaches},
  author = {Lederer, Alex R and La Manno, Gioele},
  year = {2020},
  volume = {63},
  pages = {70--78},
  file = {/Users/alextong/Zotero/storage/ITSPI9VR/Lederer and La Manno - 2020 - The emergence and promise of single-cell temporal-.pdf},
  journal = {Current Opinion in Biotechnology},
  language = {en}
}

@article{li_scalable_2020,
  title = {Scalable {{Gradients}} for {{Stochastic Differential Equations}}},
  author = {Li, Xuechen and Wong, Ting-Kam Leonard and Chen, Ricky T. Q. and Duvenaud, David},
  year = {2020},
  abstract = {The adjoint sensitivity method scalably computes gradients of solutions to ordinary differential equations. We generalize this method to stochastic differential equations, allowing time-efficient and constant-memory computation of gradients with high-order adaptive solvers. Specifically, we derive a stochastic differential equation whose solution is the gradient, a memory-efficient algorithm for caching noise, and conditions under which numerical solutions converge. In addition, we combine our method with gradient-based stochastic variational inference for latent stochastic differential equations. We use our method to fit stochastic dynamics defined by neural networks, achieving competitive performance on a 50-dimensional motion capture dataset.},
  archivePrefix = {arXiv},
  eprint = {2001.01328},
  eprinttype = {arxiv},
  file = {/Users/alextong/Zotero/storage/ASPNP6ZA/Li et al. - 2020 - Scalable Gradients for Stochastic Differential Equ.pdf},
  journal = {Artificial Intelligence and Statistics},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  language = {en}
}

@article{liero_optimal_2018,
  title = {Optimal {{Entropy}}-{{Transport}} Problems and a New {{Hellinger}}\textendash{{Kantorovich}} Distance between Positive Measures},
  author = {Liero, Matthias and Mielke, Alexander and Savar{\'e}, Giuseppe},
  year = {2018},
  volume = {211},
  pages = {969--1117},
  file = {/Users/alextong/Zotero/storage/6JH97MSR/Liero et al. - 2018 - Optimal Entropy-Transport problems and a new Helli.pdf},
  journal = {Inventiones mathematicae},
  language = {en},
  number = {3}
}

@inproceedings{lindenbaum_geometry_2018,
  title = {Geometry {{Based Data Generation}}},
  booktitle = {Advances in {{Neural Infromation Processing Systems}} 31},
  author = {Lindenbaum, Ofir and Stanley, Jay and Wolf, Guy and Krishnaswamy, Smita},
  year = {2018},
  pages = {1400--1411},
  abstract = {We propose a new type of generative model for high-dimensional data that learns a manifold geometry of the data, rather than density, and can generate points evenly along this manifold. This is in contrast to existing generative models that represent data density, and are strongly affected by noise and other artifacts of data collection. We demonstrate how this approach corrects sampling biases and artifacts, thus improves several downstream data analysis tasks, such as clustering and classification. Finally, we demonstrate that this approach is especially useful in biology where, despite the advent of single-cell technologies, rare subpopulations and gene-interaction relationships are affected by biased sampling. We show that SUGAR can generate hypothetical populations, and it is able to reveal intrinsic patterns and mutual-information relationships between genes on a single-cell RNA sequencing dataset of hematopoiesis.},
  file = {/Users/alextong/Zotero/storage/GET4UFX8/Lindenbaum et al. - Geometry Based Data Generation.pdf;/Users/alextong/Zotero/storage/PVITILWT/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
  language = {en}
}

@article{macosko_highly_2015,
  title = {Highly {{Parallel Genome}}-Wide {{Expression Profiling}} of {{Individual Cells Using Nanoliter Droplets}}},
  author = {Macosko, Evan Z. and Basu, Anindita and Satija, Rahul and Nemesh, James and Shekhar, Karthik and Goldman, Melissa and Tirosh, Itay and Bialas, Allison R. and Kamitaki, Nolan and Martersteck, Emily M. and Trombetta, John J. and Weitz, David A. and Sanes, Joshua R. and Shalek, Alex K. and Regev, Aviv and McCarroll, Steven A.},
  year = {2015},
  volume = {161},
  pages = {1202--1214},
  abstract = {Cells, the basic units of biological structure and function, vary broadly in type and state. Singlecell genomics can characterize cell identity and function, but limitations of ease and scale have prevented its broad application. Here we describe Drop-seq, a strategy for quickly profiling thousands of individual cells by separating them into nanoliter-sized aqueous droplets, associating a different barcode with each cell's RNAs, and sequencing them all together. Drop-seq analyzes mRNA transcripts from thousands of individual cells simultaneously while remembering transcripts' cell of origin. We analyzed transcriptomes from 44,808 mouse retinal cells and identified 39 transcriptionally distinct cell populations, creating a molecular atlas of gene expression for known retinal cell classes and novel candidate cell subtypes. Drop-seq will accelerate biological discovery by enabling routine transcriptional profiling at singlecell resolution.},
  file = {/Users/alextong/Zotero/storage/DIYCT8M5/Macosko et al. - 2015 - Highly Parallel Genome-wide Expression Profiling o.pdf},
  journal = {Cell},
  language = {en},
  number = {5}
}

@article{monge_memoire_1781,
  title = {M\'emoire Sur La Th\'eorie Des D\'eblais et Des Remblais},
  author = {Monge, Gaspard},
  year = {1781},
  journal = {Histoire de l'Acad\'emie Royale des Science}
}

@article{moon_visualizing_2019,
  title = {Visualizing Structure and Transitions in High-Dimensional Biological Data},
  author = {Moon, Kevin R. and {van Dijk}, David and Wang, Zheng and Gigante, Scott and Burkhardt, Daniel B. and Chen, William S. and Yim, Kristina and van den Elzen, Antonia and Hirn, Matthew J. and Coifman, Ronald R. and Ivanova, Natalia B. and Wolf, Guy and Krishnaswamy, Smita},
  year = {2019},
  volume = {37},
  pages = {1482--1492},
  file = {/Users/alextong/Zotero/storage/H6A4S39T/Moon et al. - 2019 - Visualizing structure and transitions in high-dime.pdf},
  journal = {Nature Biotechnology},
  language = {en},
  number = {12}
}

@inproceedings{muzellec_subspace_2019,
  title = {Subspace {{Detours}}: {{Building Transport Plans}} That Are {{Optimal}} on {{Subspace Projections}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Muzellec, Boris and Cuturi, Marco},
  year = {2019},
  pages = {6917--6928},
  abstract = {Computing optimal transport (OT) between measures in high dimensions is doomed by the curse of dimensionality. A popular approach to avoid this curse is to project input measures on lower-dimensional subspaces (1D lines in the case of sliced Wasserstein distances), solve the OT problem between these reduced measures, and settle for the Wasserstein distance between these reductions, rather than that between the original measures. This approach is however difficult to extend to the case in which one wants to compute an OT map (a Monge map) between the original measures. Since computations are carried out on lower-dimensional projections, classical map estimation techniques can only produce maps operating in these reduced dimensions. We propose in this work two methods to extrapolate, from an transport map that is optimal on a subspace, one that is nearly optimal in the entire space. We prove that the best optimal transport plan that takes such ``subspace detours'' is a generalization of the Knothe-Rosenblatt transport. We show that these plans can be explicitly formulated when comparing Gaussian measures (between which the Wasserstein distance is commonly referred to as the Bures or Fr\'echet distance). We provide an algorithm to select optimal subspaces given pairs of Gaussian measures, and study scenarios in which that mediating subspace can be selected using prior information. We consider applications to semantic mediation between elliptic word embeddings and domain adaptation with Gaussian mixture models.},
  file = {/Users/alextong/Zotero/storage/3RESRLXM/Muzellec and Cuturi - Subspace Detours Building Transport Plans that ar.pdf},
  language = {en}
}

@article{oeppen_broken_2002,
  title = {Broken {{Limits}} to {{Life Expectancy}}},
  author = {Oeppen, Jim and Vaupel, James W.},
  year = {2002},
  volume = {296},
  pages = {1029--1031},
  file = {/Users/alextong/Zotero/storage/XYBV338R/1029.full.pdf},
  journal = {Science},
  number = {5570}
}

@article{papadakis_optimal_2014,
  title = {Optimal {{Transport}} with {{Proximal Splitting}}},
  author = {Papadakis, Nicolas and Peyr{\'e}, Gabriel and Oudet, Edouard},
  year = {2014},
  volume = {7},
  pages = {212--238},
  abstract = {This article reviews the use of first order convex optimization schemes to solve the discretized dynamic optimal transport problem, initially proposed by Benamou and Brenier. We develop a staggered grid discretization that is well adapted to the computation of the L2 optimal transport geodesic between distributions defined on a uniform spatial grid. We show how proximal splitting schemes can be used to solve the resulting large scale convex optimization problem. A specific instantiation of this method on a centered grid corresponds to the initial algorithm developed by Benamou and Brenier. We also show how more general cost functions can be taken into account and how to extend the method to perform optimal transport on a Riemannian manifold.},
  archivePrefix = {arXiv},
  eprint = {1304.5784},
  eprinttype = {arxiv},
  file = {/Users/alextong/Zotero/storage/CX9B9SPF/Papadakis et al. - 2014 - Optimal Transport with Proximal Splitting.pdf},
  journal = {SIAM Journal on Imaging Sciences},
  keywords = {Mathematics - Numerical Analysis},
  language = {en},
  number = {1}
}

@inproceedings{papamakarios_masked_2017,
  title = {Masked {{Autoregressive Flow}} for {{Density Estimation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
  year = {2017},
  pages = {2338--2347},
  abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
  archivePrefix = {arXiv},
  eprint = {1705.07057},
  eprinttype = {arxiv},
  file = {/Users/alextong/Zotero/storage/CE9SCTPK/Papamakarios et al. - 2017 - Masked Autoregressive Flow for Density Estimation.pdf;/Users/alextong/Zotero/storage/GKQF8QWU/1705.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{paszke_pytorch_2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High}}-{{Performance Deep Learning Library}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  pages = {8026--8037},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.},
  file = {/Users/alextong/Zotero/storage/44L56GCN/Paszke et al. - PyTorch An Imperative Style, High-Performance Dee.pdf},
  language = {en}
}

@book{peyre_computational_2019,
  title = {Computational {{Optimal Transport}}},
  author = {Peyr{\'e}, Gabriel and Cuturi, Marco},
  year = {2019},
  publisher = {{arXiv:1803.00567}},
  abstract = {Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
  archivePrefix = {arXiv},
  eprint = {1803.00567},
  eprinttype = {arxiv},
  file = {/Users/alextong/Zotero/storage/U4XUP9WA/Peyré and Cuturi - 2019 - Computational Optimal Transport.pdf},
  keywords = {Statistics - Machine Learning},
  language = {en}
}

@article{peyre_entropic_2015,
  title = {Entropic {{Wasserstein Gradient Flows}}},
  author = {Peyr{\'e}, Gabriel},
  year = {2015},
  volume = {8},
  pages = {2323--2351},
  abstract = {This article details a novel numerical scheme to approximate gradient flows for optimal transport (i.e. Wasserstein) metrics. These flows have proved useful to tackle theoretically and numerically non-linear diffusion equations that model for instance porous media or crowd evolutions. These gradient flows define a suitable notion of weak solutions for these evolutions and they can be approximated in a stable way using discrete flows. These discrete flows are implicit Euler time stepping according to the Wasserstein metric. A bottleneck of these approaches is the high computational load induced by the resolution of each step. Indeed, this corresponds to the resolution of a convex optimization problem involving a Wasserstein distance to the previous iterate. Following several recent works on the approximation of Wasserstein distances, we consider a discrete flow induced by an entropic regularization of the transportation coupling. This entropic regularization allows one to trade the initial Wasserstein fidelity term for a Kulback-Leibler divergence, which is easier to deal with numerically. We show how KL proximal schemes, and in particular Dykstra's algorithm, can be used to compute each step of the regularized flow. The resulting algorithm is both fast, parallelizable and versatile, because it only requires multiplications by a Gibbs kernel. On Euclidean domains discretized on an uniform grid, this corresponds to a linear filtering (for instance a Gaussian filtering when \$c\$ is the squared Euclidean distance) which can be computed in nearly linear time. On more general domains, such as (possibly non-convex) shapes or on manifolds discretized by a triangular mesh, following a recently proposed numerical scheme for optimal transport, this Gibbs kernel multiplication is approximated by a short-time heat diffusion.},
  archivePrefix = {arXiv},
  eprint = {1502.06216},
  eprinttype = {arxiv},
  file = {/Users/alextong/Zotero/storage/DDLS47D5/Peyré - 2015 - Entropic Wasserstein Gradient Flows.pdf;/Users/alextong/Zotero/storage/F9TV6QT7/1502.html},
  journal = {SIAM Journal on Imaging Sciences},
  keywords = {Mathematics - Optimization and Control},
  number = {4}
}

@inproceedings{rezende_variational_2015,
  title = {Variational {{Inference}} with {{Normalizing Flows}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  year = {2015},
  volume = {37},
  pages = {1530--1538},
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  archivePrefix = {arXiv},
  eprint = {1505.05770},
  eprinttype = {arxiv},
  file = {/Users/alextong/Zotero/storage/3ZBGTU9P/Rezende and Mohamed - 2015 - Variational Inference with Normalizing Flows.pdf;/Users/alextong/Zotero/storage/FFBP2QWC/1505.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology}
}

@inproceedings{rifai_contractive_2011-1,
  title = {Contractive {{Auto}}-{{Encoders}}: {{Explicit Invariance During Feature Extraction}}},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Machine Learning}}},
  author = {Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot, Xavier and Bengio, Yoshua},
  year = {2011},
  pages = {833--840},
  abstract = {We present in this paper a novel approach for training deterministic auto-encoders. We show that by adding a well chosen penalty term to the classical reconstruction cost function, we can achieve results that equal or surpass those attained by other regularized autoencoders as well as denoising auto-encoders on a range of datasets. This penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. We show that this penalty term results in a localized space contraction which in turn yields robust features on the activation layer. Furthermore, we show how this penalty term is related to both regularized auto-encoders and denoising auto-encoders and how it can be seen as a link between deterministic and non-deterministic auto-encoders. We find empirically that this penalty helps to carve a representation that better captures the local directions of variation dictated by the data, corresponding to a lower-dimensional non-linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold. Finally, we show that by using the learned features to initialize a MLP, we achieve state of the art classification error on a range of datasets, surpassing other methods of pretraining.},
  file = {/Users/alextong/Zotero/storage/EIKRRD7A/Rifai et al. - Contractive Auto-Encoders Explicit Invariance Dur.pdf},
  language = {en}
}

@article{saelens_comparison_2019,
  title = {A Comparison of Single-Cell Trajectory Inference Methods},
  author = {Saelens, Wouter and Cannoodt, Robrecht and Todorov, Helena and Saeys, Yvan},
  year = {2019},
  volume = {37},
  pages = {547--554},
  file = {/Users/alextong/Zotero/storage/4TP452MX/Saelens et al. - 2019 - A comparison of single-cell trajectory inference m.pdf},
  journal = {Nature Biotechnology},
  language = {en},
  number = {5}
}

@article{schiebinger_optimal-transport_2019,
  title = {Optimal-{{Transport Analysis}} of {{Single}}-{{Cell Gene Expression Identifies Developmental Trajectories}} in {{Reprogramming}}},
  author = {Schiebinger, Geoffrey and Shu, Jian and Tabaka, Marcin and Cleary, Brian and Subramanian, Vidya and Solomon, Aryeh and Gould, Joshua and Liu, Siyan and Lin, Stacie and Berube, Peter and Lee, Lia and Chen, Jenny and Brumbaugh, Justin and Rigollet, Philippe and Hochedlinger, Konrad and Jaenisch, Rudolf and Regev, Aviv and Lander, Eric S.},
  year = {2019},
  volume = {176},
  pages = {928-943.e22},
  file = {/Users/alextong/Zotero/storage/98LH4MJI/Schiebinger et al. - 2019 - Optimal-Transport Analysis of Single-Cell Gene Exp.pdf},
  journal = {Cell},
  language = {en},
  number = {4}
}

@misc{sinkhorn_relationship_1964,
  title = {A Relationship between Arbitrary Positive Matrices and Doubly Stochastic Matrices},
  author = {Sinkhorn, Richard},
  year = {1964},
  file = {/Users/alextong/Zotero/storage/MXVQBY74/euclid.aoms.1177703591.pdf}
}

@article{trapnell_dynamics_2014,
  title = {The Dynamics and Regulators of Cell Fate Decisions Are Revealed by Pseudotemporal Ordering of Single Cells},
  author = {Trapnell, Cole and Cacchiarelli, Davide and Grimsby, Jonna and Pokharel, Prapti and Li, Shuqiang and Morse, Michael and Lennon, Niall J and Livak, Kenneth J and Mikkelsen, Tarjei S and Rinn, John L},
  year = {2014},
  volume = {32},
  pages = {381--386},
  file = {/Users/alextong/Zotero/storage/7TVSW4R9/Trapnell et al. - 2014 - The dynamics and regulators of cell fate decisions.pdf},
  journal = {Nature Biotechnology},
  language = {en},
  number = {4}
}

@book{villani_optimal_2008,
  title = {Optimal Transport, Old and New},
  author = {Villani, Cedric},
  year = {2008},
  month = jun,
  publisher = {{Springer}},
  file = {/Users/alextong/Zotero/storage/6C8Q9E85/preprint-1.pdf},
  isbn = {978-3-540-71050-9}
}

@article{vincent_stacked_2010,
  title = {Stacked {{Denoising Autoencoders}}: {{Learning Useful Representations}} in a {{Deep Network}} with a {{Local Denoising Criterion}}},
  author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  year = {2010},
  pages = {3371--3408},
  abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
  file = {/Users/alextong/Zotero/storage/KSXVY4UU/Vincent et al. - Stacked Denoising Autoencoders Learning Useful Re.pdf},
  journal = {Journal of Machine Learning Research},
  language = {en}
}

@article{waddington_epigenotype_1942,
  title = {The Epigenotype},
  author = {Waddington, C. H.},
  year = {1942},
  volume = {1},
  pages = {18--20},
  journal = {Endeavour}
}

@article{weed_sharp_2019,
  title = {Sharp Asymptotic and Finite-Sample Rates of Convergence of Empirical Measures in {{Wasserstein}} Distance},
  author = {Weed, Jonathan and Bach, Francis},
  year = {2019},
  volume = {25},
  pages = {2620--2648},
  file = {/Users/alextong/Zotero/storage/FFRRG396/Weed and Bach - 2019 - Sharp asymptotic and finite-sample rates of conver.pdf},
  journal = {Bernoulli},
  language = {en},
  number = {4A}
}

@article{weinreb_fundamental_2018,
  title = {Fundamental Limits on Dynamic Inference from Single-Cell Snapshots},
  author = {Weinreb, Caleb and Wolock, Samuel and Tusi, Betsabeh K. and Socolovsky, Merav and Klein, Allon M.},
  year = {2018},
  volume = {115},
  pages = {E2467-E2476},
  abstract = {Single-cell expression profiling reveals the molecular states of individual cells with unprecedented detail. Because these methods destroy cells in the process of analysis, they cannot measure how gene expression changes over time. However, some information on dynamics is present in the data: the continuum of molecular states in the population can reflect the trajectory of a typical cell. Many methods for extracting single-cell dynamics from population data have been proposed. However, all such attempts face a common limitation: for any measured distribution of cell states, there are multiple dynamics that could give rise to it, and by extension, multiple possibilities for underlying mechanisms of gene regulation. Here, we describe the aspects of gene expression dynamics that cannot be inferred from a static snapshot alone and identify assumptions necessary to constrain a unique solution for cell dynamics from static snapshots. We translate these constraints into a practical algorithmic approach, population balance analysis (PBA), which makes use of a method from spectral graph theory to solve a class of high-dimensional differential equations. We use simulations to show the strengths and limitations of PBA, and then apply it to single-cell profiles of hematopoietic progenitor cells (HPCs). Cell state predictions from this analysis agree with HPC fate assays reported in several papers over the past two decades. By highlighting the fundamental limits on dynamic inference faced by any method, our framework provides a rigorous basis for dynamic interpretation of a gene expression continuum and clarifies best experimental designs for trajectory reconstruction from static snapshot measurements.},
  file = {/Users/alextong/Zotero/storage/L6HG3MPR/Weinreb et al. - 2018 - Fundamental limits on dynamic inference from singl.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {10}
}

@inproceedings{yang_scalable_2019,
  title = {Scalable {{Unbalanced Optimal Transport Using Generative Adversarial Networks}}},
  booktitle = {7th {{International Conference}} on {{Learning Representations}}},
  author = {Yang, Karren D and Uhler, Caroline},
  year = {2019},
  pages = {20},
  abstract = {Generative adversarial networks (GANs) are an expressive class of neural generative models with tremendous success in modeling high-dimensional continuous measures. In this paper, we present a scalable method for unbalanced optimal transport (OT) based on the generativeadversarial framework. We formulate unbalanced OT as a problem of simultaneously learning a transport map and a scaling factor that push a source measure to a target measure in a cost-optimal manner. We provide theoretical justification for this formulation, showing that it is closely related to an existing static formulation by Liero et al. (2018). We then propose an algorithm for solving this problem based on stochastic alternating gradient updates, similar in practice to GANs, and perform numerical experiments demonstrating how this methodology can be applied to population modeling.},
  file = {/Users/alextong/Zotero/storage/K6AVYFFH/Yang and Uhler - 2019 - SCALABLE UNBALANCED OPTIMAL TRANSPORT USING GENERA.pdf},
  language = {en}
}


