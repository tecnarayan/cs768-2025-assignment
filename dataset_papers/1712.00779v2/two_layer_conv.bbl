\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2017)Agarwal, {Allen-Zhu}, Bullins, Hazan, and
  Ma]{agarwal2016finding}
Agarwal, Naman, {Allen-Zhu}, Zeyuan, Bullins, Brian, Hazan, Elad, and Ma,
  Tengyu.
\newblock {Finding Approximate Local Minima Faster Than Gradient Descent}.
\newblock In \emph{STOC}, 2017.
\newblock Full version available at \url{http://arxiv.org/abs/1611.01146}.

\bibitem[Bhojanapalli et~al.(2016)Bhojanapalli, Neyshabur, and
  Srebro]{bhojanapalli2016global}
Bhojanapalli, Srinadh, Neyshabur, Behnam, and Srebro, Nati.
\newblock Global optimality of local search for low rank matrix recovery.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3873--3881, 2016.

\bibitem[Blum \& Rivest(1989)Blum and Rivest]{blum1989training}
Blum, Avrim and Rivest, Ronald~L.
\newblock Training a 3-node neural network is {NP}-complete.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  494--501, 1989.

\bibitem[Brutzkus \& Globerson(2017)Brutzkus and
  Globerson]{brutzkus2017globally}
Brutzkus, Alon and Globerson, Amir.
\newblock Globally optimal gradient descent for a {C}onvnet with {G}aussian
  inputs.
\newblock \emph{arXiv preprint arXiv:1702.07966}, 2017.

\bibitem[Carmon et~al.(2016)Carmon, Duchi, Hinder, and
  Sidford]{carmon2016accelerated}
Carmon, Yair, Duchi, John~C, Hinder, Oliver, and Sidford, Aaron.
\newblock Accelerated methods for non-convex optimization.
\newblock \emph{arXiv preprint arXiv:1611.00756}, 2016.

\bibitem[Cho \& Saul(2009)Cho and Saul]{cho2009kernel}
Cho, Youngmin and Saul, Lawrence~K.
\newblock Kernel methods for deep learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  342--350, 2009.

\bibitem[Choromanska et~al.(2015)Choromanska, Henaff, Mathieu, Arous, and
  LeCun]{choromanska2015loss}
Choromanska, Anna, Henaff, Mikael, Mathieu, Michael, Arous, G{\'e}rard~Ben, and
  LeCun, Yann.
\newblock The loss surfaces of multilayer networks.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  192--204,
  2015.

\bibitem[Dauphin et~al.(2016)Dauphin, Fan, Auli, and
  Grangier]{dauphin2016language}
Dauphin, Yann~N, Fan, Angela, Auli, Michael, and Grangier, David.
\newblock Language modeling with gated convolutional networks.
\newblock \emph{arXiv preprint arXiv:1612.08083}, 2016.

\bibitem[Du et~al.(2017{\natexlab{a}})Du, Jin, Lee, Jordan, Poczos, and
  Singh]{du2017gradient}
Du, Simon~S, Jin, Chi, Lee, Jason~D, Jordan, Michael~I, Poczos, Barnabas, and
  Singh, Aarti.
\newblock Gradient descent can take exponential time to escape saddle points.
\newblock \emph{arXiv preprint arXiv:1705.10412}, 2017{\natexlab{a}}.

\bibitem[Du et~al.(2017{\natexlab{b}})Du, Lee, and Tian]{du2017convolutional}
Du, Simon~S, Lee, Jason~D, and Tian, Yuandong.
\newblock When is a convolutional filter easy to learn?
\newblock \emph{arXiv preprint arXiv:1709.06129}, 2017{\natexlab{b}}.

\bibitem[Feizi et~al.(2017)Feizi, Javadi, Zhang, and Tse]{feizi2017porcupine}
Feizi, Soheil, Javadi, Hamid, Zhang, Jesse, and Tse, David.
\newblock Porcupine neural networks:(almost) all local optima are global.
\newblock \emph{arXiv preprint arXiv:1710.02196}, 2017.

\bibitem[Freeman \& Bruna(2016)Freeman and Bruna]{freeman2016topology}
Freeman, C~Daniel and Bruna, Joan.
\newblock Topology and geometry of half-rectified network optimization.
\newblock \emph{arXiv preprint arXiv:1611.01540}, 2016.

\bibitem[Ge et~al.(2015)Ge, Huang, Jin, and Yuan]{ge2015escaping}
Ge, Rong, Huang, Furong, Jin, Chi, and Yuan, Yang.
\newblock Escaping from saddle points $-$ online stochastic gradient for tensor
  decomposition.
\newblock In \emph{Proceedings of The 28th Conference on Learning Theory}, pp.\
   797--842, 2015.

\bibitem[Ge et~al.(2016)Ge, Lee, and Ma]{ge2016matrix}
Ge, Rong, Lee, Jason~D, and Ma, Tengyu.
\newblock Matrix completion has no spurious local minimum.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2973--2981, 2016.

\bibitem[Ge et~al.(2017{\natexlab{a}})Ge, Jin, and Zheng]{ge2017no}
Ge, Rong, Jin, Chi, and Zheng, Yi.
\newblock No spurious local minima in nonconvex low rank problems: A unified
  geometric analysis.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, pp.\  1233--1242, 2017{\natexlab{a}}.

\bibitem[Ge et~al.(2017{\natexlab{b}})Ge, Lee, and Ma]{ge2017learning}
Ge, Rong, Lee, Jason~D, and Ma, Tengyu.
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock \emph{arXiv preprint arXiv:1711.00501}, 2017{\natexlab{b}}.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Glorot, Xavier and Bengio, Yoshua.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, pp.\  249--256, 2010.

\bibitem[Goel \& Klivans(2017{\natexlab{a}})Goel and
  Klivans]{goel2017eigenvalue}
Goel, Surbhi and Klivans, Adam.
\newblock Eigenvalue decay implies polynomial-time learnability for neural
  networks.
\newblock \emph{arXiv preprint arXiv:1708.03708}, 2017{\natexlab{a}}.

\bibitem[Goel \& Klivans(2017{\natexlab{b}})Goel and Klivans]{goel2017learning}
Goel, Surbhi and Klivans, Adam.
\newblock Learning depth-three neural networks in polynomial time.
\newblock \emph{arXiv preprint arXiv:1709.06010}, 2017{\natexlab{b}}.

\bibitem[Goel et~al.(2016)Goel, Kanade, Klivans, and Thaler]{goel2016reliably}
Goel, Surbhi, Kanade, Varun, Klivans, Adam, and Thaler, Justin.
\newblock Reliably learning the {R}e{LU} in polynomial time.
\newblock \emph{arXiv preprint arXiv:1611.10258}, 2016.

\bibitem[Haeffele \& Vidal(2015)Haeffele and Vidal]{haeffele2015global}
Haeffele, Benjamin~D and Vidal, Ren{\'e}.
\newblock Global optimality in tensor factorization, deep learning, and beyond.
\newblock \emph{arXiv preprint arXiv:1506.07540}, 2015.

\bibitem[Hardt \& Ma(2016)Hardt and Ma]{hardt2016identity}
Hardt, Moritz and Ma, Tengyu.
\newblock Identity matters in deep learning.
\newblock \emph{arXiv preprint arXiv:1611.04231}, 2016.

\bibitem[Hardt \& Price(2014)Hardt and Price]{hardt2014noisy}
Hardt, Moritz and Price, Eric.
\newblock The noisy power method: A meta algorithm with applications.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2861--2869, 2014.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  1026--1034, 2015.

\bibitem[Janzamin et~al.(2015)Janzamin, Sedghi, and
  Anandkumar]{janzamin2015beating}
Janzamin, Majid, Sedghi, Hanie, and Anandkumar, Anima.
\newblock Beating the perils of non-convexity: Guaranteed training of neural
  networks using tensor methods.
\newblock \emph{arXiv preprint arXiv:1506.08473}, 2015.

\bibitem[Kawaguchi(2016)]{kawaguchi2016deep}
Kawaguchi, Kenji.
\newblock Deep learning without poor local minima.
\newblock In \emph{Advances In Neural Information Processing Systems}, pp.\
  586--594, 2016.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1097--1105, 2012.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Orr, and
  M{\"u}ller]{lecun1998efficient}
LeCun, Yann, Bottou, L{\'e}on, Orr, Genevieve~B, and M{\"u}ller, Klaus-Robert.
\newblock Efficient backprop.
\newblock In \emph{Neural networks: Tricks of the trade}, pp.\  9--50.
  Springer, 1998.

\bibitem[Lee et~al.(2016)Lee, Simchowitz, Jordan, and Recht]{lee2016gradient}
Lee, Jason~D, Simchowitz, Max, Jordan, Michael~I, and Recht, Benjamin.
\newblock Gradient descent only converges to minimizers.
\newblock In \emph{Conference on Learning Theory}, pp.\  1246--1257, 2016.

\bibitem[Li et~al.(2016)Li, Wang, Lu, Arora, Haupt, Liu, and
  Zhao]{li2016symmetry}
Li, Xingguo, Wang, Zhaoran, Lu, Junwei, Arora, Raman, Haupt, Jarvis, Liu, Han,
  and Zhao, Tuo.
\newblock Symmetry, saddle points, and global geometry of nonconvex matrix
  factorization.
\newblock \emph{arXiv preprint arXiv:1612.09296}, 2016.

\bibitem[Li \& Yuan(2017)Li and Yuan]{li2017convergence}
Li, Yuanzhi and Yuan, Yang.
\newblock Convergence analysis of two-layer neural networks with {ReLU}
  activation.
\newblock \emph{arXiv preprint arXiv:1705.09886}, 2017.

\bibitem[Livni et~al.(2014)Livni, Shalev-Shwartz, and
  Shamir]{livni2014computational}
Livni, Roi, Shalev-Shwartz, Shai, and Shamir, Ohad.
\newblock On the computational efficiency of training neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  855--863, 2014.

\bibitem[Mei et~al.(2016)Mei, Bai, and Montanari]{mei2016landscape}
Mei, Song, Bai, Yu, and Montanari, Andrea.
\newblock The landscape of empirical risk for non-convex losses.
\newblock \emph{arXiv preprint arXiv:1607.06534}, 2016.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Salakhutdinov, and
  Srebro]{neyshabur2015path}
Neyshabur, Behnam, Salakhutdinov, Ruslan~R, and Srebro, Nati.
\newblock Path-{SGD}: Path-normalized optimization in deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2422--2430, 2015.

\bibitem[Nguyen \& Hein(2017{\natexlab{a}})Nguyen and Hein]{nguyen2017loss}
Nguyen, Quynh and Hein, Matthias.
\newblock The loss surface of deep and wide neural networks.
\newblock \emph{arXiv preprint arXiv:1704.08045}, 2017{\natexlab{a}}.

\bibitem[Nguyen \& Hein(2017{\natexlab{b}})Nguyen and Hein]{nguyen2017loss2}
Nguyen, Quynh and Hein, Matthias.
\newblock The loss surface and expressivity of deep convolutional neural
  networks.
\newblock \emph{arXiv preprint arXiv:1710.10928}, 2017{\natexlab{b}}.

\bibitem[Panigrahy et~al.(2018)Panigrahy, Rahimi, Sachdeva, and
  Zhang]{panigrahy2018convergence}
Panigrahy, Rina, Rahimi, Ali, Sachdeva, Sushant, and Zhang, Qiuyi.
\newblock Convergence results for neural networks via electrodynamics.
\newblock In \emph{LIPIcs-Leibniz International Proceedings in Informatics},
  volume~94. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2018.

\bibitem[Park et~al.(2017)Park, Kyrillidis, Carmanis, and
  Sanghavi]{park2017non}
Park, Dohyung, Kyrillidis, Anastasios, Carmanis, Constantine, and Sanghavi,
  Sujay.
\newblock Non-square matrix sensing without spurious local minima via the
  {B}urer-{M}onteiro approach.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  65--74, 2017.

\bibitem[Safran \& Shamir(2016)Safran and Shamir]{safran2016quality}
Safran, Itay and Shamir, Ohad.
\newblock On the quality of the initial basin in overspecified neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  774--782, 2016.

\bibitem[Safran \& Shamir(2017)Safran and Shamir]{safran2017spurious}
Safran, Itay and Shamir, Ohad.
\newblock Spurious local minima are common in two-layer relu neural networks.
\newblock \emph{arXiv preprint arXiv:1712.08968}, 2017.

\bibitem[Salimans \& Kingma(2016)Salimans and Kingma]{salimans2016weight}
Salimans, Tim and Kingma, Diederik~P.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  901--909, 2016.

\bibitem[Sedghi \& Anandkumar(2014)Sedghi and Anandkumar]{sedghi2014provable}
Sedghi, Hanie and Anandkumar, Anima.
\newblock Provable methods for training neural networks with sparse
  connectivity.
\newblock \emph{arXiv preprint arXiv:1412.2693}, 2014.

\bibitem[Shalev-Shwartz et~al.(2017{\natexlab{a}})Shalev-Shwartz, Shamir, and
  Shammah]{shalev2017failures}
Shalev-Shwartz, Shai, Shamir, Ohad, and Shammah, Shaked.
\newblock Failures of gradient-based deep learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3067--3075, 2017{\natexlab{a}}.

\bibitem[Shalev-Shwartz et~al.(2017{\natexlab{b}})Shalev-Shwartz, Shamir, and
  Shammah]{shalev2017weight}
Shalev-Shwartz, Shai, Shamir, Ohad, and Shammah, Shaked.
\newblock Weight sharing is crucial to succesful optimization.
\newblock \emph{arXiv preprint arXiv:1706.00687}, 2017{\natexlab{b}}.

\bibitem[Shamir(2016)]{shamir2016distribution}
Shamir, Ohad.
\newblock Distribution-specific hardness of learning neural networks.
\newblock \emph{arXiv preprint arXiv:1609.01037}, 2016.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
Silver, David, Huang, Aja, Maddison, Chris~J, Guez, Arthur, Sifre, Laurent, Van
  Den~Driessche, George, Schrittwieser, Julian, Antonoglou, Ioannis,
  Panneershelvam, Veda, Lanctot, Marc, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[{\v{S}}{\'\i}ma(2002)]{vsima2002training}
{\v{S}}{\'\i}ma, Ji{\v{r}}{\'\i}.
\newblock Training a single sigmoidal neuron is hard.
\newblock \emph{Neural Computation}, 14\penalty0 (11):\penalty0 2709--2728,
  2002.

\bibitem[Soltanolkotabi(2017)]{soltanolkotabi2017learning}
Soltanolkotabi, Mahdi.
\newblock Learning {ReLUs} via gradient descent.
\newblock \emph{arXiv preprint arXiv:1705.04591}, 2017.

\bibitem[Sun et~al.(2017)Sun, Qu, and Wright]{sun2017complete}
Sun, Ju, Qu, Qing, and Wright, John.
\newblock Complete dictionary recovery over the sphere {I}: Overview and the
  geometric picture.
\newblock \emph{IEEE Transactions on Information Theory}, 63\penalty0
  (2):\penalty0 853--884, 2017.

\bibitem[Tian(2017)]{tian2017analytical}
Tian, Yuandong.
\newblock An analytical formula of population gradient for two-layered {ReLU}
  network and its applications in convergence and critical point analysis.
\newblock \emph{arXiv preprint arXiv:1703.00560}, 2017.

\bibitem[Xie et~al.(2017)Xie, Liang, and Song]{xie2017diverse}
Xie, Bo, Liang, Yingyu, and Song, Le.
\newblock Diverse neural network learns true target functions.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1216--1224,
  2017.

\bibitem[Zhang et~al.(2015)Zhang, Lee, Wainwright, and
  Jordan]{zhang2015learning}
Zhang, Yuchen, Lee, Jason~D, Wainwright, Martin~J, and Jordan, Michael~I.
\newblock Learning halfspaces and neural networks with random initialization.
\newblock \emph{arXiv preprint arXiv:1511.07948}, 2015.

\bibitem[Zhong et~al.(2017{\natexlab{a}})Zhong, Song, and
  Dhillon]{zhong2017learning}
Zhong, Kai, Song, Zhao, and Dhillon, Inderjit~S.
\newblock Learning non-overlapping convolutional neural networks with multiple
  kernels.
\newblock \emph{arXiv preprint arXiv:1711.03440}, 2017{\natexlab{a}}.

\bibitem[Zhong et~al.(2017{\natexlab{b}})Zhong, Song, Jain, Bartlett, and
  Dhillon]{zhong2017recovery}
Zhong, Kai, Song, Zhao, Jain, Prateek, Bartlett, Peter~L, and Dhillon,
  Inderjit~S.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock \emph{arXiv preprint arXiv:1706.03175}, 2017{\natexlab{b}}.

\bibitem[Zhou \& Feng(2017)Zhou and Feng]{zhou2017landscape}
Zhou, Pan and Feng, Jiashi.
\newblock The landscape of deep learning algorithms.
\newblock \emph{arXiv preprint arXiv:1705.07038}, 2017.

\end{thebibliography}
