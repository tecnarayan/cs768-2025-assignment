\begin{thebibliography}{}

\bibitem[Agarwal et~al., 2020]{agarwal2020model}
Agarwal, A., Kakade, S., and Yang, L.~F. (2020).
\newblock Model-based reinforcement learning with a generative model is minimax
  optimal.
\newblock In {\em Conference on Learning Theory}, pages 67--83. PMLR.

\bibitem[Aumann, 1987]{aumann1987correlated}
Aumann, R.~J. (1987).
\newblock Correlated equilibrium as an expression of {B}ayesian rationality.
\newblock {\em Econometrica: Journal of the Econometric Society}, pages 1--18.

\bibitem[Azar et~al., 2013]{azar2013minimax}
Azar, M.~G., Munos, R., and Kappen, H.~J. (2013).
\newblock Minimax {PAC} bounds on the sample complexity of reinforcement
  learning with a generative model.
\newblock {\em Machine learning}, 91(3):325--349.

\bibitem[Azar et~al., 2017]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R. (2017).
\newblock Minimax regret bounds for reinforcement learning.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning}, pages 263--272. JMLR. org.

\bibitem[Bai and Jin, 2020]{bai2020provable}
Bai, Y. and Jin, C. (2020).
\newblock Provable self-play algorithms for competitive reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  551--560. PMLR.

\bibitem[Bai et~al., 2020]{bai2020near}
Bai, Y., Jin, C., and Yu, T. (2020).
\newblock Near-optimal reinforcement learning with self-play.
\newblock {\em Advances in neural information processing systems},
  33:2159--2170.

\bibitem[Beck, 2017]{beck2017first}
Beck, A. (2017).
\newblock {\em First-order methods in optimization}.
\newblock SIAM.

\bibitem[Beck and Srikant, 2012]{beck2012error}
Beck, C.~L. and Srikant, R. (2012).
\newblock Error bounds for constant step-size {Q}-learning.
\newblock {\em Systems \& control letters}, 61(12):1203--1208.

\bibitem[Bertsekas, 2017]{bertsekas2017dynamic}
Bertsekas, D.~P. (2017).
\newblock {\em Dynamic programming and optimal control (4th edition)}.
\newblock Athena Scientific.

\bibitem[Brown and Sandholm, 2019]{brown2019superhuman}
Brown, N. and Sandholm, T. (2019).
\newblock Superhuman {AI} for multiplayer poker.
\newblock {\em Science}, 365(6456):885--890.

\bibitem[Cen et~al., 2022]{cen2022faster}
Cen, S., Chi, Y., Du, S., and Xiao, L. (2022).
\newblock Faster last-iterate convergence of policy optimization in zero-sum
  markov games.
\newblock {\em arXiv preprint arXiv:2210.01050}.

\bibitem[Cen et~al., 2021]{cen2021fast}
Cen, S., Wei, Y., and Chi, Y. (2021).
\newblock Fast policy extragradient methods for competitive games with entropy
  regularization.
\newblock {\em Advances in Neural Information Processing Systems}, 34.

\bibitem[Chen et~al., 2015]{chen2015well}
Chen, X., Cheng, Y., and Tang, B. (2015).
\newblock Well-supported versus approximate {N}ash equilibria: Query complexity
  of large games.
\newblock {\em arXiv preprint arXiv:1511.00785}.

\bibitem[Chen et~al., 2021a]{chen2021sample}
Chen, Z., Ma, S., and Zhou, Y. (2021a).
\newblock Sample efficient stochastic policy extragradient algorithm for
  zero-sum {M}arkov game.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Chen et~al., 2020]{chen2020finite}
Chen, Z., Maguluri, S.~T., Shakkottai, S., and Shanmugam, K. (2020).
\newblock Finite-sample analysis of stochastic approximation using smooth
  convex envelopes.
\newblock {\em arXiv preprint arXiv:2002.00874}.

\bibitem[Chen et~al., 2021b]{chen2021almost}
Chen, Z., Zhou, D., and Gu, Q. (2021b).
\newblock Almost optimal algorithms for two-player {M}arkov games with linear
  function approximation.
\newblock {\em arXiv preprint arXiv:2102.07404}.

\bibitem[Chen et~al., 2022]{chen2022almost}
Chen, Z., Zhou, D., and Gu, Q. (2022).
\newblock Almost optimal algorithms for two-player zero-sum linear mixture
  {M}arkov games.
\newblock In {\em International Conference on Algorithmic Learning Theory},
  pages 227--261. PMLR.

\bibitem[Cui and Du, 2022a]{cui2022provably}
Cui, Q. and Du, S.~S. (2022a).
\newblock Provably efficient offline multi-agent reinforcement learning via
  strategy-wise bonus.
\newblock {\em arXiv preprint arXiv:2206.00159}.

\bibitem[Cui and Du, 2022b]{cui2022offline}
Cui, Q. and Du, S.~S. (2022b).
\newblock When is offline two-player zero-sum {M}arkov game solvable?
\newblock {\em arXiv preprint arXiv:2201.03522}.

\bibitem[Cui and Yang, 2021]{cui2021minimax}
Cui, Q. and Yang, L.~F. (2021).
\newblock Minimax sample complexity for turn-based stochastic game.
\newblock In {\em Uncertainty in Artificial Intelligence}, pages 1496--1504.
  PMLR.

\bibitem[Daskalakis, 2013]{daskalakis2013complexity}
Daskalakis, C. (2013).
\newblock On the complexity of approximating a nash equilibrium.
\newblock {\em ACM Transactions on Algorithms (TALG)}, 9(3):1--35.

\bibitem[Daskalakis et~al., 2020]{daskalakis2020independent}
Daskalakis, C., Foster, D.~J., and Golowich, N. (2020).
\newblock Independent policy gradient methods for competitive reinforcement
  learning.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 5527--5540.

\bibitem[Daskalakis et~al., 2009]{daskalakis2009complexity}
Daskalakis, C., Goldberg, P.~W., and Papadimitriou, C.~H. (2009).
\newblock The complexity of computing a {N}ash equilibrium.
\newblock {\em SIAM Journal on Computing}, 39(1):195--259.

\bibitem[Daskalakis et~al., 2022]{daskalakis2022complexity}
Daskalakis, C., Golowich, N., and Zhang, K. (2022).
\newblock The complexity of markov equilibrium in stochastic games.
\newblock {\em arXiv preprint arXiv:2204.03991}.

\bibitem[Dou et~al., 2022]{dou2022gap}
Dou, Z., Yang, Z., Wang, Z., and Du, S. (2022).
\newblock Gap-dependent bounds for two-player markov games.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 432--455.

\bibitem[Du et~al., 2020]{du2020good}
Du, S.~S., Kakade, S.~M., Wang, R., and Yang, L.~F. (2020).
\newblock Is a good representation sufficient for sample efficient
  reinforcement learning?
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Even-Dar and Mansour, 2003]{even2003learning}
Even-Dar, E. and Mansour, Y. (2003).
\newblock Learning rates for {Q}-learning.
\newblock {\em Journal of machine learning Research}, 5(Dec):1--25.

\bibitem[Freedman, 1975]{freedman1975tail}
Freedman, D.~A. (1975).
\newblock On tail probabilities for martingales.
\newblock {\em the Annals of Probability}, pages 100--118.

\bibitem[Hansen et~al., 2013]{hansen2013strategy}
Hansen, T.~D., Miltersen, P.~B., and Zwick, U. (2013).
\newblock Strategy iteration is strongly polynomial for 2-player turn-based
  stochastic games with a constant discount factor.
\newblock {\em Journal of the ACM (JACM)}, 60(1):1--16.

\bibitem[Jia et~al., 2019]{jia2019feature}
Jia, Z., Yang, L.~F., and Wang, M. (2019).
\newblock Feature-based q-learning for two-player stochastic games.
\newblock {\em arXiv preprint arXiv:1906.00423}.

\bibitem[Jin et~al., 2018]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I. (2018).
\newblock Is {Q}-learning provably efficient?
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4863--4873.

\bibitem[Jin et~al., 2021]{jin2021v}
Jin, C., Liu, Q., Wang, Y., and Yu, T. (2021).
\newblock V-learning--a simple, efficient, decentralized algorithm for
  multiagent {RL}.
\newblock {\em arXiv preprint arXiv:2110.14555}.

\bibitem[Jin et~al., 2022]{jin2022complexity}
Jin, Y., Muthukumar, V., and Sidford, A. (2022).
\newblock The complexity of infinite-horizon general-sum stochastic games.
\newblock {\em arXiv preprint arXiv:2204.04186}.

\bibitem[Jin and Sidford, 2021]{jin2021towards}
Jin, Y. and Sidford, A. (2021).
\newblock Towards tight bounds on the sample complexity of average-reward
  {MDP}s.
\newblock In {\em International Conference on Machine Learning}, pages
  5055--5064. PMLR.

\bibitem[Kakade, 2003]{kakade2003sample}
Kakade, S. (2003).
\newblock {\em On the sample complexity of reinforcement learning}.
\newblock PhD thesis, University of London.

\bibitem[Kearns et~al., 2002]{kearns2002sparse}
Kearns, M., Mansour, Y., and Ng, A.~Y. (2002).
\newblock A sparse sampling algorithm for near-optimal planning in large
  {M}arkov decision processes.
\newblock {\em Machine learning}, 49(2-3):193--208.

\bibitem[Khamaru et~al., 2021]{khamaru2020temporal}
Khamaru, K., Pananjady, A., Ruan, F., Wainwright, M.~J., and Jordan, M.~I.
  (2021).
\newblock Is temporal difference learning optimal? an instance-dependent
  analysis.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 3(4):1013--1040.

\bibitem[Lattimore and Szepesv{\'a}ri, 2020]{lattimore2020bandit}
Lattimore, T. and Szepesv{\'a}ri, C. (2020).
\newblock {\em Bandit algorithms}.
\newblock Cambridge University Press.

\bibitem[Li et~al., 2021a]{li2021q}
Li, G., Cai, C., Chen, Y., Gu, Y., Wei, Y., and Chi, Y. (2021a).
\newblock Is {Q}-learning minimax optimal? a tight sample complexity analysis.
\newblock {\em arXiv preprint arXiv:2102.06548}.

\bibitem[Li et~al., 2021b]{li2021sample}
Li, G., Chen, Y., Chi, Y., Gu, Y., and Wei, Y. (2021b).
\newblock Sample-efficient reinforcement learning is feasible for linearly
  realizable {MDP}s with limited revisiting.
\newblock {\em Advances in Neural Information Processing Systems},
  34:16671--16685.

\bibitem[Li et~al., 2022]{li2022settling}
Li, G., Shi, L., Chen, Y., Chi, Y., and Wei, Y. (2022).
\newblock Settling the sample complexity of model-based offline reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:2204.05275}.

\bibitem[Li et~al., 2021c]{li2021breaking}
Li, G., Shi, L., Chen, Y., Gu, Y., and Chi, Y. (2021c).
\newblock Breaking the sample complexity barrier to regret-optimal model-free
  reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 34.

\bibitem[Li et~al., 2020]{li2020breaking}
Li, G., Wei, Y., Chi, Y., Gu, Y., and Chen, Y. (2020).
\newblock Breaking the sample size barrier in model-based reinforcement
  learning with a generative model.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33.

\bibitem[Littman, 1994]{littman1994markov}
Littman, M.~L. (1994).
\newblock Markov games as a framework for multi-agent reinforcement learning.
\newblock In {\em Machine learning proceedings 1994}, pages 157--163. Elsevier.

\bibitem[Liu et~al., 2021]{liu2021sharp}
Liu, Q., Yu, T., Bai, Y., and Jin, C. (2021).
\newblock A sharp analysis of model-based reinforcement learning with
  self-play.
\newblock In {\em International Conference on Machine Learning}, pages
  7001--7010.

\bibitem[Mao and Ba{\c{s}}ar, 2022]{mao2022provably}
Mao, W. and Ba{\c{s}}ar, T. (2022).
\newblock Provably efficient reinforcement learning in decentralized
  general-sum {M}arkov games.
\newblock {\em Dynamic Games and Applications}, pages 1--22.

\bibitem[Matignon et~al., 2012]{matignon2012coordinated}
Matignon, L., Jeanpierre, L., and Mouaddib, A.-I. (2012).
\newblock Coordinated multi-robot exploration under communication constraints
  using decentralized markov decision processes.
\newblock In {\em Twenty-sixth AAAI conference on artificial intelligence}.

\bibitem[Mou et~al., 2020]{mou2020linear}
Mou, W., Li, C.~J., Wainwright, M.~J., Bartlett, P.~L., and Jordan, M.~I.
  (2020).
\newblock On linear stochastic approximation: Fine-grained {P}olyak-{R}uppert
  and non-asymptotic concentration.
\newblock {\em arXiv preprint arXiv:2004.04719}.

\bibitem[Moulin and Vial, 1978]{moulin1978strategically}
Moulin, H. and Vial, J.-P. (1978).
\newblock Strategically zero-sum games: the class of games whose completely
  mixed equilibria cannot be improved upon.
\newblock {\em International Journal of Game Theory}, 7(3):201--221.

\bibitem[Nash, 1951]{nash1951non}
Nash, J. (1951).
\newblock Non-cooperative games.
\newblock {\em Annals of mathematics}, pages 286--295.

\bibitem[Nash~Jr, 1950]{nash1950equilibrium}
Nash~Jr, J.~F. (1950).
\newblock Equilibrium points in $n$-person games.
\newblock {\em Proceedings of the national academy of sciences}, 36(1):48--49.

\bibitem[Ozdaglar et~al., 2021]{ozdaglar2021independent}
Ozdaglar, A., Sayin, M.~O., and Zhang, K. (2021).
\newblock Independent learning in stochastic games.
\newblock {\em arXiv preprint arXiv:2111.11743}.

\bibitem[Pananjady and Wainwright, 2020]{pananjady2020instance}
Pananjady, A. and Wainwright, M.~J. (2020).
\newblock Instance-dependent $\ell_{\infty}$-bounds for policy evaluation in
  tabular reinforcement learning.
\newblock {\em IEEE Transactions on Information Theory}, 67(1):566--585.

\bibitem[Perolat et~al., 2015]{perolat2015approximate}
Perolat, J., Scherrer, B., Piot, B., and Pietquin, O. (2015).
\newblock Approximate dynamic programming for two-player zero-sum {M}arkov
  games.
\newblock In {\em International Conference on Machine Learning}, pages
  1321--1329. PMLR.

\bibitem[Rubinstein, 2016]{rubinstein2016settling}
Rubinstein, A. (2016).
\newblock Settling the complexity of computing approximate two-player nash
  equilibria.
\newblock In {\em Annual Symposium on Foundations of Computer Science (FOCS)},
  pages 258--265.

\bibitem[Sayin et~al., 2021]{sayin2021decentralized}
Sayin, M., Zhang, K., Leslie, D., Basar, T., and Ozdaglar, A. (2021).
\newblock Decentralized {Q}-learning in zero-sum {M}arkov games.
\newblock {\em Advances in Neural Information Processing Systems}, 34.

\bibitem[Shalev-Shwartz, 2007]{shalev2007online}
Shalev-Shwartz, S. (2007).
\newblock Online learning: Theory, algorithms, and applications.

\bibitem[Shalev-Shwartz, 2012]{shalev2012online}
Shalev-Shwartz, S. (2012).
\newblock Online learning and online convex optimization.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  4(2):107--194.

\bibitem[Shalev-Shwartz et~al., 2016]{shalev2016safe}
Shalev-Shwartz, S., Shammah, S., and Shashua, A. (2016).
\newblock Safe, multi-agent, reinforcement learning for autonomous driving.
\newblock {\em arXiv preprint arXiv:1610.03295}.

\bibitem[Shalev-Shwartz and Singer, 2007]{shalev2007primal}
Shalev-Shwartz, S. and Singer, Y. (2007).
\newblock A primal-dual perspective of online learning algorithms.
\newblock {\em Machine Learning}, 69(2):115--142.

\bibitem[Shapley, 1953]{shapley1953stochastic}
Shapley, L.~S. (1953).
\newblock Stochastic games.
\newblock {\em Proceedings of the national academy of sciences},
  39(10):1095--1100.

\bibitem[Sidford et~al., 2018a]{sidford2018near}
Sidford, A., Wang, M., Wu, X., Yang, L., and Ye, Y. (2018a).
\newblock Near-optimal time and sample complexities for solving {M}arkov
  decision processes with a generative model.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5186--5196.

\bibitem[Sidford et~al., 2018b]{sidford2018variance}
Sidford, A., Wang, M., Wu, X., and Ye, Y. (2018b).
\newblock Variance reduced value iteration and faster algorithms for solving
  {M}arkov decision processes.
\newblock In {\em Symposium on Discrete Algorithms}, pages 770--787. SIAM.

\bibitem[Sidford et~al., 2020]{sidford2020solving}
Sidford, A., Wang, M., Yang, L., and Ye, Y. (2020).
\newblock Solving discounted stochastic two-player games with near-optimal time
  and sample complexity.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2992--3002. PMLR.

\bibitem[Song et~al., 2021]{song2021can}
Song, Z., Mei, S., and Bai, Y. (2021).
\newblock When can we learn general-sum {M}arkov games with a large number of
  players sample-efficiently?
\newblock {\em arXiv preprint arXiv:2110.04184}.

\bibitem[Tian et~al., 2021]{tian2021online}
Tian, Y., Wang, Y., Yu, T., and Sra, S. (2021).
\newblock Online learning in unknown markov games.
\newblock In {\em International conference on machine learning}, pages
  10279--10288. PMLR.

\bibitem[Vaswani et~al., 2022]{vaswani2022near}
Vaswani, S., Yang, L.~F., and Szepesv{\'a}ri, C. (2022).
\newblock Near-optimal sample complexity bounds for constrained {MDP}s.
\newblock {\em arXiv preprint arXiv:2206.06270}.

\bibitem[Vinyals et~al., 2019]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., and Georgiev, P. (2019).
\newblock Grandmaster level in {S}tarcraft {II} using multi-agent reinforcement
  learning.
\newblock {\em Nature}, 575(7782):350--354.

\bibitem[Wainwright, 2019a]{wainwright2019stochastic}
Wainwright, M.~J. (2019a).
\newblock Stochastic approximation with cone-contractive operators: Sharp
  $\ell_{\infty}$-bounds for {Q}-learning.
\newblock {\em arXiv preprint arXiv:1905.06265}.

\bibitem[Wainwright, 2019b]{wainwright2019variance}
Wainwright, M.~J. (2019b).
\newblock Variance-reduced {Q}-learning is minimax optimal.
\newblock {\em arXiv preprint arXiv:1906.04697}.

\bibitem[Wang et~al., 2021]{wang2021sample}
Wang, B., Yan, Y., and Fan, J. (2021).
\newblock Sample-efficient reinforcement learning for linearly-parameterized
  mdps with a generative model.
\newblock {\em Advances in Neural Information Processing Systems}, 34.

\bibitem[Wei et~al., 2017]{wei2017online}
Wei, C.-Y., Hong, Y.-T., and Lu, C.-J. (2017).
\newblock Online reinforcement learning in stochastic games.
\newblock {\em Advances in Neural Information Processing Systems}, 30.

\bibitem[Wei et~al., 2021]{wei2021last}
Wei, C.-Y., Lee, C.-W., Zhang, M., and Luo, H. (2021).
\newblock Last-iterate convergence of decentralized optimistic gradient
  descent/ascent in infinite-horizon competitive {M}arkov games.
\newblock In {\em Conference on Learning Theory}, pages 4259--4299. PMLR.

\bibitem[Weisz et~al., 2021]{weisz2021exponential}
Weisz, G., Amortila, P., and Szepesv{\'a}ri, C. (2021).
\newblock Exponential lower bounds for planning in {MDP}s with
  linearly-realizable optimal action-value functions.
\newblock In {\em Algorithmic Learning Theory}, pages 1237--1264. PMLR.

\bibitem[Xie et~al., 2020]{xie2020learning}
Xie, Q., Chen, Y., Wang, Z., and Yang, Z. (2020).
\newblock Learning zero-sum simultaneous-move {M}arkov games using function
  approximation and correlated equilibrium.
\newblock In {\em Conference on Learning Theory}, pages 3674--3682. PMLR.

\bibitem[Yan et~al., 2022]{yan2022model}
Yan, Y., Li, G., Chen, Y., and Fan, J. (2022).
\newblock Model-based reinforcement learning is minimax-optimal for offline
  zero-sum markov games.
\newblock {\em arXiv preprint arXiv:2206.04044}.

\bibitem[Yang and Wang, 2019]{yang2019sample}
Yang, L. and Wang, M. (2019).
\newblock Sample-optimal parametric {Q}-learning using linearly additive
  features.
\newblock In {\em International Conference on Machine Learning}, pages
  6995--7004.

\bibitem[Yang and Ma, 2022]{yang2022t}
Yang, Y. and Ma, C. (2022).
\newblock $ o (t^{-1}) $ convergence of
  optimistic-follow-the-regularized-leader in two-player zero-sum markov games.
\newblock {\em arXiv preprint arXiv:2209.12430}.

\bibitem[Yin et~al., 2022]{yin2022efficient}
Yin, D., Hao, B., Abbasi-Yadkori, Y., Lazi{\'c}, N., and Szepesv{\'a}ri, C.
  (2022).
\newblock Efficient local planning with linear function approximation.
\newblock In {\em International Conference on Algorithmic Learning Theory},
  pages 1165--1192. PMLR.

\bibitem[Zanette et~al., 2019]{zanette2019almost}
Zanette, A., Kochenderfer, M.~J., and Brunskill, E. (2019).
\newblock Almost horizon-free structure-aware best policy identification with a
  generative model.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Zanette et~al., 2020]{zanette2020provably}
Zanette, A., Lazaric, A., Kochenderfer, M.~J., and Brunskill, E. (2020).
\newblock Provably efficient reward-agnostic navigation with linear value
  iteration.
\newblock {\em Advances in Neural Information Processing Systems},
  33:11756--11766.

\bibitem[Zhang et~al., 2020]{zhang2020marl}
Zhang, K., Kakade, S., Basar, T., and Yang, L. (2020).
\newblock Model-based multi-agent {RL} in zero-sum {M}arkov games with
  near-optimal sample complexity.
\newblock {\em Advances in Neural Information Processing Systems}, 33.

\bibitem[Zhang et~al., 2021]{zhang2021multi}
Zhang, K., Yang, Z., and Ba{\c{s}}ar, T. (2021).
\newblock Multi-agent reinforcement learning: A selective overview of theories
  and algorithms.
\newblock {\em Handbook of Reinforcement Learning and Control}, pages 321--384.

\bibitem[Zhang et~al., 2022]{zhang2022policy}
Zhang, R., Liu, Q., Wang, H., Xiong, C., Li, N., and Bai, Y. (2022).
\newblock Policy optimization for {M}arkov games: Unified framework and faster
  convergence.
\newblock {\em arXiv preprint arXiv:2206.02640}.

\bibitem[Zhao et~al., 2021]{zhao2021provably}
Zhao, Y., Tian, Y., Lee, J.~D., and Du, S.~S. (2021).
\newblock Provably efficient policy gradient methods for two-player zero-sum
  {M}arkov games.
\newblock {\em arXiv preprint arXiv:2102.08903}.

\bibitem[Zhong et~al., 2022]{zhong2022pessimistic}
Zhong, H., Xiong, W., Tan, J., Wang, L., Zhang, T., Wang, Z., and Yang, Z.
  (2022).
\newblock Pessimistic minimax value iteration: Provably efficient equilibrium
  learning from offline datasets.
\newblock {\em arXiv preprint arXiv:2202.07511}.

\end{thebibliography}
