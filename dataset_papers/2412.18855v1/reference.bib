@article{nair2020awac,
  title={Awac: Accelerating online reinforcement learning with offline datasets},
  author={Nair, Ashvin and Gupta, Abhishek and Dalal, Murtaza and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.09359},
  year={2020}
}

@inproceedings{kostrikov2021iql,
  title={Offline Reinforcement Learning with Implicit Q-Learning},
  author={Kostrikov, Ilya and Nair, Ashvin and Levine, Sergey},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{kumar2020cql,
  title={Conservative q-learning for offline reinforcement learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1179--1191},
  year={2020}
}

@article{fujimoto2021td3bc,
  title={A minimalist approach to offline reinforcement learning},
  author={Fujimoto, Scott and Gu, Shixiang Shane},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={20132--20145},
  year={2021}
}

@inproceedings{haarnoja2018sac,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}

@inproceedings{fujimoto2018td3,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International conference on machine learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}

@article{schulman2017ppo,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{li2023proto,
  title={PROTO: Iterative Policy Regularized Offline-to-Online Reinforcement Learning},
  author={Li, Jianxiong and Hu, Xiao and Xu, Haoran and Liu, Jingjing and Zhan, Xianyuan and Zhang, Ya-Qin},
  journal={arXiv preprint arXiv:2305.15669},
  year={2023}
}

@inproceedings{wang2023miql,
  title={Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning},
  author={Wang, Shenzhi and Yang, Qisen and Gao, Jiawei and Lin, Matthieu Gaetan and CHEN, HAO and Wu, Liwei and Jia, Ning and Song, Shiji and Huang, Gao},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}

@inproceedings{uchendu2023jump,
  title={Jump-start reinforcement learning},
  author={Uchendu, Ikechukwu and Xiao, Ted and Lu, Yao and Zhu, Banghua and Yan, Mengyuan and Simon, Jos{\'e}phine and Bennice, Matthew and Fu, Chuyuan and Ma, Cong and Jiao, Jiantao and others},
  booktitle={International Conference on Machine Learning},
  pages={34556--34583},
  year={2023},
  organization={PMLR}
}

@article{guo2023sung,
  title={A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning},
  author={Guo, Siyuan and Sun, Yanchao and Hu, Jifeng and Huang, Sili and Chen, Hechang and Piao, Haiyin and Sun, Lichao and Chang, Yi},
  journal={arXiv preprint arXiv:2306.07541},
  year={2023}
}

@article{luo2023td3lmbda,
  title={Finetuning from Offline Reinforcement Learning: Challenges, Trade-offs and Practical Solutions},
  author={Luo, Yicheng and Kay, Jackie and Grefenstette, Edward and Deisenroth, Marc Peter},
  journal={arXiv preprint arXiv:2303.17396},
  year={2023}
}

@article{beeson2022td3refine,
  title={Improving td3-bc: Relaxed policy constraint for offline learning and stable online fine-tuning},
  author={Beeson, Alex and Montana, Giovanni},
  journal={arXiv preprint arXiv:2211.11802},
  year={2022}
}

@article{nakamoto2023cal,
  title={Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning},
  author={Nakamoto, Mitsuhiko and Zhai, Yuexiang and Singh, Anikait and Mark, Max Sobol and Ma, Yi and Finn, Chelsea and Kumar, Aviral and Levine, Sergey},
  journal={arXiv preprint arXiv:2303.05479},
  year={2023}
}

@article{zhao2022td3ada,
  title={Adaptive behavior cloning regularization for stable offline-to-online reinforcement learning},
  author={Zhao, Yi and Boney, Rinu and Ilin, Alexander and Kannala, Juho and Pajarinen, Joni},
  journal={arXiv preprint arXiv:2210.13846},
  year={2022}
}

@article{mao2022moore,
  title={MOORe: Model-based Offline-to-Online Reinforcement Learning},
  author={Mao, Yihuan and Wang, Chao and Wang, Bin and Zhang, Chongjie},
  journal={arXiv preprint arXiv:2201.10070},
  year={2022}
}

@inproceedings{lee2022br,
  title={Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble},
  author={Lee, Seunghyun and Seo, Younggyo and Lee, Kimin and Abbeel, Pieter and Shin, Jinwoo},
  booktitle={Conference on Robot Learning},
  pages={1702--1712},
  year={2022},
  organization={PMLR}
}

@inproceedings{yu2023aca,
  title={Actor-critic alignment for offline-to-online reinforcement learning},
  author={Yu, Zishun and Zhang, Xinhua},
  booktitle={International Conference on Machine Learning},
  pages={40452--40474},
  year={2023},
  organization={PMLR}
}

@article{zhang2023pex,
  title={Policy Expansion for Bridging Offline-to-Online Reinforcement Learning},
  author={Zhang, Haichao and Xu, We and Yu, Haonan},
  journal={arXiv preprint arXiv:2302.00935},
  year={2023}
}

@inproceedings{mark2022o3f,
  title={Fine-tuning offline policies with optimistic action selection},
  author={Mark, Max Sobol and Ghadirzadeh, Ali and Chen, Xi and Finn, Chelsea},
  booktitle={Deep Reinforcement Learning Workshop NeurIPS 2022},
  year={2022}
}

@article{xu2023ivr,
  title={Offline rl with no ood actions: In-sample learning via implicit value regularization},
  author={Xu, Haoran and Jiang, Li and Li, Jianxiong and Yang, Zhuoran and Wang, Zhaoran and Chan, Victor Wai Kin and Zhan, Xianyuan},
  journal={arXiv preprint arXiv:2303.15810},
  year={2023}
}

@article{lyu2022mildlyql,
  title={Mildly conservative Q-learning for offline reinforcement learning},
  author={Lyu, Jiafei and Ma, Xiaoteng and Li, Xiu and Lu, Zongqing},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1711--1724},
  year={2022}
}

@inproceedings{zheng2022odt,
  title={Online decision transformer},
  author={Zheng, Qinqing and Zhang, Amy and Grover, Aditya},
  booktitle={international conference on machine learning},
  pages={27042--27059},
  year={2022},
  organization={PMLR}
}

@article{zheng2023apl,
  title={Adaptive policy learning for offline-to-online reinforcement learning},
  author={Zheng, Han and Luo, Xufang and Wei, Pengfei and Song, Xuan and Li, Dongsheng and Jiang, Jing},
  journal={arXiv preprint arXiv:2303.07693},
  year={2023}
}

@inproceedings{le2019fqe,
  title={Batch policy learning under constraints},
  author={Le, Hoang and Voloshin, Cameron and Yue, Yisong},
  booktitle={International Conference on Machine Learning},
  pages={3703--3712},
  year={2019},
  organization={PMLR}
}

@inproceedings{fujimoto2019bcq,
  title={Off-policy deep reinforcement learning without exploration},
  author={Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle={International conference on machine learning},
  pages={2052--2062},
  year={2019},
  organization={PMLR}
}

@inproceedings{cheng2022atac,
  title={Adversarially trained actor critic for offline reinforcement learning},
  author={Cheng, Ching-An and Xie, Tengyang and Jiang, Nan and Agarwal, Alekh},
  booktitle={International Conference on Machine Learning},
  pages={3852--3878},
  year={2022},
  organization={PMLR}
}

@article{zhao2023ensembleq,
  title={Ensemble-based Offline-to-Online Reinforcement Learning: From Pessimistic Learning to Optimistic Exploration},
  author={Zhao, Kai and Ma, Yi and Liu, Jinyi and Zheng, Yan and Meng, Zhaopeng},
  journal={arXiv preprint arXiv:2306.06871},
  year={2023}
}

@inproceedings{jang2022UQ,
  title={Uncertainty-Driven Pessimistic Q-Ensemble for Offline-to-Online Reinforcement Learning},
  author={Jang, Ingook and Kim, Seonghyun},
  booktitle={3rd Offline RL Workshop: Offline RL as a''Launchpad''},
  year={2022}
}

@article{Sutton_Barto_2005,   title={Reinforcement Learning: An Introduction},  url={http://dx.doi.org/10.1109/tnn.2004.842673},  DOI={10.1109/tnn.2004.842673},  journal={IEEE Transactions on Neural Networks},  author={Sutton, RichardS. and Barto, AndrewG.},  year={2005},  month={Jan},  pages={285â€“286},  language={en-US}  }

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}

@article{xie2021policy,
  title={Policy finetuning: Bridging sample-efficient offline and online reinforcement learning},
  author={Xie, Tengyang and Jiang, Nan and Wang, Huan and Xiong, Caiming and Bai, Yu},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={27395--27407},
  year={2021}
}

@article{rashidinejad2021bridging,
  title={Bridging offline reinforcement learning and imitation learning: A tale of pessimism},
  author={Rashidinejad, Paria and Zhu, Banghua and Ma, Cong and Jiao, Jiantao and Russell, Stuart},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={11702--11716},
  year={2021}
}

@article{paine2020hyperparameter,
  title={Hyperparameter selection for offline reinforcement learning},
  author={Paine, Tom Le and Paduraru, Cosmin and Michi, Andrea and Gulcehre, Caglar and Zolna, Konrad and Novikov, Alexander and Wang, Ziyu and de Freitas, Nando},
  journal={arXiv preprint arXiv:2007.09055},
  year={2020}
}

@inproceedings{haarnoja2017reinforcement,
  title={Reinforcement learning with deep energy-based policies},
  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1352--1361},
  year={2017},
  organization={PMLR}
}

@inproceedings{achiam2017cpo,
  title={Constrained policy optimization},
  author={Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={22--31},
  year={2017},
  organization={PMLR}
}

@article{tessler2018rcpo,
  title={Reward constrained policy optimization},
  author={Tessler, Chen and Mankowitz, Daniel J and Mannor, Shie},
  journal={arXiv preprint arXiv:1805.11074},
  year={2018}
}

@article{dalal2018safe,
  title={Safe exploration in continuous action spaces},
  author={Dalal, Gal and Dvijotham, Krishnamurthy and Vecerik, Matej and Hester, Todd and Paduraru, Cosmin and Tassa, Yuval},
  journal={arXiv preprint arXiv:1801.08757},
  year={2018}
}

@article{garg2023extreme,
  title={Extreme q-learning: Maxent RL without entropy},
  author={Garg, Divyansh and Hejna, Joey and Geist, Matthieu and Ermon, Stefano},
  journal={arXiv preprint arXiv:2301.02328},
  year={2023}
}

@article{wu2022supported,
  title={Supported policy optimization for offline reinforcement learning},
  author={Wu, Jialong and Wu, Haixu and Qiu, Zihan and Wang, Jianmin and Long, Mingsheng},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31278--31291},
  year={2022}
}

@article{mcinroe2023planning,
  title={Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement Learning},
  author={McInroe, Trevor and Albrecht, Stefano V and Storkey, Amos},
  journal={arXiv preprint arXiv:2310.05723},
  year={2023}
}

@article{agarwal2022reincarnating,
  title={Reincarnating reinforcement learning: Reusing prior computation to accelerate progress},
  author={Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={28955--28971},
  year={2022}
}

@article{brandfonbrener2021offline,
  title={Offline rl without off-policy evaluation},
  author={Brandfonbrener, David and Whitney, Will and Ranganath, Rajesh and Bruna, Joan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={4933--4946},
  year={2021}
}

@inproceedings{mao2023stro,
  title={Supported trust region optimization for offline reinforcement learning},
  author={Mao, Yixiu and Zhang, Hongchang and Chen, Chen and Xu, Yi and Ji, Xiangyang},
  booktitle={International Conference on Machine Learning},
  pages={23829--23851},
  year={2023},
  organization={PMLR}
}

@inproceedings{
tarasov2022corl,
  title={{CORL}: Research-oriented Deep Offline Reinforcement Learning Library},
  author={Denis Tarasov and Alexander Nikulin and Dmitry Akimov and Vladislav Kurenkov and Sergey Kolesnikov},
  booktitle={3rd Offline RL Workshop: Offline RL as a ''Launchpad''},
  year={2022},
  url={https://openreview.net/forum?id=SyAS49bBcv}
}

@article{yue2023understanding,
  title={Understanding, Predicting and Better Resolving Q-Value Divergence in Offline-RL},
  author={Yue, Yang and Lu, Rui and Kang, Bingyi and Song, Shiji and Huang, Gao},
  journal={arXiv preprint arXiv:2310.04411},
  year={2023}
}

@article{ball2023efficient,
  title={Efficient online reinforcement learning with offline data},
  author={Ball, Philip J and Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
  journal={arXiv preprint arXiv:2302.02948},
  year={2023}
}

@article{fu2020d4rl,
  title={D4rl: Datasets for deep data-driven reinforcement learning},
  author={Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2004.07219},
  year={2020}
}


@article{chen2021decision,
  title={Decision transformer: Reinforcement learning via sequence modeling},
  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={15084--15097},
  year={2021}
}

@article{tang2022health,
  title={Leveraging factored action spaces for efficient offline reinforcement learning in healthcare},
  author={Tang, Shengpu and Makar, Maggie and Sjoding, Michael and Doshi-Velez, Finale and Wiens, Jenna},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34272--34286},
  year={2022}
}

@article{diehl2023driving,
  title={Uncertainty-aware model-based offline reinforcement learning for automated driving},
  author={Diehl, Christopher and Sievernich, Timo Sebastian and Kr{\"u}ger, Martin and Hoffmann, Frank and Bertram, Torsten},
  journal={IEEE Robotics and Automation Letters},
  volume={8},
  number={2},
  pages={1167--1174},
  year={2023},
  publisher={IEEE}
}

@article{schulman2015gae,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1506.02438},
  year={2015}
}

@article{hansen2023idql,
  title={Idql: Implicit q-learning as an actor-critic method with diffusion policies},
  author={Hansen-Estruch, Philippe and Kostrikov, Ilya and Janner, Michael and Kuba, Jakub Grudzien and Levine, Sergey},
  journal={arXiv preprint arXiv:2304.10573},
  year={2023}
}

@article{xu2022guide,
  title={A policy-guided imitation approach for offline reinforcement learning},
  author={Xu, Haoran and Jiang, Li and Jianxiong, Li and Zhan, Xianyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4085--4098},
  year={2022}
}

@article{lei2023uni,
  title={Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization},
  author={Lei, Kun and He, Zhengmao and Lu, Chenhao and Hu, Kaizhe and Gao, Yang and Xu, Huazhe},
  journal={arXiv preprint arXiv:2311.03351},
  year={2023}
}

@article{agarwal2019reinforcementth,
  title={Reinforcement learning: Theory and algorithms},
  author={Agarwal, Alekh and Jiang, Nan and Kakade, Sham M and Sun, Wen},
  journal={CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep},
  volume={32},
  pages={96},
  year={2019}
}

@article{fcs_rl,
  title={Open and real-world human-AI coordination by heterogeneous training with communication},
  author={GUAN, Cong and XUE, Ke and FAN, Chunpeng and CHEN, Feng and ZHANG, Lichao and YUAN, Lei and QIAN, Chao and YU, Yang},
  journal={Frontiers of Computer Science},
  volume={19},
  number={4},
  pages={194314},
  year={2025},
  publisher={Higher Education Press}
}