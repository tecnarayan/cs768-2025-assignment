@article{precup2000eligibility,
  title={Eligibility traces for off-policy policy evaluation},
  author={Precup, Doina},
  journal={Computer Science Department Faculty Publication Series},
  pages={80},
  year={2000}
}
@inproceedings{ding2020hierarchical,
  title={Hierarchical Multi-Scale Gaussian Transformer for Stock Movement Prediction.},
  author={Ding, Qianggang and Wu, Sifan and Sun, Hao and Guo, Jiadong and Guo, Jian},
  booktitle={IJCAI},
  pages={4640--4646},
  year={2020}
}
@inproceedings{liu2022learning,
  title={Learning from demonstration: Provably efficient adversarial policy imitation with linear function approximation},
  author={Liu, Zhihan and Zhang, Yufeng and Fu, Zuyue and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={14094--14138},
  year={2022},
  organization={PMLR}
}
@inproceedings{sun2021neuro,
  title={Neuro-symbolic program search for autonomous driving decision module design},
  author={Sun, Jiankai and Sun, Hao and Han, Tian and Zhou, Bolei},
  booktitle={Conference on Robot Learning},
  pages={21--30},
  year={2021},
  organization={PMLR}
}
@article{yang2022rethinking,
  title={Rethinking goal-conditioned supervised learning and its connection to offline rl},
  author={Yang, Rui and Lu, Yiming and Li, Wenzhe and Sun, Hao and Fang, Meng and Du, Yali and Li, Xiu and Han, Lei and Zhang, Chongjie},
  journal={arXiv preprint arXiv:2202.04478},
  year={2022}
}
@article{sun2022daux,
  title={Daux: a density-based approach for uncertainty explanations},
  author={Sun, Hao and van Breugel, Boris and Crabbe, Jonathan and Seedat, Nabeel and van der Schaar, Mihaela},
  journal={arXiv preprint arXiv:2207.05161},
  year={2022}
}
@inproceedings{holt2023activeobservingcontrol,
author={Samuel Holt and Alihan H\"uy\"uk and Mihaela van der Schaar},
title={Active Observing in Continuous-time Control},
booktitle={Conference on Neural Information Processing Systems},
year={2023}
}
@article{sun2022exploit,
  title={Exploit Reward Shifting in Value-Based Deep-RL: Optimistic Curiosity-Based Exploration and Conservative Exploitation via Linear Reward Shaping},
  author={Sun, Hao and Han, Lei and Yang, Rui and Ma, Xiaoteng and Guo, Jian and Zhou, Bolei},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={37719--37734},
  year={2022}
}
@article{sun2021safe,
  title={Safe exploration by solving early terminated mdp},
  author={Sun, Hao and Xu, Ziping and Fang, Meng and Peng, Zhenghao and Guo, Jiadong and Dai, Bo and Zhou, Bolei},
  journal={arXiv preprint arXiv:2107.04200},
  year={2021}
}
@inproceedings{sun2022constrained,
  title={Constrained MDPs can be Solved by Eearly-Termination with Recurrent Models},
  author={Sun, Hao and Xu, Ziping and Peng, Zhenghao and Fang, Meng and Wang, Taiyi and Dai, Bo and Zhou, Bolei},
  booktitle={NeurIPS 2022 Foundation Models for Decision Making Workshop},
  year={2022}
}
@article{sun2019policy,
  title={Policy continuation with hindsight inverse dynamics},
  author={Sun, Hao and Li, Zhizhong and Liu, Xiaotong and Zhou, Bolei and Lin, Dahua},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{strehl2010learning,
  title={Learning from logged implicit exploration data},
  author={Strehl, Alex and Langford, John and Li, Lihong and Kakade, Sham M},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}
@inproceedings{matsson2022case,
  title={Case-based off-policy evaluation using prototype learning},
  author={Matsson, Anton and Johansson, Fredrik D},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={1339--1349},
  year={2022},
  organization={PMLR}
}
@inproceedings{
Fakoor2020Meta-Q-Learning,
title={Meta-Q-Learning},
author={Rasool Fakoor and Pratik Chaudhari and Stefano Soatto and Alexander J. Smola},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJeD3CEFPH}
}
@inproceedings{williams2016aggressive,
  title={Aggressive driving with model predictive path integral control},
  author={Williams, Grady and Drews, Paul and Goldfain, Brian and Rehg, James M and Theodorou, Evangelos A},
  booktitle={2016 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1433--1440},
  year={2016},
  organization={IEEE}
}
@inproceedings{williams2017information,
  title={Information theoretic MPC for model-based reinforcement learning},
  author={Williams, Grady and Wagener, Nolan and Goldfain, Brian and Drews, Paul and Rehg, James M and Boots, Byron and Theodorou, Evangelos A},
  booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1714--1721},
  year={2017},
  organization={IEEE}
}
@inproceedings{beygelzimer2009offset,
  title={The offset tree for learning with partial labels},
  author={Beygelzimer, Alina and Langford, John},
  booktitle={Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={129--138},
  year={2009}
}
@inproceedings{su2020doubly,
  title={Doubly robust off-policy evaluation with shrinkage},
  author={Su, Yi and Dimakopoulou, Maria and Krishnamurthy, Akshay and Dud{\'\i}k, Miroslav},
  booktitle={International Conference on Machine Learning},
  pages={9167--9176},
  year={2020},
  organization={PMLR}
}

@inproceedings{wang2017optimal,
  title={Optimal and adaptive off-policy evaluation in contextual bandits},
  author={Wang, Yu-Xiang and Agarwal, Alekh and Dud{\i}k, Miroslav},
  booktitle={International Conference on Machine Learning},
  pages={3589--3597},
  year={2017},
  organization={PMLR}
}

@article{chung2014empirical,
  title={Empirical evaluation of gated recurrent neural networks on sequence modeling},
  author={Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.3555},
  year={2014}
}

@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@article{jaques2019way,
  title={Way off-policy batch deep reinforcement learning of implicit human preferences in dialog},
  author={Jaques, Natasha and Ghandeharioun, Asma and Shen, Judy Hanwen and Ferguson, Craig and Lapedriza, Agata and Jones, Noah and Gu, Shixiang and Picard, Rosalind},
  journal={arXiv preprint arXiv:1907.00456},
  year={2019}
}

@inproceedings{kalashnikov2018scalable,
  title={Scalable deep reinforcement learning for vision-based robotic manipulation},
  author={Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and others},
  booktitle={Conference on Robot Learning},
  pages={651--673},
  year={2018},
  organization={PMLR}
}

@article{kahn2021badgr,
  title={Badgr: An autonomous self-supervised learning-based navigation system},
  author={Kahn, Gregory and Abbeel, Pieter and Levine, Sergey},
  journal={IEEE Robotics and Automation Letters},
  volume={6},
  number={2},
  pages={1312--1319},
  year={2021},
  publisher={IEEE}
}
@article{pomerleau1988alvinn,
  title={Alvinn: An autonomous land vehicle in a neural network},
  author={Pomerleau, Dean A},
  journal={Advances in neural information processing systems},
  volume={1},
  year={1988}
}
@inproceedings{jiang2016doubly,
  title={Doubly robust off-policy value evaluation for reinforcement learning},
  author={Jiang, Nan and Li, Lihong},
  booktitle={International Conference on Machine Learning},
  pages={652--661},
  year={2016},
  organization={PMLR}
}

@article{dudik2011doubly,
  title={Doubly robust policy evaluation and learning},
  author={Dud{\'\i}k, Miroslav and Langford, John and Li, Lihong},
  journal={arXiv preprint arXiv:1103.4601},
  year={2011}
}

@article{swaminathan2015self,
  title={The self-normalized estimator for counterfactual learning},
  author={Swaminathan, Adith and Joachims, Thorsten},
  journal={advances in neural information processing systems},
  volume={28},
  year={2015}
}


@article{nachum2019dualdice,
  title={Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections},
  author={Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{milani2022survey,
  title={A Survey of Explainable Reinforcement Learning},
  author={Milani, Stephanie and Topin, Nicholay and Veloso, Manuela and Fang, Fei},
  journal={arXiv preprint arXiv:2202.08434},
  year={2022}
}
@article{zhang2020gendice,
  title={Gendice: Generalized offline estimation of stationary values},
  author={Zhang, Ruiyi and Dai, Bo and Li, Lihong and Schuurmans, Dale},
  journal={arXiv preprint arXiv:2002.09072},
  year={2020}
}

@inproceedings{zhang2020gradientdice,
  title={Gradientdice: Rethinking generalized offline estimation of stationary values},
  author={Zhang, Shangtong and Liu, Bo and Whiteson, Shimon},
  booktitle={International Conference on Machine Learning},
  pages={11194--11203},
  year={2020},
  organization={PMLR}
}

@article{saito2020open,
  title={Open bandit dataset and pipeline: Towards realistic and reproducible off-policy evaluation},
  author={Saito, Yuta and Aihara, Shunsuke and Matsutani, Megumi and Narita, Yusuke},
  journal={arXiv preprint arXiv:2008.07146},
  year={2020}
}
@article{fu2021benchmarks,
  title={Benchmarks for deep off-policy evaluation},
  author={Fu, Justin and Norouzi, Mohammad and Nachum, Ofir and Tucker, George and Wang, Ziyu and Novikov, Alexander and Yang, Mengjiao and Zhang, Michael R and Chen, Yutian and Kumar, Aviral and others},
  journal={arXiv preprint arXiv:2103.16596},
  year={2021}
}
@article{voloshin2019empirical,
  title={Empirical study of off-policy policy evaluation for reinforcement learning},
  author={Voloshin, Cameron and Le, Hoang M and Jiang, Nan and Yue, Yisong},
  journal={arXiv preprint arXiv:1911.06854},
  year={2019}
}

@article{seedat2022data,
  title={Data-IQ: Characterizing subgroups with heterogeneous outcomes in tabular data},
  author={Seedat, Nabeel and Crabb{\'e}, Jonathan and Bica, Ioana and van der Schaar, Mihaela},
  journal={arXiv preprint arXiv:2210.13043},
  year={2022}
}

@inproceedings{fujimoto2019off,
  title={Off-policy deep reinforcement learning without exploration},
  author={Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle={International conference on machine learning},
  pages={2052--2062},
  year={2019},
  organization={PMLR}
}

@InProceedings{pmlr-v139-hu21d,
  title = 	 {Generalizable Episodic Memory for Deep Reinforcement Learning},
  author =       {Hu, Hao and Ye, Jianing and Zhu, Guangxiang and Ren, Zhizhou and Zhang, Chongjie},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4380--4390},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/hu21d/hu21d.pdf},
  url = 	 {https://proceedings.mlr.press/v139/hu21d.html},
  abstract = 	 {Episodic memory-based methods can rapidly latch onto past successful strategies by a non-parametric memory and improve sample efficiency of traditional reinforcement learning. However, little effort is put into the continuous domain, where a state is never visited twice, and previous episodic methods fail to efficiently aggregate experience across trajectories. To address this problem, we propose Generalizable Episodic Memory (GEM), which effectively organizes the state-action values of episodic memory in a generalizable manner and supports implicit planning on memorized trajectories. GEM utilizes a double estimator to reduce the overestimation bias induced by value propagation in the planning process. Empirical evaluation shows that our method significantly outperforms existing trajectory-based methods on various MuJoCo continuous control tasks. To further show the general applicability, we evaluate our method on Atari games with discrete action space, which also shows a significant improvement over baseline algorithms.}
}

@inproceedings{keane2019case,
  title={How case-based reasoning explains neural networks: A theoretical analysis of XAI using post-hoc explanation-by-example from a survey of ANN-CBR twin-systems},
  author={Keane, Mark T and Kenny, Eoin M},
  booktitle={Case-Based Reasoning Research and Development: 27th International Conference, ICCBR 2019, Otzenhausen, Germany, September 8--12, 2019, Proceedings 27},
  pages={155--171},
  year={2019},
  organization={Springer}
}


@article{crabbe2021explaining,
  title={Explaining latent representations with a corpus of examples},
  author={Crabb{\'e}, Jonathan and Qian, Zhaozhi and Imrie, Fergus and van der Schaar, Mihaela},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12154--12166},
  year={2021}
}


@article{ma2021offline,
  title={Offline reinforcement learning with value-based episodic memory},
  author={Ma, Xiaoteng and Yang, Yiqin and Hu, Hao and Liu, Qihan and Yang, Jun and Zhang, Chongjie and Zhao, Qianchuan and Liang, Bin},
  journal={arXiv preprint arXiv:2110.09796},
  year={2021}
}
@article{li2023neural,
  title={Neural Episodic Control with State Abstraction},
  author={Li, Zhuo and Zhu, Derui and Hu, Yujing and Xie, Xiaofei and Ma, Lei and Zheng, Yan and Song, Yan and Chen, Yingfeng and Zhao, Jianjun},
  journal={arXiv preprint arXiv:2301.11490},
  year={2023}
}

@article{precup2000eligibility,
  title={Eligibility traces for off-policy policy evaluation},
  author={Precup, Doina},
  journal={Computer Science Department Faculty Publication Series},
  pages={80},
  year={2000}
}

@article{lambert2022challenges,
  title={The challenges of exploration for offline reinforcement learning},
  author={Lambert, Nathan and Wulfmeier, Markus and Whitney, William and Byravan, Arunkumar and Bloesch, Michael and Dasagi, Vibhavari and Hertweck, Tim and Riedmiller, Martin},
  journal={arXiv preprint arXiv:2201.11861},
  year={2022}
}
@article{fujimoto2021minimalist,
  title={A minimalist approach to offline reinforcement learning},
  author={Fujimoto, Scott and Gu, Shixiang Shane},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={20132--20145},
  year={2021}
}
@article{wu2019behavior,
  title={Behavior regularized offline reinforcement learning},
  author={Wu, Yifan and Tucker, George and Nachum, Ofir},
  journal={arXiv preprint arXiv:1911.11361},
  year={2019}
}
@article{jarrett2020strictly,
  title={Strictly batch imitation learning by energy-based distribution matching},
  author={Jarrett, Daniel and Bica, Ioana and van der Schaar, Mihaela},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7354--7365},
  year={2020}
}
@article{peng2019advantage,
  title={Advantage-weighted regression: Simple and scalable off-policy reinforcement learning},
  author={Peng, Xue Bin and Kumar, Aviral and Zhang, Grace and Levine, Sergey},
  journal={arXiv preprint arXiv:1910.00177},
  year={2019}
}
@article{yarats2022don,
  title={Don't change the algorithm, change the data: Exploratory data for offline reinforcement learning},
  author={Yarats, Denis and Brandfonbrener, David and Liu, Hao and Laskin, Michael and Abbeel, Pieter and Lazaric, Alessandro and Pinto, Lerrel},
  journal={arXiv preprint arXiv:2201.13425},
  year={2022}
}
@inproceedings{agarwal2020optimistic,
  title={An optimistic perspective on offline reinforcement learning},
  author={Agarwal, Rishabh and Schuurmans, Dale and Norouzi, Mohammad},
  booktitle={International Conference on Machine Learning},
  pages={104--114},
  year={2020},
  organization={PMLR}
}
@article{kumar2019stabilizing,
  title={Stabilizing off-policy q-learning via bootstrapping error reduction},
  author={Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{an2021uncertainty,
  title={Uncertainty-based offline reinforcement learning with diversified q-ensemble},
  author={An, Gaon and Moon, Seungyong and Kim, Jang-Hyun and Song, Hyun Oh},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={7436--7447},
  year={2021}
}
@article{yang2022rorl,
  title={Rorl: Robust offline reinforcement learning via conservative smoothing},
  author={Yang, Rui and Bai, Chenjia and Ma, Xiaoteng and Wang, Zhaoran and Zhang, Chongjie and Han, Lei},
  journal={arXiv preprint arXiv:2206.02829},
  year={2022}
}
@article{nikulin2023anti,
  title={Anti-Exploration by Random Network Distillation},
  author={Nikulin, Alexander and Kurenkov, Vladislav and Tarasov, Denis and Kolesnikov, Sergey},
  journal={arXiv preprint arXiv:2301.13616},
  year={2023}
}
@article{schweighofer2021understanding,
  title={Understanding the effects of dataset characteristics on offline reinforcement learning},
  author={Schweighofer, Kajetan and Hofmarcher, Markus and Dinu, Marius-Constantin and Renz, Philipp and Bitto-Nemling, Angela and Patil, Vihang and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:2111.04714},
  year={2021}
}

@article{gulcehre2020rl,
  title={Rl unplugged: A suite of benchmarks for offline reinforcement learning},
  author={Gulcehre, Caglar and Wang, Ziyu and Novikov, Alexander and Paine, Thomas and G{\'o}mez, Sergio and Zolna, Konrad and Agarwal, Rishabh and Merel, Josh S and Mankowitz, Daniel J and Paduraru, Cosmin and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7248--7259},
  year={2020}
}


@article{kumar2020conservative,
  title={Conservative q-learning for offline reinforcement learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1179--1191},
  year={2020}
}


@inproceedings{cheng2022adversarially,
  title={Adversarially trained actor critic for offline reinforcement learning},
  author={Cheng, Ching-An and Xie, Tengyang and Jiang, Nan and Agarwal, Alekh},
  booktitle={International Conference on Machine Learning},
  pages={3852--3878},
  year={2022},
  organization={PMLR}
}

@article{kidambi2020morel,
  title={Morel: Model-based offline reinforcement learning},
  author={Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21810--21823},
  year={2020}
}
@article{lin2018episodic,
  title={Episodic memory deep q-networks},
  author={Lin, Zichuan and Zhao, Tianqi and Yang, Guangwen and Zhang, Lintao},
  journal={arXiv preprint arXiv:1805.07603},
  year={2018}
}
@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@inproceedings{silver2014deterministic,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={International conference on machine learning},
  pages={387--395},
  year={2014},
  organization={Pmlr}
}
@article{wang2018exponentially,
  title={Exponentially weighted imitation learning for batched historical data},
  author={Wang, Qing and Xiong, Jiechao and Han, Lei and Liu, Han and Zhang, Tong and others},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@inproceedings{sun2022supervised,
  title={Supervised Q-Learning for Continuous Control},
  author={Sun, Hao and Xu, Ziping and Wang, Taiyi and Fang, Meng and Zhou, Bolei},
  booktitle={Deep Reinforcement Learning Workshop NeurIPS 2022},
  year={2022}
}
@article{hein2018interpretable,
  title={Interpretable policies for reinforcement learning by genetic programming},
  author={Hein, Daniel and Udluft, Steffen and Runkler, Thomas A},
  journal={Engineering Applications of Artificial Intelligence},
  volume={76},
  pages={158--169},
  year={2018},
  publisher={Elsevier}
}
@inproceedings{silva2020optimization,
  title={Optimization methods for interpretable differentiable decision trees applied to reinforcement learning},
  author={Silva, Andrew and Gombolay, Matthew and Killian, Taylor and Jimenez, Ivan and Son, Sung-Hyun},
  booktitle={International conference on artificial intelligence and statistics},
  pages={1855--1865},
  year={2020},
  organization={PMLR}
}

@mastersthesis{jhunjhunwala2019policy,
  title={Policy extraction via online q-value distillation},
  author={Jhunjhunwala, Aman},
  year={2019},
  school={University of Waterloo}
}
@article{zhang2020interpretable,
  title={Interpretable policy derivation for reinforcement learning based on evolutionary feature synthesis},
  author={Zhang, Hengzhe and Zhou, Aimin and Lin, Xin},
  journal={Complex \& Intelligent Systems},
  volume={6},
  pages={741--753},
  year={2020},
  publisher={Springer}
}
@inproceedings{liu2019toward,
  title={Toward interpretable deep reinforcement learning with linear model u-trees},
  author={Liu, Guiliang and Schulte, Oliver and Zhu, Wang and Li, Qingcan},
  booktitle={Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10--14, 2018, Proceedings, Part II 18},
  pages={414--429},
  year={2019},
  organization={Springer}
}
@inproceedings{verma2018programmatically,
  title={Programmatically interpretable reinforcement learning},
  author={Verma, Abhinav and Murali, Vijayaraghavan and Singh, Rishabh and Kohli, Pushmeet and Chaudhuri, Swarat},
  booktitle={International Conference on Machine Learning},
  pages={5045--5054},
  year={2018},
  organization={PMLR}
}

@article{atrey2019exploratory,
  title={Exploratory not explanatory: Counterfactual analysis of saliency maps for deep reinforcement learning},
  author={Atrey, Akanksha and Clary, Kaleigh and Jensen, David},
  journal={arXiv preprint arXiv:1912.05743},
  year={2019}
}
@article{mott2019towards,
  title={Towards interpretable reinforcement learning using attention augmented agents},
  author={Mott, Alexander and Zoran, Daniel and Chrzanowski, Mike and Wierstra, Daan and Jimenez Rezende, Danilo},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@inproceedings{tang2020neuroevolution,
  title={Neuroevolution of self-interpretable agents},
  author={Tang, Yujin and Nguyen, Duong and Ha, David},
  booktitle={Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
  pages={414--424},
  year={2020}
}
@inproceedings{hayes2017improving,
  title={Improving robot controller transparency through autonomous policy explanation},
  author={Hayes, Bradley and Shah, Julie A},
  booktitle={Proceedings of the 2017 ACM/IEEE international conference on human-robot interaction},
  pages={303--312},
  year={2017}
}
@inproceedings{wang2019verbal,
  title={Verbal explanations for deep reinforcement learning neural networks with attention on extracted features},
  author={Wang, Xinzhi and Yuan, Shengcheng and Zhang, Hui and Lewis, Michael and Sycara, Katia},
  booktitle={2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)},
  pages={1--7},
  year={2019},
  organization={IEEE}
}
@inproceedings{ehsan2020human,
  title={Human-centered explainable ai: Towards a reflective sociotechnical approach},
  author={Ehsan, Upol and Riedl, Mark O},
  booktitle={HCI International 2020-Late Breaking Papers: Multimodality and Intelligence: 22nd HCI International Conference, HCII 2020, Copenhagen, Denmark, July 19--24, 2020, Proceedings 22},
  pages={449--466},
  year={2020},
  organization={Springer}
}
@inproceedings{bewley2021tripletree,
  title={Tripletree: A versatile interpretable representation of black box agents and their environments},
  author={Bewley, Tom and Lawry, Jonathan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={13},
  pages={11415--11422},
  year={2021}
}
@inproceedings{zhang2021off,
  title={Off-policy differentiable logic reinforcement learning},
  author={Zhang, Li and Li, Xin and Wang, Mingzhong and Tian, Andong},
  booktitle={Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13--17, 2021, Proceedings, Part II 21},
  pages={617--632},
  year={2021},
  organization={Springer}
}

@article{vouros2022explainable,
  title={Explainable deep reinforcement learning: state of the art and challenges},
  author={Vouros, George A},
  journal={ACM Computing Surveys},
  volume={55},
  number={5},
  pages={1--39},
  year={2022},
  publisher={ACM New York, NY}
}
@article{glanois2021survey,
  title={A survey on interpretable reinforcement learning},
  author={Glanois, Claire and Weng, Paul and Zimmer, Matthieu and Li, Dong and Yang, Tianpei and Hao, Jianye and Liu, Wulong},
  journal={arXiv preprint arXiv:2112.13112},
  year={2021}
}
@article{chan2021medkit,
  title={The medkit-learn (ing) environment: Medical decision modelling through simulation},
  author={Chan, Alex J and Bica, Ioana and Huyuk, Alihan and Jarrett, Daniel and van der Schaar, Mihaela},
  journal={arXiv preprint arXiv:2106.04240},
  year={2021}
}


@article{mansimov2018simple,
  title={Simple nearest neighbor policy method for continuous control tasks},
  author={Mansimov, Elman and Cho, Kyunghyun},
  year={2018}
}
@article{shah2018q,
  title={Q-learning with nearest neighbors},
  author={Shah, Devavrat and Xie, Qiaomin},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@article{gottesman2019guidelines,
  title={Guidelines for reinforcement learning in healthcare},
  author={Gottesman, Omer and Johansson, Fredrik and Komorowski, Matthieu and Faisal, Aldo and Sontag, David and Doshi-Velez, Finale and Celi, Leo Anthony},
  journal={Nature medicine},
  volume={25},
  number={1},
  pages={16--18},
  year={2019},
  publisher={Nature Publishing Group US New York}
}
@article{charpentier2021reinforcement,
  title={Reinforcement learning in economics and finance},
  author={Charpentier, Arthur and Elie, Romuald and Remlinger, Carl},
  journal={Computational Economics},
  pages={1--38},
  year={2021},
  publisher={Springer}
}
@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}
@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}
@article{tesauro1995temporal,
  title={Temporal difference learning and TD-Gammon},
  author={Tesauro, Gerald and others},
  journal={Communications of the ACM},
  volume={38},
  number={3},
  pages={58--68},
  year={1995}
}
@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}
@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}
@article{blundell2016model,
  title={Model-free episodic control},
  author={Blundell, Charles and Uria, Benigno and Pritzel, Alexander and Li, Yazhe and Ruderman, Avraham and Leibo, Joel Z and Rae, Jack and Wierstra, Daan and Hassabis, Demis},
  journal={arXiv preprint arXiv:1606.04460},
  year={2016}
}

@article{lengyel2007hippocampal,
  title={Hippocampal contributions to control: the third way},
  author={Lengyel, M{\'a}t{\'e} and Dayan, Peter},
  journal={Advances in neural information processing systems},
  volume={20},
  year={2007}
}

@inproceedings{pritzel2017neural,
  title={Neural episodic control},
  author={Pritzel, Alexander and Uria, Benigno and Srinivasan, Sriram and Badia, Adria Puigdomenech and Vinyals, Oriol and Hassabis, Demis and Wierstra, Daan and Blundell, Charles},
  booktitle={International conference on machine learning},
  pages={2827--2836},
  year={2017},
  organization={PMLR}
}


@article{yu2020mopo,
  title={Mopo: Model-based offline policy optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James Y and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14129--14142},
  year={2020}
}

@article{fu2020d4rl,
  title={D4rl: Datasets for deep data-driven reinforcement learning},
  author={Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2004.07219},
  year={2020}
}



@article{an2023designing,
  title={Designing an offline reinforcement learning objective from scratch},
  author={An, Gaon and Lee, Junhyeok and Zuo, Xingdong and Kosaka, Norio and Kim, Kyung-Min and Song, Hyun Oh},
  journal={arXiv preprint arXiv:2301.12842},
  year={2023}
}
@inproceedings{swazinna2021measuring,
  title={Measuring data quality for dataset selection in offline reinforcement learning},
  author={Swazinna, Phillip and Udluft, Steffen and Runkler, Thomas},
  booktitle={2021 IEEE Symposium Series on Computational Intelligence (SSCI)},
  pages={1--8},
  year={2021},
  organization={IEEE}
}
@article{strehl2010learning,
  title={Learning from logged implicit exploration data},
  author={Strehl, Alex and Langford, John and Li, Lihong and Kakade, Sham M},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}
@misc{openaigym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}


@inproceedings{beygelzimer2009offset,
  title={The offset tree for learning with partial labels},
  author={Beygelzimer, Alina and Langford, John},
  booktitle={Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={129--138},
  year={2009}
}
@inproceedings{su2020doubly,
  title={Doubly robust off-policy evaluation with shrinkage},
  author={Su, Yi and Dimakopoulou, Maria and Krishnamurthy, Akshay and Dud{\'\i}k, Miroslav},
  booktitle={International Conference on Machine Learning},
  pages={9167--9176},
  year={2020},
  organization={PMLR}
}

@inproceedings{wang2017optimal,
  title={Optimal and adaptive off-policy evaluation in contextual bandits},
  author={Wang, Yu-Xiang and Agarwal, Alekh and Dud{\i}k, Miroslav},
  booktitle={International Conference on Machine Learning},
  pages={3589--3597},
  year={2017},
  organization={PMLR}
}
@InProceedings{pmlr-v162-ni22a,
  title = 	 {Recurrent Model-Free {RL} Can Be a Strong Baseline for Many {POMDP}s},
  author =       {Ni, Tianwei and Eysenbach, Benjamin and Salakhutdinov, Ruslan},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {16691--16723},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/ni22a/ni22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/ni22a.html},
  abstract = 	 {Many problems in RL, such as meta-RL, robust RL, generalization in RL, and temporal credit assignment, can be cast as POMDPs. In theory, simply augmenting model-free RL with memory-based architectures, such as recurrent neural networks, provides a general approach to solving all types of POMDPs. However, prior work has found that such recurrent model-free RL methods tend to perform worse than more specialized algorithms that are designed for specific types of POMDPs. This paper revisits this claim. We find that careful architecture and hyperparameter decisions can often yield a recurrent model-free implementation that performs on par with (and occasionally substantially better than) more sophisticated recent techniques. We compare to 21 environments from 6 prior specialized methods and find that our implementation achieves greater sample efficiency and asymptotic performance than these methods on 18/21 environments. We also release a simple and efficient implementation of recurrent model-free RL for future work to use as a baseline for POMDPs.}
}

@inproceedings{jiang2016doubly,
  title={Doubly robust off-policy value evaluation for reinforcement learning},
  author={Jiang, Nan and Li, Lihong},
  booktitle={International Conference on Machine Learning},
  pages={652--661},
  year={2016},
  organization={PMLR}
}

@article{dudik2011doubly,
  title={Doubly robust policy evaluation and learning},
  author={Dud{\'\i}k, Miroslav and Langford, John and Li, Lihong},
  journal={arXiv preprint arXiv:1103.4601},
  year={2011}
}

@article{swaminathan2015self,
  title={The self-normalized estimator for counterfactual learning},
  author={Swaminathan, Adith and Joachims, Thorsten},
  journal={advances in neural information processing systems},
  volume={28},
  year={2015}
}


@article{nachum2019dualdice,
  title={Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections},
  author={Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{zhang2020gendice,
  title={Gendice: Generalized offline estimation of stationary values},
  author={Zhang, Ruiyi and Dai, Bo and Li, Lihong and Schuurmans, Dale},
  journal={arXiv preprint arXiv:2002.09072},
  year={2020}
}

@inproceedings{zhang2020gradientdice,
  title={Gradientdice: Rethinking generalized offline estimation of stationary values},
  author={Zhang, Shangtong and Liu, Bo and Whiteson, Shimon},
  booktitle={International Conference on Machine Learning},
  pages={11194--11203},
  year={2020},
  organization={PMLR}
}

@article{saito2020open,
  title={Open bandit dataset and pipeline: Towards realistic and reproducible off-policy evaluation},
  author={Saito, Yuta and Aihara, Shunsuke and Matsutani, Megumi and Narita, Yusuke},
  journal={arXiv preprint arXiv:2008.07146},
  year={2020}
}
@article{fu2021benchmarks,
  title={Benchmarks for deep off-policy evaluation},
  author={Fu, Justin and Norouzi, Mohammad and Nachum, Ofir and Tucker, George and Wang, Ziyu and Novikov, Alexander and Yang, Mengjiao and Zhang, Michael R and Chen, Yutian and Kumar, Aviral and others},
  journal={arXiv preprint arXiv:2103.16596},
  year={2021}
}
@article{voloshin2019empirical,
  title={Empirical study of off-policy policy evaluation for reinforcement learning},
  author={Voloshin, Cameron and Le, Hoang M and Jiang, Nan and Yue, Yisong},
  journal={arXiv preprint arXiv:1911.06854},
  year={2019}
}

@article{seedat2022data,
  title={Data-IQ: Characterizing subgroups with heterogeneous outcomes in tabular data},
  author={Seedat, Nabeel and Crabb{\'e}, Jonathan and Bica, Ioana and van der Schaar, Mihaela},
  journal={arXiv preprint arXiv:2210.13043},
  year={2022}
}



@article{rupprecht2019finding,
  title={Finding and visualizing weaknesses of deep reinforcement learning agents},
  author={Rupprecht, Christian and Ibrahim, Cyril and Pal, Christopher J},
  journal={arXiv preprint arXiv:1904.01318},
  year={2019}
}
@article{bica2020learning,
  title={Learning" what-if" explanations for sequential decision-making},
  author={Bica, Ioana and Jarrett, Daniel and H{\"u}y{\"u}k, Alihan and van der Schaar, Mihaela},
  journal={arXiv preprint arXiv:2007.13531},
  year={2020}
}

@article{anderson2019explaining,
  title={Explaining reinforcement learning to mere mortals: An empirical study},
  author={Anderson, Andrew and Dodge, Jonathan and Sadarangani, Amrita and Juozapaitis, Zoe and Newman, Evan and Irvine, Jed and Chattopadhyay, Souti and Fern, Alan and Burnett, Margaret},
  journal={arXiv preprint arXiv:1903.09708},
  year={2019}
}


@article{guo2021edge,
  title={Edge: Explaining deep reinforcement learning policies},
  author={Guo, Wenbo and Wu, Xian and Khan, Usmann and Xing, Xinyu},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12222--12236},
  year={2021}
}
@inproceedings{beyret2019dot,
  title={Dot-to-dot: Explainable hierarchical reinforcement learning for robotic manipulation},
  author={Beyret, Benjamin and Shafti, Ali and Faisal, A Aldo},
  booktitle={2019 IEEE/RSJ International Conference on intelligent robots and systems (IROS)},
  pages={5014--5019},
  year={2019},
  organization={IEEE}
}

@article{alaa2017personalized,
  title={Personalized risk scoring for critical care prognosis using mixtures of gaussian processes},
  author={Alaa, Ahmed M and Yoon, Jinsung and Hu, Scott and Van der Schaar, Mihaela},
  journal={IEEE Transactions on Biomedical Engineering},
  volume={65},
  number={1},
  pages={207--218},
  year={2017},
  publisher={IEEE}
}
@inproceedings{Gottesman2020InterpretableOE,
  title={Interpretable Off-Policy Evaluation in Reinforcement Learning by Highlighting Influential Transitions},
  author={Omer Gottesman and Joseph D. Futoma and Yao Liu and Soanli Parbhoo and Leo Anthony Celi and Emma Brunskill and Finale Doshi-Velez},
  booktitle={International Conference on Machine Learning},
  year={2020}
}

@inproceedings{fujimoto2018addressing,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International conference on machine learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}


@INPROCEEDINGS{drlsr8614120,
  author={Dao, Giang and Mishra, Indrajeet and Lee, Minwoo},
  booktitle={2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)}, 
  title={Deep Reinforcement Learning Monitor for Snapshot Recording}, 
  year={2018},
  volume={},
  number={},
  pages={591-598},
  doi={10.1109/ICMLA.2018.00095}}

@inproceedings{topin2019generation,
  title={Generation of policy-level explanations for reinforcement learning},
  author={Topin, Nicholay and Veloso, Manuela},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={2514--2521},
  year={2019}
}
@inproceedings{sreedharan2020tldr,
  title={Tldr: Policy summarization for factored ssp problems using temporal abstractions},
  author={Sreedharan, Sarath and Srivastava, Siddharth and Kambhampati, Subbarao},
  booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
  volume={30},
  pages={272--280},
  year={2020}
}
@article{macglashan2022value,
  title={Value Function Decomposition for Iterative Design of Reinforcement Learning Agents},
  author={MacGlashan, James and Archer, Evan and Devlic, Alisa and Seno, Takuma and Sherstan, Craig and Wurman, Peter R and Stone, Peter},
  journal={arXiv preprint arXiv:2206.13901},
  year={2022}
}

@inproceedings{madumal2020explainable,
  title={Explainable reinforcement learning through a causal lens},
  author={Madumal, Prashan and Miller, Tim and Sonenberg, Liz and Vetere, Frank},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={03},
  pages={2493--2500},
  year={2020}
}

%%%

@inproceedings{holt2023active,
  author={S Holt and A H\"uy\"uk and M van der Schaar},
  title={Active observing continuous-time control},
  booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
  year={2023}
}

@inproceedings{holt2023neural,
  author={S Holt and A H\"uy\"uk and Z Qian and H Sun and M van der Schaar},
  title={Neural {Laplace} control for continuous-time delayed systems},
  booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2023}
}

@inproceedings{huyuk2022inverse,
  author={A H\"uy\"uk and D Jarrett and M van der Schaar},
  title={Inverse contextual bandits: Learning how behavior evolves over time},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2022}
}

@inproceedings{huyuk2022inferring,
  author={A H\"uy\"uk and W R Zame and M van der Schaar},
  title={Inferring lexicographically-ordered rewards from preferences},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2022}
}

@inproceedings{qin2021closing,
  author={Y Qin and F Imrie and A H\"uy\"uk and D Jarrett and A E Gimson and M van der Schaar},
  title={Closing the loop in medical decision support by understanding clinical decision-making: A case study on organ transplantation},
  booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
  year={2021}
}

@inproceedings{jarrett2021inverse,
  author={D Jarrett and A H\"uy\"uk and M van der Schaar},
  title={Inverse decision modeling: Learning interpretable representations of behavior},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2021}
}

@inproceedings{huyuk2021explaining,
  author={A H\"uy\"uk and D Jarrett and C Tekin and M van der Schaar},
  title={Explaining by imitating: Understanding decisions by interpretable policy learning},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021}
}