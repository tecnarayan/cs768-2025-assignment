\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{SSvdHT13}

\bibitem[BDLS17]{balakrishnan2017computationally}
Sivaraman Balakrishnan, Simon~S Du, Jerry Li, and Aarti Singh.
\newblock Computationally efficient robust sparse estimation in high
  dimensions.
\newblock In {\em Conference on Learning Theory}, pages 169--212, 2017.

\bibitem[BJK15]{bhatia2015robust}
Kush Bhatia, Prateek Jain, and Purushottam Kar.
\newblock Robust regression via hard thresholding.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  721--729, 2015.

\bibitem[BT{\etalchar{+}}12]{boucheron2012concentration}
St{\'e}phane Boucheron, Maud Thomas, et~al.
\newblock Concentration inequalities for order statistics.
\newblock {\em Electronic Communications in Probability}, 17, 2012.

\bibitem[BWY{\etalchar{+}}17]{balakrishnan2017statistical}
Sivaraman Balakrishnan, Martin~J Wainwright, Bin Yu, et~al.
\newblock Statistical guarantees for the em algorithm: From population to
  sample-based analysis.
\newblock {\em The Annals of Statistics}, 45(1):77--120, 2017.

\bibitem[CCB{\etalchar{+}}18]{chen2018detecting}
Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin
  Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava.
\newblock Detecting backdoor attacks on deep neural networks by activation
  clustering.
\newblock {\em arXiv preprint arXiv:1811.03728}, 2018.

\bibitem[CCM13]{chen2013robust}
Yudong Chen, Constantine Caramanis, and Shie Mannor.
\newblock Robust sparse regression under adversarial corruption.
\newblock In {\em International Conference on Machine Learning}, pages
  774--782, 2013.

\bibitem[{\v{C}}{\'\i}{\v{z}}08]{vcivzek2008general}
Pavel {\v{C}}{\'\i}{\v{z}}ek.
\newblock General trimmed estimation: robust approach to nonlinear and limited
  dependent variable models.
\newblock {\em Econometric Theory}, 24(6):1500--1529, 2008.

\bibitem[DKK{\etalchar{+}}18]{diakonikolas2018sever}
Ilias Diakonikolas, Gautam Kamath, Daniel~M Kane, Jerry Li, Jacob Steinhardt,
  and Alistair Stewart.
\newblock Sever: A robust meta-algorithm for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1803.02815}, 2018.

\bibitem[DLBB09]{davenport2009simple}
Mark~A Davenport, Jason~N Laska, Petros~T Boufounos, and Richard~G Baraniuk.
\newblock A simple proof that random matrices are democratic.
\newblock {\em arXiv preprint arXiv:0911.0736}, 2009.

\bibitem[FK{\etalchar{+}}14]{frenay2014comprehensive}
Beno{\^\i}t Fr{\'e}nay, Ata Kab{\'a}n, et~al.
\newblock A comprehensive introduction to label noise.
\newblock In {\em ESANN}, 2014.

\bibitem[GDGG17]{gu2017badnets}
Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock Badnets: Identifying vulnerabilities in the machine learning model
  supply chain.
\newblock {\em arXiv preprint arXiv:1708.06733}, 2017.

\bibitem[H{\"o}s95]{hossjer1995exact}
Ola H{\"o}ssjer.
\newblock Exact computation of the least trimmed squares estimate in simple
  linear regression.
\newblock {\em Computational Statistics \& Data Analysis}, 19(3):265--282,
  1995.

\bibitem[Hub11]{huber2011robust}
Peter~J Huber.
\newblock Robust statistics.
\newblock In {\em International Encyclopedia of Statistical Science}, pages
  1248--1251. Springer, 2011.

\bibitem[JZL{\etalchar{+}}17]{jiang2017mentornet}
Lu~Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li~Fei-Fei.
\newblock Mentornet: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock {\em arXiv preprint arXiv:1712.05055}, 2017.

\bibitem[KH09]{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[KKM18]{klivans2018efficient}
Adam~R. Klivans, Pravesh~K. Kothari, and Raghu Meka.
\newblock Efficient algorithms for outlier-robust regression.
\newblock In {\em {Conference on Learning Theory}}, pages 1420--1430, 2018.

\bibitem[KLA18]{khetan2017learning}
Ashish Khetan, Zachary~C. Lipton, and Anima Anandkumar.
\newblock Learning from noisy singly-labeled data.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem[LBBH98]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem[LDGG18]{liu2018fine}
Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock Fine-pruning: Defending against backdooring attacks on deep neural
  networks.
\newblock {\em arXiv preprint arXiv:1805.12185}, 2018.

\bibitem[LL18]{li2018learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning mixtures of linear regressions with nearly optimal
  complexity.
\newblock In {\em {Conference on Learning Theory}}, pages 1125--1144, 2018.

\bibitem[LSLC18]{liu2018high}
Liu Liu, Yanyao Shen, Tianyang Li, and Constantine Caramanis.
\newblock High dimensional robust sparse regression.
\newblock {\em arXiv preprint arXiv:1805.11643}, 2018.

\bibitem[MNP{\etalchar{+}}14]{mount2014least}
David~M Mount, Nathan~S Netanyahu, Christine~D Piatko, Ruth Silverman, and
  Angela~Y Wu.
\newblock On the least trimmed squares estimator.
\newblock {\em Algorithmica}, 69(1):148--183, 2014.

\bibitem[MSS17]{malach2017decoupling}
Eran Malach and Shai Shalev-Shwartz.
\newblock Decoupling" when to update" from" how to update".
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  961--971, 2017.

\bibitem[MVRN16]{menon2016learning}
Aditya~Krishna Menon, Brendan Van~Rooyen, and Nagarajan Natarajan.
\newblock Learning from binary labels with instance-dependent corruption.
\newblock {\em arXiv preprint arXiv:1605.00751}, 2016.

\bibitem[NDRT13]{natarajan2013learning}
Nagarajan Natarajan, Inderjit~S Dhillon, Pradeep~K Ravikumar, and Ambuj Tewari.
\newblock Learning with noisy labels.
\newblock In {\em Advances in neural information processing systems}, pages
  1196--1204, 2013.

\bibitem[PSBR18]{prasad2018robust}
Adarsh Prasad, Arun~Sai Suggala, Sivaraman Balakrishnan, and Pradeep Ravikumar.
\newblock Robust estimation via robust gradient estimation.
\newblock {\em arXiv preprint arXiv:1802.06485}, 2018.

\bibitem[RLA{\etalchar{+}}14]{reed2014training}
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan,
  and Andrew Rabinovich.
\newblock Training deep neural networks on noisy labels with bootstrapping.
\newblock {\em arXiv preprint arXiv:1412.6596}, 2014.

\bibitem[RMC15]{radford2015unsupervised}
Alec Radford, Luke Metz, and Soumith Chintala.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock {\em arXiv preprint arXiv:1511.06434}, 2015.

\bibitem[RNSS18]{ray2018search}
Avik Ray, Joe Neeman, Sujay Sanghavi, and Sanjay Shakkottai.
\newblock The search problem in mixture models.
\newblock {\em Journal of Machine Learning Research}, 18(206):1--61, 2018.

\bibitem[Rou84]{rousseeuw1984least}
Peter~J Rousseeuw.
\newblock Least median of squares regression.
\newblock {\em Journal of the American statistical association},
  79(388):871--880, 1984.

\bibitem[RVD06]{rousseeuw2006computing}
Peter~J Rousseeuw and Katrien Van~Driessen.
\newblock Computing lts regression for large data sets.
\newblock {\em Data mining and knowledge discovery}, 12(1):29--45, 2006.

\bibitem[RZYU18]{ren2018learning}
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun.
\newblock Learning to reweight examples for robust deep learning.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, pages 4331--4340, 2018.

\bibitem[SBH13]{scott2013classification}
Clayton Scott, Gilles Blanchard, and Gregory Handy.
\newblock Classification with asymmetric label noise: Consistency and maximal
  denoising.
\newblock In {\em Conference On Learning Theory}, pages 489--511, 2013.

\bibitem[SF14]{sukhbaatar2014learning}
Sainbayar Sukhbaatar and Rob Fergus.
\newblock Learning from noisy labels with deep neural networks.
\newblock {\em arXiv preprint arXiv:1406.2080}, 2(3):4, 2014.

\bibitem[SJA16]{sedghi2016provable}
Hanie Sedghi, Majid Janzamin, and Anima Anandkumar.
\newblock Provable tensor methods for learning mixtures of generalized linear
  models.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1223--1231,
  2016.

\bibitem[SPR18]{suggala2018connecting}
Arun Suggala, Adarsh Prasad, and Pradeep~K Ravikumar.
\newblock Connecting optimization and regularization paths.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10631--10641, 2018.

\bibitem[SSvdHT13]{shen2013approximate}
Fumin Shen, Chunhua Shen, Anton van~den Hengel, and Zhenmin Tang.
\newblock Approximate least trimmed sum of squares fitting and applications in
  image analysis.
\newblock {\em IEEE Transactions on Image Processing}, 22(5):1836--1847, 2013.

\bibitem[TLM18]{tran2018spectral}
Brandon Tran, Jerry Li, and Aleksander Madry.
\newblock Spectral signatures in backdoor attacks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8011--8021, 2018.

\bibitem[Ver10]{vershynin2010introduction}
Roman Vershynin.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock {\em arXiv preprint arXiv:1011.3027}, 2010.

\bibitem[Ver16]{vershynin2016high}
Roman Vershynin.
\newblock High dimensional probability.
\newblock {\em An Introduction with Applications}, 2016.

\bibitem[V{\'\i}{\v{s}}06]{vivsek2006least}
Jan~{\'A}mos V{\'\i}{\v{s}}ek.
\newblock The least trimmed squares. part i: Consistency.
\newblock {\em Kybernetika}, 42(1):1--36, 2006.

\bibitem[VMX17]{vainsencher2017ignoring}
Daniel Vainsencher, Shie Mannor, and Huan Xu.
\newblock Ignoring is a bliss: Learning with large noise through
  reweighting-minimization.
\newblock In {\em Conference on Learning Theory}, pages 1849--1881, 2017.

\bibitem[WYS{\etalchar{+}}]{wangneural}
Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao
  Zheng, and Ben~Y Zhao.
\newblock Neural cleanse: Identifying and mitigating backdoor attacks in neural
  networks.
\newblock In {\em Neural Cleanse: Identifying and Mitigating Backdoor Attacks
  in Neural Networks}, page~0. IEEE.

\bibitem[YCS14]{yi2014alternating}
Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi.
\newblock Alternating minimization for mixed linear regression.
\newblock In {\em International Conference on Machine Learning}, pages
  613--621, 2014.

\bibitem[YCS16]{yi2016solving}
Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi.
\newblock Solving a mixture of many random linear equations by tensor
  decomposition and alternating minimization.
\newblock {\em arXiv preprint arXiv:1608.05749}, 2016.

\bibitem[YLA{\etalchar{+}}18]{yang2018general}
Eunho Yang, Aur{\'e}lie~C Lozano, Aleksandr Aravkin, et~al.
\newblock A general family of trimmed estimators for robust high-dimensional
  data analysis.
\newblock {\em Electronic Journal of Statistics}, 12(2):3519--3553, 2018.

\bibitem[ZJD16]{zhong2016mixed}
Kai Zhong, Prateek Jain, and Inderjit~S Dhillon.
\newblock Mixed linear regression with multiple components.
\newblock In {\em Advances in neural information processing systems}, pages
  2190--2198, 2016.

\bibitem[ZK16]{Zagoruyko2016WRN}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock In {\em BMVC}, 2016.

\bibitem[ZS18]{zhang2018generalized}
Zhilu Zhang and Mert~R Sabuncu.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock {\em arXiv preprint arXiv:1805.07836}, 2018.

\end{thebibliography}
