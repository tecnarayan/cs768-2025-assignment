@inproceedings{distral,
  title={Distral: Robust multitask reinforcement learning},
  author={Teh, Yee and Bapst, Victor and Czarnecki, Wojciech M and Quan, John and Kirkpatrick, James and Hadsell, Raia and Heess, Nicolas and Pascanu, Razvan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4496--4506},
  year={2017}
}

@article{MISA,
  title={Invariant causal prediction for block {MDPs}},
  author={Zhang, Amy and Lyle, Clare and Sodhani, Shagun and Filos, Angelos and Kwiatkowska, Marta and Pineau, Joelle and Gal, Yarin and Precup, Doina},
  journal={arXiv preprint arXiv:2003.06016},
  year={2020}
}

@article{ikea_furniture,
  title={IKEA Furniture Assembly Environment for Long-Horizon Complex Manipulation Tasks},
  author={via Imitation, Solving Long-Horizon Tasks and as Sequential, Modeling Long-horizon Tasks}
}

@Article{JMLR09-taylor,
	Author="Matthew E.\ Taylor and Peter Stone",
	title="Transfer Learning for Reinforcement Learning Domains: A Survey",
        journal="Journal of Machine Learning Research",
	volume="10",number="1",
        pages="1633--1685",
	year="2009",
	abstract="The reinforcement learning paradigm is a popular way
        to address problems that have only limited environmental
        feedback, rather than correctly labeled examples, as is common
        in other machine learning contexts. While significant progress
        rn made to improve learning in a single task, the idea
        of transfer learning has only recently been applied to
        reinforcement learning tasks. The core idea of transfer is
        that experience gained in learning to perform one task can
        help improve learning performance in a related, but different,
        task. In this article we present a framework that classifies
        transfer learning methods in terms of their capabilities and
        goals, and then use it to survey the existing literature, as
        well as to suggest future directions for transfer learning
        work.",
	wwwnote={<a href="http://www.jmlr.org/papers/volume10/taylor09a/taylor09a.pdf">Official	version</a> from journal website.},
}

@article{borsa2016mtrl,
  author    = {Diana Borsa and
               Thore Graepel and
               John Shawe{-}Taylor},
  title     = {Learning Shared Representations in Multi-task Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1603.02041},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.02041},
  archivePrefix = {arXiv},
  eprint    = {1603.02041},
  timestamp = {Mon, 13 Aug 2018 16:48:37 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BorsaGS16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sharing_knowledge_in_multitask_deep_reinforcement_learning,
title={Sharing Knowledge in Multi-Task Deep Reinforcement Learning},
author={Carlo D'Eramo and Davide Tateo and Andrea Bonarini and Marcello Restelli and Jan Peters},
booktitle={International Conference on Learning Representations},
year={2020}
}

@Book{Bellman1957,
  author =       "Bellman, Richard",
  title =        "Dynamic Programming",
  publisher =    "Princeton University Press",
  year =         "1957",
  address =   "Princeton, NJ, USA",
  edition =   "1",
  bib2html_rescat = "General RL",
}

@inproceedings{harb2018waiting,
    author={Jean Harb and Pierre-Luc Bacon and Martin Klissarov and Doina Precup},
    title={When Waiting is not an Option : Learning Options with a Deliberation Cost},
    year={2018},
    publisher = {AAAI Press},
    series = {AAAI’18}
}

@inproceedings{meta-world,
  title={Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning},
  author={Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
  booktitle={Conference on Robot Learning},
  pages={1094--1100},
  year={2020}
}

@inproceedings{langford2007mab,
 author = {Langford, John and Zhang, Tong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {817--824},
 publisher = {Curran Associates, Inc.},
 title = {The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information},
 url = {https://proceedings.neurips.cc/paper/2007/file/4b04a686b0ad13dce35fa99fa4161c65-Paper.pdf},
 volume = {20},
 year = {2008}
}

@article{dmc,
  author    = {Yuval Tassa and
               Yotam Doron and
               Alistair Muldal and
               Tom Erez and
               Yazhe Li and
               Diego de Las Casas and
               David Budden and
               Abbas Abdolmaleki and
               Josh Merel and
               Andrew Lefrancq and
               Timothy P. Lillicrap and
               Martin A. Riedmiller},
  title     = {DeepMind Control Suite},
  journal   = {CoRR},
  volume    = {abs/1801.00690},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.00690},
  archivePrefix = {arXiv},
  eprint    = {1801.00690},
  timestamp = {Mon, 22 Jul 2019 16:19:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1801-00690.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lai1985mab,
author = {Lai, T.L and Robbins, Herbert},
title = {Asymptotically Efficient Adaptive Allocation Rules},
year = {1985},
issue_date = {March, 1985},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {6},
number = {1},
issn = {0196-8858},
url = {https://doi.org/10.1016/0196-8858(85)90002-8},
doi = {10.1016/0196-8858(85)90002-8},
journal = {Adv. Appl. Math.},
month = mar,
pages = {4–22},
numpages = {19}
}

@misc{ OpenAI_dota,
      author = {OpenAI},
      title = {OpenAI Five},
      howpublished = {\url{https://blog.openai.com/openai-five/}},
      year = {2018}
}
 
 @inproceedings{zheng2019metadata,
  title     = {Metadata-driven Task Relation Discovery for Multi-task Learning},
  author    = {Zheng, Zimu and Wang, Yuqi and Dai, Quanyu and Zheng, Huadi and Wang, Dan},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {4426--4432},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/615},
  url       = {https://doi.org/10.24963/ijcai.2019/615},
}
     
@ARTICLE{silver2017alphaGo,
title = "Mastering the game of Go without human knowledge",
author = "Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis",
journal = "Nature",
publisher = "nature.com",
volume =  550,
number =  7676,
pages = "354-359",
month =  oct,
year =  2017,
language = "en",
issn = "0028-0836, 1476-4687",
pmid = "29052630",
doi = "10.1038/nature24270"
}

@inproceedings{multi_task_reinforcement_learning_with_soft_modularization,
  title={Multi-Task Reinforcement Learning with Soft Modularization},
  author={Yang, Ruihan and Xu, Huazhe and Wu, Yi and Wang, Xiaolong},
  booktitle = {Advances in Neural Information Processing Systems},
  year={2020}
}


@article{proximal-policy-optimization-algorithms,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{trust-region-policy-optimization,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015}
}


@inproceedings{soft-actor-critic,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}


@inproceedings{learning-an-embedding-space-for-transferable-skills,
  title={Learning an embedding space for transferable robot skills},
  author={Hausman, Karol and Springenberg, Jost Tobias and Wang, Ziyu and Heess, Nicolas and Riedmiller, Martin},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@article{perezGeneralizedHiddenParameter2020,
	title = {Generalized {Hidden} {Parameter} {MDPs} {Transferable} {Model}-based {RL} in a {Handful} of {Trials}},
	journal = {arXiv:2002.03072 [cs, stat]},
	author = {Perez, Christian F. and Such, Felipe Petroski and Karaletsos, Theofanis},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.03072},
	file = {arXiv Fulltext PDF:/Users/amyzhang/Zotero/storage/4Q6FQN8N/Perez et al. - 2020 - Generalized Hidden Parameter MDPs Transferable Mod.pdf:application/pdf;arXiv.org Snapshot:/Users/amyzhang/Zotero/storage/BLZSHDYC/2002.html:text/html}
}


@inproceedings{killian2017hipmdp,
author = {Killian, Taylor and Daulton, Samuel and Konidaris, George and Doshi-Velez, Finale},
title = {Robust and Efficient Transfer Learning with Hidden Parameter Markov Decision Processes},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6251–6262},
numpages = {12},
location = {Long Beach, California, USA},
series = {NIPS’17}
}

@inproceedings{amit2018meta,
  title={Meta-Learning by Adjusting Priors Based on Extended {PAC-Bayes} Theory},
  author={Amit, Ron and Meir, Ron},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={205--214},
  year={2018}
}

@inproceedings{Yin2020Meta-Learning,
title={Meta-Learning without Memorization},
author={Mingzhang Yin and George Tucker and Mingyuan Zhou and Sergey Levine and Chelsea Finn},
booktitle={International Conference on Learning Representations},
year={2020},
}

@article{doshi-velezHiddenParameterMarkov2013,
	title = {Hidden {Parameter} {Markov} {Decision} {Processes}: {A} {Semiparametric} {Regression} {Approach} for {Discovering} {Latent} {Task} {Parametrizations}},
	shorttitle = {Hidden {Parameter} {Markov} {Decision} {Processes}},
	urldate = {2020-02-14},
	journal = {arXiv:1308.3513 [cs]},
	author = {Doshi-Velez, Finale and Konidaris, George},
	month = aug,
	year = {2013},
	note = {arXiv: 1308.3513},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{kaelbling1998pomdp,
author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
title = {Planning and Acting in Partially Observable Stochastic Domains},
year = {1998},
issue_date = {May, 1998},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {101},
number = {1–2},
issn = {0004-3702},
journal = {Artif. Intell.},
month = may,
pages = {99–134},
numpages = {36},
keywords = {Partially observable Markov decision processes, Uncertainty, Planning}
}
  
@Article{JMLR09-taylor,
	Author="Matthew E.\ Taylor and Peter Stone",
	title="Transfer Learning for Reinforcement Learning Domains: A Survey",
        journal="Journal of Machine Learning Research",
	volume="10",number="1",
        pages="1633--1685",
	year="2009",
	abstract="The reinforcement learning paradigm is a popular way
        to address problems that have only limited environmental
        feedback, rather than correctly labeled examples, as is common
        in other machine learning contexts. While significant progress
        has been made to improve learning in a single task, the idea
        of transfer learning has only recently been applied to
        reinforcement learning tasks. The core idea of transfer is
        that experience gained in learning to perform one task can
        help improve learning performance in a related, but different,
        task. In this article we present a framework that classifies
        transfer learning methods in terms of their capabilities and
        goals, and then use it to survey the existing literature, as
        well as to suggest future directions for transfer learning
        work.",
	wwwnote={<a href="http://www.jmlr.org/papers/volume10/taylor09a/taylor09a.pdf">Official	version</a> from journal website.},
}

@book{bertsekas1996ndprog,
author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
title = {Neuro-Dynamic Programming},
year = {1996},
isbn = {1886529108},
publisher = {Athena Scientific},
edition = {1st}
}

@inproceedings{munos2005errorbounds,
author = {Munos, R\'{e}mi},
title = {Error Bounds for Approximate Value Iteration},
year = {2005},
isbn = {157735236x},
publisher = {AAAI Press},
booktitle = {Proceedings of the 20th National Conference on Artificial Intelligence - Volume 2},
pages = {1006–1011},
numpages = {6},
location = {Pittsburgh, Pennsylvania},
series = {AAAI’05}
}

@misc{zhang2018decoupling,
    title={Decoupling Dynamics and Reward for Transfer Learning},
    author={Amy Zhang and Harsh Satija and Joelle Pineau},
    year={2018},
    eprint={1804.10689},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{du2019pcid,
  author    = {Simon S. Du and
               Akshay Krishnamurthy and
               Nan Jiang and
               Alekh Agarwal and
               Miroslav Dud{\'{\i}}k and
               John Langford},
  title     = {Provably efficient {RL} with Rich Observations via Latent State Decoding},
  journal   = {CoRR},
  volume    = {abs/1901.09018},
  year      = {2019},
  archivePrefix = {arXiv},
  eprint    = {1901.09018},
  timestamp = {Sat, 02 Feb 2019 16:56:00 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1901-09018},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{peters2016icp,
author = {J. Peters and P. B{\"u}hlmann and N. Meinshausen},
year = {2016},
title = {Causal inference using invariant prediction: identification and confidence intervals},
volume = {78},
number = {5},
pages = {947--1012},
journal = {Journal of the Royal Statistical Society, Series B (with discussion)}
}

@book{pearl2009do,
 author = {Pearl, Judea},
 title = {Causality: Models, Reasoning and Inference},
 year = {2009},
 isbn = {052189560X, 9780521895606},
 edition = {2nd},
 publisher = {Cambridge University Press},
 address = {New York, NY, USA},
} 

@article{eberhardt2007interventions,
author = {Eberhardt, Frederick and Scheines, Richard},
title = {Interventions and Causal Inference},
journal = {Philosophy of Science},
volume = {74},
number = {5},
pages = {981-995},
year = {2007},
doi = {10.1086/525638},
eprint = { 
        https://doi.org/10.1086/525638
}
}

@article{2019learning_domain_randomization_distributions_for_transfer_of_locomotion_policies,
  title={Learning Domain Randomization Distributions for Transfer of Locomotion Policies},
  author={Mozifian, Melissa and Higuera, Juan Camilo Gamboa and Meger, David and Dudek, Gregory},
  journal={arXiv preprint arXiv:1906.00410},
  year={2019}
}

@article{brunskill2013mtrl,
author = {Brunskill, Emma and Li, Lihong},
year = {2013},
month = {09},
pages = {},
title = {Sample Complexity of Multi-task Reinforcement Learning},
journal = {Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013}
}

@inproceedings{lattimore2012pac,
  title={PAC bounds for discounted MDPs},
  author={Lattimore, Tor and Hutter, Marcus},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={320--334},
  year={2012},
  organization={Springer}
}

@article{yin2019meta,
  title={Meta-Learning without Memorization},
  author={Yin, Mingzhang and Tucker, George and Zhou, Mingyuan and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:1912.03820},
  year={2019}
}


@article{liuPACContinuousState,
	title = {{PAC} {Continuous} {State} {Online} {Multitask} {Reinforcement} {Learning} with {Identiﬁcation}},
	language = {en},
	author = {Liu, Yao and Guo, Zhanhan and Brunskill, Emma},
	pages = {9},
	file = {Liu et al. - PAC Continuous State Online Multitask Reinforcemen.pdf:/Users/amyzhang/Zotero/storage/9IJQK54E/Liu et al. - PAC Continuous State Online Multitask Reinforcemen.pdf:application/pdf}
}

@article{farahmand2011regularization,
  title={Regularization in reinforcement learning},
  author={Farahmand, Amir-massoud},
  year={2011}
}

@article{barreto2019transfer,
  title={Transfer in deep reinforcement learning using successor features and generalised policy improvement},
  author={Barreto, Andr{\'e} and Borsa, Diana and Quan, John and Schaul, Tom and Silver, David and Hessel, Matteo and Mankowitz, Daniel and {\v{Z}}{\'\i}dek, Augustin and Munos, Remi},
  journal={arXiv preprint arXiv:1901.10964},
  year={2019}
}

@article{maurerBenefitMultitaskRepresentation,
author = {Maurer, Andreas and Pontil, Massimiliano and Romera-Paredes, Bernardino},
title = {The Benefit of Multitask Representation Learning},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2853–2884},
numpages = {32},
keywords = {learning-to-learn, statistical learning theory, multitask learning, representation learning, transfer learning}
}

@inproceedings{ammar2014online,
  title={Online multi-task learning for policy gradient methods},
  author={Ammar, Haitham Bou and Eaton, Eric and Ruvolo, Paul and Taylor, Matthew},
  booktitle={International conference on machine learning},
  pages={1206--1214},
  year={2014}
}

@inproceedings{
luo2018algorithmic,
title={Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees},
author={Yuping Luo and Huazhe Xu and Yuanzhi Li and Yuandong Tian and Trevor Darrell and Tengyu Ma},
booktitle={International Conference on Learning Representations},
year={2019},
}

@article{ferns2011contbisim,
author = {Ferns, Norm and Panangaden, Prakash and Precup, Doina},
title = {Bisimulation Metrics for Continuous Markov Decision Processes},
year = {2011},
issue_date = {November 2011},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
volume = {40},
number = {6},
issn = {0097-5397},
doi = {10.1137/10080484X},
journal = {SIAM J. Comput.},
month = dec,
pages = {1662–1714},
numpages = {53},
keywords = {continuous, reinforcement learning, Markov decision process, bisimulation, metrics}
}

  
@inproceedings{ferns2004bisimulation,
 author = {Ferns, Norm and Panangaden, Prakash and Precup, Doina},
 title = {Metrics for Finite Markov Decision Processes},
 booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
 series = {UAI '04},
 year = {2004},
 isbn = {0-9749039-0-6},
 location = {Banff, Canada},
 pages = {162--169},
 numpages = {8},
 acmid = {1036863},
 publisher = {AUAI Press},
 address = {Arlington, Virginia, United States},
} 

@inproceedings{zhang2020invariant,
    title={Invariant Causal Prediction for Block {MDP}s},
    author={Amy Zhang and Clare Lyle and Shagun Sodhani and Angelos Filos and Marta Kwiatkowska and Joelle Pineau and Yarin Gal and Doina Precup},
    year={2020},
    booktitle={International Conference on Machine Learning (ICML)},
    eprint={2003.06016},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{zhang2020hipbmdp,
title={Learning Robust State Abstractions for Hidden-Parameter Block {MDP}s},
author={Amy Zhang and Shagun Sodhani and Khimya Khetarpal and Joelle Pineau},
booktitle={International Conference on Learning Representations},
year={2021}
}

@inproceedings{zhang2020dbc,
    title={Learning Invariant Representations for Reinforcement Learning without Reconstruction},
    author={Amy Zhang and Rowan McAllister and Roberto Calandra and Yarin Gal and Sergey Levine},
    booktitle={International Conference on Learning Representations},
	year={2021}
}

@inproceedings{bahdanau2018learning,
title={Learning to Understand Goal Specifications by Modelling Reward},
author={Dzmitry Bahdanau and Felix Hill and Jan Leike and Edward Hughes and Pushmeet Kohli and Edward Grefenstette},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=H1xsSjC9Ym},
}

@article{mnih2015human,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2015-08-26T14:46:40.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}

@misc{mnih2013atari,
  abstract = {},
  added-at = {2014-12-14T17:55:47.000+0100},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/2f4760edc252cd402821a341bda0026bf/vch},
  description = {Playing Atari with Deep Reinforcement Learning},
  interhash = {78966703f649bae69a08a6a23a4e8879},
  intrahash = {f4760edc252cd402821a341bda0026bf},
  keywords = {arxiv cs},
  timestamp = {2014-12-14T17:55:47.000+0100},
  title = {Playing Atari with Deep Reinforcement Learning},
  year = 2013
}

@incollection{taylor2008TransferringInstancesModelBased,
	address = {Berlin, Heidelberg},
	title = {Transferring {Instances} for {Model}-{Based} {Reinforcement} {Learning}},
	volume = {5212},
	isbn = {978-3-540-87480-5 978-3-540-87481-2},
	url = {http://link.springer.com/10.1007/978-3-540-87481-2_32},
	language = {en},
	urldate = {2020-04-20},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer Berlin Heidelberg},
	author = {Taylor, Matthew E. and Jong, Nicholas K. and Stone, Peter},
	editor = {Daelemans, Walter and Goethals, Bart and Morik, Katharina},
	year = {2008},
	doi = {10.1007/978-3-540-87481-2_32},
	note = {ISSN: 0302-9743, 1611-3349
Series Title: Lecture Notes in Computer Science},
	pages = {488--505},
	file = {Taylor et al. - 2008 - Transferring Instances for Model-Based Reinforceme.pdf:/Users/amyzhang/Zotero/storage/USDMSKYV/Taylor et al. - 2008 - Transferring Instances for Model-Based Reinforceme.pdf:application/pdf}
}

@INPROCEEDINGS{li2006stateabs,
    author = {Lihong Li and Thomas J. Walsh and Michael L. Littman},
    title = {Towards a Unified Theory of State Abstraction for {MDPs}},
    booktitle = {Proceedings of the Ninth International Symposium on Artificial Intelligence and Mathematics},
    year = {2006},
    pages = {531--539}
}

@article{jiangNotesStateAbstractionsa,
	title = {Notes on {State} {Abstractions}},
	language = {en},
	author = {Jiang, Nan},
	pages = {12},
	file = {Jiang - Notes on State Abstractions.pdf:/Users/amyzhang/Zotero/storage/UXQK2VC8/Jiang - Notes on State Abstractions.pdf:application/pdf},
	year = {2018},
	url = {https://nanjiang.cs.illinois.edu/files/cs598/note4.pdf}
}

@inproceedings{castro2010bisimtransfer,
  author    = {Pablo Samuel Castro and
               Doina Precup},
  editor    = {Maria Fox and
               David Poole},
  title     = {Using Bisimulation for Policy Transfer in {MDPs}},
  booktitle = {Proceedings of the Twenty-Fourth {AAAI} Conference on Artificial Intelligence,
               {AAAI} 2010, Atlanta, Georgia, USA, July 11-15, 2010},
  publisher = {{AAAI} Press},
  year      = {2010},
  url       = {http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1907},
  timestamp = {Mon, 06 Nov 2017 17:26:30 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/CastroP10.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{calandriello2014multitask,
	title = {Sparse multi-task reinforcement learning},
	booktitle = {Advances in neural information processing systems 27},
	publisher = {Curran Associates, Inc.},
	author = {Calandriello, Daniele and Lazaric, Alessandro and Restelli, Marcello},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {819--827}
}

@article{mueller_1997, 
title={Integral Probability Metrics and Their Generating Classes of Functions}, volume={29}, DOI={10.2307/1428011}, number={2}, journal={Advances in Applied Probability}, publisher={Cambridge University Press}, author={Müller, Alfred}, year={1997}, pages={429–443}}

@inproceedings{parisotto16_actormimic,
  author    = {Emilio Parisotto and Jimmy Ba and Ruslan Salakhutdinov},
  title     = {Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning},
  booktitle = {ICLR},
  year      = {2016}
}

@misc{landolfi2019mbmtrl,
    title={A Model-based Approach for Sample-efficient Multi-task Reinforcement Learning},
    author={Nicholas C. Landolfi and Garrett Thomas and Tengyu Ma},
    year={2019},
    eprint={1907.04964},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@incollection{lazaric2011TransferMultipleMDPs,
	title = {Transfer from {Multiple} {MDPs}},
	url = {http://papers.nips.cc/paper/4435-transfer-from-multiple-mdps.pdf},
	urldate = {2020-04-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 24},
	publisher = {Curran Associates, Inc.},
	author = {Lazaric, Alessandro and Restelli, Marcello},
	editor = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
	year = {2011},
	pages = {1746--1754},
	file = {NIPS Full Text PDF:/Users/amyzhang/Zotero/storage/87AD5AV7/Lazaric and Restelli - 2011 - Transfer from Multiple MDPs.pdf:application/pdf;NIPS Snapshot:/Users/amyzhang/Zotero/storage/4NGMWLHW/4435-transfer-from-multiple-mdps.html:text/html}
}
}

@inproceedings{jiang2015abstraction,
  title={Abstraction selection in model-based reinforcement learning},
  author={Jiang, Nan and Kulesza, Alex and Singh, Satinder},
  booktitle={International Conference on Machine Learning},
  pages={179--188},
  year={2015}
}

@inproceedings{bai2013planning,
  title={Planning how to learn},
  author={Bai, Haoyu and Hsu, David and Lee, Wee Sun},
  booktitle={2013 IEEE International Conference on Robotics and Automation},
  pages={2853--2859},
  year={2013},
  organization={IEEE}
}
@article{givan2003equivalence,
  title={Equivalence notions and model minimization in Markov decision processes},
  author={Givan, Robert and Dean, Thomas and Greig, Matthew},
  journal={Artificial Intelligence},
  volume={147},
  number={1-2},
  pages={163--223},
  year={2003},
  publisher={Elsevier}
}

@inproceedings{castro2010using,
  title={Using bisimulation for policy transfer in {MDPs}},
  author={Castro, Pablo Samuel and Precup, Doina},
  booktitle={Twenty-Fourth AAAI Conference on Artificial Intelligence},
  year={2010}
}

@article{gelada2019deepmdp,
  title={Deepmdp: Learning continuous latent space models for representation learning},
  author={Gelada, Carles and Kumar, Saurabh and Buckman, Jacob and Nachum, Ofir and Bellemare, Marc G},
  journal={arXiv preprint arXiv:1906.02736},
  year={2019}
}

@article{lee2019stochastic,
  title={Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model},
  author={Lee, Alex X and Nagabandi, Anusha and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1907.00953},
  year={2019}
}

@inproceedings{abel2018state,
  title={State abstractions for lifelong reinforcement learning},
  author={Abel, David and Arumugam, Dilip and Lehnert, Lucas and Littman, Michael},
  booktitle={International Conference on Machine Learning},
  pages={10--19},
  year={2018}
}

@article{puterman1995markov,
  title={Markov decision processes: Discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  journal={Journal of the Operational Research Society},
  year={1995}
}


@InProceedings{asadi2019ambrl,
  title = 	 {Model-Based Reinforcement Learning Exploiting State-Action Equivalence},
  author = 	 {Asadi, Mahsa and Talebi, Mohammad Sadegh and Bourel, Hippolyte and Maillard, Odalric-Ambrym},
  booktitle = 	 {Proceedings of The Eleventh Asian Conference on Machine Learning},
  pages = 	 {204--219},
  year = 	 {2019},
  editor = 	 {Lee, Wee Sun and Suzuki, Taiji},
  volume = 	 {101},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Nagoya, Japan},
  month = 	 {17--19 Nov},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v101/asadi19a/asadi19a.pdf},
  url = 	 {http://proceedings.mlr.press/v101/asadi19a.html},
  abstract = 	 {Leveraging an equivalence property in the state-space of a Markov Decision Process (MDP) has been investigated in several studies. This paper studies equivalence structure in the reinforcement learning (RL) setup, where transition distributions are no longer assumed to be known. We present a notion of similarity between transition probabilities of various state-action pairs of an MDP, which naturally defines an equivalence structure in the state-action space. We present equivalence-aware confidence sets for the case where the learner knows the underlying structure in advance. These sets are provably smaller than their corresponding equivalence-oblivious counterparts. In the more challenging case of an unknown equivalence structure, we present an algorithm called ApproxEquivalence that seeks to find an (approximate) equivalence structure, and define confidence sets using the approximate equivalence. To illustrate the efficacy of the presented confidence sets, we present C-UCRL, as a natural modification of UCRL2 for RL in undiscounted MDPs. In the case of a known equivalence structure, we show that C-UCRL improves over UCRL2 in terms of \emph{regret} by a factor of $\sqrt{SA/C}$, in any communicating MDP with $S$ states, $A$ actions, and $C$ classes, which corresponds to a massive improvement when $C\ll SA$. To the best of our knowledge, this is the first work providing regret bounds for RL when an equivalence structure in the MDP is efficiently exploited. In the case of an unknown equivalence structure, we show through numerical experiments that C-UCRL combined with ApproxEquivalence outperforms UCRL2 in ergodic MDPs.}
}

@misc{hallak2015contextual,
      title={Contextual Markov Decision Processes}, 
      author={Assaf Hallak and Dotan Di Castro and Shie Mannor},
      year={2015},
      eprint={1502.02259},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@InProceedings{rakelly2019pearl,
  title = 	 {Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables},
  author = 	 {Rakelly, Kate and Zhou, Aurick and Finn, Chelsea and Levine, Sergey and Quillen, Deirdre},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5331--5340},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/rakelly19a/rakelly19a.pdf},
}


@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1126--1135},
  year={2017},
  organization={JMLR. org}
}


@inproceedings{
rothfuss2018promp,
title={Pro{MP}: Proximal Meta-Policy Search},
author={Jonas Rothfuss and Dennis Lee and Ignasi Clavera and Tamim Asfour and Pieter Abbeel},
booktitle={International Conference on Learning Representations},
year={2019}
}

@article{yarats2019improving,
    title={Improving Sample Efficiency in Model-Free Reinforcement Learning from Images},
    author={Denis Yarats and Amy Zhang and Ilya Kostrikov and Brandon Amos and Joelle Pineau and Rob Fergus},
    year={2019},
    eprint={1910.01741},
    archivePrefix={arXiv}
}

%%%%%%% MB MT RL %%%%%%%%%%
@misc{landolfi2019modelbased,
    title={A Model-based Approach for Sample-efficient Multi-task Reinforcement Learning},
    author={Nicholas C. Landolfi and Garrett Thomas and Tengyu Ma},
    year={2019},
    eprint={1907.04964},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{chen2018gradnorm,
  title={Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks},
  author={Chen, Zhao and Badrinarayanan, Vijay and Lee, Chen-Yu and Rabinovich, Andrew},
  booktitle={International Conference on Machine Learning},
  pages={794--803},
  year={2018},
  organization={PMLR}
}

@inproceedings{kipf2018interacting,
  author    = {Thomas N. Kipf and
               Ethan Fetaya and
               Kuan{-}Chieh Wang and
               Max Welling and
               Richard S. Zemel},
  editor    = {Jennifer G. Dy and
               Andreas Krause},
  title     = {Neural Relational Inference for Interacting Systems},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
               10-15, 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {2693--2702},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v80/kipf18a.html},
  timestamp = {Wed, 03 Apr 2019 18:17:30 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/KipfFWWZ18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{klink2019contextualrl,
  author    = {Pascal Klink and
               Hany Abdulsamad and
               Boris Belousov and
               Jan Peters},
  editor    = {Leslie Pack Kaelbling and
               Danica Kragic and
               Komei Sugiura},
  title     = {Self-Paced Contextual Reinforcement Learning},
  booktitle = {3rd Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan,
               October 30 - November 1, 2019, Proceedings},
  series    = {Proceedings of Machine Learning Research},
  volume    = {100},
  pages     = {513--529},
  publisher = {{PMLR}},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v100/klink20a.html},
  timestamp = {Mon, 25 May 2020 15:01:26 +0200},
  biburl    = {https://dblp.org/rec/conf/corl/KlinkAB019.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{pathak2019LearningControlSelfAssembling,
	title = {Learning to {Control} {Self}-{Assembling} {Morphologies}: {A} {Study} of {Generalization} via {Modularity}},
	shorttitle = {Learning to {Control} {Self}-{Assembling} {Morphologies}},
	urldate = {2020-06-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Pathak, Deepak and Lu, Christopher and Darrell, Trevor and Isola, Phillip and Efros, Alexei A},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alch-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {2295--2305},
	file = {NIPS Full Text PDF:/Users/amyzhang/Zotero/storage/NI8U87KM/Pathak et al. - 2019 - Learning to Control Self-Assembling Morphologies .pdf:application/pdf;NIPS Snapshot:/Users/amyzhang/Zotero/storage/APUU5J84/8501-learning-to-control-self-assembling-morphologies-a-study-of-generalization-via-modularity.html:text/html}
}

@misc{li2020causal,
    title={Causal Discovery in Physical Systems from Videos},
    author={Yunzhu Li and Antonio Torralba and Animashree Anandkumar and Dieter Fox and Animesh Garg},
    year={2020},
    eprint={2007.00631},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{radford2019language_models_are_unsupervised_multitask_learners,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{caruana1997multitask_learning,
  title={Multitask learning},
  author={Caruana, Rich},
  journal={Machine learning},
  volume={28},
  number={1},
  pages={41--75},
  year={1997},
  publisher={Springer}
}

@inproceedings{zhang2014facial_landmark_detection_by_deep_multitask_learning,
  title={Facial landmark detection by deep multi-task learning},
  author={Zhang, Zhanpeng and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},
  booktitle={European conference on computer vision},
  pages={94--108},
  year={2014},
  organization={Springer}
}

@inproceedings{kokkinos2017ubernet,
  title={Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory},
  author={Kokkinos, Iasonas},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6129--6138},
  year={2017}
}

@article{bellemare2013arcade,
  title={The arcade learning environment: An evaluation platform for general agents},
  author={Bellemare, Marc G and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  journal={Journal of Artificial Intelligence Research},
  volume={47},
  pages={253--279},
  year={2013}
}


@article{goyal2019recurrent,
  title={Recurrent independent mechanisms},
  author={Goyal, Anirudh and Lamb, Alex and Hoffmann, Jordan and Sodhani, Shagun and Levine, Sergey and Bengio, Yoshua and Sch{\"o}lkopf, Bernhard},
  journal={arXiv preprint arXiv:1909.10893},
  year={2019}
}

@article{frans2017meta_learning_shared_hierarchies,
  title={Meta learning shared hierarchies},
  author={Frans, Kevin and Ho, Jonathan and Chen, Xi and Abbeel, Pieter and Schulman, John},
  journal={arXiv preprint arXiv:1710.09767},
  year={2017}
}

@article{vezhnevets2017feudal_networks_for_hierarchical_reinforcement_learning,
  title={Feudal networks for hierarchical reinforcement learning},
  author={Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1703.01161},
  year={2017}
}


@article{epopt_learning_robust_neural_network_policies_using_model_ensembles,
  title={Epopt: Learning robust neural network policies using model ensembles},
  author={Rajeswaran, Aravind and Ghotra, Sarvjeet and Ravindran, Balaraman and Levine, Sergey},
  journal={arXiv preprint arXiv:1610.01283},
  year={2016}
}

@misc{
multitask_soft_option_learning,
title={Multitask Soft Option Learning},
author={Maximilian Igl and Andrew Gambardella and Jinke He and Nantas Nardelli and N. Siddharth and Wendelin B{\"o}hmer and Shimon Whiteson},
year={2020},
url={https://openreview.net/forum?id=BkeDGJBKvB}
}


@article{modi_markov_2017,
	title = {Markov {Decision} {Processes} with {Continuous} {Side} {Information}},
	url = {http://arxiv.org/abs/1711.05726},
	urldate = {2020-10-15},
	journal = {arXiv:1711.05726 [cs, stat]},
	author = {Modi, Aditya and Jiang, Nan and Singh, Satinder and Tewari, Ambuj},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.05726},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/amyzhang/Zotero/storage/M36JXWZG/1711.html:text/html;arXiv Fulltext PDF:/Users/amyzhang/Zotero/storage/7FPVVFIR/Modi et al. - 2017 - Markov Decision Processes with Continuous Side Inf.pdf:application/pdf;Modi et al. - Markov Decision Processes with Continuous Side Inf.pdf:/Users/amyzhang/Zotero/storage/HDSK3EDV/Modi et al. - Markov Decision Processes with Continuous Side Inf.pdf:application/pdf},
}

@inproceedings{modular_multitask_reinforcement_learning_with_policy_sketches,
  title={Modular multitask reinforcement learning with policy sketches},
  author={Andreas, Jacob and Klein, Dan and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={166--175},
  year={2017}
}

@inproceedings{multitask_reinforcement_learning_on_the_distribution_of_mdps,
  title={Multitask reinforcement learning on the distribution of {MDPs}},
  author={Tanaka, Fumihide and Yamamura, Masayuki},
  booktitle={Proceedings 2003 IEEE International Symposium on Computational Intelligence in Robotics and Automation. Computational Intelligence in Robotics and Automation for the New Millennium (Cat. No. 03EX694)},
  volume={3},
  pages={1108--1113},
  year={2003},
  organization={IEEE}
}

@inproceedings{scalable_multitask_policy_gradient_reinforcement_learning,
  title={Scalable Multitask Policy Gradient Reinforcement Learning.},
  author={El Bsat, Salam and Bou-Ammar, Haitham and Taylor, Matthew E},
  booktitle={AAAI},
  pages={1847--1853},
  year={2017}
}

@inproceedings{policy_distillation,
  author={Andrei A. Rusu and Sergio Gomez Colmenarejo and Çaglar Gülçehre and Guillaume Desjardins and James Kirkpatrick and Razvan Pascanu and Volodymyr Mnih and Koray Kavukcuoglu and Raia Hadsell},
  title={Policy Distillation},
  year={2016},
  cdate={1451606400000},
  url={http://arxiv.org/abs/1511.06295},
  booktitle={ICLR (Poster)},
}


@article{parisotto2015actor,
  title={Actor-mimic: Deep multitask and transfer reinforcement learning},
  author={Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1511.06342},
  year={2015}
}

@article{actor_mimic_deep_multitask_and_transfer_reinforcement_learning,
  title={Actor-mimic: Deep multitask and transfer reinforcement learning},
  author={Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1511.06342},
  year={2015}
}

@article{which_tasks_should_be_learned_together_in_multitask_learning?,
  title={Which Tasks Should Be Learned Together in Multi-task Learning?},
  author={Standley, Trevor and Zamir, Amir R and Chen, Dawn and Guibas, Leonidas and Malik, Jitendra and Savarese, Silvio},
  journal={arXiv preprint arXiv:1905.07553},
  year={2019}
}

@inproceedings{learning_modular_neural_network_policies_for_multitask_and_multirobot_transfer,
  title={Learning modular neural network policies for multi-task and multi-robot transfer},
  author={Devin, Coline and Gupta, Abhishek and Darrell, Trevor and Abbeel, Pieter and Levine, Sergey},
  booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={2169--2176},
  year={2017},
  organization={IEEE}
}

@inproceedings{sharing_experience_in_multitask_reinforcement_learning,
  title={Sharing experience in multitask reinforcement learning},
  author={Vuong, Tung-Long and Nguyen, Do-Van and Nguyen, Tai-Long and Bui, Cong-Minh and Kieu, Hai-Dang and Ta, Viet-Cuong and Tran, Quoc-Long and Le, Thanh-Ha},
  booktitle={Proceedings of the 28th International Joint Conference on Artificial Intelligence},
  pages={3642--3648},
  year={2019},
  organization={AAAI Press}
}

@inproceedings{multitask_deep_rl_with_popart,
  title={Multi-task deep reinforcement learning with popart},
  author={Hessel, Matteo and Soyer, Hubert and Espeholt, Lasse and Czarnecki, Wojciech and Schmitt, Simon and van Hasselt, Hado},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={3796--3803},
  year={2019}
}

@article{gradient_surgery_for_multitask_learning,
  title={Gradient surgery for multi-task learning},
  author={Yu, Tianhe and Kumar, Saurabh and Gupta, Abhishek and Levine, Sergey and Hausman, Karol and Finn, Chelsea},
  journal={arXiv preprint arXiv:2001.06782},
  year={2020}
}

@inproceedings{multitask_learning_as_multi_objective_optimization,
  title={Multi-task learning as multi-objective optimization},
  author={Sener, Ozan and Koltun, Vladlen},
  booktitle={Advances in Neural Information Processing Systems},
  pages={527--538},
  year={2018}
}

@article{regularizing_deep_multi_task_networks_using_orthogonal_gradients,
  title={Regularizing Deep Multi-Task Networks using Orthogonal Gradients},
  author={Suteu, Mihai and Guo, Yike},
  journal={arXiv preprint arXiv:1912.06844},
  year={2019}
}

@article{adapting_auxiliary_losses_using_gradient_similarity,
  title={Adapting auxiliary losses using gradient similarity},
  author={Du, Yunshu and Czarnecki, Wojciech M and Jayakumar, Siddhant M and Pascanu, Razvan and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:1812.02224},
  year={2018}
}

@article{knowledge_transfer_in_multtask_deep_reinforcement_learning_for_continuous_control,
  title={Knowledge Transfer in Multi-Task Deep Reinforcement Learning for Continuous Control},
  author={Xu, Zhiyuan and Wu, Kun and Che, Zhengping and Tang, Jian and Ye, Jieping},
  journal={arXiv e-prints},
  pages={arXiv--2010},
  year={2020}
}

@article{feudal_networks_for_hierarchical_reinforcement_learning,
  title={Feudal networks for hierarchical reinforcement learning},
  author={Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1703.01161},
  year={2017}
}

@inproceedings{option_critic_architecture,
  title={The option-critic architecture},
  author={Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
  booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
  year={2017}
}


@article{options,
  title={Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artificial intelligence},
  volume={112},
  number={1-2},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}

@inproceedings{
reinforcement_learning_with_competitive_ensembles_of_information_constrained_primitives,
title={Reinforcement Learning with Competitive  Ensembles of Information-Constrained Primitives},
author={Anirudh Goyal and Shagun Sodhani and Jonathan Binas and Xue Bin Peng and Sergey Levine and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=ryxgJTEYDr}
}

@inproceedings{
learning_to_coordinate_manipulation_skills_via_skill_behavior_diversification,
title={learning to Coordinate Manipulation Skills via Skill Behavior Diversification},
author={Youngwoon Lee and Jingyun Yang and Joseph J. Lim},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=ryxB2lBtvH}
}


@article{meta_learning_shared_hierarchies,
  title={Meta learning shared hierarchies},
  author={Frans, Kevin and Ho, Jonathan and Chen, Xi and Abbeel, Pieter and Schulman, John},
  journal={arXiv preprint arXiv:1710.09767},
  year={2017}
}

@inproceedings{learning_independent_causal_mechanisms,
  title={Learning independent causal mechanisms},
  author={Parascandolo, Giambattista and Kilbertus, Niki and Rojas-Carulla, Mateo and Sch{\"o}lkopf, Bernhard},
  booktitle={International Conference on Machine Learning},
  pages={4036--4044},
  year={2018},
  organization={PMLR}
}



@article{routing_networks_and_the_challenges_of_modular_and_compositional_computation,
  title={Routing Networks and the Challenges of Modular and Compositional Computation. apr 2019},
  author={Rosenbaum, Clemens and Cases, Ignacio and Riemer, Matthew and Klinger, Tim},
  journal={URL http://arxiv. org/abs},
  year={2019}
}

@InProceedings{neural_module_network,
author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
title = {Neural Module Networks},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@article{mozifian2020intervention,
      title={Intervention Design for Effective Sim2Real Transfer}, 
      author={Melissa Mozifian and Amy Zhang and Joelle Pineau and David Meger},
      year={2020},
      journal={arXiv preprint arXiv:2012.02055},
}

@article{roberta_a_robustly_optimized_bert_pretraining_approach,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}


@inproceedings{Mmtadata_based_clustered_multitask_learning_for_thread_mining_in_web_communities,
  title={Metadata-Based Clustered Multi-task Learning for Thread Mining in Web Communities},
  author={You, Qiang and Wu, Ou and Luo, Guan and Hu, Weiming},
  booktitle={International Conference on Machine Learning and Data Mining in Pattern Recognition},
  pages={421--434},
  year={2016},
  organization={Springer}
}

@inproceedings{babyai,
title={Baby{AI}: First Steps Towards Grounded Language Learning With a Human In the Loop},
author={Maxime Chevalier-Boisvert and Dzmitry Bahdanau and Salem Lahlou and Lucas Willems and Chitwan Saharia and Thien Huu Nguyen and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJeXCo0cYX},
}

@inproceedings{learning_to_parse_natural_language_to_grounded_reward_functions_with_weak_supervision,
  title={Learning to parse natural language to grounded reward functions with weak supervision},
  author={Williams, Edward C and Gopalan, Nakul and Rhee, Mine and Tellex, Stefanie},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1--7},
  year={2018},
  organization={IEEE}
}

@article{gated_attention_architectures_for_task_oriented_language_grounding,
  title={Gated-attention architectures for task-oriented language grounding},
  author={Chaplot, Devendra Singh and Sathyendra, Kanthashree Mysore and Pasumarthi, Rama Kumar and Rajagopal, Dheeraj and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1706.07230},
  year={2017}
}

@inproceedings{learning_to_interpret_natural_language_navigation_instructions_from_observations,
  title={Learning to interpret natural language navigation instructions from observations},
  author={Chen, David L and Mooney, Raymond J},
  booktitle={Twenty-Fifth AAAI Conference on Artificial Intelligence},
  year={2011}
}

@article{grounded_language_learning_in_a_simulated_3d_world,
  title={Grounded language learning in a simulated 3d world},
  author={Hermann, Karl Moritz and Hill, Felix and Green, Simon and Wang, Fumin and Faulkner, Ryan and Soyer, Hubert and Szepesvari, David and Czarnecki, Wojciech Marian and Jaderberg, Max and Teplyashin, Denis and others},
  journal={arXiv preprint arXiv:1706.06551},
  year={2017}
}

@inproceedings{understanding_natural_language_commands_for_robotic_navigation_and_mobile_manipulation,
  title={Understanding natural language commands for robotic navigation and mobile manipulation},
  author={Tellex, Stefanie and Kollar, Thomas and Dickerson, Steven and Walter, Matthew and Banerjee, Ashis and Teller, Seth and Roy, Nicholas},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={25},
  year={2011}
}

@inproceedings{chao2011towards,
  title={Towards grounding concepts for transfer in goal learning from demonstration},
  author={Chao, Crystal and Cakmak, Maya and Thomaz, Andrea L},
  booktitle={2011 IEEE International Conference on Development and Learning (ICDL)},
  volume={2},
  pages={1--6},
  year={2011},
  organization={IEEE}
}

@inproceedings{end_to_end_multi_task_learning_with_attention,
  title={End-to-end multi-task learning with attention},
  author={Liu, Shikun and Johns, Edward and Davison, Andrew J},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1871--1880},
  year={2019}
}

@article{automatically_composing_representation_transformations_as_a_means_for_generalization,
  title={Automatically composing representation transformations as a means for generalization},
  author={Chang, Michael B and Gupta, Abhishek and Levine, Sergey and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:1807.04640},
  year={2018}
}

@inproceedings{recommending_what_video_to_watch_next_a_multitask_ranking_system,
  title={Recommending what video to watch next: a multitask ranking system},
  author={Zhao, Zhe and Hong, Lichan and Wei, Li and Chen, Jilin and Nath, Aniruddh and Andrews, Shawn and Kumthekar, Aditee and Sathiamoorthy, Maheswaran and Yi, Xinyang and Chi, Ed},
  booktitle={Proceedings of the 13th ACM Conference on Recommender Systems},
  pages={43--51},
  year={2019}
}

@inproceedings{learning_modular_neural_network_policies_for_multi_task_and_multi_robot_transfer,
  title={Learning modular neural network policies for multi-task and multi-robot transfer},
  author={Devin, Coline and Gupta, Abhishek and Darrell, Trevor and Abbeel, Pieter and Levine, Sergey},
  booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={2169--2176},
  year={2017},
  organization={IEEE}
}

@article{kaiser2019model,
  title={Model-based reinforcement learning for atari},
  author={Kaiser, Lukasz and Babaeizadeh, Mohammad and Milos, Piotr and Osinski, Blazej and Campbell, Roy H and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and others},
  journal={arXiv preprint arXiv:1903.00374},
  year={2019}
}

@article{srinivas2020curl,
  title={Curl: Contrastive unsupervised representations for reinforcement learning},
  author={Srinivas, Aravind and Laskin, Michael and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2004.04136},
  year={2020}
}

@misc{zhang2019causal_states,
      title={Learning Causal State Representations of Partially Observable Environments}, 
      author={Amy Zhang and Zachary C. Lipton and Luis Pineda and Kamyar Azizzadenesheli and Anima Anandkumar and Laurent Itti and Joelle Pineau and Tommaso Furlanello},
      year={2019},
      eprint={1906.10437},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article {rapid_trial_and_error_learning_with_simulation_supports_flexible_tool_use_and_physical_reasoning,
	author = {Allen, Kelsey R. and Smith, Kevin A. and Tenenbaum, Joshua B.},
	title = {Rapid trial-and-error learning with simulation supports flexible tool use and physical reasoning},
	volume = {117},
	number = {47},
	pages = {29302--29310},
	year = {2020},
	doi = {10.1073/pnas.1912341117},
	publisher = {National Academy of Sciences},
	abstract = {Many animals, and an increasing number of artificial agents, display sophisticated capabilities to perceive and manipulate objects. But human beings remain distinctive in their capacity for flexible, creative tool use{\textemdash}using objects in new ways to act on the world, achieve a goal, or solve a problem. To study this type of general physical problem solving, we introduce the Virtual Tools game. In this game, people solve a large range of challenging physical puzzles in just a handful of attempts. We propose that the flexibility of human physical problem solving rests on an ability to imagine the effects of hypothesized actions, while the efficiency of human search arises from rich action priors which are updated via observations of the world. We instantiate these components in the {\textquotedblleft}sample, simulate, update{\textquotedblright} (SSUP) model and show that it captures human performance across 30 levels of the Virtual Tools game. More broadly, this model provides a mechanism for explaining how people condense general physical knowledge into actionable, task-specific plans to achieve flexible and efficient physical problem solving.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/117/47/29302},
	eprint = {https://www.pnas.org/content/117/47/29302.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{a_survey_of_reinforcement_learning_informed_by_natural_language,
  title={A survey of reinforcement learning informed by natural language},
  author={Luketina, Jelena and Nardelli, Nantas and Farquhar, Gregory and Foerster, Jakob and Andreas, Jacob and Grefenstette, Edward and Whiteson, Shimon and Rockt{\"a}schel, Tim},
  journal={arXiv preprint arXiv:1906.03926},
  year={2019}
}

@article{openai_gym,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@article{deep_reinforcement_learning_that_matters,
  title={Deep reinforcement learning that matters},
  author={Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  journal={arXiv preprint arXiv:1709.06560},
  year={2017}
}

@inproceedings{jiang2019language,
  title={Language as an abstraction for hierarchical deep reinforcement learning},
  author={Jiang, Yiding and Gu, Shixiang Shane and Murphy, Kevin P and Finn, Chelsea},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9419--9431},
  year={2019}
}

@article{multi_object_representation_learning_with_iterative_variational_inference,
  title={Multi-object representation learning with iterative variational inference},
  author={Greff, Klaus and Kaufman, Rapha{\"e}l Lopez and Kabra, Rishabh and Watters, Nick and Burgess, Chris and Zoran, Daniel and Matthey, Loic and Botvinick, Matthew and Lerchner, Alexander},
  journal={arXiv preprint arXiv:1903.00450},
  year={2019}
}

@article{object_centric_learning_with_slot_attention,
  title={Object-centric learning with slot attention},
  author={Locatello, Francesco and Weissenborn, Dirk and Unterthiner, Thomas and Mahendran, Aravindh and Heigold, Georg and Uszkoreit, Jakob and Dosovitskiy, Alexey and Kipf, Thomas},
  journal={arXiv preprint arXiv:2006.15055},
  year={2020}
}

@article{student1908probable,
  title={The probable error of a mean},
  author={Student},
  journal={Biometrika},
  pages={1--25},
  year={1908},
  publisher={JSTOR}
}


@inproceedings{film,
  title={Film: Visual reasoning with a general conditioning layer},
  author={Perez, Ethan and Strub, Florian and De Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  year={2018}
}

@inproceedings{
rtfm,
title={RTFM: Generalising to New Environment Dynamics via Reading},
author={Victor Zhong and Tim Rocktäschel and Edward Grefenstette},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJgob6NKvH}
}

@Misc{Sodhani2021MTRL,
  author =       {Shagun Sodhani and Amy Zhang},
  title =        {MTRL - Multi Task RL Algorithms},
  howpublished = {Github},
  year =         {2021},
  url =          {https://github.com/facebookresearch/mtrl}
}

@Misc{Sodhani2021MTEnv,
  author =       {Shagun Sodhani and Ludovic Denoyer and Pierre-Alexandre Kamienny and Olivier Delalleau},
  title =        {MTEnv - Environment interface for mulit-task reinforcement learning},
  howpublished = {Github},
  year =         {2021},
  url =          {https://github.com/facebookresearch/mtenv}
}

@Misc{Yadan2019Hydra,
  author =       {Omry Yadan},
  title =        {Hydra - A framework for elegantly configuring complex applications},
  howpublished = {Github},
  year =         {2019},
  url =          {https://github.com/facebookresearch/hydra}
}

@article{2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={arXiv preprint arXiv:1912.01703},
  year={2019}
}


@Article{         harris2020array,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{'{a}}ndez del
                 R{'{\i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = sep,
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

@software{reback2020pandas,
    author       = {The pandas development team},
    title        = {pandas-dev/pandas: Pandas},
    month        = feb,
    year         = 2020,
    publisher    = {Zenodo},
    version      = {latest},
    doi          = {10.5281/zenodo.3509134},
    url          = {https://doi.org/10.5281/zenodo.3509134}
}

@article{2017_hierarchical_and_interpretable_skill_acquisition_in_multi_task_reinforcement_learning,
  title={Hierarchical and interpretable skill acquisition in multi-task reinforcement learning},
  author={Shu, Tianmin and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1712.07294},
  year={2017}
}

@inproceedings{2017_zero_shot_task_generalization_with_multi_task_deep_reinforcement_learning,
  title={Zero-shot task generalization with multi-task deep reinforcement learning},
  author={Oh, Junhyuk and Singh, Satinder and Lee, Honglak and Kohli, Pushmeet},
  booktitle={International Conference on Machine Learning},
  pages={2661--2670},
  year={2017},
  organization={PMLR}
}

@article{shu2017hierarchical,
  title={Hierarchical and interpretable skill acquisition in multi-task reinforcement learning},
  author={Shu, Tianmin and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1712.07294},
  year={2017}
}

@article{mott2019towards,
  title={Towards interpretable reinforcement learning using attention augmented agents},
  author={Mott, Alex and Zoran, Daniel and Chrzanowski, Mike and Wierstra, Daan and Rezende, Danilo J},
  journal={arXiv preprint arXiv:1906.02500},
  year={2019}
}

@inproceedings{bram2019attentive,
  title={Attentive Multi-Task Deep Reinforcement Learning},
  author={Br{\"a}m, Timo and Brunner, Gino and Richter, Oliver and Wattenhofer, Roger},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={134--149},
  year={2019},
  organization={Springer}
}

@article{electronics9091363,
	abstract = {Driven by the recent technological advancements within the field of artificial intelligence research, deep learning has emerged as a promising representation learning technique across all of the machine learning classes, especially within the reinforcement learning arena. This new direction has given rise to the evolution of a new technological domain named deep reinforcement learning, which combines the representational learning power of deep learning with existing reinforcement learning methods. Undoubtedly, the inception of deep reinforcement learning has played a vital role in optimizing the performance of reinforcement learning-based intelligent agents with model-free based approaches. Although these methods could improve the performance of agents to a greater extent, they were mainly limited to systems that adopted reinforcement learning algorithms focused on learning a single task. At the same moment, the aforementioned approach was found to be relatively data-inefficient, particularly when reinforcement learning agents needed to interact with more complex and rich data environments. This is primarily due to the limited applicability of deep reinforcement learning algorithms to many scenarios across related tasks from the same environment. The objective of this paper is to survey the research challenges associated with multi-tasking within the deep reinforcement arena and present the state-of-the-art approaches by comparing and contrasting recent solutions, namely DISTRAL (DIStill &amp; TRAnsfer Learning), IMPALA(Importance Weighted Actor-Learner Architecture) and PopArt that aim to address core challenges such as scalability, distraction dilemma, partial observability, catastrophic forgetting and negative knowledge transfer.},
	article-number = {1363},
	author = {Vithayathil Varghese, Nelson and Mahmoud, Qusay H.},
	doi = {10.3390/electronics9091363},
	issn = {2079-9292},
	journal = {Electronics},
	number = {9},
	title = {A Survey of Multi-Task Deep Reinforcement Learning},
	url = {https://www.mdpi.com/2079-9292/9/9/1363},
	volume = {9},
	year = {2020},
	Bdsk-Url-1 = {https://www.mdpi.com/2079-9292/9/9/1363},
	Bdsk-Url-2 = {https://doi.org/10.3390/electronics9091363}}

@article{ruder2017overview,
  title={An overview of multi-task learning in deep neural networks},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1706.05098},
  year={2017}
}