\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amos et~al.(2018)Amos, Dinh, Cabi, Roth{\"{o}}rl, Colmenarejo, Muldal,
  Erez, Tassa, de~Freitas, and Denil]{preco}
Brandon Amos, Laurent Dinh, Serkan Cabi, Thomas Roth{\"{o}}rl, Sergio~Gomez
  Colmenarejo, Alistair Muldal, Tom Erez, Yuval Tassa, Nando de~Freitas, and
  Misha Denil.
\newblock Learning awareness models.
\newblock In \emph{ICLR}, 2018.

\bibitem[Bacon et~al.(2017)Bacon, Harb, and Precup]{OptionCritic}
Pierre{-}Luc Bacon, Jean Harb, and Doina Precup.
\newblock The option-critic architecture.
\newblock \emph{AAAI}, 2017.

\bibitem[Beattie et~al.(2016)Beattie, Leibo, Teplyashin, Ward, Wainwright,
  K{\"{u}}ttler, Lefrancq, Green, Vald{\'{e}}s, Sadik, Schrittwieser, Anderson,
  York, Cant, Cain, Bolton, Gaffney, King, Hassabis, Legg, and Petersen]{DmLab}
Charles Beattie, Joel~Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright,
  Heinrich K{\"{u}}ttler, Andrew Lefrancq, Simon Green, V{\'{\i}}ctor
  Vald{\'{e}}s, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York,
  Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis
  Hassabis, Shane Legg, and Stig Petersen.
\newblock Deepmind lab.
\newblock \emph{Arxiv}, abs/1612.03801, 2016.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare2013arcade}
Marc~G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.
\newblock The {A}rcade {L}earning {E}nvironment: An evaluation platform for
  general agents.
\newblock \emph{JAIR}, 2013.

\bibitem[Bellman(1957)]{bellman:1957}
Richard Bellman.
\newblock A markovian decision process.
\newblock \emph{Journal of Mathematics and Mechanics}, 1957.

\bibitem[Conn et~al.(2000)Conn, Gould, and Toint]{Conn:2000}
Andrew~R. Conn, Nicholas I.~M. Gould, and {\relax Ph}ilippe~L. Toint.
\newblock \emph{Trust-Region Methods}.
\newblock SIAM, Philadelphia, PA, USA, 2000.

\bibitem[Degris et~al.(2012)Degris, White, and Sutton]{Degris2012OffPolicyA}
Thomas Degris, Martha White, and Richard~S. Sutton.
\newblock Off-policy actor-critic.
\newblock In \emph{ICML}, 2012.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, Legg, and Kavukcuoglu]{IMPALA}
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom
  Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and
  Koray Kavukcuoglu.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In \emph{ICML}, 2018.

\bibitem[Gruslys et~al.(2018)Gruslys, Dabney, Azar, Piot, Bellemare, and
  Munos]{Reactor}
Audrunas Gruslys, Will Dabney, Mohammad~Gheshlaghi Azar, Bilal Piot, Marc
  Bellemare, and Remi Munos.
\newblock The reactor: A fast and sample-efficient actor-critic agent for
  reinforcement learning.
\newblock In \emph{ICLR}, 2018.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{Haarnoja:2018}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{ICML}, 2018.

\bibitem[Hessel et~al.(2017)Hessel, Modayil, van Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{Rainbow}
Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski,
  Will Dabney, Daniel Horgan, Bilal Piot, Mohammad~Gheshlaghi Azar, and David
  Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock \emph{Arxiv}, abs/1710.02298, 2017.

\bibitem[Hessel et~al.(2019)Hessel, Soyer, Espeholt, Czarnecki, Schmitt, and
  van Hasselt]{hessel2018multitask}
Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt,
  and Hado van Hasselt.
\newblock Multi-task deep reinforcement learning with popart.
\newblock In \emph{AAAI}, 2019.

\bibitem[Hester et~al.(2017)Hester, Vecerik, Pietquin, Lanctot, Schaul, Piot,
  Sendonaris, Dulac{-}Arnold, Osband, Agapiou, Leibo, and Gruslys]{Hester:2017}
Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal
  Piot, Andrew Sendonaris, Gabriel Dulac{-}Arnold, Ian Osband, John Agapiou,
  Joel~Z. Leibo, and Audrunas Gruslys.
\newblock Learning from demonstrations for real world reinforcement learning.
\newblock In \emph{AAAI}, 2017.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{Hochreiter:1997}
Sepp Hochreiter and J\"{u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural Comput.}, 9\penalty0 (8):\penalty0 1735--1780, November
  1997.
\newblock ISSN 0899-7667.
\newblock \doi{10.1162/neco.1997.9.8.1735}.

\bibitem[Horgan et~al.(2018)Horgan, Quan, Budden, Barth-Maron, Hessel, van
  Hasselt, and Silver]{ApeX}
Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado
  van Hasselt, and David Silver.
\newblock Distributed prioritized experience replay.
\newblock In \emph{ICLR}, 2018.

\bibitem[Jaderberg et~al.(2017{\natexlab{a}})Jaderberg, Dalibard, Osindero,
  Czarnecki, Donahue, Razavi, Vinyals, Green, Dunning, Simonyan, Fernando, and
  Kavukcuoglu]{jaderberg2017pbt}
Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech~M. Czarnecki, Jeff
  Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan,
  Chrisantha Fernando, and Koray Kavukcuoglu.
\newblock Population based training of neural networks.
\newblock \emph{Arxiv}, abs/1711.09846, 2017{\natexlab{a}}.

\bibitem[Jaderberg et~al.(2017{\natexlab{b}})Jaderberg, Mnih, Czarnecki,
  Schaul, Leibo, Silver, and Kavukcuoglu]{jaderberg2016reinforcement}
Max Jaderberg, Volodymyr Mnih, Wojciech~Marian Czarnecki, Tom Schaul, Joel~Z
  Leibo, David Silver, and Koray Kavukcuoglu.
\newblock Reinforcement learning with unsupervised auxiliary tasks.
\newblock \emph{ICLR}, 2017{\natexlab{b}}.

\bibitem[Kahn(1955)]{Kahn:1955}
Herman Kahn.
\newblock Use of different monte carlo sampling techniques.
\newblock \emph{Santa Monica, Calif.}, 1955.

\bibitem[Kaiser et~al.(2019)Kaiser, Babaeizadeh, Milos, Osinski, Campbell,
  Czechowski, Erhan, Finn, Kozakowski, Levine, Sepassi, Tucker, and
  Michalewski]{Kaiser:2019}
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy~H.
  Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski,
  Sergey Levine, Ryan Sepassi, George Tucker, and Henryk Michalewski.
\newblock Model-based reinforcement learning for atari.
\newblock \emph{Arxiv}, abs/1903.00374, 2019.

\bibitem[Kapturowski et~al.(2019)Kapturowski, Ostrovski, Quan, Munos, and
  Dabney]{Kapturowski:2019}
Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney.
\newblock Recurrent experience replay in distributed reinforcement learning.
\newblock In \emph{ICLR}, 2019.

\bibitem[{Kretchmar}(2002)]{Kretchmar:2002}
R.M {Kretchmar}.
\newblock Parallel reinforcement learning.
\newblock \emph{SCI2002. The 6th World Conference on Systemics, Cybernetics,
  and Informatics. Orlando, FL}, 2002.

\bibitem[Lample \& Chaplot(2016)Lample and Chaplot]{AuxTasksFPS}
Guillaume Lample and Devendra~Singh Chaplot.
\newblock Playing {FPS} games with deep reinforcement learning.
\newblock In \emph{AAAI}, 2016.

\bibitem[Lin(1992)]{Lin:1992}
Long-Ji Lin.
\newblock \emph{Reinforcement Learning for Robots Using Neural Networks}.
\newblock PhD thesis, Pittsburgh, PA, USA, 1992.
\newblock UMI Order No. GAX93-22750.

\bibitem[Mankowitz et~al.(2018)Mankowitz, Z{\'{\i}}dek, Barreto, Horgan,
  Hessel, Quan, Oh, van Hasselt, Silver, and Schaul]{Mankowitz:2018}
Daniel~J. Mankowitz, Augustin Z{\'{\i}}dek, Andr{\'{e}} Barreto, Dan Horgan,
  Matteo Hessel, John Quan, Junhyuk Oh, Hado van Hasselt, David Silver, and Tom
  Schaul.
\newblock Unicorn: Continual learning with a universal, off-policy agent.
\newblock \emph{Arxiv}, abs/1802.08294, 2018.

\bibitem[Mirowski et~al.(2017)Mirowski, Pascanu, Viola, Soyer, Ballard, Banino,
  Denil, Goroshin, Sifre, Kavukcuoglu, Kumaran, and Hadsell]{AuxTasksDepth}
Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew~J. Ballard,
  Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu,
  Dharshan Kumaran, and Raia Hadsell.
\newblock Learning to navigate in complex environments.
\newblock In \emph{ICLR}, 2017.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{dqn-arxiv}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin~A. Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{Arxiv}, abs/1312.5602, 2013.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih15human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{Mnih:2016}
Volodymyr Mnih, Adri{\`{a}}~Puigdom{\`{e}}nech Badia, Mehdi Mirza, Alex Graves,
  Timothy~P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{ICML}, 2016.

\bibitem[Moore \& Atkeson(1993)Moore and Atkeson]{Moore:1993}
Andrew Moore and C.~G. Atkeson.
\newblock Prioritized sweeping: Reinforcement learning with less data and less
  real time.
\newblock \emph{Machine Learning}, 13, October 1993.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{Munos:2016}
Remi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock In \emph{NIPS}. 2016.

\bibitem[Nair et~al.(2015)Nair, Srinivasan, Blackwell, Alcicek, Fearon,
  De~Maria, Panneershelvam, Suleyman, Beattie, Petersen, Legg, Mnih,
  Kavukcuoglu, and Silver]{Nair:2015}
Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon,
  Alessandro De~Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles
  Beattie, Stig Petersen, Shane Legg, Volodymyr Mnih, Koray Kavukcuoglu, and
  David Silver.
\newblock Massively parallel methods for deep reinforcement learning.
\newblock \emph{Arxiv}, abs/1507.04296, 2015.

\bibitem[Olton(1979)]{Olton:1979}
David~S. Olton.
\newblock Mazes, maps, and memory.
\newblock \emph{American Psychologist}, 1979.

\bibitem[Pascanu et~al.(2012)Pascanu, Mikolov, and Bengio]{Pascanu:2012}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock Understanding the exploding gradient problem.
\newblock \emph{Arxiv}, abs/1211.5063, 2012.

\bibitem[Precup et~al.(2000)Precup, Sutton, and Singh]{Precup:2000}
Doina Precup, Richard~S. Sutton, and Satinder~P. Singh.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock In \emph{ICML}, 2000.

\bibitem[Riedmiller(2005)]{Riedmiller:2005}
Martin Riedmiller.
\newblock Neural fitted q iteration -- first experiences with a data efficient
  neural reinforcement learning method.
\newblock In \emph{ECML}, 2005.

\bibitem[Schaul et~al.(2015)Schaul, Quan, Antonoglou, and
  Silver]{schaul2015prioritized}
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
\newblock Prioritized experience replay.
\newblock In \emph{ICLR}, 2015.

\bibitem[Schaul et~al.(2019)Schaul, Borsa, Modayil, and Pascanu]{Schaul:2019}
Tom Schaul, Diana Borsa, Joseph Modayil, and Razvan Pascanu.
\newblock Ray interference: a source of plateaus in deep reinforcement
  learning.
\newblock 2019.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{Schulman:15}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In Francis Bach and David Blei (eds.), \emph{Proceedings of the 32nd
  International Conference on Machine Learning}, volume~37 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1889--1897, Lille, France, 07--09 Jul
  2015. PMLR.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{Schulman:17}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{Arxiv}, abs/1707.06347, 2017.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{Sutton:1999}
Richard~S. Sutton, Doina Precup, and Satinder Singh.
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in
  reinforcement learning.
\newblock \emph{Artif. Intell.}, 112\penalty0 (1-2):\penalty0 181--211, August
  1999.
\newblock ISSN 0004-3702.
\newblock \doi{10.1016/S0004-3702(99)00052-1}.

\bibitem[Sutton et~al.(2011)Sutton, Modayil, Delp, Degris, Pilarski, White, and
  Precup]{Sutton:2011}
Richard~S. Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick~M.
  Pilarski, Adam White, and Doina Precup.
\newblock Horde: A scalable real-time architecture for learning knowledge from
  unsupervised sensorimotor interaction.
\newblock \emph{Proc. of 10th Int. Conf. on Autonomous Agents and Multiagent
  Systems (AAMAS 2011)}, 2011.

\bibitem[Sutton et~al.(2014)Sutton, Mahmood, Precup, and van
  Hasselt]{Sutton:2014}
Richard~S. Sutton, Ashique~Rupam Mahmood, Doina Precup, and Hado van Hasselt.
\newblock A new q($\lambda$) with interim forward view and {Monte Carlo}
  equivalence.
\newblock In \emph{ICML}, 2014.

\bibitem[Sutton et~al.(2018)Sutton, Barto, and Bach]{sutton-barto18}
Richard~S. Sutton, Andrew~G. Barto, and Francis Bach.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock MIT press, 2018.

\bibitem[Tesauro(1995)]{Tesauro:1995}
Gerald Tesauro.
\newblock Temporal difference learning and td-gammon.
\newblock \emph{Commun. ACM}, 38\penalty0 (3):\penalty0 58--68, March 1995.
\newblock ISSN 0001-0782.
\newblock \doi{10.1145/203330.203343}.

\bibitem[Tolman(1948)]{Tolman:1948}
Edward~C. Tolman.
\newblock Cognitive maps in rats and men.
\newblock \emph{The Psychological Review}, 1948.

\bibitem[{van Hasselt} et~al.(2016){van Hasselt}, Guez, and
  Silver]{vanHasselt:2016}
Hado {van Hasselt}, Arthur Guez, and David Silver.
\newblock Deep reinforcement learning with double {Q}-learning.
\newblock In \emph{AAAI}, 2016.

\bibitem[Wang et~al.(2017)Wang, Bapst, Heess, Mnih, Munos, Kavukcuoglu, and
  de~Freitas]{Ziyu2017}
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray
  Kavukcuoglu, and Nando de~Freitas.
\newblock Sample efficient actor-critic with experience replay.
\newblock In \emph{ICLR}, 2017.

\bibitem[Williams(1992)]{Williams1992}
Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{ML}, 1992.

\bibitem[Xu et~al.(2018)Xu, van Hasselt, and Silver]{Xu:2018}
Zhongwen Xu, Hado~P van Hasselt, and David Silver.
\newblock Meta-gradient reinforcement learning.
\newblock In \emph{NIPS}. 2018.

\bibitem[Zhang \& Sutton(2017)Zhang and Sutton]{Zhang:2018}
Shangtong Zhang and Richard~S. Sutton.
\newblock A deeper look at experience replay.
\newblock \emph{Arxiv}, abs/1712.01275, 2017.

\end{thebibliography}
