\begin{thebibliography}{10}

\bibitem{bengio2013estimating}
Yoshua Bengio, Nicholas L{\'e}onard, and Aaron Courville.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock {\em arXiv preprint arXiv:1308.3432}, 2013.

\bibitem{chmiel2022optimal}
Brian Chmiel, Itay Hubara, Ron Banner, and Daniel Soudry.
\newblock Optimal fine-grained n: M sparsity for activations and neural
  gradients.
\newblock {\em arXiv preprint arXiv:2203.10991}, 2022.

\bibitem{croitoru2019unsupervised}
Ioana Croitoru, Simion-Vlad Bogolin, and Marius Leordeanu.
\newblock Unsupervised learning of foreground object segmentation.
\newblock {\em International Journal of Computer Vision (IJCV)},
  127(9):1279--1302, 2019.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 248--255, 2009.

\bibitem{ding2019approximated}
Xiaohan Ding, Guiguang Ding, Yuchen Guo, Jungong Han, and Chenggang Yan.
\newblock Approximated oracle filter pruning for destructive cnn width
  optimization.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  1607--1616. PMLR, 2019.

\bibitem{ding2019global}
Xiaohan Ding, Xiangxin Zhou, Yuchen Guo, Jungong Han, Ji~Liu, et~al.
\newblock Global sparse momentum sgd for pruning very deep neural networks.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 6382--6394, 2019.

\bibitem{evci2020rigging}
Utku Evci, Trevor Gale, Jacob Menick, Pablo~Samuel Castro, and Erich Elsen.
\newblock Rigging the lottery: Making all tickets winners.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  2943--2952, 2020.

\bibitem{girshick2015fast}
Ross Girshick.
\newblock Fast r-cnn.
\newblock In {\em IEEE International Conference on Computer Vision (ICCV)},
  pages 1440--1448, 2015.

\bibitem{girshick2014rich}
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.
\newblock Rich feature hierarchies for accurate object detection and semantic
  segmentation.
\newblock In {\em IEEE International Conference on Computer Vision (ICCV)},
  pages 580--587, 2014.

\bibitem{han2015learning}
Song Han, Jeff Pool, John Tran, and William Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 1135--1143, 2015.

\bibitem{he2017mask}
Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick.
\newblock Mask r-cnn.
\newblock In {\em IEEE International Conference on Computer Vision (ICCV)},
  2017.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 770--778, 2016.

\bibitem{he2018soft}
Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi~Yang.
\newblock Soft filter pruning for accelerating deep convolutional neural
  networks.
\newblock In {\em International Joint Conference on Artificial Intelligence
  (IJCAI)}, pages 2234--2240, 2018.

\bibitem{he2019filter}
Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi~Yang.
\newblock Filter pruning via geometric median for deep convolutional neural
  networks acceleration.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 4340--4349, 2019.

\bibitem{hubara2021accelerated}
Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph Naor, and Daniel
  Soudry.
\newblock Accelerated sparse neural training: A provable and efficient method
  to find n: m transposable masks.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  34, 2021.

\bibitem{jayakumar2020top}
Siddhant Jayakumar, Razvan Pascanu, Jack Rae, Simon Osindero, and Erich Elsen.
\newblock Top-kast: Top-k always sparse training.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 20744--20754, 2020.

\bibitem{kusupati2020soft}
Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek
  Jain, Sham Kakade, and Ali Farhadi.
\newblock Soft threshold weight reparameterization for learnable sparsity.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  5544--5555, 2020.

\bibitem{lecun1989optimal}
Yann LeCun, John Denker, and Sara Solla.
\newblock Optimal brain damage.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 598--605, 1989.

\bibitem{lee2018snip}
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{lin2020hrank}
Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong
  Tian, and Ling Shao.
\newblock Hrank: Filter pruning using high-rank feature map.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 1529--1538, 2020.

\bibitem{lin2020channel}
Mingbao Lin, Rongrong Ji, Yuxin Zhang, Baochang Zhang, Yongjian Wu, and
  Yonghong Tian.
\newblock Channel pruning via automatic structure search.
\newblock In {\em International Joint Conference on Artificial Intelligence
  (IJCAI)}, pages 673--679, 2020.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em European Conference on Computer Vision (ECCV)}, pages
  740--755, 2014.

\bibitem{liu2021sparse}
Shiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi, Lu~Yin, Huanyu Kou,
  Li~Shen, Mykola Pechenizkiy, Zhangyang Wang, and Decebal~Constantin Mocanu.
\newblock Sparse training via boosting pruning plasticity with
  neuroregeneration.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem{liu2019metapruning}
Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Tim Kwang-Ting
  Cheng, and Jian Sun.
\newblock Metapruning: Meta learning for automatic neural network channel
  pruning.
\newblock In {\em IEEE International Conference on Computer Vision (ICCV)},
  pages 3296--3305, 2019.

\bibitem{louizos2017learning}
Christos Louizos, Max Welling, and Diederik~P Kingma.
\newblock Learning sparse neural networks through $ l\_0 $ regularization.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2017.

\bibitem{luo2020autopruner}
Jian-Hao Luo and Jianxin Wu.
\newblock Autopruner: An end-to-end trainable filter pruning method for
  efficient deep model inference.
\newblock {\em Pattern Recognition (PR)}, page 107461, 2020.

\bibitem{luo2017thinet}
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin.
\newblock Thinet: A filter level pruning method for deep neural network
  compression.
\newblock In {\em IEEE International Conference on Computer Vision (ICCV)},
  pages 5058--5066, 2017.

\bibitem{molchanov2016pruning}
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2017.

\bibitem{ning2020dsa}
Xuefei Ning, Tianchen Zhao, Wenshuo Li, Peng Lei, Yu~Wang, and Huazhong Yang.
\newblock Dsa: More efficient budgeted pruning via differentiable sparsity
  allocation.
\newblock In {\em European Conference on Computer Vision (ECCV)}, pages
  592--607. Springer, 2020.

\bibitem{nvidia2020a100}
Nvidia.
\newblock Nvidia a100 tensor core gpu architecture.
\newblock \url{https://www.nvidia.com/content/dam/en-
  zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf}, 2020.

\bibitem{pytorch2015}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 8026--8037, 2019.

\bibitem{pool2021channel}
Jeff Pool and Chong Yu.
\newblock Channel permutations for n: M sparsity.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  34, 2021.

\bibitem{ronny2020nvidia}
Olivier Giroux et~al. Ronny~Krashinsky.
\newblock Nvidia ampere sparse tensor core.
\newblock \url{https://
  developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/}, 2020.

\bibitem{simonyan2015very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2015.

\bibitem{sun2021dominosearch}
Wei Sun, Aojun Zhou, Sander Stuijk, Rob Wijnhoven, Andrew~O Nelson, Henk
  Corporaal, et~al.
\newblock Dominosearch: Find layer-wise fine-grained n: M sparse schemes from
  dense neural networks.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  34, 2021.

\bibitem{touvron2021training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  10347--10357, 2021.

\bibitem{wang2020sparse}
Ziheng Wang.
\newblock Sparsert: Accelerating unstructured sparsity on gpus for deep
  learning inference.
\newblock In {\em Proceedings of the ACM International Conference on Parallel
  Architectures and Compilation Techniques (ICPACT)}, pages 31--42.

\bibitem{rw2019timm}
Ross Wightman.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem{zhang2022carrying}
Yuxin Zhang, Mingbao Lin, Chia-Wen Lin, Jie Chen, Yongjian Wu, Yonghong Tian,
  and Rongrong Ji.
\newblock Carrying out cnn channel pruning in a white box.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS)}, 2022.

\bibitem{zhou2021learning}
Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu
  Sun, and Hongsheng Li.
\newblock Learning n: M fine-grained structured sparse neural networks from
  scratch.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2021.

\bibitem{zhou2017incremental}
Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen.
\newblock Incremental network quantization: Towards lossless cnns with
  low-precision weights.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2017.

\bibitem{zhu2017prune}
Michael Zhu and Suyog Gupta.
\newblock To prune, or not to prune: exploring the efficacy of pruning for
  model compression.
\newblock In {\em International Conference on Learning Representations Workshop
  (ICLRW)}, 2017.

\end{thebibliography}
