\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2016)Agarwal, Allen-Zhu, Bullins, Hazan, and
  Ma]{agarwal2016finding}
N.~Agarwal, Z.~Allen-Zhu, B.~Bullins, E.~Hazan, and T.~Ma.
\newblock Finding approximate local minima for nonconvex optimization in linear
  time.
\newblock \emph{arXiv preprint arXiv:1611.01146}, 2016.

\bibitem[Beaton and Tukey(1974)]{beaton1974fitting}
A.~E. Beaton and J.~W. Tukey.
\newblock The fitting of power series, meaning polynomials, illustrated on
  band-spectroscopic data.
\newblock \emph{Technometrics}, 16\penalty0 (2):\penalty0 147--185, 1974.

\bibitem[Beck and Teboulle(2009)]{beck2009gradient}
A.~Beck and M.~Teboulle.
\newblock Gradient-based algorithms with applications to signal recovery.
\newblock \emph{Convex optimization in signal processing and communications},
  pages 42--88, 2009.

\bibitem[Bubeck(2014)]{bubeck2014convex}
S.~Bubeck.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{arXiv preprint arXiv:1405.4980}, 2014.

\bibitem[Cand{\`e}s et~al.(2015)Cand{\`e}s, Li, and
  Soltanolkotabi]{CandesLiSo15}
E.~J. Cand{\`e}s, X.~Li, and M.~Soltanolkotabi.
\newblock Phase retrieval via {W}irtinger flow: Theory and algorithms.
\newblock \emph{IEEE Transactions on Information Theory}, 61\penalty0
  (4):\penalty0 1985--2007, 2015.

\bibitem[Carmon et~al.(2016)Carmon, Duchi, Hinder, and
  Sidford]{carmon2016accelerated}
Y.~Carmon, J.~C. Duchi, O.~Hinder, and A.~Sidford.
\newblock Accelerated methods for non-convex optimization.
\newblock \emph{arXiv preprint arXiv:1611.00756}, 2016.

\bibitem[Cartis et~al.(2010)Cartis, Gould, and Toint]{cartis2010complexity}
C.~Cartis, N.~I. Gould, and P.~L. Toint.
\newblock On the complexity of steepest descent, {N}ewton's and regularized
  {N}ewton's methods for nonconvex unconstrained optimization problems.
\newblock \emph{{SIAM} journal on optimization}, 20\penalty0 (6):\penalty0
  2833--2852, 2010.

\bibitem[Glorot and Bengio(2010)]{glorot2010understanding}
X.~Glorot and Y.~Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Aistats}, volume~9, pages 249--256, 2010.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Vinyals, and
  Saxe]{goodfellow2014qualitatively}
I.~J. Goodfellow, O.~Vinyals, and A.~M. Saxe.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Hager and Zhang(2006)]{hager2006survey}
W.~W. Hager and H.~Zhang.
\newblock A survey of nonlinear conjugate gradient methods.
\newblock \emph{Pacific Journal of Optimization}, 2\penalty0 (1):\penalty0
  35--58, 2006.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
D.~Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Koren et~al.(2009)Koren, Bell, and Volinsky]{koren2009matrix}
Y.~Koren, R.~Bell, and C.~Volinsky.
\newblock Matrix factorization techniques for recommender systems.
\newblock \emph{Computer}, 42\penalty0 (8), 2009.

\bibitem[Le{C}un et~al.(1998)Le{C}un, Cortes, and Burges]{lecun1998mnist}
Y.~Le{C}un, C.~Cortes, and C.~J. Burges.
\newblock The {MNIST} database of handwritten digits, 1998.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
Y.~LeCun, Y.~Bengio, and G.~Hinton.
\newblock Deep learning.
\newblock \emph{Nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Lee et~al.(2016)Lee, Simchowitz, Jordan, and Recht]{lee2016gradient}
J.~D. Lee, M.~Simchowitz, M.~I. Jordan, and B.~Recht.
\newblock Gradient descent only converges to minimizers.
\newblock In \emph{29th Annual Conference on Learning Theory (COLT)}, pages
  1246---1257, 2016.

\bibitem[Liu and Nocedal(1989)]{liu1989limited}
D.~C. Liu and J.~Nocedal.
\newblock On the limited memory {BFGS} method for large scale optimization.
\newblock \emph{Mathematical Programming}, 45\penalty0 (1):\penalty0 503--528,
  1989.

\bibitem[Mairal et~al.(2008)Mairal, Bach, Ponce, Sapiro, and
  Zisserman]{MairalBaPoSaZi08}
J.~Mairal, F.~Bach, J.~Ponce, G.~Sapiro, and A.~Zisserman.
\newblock Supervised dictionary learning.
\newblock In \emph{Advances in Neural Information Processing Systems 21}, 2008.

\bibitem[Murty and Kabadi(1987)]{MurtyKa87}
K.~Murty and S.~Kabadi.
\newblock Some {NP}-complete problems in quadratic and nonlinear programming.
\newblock \emph{Mathematical Programming}, 39:\penalty0 117--129, 1987.

\bibitem[Nemirovski(1999)]{Nemirovski1999}
A.~Nemirovski.
\newblock Optimization {II}: Standard numerical methods for nonlinear
  continuous optimization.
\newblock Technion -- Israel Institute of Technology, 1999.
\newblock URL \url{http://www2.isye.gatech.edu/~nemirovs/Lect_OptII.pdf}.

\bibitem[Nemirovski and Yudin(1983)]{NemirovskiYu83}
A.~Nemirovski and D.~Yudin.
\newblock \emph{Problem Complexity and Method Efficiency in Optimization}.
\newblock Wiley, 1983.

\bibitem[Nesterov(1983)]{nesterov1983method}
Y.~Nesterov.
\newblock A method of solving a convex programming problem with convergence
  rate ${O}(1/k^2)$.
\newblock \emph{Soviet Mathematics Doklady}, 27\penalty0 (2):\penalty0
  372--376, 1983.

\bibitem[Nesterov(2000)]{Nesterov00}
Y.~Nesterov.
\newblock Squared functional systems and optimization problems.
\newblock In \emph{High Performance Optimization}, volume~33 of \emph{Applied
  Optimization}, pages 405--440. Springer, 2000.

\bibitem[Nesterov(2004)]{Nesterov04}
Y.~Nesterov.
\newblock \emph{Introductory Lectures on Convex Optimization}.
\newblock Kluwer Academic Publishers, 2004.

\bibitem[Nesterov(2012)]{NesterovGradSmall2012}
Y.~Nesterov.
\newblock How to make the gradients small.
\newblock \emph{Optima 88}, 2012.

\bibitem[Nesterov and Polyak(2006)]{nesterov2006cubic}
Y.~Nesterov and B.~T. Polyak.
\newblock Cubic regularization of {N}ewton method and its global performance.
\newblock \emph{Mathematical Programming}, 108\penalty0 (1):\penalty0 177--205,
  2006.

\bibitem[Nocedal and Wright(2006)]{NocedalWr06}
J.~Nocedal and S.~J. Wright.
\newblock \emph{Numerical Optimization}.
\newblock Springer, 2006.

\bibitem[O'Donoghue and Cand{\`e}s(2015)]{o2015adaptive}
B.~O'Donoghue and E.~Cand{\`e}s.
\newblock Adaptive restart for accelerated gradient schemes.
\newblock \emph{Foundations of Computational Mathematics}, 15\penalty0
  (3):\penalty0 715--732, 2015.

\bibitem[Polak and Ribi{\`e}re(1969)]{polaknote}
E.~Polak and G.~Ribi{\`e}re.
\newblock Note sur la convergence de directions conjug{\'e}es.
\newblock \emph{Rev. Fr. Inform. Rech. Oper. v16}, pages 35--43, 1969.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and
  Williams]{RumelhartHiWi86}
D.~E. Rumelhart, G.~E. Hinton, and R.~J. Williams.
\newblock Learning internal representations by error propagation.
\newblock In D.~E. Rumelhart and J.~L. McClelland, editors, \emph{Parallel
  Distributed Processing -- Explorations in the Microstructure of Cognition},
  chapter~8, pages 318--362. MIT Press, 1986.

\bibitem[Wang et~al.(2016)Wang, Giannakis, and Eldar]{WangGiEl16}
G.~Wang, G.~B. Giannakis, and Y.~C. Eldar.
\newblock Solving systems of random quadratic equations via truncated amplitude
  flow.
\newblock \emph{arXiv:1605.08285 [stat.ML]}, 2016.

\end{thebibliography}
