@article{kaushik2020learning,
  title={Learning the Difference that Makes a Difference with Counterfactually Augmented Data},
  author={Kaushik, Divyansh and Hovy, Eduard and Lipton, Zachary C},
  journal={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@article{kaushik2021learning,
  title={Explaining the Efficacy of Counterfactually Augmented Data},
  author={Kaushik, Divyansh and Setlur, Amrith and Hovy, Eduard and Lipton, Zachary C},
  journal={International Conference on Learning Representations (ICLR)},
  year={2021}
}

@article{peters2016,
author = {Peters, Jonas and Bühlmann, Peter and Meinshausen, Nicolai},
title = {Causal inference by using invariant prediction: identification and confidence intervals},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {78},
number = {5},
pages = {947-1012},
keywords = {Causal discovery, Causal inference, Confidence intervals, Invariant prediction},
doi = {https://doi.org/10.1111/rssb.12167},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12167},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12167},
year = {2016}
}

@inproceedings{jia-liang-2017-adversarial,
    title = "Adversarial Examples for Evaluating Reading Comprehension Systems",
    author = "Jia, Robin  and
      Liang, Percy",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1215",
    doi = "10.18653/v1/D17-1215",
    pages = "2021--2031",
    abstract = "Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75{\%} F1 score to 36{\%}; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7{\%}. We hope our insights will motivate the development of new models that understand language more precisely.",
}

@misc{jo2017measuring,
      title={Measuring the tendency of CNNs to Learn Surface Statistical Regularities}, 
      author={Jason Jo and Yoshua Bengio},
      year={2017},
      eprint={1711.11561},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{geirhos2019imagenettrained,
      title={ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness}, 
      author={Robert Geirhos and Patricia Rubisch and Claudio Michaelis and Matthias Bethge and Felix A. Wichmann and Wieland Brendel},
      year={2019},
      eprint={1811.12231},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{Beery_2018_ECCV,
author = {Beery, Sara and Van Horn, Grant and Perona, Pietro},
title = {Recognition in Terra Incognita},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@inproceedings{kaushik-lipton-2018-much,
    title = "How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks",
    author = "Kaushik, Divyansh  and
      Lipton, Zachary C.",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1546",
    doi = "10.18653/v1/D18-1546",
    pages = "5010--5015",
    abstract = "Many recent papers address reading comprehension, where examples consist of (question, passage, answer) tuples. Presumably, a model must combine information from both questions and passages to predict corresponding answers. However, despite intense interest in the topic, with hundreds of published papers vying for leaderboard dominance, basic questions about the difficulty of many popular benchmarks remain unanswered. In this paper, we establish sensible baselines for the bAbI, SQuAD, CBT, CNN, and Who-did-What datasets, finding that question- and passage-only models often perform surprisingly well. On 14 out of 20 bAbI tasks, passage-only models achieve greater than 50{\%} accuracy, sometimes matching the full model. Interestingly, while CBT provides 20-sentence passages, only the last is needed for accurate prediction. By comparison, SQuAD and CNN appear better-constructed.",
}

@inproceedings{poliak-etal-2018-hypothesis,
    title = "Hypothesis Only Baselines in Natural Language Inference",
    author = "Poliak, Adam  and
      Naradowsky, Jason  and
      Haldar, Aparajita  and
      Rudinger, Rachel  and
      Van Durme, Benjamin",
    booktitle = "Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/S18-2023",
    doi = "10.18653/v1/S18-2023",
    pages = "180--191",
    abstract = "We propose a hypothesis only baseline for diagnosing Natural Language Inference (NLI). Especially when an NLI dataset assumes inference is occurring based purely on the relationship between a context and a hypothesis, it follows that assessing entailment relations while ignoring the provided context is a degenerate solution. Yet, through experiments on 10 distinct NLI datasets, we find that this approach, which we refer to as a hypothesis-only model, is able to significantly outperform a majority-class baseline across a number of NLI datasets. Our analysis suggests that statistical irregularities may allow a model to perform NLI in some datasets beyond what should be achievable without access to the context.",
}

@inproceedings{objectHallucination, 
        title = {Object Hallucination in Image Captioning.}, 
        author = {Rohrbach, Anna and Hendricks, Lisa Anne and Burns, Kaylee and Darrell, Trevor and Saenko, Kate}, 
        booktitle = {Empirical Methods in Natural Language Processing (EMNLP)}, 
        year = {2018} 
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@misc{arjovsky2020invariant,
      title={Invariant Risk Minimization}, 
      author={Martin Arjovsky and Léon Bottou and Ishaan Gulrajani and David Lopez-Paz},
      year={2020},
      eprint={1907.02893},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@InProceedings{pmlr-v119-filos20a, title = {Can Autonomous Vehicles Identify, Recover From, and Adapt to Distribution Shifts?}, author = {Filos, Angelos and Tigkas, Panagiotis and Mcallister, Rowan and Rhinehart, Nicholas and Levine, Sergey and Gal, Yarin}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {3145--3153}, year = {2020}, editor = {Hal Daumé III and Aarti Singh}, volume = {119}, series = {Proceedings of Machine Learning Research}, month = {13--18 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v119/filos20a/filos20a.pdf}, url = { http://proceedings.mlr.press/v119/filos20a.html }, abstract = {Out-of-training-distribution (OOD) scenarios are a common challenge of learning agents at deployment, typically leading to arbitrary deductions and poorly-informed decisions. In principle, detection of and adaptation to OOD scenes can mitigate their adverse effects. In this paper, we highlight the limitations of current approaches to novel driving scenes and propose an epistemic uncertainty-aware planning method, called \emph{robust imitative planning} (RIP). Our method can detect and recover from some distribution shifts, reducing the overconfident and catastrophic extrapolations in OOD scenes. If the model’s uncertainty is too great to suggest a safe course of action, the model can instead query the expert driver for feedback, enabling sample-efficient online adaptation, a variant of our method we term \emph{adaptive robust imitative planning} (AdaRIP). Our methods outperform current state-of-the-art approaches in the nuScenes \emph{prediction} challenge, but since no benchmark evaluating OOD detection and adaption currently exists to assess \emph{control}, we introduce an autonomous car novel-scene benchmark, \texttt{CARNOVEL}, to evaluate the robustness of driving agents to a suite of tasks with distribution shifts, where our methods outperform all the baselines.} }

@book{quinonero2009dataset,
  title={Dataset shift in machine learning},
  author={Qui{\~n}onero-Candela, Joaquin and Sugiyama, Masashi and Lawrence, Neil D and Schwaighofer, Anton},
  year={2009},
  publisher={Mit Press}
}

@inproceedings{GrgicHlaca2018BeyondDF,
  title={Beyond Distributive Fairness in Algorithmic Decision Making: Feature Selection for Procedurally Fair Learning},
  author={Nina Grgic-Hlaca and M. Zafar and K. Gummadi and Adrian Weller},
  booktitle={AAAI},
  year={2018}
}

@inproceedings{Ovadia2019CanYT,
  title={Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift},
  author={Yaniv Ovadia and E. Fertig and J. Ren and Zachary Nado and D. Sculley and S. Nowozin and Joshua V. Dillon and Balaji Lakshminarayanan and Jasper Snoek},
  booktitle={NeurIPS},
  year={2019}
}

@misc{szegedy2014intriguing,
      title={Intriguing properties of neural networks}, 
      author={Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
      year={2014},
      eprint={1312.6199},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{Peters2015CausalIU,
  title={Causal inference using invariant prediction: identification and confidence intervals},
  author={J. Peters and Peter Buhlmann and N. Meinshausen},
  journal={arXiv: Methodology},
  year={2015}
}

@inproceedings{Lipton2018DoesMM,
  title={Does mitigating ML's impact disparity require treatment disparity?},
  author={Zachary Chase Lipton and Julian McAuley and A. Chouldechova},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{chang2020invariant,
  title={Invariant rationalization},
  author={Chang, Shiyu and Zhang, Yang and Yu, Mo and Jaakkola, Tommi},
  booktitle={International Conference on Machine Learning},
  pages={1448--1458},
  year={2020},
  organization={PMLR}
}

@book{pearl2009causality,
  title={Causality},
  author={Pearl, Judea},
  year={2009},
  publisher={Cambridge university press}
}

@article{teney2020learning,
  title={Learning what makes a difference from counterfactual examples and gradient supervision},
  author={Teney, Damien and Abbasnedjad, Ehsan and Hengel, Anton van den},
  journal={arXiv preprint arXiv:2004.09034},
  year={2020}
}

@inproceedings{liang-etal-2020-learning,
    title = "Learning to Contrast the Counterfactual Samples for Robust Visual Question Answering",
    author = "Liang, Zujie  and
      Jiang, Weitao  and
      Hu, Haifeng  and
      Zhu, Jiaying",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.265",
    doi = "10.18653/v1/2020.emnlp-main.265",
    pages = "3285--3292",
    abstract = "In the task of Visual Question Answering (VQA), most state-of-the-art models tend to learn spurious correlations in the training set and achieve poor performance in out-of-distribution test data. Some methods of generating counterfactual samples have been proposed to alleviate this problem. However, the counterfactual samples generated by most previous methods are simply added to the training data for augmentation and are not fully utilized. Therefore, we introduce a novel self-supervised contrastive learning mechanism to learn the relationship between original samples, factual samples and counterfactual samples. With the better cross-modal joint embeddings learned from the auxiliary training objective, the reasoning capability and robustness of the VQA model are boosted significantly. We evaluate the effectiveness of our method by surpassing current state-of-the-art models on the VQA-CP dataset, a diagnostic benchmark for assessing the VQA model{'}s robustness.",
}

@article{srivastava2020robustness,
  title={Robustness to Spurious Correlations via Human Annotations},
  author={Srivastava, Megha and Hashimoto, Tatsunori and Liang, Percy},
  journal={International Conference on Machine Learning (ICML)},
  year={2020}
}

@inproceedings{ghassami2017learning,
  title={Learning causal structures using regression invariance},
  author={Ghassami, AmirEmad and Salehkaleybar, Saber and Kiyavash, Negar and Zhang, Kun},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2017}
}

@article{lu2018gender,
  title={Gender bias in neural natural language processing},
  author={Lu, Kaiji and Mardziel, Piotr and Wu, Fangjing and Amancharla, Preetam and Datta, Anupam},
  journal={arXiv preprint arXiv:1807.11714},
  year={2018}
}

@inproceedings{zmigrod-etal-2019-counterfactual,
    title = "Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology",
    author = "Zmigrod, Ran  and
      Mielke, Sebastian J.  and
      Wallach, Hanna  and
      Cotterell, Ryan",
    booktitle = "Association for Computational Linguistics (ACL)",
    year = "2019",
}

@article{maudslay2019s,
  title={It's All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution},
  author={Maudslay, Rowan Hall and Gonen, Hila and Cotterell, Ryan and Teufel, Simone},
  journal={arXiv preprint arXiv:1909.00871},
  year={2019}
}

@inproceedings{shalit2017estimating,
  title={Estimating individual treatment effect: generalization bounds and algorithms},
  author={Shalit, Uri and Johansson, Fredrik D and Sontag, David},
  booktitle={International Conference on Machine Learning},
  pages={3076--3085},
  year={2017},
  organization={PMLR}
}

@article{bowman2015large,
  title={A large annotated corpus for learning natural language inference},
  author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},
  journal={arXiv preprint arXiv:1508.05326},
  year={2015}
}

@article{williams2017broad,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1704.05426},
  year={2017}
}

@inproceedings{you2016image,
  title={Image captioning with semantic attention},
  author={You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4651--4659},
  year={2016}
}

@inproceedings{shpitser16identification,
author = {Shpitser, Ilya and Pearl, Judea},
title = {Identification of Conditional Interventional Distributions},
year = {2006},
isbn = {0974903922},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {The subject of this paper is the elucidation of effects of actions from causal assumptions represented as a directed graph, and statistical knowledge given as a probability distribution. In particular, we are interested in predicting distributions on post-action outcomes given a set of measurements. We provide a necessary and sufficient graphical condition for the cases where such distributions can be uniquely computed from the available information, as well as an algorithm which performs this computation whenever the condition holds. Furthermore, we use our results to prove completeness of do-calculus [Pearl, 1995] for the same identification problem, and show applications to sequential decision making.},
booktitle = {Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence},
pages = {437–444},
numpages = {8},
location = {Cambridge, MA, USA},
series = {UAI'06}
}

@article{chen2015microsoft,
  title={Microsoft coco captions: Data collection and evaluation server},
  author={Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  journal={arXiv preprint arXiv:1504.00325},
  year={2015}
}

@article{pearl2017detecting,
  title={Detecting latent heterogeneity},
  author={Pearl, Judea},
  journal={Sociological Methods \& Research},
  volume={46},
  number={3},
  pages={370--389},
  year={2017},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@inproceedings{hassanpour2019learning,
  title={Learning disentangled representations for counterfactual regression},
  author={Hassanpour, Negar and Greiner, Russell},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{swaminathan2015batch,
  title={Batch learning from logged bandit feedback through counterfactual risk minimization},
  author={Swaminathan, Adith and Joachims, Thorsten},
  journal={The Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={1731--1755},
  year={2015},
  publisher={JMLR. org}
}

@inproceedings{swaminathan2015counterfactual,
  title={Counterfactual risk minimization: Learning from logged bandit feedback},
  author={Swaminathan, Adith and Joachims, Thorsten},
  booktitle={International Conference on Machine Learning},
  pages={814--823},
  year={2015},
  organization={PMLR}
}

@inproceedings{swaminathan2015self,
  title={The self-normalized estimator for counterfactual learning},
  author={Swaminathan, Adith and Joachims, Thorsten},
  booktitle={advances in neural information processing systems},
  pages={3231--3239},
  year={2015},
  organization={Citeseer}
}

@article{austin2011introduction,
  title={An introduction to propensity score methods for reducing the effects of confounding in observational studies},
  author={Austin, Peter C},
  journal={Multivariate behavioral research},
  volume={46},
  number={3},
  pages={399--424},
  year={2011},
  publisher={Taylor \& Francis}
}

@article{greenland1999causal,
  title={Causal diagrams for epidemiologic research},
  author={Greenland, Sander and Pearl, Judea and Robins, James M},
  journal={Epidemiology},
  pages={37--48},
  year={1999},
  publisher={JSTOR}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@InProceedings{pmlr-v37-rezende15, 
title = {Variational Inference with Normalizing Flows}, 
author = {Rezende, Danilo and Mohamed, Shakir}, 
booktitle = {Proceedings of the 32nd International Conference on Machine Learning}, 
pages = {1530--1538}, 
year = {2015}, 
editor = {Bach, Francis and Blei, David}, 
volume = {37}, 
series = {Proceedings of Machine Learning Research}, 
address = {Lille, France}, 
month = {07--09 Jul}, 
publisher = {PMLR}, 
pdf = {http://proceedings.mlr.press/v37/rezende15.pdf}, 
url = { http://proceedings.mlr.press/v37/rezende15.html }, 
abstract = {The choice of the approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.} }

@article{tran2019discrete,
  title={Discrete Flows: Invertible Generative Models of Discrete Data},
  author={Tran, Dustin and Vafa, Keyon and Agrawal, Kumar and Dinh, Laurent and Poole, Ben},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={14719--14728},
  year={2019}
}

@article{sriperumbudur2012empirical,
  title={On the empirical estimation of integral probability metrics},
  author={Sriperumbudur, Bharath K and Fukumizu, Kenji and Gretton, Arthur and Sch{\"o}lkopf, Bernhard and Lanckriet, Gert RG and others},
  journal={Electronic Journal of Statistics},
  volume={6},
  pages={1550--1599},
  year={2012},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}

@article{muller1997integral,
  title={Integral probability metrics and their generating classes of functions},
  author={M{\"u}ller, Alfred},
  journal={Advances in Applied Probability},
  pages={429--443},
  year={1997},
  publisher={JSTOR}
}

@book{villani2008optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric},
  volume={338},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@inproceedings{cuturi2014fast,
  title={Fast computation of Wasserstein barycenters},
  author={Cuturi, Marco and Doucet, Arnaud},
  booktitle={International conference on machine learning},
  pages={685--693},
  year={2014},
  organization={PMLR}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@article{jang2016categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}

@inproceedings{girshick2015fast,
  title={Fast r-cnn},
  author={Girshick, Ross},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1440--1448},
  year={2015}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@inproceedings{lewis-etal-2020-bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}

@InProceedings{N18-1101,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for 
           Sentence Understanding through Inference",
  booktitle = "Proceedings of the 2018 Conference of 
               the North American Chapter of the 
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  pages = "1112--1122",
  location = "New Orleans, Louisiana",
  url = "http://aclweb.org/anthology/N18-1101"
}

@inproceedings{nie-etal-2020-adversarial,
    title = "Adversarial {NLI}: A New Benchmark for Natural Language Understanding",
    author = "Nie, Yixin  and
      Williams, Adina  and
      Dinan, Emily  and
      Bansal, Mohit  and
      Weston, Jason  and
      Kiela, Douwe",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    year = "2020",
    publisher = "Association for Computational Linguistics",
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{karpathy2015deep,
  title={Deep visual-semantic alignments for generating image descriptions},
  author={Karpathy, Andrej and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3128--3137},
  year={2015}
}

@inproceedings{rohrbach2018object,
  title={Object Hallucination in Image Captioning},
  author={Rohrbach, Anna and Hendricks, Lisa Anne and Burns, Kaylee and Darrell, Trevor and Saenko, Kate},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={4035--4045},
  year={2018}
}

@inproceedings{tan2019lxmert,
  title={LXMERT: Learning Cross-Modality Encoder Representations from Transformers},
  author={Tan, Hao and Bansal, Mohit},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={5103--5114},
  year={2019}
}

@inproceedings{banerjee2005meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages={65--72},
  year={2005}
}

@inproceedings{anderson2016spice,
  title={Spice: Semantic propositional image caption evaluation},
  author={Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
  booktitle={European conference on computer vision},
  pages={382--398},
  year={2016},
  organization={Springer}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@inproceedings{vedantam2015cider,
  title={Cider: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4566--4575},
  year={2015}
}

@article{pitis2020counterfactual,
  title={Counterfactual Data Augmentation using Locally Factored Dynamics},
  author={Pitis, Silviu and Creager, Elliot and Garg, Animesh},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@inproceedings{ijcai2020-124,
  author    = {Yubo Zhang and
               Hao Tan and
               Mohit Bansal},
  editor    = {Christian Bessiere},
  title     = {Diagnosing the Environment Bias in Vision-and-Language Navigation},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI} 2020},
  pages     = {890--897},
  publisher = {ijcai.org},
  year      = {2020},
  url       = {https://doi.org/10.24963/ijcai.2020/124},
  doi       = {10.24963/ijcai.2020/124},
  timestamp = {Mon, 20 Jul 2020 12:38:52 +0200},
  biburl    = {https://dblp.org/rec/conf/ijcai/ZhangTB20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{madaan2020politeness,
  title={Politeness Transfer: A Tag and Generate Approach},
  author={Madaan, Aman and Setlur, Amrith and Parekh, Tanmay and Poczos, Barnabas and Neubig, Graham and Yang, Yiming and Salakhutdinov, Ruslan and Black, Alan W and Prabhumoye, Shrimai},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={1869--1881},
  year={2020}
}

@article{gatys2016neural,
  title={A Neural Algorithm of Artistic Style},
  author={Gatys, Leon and Ecker, Alexander and Bethge, Matthias},
  journal={Journal of Vision},
  volume={16},
  number={12},
  pages={326--326},
  year={2016},
  publisher={The Association for Research in Vision and Ophthalmology}
}

@InProceedings{paws2019naacl,
  title = {{PAWS: Paraphrase Adversaries from Word Scrambling}},
  author = {Zhang, Yuan and Baldridge, Jason and He, Luheng},
  booktitle = {Proc. of NAACL},
  year = {2019}
}

@inproceedings{
khalifa2021a,
title={A Distributional Approach to Controlled Text Generation},
author={Muhammad Khalifa and Hady Elsahar and Marc Dymetman},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=jWkw45-9AbL}
}

@inproceedings{DBLP:conf/icml/HuYLSX17,
  author={Zhiting Hu and Zichao Yang and Xiaodan Liang and Ruslan Salakhutdinov and Eric P. Xing},
  title={Toward Controlled Generation of Text},
  year={2017},
  cdate={1483228800000},
  pages={1587-1596},
  url={http://proceedings.mlr.press/v70/hu17e.html},
  booktitle={ICML},
}

@article{mirza2014conditional,
  title={Conditional generative adversarial nets},
  author={Mirza, Mehdi and Osindero, Simon},
  journal={arXiv preprint arXiv:1411.1784},
  year={2014}
}

@article{chen2021human,
  title={Human-like Controllable Image Captioning with Verb-specific Semantic Roles},
  author={Chen, Long and Jiang, Zhihong and Xiao, Jun and Liu, Wei},
  journal={arXiv preprint arXiv:2103.12204},
  year={2021}
}

@article{deng2020length,
  title={Length-Controllable Image Captioning},
  author={Deng, Chaorui and Ding, Ning and Tan, Mingkui and Wu, Qi},
  journal={arXiv preprint arXiv:2007.09580},
  year={2020}
}

@inproceedings{lan2018toolkit,
  author     = {Lan, Wuwei and Xu, Wei},
  title      = {Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering},
  booktitle  = {Proceedings of COLING 2018},
  year       = {2018}
} 

@inproceedings{suhr-etal-2017-corpus,
    title = "A Corpus of Natural Language for Visual Reasoning",
    author = "Suhr, Alane  and
      Lewis, Mike  and
      Yeh, James  and
      Artzi, Yoav",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-2034",
    doi = "10.18653/v1/P17-2034",
    pages = "217--223",
    abstract = "We present a new visual reasoning language dataset, containing 92,244 pairs of examples of natural statements grounded in synthetic images with 3,962 unique sentences. We describe a method of crowdsourcing linguistically-diverse data, and present an analysis of our data. The data demonstrates a broad set of linguistic phenomena, requiring visual and set-theoretic reasoning. We experiment with various models, and show the data presents a strong challenge for future research.",
}

@article{pryzant2020causal,
  title={Causal Effects of Linguistic Properties},
  author={Pryzant, Reid and Card, Dallas and Jurafsky, Dan and Veitch, Victor and Sridhar, Dhanya},
  journal={arXiv preprint arXiv:2010.12919},
  year={2020}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}

@article{luo2018discriminability,
  title={Discriminability objective for training descriptive captions},
  author={Luo, Ruotian and Price, Brian and Cohen, Scott and Shakhnarovich, Gregory},
  journal={arXiv preprint arXiv:1803.04376},
  year={2018}
}

@inproceedings{NIPS2017_b2eeb736,
 author = {Li, Sheng and Fu, Yun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Matching on Balanced Nonlinear Representations for Treatment Effects Estimation},
 url = {https://proceedings.neurips.cc/paper/2017/file/b2eeb7362ef83deff5c7813a67e14f0a-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{johansson2016learning,
  title={Learning representations for counterfactual inference},
  author={Johansson, Fredrik and Shalit, Uri and Sontag, David},
  booktitle={International conference on machine learning},
  pages={3020--3029},
  year={2016},
  organization={PMLR}
}

@inproceedings{alaa2017bayesian,
  title={Bayesian inference of individualized treatment effects using multi-task Gaussian processes},
  author={Alaa, Ahmed M and van der Schaar, Mihaela},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={3427--3435},
  year={2017}
}

@inproceedings{yoon2018ganite,
  title={GANITE: Estimation of individualized treatment effects using generative adversarial nets},
  author={Yoon, Jinsung and Jordon, James and Van Der Schaar, Mihaela},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{NEURIPS2018_a50abba8,
 author = {Yao, Liuyi and Li, Sheng and Li, Yaliang and Huai, Mengdi and Gao, Jing and Zhang, Aidong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Representation Learning for Treatment Effect Estimation from Observational Data},
 url = {https://proceedings.neurips.cc/paper/2018/file/a50abba8132a77191791390c3eb19fe7-Paper.pdf},
 volume = {31},
 year = {2018}
}
