\begin{thebibliography}{10}

\bibitem{alamdari2023protein}
S.~Alamdari, N.~Thakkar, R.~van~den Berg, A.~X. Lu, N.~Fusi, A.~P. Amini, and K.~K. Yang.
\newblock Protein generation with evolutionary diffusion: sequence is all you need.
\newblock {\em bioRxiv}, pages 2023--09, 2023.

\bibitem{amari1998natural}
S.-I. Amari.
\newblock Natural gradient works efficiently in learning.
\newblock {\em Neural computation}, 10(2):251--276, 1998.

\bibitem{amari1998adaptive}
S.-i. Amari and A.~Cichocki.
\newblock Adaptive blind signal processing-neural network approaches.
\newblock {\em Proceedings of the IEEE}, 86(10):2026--2048, 1998.

\bibitem{amari2000methods}
S.-i. Amari and H.~Nagaoka.
\newblock {\em Methods of information geometry}, volume 191.
\newblock American Mathematical Soc., 2000.

\bibitem{atkinson1981rao}
C.~Atkinson and A.~F. Mitchell.
\newblock Rao's distance measure.
\newblock {\em Sankhy{\=a}: The Indian Journal of Statistics, Series A}, pages 345--365, 1981.

\bibitem{austin2021structured}
J.~Austin, D.~D. Johnson, J.~Ho, D.~Tarlow, and R.~Van Den~Berg.
\newblock Structured denoising diffusion models in discrete state-spaces.
\newblock {\em Advances in Neural Information Processing Systems}, 34:17981--17993, 2021.

\bibitem{avdeyev2023dirichlet}
P.~Avdeyev, C.~Shi, Y.~Tan, K.~Dudnyk, and J.~Zhou.
\newblock Dirichlet diffusion score model for biological sequence generation.
\newblock In {\em International Conference on Machine Learning}, pages 1276--1301. PMLR, 2023.

\bibitem{ay2017information}
N.~Ay, J.~Jost, H.~V{\^a}n~L{\^e}, and L.~Schwachh{\"o}fer.
\newblock {\em Information geometry}, volume~64.
\newblock Springer, 2017.

\bibitem{ben2022matching}
H.~Ben-Hamu, S.~Cohen, J.~Bose, B.~Amos, A.~Grover, M.~Nickel, R.~T. Chen, and Y.~Lipman.
\newblock Matching normalizing flows and probability paths on manifolds.
\newblock {\em arXiv preprint arXiv:2207.04711}, 2022.

\bibitem{bose2023se}
A.~J. Bose, T.~Akhound-Sadegh, K.~Fatras, G.~Huguet, J.~Rector-Brooks, C.-H. Liu, A.~C. Nica, M.~Korablyov, M.~Bronstein, and A.~Tong.
\newblock Se (3)-stochastic flow matching for protein backbone generation.
\newblock {\em arXiv preprint arXiv:2310.02391}, 2023.

\bibitem{campbell2022continuous}
A.~Campbell, J.~Benton, V.~De~Bortoli, T.~Rainforth, G.~Deligiannidis, and A.~Doucet.
\newblock A continuous time framework for discrete denoising models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:28266--28279, 2022.

\bibitem{campbell2024generative}
A.~Campbell, J.~Yim, R.~Barzilay, T.~Rainforth, and T.~Jaakkola.
\newblock Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design.
\newblock {\em arXiv preprint arXiv:2402.04997}, 2024.

\bibitem{chen2022sequence}
K.~M. Chen, A.~K. Wong, O.~G. Troyanskaya, and J.~Zhou.
\newblock A sequence-based global map of regulatory activity for deciphering human genetics.
\newblock {\em Nature genetics}, 54(7):940--949, 2022.

\bibitem{chen2023riemannian}
R.~T. Chen and Y.~Lipman.
\newblock Riemannian flow matching on general geometries.
\newblock {\em arXiv preprint arXiv:2302.03660}, 2023.

\bibitem{chen2022analog}
T.~Chen, R.~Zhang, and G.~Hinton.
\newblock Analog bits: Generating discrete data using diffusion models with self-conditioning.
\newblock {\em arXiv preprint arXiv:2208.04202}, 2022.

\bibitem{dai2019transformer}
Z.~Dai, Z.~Yang, Y.~Yang, J.~Carbonell, Q.~V. Le, and R.~Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length context.
\newblock {\em arXiv preprint arXiv:1901.02860}, 2019.

\bibitem{dao2023flow}
Q.~Dao, H.~Phung, B.~Nguyen, and A.~Tran.
\newblock Flow matching in latent space.
\newblock {\em arXiv preprint arXiv:2307.08698}, 2023.

\bibitem{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{DORMAND198019}
J.~Dormand and P.~Prince.
\newblock A family of embedded runge-kutta formulae.
\newblock {\em Journal of Computational and Applied Mathematics}, 6(1):19--26, 1980.

\bibitem{fatras2021minibatch}
K.~Fatras, Y.~Zine, S.~Majewski, R.~Flamary, R.~Gribonval, and N.~Courty.
\newblock Minibatch optimal transport distances; analysis and applications.
\newblock {\em arXiv preprint arXiv:2101.01792}, 2021.

\bibitem{gallot2004riemannian}
S.~Gallot, D.~Hulin, and J.~Lafontaine.
\newblock {\em Riemannian Geometry}.
\newblock Universitext. Springer Berlin Heidelberg, 2004.

\bibitem{gat2024discrete}
I.~Gat, T.~Remez, N.~Shaul, F.~Kreuk, R.~T. Chen, G.~Synnaeve, Y.~Adi, and Y.~Lipman.
\newblock Discrete flow matching.
\newblock {\em arXiv preprint arXiv:2407.15595}, 2024.

\bibitem{graves2013generating}
A.~Graves.
\newblock Generating sequences with recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1308.0850}, 2013.

\bibitem{graves2023bayesian}
A.~Graves, R.~K. Srivastava, T.~Atkinson, and F.~Gomez.
\newblock Bayesian flow networks.
\newblock {\em arXiv preprint arXiv:2308.07037}, 2023.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778, 2016.

\bibitem{ho2020denoising}
J.~Ho, A.~Jain, and P.~Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in neural information processing systems}, 33:6840--6851, 2020.

\bibitem{hon2017atlas}
C.-C. Hon, J.~A. Ramilowski, J.~Harshbarger, N.~Bertin, O.~J. Rackham, J.~Gough, E.~Denisenko, S.~Schmeier, T.~M. Poulsen, J.~Severin, et~al.
\newblock An atlas of human long non-coding rnas with accurate 5' ends.
\newblock {\em Nature}, 543(7644):199--204, 2017.

\bibitem{hoogeboom2021argmax}
E.~Hoogeboom, D.~Nielsen, P.~Jaini, P.~Forr{\'e}, and M.~Welling.
\newblock Argmax flows and multinomial diffusion: Learning categorical distributions.
\newblock {\em Advances in Neural Information Processing Systems}, 34:12454--12465, 2021.

\bibitem{hu2024latent}
V.~T. Hu, W.~Zhang, M.~Tang, P.~Mettes, D.~Zhao, and C.~Snoek.
\newblock Latent space editing in transformer-based flow matching.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pages 2247--2255, 2024.

\bibitem{hutchinson1989stochastic}
M.~F. Hutchinson.
\newblock A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines.
\newblock {\em Communications in Statistics-Simulation and Computation}, 18(3):1059--1076, 1989.

\bibitem{klein2024equivariant}
L.~Klein, A.~Kr{\"a}mer, and F.~No{\'e}.
\newblock Equivariant flow matching.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{le2021fisher}
A.~Le~Brigant, S.~C. Preston, and S.~Puechmorel.
\newblock Fisher-rao geometry of dirichlet distributions.
\newblock {\em Differential Geometry and its Applications}, 74:101702, 2021.

\bibitem{lecun2010mnist}
Y.~LeCun, C.~Cortes, and C.~Burges.
\newblock Mnist handwritten digit database.
\newblock {\em ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist}, 2, 2010.

\bibitem{lee2022statistical}
Y.~Lee, S.~Kim, J.~Choi, and F.~Park.
\newblock A statistical manifold framework for point cloud data.
\newblock In {\em International Conference on Machine Learning}, pages 12378--12402. PMLR, 2022.

\bibitem{li2024full}
J.~Li, C.~Cheng, Z.~Wu, R.~Guo, S.~Luo, Z.~Ren, J.~Peng, and J.~Ma.
\newblock Full-atom peptide design based on multi-modal flow matching.
\newblock {\em arXiv preprint arXiv:2406.00735}, 2024.

\bibitem{lin2017refinenet}
G.~Lin, A.~Milan, C.~Shen, and I.~Reid.
\newblock Refinenet: Multi-path refinement networks for high-resolution semantic segmentation.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 1925--1934, 2017.

\bibitem{lipman2022flow}
Y.~Lipman, R.~T. Chen, H.~Ben-Hamu, M.~Nickel, and M.~Le.
\newblock Flow matching for generative modeling.
\newblock {\em arXiv preprint arXiv:2210.02747}, 2022.

\bibitem{lou2023discrete}
A.~Lou, C.~Meng, and S.~Ermon.
\newblock Discrete diffusion language modeling by estimating the ratios of the data distribution.
\newblock {\em arXiv preprint arXiv:2310.16834}, 2023.

\bibitem{mahabadi2023tess}
R.~K. Mahabadi, J.~Tae, H.~Ivison, J.~Henderson, I.~Beltagy, M.~E. Peters, and A.~Cohan.
\newblock Tess: Text-to-text self-conditioned simplex diffusion.
\newblock {\em arXiv preprint arXiv:2305.08379}, 2023.

\bibitem{mahoney2011large}
M.~Mahoney.
\newblock Large text compression benchmark, 2011.

\bibitem{martens2020new}
J.~Martens.
\newblock New insights and perspectives on the natural gradient method.
\newblock {\em Journal of Machine Learning Research}, 21(146):1--76, 2020.

\bibitem{mathieu2020riemannian}
E.~Mathieu and M.~Nickel.
\newblock Riemannian continuous normalizing flows.
\newblock {\em Advances in Neural Information Processing Systems}, 33:2503--2515, 2020.

\bibitem{miyamoto2023closed}
H.~K. Miyamoto, F.~C. Meneghetti, and S.~I. Costa.
\newblock On closed-form expressions for the fisher-rao distance.
\newblock {\em arXiv preprint arXiv:2304.14885}, 2023.

\bibitem{nguyen2022improving}
K.~Nguyen, D.~Nguyen, T.~Pham, N.~Ho, et~al.
\newblock Improving mini-batch optimal transport via partial transportation.
\newblock In {\em International Conference on Machine Learning}, pages 16656--16690. PMLR, 2022.

\bibitem{nurbekyan2023efficient}
L.~Nurbekyan, W.~Lei, and Y.~Yang.
\newblock Efficient natural gradient descent methods for large-scale pde-based optimization problems.
\newblock {\em SIAM Journal on Scientific Computing}, 45(4):A1621--A1655, 2023.

\bibitem{palma2023modelling}
A.~Palma, S.~Rybakov, L.~Hetzel, and F.~J. Theis.
\newblock Modelling single-cell rna-seq trajectories on a flat statistical manifold.
\newblock In {\em NeurIPS 2023 AI for Science Workshop}, 2023.

\bibitem{park2000adaptive}
H.~Park, S.-I. Amari, and K.~Fukumizu.
\newblock Adaptive natural gradient learning algorithms for various stochastic models.
\newblock {\em Neural Networks}, 13(7):755--764, 2000.

\bibitem{peebles2023scalable}
W.~Peebles and S.~Xie.
\newblock Scalable diffusion models with transformers.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 4195--4205, 2023.

\bibitem{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{rao1992information}
C.~R. Rao.
\newblock Information and the accuracy attainable in the estimation of statistical parameters.
\newblock In {\em Breakthroughs in Statistics: Foundations and basic theory}, pages 235--247. Springer, 1992.

\bibitem{sahoo2024simple}
S.~S. Sahoo, M.~Arriola, Y.~Schiff, A.~Gokaslan, E.~Marroquin, J.~T. Chiu, A.~Rush, and V.~Kuleshov.
\newblock Simple and effective masked diffusion language models.
\newblock {\em arXiv preprint arXiv:2406.07524}, 2024.

\bibitem{salakhutdinov2008quantitative}
R.~Salakhutdinov and I.~Murray.
\newblock On the quantitative analysis of deep belief networks.
\newblock In {\em Proceedings of the 25th international conference on Machine learning}, pages 872--879. ACM, 2008.

\bibitem{salazar2019masked}
J.~Salazar, D.~Liang, T.~Q. Nguyen, and K.~Kirchhoff.
\newblock Masked language model scoring.
\newblock {\em arXiv preprint arXiv:1910.14659}, 2019.

\bibitem{santos2023blackout}
J.~E. Santos, Z.~R. Fox, N.~Lubbers, and Y.~T. Lin.
\newblock Blackout diffusion: generative diffusion models in discrete-state spaces.
\newblock In {\em International Conference on Machine Learning}, pages 9034--9059. PMLR, 2023.

\bibitem{song2019generative}
Y.~Song and S.~Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{song2020improved}
Y.~Song and S.~Ermon.
\newblock Improved techniques for training score-based generative models.
\newblock {\em Advances in neural information processing systems}, 33:12438--12448, 2020.

\bibitem{song2024equivariant}
Y.~Song, J.~Gong, M.~Xu, Z.~Cao, Y.~Lan, S.~Ermon, H.~Zhou, and W.-Y. Ma.
\newblock Equivariant flow matching with hybrid probability transport for 3d molecule generation.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{stark2023harmonic}
H.~Stark, B.~Jing, R.~Barzilay, and T.~Jaakkola.
\newblock Harmonic prior self-conditioned flow matching for multi-ligand docking and binding site design.
\newblock In {\em NeurIPS 2023 AI for Science Workshop}, 2023.

\bibitem{stark2024dirichlet}
H.~Stark, B.~Jing, C.~Wang, G.~Corso, B.~Berger, R.~Barzilay, and T.~Jaakkola.
\newblock Dirichlet flow matching with applications to dna sequence design.
\newblock {\em arXiv preprint arXiv:2402.05841}, 2024.

\bibitem{szegedy2016rethinking}
C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 2818--2826, 2016.

\bibitem{theis2015note}
L.~Theis, A.~v.~d. Oord, and M.~Bethge.
\newblock A note on the evaluation of generative models.
\newblock {\em arXiv preprint arXiv:1511.01844}, 2015.

\bibitem{tong2023improving}
A.~Tong, N.~Malkin, G.~Huguet, Y.~Zhang, J.~Rector-Brooks, K.~Fatras, G.~Wolf, and Y.~Bengio.
\newblock Improving and generalizing flow-based generative models with minibatch optimal transport.
\newblock {\em arXiv preprint arXiv:2302.00482}, 2023.

\bibitem{tran2019discrete}
D.~Tran, K.~Vafa, K.~Agrawal, L.~Dinh, and B.~Poole.
\newblock Discrete flows: Invertible generative models of discrete data.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{vaddi2022autonomous}
K.~Vaddi, H.~T. Chiang, and L.~D. Pozzo.
\newblock Autonomous retrosynthesis of gold nanoparticles via spectral shape matching.
\newblock {\em Digital Discovery}, 1(4):502--510, 2022.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{gpt-j}
B.~Wang and A.~Komatsuzaki.
\newblock {GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}.
\newblock \url{https://github.com/kingoflolz/mesh-transformer-jax}, May 2021.

\bibitem{yim2023fast}
J.~Yim, A.~Campbell, A.~Y. Foong, M.~Gastegger, J.~Jim{\'e}nez-Luna, S.~Lewis, V.~G. Satorras, B.~S. Veeling, R.~Barzilay, T.~Jaakkola, et~al.
\newblock Fast protein backbone generation with se (3) flow matching.
\newblock {\em arXiv preprint arXiv:2310.05297}, 2023.

\end{thebibliography}
