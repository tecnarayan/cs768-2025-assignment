\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{DKMR22}

\bibitem[ATV23]{ATV22}
P.~Awasthi, A.~Tang, and A.~Vijayaraghavan.
\newblock Agnostic learning of general {ReLU} activation using gradient
  descent.
\newblock In {\em The Eleventh International Conference on Learning
  Representations, {ICLR}}, 2023.

\bibitem[BNPS17]{bolte2017error}
J.~Bolte, T.~P. Nguyen, J.~Peypouquet, and B.~W. Suter.
\newblock From error bounds to the complexity of first-order descent methods
  for convex functions.
\newblock {\em Mathematical Programming}, 165(2):471--507, 2017.

\bibitem[BNS16]{bhojanapalli2016global}
S.~Bhojanapalli, B.~Neyshabur, and N.~Srebro.
\newblock Global optimality of local search for low rank matrix recovery.
\newblock {\em Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[DGK{\etalchar{+}}20]{DGKKS20}
I.~Diakonikolas, S.~Goel, S.~Karmalkar, A.~R. Klivans, and M.~Soltanolkotabi.
\newblock Approximation schemes for {ReLU} regression.
\newblock In {\em Conference on Learning Theory, {COLT}}, volume 125 of {\em
  Proceedings of Machine Learning Research}, pages 1452--1485. {PMLR}, 2020.

\bibitem[DH18]{DudejaH18}
R.~Dudeja and D.~Hsu.
\newblock Learning single-index models in {Gaussian} space.
\newblock In {\em Conference on Learning Theory, {COLT}}, volume~75 of {\em
  Proceedings of Machine Learning Research}, pages 1887--1930. {PMLR}, 2018.

\bibitem[DJS08]{dalalyan2008new}
A.~S. Dalalyan, A.~Juditsky, and V.~Spokoiny.
\newblock A new algorithm for estimating the effective dimension-reduction
  subspace.
\newblock {\em The Journal of Machine Learning Research}, 9:1647--1678, 2008.

\bibitem[DKMR22]{DKMR22}
I.~Diakonikolas, D.~Kane, P.~Manurangsi, and L.~Ren.
\newblock Hardness of learning a single neuron with adversarial label noise.
\newblock In {\em Proceedings of the 25th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, 2022.

\bibitem[DKPZ21]{DKPZ21}
I.~Diakonikolas, D.~M. Kane, T.~Pittas, and N.~Zarifis.
\newblock The optimality of polynomial regression for agnostic learning under
  {Gaussian} marginals in the {SQ} model.
\newblock In {\em Proceedings of The 34\textsuperscript{th} Conference on
  Learning Theory, {COLT}}, 2021.

\bibitem[DKR23]{DKR23}
I.~Diakonikolas, D.~M. Kane, and L.~Ren.
\newblock Near-optimal cryptographic hardness of agnostically learning
  halfspaces and {ReLU} regression under {Gaussian} marginals.
\newblock In {\em ICML}, 2023.

\bibitem[DKTZ20]{DKTZ20}
I.~Diakonikolas, V.~Kontonis, C.~Tzamos, and N.~Zarifis.
\newblock Learning halfspaces with massart noise under structured
  distributions.
\newblock In {\em Conference on Learning Theory, {COLT}}, 2020.

\bibitem[DKTZ22]{DKTZ22}
I.~Diakonikolas, V.~Kontonis, C.~Tzamos, and N.~Zarifis.
\newblock Learning a single neuron with adversarial label noise via gradient
  descent.
\newblock In {\em Conference on Learning Theory (COLT)}, pages 4313--4361,
  2022.

\bibitem[DKZ20]{DKZ20}
I.~Diakonikolas, D.~M. Kane, and N.~Zarifis.
\newblock Near-optimal {SQ} lower bounds for agnostically learning halfspaces
  and {ReLUs} under {G}aussian marginals.
\newblock In {\em Advances in Neural Information Processing Systems,
  {NeurIPS}}, 2020.

\bibitem[FCG20]{FCG20}
S.~Frei, Y.~Cao, and Q.~Gu.
\newblock Agnostic learning of a single neuron with gradient descent.
\newblock In {\em Advances in Neural Information Processing Systems,
  {NeurIPS}}, 2020.

\bibitem[FP03]{facchinei2003finite}
F.~Facchinei and J-S. Pang.
\newblock {\em Finite-dimensional variational inequalities and complementarity
  problems}.
\newblock Springer, 2003.

\bibitem[GGK20]{GGK20}
S.~Goel, A.~Gollakota, and A.~R. Klivans.
\newblock Statistical-query lower bounds via functional gradients.
\newblock In {\em Advances in Neural Information Processing Systems,
  {NeurIPS}}, 2020.

\bibitem[GGKS23]{GGKS23}
A.~Gollakota, P.~Gopalan, A.~R. Klivans, and K.~Stavropoulos.
\newblock Agnostically learning single-index models using omnipredictors.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.

\bibitem[Hau92]{Haussler:92}
D.~Haussler.
\newblock {Decision theoretic generalizations of the PAC model for neural net
  and other learning applications}.
\newblock {\em Information and Computation}, 100:78--150, 1992.

\bibitem[HJS01]{hristache2001direct}
M.~Hristache, A.~Juditsky, and V.~Spokoiny.
\newblock Direct estimation of the index coefficient in a single-index model.
\newblock {\em Annals of Statistics}, pages 595--623, 2001.

\bibitem[HMS{\etalchar{+}}04]{hardle2004nonparametric}
W.~H{\"a}rdle, M.~M{\"u}ller, S.~Sperlich, A.~Werwatz, et~al.
\newblock {\em Nonparametric and semiparametric models}, volume~1.
\newblock Springer, 2004.

\bibitem[Hof52]{hoffman2003approximate}
A.~J. Hoffman.
\newblock On approximate solutions of systems of linear inequalities.
\newblock {\em Journal of Research of the National Bureau of Standards},
  49:263--265, 1952.

\bibitem[Ich93]{ichimura1993semiparametric}
H.~Ichimura.
\newblock Semiparametric least squares {(SLS)} and weighted {SLS} estimation of
  single-index models.
\newblock {\em Journal of econometrics}, 58(1-2):71--120, 1993.

\bibitem[JGN{\etalchar{+}}17]{jin2017escape}
C.~Jin, R.~Ge, P.~Netrapalli, S.~Kakade, and M.~Jordan.
\newblock How to escape saddle points efficiently.
\newblock In {\em International conference on machine learning}, pages
  1724--1732. PMLR, 2017.

\bibitem[KKSK11]{kakade2011efficient}
S.~M Kakade, V.~Kanade, O.~Shamir, and A.~Kalai.
\newblock Efficient learning of generalized linear and single index models with
  isotonic regression.
\newblock {\em Advances in Neural Information Processing Systems}, 24, 2011.

\bibitem[KNS16]{karimi2016linear}
H.~Karimi, J.~Nutini, and M.~Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the {P}olyak-{\l}ojasiewicz condition.
\newblock In {\em Joint European conference on machine learning and knowledge
  discovery in databases}, pages 795--811, 2016.

\bibitem[KS09]{kalai2009isotron}
A.~T. Kalai and R.~Sastry.
\newblock The isotron algorithm: High-dimensional isotonic regression.
\newblock In {\em COLT}, 2009.

\bibitem[KSS94]{KSS:94}
M.~Kearns, R.~Schapire, and L.~Sellie.
\newblock Toward efficient agnostic learning.
\newblock {\em Machine Learning}, 17(2/3):115--141, 1994.

\bibitem[LCP22]{liu2022solving}
J.~Liu, Y.~Cui, and J-S. Pang.
\newblock Solving nonsmooth and nonconvex compound stochastic programs with
  applications to risk measure minimization.
\newblock {\em Mathematics of Operations Research}, 2022.

\bibitem[LH22]{LH2022}
C.~Lu and D.~S. Hochbaum.
\newblock A unified approach for a {1D} generalized total variation problem.
\newblock {\em Mathematical Programming}, 194(1-2):415--442, 2022.

\bibitem[{\L}oj63]{lojasiewicz1963propriete}
S.~{\L}ojasiewicz.
\newblock Une propri{\'e}t{\'e} topologique des sous-ensembles analytiques
  r{\'e}els.
\newblock {\em Les {\'e}quations aux d{\'e}riv{\'e}es partielles}, 117:87--89,
  1963.

\bibitem[{\L}oj93]{lojasiewicz1993geometrie}
S.~{\L}ojasiewicz.
\newblock Sur la g{\'e}om{\'e}trie semi-et sous-analytique.
\newblock In {\em Annales de l'institut Fourier}, volume~43, pages 1575--1595,
  1993.

\bibitem[MR18]{MR18}
P.~Manurangsi and D.~Reichman.
\newblock The computational complexity of training {ReLU}(s).
\newblock {\em arXiv preprint arXiv:1810.04207}, 2018.

\bibitem[Rd17]{roulet2017sharpness}
V.~Roulet and A.~d'Aspremont.
\newblock Sharpness, restart and acceleration.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[S{\'{\i}}m02]{Sima02}
J.~S{\'{\i}}ma.
\newblock Training a single sigmoidal neuron is hard.
\newblock {\em Neural Computation}, 14(11):2709--2728, 2002.

\bibitem[WZDD23]{WZDD2023}
P.~Wang, N.~Zarifis, I.~Diakonikolas, and J.~Diakonikolas.
\newblock Robustly learning a single neuron via sharpness.
\newblock {\em 40th International Conference on Machine Learning}, 2023.

\bibitem[ZL16]{zheng2016convergence}
Q.~Zheng and J.~Lafferty.
\newblock Convergence analysis for rectangular matrix completion using
  {Burer-Monteiro} factorization and gradient descent.
\newblock {\em arXiv preprint arXiv:1605.07051}, 2016.

\bibitem[ZY13]{ZY2013}
H.~Zhang and W.~Yin.
\newblock Gradient methods for convex minimization: better rates under weaker
  conditions.
\newblock {\em arXiv preprint arXiv:1303.4645}, 2013.

\end{thebibliography}
