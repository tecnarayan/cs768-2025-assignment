@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{wen2023f,
  title={f-Divergence Minimization for Sequence-Level Knowledge Distillation},
  author={Wen, Yuqiao and Li, Zichao and Du, Wenyu and Mou, Lili},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={10817--10834},
  year={2023}
}

@inproceedings{gu2023minillm,
  title={MiniLLM: Knowledge distillation of large language models},
  author={Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{kim2016sequence,
  title={Sequence-Level Knowledge Distillation},
  author={Kim, Yoon and Rush, Alexander M},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  year={2016},
  organization={Association for Computational Linguistics}
}

@inproceedings{lin2020autoregressive,
  title={Autoregressive Knowledge Distillation through Imitation Learning},
  author={Lin, Alexander and Wohlwend, Jeremy and Chen, Howard and Lei, Tao},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={6121--6133},
  year={2020}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{agarwal2024policy,
  title={On-policy distillation of language models: Learning from self-generated mistakes},
  author={Agarwal, Rishabh and Vieillard, Nino and Zhou, Yongchao and Stanczyk, Piotr and Garea, Sabela Ramos and Geist, Matthieu and Bachem, Olivier},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@inproceedings{swamy2021moments,
  title={Of moments and matching: A game-theoretic framework for closing the imitation gap},
  author={Swamy, Gokul and Choudhury, Sanjiban and Bagnell, J Andrew and Wu, Steven},
  booktitle={International Conference on Machine Learning},
  pages={10022--10032},
  year={2021},
  organization={PMLR}
}

@article{ko2024distillm,
  title={DistiLLM: Towards Streamlined Distillation for Large Language Models},
  author={Ko, Jongwoo and Kim, Sungnyun and Chen, Tianyi and Yun, Se-Young},
  journal={arXiv preprint arXiv:2402.03898},
  year={2024}
}

@article{xu2024survey,
  title={A survey on knowledge distillation of large language models},
  author={Xu, Xiaohan and Li, Ming and Tao, Chongyang and Shen, Tao and Cheng, Reynold and Li, Jinyang and Xu, Can and Tao, Dacheng and Zhou, Tianyi},
  journal={arXiv preprint arXiv:2402.13116},
  year={2024}
}

@misc{taori2023stanford,
  title={Stanford alpaca: An instruction-following llama model},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  year={2023}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  volume={2},
  number={3},
  pages={6},
  year={2023}
}

@article{wu2024rethinking,
  title={Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models},
  author={Wu, Taiqiang and Tao, Chaofan and Wang, Jiahao and Zhao, Zhe and Wong, Ngai},
  journal={arXiv preprint arXiv:2404.02657},
  year={2024}
}

@inproceedings{liang2023less,
  title={Less is more: Task-aware layer-wise distillation for language model compression},
  author={Liang, Chen and Zuo, Simiao and Zhang, Qingru and He, Pengcheng and Chen, Weizhu and Zhao, Tuo},
  booktitle={International Conference on Machine Learning},
  pages={20852--20867},
  year={2023},
  organization={PMLR}
}

@inproceedings{wu2021textgail,
  title={Textgail: Generative adversarial imitation learning for text generation},
  author={Wu, Qingyang and Li, Lei and Yu, Zhou},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={16},
  pages={14067--14075},
  year={2021}
}

@inproceedings{lampouras2016imitation,
  title={Imitation learning for language generation from unaligned data},
  author={Lampouras, Gerasimos and Vlachos, Andreas},
  booktitle={Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
  pages={1101--1112},
  year={2016},
  organization={The COLING 2016 Organizing Committee}
}

@inproceedings{yu2017seqgan,
  title={Seqgan: Sequence generative adversarial nets with policy gradient},
  author={Yu, Lantao and Zhang, Weinan and Wang, Jun and Yu, Yong},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={31},
  number={1},
  year={2017}
}

@inproceedings{ciosek2021imitation,
  title={Imitation Learning by Reinforcement Learning},
  author={Ciosek, Kamil},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{swamy2022sequence,
  title={Sequence model imitation learning with unobserved contexts},
  author={Swamy, Gokul and Choudhury, Sanjiban and Bagnell, J and Wu, Steven Z},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17665--17676},
  year={2022}
}

@article{sriperumbudur2009integral,
  title={On integral probability metrics,$\backslash$phi-divergences and binary classification},
  author={Sriperumbudur, Bharath K and Fukumizu, Kenji and Gretton, Arthur and Sch{\"o}lkopf, Bernhard and Lanckriet, Gert RG},
  journal={arXiv preprint arXiv:0901.2698},
  year={2009}
}

@article{hao2022teacher,
  title={Teacher forcing recovers reward functions for text generation},
  author={Hao, Yongchang and Liu, Yuxin and Mou, Lili},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={12594--12607},
  year={2022}
}

@inproceedings{pang2021text,
  title={TEXT GENERATION BY LEARNING FROM DEMONSTRATIONS},
  author={Pang, Richard Yuanzhe and He, He},
  booktitle={9th International Conference on Learning Representations, ICLR 2021},
  year={2021}
}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@article{cui2023ultrafeedback,
  title={Ultrafeedback: Boosting language models with high-quality feedback},
  author={Cui, Ganqu and Yuan, Lifan and Ding, Ning and Yao, Guanming and Zhu, Wei and Ni, Yuan and Xie, Guotong and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2310.01377},
  year={2023}
}



@misc{dolly2023introducing,
  title={Introducing the World’s First Truly Open Instruction-Tuned LLM. databricks. com},
  author={Dolly, Free},
  year={2023}
}

@article{wang2022self,
  title={Self-instruct: Aligning language models with self-generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@article{wang2022super,
  title={Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  journal={arXiv preprint arXiv:2204.07705},
  year={2022}
}

@article{honovich2022unnatural,
  title={Unnatural instructions: Tuning language models with (almost) no human labor},
  author={Honovich, Or and Scialom, Thomas and Levy, Omer and Schick, Timo},
  journal={arXiv preprint arXiv:2212.09689},
  year={2022}
}

@article{gokaslan,
  title={Openwebtext corpus},
  author={Gokaslan, Aaron  and Aaron, Vanya and Pavlick, Ellie and Tellex, Stefanie},
  year={2019}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@article{gliwa2019samsum,
  title={SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization},
  author={Gliwa, Bogdan and Mochol, Iwona and Biesek, Maciej and Wawer, Aleksander},
  journal={arXiv preprint arXiv:1911.12237},
  year={2019}
}

@inproceedings{cettolo2017overview,
  title={Overview of the iwslt 2017 evaluation campaign},
  author={Cettolo, Mauro and Federico, Marcello and Bentivogli, Luisa and Niehues, Jan and St{\"u}ker, Sebastian and Sudoh, Katsuitho and Yoshino, Koichiro and Federmann, Christian},
  booktitle={Proceedings of the 14th International Workshop on Spoken Language Translation},
  pages={2--14},
  year={2017}
}

@article{geva2021did,
  title={Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies},
  author={Geva, Mor and Khashabi, Daniel and Segal, Elad and Khot, Tushar and Roth, Dan and Berant, Jonathan},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={346--361},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{xue2020mt5,
  title={mT5: A massively multilingual pre-trained text-to-text transformer},
  author={Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  journal={arXiv preprint arXiv:2010.11934},
  year={2020}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{geng2023openllama,
  title={Openllama: An open reproduction of llama},
  author={Geng, Xinyang and Liu, Hao},
  journal={URL: https://github. com/openlm-research/open\_llama},
  year={2023}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}