\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Tsitsiklis et~al.(1986)Tsitsiklis, Bertsekas, and
  Athans]{Tsitsiklis:1986ee}
John~N Tsitsiklis, Dimitri~P Bertsekas, and Michael Athans.
\newblock {Distributed asynchronous deterministic and stochastic gradient
  optimization algorithms}.
\newblock \emph{IEEE Transactions on Automatic Control}, 31\penalty0
  (9):\penalty0 803--812, 1986.

\bibitem[Nedic and Ozdaglar(2009)]{nedic2009distributed}
Angelia Nedic and Asuman Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 54\penalty0
  (1):\penalty0 48--61, 2009.

\bibitem[Duchi et~al.(2012)Duchi, Agarwal, and Wainwright]{duchi2012ddual}
J~C Duchi, A~Agarwal, and M~J Wainwright.
\newblock {Dual Averaging for Distributed Optimization: Convergence Analysis
  and Network Scaling}.
\newblock \emph{IEEE Transactions on Automatic Control}, 57\penalty0
  (3):\penalty0 592--606, March 2012.

\bibitem[Shi et~al.(2015)Shi, Ling, Wu, and Yin]{shi2015extra}
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin.
\newblock Extra: An exact first-order algorithm for decentralized consensus
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (2):\penalty0
  944--966, 2015.

\bibitem[Mokhtari and Ribeiro(2016)]{mokhtari2016dsa}
Aryan Mokhtari and Alejandro Ribeiro.
\newblock {DSA}: Decentralized double stochastic averaging gradient algorithm.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (61):\penalty0 1--35, 2016.

\bibitem[Nedic et~al.(2017)Nedic, Olshevsky, and Shi]{nedic2017achieving}
Angelia Nedic, Alex Olshevsky, and Wei Shi.
\newblock Achieving geometric convergence for distributed optimization over
  time-varying graphs.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (4):\penalty0
  2597--2633, 2017.

\bibitem[Cevher et~al.(2014)Cevher, Becker, and Schmidt]{cevher2014review}
Volkan Cevher, Stephen Becker, and Mark Schmidt.
\newblock {Convex Optimization for Big Data: Scalable, randomized, and parallel
  algorithms for big data analytics}.
\newblock \emph{IEEE Signal Processing Magazine}, 31\penalty0 (5):\penalty0
  32--43, 2014.

\bibitem[Smith et~al.(2018)Smith, Forte, Ma, Tak{\'a}c, Jordan, and
  Jaggi]{smith2016cocoa}
Virginia Smith, Simone Forte, Chenxin Ma, Martin Tak{\'a}c, Michael~I Jordan,
  and Martin Jaggi.
\newblock {CoCoA: A General Framework for Communication-Efficient Distributed
  Optimization}.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (230):\penalty0 1--49, 2018.

\bibitem[Zhang et~al.(2015)Zhang, Choromanska, and LeCun]{zhang2015elastic}
Sixin Zhang, Anna~E Choromanska, and Yann LeCun.
\newblock {Deep learning with Elastic Averaging SGD}.
\newblock In \emph{NIPS 2015 - Advances in Neural Information Processing
  Systems 28}, pages 685--693, 2015.

\bibitem[Wang et~al.(2017)Wang, Wang, and Srebro]{Wang:2017th}
Jialei Wang, Weiran Wang, and Nathan Srebro.
\newblock {Memory and Communication Efficient Distributed Stochastic
  Optimization with Minibatch Prox}.
\newblock In \emph{ICML 2017 - Proceedings of the 34th International Conference
  on Machine Learning}, pages 1882--1919, June 2017.

\bibitem[D{\"u}nner et~al.(2016)D{\"u}nner, Forte, Tak{\'a}c, and
  Jaggi]{Dunner:2016vga}
Celestine D{\"u}nner, Simone Forte, Martin Tak{\'a}c, and Martin Jaggi.
\newblock {Primal-Dual Rates and Certificates}.
\newblock In \emph{ICML 2016 - Proceedings of the 33th International Conference
  on Machine Learning}, pages 783--792, 2016.

\bibitem[Jaggi et~al.(2014)Jaggi, Smith, Tak{\'a}c, Terhorst, Krishnan,
  Hofmann, and Jordan]{jaggi2014communication}
Martin Jaggi, Virginia Smith, Martin Tak{\'a}c, Jonathan Terhorst, Sanjay
  Krishnan, Thomas Hofmann, and Michael~I Jordan.
\newblock Communication-efficient distributed dual coordinate ascent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3068--3076, 2014.

\bibitem[Scaman et~al.(2017)Scaman, Bach, Bubeck, Lee, and
  Massouli{\'{e}}]{icmlScamanBBLM17}
Kevin Scaman, Francis~R. Bach, S{\'{e}}bastien Bubeck, Yin~Tat Lee, and Laurent
  Massouli{\'{e}}.
\newblock Optimal algorithms for smooth and strongly convex distributed
  optimization in networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017}, pages
  3027--3036, 2017.

\bibitem[Scaman et~al.(2018)Scaman, Bach, Bubeck, Lee, and
  Massouli{\'e}]{scaman2018optimal}
Kevin Scaman, Francis Bach, S{\'e}bastien Bubeck, Yin~Tat Lee, and Laurent
  Massouli{\'e}.
\newblock Optimal algorithms for non-smooth distributed optimization in
  networks.
\newblock \emph{arXiv preprint arXiv:1806.00291}, 2018.

\bibitem[Jakoveti{c} et~al.(2012)Jakoveti{c}, Xavier, and
  Moura]{jakovetic2012convergence}
Dusan Jakoveti{c}, Joao Xavier, and Jos{e}~MF Moura.
\newblock Convergence rate analysis of distributed gradient methods for smooth
  optimization.
\newblock In \emph{Telecommunications Forum (TELFOR), 2012 20th}, pages
  867--870. IEEE, 2012.

\bibitem[Yuan et~al.(2016)Yuan, Ling, and Yin]{yuan2016convergence}
Kun Yuan, Qing Ling, and Wotao Yin.
\newblock On the convergence of decentralized gradient descent.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (3):\penalty0
  1835--1854, 2016.

\bibitem[Shi et~al.(2014)Shi, Ling, Yuan, Wu, and Yin]{Shi:2014js}
Wei Shi, Qing Ling, Kun Yuan, Gang Wu, and Wotao Yin.
\newblock {On the Linear Convergence of the ADMM in Decentralized Consensus
  Optimization}.
\newblock \emph{IEEE Transactions on Signal Processing}, 62\penalty0
  (7):\penalty0 1750--1761, 2014.

\bibitem[Wei and Ozdaglar(2013)]{Wei:2013wy}
Ermin Wei and Asuman Ozdaglar.
\newblock {On the O(1/k) Convergence of Asynchronous Distributed Alternating
  Direction Method of Multipliers}.
\newblock \emph{arXiv}, July 2013.

\bibitem[Bianchi et~al.(2016)Bianchi, Hachem, and
  Iutzeler]{bianchi2016coordinate}
Pascal Bianchi, Walid Hachem, and Franck Iutzeler.
\newblock A coordinate descent primal-dual algorithm and application to
  distributed asynchronous optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 61\penalty0
  (10):\penalty0 2947--2957, 2016.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017can}
Xiangru Lian, Ce~Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5336--5346, 2017.

\bibitem[Lian et~al.(2018)Lian, Zhang, Zhang, and Liu]{lian2017asynchronous}
Xiangru Lian, Wei Zhang, Ce~Zhang, and Ji~Liu.
\newblock Asynchronous decentralized parallel stochastic gradient descent.
\newblock In \emph{ICML 2018 - Proceedings of the 35th International Conference
  on Machine Learning}, 2018.

\bibitem[Tang et~al.(2018{\natexlab{a}})Tang, Lian, Yan, Zhang, and
  Liu]{tang2018d}
Hanlin Tang, Xiangru Lian, Ming Yan, Ce~Zhang, and Ji~Liu.
\newblock D$^2$: Decentralized training over decentralized data.
\newblock \emph{arXiv preprint arXiv:1803.07068}, 2018{\natexlab{a}}.

\bibitem[Tang et~al.(2018{\natexlab{b}})Tang, Gan, Zhang, Zhang, and
  Liu]{tang2018decentralized}
Hanlin Tang, Shaoduo Gan, Ce~Zhang, Tong Zhang, and Ji~Liu.
\newblock Communication compression for decentralized training.
\newblock In \emph{NIPS 2018 - Advances in Neural Information Processing
  Systems}, 2018{\natexlab{b}}.

\bibitem[Wu et~al.(2018)Wu, Yuan, Ling, Yin, and Sayed]{wu2018decentralized}
Tianyu Wu, Kun Yuan, Qing Ling, Wotao Yin, and Ali~H Sayed.
\newblock Decentralized consensus optimization with asynchrony and delays.
\newblock \emph{IEEE Transactions on Signal and Information Processing over
  Networks}, 4\penalty0 (2):\penalty0 293--307, 2018.

\bibitem[Sirb and Ye(2018)]{sirb2018decentralized}
Benjamin Sirb and Xiaojing Ye.
\newblock Decentralized consensus algorithm with delayed and stochastic
  gradients.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (2):\penalty0
  1232--1254, 2018.

\bibitem[Yang(2013)]{yang2013disdca}
Tianbao Yang.
\newblock {Trading Computation for Communication: Distributed Stochastic Dual
  Coordinate Ascent}.
\newblock In \emph{NIPS 2014 - Advances in Neural Information Processing
  Systems 27}, 2013.

\bibitem[Ma et~al.(2015)Ma, Smith, Jaggi, Jordan, Richt{\'a}rik, and
  Tak{\'a}c]{ma2015adding}
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael~I Jordan, Peter
  Richt{\'a}rik, and Martin Tak{\'a}c.
\newblock {Adding vs. Averaging in Distributed Primal-Dual Optimization}.
\newblock In \emph{ICML 2015 - Proceedings of the 32th International Conference
  on Machine Learning}, pages 1973--1982, 2015.

\bibitem[D\"unner et~al.(2018)D\"unner, Lucchi, Gargiani, Bian, Hofmann, and
  Jaggi]{duenner2018trust}
Celestine D\"unner, Aurelien Lucchi, Matilde Gargiani, An~Bian, Thomas Hofmann,
  and Martin Jaggi.
\newblock {A Distributed Second-Order Algorithm You Can Trust}.
\newblock In \emph{ICML 2018 - Proceedings of the 35th International Conference
  on Machine Learning}, pages 1357--1365, July 2018.

\bibitem[Agarwal and Duchi(2011)]{agarwal2011distributed}
Alekh Agarwal and John~C Duchi.
\newblock Distributed delayed stochastic optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  873--881, 2011.

\bibitem[Zinkevich et~al.(2010)Zinkevich, Weimer, Li, and
  Smola]{zinkevich2010parallelized}
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex~J Smola.
\newblock Parallelized stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2595--2603, 2010.

\bibitem[Zhang and Lin(2015)]{zhang2015disco}
Yuchen Zhang and Xiao Lin.
\newblock Disco: Distributed optimization for self-concordant empirical loss.
\newblock In \emph{International conference on machine learning}, pages
  362--370, 2015.

\bibitem[Reddi et~al.(2016)Reddi, Kone{{c}}n{\`y}, Richt{\'a}rik,
  P{\'o}cz{\'o}s, and Smola]{reddi2016aide}
Sashank~J Reddi, Jakub Kone{{c}}n{\`y}, Peter Richt{\'a}rik, Barnab{\'a}s
  P{\'o}cz{\'o}s, and Alex Smola.
\newblock Aide: Fast and communication efficient distributed optimization.
\newblock \emph{arXiv preprint arXiv:1608.06879}, 2016.

\bibitem[Gargiani(2017)]{gargiani2017master}
Matilde Gargiani.
\newblock {Hessian-CoCoA: a general parallel and distributed framework for
  non-strongly convex regularizers}.
\newblock Master's thesis, ETH Zurich, June 2017.

\bibitem[Lee and Chang(2017)]{lee2017distributed}
Ching-pei Lee and Kai-Wei Chang.
\newblock Distributed block-diagonal approximation methods for regularized
  empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1709.03043}, 2017.

\bibitem[Lee et~al.(2018)Lee, Lim, and Wright]{lee2018distributed}
Ching-pei Lee, Cong~Han Lim, and Stephen~J Wright.
\newblock A distributed quasi-newton algorithm for empirical risk minimization
  with nonsmooth regularization.
\newblock In \emph{ACM International Conference on Knowledge Discovery and Data
  Mining}, 2018.

\bibitem[Smith et~al.(2017)Smith, Chiang, Sanjabi, and
  Talwalkar]{smith2017federated}
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet Talwalkar.
\newblock {Federated Multi-Task Learning}.
\newblock In \emph{NIPS 2017 - Advances in Neural Information Processing
  Systems 30}, 2017.

\bibitem[Kone{{c}}n{\`y} et~al.(2015)Kone{{c}}n{\`y}, McMahan, and
  Ramage]{konevcny2015federated}
Jakub Kone{{c}}n{\`y}, Brendan McMahan, and Daniel Ramage.
\newblock Federated optimization: Distributed optimization beyond the
  datacenter.
\newblock \emph{arXiv preprint arXiv:1511.03575}, 2015.

\bibitem[Kone{{c}}n{\`y} et~al.(2016)Kone{{c}}n{\`y}, McMahan, Yu, Richt{a}rik,
  Suresh, and Bacon]{konevcny2016federated}
Jakub Kone{{c}}n{\`y}, H~Brendan McMahan, Felix~X Yu, Peter Richt{a}rik,
  Ananda~Theertha Suresh, and Dave Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock \emph{arXiv preprint arXiv:1610.05492}, 2016.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 1273--1282,
  2017.

\bibitem[Boyd et~al.(2011)Boyd, Parikh, Chu, Peleato, Eckstein,
  et~al.]{boyd2011distributed}
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et~al.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock \emph{Foundations and Trends{\textregistered} in Machine learning},
  3\penalty0 (1):\penalty0 1--122, 2011.

\bibitem[Stich et~al.(2018)Stich, Cordonnier, and Jaggi]{stich2018sparsified}
Sebastian~U. Stich, Jean-Baptiste Cordonnier, and Martin Jaggi.
\newblock Sparsified sgd with memory.
\newblock In \emph{NIPS 2018 - Advances in Neural Information Processing
  Systems}, 2018.

\bibitem[Rockafellar(2015)]{rockafellar2015convex}
Ralph~Tyrell Rockafellar.
\newblock \emph{Convex analysis}.
\newblock Princeton university press, 2015.

\bibitem[Hastings(1970)]{hastings1970monte}
W~Keith Hastings.
\newblock Monte carlo sampling methods using markov chains and their
  applications.
\newblock \emph{Biometrika}, 57\penalty0 (1):\penalty0 97--109, 1970.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock In \emph{NIPS Workshop on Autodiff}, 2017.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,
  Cournapeau, Brucher, Perrot, and Duchesnay]{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2825--2830,
  2011.

\end{thebibliography}
