\begin{thebibliography}{10}

\bibitem{Nadaraja64}
\`E.~A. Nadaraja.
\newblock On a regression estimate.
\newblock {\em Teor. Verojatnost. i Primenen.}, 9:157--159, 1964.

\bibitem{Watson64}
Geoffrey~S. Watson.
\newblock Smooth regression analysis.
\newblock {\em Sankhyā: The Indian Journal of Statistics, Series A
  (1961-2002)}, 26(4):359--372, 1964.

\bibitem{Neal1996}
Radford~M. Neal.
\newblock {\em Priors for Infinite Networks}, pages 29--53.
\newblock Springer New York, New York, NY, 1996.

\bibitem{Williams1996}
Christopher K.~I. Williams.
\newblock Computing with infinite networks.
\newblock In {\em Proceedings of the 9th International Conference on Neural
  Information Processing Systems}, NIPS'96, page 295–301, Cambridge, MA, USA,
  1996. MIT Press.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem{Lee2018}
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel~S. Schoenholz, Jeffrey
  Pennington, and Jascha Sohl-Dickstein.
\newblock Deep neural networks as {G}aussian processes, 2018.
\newblock ICLR 2018, arXiv:1711.00165.

\bibitem{Chizat2019}
L\'{e}na\"{\i}c Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{pillaud2018statistical}
Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach.
\newblock Statistical optimality of stochastic gradient descent on hard
  learning problems through multiple passes.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~31, pages 8114--8124, 2018.

\bibitem{Berthier2020TightNC}
Raphael Berthier, F.~Bach, and P.~Gaillard.
\newblock Tight nonparametric convergence rates for stochastic gradient descent
  under the noiseless linear model.
\newblock {\em ArXiv}, abs/2006.08212, 2020.

\bibitem{Caponnetto2005FastRF}
A.~Caponnetto and E.~D. Vito.
\newblock Fast rates for regularized least-squares algorithm.
\newblock In {\em .}, 2005.

\bibitem{steinwart2009optimal}
Ingo Steinwart, Don~R Hush, Clint Scovel, et~al.
\newblock Optimal rates for regularized least squares regression.
\newblock In {\em COLT}, pages 79--93, 2009.

\bibitem{spigler2019asymptotic}
Stefano Spigler, Mario Geiger, and Matthieu Wyart.
\newblock Asymptotic learning curves of kernel methods: empirical data versus
  teacher{\textendash}student paradigm.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment},
  2020(12):124001, 2020.

\bibitem{bordelon2020}
Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan.
\newblock Spectrum dependent learning curves in kernel regression and wide
  neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1024--1034. PMLR, 2020.

\bibitem{caponnetto2007optimal}
Andrea Caponnetto and Ernesto De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock {\em Foundations of Computational Mathematics}, 7(3):331--368, 2007.

\bibitem{fischer2017sobolev}
Simon Fischer and Ingo Steinwart.
\newblock Sobolev norm learning rates for regularized least-squares algorithms.
\newblock {\em Journal of Machine Learning Research}, 21(205):1--38, 2020.

\bibitem{Lin2018OptimalRF}
Junhong Lin, Alessandro Rudi, L.~Rosasco, and V.~Cevher.
\newblock Optimal rates for spectral algorithms with least-squares regression
  over hilbert spaces.
\newblock {\em Applied and Computational Harmonic Analysis}, 48:868--890, 2018.

\bibitem{Bartlett2020BenignOI}
P.~Bartlett, Philip~M. Long, G.~Lugosi, and Alexander Tsigler.
\newblock Benign overfitting in linear regression.
\newblock {\em Proceedings of the National Academy of Sciences}, 117:30063 --
  30070, 2020.

\bibitem{polyak1992acceleration}
Boris~T Polyak and Anatoli~B Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock {\em SIAM journal on control and optimization}, 30(4):838--855, 1992.

\bibitem{nemirovskij1983problem}
Arkadij~Semenovi{\v{c}} Nemirovskij and David~Borisovich Yudin.
\newblock {\em Problem complexity and method efficiency in optimization}.
\newblock Wiley-Interscience, 1983.

\bibitem{Jun2019KernelTR}
Kwang-Sung Jun, Ashok Cutkosky, and Francesco Orabona.
\newblock Kernel truncated randomized ridge regression: Optimal rates and low
  noise acceleration.
\newblock In {\em NeurIPS}, 2019.

\bibitem{Varre2021LastIC}
Aditya Varre, Loucas Pillaud-Vivien, and Nicolas Flammarion.
\newblock Last iterate convergence of sgd for least-squares in the
  interpolation regime.
\newblock {\em ArXiv}, abs/2102.03183, 2021.

\bibitem{Kanagawa2018}
M.~Kanagawa, P.~Hennig, D.~Sejdinovic, and B.~K. Sriperumbudur.
\newblock Gaussian processes and kernel methods: A review on connections and
  equivalences.
\newblock {\em Arxiv e-prints}, arXiv:1805.08845v1 [stat.ML], 2018.

\bibitem{dicker2016ridge}
Lee~H Dicker et~al.
\newblock Ridge regression and asymptotic minimax estimation over spheres of
  growing dimension.
\newblock {\em Bernoulli}, 22(1):1--37, 2016.

\bibitem{Hsu2012}
Daniel Hsu, Sham~M. Kakade, and Tong Zhang.
\newblock Random design analysis of ridge regression.
\newblock In Shie Mannor, Nathan Srebro, and Robert~C. Williamson, editors,
  {\em Proceedings of the 25th Annual Conference on Learning Theory}, volume~23
  of {\em Proceedings of Machine Learning Research}, pages 9.1--9.24,
  Edinburgh, Scotland, 25--27 Jun 2012. JMLR Workshop and Conference
  Proceedings.

\bibitem{dobriban2018high}
Edgar Dobriban and Stefan Wager.
\newblock High-dimensional asymptotics of prediction: Ridge regression and
  classification.
\newblock {\em The Annals of Statistics}, 46(1):247--279, 2018.

\bibitem{ledoit2011eigenvectors}
Olivier Ledoit and Sandrine P{\'e}ch{\'e}.
\newblock Eigenvectors of some large sample covariance matrix ensembles.
\newblock {\em Probability Theory and Related Fields}, 151(1):233--264, 2011.

\bibitem{Loureiro2021CapturingTL}
Bruno Loureiro, C\'edric Gerbelot, Hugo Cui, Sebastian Goldt, F.~Krzakala,
  M.~M\'ezard, and Lenka Zdeborov\'a.
\newblock Capturing the learning curves of generic features maps for realistic
  data sets with a teacher-student model.
\newblock {\em ArXiv}, abs/2102.08127, 2021.

\bibitem{dietrich1999statistical}
Rainer Dietrich, Manfred Opper, and Haim Sompolinsky.
\newblock Statistical mechanics of support vector networks.
\newblock {\em Physical review letters}, 82(14):2975, 1999.

\bibitem{opper1996statistical}
Manfred Opper and Wolfgang Kinzel.
\newblock Statistical mechanics of generalization.
\newblock In {\em Models of neural networks III}, pages 151--209. Springer,
  1996.

\bibitem{Opper2001}
M.~Opper and R.~Urbanczik.
\newblock Universal learning curves of support vector machines.
\newblock {\em Phys. Rev. Lett.}, 86:4410--4413, May 2001.

\bibitem{kabashima2008inference}
Yoshiyuki Kabashima.
\newblock Inference from correlated patterns: a unified theory for perceptron
  learning and linear vector channels.
\newblock In {\em Journal of Physics: Conference Series}, volume~95, page
  012001. IOP Publishing, 2008.

\bibitem{advani2020high}
Madhu~S Advani, Andrew~M Saxe, and Haim Sompolinsky.
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock {\em Neural Networks}, 132:428--446, 2020.

\bibitem{belkin2020two}
Mikhail Belkin, Daniel Hsu, and Ji~Xu.
\newblock Two models of double descent for weak features.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 2(4):1167--1180,
  2020.

\bibitem{hastie2019surprises}
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan~J Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock {\em arXiv preprint arXiv:1903.08560}, 2019.

\bibitem{mei2019generalization}
Song Mei and Andrea Montanari.
\newblock The generalization error of random features regression: Precise
  asymptotics and double descent curve.
\newblock {\em arXiv preprint arXiv:1908.05355}, 2019.

\bibitem{gerace2020generalisation}
F.~Gerace, B.~Loureiro, F.~Krzakala, M.~M{\'e}zard, and L.~Zdeborov{\'a}.
\newblock Generalisation error in learning with random features and the hidden
  manifold model.
\newblock In {\em 37th International Conference on Machine Learning}, 2020.

\bibitem{ghorbani2019limitations}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Limitations of lazy training of two-layers neural network.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\' Alch\'{e}-Buc,
  E.~Fox, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems}, volume~32, pages 9111--9121, 2019.

\bibitem{kobak2020optimal}
Dmitry Kobak, Jonathan Lomond, and Benoit Sanchez.
\newblock The optimal ridge penalty for real-world high-dimensional data can be
  zero or negative due to the implicit ridge regularization.
\newblock {\em Journal of Machine Learning Research}, 21(169):1--16, 2020.

\bibitem{wu2020optimal}
Denny Wu and Ji~Xu.
\newblock On the optimal weighted $\ell_2$ regularization in overparameterized
  linear regression.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem{richards2021asymptotics}
Dominic Richards, Jaouad Mourtada, and Lorenzo Rosasco.
\newblock Asymptotics of ridge (less) regression under general source
  condition.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 3889--3897. PMLR, 2021.

\bibitem{liao2020random}
Zhenyu Liao, Romain Couillet, and Michael~W Mahoney.
\newblock A random matrix analysis of random fourier features: beyond the
  gaussian kernel, a precise phase transition, and the corresponding double
  descent.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem{jacot2020kernel}
Arthur Jacot, Berfin {\c{S}}im{\c{s}}ek, Francesco Spadaro, Cl{\'e}ment
  Hongler, and Franck Gabriel.
\newblock Kernel alignment risk estimator: Risk prediction from training data.
\newblock {\em arXiv preprint arXiv:2006.09796}, 2020.

\bibitem{ghorbani2020neural}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock When do neural networks outperform kernel methods?
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem{liu2020kernel}
Fanghui Liu, Zhenyu Liao, and Johan~AK Suykens.
\newblock Kernel regression in high dimension: Refined analysis beyond double
  descent.
\newblock {\em arXiv preprint arXiv:2010.02681}, 2020.

\bibitem{Tsigler2020BenignOI}
Alexander Tsigler and P.~Bartlett.
\newblock Benign overfitting in ridge regression.
\newblock {\em arXiv preprint arXiv:2009.14286}, 2020.

\bibitem{Talagrand1994ConcentrationOM}
Michel Talagrand.
\newblock Concentration of measure and isoperimetric inequalities in product
  spaces.
\newblock {\em Publications Math{\'e}matiques de l'Institut des Hautes
  {\'E}tudes Scientifiques}, 81:73--205, 1994.

\bibitem{Louart2017ARM}
Cosme Louart, Zhenyu Liao, and Romain Couillet.
\newblock A random matrix approach to neural networks.
\newblock {\em ArXiv}, abs/1702.05419, 2017.

\bibitem{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock {\em Journal of Machine Learning Research}, 12:2825--2830, 2011.

\bibitem{Canatar2021SpectralBA}
Abdulkadir Canatar, B.~Bordelon, and C.~Pehlevan.
\newblock Spectral bias and task-model alignment explain generalization in
  kernel regression and infinitely wide neural networks.
\newblock {\em Nature Communications}, 12:1--12, 2021.

\bibitem{xiao2017}
H.~Xiao, K.~Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock {\em ArXiv}, abs/1708.07747, 2017.

\bibitem{Hamidieh2018ADS}
Kam Hamidieh.
\newblock A data-driven statistical model for predicting the critical
  temperature of a superconductor.
\newblock {\em Computational Materials Science}, 154:346--354, 2018.

\bibitem{karoui2013asymptotic}
Noureddine~El Karoui.
\newblock Asymptotic behavior of unregularized and ridge-regularized
  high-dimensional robust regression estimators : rigorous results, 2013.

\bibitem{thrampoulidis2018precise}
Christos Thrampoulidis, Ehsan Abbasi, and Babak Hassibi.
\newblock Precise error analysis of regularized $ m $-estimators in high
  dimensions.
\newblock {\em IEEE Transactions on Information Theory}, 64(8):5592--5628,
  2018.

\end{thebibliography}
