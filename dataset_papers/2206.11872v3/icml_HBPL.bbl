\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Altschuler et~al.(2021)Altschuler, Chewi, Gerber, and Stromme]{ACGS21}
Altschuler, J., Chewi, S., Gerber, P., and Stromme, A.
\newblock Averaging on the bures-wasserstein manifold: dimension-free
  convergence of gradient descent.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Apidopoulos et~al.(2021)Apidopoulos, Ginatta, and Villa]{AGV21}
Apidopoulos, V., Ginatta, N., and Villa, S.
\newblock Convergence rates for the heavy-ball continuous dynamics for
  non-convex optimization, under polyak-Łojasiewicz conditioning.
\newblock \emph{arXiv:2107.10123}, 2021.

\bibitem[Aujol et~al.(2020)Aujol, Dossal, and Rondepierre]{ADR20}
Aujol, J.-F., Dossal, C., and Rondepierre, A.
\newblock Convergence rates of the heavy-ball method with lojasiewicz property.
\newblock \emph{hal-02928958}, 2020.

\bibitem[Bhatia(2007)]{B07}
Bhatia, R.
\newblock Perturbation bounds for matrix eigenvalues.
\newblock \emph{SIAM}, 2007.

\bibitem[Can et~al.(2019)Can, G{\"u}rb{\"u}zbalaban, and Zhu]{CGZ19}
Can, B., G{\"u}rb{\"u}zbalaban, M., and Zhu, L.
\newblock Accelerated linear convergence of stochastic momentum methods in
  wasserstein distances.
\newblock \emph{ICML}, 2019.

\bibitem[Chizat(2021)]{C21}
Chizat, L.
\newblock Sparse optimization on measures with over-parameterized gradient
  descent.
\newblock \emph{Mathematical Programming}, 2021.

\bibitem[Cutkosky \& Mehta(2021)Cutkosky and Mehta]{CM21}
Cutkosky, A. and Mehta, H.
\newblock High-probability bounds for non-convex stochastic optimization with
  heavy tails.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Diakonikolas \& Jordan(2019)Diakonikolas and Jordan]{DJ19}
Diakonikolas, J. and Jordan, M.~I.
\newblock Generalized momentum-based methods: A hamiltonian perspective.
\newblock \emph{arXiv:1906.00436}, 2019.

\bibitem[Flammarion \& Bach(2015)Flammarion and Bach]{NB15}
Flammarion, N. and Bach, F.
\newblock From averaging to acceleration, there is only a step-size.
\newblock \emph{COLT}, 2015.

\bibitem[Gelfand(1941)]{G41}
Gelfand, I.
\newblock Normierte ringe.
\newblock \emph{Mat. Sbornik}, 1941.

\bibitem[Ghadimi et~al.(2015)Ghadimi, Feyzmahdavian, and Johansson]{GFJ15}
Ghadimi, E., Feyzmahdavian, H.~R., and Johansson, M.
\newblock Global convergence of the heavy-ball method for convex optimization.
\newblock \emph{ECC}, 2015.

\bibitem[Gidel et~al.(2019)Gidel, Bach, and Lacoste-Julien]{GBL19}
Gidel, G., Bach, F., and Lacoste-Julien, S.
\newblock Implicit regularization of discrete gradient dynamics in linear
  neural networks.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Gissin et~al.(2020)Gissin, Shalev-Shwartz, and Daniely]{GSD20}
Gissin, D., Shalev-Shwartz, S., and Daniely, A.
\newblock The implicit bias of depth: How incremental learning drives
  generalization.
\newblock \emph{ICLR}, 2020.

\bibitem[Goujaud et~al.(2023)Goujaud, Taylor, and
  Dieuleveut]{goujaud2023provable}
Goujaud, B., Taylor, A., and Dieuleveut, A.
\newblock Provable non-accelerations of the heavy-ball method.
\newblock \emph{arXiv preprint arXiv:2307.11291}, 2023.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{Rnet16}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock \emph{Conference on Computer Vision and Pattern Recognition (CVPR)},
  2016.

\bibitem[Hu(2020)]{HU20}
Hu, B.
\newblock Ece 598: (lecture 2) {U}nifying the analysis in control and
  optimization via semidefinite programs.
\newblock 2020.
\newblock URL
  \url{https://binhu7.github.io/courses/ECE598/Fall2020/files/LectureNote2_SDPLTI.pdf}.

\bibitem[Jain et~al.(2018)Jain, Kakade, Kidambi, Netrapalli, and
  Sidford]{JKKNS18}
Jain, P., Kakade, S.~M., Kidambi, R., Netrapalli, P., and Sidford, A.
\newblock Accelerating stochastic gradient descent for least squares
  regression.
\newblock \emph{COLT}, 2018.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{KNS16}
Karimi, H., Nutini, J., and Schmidt, M.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-lojasiewicz condition.
\newblock \emph{ECML}, 2016.

\bibitem[Karimi et~al.(2017)Karimi, Nutini, and Schmidt]{KNS17}
Karimi, H., Nutini, J., and Schmidt, M.
\newblock Linear convergence under the polyak-Łojasiewicz inequality.
\newblock 2017.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{KB15}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{ICLR}, 2015.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{KSH12}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{NeurIPS}, 2012.

\bibitem[Kulakova et~al.(2018)Kulakova, Danilova, and Polyak]{KDP18}
Kulakova, A., Danilova, M., and Polyak, B.
\newblock Non-monotone behavior of the heavy ball method.
\newblock \emph{arXiv:1811.00658}, 2018.

\bibitem[Lessard et~al.(2016)Lessard, Recht, and Packard]{LRP16}
Lessard, L., Recht, B., and Packard, A.
\newblock Analysis and design of optimization algorithms via integral quadratic
  constraints.
\newblock \emph{SIAM Journal on Optimization}, 2016.

\bibitem[Liu et~al.(2021)Liu, Zhu, and Belkin]{LZB21}
Liu, C., Zhu, L., and Belkin, M.
\newblock Loss landscapes and optimization in over-parameterized non-linear
  systems and neural networks.
\newblock 2021.

\bibitem[Liu et~al.(2020)Liu, Gao, and Yin]{LGY20}
Liu, Y., Gao, Y., and Yin, W.
\newblock An improved analysis of stochastic gradient descent with momentum.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Loizou \& Richt{\'a}rik(2020)Loizou and Richt{\'a}rik]{LR17}
Loizou, N. and Richt{\'a}rik, P.
\newblock Momentum and stochastic momentum for stochastic gradient, newton,
  proximal point and subspace descent methods.
\newblock \emph{Computational Optimization and Applications}, 2020.

\bibitem[Merigot et~al.(2021)Merigot, Santambrogio, and Sarrazin]{MSS21}
Merigot, Q., Santambrogio, F., and Sarrazin, C.
\newblock Non-asymptotic convergence bounds for wasserstein approximation using
  point clouds.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Mitliagkas(2019)]{M19}
Mitliagkas, I.
\newblock Accelerated methods - polyak’s momentum (heavy ball method).
\newblock \emph{Online Lecture Note}, 2019.

\bibitem[Nesterov(2013)]{N13}
Nesterov, Y.
\newblock Introductory lectures on convex optimization: a basic course.
\newblock \emph{Springer}, 2013.

\bibitem[Ochs et~al.(2015)Ochs, Brox, and Pock]{OBP15}
Ochs, P., Brox, T., and Pock, T.
\newblock {iPiasco}: Inertial proximal algorithm for strongly convex
  optimization.
\newblock \emph{Journal of Mathematical Imaging and Vision}, 2015.

\bibitem[Oymak \& Soltanolkotabi(2019)Oymak and Soltanolkotabi]{OS19}
Oymak, S. and Soltanolkotabi, M.
\newblock Overparameterized nonlinear learning: Gradient descent takes the
  shortest path?
\newblock \emph{ICML}, 2019.

\bibitem[Paquette \& Paquette(2021)Paquette and Paquette]{PP21}
Paquette, C. and Paquette, E.
\newblock Dynamics of stochastic momentum methods on large-scale, quadratic
  models.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Polyak(1963)]{P63}
Polyak, B.
\newblock Gradient methods for minimizing functionals.
\newblock \emph{Zhurnal Vychislitel’noi Matematiki i Matematicheskoi Fiziki},
  1963.

\bibitem[Polyak(1964)]{P64}
Polyak, B.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics}, 1964.

\bibitem[Recht(2018)]{R18}
Recht, B.
\newblock Lyapunov analysis and the heavy ball method.
\newblock \emph{Lecture note}, 2018.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{RKK18}
Reddi, S.~J., Kale, S., and Kumar, S.
\newblock On the convergence of adam and beyond.
\newblock \emph{ICLR}, 2018.

\bibitem[Scieur \& Pedregosa(2020)Scieur and Pedregosa]{SP20}
Scieur, D. and Pedregosa, F.
\newblock Universal average-case optimality of polyak momentum.
\newblock \emph{ICML}, 2020.

\bibitem[Shi et~al.(2021)Shi, Du, Jordan, and Su]{SDJS18}
Shi, B., Du, S.~S., Jordan, M.~I., and Su, W.~J.
\newblock Understanding the acceleration phenomenon via high-resolution
  differential equations.
\newblock \emph{Mathematical Programming}, 2021.

\bibitem[Va{\v s}kevi{\v c}ius et~al.(2019)Va{\v s}kevi{\v c}ius, Kanade, and
  Rebeschini]{VKR19}
Va{\v s}kevi{\v c}ius, T., Kanade, V., and Rebeschini, P.
\newblock Implicit regularization for optimal sparse recovery.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Wang \& Abernethy(2018)Wang and Abernethy]{wang2018acceleration}
Wang, J.-K. and Abernethy, J.~D.
\newblock Acceleration through optimistic no-regret dynamics.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Wang et~al.(2020)Wang, Lin, and Abernethy]{WCA20}
Wang, J.-K., Lin, C.-H., and Abernethy, J.
\newblock Escaping saddle points faster with stochastic momentum.
\newblock \emph{ICLR}, 2020.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Abernethy, and Levy]{WAL21}
Wang, J.-K., Abernethy, J., and Levy, K.~Y.
\newblock No-regret dynamics in the fenchel game: A unified framework for
  algorithmic convex optimization.
\newblock \emph{arXiv:2111.11309}, 2021{\natexlab{a}}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Lin, and Abernethy]{WLA21}
Wang, J.-K., Lin, C.-H., and Abernethy, J.
\newblock A modular analysis of provable acceleration via polyak's momentum:
  Training a wide relu network and a deep linear network.
\newblock \emph{ICML}, 2021{\natexlab{b}}.

\bibitem[Wang et~al.(2022)Wang, Lin, Wibisono, and Hu]{wang2022provable}
Wang, J.-K., Lin, C.-H., Wibisono, A., and Hu, B.
\newblock Provable acceleration of heavy ball beyond quadratics for a class of
  polyak-lojasiewicz functions when the non-convexity is averaged-out.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  22839--22864. PMLR, 2022.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, , and
  Recht.]{WRSSR17}
Wilson, A.~C., Roelofs, R., Stern, M., Srebro, N., , and Recht., B.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock \emph{NIPS}, 2017.

\bibitem[Woodworth et~al.(2020)Woodworth, Gunasekar, Savarese, Moroshko, Golan,
  Lee, Soudry, and Srebro]{WGSMGLSS20}
Woodworth, B., Gunasekar, S., Savarese, P., Moroshko, E., Golan, I., Lee, J.,
  Soudry, D., and Srebro, N.
\newblock Kernel and rich regimes in overparametrized models.
\newblock \emph{COLT}, 2020.

\bibitem[Xiong et~al.(2020)Xiong, Chi, Hu, and Zhang]{XCHZ20}
Xiong, H., Chi, Y., Hu, B., and Zhang, W.
\newblock Analytical convergence regions of accelerated gradient descent in
  nonconvex optimization under regularity condition.
\newblock \emph{Automatica}, 2020.

\end{thebibliography}
