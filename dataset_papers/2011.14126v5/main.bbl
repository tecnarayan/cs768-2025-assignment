\begin{thebibliography}{10}

\bibitem{ahmadi2012entropic}
Amir Ahmadi-Javid.
\newblock Entropic value-at-risk: A new coherent risk measure.
\newblock {\em Journal of Optimization Theory and Applications},
  155(3):1105--1123, 2012.

\bibitem{alquier2020non}
Pierre Alquier.
\newblock Non-exponentially weighted aggregation: regret bounds for unbounded
  loss functions.
\newblock {\em arXiv preprint arXiv:2009.03017}, 2020.

\bibitem{audibert2004pac}
Jean-Yves Audibert.
\newblock {PAC-B}ayesian statistical learning theory.
\newblock {\em These de doctorat de l’Universit{\'e} Paris}, 6:29, 2004.

\bibitem{bartlett2006convexity}
Peter~L. Bartlett, Michael~I. Jordan, and Jon~D. McAuliffe.
\newblock Convexity, classification, and risk bounds.
\newblock {\em Journal of the American Statistical Association},
  101(473):138--156, 2006.

\bibitem{bartlett2006empirical}
Peter~L. Bartlett and Shahar Mendelson.
\newblock Empirical minimization.
\newblock {\em Probability Theory and Related Fields}, 135(3):311--334, 2006.

\bibitem{belkin2018reconciling}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine learning and the bias-variance trade-off.
\newblock {\em arXiv preprint arXiv:1812.11118}, 2018.

\bibitem{belkin2019two}
Mikhail Belkin, Daniel Hsu, and Ji~Xu.
\newblock Two models of double descent for weak features.
\newblock {\em arXiv preprint arXiv:1903.07571}, 2019.

\bibitem{ben2012minimizing}
Shai Ben-David, David Loker, Nathan Srebro, and Karthik Sridharan.
\newblock Minimizing the misclassification error rate using a surrogate convex
  loss.
\newblock In {\em Proceedings of the 29th International Coference on
  International Conference on Machine Learning}, pages 83--90, 2012.

\bibitem{ben2011universal}
Shai Ben-David, Nathan Srebro, and Ruth Urner.
\newblock Universal learning vs. no free lunch results.
\newblock In {\em Philosophy and Machine Learning Workshop NIPS}, 2011.

\bibitem{cesa06}
Nicolo Cesa-Bianchi and G{\'a}bor Lugosi.
\newblock {\em Prediction, learning, and games}.
\newblock Cambridge university press, 2006.

\bibitem{cheema2020geometric}
Prasad Cheema and Mahito Sugiyama.
\newblock A geometric look at double descent risk: Volumes, singularities, and
  distinguishabilities.
\newblock {\em arXiv preprint arXiv:2006.04366}, 2020.

\bibitem{chen2020multiple}
Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi.
\newblock Multiple descent: Design your own generalization curve.
\newblock {\em arXiv preprint arXiv:2008.01036}, 2020.

\bibitem{darling1967iterated}
DA~Darling and Herbert Robbins.
\newblock Iterated logarithm inequalities.
\newblock {\em Proceedings of the National Academy of Sciences of the United
  States of America}, 57(5):1188, 1967.

\bibitem{d2020triple}
St{\'e}phane d'Ascoli, Levent Sagun, and Giulio Biroli.
\newblock Triple descent and the two kinds of overfitting: Where \& why do they
  appear?
\newblock {\em arXiv preprint arXiv:2006.03509}, 2020.

\bibitem{deng2019model}
Zeyu Deng, Abla Kammoun, and Christos Thrampoulidis.
\newblock A model of double descent for high-dimensional binary linear
  classification.
\newblock {\em arXiv preprint arXiv:1911.05822}, 2019.

\bibitem{derezinski2020exact}
Michal Derezinski, Feynman~T Liang, and Michael~W Mahoney.
\newblock Exact expressions for double descent and implicit regularization via
  surrogate random design.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{dereziski2019exact}
Michał Dereziński, Feynman Liang, and Michael~W. Mahoney.
\newblock Exact expressions for double descent and implicit regularization via
  surrogate random design, 2019.

\bibitem{duin1995}
Robert~P.W. Duin.
\newblock Small sample size generalization.
\newblock In {\em Proceedings of the Scandinavian Conference on Image
  Analysis}, volume~2, pages 957--964, 1995.

\bibitem{duin2000classifiers}
Robert~P.W. Duin.
\newblock Classifiers in almost empty spaces.
\newblock In {\em Proceedings 15th International Conference on Pattern
  Recognition. ICPR-2000}, volume~2, pages 1--7. IEEE, 2000.

\bibitem{Opper2001}
Robert~P.W. Duin.
\newblock Learning to generalize.
\newblock {\em Frontiers of Life}, 3(part 2):763--775, 2001.

\bibitem{erven2015fast}
Tim~{Van} Erven, Nishant~A. Mehta, Mark~D. Reid, and Robert~C. Williamson.
\newblock Fast rates in statistical and online learning.
\newblock {\em Journal of Machine Learning Research}, 16:1793--1861, 2015.

\bibitem{Foster2018}
Dylan~J. Foster, Alexander Rakhlin, and Karthik Sridharan.
\newblock Online learning: Sufficient statistics and the {B}urkholder method.
\newblock In {\em Conference On Learning Theory, {COLT} 2018, Stockholm,
  Sweden, 6-9 July 2018.}, pages 3028--3064, 2018.

\bibitem{freedman1975tail}
David~A Freedman.
\newblock On tail probabilities for martingales.
\newblock {\em the Annals of Probability}, pages 100--118, 1975.

\bibitem{grunwald2011bounds}
Peter~D Gr{\"u}nwald and Wojciech Kot{\l}owski.
\newblock Bounds on individual risk for log-loss predictors.
\newblock In {\em Proceedings of the 24th Annual Conference on Learning
  Theory}, pages 813--816, 2011.

\bibitem{guedj2019primer}
Benjamin Guedj.
\newblock A primer on pac-bayesian learning.
\newblock {\em arXiv preprint arXiv:1901.05353}, 2019.

\bibitem{howard2021time}
Steven~R Howard, Aaditya Ramdas, Jon McAuliffe, and Jasjeet Sekhon.
\newblock Time-uniform, nonparametric, nonasymptotic confidence sequences.
\newblock {\em The Annals of Statistics}, 49(2):1055--1080, 2021.

\bibitem{howard2020time}
Steven~R Howard, Aaditya Ramdas, Jon McAuliffe, Jasjeet Sekhon, et~al.
\newblock Time-uniform chernoff bounds via nonnegative supermartingales.
\newblock {\em Probability Surveys}, 17:257--317, 2020.

\bibitem{jun2019parameter}
Kwang-Sung Jun and Francesco Orabona.
\newblock Parameter-free online convex optimization with sub-exponential noise.
\newblock In {\em Conference on Learning Theory}, pages 1802--1823, 2019.

\bibitem{koolen2016combining}
Wouter~M. Koolen, Peter~D. Gr{\"u}nwald, and Tim {Van}~Erven.
\newblock Combining adversarial guarantees and stochastic fast rates in online
  learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4457--4465, 2016.

\bibitem{kramer2009peaking}
Nicole Kr{\"a}mer.
\newblock On the peaking phenomenon of the lasso in model selection.
\newblock {\em arXiv preprint arXiv:0904.4416}, 2009.

\bibitem{loog2015contrastive}
Marco Loog.
\newblock Contrastive pessimistic likelihood estimation for semi-supervised
  classification.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  38(3):462--475, 2015.

\bibitem{loog2012dipping}
Marco Loog and Robert~P.W. Duin.
\newblock The dipping phenomenon.
\newblock In {\em Joint IAPR International Workshops on Statistical Techniques
  in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition
  (SSPR)}, pages 310--317. Springer, 2012.

\bibitem{loog2019minimizers}
Marco Loog, Tom Viering, and Alexander Mey.
\newblock Minimizers of the empirical risk and risk monotonicity.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7476--7485, 2019.

\bibitem{Loog10625}
Marco Loog, Tom Viering, Alexander Mey, Jesse~H. Krijthe, and David M.~J. Tax.
\newblock A brief prehistory of double descent.
\newblock {\em Proceedings of the National Academy of Sciences},
  117(20):10625--10626, 2020.

\bibitem{maurer2009empirical}
Andreas Maurer and Massimiliano Pontil.
\newblock Empirical bernstein bounds and sample variance penalization.
\newblock {\em arXiv preprint arXiv:0907.3740}, 2009.

\bibitem{MaurerP09}
Andreas Maurer and Massimiliano Pontil.
\newblock Empirical bernstein bounds and sample-variance penalization.
\newblock In {\em {COLT} 2009 - The 22nd Conference on Learning Theory,
  Montreal, Quebec, Canada, June 18-21, 2009}, 2009.

\bibitem{mei2019generalization}
Song Mei and Andrea Montanari.
\newblock The generalization error of random features regression: Precise
  asymptotics and double descent curve.
\newblock {\em arXiv preprint arXiv:1908.05355}, 2019.

\bibitem{mhammedi2019pac}
Zakaria Mhammedi, Peter Gr{\"u}nwald, and Benjamin Guedj.
\newblock Pac-bayes un-expected bernstein inequality.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  12202--12213, 2019.

\bibitem{MhammediK20}
Zakaria Mhammedi and Wouter~M. Koolen.
\newblock Lipschitz and comparator-norm adaptivity in online learning.
\newblock In {\em Conference on Learning Theory, {COLT} 2020, 9-12 July 2020,
  Virtual Event [Graz, Austria]}, volume 125 of {\em Proceedings of Machine
  Learning Research}, pages 2858--2887. {PMLR}, 2020.

\bibitem{nakkiran2019more}
Preetum Nakkiran.
\newblock More data can hurt for linear regression: Sample-wise double descent.
\newblock {\em arXiv preprint arXiv:1912.07242}, 2019.

\bibitem{nakkiran2020deep}
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya
  Sutskever.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{nakkiran2020optimal}
Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma.
\newblock Optimal regularization can mitigate double descent.
\newblock {\em arXiv preprint arXiv:2003.01897}, 2020.

\bibitem{opper1996statistical}
Manfred Opper and Wolfgang Kinzel.
\newblock Statistical mechanics of generalization.
\newblock In {\em Models of neural networks III}, pages 151--209. Springer,
  1996.

\bibitem{pena2008self}
V.H. Pe{\~n}a, T.L. Lai, and Q.M. Shao.
\newblock {\em Self-Normalized Processes: Limit Theory and Statistical
  Applications}.
\newblock Probability and Its Applications. Springer Berlin Heidelberg, 2008.

\bibitem{rakhlin2017equivalence}
Alexander Rakhlin and Karthik Sridharan.
\newblock On equivalence of martingale tail bounds and deterministic regret
  inequalities.
\newblock In {\em Conference on Learning Theory}, pages 1704--1722. PMLR, 2017.

\bibitem{rivasplata2020pac}
Omar Rivasplata, Ilja Kuzborskij, Csaba Szepesv{\'a}ri, and John Shawe-Taylor.
\newblock Pac-bayes analysis beyond the usual bounds.
\newblock In {\em NeurIPS}, 2020.

\bibitem{shalev2014understanding}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock {\em Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem{smola2000advances}
Alexander~J. Smola, Peter~J. Bartlett, Dale Schuurmans, and Bernhard
  Sch{\"o}lkopf.
\newblock {\em Advances in large margin classifiers}.
\newblock MIT press, 2000.

\bibitem{spigler2018jamming}
Stefano Spigler, Mario Geiger, St{\'e}phane d'Ascoli, Levent Sagun, Giulio
  Biroli, and Matthieu Wyart.
\newblock A jamming transition from under-to over-parametrization affects loss
  landscape and generalization.
\newblock {\em arXiv preprint arXiv:1810.09665}, 2018.

\bibitem{TolstikhinS13}
Ilya~O. Tolstikhin and Yevgeny Seldin.
\newblock Pac-bayes-empirical-bernstein inequality.
\newblock In {\em Advances in Neural Information Processing Systems 26: 27th
  Annual Conference on Neural Information Processing Systems 2013. Proceedings
  of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States},
  pages 109--117, 2013.

\bibitem{viallard2021general}
Paul Viallard, Pascal Germain, Amaury Habrard, and Emilie Morvant.
\newblock A general framework for the disintegration of pac-bayesian bounds,
  2021.

\bibitem{viering2021shape}
Tom Viering and Marco Loog.
\newblock The shape of learning curves: a review.
\newblock {\em arXiv preprint arXiv:2103.10948}, 2021.

\bibitem{viering2019open}
Tom Viering, Alexander Mey, and Marco Loog.
\newblock Open problem: Monotonicity of learning.
\newblock In {\em Conference on Learning Theory}, pages 3198--3201, 2019.

\bibitem{viering2019making}
Tom~J. Viering, Alexander Mey, and Marco Loog.
\newblock Making learners (more) monotone, 2019.

\bibitem{ZhouVAAO19}
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan~P. Adams, and Peter Orbanz.
\newblock Non-vacuous generalization bounds at the imagenet scale: a
  pac-bayesian compression approach.
\newblock In {\em 7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}, 2019.

\end{thebibliography}
