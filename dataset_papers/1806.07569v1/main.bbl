\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrew \& Gao(2007)Andrew and Gao]{Andrew:2007cu}
Andrew, G. and Gao, J.
\newblock {Scalable training of L1-regularized log-linear models}.
\newblock In \emph{International Conference on Machine Learning}, 2007.

\bibitem[Bach et~al.(2010)]{bach}
Bach, F. et~al.
\newblock Self-concordant analysis for logistic regression.
\newblock \emph{Electronic Journal of Statistics}, 4:\penalty0 384--414, 2010.

\bibitem[Cartis et~al.(2011{\natexlab{a}})Cartis, Gould, and Toint]{Cartis2011}
Cartis, C., Gould, N. I.~M., and Toint, P.~L.
\newblock Adaptive cubic regularisation methods for unconstrained optimization.
  part i: motivation, convergence and numerical results.
\newblock \emph{Mathematical Programming}, 127\penalty0 (2):\penalty0 245--295,
  Apr 2011{\natexlab{a}}.
\newblock ISSN 1436-4646.

\bibitem[Cartis et~al.(2011{\natexlab{b}})Cartis, Gould, and
  Toint]{Cartis2011b}
Cartis, C., Gould, N. I.~M., and Toint, P.~L.
\newblock Adaptive cubic regularisation methods for unconstrained optimization.
  part ii: worst case function and derivative evaluation complexity.
\newblock \emph{Mathematical Programming}, 127\penalty0 (2):\penalty0 245--295,
  Apr 2011{\natexlab{b}}.
\newblock ISSN 1436-4646.

\bibitem[Conn et~al.(2000)Conn, Gould, and Toint]{conn2000trust}
Conn, A.~R., Gould, N.~I., and Toint, P.~L.
\newblock \emph{Trust region methods}.
\newblock SIAM, 2000.

\bibitem[D{\"u}nner et~al.(2016)D{\"u}nner, Forte, Tak{\'a}{\v c}, and
  Jaggi]{duenner16}
D{\"u}nner, C., Forte, S., Tak{\'a}{\v c}, M., and Jaggi, M.
\newblock Primal-dual rates and certificates.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Gargiani(2017)]{gargiani2017hessian}
Gargiani, M.
\newblock Hessian-cocoa: a general parallel and distributed framework for
  non-strongly convex regularizers.
\newblock Master's thesis, ETH Zurich, 2017.

\bibitem[Hsieh et~al.(2016)Hsieh, Si, and Dhillon]{Hsieh:2016wg}
Hsieh, C.-J., Si, S., and Dhillon, I.~S.
\newblock {Communication-Efficient Parallel Block Minimization for Kernel
  Machines}.
\newblock \emph{arXiv}, August 2016.

\bibitem[Jaggi et~al.(2014)Jaggi, Smith, Tak{\'a}{\v c}, Terhorst, Krishnan,
  Hofmann, and Jordan]{Jaggi:2014vi}
Jaggi, M., Smith, V., Tak{\'a}{\v c}, M., Terhorst, J., Krishnan, S., Hofmann,
  T., and Jordan, M.~I.
\newblock {Communication-efficient distributed dual coordinate ascent}.
\newblock In \emph{Neural Information Processing Systems}, 2014.

\bibitem[Karimireddy et~al.(2018)Karimireddy, Stich, and
  Jaggi]{karimireddy2018newton}
Karimireddy, S.~P., Stich, S.~U., and Jaggi, M.
\newblock {Global linear convergence of Newton's method without
  strong-convexity or Lipschitz gradients}.
\newblock \emph{arXiv}, June 2018.

\bibitem[Lee \& Chang(2017)Lee and Chang]{lee2017distributed}
Lee, C.-p. and Chang, K.-W.
\newblock Distributed block-diagonal approximation methods for regularized
  empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1709.03043}, 2017.

\bibitem[Lee \& Wright(2018)Lee and Wright]{lee2018inexact}
Lee, C.-p. and Wright, S.~J.
\newblock Inexact successive quadratic approximation for regularized
  optimization.
\newblock \emph{arXiv preprint arXiv:1803.01298}, 2018.

\bibitem[Lee et~al.(2018)Lee, Lim, and Wright]{lee2018distributed}
Lee, C.-p., Lim, C.~H., and Wright, S.~J.
\newblock A distributed quasi-newton algorithm for empirical risk minimization
  with nonsmooth regularization.
\newblock In \emph{KDD 2018 - The 24th ACM SIGKDD International Conference on
  Knowledge Discovery \& Data Mining}, August 2018.

\bibitem[Lee et~al.(2015)Lee, Lin, Ma, and Yang]{lee2015distributed}
Lee, J.~D., Lin, Q., Ma, T., and Yang, T.
\newblock Distributed stochastic variance reduced gradient methods and a lower
  bound for communication complexity.
\newblock \emph{arXiv preprint arXiv:1507.07595}, 2015.

\bibitem[Mahajan et~al.(2017)Mahajan, Keerthi, and
  Sundararajan]{mahajan2017distributed}
Mahajan, D., Keerthi, S.~S., and Sundararajan, S.
\newblock A distributed block coordinate descent method for training l 1
  regularized linear classifiers.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (91):\penalty0 1--35, 2017.

\bibitem[Nesterov \& Polyak(2006)Nesterov and Polyak]{nesterov2006cubic}
Nesterov, Y. and Polyak, B.~T.
\newblock Cubic regularization of newton method and its global performance.
\newblock \emph{Mathematical Programming}, 108\penalty0 (1):\penalty0 177--205,
  2006.

\bibitem[Niu et~al.(2011)Niu, Recht, R{\'e}, and Wright]{Niu:2011wo}
Niu, F., Recht, B., R{\'e}, C., and Wright, S.~J.
\newblock Hogwild!: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In \emph{Neural Information Processing Systems}, 2011.

\bibitem[Reddi et~al.(2016)Reddi, Kone{\v{c}}n{\`y}, Richt{\'a}rik,
  P{\'o}cz{\'o}s, and Smola]{reddi2016aide}
Reddi, S.~J., Kone{\v{c}}n{\`y}, J., Richt{\'a}rik, P., P{\'o}cz{\'o}s, B., and
  Smola, A.
\newblock Aide: Fast and communication efficient distributed optimization.
\newblock \emph{arXiv preprint arXiv:1608.06879}, 2016.

\bibitem[Richt{\'a}rik \& Tak{\'a}{\v{c}}(2016)Richt{\'a}rik and
  Tak{\'a}{\v{c}}]{richtarik2013distributed}
Richt{\'a}rik, P. and Tak{\'a}{\v{c}}, M.
\newblock Distributed coordinate descent method for learning with big data.
\newblock \emph{Journal of Machine Learning Research}, 17:\penalty0 1--25,
  2016.

\bibitem[Shalev-Shwartz \& Zhang(2013)Shalev-Shwartz and Zhang]{sdca}
Shalev-Shwartz, S. and Zhang, T.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock \emph{Journal of Machine Learning Research}, 14\penalty0
  (Feb):\penalty0 567--599, 2013.

\bibitem[Shamir et~al.(2014)Shamir, Srebro, and Zhang]{shamir2014communication}
Shamir, O., Srebro, N., and Zhang, T.
\newblock Communication-efficient distributed optimization using an approximate
  newton-type method.
\newblock In \emph{International conference on machine learning}, pp.\
  1000--1008, 2014.

\bibitem[Smith et~al.(2018)Smith, Forte, Ma, Tak{\'a}{\v c}, Jordan, and
  Jaggi]{Smith:2016wp}
Smith, V., Forte, S., Ma, C., Tak{\'a}{\v c}, M., Jordan, M.~I., and Jaggi, M.
\newblock {CoCoA: A General Framework for Communication-Efficient Distributed
  Optimization}.
\newblock \emph{Journal of Machine Learning Research (and arXiv:1611.02189)},
  2018.

\bibitem[Trofimov \& Genkin(2017)Trofimov and Genkin]{Trofimov:2017ho}
Trofimov, I. and Genkin, A.
\newblock {Distributed coordinate descent for generalized linear models with
  regularization}.
\newblock \emph{Pattern Recognition and Image Analysis}, 27\penalty0
  (2):\penalty0 349--364, June 2017.

\bibitem[Wang et~al.(2017)Wang, Roosta-Khorasani, Xu, and
  Mahoney]{wang2017giant}
Wang, S., Roosta-Khorasani, F., Xu, P., and Mahoney, M.~W.
\newblock Giant: Globally improved approximate newton method for distributed
  optimization.
\newblock \emph{arXiv preprint arXiv:1709.03528}, 2017.

\bibitem[Yang(2013)]{yang2013trading}
Yang, T.
\newblock Trading computation for communication: Distributed stochastic dual
  coordinate ascent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  629--637, 2013.

\bibitem[Zhang \& Lin(2015)Zhang and Lin]{zhang2015disco}
Zhang, Y. and Lin, X.
\newblock Disco: Distributed optimization for self-concordant empirical loss.
\newblock In \emph{International conference on machine learning}, pp.\
  362--370, 2015.

\bibitem[Zheng et~al.(2017)Zheng, Wang, Xia, Xu, and Zhang]{zheng2017general}
Zheng, S., Wang, J., Xia, F., Xu, W., and Zhang, T.
\newblock A general distributed dual coordinate optimization framework for
  regularized loss minimization.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (115):\penalty0 1--52, 2017.

\end{thebibliography}
