@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice",
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}
This file was created with JabRef 1.8.1.
Encoding: Cp1252

@STRING{aaai = {Proceedings of \ AAAI Conference on Artificial Intelligence}}

@STRING{colt = {Proceedings \ Annual Conf.\ on Learning Theory}}

@STRING{cvpr = {Proc.\ CVPR}}

@STRING{ecml = {Proc.\ European Conf.\ on Mach. Learn.}}

@STRING{ewcbr = {Proc.\ the European Workshop on Advances in Case-Based Reasoning}}

@STRING{fg = {Proc.\ Int.\ Conf.\ on Face and Gesture Recognition}}

@STRING{iccv = {Proc.\ Int.\ Conf.\ on Computer Vision}}

@STRING{icdm = {Proc.\ Int.\ Conf.\ on Data Mining}}

@STRING{icml = {Proceedings of the International Conference on Machine Learning}}

@STRING{icpr = {Proc.\ Int.\ Conf.\ on Pattern Recognition}}

@STRING{icvp = {Proc.\ Int.\ Conf.\ Vision and Pattern Recognition}}

@STRING{ijcai = {Proc.\ Int.\ Joint Conf.\ on Artificial Intelligence}}

@STRING{ijcv = {Int.\ J.\ of Computer Vision}}

@STRING{kdd = {Proc.\ ACM SIGKDD Int.\ Conf.\ on Knowledge Discovery and Data Mining}}

@STRING{mm = {Proc.\ ACM Multimedia}}

@STRING{nips = {NIPS}}

@STRING{pami = {IEEE Trans.\ Pattern Anal.\ Mach.\ Intell.}}

@STRING{sigir = {Proc.\ ACM SIGIR}}

@STRING{tvcg = {IEEE Trans. Visualization and Computer Graphics}}

@STRING{uai = {Proc.\ Uncertainty in Artificial Intelligence}}

@STRING{pcm = {Proc.\ Pacific-Rim Conf.\ on Multimedia}}

@STRING{nips = {Proc.\ Conf.\ on Advances in Neural Information Processing Systems}}


This file was created with JabRef 1.8.1.
Encoding: Cp1252

@STRING{aaai = {Proceedings of \ AAAI Conference on Artificial Intelligence}}

@STRING{colt = {Proceedings \ Annual Conf.\ on Learning Theory}}

@STRING{cvpr = {Proc.\ CVPR}}

@STRING{ecml = {Proc.\ European Conf.\ on Mach. Learn.}}

@STRING{ewcbr = {Proc.\ the European Workshop on Advances in Case-Based Reasoning}}

@STRING{fg = {Proc.\ Int.\ Conf.\ on Face and Gesture Recognition}}

@STRING{iccv = {Proc.\ Int.\ Conf.\ on Computer Vision}}

@STRING{icdm = {Proc.\ Int.\ Conf.\ on Data Mining}}

@STRING{icml = {Proceedings of the International Conference on Machine Learning}}

@STRING{icpr = {Proc.\ Int.\ Conf.\ on Pattern Recognition}}

@STRING{icvp = {Proc.\ Int.\ Conf.\ Vision and Pattern Recognition}}

@STRING{ijcai = {Proc.\ Int.\ Joint Conf.\ on Artificial Intelligence}}

@STRING{ijcv = {Int.\ J.\ of Computer Vision}}

@STRING{kdd = {Proc.\ ACM SIGKDD Int.\ Conf.\ on Knowledge Discovery and Data Mining}}

@STRING{mm = {Proc.\ ACM Multimedia}}

@STRING{nips = {NIPS}}

@STRING{pami = {IEEE Trans.\ Pattern Anal.\ Mach.\ Intell.}}

@STRING{sigir = {Proc.\ ACM SIGIR}}

@STRING{tvcg = {IEEE Trans. Visualization and Computer Graphics}}

@STRING{uai = {Proc.\ Uncertainty in Artificial Intelligence}}

@STRING{pcm = {Proc.\ Pacific-Rim Conf.\ on Multimedia}}

@STRING{nips = {Proc.\ Conf.\ on Advances in Neural Information Processing Systems}}


	@article{nemirovski2009robust,
	title={Robust stochastic approximation approach to stochastic programming},
	author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
	journal={SIAM Journal on optimization},
	volume={19},
	number={4},
	pages={1574--1609},
	year={2009},
	publisher={SIAM}
	}

	@article{hazan2014beyond,
		title={Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization.},
		author={Hazan, Elad and Kale, Satyen},
		journal={Journal of Machine Learning Research},
		volume={15},
		number={1},
		pages={2489--2512},
		year={2014}
	}
@inproceedings{ying2016stochastic,
  title={Stochastic Online AUC Maximization},
  author={Ying, Yiming and Wen, Longyin and Lyu, Siwei},
  booktitle={Advances in Neural Information Processing Systems},
  pages={451--459},
  year={2016}
}
@article{li2013global,
  title={Global error bounds for piecewise convex polynomials},
  author={Li, Guoyin},
  journal={Mathematical Programming},
  pages={1--28},
  year={2013},
  publisher={Springer}
}
@article{clemenccon2008ranking,
	title={Ranking and empirical minimization of U-statistics},
	author={Cl{\'e}men{\c{c}}on, St{\'e}phan and Lugosi, G{\'a}bor and Vayatis, Nicolas},
	journal={The Annals of Statistics},
	pages={844--874},
	year={2008},
	publisher={JSTOR}
}
@inproceedings{zhao2011online,
	title={Online AUC maximization},
	author={Zhao, Peilin and Jin, Rong and Yang, Tianbao and Hoi, Steven C},
	booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
	pages={233--240},
	year={2011}
}
@inproceedings{gao2013one,
	title={One-Pass AUC Optimization.},
	author={Gao, Wei and Jin, Rong and Zhu, Shenghuo and Zhou, Zhi-Hua},
	booktitle={ICML (3)},
	pages={906--914},
	year={2013}
}
@article{hanley1982meaning,
	title={The meaning and use of the area under a receiver operating characteristic (ROC) curve.},
	author={Hanley, James A and McNeil, Barbara J},
	journal={Radiology},
	volume={143},
	number={1},
	pages={29--36},
	year={1982}
}
@article{powers2011evaluation,
	title={Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation},
	author={Powers, David Martin},
	year={2011},
	publisher={Bioinfo Publications}
}
@article{vitter1985random,
	title={Random sampling with a reservoir},
	author={Vitter, Jeffrey S},
	journal={ACM Transactions on Mathematical Software (TOMS)},
	volume={11},
	number={1},
	pages={37--57},
	year={1985},
	publisher={ACM}
}
@article{nemirovski2004prox,
	title={Prox-method with rate of convergence O (1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems},
	author={Nemirovski, Arkadi},
	journal={SIAM Journal on Optimization},
	volume={15},
	number={1},
	pages={229--251},
	year={2004},
	publisher={SIAM}
}
@article{korpelevich1976extragradient,
	title={The extragradient method for finding saddle points and other problems},
	author={Korpelevich, GM},
	journal={Matecon},
	volume={12},
	pages={747--756},
	year={1976}
}
@article{juditsky2011solving,
	title={Solving variational inequalities with stochastic mirror-prox algorithm},
	author={Juditsky, Anatoli and Nemirovski, Arkadi and Tauvel, Claire and others},
	journal={Stochastic Systems},
	volume={1},
	number={1},
	pages={17--58},
	year={2011},
	publisher={INFORMS Applied Probability Society}
}
@inproceedings{zinkevich2003online,
	title={Online convex programming and generalized infinitesimal gradient ascent},
	author={Zinkevich, Martin},
	booktitle={Proceedings of the 20th International Conference on Machine Learning (ICML-03)},
	pages={928--936},
	year={2003}
}
@article{nemirovskii1983problem,
	title={Problem complexity and method efficiency in optimization},
	author={Nemirovskii, Arkadii and Yudin, David Borisovich and Dawson, Edgar Ronald},
	year={1983},
	publisher={Wiley}
}
@book{shalev2014understanding,
	title={Understanding machine learning: From theory to algorithms},
	author={Shalev-Shwartz, Shai and Ben-David, Shai},
	year={2014},
	publisher={Cambridge university press}
}
@book{vapnik1998statistical,
	title={Statistical learning theory},
	author={Vapnik, Vladimir Naumovich and Vapnik, Vlamimir},
	volume={1},
	year={1998},
	publisher={Wiley New York}
}
@article{shalev2010learnability,
	title={Learnability, stability and uniform convergence},
	author={Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
	journal={Journal of Machine Learning Research},
	volume={11},
	number={Oct},
	pages={2635--2670},
	year={2010}
}
@article{nemirovski2009robust,
	title={Robust stochastic approximation approach to stochastic programming},
	author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
	journal={SIAM Journal on optimization},
	volume={19},
	number={4},
	pages={1574--1609},
	year={2009},
	publisher={SIAM}
}
@incollection{bottou2010large,
	title={Large-scale machine learning with stochastic gradient descent},
	author={Bottou, L{\'e}on},
	booktitle={Proceedings of COMPSTAT'2010},
	pages={177--186},
	year={2010},
	publisher={Springer}
}
@inproceedings{defazio2014saga,
	title={Saga: A fast incremental gradient method with support for non-strongly convex composite objectives},
	author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1646--1654},
	year={2014}
}
@article{allen2016katyusha,
	title={Katyusha: Accelerated variance reduction for faster SGD},
	author={Allen-Zhu, Zeyuan},
	journal={ArXiv e-prints, abs/1603.05953},
	year={2016}
}
@article{lan2015optimal,
	title={An optimal randomized incremental gradient method},
	author={Lan, Guanghui and Zhou, Yi},
	journal={arXiv preprint arXiv:1507.02000},
	year={2015}
}
@inproceedings{zhang2015stochastic,
	title={Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization.},
	author={Zhang, Yuchen and Xiao, Lin},
	booktitle={ICML},
	pages={353--361},
	year={2015}
}
@inproceedings{lin2014accelerated,
	title={An accelerated proximal coordinate gradient method},
	author={Lin, Qihang and Lu, Zhaosong and Xiao, Lin},
	booktitle={Advances in Neural Information Processing Systems},
	pages={3059--3067},
	year={2014}
}
@inproceedings{shalev2014accelerated,
	title={Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization.},
	author={Shalev-Shwartz, Shai and Zhang, Tong},
	booktitle={ICML},
	pages={64--72},
	year={2014}
}
@article{xiao2014proximal,
	title={A proximal stochastic gradient method with progressive variance reduction},
	author={Xiao, Lin and Zhang, Tong},
	journal={SIAM Journal on Optimization},
	volume={24},
	number={4},
	pages={2057--2075},
	year={2014},
	publisher={SIAM}
}
@inproceedings{shalev2013accelerated,
	title={Accelerated mini-batch stochastic dual coordinate ascent},
	author={Shalev-Shwartz, Shai and Zhang, Tong},
	booktitle={Advances in Neural Information Processing Systems},
	pages={378--385},
	year={2013}
}
@article{ouyang2013stochastic,
	title={Stochastic Alternating Direction Method of Multipliers.},
	author={Ouyang, Hua and He, Niao and Tran, Long and Gray, Alexander G},
	journal={ICML (1)},
	volume={28},
	pages={80--88},
	year={2013}
}
@inproceedings{suzuki2013dual,
	title={Dual Averaging and Proximal Gradient Descent for Online Alternating Direction Multiplier Method.},
	author={Suzuki, Taiji and others},
	booktitle={ICML (1)},
	pages={392--400},
	year={2013}
}
@inproceedings{recht2011hogwild,
	title={Hogwild: A lock-free approach to parallelizing stochastic gradient descent},
	author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
	booktitle={Advances in Neural Information Processing Systems},
	pages={693--701},
	year={2011}
}
@inproceedings{zinkevich2010parallelized,
	title={Parallelized stochastic gradient descent},
	author={Zinkevich, Martin and Weimer, Markus and Li, Lihong and Smola, Alex J},
	booktitle={Advances in neural information processing systems},
	pages={2595--2603},
	year={2010}
}
@inproceedings{yang2013trading,
	title={Trading computation for communication: Distributed stochastic dual coordinate ascent},
	author={Yang, Tianbao},
	booktitle={Advances in Neural Information Processing Systems},
	pages={629--637},
	year={2013}
}
@inproceedings{zhang2014asynchronous,
	title={Asynchronous Distributed ADMM for Consensus Optimization.},
	author={Zhang, Ruiliang and Kwok, James T},
	booktitle={ICML},
	pages={1701--1709},
	year={2014}
}
@inproceedings{roux2012stochastic,
	title={A stochastic gradient method with an exponential convergence \_rate for finite training sets},
	author={Roux, Nicolas L and Schmidt, Mark and Bach, Francis R},
	booktitle={Advances in Neural Information Processing Systems},
	pages={2663--2671},
	year={2012}
}
@inproceedings{defazio2016simple,
	title={A simple practical accelerated method for finite sums},
	author={Defazio, Aaron},
	booktitle={Advances In Neural Information Processing Systems},
	pages={676--684},
	year={2016}
}
@inproceedings{li2014scaling,
	title={Scaling Distributed Machine Learning with the Parameter Server.},
	author={Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
	booktitle={OSDI},
	volume={14},
	pages={583--598},
	year={2014}
}
@article{yang2013analysis,
	title={Analysis of distributed stochastic dual coordinate ascent},
	author={Yang, Tianbao and Zhu, Shenghuo and Jin, Rong and Lin, Yuanqing},
	journal={arXiv preprint arXiv:1312.1031},
	year={2013}
}

@inproceedings{recht2011hogwild,
	title={Hogwild: A lock-free approach to parallelizing stochastic gradient descent},
	author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
	booktitle={Advances in Neural Information Processing Systems},
	pages={693--701},
	year={2011}
}
@inproceedings{agarwal2011distributed,
	title={Distributed delayed stochastic optimization},
	author={Agarwal, Alekh and Duchi, John C},
	booktitle={Advances in Neural Information Processing Systems},
	pages={873--881},
	year={2011}
}
@article{gabay1976dual,
	title={A dual algorithm for the solution of nonlinear variational problems via finite element approximation},
	author={Gabay, Daniel and Mercier, Bertrand},
	journal={Computers \& Mathematics with Applications},
	volume={2},
	number={1},
	pages={17--40},
	year={1976},
	publisher={Elsevier}
}
@inproceedings{lian2015asynchronous,
	title={Asynchronous parallel stochastic gradient for nonconvex optimization},
	author={Lian, Xiangru and Huang, Yijun and Li, Yuncheng and Liu, Ji},
	booktitle={Advances in Neural Information Processing Systems},
	pages={2737--2745},
	year={2015}
}
@inproceedings{shalev2014accelerated,
	title={Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization},
	author={Shalev-Shwartz, Shai and Zhang, Tong},
	booktitle={International Conference on Machine Learning},
	pages={64--72},
	year={2014}
}
@inproceedings{johnson2013accelerating,
	title={Accelerating stochastic gradient descent using predictive variance reduction},
	author={Johnson, Rie and Zhang, Tong},
	booktitle={Advances in neural information processing systems},
	pages={315--323},
	year={2013}
}
@inproceedings{defazio2014saga,
	title={Saga: A fast incremental gradient method with support for non-strongly convex composite objectives},
	author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1646--1654},
	year={2014}
}
@article{shalev2013stochastic,
	title={Stochastic dual coordinate ascent methods for regularized loss minimization},
	author={Shalev-Shwartz, Shai and Zhang, Tong},
	journal={Journal of Machine Learning Research},
	volume={14},
	number={Feb},
	pages={567--599},
	year={2013}
}
@incollection{bottou2010large,
	title={Large-scale machine learning with stochastic gradient descent},
	author={Bottou, L{\'e}on},
	booktitle={Proceedings of COMPSTAT'2010},
	pages={177--186},
	year={2010},
	publisher={Springer}
}
@article{bottou2016optimization,
	title={Optimization methods for large-scale machine learning},
	author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
	journal={arXiv preprint arXiv:1606.04838},
	year={2016}
}
@book{cesa2006prediction,
	title={Prediction, learning, and games},
	author={Cesa-Bianchi, Nicolo and Lugosi, G{\'a}bor},
	year={2006},
	publisher={Cambridge university press}
}
@book{boyd2004convex,
	title={Convex optimization},
	author={Boyd, Stephen and Vandenberghe, Lieven},
	year={2004},
	publisher={Cambridge university press}
}
@inproceedings{busa2015online,
	title={Online F-measure optimization},
	author={Busa-Fekete, R{\'o}bert and Sz{\"o}r{\'e}nyi, Bal{\'a}zs and Dembczynski, Krzysztof and H{\"u}llermeier, Eyke},
	booktitle={Advances in Neural Information Processing Systems},
	pages={595--603},
	year={2015}
}
@article{hardt2015train,
	title={Train faster, generalize better: Stability of stochastic gradient descent},
	author={Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
	journal={arXiv preprint arXiv:1509.01240},
	year={2015}
}
@article{lecun2015deep,
	title={Deep learning},
	author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	journal={Nature},
	volume={521},
	number={7553},
	pages={436--444},
	year={2015},
	publisher={Nature Research}
}
@inproceedings{nesterov1983method,
  title={A method of solving a convex programming problem with convergence rate O (1/k2)},
  author={Nesterov, Yurii},
  booktitle={Soviet Mathematics Doklady},
  volume={27},
  number={2},
  pages={372--376},
  year={1983}
}
@book{hestenes1952methods,
  title={Methods of conjugate gradients for solving linear systems},
  author={Hestenes, Magnus Rudolph and Stiefel, Eduard},
  volume={49},
  number={1},
  year={1952},
  publisher={NBS}
}
@article{cauchy1847methode,
  title={M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des systemes d’{\'e}quations simultan{\'e}es},
  author={Cauchy, Augustin},
  journal={Comp. Rend. Sci. Paris},
  volume={25},
  number={1847},
  pages={536--538},
  year={1847}
}
@article{shalev2007online,
  title={Online learning: Theory, algorithms, and applications},
  author={Shalev-Shwartz, Shai and Singer, Yoram},
  year={2007},
  publisher={Citeseer}
}
@article{cesa2004generalization,
  title={On the generalization ability of on-line learning algorithms},
  author={Cesa-Bianchi, Nicolo and Conconi, Alex and Gentile, Claudio},
  journal={IEEE Transactions on Information Theory},
  volume={50},
  number={9},
  pages={2050--2057},
  year={2004},
  publisher={IEEE}
}
@inproceedings{shamir2013stochastic,
	title={Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes},
	author={Shamir, Ohad and Zhang, Tong},
	booktitle={International Conference on Machine Learning},
	pages={71--79},
	year={2013}
}
@article{hazan2014beyond,
	title={Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization.},
	author={Hazan, Elad and Kale, Satyen},
	journal={Journal of Machine Learning Research},
	volume={15},
	number={1},
	pages={2489--2512},
	year={2014}
}
@inproceedings{hazan2006logarithmic,
	title={Logarithmic regret algorithms for online convex optimization},
	author={Hazan, Elad and Kalai, Adam and Kale, Satyen and Agarwal, Amit},
	booktitle={COLT},
	volume={4005},
	pages={499--513},
	year={2006},
	organization={Springer}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@inproceedings{girshick2015fast,
  title={Fast r-cnn},
  author={Girshick, Ross},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1440--1448},
  year={2015}
}
@inproceedings{ren2015faster,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  booktitle={Advances in neural information processing systems},
  pages={91--99},
  year={2015}
}
@inproceedings{liu2018fast,
  title={Fast stochastic AUC maximization with O (1/n)-convergence rate},
  author={Liu, Mingrui and Zhang, Xiaoxuan and Chen, Zaiyi and Wang, Xiaoyu and Yang, Tianbao},
  booktitle={International Conference on Machine Learning},
  pages={3195--3203},
  year={2018}
}
@article{rafique2018non,
  title={Non-convex min-max optimization: Provable algorithms and applications in machine learning},
  author={Rafique, Hassan and Liu, Mingrui and Lin, Qihang and Yang, Tianbao},
  journal={arXiv preprint arXiv:1810.02060},
  year={2018}
}
@article{davis2017proximally,
  title={Proximally Guided Stochastic Subgradient Method for Nonsmooth, Nonconvex Problems},
  author={Davis, Damek and Grimmer, Benjamin},
  journal={arXiv preprint arXiv:1707.03505},
  year={2017}
}
@book{nesterov2013introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2013},
  publisher={Springer Science \& Business Media}
}
@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Jul},
  pages={2121--2159},
  year={2011}
}
@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}
@article{daskalakis2017training,
  title={Training gans with optimism},
  author={Daskalakis, Constantinos and Ilyas, Andrew and Syrgkanis, Vasilis and Zeng, Haoyang},
  journal={arXiv preprint arXiv:1711.00141},
  year={2017}
}
@inproceedings{rakhlin2013optimization,
  title={Optimization, learning, and games with predictable sequences},
  author={Rakhlin, Sasha and Sridharan, Karthik},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3066--3074},
  year={2013}
}
@article{allen2018convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  journal={arXiv preprint arXiv:1811.03962},
  year={2018}
}
@article{kleinberg2018alternative,
  title={An Alternative View: When Does SGD Escape Local Minima?},
  author={Kleinberg, Robert and Li, Yuanzhi and Yuan, Yang},
  journal={arXiv preprint arXiv:1802.06175},
  year={2018}
}
@inproceedings{li2017convergence,
  title={Convergence analysis of two-layer neural networks with relu activation},
  author={Li, Yuanzhi and Yuan, Yang},
  booktitle={Advances in Neural Information Processing Systems},
  pages={597--607},
  year={2017}
}
@article{zhou2019sgd,
  title={SGD converges to global minimum in deep learning via star-convex path},
  author={Zhou, Yi and Yang, Junjie and Zhang, Huishuai and Liang, Yingbin and Tarokh, Vahid},
  journal={arXiv preprint arXiv:1901.00451},
  year={2019}
}
@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}
@article{iusem2017extragradient,
  title={Extragradient method with variance reduction for stochastic variational inequalities},
  author={Iusem, AN and Jofr{\'e}, Alejandro and Oliveira, Roberto I and Thompson, Philip},
  journal={SIAM Journal on Optimization},
  volume={27},
  number={2},
  pages={686--724},
  year={2017},
  publisher={SIAM}
}
@inproceedings{chiang2012online,
  title={Online optimization with gradual variations},
  author={Chiang, Chao-Kai and Yang, Tianbao and Lee, Chia-Jung and Mahdavi, Mehrdad and Lu, Chi-Jen and Jin, Rong and Zhu, Shenghuo},
  booktitle={Conference on Learning Theory},
  pages={6--1},
  year={2012}
}
@inproceedings{lian2017can,
  title={Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent},
  author={Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5330--5340},
  year={2017}
}


@article{dekel2012optimal,
  title={Optimal distributed online prediction using mini-batches},
  author={Dekel, Ofer and Gilad-Bachrach, Ran and Shamir, Ohad and Xiao, Lin},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={Jan},
  pages={165--202},
  year={2012}
}
@inproceedings{li2014scaling,
  title={Scaling distributed machine learning with the parameter server},
  author={Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
  booktitle={11th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 14)},
  pages={583--598},
  year={2014}
}


@inproceedings{mcdonald2010distributed,
  title={Distributed training strategies for the structured perceptron},
  author={McDonald, Ryan and Hall, Keith and Mann, Gideon},
  booktitle={Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  pages={456--464},
  year={2010},
  organization={Association for Computational Linguistics}
}
@article{stich2018local,
  title={Local SGD converges fast and communicates little},
  author={Stich, Sebastian U},
  journal={arXiv preprint arXiv:1805.09767},
  year={2018}
}
@inproceedings{zhang2012communication,
  title={Communication-efficient algorithms for statistical optimization},
  author={Zhang, Yuchen and Wainwright, Martin J and Duchi, John C},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1502--1510},
  year={2012}
}
@article{jain2017parallelizing,
  title={Parallelizing Stochastic Gradient Descent for Least Squares Regression: Mini-batching, Averaging, and Model Misspecification.},
  author={Jain, Prateek and Kakade, Sham M and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
  journal={Journal of Machine Learning Research},
  volume={18},
  pages={223--1},
  year={2017}
}

@article{hendrikx2018accelerated,
  title={Accelerated decentralized optimization with local updates for smooth and strongly convex objectives},
  author={Hendrikx, Hadrien and Massouli{\'e}, Laurent and Bach, Francis},
  journal={arXiv preprint arXiv:1810.02660},
  year={2018}
}
@article{shi2015extra,
  title={Extra: An exact first-order algorithm for decentralized consensus optimization},
  author={Shi, Wei and Ling, Qing and Wu, Gang and Yin, Wotao},
  journal={SIAM Journal on Optimization},
  volume={25},
  number={2},
  pages={944--966},
  year={2015},
  publisher={SIAM}
}

@article{hendrikx2018accelerated,
  title={Accelerated decentralized optimization with local updates for smooth and strongly convex objectives},
  author={Hendrikx, Hadrien and Massouli{\'e}, Laurent and Bach, Francis},
  journal={arXiv preprint arXiv:1810.02660},
  year={2018}
}

@article{mertikopoulos2018mirror,
  title={Mirror descent in saddle-point problems: Going the extra (gradient) mile},
  author={Mertikopoulos, Panayotis and Zenati, Houssam and Lecouat, Bruno and Foo, Chuan-Sheng and Chandrasekhar, Vijay and Piliouras, Georgios},
  journal={arXiv preprint arXiv:1807.02629},
  year={2018}
}

@inproceedings{gulrajani2017improved,
  title={Improved training of wasserstein gans},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  booktitle={Advances in neural information processing systems},
  pages={5767--5777},
  year={2017}
}

@article{yuan2016influence,
  title={On the influence of momentum acceleration on online learning},
  author={Yuan, Kun and Ying, Bicheng and Sayed, Ali H},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={6602--6667},
  year={2016},
  publisher={JMLR. org}
}
@inproceedings{
Liu2020Accelerating,
title={Accelerating SGD with momentum for over-parameterized learning},
author={Chaoyue Liu and Mikhail Belkin},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=r1gixp4FPH}
}
@article{jain2017accelerating,
  title={Accelerating stochastic gradient descent for least squares regression},
  author={Jain, Prateek and Kakade, Sham M and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
  journal={arXiv preprint arXiv:1704.08227},
  year={2017}
}
@article{zhang2019gradient,
	title={Why gradient clipping accelerates training: A theoretical justification for adaptivity},
	author={Zhang, Jingzhao and He, Tianxing and Sra, Suvrit and Jadbabaie, Ali},
	journal={arXiv preprint arXiv:1905.11881},
	year={2019}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@inproceedings{yu_linear,
  author    = {Hao Yu and
               Rong Jin and
               Sen Yang},
  title     = {On the Linear Speedup Analysis of Communication Efficient Momentum
               {SGD} for Distributed Non-Convex Optimization},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  pages     = {7184--7193},
  year      = {2019}
}


@inproceedings{DBLP:conf/nips/Yuan0JY19,
  author    = {Zhuoning Yuan and
               Yan Yan and
               Rong Jin and
               Tianbao Yang},
  title     = {Stagewise Training Accelerates Convergence of Testing Error Over {SGD}},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14
               December 2019, Vancouver, BC, Canada},
  pages     = {2604--2614},
  year      = {2019},
}

@inproceedings{yu_dynamic,
  author    = {Hao Yu and
               Rong Jin},
  title     = {On the Computation and Communication Complexity of Parallel {SGD}
               with Dynamic Batch Sizes for Stochastic Non-Convex Optimization},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  pages     = {7174--7183},
  year      = {2019}
}

@inproceedings{haddadpour2019trading,
  title={Trading Redundancy for Communication: Speeding up Distributed SGD for Non-convex Optimization},
  author={Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad and Cadambe, Viveck},
  booktitle={International Conference on Machine Learning},
  pages={2545--2554},
  year={2019}
}

@article{zhang2019distributed,
  title={Distributed Optimization for Over-Parameterized Learning},
  author={Zhang, Chi and Li, Qianxiao},
  journal={arXiv preprint arXiv:1906.06205},
  year={2019}
}

@book{DBLP:books/sp/Nesterov04,
  author    = {Yurii E. Nesterov},
  title     = {Introductory Lectures on Convex Optimization - {A} Basic Course},
  series    = {Applied Optimization},
  volume    = {87},
  publisher = {Springer},
  year      = {2004}
}

@article{liu2019stochastic,
  title={Stochastic AUC Maximization with Deep Neural Networks},
  author={Liu, Mingrui and Yuan, Zhuoning and Ying, Yiming and Yang, Tianbao},
  journal={ICLR},
  year={2020}
}

@inproceedings{DBLP:conf/iclr/Stich19,
  author    = {Sebastian U. Stich},
  title     = {Local {SGD} Converges Fast and Communicates Little},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019},
  year = {2019}
}
@inproceedings{li2014scaling,
  title={Scaling distributed machine learning with the parameter server},
  author={Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
  booktitle={11th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 14)},
  pages={583--598},
  year={2014}
}
@inproceedings{dean2012large,
  title={Large scale distributed deep networks},
  author={Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and others},
  booktitle={Advances in neural information processing systems},
  pages={1223--1231},
  year={2012}
}
@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}
@article{brock2018large,
  title={Large scale gan training for high fidelity natural image synthesis},
  author={Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  journal={arXiv preprint arXiv:1809.11096},
  year={2018}
}
@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}
@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}
@inproceedings{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={5754--5764},
  year={2019}
}
@article{yu2019linear,
  title={On the linear speedup analysis of communication efficient momentum sgd for distributed non-convex optimization},
  author={Yu, Hao and Jin, Rong and Yang, Sen},
  journal={arXiv preprint arXiv:1905.03817},
  year={2019}
}
@inproceedings{yu2019parallel,
  title={Parallel restarted SGD with faster convergence and less communication: Demystifying why model averaging works for deep learning},
  author={Yu, Hao and Yang, Sen and Zhu, Shenghuo},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={5693--5700},
  year={2019}
}
@inproceedings{jiang2018linear,
  title={A linear speedup analysis of distributed deep learning with sparse and quantized communication},
  author={Jiang, Peng and Agrawal, Gagan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2525--2536},
  year={2018}
}

@article{wang2018cooperative,
  title={Cooperative SGD: A unified framework for the design and analysis of communication-efficient SGD algorithms},
  author={Wang, Jianyu and Joshi, Gauri},
  journal={arXiv preprint arXiv:1808.07576},
  year={2018}
}
@inproceedings{basu2019qsparse,
  title={Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification and Local Computations},
  author={Basu, Debraj and Data, Deepesh and Karakus, Can and Diggavi, Suhas},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14668--14679},
  year={2019}
}
@article{zhou2017convergence,
  title={On the convergence properties of a $ K $-step averaging stochastic gradient descent algorithm for nonconvex optimization},
  author={Zhou, Fan and Cong, Guojing},
  journal={arXiv preprint arXiv:1708.01012},
  year={2017}
}
@inproceedings{haddadpour2019local,
  title={Local SGD with periodic averaging: Tighter analysis and adaptive synchronization},
  author={Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad and Cadambe, Viveck},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11080--11092},
  year={2019}
}
@article{wang2018adaptive,
  title={Adaptive communication strategies to achieve the best error-runtime trade-off in local-update SGD},
  author={Wang, Jianyu and Joshi, Gauri},
  journal={arXiv preprint arXiv:1810.08313},
  year={2018}
}
@inproceedings{stich2018sparsified,
  title={Sparsified SGD with memory},
  author={Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4447--4458},
  year={2018}
}
@inproceedings{wangni2018gradient,
  title={Gradient sparsification for communication-efficient distributed optimization},
  author={Wangni, Jianqiao and Wang, Jialei and Liu, Ji and Zhang, Tong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1299--1309},
  year={2018}
}
This file was created with JabRef 1.8.1.
Encoding: Cp1252

@STRING{aaai = {Proceedings of \ AAAI Conference on Artificial Intelligence}}

@STRING{colt = {Proceedings \ Annual Conf.\ on Learning Theory}}

@STRING{cvpr = {Proc.\ CVPR}}

@STRING{ecml = {Proc.\ European Conf.\ on Mach. Learn.}}

@STRING{ewcbr = {Proc.\ the European Workshop on Advances in Case-Based Reasoning}}

@STRING{fg = {Proc.\ Int.\ Conf.\ on Face and Gesture Recognition}}

@STRING{iccv = {Proc.\ Int.\ Conf.\ on Computer Vision}}

@STRING{icdm = {Proc.\ Int.\ Conf.\ on Data Mining}}

@STRING{icml = {Proceedings of the International Conference on Machine Learning}}

@STRING{icpr = {Proc.\ Int.\ Conf.\ on Pattern Recognition}}

@STRING{icvp = {Proc.\ Int.\ Conf.\ Vision and Pattern Recognition}}

@STRING{ijcai = {Proc.\ Int.\ Joint Conf.\ on Artificial Intelligence}}

@STRING{ijcv = {Int.\ J.\ of Computer Vision}}

@STRING{kdd = {Proc.\ ACM SIGKDD Int.\ Conf.\ on Knowledge Discovery and Data Mining}}

@STRING{mm = {Proc.\ ACM Multimedia}}

@STRING{nips = {NIPS}}

@STRING{pami = {IEEE Trans.\ Pattern Anal.\ Mach.\ Intell.}}

@STRING{sigir = {Proc.\ ACM SIGIR}}

@STRING{tvcg = {IEEE Trans. Visualization and Computer Graphics}}

@STRING{uai = {Proc.\ Uncertainty in Artificial Intelligence}}

@STRING{pcm = {Proc.\ Pacific-Rim Conf.\ on Multimedia}}

@STRING{nips = {Proc.\ Conf.\ on Advances in Neural Information Processing Systems}}


This file was created with JabRef 1.8.1.
Encoding: Cp1252

@STRING{aaai = {Proceedings of \ AAAI Conference on Artificial Intelligence}}

@STRING{colt = {Proceedings \ Annual Conf.\ on Learning Theory}}

@STRING{cvpr = {Proc.\ CVPR}}

@STRING{ecml = {Proc.\ European Conf.\ on Mach. Learn.}}

@STRING{ewcbr = {Proc.\ the European Workshop on Advances in Case-Based Reasoning}}

@STRING{fg = {Proc.\ Int.\ Conf.\ on Face and Gesture Recognition}}

@STRING{iccv = {Proc.\ Int.\ Conf.\ on Computer Vision}}

@STRING{icdm = {Proc.\ Int.\ Conf.\ on Data Mining}}

@STRING{icml = {Proceedings of the International Conference on Machine Learning}}

@STRING{icpr = {Proc.\ Int.\ Conf.\ on Pattern Recognition}}

@STRING{icvp = {Proc.\ Int.\ Conf.\ Vision and Pattern Recognition}}

@STRING{ijcai = {Proc.\ Int.\ Joint Conf.\ on Artificial Intelligence}}

@STRING{ijcv = {Int.\ J.\ of Computer Vision}}

@STRING{kdd = {Proc.\ ACM SIGKDD Int.\ Conf.\ on Knowledge Discovery and Data Mining}}

@STRING{mm = {Proc.\ ACM Multimedia}}

@STRING{nips = {NIPS}}

@STRING{pami = {IEEE Trans.\ Pattern Anal.\ Mach.\ Intell.}}

@STRING{sigir = {Proc.\ ACM SIGIR}}

@STRING{tvcg = {IEEE Trans. Visualization and Computer Graphics}}

@STRING{uai = {Proc.\ Uncertainty in Artificial Intelligence}}

@STRING{pcm = {Proc.\ Pacific-Rim Conf.\ on Multimedia}}

@STRING{nips = {Proc.\ Conf.\ on Advances in Neural Information Processing Systems}}


@article{nemirovski2009robust,
	title={Robust stochastic approximation approach to stochastic programming},
	author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
	journal={SIAM Journal on optimization},
	volume={19},
	number={4},
	pages={1574--1609},
	year={2009},
	publisher={SIAM}
}

@article{hazan2014beyond,
	title={Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization.},
	author={Hazan, Elad and Kale, Satyen},
	journal={Journal of Machine Learning Research},
	volume={15},
	number={1},
	pages={2489--2512},
	year={2014}
}
@inproceedings{ying2016stochastic,
	title={Stochastic Online AUC Maximization},
	author={Ying, Yiming and Wen, Longyin and Lyu, Siwei},
	booktitle={Advances in Neural Information Processing Systems},
	pages={451--459},
	year={2016}
}
@article{li2013global,
	title={Global error bounds for piecewise convex polynomials},
	author={Li, Guoyin},
	journal={Mathematical Programming},
	pages={1--28},
	year={2013},
	publisher={Springer}
}
@article{clemenccon2008ranking,
	title={Ranking and empirical minimization of U-statistics},
	author={Cl{\'e}men{\c{c}}on, St{\'e}phan and Lugosi, G{\'a}bor and Vayatis, Nicolas},
	journal={The Annals of Statistics},
	pages={844--874},
	year={2008},
	publisher={JSTOR}
}
@inproceedings{zhao2011online,
	title={Online AUC maximization},
	author={Zhao, Peilin and Jin, Rong and Yang, Tianbao and Hoi, Steven C},
	booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
	pages={233--240},
	year={2011}
}
@inproceedings{gao2013one,
	title={One-Pass AUC Optimization.},
	author={Gao, Wei and Jin, Rong and Zhu, Shenghuo and Zhou, Zhi-Hua},
	booktitle={ICML (3)},
	pages={906--914},
	year={2013}
}
@article{powers2011evaluation,
	title={Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation},
	author={Powers, David Martin},
	year={2011},
	publisher={Bioinfo Publications}
}
@article{vitter1985random,
	title={Random sampling with a reservoir},
	author={Vitter, Jeffrey S},
	journal={ACM Transactions on Mathematical Software (TOMS)},
	volume={11},
	number={1},
	pages={37--57},
	year={1985},
	publisher={ACM}
}
@article{nemirovski2004prox,
	title={Prox-method with rate of convergence O (1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems},
	author={Nemirovski, Arkadi},
	journal={SIAM Journal on Optimization},
	volume={15},
	number={1},
	pages={229--251},
	year={2004},
	publisher={SIAM}
}
@article{korpelevich1976extragradient,
	title={The extragradient method for finding saddle points and other problems},
	author={Korpelevich, GM},
	journal={Matecon},
	volume={12},
	pages={747--756},
	year={1976}
}
@article{juditsky2011solving,
	title={Solving variational inequalities with stochastic mirror-prox algorithm},
	author={Juditsky, Anatoli and Nemirovski, Arkadi and Tauvel, Claire and others},
	journal={Stochastic Systems},
	volume={1},
	number={1},
	pages={17--58},
	year={2011},
	publisher={INFORMS Applied Probability Society}
}
@inproceedings{zinkevich2003online,
	title={Online convex programming and generalized infinitesimal gradient ascent},
	author={Zinkevich, Martin},
	booktitle={Proceedings of the 20th International Conference on Machine Learning (ICML-03)},
	pages={928--936},
	year={2003}
}
@article{nemirovskii1983problem,
	title={Problem complexity and method efficiency in optimization},
	author={Nemirovskii, Arkadii and Yudin, David Borisovich and Dawson, Edgar Ronald},
	year={1983},
	publisher={Wiley}
}
@book{shalev2014understanding,
	title={Understanding machine learning: From theory to algorithms},
	author={Shalev-Shwartz, Shai and Ben-David, Shai},
	year={2014},
	publisher={Cambridge university press}
}
@book{vapnik1998statistical,
	title={Statistical learning theory},
	author={Vapnik, Vladimir Naumovich and Vapnik, Vlamimir},
	volume={1},
	year={1998},
	publisher={Wiley New York}
}
@article{shalev2010learnability,
	title={Learnability, stability and uniform convergence},
	author={Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
	journal={Journal of Machine Learning Research},
	volume={11},
	number={Oct},
	pages={2635--2670},
	year={2010}
}
@incollection{bottou2010large,
	title={Large-scale machine learning with stochastic gradient descent},
	author={Bottou, L{\'e}on},
	booktitle={Proceedings of COMPSTAT'2010},
	pages={177--186},
	year={2010},
	publisher={Springer}
}
@inproceedings{defazio2014saga,
	title={Saga: A fast incremental gradient method with support for non-strongly convex composite objectives},
	author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1646--1654},
	year={2014}
}
@article{allen2016katyusha,
	title={Katyusha: Accelerated variance reduction for faster SGD},
	author={Allen-Zhu, Zeyuan},
	journal={ArXiv e-prints, abs/1603.05953},
	year={2016}
}
@article{lan2015optimal,
	title={An optimal randomized incremental gradient method},
	author={Lan, Guanghui and Zhou, Yi},
	journal={arXiv preprint arXiv:1507.02000},
	year={2015}
}
@inproceedings{zhang2015stochastic,
	title={Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization.},
	author={Zhang, Yuchen and Xiao, Lin},
	booktitle={ICML},
	pages={353--361},
	year={2015}
}
@inproceedings{lin2014accelerated,
	title={An accelerated proximal coordinate gradient method},
	author={Lin, Qihang and Lu, Zhaosong and Xiao, Lin},
	booktitle={Advances in Neural Information Processing Systems},
	pages={3059--3067},
	year={2014}
}
@inproceedings{shalev2014accelerated,
	title={Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization.},
	author={Shalev-Shwartz, Shai and Zhang, Tong},
	booktitle={ICML},
	pages={64--72},
	year={2014}
}
@article{xiao2014proximal,
	title={A proximal stochastic gradient method with progressive variance reduction},
	author={Xiao, Lin and Zhang, Tong},
	journal={SIAM Journal on Optimization},
	volume={24},
	number={4},
	pages={2057--2075},
	year={2014},
	publisher={SIAM}
}
@inproceedings{shalev2013accelerated,
	title={Accelerated mini-batch stochastic dual coordinate ascent},
	author={Shalev-Shwartz, Shai and Zhang, Tong},
	booktitle={Advances in Neural Information Processing Systems},
	pages={378--385},
	year={2013}
}
@article{ouyang2013stochastic,
	title={Stochastic Alternating Direction Method of Multipliers.},
	author={Ouyang, Hua and He, Niao and Tran, Long and Gray, Alexander G},
	journal={ICML (1)},
	volume={28},
	pages={80--88},
	year={2013}
}
@inproceedings{suzuki2013dual,
	title={Dual Averaging and Proximal Gradient Descent for Online Alternating Direction Multiplier Method.},
	author={Suzuki, Taiji and others},
	booktitle={ICML (1)},
	pages={392--400},
	year={2013}
}
@inproceedings{recht2011hogwild,
	title={Hogwild: A lock-free approach to parallelizing stochastic gradient descent},
	author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
	booktitle={Advances in Neural Information Processing Systems},
	pages={693--701},
	year={2011}
}
@inproceedings{yang2013trading,
	title={Trading computation for communication: Distributed stochastic dual coordinate ascent},
	author={Yang, Tianbao},
	booktitle={Advances in Neural Information Processing Systems},
	pages={629--637},
	year={2013}
}
@inproceedings{zhang2014asynchronous,
	title={Asynchronous Distributed ADMM for Consensus Optimization.},
	author={Zhang, Ruiliang and Kwok, James T},
	booktitle={ICML},
	pages={1701--1709},
	year={2014}
}
@inproceedings{roux2012stochastic,
	title={A stochastic gradient method with an exponential convergence \_rate for finite training sets},
	author={Roux, Nicolas L and Schmidt, Mark and Bach, Francis R},
	booktitle={Advances in Neural Information Processing Systems},
	pages={2663--2671},
	year={2012}
}
@inproceedings{defazio2016simple,
	title={A simple practical accelerated method for finite sums},
	author={Defazio, Aaron},
	booktitle={Advances In Neural Information Processing Systems},
	pages={676--684},
	year={2016}
}
@article{yang2013analysis,
	title={Analysis of distributed stochastic dual coordinate ascent},
	author={Yang, Tianbao and Zhu, Shenghuo and Jin, Rong and Lin, Yuanqing},
	journal={arXiv preprint arXiv:1312.1031},
	year={2013}
}
@inproceedings{recht2011hogwild,
	title={Hogwild: A lock-free approach to parallelizing stochastic gradient descent},
	author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
	booktitle={Advances in Neural Information Processing Systems},
	pages={693--701},
	year={2011}
}
@inproceedings{agarwal2011distributed,
	title={Distributed delayed stochastic optimization},
	author={Agarwal, Alekh and Duchi, John C},
	booktitle={Advances in Neural Information Processing Systems},
	pages={873--881},
	year={2011}
}
@article{gabay1976dual,
	title={A dual algorithm for the solution of nonlinear variational problems via finite element approximation},
	author={Gabay, Daniel and Mercier, Bertrand},
	journal={Computers \& Mathematics with Applications},
	volume={2},
	number={1},
	pages={17--40},
	year={1976},
	publisher={Elsevier}
}
@inproceedings{lian2015asynchronous,
	title={Asynchronous parallel stochastic gradient for nonconvex optimization},
	author={Lian, Xiangru and Huang, Yijun and Li, Yuncheng and Liu, Ji},
	booktitle={Advances in Neural Information Processing Systems},
	pages={2737--2745},
	year={2015}
}
@inproceedings{shalev2014accelerated,
	title={Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization},
	author={Shalev-Shwartz, Shai and Zhang, Tong},
	booktitle={International Conference on Machine Learning},
	pages={64--72},
	year={2014}
}
@inproceedings{johnson2013accelerating,
	title={Accelerating stochastic gradient descent using predictive variance reduction},
	author={Johnson, Rie and Zhang, Tong},
	booktitle={Advances in neural information processing systems},
	pages={315--323},
	year={2013}
}
@inproceedings{defazio2014saga,
	title={Saga: A fast incremental gradient method with support for non-strongly convex composite objectives},
	author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1646--1654},
	year={2014}
}
@article{shalev2013stochastic,
	title={Stochastic dual coordinate ascent methods for regularized loss minimization},
	author={Shalev-Shwartz, Shai and Zhang, Tong},
	journal={Journal of Machine Learning Research},
	volume={14},
	number={Feb},
	pages={567--599},
	year={2013}
}
@incollection{bottou2010large,
	title={Large-scale machine learning with stochastic gradient descent},
	author={Bottou, L{\'e}on},
	booktitle={Proceedings of COMPSTAT'2010},
	pages={177--186},
	year={2010},
	publisher={Springer}
}
@article{bottou2016optimization,
	title={Optimization methods for large-scale machine learning},
	author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
	journal={arXiv preprint arXiv:1606.04838},
	year={2016}
}
@book{cesa2006prediction,
	title={Prediction, learning, and games},
	author={Cesa-Bianchi, Nicolo and Lugosi, G{\'a}bor},
	year={2006},
	publisher={Cambridge university press}
}
@book{boyd2004convex,
	title={Convex optimization},
	author={Boyd, Stephen and Vandenberghe, Lieven},
	year={2004},
	publisher={Cambridge university press}
}
@inproceedings{busa2015online,
	title={Online F-measure optimization},
	author={Busa-Fekete, R{\'o}bert and Sz{\"o}r{\'e}nyi, Bal{\'a}zs and Dembczynski, Krzysztof and H{\"u}llermeier, Eyke},
	booktitle={Advances in Neural Information Processing Systems},
	pages={595--603},
	year={2015}
}
@article{hardt2015train,
	title={Train faster, generalize better: Stability of stochastic gradient descent},
	author={Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
	journal={arXiv preprint arXiv:1509.01240},
	year={2015}
}
@article{lecun2015deep,
	title={Deep learning},
	author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	journal={Nature},
	volume={521},
	number={7553},
	pages={436--444},
	year={2015},
	publisher={Nature Research}
}
@inproceedings{nesterov1983method,
	title={A method of solving a convex programming problem with convergence rate O (1/k2)},
	author={Nesterov, Yurii},
	booktitle={Soviet Mathematics Doklady},
	volume={27},
	number={2},
	pages={372--376},
	year={1983}
}
@book{hestenes1952methods,
	title={Methods of conjugate gradients for solving linear systems},
	author={Hestenes, Magnus Rudolph and Stiefel, Eduard},
	volume={49},
	number={1},
	year={1952},
	publisher={NBS}
}
@article{cauchy1847methode,
	title={M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des systemes d’{\'e}quations simultan{\'e}es},
	author={Cauchy, Augustin},
	journal={Comp. Rend. Sci. Paris},
	volume={25},
	number={1847},
	pages={536--538},
	year={1847}
}
@article{shalev2007online,
	title={Online learning: Theory, algorithms, and applications},
	author={Shalev-Shwartz, Shai and Singer, Yoram},
	year={2007},
	publisher={Citeseer}
}
@article{cesa2004generalization,
	title={On the generalization ability of on-line learning algorithms},
	author={Cesa-Bianchi, Nicolo and Conconi, Alex and Gentile, Claudio},
	journal={IEEE Transactions on Information Theory},
	volume={50},
	number={9},
	pages={2050--2057},
	year={2004},
	publisher={IEEE}
}
@inproceedings{shamir2013stochastic,
	title={Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes},
	author={Shamir, Ohad and Zhang, Tong},
	booktitle={International Conference on Machine Learning},
	pages={71--79},
	year={2013}
}
@article{hazan2014beyond,
	title={Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization.},
	author={Hazan, Elad and Kale, Satyen},
	journal={Journal of Machine Learning Research},
	volume={15},
	number={1},
	pages={2489--2512},
	year={2014}
}
@inproceedings{hazan2006logarithmic,
	title={Logarithmic regret algorithms for online convex optimization},
	author={Hazan, Elad and Kalai, Adam and Kale, Satyen and Agarwal, Amit},
	booktitle={COLT},
	volume={4005},
	pages={499--513},
	year={2006},
	organization={Springer}
}

@inproceedings{girshick2015fast,
	title={Fast r-cnn},
	author={Girshick, Ross},
	booktitle={Proceedings of the IEEE international conference on computer vision},
	pages={1440--1448},
	year={2015}
}
@inproceedings{ren2015faster,
	title={Faster r-cnn: Towards real-time object detection with region proposal networks},
	author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	booktitle={Advances in neural information processing systems},
	pages={91--99},
	year={2015}
}
@inproceedings{liu2018fast,
	title={Fast stochastic AUC maximization with O (1/n)-convergence rate},
	author={Liu, Mingrui and Zhang, Xiaoxuan and Chen, Zaiyi and Wang, Xiaoyu and Yang, Tianbao},
	booktitle={International Conference on Machine Learning},
	pages={3195--3203},
	year={2018}
}
@article{rafique2018non,
	title={Non-convex min-max optimization: Provable algorithms and applications in machine learning},
	author={Rafique, Hassan and Liu, Mingrui and Lin, Qihang and Yang, Tianbao},
	journal={arXiv preprint arXiv:1810.02060},
	year={2018}
}
@article{davis2017proximally,
	title={Proximally Guided Stochastic Subgradient Method for Nonsmooth, Nonconvex Problems},
	author={Davis, Damek and Grimmer, Benjamin},
	journal={arXiv preprint arXiv:1707.03505},
	year={2017}
}

@article{duchi2011adaptive,
	title={Adaptive subgradient methods for online learning and stochastic optimization},
	author={Duchi, John and Hazan, Elad and Singer, Yoram},
	journal={Journal of Machine Learning Research},
	volume={12},
	number={Jul},
	pages={2121--2159},
	year={2011}
}
@article{hanley1982meaning,
	title={The meaning and use of the area under a receiver operating characteristic (ROC) curve.},
	author={Hanley, James A and McNeil, Barbara J},
	journal={Radiology},
	volume={143},
	number={1},
	pages={29--36},
	year={1982}
}
@article{hanley1983method,
	title={A method of comparing the areas under receiver operating characteristic curves derived from the same cases.},
	author={Hanley, James A and McNeil, Barbara J},
	journal={Radiology},
	volume={148},
	number={3},
	pages={839--843},
	year={1983}
}
@inproceedings{elkan2001foundations,
	title={The foundations of cost-sensitive learning},
	author={Elkan, Charles},
	booktitle={International joint conference on artificial intelligence},
	volume={17},
	number={1},
	pages={973--978},
	year={2001},
	organization={Lawrence Erlbaum Associates Ltd}
}
@inproceedings{natole2018stochastic,
	title={Stochastic proximal algorithms for AUC maximization},
	author={Natole, Michael and Ying, Yiming and Lyu, Siwei},
	booktitle={International Conference on Machine Learning},
	pages={3707--3716},
	year={2018}
}
@inproceedings{krizhevsky2012imagenet,
	title={Imagenet classification with deep convolutional neural networks},
	author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	booktitle={Advances in neural information processing systems},
	pages={1097--1105},
	year={2012}
}
@article{simonyan2014very,
	title={Very deep convolutional networks for large-scale image recognition},
	author={Simonyan, Karen and Zisserman, Andrew},
	journal={arXiv preprint arXiv:1409.1556},
	year={2014}
}
@inproceedings{karimi2016linear,
	title={Linear convergence of gradient and proximal-gradient methods under the polyak-{\l}ojasiewicz condition},
	author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
	booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
	pages={795--811},
	year={2016},
	organization={Springer}
}
@article{polyak1963gradient,
	title={Gradient methods for minimizing functionals},
	author={Polyak, Boris Teodorovich},
	journal={Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
	volume={3},
	number={4},
	pages={643--653},
	year={1963},
	publisher={Russian Academy of Sciences, Branch of Mathematical Sciences}
}
@inproceedings{lei2017non,
	title={Non-convex finite-sum optimization via scsg methods},
	author={Lei, Lihua and Ju, Cheng and Chen, Jianbo and Jordan, Michael I},
	booktitle={Advances in Neural Information Processing Systems},
	pages={2348--2358},
	year={2017}
}
@inproceedings{reddi2016stochastic,
	title={Stochastic variance reduction for nonconvex optimization},
	author={Reddi, Sashank J and Hefny, Ahmed and Sra, Suvrit and Poczos, Barnabas and Smola, Alex},
	booktitle={International conference on machine learning},
	pages={314--323},
	year={2016}
}
@inproceedings{allen2016variance,
	title={Variance reduction for faster non-convex optimization},
	author={Allen-Zhu, Zeyuan and Hazan, Elad},
	booktitle={International conference on machine learning},
	pages={699--707},
	year={2016}
}
@article{hardt2016identity,
	title={Identity matters in deep learning},
	author={Hardt, Moritz and Ma, Tengyu},
	journal={arXiv preprint arXiv:1611.04231},
	year={2016}
}
@article{zhou2017characterization,
	title={Characterization of gradient dominance and regularity conditions for neural networks},
	author={Zhou, Yi and Liang, Yingbin},
	journal={arXiv preprint arXiv:1710.06910},
	year={2017}
}
@article{charles2017stability,
	title={Stability and generalization of learning algorithms that converge to global optima},
	author={Charles, Zachary and Papailiopoulos, Dimitris},
	journal={arXiv preprint arXiv:1710.08402},
	year={2017}
}
@article{arora2018convergence,
	title={A convergence analysis of gradient descent for deep linear neural networks},
	author={Arora, Sanjeev and Cohen, Nadav and Golowich, Noah and Hu, Wei},
	journal={arXiv preprint arXiv:1810.02281},
	year={2018}
}
@inproceedings{li2017convergence,
	title={Convergence analysis of two-layer neural networks with relu activation},
	author={Li, Yuanzhi and Yuan, Yang},
	booktitle={Advances in Neural Information Processing Systems},
	pages={597--607},
	year={2017}
}
@article{allen2018convergence,
	title={A convergence theory for deep learning via over-parameterization},
	author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
	journal={arXiv preprint arXiv:1811.03962},
	year={2018}
}
@article{du2018gradient,
	title={Gradient descent provably optimizes over-parameterized neural networks},
	author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
	journal={arXiv preprint arXiv:1810.02054},
	year={2018}
}
@inproceedings{li2018learning,
	title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
	author={Li, Yuanzhi and Liang, Yingyu},
	booktitle={Advances in Neural Information Processing Systems},
	pages={8157--8166},
	year={2018}
}
@inproceedings{duchi2010composite,
	title={Composite Objective Mirror Descent.},
	author={Duchi, John C and Shalev-Shwartz, Shai and Singer, Yoram and Tewari, Ambuj},
	booktitle={COLT},
	pages={14--26},
	year={2010}
}
@article{juditsky2011solving,
	title={Solving variational inequalities with stochastic mirror-prox algorithm},
	author={Juditsky, Anatoli and Nemirovski, Arkadi and Tauvel, Claire},
	journal={Stochastic Systems},
	volume={1},
	number={1},
	pages={17--58},
	year={2011},
	publisher={INFORMS}
}
@article{rockafellar1976monotone,
	title={Monotone operators and the proximal point algorithm},
	author={Rockafellar, R Tyrrell},
	journal={SIAM journal on control and optimization},
	volume={14},
	number={5},
	pages={877--898},
	year={1976},
	publisher={SIAM}
}

@article{hand2001simple,
	title={A simple generalisation of the area under the ROC curve for multiple class classification problems},
	author={Hand, David J and Till, Robert J},
	journal={Machine learning},
	volume={45},
	number={2},
	pages={171--186},
	year={2001},
	publisher={Springer}
}
@article{graves2013generating,
	title={Generating sequences with recurrent neural networks},
	author={Graves, Alex},
	journal={arXiv preprint arXiv:1308.0850},
	year={2013}
}
@article{bahdanau2014neural,
	title={Neural machine translation by jointly learning to align and translate},
	author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	journal={arXiv preprint arXiv:1409.0473},
	year={2014}
}
@inproceedings{sutskever2014sequence,
	title={Sequence to sequence learning with neural networks},
	author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
	booktitle={Advances in neural information processing systems},
	pages={3104--3112},
	year={2014}
}

@article{hinton2012deep,
	title={Deep neural networks for acoustic modeling in speech recognition},
	author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Kingsbury, Brian and others},
	journal={IEEE Signal processing magazine},
	volume={29},
	year={2012}
}
@article{mohamed2012acoustic,
	title={Acoustic modeling using deep belief networks},
	author={Mohamed, Abdel-rahman and Dahl, George E and Hinton, Geoffrey},
	journal={IEEE Transactions on Audio, Speech, and Language Processing},
	volume={20},
	number={1},
	pages={14--22},
	year={2012},
	publisher={IEEE}
}
@inproceedings{graves2013speech,
	title={Speech recognition with deep recurrent neural networks},
	author={Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
	booktitle={2013 IEEE international conference on acoustics, speech and signal processing},
	pages={6645--6649},
	year={2013},
	organization={IEEE}
}

@article{lin2018solving,
	title={Solving weakly-convex-weakly-concave saddle-point problems as weakly-monotone variational inequality},
	author={Lin, Qihang and Liu, Mingrui and Rafique, Hassan and Yang, Tianbao},
	journal={arXiv preprint arXiv:1810.10207},
	year={2018}
}
@article{sanjabi2018solving,
	title={Solving Non-Convex Non-Concave Min-Max Games Under Polyak-L ojasiewicz Condition},
	author={Sanjabi, Maziar and Razaviyayn, Meisam and Lee, Jason D},
	journal={arXiv preprint arXiv:1812.02878},
	year={2018}
}
@article{lu2019hybrid,
	title={Hybrid Block Successive Approximation for One-Sided Non-Convex Min-Max Problems: Algorithms and Applications},
	author={Lu, Songtao and Tsaknakis, Ioannis and Hong, Mingyi and Chen, Yongxin},
	journal={arXiv preprint arXiv:1902.08294},
	year={2019}
}
@article{jin2019minmax,
	title={Minmax Optimization: Stable Limit Points of Gradient Descent Ascent are Locally Optimal},
	author={Jin, Chi and Netrapalli, Praneeth and Jordan, Michael I},
	journal={arXiv preprint arXiv:1902.00618},
	year={2019}
}

@inproceedings{
	chen2018universal,
	title={Universal  Stagewise Learning for Non-Convex Problems with  Convergence on  Averaged Solutions},
	author={Zaiyi Chen and Zhuoning Yuan and Jinfeng Yi and Bowen Zhou and Enhong Chen and Tianbao Yang},
	booktitle={International Conference on Learning Representations},
	year={2019},
	url={https://openreview.net/forum?id=Syx5V2CcFm},
}


@article{girosi1995regularization,
	title={Regularization theory and neural networks architectures},
	author={Girosi, Federico and Jones, Michael and Poggio, Tomaso},
	journal={Neural computation},
	volume={7},
	number={2},
	pages={219--269},
	year={1995},
	publisher={MIT Press}
}
@inproceedings{zhou2018stochastic,
	title={Stochastic nested variance reduced gradient descent for nonconvex optimization},
	author={Zhou, Dongruo and Xu, Pan and Gu, Quanquan},
	booktitle={Advances in Neural Information Processing Systems},
	pages={3921--3932},
	year={2018}
}
@inproceedings{li2018simple,
	title={A Simple Proximal Stochastic Gradient Method for Nonsmooth Nonconvex Optimization},
	author={Li, Zhize and Li, Jian},
	booktitle={Advances in Neural Information Processing Systems},
	pages={5564--5574},
	year={2018}
}
@article{wang2018spiderboost,
	title={SpiderBoost: A class of faster variance-reduced algorithms for nonconvex optimization},
	author={Wang, Zhe and Ji, Kaiyi and Zhou, Yi and Liang, Yingbin and Tarokh, Vahid},
	journal={arXiv preprint arXiv:1810.10690},
	year={2018}
}
@inproceedings{nguyen2017sarah,
	title={SARAH: A novel method for machine learning problems using stochastic recursive gradient},
	author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Takac, Martin},
	booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	pages={2613--2621},
	year={2017},
	organization={JMLR. org}
}
@article{nguyen2017stochastic,
	title={Stochastic recursive gradient algorithm for nonconvex optimization},
	author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Takac, Martin},
	journal={arXiv preprint arXiv:1705.07261},
	year={2017}
}

@article{zou2018stochastic,
	title={Stochastic gradient descent optimizes over-parameterized deep relu networks},
	author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
	journal={arXiv preprint arXiv:1811.08888},
	year={2018}
}
@article{du2018gradient1,
	title={Gradient descent finds global minima of deep neural networks},
	author={Du, Simon S and Lee, Jason D and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
	journal={arXiv preprint arXiv:1811.03804},
	year={2018}
}

@article{zou2019improved,
	title={An Improved Analysis of Training Over-parameterized Deep Neural Networks},
	author={Zou, Difan and Gu, Quanquan},
	journal={arXiv preprint arXiv:1906.04688},
	year={2019}
}


@article{code,
	title={\url{https://drive.google.com/drive/folders/1nPM6fmvN5fTsSaWsOcGFbhMVW7Fxso-Y?usp=sharing}},
	author={Anonymous},
	journal={ },
	year={ }
}

@misc{code_v2,
	author = {Anonymous.},
	title = {{}},
	howpublished ="\url{https://drive.google.com/drive/folders/1nPM6fmvN5fTsSaWsOcGFbhMVW7Fxso-Y?usp=sharing}",
	year = {},
	note = ""
}


@article{zhang2016parallel,
  title={Parallel SGD: When does averaging help?},
  author={Zhang, Jian and De Sa, Christopher and Mitliagkas, Ioannis and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:1606.07365},
  year={2016}
}

@article{godichon2017rates,
  title={On the rates of convergence of parallelized averaged stochastic gradient algorithms},
  author={Godichon-Baggioni, Antoine and Saadane, Sofiane},
  journal={arXiv preprint arXiv:1710.07926},
  year={2017}
}

@article{povey2014parallel,
  title={Parallel training of DNNs with natural gradient and parameter averaging},
  author={Povey, Daniel and Zhang, Xiaohui and Khudanpur, Sanjeev},
  journal={arXiv preprint arXiv:1410.7455},
  year={2014}
}
@article{bernstein2018signsgd,
  title={signSGD: Compressed optimisation for non-convex problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1802.04434},
  year={2018}
}
@inproceedings{chen2016scalable,
  title={Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise model-update filtering},
  author={Chen, Kai and Huo, Qiang},
  booktitle={2016 ieee international conference on acoustics, speech and signal processing (icassp)},
  pages={5880--5884},
  year={2016},
  organization={IEEE}
}
@article{mcmahan2016communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, H Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and others},
  journal={AISTATS},
  year={2017}
}
@article{su2015experiments,
  title={Experiments on parallel training of deep neural network using model averaging},
  author={Su, Hang and Chen, Haoyu},
  journal={arXiv preprint arXiv:1507.01239},
  year={2015}
}
@inproceedings{kamp2018efficient,
  title={Efficient decentralized deep learning by dynamic model averaging},
  author={Kamp, Michael and Adilova, Linara and Sicking, Joachim and H{\"u}ger, Fabian and Schlicht, Peter and Wirtz, Tim and Wrobel, Stefan},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={393--409},
  year={2018},
  organization={Springer}
}
@article{lin2018don,
  title={Don't Use Large Mini-Batches, Use Local SGD},
  author={Lin, Tao and Stich, Sebastian U and Patel, Kumar Kshitij and Jaggi, Martin},
  journal={arXiv preprint arXiv:1808.07217},
  year={2018}
}

@inproceedings{
liu2020towards,
title={Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets},
author={Mingrui Liu and Youssef Mroueh and Jerret Ross and Wei Zhang and Xiaodong Cui and Payel Das and Tianbao Yang},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJxIm0VtwH}
}
@article{lin2019gradient,
  title={On gradient descent ascent for nonconvex-concave minimax problems},
  author={Lin, Tianyi and Jin, Chi and Jordan, Michael I},
  journal={arXiv preprint arXiv:1906.00331},
  year={2019}
}
@inproceedings{paszke2019pytorch,
  title={PyTorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8024--8035},
  year={2019}
}

@inproceedings{DBLP:conf/nips/JaggiSTTKHJ14,
  author    = {Martin Jaggi and
               Virginia Smith and
               Martin Tak{\'{a}}c and
               Jonathan Terhorst and
               Sanjay Krishnan and
               Thomas Hofmann and
               Michael I. Jordan},
  title     = {Communication-Efficient Distributed Dual Coordinate Ascent},
  booktitle = {Advances in Neural Information Processing Systems 27: Annual Conference
               on Neural Information Processing Systems 2014, December 8-13 2014,
               Montreal, Quebec, Canada},
  pages     = {3068--3076},
  year      = {2014}
}

@inproceedings{
koloskova2020decentralized,
title={Decentralized Deep Learning with Arbitrary Communication Compression},
author={Anastasia Koloskova and Tao Lin and Sebastian U Stich and Martin Jaggi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkgGCkrKvH}
}

@inproceedings{DBLP:conf/icml/KoloskovaSJ19,
  author    = {Anastasia Koloskova and
               Sebastian U. Stich and
               Martin Jaggi},
  title     = {Decentralized Stochastic Optimization and Gossip Algorithms with Compressed
               Communication},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  pages     = {3478--3487},
  year      = {2019}
}
@article{you2019large,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}

@article{pascanu2012understanding,
  title={Understanding the exploding gradient problem. CoRR abs/1211.5063 (2012)},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1211.5063},
  year={2012}
}
@inproceedings{gehring2017convolutional,
  title={Convolutional sequence to sequence learning},
  author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
  booktitle={International Conference on Machine Learning},
  pages={1243--1252},
  year={2017},
  organization={PMLR}
}
@article{peters2018deep,
  title={Deep contextualized word representations},
  author={Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1802.05365},
  year={2018}
}
@inproceedings{
merity2018regularizing,
title={Regularizing and Optimizing {LSTM} Language Models},
author={Stephen Merity and Nitish Shirish Keskar and Richard Socher},
booktitle={International Conference on Learning Representations},
year={2018}
}
@book{shor2012minimization,
  title={Minimization methods for non-differentiable functions},
  author={Shor, Naum Zuselevich},
  volume={3},
  year={2012},
  publisher={Springer Science \& Business Media}
}
@article{ermoliev1988stochastic,
  title={Stochastic quasigradient methods. numerical techniques for stochastic optimization},
  author={Ermoliev, Yuri},
  journal={Springer Series in Computational Mathematics},
  number={10},
  pages={141--185},
  year={1988},
  publisher={Springer}
}
@article{alber1998projected,
  title={On the projected subgradient method for nonsmooth convex optimization in a Hilbert space},
  author={Alber, Ya I and Iusem, Alfredo N. and Solodov, Mikhail V.},
  journal={Mathematical Programming},
  volume={81},
  number={1},
  pages={23--35},
  year={1998},
  publisher={Springer}
}

@article{nesterov1984minimization,
  title={Minimization methods for nonsmooth convex and quasiconvex functions},
  author={Nesterov, Yurii E},
  journal={Matekon},
  volume={29},
  pages={519--531},
  year={1984}
}
@article{mai2021stability,
  title={Stability and Convergence of Stochastic Gradient Clipping: Beyond Lipschitz Continuity and Smoothness},
  author={Mai, Vien V and Johansson, Mikael},
  journal={arXiv preprint arXiv:2102.06489},
  year={2021}
}
@inproceedings{cutkosky2020momentum,
  title={Momentum improves normalized sgd},
  author={Cutkosky, Ashok and Mehta, Harsh},
  booktitle={International Conference on Machine Learning},
  pages={2260--2268},
  year={2020},
  organization={PMLR}
}
@article{you2017scaling,
  title={Scaling sgd batch size to 32k for imagenet training},
  author={You, Yang and Gitman, Igor and Ginsburg, Boris},
  journal={arXiv preprint arXiv:1708.03888},
  volume={6},
  pages={12},
  year={2017}
}
@article{gorbunov2020stochastic,
  title={Stochastic optimization with heavy-tailed noise via accelerated gradient clipping},
  author={Gorbunov, Eduard and Danilova, Marina and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:2005.10785},
  year={2020}
}
@article{levy2016power,
  title={The power of normalization: Faster evasion of saddle points},
  author={Levy, Kfir Y},
  journal={arXiv preprint arXiv:1611.04831},
  year={2016}
}
@inproceedings{menon2019can,
  title={Can gradient clipping mitigate label noise?},
  author={Menon, Aditya Krishna and Rawat, Ankit Singh and Reddi, Sashank J and Kumar, Sanjiv},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@article{zhang2013communication,
  title={Communication-efficient algorithms for statistical optimization},
  author={Zhang, Yuchen and Duchi, John C and Wainwright, Martin J},
  journal={The Journal of Machine Learning Research},
  volume={14},
  number={1},
  pages={3321--3363},
  year={2013},
  publisher={JMLR. org}
}
@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and trends{\textregistered} in machine learning},
  volume={14},
  number={1--2},
  pages={1--210},
  year={2021},
  publisher={Now Publishers, Inc.}
}
@article{woodworth2021min,
  title={The Min-Max Complexity of Distributed Stochastic Convex Optimization with Intermittent Communication},
  author={Woodworth, Blake and Bullins, Brian and Shamir, Ohad and Srebro, Nathan},
  journal={arXiv preprint arXiv:2102.01583},
  year={2021}
}
@inproceedings{gorbunov2021local,
  title={Local sgd: Unified theory and new efficient methods},
  author={Gorbunov, Eduard and Hanzely, Filip and Richt{\'a}rik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3556--3564},
  year={2021},
  organization={PMLR}
}



@inproceedings{khaled2020tighter,
  title={Tighter theory for local SGD on identical and heterogeneous data},
  author={Khaled, Ahmed and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4519--4529},
  year={2020},
  organization={PMLR}
}
@inproceedings{yuan2021federated,
  title={Federated composite optimization},
  author={Yuan, Honglin and Zaheer, Manzil and Reddi, Sashank},
  booktitle={International Conference on Machine Learning},
  pages={12253--12266},
  year={2021},
  organization={PMLR}
}
@article{dieuleveut2019communication,
  title={Communication trade-offs for Local-SGD with large step size},
  author={Dieuleveut, Aymeric and Patel, Kumar Kshitij},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={13601--13612},
  year={2019}
}

@article{marcus1993building,
author = {Marcus, Mitchell P. and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
title = {Building a Large Annotated Corpus of English: The Penn Treebank},
year = {1993},
issue_date = {June 1993},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {19},
number = {2},
issn = {0891-2017},
journal = {Comput. Linguist.},
month = jun,
pages = {313–330},
numpages = {18}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}
@article{chen2020understanding,
  title={Understanding gradient clipping in private SGD: a geometric perspective},
  author={Chen, Xiangyi and Wu, Steven Z and Hong, Mingyi},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
@article{reddi2020adaptive,
  title={Adaptive federated optimization},
  author={Reddi, Sashank and Charles, Zachary and Zaheer, Manzil and Garrett, Zachary and Rush, Keith and Konecny, Jakub and Kumar, Sanjiv and McMahan, H Brendan},
  journal={ICLR},
  year={2021}
}

@article{zhang2021understanding,
  title={Understanding Clipping for Federated Learning: Convergence and Client-Level Differential Privacy},
  author={Zhang, Xinwei and Chen, Xiangyi and Hong, Mingyi and Wu, Zhiwei Steven and Yi, Jinfeng},
  journal={arXiv preprint arXiv:2106.13673},
  year={2021}
}
@inproceedings{koloskova2020unified,
  title={A unified theory of decentralized SGD with changing topology and local updates},
  author={Koloskova, Anastasia and Loizou, Nicolas and Boreiri, Sadra and Jaggi, Martin and Stich, Sebastian},
  booktitle={International Conference on Machine Learning},
  pages={5381--5393},
  year={2020},
  organization={PMLR}
}
@article{zhang2019adaptive,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank J and Kumar, Sanjiv and Sra, Suvrit},
  journal={arXiv preprint arXiv:1912.03194},
  year={2019}
}
@inproceedings{zhao2011online,
	title={Online AUC maximization},
	author={Zhao, Peilin and Jin, Rong and Yang, Tianbao and Hoi, Steven C},
	booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
	pages={233--240},
	year={2011}
}
@inproceedings{gao2013one,
	title={One-Pass AUC Optimization.},
	author={Gao, Wei and Jin, Rong and Zhu, Shenghuo and Zhou, Zhi-Hua},
	booktitle={ICML (3)},
	pages={906--914},
	year={2013}
}
@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group}
}
@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}
@article{werbos1988generalization,
  title={Generalization of backpropagation with application to a recurrent gas market model},
  author={Werbos, Paul J},
  journal={Neural networks},
  volume={1},
  number={4},
  pages={339--356},
  year={1988},
  publisher={Elsevier}
}


@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}
@inproceedings{snli:emnlp2015,
	Author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
	Booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	Publisher = {Association for Computational Linguistics},
	Title = {A large annotated corpus for learning natural language inference},
	Year = {2015}
}
@inproceedings{conneau2017supervised,
  title={Supervised learning of universal sentence representations from natural language inference data},
  author={Conneau, A and Kiela, D and Schwenk, H and Barrault, L and Bordes, A},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={670--680},
  year={2017},
  organization={Association for Computational Linguistics}
}
@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{jin2021non,
  title={Non-convex distributionally robust optimization: Non-asymptotic analysis},
  author={Jin, Jikai and Zhang, Bohang and Wang, Haiyang and Wang, Liwei},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2771--2782},
  year={2021}
}
@article{li2020federated1,
  title={Federated learning: Challenges, methods, and future directions},
  author={Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={3},
  pages={50--60},
  year={2020},
  publisher={IEEE}
}
@article{zhang2020fedpd,
  title={Fedpd: A federated learning framework with optimal rates and adaptivity to non-iid data},
  author={Zhang, Xinwei and Hong, Mingyi and Dhople, Sairaj and Yin, Wotao and Liu, Yang},
  journal={arXiv preprint arXiv:2005.11418},
  year={2020}
}
@inproceedings{acar2021federated,
  title={Federated Learning Based on Dynamic Regularization},
  author={Acar, Durmus Alp Emre and Zhao, Yue and Matas, Ramon and Mattina, Matthew and Whatmough, Paul and Saligrama, Venkatesh},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@article{chen2020understanding,
  title={Understanding gradient clipping in private SGD: A geometric perspective},
  author={Chen, Xiangyi and Wu, Steven Z and Hong, Mingyi},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13773--13782},
  year={2020}
}
@article{andrew2021differentially,
  title={Differentially private learning with adaptive clipping},
  author={Andrew, Galen and Thakkar, Om and McMahan, Brendan and Ramaswamy, Swaroop},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17455--17466},
  year={2021}
}
@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}
@article{mcmahan2017learning,
  title={Learning differentially private recurrent language models},
  author={McMahan, H Brendan and Ramage, Daniel and Talwar, Kunal and Zhang, Li},
  journal={arXiv preprint arXiv:1710.06963},
  year={2017}
}

@article{li2020federatedprox,
  title={Federated optimization in heterogeneous networks},
  author={Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={429--450},
  year={2020}
}
@inproceedings{bao2022fast,
  title={Fast Composite Optimization and Statistical Recovery in Federated Learning},
  author={Bao, Yajie and Crawshaw, Michael and Luo, Shan and Liu, Mingrui},
  booktitle={International Conference on Machine Learning},
  pages={1508--1536},
  year={2022},
  organization={PMLR}
}
@article{yuan2020federated,
  title={Federated accelerated stochastic gradient descent},
  author={Yuan, Honglin and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5332--5344},
  year={2020}
}
@inproceedings{glasgow2022sharp,
  title={Sharp bounds for federated averaging (Local SGD) and continuous perspective},
  author={Glasgow, Margalit R and Yuan, Honglin and Ma, Tengyu},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={9050--9090},
  year={2022},
  organization={PMLR}
}
@inproceedings{mishchenko2022proximal,
  title={Proximal and federated random reshuffling},
  author={Mishchenko, Konstantin and Khaled, Ahmed and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  pages={15718--15749},
  year={2022},
  organization={PMLR}
}
@article{reisizadeh2023variance,
  title={Variance-reduced Clipping for Non-convex Optimization},
  author={Reisizadeh, Amirhossein and Li, Haochuan and Das, Subhro and Jadbabaie, Ali},
  journal={arXiv preprint arXiv:2303.00883},
  year={2023}
}
@article{reisizadeh2023variance,
  title={Variance-reduced Clipping for Non-convex Optimization},
  author={Reisizadeh, Amirhossein and Li, Haochuan and Das, Subhro and Jadbabaie, Ali},
  journal={arXiv preprint arXiv:2303.00883},
  year={2023}
}
@article{faw2023beyond,
  title={Beyond Uniform Smoothness: A Stopped Analysis of Adaptive SGD},
  author={Faw, Matthew and Rout, Litu and Caramanis, Constantine and Shakkottai, Sanjay},
  journal={arXiv preprint arXiv:2302.06570},
  year={2023}
}
@article{chen2020optimal,
  title={Optimal client sampling for federated learning},
  author={Chen, Wenlin and Horvath, Samuel and Richtarik, Peter},
  journal={arXiv preprint arXiv:2010.13723},
  year={2020}
}
@inproceedings{woodworth2020local,
  title={Is local SGD better than minibatch SGD?},
  author={Woodworth, Blake and Patel, Kumar Kshitij and Stich, Sebastian and Dai, Zhen and Bullins, Brian and Mcmahan, Brendan and Shamir, Ohad and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={10334--10343},
  year={2020},
  organization={PMLR}
}
@inproceedings{karimireddy2020scaffold,
  title={Scaffold: Stochastic controlled averaging for federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle={International conference on machine learning},
  pages={5132--5143},
  year={2020},
  organization={PMLR}
}
@article{wang2022unified,
  title={A unified analysis of federated learning with arbitrary client participation},
  author={Wang, Shiqiang and Ji, Mingyue},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={19124--19137},
  year={2022}
}
@article{cho2023convergence,
  title={On the convergence of federated averaging with cyclic client participation},
  author={Cho, Yae Jee and Sharma, Pranay and Joshi, Gauri and Xu, Zheng and Kale, Satyen and Zhang, Tong},
  journal={arXiv preprint arXiv:2302.03109},
  year={2023}
}
@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}
@article{malinovsky2023federated,
  title={Federated learning with regularized client participation},
  author={Malinovsky, Grigory and Horv{\'a}th, Samuel and Burlachenko, Konstantin and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2302.03662},
  year={2023}
}
@article{woodworth2020minibatch,
  title={Minibatch vs local sgd for heterogeneous distributed learning},
  author={Woodworth, Blake E and Patel, Kumar Kshitij and Srebro, Nati},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6281--6292},
  year={2020}
}
@article{li2019convergence,
  title={On the convergence of fedavg on non-iid data},
  author={Li, Xiang and Huang, Kaixuan and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
  journal={arXiv preprint arXiv:1907.02189},
  year={2019}
}
@inproceedings{safran2020good,
  title={How good is SGD with random shuffling?},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={3250--3284},
  year={2020},
  organization={PMLR}
}
@inproceedings{ruan2021towards,
  title={Towards flexible device participation in federated learning},
  author={Ruan, Yichen and Zhang, Xiaoxi and Liang, Shu-Che and Joe-Wong, Carlee},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3403--3411},
  year={2021},
  organization={PMLR}
}
@article{cho2020client,
  title={Client selection in federated learning: Convergence analysis and power-of-choice selection strategies},
  author={Cho, Yae Jee and Wang, Jianyu and Joshi, Gauri},
  journal={arXiv preprint arXiv:2010.01243},
  year={2020}
}
@inproceedings{avdiukhin2021federated,
  title={Federated learning under arbitrary communication patterns},
  author={Avdiukhin, Dmitrii and Kasiviswanathan, Shiva},
  booktitle={International Conference on Machine Learning},
  pages={425--435},
  year={2021},
  organization={PMLR}
}

@inproceedings{cho2022towards,
  title={Towards understanding biased client selection in federated learning},
  author={Cho, Yae Jee and Wang, Jianyu and Joshi, Gauri},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={10351--10375},
  year={2022},
  organization={PMLR}
}
@article{gu2021fast,
  title={Fast federated learning in the presence of arbitrary device unavailability},
  author={Gu, Xinran and Huang, Kaixuan and Zhang, Jingzhao and Huang, Longbo},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12052--12064},
  year={2021}
}
@article{yan2020distributed,
  title={Distributed non-convex optimization with sublinear speedup under intermittent client availability},
  author={Yan, Yikai and Niu, Chaoyue and Ding, Yucheng and Zheng, Zhenzhe and Wu, Fan and Chen, Guihai and Tang, Shaojie and Wu, Zhihua},
  journal={arXiv preprint arXiv:2002.07399},
  year={2020}
}

@inproceedings{fraboni2021clustered,
  title={Clustered sampling: Low-variance and improved representativity for clients selection in federated learning},
  author={Fraboni, Yann and Vidal, Richard and Kameni, Laetitia and Lorenzi, Marco},
  booktitle={International Conference on Machine Learning},
  pages={3407--3416},
  year={2021},
  organization={PMLR}
}
@article{li2020federated,
  title={Federated optimization in heterogeneous networks},
  author={Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  journal={Proceedings of Machine learning and systems},
  volume={2},
  pages={429--450},
  year={2020}
}
@article{xiao2017/online,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  date         = {2017-08-28},
  year         = {2017},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  eprint       = {cs.LG/1708.07747},
}
@inproceedings{eichner2019semi,
  title={Semi-cyclic stochastic gradient descent},
  author={Eichner, Hubert and Koren, Tomer and McMahan, Brendan and Srebro, Nathan and Talwar, Kunal},
  booktitle={International Conference on Machine Learning},
  pages={1764--1773},
  year={2019},
  organization={PMLR}
}
@article{bonawitz2019towards,
  title={Towards federated learning at scale: System design},
  author={Bonawitz, Keith and Eichner, Hubert and Grieskamp, Wolfgang and Huba, Dzmitry and Ingerman, Alex and Ivanov, Vladimir and Kiddon, Chloe and Kone{\v{c}}n{\`y}, Jakub and Mazzocchi, Stefano and McMahan, Brendan and others},
  journal={Proceedings of machine learning and systems},
  volume={1},
  pages={374--388},
  year={2019}
}
@article{wang2021field,
  title={A field guide to federated optimization},
  author={Wang, Jianyu and Charles, Zachary and Xu, Zheng and Joshi, Gauri and McMahan, H Brendan and Al-Shedivat, Maruan and Andrew, Galen and Avestimehr, Salman and Daly, Katharine and Data, Deepesh and others},
  journal={arXiv preprint arXiv:2107.06917},
  year={2021}
}
@article{mothukuri2021survey,
  title={A survey on security and privacy of federated learning},
  author={Mothukuri, Viraaji and Parizi, Reza M and Pouriyeh, Seyedamin and Huang, Yan and Dehghantanha, Ali and Srivastava, Gautam},
  journal={Future Generation Computer Systems},
  volume={115},
  pages={619--640},
  year={2021},
  publisher={Elsevier}
}
@article{zhu2021federated,
  title={Federated learning on non-IID data: A survey},
  author={Zhu, Hangyu and Xu, Jinjin and Liu, Shiqing and Jin, Yaochu},
  journal={Neurocomputing},
  volume={465},
  pages={371--390},
  year={2021},
  publisher={Elsevier}
}
@article{lim2020federated,
  title={Federated learning in mobile edge networks: A comprehensive survey},
  author={Lim, Wei Yang Bryan and Luong, Nguyen Cong and Hoang, Dinh Thai and Jiao, Yutao and Liang, Ying-Chang and Yang, Qiang and Niyato, Dusit and Miao, Chunyan},
  journal={IEEE Communications Surveys \& Tutorials},
  volume={22},
  number={3},
  pages={2031--2063},
  year={2020},
  publisher={IEEE}
}
@ARTICLE{wei2020federated,
  author={Wei, Kang and Li, Jun and Ding, Ming and Ma, Chuan and Yang, Howard H. and Farokhi, Farhad and Jin, Shi and Quek, Tony Q. S. and Vincent Poor, H.},
  journal={IEEE Transactions on Information Forensics and Security},
  title={Federated Learning With Differential Privacy: Algorithms and Performance Analysis},
  year={2020},
  volume={15},
  number={},
  pages={3454-3469},
  keywords={Convergence;Privacy;Servers;Training;Analytical models;Distributed databases;Federated learning;differential privacy;convergence performance;information leakage;client selection},
  doi={10.1109/TIFS.2020.2988575}}
@article{konevcny2016federated,
  title={Federated learning: Strategies for improving communication efficiency},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  journal={arXiv preprint arXiv:1610.05492},
  year={2016}
}
@article{zhang2021survey,
  title={A survey on federated learning},
  author={Zhang, Chen and Xie, Yu and Bai, Hang and Yu, Bin and Li, Weihong and Gao, Yuan},
  journal={Knowledge-Based Systems},
  volume={216},
  pages={106775},
  year={2021},
  publisher={Elsevier}
}
@article{yang2018applied,
  title={Applied federated learning: Improving google keyboard query suggestions},
  author={Yang, Timothy and Andrew, Galen and Eichner, Hubert and Sun, Haicheng and Li, Wei and Kong, Nicholas and Ramage, Daniel and Beaufays, Fran{\c{c}}oise},
  journal={arXiv preprint arXiv:1812.02903},
  year={2018}
}
@article{ding2017collecting,
  title={Collecting telemetry data privately},
  author={Ding, Bolin and Kulkarni, Janardhan and Yekhanin, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}
@article{paulik2021federated,
  title={Federated evaluation and tuning for on-device personalization: System design \& applications},
  author={Paulik, Matthias and Seigel, Matt and Mason, Henry and Telaar, Dominic and Kluivers, Joris and van Dalen, Rogier and Lau, Chi Wai and Carlson, Luke and Granqvist, Filip and Vandevelde, Chris and others},
  journal={arXiv preprint arXiv:2102.08503},
  year={2021}
}
@article{huba2022papaya,
  title={Papaya: Practical, private, and scalable federated learning},
  author={Huba, Dzmitry and Nguyen, John and Malik, Kshitiz and Zhu, Ruiyu and Rabbat, Mike and Yousefpour, Ashkan and Wu, Carole-Jean and Zhan, Hongyuan and Ustinov, Pavel and Srinivas, Harish and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={814--832},
  year={2022}
}
@inproceedings{zhu2021diurnal,
  title={Diurnal or nocturnal? federated learning of multi-branch networks from periodically shifting distributions},
  author={Zhu, Chen and Xu, Zheng and Chen, Mingqing and Kone{\v{c}}n{\`y}, Jakub and Hard, Andrew and Goldstein, Tom},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@inproceedings{yang2020achieving,
  title={Achieving Linear Speedup with Partial Worker Participation in Non-IID Federated Learning},
  author={Yang, Haibo and Fang, Minghong and Liu, Jia},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@inproceedings{yang2022anarchic,
  title={Anarchic federated learning},
  author={Yang, Haibo and Zhang, Xin and Khanduri, Prashant and Liu, Jia},
  booktitle={International Conference on Machine Learning},
  pages={25331--25363},
  year={2022},
  organization={PMLR}
}

@article{wang2023lightweight,
  title={A Lightweight Method for Tackling Unknown Participation Probabilities in Federated Averaging},
  author={Wang, Shiqiang and Ji, Mingyue},
  journal={arXiv preprint arXiv:2306.03401},
  year={2023}
}
@article{rizk2022federated,
  title={Federated learning under importance sampling},
  author={Rizk, Elsa and Vlaski, Stefan and Sayed, Ali H},
  journal={IEEE Transactions on Signal Processing},
  volume={70},
  pages={5381--5396},
  year={2022},
  publisher={IEEE}
}
@inproceedings{lian2018asynchronous,
  title={Asynchronous decentralized parallel stochastic gradient descent},
  author={Lian, Xiangru and Zhang, Wei and Zhang, Ce and Liu, Ji},
  booktitle={International Conference on Machine Learning},
  pages={3043--3052},
  year={2018},
  organization={PMLR}
}
@inproceedings{patel2022towards,
 title={Towards Optimal Communication Complexity in Distributed Non-Convex Optimization},
 author={Patel, Kumar Kshitij and Wang, Lingxiao and Woodworth, Blake E and Bullins, Brian and Srebro, Nati},
 booktitle={Advances in Neural Information Processing Systems},
 editor={S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages={13316--13328},
 publisher={Curran Associates, Inc.},
 url={https://proceedings.neurips.cc/paper_files/paper/2022/file/56bd21259e28ebdc4d7e1503733bf421-Paper-Conference.pdf},
 volume={35},
 year={2022}
}
@article{tyurin2022sharper,
  title={Sharper Rates and Flexible Framework for Nonconvex SGD with Client and Data Sampling},
  author={Tyurin, Alexander and Sun, Lukang and Burlachenko, Konstantin Pavlovich and Richt{\'a}rik, Peter},
  journal={Transactions on Machine Learning Research},
  year={2022}
}
@inproceedings{grudzien2023improving,
  title={Improving Accelerated Federated Learning with Compression and Importance Sampling},
  author={Grudzie{\'n}, Micha{\l} and Malinovsky, Grigory and Richt{\'a}rik, Peter},
  booktitle={Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities},
  year={2023}
}
@inproceedings{cheng2023momentum,
  title={Momentum Benefits Non-iid Federated Learning Simply and Provably},
  author={Cheng, Ziheng and Huang, Xinmeng and Wu, Pengfei and Yuan, Kun},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}
