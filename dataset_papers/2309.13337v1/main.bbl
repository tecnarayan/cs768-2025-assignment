\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andreas~Christmann(2008)]{andreaschristmann2008_SupportVector}
Ingo Steinwart~(auth.) Andreas~Christmann.
\newblock \emph{Support Vector Machines}.
\newblock Information {{Science}} and {{Statistics}}. {Springer-Verlag New York}, {New York, NY}, 1 edition, 2008.
\newblock ISBN 0-387-77242-1 0-387-77241-3 978-0-387-77241-7 978-0-387-77242-4.
\newblock \doi{10.1007/978-0-387-77242-4}.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and Tsigler]{bartlett2020_BenignOverfitting}
Peter~L. Bartlett, Philip~M. Long, G{\'a}bor Lugosi, and Alexander Tsigler.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0 (48):\penalty0 30063--30070, December 2020.
\newblock ISSN 0027-8424, 1091-6490.
\newblock \doi{10.1073/pnas.1907378117}.

\bibitem[Beaglehole et~al.(2022)Beaglehole, Belkin, and Pandit]{beaglehole2022_KernelRidgeless}
Daniel Beaglehole, Mikhail Belkin, and Parthe Pandit.
\newblock Kernel ridgeless regression is inconsistent in low dimensions.
\newblock \penalty0 (arXiv:2205.13525), June 2022.
\newblock \doi{10.48550/arXiv.2205.13525}.

\bibitem[Belkin et~al.(2019)Belkin, Hsu, Ma, and Mandal]{belkin2019_ReconcilingModern}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine-learning practice and the classical bias\textendash variance trade-off.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0 (32):\penalty0 15849--15854, 2019.

\bibitem[Bietti and Bach(2020)]{bietti2020_DeepEquals}
Alberto Bietti and Francis Bach.
\newblock Deep equals shallow for {{ReLU}} networks in kernel regimes.
\newblock \emph{arXiv preprint arXiv:2009.14397}, 2020.

\bibitem[Bietti and Mairal(2019)]{bietti2019_InductiveBias}
Alberto Bietti and Julien Mairal.
\newblock On the inductive bias of neural tangent kernels.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, volume~32, 2019.

\bibitem[Bordelon et~al.(2020)Bordelon, Canatar, and Pehlevan]{bordelon2020_SpectrumDependent}
Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan.
\newblock Spectrum dependent learning curves in kernel regression and wide neural networks.
\newblock In \emph{Proceedings of the 37th {{International Conference}} on {{Machine Learning}}}, pages 1024--1034. {PMLR}, November 2020.

\bibitem[Buchholz(2022)]{buchholz2022_KernelInterpolation}
Simon Buchholz.
\newblock Kernel interpolation in {{Sobolev}} spaces is not consistent in low dimensions.
\newblock In Po-Ling Loh and Maxim Raginsky, editors, \emph{Proceedings of Thirty Fifth Conference on Learning Theory}, volume 178 of \emph{Proceedings of Machine Learning Research}, pages 3410--3440. {PMLR}, July 2022.

\bibitem[Caponnetto and De~Vito(2007)]{caponnetto2007_OptimalRates}
Andrea Caponnetto and Ernesto De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock \emph{Foundations of Computational Mathematics}, 7\penalty0 (3):\penalty0 331--368, 2007.
\newblock \doi{10.1007/s10208-006-0196-8}.

\bibitem[Cui et~al.(2021)Cui, Loureiro, Krzakala, and Zdeborov{\'a}]{cui2021_GeneralizationError}
Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborov{\'a}.
\newblock Generalization error rates in kernel regression: {{The}} crossover from the noiseless to noisy regime.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 10131--10143, 2021.

\bibitem[Fischer and Steinwart(2020)]{fischer2020_SobolevNorm}
Simon-Raphael Fischer and Ingo Steinwart.
\newblock Sobolev norm learning rates for regularized least-squares algorithms.
\newblock \emph{Journal of Machine Learning Research}, 21:\penalty0 205:1--205:38, 2020.

\bibitem[Fujii et~al.(1993)Fujii, Fujii, Furuta, and Nakamoto]{fujii1993_NormInequalities}
Junichi Fujii, Masatoshi Fujii, Takayuki Furuta, and Ritsuo Nakamoto.
\newblock Norm inequalities equivalent to {{Heinz}} inequality.
\newblock \emph{Proceedings of the American Mathematical Society}, 118\penalty0 (3):\penalty0 827--830, 1993.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018_NeuralTangent}
Arthur Jacot, Franck Gabriel, and Clement Hongler.
\newblock Neural tangent kernel: {{Convergence}} and generalization in neural networks.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~{Cesa-Bianchi}, and R.~Garnett, editors, \emph{Advances in Neural Information Processing Systems}, volume~31. {Curran Associates, Inc.}, 2018.

\bibitem[Jacot et~al.(2020)Jacot, {\c S}im{\c s}ek, Spadaro, Hongler, and Gabriel]{jacot2020_KernelAlignment}
Arthur Jacot, Berfin {\c S}im{\c s}ek, Francesco Spadaro, Cl{\'e}ment Hongler, and Franck Gabriel.
\newblock Kernel alignment risk estimator: {{Risk}} prediction from training data.
\newblock June 2020.
\newblock \doi{10.48550/arXiv.2006.09796}.

\bibitem[Jin et~al.(2021)Jin, Banerjee, and Mont{\'u}far]{jin2021_LearningCurves}
Hui Jin, Pradeep~Kr Banerjee, and Guido Mont{\'u}far.
\newblock Learning curves for {{Gaussian}} process regression with power-law priors and targets.
\newblock \penalty0 (arXiv:2110.12231), November 2021.

\bibitem[Kanagawa et~al.(2018)Kanagawa, Hennig, Sejdinovic, and Sriperumbudur]{kanagawa2018_GaussianProcesses}
Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, and Bharath~K. Sriperumbudur.
\newblock Gaussian processes and kernel methods: {{A}} review on connections and equivalences.
\newblock \emph{arXiv preprint arXiv:1807.02582}, 2018.

\bibitem[Lai et~al.(2023)Lai, Xu, Chen, and Lin]{lai2023_GeneralizationAbility}
Jianfa Lai, Manyun Xu, Rui Chen, and Qian Lin.
\newblock Generalization ability of wide neural networks on {{R}}.
\newblock \penalty0 (arXiv:2302.05933), February 2023.
\newblock \doi{10.48550/arXiv.2302.05933}.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Zhang, and Lin]{li2023_KernelInterpolation}
Yicheng Li, Haobo Zhang, and Qian Lin.
\newblock Kernel interpolation generalizes poorly.
\newblock \penalty0 (arXiv:2303.15809), March 2023{\natexlab{a}}.
\newblock \doi{10.48550/arXiv.2303.15809}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Zhang, and Lin]{li2023_SaturationEffect}
Yicheng Li, Haobo Zhang, and Qian Lin.
\newblock On the saturation effect of kernel ridge regression.
\newblock In \emph{International {{Conference}} on {{Learning Representations}}}, February 2023{\natexlab{b}}.

\bibitem[Liang and Rakhlin(2020)]{liang2020_JustInterpolate}
Tengyuan Liang and Alexander Rakhlin.
\newblock Just interpolate: {{Kernel}} "ridgeless" regression can generalize.
\newblock \emph{The Annals of Statistics}, 48\penalty0 (3), June 2020.
\newblock ISSN 0090-5364.
\newblock \doi{10.1214/19-AOS1849}.

\bibitem[Lin et~al.(2018)Lin, Rudi, Rosasco, and Cevher]{lin2018_OptimalRates}
Junhong Lin, Alessandro Rudi, L.~Rosasco, and V.~Cevher.
\newblock Optimal rates for spectral algorithms with least-squares regression over {{Hilbert}} spaces.
\newblock \emph{Applied and Computational Harmonic Analysis}, 48:\penalty0 868--890, 2018.
\newblock \doi{10.1016/j.acha.2018.09.009}.

\bibitem[Lin et~al.(2021)Lin, Chang, and Sun]{lin2021_KernelInterpolation}
Shao-Bo Lin, Xiangyu Chang, and Xingping Sun.
\newblock Kernel interpolation of high dimensional scattered data.
\newblock \penalty0 (arXiv:2009.01514), September 2021.

\bibitem[Mallinar et~al.(2022)Mallinar, Simon, Abedsoltan, Pandit, Belkin, and Nakkiran]{mallinar2022_BenignTempered}
Neil Mallinar, James~B. Simon, Amirhesam Abedsoltan, Parthe Pandit, Mikhail Belkin, and Preetum Nakkiran.
\newblock Benign, tempered, or catastrophic: {{A}} taxonomy of overfitting.
\newblock \penalty0 (arXiv:2207.06569), July 2022.
\newblock \doi{10.48550/arXiv.2207.06569}.

\bibitem[Micchelli and Wahba(1979)]{micchelli1979_DesignProblems}
Charles~A. Micchelli and Grace Wahba.
\newblock Design problems for optimal surface interpolation.
\newblock Technical report, {Wisconsin Univ-Madison Dept of Statistics}, 1979.

\bibitem[Rakhlin and Zhai(2018)]{rakhlin2018_ConsistencyInterpolation}
Alexander Rakhlin and Xiyu Zhai.
\newblock Consistency of interpolation with {{Laplace}} kernels is a high-dimensional phenomenon.
\newblock \penalty0 (arXiv:1812.11167), December 2018.

\bibitem[Simon(2015)]{simon2015_OperatorTheorya}
Barry Simon.
\newblock \emph{Operator Theory}.
\newblock {American Mathematical Society}, {Providence, Rhode Island}, November 2015.
\newblock ISBN 978-1-4704-1103-9 978-1-4704-2763-4.
\newblock \doi{10.1090/simon/004}.

\bibitem[Steinwart and Scovel(2012)]{steinwart2012_MercerTheorem}
Ingo Steinwart and C.~Scovel.
\newblock Mercer's theorem on general domains: {{On}} the interaction between measures, kernels, and {{RKHSs}}.
\newblock \emph{Constructive Approximation}, 35\penalty0 (3):\penalty0 363--417, 2012.
\newblock \doi{10.1007/S00365-012-9153-3}.

\bibitem[Steinwart et~al.(2009)Steinwart, Hush, and Scovel]{steinwart2009_OptimalRates}
Ingo Steinwart, D.~Hush, and C.~Scovel.
\newblock Optimal rates for regularized least squares regression.
\newblock In \emph{{{COLT}}}, pages 79--93, 2009.

\bibitem[Vapnik(1999)]{vapnik1999_NatureStatistical}
Vladimir Vapnik.
\newblock \emph{The Nature of Statistical Learning Theory}.
\newblock {Springer science \& business media}, 1999.

\bibitem[Wainwright(2019)]{wainwright2019_HighdimensionalStatistics}
Martin~J. Wainwright.
\newblock \emph{High-Dimensional Statistics: {{A}} Non-Asymptotic Viewpoint}.
\newblock Cambridge {{Series}} in {{Statistical}} and {{Probabilistic Mathematics}}. {Cambridge University Press}, 2019.
\newblock \doi{10.1017/9781108627771}.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and Vinyals]{zhang2017_UnderstandingDeep}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \penalty0 (arXiv:1611.03530), February 2017.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Li, and Lin]{zhang2023_OptimalityMisspecified}
Haobo Zhang, Yicheng Li, and Qian Lin.
\newblock On the optimality of misspecified spectral algorithms.
\newblock \penalty0 (arXiv:2303.14942), March 2023{\natexlab{a}}.
\newblock \doi{10.48550/arXiv.2303.14942}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Li, Lu, and Lin]{zhang2023_OptimalityMisspecifieda}
Haobo Zhang, Yicheng Li, Weihao Lu, and Qian Lin.
\newblock On the optimality of misspecified kernel ridge regression.
\newblock In \emph{International {{Conference}} on {{Machine Learning}}}, 2023{\natexlab{b}}.

\end{thebibliography}
