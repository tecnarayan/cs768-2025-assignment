\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Schmidhuber(1987)]{Schmidhuber1987b}
Juergen Schmidhuber.
\newblock \emph{{Evolutionary principles in self-referential learning. On
  learning how to learn: The meta-meta-... hook}}.
\newblock PhD thesis, Technische Universitaet Munchen, 1987.

\bibitem[Thrun and Pratt(1998)]{thrun1998}
Sebastian Thrun and Lorien Pratt, editors.
\newblock \emph{Learning to Learn}.
\newblock Kluwer Academic Publishers, 1998.

\bibitem[Baxter(2000)]{baxter2000model}
Jonathan Baxter.
\newblock A model of inductive bias learning.
\newblock \emph{Journal of Artificial Intelligence Research}, 2000.

\bibitem[Koch et~al.(2015)Koch, Zemel, and Salakhutdinov]{koch2015siamese}
Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov.
\newblock Siamese neural networks for one-shot image recognition.
\newblock In \emph{ICML deep learning workshop}, volume~2. Lille, 2015.

\bibitem[Santoro et~al.(2016)Santoro, Bartunov, Botvinick, Wierstra, Lillicrap,
  and Deepmind]{Santoro2016}
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, Timothy
  Lillicrap, and Google Deepmind.
\newblock {Meta-Learning with Memory-Augmented Neural Networks}.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Rothfuss et~al.(2019{\natexlab{a}})Rothfuss, Lee, Clavera, Asfour, and
  Abbeel]{rothfuss2019promp}
Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel.
\newblock {ProMP: Proximal Meta-Policy Search}.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Yin et~al.(2020)Yin, Tucker, Zhou, Levine, and Finn]{yin2020meta}
Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, and Chelsea Finn.
\newblock Meta-learning without memorization.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Finn et~al.(2018)Finn, Xu, and Levine]{finn2018probabilistic}
Chelsea Finn, Kelvin Xu, and Sergey Levine.
\newblock Probabilistic model-agnostic meta-learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Yoon et~al.(2018)Yoon, Kim, Dia, Kim, Bengio, and
  Ahn]{yoon2018bayesian}
Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin
  Ahn.
\newblock Bayesian model-agnostic meta-learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  7332--7342, 2018.

\bibitem[Garnelo et~al.(2018)Garnelo, Rosenbaum, Maddison, Ramalho, Saxton,
  Shanahan, Teh, Rezende, and Eslami]{garnelo18a}
Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David
  Saxton, Murray Shanahan, Yee~Whye Teh, Danilo Rezende, and S.~M.~Ali Eslami.
\newblock Conditional neural processes.
\newblock In \emph{International Conference on Machine Learning}, volume~80,
  pages 1704--1713, 2018.

\bibitem[Rothfuss et~al.(2021)Rothfuss, Fortuin, Josifoski, and
  Krause]{rothfuss2020pacoh}
Jonas Rothfuss, Vincent Fortuin, Martin Josifoski, and Andreas Krause.
\newblock {PACOH: Bayes-optimal meta-learning with PAC-guarantees}.
\newblock In \emph{International Conference for Machine Learning (ICML)}, 2021.

\bibitem[Shahriari et~al.(2016)Shahriari, Swersky, Wang, Adams, and
  de~Freitas]{shahriari2016bo}
Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan~P. Adams, and Nando de~Freitas.
\newblock {Taking the Human Out of the Loop: A Review of Bayesian
  Optimization}.
\newblock \emph{Proceedings of the IEEE}, 104\penalty0 (1):\penalty0 148--175,
  2016.
\newblock \doi{10.1109/JPROC.2015.2494218}.

\bibitem[Frazier(2018)]{frazier2018tutorial}
Peter~I. Frazier.
\newblock {A Tutorial on Bayesian Optimization}.
\newblock \emph{arXiv preprint arXiv:1807.02811}, 2018.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Kaelbling et~al.(1996)Kaelbling, Littman, and
  Moore]{kaelbling1996reinforcement}
Leslie~Pack Kaelbling, Michael~L Littman, and Andrew~W Moore.
\newblock Reinforcement learning: A survey.
\newblock \emph{Journal of Machine Learning Research}, 4:\penalty0 237--285,
  1996.

\bibitem[Qin et~al.(2018)Qin, Zhang, Zhao, Wang, Shi, Qi, Shi, and
  Lei]{qin2018rethink}
Yunxiao Qin, Weiguo Zhang, Chenxu Zhao, Zezheng Wang, Hailin Shi, Guojun Qi,
  Jingping Shi, and Zhen Lei.
\newblock Rethink and redesign meta learning.
\newblock \emph{arXiv}, 2018.

\bibitem[Balaji et~al.(2018)Balaji, Sankaranarayanan, and
  Chellappa]{balaji2018_meta_reg}
Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa.
\newblock {MetaReg: Towards Domain Generalization using Meta-Regularization}.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem[Gneiting et~al.(2007)Gneiting, Balabdaoui, and Raftery]{gneiting2007}
Tilmann Gneiting, Fadoua Balabdaoui, and Adrian~E. Raftery.
\newblock Probabilistic forecasts, calibration and sharpness.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 69\penalty0 (2):\penalty0 243--268, 2007.
\newblock \doi{10.1111/j.1467-9868.2007.00587.x}.

\bibitem[Kuleshov et~al.(2018)Kuleshov, Fenner, and Ermon]{Kuleshov2018}
Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon.
\newblock {Accurate Uncertainties for Deep Learning Using Calibrated
  Regression}.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Sun et~al.(2019)Sun, Zhang, Shi, and Grosse]{sun2019functional}
Shengyang Sun, Guodong Zhang, Jiaxin Shi, and Roger Grosse.
\newblock Functional variational bayesian neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Pentina and Lampert(2014)]{pentina2014pac}
Anastasia Pentina and Christoph Lampert.
\newblock {A PAC-Bayesian bound for lifelong learning}.
\newblock In \emph{International Conference on Machine Learning}, 2014.

\bibitem[Amit and Meir(2018)]{amit2018meta}
Ron Amit and Ron Meir.
\newblock {Meta-learning by adjusting priors based on extended PAC-Bayes
  theory}.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Guedj(2019)]{guedj2019primer}
Benjamin Guedj.
\newblock {A primer on PAC-Bayesian learning}.
\newblock In \emph{2nd Congress of the French Mathematical Society}, 2019.

\bibitem[Rasmussen and Williams(2006)]{rasmussen2003gaussian}
Carl~Edward Rasmussen and Christopher K.~I. Williams.
\newblock \emph{{Gaussian processes in machine learning}}.
\newblock MIT Press, 2006.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{auer2002finite}
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock \emph{Machine learning}, 47\penalty0 (2):\penalty0 235--256, 2002.

\bibitem[Srinivas et~al.(2010)Srinivas, Krause, Kakade, and
  Seeger]{srinivas2010ucb}
Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger.
\newblock {Gaussian Process Optimization in the Bandit Setting: No Regret and
  Experimental Design}.
\newblock In \emph{International Conference on Machine Learning}, pages
  1015--1022, 07 2010.

\bibitem[Thompson(1933)]{tompson_sampling_original}
William~R. Thompson.
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock \emph{Biometrika}, 25\penalty0 (3/4):\penalty0 285--294, 1933.
\newblock ISSN 00063444.
\newblock URL \url{http://www.jstor.org/stable/2332286}.

\bibitem[Wang and Jegelka(2017)]{wang2017max}
Zi~Wang and Stefanie Jegelka.
\newblock Max-value entropy search for efficient bayesian optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  3627--3635. PMLR, 2017.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{snell2017prototypical}
Jake Snell, Kevin Swersky, and Richard Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, Wierstra,
  et~al.]{vinyals2016matching}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et~al.
\newblock Matching networks for one shot learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Harrison et~al.(2018)Harrison, Sharma, and Pavone]{harrison2018meta}
James Harrison, Apoorva Sharma, and Marco Pavone.
\newblock Meta-learning priors for efficient online bayesian regression.
\newblock In \emph{International Workshop on the Algorithmic Foundations of
  Robotics}, pages 318--337. Springer, 2018.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Younger, and
  Conwell]{Hochreiter2001}
Sepp Hochreiter, A.~Steven Younger, and Peter~R. Conwell.
\newblock {Learning To Learn Using Gradient Descent}.
\newblock In \emph{International Conference on Artificial Neural Networks},
  2001.

\bibitem[Andrychowicz et~al.(2016)Andrychowicz, Denil, Colmenarejo, Hoffman,
  Pfau, Schaul, Shillingford, and De~Freitas]{Andrychowicz2016}
Marcin Andrychowicz, Misha Denil, Sergio~GÃ³mez Colmenarejo, Matthew~W.
  Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando De~Freitas.
\newblock {Learning to learn by gradient descent by gradient descent}.
\newblock \emph{arXiv}, 2016.

\bibitem[Chen et~al.(2017)Chen, Hoffman, Colmenarejo, Denil, Lillicrap,
  Botvinick, and De~Freitas]{Chen2017a}
Yutian Chen, Matthew~W Hoffman, Sergio~GÃ³mez Colmenarejo, Misha Denil,
  Timothy~P Lillicrap, Matt Botvinick, and Nando De~Freitas.
\newblock {Learning to Learn without Gradient Descent by Gradient Descent}.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Nichol et~al.(2018)Nichol, Achiam, and Schulman]{nichol2018firstorder}
Alex Nichol, Joshua Achiam, and John Schulman.
\newblock {On First-Order Meta-Learning Algorithms}.
\newblock \emph{arXiv}, 2018.

\bibitem[Kim et~al.(2018)Kim, Yoon, Dia, Kim, Bengio, and Ahn]{kim2018bayesian}
Taesup Kim, Jaesik Yoon, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin
  Ahn.
\newblock Bayesian model-agnostic meta-learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Fortuin and R{\"a}tsch(2019)]{fortuin2019deep}
Vincent Fortuin and Gunnar R{\"a}tsch.
\newblock Deep mean functions for meta-learning in gaussian processes.
\newblock \emph{arXiv preprint arXiv:1901.08098}, 2019.

\bibitem[Ma et~al.(2019)Ma, Li, and Hern{\'a}ndez-Lobato]{ma2019variational}
Chao Ma, Yingzhen Li, and Jos{\'e}~Miguel Hern{\'a}ndez-Lobato.
\newblock Variational implicit processes.
\newblock In \emph{International Conference on Machine Learning}, pages
  4222--4233. PMLR, 2019.

\bibitem[Matthews et~al.(2016)Matthews, Hensman, Turner, and
  Ghahramani]{matthews2016}
Alexander Matthews, James Hensman, Richard Turner, and Zoubin Ghahramani.
\newblock {On sparse variational methods and the kullback- leibler divergence
  between stochastic processes}.
\newblock \emph{Journal of Machine Learning Research}, 51:\penalty0 231â239,
  2016.

\bibitem[Burt et~al.(2020)Burt, Ober, Garriga-Alonso, and van~der
  Wilk]{burt2020understanding}
David~R Burt, Sebastian~W Ober, Adri{\`a} Garriga-Alonso, and Mark van~der
  Wilk.
\newblock Understanding variational inference in function-space.
\newblock \emph{arXiv preprint arXiv:2011.09421}, 2020.

\bibitem[Feurer et~al.(2014)Feurer, Springenberg, and Hutter]{feurer2014using}
Matthias Feurer, Jost~Tobias Springenberg, and Frank Hutter.
\newblock Using meta-learning to initialize bayesian optimization of
  hyperparameters.
\newblock In \emph{MetaSel@ ECAI}, pages 3--10. Citeseer, 2014.

\bibitem[Lindauer and Hutter(2018)]{lindauer2018warmstarting}
Marius Lindauer and Frank Hutter.
\newblock Warmstarting of model-based algorithm configuration.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Wei et~al.(2019)Wei, Zhao, Yao, and Huang]{Wei2019TransferableNP}
Y.~Wei, P.~Zhao, Huaxiu Yao, and J.~Huang.
\newblock {Transferable Neural Processes for Hyperparameter Optimization}.
\newblock \emph{arXiv preprint arXiv:1909.03209}, 2019.

\bibitem[Swersky et~al.(2013)Swersky, Snoek, and Adams]{swersky2013}
Kevin Swersky, Jasper Snoek, and Ryan~P Adams.
\newblock Multi-task bayesian optimization.
\newblock In C.~J.~C. Burges, L.~Bottou, M.~Welling, Z.~Ghahramani, and K.~Q.
  Weinberger, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~26. Curran Associates, Inc., 2013.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2013/file/f33ba15effa5c10e873bf3842afb46a6-Paper.pdf}.

\bibitem[Golovin et~al.(2017)Golovin, Solnik, Moitra, Kochanski, Karro, and
  Sculley]{golovin}
Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro,
  and D.~Sculley.
\newblock {Google Vizier: A Service for Black-Box Optimization}.
\newblock In \emph{{Proceedings of the 23rd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}}, KDD '17, page 1487â1495, New
  York, NY, USA, 2017. Association for Computing Machinery.
\newblock ISBN 9781450348874.
\newblock \doi{10.1145/3097983.3098043}.
\newblock URL \url{https://doi.org/10.1145/3097983.3098043}.

\bibitem[Feurer et~al.(2018)Feurer, Letham, and Bakshy]{feurer2018scalable}
Matthias Feurer, Benjamin Letham, and Eytan Bakshy.
\newblock {Scalable meta-learning for bayesian optimization using
  ranking-weighted gaussian process ensembles}.
\newblock In \emph{AutoML Workshop at ICML}, volume~7, 2018.

\bibitem[Law et~al.(2019)Law, Zhao, Chan, Huang, and
  Sejdinovic]{law2018hyperparameter}
Ho~Chung~Leon Law, Peilin Zhao, Lucian Chan, Junzhou Huang, and Dino
  Sejdinovic.
\newblock Hyperparameter learning via distributional transfer.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Springenberg et~al.(2016)Springenberg, Klein, Falkner, and
  Hutter]{springenberg2016bayesian}
Jost~Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter.
\newblock {Bayesian optimization with robust Bayesian neural networks}.
\newblock \emph{Advances in Neural Information Processing Systems},
  29:\penalty0 4134--4142, 2016.

\bibitem[Perrone et~al.(2018)Perrone, Jenatton, Seeger, and
  Archambeau]{perrone2018}
Valerio Perrone, Rodolphe Jenatton, Matthias~W Seeger, and Cedric Archambeau.
\newblock {Scalable Hyperparameter Transfer Learning}.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31, 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/14c879f3f5d8ed93a09f6090d77c2cc3-Paper.pdf}.

\bibitem[Ãksendal(2000)]{stochastic_differential_equations}
Bernt Ãksendal.
\newblock \emph{{Stochastic Differential Equations: An Introduction with
  Applications}}, volume~82.
\newblock 01 2000.
\newblock ISBN 978-3-540-60243-9.
\newblock \doi{10.1007/978-3-662-03185-8}.

\bibitem[Lyne et~al.(2015)Lyne, Girolami, AtchadÃ©, Strathmann, and
  Simpson]{russian_roulette_estimator2015}
Anne-Marie Lyne, Mark Girolami, Yves AtchadÃ©, Heiko Strathmann, and Daniel
  Simpson.
\newblock {On Russian Roulette Estimates for Bayesian Inference with
  Doubly-Intractable Likelihoods}.
\newblock \emph{Statistical Science}, 30\penalty0 (4):\penalty0 443 -- 467,
  2015.
\newblock \doi{10.1214/15-STS523}.
\newblock URL \url{https://doi.org/10.1214/15-STS523}.

\bibitem[Luo et~al.(2020)Luo, Beatson, Norouzi, Zhu, Duvenaud, Adams, and
  Chen]{luo2020sumo}
Yucen Luo, Alex Beatson, Mohammad Norouzi, Jun Zhu, David Duvenaud, Ryan~P
  Adams, and Ricky~TQ Chen.
\newblock {SUMO: Unbiased estimation of log marginal probability for latent
  variable models}.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Shi et~al.(2018)Shi, Sun, and Zhu]{shi2018spectral}
Jiaxin Shi, Shengyang Sun, and Jun Zhu.
\newblock A spectral approach to gradient estimation for implicit
  distributions.
\newblock In \emph{International Conference on Machine Learning}, pages
  4644--4653. PMLR, 2018.

\bibitem[Song et~al.(2020)Song, Garg, Shi, and Ermon]{song2020sliced}
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon.
\newblock Sliced score matching: A scalable approach to density and score
  estimation.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 574--584.
  PMLR, 2020.

\bibitem[Wilson et~al.(2016)Wilson, Hu, Salakhutdinov, and
  Xing]{wilson2016deep}
Andrew~Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric~P Xing.
\newblock Deep kernel learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 370--378. PMLR, 2016.

\bibitem[Qui{{\~n}}onero-Candela and Rasmussen(2005)]{quinonero-candela05a}
Joaquin Qui{{\~n}}onero-Candela and Carl~Edward Rasmussen.
\newblock {A Unifying View of Sparse Approximate Gaussian Process Regression}.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (65):\penalty0 1939--1959, 2005.
\newblock URL \url{http://jmlr.org/papers/v6/quinonero-candela05a.html}.

\bibitem[Bauer et~al.(2016)Bauer, van~der Wilk, and
  Rasmussen]{bauer2016understanding}
Matthias Bauer, Mark van~der Wilk, and Carl~Edward Rasmussen.
\newblock Understanding probabilistic sparse gaussian process approximations.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Dixon and Szego(1978)]{dixon1978}
L.~C.~W. Dixon and G.~P. Szego.
\newblock {The Global Optimization Problem: An Introduction}.
\newblock In \emph{Towards Global Optimisation 2}. North-Holland Pub. Co, 1978.

\bibitem[Berkenkamp et~al.(2021)Berkenkamp, Eivazi, Grossberger, Skubch, Spitz,
  Daniel, and Falkner]{berkenkamp2021}
Felix Berkenkamp, Anna Eivazi, Lukas Grossberger, Kathrin Skubch, Jonathan
  Spitz, Christian Daniel, and Stefan Falkner.
\newblock {Probabilistic Meta-Learning for Bayesian Optimization}.
\newblock \url{https://openreview.net/pdf?id=fdZvTFn8Yq}, 2021.

\bibitem[Molga and Smutnicki(2005)]{molga2005test}
Marcin Molga and Czes{\l}aw Smutnicki.
\newblock Test functions for optimization needs.
\newblock \emph{Test functions for optimization needs}, 101:\penalty0 48, 2005.

\bibitem[Friedman et~al.(2010)Friedman, Hastie, and
  Tibshirani]{friedman2010regularization}
Jerome Friedman, Trevor Hastie, and Rob Tibshirani.
\newblock Regularization paths for generalized linear models via coordinate
  descent.
\newblock \emph{Journal of statistical software}, 33\penalty0 (1):\penalty0 1,
  2010.

\bibitem[Breiman et~al.(1984)Breiman, Friedman, Stone, and
  Olshen]{breiman1984classification}
Leo Breiman, Jerome Friedman, Charles~J Stone, and Richard~A Olshen.
\newblock \emph{Classification and regression trees}.
\newblock CRC press, 1984.

\bibitem[Therneau et~al.(2018)Therneau, Atkinson, Ripley, and
  Ripley]{therneau2018package}
Terry Therneau, Beth Atkinson, Brian Ripley, and Maintainer~Brian Ripley.
\newblock rpart: Recursive partitioning and regression trees, 2018.
\newblock URL \url{https://cran.r-project.org/web/packages/rpart/index.html}.

\bibitem[Chen and Guestrin(2016)]{chen2016xgboost}
Tianqi Chen and Carlos Guestrin.
\newblock Xgboost: A scalable tree boosting system.
\newblock In \emph{Proceedings of the 22nd acm sigkdd international conference
  on knowledge discovery and data mining}, pages 785--794, 2016.

\bibitem[K{\"u}hn et~al.(2018)K{\"u}hn, Probst, Thomas, and
  Bischl]{kuhn2018automatic}
Daniel K{\"u}hn, Philipp Probst, Janek Thomas, and Bernd Bischl.
\newblock Automatic exploration of machine learning experiments on openml.
\newblock \emph{arXiv preprint arXiv:1806.10961}, 2018.

\bibitem[Bischl et~al.(2017)Bischl, Casalicchio, Feurer, Hutter, Lang,
  Mantovani, Rijn, and Vanschoren]{Bischl2017OpenMLBS}
B.~Bischl, Giuseppe Casalicchio, Matthias Feurer, F.~Hutter, Michel Lang,
  R.~Mantovani, J.~N. Rijn, and J.~Vanschoren.
\newblock Openml benchmarking suites and the openml100.
\newblock \emph{arXiv preprint arXiv:1708.03731}, 2017.

\bibitem[Milne et~al.(2017)Milne, Schietinger, Aiba, Alarcon, Alex, Anghel,
  Arsov, Beard, Beaud, Bettoni, et~al.]{milne2017swissfel}
Christopher~J Milne, Thomas Schietinger, Masamitsu Aiba, Arturo Alarcon,
  J{\"u}rgen Alex, Alexander Anghel, Vladimir Arsov, Carl Beard, Paul Beaud,
  Simona Bettoni, et~al.
\newblock Swissfel: the swiss x-ray free electron laser.
\newblock \emph{Applied Sciences}, 2017.

\bibitem[Pflug(2012)]{pflug2012optimization}
Georg~Ch Pflug.
\newblock \emph{Optimization of stochastic models: the interface between
  simulation and optimization}, volume 373.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Gardner et~al.(2018)Gardner, Pleiss, Weinberger, Bindel, and
  Wilson]{gpytorch}
Jacob Gardner, Geoff Pleiss, Kilian~Q Weinberger, David Bindel, and Andrew~G
  Wilson.
\newblock Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu
  acceleration.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/27e8e17134dd7083b050476733207ea1-Paper.pdf}.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{ICLR}, 2019.

\bibitem[Rothfuss et~al.(2019{\natexlab{b}})Rothfuss, Ferreira, Boehm, Walther,
  Ulrich, Asfour, and Krause]{rothfuss2019noise}
Jonas Rothfuss, Fabio Ferreira, Simon Boehm, Simon Walther, Maxim Ulrich, Tamim
  Asfour, and Andreas Krause.
\newblock {Noise Regularization for Conditional Density Estimation}.
\newblock \emph{arXiv preprint arXiv:1907.08982}, 2019{\natexlab{b}}.

\end{thebibliography}
