\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam \& Sastry(2017)Achiam and Sastry]{Achiam2017SurpriseBasedIM}
Achiam, J. and Sastry, S.~S.
\newblock Surprise-based intrinsic motivation for deep reinforcement learning.
\newblock \emph{ArXiv}, abs/1703.01732, 2017.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Abbeel, and Zaremba]{her}
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P.,
  McGrew, B., Tobin, J., Abbeel, P., and Zaremba, W.
\newblock Hindsight experience replay.
\newblock \emph{CoRR}, abs/1707.01495, 2017.
\newblock URL \url{http://arxiv.org/abs/1707.01495}.

\bibitem[Arulkumaran et~al.(2017)Arulkumaran, Deisenroth, Brundage, and
  Bharath]{Arulkumaran2017DeepRL}
Arulkumaran, K., Deisenroth, M.~P., Brundage, M., and Bharath, A.~A.
\newblock Deep reinforcement learning: A brief survey.
\newblock \emph{IEEE Signal Processing Magazine}, 34:\penalty0 26--38, 2017.

\bibitem[Baldassarre \& Mirolli(2013)Baldassarre and Mirolli]{Baldassarre2013}
Baldassarre, G. and Mirolli, M.
\newblock \emph{Intrinsically Motivated Learning Systems: An Overview}, pp.\
  1--14.
\newblock Springer Berlin Heidelberg, Berlin, Heidelberg, 2013.
\newblock ISBN 978-3-642-32375-1.
\newblock \doi{10.1007/978-3-642-32375-1_1}.
\newblock URL \url{https://doi.org/10.1007/978-3-642-32375-1_1}.

\bibitem[Barto et~al.(2004)Barto, Singh, and Chentanez]{chentanez}
Barto, A.~G., Singh, S., and Chentanez, N.
\newblock Intrinsically motivated learning of hierarchical collections of
  skills.
\newblock In \emph{Proceedings of International Conference on Developmental
  Learning (ICDL)}. MIT Press, Cambridge, MA, 2004.

\bibitem[Begus et~al.(2014)Begus, Gliga, and
  Southgate]{begus_infantslearnwhattheywant}
Begus, K., Gliga, T., and Southgate, V.
\newblock Infants learn what they want to learn: Responding to infant pointing
  leads to superior learning.
\newblock \emph{PLOS ONE}, 9\penalty0 (10):\penalty0 1--4, 10 2014.
\newblock \doi{10.1371/journal.pone.0108817}.

\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,
  Saxton, and Munos]{bellemare2016unifying}
Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and
  Munos, R.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1471--1479, 2016.

\bibitem[Bellman(1957)]{Bellman1957}
Bellman, R.
\newblock A {M}arkovian decision process.
\newblock \emph{Journal of Mathematics and Mechanics}, 6:\penalty0 679--684,
  1957.

\bibitem[{Benureau} \& {Oudeyer}(2015){Benureau} and {Oudeyer}]{7346130}
{Benureau}, F. and {Oudeyer}, P.
\newblock Diversity-driven selection of exploration strategies in multi-armed
  bandits.
\newblock In \emph{2015 Joint IEEE International Conference on Development and
  Learning and Epigenetic Robotics (ICDL-EpiRob)}, pp.\  135--142, 2015.

\bibitem[Burda et~al.(2019)Burda, Edwards, Pathak, Storkey, Darrell, and
  Efros]{pathak18largescale}
Burda, Y., Edwards, H., Pathak, D., Storkey, A., Darrell, T., and Efros, A.~A.
\newblock Large-scale study of curiosity-driven learning.
\newblock In \emph{ICLR}, 2019.

\bibitem[Chitnis et~al.(2016)Chitnis, Hadfield-Menell, Gupta, Srivastava,
  Groshev, Lin, and Abbeel]{learnedheuristics}
Chitnis, R., Hadfield-Menell, D., Gupta, A., Srivastava, S., Groshev, E., Lin,
  C., and Abbeel, P.
\newblock Guided search for task and motion plans using learned heuristics.
\newblock In \emph{2016 IEEE International Conference on Robotics and
  Automation, ICRA 2016}, volume 2016-June, pp.\  447--454. Institute of
  Electrical and Electronics Engineers Inc., 6 2016.
\newblock \doi{10.1109/ICRA.2016.7487165}.

\bibitem[Chitnis et~al.(2020)Chitnis, Tulsiani, Gupta, and
  Gupta]{Chitnis2020IntrinsicMF}
Chitnis, R., Tulsiani, S., Gupta, S., and Gupta, A.
\newblock Intrinsic motivation for encouraging synergistic behavior.
\newblock \emph{ArXiv}, abs/2002.05189, 2020.

\bibitem[Choi et~al.(2019)Choi, Guo, Moczulski, Oh, Wu, Norouzi, and
  Lee]{choi2018contingencyaware}
Choi, J., Guo, Y., Moczulski, M., Oh, J., Wu, N., Norouzi, M., and Lee, H.
\newblock Contingency-aware exploration in reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HyxGB2AcY7}.

\bibitem[Coumans(2015)]{bullet}
Coumans, E.
\newblock Bullet physics simulation.
\newblock In \emph{ACM SIGGRAPH 2015 Courses}, SIGGRAPH '15, New York, NY, USA,
  2015. ACM.
\newblock ISBN 978-1-4503-3634-5.
\newblock \doi{10.1145/2776880.2792704}.
\newblock URL \url{http://doi.acm.org/10.1145/2776880.2792704}.

\bibitem[Dantam et~al.(2016)Dantam, Kingston, Chaudhuri, and
  Kavraki]{Dantam2016IncrementalTA}
Dantam, N.~T., Kingston, Z.~K., Chaudhuri, S., and Kavraki, L.~E.
\newblock Incremental task and motion planning: A constraint-based approach.
\newblock In \emph{Robotics: Science and Systems}, 2016.

\bibitem[Deisenroth et~al.(2011)Deisenroth, Rasmussen, and Fox]{lowcostmanip}
Deisenroth, M., Rasmussen, C., and Fox, D.
\newblock Learning to control a low-cost manipulator using data-efficient
  reinforcement learning.
\newblock 06 2011.
\newblock \doi{10.15607/RSS.2011.VII.008}.

\bibitem[Fikes \& Nilsson(1971)Fikes and Nilsson]{strips}
Fikes, R.~E. and Nilsson, N.~J.
\newblock Strips: A new approach to the application of theorem proving to
  problem solving.
\newblock \emph{Artificial Intelligence}, 2:\penalty0 189, 1971.

\bibitem[Garrett(2018)]{sspybullet}
Garrett, C.
\newblock Pybullet planning.
\newblock 2018.

\bibitem[Garrett et~al.(2018)Garrett, Lozano{-}P{\'{e}}rez, and
  Kaelbling]{factored}
Garrett, C.~R., Lozano{-}P{\'{e}}rez, T., and Kaelbling, L.~P.
\newblock Sampling-based methods for factored task and motion planning.
\newblock \emph{CoRR}, abs/1801.00680, 2018.
\newblock URL \url{http://arxiv.org/abs/1801.00680}.

\bibitem[Gopnik et~al.(2009)Gopnik, Meltzoff, and Kuhl]{gopnik_scientistincrib}
Gopnik, A., Meltzoff, A., and Kuhl, P.
\newblock \emph{The Scientist In The Crib: Minds, Brains, And How Children
  Learn}.
\newblock HarperCollins, 2009.
\newblock ISBN 9780061846915.

\bibitem[Gravot et~al.(2005)Gravot, Cambon, and Alami]{asymov}
Gravot, F., Cambon, S., and Alami, R.
\newblock asymov: A planner that deals with intricate symbolic and geometric
  problems.
\newblock In Dario, P. and Chatila, R. (eds.), \emph{Robotics Research. The
  Eleventh International Symposium}, pp.\  100--110, Berlin, Heidelberg, 2005.
  Springer Berlin Heidelberg.
\newblock ISBN 978-3-540-31508-7.

\bibitem[Haber et~al.(2018)Haber, Mrowca, Fei-Fei, and
  Yamins]{haber2018learning}
Haber, N., Mrowca, D., Fei-Fei, L., and Yamins, D.~L.
\newblock Learning to play with intrinsically-motivated self-aware agents.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Helmert(2006)]{Helmert06thefast}
Helmert, M.
\newblock The fast downward planning system.
\newblock \emph{Journal of Artificial Intelligence Research}, 26:\penalty0
  191--246, 2006.

\bibitem[Hertle et~al.(2012)Hertle, Dornhege, Keller, and
  Nebel]{semanticattachments}
Hertle, A., Dornhege, C., Keller, T., and Nebel, B.
\newblock Planning with semantic attachments: An object-oriented view.
\newblock In \emph{Proceedings of the 20th European Conference on Artificial
  Intelligence}, ECAI'12, pp.\  402--407, Amsterdam, The Netherlands, The
  Netherlands, 2012. IOS Press.
\newblock ISBN 978-1-61499-097-0.
\newblock \doi{10.3233/978-1-61499-098-7-402}.
\newblock URL \url{https://doi.org/10.3233/978-1-61499-098-7-402}.

\bibitem[Hessel et~al.(2017)Hessel, Modayil, van Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{Hessel2017RainbowCI}
Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney,
  W., Horgan, D., Piot, B., Azar, M.~G., and Silver, D.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In \emph{AAAI}, 2017.

\bibitem[Hoffmann \& Nebel(2001)Hoffmann and Nebel]{FF}
Hoffmann, J. and Nebel, B.
\newblock The ff planning system: Fast plan generation through heuristic
  search.
\newblock \emph{J. Artif. Int. Res.}, 14\penalty0 (1):\penalty0 253--302, May
  2001.
\newblock ISSN 1076-9757.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=1622394.1622404}.

\bibitem[Jaderberg et~al.(2016)Jaderberg, Mnih, Czarnecki, Schaul, Leibo,
  Silver, and Kavukcuoglu]{aux}
Jaderberg, M., Mnih, V., Czarnecki, W.~M., Schaul, T., Leibo, J.~Z., Silver,
  D., and Kavukcuoglu, K.
\newblock Reinforcement learning with unsupervised auxiliary tasks.
\newblock \emph{CoRR}, abs/1611.05397, 2016.
\newblock URL \url{http://arxiv.org/abs/1611.05397}.

\bibitem[Kaelbling \& Lozano-P{\'e}rez(2013)Kaelbling and
  Lozano-P{\'e}rez]{hpn}
Kaelbling, L.~P. and Lozano-P{\'e}rez, T.
\newblock Integrated task and motion planning in belief space.
\newblock \emph{Int. J. Rob. Res.}, 32\penalty0 (9-10):\penalty0 1194--1227,
  August 2013.
\newblock ISSN 0278-3649.
\newblock \doi{10.1177/0278364913484072}.
\newblock URL \url{http://dx.doi.org/10.1177/0278364913484072}.

\bibitem[Kakade(2003)]{kakade2003sample}
Kakade, S.~M.
\newblock \emph{On the sample complexity of reinforcement learning}.
\newblock PhD thesis, University of London London, England, 2003.

\bibitem[Kavraki et~al.(1996)Kavraki, Svestka, Latombe, and Overmars]{prm}
Kavraki, L., Svestka, P., Latombe, J., and Overmars, M.
\newblock Probabilistic roadmaps for path planning in high-dimensional
  configuration spaces.
\newblock \emph{Robotics and Automation, IEEE Transactions on}, 12:\penalty0
  566 -- 580, 09 1996.
\newblock \doi{10.1109/70.508439}.

\bibitem[Kim et~al.(2018)Kim, Kaelbling, and Lozano-Perez]{kimAAAI2018}
Kim, B., Kaelbling, L., and Lozano-Perez, T.
\newblock Guiding search in continuous state-action spaces by learning an
  action sampler from off-target search experience.
\newblock In \emph{Proceedings of the 32th AAAI Conference on Artificial
  Intelligence (AAAI). To appear}. AAAI Press, 2018.
\newblock URL \url{http://lis.csail.mit.edu/pubs/kim-aaai18.pdf}.

\bibitem[Kim et~al.(2019)Kim, Kaelbling, and Lozano-Perez]{kimAAAI2019}
Kim, B., Kaelbling, L.~P., and Lozano-Perez, T.
\newblock Adversarial actor-critic method for task and motion planning problems
  using planning experience.
\newblock In \emph{AAAI Conference on Artificial Intelligence (AAAI)}, 2019.
\newblock URL \url{http://lis.csail.mit.edu/pubs/kim-aaai19.pdf}.

\bibitem[Kingston et~al.(2018)Kingston, Moll, and Kavraki]{constraints}
Kingston, Z., Moll, M., and Kavraki, L.~E.
\newblock Sampling-based methods for motion planning with constraints.
\newblock \emph{Annual Review of Control, Robotics, and Autonomous Systems},
  1\penalty0 (1):\penalty0 159--185, 2018.
\newblock \doi{10.1146/annurev-control-060117-105226}.
\newblock URL \url{https://doi.org/10.1146/annurev-control-060117-105226}.

\bibitem[Kroemer \& Sukhatme(2016)Kroemer and Sukhatme]{Kroemer2016LearningSP}
Kroemer, O. and Sukhatme, G.~S.
\newblock Learning spatial preconditions of manipulation skills using random
  forests.
\newblock \emph{2016 IEEE-RAS 16th International Conference on Humanoid Robots
  (Humanoids)}, pp.\  676--683, 2016.

\bibitem[Kuffner \& LaValle(2000)Kuffner and LaValle]{rrtconnect}
Kuffner, J. and LaValle, S.
\newblock Rrt-connect: An efficient approach to single-query path planning.
\newblock volume~2, pp.\  995--1001, 01 2000.
\newblock \doi{10.1109/ROBOT.2000.844730}.

\bibitem[Kulkarni et~al.(2016)Kulkarni, Narasimhan, Saeedi, and
  Tenenbaum]{kulkarni2016hierarchical}
Kulkarni, T.~D., Narasimhan, K., Saeedi, A., and Tenenbaum, J.
\newblock Hierarchical deep reinforcement learning: Integrating temporal
  abstraction and intrinsic motivation.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3675--3683, 2016.

\bibitem[Lavalle(1998)]{rrt}
Lavalle, S.~M.
\newblock Rapidly-exploring random trees: A new tool for path planning.
\newblock 1998.

\bibitem[Laversanne{-}Finot et~al.(2018)Laversanne{-}Finot, P{\'{e}}r{\'{e}},
  and Oudeyer]{imgep}
Laversanne{-}Finot, A., P{\'{e}}r{\'{e}}, A., and Oudeyer, P.
\newblock Curiosity driven exploration of learned disentangled goal spaces.
\newblock \emph{CoRR}, abs/1807.01521, 2018.
\newblock URL \url{http://arxiv.org/abs/1807.01521}.

\bibitem[Li et~al.(2019)Li, Jabri, Darrell, and Agrawal]{li19relationalrl}
Li, R., Jabri, A., Darrell, T., and Agrawal, P.
\newblock Towards practical multi-object manipulation using relational
  reinforcement learning.
\newblock In \emph{arXiv preprint arXiv:1912.11032}, 2019.

\bibitem[Lillicrap et~al.(2015{\natexlab{a}})Lillicrap, Hunt, Pritzel, Heess,
  Erez, Tassa, Silver, and Wierstra]{ddpg}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning,
  2015{\natexlab{a}}.

\bibitem[Lillicrap et~al.(2015{\natexlab{b}})Lillicrap, Hunt, Pritzel, Heess,
  Erez, Tassa, Silver, and Wierstra]{Lillicrap2015ContinuousCW}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N. M.~O., Erez, T., Tassa,
  Y., Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{CoRR}, abs/1509.02971, 2015{\natexlab{b}}.

\bibitem[Luh et~al.(1980)Luh, Walker, and Paul]{RNEA}
Luh, J. Y.~S., Walker, M.~W., and Paul, R. P.~C.
\newblock {On-Line Computational Scheme for Mechanical Manipulators}.
\newblock \emph{Journal of Dynamic Systems, Measurement, and Control},
  102\penalty0 (2):\penalty0 69--76, 06 1980.
\newblock ISSN 0022-0434.
\newblock \doi{10.1115/1.3149599}.
\newblock URL \url{https://doi.org/10.1115/1.3149599}.

\bibitem[Machado et~al.(2017)Machado, Bellemare, and
  Bowling]{machado2017laplacian}
Machado, M.~C., Bellemare, M.~G., and Bowling, M.
\newblock A laplacian framework for option discovery in reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  2295--2304. JMLR. org, 2017.

\bibitem[Mitash et~al.(2017)Mitash, Bekris, and Boularias]{poseestimation}
Mitash, C., Bekris, K.~E., and Boularias, A.
\newblock A self-supervised learning system for object detection using physics
  simulation and multi-view pose estimation.
\newblock In \emph{IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)}, Vancouver, Canada, 09/2017 2017.
\newblock URL
  \url{https://www.cs.rutgers.edu/~kb572/pubs/physics_object_detection.pdf}.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529, 2015.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{a2c}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T.~P., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock \emph{CoRR}, abs/1602.01783, 2016.
\newblock URL \url{http://arxiv.org/abs/1602.01783}.

\bibitem[Mrowca et~al.(2018)Mrowca, Zhuang, Wang, Haber, Fei-Fei, Tenenbaum,
  and Yamins]{hrn}
Mrowca, D., Zhuang, C., Wang, E., Haber, N., Fei-Fei, L., Tenenbaum, J.~B., and
  Yamins, D. L.~K.
\newblock Flexible neural representation for physics prediction.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Nair et~al.(2018)Nair, McGrew, Andrychowicz, Zaremba, and Abbeel]{oer}
Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., and Abbeel, P.
\newblock Overcoming exploration in reinforcement learning with demonstrations.
\newblock pp.\  6292--6299, 05 2018.
\newblock \doi{10.1109/ICRA.2018.8463162}.

\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and
  Van~Roy]{osband2016deep}
Osband, I., Blundell, C., Pritzel, A., and Van~Roy, B.
\newblock Deep exploration via bootstrapped dqn.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  4026--4034, 2016.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and
  Darrell]{pathak2017curiosity}
Pathak, D., Agrawal, P., Efros, A.~A., and Darrell, T.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition Workshops}, pp.\  16--17, 2017.

\bibitem[Puterman(1994)]{Puterman94}
Puterman, M.~L.
\newblock \emph{Markov Decision Processes---Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons, Inc., New York, NY, 1994.

\bibitem[Riedmiller et~al.(2018{\natexlab{a}})Riedmiller, Hafner, Lampe,
  Neunert, Degrave, van~de Wiele, Mnih, Heess, and
  Springenberg]{pmlr-v80-riedmiller18a}
Riedmiller, M., Hafner, R., Lampe, T., Neunert, M., Degrave, J., van~de Wiele,
  T., Mnih, V., Heess, N., and Springenberg, J.~T.
\newblock Learning by playing solving sparse reward tasks from scratch.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  4344--4353, Stockholmsmässan, Stockholm
  Sweden, 10--15 Jul 2018{\natexlab{a}}. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v80/riedmiller18a.html}.

\bibitem[Riedmiller et~al.(2018{\natexlab{b}})Riedmiller, Hafner, Lampe,
  Neunert, Degrave, de~Wiele, Mnih, Heess, and Springenberg]{sparse1}
Riedmiller, M.~A., Hafner, R., Lampe, T., Neunert, M., Degrave, J., de~Wiele,
  T.~V., Mnih, V., Heess, N., and Springenberg, J.~T.
\newblock Learning by playing - solving sparse reward tasks from scratch.
\newblock \emph{CoRR}, abs/1802.10567, 2018{\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/1802.10567}.

\bibitem[Schmidhuber(2010)]{Schmidhuber2010FormalTO}
Schmidhuber, J.
\newblock Formal theory of creativity, fun, and intrinsic motivation
  (1990–2010).
\newblock \emph{IEEE Transactions on Autonomous Mental Development},
  2:\penalty0 230--247, 2010.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{ppo}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{CoRR}, abs/1707.06347, 2017.
\newblock URL \url{http://arxiv.org/abs/1707.06347}.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, van~den
  Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, Dieleman,
  Grewe, Nham, Kalchbrenner, Sutskever, Lillicrap, Leach, Kavukcuoglu, Graepel,
  and Hassabis]{Silver2016MasteringTG}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., van~den Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I.,
  Lillicrap, T.~P., Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, D.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 529:\penalty0 484--489, 2016.

\bibitem[Singh et~al.(2005)Singh, Barto, and
  Chentanez]{chentanez2005intrinsically}
Singh, S.~P., Barto, A.~G., and Chentanez, N.
\newblock Intrinsically motivated reinforcement learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1281--1288, 2005.

\bibitem[Srivastava et~al.(2014)Srivastava, Fang, Riano, Chitnis, Russell, and
  Abbeel]{interface}
Srivastava, S., Fang, E., Riano, L., Chitnis, R., Russell, S., and Abbeel, P.
\newblock Combined task and motion planning through an extensible
  planner-independent interface layer.
\newblock \emph{Proceedings - IEEE International Conference on Robotics and
  Automation}, pp.\  639--646, 1 2014.
\newblock ISSN 1050-4729.
\newblock \doi{10.1109/ICRA.2014.6906922}.

\bibitem[Tang et~al.(2017)Tang, Houthooft, Foote, Stooke, Chen, Duan, Schulman,
  Turck, and Abbeel]{Tang2017ExplorationAS}
Tang, H., Houthooft, R., Foote, D., Stooke, A., Chen, X., Duan, Y., Schulman,
  J., Turck, F.~D., and Abbeel, P.
\newblock Exploration: A study of count-based exploration for deep
  reinforcement learning.
\newblock In \emph{NIPS}, 2017.

\bibitem[Thrun(1992)]{thrun1992efficient}
Thrun, S.~B.
\newblock Efficient exploration in reinforcement learning.
\newblock 1992.

\bibitem[Toussaint et~al.(2019)Toussaint, Allen, Smith, and
  Tenenbaum]{diff_phys}
Toussaint, M., Allen, K.~R., Smith, K.~A., and Tenenbaum, J.~B.
\newblock Differentiable physics and stable modes for tool-use and manipulation
  planning -- extended abstract, 2019.
\newblock Sister Conference Best Paper Track -- Extended abstract of the
  R:SS'18 paper.

\bibitem[Vega-Brown \& Roy(2018)Vega-Brown and Roy]{analytic}
Vega-Brown, W. and Roy, N.
\newblock Asymptotically optimal planning under piecewise-analytic constraints.
\newblock 2018.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, Oh, Horgan, Kroiss, Danihelka, Huang,
  Sifre, Cai, Agapiou, Jaderberg, Vezhnevets, Leblond, Pohlen, Dalibard,
  Budden, Sulsky, Molloy, Paine, Gulcehre, Wang, Pfaff, Wu, Ring, Yogatama,
  W{\"u}nsch, McKinney, Smith, Schaul, Lillicrap, Kavukcuoglu, Hassabis, Apps,
  and Silver]{Vinyals2019GrandmasterLI}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A.~J.,
  Chung, J., Choi, D.~H., Powell, R.~W., Ewalds, T., Georgiev, P., Oh, J.,
  Horgan, D., Kroiss, M., Danihelka, I., Huang, A., Sifre, L., Cai, T.,
  Agapiou, J.~P., Jaderberg, M., Vezhnevets, A.~S., Leblond, R., Pohlen, T.,
  Dalibard, V., Budden, D., Sulsky, Y., Molloy, J., Paine, T.~L., Gulcehre, C.,
  Wang, Z., Pfaff, T., Wu, Y., Ring, R., Yogatama, D., W{\"u}nsch, D.,
  McKinney, K., Smith, O., Schaul, T., Lillicrap, T.~P., Kavukcuoglu, K.,
  Hassabis, D., Apps, C., and Silver, D.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, pp.\  1--5, 2019.

\bibitem[Wang et~al.(2018)Wang, Garrett, Kaelbling, and
  Lozano-Perez]{wangIROS2018}
Wang, Z., Garrett, C.~R., Kaelbling, L.~P., and Lozano-Perez, T.
\newblock Active model learning and diverse action sampling for task and motion
  planning.
\newblock In \emph{International Conference on Intelligent Robots and Systems
  (IROS)}, 2018.
\newblock URL \url{http://lis.csail.mit.edu/pubs/wang-iros18.pdf}.

\end{thebibliography}
