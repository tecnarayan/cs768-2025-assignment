\begin{thebibliography}{10}

\bibitem{resnet}
He, K., X.~Zhang, S.~Ren, et~al.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778. 2016.

\bibitem{densenet}
Huang, G., Z.~Liu, L.~Van Der~Maaten, et~al.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4700--4708. 2017.

\bibitem{liu2019bidirectional}
Liu, C., H.~Xie, Z.~Zha, et~al.
\newblock Bidirectional attention-recognition model for fine-grained object
  classification.
\newblock \emph{IEEE Transactions on Multimedia}, 22(7):1785--1795, 2019.

\bibitem{senet}
Hu, J., L.~Shen, G.~Sun.
\newblock Squeeze-and-excitation networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7132--7141. 2018.

\bibitem{min2020domain}
Min, S., H.~Yao, H.~Xie, et~al.
\newblock Domain-oriented semantic embedding for zero-shot learning.
\newblock \emph{IEEE Transactions on Multimedia}, 23:3919--3930, 2020.

\bibitem{fu2017look}
Fu, J., H.~Zheng, T.~Mei.
\newblock Look closer to see better: Recurrent attention convolutional neural
  network for fine-grained image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4438--4446. 2017.

\bibitem{min2020multi}
Min, S., H.~Yao, H.~Xie, et~al.
\newblock Multi-objective matrix normalization for fine-grained visual
  recognition.
\newblock \emph{IEEE Transactions on Image Processing}, 29:4996--5009, 2020.

\bibitem{fasterrcnn}
Ren, S., K.~He, R.~Girshick, et~al.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock \emph{Advances in neural information processing systems}, 28, 2015.

\bibitem{retina}
Lin, T.-Y., P.~Goyal, R.~Girshick, et~al.
\newblock Focal loss for dense object detection.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 2980--2988. 2017.

\bibitem{fcos}
Tian, Z., C.~Shen, H.~Chen, et~al.
\newblock Fcos: Fully convolutional one-stage object detection.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 9627--9636. 2019.

\bibitem{maskrcnn}
He, K., G.~Gkioxari, P.~Doll{\'a}r, et~al.
\newblock Mask r-cnn.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 2961--2969. 2017.

\bibitem{deeplab}
Chen, L.-C., G.~Papandreou, I.~Kokkinos, et~al.
\newblock Deeplab: Semantic image segmentation with deep convolutional nets,
  atrous convolution, and fully connected crfs.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 40(4):834--848, 2017.

\bibitem{unet}
Ronneberger, O., P.~Fischer, T.~Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{International Conference on Medical image computing and
  computer-assisted intervention}, pages 234--241. Springer, 2015.

\bibitem{vit}
Dosovitskiy, A., L.~Beyer, A.~Kolesnikov, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{ceit}
Yuan, K., S.~Guo, Z.~Liu, et~al.
\newblock Incorporating convolution designs into visual transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, pages 579--588. 2021.

\bibitem{pvt}
Wang, W., E.~Xie, X.~Li, et~al.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 568--578. 2021.

\bibitem{swin}
Liu, Z., Y.~Lin, Y.~Cao, et~al.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 10012--10022. 2021.

\bibitem{cvt}
Wu, H., B.~Xiao, N.~Codella, et~al.
\newblock Cvt: Introducing convolutions to vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 22--31. 2021.

\bibitem{he2022transfg}
He, J., J.-N. Chen, S.~Liu, et~al.
\newblock Transfg: A transformer architecture for fine-grained recognition.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, vol.~36, pages 852--860. 2022.

\bibitem{detr}
Carion, N., F.~Massa, G.~Synnaeve, et~al.
\newblock End-to-end object detection with transformers.
\newblock In \emph{European conference on computer vision}, pages 213--229.
  Springer, 2020.

\bibitem{updetr}
Dai, Z., B.~Cai, Y.~Lin, et~al.
\newblock Up-detr: Unsupervised pre-training for object detection with
  transformers.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 1601--1610. 2021.

\bibitem{deformdetr}
Zhu, X., W.~Su, L.~Lu, et~al.
\newblock Deformable detr: Deformable transformers for end-to-end object
  detection.
\newblock \emph{arXiv preprint arXiv:2010.04159}, 2020.

\bibitem{segmenter}
Strudel, R., R.~Garcia, I.~Laptev, et~al.
\newblock Segmenter: Transformer for semantic segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 7262--7272. 2021.

\bibitem{sotr}
Guo, R., D.~Niu, L.~Qu, et~al.
\newblock Sotr: Segmenting objects with transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 7157--7166. 2021.

\bibitem{transformer}
Vaswani, A., N.~Shazeer, N.~Parmar, et~al.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem{imagenet}
Russakovsky, O., J.~Deng, H.~Su, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 115(3):211--252,
  2015.

\bibitem{raghu2021vision}
Raghu, M., T.~Unterthiner, S.~Kornblith, et~al.
\newblock Do vision transformers see like convolutional neural networks?
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{park2022vision}
Park, N., S.~Kim.
\newblock How do vision transformers work?
\newblock \emph{arXiv preprint arXiv:2202.06709}, 2022.

\bibitem{convit}
dâ€™Ascoli, S., H.~Touvron, M.~L. Leavitt, et~al.
\newblock Convit: Improving vision transformers with soft convolutional
  inductive biases.
\newblock In \emph{International Conference on Machine Learning}, pages
  2286--2296. PMLR, 2021.

\bibitem{localvit}
Li, Y., K.~Zhang, J.~Cao, et~al.
\newblock Localvit: Bringing locality to vision transformers.
\newblock \emph{arXiv preprint arXiv:2104.05707}, 2021.

\bibitem{conformer}
Peng, Z., W.~Huang, S.~Gu, et~al.
\newblock Conformer: Local features coupling global representations for visual
  recognition.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 367--376. 2021.

\bibitem{Visformer}
Chen, Z., L.~Xie, J.~Niu, et~al.
\newblock Visformer: The vision-friendly transformer.
\newblock \emph{CoRR}, abs/2104.12533, 2021.

\bibitem{spach}
Zhao, Y., G.~Wang, C.~Tang, et~al.
\newblock A battle of network structures: An empirical study of cnn,
  transformer, and mlp.
\newblock \emph{arXiv preprint arXiv:2108.13002}, 2021.

\bibitem{pit}
Heo, B., S.~Yun, D.~Han, et~al.
\newblock Rethinking spatial dimensions of vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 11936--11945. 2021.

\bibitem{nest}
Zhang, Z., H.~Zhang, L.~Zhao, et~al.
\newblock Nested hierarchical transformer: Towards accurate, data-efficient and
  interpretable visual understanding.
\newblock In \emph{AAAI Conference on Artificial Intelligence (AAAI), 2022}.
  2022.

\bibitem{mobilenets}
Howard, A.~G., M.~Zhu, B.~Chen, et~al.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{arXiv preprint arXiv:1704.04861}, 2017.

\bibitem{domain}
Peng, X., Q.~Bai, X.~Xia, et~al.
\newblock Moment matching for multi-source domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 1406--1415. 2019.

\bibitem{alexnet}
Krizhevsky, A., I.~Sutskever, G.~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Advances in neural information processing systems}, 25, 2012.

\bibitem{inception}
Szegedy, C., W.~Liu, Y.~Jia, et~al.
\newblock Going deeper with convolutions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1--9. 2015.

\bibitem{resnext}
Xie, S., R.~Girshick, P.~Doll{\'a}r, et~al.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1492--1500. 2017.

\bibitem{sknet}
Li, X., W.~Wang, X.~Hu, et~al.
\newblock Selective kernel networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 510--519. 2019.

\bibitem{deit}
Touvron, H., M.~Cord, M.~Douze, et~al.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In \emph{International Conference on Machine Learning}, pages
  10347--10357. PMLR, 2021.

\bibitem{transgan}
Jiang, Y., S.~Chang, Z.~Wang.
\newblock Transgan: Two pure transformers can make one strong gan, and that can
  scale up.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{vivit}
Arnab, A., M.~Dehghani, G.~Heigold, et~al.
\newblock Vivit: A video vision transformer.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 6836--6846. 2021.

\bibitem{empirical}
Chen, X., S.~Xie, K.~He.
\newblock An empirical study of training self-supervised vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9640--9649. 2021.

\bibitem{non-local}
Wang, X., R.~Girshick, A.~Gupta, et~al.
\newblock Non-local neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7794--7803. 2018.

\bibitem{lrnet}
Hu, H., Z.~Zhang, Z.~Xie, et~al.
\newblock Local relation networks for image recognition.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 3464--3473. 2019.

\bibitem{igpt}
Chen, M., A.~Radford, R.~Child, et~al.
\newblock Generative pretraining from pixels.
\newblock In \emph{International Conference on Machine Learning}, pages
  1691--1703. PMLR, 2020.

\bibitem{bert}
Devlin, J., M.-W. Chang, K.~Lee, et~al.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{tokenlabel}
Jiang, Z.-H., Q.~Hou, L.~Yuan, et~al.
\newblock All tokens matter: Token labeling for training better vision
  transformers.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{early}
Xiao, T., M.~Singh, E.~Mintun, et~al.
\newblock Early convolutions help transformers see better.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:30392--30400, 2021.

\bibitem{t2tvit}
Yuan, L., Y.~Chen, T.~Wang, et~al.
\newblock Tokens-to-token vit: Training vision transformers from scratch on
  imagenet.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 558--567. 2021.

\bibitem{cpvt}
Chu, X., Z.~Tian, B.~Zhang, et~al.
\newblock Conditional positional encodings for vision transformers.
\newblock \emph{arXiv preprint arXiv:2102.10882}, 2021.

\bibitem{pvtv2}
Wang, W., E.~Xie, X.~Li, et~al.
\newblock Pvt v2: Improved baselines with pyramid vision transformer.
\newblock \emph{Computational Visual Media}, pages 1--10, 2022.

\bibitem{ren2021shunted}
Ren, S., D.~Zhou, S.~He, et~al.
\newblock Shunted self-attention via multi-scale token aggregation, 2021.

\bibitem{cmt}
Guo, J., K.~Han, H.~Wu, et~al.
\newblock Cmt: Convolutional neural networks meet vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 12175--12185. 2022.

\bibitem{rvt}
Mao, X., G.~Qi, Y.~Chen, et~al.
\newblock Towards robust vision transformer.
\newblock \emph{arXiv preprint arXiv:2105.07926}, 2021.

\bibitem{mixformer}
Chen, Q., Q.~Wu, J.~Wang, et~al.
\newblock Mixformer: Mixing features across windows and dimensions.
\newblock \emph{arXiv preprint arXiv:2204.02557}, 2022.

\bibitem{vitae}
Xu, Y., Q.~Zhang, J.~Zhang, et~al.
\newblock Vitae: Vision transformer advanced by exploring intrinsic inductive
  bias.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{xcit}
Ali, A., H.~Touvron, M.~Caron, et~al.
\newblock Xcit: Cross-covariance image transformers.
\newblock \emph{Advances in neural information processing systems}, 34, 2021.

\bibitem{davit}
Ding, M., B.~Xiao, N.~Codella, et~al.
\newblock Davit: Dual attention vision transformer.
\newblock \emph{arXiv preprint arXiv:2204.03645}, 2022.

\bibitem{coat}
Xu, W., Y.~Xu, T.~Chang, et~al.
\newblock Co-scale conv-attentional image transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, pages 9981--9990. 2021.

\bibitem{tokenlearner}
Ryoo, M., A.~Piergiovanni, A.~Arnab, et~al.
\newblock Tokenlearner: Adaptive space-time tokenization for videos.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{msg}
Fang, J., L.~Xie, X.~Wang, et~al.
\newblock Msg-transformer: Exchanging local spatial information by manipulating
  messenger tokens.
\newblock In \emph{CVPR}. 2022.

\bibitem{evit}
Liang, Y., C.~Ge, Z.~Tong, et~al.
\newblock Not all patches are what you need: Expediting vision transformers via
  token reorganizations.
\newblock In \emph{International Conference on Learning Representations}. 2022.

\bibitem{vtdrloc}
Liu, Y., E.~Sangineto, W.~Bi, et~al.
\newblock Efficient training of visual transformers with small datasets.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{cct}
Hassani, A., S.~Walton, N.~Shah, et~al.
\newblock Escaping the big data paradigm with compact transformers.
\newblock \emph{arXiv preprint arXiv:2104.05704}, 2021.

\bibitem{slvit}
Lee, S.~H., S.~Lee, B.~C. Song.
\newblock Vision transformer for small-size datasets, 2021.

\bibitem{gelu}
Hendrycks, D., K.~Gimpel.
\newblock Gaussian error linear units (gelus), 2016.

\bibitem{layernorm}
Ba, J.~L., J.~R. Kiros, G.~E. Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{cifar}
Krizhevsky, A., G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Master's thesis, Department of Computer Science, University of
  Toronto}, 2009.

\bibitem{adamw}
Loshchilov, I., F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{wideresnet}
Zagoruyko, S., N.~Komodakis.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem{mixup}
Zhang, H., M.~Cisse, Y.~N. Dauphin, et~al.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{International Conference on Learning Representations}. 2018.

\bibitem{cutmix}
Yun, S., D.~Han, S.~J. Oh, et~al.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 6023--6032. 2019.

\bibitem{autoaugment}
Cubuk, E.~D., B.~Zoph, D.~Mane, et~al.
\newblock Autoaugment: Learning augmentation strategies from data.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 113--123. 2019.

\bibitem{res2net}
Gao, S.-H., M.-M. Cheng, K.~Zhao, et~al.
\newblock Res2net: A new multi-scale backbone architecture.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 43(2):652--662, 2019.

\bibitem{regnet}
Radosavovic, I., R.~P. Kosaraju, R.~Girshick, et~al.
\newblock Designing network design spaces.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 10428--10436. 2020.

\bibitem{convnext}
Liu, Z., H.~Mao, C.-Y. Wu, et~al.
\newblock A convnet for the 2020s.
\newblock \emph{arXiv preprint arXiv:2201.03545}, 2022.

\bibitem{crossvit}
Chen, C.-F.~R., Q.~Fan, R.~Panda.
\newblock Crossvit: Cross-attention multi-scale vision transformer for image
  classification.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 357--366. 2021.

\bibitem{tnt}
Han, K., A.~Xiao, E.~Wu, et~al.
\newblock Transformer in transformer.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{twins}
Chu, X., Z.~Tian, Y.~Wang, et~al.
\newblock Twins: Revisiting the design of spatial attention in vision
  transformers.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{cait}
Touvron, H., M.~Cord, A.~Sablayrolles, et~al.
\newblock Going deeper with image transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 32--42. 2021.

\bibitem{vil}
Zhang, P., X.~Dai, J.~Yang, et~al.
\newblock Multi-scale vision longformer: A new vision transformer for
  high-resolution image encoding.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 2998--3008. 2021.

\bibitem{lg}
Li, J., Y.~Yan, S.~Liao, et~al.
\newblock Local-to-global self-attention in vision transformers.
\newblock \emph{arXiv preprint arXiv:2107.04735}, 2021.

\bibitem{focal}
Yang, J., C.~Li, P.~Zhang, et~al.
\newblock Focal self-attention for local-global interactions in vision
  transformers.
\newblock \emph{arXiv preprint arXiv:2107.00641}, 2021.

\bibitem{labelsmooth}
Szegedy, C., V.~Vanhoucke, S.~Ioffe, et~al.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826. 2016.

\bibitem{randomerase}
Zhong, Z., L.~Zheng, G.~Kang, et~al.
\newblock Random erasing data augmentation.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, vol.~34, pages 13001--13008. 2020.

\end{thebibliography}
