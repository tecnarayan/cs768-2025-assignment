@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@misc{han2015minimax,
      title={Minimax Estimation of Discrete Distributions under $\ell_1$ Loss}, 
      author={Yanjun Han and Jiantao Jiao and Tsachy Weissman},
      year={2015},
      eprint={1411.1467},
      archivePrefix={arXiv},
      primaryClass={cs.IT}
}


@inproceedings{
rajaraman2021on,
title={On the Value of Interaction and Function Approximation in Imitation Learning},
author={Nived Rajaraman and Yanjun Han and Lin Yang and Jingbo Liu and Jiantao Jiao and Kannan Ramchandran},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=Xa9Ba6NsJ6}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@article{DBLP:journals/corr/DanielySBS13,
  author    = {Amit Daniely and
               Sivan Sabato and
               Shai Ben{-}David and
               Shai Shalev{-}Shwartz},
  title     = {Multiclass learnability and the {ERM} principle},
  journal   = {CoRR},
  volume    = {abs/1308.2893},
  year      = {2013},
  url       = {http://arxiv.org/abs/1308.2893},
  eprinttype = {arXiv},
  eprint    = {1308.2893},
  timestamp = {Mon, 13 Aug 2018 16:48:50 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/DanielySBS13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mammen-tsybakov,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2673938},
 abstract = {Discriminant analysis for two data sets in Rd with probability densities f and g can be based on the estimation of the set G = x: f(x) ≥ g(x). We consider applications where it is appropriate to assume that the region G has a smooth boundary or belongs to another nonparametric class of sets. In particular, this assumption makes sense if discrimination is used as a data analytic tool. Decision rules based on minimization of empirical risk over the whole class of sets and over sieves are considered. Their rates of convergence are obtained. We show that these rules achieve optimal rates for estimation of G and optimal rates of convergence for Bayes risks. An interesting conclusion is that the optimal rates for Bayes risks can be very fast, in particular, faster than the "parametric" root-n rate. These fast rates cannot be guaranteed for plug-in rules.},
 author = {Enno Mammen and Alexandre B. Tsybakov},
 journal = {The Annals of Statistics},
 number = {6},
 pages = {1808--1829},
 publisher = {Institute of Mathematical Statistics},
 title = {Smooth Discrimination Analysis},
 urldate = {2022-05-18},
 volume = {27},
 year = {1999}
}


@article{audibert-tsybakov,
author = {Jean-Yves Audibert and Alexandre B. Tsybakov},
title = {{Fast learning rates for plug-in classifiers}},
volume = {35},
journal = {The Annals of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {608 -- 633},
keywords = {‎classification‎, excess risk, fast rates of convergence, minimax lower bounds, plug-in classifiers, Statistical learning},
year = {2007},
doi = {10.1214/009053606000001217},
URL = {https://doi.org/10.1214/009053606000001217}
}


@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@inproceedings{maxent,
  added-at = {2012-12-10T00:00:00.000+0100},
  author = {Ziebart, Brian D. and Maas, Andrew L. and Bagnell, J. Andrew and Dey, Anind K.},
  biburl = {https://www.bibsonomy.org/bibtex/2231d8449e296188b91673570bc70b1d6/dblp},
  booktitle = {AAAI},
  editor = {Fox, Dieter and Gomes, Carla P.},
  ee = {http://www.aaai.org/Library/AAAI/2008/aaai08-227.php},
  interhash = {b7a2d62a0a6b56f0bf94f4392f8f445a},
  intrahash = {231d8449e296188b91673570bc70b1d6},
  isbn = {978-1-57735-368-3},
  keywords = {dblp},
  pages = {1433-1438},
  publisher = {AAAI Press},
  timestamp = {2012-12-11T11:46:42.000+0100},
  title = {Maximum Entropy Inverse Reinforcement Learning.},
  url = {http://dblp.uni-trier.de/db/conf/aaai/aaai2008.html#ZiebartMBD08},
  year = 2008
}

@article{billingsley,
author = {Patrick Billingsley},
title = {{Statistical Methods in Markov Chains}},
volume = {32},
journal = {The Annals of Mathematical Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {12 -- 40},
year = {1961},
doi = {10.1214/aoms/1177705136},
URL = {https://doi.org/10.1214/aoms/1177705136}
}


@inproceedings{apprenticeship,
author = {Abbeel, Pieter and Ng, Andrew Y.},
title = {Apprenticeship Learning via Inverse Reinforcement Learning},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015430},
doi = {10.1145/1015330.1015430},
abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {1},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@article{deepmaxent,
  author    = {Markus Wulfmeier and
               Peter Ondruska and
               Ingmar Posner},
  title     = {Deep Inverse Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1507.04888},
  year      = {2015},
  url       = {http://arxiv.org/abs/1507.04888},
  archivePrefix = {arXiv},
  eprint    = {1507.04888},
  timestamp = {Mon, 13 Aug 2018 16:46:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/WulfmeierOP15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{gcl,
  author    = {Chelsea Finn and
               Sergey Levine and
               Pieter Abbeel},
  title     = {Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization},
  journal   = {CoRR},
  volume    = {abs/1603.00448},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.00448},
  archivePrefix = {arXiv},
  eprint    = {1603.00448},
  timestamp = {Mon, 13 Aug 2018 16:48:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/FinnLA16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bayesian,
author = {Ramachandran, Deepak and Amir, Eyal},
title = {Bayesian Inverse Reinforcement Learning},
year = {2007},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert's actions to derive a probability distribution over the space of reward functions. We present efficient algorithms that find solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.},
booktitle = {Proceedings of the 20th International Joint Conference on Artifical Intelligence},
pages = {2586–2591},
numpages = {6},
location = {Hyderabad, India},
series = {IJCAI'07}
}

@misc{fasterbayesian,
      title={Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences}, 
      author={Daniel S. Brown and Russell Coleman and Ravi Srinivasan and Scott Niekum},
      year={2020},
      eprint={2002.09089},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{DBLP:journals/corr/FinnLA16,
  author    = {Chelsea Finn and
               Sergey Levine and
               Pieter Abbeel},
  title     = {Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization},
  journal   = {CoRR},
  volume    = {abs/1603.00448},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.00448},
  archivePrefix = {arXiv},
  eprint    = {1603.00448},
  timestamp = {Mon, 13 Aug 2018 16:48:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/FinnLA16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.1145/1143844.1143936,
author = {Ratliff, Nathan D. and Bagnell, J. Andrew and Zinkevich, Martin A.},
title = {Maximum Margin Planning},
year = {2006},
isbn = {1595933832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143844.1143936},
doi = {10.1145/1143844.1143936},
abstract = {Imitation learning of sequential, goal-directed behavior by standard supervised techniques is often difficult. We frame learning such behaviors as a maximum margin structured prediction problem over a space of policies. In this approach, we learn mappings from features to cost so an optimal policy in an MDP with these cost mimics the expert's behavior. Further, we demonstrate a simple, provably efficient approach to structured maximum margin learning, based on the subgradient method, that leverages existing fast algorithms for inference. Although the technique is general, it is particularly relevant in problems where A* and dynamic programming approaches make learning policies tractable in problems beyond the limitations of a QP formulation. We demonstrate our approach applied to route planning for outdoor mobile robots, where the behavior a designer wishes a planner to execute is often clear, while specifying cost functions that engender this behavior is a much more difficult task.},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
pages = {729–736},
numpages = {8},
location = {Pittsburgh, Pennsylvania, USA},
series = {ICML '06}
}

@article{DBLP:journals/corr/abs-2102-06483,
  author    = {Alex J. Chan and
               Mihaela van der Schaar},
  title     = {Scalable Bayesian Inverse Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/2102.06483},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.06483},
  archivePrefix = {arXiv},
  eprint    = {2102.06483},
  timestamp = {Thu, 18 Feb 2021 15:26:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-06483.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.5555/645528.657613,
author = {Ng, Andrew Y. and Harada, Daishi and Russell, Stuart J.},
title = {Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping},
year = {1999},
isbn = {1558606122},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Sixteenth International Conference on Machine Learning},
pages = {278–287},
numpages = {10},
series = {ICML '99}
}

@article{DBLP:journals/corr/abs-1710-11248,
  author    = {Justin Fu and
               Katie Luo and
               Sergey Levine},
  title     = {Learning Robust Rewards with Adversarial Inverse Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1710.11248},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.11248},
  archivePrefix = {arXiv},
  eprint    = {1710.11248},
  timestamp = {Mon, 13 Aug 2018 16:46:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1710-11248.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/MnihBMGLHSK16,
  author    = {Volodymyr Mnih and
               Adri{\`{a}} Puigdom{\`{e}}nech Badia and
               Mehdi Mirza and
               Alex Graves and
               Timothy P. Lillicrap and
               Tim Harley and
               David Silver and
               Koray Kavukcuoglu},
  title     = {Asynchronous Methods for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1602.01783},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.01783},
  archivePrefix = {arXiv},
  eprint    = {1602.01783},
  timestamp = {Mon, 13 Aug 2018 16:47:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MnihBMGLHSK16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/DuanCHSA16,
  author    = {Yan Duan and
               Xi Chen and
               Rein Houthooft and
               John Schulman and
               Pieter Abbeel},
  title     = {Benchmarking Deep Reinforcement Learning for Continuous Control},
  journal   = {CoRR},
  volume    = {abs/1604.06778},
  year      = {2016},
  url       = {http://arxiv.org/abs/1604.06778},
  archivePrefix = {arXiv},
  eprint    = {1604.06778},
  timestamp = {Mon, 03 Sep 2018 12:15:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/DuanCHSA16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1811-12560,
  author    = {Vincent Fran{\c{c}}ois{-}Lavet and
               Peter Henderson and
               Riashat Islam and
               Marc G. Bellemare and
               Joelle Pineau},
  title     = {An Introduction to Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1811.12560},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.12560},
  archivePrefix = {arXiv},
  eprint    = {1811.12560},
  timestamp = {Mon, 03 Dec 2018 07:50:28 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-12560.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.5555/645529.657801,
author = {Ng, Andrew Y. and Russell, Stuart J.},
title = {Algorithms for Inverse Reinforcement Learning},
year = {2000},
isbn = {1558607072},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Seventeenth International Conference on Machine Learning},
pages = {663–670},
numpages = {8},
series = {ICML '00}
}

@article{DBLP:journals/corr/abs-1801-01290,
  author    = {Tuomas Haarnoja and
               Aurick Zhou and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
               with a Stochastic Actor},
  journal   = {CoRR},
  volume    = {abs/1801.01290},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.01290},
  archivePrefix = {arXiv},
  eprint    = {1801.01290},
  timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1801-01290.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/SchulmanWDRK17,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {CoRR},
  volume    = {abs/1707.06347},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.06347},
  archivePrefix = {arXiv},
  eprint    = {1707.06347},
  timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SchulmanWDRK17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NIPS1999_6449f44a,
 author = {Konda, Vijay and Tsitsiklis, John},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 pages = {},
 publisher = {MIT Press},
 title = {Actor-Critic Algorithms},
 url = {https://proceedings.neurips.cc/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf},
 volume = {12},
 year = {2000}
}

@article{DBLP:journals/corr/abs-2010-04767,
  author    = {Tanmay Vilas Samak and
               Chinmay Vilas Samak and
               Sivanathan Kandhasamy},
  title     = {Robust Behavioral Cloning for Autonomous Vehicles using End-to-End
               Imitation Learning},
  journal   = {CoRR},
  volume    = {abs/2010.04767},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.04767},
  archivePrefix = {arXiv},
  eprint    = {2010.04767},
  timestamp = {Tue, 20 Oct 2020 15:08:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-04767.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{DBLP:journals/corr/abs-1809-02925,
  author    = {Ilya Kostrikov and
               Kumar Krishna Agrawal and
               Sergey Levine and
               Jonathan Tompson},
  title     = {Addressing Sample Inefficiency and Reward Bias in Inverse Reinforcement
               Learning},
  journal   = {CoRR},
  volume    = {abs/1809.02925},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.02925},
  archivePrefix = {arXiv},
  eprint    = {1809.02925},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1809-02925.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2102-02454,
  author    = {Mingqi Yuan and
               Man{-}On Pun and
               Yi Chen and
               Qi Cao},
  title     = {Hybrid Adversarial Inverse Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/2102.02454},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.02454},
  archivePrefix = {arXiv},
  eprint    = {2102.02454},
  timestamp = {Tue, 09 Feb 2021 13:35:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-02454.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/FinnCAL16,
  author    = {Chelsea Finn and
               Paul F. Christiano and
               Pieter Abbeel and
               Sergey Levine},
  title     = {A Connection between Generative Adversarial Networks, Inverse Reinforcement
               Learning, and Energy-Based Models},
  journal   = {CoRR},
  volume    = {abs/1611.03852},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.03852},
  archivePrefix = {arXiv},
  eprint    = {1611.03852},
  timestamp = {Mon, 13 Aug 2018 16:47:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/FinnCAL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{goodfellow2014generative,
      title={Generative Adversarial Networks}, 
      author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
      year={2014},
      eprint={1406.2661},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@INPROCEEDINGS{6386109,
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
  title={MuJoCo: A physics engine for model-based control}, 
  year={2012},
  volume={},
  number={},
  pages={5026-5033},
  doi={10.1109/IROS.2012.6386109}}
  
  @article{DBLP:journals/corr/abs-2010-03691,
  author    = {Wonseok Jeon and
               Chen{-}Yang Su and
               Paul Barde and
               Thang Doan and
               Derek Nowrouzezahrai and
               Joelle Pineau},
  title     = {Regularized Inverse Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/2010.03691},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.03691},
  archivePrefix = {arXiv},
  eprint    = {2010.03691},
  timestamp = {Tue, 13 Oct 2020 15:25:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-03691.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1806-06877,
  author    = {Saurabh Arora and
               Prashant Doshi},
  title     = {A Survey of Inverse Reinforcement Learning: Challenges, Methods and
               Progress},
  journal   = {CoRR},
  volume    = {abs/1806.06877},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.06877},
  archivePrefix = {arXiv},
  eprint    = {1806.06877},
  timestamp = {Mon, 13 Aug 2018 16:46:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-06877.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NEURIPS2018_2d969e2c,
 author = {Mendez, Jorge and Shivkumar, Shashank and Eaton, Eric},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Lifelong Inverse Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2018/file/2d969e2cee8cfa07ce7ca0bb13c7a36d-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{DBLP:journals/corr/abs-1805-07687,
  author    = {Daniel S. Brown and
               Scott Niekum},
  title     = {Machine Teaching for Inverse Reinforcement Learning: Algorithms and
               Applications},
  journal   = {CoRR},
  volume    = {abs/1805.07687},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.07687},
  archivePrefix = {arXiv},
  eprint    = {1805.07687},
  timestamp = {Mon, 13 Aug 2018 16:46:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1805-07687.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1208-2112,
  author    = {Qifeng Qiao and
               Peter A. Beling},
  title     = {Inverse Reinforcement Learning with Gaussian Process},
  journal   = {CoRR},
  volume    = {abs/1208.2112},
  year      = {2012},
  url       = {http://arxiv.org/abs/1208.2112},
  archivePrefix = {arXiv},
  eprint    = {1208.2112},
  timestamp = {Mon, 13 Aug 2018 16:46:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1208-2112.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1905-08513,
  author    = {Ce Ju and
               Dong Eui Chang},
  title     = {Stochastic Inverse Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1905.08513},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.08513},
  archivePrefix = {arXiv},
  eprint    = {1905.08513},
  timestamp = {Wed, 29 May 2019 11:27:50 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-08513.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{swamy2021moments,
  author       = {Gokul Swamy and Sanjiban Choudhury and J. Andrew Bagnell and Zhiwei Steven Wu},
  title        = {Of Moments and Matching: A Game-Theoretic Framework for Closing the Imitation Gap},
  conference   = {Proceedings of the 38th International Conference on Machine Learning},
  url          = {https://arxiv.org/abs/2103.03236},
  year={2021}
}

@techreport{pomerleau1989alvinn,
  title={Alvinn: An autonomous land vehicle in a neural network},
  author={Pomerleau, Dean A},
  year={1989},
  institution={CARNEGIE-MELLON UNIV PITTSBURGH PA ARTIFICIAL INTELLIGENCE AND PSYCHOLOGY~…}
}


@misc{ho2016generative,
      title={Generative Adversarial Imitation Learning}, 
      author={Jonathan Ho and Stefano Ermon},
      year={2016},
      eprint={1606.03476},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{rajaraman2020toward,
  title={Toward the fundamental limits of imitation learning},
  author={Rajaraman, Nived and Yang, Lin F and Jiao, Jiantao and Ramachandran, Kannan},
  journal={arXiv preprint arXiv:2009.05990},
  year={2020}
}

@article{hadfield2017inverse,
  title={Inverse reward design},
  author={Hadfield-Menell, Dylan and Milli, Smitha and Abbeel, Pieter and Russell, Stuart and Dragan, Anca},
  journal={arXiv preprint arXiv:1711.02827},
  year={2017}
}

@article{ZADEH1965338,
title = {Fuzzy sets},
journal = {Information and Control},
volume = {8},
number = {3},
pages = {338-353},
year = {1965},
issn = {0019-9958},
doi = {https://doi.org/10.1016/S0019-9958(65)90241-X},
url = {https://www.sciencedirect.com/science/article/pii/S001999586590241X},
author = {L.A. Zadeh},
abstract = {A fuzzy set is a class of objects with a continuum of grades of membership. Such a set is characterized by a membership (characteristic) function which assigns to each object a grade of membership ranging between zero and one. The notions of inclusion, union, intersection, complement, relation, convexity, etc., are extended to such sets, and various properties of these notions in the context of fuzzy sets are established. In particular, a separation theorem for convex fuzzy sets is proved without requiring that the fuzzy sets be disjoint.}
}

@inproceedings{ke2020imitation,
  title={Imitation learning as f-divergence minimization},
  author={Ke, Liyiming and Choudhury, Sanjiban and Barnes, Matt and Sun, Wen and Lee, Gilwoo and Srinivasa, Siddhartha},
  booktitle={International Workshop on the Algorithmic Foundations of Robotics},
  pages={313--329},
  year={2020},
  organization={Springer}
}


@InProceedings{pmlr-v9-ross10a,
  title = 	 {Efficient Reductions for Imitation Learning},
  author = 	 {Ross, Stephane and Bagnell, Drew},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {661--668},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/ross10a/ross10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/ross10a.html}
}

@article{multiclass,
  author    = {Amit Daniely and
               Shai Shalev{-}Shwartz},
  title     = {Optimal Learners for Multiclass Problems},
  journal   = {CoRR},
  volume    = {abs/1405.2420},
  year      = {2014},
  url       = {http://arxiv.org/abs/1405.2420},
  eprinttype = {arXiv},
  eprint    = {1405.2420},
  timestamp = {Mon, 13 Aug 2018 16:48:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/DanielyS14a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{GT-MM-conc,
author = {McAllester, David A. and Schapire, Robert E.},
title = {On the Convergence Rate of Good-Turing Estimators},
year = {2000},
isbn = {155860703X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Thirteenth Annual Conference on Computational Learning Theory},
pages = {1–6},
numpages = {6},
series = {COLT ’00}
}

@misc{rajaraman2021provably,
      title={Provably Breaking the Quadratic Error Compounding Barrier in Imitation Learning, Optimally}, 
      author={Nived Rajaraman and Yanjun Han and Lin F. Yang and Kannan Ramchandran and Jiantao Jiao},
      year={2021},
      eprint={2102.12948},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ross2011reduction,
      title={A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning}, 
      author={Stephane Ross and Geoffrey J. Gordon and J. Andrew Bagnell},
      year={2011},
      eprint={1011.0686},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{muller1997integral,
  title={Integral probability metrics and their generating classes of functions},
  author={M{\"u}ller, Alfred},
  journal={Advances in Applied Probability},
  volume={29},
  number={2},
  pages={429--443},
  year={1997},
  publisher={Cambridge University Press}
}

@inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K and others},
  booktitle={Aaai},
  volume={8},
  pages={1433--1438},
  year={2008},
  organization={Chicago, IL, USA}
}

@inproceedings{brantley2019disagreement,
  title={Disagreement-regularized imitation learning},
  author={Brantley, Kiant{\'e} and Sun, Wen and Henaff, Mikael},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@misc{pathak2019selfsupervised,
      title={Self-Supervised Exploration via Disagreement}, 
      author={Deepak Pathak and Dhiraj Gandhi and Abhinav Gupta},
      year={2019},
      eprint={1906.04161},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{wang2019random,
  title={Random expert distillation: Imitation learning via expert policy support estimation},
  author={Wang, Ruohan and Ciliberto, Carlo and Amadori, Pierluigi Vito and Demiris, Yiannis},
  booktitle={International Conference on Machine Learning},
  pages={6536--6544},
  year={2019},
  organization={PMLR}
}

@article{burda2018exploration,
  title={Exploration by random network distillation},
  author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  journal={arXiv preprint arXiv:1810.12894},
  year={2018}
}

@article{freund1997decision,
  title={A decision-theoretic generalization of on-line learning and an application to boosting},
  author={Freund, Yoav and Schapire, Robert E},
  journal={Journal of computer and system sciences},
  volume={55},
  number={1},
  pages={119--139},
  year={1997},
  publisher={Elsevier}
}

@misc{gulrajani2017improved,
      title={Improved Training of Wasserstein GANs}, 
      author={Ishaan Gulrajani and Faruk Ahmed and Martin Arjovsky and Vincent Dumoulin and Aaron Courville},
      year={2017},
      eprint={1704.00028},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{arjovsky2017wasserstein,
      title={Wasserstein GAN}, 
      author={Martin Arjovsky and Soumith Chintala and Léon Bottou},
      year={2017},
      eprint={1701.07875},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{haarnoja2018soft,
      title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}, 
      author={Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
      year={2018},
      eprint={1801.01290},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{daskalakis2017training,
  title={Training gans with optimism},
  author={Daskalakis, Constantinos and Ilyas, Andrew and Syrgkanis, Vasilis and Zeng, Haoyang},
  journal={arXiv preprint arXiv:1711.00141},
  year={2017}
}

@article{syrgkanis2015fast,
  title={Fast convergence of regularized learning in games},
  author={Syrgkanis, Vasilis and Agarwal, Alekh and Luo, Haipeng and Schapire, Robert E},
  journal={arXiv preprint arXiv:1507.00407},
  year={2015}
}

@misc{kidambi2021morel,
      title={MOReL : Model-Based Offline Reinforcement Learning}, 
      author={Rahul Kidambi and Aravind Rajeswaran and Praneeth Netrapalli and Thorsten Joachims},
      year={2021},
      eprint={2005.05951},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@MISC{coumans2019,
author =   {Erwin Coumans and Yunfei Bai},
title =    {PyBullet, a Python module for physics simulation for games, robotics and machine learning},
howpublished = {\url{http://pybullet.org}},
year = {2016--2019}
}

@misc{reddy2019sqil,
      title={SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards}, 
      author={Siddharth Reddy and Anca D. Dragan and Sergey Levine},
      year={2019},
      eprint={1905.11108},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{ratliff2006maximum,
  title={Maximum margin planning},
  author={Ratliff, Nathan D and Bagnell, J Andrew and Zinkevich, Martin A},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={729--736},
  year={2006}
}

@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}

@misc{rl-zoo3,
  author = {Raffin, Antonin},
  title = {RL Baselines3 Zoo},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/DLR-RM/rl-baselines3-zoo}},
}