\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aghajanyan et~al.(2021)Aghajanyan, Gupta, Shrivastava, Chen,
  Zettlemoyer, and Gupta]{aghajanyan2021muppet}
Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke
  Zettlemoyer, and Sonal Gupta.
\newblock Muppet: Massive multi-task representations with pre-finetuning.
\newblock \emph{arXiv preprint arXiv:2101.11038}, 2021.

\bibitem[Bender and Koller(2020)]{bender2020climbing}
Emily~M Bender and Alexander Koller.
\newblock Climbing towards {NLU}: On meaning, form, and understanding in the
  age of data.
\newblock In \emph{Proc. of ACL}, 2020.

\bibitem[Bisk et~al.(2020)Bisk, Holtzman, Thomason, Andreas, Bengio, Chai,
  Lapata, Lazaridou, May, Nisnevich, Pinto, and Turian]{bisk2020experience}
Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce
  Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich,
  Nicolas Pinto, and Joseph Turian.
\newblock Experience grounds language, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{brown-gpt3-neurips}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 1877--1901. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Deits(2015)]{deits_python}
Robin Deits.
\newblock rdeits/cryptics, 2015.
\newblock URL \url{https://github.com/rdeits/cryptics}.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Efrat et~al.(2021)Efrat, Shaham, Kilman, and
  Levy]{efrat2021cryptonite}
Avia Efrat, Uri Shaham, Dan Kilman, and Omer Levy.
\newblock Cryptonite: A cryptic crossword benchmark for extreme ambiguity in
  language, 2021.

\bibitem[Fellbaum(1998)]{fellbaum1998wordnet}
C.~Fellbaum.
\newblock \emph{{WordNet: An electronic lexical database}}.
\newblock MIT Press, Cambridge, MA, 1998.

\bibitem[French(1999)]{FRENCH1999128_catastrophic}
Robert~M. French.
\newblock Catastrophic forgetting in connectionist networks.
\newblock \emph{Trends in Cognitive Sciences}, 3\penalty0 (4):\penalty0
  128--135, 1999.
\newblock ISSN 1364-6613.
\newblock \doi{https://doi.org/10.1016/S1364-6613(99)01294-2}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S1364661399012942}.

\bibitem[Friedlander and Fine(2018)]{friedlander2018penny}
Kathryn~J Friedlander and Philip~A Fine.
\newblock The penny drops: Investigating insight through the medium of cryptic
  crosswords.
\newblock \emph{Frontiers in psychology}, 9:\penalty0 904, 2018.

\bibitem[Friedlander and Fine(2020)]{friedlander2020fluid}
Kathryn~J Friedlander and Philip~A Fine.
\newblock Fluid intelligence is key to successful cryptic crossword solving.
\newblock \emph{Journal of Expertise}, 3\penalty0 (2):\penalty0 101--132, 2020.

\bibitem[Geva et~al.(2020)Geva, Gupta, and Berant]{geva-etal-2020-injecting}
Mor Geva, Ankit Gupta, and Jonathan Berant.
\newblock Injecting numerical reasoning skills into language models.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 946--958, Online, July 2020.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.89}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.89}.

\bibitem[Ginsberg(2011)]{ginsberg2011dr}
Matthew~L Ginsberg.
\newblock Dr. fill: Crosswords and an implemented solver for singly weighted
  csps.
\newblock \emph{Journal of Artificial Intelligence Research}, 42:\penalty0
  851--886, 2011.

\bibitem[Hardcastle(2001)]{Hardcastle2001Using}
D~Hardcastle.
\newblock Using the bnc to produce dialectic cryptic crossword clues.
\newblock In \emph{Corpus Linguistics 2001}, pages 256--265, 2001.

\bibitem[Hardcastle(2007)]{hardcastle2007riddle}
David Hardcastle.
\newblock \emph{Riddle posed by computer (6): the computer generation of
  cryptic crossword clues.}
\newblock PhD thesis, Citeseer, 2007.

\bibitem[Hart and Davis(1992)]{hart1992cryptic}
M~Hart and Robert~H Davis.
\newblock Cryptic crossword clue interpreter.
\newblock \emph{Information and Software Technology}, 34\penalty0 (1):\penalty0
  16--27, 1992.

\bibitem[He et~al.(2019)He, Peng, and Liang]{punhe2019pun}
He~He, Nanyun Peng, and Percy Liang.
\newblock Pun generation with surprise.
\newblock \emph{arXiv preprint arXiv:1904.06828}, 2019.

\bibitem[Itzhak and Levy(2021)]{spellingbee-itzhak2021models}
Itay Itzhak and Omer Levy.
\newblock Models in a spelling bee: Language models implicitly learn the
  character composition of tokens.
\newblock \emph{arXiv preprint arXiv:2108.11193}, 2021.

\bibitem[Jascob(2019)]{lemminflect}
Brad Jascob.
\newblock bjascob/lemminflect, 2019.
\newblock URL \url{https://github.com/bjascob/LemmInflect}.

\bibitem[Kao et~al.(2013)Kao, Levy, and Goodman]{punkao2013funny}
Justine~T Kao, Roger Levy, and Noah~D Goodman.
\newblock The funny thing about incongruity: A computational model of humor in
  puns.
\newblock In \emph{Proceedings of the Annual Meeting of the Cognitive Science
  Society}, volume~35, 2013.

\bibitem[Kudo and Richardson(2018)]{kudo2018sentencepiece}
Taku Kudo and John Richardson.
\newblock Sentencepiece: A simple and language independent subword tokenizer
  and detokenizer for neural text processing.
\newblock \emph{arXiv preprint arXiv:1808.06226}, 2018.

\bibitem[Lake et~al.(2017)Lake, Ullman, Tenenbaum, and
  Gershman]{lake2017building}
Brenden~M Lake, Tomer~D Ullman, Joshua~B Tenenbaum, and Samuel~J Gershman.
\newblock Building machines that learn and think like people.
\newblock \emph{Behavioral and brain sciences}, 40, 2017.

\bibitem[Littman et~al.(2002)Littman, Keim, and
  Shazeer]{littman2002probabilistic}
Michael~L Littman, Greg~A Keim, and Noam Shazeer.
\newblock A probabilistic approach to solving crossword puzzles.
\newblock \emph{Artificial Intelligence}, 134\penalty0 (1-2):\penalty0 23--55,
  2002.

\bibitem[Luo et~al.(2019)Luo, Li, Yang, Chang, Sui, Sun, et~al.]{punluo2019pun}
Fuli Luo, Shunyao Li, Pengcheng Yang, Baobao Chang, Zhifang Sui, Xu~Sun, et~al.
\newblock Pun-gan: Generative adversarial network for pun generation.
\newblock \emph{arXiv preprint arXiv:1910.10950}, 2019.

\bibitem[Manning et~al.(2020)Manning, Clark, Hewitt, Khandelwal, and
  Levy]{manning2020emergent}
Christopher~D Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer
  Levy.
\newblock Emergent linguistic structure in artificial neural networks trained
  by self-supervision.
\newblock \emph{Proceedings of the National Academy of Sciences}, 2020.

\bibitem[Marcus(2020)]{marcus2020next}
Gary Marcus.
\newblock The next decade in {AI}: Four steps towards robust artificial
  intelligence.
\newblock \emph{arXiv preprint arXiv:2002.06177}, 2020.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,
  Cournapeau, Brucher, Perrot, and Duchesnay]{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2825--2830,
  2011.

\bibitem[Pwanson(2021)]{xd_cw_paul}
Saul Pwanson, 2021.
\newblock URL \url{http://xd.saul.pw/data/}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI Blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{2020t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (140):\penalty0 1--67, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Rogers et~al.(2020)Rogers, Kovaleva, and Rumshisky]{rogers2020primer}
Anna Rogers, Olga Kovaleva, and Anna Rumshisky.
\newblock A primer in bertology: What we know about how bert works.
\newblock \emph{arXiv preprint arXiv:2002.12327}, 2020.

\bibitem[Shazeer and Stern(2018)]{shazeer2018adafactor}
Noam Shazeer and Mitchell Stern.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock In \emph{International Conference on Machine Learning}, pages
  4596--4604. PMLR, 2018.

\bibitem[Shazeer et~al.(1999)Shazeer, Littman, and Keim]{shazeer1999solving}
Noam~M Shazeer, Michael~L Littman, and Greg~A Keim.
\newblock Solving crossword puzzles as probabilistic constraint satisfaction.
\newblock In \emph{AAAI/IAAI}, pages 156--162, 1999.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Silver et~al.(2018)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2018general}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
  Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
  Graepel, et~al.
\newblock A general reinforcement learning algorithm that masters chess, shogi,
  and go through self-play.
\newblock \emph{Science}, 362\penalty0 (6419):\penalty0 1140--1144, 2018.

\bibitem[Tafjord et~al.(2019)Tafjord, Gardner, Lin, and
  Clark]{tafjord-etal-2019-quartz}
Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter Clark.
\newblock {Q}ua{RT}z: An open-domain dataset of qualitative relationship
  questions.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 5941--5946, Hong Kong,
  China, November 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-1608}.
\newblock URL \url{https://aclanthology.org/D19-1608}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{Vaswani-etal:2017}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems 30}, pages 5998--6008. Curran Associates,
  Inc., 2017.
\newblock URL
  \url{http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Scao, Gugger, Drame, Lhoest, and Rush]{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 38--45, Online,
  October 2020. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/2020.emnlp-demos.6}.

\bibitem[Wu et~al.(2021)Wu, Rabe, Li, Ba, Grosse, and Szegedy]{wu2021lime}
Yuhuai Wu, Markus Rabe, Wenda Li, Jimmy Ba, Roger Grosse, and Christian
  Szegedy.
\newblock Lime: Learning inductive bias for primitives of mathematical
  reasoning.
\newblock \emph{arXiv preprint arXiv:2101.06223}, 2021.

\end{thebibliography}
