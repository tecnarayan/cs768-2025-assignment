@incollection{NIPS2017_7181,
	title        = {Attention is All you Need},
	author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
	year         = 2017,
	booktitle    = {Advances in Neural Information Processing Systems 30},
	publisher    = {Curran Associates, Inc.},
	volume       = 30,
	pages        = {5998--6008},
	url          = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf},
	editor       = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett}
}
@incollection{DBLP:journals/corr/SutskeverVL14,
	title        = {Sequence to Sequence Learning with Neural Networks},
	author       = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
	year         = 2014,
	journal      = {Advances in NIPS},
	booktitle    = {Advances in Neural Information Processing Systems 27},
	publisher    = {Curran Associates, Inc.},
	pages        = {3104--3112},
	editor       = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger}
}
@book{10.5555/1734086,
	title        = {Statistical Machine Translation},
	author       = {Koehn, Philipp},
	year         = 2010,
	publisher    = {Cambridge University Press},
	address      = {USA},
	isbn         = {0521874157},
	edition      = {1st},
	abstract     = {This introductory text to statistical machine translation (SMT) provides all of the theories and methods needed to build a statistical machine translator, such as Google Language Tools and Babelfish. In general, statistical techniques allow automatic translation systems to be built quickly for any language-pair using only translated texts and generic software. With increasing globalization, statistical machine translation will be central to communication and commerce. Based on courses and tutorials, and classroom-tested globally, it is ideal for instruction or self-study, for advanced undergraduates and graduate students in computer science and/or computational linguistics, and researchers in natural language processing. The companion website provides open-source corpora and tool-kits.}
}
@misc{lample2019crosslingual,
      title={Cross-lingual Language Model Pretraining}, 
      author={Guillaume Lample and Alexis Conneau},
      year={2019},
      eprint={1901.07291},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{DBLP:journals/corr/abs-2112-06905,
  author    = {Nan Du and
               Yanping Huang and
               Andrew M. Dai and
               Simon Tong and
               Dmitry Lepikhin and
               Yuanzhong Xu and
               Maxim Krikun and
               Yanqi Zhou and
               Adams Wei Yu and
               Orhan Firat and
               Barret Zoph and
               Liam Fedus and
               Maarten Bosma and
               Zongwei Zhou and
               Tao Wang and
               Yu Emma Wang and
               Kellie Webster and
               Marie Pellat and
               Kevin Robinson and
               Kathy Meier{-}Hellstern and
               Toju Duke and
               Lucas Dixon and
               Kun Zhang and
               Quoc V. Le and
               Yonghui Wu and
               Zhifeng Chen and
               Claire Cui},
  title     = {GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
  journal   = {CoRR},
  volume    = {abs/2112.06905},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.06905},
  eprinttype = {arXiv},
  eprint    = {2112.06905},
  timestamp = {Mon, 03 Jan 2022 15:45:35 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-06905.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{rae2022scaling,
      title={Scaling Language Models: Methods, Analysis & Insights from Training Gopher}, 
      author={Jack W. Rae and Sebastian Borgeaud and Trevor Cai and Katie Millican and Jordan Hoffmann and Francis Song and John Aslanides and Sarah Henderson and Roman Ring and Susannah Young and Eliza Rutherford and Tom Hennigan and Jacob Menick and Albin Cassirer and Richard Powell and George van den Driessche and Lisa Anne Hendricks and Maribeth Rauh and Po-Sen Huang and Amelia Glaese and Johannes Welbl and Sumanth Dathathri and Saffron Huang and Jonathan Uesato and John Mellor and Irina Higgins and Antonia Creswell and Nat McAleese and Amy Wu and Erich Elsen and Siddhant Jayakumar and Elena Buchatskaya and David Budden and Esme Sutherland and Karen Simonyan and Michela Paganini and Laurent Sifre and Lena Martens and Xiang Lorraine Li and Adhiguna Kuncoro and Aida Nematzadeh and Elena Gribovskaya and Domenic Donato and Angeliki Lazaridou and Arthur Mensch and Jean-Baptiste Lespiau and Maria Tsimpoukelli and Nikolai Grigorev and Doug Fritz and Thibault Sottiaux and Mantas Pajarskas and Toby Pohlen and Zhitao Gong and Daniel Toyama and Cyprien de Masson d'Autume and Yujia Li and Tayfun Terzi and Vladimir Mikulik and Igor Babuschkin and Aidan Clark and Diego de Las Casas and Aurelia Guy and Chris Jones and James Bradbury and Matthew Johnson and Blake Hechtman and Laura Weidinger and Iason Gabriel and William Isaac and Ed Lockhart and Simon Osindero and Laura Rimell and Chris Dyer and Oriol Vinyals and Kareem Ayoub and Jeff Stanway and Lorrayne Bennett and Demis Hassabis and Koray Kavukcuoglu and Geoffrey Irving},
      year={2022},
      eprint={2112.11446},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{bahdanau+al-2014-nmt,
	title        = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
	author       = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	year         = 2015,
	month        = sep,
	journal      = {arXiv preprint arXiv:1409.0473},
	booktitle    = {{Proceedings of the International Conference on Learning Representations (ICLR)}},
	volume       = {abs/1409.0473},
	bibsource    = {dblp computer science bibliography, http://dblp.org},
	timestamp    = {Wed, 01 Oct 2014 15:00:04 +0200}
}

@inproceedings{aharoni-etal-2019-massively,
	title        = {Massively Multilingual Neural Machine Translation},
	author       = {Aharoni, Roee  and Johnson, Melvin  and Firat, Orhan},
	year         = 2019,
	month        = jun,
	booktitle    = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Minneapolis, Minnesota},
	pages        = {3874--3884},
	doi          = {10.18653/v1/N19-1388},
	url          = {https://www.aclweb.org/anthology/N19-1388}
}
@article{DBLP:journals/corr/abs-1907-05019,
	title        = {Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges},
	author       = {Naveen Arivazhagan and Ankur Bapna and Orhan Firat and Dmitry Lepikhin and Melvin Johnson and Maxim Krikun and Mia Xu Chen and Yuan Cao and George Foster and Colin Cherry and Wolfgang Macherey and Zhifeng Chen and Yonghui Wu},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1907.05019},
	url          = {http://arxiv.org/abs/1907.05019},
	archiveprefix = {arXiv},
	eprint       = {1907.05019},
	timestamp    = {Wed, 17 Jul 2019 10:27:36 +0200}
}
@inproceedings{zhang-etal-2020-improving,
	title        = {Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation},
	author       = {Zhang, Biao  and Williams, Philip  and Titov, Ivan  and Sennrich, Rico},
	year         = 2020,
	month        = jul,
	booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {1628--1639},
	doi          = {10.18653/v1/2020.acl-main.148},
	url          = {https://www.aclweb.org/anthology/2020.acl-main.148}
}
@inproceedings{NEURIPS2020_1457c0d6,
	title        = {Language Models are Few-Shot Learners},
	author       = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year         = 2020,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 33,
	pages        = {1877--1901},
	url          = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin}
}

@inproceedings{devlin-etal-2019-bert,
	title        = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	author       = {Devlin, Jacob  and Chang, Ming-Wei  and Lee, Kenton  and Toutanova, Kristina},
	year         = 2019,
	month        = jun,
	journal      = {arXiv preprint arXiv:1810.04805},
	booktitle    = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Minneapolis, Minnesota},
	pages        = {4171--4186},
	doi          = {10.18653/v1/N19-1423},
	url          = {https://aclanthology.org/N19-1423},
	abstract     = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}
@article{kaplan2020scaling,
	title        = {Scaling laws for neural language models},
	author       = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2001.08361}
}
@inproceedings{
    gordon2021data,
    title={Data and Parameter Scaling Laws for Neural Machine Translation},
    author={Mitchell A Gordon and Kevin Duh and Jared Kaplan},
    booktitle={ACL Rolling Review - May 2021},
    year={2021},
    url={https://openreview.net/forum?id=IKA7MLxsLSu}
}
@article{Ghorbani2021ScalingLF,
  title={Scaling Laws for Neural Machine Translation},
  author={B. Ghorbani and Orhan Firat and Markus Freitag and Ankur Bapna and Maxim Krikun and Xavier Garc{\'i}a and Ciprian Chelba and Colin Cherry},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.07740}
}
@inproceedings{barrault-etal-2020-findings,
    title = "Findings of the 2020 Conference on Machine Translation ({WMT}20)",
    author = {Barrault, Lo{\"\i}c  and
      Biesialska, Magdalena  and
      Bojar, Ond{\v{r}}ej  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Haddow, Barry  and
      Huck, Matthias  and
      Joanis, Eric  and
      Kocmi, Tom  and
      Koehn, Philipp  and
      Lo, Chi-kiu  and
      Ljube{\v{s}}i{\'c}, Nikola  and
      Monz, Christof  and
      Morishita, Makoto  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Pal, Santanu  and
      Post, Matt  and
      Zampieri, Marcos},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.1",
    pages = "1--55",
    abstract = "This paper presents the results of the news translation task and the similar language translation task, both organised alongside the Conference on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages.",
}
@inproceedings{ansari-etal-2020-findings,
	title        = {{FINDINGS} {OF} {THE} {IWSLT} 2020 {EVALUATION} {CAMPAIGN}},
	author       = {Ansari, Ebrahim  and Axelrod, Amittai  and Bach, Nguyen  and Bojar, Ond{\v{r}}ej  and Cattoni, Roldano  and Dalvi, Fahim  and Durrani, Nadir  and Federico, Marcello  and Federmann, Christian  and Gu, Jiatao  and Huang, Fei  and Knight, Kevin  and Ma, Xutai  and Nagesh, Ajay  and Negri, Matteo  and Niehues, Jan  and Pino, Juan  and Salesky, Elizabeth  and Shi, Xing  and St{\"u}ker, Sebastian  and Turchi, Marco  and Waibel, Alexander  and Wang, Changhan},
	year         = 2020,
	month        = jul,
	booktitle    = {Proceedings of the 17th International Conference on Spoken Language Translation},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {1--34},
	doi          = {10.18653/v1/2020.iwslt-1.1},
	url          = {https://www.aclweb.org/anthology/2020.iwslt-1.1}
}
@inproceedings{zhang-etal-2018-accelerating,
    title = "Accelerating Neural Transformer via an Average Attention Network",
    author = "Zhang, Biao  and
      Xiong, Deyi  and
      Su, Jinsong",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1166",
    doi = "10.18653/v1/P18-1166",
    pages = "1789--1798",
    abstract = "With parallelizable attention networks, the neural Transformer is very fast to train. However, due to the auto-regressive architecture and self-attention in the decoder, the decoding procedure becomes slow. To alleviate this issue, we propose an average attention network as an alternative to the self-attention network in the decoder of the neural Transformer. The average attention network consists of two layers, with an average layer that models dependencies on previous positions and a gating layer that is stacked over the average layer to enhance the expressiveness of the proposed attention network. We apply this network on the decoder part of the neural Transformer to replace the original target-side self-attention model. With masking tricks and dynamic programming, our model enables the neural Transformer to decode sentences over four times faster than its original version with almost no loss in training time and translation performance. We conduct a series of experiments on WMT17 translation tasks, where on 6 different language pairs, we obtain robust and consistent speed-ups in decoding.",
}
@inproceedings{
kasai2021deep,
title={Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation},
author={Jungo Kasai and Nikolaos Pappas and Hao Peng and James Cross and Noah Smith},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=KpfasTaLUpq}
}
@inproceedings{chen-etal-2018-best,
    title = "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
    author = "Chen, Mia Xu  and
      Firat, Orhan  and
      Bapna, Ankur  and
      Johnson, Melvin  and
      Macherey, Wolfgang  and
      Foster, George  and
      Jones, Llion  and
      Schuster, Mike  and
      Shazeer, Noam  and
      Parmar, Niki  and
      Vaswani, Ashish  and
      Uszkoreit, Jakob  and
      Kaiser, Lukasz  and
      Chen, Zhifeng  and
      Wu, Yonghui  and
      Hughes, Macduff",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1008",
    doi = "10.18653/v1/P18-1008",
    pages = "76--86",
    abstract = "The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT{'}14 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.",
}
@article{Wang2021LanguageMA,
  title={Language Models are Good Translators},
  author={Shuo Wang and Zhaopeng Tu and Zhixing Tan and Wenxuan Wang and Maosong Sun and Yang Liu},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.13627}
}
@inproceedings{NEURIPS2018_4fb8a7a2,
 author = {He, Tianyu and Tan, Xu and Xia, Yingce and He, Di and Qin, Tao and Chen, Zhibo and Liu, Tie-Yan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Layer-Wise Coordination between Encoder and Decoder for Neural Machine Translation},
 url = {https://proceedings.neurips.cc/paper/2018/file/4fb8a7a22a82c80f2c26fe6c1e0dcbb3-Paper.pdf},
 volume = {31},
 year = {2018}
}
@inproceedings{NEURIPS2019_c20bb2d9,
 author = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Unified Language Model Pre-training for Natural Language Understanding and Generation},
 url = {https://proceedings.neurips.cc/paper/2019/file/c20bb2d9a50d5ac1f713f8b34d9aac5a-Paper.pdf},
 volume = {32},
 year = {2019}
}
@article{Fonollosa2019JointSS,
  title={Joint Source-Target Self Attention with Locality Constraints},
  author={Jos{\'e} A. R. Fonollosa and Noe Casas and Marta Ruiz Costa-juss{\`a}},
  journal={ArXiv},
  year={2019},
  volume={abs/1905.06596}
}
@article{Zhai2021ScalingVT,
  title={Scaling Vision Transformers},
  author={Xiaohua Zhai and Alexander Kolesnikov and Neil Houlsby and Lucas Beyer},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.04560}
}
@inproceedings{Stolcke2002SRILMA,
  title={SRILM - an extensible language modeling toolkit},
  author={Andreas Stolcke},
  booktitle={INTERSPEECH},
  year={2002}
}
@inproceedings{heafield-2011-kenlm,
    title = "{K}en{LM}: Faster and Smaller Language Model Queries",
    author = "Heafield, Kenneth",
    booktitle = "Proceedings of the Sixth Workshop on Statistical Machine Translation",
    month = jul,
    year = "2011",
    address = "Edinburgh, Scotland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W11-2123",
    pages = "187--197",
}
@INPROCEEDINGS{614693,  author={Neco, R.P. and Forcada, M.L.},  booktitle={Proceedings of International Conference on Neural Networks (ICNN'97)},   title={Asynchronous translations with recurrent neural nets},   year={1997},  volume={4},  number={},  pages={2535-2540 vol.4},  doi={10.1109/ICNN.1997.614693}}
@inproceedings{kalchbrenner-blunsom-2013-recurrent,
    title = "Recurrent Continuous Translation Models",
    author = "Kalchbrenner, Nal  and
      Blunsom, Phil",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1176",
    pages = "1700--1709",
}
@inproceedings{castano97_eurospeech,
  author={Asuncion Castano and Francisco Casacuberta},
  title={{A connectionist approach to machine translation}},
  year=1997,
  booktitle={Proc. 5th European Conference on Speech Communication and Technology (Eurospeech 1997)},
  pages={91--94}
}
@article{Bahri2021ExplainingNS,
  title={Explaining Neural Scaling Laws},
  author={Yasaman Bahri and Ethan Dyer and Jared Kaplan and Jaehoon Lee and Utkarsh Sharma},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.06701}
}
@article{Hernandez2021ScalingLF,
  title={Scaling Laws for Transfer},
  author={Danny Hernandez and Jared Kaplan and T. J. Henighan and Sam McCandlish},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.01293}
}
@article{Henighan2020ScalingLF,
  title={Scaling Laws for Autoregressive Generative Modeling},
  author={T. J. Henighan and Jared Kaplan and Mor Katz and Mark Chen and Christopher Hesse and Jacob Jackson and Heewoo Jun and Tom B. Brown and Prafulla Dhariwal and Scott Gray and Chris Hallacy and Benjamin Mann and Alec Radford and Aditya Ramesh and Nick Ryder and Daniel M. Ziegler and John Schulman and Dario Amodei and Sam McCandlish},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.14701}
}
@inproceedings{stahlberg-byrne-2019-nmt,
    title = "On {NMT} Search Errors and Model Errors: Cat Got Your Tongue?",
    author = "Stahlberg, Felix  and
      Byrne, Bill",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1331",
    doi = "10.18653/v1/D19-1331",
    pages = "3356--3362",
    abstract = "We report on search errors and model errors in neural machine translation (NMT). We present an exact inference procedure for neural sequence models based on a combination of beam search and depth-first search. We use our exact search to find the global best model scores under a Transformer base model for the entire WMT15 English-German test set. Surprisingly, beam search fails to find these global best model scores in most cases, even with a very large beam size of 100. For more than 50{\%} of the sentences, the model in fact assigns its global best score to the empty translation, revealing a massive failure of neural models in properly accounting for adequacy. We show by constraining search with a minimum translation length that at the root of the problem of empty translations lies an inherent bias towards shorter translations. We conclude that vanilla NMT in its current form requires just the right amount of beam search errors, which, from a modelling perspective, is a highly unsatisfactory conclusion indeed, as the model often prefers an empty translation.",
}
@article{johnson-etal-2017-googles,
	title        = {{G}oogle{'}s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation},
	author       = {Johnson, Melvin  and Schuster, Mike  and Le, Quoc V.  and Krikun, Maxim  and Wu, Yonghui  and Chen, Zhifeng  and Thorat, Nikhil  and Vi{\'e}gas, Fernanda  and Wattenberg, Martin  and Corrado, Greg  and Hughes, Macduff  and Dean, Jeffrey},
	year         = 2017,
	journal      = {Transactions of the Association for Computational Linguistics},
	volume       = 5,
	pages        = {339--351},
	doi          = {10.1162/tacl_a_00065},
	url          = {https://www.aclweb.org/anthology/Q17-1024}
}
@inproceedings{firat-etal-2016-zero,
	title        = {Zero-Resource Translation with Multi-Lingual Neural Machine Translation},
	author       = {Firat, Orhan  and Sankaran, Baskaran  and Al-onaizan, Yaser  and Yarman Vural, Fatos T.  and Cho, Kyunghyun},
	year         = 2016,
	month        = nov,
	booktitle    = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Austin, Texas},
	pages        = {268--277},
	doi          = {10.18653/v1/D16-1026},
	url          = {https://www.aclweb.org/anthology/D16-1026}
}
@article{firat2016multi,
  title={Multi-way, multilingual neural machine translation with a shared attention mechanism},
  author={Firat, Orhan and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1601.01073},
  year={2016}
}
@article{DBLP:journals/corr/abs-1903-07091,
	title        = {The Missing Ingredient in Zero-Shot Neural Machine Translation},
	author       = {Naveen Arivazhagan and Ankur Bapna and Orhan Firat and Roee Aharoni and Melvin Johnson and Wolfgang Macherey},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1903.07091},
	url          = {http://arxiv.org/abs/1903.07091},
	archiveprefix = {arXiv},
	eprint       = {1903.07091},
	timestamp    = {Mon, 01 Apr 2019 14:07:37 +0200}
}
@misc{cho2014learning,
      title={Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}, 
      author={Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
      year={2014},
      eprint={1406.1078},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{wang-etal-2020-negative,
    title = "On Negative Interference in Multilingual Models: Findings and A Meta-Learning Treatment",
    author = "Wang, Zirui  and
      Lipton, Zachary C.  and
      Tsvetkov, Yulia",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.359",
    doi = "10.18653/v1/2020.emnlp-main.359",
    pages = "4438--4450",
}
@inproceedings{
zhang2021share,
title={Share or Not? Learning to Schedule Language-Specific Capacity for Multilingual Translation},
author={Biao Zhang and Ankur Bapna and Rico Sennrich and Orhan Firat},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=Wj4ODo0uyCF}
}
@inproceedings{sennrich-etal-2016-neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}
@misc{kudugunta2019investigating,
      title={Investigating Multilingual NMT Representations at Scale}, 
      author={Sneha Reddy Kudugunta and Ankur Bapna and Isaac Caswell and Naveen Arivazhagan and Orhan Firat},
      year={2019},
      eprint={1909.02197},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{kudo-richardson-2018-sentencepiece,
    title = "{S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
    author = "Kudo, Taku  and
      Richardson, John",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-2012",
    doi = "10.18653/v1/D18-2012",
    pages = "66--71",
    abstract = "This paper describes SentencePiece, a language-independent subword",
}
@inproceedings{graham-etal-2020-statistical,
    title = "Statistical Power and Translationese in Machine Translation Evaluation",
    author = "Graham, Yvette  and
      Haddow, Barry  and
      Koehn, Philipp",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.6",
    doi = "10.18653/v1/2020.emnlp-main.6",
    pages = "72--81",
    abstract = "The term translationese has been used to describe features of translated text, and in this paper, we provide detailed analysis of potential adverse effects of translationese on machine translation evaluation. Our analysis shows differences in conclusions drawn from evaluations that include translationese in test data compared to experiments that tested only with text originally composed in that language. For this reason we recommend that reverse-created test data be omitted from future machine translation test sets. In addition, we provide a re-evaluation of a past machine translation evaluation claiming human-parity of MT. One important issue not previously considered is statistical power of significance tests applied to comparison of human and machine translation. Since the very aim of past evaluations was investigation of ties between human and MT systems, power analysis is of particular importance, to avoid, for example, claims of human parity simply corresponding to Type II error resulting from the application of a low powered test. We provide detailed analysis of tests used in such evaluations to provide an indication of a suitable minimum sample size for future studies.",
}
@inproceedings{freitag-etal-2020-bleu,
    title = "{BLEU} might be Guilty but References are not Innocent",
    author = "Freitag, Markus  and
      Grangier, David  and
      Caswell, Isaac",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.5",
    doi = "10.18653/v1/2020.emnlp-main.5",
    pages = "61--71",
    abstract = "The quality of automatic metrics for machine translation has been increasingly called into question, especially for high-quality systems. This paper demonstrates that, while choice of metric is important, the nature of the references is also critical. We study different methods to collect references and compare their value in automated evaluation by reporting correlation with human evaluation for a variety of systems and metrics. Motivated by the finding that typical references exhibit poor diversity, concentrating around translationese language, we develop a paraphrasing task for linguists to perform on existing reference translations, which counteracts this bias. Our method yields higher correlation with human judgment not only for the submissions of WMT 2019 English to German, but also for Back-translation and APE augmented MT output, which have been shown to have low correlation with automatic metrics using standard references. We demonstrate that our methodology improves correlation with all modern evaluation metrics we look at, including embedding-based methods.To complete this picture, we reveal that multi-reference BLEU does not improve the correlation for high quality output, and present an alternative multi-reference formulation that is more effective.",
}
@inproceedings{post-2018-call,
    title = "A Call for Clarity in Reporting {BLEU} Scores",
    author = "Post, Matt",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6319",
    doi = "10.18653/v1/W18-6319",
    pages = "186--191",
    abstract = "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to {``}the{''} BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SACREBLEU, to facilitate this.",
}

@article{liu2020multilingual,
  title={Multilingual denoising pre-training for neural machine translation},
  author={Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={726--742},
  year={2020},
  publisher={MIT Press}
}

@InProceedings{pmlr-v80-shazeer18a,
  title = 	 {Adafactor: Adaptive Learning Rates with Sublinear Memory Cost},
  author =       {Shazeer, Noam and Stern, Mitchell},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4596--4604},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/shazeer18a/shazeer18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/shazeer18a.html},
  abstract = 	 {In several recently proposed stochastic optimization methods (e.g. RMSProp, Adam, Adadelta), parameter updates are scaled by the inverse square roots of exponential moving averages of squared past gradients. Maintaining these per-parameter second-moment estimators requires memory equal to the number of parameters. For the case of neural network weight matrices, we propose maintaining only the per-row and per-column sums of these moving averages, and estimating the per-parameter second moments based on these sums. We demonstrate empirically that this method produces similar results to the baseline. Secondly, we show that adaptive methods can produce larger-than-desired updates when the decay rate of the second moment accumulator is too slow. We propose update clipping and a gradually increasing decay rate scheme as remedies. Combining these methods and dropping momentum, we achieve comparable results to the published Adam regime in training the Transformer model on the WMT 2014 English-German machine translation task, while using very little auxiliary storage in the optimizer. Finally, we propose scaling the parameter updates based on the scale of the parameters themselves.}
}
@article{JMLR:v21:20-074,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}
@inproceedings{NEURIPS2020_ff4dfdf5,
 author = {Levine, Yoav and Wies, Noam and Sharir, Or and Bata, Hofit and Shashua, Amnon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {22640--22651},
 publisher = {Curran Associates, Inc.},
 title = {Limits to Depth Efficiencies of Self-Attention},
 url = {https://proceedings.neurips.cc/paper/2020/file/ff4dfdf5904e920ce52b48c1cef97829-Paper.pdf},
 volume = {33},
 year = {2020}
}
@inproceedings{wu-etal-2021-language,
    title = "Language Tags Matter for Zero-Shot Neural Machine Translation",
    author = "Wu, Liwei  and
      Cheng, Shanbo  and
      Wang, Mingxuan  and
      Li, Lei",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.264",
    doi = "10.18653/v1/2021.findings-acl.264",
    pages = "3001--3007",
}
@inproceedings{xue-etal-2021-mt5,
    title = "m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    author = "Xue, Linting  and
      Constant, Noah  and
      Roberts, Adam  and
      Kale, Mihir  and
      Al-Rfou, Rami  and
      Siddhant, Aditya  and
      Barua, Aditya  and
      Raffel, Colin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.41",
    doi = "10.18653/v1/2021.naacl-main.41",
    pages = "483--498",
    abstract = "The recent {``}Text-to-Text Transfer Transformer{''} (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent {``}accidental translation{''} in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",
}
@inproceedings{wang-etal-2019-learning-deep,
    title = "Learning Deep Transformer Models for Machine Translation",
    author = "Wang, Qiang  and
      Li, Bei  and
      Xiao, Tong  and
      Zhu, Jingbo  and
      Li, Changliang  and
      Wong, Derek F.  and
      Chao, Lidia S.",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1176",
    doi = "10.18653/v1/P19-1176",
    pages = "1810--1822",
    abstract = "Transformer is the state-of-the-art model in recent machine translation evaluations. Two strands of research are promising to improve models of this kind: the first uses wide networks (a.k.a. Transformer-Big) and has been the de facto standard for development of the Transformer system, and the other uses deeper language representation but faces the difficulty arising from learning deep networks. Here, we continue the line of research on the latter. We claim that a truly deep Transformer model can surpass the Transformer-Big counterpart by 1) proper use of layer normalization and 2) a novel way of passing the combination of previous layers to the next. On WMT{'}16 English-German and NIST OpenMT{'}12 Chinese-English tasks, our deep system (30/25-layer encoder) outperforms the shallow Transformer-Big/Base baseline (6-layer encoder) by 0.4-2.4 BLEU points. As another bonus, the deep model is 1.6X smaller in size and 3X faster in training than Transformer-Big.",
}
@inproceedings{zhang-etal-2019-improving,
    title = "Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention",
    author = "Zhang, Biao  and
      Titov, Ivan  and
      Sennrich, Rico",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1083",
    doi = "10.18653/v1/D19-1083",
    pages = "898--909",
    abstract = "The general trend in NLP is towards increasing model capacity and performance via deeper neural networks. However, simply stacking more layers of the popular Transformer architecture for machine translation results in poor convergence and high computational overhead. Our empirical analysis suggests that convergence is poor due to gradient vanishing caused by the interaction between residual connection and layer normalization. We propose depth-scaled initialization (DS-Init), which decreases parameter variance at the initialization stage, and reduces output variance of residual connections so as to ease gradient back-propagation through normalization layers. To address computational cost, we propose a merged attention sublayer (MAtt) which combines a simplified average-based self-attention sublayer and the encoder-decoder attention sublayer on the decoder side. Results on WMT and IWSLT translation tasks with five translation directions show that deep Transformers with DS-Init and MAtt can substantially outperform their base counterpart in terms of BLEU (+1.1 BLEU on average for 12-layer models), while matching the decoding speed of the baseline model thanks to the efficiency improvements of MAtt. Source code for reproduction will be released soon.",
}

@inproceedings{tenney-etal-2019-bert,
    title = "{BERT} Rediscovers the Classical {NLP} Pipeline",
    author = "Tenney, Ian  and
      Das, Dipanjan  and
      Pavlick, Ellie",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1452",
    doi = "10.18653/v1/P19-1452",
    pages = "4593--4601",
    abstract = "Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.",
}

