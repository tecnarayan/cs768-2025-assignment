\begin{thebibliography}{10}

\bibitem{adriaensen2022efficient}
Steven Adriaensen, Herilalaina Rakotoarison, Samuel M{\"u}ller, and Frank Hutter.
\newblock Efficient bayesian learning curve extrapolation using prior-data fitted networks.
\newblock In {\em Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem{agarwal2005geometric}
Pankaj~K Agarwal, Sariel Har-Peled, Kasturi~R Varadarajan, et~al.
\newblock Geometric approximation via coresets.
\newblock {\em Combinatorial and computational geometry}, 2005.

\bibitem{ahmed2020k}
Mohiuddin Ahmed, Raihan Seraj, and Syed Mohammed~Shamsul Islam.
\newblock The k-means algorithm: A comprehensive survey and performance evaluation.
\newblock {\em Electronics}, 2020.

\bibitem{akiba2019optuna}
Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama.
\newblock Optuna: A next-generation hyperparameter optimization framework.
\newblock In {\em Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining}, pages 2623--2631, 2019.

\bibitem{angwin2022machine}
Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner.
\newblock Machine bias.
\newblock In {\em Ethics of data and analytics}, pages 254--264. Auerbach Publications, 2022.

\bibitem{arik2021tabnet}
Sercan~{\"O} Arik and Tomas Pfister.
\newblock Tabnet: Attentive interpretable tabular learning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)}, 2021.

\bibitem{arun2016loan}
Kumar Arun, Garg Ishan, and Kaur Sanmeet.
\newblock Loan approval prediction based on machine learning approach.
\newblock {\em IOSR J. Comput. Eng}, 18(3):18--21, 2016.

\bibitem{asuncion2007uci}
Arthur Asuncion and David Newman.
\newblock Uci machine learning repository, 2007.

\bibitem{barocas2023fairness}
Solon Barocas, Moritz Hardt, and Arvind Narayanan.
\newblock {\em Fairness and machine learning: Limitations and opportunities}.
\newblock MIT Press, 2023.

\bibitem{borisov2021deep}
Vadim Borisov, Tobias Leemann, Kathrin Se{\ss}ler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci.
\newblock Deep neural networks and tabular data: A survey.
\newblock {\em arXiv preprint arXiv:2110.01889}, 2021.

\bibitem{buczak2015survey}
Anna~L Buczak and Erhan Guven.
\newblock A survey of data mining and machine learning methods for cyber security intrusion detection.
\newblock {\em IEEE Communications surveys \& tutorials}, 18(2):1153--1176, 2015.

\bibitem{chandola2009anomaly}
Varun Chandola, Arindam Banerjee, and Vipin Kumar.
\newblock Anomaly detection: A survey.
\newblock {\em ACM computing surveys (CSUR)}, 41(3):1--58, 2009.

\bibitem{chandrashekar2014survey}
Girish Chandrashekar and Ferat Sahin.
\newblock A survey on feature selection methods.
\newblock {\em Computers \& Electrical Engineering}, 2014.

\bibitem{chen2022danets}
Jintai Chen, Kuanlun Liao, Yao Wan, Danny~Z Chen, and Jian Wu.
\newblock Danets: Deep abstract networks for tabular data classification and regression.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)}, 2022.

\bibitem{chen2024excelformerneuralnetworksurpassing}
Jintai Chen, Jiahuan Yan, Qiyuan Chen, Danny~Ziyi Chen, Jian Wu, and Jimeng Sun.
\newblock Excelformer: A neural network surpassing gbdts on tabular data, 2024.

\bibitem{chen2016xgboost}
Tianqi Chen and Carlos Guestrin.
\newblock Xgboost: A scalable tree boosting system.
\newblock In {\em Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining}, pages 785--794, 2016.

\bibitem{cherepanova2023performance}
Valeriia Cherepanova, Roman Levin, Gowthami Somepalli, Jonas Geiping, C~Bayan Bruss, Andrew~Gordon Wilson, Tom Goldstein, and Micah Goldblum.
\newblock A performance-driven benchmark for feature selection in tabular deep learning.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2023.

\bibitem{clements2020sequential}
Jillian~M Clements, Di~Xu, Nooshin Yousefi, and Dmitry Efimov.
\newblock Sequential deep learning for credit risk monitoring with tabular financial data.
\newblock {\em arXiv preprint arXiv:2012.15330}, 2020.

\bibitem{conover1999practical}
William~Jay Conover.
\newblock {\em Practical nonparametric statistics}, volume 350.
\newblock john wiley \& sons, 1999.

\bibitem{cortes1995support}
Corinna Cortes and Vladimir Vapnik.
\newblock Support-vector networks.
\newblock {\em Machine learning}, 1995.

\bibitem{cover1967nearest}
Thomas Cover and Peter Hart.
\newblock Nearest neighbor pattern classification.
\newblock {\em IEEE transactions on information theory}, 1967.

\bibitem{cox1958regression}
David~R Cox.
\newblock The regression analysis of binary sequences.
\newblock {\em Journal of the Royal Statistical Society: Series B (Methodological)}, 1958.

\bibitem{dettmers2023qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock In {\em Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem{dooley2023forecastpfn}
Samuel Dooley, Gurnoor~Singh Khurana, Chirag Mohapatra, Siddartha~Venkat Naidu, and Colin White.
\newblock Forecastpfn: Synthetically-trained zero-shot forecasting.
\newblock In {\em Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem{dwork2012fairness}
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel.
\newblock Fairness through awareness.
\newblock In {\em Proceedings of the 3rd innovations in theoretical computer science conference}, 2012.

\bibitem{farthestpointsampling}
Y.~Eldar, M.~Lindenbaum, M.~Porat, and Y.Y. Zeevi.
\newblock The farthest point strategy for progressive image sampling.
\newblock In {\em Proceedings of the 12th IAPR International Conference on Pattern Recognition, Vol. 2 - Conference B: Computer Vision and Image Processing. (Cat. No.94CH3440-5)}, pages 93--97 vol.3, 1994.

\bibitem{feng2022latent}
Leo Feng, Hossein Hajimirsadeghi, Yoshua Bengio, and Mohamed~Osama Ahmed.
\newblock Latent bottlenecked attentive neural processes.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{feng2023memory}
Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Yoshua Bengio, and Mohamed~Osama Ahmed.
\newblock Memory efficient neural processes via constant memory attention block.
\newblock {\em OpenReview}, 2023.

\bibitem{friedman2001greedy}
Jerome~H Friedman.
\newblock Greedy function approximation: a gradient boosting machine.
\newblock {\em Annals of statistics}, pages 1189--1232, 2001.

\bibitem{friedman1937use}
Milton Friedman.
\newblock The use of ranks to avoid the assumption of normality implicit in the analysis of variance.
\newblock {\em Journal of the american statistical association}, 1937.

\bibitem{gorishniy2021revisiting}
Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.
\newblock Revisiting deep learning models for tabular data.
\newblock {\em Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem{grinsztajn2022tree}
Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux.
\newblock Why do tree-based models still outperform deep learning on typical tabular data?
\newblock In {\em Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2022.

\bibitem{guo2017deepfm}
Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He.
\newblock Deepfm: a factorization-machine based neural network for ctr prediction.
\newblock In {\em IJCAI}, 2017.

\bibitem{guo2022versatile}
Zongyu Guo, Cuiling Lan, Zhizheng Zhang, Yan Lu, and Zhibo Chen.
\newblock Versatile neural processes for learning implicit neural representations.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{hollmann2022tabpfn}
Noah Hollmann, Samuel M{\"u}ller, Katharina Eggensperger, and Frank Hutter.
\newblock Tabpfn: A transformer that solves small tabular classification problems in a second.
\newblock In {\em Proceedings of the International Conference on Learning Representations (ICLR)}, 2023.

\bibitem{holm1979simple}
Sture Holm.
\newblock A simple sequentially rejective multiple test procedure.
\newblock {\em Scandinavian journal of statistics}, pages 65--70, 1979.

\bibitem{hu2021lora}
Edward~J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, Weizhu Chen, et~al.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In {\em Proceedings of the International Conference on Learning Representations (ICLR)}, 2022.

\bibitem{huang2020tabtransformer}
Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin.
\newblock Tabtransformer: Tabular data modeling using contextual embeddings.
\newblock {\em arXiv preprint arXiv:2012.06678}, 2020.

\bibitem{jiang2024mixtral}
Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al.
\newblock Mixtral of experts.
\newblock {\em arXiv preprint arXiv:2401.04088}, 2024.

\bibitem{jiang2020can}
Zhengbao Jiang, Frank~F Xu, Jun Araki, and Graham Neubig.
\newblock How can we know what language models know?
\newblock {\em Transactions of the Association for Computational Linguistics}, 2020.

\bibitem{johnson2016mimic}
Alistair~EW Johnson, Tom~J Pollard, Lu~Shen, Li-wei~H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony~Celi, and Roger~G Mark.
\newblock Mimic-iii, a freely accessible critical care database.
\newblock {\em Scientific data}, 3(1):1--9, 2016.

\bibitem{kadra2021well}
Arlind Kadra, Marius Lindauer, Frank Hutter, and Josif Grabocka.
\newblock Well-tuned simple nets excel on tabular datasets.
\newblock {\em Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS)}, 34, 2021.

\bibitem{ke2017lightgbm}
Guolin Ke, Qi~Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu.
\newblock Lightgbm: A highly efficient gradient boosting decision tree.
\newblock In {\em Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem{kirichenko2022last}
Polina Kirichenko, Pavel Izmailov, and Andrew~Gordon Wilson.
\newblock Last layer re-training is sufficient for robustness to spurious correlations.
\newblock {\em arXiv preprint arXiv:2204.02937}, 2022.

\bibitem{le2023last}
Phuong~Quynh Le, J{\"o}rg Schl{\"o}tterer, and Christin Seifert.
\newblock Is last layer re-training truly sufficient for robustness to spurious correlations?
\newblock {\em arXiv preprint arXiv:2308.00473}, 2023.

\bibitem{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, 2021.

\bibitem{levin2023transfer}
Roman Levin, Valeriia Cherepanova, Avi Schwarzschild, Arpit Bansal, C~Bayan Bruss, Tom Goldstein, Andrew~Gordon Wilson, and Micah Goldblum.
\newblock Transfer learning with deep tabular models.
\newblock {\em ICLR}, 2023.

\bibitem{liaw2002classification}
Andy Liaw, Matthew Wiener, et~al.
\newblock Classification and regression by randomforest.
\newblock {\em R news}, 2(3):18--22, 2002.

\bibitem{liu2023pre}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.
\newblock {\em ACM Computing Surveys}, 2023.

\bibitem{luo2018neural}
Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu.
\newblock Neural architecture optimization.
\newblock In {\em Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem{mcelfresh2023neural}
Duncan McElfresh, Sujay Khandagale, Jonathan Valverde, Ganesh Ramakrishnan, Vishak Prasad, Micah Goldblum, and Colin White.
\newblock When do neural nets outperform boosted trees on tabular data?
\newblock In {\em Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem{mcmahan2013ad}
H~Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, et~al.
\newblock Ad click prediction: a view from the trenches.
\newblock In {\em Proceedings of the Annual Conference on Knowledge Discovery and Data Mining (KDD)}, pages 1222--1230, 2013.

\bibitem{mehrabi2021survey}
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan.
\newblock A survey on bias and fairness in machine learning.
\newblock {\em ACM computing surveys (CSUR)}, 2021.

\bibitem{muller2023pfns4bo}
Samuel M{\"u}ller, Matthias Feurer, Noah Hollmann, and Frank Hutter.
\newblock Pfns4bo: In-context learning for bayesian optimization.
\newblock In {\em Proceedings of the International Conference on Machine Learning (ICML)}, 2023.

\bibitem{muller2022transformers}
Samuel M{\"u}ller, Noah Hollmann, Sebastian~Pineda Arango, Josif Grabocka, and Frank Hutter.
\newblock Transformers can do bayesian inference.
\newblock In {\em Proceedings of the International Conference on Learning Representations (ICLR)}, 2022.

\bibitem{munteanu2018coresets}
Alexander Munteanu and Chris Schwiegelshohn.
\newblock Coresets-methods and history: A theoreticians design pattern for approximation and streaming algorithms.
\newblock {\em KI-K{\"u}nstliche Intelligenz}, 2018.

\bibitem{müller2023mothernet}
Andreas Müller, Carlo Curino, and Raghu Ramakrishnan.
\newblock Mothernet: A foundational hypernetwork for tabular classification, 2023.

\bibitem{nagler2023statistical}
Thomas Nagler.
\newblock Statistical foundations of prior-data fitted networks.
\newblock In {\em Proceedings of the International Conference on Machine Learning (ICML)}, 2023.

\bibitem{pedregosa2011scikit}
Fabian Pedregosa, Ga{\"e}l Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et~al.
\newblock Scikit-learn: Machine learning in python.
\newblock {\em the Journal of machine Learning research}, 12:2825--2830, 2011.

\bibitem{Popov2020Neural}
Sergei Popov, Stanislav Morozov, and Artem Babenko.
\newblock Neural oblivious decision ensembles for deep learning on tabular data.
\newblock In {\em Proceedings of the International Conference on Learning Representations (ICLR)}, 2020.

\bibitem{prokhorenkova2018catboost}
Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna~Veronika Dorogush, and Andrey Gulin.
\newblock Catboost: unbiased boosting with categorical features.
\newblock {\em Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem{quinlan1986induction}
J.~Ross Quinlan.
\newblock Induction of decision trees.
\newblock {\em Machine learning}, 1986.

\bibitem{rastogi2022semi}
Richa Rastogi, Yair Schiff, Alon Hacohen, Zhaozhi Li, Ian Lee, Yuntian Deng, Mert~R Sabuncu, and Volodymyr Kuleshov.
\newblock Semi-parametric inducing point networks and neural processes.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{richardson2007predicting}
Matthew Richardson, Ewa Dominowska, and Robert Ragno.
\newblock Predicting clicks: estimating the click-through rate for new ads.
\newblock In {\em Proceedings of the 16th international conference on World Wide Web}, pages 521--530, 2007.

\bibitem{rubachev2022revisiting}
Ivan Rubachev, Artem Alekberov, Yury Gorishniy, and Artem Babenko.
\newblock Revisiting pretraining objectives for tabular deep learning.
\newblock {\em arXiv preprint arXiv:2207.03208}, 2022.

\bibitem{savani2020intra}
Yash Savani, Colin White, and Naveen~Sundar Govindarajulu.
\newblock Intra-processing methods for debiasing neural networks.
\newblock {\em Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem{shwartz2022tabular}
Ravid Shwartz-Ziv and Amitai Armon.
\newblock Tabular data: Deep learning is not all you need.
\newblock {\em Information Fusion}, 81:84--90, 2022.

\bibitem{smith1988using}
Jack~W Smith, James~E Everhart, WC~Dickson, William~C Knowler, and Robert~Scott Johannes.
\newblock Using the adap learning algorithm to forecast the onset of diabetes mellitus.
\newblock In {\em Proceedings of the annual symposium on computer application in medical care}, page 261. American Medical Informatics Association, 1988.

\bibitem{somepalli2021saint}
Gowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C~Bayan Bruss, and Tom Goldstein.
\newblock Saint: Improved neural networks for tabular data via row attention and contrastive pre-training.
\newblock {\em arXiv preprint arXiv:2106.01342}, 2021.

\bibitem{tsimpoukelli2021multimodal}
Maria Tsimpoukelli, Jacob~L Menick, Serkan Cabi, SM~Eslami, Oriol Vinyals, and Felix Hill.
\newblock Multimodal few-shot learning with frozen language models.
\newblock {\em Advances in Neural Information Processing Systems}, 34:200--212, 2021.

\bibitem{ulmer2020trust}
Dennis Ulmer, Lotta Meijerink, and Giovanni Cin{\`a}.
\newblock Trust issues: Uncertainty estimation does not enable reliable ood detection on medical tabular data.
\newblock In {\em Machine Learning for Health}, pages 341--354. PMLR, 2020.

\bibitem{urban2021deep}
Christopher~J Urban and Kathleen~M Gates.
\newblock Deep learning: A primer for psychologists.
\newblock {\em Psychological Methods}, 2021.

\bibitem{BLS2023}
{US Bureau of Labor Statistics}.
\newblock National longitudinal surveys of youth data set, 2023.

\bibitem{vanschoren2014openml}
Joaquin Vanschoren, Jan~N Van~Rijn, Bernd Bischl, and Luis Torgo.
\newblock Openml: networked science in machine learning.
\newblock {\em ACM SIGKDD Explorations Newsletter}, 2014.

\bibitem{vergara2014review}
Jorge~R Vergara and Pablo~A Est{\'e}vez.
\newblock A review of feature selection methods based on mutual information.
\newblock {\em Neural computing and applications}, 24:175--186, 2014.

\bibitem{verma2018fairness}
Sahil Verma and Julia Rubin.
\newblock Fairness definitions explained.
\newblock In {\em Proceedings of the international workshop on software fairness}, pages 1--7, 2018.

\bibitem{wallace2019universal}
Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh.
\newblock Universal adversarial triggers for attacking and analyzing nlp.
\newblock In {\em Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, 2019.

\bibitem{misc_breast_cancer_wisconsin_(diagnostic)_17}
William Wolberg, Olvi Mangasarian, Nick Street, and W.~Street.
\newblock {Breast Cancer Wisconsin (Diagnostic)}.
\newblock UCI Machine Learning Repository, 1995.
\newblock {DOI}: https://doi.org/10.24432/C5DW2B.

\bibitem{xie2022explanation}
Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{yamada2020feature}
Yutaro Yamada, Ofir Lindenbaum, Sahand Negahban, and Yuval Kluger.
\newblock Feature selection using stochastic gates.
\newblock In {\em Proceedings of the International Conference on Machine Learning (ICML)}, 2020.

\bibitem{yoon2020vime}
Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van~der Schaar.
\newblock Vime: Extending the success of self-and semi-supervised learning to tabular domain.
\newblock {\em Advances in Neural Information Processing Systems}, 33:11033--11043, 2020.

\bibitem{zhao2023survey}
Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et~al.
\newblock A survey of large language models.
\newblock {\em arXiv preprint arXiv:2303.18223}, 2023.

\bibitem{zheng2018fairness}
Yong Zheng, Tanaya Dave, Neha Mishra, and Harshit Kumar.
\newblock Fairness in reciprocal recommendations: A speed-dating study.
\newblock In {\em Adjunct publication of the 26th conference on user modeling, adaptation and personalization}, 2018.

\bibitem{zhong2021factual}
Zexuan Zhong, Dan Friedman, and Danqi Chen.
\newblock Factual probing is [mask]: Learning vs. learning to recall.
\newblock In {\em Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, 2021.

\bibitem{zhou2022conditional}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Conditional prompt learning for vision-language models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2022.

\bibitem{zhou2022learning}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Learning to prompt for vision-language models.
\newblock {\em International Journal of Computer Vision}, 2022.

\end{thebibliography}
