\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Barber et~al.(2021)Barber, Candes, Ramdas, and
  Tibshirani]{barber2021predictive}
Barber, R.~F., Candes, E.~J., Ramdas, A., and Tibshirani, R.~J.
\newblock Predictive inference with the jackknife+.
\newblock \emph{The Annals of Statistics}, 49\penalty0 (1):\penalty0 486--507,
  2021.

\bibitem[Berkenkamp et~al.(2017)Berkenkamp, Turchetta, Schoellig, and
  Krause]{Berkenkamp2017}
Berkenkamp, F., Turchetta, M., Schoellig, A., and Krause, A.
\newblock Safe model-based reinforcement learning with stability guarantees.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 30}, pp.\  908--918. Curran Associates, Inc.,
  2017.

\bibitem[Bishop(1994)]{bishop1994mixture}
Bishop, C.~M.
\newblock Mixture density networks.
\newblock 1994.

\bibitem[Chua et~al.(2018)Chua, Calandra, McAllister, and Levine]{chua2018}
Chua, K., Calandra, R., McAllister, R., and Levine, S.
\newblock Deep reinforcement learning in a handful of trials using
  probabilistic dynamics models.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31}, pp.\  4759--4770. Curran Associates,
  Inc., 2018.

\bibitem[Dawid(1984)]{dawid1984prequential}
Dawid, A.~P.
\newblock Present position and potential developments: Some personal views:
  Statistical theory: The prequential approach.
\newblock \emph{Journal of the Royal Statistical Society. Series A (General)},
  147:\penalty0 278--292, 1984.

\bibitem[Deisenroth \& Rasmussen(2011)Deisenroth and
  Rasmussen]{deisenroth2011pilco}
Deisenroth, M. and Rasmussen, C.~E.
\newblock Pilco: A model-based and data-efficient approach to policy search.
\newblock In \emph{Proceedings of the 28th International Conference on machine
  learning (ICML-11)}, pp.\  465--472, 2011.

\bibitem[Deshpande \& Kuleshov(2021)Deshpande and
  Kuleshov]{deshpande2021calibration}
Deshpande, S. and Kuleshov, V.
\newblock Calibration improves bayesian optimization.
\newblock \emph{arXiv preprint arXiv:2112.04620}, 2021.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{Gal2016Dropout}
Gal, Y. and Ghahramani, Z.
\newblock Dropout as a {B}ayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning (ICML-16)}, 2016.

\bibitem[Gal et~al.(2017)Gal, Hron, and Kendall]{gal2017concrete}
Gal, Y., Hron, J., and Kendall, A.
\newblock Concrete dropout.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3584--3593, 2017.

\bibitem[Gneiting \& Raftery(2005)Gneiting and Raftery]{gneiting2005weather}
Gneiting, T. and Raftery, A.~E.
\newblock Weather forecasting with ensemble methods.
\newblock \emph{Science}, 310\penalty0 (5746):\penalty0 248--249, 2005.

\bibitem[Gneiting \& Raftery(2007)Gneiting and Raftery]{gneiting2007strictly}
Gneiting, T. and Raftery, A.~E.
\newblock Strictly proper scoring rules, prediction, and estimation.
\newblock \emph{Journal of the American Statistical Association}, 102\penalty0
  (477):\penalty0 359--378, 2007.

\bibitem[Gneiting et~al.(2007)Gneiting, Balabdaoui, and
  Raftery]{gneiting2007probabilistic}
Gneiting, T., Balabdaoui, F., and Raftery, A.~E.
\newblock Probabilistic forecasts, calibration and sharpness.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 69\penalty0 (2):\penalty0 243--268, 2007.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
\newblock On calibration of modern neural networks.
\newblock \emph{arXiv preprint arXiv:1706.04599}, 2017.

\bibitem[Hersbach(2000)]{hersbach2000decomposition}
Hersbach, H.
\newblock Decomposition of the continuous ranked probability score for ensemble
  prediction systems.
\newblock \emph{Weather and Forecasting}, 15\penalty0 (5):\penalty0 559--570,
  2000.

\bibitem[Kuleshov \& Ermon(2017{\natexlab{a}})Kuleshov and
  Ermon]{kuleshov2017deep}
Kuleshov, V. and Ermon, S.
\newblock Deep hybrid models: bridging discriminative and generative
  approaches.
\newblock In \emph{Uncertainty in Artificial Intelligence}, 2017{\natexlab{a}}.

\bibitem[Kuleshov \& Ermon(2017{\natexlab{b}})Kuleshov and
  Ermon]{kuleshov2017estimating}
Kuleshov, V. and Ermon, S.
\newblock Estimating uncertainty online against an adversary.
\newblock In \emph{AAAI}, pp.\  2110--2116, 2017{\natexlab{b}}.

\bibitem[Kuleshov \& Liang(2015)Kuleshov and Liang]{kuleshov2015calibrated}
Kuleshov, V. and Liang, P.
\newblock Calibrated structured prediction.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2015.

\bibitem[Kuleshov et~al.(2018)Kuleshov, Fenner, and
  Ermon]{kuleshov2018accurate}
Kuleshov, V., Fenner, N., and Ermon, S.
\newblock Accurate uncertainties for deep learning using calibrated regression.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2796--2804. PMLR, 2018.

\bibitem[Kuleshov et~al.(2019)Kuleshov, Ding, Vo, Hancock, Ratner, Li, R{\'e},
  Batzoglou, and Snyder]{kuleshov2019machine}
Kuleshov, V., Ding, J., Vo, C., Hancock, B., Ratner, A., Li, Y., R{\'e}, C.,
  Batzoglou, S., and Snyder, M.
\newblock A machine-compiled database of genome-wide association studies.
\newblock \emph{Nature communications}, 10\penalty0 (1):\penalty0 1--8, 2019.

\bibitem[Kull \& Flach(2015)Kull and Flach]{kull2015novel}
Kull, M. and Flach, P.
\newblock Novel decompositions of proper scoring rules for classification:
  Score adjustment as precursor to calibration.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pp.\  68--85. Springer, 2015.

\bibitem[Kumar \& Sarawagi(2019)Kumar and Sarawagi]{kumar2019calibration}
Kumar, A. and Sarawagi, S.
\newblock Calibration of encoder decoder models for neural machine translation.
\newblock \emph{arXiv preprint arXiv:1903.00802}, 2019.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{lakshminarayanan2016simple}
Lakshminarayanan, B., Pritzel, A., and Blundell, C.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock \emph{arXiv preprint arXiv:1612.01474}, 2017.

\bibitem[MacKay(1992)]{mackay1992bayesian}
MacKay, D.~J.
\newblock Bayesian interpolation.
\newblock \emph{Neural computation}, 4\penalty0 (3):\penalty0 415--447, 1992.

\bibitem[Malik et~al.(2019)Malik, Kuleshov, Song, Nemer, Seymour, and
  Ermon]{malik2019calibrated}
Malik, A., Kuleshov, V., Song, J., Nemer, D., Seymour, H., and Ermon, S.
\newblock Calibrated model-based deep reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4314--4323. PMLR, 2019.

\bibitem[Murphy(1973)]{murphy1973vector}
Murphy, A.~H.
\newblock A new vector partition of the probability score.
\newblock \emph{Journal of Applied Meteorology}, 12\penalty0 (4):\penalty0
  595--600, 1973.

\bibitem[Nguyen \& O'Connor(2015)Nguyen and O'Connor]{nguyen2015posterior}
Nguyen, K. and O'Connor, B.
\newblock Posterior calibration and exploratory analysis for natural language
  processing models.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)},
  pp.\  1587--1598, 2015.

\bibitem[Niculescu-Mizil \& Caruana(2005)Niculescu-Mizil and
  Caruana]{niculescu2005predicting}
Niculescu-Mizil, A. and Caruana, R.
\newblock Predicting good probabilities with supervised learning.
\newblock In \emph{Proceedings of the 22nd international conference on Machine
  learning}, pp.\  625--632, 2005.

\bibitem[Platt(1999)]{platt1999probabilistic}
Platt, J.
\newblock Probabilistic outputs for support vector machines and comparisons to
  regularized likelihood methods.
\newblock \emph{Advances in Large Margin Classifiers}, 10\penalty0
  (3):\penalty0 61--74, 1999.

\bibitem[Raftery et~al.(2005)Raftery, Gneiting, Balabdaoui, and
  Polakowski]{raftery2005using}
Raftery, A.~E., Gneiting, T., Balabdaoui, F., and Polakowski, M.
\newblock Using bayesian model averaging to calibrate forecast ensembles.
\newblock \emph{Monthly weather review}, 133\penalty0 (5):\penalty0 1155--1174,
  2005.

\bibitem[Rajeswaran et~al.(2016)Rajeswaran, Ghotra, Ravindran, and
  Levine]{rajeswaran2016epopt}
Rajeswaran, A., Ghotra, S., Ravindran, B., and Levine, S.
\newblock Epopt: Learning robust neural network policies using model ensembles.
\newblock \emph{arXiv preprint arXiv:1610.01283}, 2016.

\bibitem[Ratner et~al.(2016)Ratner, De~Sa, Wu, Selsam, and
  R{\'e}]{ratner2016data}
Ratner, A.~J., De~Sa, C.~M., Wu, S., Selsam, D., and R{\'e}, C.
\newblock Data programming: Creating large training sets, quickly.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 3567--3575, 2016.

\bibitem[Ren et~al.(2018)Ren, Stewart, Song, Kuleshov, and
  Ermon]{ren2018learning}
Ren, H., Stewart, R., Song, J., Kuleshov, V., and Ermon, S.
\newblock Learning with weak supervision from physics and data-driven
  constraints.
\newblock \emph{AI Magazine}, 39\penalty0 (1):\penalty0 27--38, 2018.

\bibitem[Romano et~al.(2019)Romano, Patterson, and
  Candes]{romano2019conformalized}
Romano, Y., Patterson, E., and Candes, E.
\newblock Conformalized quantile regression.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Romano et~al.(2020)Romano, Sesia, and
  Candes]{romano2020classification}
Romano, Y., Sesia, M., and Candes, E.
\newblock Classification with valid and adaptive coverage.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3581--3591, 2020.

\bibitem[Shafer \& Vovk(2008)Shafer and Vovk]{shafer2008tutorial}
Shafer, G. and Vovk, V.
\newblock A tutorial on conformal prediction.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0 (3), 2008.

\bibitem[Si et~al.(2021)Si, Bishop, and Kuleshov]{si2021autoregressive}
Si, P., Bishop, A., and Kuleshov, V.
\newblock Autoregressive quantile flows for predictive uncertainty estimation,
  2021.

\bibitem[Song et~al.(2019)Song, Diethe, Kull, and Flach]{song2019distribution}
Song, H., Diethe, T., Kull, M., and Flach, P.
\newblock Distribution calibration for regression.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5897--5906. PMLR, 2019.

\bibitem[Vovk et~al.(2005)Vovk, Gammerman, and Shafer]{vovk2005algorithmic}
Vovk, V., Gammerman, A., and Shafer, G.
\newblock \emph{Algorithmic learning in a random world}.
\newblock Springer Science \& Business Media, 2005.

\bibitem[Wasserman(2006)]{wasserman2006all}
Wasserman, L.
\newblock \emph{All of nonparametric statistics}.
\newblock Springer Science \& Business Media, 2006.

\bibitem[Werling et~al.(2015)Werling, Chaganty, Liang, and
  Manning]{werling15adaptive}
Werling, K., Chaganty, A.~T., Liang, P.~S., and Manning, C.~D.
\newblock On-the-job learning with bayesian decision theory.
\newblock \emph{Neural Information Processing Systems}, 2015.

\bibitem[Zadrozny \& Elkan(2002)Zadrozny and Elkan]{zadrozny2002transforming}
Zadrozny, B. and Elkan, C.
\newblock Transforming classifier scores into accurate multiclass probability
  estimates.
\newblock In \emph{International Conference on Knowledge Discovery and Data
  Mining (KDD)}, pp.\  694--699, 2002.

\end{thebibliography}
