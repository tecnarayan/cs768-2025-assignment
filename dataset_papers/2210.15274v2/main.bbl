\begin{thebibliography}{10}

\bibitem{semckd}
Defang Chen, Jian-Ping Mei, Yuan Zhang, Can Wang, Zhe Wang, Yan Feng, and Chun
  Chen.
\newblock Cross-layer distillation with semantic calibration.
\newblock In {\em AAAI}, pages 7028--7036, 2021.

\bibitem{kr}
Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia.
\newblock Distilling knowledge via knowledge review.
\newblock In {\em CVPR}, pages 5008--5017, 2021.

\bibitem{da1}
Xinlei Chen and Kaiming He.
\newblock Exploring simple siamese representation learning.
\newblock In {\em CVPR}, pages 15750--15758, 2021.

\bibitem{zhi1}
Zhi Chen, Yadan Luo, Ruihong Qiu, Sen Wang, Zi~Huang, Jingjing Li, and Zheng
  Zhang.
\newblock Semantics disentangling for generalized zero-shot learning.
\newblock In {\em ICCV}, 2021.

\bibitem{zhi2}
Zhi Chen, Sen Wang, Jingjing Li, and Zi~Huang.
\newblock Rethinking generative zero-shot learning: An ensemble learning
  perspective for recognising visual patches.
\newblock In {\em ACM MM}, pages 3413--3421, 2020.

\bibitem{cid}
Xiang Deng and Zhongfei Zhang.
\newblock Comprehensive knowledge distillation with causal intervention.
\newblock In {\em NeurIPS}, 2021.

\bibitem{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{kdsurvey}
Jianping Gou, Baosheng Yu, Stephen~J Maybank, and Dacheng Tao.
\newblock Knowledge distillation: A survey.
\newblock {\em IJCV}, 129(6):1789--1819, 2021.

\bibitem{da2}
Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, Corentin Tallec, Pierre
  Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila~Pires, Zhaohan
  Guo, Mohammad Gheshlaghi~Azar, et~al.
\newblock Bootstrap your own latent-a new approach to self-supervised learning.
\newblock In {\em NeurIPS}, pages 21271--21284, 2020.

\bibitem{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, pages 770--778, 2016.

\bibitem{gelu}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock {\em arXiv preprint arXiv:1606.08415}, 2016.

\bibitem{ofd}
Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, and Jin~Young
  Choi.
\newblock A comprehensive overhaul of feature distillation.
\newblock In {\em ICCV}, pages 1921--1930, 2019.

\bibitem{kd}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{mobilenets}
Andrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock {\em arXiv preprint arXiv:1704.04861}, 2017.

\bibitem{densenet}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em CVPR}, pages 4700--4708, 2017.

\bibitem{afd}
Mingi Ji, Byeongho Heo, and Sungrae Park.
\newblock Show, attend and distill: Knowledge distillation via attention-based
  feature matching.
\newblock In {\em AAAI}, pages 7945--7952, 2021.

\bibitem{ft}
Jangho Kim, SeongUk Park, and Nojun Kwak.
\newblock Paraphrasing complex network: Network compression via factor
  transfer.
\newblock In {\em NeurIPS}, 2018.

\bibitem{at}
Nikos Komodakis and Sergey Zagoruyko.
\newblock Paying more attention to attention: improving the performance of
  convolutional neural networks via attention transfer.
\newblock In {\em ICLR}, 2017.

\bibitem{cifar100}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock {\em Technical report}, 2009.

\bibitem{alexnet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em NeurIPS}, pages 1097--1105, 2012.

\bibitem{convnext}
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,
  and Saining Xie.
\newblock A convnet for the 2020s.
\newblock {\em arXiv preprint arXiv:2201.03545}, 2022.

\bibitem{takd}
Seyed~Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa,
  and Hassan Ghasemzadeh.
\newblock Improved knowledge distillation via teacher assistant.
\newblock In {\em AAAI}, pages 5191--5198, 2020.

\bibitem{rkd}
Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho.
\newblock Relational knowledge distillation.
\newblock In {\em CVPR}, pages 3967--3976, 2019.

\bibitem{cc}
Baoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li, Yichao Wu, Yu~Liu, Shunfeng
  Zhou, and Zhaoning Zhang.
\newblock Correlation congruence for knowledge distillation.
\newblock In {\em ICCV}, pages 5007--5016, 2019.

\bibitem{fitnets}
Adriana Romero, Nicolas Ballas, Samira~Ebrahimi Kahou, Antoine Chassang, Carlo
  Gatta, and Yoshua Bengio.
\newblock Fitnets: Hints for thin deep nets.
\newblock {\em arXiv preprint arXiv:1412.6550}, 2014.

\bibitem{imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em IJCV}, 115(3):211--252, 2015.

\bibitem{mobilenetsv2}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In {\em CVPR}, pages 4510--4520, 2018.

\bibitem{vgg}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{densetakd}
Wonchul Son, Jaemin Na, Junyong Choi, and Wonjun Hwang.
\newblock Densely guided knowledge distillation using multiple teacher
  assistants.
\newblock In {\em ICCV}, pages 9395--9404, 2021.

\bibitem{inception}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
  Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
\newblock Going deeper with convolutions.
\newblock In {\em CVPR}, pages 1--9, 2015.

\bibitem{crd}
Yonglong Tian, Dilip Krishnan, and Phillip Isola.
\newblock Contrastive representation distillation.
\newblock In {\em ICLR}, 2020.

\bibitem{sp}
Frederick Tung and Greg Mori.
\newblock Similarity-preserving knowledge distillation.
\newblock In {\em ICCV}, pages 1365--1374, 2019.

\bibitem{ensemblepaper}
Xiaofang Wang, Dan Kondratyuk, Eric Christiansen, Kris~M Kitani, Yair Alon, and
  Elad Eban.
\newblock Wisdom of committees: An overlooked approach to faster and more
  accurate models.
\newblock {\em arXiv preprint arXiv:2012.01988}, 2020.

\bibitem{srrl}
Jing Yang, Brais Martinez, Adrian Bulat, and Georgios Tzimiropoulos.
\newblock Knowledge distillation via softmax regression representation
  learning.
\newblock In {\em ICLR}, 2021.

\bibitem{wrn}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock {\em arXiv preprint arXiv:1605.07146}, 2016.

\bibitem{dml}
Ying Zhang, Tao Xiang, Timothy~M Hospedales, and Huchuan Lu.
\newblock Deep mutual learning.
\newblock In {\em CVPR}, pages 4320--4328, 2018.

\bibitem{pad}
Youcai Zhang, Zhonghao Lan, Yuchen Dai, Fangao Zeng, Yan Bai, Jie Chang, and
  Yichen Wei.
\newblock Prime-aware adaptive distillation.
\newblock In {\em ECCV}, pages 658--674, 2020.

\bibitem{ensemblepaper2}
Zhi-Hua Zhou, Jianxin Wu, and Wei Tang.
\newblock Ensembling neural networks: many could be better than all.
\newblock {\em Artificial intelligence}, 137(1-2):239--263, 2002.

\end{thebibliography}
