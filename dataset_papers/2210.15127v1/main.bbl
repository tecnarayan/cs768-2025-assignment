\begin{thebibliography}{73}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Gu et~al.(2019)Gu, Dolan-Gavitt, and Garg]{gu2017badnets}
Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock Badnets: Identifying vulnerabilities in the machine learning model
  supply chain.
\newblock \emph{IEEE Access}, 2019.

\bibitem[Liu et~al.(2018{\natexlab{a}})Liu, Ma, Aafer, Lee, Zhai, Wang, and
  Zhang]{liu2017trojaning}
Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang,
  and Xiangyu Zhang.
\newblock Trojaning attack on neural networks.
\newblock \emph{Proceedings of the network and distributed system security
  symposium}, 2018{\natexlab{a}}.

\bibitem[Cheng et~al.(2021)Cheng, Liu, Ma, and Zhang]{cheng2020deep}
Siyuan Cheng, Yingqi Liu, Shiqing Ma, and Xiangyu Zhang.
\newblock Deep feature space trojan attack of neural networks by controlled
  detoxification.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 1148--1156, 2021.

\bibitem[Doan et~al.(2021)Doan, Lao, Zhao, and Li]{doan2021lira}
Khoa Doan, Yingjie Lao, Weijie Zhao, and Ping Li.
\newblock Lira: Learnable, imperceptible and robust backdoor attacks.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 11966--11976, 2021.

\bibitem[Salem et~al.(2022)Salem, Wen, Backes, Ma, and Zhang]{salem2022dynamic}
Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, and Yang Zhang.
\newblock Dynamic backdoor attacks against machine learning models.
\newblock In \emph{2022 IEEE 7th European Symposium on Security and Privacy
  (EuroS\&P)}, pages 703--718. IEEE, 2022.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Zhai, and Ma]{wang2022bppattack}
Zhenting Wang, Juan Zhai, and Shiqing Ma.
\newblock Bppattack: Stealthy and efficient trojan attacks against deep neural
  networks via image quantization and contrastive adversarial learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 15074--15084, 2022{\natexlab{a}}.

\bibitem[Nguyen and Tran(2021)]{nguyen2021wanet}
Tuan~Anh Nguyen and Anh~Tuan Tran.
\newblock Wanet-imperceptible warping-based backdoor attack.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Nguyen and Tran(2020)]{nguyen2020input}
Tuan~Anh Nguyen and Anh Tran.
\newblock Input-aware dynamic backdoor attack.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3454--3464, 2020.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Li, Wu, Li, He, and
  Lyu]{li2021invisible}
Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu.
\newblock Invisible backdoor attack with sample-specific triggers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 16463--16472, 2021{\natexlab{a}}.

\bibitem[Severi et~al.(2021)Severi, Meyer, Coull, and
  Oprea]{severi2021explanation}
Giorgio Severi, Jim Meyer, Scott Coull, and Alina Oprea.
\newblock $\{$Explanation-Guided$\}$ backdoor poisoning attacks against malware
  classifiers.
\newblock In \emph{30th USENIX Security Symposium (USENIX Security 21)}, pages
  1487--1504, 2021.

\bibitem[Sarkar et~al.(2020)Sarkar, Benkraouda, and
  Maniatakos]{sarkar2020facehack}
Esha Sarkar, Hadjer Benkraouda, and Michail Maniatakos.
\newblock Facehack: Triggering backdoored facial recognition systems using
  facial characteristics.
\newblock \emph{arXiv preprint arXiv:2006.11623}, 2020.

\bibitem[Du et~al.(2020)Du, Jia, and Song]{du2019robust}
Min Du, Ruoxi Jia, and Dawn Song.
\newblock Robust anomaly detection and backdoor attack detection via
  differential privacy.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Chen et~al.(2019{\natexlab{a}})Chen, Carvalho, Baracaldo, Ludwig,
  Edwards, Lee, Molloy, and Srivastava]{chen2018detecting}
Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin
  Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava.
\newblock Detecting backdoor attacks on deep neural networks by activation
  clustering.
\newblock In \emph{SafeAI@ AAAI}, 2019{\natexlab{a}}.

\bibitem[Tran et~al.(2018)Tran, Li, and Madry]{tran2018spectral}
Brandon Tran, Jerry Li, and Aleksander Madry.
\newblock Spectral signatures in backdoor attacks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Gao et~al.(2019)Gao, Xu, Wang, Chen, Ranasinghe, and
  Nepal]{gao2019strip}
Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith~C Ranasinghe, and
  Surya Nepal.
\newblock Strip: A defence against trojan attacks on deep neural networks.
\newblock In \emph{Proceedings of the 35th Annual Computer Security
  Applications Conference}, pages 113--125, 2019.

\bibitem[Chou et~al.(2020)Chou, Tramer, and Pellegrino]{chou2018sentinet}
Edward Chou, Florian Tramer, and Giancarlo Pellegrino.
\newblock Sentinet: Detecting localized universal attacks against deep learning
  systems.
\newblock In \emph{2020 IEEE Security and Privacy Workshops (SPW)}, pages
  48--54. IEEE, 2020.

\bibitem[Doan et~al.(2020)Doan, Abbasnejad, and Ranasinghe]{doan2020februus}
Bao~Gia Doan, Ehsan Abbasnejad, and Damith~C Ranasinghe.
\newblock Februus: Input purification defense against trojan attacks on deep
  neural network systems.
\newblock In \emph{Annual Computer Security Applications Conference}, pages
  897--912, 2020.

\bibitem[Ma et~al.(2019)Ma, Liu, Tao, Lee, and Zhang]{ma2019nic}
Shiqing Ma, Yingqi Liu, Guanhong Tao, Wen-Chuan Lee, and Xiangyu Zhang.
\newblock Nic: Detecting adversarial samples with neural network invariant
  checking.
\newblock \emph{Proceedings of the network and distributed system security
  symposium}, 2019.

\bibitem[Wang et~al.(2019)Wang, Yao, Shan, Li, Viswanath, Zheng, and
  Zhao]{wang2019neural}
Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao
  Zheng, and Ben~Y Zhao.
\newblock Neural cleanse: Identifying and mitigating backdoor attacks in neural
  networks.
\newblock In \emph{2019 IEEE Symposium on Security and Privacy (SP)}, pages
  707--723. IEEE, 2019.

\bibitem[Liu et~al.(2019)Liu, Lee, Tao, Ma, Aafer, and Zhang]{liu2019abs}
Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xiangyu
  Zhang.
\newblock Abs: Scanning neural networks for back-doors by artificial brain
  stimulation.
\newblock In \emph{Proceedings of the 2019 ACM SIGSAC Conference on Computer
  and Communications Security}, pages 1265--1282, 2019.

\bibitem[Shen et~al.(2021)Shen, Liu, Tao, An, Xu, Cheng, Ma, and
  Zhang]{shen2021backdoor}
Guangyu Shen, Yingqi Liu, Guanhong Tao, Shengwei An, Qiuling Xu, Siyuan Cheng,
  Shiqing Ma, and Xiangyu Zhang.
\newblock Backdoor scanning for deep neural networks through k-arm
  optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  9525--9536. PMLR, 2021.

\bibitem[Guo et~al.(2020)Guo, Wang, Xing, Du, and Song]{guo2019tabor}
Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, and Dawn Song.
\newblock Tabor: A highly accurate approach to inspecting and restoring trojan
  backdoors in ai systems.
\newblock \emph{IEEE International Conference on Data Mining (ICDM)}, 2020.

\bibitem[Chen et~al.(2019{\natexlab{b}})Chen, Fu, Zhao, and
  Koushanfar]{chen2019deepinspect}
Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar.
\newblock Deepinspect: A black-box trojan detection and mitigation framework
  for deep neural networks.
\newblock In \emph{IJCAI}, pages 4658--4664, 2019{\natexlab{b}}.

\bibitem[Chen et~al.(2017)Chen, Liu, Li, Lu, and Song]{chen2017targeted}
Xinyun Chen, Chang Liu, Bo~Li, Kimberly Lu, and Dawn Song.
\newblock Targeted backdoor attacks on deep learning systems using data
  poisoning.
\newblock \emph{arXiv preprint arXiv:1712.05526}, 2017.

\bibitem[Lin et~al.(2020)Lin, Xu, Liu, and Zhang]{lin2020composite}
Junyu Lin, Lei Xu, Yingqi Liu, and Xiangyu Zhang.
\newblock Composite backdoor attack for deep neural network by mixing existing
  benign features.
\newblock In \emph{Proceedings of the 2020 ACM SIGSAC Conference on Computer
  and Communications Security}, pages 113--131, 2020.

\bibitem[Turner et~al.(2019)Turner, Tsipras, and Madry]{turner2019label}
Alexander Turner, Dimitris Tsipras, and Aleksander Madry.
\newblock Label-consistent backdoor attacks.
\newblock \emph{arXiv preprint arXiv:1912.02771}, 2019.

\bibitem[Barni et~al.(2019)Barni, Kallas, and Tondi]{barni2019new}
Mauro Barni, Kassem Kallas, and Benedetta Tondi.
\newblock A new backdoor attack in cnns by training set corruption without
  label poisoning.
\newblock In \emph{2019 IEEE International Conference on Image Processing
  (ICIP)}, pages 101--105. IEEE, 2019.

\bibitem[Xi et~al.(2021)Xi, Pang, Ji, and Wang]{xi2021graph}
Zhaohan Xi, Ren Pang, Shouling Ji, and Ting Wang.
\newblock Graph backdoor.
\newblock In \emph{30th USENIX Security Symposium (USENIX Security 21)}, pages
  1523--1540, 2021.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Jia, Wang, and
  Gong]{zhang2021backdoor}
Zaixi Zhang, Jinyuan Jia, Binghui Wang, and Neil~Zhenqiang Gong.
\newblock Backdoor attacks to graph neural networks.
\newblock In \emph{Proceedings of the 26th ACM Symposium on Access Control
  Models and Technologies}, pages 15--26, 2021{\natexlab{a}}.

\bibitem[Kiourti et~al.(2020)Kiourti, Wardega, Jha, and Li]{kiourti2020trojdrl}
Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li.
\newblock Trojdrl: evaluation of backdoor attacks on deep reinforcement
  learning.
\newblock In \emph{2020 57th ACM/IEEE Design Automation Conference (DAC)},
  pages 1--6. IEEE, 2020.

\bibitem[Wang et~al.(2021)Wang, Javed, Wu, Guo, Xing, and
  Song]{wang2021backdoorl}
Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, and Dawn Song.
\newblock Backdoorl: Backdoor attack against competitive reinforcement
  learning.
\newblock In \emph{30th International Joint Conference on Artificial
  Intelligence, IJCAI 2021}, pages 3699--3705. International Joint Conferences
  on Artificial Intelligence, 2021.

\bibitem[Chen et~al.(2021)Chen, Salem, Chen, Backes, Ma, Shen, Wu, and
  Zhang]{chen2021badnl}
Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni
  Shen, Zhonghai Wu, and Yang Zhang.
\newblock Badnl: Backdoor attacks against nlp models with semantic-preserving
  improvements.
\newblock In \emph{Annual Computer Security Applications Conference}, pages
  554--569, 2021.

\bibitem[Chan et~al.(2020)Chan, Tay, Ong, and Zhang]{chan2020poison}
Alvin Chan, Yi~Tay, Yew-Soon Ong, and Aston Zhang.
\newblock Poison attacks against text datasets with conditional adversarially
  regularized autoencoder.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pages 4175--4189, 2020.

\bibitem[Yang et~al.(2021{\natexlab{a}})Yang, Li, Zhang, Ren, Sun, and
  He]{yang2021careful}
Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu~Sun, and Bin He.
\newblock Be careful about poisoned word embeddings: Exploring the
  vulnerability of the embedding layers in nlp models.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 2048--2058, 2021{\natexlab{a}}.

\bibitem[Qi et~al.(2021{\natexlab{a}})Qi, Chen, Zhang, Li, Liu, and
  Sun]{qi2021mind}
Fanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li, Zhiyuan Liu, and Maosong Sun.
\newblock Mind the style of text! adversarial and backdoor attacks based on
  text style transfer.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 4569--4580, 2021{\natexlab{a}}.

\bibitem[Yang et~al.(2021{\natexlab{b}})Yang, Lin, Li, Zhou, and
  Sun]{yang2021rethinking}
Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu~Sun.
\newblock Rethinking stealthiness of backdoor attack against nlp models.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 5543--5557,
  2021{\natexlab{b}}.

\bibitem[Qi et~al.(2021{\natexlab{b}})Qi, Li, Chen, Zhang, Liu, Wang, and
  Sun]{qi2021hidden}
Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang,
  and Maosong Sun.
\newblock Hidden killer: Invisible textual backdoor attacks with syntactic
  trigger.
\newblock In \emph{ACL/IJCNLP (1)}, 2021{\natexlab{b}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Yin, Chen, Huang, Nguyen, and
  Cui]{zhang2021pipattack}
Shijie Zhang, Hongzhi Yin, Tong Chen, Zi~Huang, Quoc Viet~Hung Nguyen, and
  Lizhen Cui.
\newblock Pipattack: Poisoning federated recommender systems formanipulating
  item promotion.
\newblock \emph{arXiv preprint arXiv:2110.10926}, 2021{\natexlab{b}}.

\bibitem[Yao et~al.(2019)Yao, Li, Zheng, and Zhao]{yao2019latent}
Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben~Y Zhao.
\newblock Latent backdoor attacks on deep neural networks.
\newblock In \emph{Proceedings of the 2019 ACM SIGSAC Conference on Computer
  and Communications Security}, pages 2041--2055, 2019.

\bibitem[Jia et~al.(2022)Jia, Liu, and Gong]{jia2021badencoder}
Jinyuan Jia, Yupei Liu, and Neil~Zhenqiang Gong.
\newblock Badencoder: Backdoor attacks to pre-trained encoders in
  self-supervised learning.
\newblock In \emph{2022 IEEE Symposium on Security and Privacy (SP)}, pages
  2043--2059. IEEE, 2022.

\bibitem[Vicarte et~al.(2021)Vicarte, Wang, and Fletcher]{vicarte2021double}
Jose Rodrigo~Sanchez Vicarte, Gang Wang, and Christopher~W Fletcher.
\newblock $\{$Double-Cross$\}$ attacks: Subverting active learning systems.
\newblock In \emph{30th USENIX Security Symposium (USENIX Security 21)}, pages
  1593--1610, 2021.

\bibitem[Bagdasaryan et~al.(2020)Bagdasaryan, Veit, Hua, Estrin, and
  Shmatikov]{bagdasaryan2020backdoor}
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
  Shmatikov.
\newblock How to backdoor federated learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2938--2948. PMLR, 2020.

\bibitem[Xie et~al.(2019)Xie, Huang, Chen, and Li]{xie2019dba}
Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo~Li.
\newblock Dba: Distributed backdoor attacks against federated learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Hayase et~al.(2021)Hayase, Kong, Somani, and Oh]{hayase2021defense}
Jonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh.
\newblock Defense against backdoor attacks via robust covariance estimation.
\newblock In \emph{International Conference on Machine Learning}, pages
  4129--4139. PMLR, 2021.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Lyu, Koren, Lyu, Li, and
  Ma]{li2021anti}
Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo~Li, and Xingjun Ma.
\newblock Anti-backdoor learning: Training clean models on poisoned data.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{b}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Ding, Zhai, and
  Ma]{wang2022towards}
Zhenting Wang, Hailun Ding, Juan Zhai, and Shiqing Ma.
\newblock Towards understanding and defending input space trojans.
\newblock \emph{arXiv preprint arXiv:2202.06382}, 2022{\natexlab{b}}.

\bibitem[Huang et~al.(2022)Huang, Li, Wu, Qin, and Ren]{huang2022backdoor}
Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren.
\newblock Backdoor defense via decoupling the training process.
\newblock \emph{International Conference on Learning Representations}, 2022.

\bibitem[Tao et~al.(2022)Tao, Shen, Liu, An, Xu, Ma, Li, and
  Zhang]{tao2022better}
Guanhong Tao, Guangyu Shen, Yingqi Liu, Shengwei An, Qiuling Xu, Shiqing Ma,
  Pan Li, and Xiangyu Zhang.
\newblock Better trigger inversion optimization in backdoor scanning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 13368--13378, 2022.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Shen, Tao, An, Ma, and
  Zhang]{liu2022piccolo}
Yingqi Liu, Guangyu Shen, Guanhong Tao, Shengwei An, Shiqing Ma, and Xiangyu
  Zhang.
\newblock Piccolo: Exposing complex backdoors in nlp transformer models.
\newblock In \emph{2022 IEEE Symposium on Security and Privacy (SP)}, pages
  1561--1561. IEEE Computer Society, 2022{\natexlab{a}}.

\bibitem[Shen et~al.(2022)Shen, Liu, Tao, Xu, Zhang, An, Ma, and
  Zhang]{shen2022constrained}
Guangyu Shen, Yingqi Liu, Guanhong Tao, Qiuling Xu, Zhuo Zhang, Shengwei An,
  Shiqing Ma, and Xiangyu Zhang.
\newblock Constrained optimization with dynamic bound-scaling for effective
  nlpbackdoor defense.
\newblock 2022.

\bibitem[Xu et~al.(2021)Xu, Wang, Li, Borisov, Gunter, and Li]{xu2019detecting}
Xiaojun Xu, Qi~Wang, Huichen Li, Nikita Borisov, Carl~A Gunter, and Bo~Li.
\newblock Detecting ai trojans using meta neural analysis.
\newblock In \emph{2021 IEEE Symposium on Security and Privacy (SP)}, pages
  103--120. IEEE, 2021.

\bibitem[Lundberg and Lee(2017)]{lundberg2017unified}
Scott~M Lundberg and Su-In Lee.
\newblock A unified approach to interpreting model predictions.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Hornik et~al.(1989)Hornik, Stinchcombe, and
  White]{hornik1989multilayer}
Kurt Hornik, Maxwell Stinchcombe, and Halbert White.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural networks}, 2\penalty0 (5):\penalty0 359--366, 1989.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{ronneberger2015u}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{International Conference on Medical image computing and
  computer-assisted intervention}, pages 234--241. Springer, 2015.

\bibitem[Zhao et~al.(2021)Zhao, Zhu, Chen, and Zhang]{zhao2021ai}
Yue Zhao, Hong Zhu, Kai Chen, and Shengzhi Zhang.
\newblock Ai-lancet: Locating error-inducing neurons to optimize neural
  networks.
\newblock In \emph{Proceedings of the 2021 ACM SIGSAC Conference on Computer
  and Communications Security}, pages 141--158, 2021.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Stallkamp et~al.(2012)Stallkamp, Schlipsing, Salmen, and
  Igel]{stallkamp2012man}
Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel.
\newblock Man vs. computer: Benchmarking machine learning algorithms for
  traffic sign recognition.
\newblock \emph{Neural networks}, 32:\penalty0 323--332, 2012.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[He et~al.(2016{\natexlab{a}})He, Zhang, Ren, and Sun]{he2016identity}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{European conference on computer vision}, pages 630--645.
  Springer, 2016{\natexlab{a}}.

\bibitem[He et~al.(2016{\natexlab{b}})He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016{\natexlab{b}}.

\bibitem[Kolouri et~al.(2020)Kolouri, Saha, Pirsiavash, and
  Hoffmann]{kolouri2020universal}
Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann.
\newblock Universal litmus patterns: Revealing backdoor attacks in cnns.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 301--310, 2020.

\bibitem[Liu et~al.(2018{\natexlab{b}})Liu, Dolan-Gavitt, and
  Garg]{liu2018fine}
Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock Fine-pruning: Defending against backdooring attacks on deep neural
  networks.
\newblock In \emph{International Symposium on Research in Attacks, Intrusions,
  and Defenses}, pages 273--294. Springer, 2018{\natexlab{b}}.

\bibitem[Veldanda et~al.(2020)Veldanda, Liu, Tan, Krishnamurthy, Khorrami,
  Karri, Dolan-Gavitt, and Garg]{veldanda2020nnoculation}
Akshaj~Kumar Veldanda, Kang Liu, Benjamin Tan, Prashanth Krishnamurthy, Farshad
  Khorrami, Ramesh Karri, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock Nnoculation: broad spectrum and targeted treatment of backdoored
  dnns.
\newblock \emph{arXiv preprint arXiv:2002.08313}, 2020.

\bibitem[Li et~al.(2021{\natexlab{c}})Li, Lyu, Koren, Lyu, Li, and
  Ma]{li2021neural}
Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo~Li, and Xingjun Ma.
\newblock Neural attention distillation: Erasing backdoor triggers from deep
  neural networks.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{c}}.

\bibitem[Zeng et~al.(2022)Zeng, Chen, Park, Mao, Jin, and
  Jia]{zeng2021adversarial}
Yi~Zeng, Si~Chen, Won Park, Zhuoqing Mao, Ming Jin, and Ruoxi Jia.
\newblock Adversarial unlearning of backdoors via implicit hypergradient.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Zhou et~al.(2018)Zhou, Sun, Bau, and Torralba]{zhou2018interpretable}
Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba.
\newblock Interpretable basis decomposition for visual explanation.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 119--134, 2018.

\bibitem[Bau et~al.(2020)Bau, Zhu, Strobelt, Lapedriza, Zhou, and
  Torralba]{bau2020understanding}
David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, and
  Antonio Torralba.
\newblock Understanding the role of individual units in a deep neural network.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (48):\penalty0 30071--30078, 2020.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Shen, Tao, Wang, Ma, and
  Zhang]{liu2022complex}
Yingqi Liu, Guangyu Shen, Guanhong Tao, Zhenting Wang, Shiqing Ma, and Xiangyu
  Zhang.
\newblock Complex backdoor detection by symmetric feature differencing.
\newblock 2022{\natexlab{b}}.

\bibitem[Moosavi-Dezfooli et~al.(2017)Moosavi-Dezfooli, Fawzi, Fawzi, and
  Frossard]{moosavi2017universal}
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal
  Frossard.
\newblock Universal adversarial perturbations.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1765--1773, 2017.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock In \emph{British Machine Vision Conference 2016}. British Machine
  Vision Association, 2016.

\end{thebibliography}
