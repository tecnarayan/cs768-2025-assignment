\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{gpt3}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Chaudhry et~al.(2019)Chaudhry, Rohrbach, Elhoseiny, Ajanthan, Dokania,
  Torr, and Ranzato]{chaudhry2019tiny}
Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T., Dokania, P.~K., Torr,
  P.~H., and Ranzato, M.
\newblock On tiny episodic memories in continual learning.
\newblock \emph{arXiv preprint arXiv:1902.10486}, 2019.

\bibitem[Chrabaszcz et~al.(2017)Chrabaszcz, Loshchilov, and
  Hutter]{downsampled_imagenet}
Chrabaszcz, P., Loshchilov, I., and Hutter, F.
\newblock A downsampled variant of imagenet as an alternative to the cifar
  datasets.
\newblock \emph{arXiv preprint arXiv:1707.08819}, 2017.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{imagenet_dataset}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In \emph{CVPR09}, 2009.

\bibitem[Fort et~al.(2020)Fort, Dziugaite, Paul, Kharaghani, Roy, and
  Ganguli]{kernel_velocity}
Fort, S., Dziugaite, G.~K., Paul, M., Kharaghani, S., Roy, D.~M., and Ganguli,
  S.
\newblock Deep learning versus kernel learning: an empirical study of loss
  landscape geometry and the time evolution of the neural tangent kernel.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Gidaris \& Komodakis(2018)Gidaris and Komodakis]{convnet}
Gidaris, S. and Komodakis, N.
\newblock Dynamic few-shot visual learning without forgetting.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  4367--4375, 2018.

\bibitem[Goodfellow et~al.(2013)Goodfellow, Mirza, Xiao, Courville, and
  Bengio]{catastrophic_forgetting}
Goodfellow, I.~J., Mirza, M., Xiao, D., Courville, A., and Bengio, Y.
\newblock An empirical investigation of catastrophic forgetting in
  gradient-based neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6211}, 2013.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hern{\'a}ndez-Garc{\'\i}a \& K{\"o}nig(2018)Hern{\'a}ndez-Garc{\'\i}a
  and K{\"o}nig]{da_reg}
Hern{\'a}ndez-Garc{\'\i}a, A. and K{\"o}nig, P.
\newblock Data augmentation instead of explicit regularization.
\newblock \emph{arXiv preprint arXiv:1806.03852}, 2018.

\bibitem[Jastrzebski et~al.(2020)Jastrzebski, Szymczak, Fort, Arpit, Tabor,
  Cho, and Geras]{break-even_point}
Jastrzebski, S., Szymczak, M., Fort, S., Arpit, D., Tabor, J., Cho, K., and
  Geras, K.
\newblock The break-even point on optimization trajectories of deep neural
  networks.
\newblock \emph{arXiv preprint arXiv:2002.09572}, 2020.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and
  Duerig]{align_google}
Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q.~V., Sung,
  Y., Li, Z., and Duerig, T.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock \emph{arXiv preprint arXiv:2102.05918}, 2021.

\bibitem[Jiang et~al.(2021)Jiang, Zhang, Talwar, and Mozer]{cscores}
Jiang, Z., Zhang, C., Talwar, K., and Mozer, M.~C.
\newblock Characterizing structural regularities of labeled data in
  overparameterized models.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  5034--5044. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/jiang21k.html}.

\bibitem[Kim et~al.(2018)Kim, Kim, Seo, Kim, Park, Park, Jo, Kim, Yang, Kim,
  et~al.]{nsml2}
Kim, H., Kim, M., Seo, D., Kim, J., Park, H., Park, S., Jo, H., Kim, K., Yang,
  Y., Kim, Y., et~al.
\newblock Nsml: Meet the mlaas platform with a real-world case study.
\newblock \emph{arXiv preprint arXiv:1810.09957}, 2018.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{cifar_dataset}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{alexnet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Advances in neural information processing systems},
  25:\penalty0 1097--1105, 2012.

\bibitem[LeCun(1998)]{mnist_dataset}
LeCun, Y.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lenet}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lee et~al.(2021)Lee, Park, Lee, Yi, Lee, and Yoon]{oat}
Lee, S., Park, C., Lee, H., Yi, J., Lee, J., and Yoon, S.
\newblock Removing undesirable feature contributions using out-of-distribution
  data.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=eIHYL6fpbkA}.

\bibitem[Liu et~al.(2020)Liu, Jiang, Bai, Chen, and Wang]{gsnr}
Liu, J., Jiang, G., Bai, Y., Chen, T., and Wang, H.
\newblock Understanding why neural networks generalize well through gsnr of
  parameters.
\newblock \emph{arXiv preprint arXiv:2001.07384}, 2020.

\bibitem[Mahajan et~al.(2018)Mahajan, Girshick, Ramanathan, He, Paluri, Li,
  Bharambe, and Van Der~Maaten]{instagramnet}
Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri, M., Li, Y.,
  Bharambe, A., and Van Der~Maaten, L.
\newblock Exploring the limits of weakly supervised pretraining.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, pp.\  181--196, 2018.

\bibitem[Miller(1998)]{wordnet}
Miller, G.~A.
\newblock \emph{WordNet: An electronic lexical database}.
\newblock MIT press, 1998.

\bibitem[Mirzasoleiman et~al.(2020)Mirzasoleiman, Bilmes, and
  Leskovec]{coresets_for_efficient}
Mirzasoleiman, B., Bilmes, J., and Leskovec, J.
\newblock Coresets for data-efficient training of machine learning models.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  6950--6960. PMLR,
  13--18 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/mirzasoleiman20a.html}.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{svhn_dataset}
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A.~Y.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Nguyen et~al.(2020)Nguyen, Chen, and Lee]{kip}
Nguyen, T., Chen, Z., and Lee, J.
\newblock Dataset meta-learning from kernel ridge-regression.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Nguyen et~al.(2021)Nguyen, Novak, Xiao, and Lee]{dd_infinite}
Nguyen, T., Novak, R., Xiao, L., and Lee, J.
\newblock Dataset distillation with infinitely wide convolutional networks.
\newblock In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W.
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=hXWPpJedrVP}.

\bibitem[Paul et~al.(2021)Paul, Ganguli, and Dziugaite]{diet}
Paul, M., Ganguli, S., and Dziugaite, G.~K.
\newblock Deep learning on a data diet: Finding important examples early in
  training.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Rebuffi et~al.(2017)Rebuffi, Kolesnikov, Sperl, and
  Lampert]{rebuffi2017icarl}
Rebuffi, S.-A., Kolesnikov, A., Sperl, G., and Lampert, C.~H.
\newblock icarl: Incremental classifier and representation learning.
\newblock In \emph{Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pp.\  2001--2010, 2017.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{vgg}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Stallkamp et~al.(2011)Stallkamp, Schlipsing, Salmen, and
  Igel]{stallkamp2011german}
Stallkamp, J., Schlipsing, M., Salmen, J., and Igel, C.
\newblock The german traffic sign recognition benchmark: a multi-class
  classification competition.
\newblock In \emph{The 2011 international joint conference on neural networks},
  pp.\  1453--1460. IEEE, 2011.

\bibitem[Sung et~al.(2017)Sung, Kim, Jo, Yang, Kim, Lausen, Kim, Lee, Kwak, Ha,
  et~al.]{nsml1}
Sung, N., Kim, M., Jo, H., Yang, Y., Kim, J., Lausen, L., Kim, Y., Lee, G.,
  Kwak, D., Ha, J.-W., et~al.
\newblock Nsml: A machine learning platform that enables you to focus on your
  models.
\newblock \emph{arXiv preprint arXiv:1712.05902}, 2017.

\bibitem[Wang \& Isola(2020)Wang and Isola]{wang2020uniformity}
Wang, T. and Isola, P.
\newblock Understanding contrastive representation learning through alignment
  and uniformity on the hypersphere.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9929--9939. PMLR, 2020.

\bibitem[Wang et~al.(2018)Wang, Zhu, Torralba, and Efros]{dd}
Wang, T., Zhu, J.-Y., Torralba, A., and Efros, A.~A.
\newblock Dataset distillation.
\newblock \emph{arXiv preprint arXiv:1811.10959}, 2018.

\bibitem[Yun et~al.(2019)Yun, Han, Oh, Chun, Choe, and Yoo]{cutmix}
Yun, S., Han, D., Oh, S.~J., Chun, S., Choe, J., and Yoo, Y.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  6023--6032, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and Lopez-Paz]{mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017.

\bibitem[Zhao \& Bilen(2021)Zhao and Bilen]{dsa}
Zhao, B. and Bilen, H.
\newblock Dataset condensation with differentiable siamese augmentation.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Zhao et~al.(2021)Zhao, Mopuri, and Bilen]{dc}
Zhao, B., Mopuri, K.~R., and Bilen, H.
\newblock Dataset condensation with gradient matching.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=mSAKhLYLSsl}.

\end{thebibliography}
