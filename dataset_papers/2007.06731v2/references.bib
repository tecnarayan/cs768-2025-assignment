@inproceedings{nested-dropout,
  title={Learning ordered representations with nested dropout},
  author={Rippel, Oren and Gelbart, Michael and Adams, Ryan},
  booktitle={International Conference on Machine Learning},
  pages={1746--1754},
  year={2014}
}

@article{dropout,
  title={Dropout: {A} simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@article{ae-loss-landscape,
  title={Loss landscapes of regularized linear autoencoders},
  author={Kunin, Daniel and Bloom, Jonathan M and Goeva, Aleksandrina and Seed, Cotton},
  journal={arXiv preprint arXiv:1901.08168},
  year={2019}
}

@article{iter-avg-as-regularization,
  title={Iterate averaging as regularization for stochastic gradient descent},
  author={Neu, Gergely and Rosasco, Lorenzo},
  journal={arXiv preprint arXiv:1802.08009},
  year={2018}
}

@article{saxe2018,
	author={Andrew M. Saxe and James L. McClelland and Surya Ganguli},
	year={2019},
	month={Jun 4,},
	title={A mathematical theory of semantic development in deep neural networks},
	journal={Proceedings of the National Academy of Sciences of the United States of America},
	volume={116},
	number={23},
	pages={11537-11546},
	isbn={0027-8424},
	language={English},
	url={https://www.ncbi.nlm.nih.gov/pubmed/31101713},
	doi={10.1073/pnas.1820226116},
	pmid={31101713}
}

@article{oja1982simplified,
  title={Simplified neuron model as a principal component analyzer},
  author={Oja, Erkki},
  journal={Journal of mathematical biology},
  volume={15},
  number={3},
  pages={267--273},
  year={1982},
  publisher={Springer}
}

@article{generalized-hebbian-alg,
	author={Terence D. Sanger},
	year={1989},
	title={Optimal unsupervised learning in a single-layer linear feedforward neural network},
	journal={Neural Networks},
	volume={2},
	number={6},
	pages={459-473},
	isbn={0893-6080},
	language={English},
	url={https://www.sciencedirect.com/science/article/pii/0893608089900440},
	doi={10.1016/0893-6080(89)90044-0}
}
@inproceedings{dont-blame-elbo,
  title={Don't Blame the {ELBO}! {A} Linear VAE Perspective on Posterior Collapse},
  author={Lucas, James and Tucker, George and Grosse, Roger B and Norouzi, Mohammad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9403--9413},
  year={2019}
}

@article{baldi1989neural,
  title={Neural networks and principal component analysis: {L}earning from examples without local minima},
  author={Baldi, Pierre and Hornik, Kurt},
  journal={Neural networks},
  volume={2},
  number={1},
  pages={53--58},
  year={1989},
  publisher={Elsevier}
}

@article{gidel2019implicit,
  title={Implicit regularization of discrete gradient dynamics in deep linear neural networks},
  author={Gidel, Gauthier and Bach, Francis and Lacoste-Julien, Simon},
  journal={arXiv preprint arXiv:1904.13262},
  year={2019}
}

@inproceedings{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7411--7422},
  year={2019}
}

@inproceedings{imagenet,
        AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        TITLE = {{Image{N}et: {A} Large-Scale Hierarchical Image Database}},
        BOOKTITLE = {CVPR09},
        YEAR = {2009},
        BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"}
        
@inproceedings{ntk,
  title={Neural {T}angent {K}ernel: {C}onvergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}

@article{mnist,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{tipping1999probabilistic,
  title={Probabilistic principal component analysis},
  author={Tipping, Michael E and Bishop, Christopher M},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={61},
  number={3},
  pages={611--622},
  year={1999},
  publisher={Wiley Online Library}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{grossberg1987competitive,
  title={Competitive learning: {F}rom interactive activation to adaptive resonance},
  author={Grossberg, Stephen},
  journal={Cognitive science},
  volume={11},
  number={1},
  pages={23--63},
  year={1987},
  publisher={Elsevier}
}

@article{crick1989recent,
  title={The recent excitement about neural networks},
  author={Crick, Francis},
  journal={Nature},
  volume={337},
  number={6203},
  pages={129--132},
  year={1989},
  publisher={Springer}
}

@inproceedings{tang2019exponentially,
  title={Exponentially convergent stochastic k-{PCA} without variance reduction},
  author={Tang, Cheng},
  booktitle={Advances in Neural Information Processing Systems},
  pages={12393--12404},
  year={2019}
}

@article{bamler2018improving,
  title={Improving optimization for models with continuous symmetry breaking},
  author={Bamler, Robert and Mandt, Stephan},
  journal={arXiv preprint arXiv:1803.03234},
  year={2018}
}

@article{pearson1901liii,
  title={LIII. On lines and planes of closest fit to systems of points in space},
  author={Pearson, Karl},
  journal={The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume={2},
  number={11},
  pages={559--572},
  year={1901},
  publisher={Taylor \& Francis}
}

@article{kingma2014adam,
  title={Adam: {A} method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{kumar2020implicit,
  title={On Implicit Regularization in $\beta$-{VAE}s},
  author={Kumar, Abhishek and Poole, Ben},
  journal={arXiv preprint arXiv:2002.00041},
  year={2020}
}

@article{pfau2018spectral,
  title={Spectral inference networks: {U}nifying deep and spectral learning},
  author={Pfau, David and Petersen, Stig and Agarwal, Ashish and Barrett, David GT and Stachenfeld, Kimberly L},
  journal={arXiv preprint arXiv:1806.02215},
  year={2018}
}

@book{bellman1997introduction,
  title={Introduction to matrix analysis},
  author={Bellman, Richard},
  volume={19},
  year={1997},
  publisher={Siam}
}

@inproceedings{zhang2016global,
  title={Global convergence of a {G}rassmannian gradient descent algorithm for subspace estimation.},
  author={Zhang, Dejiao and Balzano, Laura},
  booktitle={AISTATS},
  pages={1460--1468},
  year={2016}
}

@article{higgins2017beta,
  title={beta-{VAE}: {L}earning Basic Visual Concepts with a Constrained Variational Framework.},
  author={Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  journal={Iclr},
  volume={2},
  number={5},
  pages={6},
  year={2017}
}

@InProceedings{pmlr-v40-Ge15,
  title = 	 {Escaping From Saddle Points --- Online Stochastic Gradient for Tensor Decomposition},
  author = 	 {Rong Ge and Furong Huang and Chi Jin and Yang Yuan},
  booktitle = 	 {Proceedings of The 28th Conference on Learning Theory},
  pages = 	 {797--842},
  year = 	 {2015},
  editor = 	 {Peter Gr√ºnwald and Elad Hazan and Satyen Kale},
  volume = 	 {40},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Paris, France},
  month = 	 {03--06 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v40/Ge15.pdf},
  url = 	 {http://proceedings.mlr.press/v40/Ge15.html},
  abstract = 	 {We analyze stochastic gradient descent for optimizing non-convex functions. In many cases for non-convex functions the goal is to find a reasonable local minimum, and the main concern is that gradient updates are trapped in \em saddle points. In this paper we identify \em strict saddle property for non-convex problem that allows for efficient optimization. Using this property we show that from an \em arbitrary starting point,  stochastic gradient descent converges to a local minimum in a polynomial number of iterations. To the best of our knowledge this is the first work that gives \em global convergence guarantees for stochastic gradient descent on non-convex functions with exponentially many local minima and saddle points. Our analysis can be applied to orthogonal tensor decomposition, which is widely used in learning a rich class of  latent variable models. We propose a new optimization formulation for the tensor decomposition problem that has strict saddle property. As a result we get the first online algorithm for orthogonal tensor decomposition with global convergence guarantee.}
}

@InProceedings{pmlr-v70-jin17a,
  title = 	 {How to Escape Saddle Points Efficiently},
  author = 	 {Chi Jin and Rong Ge and Praneeth Netrapalli and Sham M. Kakade and Michael I. Jordan},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1724--1732},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/jin17a/jin17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/jin17a.html},
  abstract = 	 {This paper shows that a perturbed form of gradient descent converges to a second-order stationary point in a number iterations which depends only poly-logarithmically on dimension (i.e., it is almost ‚Äúdimension-free‚Äù). The convergence rate of this procedure matches the well-known convergence rate of gradient descent to first-order stationary points, up to log factors. When all saddle points are non-degenerate, all second-order stationary points are local minima, and our result thus shows that perturbed gradient descent can escape saddle points almost for free. Our results can be directly applied to many machine learning applications, including deep learning. As a particular concrete example of such an application, we show that our results can be used directly to establish sharp global convergence rates for matrix factorization. Our results rely on a novel characterization of the geometry around saddle points, which may be of independent interest to the non-convex optimization community.}
}

@book{khalil2002nonlinear,
  title={Nonlinear systems},
  author={Khalil, Hassan K and Grizzle, Jessy W},
  volume={3},
  year={2002},
  publisher={Prentice hall Upper Saddle River, NJ}
}

@book{absil2009optimization,
  title={Optimization algorithms on matrix manifolds},
  author={Absil, P-A and Mahony, Robert and Sepulchre, Rodolphe},
  year={2009},
  publisher={Princeton University Press}
}

@article{dai2018connections,
  title={Connections with robust PCA and the role of emergent sparsity in variational autoencoder models},
  author={Dai, Bin and Wang, Yu and Aston, John and Hua, Gang and Wipf, David},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={1573--1614},
  year={2018},
  publisher={JMLR. org}
}

@inproceedings{rolinek2019variational,
  title={Variational autoencoders pursue pca directions (by accident)},
  author={Rolinek, Michal and Zietlow, Dominik and Martius, Georg},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={12406--12415},
  year={2019}
}

@inproceedings{oftadeheliminating,
  title={Eliminating the Invariance on the Loss Landscape of Linear Autoencoders},
  author={Oftadeh, Reza and Shen, Jiayi and Wang, Atlas and Shell, Dylan},
  booktitle={Proceedings of the 37th International Conference on Machine Learning},
  pages={5726--5734},
  year={2020}
}

@article{gemp2020eigengame,
  title={EigenGame: PCA as a Nash Equilibrium},
  author={Gemp, Ian and McWilliams, Brian and Vernade, Claire and Graepel, Thore},
  journal={arXiv preprint arXiv:2010.00554},
  year={2020}
}