\begin{thebibliography}{10}

\bibitem{SurveyMCTS12}
C.~Browne, E.~Powley, D.~Whitehouse, S.~Lucas, P.~Cowling, P.~Rohlfshagen,
  S.~Tavener, D.~Perez, S.~Samothrakis, and S.~Colton.
\newblock {A Survey of Monte Carlo Tree Search Methods}.
\newblock {\em IEEE Transactions on Computational Intelligence and AI in
  games,}, 4(1):1--49, 2012.

\bibitem{bubeck2010open}
S~Bubeck and R~Munos.
\newblock Open loop optimistic planning.
\newblock In {\em Conference on Learning Theory}, 2010.

\bibitem{busoniu2012optimistic}
Lucian Busoniu and R{\'e}mi Munos.
\newblock {Optimistic planning for Markov decision processes}.
\newblock In {\em Artificial Intelligence and Statistics}, pages 182--189,
  2012.

\bibitem{KLUCBJournal}
O.~Capp{\'e}, A.~Garivier, O-A. Maillard, R.~Munos, and G.~Stoltz.
\newblock {{K}ullback-{L}eibler upper confidence bounds for optimal sequential
  allocation}.
\newblock {\em Annals of Statistics}, 41(3):1516--1541, 2013.

\bibitem{cover2012elements}
Thomas~M Cover and Joy~A Thomas.
\newblock {\em Elements of information theory}.
\newblock John Wiley \& Sons, 2012.

\bibitem{dann2017unifying}
Christoph Dann, Tor Lattimore, and Emma Brunskill.
\newblock {Unifying PAC and regret: Uniform PAC bounds for episodic
  reinforcement learning}.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5713--5723, 2017.

\bibitem{de2004self}
Victor~H de~la Pena, Michael~J Klass, and Tze~Leung Lai.
\newblock Self-normalized processes: exponential inequalities, moment bounds
  and iterated logarithm laws.
\newblock {\em Annals of probability}, pages 1902--1933, 2004.

\bibitem{Feldman14BRUE}
Zohar Feldman and Carmel Domshlak.
\newblock {Simple Regret Optimization in Online Planning for Markov Decision
  Processes}.
\newblock {\em Journal of Artifial Intelligence Research}, 51:165--205, 2014.

\bibitem{filippi2010optimism}
S.~Filippi, O.~Capp{\'e}, and A.~Garivier.
\newblock {Optimism in Reinforcement Learning and {K}ullback-{L}eibler
  Divergence}.
\newblock In {\em {Allerton Conference on Communication, Control, and
  Computing}}, 2010.

\bibitem{gabillon2012best}
Victor Gabillon, Mohammad Ghavamzadeh, and Alessandro Lazaric.
\newblock {Best arm identification: A unified approach to fixed budget and
  fixed confidence}.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3212--3220, 2012.

\bibitem{garivier2011kl}
Aur{\'e}lien Garivier and Olivier Capp{\'e}.
\newblock {The KL-UCB algorithm for bounded stochastic bandits and beyond}.
\newblock In {\em Proceedings of the 24th annual conference on learning
  theory}, pages 359--376, 2011.

\bibitem{garivier2018kl}
Aur{\'e}lien Garivier, H{\'e}di Hadiji, Pierre Menard, and Gilles Stoltz.
\newblock {KL-UCB-switch: optimal regret bounds for stochastic bandits from
  both a distribution-dependent and a distribution-free viewpoints}.
\newblock {\em arXiv preprint arXiv:1805.05071}, 2018.

\bibitem{TrailBlazer16}
J.-B. Grill, M.~Valko, and R.~Munos.
\newblock {Blazing the trails before beating the path: Sample-efficient
  Monte-Carlo planning}.
\newblock In {\em Neural Information Processing Systems (NIPS)}, 2016.

\bibitem{SmoothCruiser19}
Jean{-}Bastien Grill, Omar~Darwiche Domingues, Pierre M{\'{e}}nard, R{\'{e}}mi
  Munos, and Michal Valko.
\newblock {Planning in entropy-regularized Markov decision processes and
  games}.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem{hren2008optimistic}
Jean-Francois Hren and RÃ©mi Munos.
\newblock {Optimistic planning of deterministic systems}.
\newblock In {\em European Workshop on Reinforcement Learning}, 2008.

\bibitem{Huang17StructuredBAI}
Ruitong Huang, Mohammad~M. Ajallooeian, Csaba Szepesv{\'{a}}ri, and Martin
  M{\"{u}}ller.
\newblock {Structured Best Arm Identification with Fixed Confidence}.
\newblock In {\em International Conference on Algorithmic Learning Theory
  (ALT)}, 2017.

\bibitem{UCRL10}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 11:1563--1600, 2010.

\bibitem{BAIMCTS17}
Emilie Kaufmann and Wouter~M. Koolen.
\newblock {M}onte-{C}arlo tree search by best arm identification.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2017.

\bibitem{Kearns02SS}
Michael~J. Kearns, Yishay Mansour, and Andrew~Y. Ng.
\newblock {A Sparse Sampling Algorithm for Near-Optimal Planning in Large
  Markov Decision Processes}.
\newblock {\em Machine Learning}, 49(2-3):193--208, 2002.

\bibitem{Kocsis06UCT}
Levente Kocsis and Csaba Szepesv\'{a}ri.
\newblock {Bandit Based Monte-Carlo Planning}.
\newblock In {\em Proceedings of the 17th European Conference on Machine
  Learning (ECML)}, 2006.

\bibitem{leurent2019practical}
Edouard Leurent and Odalric-Ambrym Maillard.
\newblock Practical open-loop optimistic planning.
\newblock In {\em Proceedings of the 19th European Conference on Machine
  Learning and Principles and Practice (ECML-PKDD)}, 2019.

\bibitem{SurveyRemiMCTS}
R.~Munos.
\newblock {\em {From bandits to Monte-Carlo Tree Search: The optimistic
  principle applied to optimization and planning}}, volume~7.
\newblock Foundations and Trends in Machine Learning, 2014.

\bibitem{Pepels14SimpleMCTS}
Tom Pepels, Tristan Cazenave, Mark H.~M. Winands, and Marc Lanctot.
\newblock {Minimizing Simple and Cumulative Regret in Monte-Carlo Tree Search}.
\newblock In {\em Third Workshop on Computer Games (CGW)}, pages 1--15, 2014.

\bibitem{MuZero}
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan,
  Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis,
  Thore Graepel, Timothy~P. Lillicrap, and David Silver.
\newblock {Mastering Atari, Go, Chess and Shogi by Planning with a Learned
  Model}.
\newblock {\em arXiv:1911.08265}, 2019.

\bibitem{AlphaZero}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
  Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
  Graepel, Timothy~P. Lillicrap, Karen Simonyan, and Demis Hassabis.
\newblock {A general reinforcement learning algorithm that masters chess,
  shogi, and Go through self-play}.
\newblock {\em Science}, 362, 2018.

\bibitem{simchowitz2019gaps}
Max Simchowitz and Kevin~G Jamieson.
\newblock {Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs}.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  1153--1162, 2019.

\bibitem{STOP14}
B.~Szorenyi, G.~Kedenburg, and R.~Munos.
\newblock {Optimistic Planning in Markov Decision Processes using a generative
  model}.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2014.

\bibitem{Tolpin12SRMCTS}
David Tolpin and Solomon~Eyal Shimony.
\newblock {MCTS} based on simple regret.
\newblock In {\em Proceedings of the Twenty-Sixth {AAAI} Conference on
  Artificial Intelligence, July 22-26, 2012, Toronto, Ontario, Canada.}, 2012.

\bibitem{zanette2019gaps}
Andrea Zanette and Emma Brunskill.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}, pages 7304--7312, 2019.

\end{thebibliography}
