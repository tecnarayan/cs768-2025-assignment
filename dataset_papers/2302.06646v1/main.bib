@article{scao2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@inproceedings{rae2019compressive,
  title={Compressive Transformers for Long-Range Sequence Modelling},
  author={Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Hillier, Chloe and Lillicrap, Timothy P},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@book{oppenheim2001discrete,
  title={Discrete-time signal processing. Vol. 2},
  author={Oppenheim, Alan V and Buck, John R and Schafer, Ronald W},
  year={2001},
  publisher={Upper Saddle River, NJ: Prentice Hall}
}

@article{oppenheim1978applications,
  title={Applications of digital signal processing},
  author={Oppenheim, Alan V},
  journal={Englewood Cliffs},
  year={1978}
}



@misc{nvidia2017nvidia,
  title={Nvidia {T}esla {V}100 {GPU} architecture},
  author={NVIDIA},
  year={2017},
  publisher={Aug}
}

@misc{nvidia2020nvidia,
  title={Nvidia {A}100 Tensor Core {GPU} Architecture},
  author={NVIDIA},
  year={2020}
}

@misc{nvidia2022nvidia,
  title={Nvidia {H}100 Tensor Core {GPU} Architecture},
  author={NVIDIA},
  year={2022}
}

@article{li2022makes,
  title={What Makes Convolutional Models Great on Long Sequence Modeling?},
  author={Li, Yuhong and Cai, Tianle and Zhang, Yi and Chen, Deming and Dey, Debadeepta},
  journal={arXiv preprint arXiv:2210.09298},
  year={2022}
}

@inproceedings{romero2021ckconv,
  title={CKConv: Continuous Kernel Convolution For Sequential Data},
  author={Romero, David W and Kuzina, Anna and Bekkers, Erik J and Tomczak, Jakub Mikolaj and Hoogendoorn, Mark},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{gu2022efficiently,
  title={Efficiently Modeling Long Sequences with Structured State Spaces},
  author={Gu, Albert and Goel, Karan and R\'e, Christopher},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2022}
}

@inproceedings{gu2022parameterization,
  title={On the Parameterization and Initialization of Diagonal State Space Models},
  author={Gu, Albert and Gupta, Ankit and Goel, Karan and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{hasani2022liquid,
  title={Liquid Structural State-Space Models},
  author={Hasani, Ramin and Lechner, Mathias and Wang, Tsun-Huang and Chahine, Makram and Amini, Alexander and Rus, Daniela},
  journal={arXiv preprint arXiv:2209.12951},
  year={2022}
}

@inproceedings{gupta2022diagonal,
  title={Diagonal State Spaces are as Effective as Structured State Spaces},
  author={Gupta, Ankit and Gu, Albert and Berant, Jonathan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{mehta2022long,
  title={Long range language modeling via gated state spaces},
  author={Mehta, Harsh and Gupta, Ankit and Cutkosky, Ashok and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2206.13947},
  year={2022}
}

@inproceedings{tay2020long,
  title={Long Range Arena: A Benchmark for Efficient Transformers},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{goel2022s,
  title={It's Raw! Audio Generation with State-Space Models},
  author={Goel, Karan and Gu, Albert and Donahue, Chris and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2202.09729},
  year={2022}
}

@article{gu2022train,
  title={How to train your hippo: State space models with generalized orthogonal basis projections},
  author={Gu, Albert and Johnson, Isys and Timalsina, Aman and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2206.12037},
  year={2022}
}

@article{dao2022hungry,
  title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
  author={Dao, Tri and Fu, Daniel Y and Saab, Khaled K and Thomas, Armin W and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2212.14052},
  year={2022}
}

@misc{Gokaslan2019OpenWeb,
    title={OpenWebText Corpus},
    author={Aaron Gokaslan and Vanya Cohen and Ellie Pavlick and Stefanie Tellex},
    year={2019}
}

@inproceedings{nguyen2022s4nd,
  title={S4nd: Modeling images and videos as multidimensional signals with state spaces},
  author={Nguyen, Eric and Goel, Karan and Gu, Albert and Downs, Gordon and Shah, Preey and Dao, Tri and Baccus, Stephen and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@inproceedings{dao2022monarch,
  title={Monarch: Expressive structured matrices for efficient and accurate training},
  author={Dao, Tri and Chen, Beidi and Sohoni, Nimit S and Desai, Arjun and Poli, Michael and Grogan, Jessica and Liu, Alexander and Rao, Aniruddh and Rudra, Atri and R{\'e}, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={4690--4721},
  year={2022},
  organization={PMLR}
}

@inproceedings{chen2021pixelated,
  title={Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models},
  author={Chen, Beidi and Dao, Tri and Liang, Kaizhao and Yang, Jiaming and Song, Zhao and Rudra, Atri and Re, Christopher},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{cooley1965an,
 ISSN = {00255718, 10886842},
 URL = {http://www.jstor.org/stable/2003354},
 author = {James W. Cooley and John W. Tukey},
 journal = {Mathematics of Computation},
 number = {90},
 pages = {297--301},
 publisher = {American Mathematical Society},
 title = {An Algorithm for the Machine Calculation of Complex Fourier Series},
 volume = {19},
 year = {1965}
}

@article{bailey1990ffts,
  title={{FFT}s in external or hierarchical memory},
  author={Bailey, David H},
  journal={The journal of Supercomputing},
  volume={4},
  number={1},
  pages={23--35},
  year={1990},
  publisher={Springer}
}

@article{gu2020hippo,
  title={Hippo: Recurrent memory with optimal polynomial projections},
  author={Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1474--1487},
  year={2020}
}

@inproceedings{dao2022flashattention,
  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{parikh2014proximal,
  title={Proximal algorithms},
  author={Parikh, Neal and Boyd, Stephen and others},
  journal={Foundations and trends{\textregistered} in Optimization},
  volume={1},
  number={3},
  pages={127--239},
  year={2014},
  publisher={Now Publishers, Inc.}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{smith2022simplified,
  title={Simplified state space layers for sequence modeling},
  author={Smith, Jimmy TH and Warrington, Andrew and Linderman, Scott W},
  journal={arXiv preprint arXiv:2208.04933},
  year={2022}
}

@article{islam2022long,
  title={Long movie clip classification with state-space video models},
  author={Islam, Md Mohaiminul and Bertasius, Gedas},
  journal={arXiv preprint arXiv:2204.01692},
  year={2022}
}

@inproceedings{kitaev2019reformer,
  title={Reformer: The Efficient Transformer},
  author={Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17283--17297},
  year={2020}
}

@article{ma2022mega,
  title={Mega: moving average equipped gated attention},
  author={Ma, Xuezhe and Zhou, Chunting and Kong, Xiang and He, Junxian and Gui, Liangke and Neubig, Graham and May, Jonathan and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2209.10655},
  year={2022}
}

@misc{cufft,
  title={cuFFT v11.7.1 Documentation},
  author={NVIDIA},
  year={2022},
  note={https://docs.nvidia.com/cuda/cufft/index.html}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{dosovitskiy2020image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{rockafellar1976monotone,
  title={Monotone operators and the proximal point algorithm},
  author={Rockafellar, R Tyrrell},
  journal={SIAM journal on control and optimization},
  volume={14},
  number={5},
  pages={877--898},
  year={1976},
  publisher={SIAM}
}

@article{guibas2021adaptive,
  title={Adaptive fourier neural operators: Efficient token mixers for transformers},
  author={Guibas, John and Mardani, Morteza and Li, Zongyi and Tao, Andrew and Anandkumar, Anima and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2111.13587},
  year={2021}
}

@article{lee2021fnet,
  title={Fnet: Mixing tokens with fourier transforms},
  author={Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
  journal={arXiv preprint arXiv:2105.03824},
  year={2021}
}

@article{varol2017long,
  title={Long-term temporal convolutions for action recognition},
  author={Varol, G{\"u}l and Laptev, Ivan and Schmid, Cordelia},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={6},
  pages={1510--1517},
  year={2017},
  publisher={IEEE}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{krizhevsky2017imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Communications of the ACM},
  volume={60},
  number={6},
  pages={84--90},
  year={2017},
  publisher={AcM New York, NY, USA}
}

@article{ayinala2011pipelined,
  title={Pipelined parallel FFT architectures via folding transformation},
  author={Ayinala, Manohar and Brown, Michael and Parhi, Keshab K},
  journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  volume={20},
  number={6},
  pages={1068--1081},
  year={2011},
  publisher={IEEE}
}

@book{chu1999inside,
  title={Inside the FFT black box: serial and parallel fast Fourier transform algorithms},
  author={Chu, Eleanor and George, Alan},
  year={1999},
  publisher={CRC press}
}

@article{bahn2009parallel,
  title={Parallel FFT algorithms on network-on-chips},
  author={Bahn, Jun Ho and Yang, Jung Sook and Hu, Wen-Hsiang and Bagherzadeh, Nader},
  journal={Journal of Circuits, Systems, and Computers},
  volume={18},
  number={02},
  pages={255--269},
  year={2009},
  publisher={World Scientific}
}

@article{bekele2016cooley,
  title={Cooley-tukey fft algorithms},
  author={Bekele, AJAA},
  journal={Advanced algorithms},
  year={2016}
}

@article{trockman2022patches,
  title={Patches are all you need?},
  author={Trockman, Asher and Kolter, J Zico},
  journal={arXiv preprint arXiv:2201.09792},
  year={2022}
}

@inproceedings{liang2021evit,
  title={EViT: Expediting Vision Transformers via Token Reorganizations},
  author={Liang, Youwei and Chongjian, GE and Tong, Zhan and Song, Yibing and Wang, Jue and Xie, Pengtao},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{yu2016orthogonal,
  title={Orthogonal random features},
  author={Yu, Felix Xinnan X and Suresh, Ananda Theertha and Choromanski, Krzysztof M and Holtmann-Rice, Daniel N and Kumar, Sanjiv},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{le2013fastfood,
  title={Fastfood-computing hilbert space expansions in loglinear time},
  author={Le, Quoc and Sarl{\'o}s, Tam{\'a}s and Smola, Alexander},
  booktitle={International Conference on Machine Learning},
  pages={244--252},
  year={2013},
  organization={PMLR}
}

@misc{jurafsky2000speech,
  title={Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition},
  author={Jurafsky, Daniel and Martin, James H},
  journal={Prentice Hall series in artificial intelligence},
  pages={I--XXVI},
  year={2000},
  publisher={Citeseer}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@article{tang2022spatiotemporal,
  title={Spatiotemporal Modeling of Multivariate Signals With Graph Neural Networks and Structured State Space Models},
  author={Tang, Siyi and Dunnmon, Jared A and Qu, Liangqiong and Saab, Khaled K and Lee-Messer, Christopher and Rubin, Daniel L},
  journal={arXiv preprint arXiv:2211.11176},
  year={2022}
}

@inproceedings{zhang2023effectively,
  title={Effectively Modeling Time Series with Simple Discrete State Spaces},
  author={Zhang, Michael and Saab, Khaled K and Poli, Michael and Goel, Karan and Dao, Tri and R\'{e}, Christopher},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@article{zhou2022deep,
  title={Deep Latent State Space Models for Time-Series Generation},
  author={Zhou, Linqi and Poli, Michael and Xu, Winnie and Massaroli, Stefano and Ermon, Stefano},
  journal={arXiv preprint arXiv:2212.12749},
  year={2022}
}

@article{thomas2022self,
  title={Self-supervised learning of brain dynamics from broad neuroimaging data},
  author={Thomas, Armin W and R{\'e}, Christopher and Poldrack, Russell A},
  journal={arXiv preprint arXiv:2206.11417},
  year={2022}
}

@article{thomas2022interpreting,
  title={Interpreting mental state decoding with deep learning models},
  author={Thomas, Armin W and R{\'e}, Christopher and Poldrack, Russell A},
  journal={Trends in Cognitive Sciences},
  volume={26},
  number={11},
  pages={972--986},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{de2018two,
  title={A two-pronged progress in structured dense matrix vector multiplication},
  author={De Sa, Christopher and Cu, Albert and Puttagunta, Rohan and R{\'e}, Christopher and Rudra, Atri},
  booktitle={Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages={1060--1079},
  year={2018},
  organization={SIAM}
}

@article{king2019functional,
  title={Functional boundaries in the human cerebellum revealed by a multi-domain task battery},
  author={King, Maedbh and Hernandez-Castillo, Carlos R and Poldrack, Russell A and Ivry, Richard B and Diedrichsen, J{\"o}rn},
  journal={Nature neuroscience},
  volume={22},
  number={8},
  pages={1371--1378},
  year={2019},
  publisher={Nature Publishing Group US New York}
}

@article{barch2013function,
  title={Function in the human connectome: task-fMRI and individual differences in behavior},
  author={Barch, Deanna M and Burgess, Gregory C and Harms, Michael P and Petersen, Steven E and Schlaggar, Bradley L and Corbetta, Maurizio and Glasser, Matthew F and Curtiss, Sandra and Dixit, Sachin and Feldt, Cindy and others},
  journal={Neuroimage},
  volume={80},
  pages={169--189},
  year={2013},
  publisher={Elsevier}
}

@article{dadi2020fine,
  title={Fine-grain atlases of functional modes for fMRI analysis},
  author={Dadi, Kamalaker and Varoquaux, Ga{\"e}l and Machlouzarides-Shalit, Antonia and Gorgolewski, Krzysztof J and Wassermann, Demian and Thirion, Bertrand and Mensch, Arthur},
  journal={NeuroImage},
  volume={221},
  pages={117126},
  year={2020},
  publisher={Elsevier}
}

@article{williams2009roofline,
  title={Roofline: an insightful visual performance model for multicore architectures},
  author={Williams, Samuel and Waterman, Andrew and Patterson, David},
  journal={Communications of the ACM},
  volume={52},
  number={4},
  pages={65--76},
  year={2009},
  publisher={ACM New York, NY, USA}
}

@article{olsson2022context,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}

@inproceedings{zhou2021informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={12},
  pages={11106--11115},
  year={2021}
}

@book{brigham1988fast,
  title={The fast Fourier transform and its applications},
  author={Brigham, E Oran},
  year={1988},
  publisher={Prentice-Hall, Inc.}
}

@article{fischl2012freesurfer,
  title={FreeSurfer},
  author={Fischl, Bruce},
  journal={Neuroimage},
  volume={62},
  number={2},
  pages={774--781},
  year={2012},
  publisher={Elsevier}
}

@book{parker1995random,
  title={Random Butterfly Transformations with Applications in Computational Linear Algebra},
  author={Parker, D.S.},
  series={CSD (Series)},
  year={1995},
  publisher={UCLA Computer Science Department}
}

@inproceedings{dao2019learning,
  title={Learning fast algorithms for linear transforms using butterfly factorizations},
  author={Dao, Tri and Gu, Albert and Eichhorn, Matthew and Rudra, Atri and R{\'e}, Christopher},
  booktitle={International conference on machine learning},
  pages={1517--1527},
  year={2019},
  organization={PMLR}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{gao2020pile,
  title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@misc{smith1997scientist,
  title={The scientist and engineer's guide to digital signal processing},
  author={Smith, Steven W and others},
  year={1997},
  publisher={California Technical Pub. San Diego}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{romero2021flexconv,
  title={Flexconv: Continuous kernel convolutions with differentiable kernel sizes},
  author={Romero, David W and Bruintjes, Robert-Jan and Tomczak, Jakub M and Bekkers, Erik J and Hoogendoorn, Mark and van Gemert, Jan C},
  journal={arXiv preprint arXiv:2110.08059},
  year={2021}
}

@inproceedings{dai2019transformer,
  title={Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G and Le, Quoc and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2978--2988},
  year={2019}
}

@inproceedings{wu2022memorizing,
  title={Memorizing Transformers},
  author={Wu, Yuhuai and Rabe, Markus Norman and Hutchins, DeLesley and Szegedy, Christian},
  year={2022},
  booktitle={International Conference on Learning Representations}
}

@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@article{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{sanh2020movement,
  title={Movement pruning: Adaptive sparsity by fine-tuning},
  author={Sanh, Victor and Wolf, Thomas and Rush, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20378--20389},
  year={2020}
}

@article{lin2017runtime,
  title={Runtime neural pruning},
  author={Lin, Ji and Rao, Yongming and Lu, Jiwen and Zhou, Jie},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{dong2017learning,
  title={Learning to prune deep neural networks via layer-wise optimal brain surgeon},
  author={Dong, Xin and Chen, Shangyu and Pan, Sinno},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{frankle2019stabilizing,
  title={Stabilizing the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M and Carbin, Michael},
  journal={arXiv preprint arXiv:1903.01611},
  year={2019}
}

@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle={International Conference on Machine Learning},
  pages={3259--3269},
  year={2020},
  organization={PMLR}
}

@article{sindhwani2015structured,
  title={Structured transforms for small-footprint deep learning},
  author={Sindhwani, Vikas and Sainath, Tara and Kumar, Sanjiv},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  year={2015}
}

@article{kailath1979displacement,
  title={Displacement ranks of matrices and linear equations},
  author={Kailath, Thomas and Kung, Sun-Yuan and Morf, Martin},
  journal={Journal of Mathematical Analysis and Applications},
  volume={68},
  number={2},
  pages={395--407},
  year={1979},
  publisher={Elsevier}
}

@article{eidelman1999new,
  title={On a new class of structured matrices},
  author={Eidelman, Yuli and Gohberg, Israel},
  journal={Integral Equations and Operator Theory},
  volume={34},
  number={3},
  pages={293--324},
  year={1999},
  publisher={Springer}
}

@inproceedings{choromanski2019unifying,
  title={Unifying orthogonal monte carlo methods},
  author={Choromanski, Krzysztof and Rowland, Mark and Chen, Wenyu and Weller, Adrian},
  booktitle={International Conference on Machine Learning},
  pages={1203--1212},
  year={2019},
  organization={PMLR}
}

@article{munkhoeva2018quadrature,
  title={Quadrature-based features for kernel approximation},
  author={Munkhoeva, Marina and Kapushev, Yermek and Burnaev, Evgeny and Oseledets, Ivan},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{prabhu2020butterfly,
  title={Butterfly transform: An efficient fft based neural architecture design},
  author={Prabhu, Anish and Farhadi, Ali and Rastegari, Mohammad and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12024--12033},
  year={2020}
}

@article{lin2021deformable,
  title={Deformable butterfly: A highly structured and sparse linear transform},
  author={Lin, Rui and Ran, Jie and Chiu, King Hung and Chesi, Graziano and Wong, Ngai},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16145--16157},
  year={2021}
}

@inproceedings{ailon2021sparse,
  title={Sparse linear networks with a fixed butterfly structure: theory and practice},
  author={Ailon, Nir and Leibovitch, Omer and Nair, Vineet},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={1174--1184},
  year={2021},
  organization={PMLR}
}

@article{li2021tcfft,
  title={tcfft: Accelerating half-precision FFT through tensor cores},
  author={Li, Binrui and Cheng, Shenggan and Lin, James},
  journal={arXiv preprint arXiv:2104.11471},
  year={2021}
}
