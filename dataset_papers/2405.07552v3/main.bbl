\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bao \& Xiong(2021)Bao and Xiong]{bao2021one}
Bao, Y. and Xiong, W.
\newblock One-round communication efficient distributed {M}-estimation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  46--54. PMLR, 2021.

\bibitem[Belloni \& Chernozhukov(2011)Belloni and Chernozhukov]{belloni2011}
Belloni, A. and Chernozhukov, V.
\newblock $\ell_1$-penalized quantile regression in high-dimensional sparse
  models.
\newblock \emph{The Annals of Statistics}, 39\penalty0 (1):\penalty0 82--130,
  2011.

\bibitem[Bradic \& Kolar(2017)Bradic and Kolar]{bradic2017uniform}
Bradic, J. and Kolar, M.
\newblock Uniform inference for high-dimensional quantile regression: linear
  functionals and regression rank scores.
\newblock \emph{arXiv preprint arXiv:1702.06209}, 2017.

\bibitem[Cai \& Liu(2011)Cai and Liu]{cai2011adaptive}
Cai, T. and Liu, W.
\newblock Adaptive thresholding for sparse covariance matrix estimation.
\newblock \emph{Journal of the American Statistical Association}, 106\penalty0
  (494):\penalty0 672--684, 2011.

\bibitem[Cai et~al.(2010)Cai, Zhang, and Zhou]{cai2010optimal}
Cai, T.~T., Zhang, C.-H., and Zhou, H.~H.
\newblock Optimal rates of convergence for covariance matrix estimation.
\newblock \emph{The Annals of Statistics}, 38\penalty0 (4):\penalty0
  2118--2144, 2010.

\bibitem[Chen \& Zhou(2020)Chen and Zhou]{chen2020quantile}
Chen, L. and Zhou, Y.
\newblock Quantile regression in big data: A divide and conquer based strategy.
\newblock \emph{Computational Statistics \& Data Analysis}, 144:\penalty0
  106892, 2020.

\bibitem[Chen \& Xie(2014)Chen and Xie]{chen2014split}
Chen, X. and Xie, M.-g.
\newblock A split-and-conquer approach for analysis of extraordinarily large
  data.
\newblock \emph{Statistica Sinica}, pp.\  1655--1684, 2014.

\bibitem[Chen et~al.(2019)Chen, Liu, and Zhang]{chen2019quantile}
Chen, X., Liu, W., and Zhang, Y.
\newblock Quantile regression under memory constraint.
\newblock \emph{The Annals of Statistics}, 47\penalty0 (6):\penalty0
  3244--3273, 2019.

\bibitem[Chen et~al.(2020)Chen, Liu, Mao, and Yang]{chen2020distributed}
Chen, X., Liu, W., Mao, X., and Yang, Z.
\newblock Distributed high-dimensional regression under a quantile loss
  function.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 7432--7474, 2020.

\bibitem[Fan \& Li(2001)Fan and Li]{fan2001variable}
Fan, J. and Li, R.
\newblock Variable selection via nonconcave penalized likelihood and its oracle
  properties.
\newblock \emph{Journal of the American statistical Association}, 96\penalty0
  (456):\penalty0 1348--1360, 2001.

\bibitem[Fan et~al.(2014)Fan, Fan, and Barut]{fan2014adaptive}
Fan, J., Fan, Y., and Barut, E.
\newblock Adaptive robust variable selection.
\newblock \emph{The Annals of Statistics}, 42\penalty0 (1):\penalty0 324--351,
  2014.

\bibitem[Fan et~al.(2021)Fan, Guo, and Wang]{fan2021communication}
Fan, J., Guo, Y., and Wang, K.
\newblock Communication-efficient accurate statistical estimation.
\newblock \emph{Journal of the American Statistical Association}, 116:\penalty0
  1--11, 2021.

\bibitem[Feng et~al.(2023)Feng, Liu, and Wang]{feng2023lack}
Feng, X., Liu, Q., and Wang, C.
\newblock A lack-of-fit test for quantile regression process models.
\newblock \emph{Statistics \& Probability Letters}, 192:\penalty0 109680, 2023.

\bibitem[Fernandes et~al.(2021)Fernandes, Guerre, and
  Horta]{fernandes2021smoothing}
Fernandes, M., Guerre, E., and Horta, E.
\newblock Smoothing quantile regressions.
\newblock \emph{Journal of Business \& Economic Statistics}, 39\penalty0
  (1):\penalty0 338--357, 2021.

\bibitem[Gao et~al.(2022)Gao, Liu, Wang, Wang, Yan, and Zhang]{gao2022review}
Gao, Y., Liu, W., Wang, H., Wang, X., Yan, Y., and Zhang, R.
\newblock A review of distributed statistical inference.
\newblock \emph{Statistical Theory and Related Fields}, 6\penalty0
  (2):\penalty0 89--99, 2022.

\bibitem[Hastie et~al.(2015)Hastie, Tibshirani, and
  Wainwright]{hastie2015statistical}
Hastie, T., Tibshirani, R., and Wainwright, M.
\newblock \emph{Statistical learning with sparsity: the lasso and
  generalizations}.
\newblock CRC press, 2015.

\bibitem[He et~al.(2023)He, Pan, Tan, and Zhou]{he2021smoothed}
He, X., Pan, X., Tan, K.~M., and Zhou, W.-X.
\newblock Smoothed quantile regression with large-scale inference.
\newblock \emph{Journal of Econometrics}, 232\penalty0 (2):\penalty0 367--388,
  2023.

\bibitem[Hu et~al.(2021)Hu, Li, and Wu]{hu2021communication}
Hu, A., Li, C., and Wu, J.
\newblock Communication-efficient modeling with penalized quantile regression
  for distributed data.
\newblock \emph{Complexity}, 2021:\penalty0 1--16, 1 2021.

\bibitem[Huang \& Huo(2019)Huang and Huo]{huang2019distributed}
Huang, C. and Huo, X.
\newblock A distributed one-step estimator.
\newblock \emph{Mathematical Programming}, 174:\penalty0 41--76, 2019.

\bibitem[Huang et~al.(2018)Huang, Jiao, Lu, and Zhu]{huang2018robust}
Huang, J., Jiao, Y., Lu, X., and Zhu, L.
\newblock Robust decoding from 1-bit compressive sampling with ordinary and
  regularized least squares.
\newblock \emph{SIAM Journal on Scientific Computing}, 40\penalty0
  (4):\penalty0 A2062--A2086, 2018.

\bibitem[Jiang \& Yu(2021)Jiang and Yu]{jiang2021smoothing}
Jiang, R. and Yu, K.
\newblock Smoothing quantile regression for a distributed system.
\newblock \emph{Neurocomputing}, 466:\penalty0 311--326, 2021.

\bibitem[Jordan et~al.(2019)Jordan, Lee, and Yang]{jordan2018communication}
Jordan, M.~I., Lee, J.~D., and Yang, Y.
\newblock Communication-efficient distributed statistical inference.
\newblock \emph{Journal of the American Statistical Association}, 114\penalty0
  (526):\penalty0 668--681, 2019.

\bibitem[Koenker(2005)]{koenker2005quantile}
Koenker, R.
\newblock \emph{Quantile regression}, volume~38.
\newblock Cambridge University Press, 2005.

\bibitem[Koenker \& Bassett~Jr(1978)Koenker and
  Bassett~Jr]{koenker1978regression}
Koenker, R. and Bassett~Jr, G.
\newblock Regression quantiles.
\newblock \emph{Econometrica}, 46\penalty0 (1):\penalty0 33--50, 1978.

\bibitem[Lee et~al.(2017)Lee, Liu, Sun, and Taylor]{lee2017communication}
Lee, J.~D., Liu, Q., Sun, Y., and Taylor, J.~E.
\newblock Communication-efficient sparse regression.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 115--144, 2017.

\bibitem[Li et~al.(2013)Li, Lin, and Li]{li2013statistical}
Li, R., Lin, D.~K., and Li, B.
\newblock Statistical inference in massive data sets.
\newblock \emph{Applied Stochastic Models in Business and Industry},
  29\penalty0 (5):\penalty0 399--409, 2013.

\bibitem[Li et~al.(2020)Li, Sahu, Talwalkar, and Smith]{li2020federated}
Li, T., Sahu, A.~K., Talwalkar, A., and Smith, V.
\newblock Federated learning: Challenges, methods, and future directions.
\newblock \emph{IEEE Signal Processing Magazine}, 37\penalty0 (3):\penalty0
  50--60, 2020.

\bibitem[Neykov et~al.(2016)Neykov, Liu, and Cai]{neykov2016l1}
Neykov, M., Liu, J.~S., and Cai, T.
\newblock $\ell_1$-regularized least squares for support recovery of high
  dimensional single index models with {G}aussian designs.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2976--3012, 2016.

\bibitem[Rhee et~al.(2003)Rhee, Gonzales, Kantor, Betts, Ravela, and
  Shafer]{rhee2003human}
Rhee, S.-Y., Gonzales, M.~J., Kantor, R., Betts, B.~J., Ravela, J., and Shafer,
  R.~W.
\newblock Human immunodeficiency virus reverse transcriptase and protease
  sequence database.
\newblock \emph{Nucleic Acids Research}, 31\penalty0 (1):\penalty0 298--303,
  2003.

\bibitem[Schmidt(2010)]{schmidt2010graphical}
Schmidt, M.
\newblock Graphical model structure learning with l1-regularization.
\newblock \emph{University of British Columbia}, 2010.

\bibitem[Shamir et~al.(2014)Shamir, Srebro, and Zhang]{shamir2014communication}
Shamir, O., Srebro, N., and Zhang, T.
\newblock Communication-efficient distributed optimization using an approximate
  newton-type method.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1000--1008. PMLR, 2014.

\bibitem[Solntsev et~al.(2015)Solntsev, Nocedal, and
  Byrd]{solntsev2015algorithm}
Solntsev, S., Nocedal, J., and Byrd, R.~H.
\newblock An algorithm for quadratic $\ell_1$-regularized optimization with a
  flexible active-set strategy.
\newblock \emph{Optimization Methods and Software}, 30\penalty0 (6):\penalty0
  1213--1237, 2015.

\bibitem[Tan et~al.(2022)Tan, Battey, and Zhou]{tan2022communication}
Tan, K.~M., Battey, H., and Zhou, W.-X.
\newblock Communication-constrained distributed quantile regression with
  optimal statistical guarantees.
\newblock \emph{Journal of Machine Learning Research}, 23:\penalty0 1--61,
  2022.

\bibitem[Wainwright(2009)]{wainwright2009sharp}
Wainwright, M.~J.
\newblock Sharp thresholds for high-dimensional and noisy sparsity recovery
  using $\ell_1$-constrained quadratic programming (lasso).
\newblock \emph{IEEE Transactions on Information Theory}, 55\penalty0
  (5):\penalty0 2183--2202, 2009.

\bibitem[Wainwright(2019)]{wainwright2019high}
Wainwright, M.~J.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Wang et~al.(2017)Wang, Kolar, Srebro, and Zhang]{wang2017efficient}
Wang, J., Kolar, M., Srebro, N., and Zhang, T.
\newblock Efficient distributed learning with sparsity.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3636--3645. PMLR, 2017.

\bibitem[Wright(2015)]{wright2015coordinate}
Wright, S.~J.
\newblock Coordinate descent algorithms.
\newblock \emph{Mathematical Programming}, 151\penalty0 (1):\penalty0 3--34,
  2015.

\bibitem[Xu et~al.(2020)Xu, Cai, Jiang, Sun, and Huang]{xu2020block}
Xu, Q., Cai, C., Jiang, C., Sun, F., and Huang, X.
\newblock Block average quantile regression for massive dataset.
\newblock \emph{Statistical Papers}, 61\penalty0 (1):\penalty0 141--165, 2020.

\bibitem[Zhang(2010)]{zhang2010nearly}
Zhang, C.-H.
\newblock Nearly unbiased variable selection under minimax concave penalty.
\newblock \emph{The Annals of Statistics}, 38\penalty0 (2):\penalty0 894--942,
  2010.

\bibitem[Zhang \& Xiao(2018)Zhang and Xiao]{zhang2018communication}
Zhang, Y. and Xiao, L.
\newblock Communication-efficient distributed optimization of self-concordant
  empirical loss.
\newblock In \emph{Large-Scale and Distributed Optimization}, pp.\  289--341.
  Springer, 2018.

\bibitem[Zhang et~al.(2013)Zhang, Duchi, and Wainwright]{zhang13}
Zhang, Y., Duchi, J.~C., and Wainwright, M.~J.
\newblock Communication-efficient algorithms for statistical optimization.
\newblock \emph{Journal of Machine Learning Research}, 14\penalty0
  (68):\penalty0 3321--3363, 2013.

\bibitem[Zhang et~al.(2015)Zhang, Duchi, and Wainwright]{zhang2015divide}
Zhang, Y., Duchi, J., and Wainwright, M.
\newblock Divide and conquer kernel ridge regression: A distributed algorithm
  with minimax optimal rates.
\newblock \emph{Journal of Machine Learning Research}, 16\penalty0
  (1):\penalty0 3299--3340, 2015.

\bibitem[Zhao \& Yu(2006)Zhao and Yu]{zhao2006model}
Zhao, P. and Yu, B.
\newblock On model selection consistency of lasso.
\newblock \emph{Journal of Machine Learning Research}, 7:\penalty0 2541--2563,
  2006.

\bibitem[Zhao et~al.(2014)Zhao, Kolar, and Liu]{zhao2014general}
Zhao, T., Kolar, M., and Liu, H.
\newblock A general framework for robust testing and confidence regions in
  high-dimensional quantile regression.
\newblock \emph{arXiv preprint arXiv:1412.8724}, 2014.

\end{thebibliography}
