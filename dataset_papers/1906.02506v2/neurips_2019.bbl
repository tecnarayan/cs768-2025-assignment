\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anderson and Peterson(1987)]{anderson1987mean}
James~R Anderson and Carsten Peterson.
\newblock A mean field theory learning algorithm for neural networks.
\newblock \emph{Complex Systems}, 1:\penalty0 995--1019, 1987.

\bibitem[Barber and Bishop(1998)]{barber1998ensemble}
David Barber and Christopher~M Bishop.
\newblock Ensemble learning in {Bayesian} neural networks.
\newblock \emph{Generalization in Neural Networks and Machine Learning},
  168:\penalty0 215--238, 1998.

\bibitem[Bishop(2006)]{bishop2006pattern}
Christopher~M. Bishop.
\newblock \emph{Pattern Recognition and Machine Learning (Information Science
  and Statistics)}.
\newblock Springer-Verlag, Berlin, Heidelberg, 2006.
\newblock ISBN 0387310738.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell2015weight}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  1613--1622, 2015.

\bibitem[Bottou et~al.(2016)Bottou, Curtis, and
  Nocedal]{bottou2016optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{arXiv preprint arXiv:1606.04838}, 2016.

\bibitem[Bradshaw et~al.(2017)Bradshaw, Matthews, and
  Ghahramani]{bradshaw2017adversarial}
John Bradshaw, Alexander G de~G Matthews, and Zoubin Ghahramani.
\newblock Adversarial examples, uncertainty, and transfer testing robustness in
  {G}aussian process hybrid deep networks.
\newblock \emph{arXiv preprint arXiv:1707.02476}, 2017.

\bibitem[DeGroot and Fienberg(1983)]{degroot1983comparison}
Morris~H. DeGroot and Stephen~E. Fienberg.
\newblock The comparison and evaluation of forecasters.
\newblock \emph{The Statistician: Journal of the Institute of Statisticians},
  32:\penalty0 12--22, 1983.

\bibitem[DeVries and Taylor(2018)]{devries2018learning}
Terrance DeVries and Graham~W. Taylor.
\newblock Learning confidence for out-of-distribution detection in neural
  networks.
\newblock \emph{arXiv preprint arXiv:1802.04865}, 2018.

\bibitem[Gal and Ghahramani(2016)]{yarin16dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a {Bayesian} approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1050--1059, 2016.

\bibitem[Ghosal and Van~der Vaart(2017)]{ghosal2017fundamentals}
S.~Ghosal and A.~Van~der Vaart.
\newblock \emph{Fundamentals of nonparametric {B}ayesian inference}, volume~44.
\newblock Cambridge {U}niversity {P}ress, 2017.

\bibitem[Glorot and Bengio(2010)]{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256, 2010.

\bibitem[Goodfellow(2015)]{goodfellow2015efficient}
Ian Goodfellow.
\newblock {Efficient Per-Example Gradient Computations}.
\newblock \emph{ArXiv e-prints}, October 2015.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'{a}}r, Girshick, Noordhuis,
  Wesolowski, Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Doll{\'{a}}r, Ross~B. Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch {SGD:} training imagenet in 1 hour.
\newblock \emph{CoRR}, abs/1706.02677, 2017.

\bibitem[Graves(2011)]{graves2011practical}
Alex Graves.
\newblock Practical variational inference for neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2348--2356, 2011.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and
  Weinberger]{guo2017oncalibration}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q Weinberger.
\newblock On calibration of modern neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1321--1330. JMLR. org, 2017.

\bibitem[Hendrycks and Gimpel(2017)]{hendrycks2017baseline}
Dan Hendrycks and Kevin Gimpel.
\newblock A baseline for detecting misclassified and out-of-distribution
  examples in neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Hinton et~al.(2012)Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior,
  Vanhoucke, Nguyen, Kingsbury, et~al.]{hinton2012deep}
Geoffrey Hinton, Li~Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep
  Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Brian Kingsbury,
  et~al.
\newblock Deep neural networks for acoustic modeling in speech recognition.
\newblock \emph{IEEE Signal processing magazine}, 29, 2012.

\bibitem[Hinton and Van~Camp(1993)]{hinton1993keeping}
Geoffrey~E Hinton and Drew Van~Camp.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In \emph{Annual Conference on Computational Learning Theory}, pages
  5--13, 1993.

\bibitem[Hoeting et~al.(1999)Hoeting, Madigan, Raftery, and
  Volinsky]{hoeting1999bayesian}
Jennifer~A Hoeting, David Madigan, Adrian~E Raftery, and Chris~T Volinsky.
\newblock Bayesian model averaging: a tutorial.
\newblock \emph{Statistical science}, pages 382--401, 1999.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batchnorm}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{CoRR}, abs/1502.03167, 2015.
\newblock URL \url{http://arxiv.org/abs/1502.03167}.

\bibitem[Khan(2012)]{khan2012variational}
Mohammad Khan.
\newblock \emph{Variational learning for latent Gaussian model of discrete
  data}.
\newblock PhD thesis, University of British Columbia, 2012.

\bibitem[Khan and Lin(2017)]{khan2017conjugate}
Mohammad~Emtiyaz Khan and Wu~Lin.
\newblock Conjugate-computation variational inference: converting variational
  inference in non-conjugate models to inferences in conjugate models.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 878--887, 2017.

\bibitem[Khan and Nielsen(2018)]{khan2018fast1}
Mohammad~Emtiyaz Khan and Didrik Nielsen.
\newblock Fast yet simple natural-gradient descent for variational inference in
  complex models.
\newblock In \emph{2018 International Symposium on Information Theory and Its
  Applications (ISITA)}, pages 31--35. IEEE, 2018.

\bibitem[Khan et~al.(2018)Khan, Nielsen, Tangkaratt, Lin, Gal, and
  Srivastava]{khan2018fast}
Mohammad~Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu~Lin, Yarin Gal, and
  Akash Srivastava.
\newblock Fast and scalable {B}ayesian deep learning by weight-perturbation in
  {A}dam.
\newblock In \emph{International Conference on Machine Learning}, pages
  2616--2625, 2018.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and
  Welling]{kingma2015variational}
Diederik~P Kingma, Tim Salimans, and Max Welling.
\newblock Variational dropout and the local reparameterization trick.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2575--2583, 2015.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.

\bibitem[Krizhevsky and Hinton(2009)]{krizhevsky2009cifar}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{lakshminarayanan2017simple}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pages
  6402--6413. Curran Associates, Inc., 2017.

\bibitem[Lee et~al.(2018)Lee, Lee, Lee, and Shin]{lee2018training}
Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin.
\newblock Training confidence-calibrated classifiers for detecting
  out-of-distribution samples.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Liang et~al.(2018)Liang, Li, and Srikant]{liang2018enhancing}
Shiyu Liang, Yixuan Li, and R.~Srikant.
\newblock Enhancing the reliability of out-of-distribution image detection in
  neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Lopez-Paz and Ranzato(2017)]{lopez2017gradient}
David Lopez-Paz and Marc~Aurelio Ranzato.
\newblock Gradient episodic memory for continual learning.
\newblock In \emph{NIPS}, 2017.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Mackay(1991)]{mackay1991thesis}
David Mackay.
\newblock \emph{Bayesian Methods for Adaptive Models}.
\newblock PhD thesis, California Institute of Technology, 1991.

\bibitem[MacKay(2003)]{mackay2003information}
David~JC MacKay.
\newblock \emph{Information theory, inference and learning algorithms}.
\newblock Cambridge university press, 2003.

\bibitem[Maddox et~al.(2019)Maddox, Garipov, Izmailov, Vetrov, and
  Wilson]{maddox2019simple}
Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, and Andrew~Gordon
  Wilson.
\newblock A simple baseline for {B}ayesian uncertainty in deep learning.
\newblock \emph{arXiv preprint arXiv:1902.02476}, 2019.

\bibitem[Mandt et~al.(2017)Mandt, Hoffman, and Blei]{mandt2017stochastic}
Stephan Mandt, Matthew~D Hoffman, and David~M Blei.
\newblock Stochastic gradient descent as approximate {B}ayesian inference.
\newblock \emph{Journal of Machine Learning Research}, 18:\penalty0 1--35,
  2017.

\bibitem[Mikolov et~al.(2013)Mikolov, Chen, Corrado, and
  Dean]{mikolov2013efficient}
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
\newblock Efficient estimation of word representations in vector space.
\newblock \emph{arXiv preprint arXiv:1301.3781}, 2013.

\bibitem[Naeini et~al.(2015)Naeini, Cooper, and
  Hauskrecht]{naeini2015obtaining}
Mahdi~Pakdaman Naeini, Gregory~F. Cooper, and Milos Hauskrecht.
\newblock Obtaining well calibrated probabilities using {B}ayesian binning.
\newblock In \emph{Proceedings of the Twenty-Ninth AAAI Conference on
  Artificial Intelligence}, AAAI'15, pages 2901--2907. AAAI Press, 2015.

\bibitem[Neal(1995)]{neal95}
Redford~M Neal.
\newblock \emph{{B}ayesian learning for neural networks}.
\newblock PhD thesis, University of Toronto, 1995.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y.
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In \emph{NIPS Workshop on Deep Learning and Unsupervised Feature
  Learning 2011}, 2011.

\bibitem[Nguyen et~al.(2017)Nguyen, Li, Bui, and Turner]{nguyen2017variational}
Cuong~V Nguyen, Yingzhen Li, Thang~D Bui, and Richard~E Turner.
\newblock Variational continual learning.
\newblock \emph{arXiv preprint arXiv:1710.10628}, 2017.

\bibitem[Osawa et~al.(2018)Osawa, Tsuji, Ueno, Naruse, Yokota, and
  Matsuoka]{osawa2018secondorder}
Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi
  Matsuoka.
\newblock Second-order optimization method for large mini-batch: Training
  resnet-50 on imagenet in 35 epochs.
\newblock \emph{CoRR}, abs/1811.12019, 2018.

\bibitem[Riquelme et~al.(2018)Riquelme, Tucker, and Snoek]{riquelme2018deep}
Carlos Riquelme, George Tucker, and Jasper Snoek.
\newblock Deep {B}ayesian bandits showdown: An empirical comparison of
  {B}ayesian deep networks for {T}hompson sampling.
\newblock \emph{arXiv preprint arXiv:1802.09127}, 2018.

\bibitem[Ritter et~al.(2018)Ritter, Botev, and Barber]{ritter2018scalable}
Hippolyt Ritter, Aleksandar Botev, and David Barber.
\newblock A scalable {L}aplace approximation for neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[Rusu et~al.(2016)Rusu, Rabinowitz, Desjardins, Soyer, Kirkpatrick,
  Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}
Andrei~A. Rusu, Neil~C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
  Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
\newblock Progressive neural networks.
\newblock \emph{arXiv preprint arXiv:1606.04671}, 2016.

\bibitem[Saul et~al.(1996)Saul, Jaakkola, and Jordan]{saul1996mean}
Lawrence~K Saul, Tommi Jaakkola, and Michael~I Jordan.
\newblock Mean field theory for sigmoid belief networks.
\newblock \emph{Journal of Artificial Intelligence Research}, 4:\penalty0
  61--76, 1996.

\bibitem[Schwarz et~al.(2018)Schwarz, Luketina, Czarnecki, Grabska-Barwinska,
  Teh, Pascanu, and Hadsell]{schwarz2018progress}
Jonathan Schwarz, Jelena Luketina, Wojciech~M. Czarnecki, Agnieszka
  Grabska-Barwinska, Yee~Whye Teh, Razvan Pascanu, and Raia Hadsell.
\newblock Progress \& compress: A scalable framework for continual learning.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{sutskever2013importance}
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{International conference on machine learning}, pages
  1139--1147, 2013.

\bibitem[Swaroop et~al.(2019)Swaroop, Nguyen, Bui, and
  Turner]{swaroop2019improving}
Siddharth Swaroop, Cuong~V. Nguyen, Thang~D. Bui, and Richard~E. Turner.
\newblock Improving and understanding variational continual learning.
\newblock \emph{arXiv preprint arXiv:1905.02099}, 2019.

\bibitem[Tieleman and Hinton(2012)]{hintonTieleman}
Tijmen Tieleman and Geoffrey Hinton.
\newblock {Lecture 6.5-{R}MSprop: Divide the gradient by a running average of
  its recent magnitude.}
\newblock \emph{COURSERA: Neural Networks for Machine Learning 4}, 2012.

\bibitem[Vovk(1990)]{Vovk:1990:AS:92571.92672}
V.~G. Vovk.
\newblock Aggregating strategies.
\newblock In \emph{Proceedings of the Third Annual Workshop on Computational
  Learning Theory}, COLT '90, pages 371--386, San Francisco, CA, USA, 1990.
  Morgan Kaufmann Publishers Inc.
\newblock ISBN 1-55860-146-5.

\bibitem[Yu et~al.(2015)Yu, Zhang, Song, Seff, and Xiao]{yu2015lsun}
Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao.
\newblock {LSUN:} construction of a large-scale image dataset using deep
  learning with humans in the loop.
\newblock \emph{CoRR}, abs/1506.03365, 2015.

\bibitem[Zhang et~al.(2018)Zhang, Sun, Duvenaud, and Grosse]{zhang2018noisy}
Guodong Zhang, Shengyang Sun, David~K. Duvenaud, and Roger~B. Grosse.
\newblock Noisy natural gradient as variational inference.
\newblock \emph{arXiv preprint arXiv:1712.02390}, 2018.

\end{thebibliography}
