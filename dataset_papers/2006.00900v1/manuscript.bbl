\begin{thebibliography}{10}

\bibitem{MPPO}
A.~Abdolmaleki, J.~T. Springenberg, Y.~Tassa, R.~Munos, N.~Heess, and
  M.~Riedmiller.
\newblock Maximum a posteriori policy optimisation.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{HER}
M.~Andrychowicz, F.~Wolski, A.~Ray, J.~Schneider, R.~Fong, P.~Welinder,
  B.~McGrew, J.~Tobin, O.~Pieter~Abbeel, and W.~Zaremba.
\newblock Hindsight experience replay.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems 30}, pages 5048--5058. Curran Associates, Inc., 2017.

\bibitem{wgan}
M.~Arjovsky, S.~Chintala, and L.~Bottou.
\newblock Wasserstein gan.
\newblock {\em arXiv:1701.07875}, 2017.

\bibitem{GATS}
K.~Azizzadenesheli, B.~Yang, W.~Liu, E.~Brunskill, Z.~C. Lipton, and
  A.~Anandkumar.
\newblock Sample-efficient deep {RL} with generative adversarial tree search.
\newblock {\em CoRR}, abs/1806.05780, 2018.

\bibitem{agent57}
A.~P. Badia, B.~Piot, S.~Kapturowski, P.~Sprechmann, A.~Vitvitskyi, D.~Guo, and
  C.~Blundell.
\newblock Agent57: Outperforming the atari human benchmark, 2020.

\bibitem{d4pg}
G.~Barth-Maron, M.~W. Hoffman, D.~Budden, W.~Dabney, D.~Horgan, D.~TB,
  A.~Muldal, N.~Heess, and T.~Lillicrap.
\newblock Distributional policy gradients.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{countexploration}
M.~Bellemare, S.~Srinivasan, G.~Ostrovski, T.~Schaul, D.~Saxton, and R.~Munos.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In D.~D. Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett,
  editors, {\em Advances in Neural Information Processing Systems 29}, pages
  1471--1479. Curran Associates, Inc., 2016.

\bibitem{STEVE}
J.~Buckman, D.~Hafner, G.~Tucker, E.~Brevdo, and H.~Lee.
\newblock Sample-efficient reinforcement learning with stochastic ensemble
  value expansion.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, {\em Advances in Neural Information Processing
  Systems 31}, pages 8224--8234. Curran Associates, Inc., 2018.

\bibitem{RND}
Y.~Burda, H.~Edwards, A.~Storkey, and O.~Klimov.
\newblock Exploration by random network distillation.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{baselines}
P.~Dhariwal, C.~Hesse, O.~Klimov, A.~Nichol, M.~Plappert, A.~Radford,
  J.~Schulman, S.~Sidor, Y.~Wu, and P.~Zhokhov.
\newblock Openai baselines.
\newblock \url{https://github.com/openai/baselines}, 2017.

\bibitem{curriculumHER}
M.~Fang, T.~Zhou, Y.~Du, L.~Han, and Z.~Zhang.
\newblock Curriculum-guided hindsight experience replay.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  12623--12634. Curran Associates, Inc., 2019.

\bibitem{goalgan}
C.~Florensa, D.~Held, X.~Geng, and P.~Abbeel.
\newblock Automatic goal generation for reinforcement learning agents, 2017.

\bibitem{noRLGC}
D.~Ghosh, A.~Gupta, J.~Fu, A.~Reddy, C.~Devin, B.~Eysenbach, and S.~Levine.
\newblock Learning to reach goals without reinforcement learning.
\newblock abs/1912.06088, 2019.

\bibitem{GANS}
I.~J. Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio.
\newblock Generative adversarial networks, 2014.

\bibitem{dreamer}
D.~Hafner, T.~Lillicrap, J.~Ba, and M.~Norouzi.
\newblock Dream to control: Learning behaviors by latent imagination.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{hafner2019planet}
D.~Hafner, T.~Lillicrap, I.~Fischer, R.~Villegas, D.~Ha, H.~Lee, and
  J.~Davidson.
\newblock Learning latent dynamics for planning from pixels.
\newblock In {\em International Conference on Machine Learning}, pages
  2555--2565, 2019.

\bibitem{SHER}
Q.~He, L.~Zhuang, and H.~Li.
\newblock Soft hindsight experience replay, 2020.

\bibitem{simple}
L.~Kaiser, M.~Babaeizadeh, P.~Milos, B.~Osinski, R.~H. Campbell, K.~Czechowski,
  D.~Erhan, C.~Finn, P.~Kozakowski, S.~Levine, A.~Mohiuddin, R.~Sepassi,
  G.~Tucker, and H.~Michalewski.
\newblock Model-based reinforcement learning for atari, 2019.

\bibitem{modelensembleTRPO}
T.~Kurutach, I.~Clavera, Y.~Duan, A.~Tamar, and P.~Abbeel.
\newblock Model-ensemble trust-region policy optimization.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{MPC}
D.~Liberzon.
\newblock {\em Calculus of Variations and Optimal Control Theory: A Concise
  Introduction}.
\newblock Princeton University Press, 2012.

\bibitem{ddpg}
T.~P. Lillicrap, J.~J. Hunt, A.~Pritzel, N.~Heess, T.~Erez, Y.~Tassa,
  D.~Silver, and D.~Wierstra.
\newblock Continuous control with deep reinforcement learning, 2015.

\bibitem{CER}
H.~Liu, A.~Trott, R.~Socher, and C.~Xiong.
\newblock Competitive experience replay.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{spectralnorm}
T.~Miyato, T.~Kataoka, M.~Koyama, and Y.~Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock {\em ICLR}, 2018.

\bibitem{atari2013}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~Graves, I.~Antonoglou, D.~Wierstra, and
  M.~Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock In {\em NIPS Deep Learning Workshop}. 2013.

\bibitem{DQN}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, S.~Petersen,
  C.~Beattie, A.~Sadik, I.~Antonoglou, H.~King, D.~Kumaran, D.~Wierstra,
  S.~Legg, and D.~Hassabis.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529--533, 2015.

\bibitem{nn_model}
A.~Nagabandi, G.~Kahn, R.~Fearing, and S.~Levine.
\newblock Neural network dynamics for model-based deep reinforcement learning
  with model-free fine-tuning.
\newblock pages 7559--7566, 05 2018.

\bibitem{PDDM}
A.~Nagabandi, K.~Konoglie, S.~Levine, and V.~Kumar.
\newblock {Deep Dynamics Models for Learning Dexterous Manipulation}.
\newblock In {\em Conference on Robot Learning (CoRL)}, 2019.

\bibitem{nair2018visual}
A.~Nair, V.~Pong, M.~Dalal, S.~Bahl, S.~Lin, and S.~Levine.
\newblock Visual reinforcement learning with imagined goals, 2018.

\bibitem{nasiriany2019planning}
S.~Nasiriany, V.~Pong, S.~Lin, and S.~Levine.
\newblock Planning with goal-conditioned policies.
\newblock {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{pathak_curiosity}
D.~Pathak, P.~Agrawal, A.~A. Efros, and T.~Darrell.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In {\em ICML}, 2017.

\bibitem{TDMs}
V.~Pong*, S.~Gu*, M.~Dalal, and S.~Levine.
\newblock Temporal difference models: Model-free deep {RL} for model-based
  control.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{I2A}
S.~Racani\`{e}re, T.~Weber, D.~P. Reichert, L.~Buesing, A.~Guez, D.~Rezende,
  A.~P. Badia, O.~Vinyals, N.~Heess, Y.~Li, R.~Pascanu, P.~Battaglia,
  D.~Hassabis, D.~Silver, and D.~Wierstra.
\newblock Imagination-augmented agents for deep reinforcement learning.
\newblock In {\em Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, NIPS’17, page 5694–5705, Red Hook, NY,
  USA, 2017. Curran Associates Inc.

\bibitem{DAPG}
A.~Rajeswaran*, V.~Kumar*, A.~Gupta, G.~Vezzani, J.~Schulman, E.~Todorov, and
  S.~Levine.
\newblock {Learning Complex Dexterous Manipulation with Deep Reinforcement
  Learning and Demonstrations}.
\newblock In {\em Proceedings of Robotics: Science and Systems (RSS)}, 2018.

\bibitem{muzero}
J.~Schrittwieser, I.~Antonoglou, T.~Hubert, K.~Simonyan, L.~Sifre, S.~Schmitt,
  A.~Guez, E.~Lockhart, D.~Hassabis, T.~Graepel, T.~Lillicrap, and D.~Silver.
\newblock Mastering atari, go, chess and shogi by planning with a learned
  model, 2019.

\bibitem{alphago}
D.~Silver, A.~Huang, C.~J. Maddison, A.~Guez, L.~Sifre, G.~van~den Driessche,
  J.~Schrittwieser, I.~Antonoglou, V.~Panneershelvam, M.~Lanctot, S.~Dieleman,
  D.~Grewe, J.~Nham, N.~Kalchbrenner, I.~Sutskever, T.~Lillicrap, M.~Leach,
  K.~Kavukcuoglu, T.~Graepel, and D.~Hassabis.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em Nature}, 529:484--503, 2016.

\bibitem{alphazero}
D.~Silver, T.~Hubert, J.~Schrittwieser, I.~Antonoglou, M.~Lai, A.~Guez,
  M.~Lanctot, L.~Sifre, D.~Kumaran, T.~Graepel, T.~Lillicrap, K.~Simonyan, and
  D.~Hassabis.
\newblock A general reinforcement learning algorithm that masters chess, shogi,
  and go through self-play.
\newblock {\em Science}, 362(6419):1140--1144, 2018.

\bibitem{curl}
A.~Srinivas, M.~Laskin, and P.~Abbeel.
\newblock Curl: Contrastive unsupervised representations for reinforcement
  learning, 2020.

\bibitem{mujoco}
E.~Todorov, T.~Erez, and Y.~Tassa.
\newblock Mujoco: A physics engine for model-based control, 2012.

\bibitem{mppioriginal}
G.~Williams, B.~Goldfain, P.~Drews, K.~Saigol, J.~Rehg, and E.~Theodorou.
\newblock Robust sampling based model predictive control with sparse objective
  information.
\newblock 06 2018.

\bibitem{energyHER}
R.~Zhao and V.~Tresp.
\newblock Energy-based hindsight experience prioritization.
\newblock In A.~Billard, A.~Dragan, J.~Peters, and J.~Morimoto, editors, {\em
  Proceedings of The 2nd Conference on Robot Learning}, volume~87 of {\em
  Proceedings of Machine Learning Research}, pages 113--122. PMLR, 29--31 Oct
  2018.

\end{thebibliography}
