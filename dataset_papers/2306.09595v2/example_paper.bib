@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}


@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = CVPR,
pages = {234--778},
year = 2005
}


@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@inproceedings{
    zhang2021personalized,
    title={Personalized Federated Learning with First Order Model Optimization},
    author={Michael Zhang and Karan Sapra and Sanja Fidler and Serena Yeung and Jose M. Alvarez},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=ehJqJQk9cw}
}


@InProceedings{esfandiari2021cross,
  title =    {Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data},
  author =       {Esfandiari, Yasaman and Tan, Sin Yong and Jiang, Zhanhong and Balu, Aditya and Herron, Ethan and Hegde, Chinmay and Sarkar, Soumik},
  booktitle =    {Proceedings of the 38th International Conference on Machine Learning},
  pages =    {3036--3046},
  year =   {2021},
  editor =   {Meila, Marina and Zhang, Tong},
  volume =   {139},
  series =   {Proceedings of Machine Learning Research},
  month =    {18--24 Jul},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v139/esfandiari21a/esfandiari21a.pdf},
  url =    {https://proceedings.mlr.press/v139/esfandiari21a.html},
  abstract =   {Decentralized learning enables a group of collaborative agents to learn models using a distributed dataset without the need for a central parameter server. Recently, decentralized learning algorithms have demonstrated state-of-the-art results on benchmark data sets, comparable with centralized algorithms. However, the key assumption to achieve competitive performance is that the data is independently and identically distributed (IID) among the agents which, in real-life applications, is often not applicable. Inspired by ideas from continual learning, we propose Cross-Gradient Aggregation (CGA), a novel decentralized learning algorithm where (i) each agent aggregates cross-gradient information, i.e., derivatives of its model with respect to its neighbors’ datasets, and (ii) updates its model using a projected gradient based on quadratic programming (QP). We theoretically analyze the convergence characteristics of CGA and demonstrate its efficiency on non-IID data distributions sampled from the MNIST and CIFAR-10 datasets. Our empirical comparisons show superior learning performance of CGA over existing state-of-the-art decentralized learning algorithms, as well as maintaining the improved performance under information compression to reduce peer-to-peer communication overhead. The code is available here on GitHub.}
}

@inproceedings{NIPS2017_f7552665,
 author = {Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent},
 url = {https://proceedings.neurips.cc/paper/2017/file/f75526659f31040afeb61cb7133e4e6d-Paper.pdf},
 volume = {30},
 year = {2017}
}
@inproceedings{li2021ditto,
    title={Ditto: Fair and robust federated learning through personalization},
    author={Li, Tian and Hu, Shengyuan and Beirami, Ahmad and Smith, Virginia},
    booktitle={International Conference on Machine Learning},
    pages={6357--6368},
    year={2021},
    organization={PMLR}
}
@inproceedings{NIPS2017_a74c3bae,
    author = {Jiang, Zhanhong and Balu, Aditya and Hegde, Chinmay and Sarkar, Soumik},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Collaborative Deep Learning in Fixed Topology Networks},
    url = {https://proceedings.neurips.cc/paper/2017/file/a74c3bae3e13616104c1b25f9da1f11f-Paper.pdf},
    volume = {30},
    year = {2017}
}
@article{kingma2014adam,
    title={Adam: A method for stochastic optimization},
    author={Kingma, Diederik P and Ba, Jimmy},
    journal={arXiv preprint arXiv:1412.6980},
    year={2014}
}
@article{krizhevsky2009learning,
    title={Learning multiple layers of features from tiny images},
    author={Krizhevsky, Alex and Hinton, Geoffrey and others},
    year={2009},
    publisher={Citeseer}
}
@article{blot2016gossip,
  title={Gossip training for deep learning},
  author={Blot, Michael and Picard, David and Cord, Matthieu and Thome, Nicolas},
  journal={arXiv preprint arXiv:1611.09726},
  year={2016}
}
@inproceedings{hsieh2020non,
  title={The non-iid data quagmire of decentralized machine learning},
  author={Hsieh, Kevin and Phanishayee, Amar and Mutlu, Onur and Gibbons, Phillip},
  booktitle={International Conference on Machine Learning},
  pages={4387--4398},
  year={2020},
  organization={PMLR}
}

@InProceedings{lin2021quasi,
  title =    {Quasi-global Momentum: Accelerating Decentralized Deep Learning on Heterogeneous Data},
  author =       {Lin, Tao and Karimireddy, Sai Praneeth and Stich, Sebastian and Jaggi, Martin},
  booktitle =    {Proceedings of the 38th International Conference on Machine Learning},
  pages =    {6654--6665},
  year =   {2021},
  editor =   {Meila, Marina and Zhang, Tong},
  volume =   {139},
  series =   {Proceedings of Machine Learning Research},
  month =    {18--24 Jul},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v139/lin21c/lin21c.pdf},
  url =    {https://proceedings.mlr.press/v139/lin21c.html},
  abstract =   {Decentralized training of deep learning models is a key element for enabling data privacy and on-device learning over networks. In realistic learning scenarios, the presence of heterogeneity across different clients’ local datasets poses an optimization challenge and may severely deteriorate the generalization performance. In this paper, we investigate and identify the limitation of several decentralized optimization algorithms for different degrees of data heterogeneity. We propose a novel momentum-based method to mitigate this decentralized training difficulty. We show in extensive empirical experiments on various CV/NLP datasets (CIFAR-10, ImageNet, and AG News) and several network topologies (Ring and Social Network) that our method is much more robust to the heterogeneity of clients’ data than other existing methods, by a significant improvement in test performance (1%-20%).}
}


@article{khawatmi2017decentralized,
  title={Decentralized clustering and linking by networked agents},
  author={Khawatmi, Sahar and Sayed, Ali H and Zoubir, Abdelhak M},
  journal={IEEE Transactions on Signal Processing},
  volume={65},
  number={13},
  pages={3526--3537},
  year={2017},
  publisher={IEEE}
}
@article{zhao2018federated,
  title={Federated learning with non-iid data},
  author={Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
  journal={arXiv preprint arXiv:1806.00582},
  year={2018}
}
@article{li2018federated,
  title={Federated optimization in heterogeneous networks},
  author={Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  journal={arXiv preprint arXiv:1812.06127},
  year={2018}
}
@inproceedings{
acar2021federated,
title={Federated Learning Based on Dynamic Regularization},
author={Durmus Alp Emre Acar and Yue Zhao and Ramon Matas and Matthew Mattina and Paul Whatmough and Venkatesh Saligrama},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=B7v4QMR6Z9w}
}

@article{xie2021multi,
  title={Multi-center federated learning},
  author={Xie, Ming and Long, Guodong and Shen, Tao and Zhou, Tianyi and Wang, Xianzhi and Jiang, Jing and Zhang, Chengqi},
  journal={arXiv preprint arXiv:2108.08647},
  year={2021}
}
@article{sattler2020clustered,
  title={Clustered federated learning: Model-agnostic distributed multitask optimization under privacy constraints},
  author={Sattler, Felix and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={IEEE transactions on neural networks and learning systems},
  year={2020},
  publisher={IEEE}
}
@article{ghosh2020efficient,
  title={An efficient framework for clustered federated learning},
  author={Ghosh, Avishek and Chung, Jichan and Yin, Dong and Ramchandran, Kannan},
  journal={arXiv preprint arXiv:2006.04088},
  year={2020}
}
@article{liang2020think,
  title={Think locally, act globally: Federated learning with local and global representations},
  author={Liang, Paul Pu and Liu, Terrance and Ziyin, Liu and Allen, Nicholas B and Auerbach, Randy P and Brent, David and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:2001.01523},
  year={2020}
}

@InProceedings{zhu2021data,
  title =    {Data-Free Knowledge Distillation for Heterogeneous Federated Learning},
  author =       {Zhu, Zhuangdi and Hong, Junyuan and Zhou, Jiayu},
  booktitle =    {Proceedings of the 38th International Conference on Machine Learning},
  pages =    {12878--12889},
  year =   {2021},
  editor =   {Meila, Marina and Zhang, Tong},
  volume =   {139},
  series =   {Proceedings of Machine Learning Research},
  month =    {18--24 Jul},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v139/zhu21b/zhu21b.pdf},
  url =    {https://proceedings.mlr.press/v139/zhu21b.html},
  abstract =   {Federated Learning (FL) is a decentralized machine-learning paradigm, in which a global server iteratively averages the model parameters of local users without accessing their data. User heterogeneity has imposed significant challenges to FL, which can incur drifted global models that are slow to converge. Knowledge Distillation has recently emerged to tackle this issue, by refining the server model using aggregated knowledge from heterogeneous users, other than directly averaging their model parameters. This approach, however, depends on a proxy dataset, making it impractical unless such a prerequisite is satisfied. Moreover, the ensemble knowledge is not fully utilized to guide local model learning, which may in turn affect the quality of the aggregated model. Inspired by the prior art, we propose a data-free knowledge distillation approach to address heterogeneous FL, where the server learns a lightweight generator to ensemble user information in a data-free manner, which is then broadcasted to users, regulating local training using the learned knowledge as an inductive bias. Empirical studies powered by theoretical implications show that our approach facilitates FL with better generalization performance using fewer communication rounds, compared with the state-of-the-art.}
}

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}
@article{kipf2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}
@article{velivckovic2017graph,
  title={Graph attention networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1710.10903},
  year={2017}
}
@inproceedings{karimireddy2020scaffold,
  title={Scaffold: Stochastic controlled averaging for federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle={International Conference on Machine Learning},
  pages={5132--5143},
  year={2020},
  organization={PMLR}
}
@inproceedings{
Li2020Fair,
title={Fair Resource Allocation in Federated Learning},
author={Tian Li and Maziar Sanjabi and Ahmad Beirami and Virginia Smith},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=ByexElSYDr}
}
@inproceedings{NEURIPS2020_18df51b9,
 author = {Lin, Tao and Kong, Lingjing and Stich, Sebastian U and Jaggi, Martin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {2351--2363},
 publisher = {Curran Associates, Inc.},
 title = {Ensemble Distillation for Robust Model Fusion in Federated Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/18df51b97ccd68128e994804f3eccc87-Paper.pdf},
 volume = {33},
 year = {2020}
}


@InProceedings{pmlr-v139-fraboni21a,
  title =    {Clustered Sampling: Low-Variance and Improved Representativity for Clients Selection in Federated Learning},
  author =       {Fraboni, Yann and Vidal, Richard and Kameni, Laetitia and Lorenzi, Marco},
  booktitle =    {Proceedings of the 38th International Conference on Machine Learning},
  pages =    {3407--3416},
  year =   {2021},
  editor =   {Meila, Marina and Zhang, Tong},
  volume =   {139},
  series =   {Proceedings of Machine Learning Research},
  month =    {18--24 Jul},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v139/fraboni21a/fraboni21a.pdf},
  url =    {https://proceedings.mlr.press/v139/fraboni21a.html},
  abstract =   {This work addresses the problem of optimizing communications between server and clients in federated learning (FL). Current sampling approaches in FL are either biased, or non optimal in terms of server-clients communications and training stability. To overcome this issue, we introduce clustered sampling for clients selection. We prove that clustered sampling leads to better clients representatitivity and to reduced variance of the clients stochastic aggregation weights in FL. Compatibly with our theory, we provide two different clustering approaches enabling clients aggregation based on 1) sample size, and 2) models similarity. Through a series of experiments in non-iid and unbalanced scenarios, we demonstrate that model aggregation through clustered sampling consistently leads to better training convergence and variability when compared to standard sampling approaches. Our approach does not require any additional operation on the clients side, and can be seamlessly integrated in standard FL implementations. Finally, clustered sampling is compatible with existing methods and technologies for privacy enhancement, and for communication reduction through model compression.}
}
@inproceedings{
chen2021fedbe,
title={Fed{\{}BE{\}}: Making Bayesian Model Ensemble Applicable to Federated Learning},
author={Hong-You Chen and Wei-Lun Chao},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=dgtpE6gKjHn}
}
@inproceedings{
Wang2020Federated,
title={Federated Learning with Matched Averaging},
author={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BkluqlSFDS}
}

@inproceedings{NEURIPS2020_f4f1f13c,
 author = {T. Dinh, Canh and Tran, Nguyen and Nguyen, Josh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {21394--21405},
 publisher = {Curran Associates, Inc.},
 title = {Personalized Federated Learning with Moreau Envelopes},
 url = {https://proceedings.neurips.cc/paper/2020/file/f4f1f13c8289ac1b1ee0ff176b56fc60-Paper.pdf},
 volume = {33},
 year = {2020}
}
@inproceedings{
li2021fedbn,
title={Fed{BN}: Federated Learning on Non-{IID} Features via Local Batch Normalization},
author={Xiaoxiao Li and Meirui JIANG and Xiaofei Zhang and Michael Kamp and Qi Dou},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=6YEQUn0QICG}
}

@InProceedings{pmlr-v139-collins21a,
  title =    {Exploiting Shared Representations for Personalized Federated Learning},
  author =       {Collins, Liam and Hassani, Hamed and Mokhtari, Aryan and Shakkottai, Sanjay},
  booktitle =    {Proceedings of the 38th International Conference on Machine Learning},
  pages =    {2089--2099},
  year =   {2021},
  editor =   {Meila, Marina and Zhang, Tong},
  volume =   {139},
  series =   {Proceedings of Machine Learning Research},
  month =    {18--24 Jul},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v139/collins21a/collins21a.pdf},
  url =    {https://proceedings.mlr.press/v139/collins21a.html},
  abstract =   {Deep neural networks have shown the ability to extract universal feature representations from data such as images and text that have been useful for a variety of learning tasks. However, the fruits of representation learning have yet to be fully-realized in federated settings. Although data in federated settings is often non-i.i.d. across clients, the success of centralized deep learning suggests that data often shares a global {\em feature representation}, while the statistical heterogeneity across clients or tasks is concentrated in the {\em labels}. Based on this intuition, we propose a novel federated learning framework and algorithm for learning a shared data representation across clients and unique local heads for each client. Our algorithm harnesses the distributed computational power across clients to perform many local-updates with respect to the low-dimensional local parameters for every update of the representation. We prove that this method obtains linear convergence to the ground-truth representation with near-optimal sample complexity in a linear setting, demonstrating that it can efficiently reduce the problem dimension for each client. Further, we provide extensive experimental results demonstrating the improvement of our method over alternative personalized federated learning approaches in heterogeneous settings.}
}
@article{fallah2020personalized,
  title={Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach},
  author={Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@InProceedings{pmlr-v139-shamsian21a,
  title =    {Personalized Federated Learning using Hypernetworks},
  author =       {Shamsian, Aviv and Navon, Aviv and Fetaya, Ethan and Chechik, Gal},
  booktitle =    {Proceedings of the 38th International Conference on Machine Learning},
  pages =    {9489--9502},
  year =   {2021},
  editor =   {Meila, Marina and Zhang, Tong},
  volume =   {139},
  series =   {Proceedings of Machine Learning Research},
  month =    {18--24 Jul},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v139/shamsian21a/shamsian21a.pdf},
  url =    {https://proceedings.mlr.press/v139/shamsian21a.html},
  abstract =   {Personalized federated learning is tasked with training machine learning models for multiple clients, each with its own data distribution. The goal is to train personalized models collaboratively while accounting for data disparities across clients and reducing communication costs. We propose a novel approach to this problem using hypernetworks, termed pFedHN for personalized Federated HyperNetworks. In this approach, a central hypernetwork model is trained to generate a set of models, one model for each client. This architecture provides effective parameter sharing across clients while maintaining the capacity to generate unique and diverse personal models. Furthermore, since hypernetwork parameters are never transmitted, this approach decouples the communication cost from the trainable model size. We test pFedHN empirically in several personalized federated learning challenges and find that it outperforms previous methods. Finally, since hypernetworks share information across clients, we show that pFedHN can generalize better to new clients whose distributions differ from any client observed during training.}
}
@article{li2019fair,
  title={Fair resource allocation in federated learning},
  author={Li, Tian and Sanjabi, Maziar and Beirami, Ahmad and Smith, Virginia},
  journal={arXiv preprint arXiv:1905.10497},
  year={2019}
}
@inproceedings{ravi2017optimization,
    title={Optimization as a model for few-shot learning},
    author={Ravi, Sachin and Larochelle, Hugo},
    booktitle={International Conference on Learning Representations},
    year={2017}
}
@article{osti_10137607,
place = {Country unknown/Code not available}, title = {Cooperative SGD: A Unified Framework for the Design and Analysis of Communication-Efficient SGD Algorithms}, url = {https://par.nsf.gov/biblio/10137607}, abstractNote = {Communication-efficient SGD algorithms, which allow nodes to perform local updates and periodically synchronize local models, are highly effective in improving the speed and scalability of distributed SGD. However, a rigorous convergence analysis and comparative study of different communication-reduction strategies remains a largely open problem. This paper presents a unified framework called Cooperative SGD that subsumes existing communication-efficient SGD algorithms such as periodic-averaging, elastic-averaging, and decentralized SGD. By analyzing Cooperative SGD, we provide novel convergence guarantees for existing algorithms. Moreover, this framework enables us to design new communication-efficient SGD algorithms that strike the best balance between reducing communication overhead and achieving fast error convergence with a low error floor.}, journal = {ICML Workshop on Coding Theory for Machine Learning}, author = {Wang, Jianyu and Joshi, Gauri}, }


@InProceedings{pmlr-v37-ioffe15,
  title =    {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author =   {Ioffe, Sergey and Szegedy, Christian},
  booktitle =    {Proceedings of the 32nd International Conference on Machine Learning},
  pages =    {448--456},
  year =   {2015},
  editor =   {Bach, Francis and Blei, David},
  volume =   {37},
  series =   {Proceedings of Machine Learning Research},
  address =    {Lille, France},
  month =    {07--09 Jul},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v37/ioffe15.pdf},
  url =    {https://proceedings.mlr.press/v37/ioffe15.html},
  abstract =   {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.}
}
@InProceedings{Wu_2018_ECCV,
author = {Wu, Yuxin and He, Kaiming},
title = {Group Normalization},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}
@inproceedings{NEURIPS2019_e58cc5ca,
 author = {Shu, Jun and Xie, Qi and Yi, Lixuan and Zhao, Qian and Zhou, Sanping and Xu, Zongben and Meng, Deyu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting},
 url = {https://proceedings.neurips.cc/paper/2019/file/e58cc5ca94270acaceed13bc82dfedf7-Paper.pdf},
 volume = {32},
 year = {2019}
}
@article{gibiansky2017bringing,
  title={Bringing HPC techniques to deep learning},
  author={Gibiansky, Andrew},
  journal={Baidu Research, Tech. Rep.},
  year={2017}
}
@inproceedings{
balakrishnan2022diverse,
title={Diverse Client Selection for Federated Learning via Submodular Maximization},
author={Ravikumar Balakrishnan and Tian Li and Tianyi Zhou and Nageen Himayat and Virginia Smith and Jeff Bilmes},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nwKXyFvaUm}
}
@inproceedings{tan2022fedproto,
  title={FedProto: Federated Prototype Learning across Heterogeneous Clients},
  author={Tan, Yue and Long, Guodong and Liu, Lu and Zhou, Tianyi and Lu, Qinghua and Jiang, Jing and Zhang, Chengqi},
  booktitle={AAAI Conference on Artificial Intelligence},
  volume={1},
  year={2022}
}
@article{liu2019learning,
  title={Learning to propagate for graph meta-learning},
  author={Liu, Lu and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Zhang, Chengqi},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{zhou2014multi,
  title={Multi-task copula by sparse graph regression},
  author={Zhou, Tianyi and Tao, Dacheng},
  booktitle={Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={771--780},
  year={2014}
}

@inproceedings{
xu2022acceleration,
title={Acceleration of Federated Learning with Alleviated Forgetting in Local Training},
author={Chencheng Xu and Zhiwei Hong and Minlie Huang and Tao Jiang},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=541PxiEKN3F}
}

@inproceedings{
oh2022fedbabu,
title={Fed{BABU}: Toward Enhanced Representation for Federated Image Classification},
author={Jaehoon Oh and SangMook Kim and Se-Young Yun},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=HuaYQfggn5u}
}


@inproceedings{
afonin2022towards,
title={Towards Model Agnostic Federated Learning Using Knowledge Distillation},
author={Andrei Afonin and Sai Praneeth Karimireddy},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=lQI_mZjvBxj}
}

@inproceedings{DBLP:conf/icml/Dai0H0T22,
  author    = {Rong Dai and
               Li Shen and
               Fengxiang He and
               Xinmei Tian and
               Dacheng Tao},
  editor    = {Kamalika Chaudhuri and
               Stefanie Jegelka and
               Le Song and
               Csaba Szepesv{\'{a}}ri and
               Gang Niu and
               Sivan Sabato},
  title     = {DisPFL: Towards Communication-Efficient Personalized Federated Learning
               via Decentralized Sparse Training},
  booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
               2022, Baltimore, Maryland, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  pages     = {4587--4604},
  publisher = {{PMLR}},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v162/dai22b.html},
  timestamp = {Tue, 12 Jul 2022 17:36:52 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/Dai0H0T22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
tan2022federated,
title={Federated Learning from Pre-Trained Models: A Contrastive Learning Approach},
author={Yue Tan and Guodong Long and Jie Ma and Lu Liu and Tianyi Zhou and Jing Jiang},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=mhQLcMjWw75}
}

@inproceedings{DBLP:conf/icml/HuangSZYX22,
  author    = {Yan Huang and
               Ying Sun and
               Zehan Zhu and
               Changzhi Yan and
               Jinming Xu},
  editor    = {Kamalika Chaudhuri and
               Stefanie Jegelka and
               Le Song and
               Csaba Szepesv{\'{a}}ri and
               Gang Niu and
               Sivan Sabato},
  title     = {Tackling Data Heterogeneity: {A} New Unified Framework for Decentralized
               {SGD} with Sample-induced Topology},
  booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
               2022, Baltimore, Maryland, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  pages     = {9310--9345},
  publisher = {{PMLR}},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v162/huang22i.html},
  timestamp = {Wed, 13 Jul 2022 16:58:13 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/HuangSZYX22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
yuan2022revisiting,
title={Revisiting Optimal Convergence Rate for Smooth and Non-convex Stochastic Decentralized Optimization},
author={Kun Yuan and Xinmeng Huang and Yiming Chen and Xiaohan Zhang and Yingya Zhang and Pan Pan},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=eHePKMLuNmy}
}

@inproceedings{
song2022communicationefficient,
title={Communication-Efficient Topologies for Decentralized Learning with \$O(1)\$ Consensus Rate},
author={Zhuoqing Song and Weijian Li and Kexin Jin and Lei Shi and Ming Yan and Wotao Yin and Kun Yuan},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=AyiiHcRzTd}
}

@inproceedings{
vogels2022beyond,
title={Beyond spectral gap: the role of the topology in decentralized learning},
author={Thijs Vogels and Hadrien Hendrikx and Martin Jaggi},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=AQgmyyEWg8}
}

@book{koller2009probabilistic,
  title={Probabilistic graphical models: principles and techniques},
  author={Koller, Daphne and Friedman, Nir},
  year={2009}
}

@article{caruana1997multitask,
  title={Multitask learning},
  author={Caruana, Rich},
  journal={Machine learning},
  volume={28},
  number={1},
  pages={41--75},
  year={1997},
  publisher={Springer}
}

@inproceedings{lu2022decentralized,
  title={Decentralized Bilevel Optimization for Personalized Client Learning},
  author={Lu, Songtao and Cui, Xiaodong and Squillante, Mark S and Kingsbury, Brian and Horesh, Lior},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5543--5547},
  year={2022},
  organization={IEEE}
}

@article{belkin2006manifold,
  title={Manifold regularization: A geometric framework for learning from labeled and unlabeled examples.},
  author={Belkin, Mikhail and Niyogi, Partha and Sindhwani, Vikas},
  journal={Journal of machine learning research},
  volume={7},
  number={11},
  year={2006}
}

@article{holland1983stochastic,
  title={Stochastic blockmodels: First steps},
  author={Holland, Paul W and Laskey, Kathryn Blackmond and Leinhardt, Samuel},
  journal={Social networks},
  volume={5},
  number={2},
  pages={109--137},
  year={1983},
  publisher={Elsevier}
}

@article{DBLP:journals/ml/JordanGJS99,
  author    = {Michael I. Jordan and
               Zoubin Ghahramani and
               Tommi S. Jaakkola and
               Lawrence K. Saul},
  title     = {An Introduction to Variational Methods for Graphical Models},
  journal   = {Mach. Learn.},
  volume    = {37},
  number    = {2},
  pages     = {183--233},
  year      = {1999},
  url       = {https://doi.org/10.1023/A:1007665907178},
  doi       = {10.1023/A:1007665907178},
  timestamp = {Mon, 02 Mar 2020 16:28:57 +0100},
  biburl    = {https://dblp.org/rec/journals/ml/JordanGJS99.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{li2022learning,
  title={Learning to Collaborate in Decentralized Learning of Personalized Models},
  author={Li, Shuangtong and Zhou, Tianyi and Tian, Xinmei and Tao, Dacheng},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022}
}

@inproceedings{DualPersonalization,
  title={Dual Personalization on Federated Recommendation},
  author={Chunxu Zhang and Guodong Long and Tianyi Zhou and Peng Yan and Zijian Zhang and Chengqi Zhang and Bo Yang},
  booktitle={International Joint Conference on Artificial Intelligence (IJCAI)},
  year={2023}
}

@inproceedings{Personalized_Federated_Learning_With_Structural_Information,
  title={Personalized Federated Learning With Structural Information},
  author={Fengwen Chen and Guodong Long and Zonghan Wu and Tianyi Zhou and Jing Jiang},
  booktitle={International Joint Conference on Artificial Intelligence (IJCAI)},
  year={2022}
}

@article{Multi-Center_Federated_Learning,
  title={Multi-Center Federated Learning: clients clustering for better personalization},
  author={Guodong Long and Ming Xie and Tao Shen and Tianyi Zhou and Xianzhi Wang and Jing Jiang},
  journal={World Wide Web Journal (Springer)},
  year={2022}
}

@inproceedings{zantedeschi2020fully,
  title={Fully decentralized joint learning of personalized models and collaboration graphs},
  author={Zantedeschi, Valentina and Bellet, Aur{\'e}lien and Tommasi, Marc},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={864--874},
  year={2020},
  organization={PMLR}
}
