\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Avron et~al.(2017)Avron, Clarkson, and Woodruff]{avron2017sharper}
Haim Avron, Kenneth~L. Clarkson, and David~P. Woodruff.
\newblock {Sharper Bounds for Regularized Data Fitting}.
\newblock In \emph{Approximation, Randomization, and Combinatorial
  Optimization. Algorithms and Techniques}, volume~81, pages 27:1--27:22,
  Dagstuhl, Germany, 2017. Schloss Dagstuhl.

\bibitem[Charikar et~al.(2004)Charikar, Chen, and
  Farach-Colton]{charikar2004finding}
Moses Charikar, Kevin Chen, and Martin Farach-Colton.
\newblock {Finding Frequent Items in Data Streams}.
\newblock \emph{Theoretical Computer Science}, 312\penalty0 (1):\penalty0
  3--15, 2004.

\bibitem[Clarkson and Woodruff(2013)]{clarkson2013low}
Kenneth~L. Clarkson and David~P. Woodruff.
\newblock {Low Rank Approximation and Regression in Input Sparsity Time}.
\newblock In \emph{Annual ACM Symposium on Theory of Computing (STOC)}, 2013.

\bibitem[Derezinski and Warmuth(2017)]{derezinski2017unbiased}
Michal Derezinski and Manfred~K. Warmuth.
\newblock Unbiased estimates for linear regression via volume sampling.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[Derezinski and Warmuth(2018)]{derezinski2018subsampling}
Michal Derezinski and Manfred~K. Warmuth.
\newblock Subsampling for ridge regression via regularized volume sampling.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2018.

\bibitem[Drineas and Mahoney(2016)]{drineas2016randnla}
Petros Drineas and Michael~W. Mahoney.
\newblock {RandNLA: Randomized Numerical Linear Algebra}.
\newblock \emph{Communications of the ACM}, 59\penalty0 (6):\penalty0 80--90,
  2016.

\bibitem[Drineas et~al.(2006{\natexlab{a}})Drineas, Kannan, and
  Mahoney]{drineas06fastmonte1}
Petros Drineas, Ravi Kannan, and Michael~W. Mahoney.
\newblock {Fast Monte Carlo Algorithms for Matrices I: Approximating Matrix
  Multiplication}.
\newblock \emph{SIAM Journal on Computing}, 36\penalty0 (1):\penalty0 132--157,
  2006{\natexlab{a}}.

\bibitem[Drineas et~al.(2006{\natexlab{b}})Drineas, Mahoney, and
  Muthukrishnan]{drineas2006sampling}
Petros Drineas, Michael~W. Mahoney, and S.~Muthukrishnan.
\newblock {Sampling Algorithms for $\ell_2$ Regression and Applications}.
\newblock In \emph{Annual ACM-SIAM Symposium on Discrete Algorithm (SODA)},
  2006{\natexlab{b}}.

\bibitem[Drineas et~al.(2008)Drineas, Mahoney, and
  Muthukrishnan]{drineas2008cur}
Petros Drineas, Michael~W. Mahoney, and S.~Muthukrishnan.
\newblock {Relative-Error CUR Matrix Decompositions}.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 30\penalty0
  (2):\penalty0 844--881, September 2008.

\bibitem[Drineas et~al.(2011)Drineas, Mahoney, Muthukrishnan, and
  Sarl{\'o}s]{drineas2011faster}
Petros Drineas, Michael~W. Mahoney, S.~Muthukrishnan, and Tam{\'a}s Sarl{\'o}s.
\newblock {Faster Least Squares Approximation}.
\newblock \emph{Numerische Mathematik}, 117\penalty0 (2):\penalty0 219--249,
  2011.

\bibitem[Drineas et~al.(2012)Drineas, Magdon-Ismail, Mahoney, and
  Woodruff]{drineas2012fast}
Petros Drineas, Malik Magdon-Ismail, Michael~W. Mahoney, and David~P. Woodruff.
\newblock {Fast Approximation of Matrix Coherence and Statistical Leverage}.
\newblock \emph{Journal of Machine Learning Research}, 13:\penalty0 3441--3472,
  2012.

\bibitem[Gittens(2011)]{gittens2011spectral}
Alex Gittens.
\newblock {The Spectral Norm Error of the Naive {N}ystr\"om Extension}.
\newblock \emph{arXiv preprint arXiv:1110.5305}, 2011.

\bibitem[Hsu et~al.(2014)Hsu, Kakade, and Zhang]{hsu2014random}
Daniel Hsu, Sham~M. Kakade, and Tong Zhang.
\newblock Random design analysis of ridge regression.
\newblock \emph{Foundations of Computational Mathematics}, 14\penalty0
  (3):\penalty0 569--600, 2014.

\bibitem[Johnson and Lindenstrauss(1984)]{johnson1984extensions}
William~B. Johnson and Joram Lindenstrauss.
\newblock Extensions of {L}ipschitz mappings into a {H}ilbert space.
\newblock \emph{Contemporary Mathematics}, 26\penalty0 (189-206), 1984.

\bibitem[Lu et~al.(2013)Lu, Dhillon, Foster, and Ungar]{lu2013faster}
Yichao Lu, Paramveer Dhillon, Dean~P. Foster, and Lyle Ungar.
\newblock {Faster Ridge Regression via the Subsampled Randomized Hadamard
  Transform}.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2013.

\bibitem[Ma et~al.(2015)Ma, Mahoney, and Yu]{ma2014statistical}
Ping Ma, Michael~W. Mahoney, and Bin Yu.
\newblock {A Statistical Perspective on Algorithmic Leveraging}.
\newblock \emph{Journal of Machine Learning Research}, 16\penalty0
  (1):\penalty0 861--911, 2015.

\bibitem[Mahoney(2011)]{mahoney2011ramdomized}
Michael~W. Mahoney.
\newblock {Randomized Algorithms for Matrices and Data}.
\newblock \emph{Foundations and Trends in Machine Learning}, 3\penalty0
  (2):\penalty0 123--224, 2011.

\bibitem[Meng and Mahoney(2013)]{meng2013low}
Xiangrui Meng and Michael~W. Mahoney.
\newblock {Low-Distortion Subspace Embeddings in Input-Sparsity Time and
  Applications to Robust Linear Regression}.
\newblock In \emph{{Annual ACM Symposium on Theory of Computing (STOC)}}, 2013.

\bibitem[Nelson and Nguy{\^e}n(2013)]{nelson2013osnap}
John Nelson and Huy~L. Nguy{\^e}n.
\newblock {OSNAP: Faster Numerical Linear Algebra Algorithms via Sparser
  Subspace Embeddings}.
\newblock In \emph{IEEE Annual Symposium on Foundations of Computer Science
  (FOCS)}, 2013.

\bibitem[Patrascu and Thorup(2012)]{patrascu2012power}
Mihai Patrascu and Mikkel Thorup.
\newblock {The Power of Simple Tabulation-Based Hashing}.
\newblock \emph{Journal of the ACM}, 59\penalty0 (3), 2012.

\bibitem[Pham and Pagh(2013)]{pham2013fast}
Ninh Pham and Rasmus Pagh.
\newblock {Fast and Scalable Polynomial Kernels via Explicit Feature Maps}.
\newblock In \emph{ACM SIGKDD International Conference on Knowledge Discovery
  and Data Mining (KDD)}, 2013.

\bibitem[Pilanci and Wainwright(2015)]{pilanci2015iterative}
Mert Pilanci and Martin~J. Wainwright.
\newblock {Iterative Hessian Sketch: Fast and Accurate Solution Approximation
  for Constrained Least-Squares}.
\newblock \emph{Journal of Machine Learning Research}, pages 1--33, 2015.

\bibitem[Raskutti and Mahoney(2016)]{raskutti2015statistical}
Garvesh Raskutti and Michael~W. Mahoney.
\newblock {A Statistical Perspective on Randomized Sketching for Ordinary
  Least-Squares}.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (214):\penalty0 1--31, 2016.

\bibitem[Thanei et~al.(2017)Thanei, Heinze, and Meinshausen]{thanei2017random}
Gian-Andrea Thanei, Christina Heinze, and Nicolai Meinshausen.
\newblock {Random Projections For Large-Scale Regression}.
\newblock In \emph{{Big and Complex Data Analysis}}. Springer, 2017.

\bibitem[Tropp(2011)]{tropp2011improved}
Joel~A. Tropp.
\newblock {Improved Analysis of the Subsampled Randomized Hadamard Transform}.
\newblock \emph{Advances in Adaptive Data Analysis}, 3\penalty0
  (01n02):\penalty0 115--126, 2011.

\bibitem[Tu et~al.(2016)Tu, Roelofs, Venkataraman, and Recht]{tu2016large}
Stephen Tu, Rebecca Roelofs, Shivaram Venkataraman, and Benjamin Recht.
\newblock {Large Scale Kernel Learning using Block Coordinate Descent}.
\newblock \emph{arXiv preprint arXiv:1602.05310}, 2016.

\bibitem[Vershynin(2012)]{vershynin2010introduction}
Roman Vershynin.
\newblock \emph{{Introduction to the Non-Asymptotic Analysis of Random
  Matrices}}, pages 210--268.
\newblock Cambridge University Press, 2012.

\bibitem[Wang et~al.(2017{\natexlab{a}})Wang, Lee, Mahdavi, Kolar, and
  Srebro]{wang2016sketching}
Jialei Wang, Jason~D. Lee, Mehrdad Mahdavi, Mladen Kolar, and Nathan Srebro.
\newblock {Sketching Meets Random Projection in the Dual: a Provable Recovery
  Algorithm for Big and High-Dimensional Data}.
\newblock \emph{Electronic Journal of Statistics}, 11\penalty0 (2):\penalty0
  4896--4944, 2017{\natexlab{a}}.

\bibitem[Wang et~al.(2016{\natexlab{a}})Wang, Luo, and Zhang]{wang2016spsd}
Shusen Wang, Luo Luo, and Zhihua Zhang.
\newblock {SPSD Matrix Approximation via Column Selection: Theories,
  Algorithms, and Extensions}.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (49):\penalty0 1--49, 2016{\natexlab{a}}.

\bibitem[Wang et~al.(2016{\natexlab{b}})Wang, Zhang, and
  Zhang]{wang2015towards}
Shusen Wang, Zhihua Zhang, and Tong Zhang.
\newblock {Towards More Efficient SPSD Matrix Approximation and CUR Matrix
  Decomposition}.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (210):\penalty0 1--49, 2016{\natexlab{b}}.

\bibitem[Wang et~al.(2017{\natexlab{b}})Wang, Roosta-Khorasani, Xu, and
  Mahoney]{wang2017giant}
Shusen Wang, Farbod Roosta-Khorasani, Peng Xu, and Michael~W. Mahoney.
\newblock {GIANT: Globally Improved Approximate Newton Method for Distributed
  Optimization}.
\newblock \emph{arXiv preprint arXiv:1709.03528}, 2017{\natexlab{b}}.

\bibitem[Wang et~al.(2017{\natexlab{c}})Wang, Yu, and
  Singh]{wang2016computationally}
Yining Wang, Adams~Wei Yu, and Aarti Singh.
\newblock On computationally tractable selection of experiments in
  measurement-constrained regression models.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (143):\penalty0 1--41, 2017{\natexlab{c}}.

\bibitem[Weinberger et~al.(2009)Weinberger, Dasgupta, Langford, Smola, and
  Attenberg]{weinberger2009feature}
Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh
  Attenberg.
\newblock {Feature Hashing for Large Scale Multitask Learning}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2009.

\bibitem[Woodruff(2014)]{woodruff2014sketching}
David~P. Woodruff.
\newblock {Sketching as a Tool for Numerical Linear Algebra}.
\newblock \emph{Foundations and Trends in Theoretical Computer Science},
  10\penalty0 (1--2):\penalty0 1--157, 2014.

\bibitem[Woolfe et~al.(2008)Woolfe, Liberty, Rokhlin, and
  Tygert]{woolfe2008fast}
Franco Woolfe, Edo Liberty, Vladimir Rokhlin, and Mark Tygert.
\newblock {A Fast Randomized Algorithm for the Approximation of Matrices}.
\newblock \emph{Applied and Computational Harmonic Analysis}, 25\penalty0
  (3):\penalty0 335--366, 2008.

\bibitem[Yang et~al.(2016)Yang, Meng, and Mahoney]{yang2015implementing}
Jiyan Yang, Xiangrui Meng, and Michael~W. Mahoney.
\newblock {Implementing Randomized Matrix Algorithms in Parallel and
  Distributed Environments}.
\newblock \emph{Proceedings of the IEEE}, 104\penalty0 (1):\penalty0 58--92,
  2016.

\bibitem[Zaharia et~al.(2010)Zaharia, Chowdhury, Franklin, Shenker, and
  Stoica]{zaharia2010spark}
Matei Zaharia, Mosharaf Chowdhury, Michael~J. Franklin, Scott Shenker, and Ion
  Stoica.
\newblock {Spark: Cluster Computing with Working Sets}.
\newblock \emph{HotCloud}, 10\penalty0 (10-10):\penalty0 95, 2010.

\bibitem[Zhang et~al.(2013)Zhang, Duchi, and
  Wainwright]{zhang2013communication}
Yuchen Zhang, John~C. Duchi, and Martin~J. Wainwright.
\newblock {Communication-Efficient Algorithms for Statistical Optimization}.
\newblock \emph{Journal of Machine Learning Research}, 14:\penalty0 3321--3363,
  2013.

\bibitem[Zhang et~al.(2015)Zhang, Duchi, and Wainwright]{zhang2014divide}
Yuchen Zhang, John~C. Duchi, and Martin~J. Wainwright.
\newblock {Divide and Conquer Kernel Ridge Regression: a Distributed Algorithm
  with Minimax Optimal Rates}.
\newblock \emph{Journal of Machine Learning Research}, 16:\penalty0 3299--3340,
  2015.

\end{thebibliography}
