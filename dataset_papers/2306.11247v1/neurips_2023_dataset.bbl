\begin{thebibliography}{10}

\bibitem{akhtar2020modeling}
Sohail Akhtar, Valerio Basile, and Viviana Patti.
\newblock Modeling annotator perspective and polarized opinions to improve hate
  speech detection.
\newblock In {\em Proceedings of the AAAI Conference on Human Computation and
  Crowdsourcing}, volume~8, pages 151--154, 2020.

\bibitem{akhtar2021whose}
Sohail Akhtar, Valerio Basile, and Viviana Patti.
\newblock Whose opinions matter? perspective-aware models to identify opinions
  of hate speech victims in abusive language detection.
\newblock {\em arXiv preprint arXiv:2106.15896}, 2021.

\bibitem{al2020identifying}
Hala Al~Kuwatly, Maximilian Wich, and Georg Groh.
\newblock Identifying and measuring annotator bias based on annotators’
  demographic characteristics.
\newblock In {\em Proceedings of the Fourth Workshop on Online Abuse and
  Harms}, pages 184--190, 2020.

\bibitem{alkomah2022literature}
Fatimah Alkomah and Xiaogang Ma.
\newblock A literature review of textual hate speech detection methods and
  datasets.
\newblock {\em Information}, 13(6):273, 2022.

\bibitem{arhin2021groundtruth}
Kofi Arhin, Ioana Baldini, Dennis Wei, Karthikeyan~Natesan Ramamurthy, and
  Moninder Singh.
\newblock Ground-truth, whose truth? -- examining the challenges with
  annotating toxic text datasets, 2021.

\bibitem{aroyo2015truth}
Lora Aroyo and Chris Welty.
\newblock Truth is a lie: Crowd truth and the seven myths of human annotation.
\newblock {\em AI Magazine}, 36(1):15--24, 2015.

\bibitem{basile2021perspectivist}
Valerio Basile, Federico Cabitza, Andrea Campagner, and Michael Fell.
\newblock Toward a perspectivist turn in ground truthing for predictive
  computing, 2021.

\bibitem{basile2021toward}
Valerio Basile, Federico Cabitza, Andrea Campagner, and Michael Fell.
\newblock Toward a perspectivist turn in ground truthing for predictive
  computing.
\newblock {\em arXiv preprint arXiv:2109.04270}, 2021.

\bibitem{bian2023drop}
Ning Bian, Peilin Liu, Xianpei Han, Hongyu Lin, Yaojie Lu, Ben He, and Le~Sun.
\newblock A drop of ink may make a million think: The spread of false
  information in large language models.
\newblock {\em arXiv preprint arXiv:2305.04812}, 2023.

\bibitem{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock {\em arXiv preprint arXiv:2108.07258}, 2021.

\bibitem{davani2022dealing}
Aida~Mostafazadeh Davani, Mark D{\'\i}az, and Vinodkumar Prabhakaran.
\newblock Dealing with disagreements: Looking beyond the majority vote in
  subjective annotations.
\newblock {\em Transactions of the Association for Computational Linguistics},
  10:92--110, 2022.

\bibitem{deng2023recent}
Jiawen Deng, Hao Sun, Zhexin Zhang, Jiale Cheng, and Minlie Huang.
\newblock Recent advances towards safe, responsible, and moral dialogue
  systems: A survey, 2023.

\bibitem{denton2021whose}
Emily Denton, Mark D{\'\i}az, Ian Kivlichan, Vinodkumar Prabhakaran, and Rachel
  Rosen.
\newblock Whose ground truth? accounting for individual and collective
  identities underlying dataset annotation.
\newblock {\em arXiv preprint arXiv:2112.04554}, 2021.

\bibitem{dinan2021anticipating}
Emily Dinan, Gavin Abercrombie, A~Stevie Bergman, Shannon Spruit, Dirk Hovy,
  Y-Lan Boureau, and Verena Rieser.
\newblock Anticipating safety issues in e2e conversational ai: Framework and
  tooling.
\newblock {\em arXiv preprint arXiv:2107.03451}, 2021.

\bibitem{dinan2022safetykit}
Emily Dinan, Gavin Abercrombie, Stevie~A Bergman, Shannon Spruit, Dirk Hovy,
  Y-Lan Boureau, Verena Rieser, et~al.
\newblock Safetykit: First aid for measuring safety in open-domain
  conversational systems.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}. Association for
  Computational Linguistics, 2022.

\bibitem{garg2022handling}
Tanmay Garg, Sarah Masud, Tharun Suresh, and Tanmoy Chakraborty.
\newblock Handling bias in toxic speech detection: A survey.
\newblock {\em ACM Computing Surveys}, 2022.

\bibitem{goyal2022your}
Nitesh Goyal, Ian Kivlichan, Rachel Rosen, and Lucy Vasserman.
\newblock Is your toxicity my toxicity? exploring the impact of rater identity
  on toxicity annotation.
\newblock {\em arXiv preprint arXiv:2205.00501}, 2022.

\bibitem{halevy2021mitigating}
Matan Halevy, Camille Harris, Amy Bruckman, Diyi Yang, and Ayanna Howard.
\newblock Mitigating racial biases in toxic language detection with an
  equity-based ensemble framework.
\newblock In {\em Equity and Access in Algorithms, Mechanisms, and
  Optimization}, EAAMO '21, New York, NY, USA, 2021. Association for Computing
  Machinery.

\bibitem{hendrycks2022unsolved}
Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt.
\newblock Unsolved problems in ml safety, 2022.

\bibitem{huang2023chatgpt}
Fan Huang, Haewoon Kwak, and Jisun An.
\newblock Is chatgpt better than human annotators? potential and limitations of
  chatgpt in explaining implicit hate speech.
\newblock In {\em Companion Proceedings of the ACM Web Conference 2023}, WWW
  '23 Companion, page 294–297, New York, NY, USA, 2023. Association for
  Computing Machinery.

\bibitem{jaton2021assessing}
Florian Jaton.
\newblock Assessing biases, relaxing moralism: On ground-truthing practices in
  machine learning design and application.
\newblock {\em Big Data and Society}, 8(1), 2021.

\bibitem{kocon2021offensive}
Jan Kocoń, Alicja Figas, Marcin Gruza, Daria Puchalska, Tomasz Kajdanowicz,
  and Przemysław Kazienko.
\newblock Offensive, aggressive, and hate speech analysis: From data-centric to
  human-centered approach.
\newblock {\em Information Processing and Management}, 58(5):102643, 2021.

\bibitem{kocon2021learning}
Jan Kocoń, Marcin Gruza, Julita Bielaniewicz, Damian Grimling, Kamil Kanclerz,
  Piotr Miłkowski, and Przemysław Kazienko.
\newblock Learning personal human biases and representations for subjective
  tasks in natural language processing.
\newblock In {\em 2021 IEEE International Conference on Data Mining (ICDM)},
  pages 1168--1173, 2021.

\bibitem{lahnala-etal-2022-mitigating}
Allison Lahnala, Charles Welch, B{\'e}la Neuendorf, and Lucie Flek.
\newblock Mitigating toxic degeneration with empathetic data: Exploring the
  relationship between toxicity and empathy.
\newblock In {\em Proceedings of the 2022 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 4926--4938, Seattle, United States, July 2022.
  Association for Computational Linguistics.

\bibitem{leonardelli2021agreeing}
Elisa Leonardelli, Stefano Menini, Alessio~Palmero Aprosio, Marco Guerini, and
  Sara Tonelli.
\newblock Agreeing to disagree: Annotating offensive language datasets with
  annotators' disagreement.
\newblock {\em arXiv preprint arXiv:2109.13563}, 2021.

\bibitem{mathew2021hatexplain}
Binny Mathew, Punyajoy Saha, Seid~Muhie Yimam, Chris Biemann, Pawan Goyal, and
  Animesh Mukherjee.
\newblock Hatexplain: A benchmark dataset for explainable hate speech
  detection.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 14867--14875, 2021.

\bibitem{pavlopoulos2020toxicity}
John Pavlopoulos, Jeffrey Sorensen, Lucas Dixon, Nithum Thain, and Ion
  Androutsopoulos.
\newblock Toxicity detection: Does context really matter?
\newblock {\em arXiv preprint arXiv:2006.00998}, 2020.

\bibitem{perez2022red}
Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John
  Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving.
\newblock Red teaming language models with language models, 2022.

\bibitem{prabhakaran2021releasing}
Vinodkumar Prabhakaran, Aida~Mostafazadeh Davani, and Mark Diaz.
\newblock On releasing annotator-level labels and information in datasets.
\newblock {\em arXiv preprint arXiv:2110.05699}, 2021.

\bibitem{roller2020opendomain}
Stephen Roller, Y-Lan Boureau, Jason Weston, Antoine Bordes, Emily Dinan,
  Angela Fan, David Gunning, Da~Ju, Margaret Li, Spencer Poff, Pratik Ringshia,
  Kurt Shuster, Eric~Michael Smith, Arthur Szlam, Jack Urbanek, and Mary
  Williamson.
\newblock Open-domain conversational agents: Current progress, open problems,
  and future directions, 2020.

\bibitem{rottger2021two}
Paul R{\"o}ttger, Bertie Vidgen, Dirk Hovy, and Janet~B Pierrehumbert.
\newblock Two contrasting data annotation paradigms for subjective nlp tasks.
\newblock {\em arXiv preprint arXiv:2112.07475}, 2021.

\bibitem{ruane2019conversational}
Elayne Ruane, Abeba Birhane, and Anthony Ventresque.
\newblock Conversational ai: Social and ethical considerations.
\newblock In {\em AICS}, pages 104--115, 2019.

\bibitem{sahoo2022detecting}
Nihar Sahoo, Himanshu Gupta, and Pushpak Bhattacharyya.
\newblock Detecting unintended social bias in toxic language datasets, 2022.

\bibitem{sandri2023don}
Marta Sandri, Elisa Leonardelli, Sara Tonelli, and Elisabetta Je{\v{z}}ek.
\newblock Why don’t you do it right? analysing annotators’ disagreement in
  subjective tasks.
\newblock In {\em Proceedings of the 17th Conference of the European Chapter of
  the Association for Computational Linguistics}, pages 2420--2433, 2023.

\bibitem{santurkar2023whose}
Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and
  Tatsunori Hashimoto.
\newblock Whose opinions do language models reflect?
\newblock {\em arXiv preprint arXiv:2303.17548}, 2023.

\bibitem{sap2022annotators}
Maarten Sap, Swabha Swayamdipta, Laura Vianna, Xuhui Zhou, Yejin Choi, and
  Noah~A. Smith.
\newblock Annotators with attitudes: How annotator beliefs and identities bias
  toxic language detection, 2022.

\bibitem{si2022toxic}
Wai~Man Si, Michael Backes, Jeremy Blackburn, Emiliano De~Cristofaro, Gianluca
  Stringhini, Savvas Zannettou, and Yang Zhang.
\newblock Why so toxic? measuring and triggering toxic behavior in open-domain
  chatbots.
\newblock In {\em Proceedings of the 2022 ACM SIGSAC Conference on Computer and
  Communications Security}, CCS '22, page 2659–2673, New York, NY, USA, 2022.
  Association for Computing Machinery.

\bibitem{solaiman2021process}
Irene Solaiman and Christy Dennison.
\newblock Process for adapting language models to society (palms) with
  values-targeted datasets.
\newblock {\em Advances in Neural Information Processing Systems},
  34:5861--5873, 2021.

\bibitem{sun2022safety}
Hao Sun, Guangxuan Xu, Jiawen Deng, Jiale Cheng, Chujie Zheng, Hao Zhou, Nanyun
  Peng, Xiaoyan Zhu, and Minlie Huang.
\newblock On the safety of conversational models: Taxonomy, dataset, and
  benchmark, 2022.

\bibitem{thoppilan2022lamda}
Romal Thoppilan, Daniel~De Freitas, Jamie Hall, Noam Shazeer, Apoorv
  Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du,
  YaGuang Li, Hongrae Lee, Huaixiu~Steven Zheng, Amin Ghafouri, Marcelo
  Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao
  Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao,
  Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett,
  Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith~Ringel
  Morris, Tulsee Doshi, Renelito~Delos Santos, Toju Duke, Johnny Soraker, Ben
  Zevenbergen, Vinodkumar akaran, Mark Diaz, Ben Hutchinson, Kristen Olson,
  Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar,
  Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen,
  Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian
  Croak, Ed~Chi, and Quoc Le.
\newblock Lamda: Language models for dialog applications, 2022.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wich2020graph}
Maximilian Wich, Hala Al~Kuwatly, and Georg Groh.
\newblock Investigating annotator bias with a graph-based approach.
\newblock In {\em Proceedings of the Fourth Workshop on Online Abuse and
  Harms}, pages 191--199, Online, November 2020. Association for Computational
  Linguistics.

\bibitem{wich2021abusive}
Maximilian Wich, Christian Widmer, Gerhard Hagerer, and Georg Groh.
\newblock Investigating annotator bias in abusive language datasets.
\newblock In {\em Proceedings of the International Conference on Recent
  Advances in Natural Language Processing (RANLP 2021)}, pages 1515--1525,
  2021.

\bibitem{wiegreffe2021teach}
Sarah Wiegreffe and Ana Marasovic.
\newblock Teach me to explain: A review of datasets for explainable natural
  language processing.
\newblock In J.~Vanschoren and S.~Yeung, editors, {\em Proceedings of the
  Neural Information Processing Systems Track on Datasets and Benchmarks},
  volume~1. Curran, 2021.

\bibitem{xenos2022toxicity}
Alexandros Xenos, John Pavlopoulos, Ion Androutsopoulos, Lucas Dixon, Jeffrey
  Sorensen, and Léo Laugier.
\newblock Toxicity detection sensitive to conversational context.
\newblock {\em First Monday}, 27(5), Sep. 2022.

\bibitem{xu2021recipes}
Jing Xu, Da~Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan.
\newblock Recipes for safety in open-domain chatbots, 2021.

\bibitem{ziegler2022adversarial}
Daniel~M. Ziegler, Seraphina Nix, Lawrence Chan, Tim Bauman, Peter
  Schmidt-Nielsen, Tao Lin, Adam Scherlis, Noa Nabeshima, Ben Weinstein-Raun,
  Daniel de~Haas, Buck Shlegeris, and Nate Thomas.
\newblock Adversarial training for high-stakes reliability, 2022.

\end{thebibliography}
