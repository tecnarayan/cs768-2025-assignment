\begin{thebibliography}{91}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ainsworth et~al.(2022)Ainsworth, Hayase, and
  Srinivasa]{ainsworth2022git}
S.~K. Ainsworth, J.~Hayase, and S.~Srinivasa.
\newblock Git re-basin: Merging models modulo permutation symmetries, 2022.
\newblock \url{https://arxiv.org/abs/2209.04836}.

\bibitem[Albalak et~al.(2023)Albalak, Raffel, and Wang]{albalak2023improving}
A.~Albalak, C.~Raffel, and W.~Y. Wang.
\newblock Improving few-shot generalization by exploring and exploiting
  auxiliary data.
\newblock \emph{arXiv preprint arXiv:2302.00674}, 2023.

\bibitem[Amari(1996)]{Amari1996fisher}
S.~Amari.
\newblock Neural learning in structured parameter spaces - natural riemannian
  gradient.
\newblock In \emph{NIPS}, 1996.

\bibitem[Arpit et~al.(2021)Arpit, Wang, Zhou, and Xiong]{arpit2021ensemble}
D.~Arpit, H.~Wang, Y.~Zhou, and C.~Xiong.
\newblock Ensemble of averages: Improving model selection and boosting
  performance in domain generalization.
\newblock \emph{arXiv preprint arXiv:2110.10832}, 2021.

\bibitem[Bach et~al.(2022)Bach, Sanh, Yong, Webson, Raffel, Nayak, Sharma, Kim,
  Bari, F{\'e}vry, et~al.]{bach2022promptsource}
S.~H. Bach, V.~Sanh, Z.-X. Yong, A.~Webson, C.~Raffel, N.~V. Nayak, A.~Sharma,
  T.~Kim, M.~S. Bari, T.~F{\'e}vry, et~al.
\newblock {PromptSource}: An integrated development environment and repository
  for natural language prompts.
\newblock \emph{arXiv preprint arXiv:2202.01279}, 2022.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von
  Arx, Bernstein, Bohg, Bosselut, Brunskill,
  et~al.]{bommasani2021opportunities}
R.~Bommasani, D.~A. Hudson, E.~Adeli, R.~Altman, S.~Arora, S.~von Arx, M.~S.
  Bernstein, J.~Bohg, A.~Bosselut, E.~Brunskill, et~al.
\newblock On the opportunities and risks of foundation models, 2021.
\newblock \url{https://arxiv.org/abs/2108.07258}.

\bibitem[Cha et~al.(2021)Cha, Chun, Lee, Cho, Park, Lee, and Park]{cha2021swad}
J.~Cha, S.~Chun, K.~Lee, H.-C. Cho, S.~Park, Y.~Lee, and S.~Park.
\newblock Swad: Domain generalization by seeking flat minima.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 22405--22418, 2021.

\bibitem[Cheng et~al.(2017)Cheng, Han, and Lu]{cheng2017remote}
G.~Cheng, J.~Han, and X.~Lu.
\newblock Remote sensing image scene classification: Benchmark and state of the
  art.
\newblock \emph{Proceedings of the Institute of Electrical and Electronics
  Engineers (IEEE)}, 2017.
\newblock \url{https://ieeexplore.ieee.org/abstract/document/7891544}.

\bibitem[Choshen et~al.(2022)Choshen, Venezian, Slonim, and
  Katz]{choshen2022fusing}
L.~Choshen, E.~Venezian, N.~Slonim, and Y.~Katz.
\newblock Fusing finetuned models for better pretraining, 2022.
\newblock \url{https://arxiv.org/abs/2204.03044}.

\bibitem[Cimpoi et~al.(2014)Cimpoi, Maji, Kokkinos, Mohamed, and Vedaldi]{dtd}
M.~Cimpoi, S.~Maji, I.~Kokkinos, S.~Mohamed, and A.~Vedaldi.
\newblock Describing textures in the wild.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2014.
\newblock
  \url{https://openaccess.thecvf.com/content_cvpr_2014/html/Cimpoi_Describing_Textures_in_2014_CVPR_paper.html}.

\bibitem[Dagan et~al.(2005)Dagan, Glickman, and Magnini]{dagan2005pascal}
I.~Dagan, O.~Glickman, and B.~Magnini.
\newblock The pascal recognising textual entailment challenge.
\newblock In \emph{Machine Learning Challenges Workshop}, 2005.
\newblock \url{https://link.springer.com/chapter/10.1007/11736790_9}.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Don-Yehiya et~al.(2022)Don-Yehiya, Venezian, Raffel, Slonim, Katz, and
  Choshen]{don2022cold}
S.~Don-Yehiya, E.~Venezian, C.~Raffel, N.~Slonim, Y.~Katz, and L.~Choshen.
\newblock Cold fusion: Collaborative descent for distributed multitask
  finetuning, 2022.
\newblock \url{https://arxiv.org/abs/2212.01378}.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2021an}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit,
  and N.~Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.
\newblock \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[Draxler et~al.(2018)Draxler, Veschgini, Salmhofer, and
  Hamprecht]{draxler2018essentially}
F.~Draxler, K.~Veschgini, M.~Salmhofer, and F.~Hamprecht.
\newblock Essentially no barriers in neural network energy landscape.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.
\newblock \url{https://arxiv.org/abs/1803.00885}.

\bibitem[Entezari et~al.(2021)Entezari, Sedghi, Saukh, and
  Neyshabur]{entezari2021role}
R.~Entezari, H.~Sedghi, O.~Saukh, and B.~Neyshabur.
\newblock The role of permutation invariance in linear mode connectivity of
  neural networks.
\newblock \emph{arXiv preprint arXiv:2110.06296}, 2021.

\bibitem[Fifty et~al.(2021)Fifty, Amid, Zhao, Yu, Anil, and
  Finn]{fifty2021efficiently}
C.~Fifty, E.~Amid, Z.~Zhao, T.~Yu, R.~Anil, and C.~Finn.
\newblock Efficiently identifying task groupings for multi-task learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 27503--27516, 2021.

\bibitem[Fisher(1922)]{fisher1922mathematical}
R.~A. Fisher.
\newblock On the mathematical foundations of theoretical statistics.
\newblock \emph{Philosophical transactions of the Royal Society of London.
  Series A, containing papers of a mathematical or physical character},
  222\penalty0 (594-604):\penalty0 309--368, 1922.

\bibitem[Frankle et~al.(2020)Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020linear}
J.~Frankle, G.~K. Dziugaite, D.~Roy, and M.~Carbin.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.
\newblock \url{https://proceedings.mlr.press/v119/frankle20a.html}.

\bibitem[Freeman and Bruna(2016)]{freeman2016topology}
C.~D. Freeman and J.~Bruna.
\newblock Topology and geometry of half-rectified network optimization.
\newblock \emph{arXiv preprint arXiv:1611.01540}, 2016.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{garipov2018loss}
T.~Garipov, P.~Izmailov, D.~Podoprikhin, D.~Vetrov, and A.~G. Wilson.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.
\newblock \url{https://arxiv.org/abs/1802.10026}.

\bibitem[Gueta et~al.(2023)Gueta, Venezian, Raffel, Slonim, Katz, and
  Choshen]{gueta2023knowledge}
A.~Gueta, E.~Venezian, C.~Raffel, N.~Slonim, Y.~Katz, and L.~Choshen.
\newblock Knowledge is a region in weight space for fine-tuned language models.
\newblock \emph{arXiv preprint arXiv:2302.04863}, 2023.

\bibitem[Gupta et~al.(2020)Gupta, Serrano, and DeCoste]{gupta2020stochastic}
V.~Gupta, S.~A. Serrano, and D.~DeCoste.
\newblock Stochastic weight averaging in parallel: Large-batch training that
  generalizes well.
\newblock \emph{International Conference on Learning Representations}, 2020.

\bibitem[Helber et~al.(2019)Helber, Bischke, Dengel, and Borth]{eurosat}
P.~Helber, B.~Bischke, A.~Dengel, and D.~Borth.
\newblock Eurosat: A novel dataset and deep learning benchmark for land use and
  land cover classification.
\newblock \emph{Journal of Selected Topics in Applied Earth Observations and
  Remote Sensing}, 2019.
\newblock \url{https://arxiv.org/abs/1709.00029}.

\bibitem[Hoefler et~al.(2021)Hoefler, Alistarh, Ben-Nun, Dryden, and
  Peste]{hoefler2021sparsitysurvey}
T.~Hoefler, D.~Alistarh, T.~Ben-Nun, N.~Dryden, and A.~Peste.
\newblock Sparsity in deep learning: Pruning and growth for efficient inference
  and training in neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0
  (1):\penalty0 10882--11005, 2021.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, and
  Chen]{hu2021lora}
E.~J. Hu, Y.~Shen, P.~Wallis, Z.~Allen-Zhu, Y.~Li, S.~Wang, and W.~Chen.
\newblock {LoRA}: Low-rank adaptation of large language models.
\newblock \emph{ArXiv}, abs/2106.09685, 2021.

\bibitem[Huang et~al.(2019)Huang, Le~Bras, Bhagavatula, and
  Choi]{huang2019cosmos}
L.~Huang, R.~Le~Bras, C.~Bhagavatula, and Y.~Choi.
\newblock Cosmos qa: Machine reading comprehension with contextual commonsense
  reasoning.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 2391--2401, 2019.

\bibitem[Ilharco et~al.(2022)Ilharco, Wortsman, Gadre, Song, Hajishirzi,
  Kornblith, Farhadi, and Schmidt]{ilharco2022patching}
G.~Ilharco, M.~Wortsman, S.~Y. Gadre, S.~Song, H.~Hajishirzi, S.~Kornblith,
  A.~Farhadi, and L.~Schmidt.
\newblock Patching open-vocabulary models by interpolating weights.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2022.
\newblock \url{https://arXiv.org/abs/2208.05592}.

\bibitem[Ilharco et~al.(2023)Ilharco, Ribeiro, Wortsman, Schmidt, Hajishirzi,
  and Farhadi]{ilharco2023editing}
G.~Ilharco, M.~T. Ribeiro, M.~Wortsman, L.~Schmidt, H.~Hajishirzi, and
  A.~Farhadi.
\newblock Editing models with task arithmetic.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=6t0Kwf8-jrj}.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2018averaging}
P.~Izmailov, D.~Podoprikhin, T.~Garipov, D.~Vetrov, and A.~G. Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence (UAI)},
  2018.
\newblock \url{https://arxiv.org/abs/1803.05407}.

\bibitem[Jin et~al.(2023)Jin, Ren, Preotiuc-Pietro, and Cheng]{jin2023regmean}
X.~Jin, X.~Ren, D.~Preotiuc-Pietro, and P.~Cheng.
\newblock Dataless knowledge fusion by merging weights of language models.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=FCnohuR6AnM}.

\bibitem[Jordan et~al.(2023)Jordan, Sedghi, Saukh, Entezari, and
  Neyshabur]{jordan2023repair}
K.~Jordan, H.~Sedghi, O.~Saukh, R.~Entezari, and B.~Neyshabur.
\newblock {REPAIR}: {RE}normalizing permuted activations for interpolation
  repair.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=gU5sJ6ZggcX}.

\bibitem[Khot et~al.(2020)Khot, Clark, Guerquin, Jansen, and
  Sabharwal]{khot2020qasc}
T.~Khot, P.~Clark, M.~Guerquin, P.~Jansen, and A.~Sabharwal.
\newblock Qasc: A dataset for question answering via sentence composition.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 8082--8090, 2020.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{kirkpatrick2017overcoming}
J.~Kirkpatrick, R.~Pascanu, N.~Rabinowitz, J.~Veness, G.~Desjardins, A.~A.
  Rusu, K.~Milan, J.~Quan, T.~Ramalho, A.~Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences (PNAS)}, 2017.
\newblock \url{https://arxiv.org/abs/1612.00796}.

\bibitem[Krause et~al.(2013)Krause, Stark, Deng, and Fei-Fei]{cars}
J.~Krause, M.~Stark, J.~Deng, and L.~Fei-Fei.
\newblock 3d object representations for fine-grained categorization.
\newblock In \emph{International Conference on Computer Vision Workshops
  (ICML)}, 2013.
\newblock
  \url{https://www.cv-foundation.org/openaccess/content_iccv_workshops_2013/W19/html/Krause_3D_Object_Representations_2013_ICCV_paper.html}.

\bibitem[LeCun(1998)]{lecun1998mnist}
Y.~LeCun.
\newblock The mnist database of handwritten digits, 1998.
\newblock \url{http://yann.lecun.com/exdb/mnist/}.

\bibitem[Levesque et~al.(2012)Levesque, Davis, and Morgenstern]{wsc}
H.~Levesque, E.~Davis, and L.~Morgenstern.
\newblock The winograd schema challenge.
\newblock \emph{Thirteenth International Conference on the Principles of
  Knowledge Representation and Reasoning}, 2012.

\bibitem[Li et~al.(2022)Li, Gururangan, Dettmers, Lewis, Althoff, Smith, and
  Zettlemoyer]{li2022branch}
M.~Li, S.~Gururangan, T.~Dettmers, M.~Lewis, T.~Althoff, N.~A. Smith, and
  L.~Zettlemoyer.
\newblock Branch-train-merge: Embarrassingly parallel training of expert
  language models, 2022.
\newblock \url{https://arxiv.org/abs/2208.03306}.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Zhang, Yadav, Sung, Cheng, Bansal,
  and Chen]{li2023mc-smoe}
P.~Li, Z.~Zhang, P.~Yadav, Y.-L. Sung, Y.~Cheng, M.~Bansal, and T.~Chen.
\newblock Merge, then compress: Demystify efficient smoe with hints from its
  routing policy, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Peng, Zhang, Ding, Hu, and
  Shen]{li2023deep}
W.~Li, Y.~Peng, M.~Zhang, L.~Ding, H.~Hu, and L.~Shen.
\newblock Deep model fusion: A survey.
\newblock \emph{arXiv preprint arXiv:2309.15698}, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2019)Li, Huang, Yang, Wang, and Zhang]{li2019convergence}
X.~Li, K.~Huang, W.~Yang, S.~Wang, and Z.~Zhang.
\newblock On the convergence of fedavg on non-iid data.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Li et~al.(2015)Li, Yosinski, Clune, Lipson, and
  Hopcroft]{li2015convergent}
Y.~Li, J.~Yosinski, J.~Clune, H.~Lipson, and J.~Hopcroft.
\newblock Convergent learning: Do different neural networks learn the same
  representations?
\newblock \emph{arXiv preprint arXiv:1511.07543}, 2015.

\bibitem[Liu et~al.(2022)Liu, Tam, Muqeeth, Mohta, Huang, Bansal, and
  Raffel]{liu2022tfew}
H.~Liu, D.~Tam, M.~Muqeeth, J.~Mohta, T.~Huang, M.~Bansal, and C.~A. Raffel.
\newblock Few-shot parameter-efficient fine-tuning is better and cheaper than
  in-context learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 1950--1965, 2022.

\bibitem[Marneffe et~al.(2019)Marneffe, Simons, and Tonhauser]{cb}
M.-C.~d. Marneffe, M.~Simons, and J.~Tonhauser.
\newblock {The CommitmentBank}: Investigating projection in naturally occurring
  discourse.
\newblock \emph{Proceedings of Sinn und Bedeutung 23}, 2019.

\bibitem[Matena and Raffel(2021)]{matena2021merging}
M.~Matena and C.~Raffel.
\newblock Merging models with fisher-weighted averaging.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.
\newblock \url{https://arxiv.org/abs/2111.09832}.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
B.~McMahan, E.~Moore, D.~Ramage, S.~Hampson, and B.~A. y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial intelligence and statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and Ng]{svhn}
Y.~Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, and A.~Y. Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)
  Workshops}, 2011.
\newblock
  \url{https://storage.googleapis.com/pub-tools-public-publication-data/pdf/37648.pdf}.

\bibitem[Neyshabur et~al.(2020)Neyshabur, Sedghi, and
  Zhang]{neyshabur2020being}
B.~Neyshabur, H.~Sedghi, and C.~Zhang.
\newblock What is being transferred in transfer learning?
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 512--523, 2020.

\bibitem[Nie et~al.(2019)Nie, Williams, Dinan, Bansal, Weston, and
  Kiela]{nie2019adversarial}
Y.~Nie, A.~Williams, E.~Dinan, M.~Bansal, J.~Weston, and D.~Kiela.
\newblock {Adversarial NLI}: A new benchmark for natural language
  understanding.
\newblock \emph{arXiv preprint arXiv:1910.14599}, 2019.

\bibitem[Orgad et~al.(2023)Orgad, Kawar, and Belinkov]{orgad2023editing}
H.~Orgad, B.~Kawar, and Y.~Belinkov.
\newblock Editing implicit assumptions in text-to-image diffusion models.
\newblock \emph{arXiv preprint arXiv:2303.08084}, 2023.

\bibitem[Ortiz{-}Jim{\'{e}}nez et~al.(2023)Ortiz{-}Jim{\'{e}}nez, Favero, and
  Frossard]{ortizjimenez2023tangent}
G.~Ortiz{-}Jim{\'{e}}nez, A.~Favero, and P.~Frossard.
\newblock Task arithmetic in the tangent space: Improved editing of pre-trained
  models.
\newblock \emph{NeurIPS}, 2023.
\newblock \url{https://arxiv.org/abs/2305:12827}.

\bibitem[Phang et~al.(2018)Phang, F{\'e}vry, and Bowman]{phang2018sentence}
J.~Phang, T.~F{\'e}vry, and S.~R. Bowman.
\newblock Sentence encoders on stilts: Supplementary training on intermediate
  labeled-data tasks.
\newblock \emph{arXiv preprint arXiv:1811.01088}, 2018.

\bibitem[Pilehvar and Camacho-Collados(2019)]{pilehvar2018wic}
M.~T. Pilehvar and J.~Camacho-Collados.
\newblock {WiC}: The word-in-context dataset for evaluating context-sensitive
  meaning representations.
\newblock In \emph{Proceedings of NAACL-HLT}, 2019.

\bibitem[Poth et~al.(2021)Poth, Pfeiffer, R{\"u}ckl{\'e}, and
  Gurevych]{poth-etal-2021-pre}
C.~Poth, J.~Pfeiffer, A.~R{\"u}ckl{\'e}, and I.~Gurevych.
\newblock {W}hat to pre-train on? {E}fficient intermediate task selection.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 10585--10605, Online and Punta Cana,
  Dominican Republic, Nov. 2021.

\bibitem[Pruksachatkun et~al.(2020)Pruksachatkun, Phang, Liu, Htut, Zhang,
  Pang, Vania, Kann, and Bowman]{pruksachatkun-etal-2020-intermediate}
Y.~Pruksachatkun, J.~Phang, H.~Liu, P.~M. Htut, X.~Zhang, R.~Y. Pang, C.~Vania,
  K.~Kann, and S.~R. Bowman.
\newblock Intermediate-task transfer learning with pretrained language models:
  When and why does it work?
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 5231--5247, Online, July 2020.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, Krueger, and Sutskever]{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark, G.~Krueger, and I.~Sutskever.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.
\newblock \url{https://arxiv.org/abs/2103.00020}.

\bibitem[Raffel et~al.(2020{\natexlab{a}})Raffel, Shazeer, Roberts, Lee,
  Narang, Matena, Zhou, Li, and Liu]{colin2020exploring}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,
  W.~Li, and P.~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{Journal of Machine Learning Research (JMLR)},
  2020{\natexlab{a}}.
\newblock \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Raffel et~al.(2020{\natexlab{b}})Raffel, Shazeer, Roberts, Lee,
  Narang, Matena, Zhou, Li, and Liu]{raffel2019exploring}
C.~Raffel, N.~M. Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,
  W.~Li, and P.~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{ArXiv}, abs/1910.10683, 2020{\natexlab{b}}.

\bibitem[Ram{\'e} et~al.(2022)Ram{\'e}, Ahuja, Zhang, Cord, Bottou, and
  Lopez-Paz]{rame2022modelrat}
A.~Ram{\'e}, K.~Ahuja, J.~Zhang, M.~Cord, L.~Bottou, and D.~Lopez-Paz.
\newblock Model ratatouille: Recycling diverse models for out-of-distribution
  generalization.
\newblock \emph{arXiv preprint arXiv:2212.10445}, 2022.

\bibitem[Ram{\'e} et~al.(2023)Ram{\'e}, Kirchmeyer, Rahier, Rakotomamonjy,
  Gallinari, and Cord]{Ram2022DiverseWA}
A.~Ram{\'e}, M.~Kirchmeyer, T.~Rahier, A.~Rakotomamonjy, P.~Gallinari, and
  M.~Cord.
\newblock Diverse weight averaging for out-of-distribution generalization.
\newblock \emph{ICML}, 2023.

\bibitem[Roemmele et~al.(2011)Roemmele, Bejan, and Gordon]{copa}
M.~Roemmele, C.~A. Bejan, and A.~S. Gordon.
\newblock Choice of plausible alternatives: An evaluation of commonsense causal
  reasoning.
\newblock \emph{2011 AAAI Spring Symposium Series}, 2011.

\bibitem[Rogers et~al.(2020)Rogers, Kovaleva, Downey, and
  Rumshisky]{quail_dataset}
A.~Rogers, O.~Kovaleva, M.~Downey, and A.~Rumshisky.
\newblock Getting closer to {AI} complete question answering: {A} set of
  prerequisite real tasks.
\newblock In \emph{The Thirty-Fourth {AAAI} Conference on Artificial
  Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of
  Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium
  on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York,
  NY, USA, February 7-12, 2020}, pages 8722--8731. {AAAI} Press, 2020.
\newblock URL \url{https://aaai.org/ojs/index.php/AAAI/article/view/6398}.

\bibitem[Ruder(2016)]{ruder2016overview}
S.~Ruder.
\newblock An overview of gradient descent optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1609.04747}, 2016.

\bibitem[Sakaguchi et~al.(2020)Sakaguchi, Le~Bras, Bhagavatula, and
  Choi]{sakaguchi2020winogrande}
K.~Sakaguchi, R.~Le~Bras, C.~Bhagavatula, and Y.~Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2020.

\bibitem[Sanh et~al.(2021{\natexlab{a}})Sanh, Webson, Raffel, Bach, Sutawika,
  Alyafeai, Chaffin, Stiegler, Scao, Raja, et~al.]{sanh2021multitask}
V.~Sanh, A.~Webson, C.~Raffel, S.~H. Bach, L.~Sutawika, Z.~Alyafeai,
  A.~Chaffin, A.~Stiegler, T.~L. Scao, A.~Raja, et~al.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock \emph{arXiv preprint arXiv:2110.08207}, 2021{\natexlab{a}}.

\bibitem[Sanh et~al.(2021{\natexlab{b}})Sanh, Webson, Raffel, Bach, Sutawika,
  Alyafeai, Chaffin, Stiegler, Scao, Raja, et~al.]{sanh2022multitask}
V.~Sanh, A.~Webson, C.~Raffel, S.~H. Bach, L.~Sutawika, Z.~Alyafeai,
  A.~Chaffin, A.~Stiegler, T.~L. Scao, A.~Raja, et~al.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021{\natexlab{b}}.
\newblock \url{https://arxiv.org/abs/2110.08207}.

\bibitem[Sap et~al.(2019)Sap, Rashkin, Chen, Le~Bras, and Choi]{sap2019social}
M.~Sap, H.~Rashkin, D.~Chen, R.~Le~Bras, and Y.~Choi.
\newblock Social iqa: Commonsense reasoning about social interactions.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 4463--4473, 2019.

\bibitem[Sharma et~al.(2018)Sharma, Allen, Bakhshandeh, and
  Mostafazadeh]{sharma2018tackling}
R.~Sharma, J.~Allen, O.~Bakhshandeh, and N.~Mostafazadeh.
\newblock Tackling the story ending biases in the story cloze test.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, pages 752--757,
  2018.

\bibitem[Shnarch et~al.(2022)Shnarch, Halfon, Gera, Danilevsky, Katsis,
  Choshen, Cooper, Epelboim, Zhang, Wang, et~al.]{shnarch2022label}
E.~Shnarch, A.~Halfon, A.~Gera, M.~Danilevsky, Y.~Katsis, L.~Choshen, M.~S.
  Cooper, D.~Epelboim, Z.~Zhang, D.~Wang, et~al.
\newblock Label sleuth: From unlabeled text to a classifier in a few hours.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, 2022.

\bibitem[Singh and Jaggi(2020)]{singh2020model}
S.~P. Singh and M.~Jaggi.
\newblock Model fusion via optimal transport.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 22045--22055, 2020.

\bibitem[Stallkamp et~al.(2011)Stallkamp, Schlipsing, Salmen, and Igel]{gtsrb}
J.~Stallkamp, M.~Schlipsing, J.~Salmen, and C.~Igel.
\newblock The german traffic sign recognition benchmark: a multi-class
  classification competition.
\newblock In \emph{International Joint Conference on Neural Networks (IJCNN)},
  2011.
\newblock \url{https://ieeexplore.ieee.org/document/6033395}.

\bibitem[Sung et~al.(2023)Sung, Li, Lin, Gan, Bansal, and
  Wang]{Sung2023AnEmpiricalSO}
Y.-L. Sung, L.~Li, K.~Lin, Z.~Gan, M.~Bansal, and L.~Wang.
\newblock An empirical study of multimodal model merging.
\newblock \emph{Empirical Methods in Natural Language Processing (Findings)},
  2023.

\bibitem[Tafjord et~al.(2019)Tafjord, Gardner, Lin, and
  Clark]{tafjord2019quartz}
O.~Tafjord, M.~Gardner, K.~Lin, and P.~Clark.
\newblock Quartz: An open-domain dataset of qualitative relationship questions.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 5941--5946, 2019.

\bibitem[Tatro et~al.(2020)Tatro, Chen, Das, Melnyk, Sattigeri, and
  Lai]{tatro2020optimizing}
N.~Tatro, P.-Y. Chen, P.~Das, I.~Melnyk, P.~Sattigeri, and R.~Lai.
\newblock Optimizing mode connectivity via neuron alignment.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15300--15311, 2020.

\bibitem[Tay et~al.(2022)Tay, Dehghani, Tran, Garcia, Wei, Wang, Chung, Bahri,
  Schuster, Zheng, et~al.]{tay2022ul2}
Y.~Tay, M.~Dehghani, V.~Q. Tran, X.~Garcia, J.~Wei, X.~Wang, H.~W. Chung,
  D.~Bahri, T.~Schuster, S.~Zheng, et~al.
\newblock Ul2: Unifying language learning paradigms.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2022.

\bibitem[Thimm and Fiesler(1995)]{thimm1995evaluatingtopk}
G.~Thimm and E.~Fiesler.
\newblock Evaluating pruning methods.
\newblock In \emph{International Symposium on Artificial Neural Networks},
  1995.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2017.
\newblock \url{https://arxiv.org/abs/1706.03762}.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2018glue}
A.~Wang, A.~Singh, J.~Michael, F.~Hill, O.~Levy, and S.~R. Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.
\newblock \url{https://arxiv.org/abs/1804.07461}.

\bibitem[Wang et~al.(2020)Wang, Yurochkin, Sun, Papailiopoulos, and
  Khazaeni]{wang2020federated}
H.~Wang, M.~Yurochkin, Y.~Sun, D.~Papailiopoulos, and Y.~Khazaeni.
\newblock Federated learning with matched averaging.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Weller et~al.(2022)Weller, Seppi, and Gardner]{weller-etal-2022-use}
O.~Weller, K.~Seppi, and M.~Gardner.
\newblock When to use multi-task learning vs intermediate fine-tuning for
  pre-trained encoder transfer learning.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, pages 272--282,
  Dublin, Ireland, May 2022.

\bibitem[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, et~al.]{wolf2019huggingface}
T.~Wolf, L.~Debut, V.~Sanh, J.~Chaumond, C.~Delangue, A.~Moi, P.~Cistac,
  T.~Rault, R.~Louf, M.~Funtowicz, et~al.
\newblock Huggingface's transformers: State-of-the-art natural language
  processing, 2019.
\newblock \url{https://arxiv.org/abs/1910.03771}.

\bibitem[Wortsman et~al.(2022{\natexlab{a}})Wortsman, Ilharco, Gadre, Roelofs,
  Gontijo-Lopes, Morcos, Namkoong, Farhadi, Carmon, Kornblith,
  et~al.]{wortsman2022model}
M.~Wortsman, G.~Ilharco, S.~Y. Gadre, R.~Roelofs, R.~Gontijo-Lopes, A.~S.
  Morcos, H.~Namkoong, A.~Farhadi, Y.~Carmon, S.~Kornblith, et~al.
\newblock Model soups: averaging weights of multiple fine-tuned models improves
  accuracy without increasing inference time.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2022{\natexlab{a}}.
\newblock \url{https://arxiv.org/abs/2203.05482}.

\bibitem[Wortsman et~al.(2022{\natexlab{b}})Wortsman, Ilharco, Li, Kim,
  Hajishirzi, Farhadi, Namkoong, and Schmidt]{wortsman2021robust}
M.~Wortsman, G.~Ilharco, M.~Li, J.~W. Kim, H.~Hajishirzi, A.~Farhadi,
  H.~Namkoong, and L.~Schmidt.
\newblock Robust fine-tuning of zero-shot models.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2022{\natexlab{b}}.
\newblock \url{https://arxiv.org/abs/2109.01903}.

\bibitem[Xiao et~al.(2016)Xiao, Ehinger, Hays, Torralba, and Oliva]{sun397}
J.~Xiao, K.~A. Ehinger, J.~Hays, A.~Torralba, and A.~Oliva.
\newblock Sun database: Exploring a large collection of scene categories.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 2016.
\newblock \url{https://link.springer.com/article/10.1007/s11263-014-0748-y}.

\bibitem[Yadav and Bansal(2023)]{yadav2023exssnet}
P.~Yadav and M.~Bansal.
\newblock Exclusive supermask subnetwork training for continual learning.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL 2023}, pages 569--587, Toronto, Canada, July 2023. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2023.findings-acl.36}.
\newblock URL \url{https://aclanthology.org/2023.findings-acl.36}.

\bibitem[Yadav et~al.(2023)Yadav, Sun, Ding, Li, Zhang, Tan, Bhatia, Ma,
  Nallapati, Ramanathan, Bansal, and Xiang]{yadav2023codecl}
P.~Yadav, Q.~Sun, H.~Ding, X.~Li, D.~Zhang, M.~Tan, P.~Bhatia, X.~Ma,
  R.~Nallapati, M.~K. Ramanathan, M.~Bansal, and B.~Xiang.
\newblock Exploring continual learning for code generation models.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, pages 782--792,
  Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-short.68}.
\newblock URL \url{https://aclanthology.org/2023.acl-short.68}.

\bibitem[Yang et~al.(2015)Yang, Yih, and Meek]{yang-etal-2015-wikiqa}
Y.~Yang, W.-t. Yih, and C.~Meek.
\newblock {W}iki{QA}: A challenge dataset for open-domain question answering.
\newblock In \emph{Proceedings of the 2015 Conference on Empirical Methods in
  Natural Language Processing}, pages 2013--2018, Lisbon, Portugal, Sept. 2015.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D15-1237}.
\newblock URL \url{https://aclanthology.org/D15-1237}.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and
  Choi]{zellers2019hellaswag}
R.~Zellers, A.~Holtzman, Y.~Bisk, A.~Farhadi, and Y.~Choi.
\newblock {HellaSwag}: Can a machine really finish your sentence?
\newblock \emph{arXiv preprint arXiv:1905.07830}, 2019.

\bibitem[Zhang and Yang(2021)]{zhang2021survey}
Y.~Zhang and Q.~Yang.
\newblock A survey on multi-task learning.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering},
  34\penalty0 (12):\penalty0 5586--5609, 2021.

\bibitem[Zhang et~al.(2019)Zhang, Baldridge, and He]{paws2019naacl}
Y.~Zhang, J.~Baldridge, and L.~He.
\newblock {PAWS: Paraphrase Adversaries from Word Scrambling}.
\newblock In \emph{Proc. of NAACL}, 2019.

\bibitem[Zhuang et~al.(2020)Zhuang, Qi, Duan, Xi, Zhu, Zhu, Xiong, and
  He]{zhuang2020comprehensive}
F.~Zhuang, Z.~Qi, K.~Duan, D.~Xi, Y.~Zhu, H.~Zhu, H.~Xiong, and Q.~He.
\newblock A comprehensive survey on transfer learning.
\newblock \emph{Proceedings of the IEEE}, 2020.
\newblock \url{https://arxiv.org/abs/1911.02685}.

\end{thebibliography}
