\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2021)Agarwal, Kakade, Lee, and Mahajan]{agarwal2021theory}
Agarwal, A., Kakade, S.~M., Lee, J.~D., and Mahajan, G.
\newblock On the theory of policy gradient methods: Optimality, approximation, and distribution shift.
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0 (1):\penalty0 4431--4506, 2021.

\bibitem[Baird(1995)]{baird1995residual}
Baird, L.
\newblock Residual algorithms: Reinforcement learning with function approximation.
\newblock In \emph{Machine Learning Proceedings}, pp.\  30--37. Elsevier, 1995.

\bibitem[Barakat et~al.(2022)Barakat, Bianchi, and Lehmann]{barakat2022analysis}
Barakat, A., Bianchi, P., and Lehmann, J.
\newblock Analysis of a target-based actor-critic algorithm with linear function approximation.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  991--1040. PMLR, 2022.

\bibitem[Barto et~al.(1983)Barto, Sutton, and Anderson]{Barto1983}
Barto, A.~G., Sutton, R.~S., and Anderson, C.~W.
\newblock Neuronlike adaptive elements that can solve difficult learning control problems.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics}, SMC-13\penalty0 (5):\penalty0 834--846, 1983.
\newblock \doi{10.1109/TSMC.1983.6313077}.

\bibitem[Bhandari \& Russo(2019)Bhandari and Russo]{bhandari2019global}
Bhandari, J. and Russo, D.
\newblock Global optimality guarantees for policy gradient methods.
\newblock \emph{arXiv e-prints}, pp.\  arXiv--1906, 2019.

\bibitem[Bhandari \& Russo(2021)Bhandari and Russo]{bhandari2021linear}
Bhandari, J. and Russo, D.
\newblock On the linear convergence of policy gradient methods for finite {MDP}s.
\newblock In \emph{Proc. International Conference on Artifical Intelligence and Statistics (AISTATS)}, pp.\  2386--2394. PMLR, 2021.

\bibitem[Bhandari et~al.(2018)Bhandari, Russo, and Singal]{bhandari2018finite}
Bhandari, J., Russo, D., and Singal, R.
\newblock A finite time analysis of temporal difference learning with linear function approximation.
\newblock In \emph{Proc. Annual Conference on Learning Theory (CoLT)}, pp.\  1691--1692, 2018.

\bibitem[Bhatnagar et~al.(2009)Bhatnagar, Sutton, Ghavamzadeh, and Lee]{bhatnagar2009natural}
Bhatnagar, S., Sutton, R.~S., Ghavamzadeh, M., and Lee, M.
\newblock Natural actor--critic algorithms.
\newblock \emph{Automatica}, 45\penalty0 (11):\penalty0 2471--2482, 2009.

\bibitem[Cai et~al.(2019)Cai, Yang, Lee, and Wang]{cai2019neural}
Cai, Q., Yang, Z., Lee, J.~D., and Wang, Z.
\newblock Neural temporal-difference learning converges to global optima.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems (NeurIPS)}, pp.\  11312--11322, 2019.

\bibitem[Cayci et~al.(2022)Cayci, He, and Srikant]{cayci2022finite}
Cayci, S., He, N., and Srikant, R.
\newblock Finite-time analysis of entropy-regularized neural natural actor-critic algorithm.
\newblock \emph{arXiv preprint arXiv:2206.00833}, 2022.

\bibitem[Cen et~al.(2021)Cen, Cheng, Chen, Wei, and Chi]{cen2021fast}
Cen, S., Cheng, C., Chen, Y., Wei, Y., and Chi, Y.
\newblock Fast global convergence of natural policy gradient methods with entropy regularization.
\newblock \emph{Operations Research}, 2021.

\bibitem[Chen et~al.(2021)Chen, Sun, and Yin]{chen2021closing}
Chen, T., Sun, Y., and Yin, W.
\newblock Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 25294--25307, 2021.

\bibitem[Chen et~al.(2023)Chen, Duan, Liang, and Zhao]{chen2023global}
Chen, X., Duan, J., Liang, Y., and Zhao, L.
\newblock Global convergence of two-timescale actor-critic for solving linear quadratic regulator.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pp.\  7087--7095, 2023.

\bibitem[Chen et~al.(2022)Chen, Khodadadian, and Maguluri]{chen2022finite}
Chen, Z., Khodadadian, S., and Maguluri, S.~T.
\newblock Finite-sample analysis of off-policy natural actor--critic with linear function approximation.
\newblock \emph{IEEE Control Systems Letters}, 6:\penalty0 2611--2616, 2022.

\bibitem[Dalal et~al.(2018)Dalal, Sz{\"o}r{\'e}nyi, Thoppe, and Mannor]{dalal2018finite}
Dalal, G., Sz{\"o}r{\'e}nyi, B., Thoppe, G., and Mannor, S.
\newblock Finite sample analysis of two-timescale stochastic approximation with applications to reinforcement learning.
\newblock \emph{Proceedings of Machine Learning Research}, 75:\penalty0 1--35, 2018.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International conference on machine learning}, pp.\  1675--1685. PMLR, 2019.

\bibitem[Gordon(1996)]{gordon1996chattering}
Gordon, G.~J.
\newblock Chattering in {SARSA} ($\lambda$).
\newblock \emph{CMU Learning Lab Technical Report}, 1996.

\bibitem[Gupta et~al.(2019)Gupta, Srikant, and Ying]{gupta2019finite}
Gupta, H., Srikant, R., and Ying, L.
\newblock Finite-time performance bounds and adaptive learning rate selection for two time-scale reinforcement learning.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems (NeurIPS)}, pp.\  4706--4715, 2019.

\bibitem[Heidergott \& Hordijk(2003)Heidergott and Hordijk]{heidergott2003taylor}
Heidergott, B. and Hordijk, A.
\newblock Taylor series expansions for stationary markov chains.
\newblock \emph{Advances in Applied Probability}, 35\penalty0 (4):\penalty0 1046--1070, 2003.

\bibitem[Kakade(2001)]{kakade2001natural}
Kakade, S.~M.
\newblock A natural policy gradient.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems (NIPS)}, volume~14, pp.\  1531--1538, 2001.

\bibitem[Kaledin et~al.(2020)Kaledin, Moulines, Naumov, Tadic, and Wai]{kaledin2020finite}
Kaledin, M., Moulines, E., Naumov, A., Tadic, V., and Wai, H.-T.
\newblock Finite time analysis of linear two-timescale stochastic approximation with {M}arkovian noise.
\newblock In \emph{Proc. Annual Conference on Learning Theory (CoLT)}, pp.\  2144--2203, 2020.

\bibitem[Khodadadian et~al.(2021)Khodadadian, Chen, and Maguluri]{khodadadian2021finite}
Khodadadian, S., Chen, Z., and Maguluri, S.~T.
\newblock Finite-sample analysis of off-policy natural actor-critic algorithm.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5420--5431. PMLR, 2021.

\bibitem[Khodadadian et~al.(2022)Khodadadian, Doan, Romberg, and Maguluri]{khodadadian2022finite}
Khodadadian, S., Doan, T.~T., Romberg, J., and Maguluri, S.~T.
\newblock Finite sample analysis of two-time-scale natural actor-critic algorithm.
\newblock \emph{IEEE Transactions on Automatic Control}, 2022.

\bibitem[Konda \& Tsitsiklis(2003)Konda and Tsitsiklis]{konda2003onactor}
Konda, V.~R. and Tsitsiklis, J.~N.
\newblock On actor-critic algorithms.
\newblock \emph{SIAM Journal on Control and Optimization}, 42\penalty0 (4):\penalty0 1143--1166, 2003.

\bibitem[Kumar et~al.(2023)Kumar, Koppel, and Ribeiro]{kumar2023sample}
Kumar, H., Koppel, A., and Ribeiro, A.
\newblock On the sample complexity of actor-critic method for reinforcement learning with function approximation.
\newblock \emph{Machine Learning}, pp.\  1--35, 2023.

\bibitem[Lakshminarayanan \& Szepesvari(2018)Lakshminarayanan and Szepesvari]{lakshminarayanan2018linear}
Lakshminarayanan, C. and Szepesvari, C.
\newblock Linear stochastic approximation: {H}ow far does constant step-size and iterate averaging go?
\newblock In \emph{Proc. International Conference on Artificial Intelligence and Statistics}, pp.\  1347--1355, 2018.

\bibitem[Laroche \& des Combes(2021)Laroche and des Combes]{laroche2021dr}
Laroche, R. and des Combes, R.~T.
\newblock Dr jekyll \& mr hyde: the strange case of off-policy policy updates.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Wei, Chi, Gu, and Chen]{li2021softmax}
Li, G., Wei, Y., Chi, Y., Gu, Y., and Chen, Y.
\newblock Softmax policy gradient methods can take exponential time to converge.
\newblock \emph{arXiv preprint arXiv:2102.11270}, 2021{\natexlab{a}}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Guan, Zou, Xu, Liang, and Lan]{li2021faster}
Li, T., Guan, Z., Zou, S., Xu, T., Liang, Y., and Lan, G.
\newblock Faster algorithm and sharper analysis for constrained markov decision process.
\newblock \emph{arXiv preprint arXiv:2110.10351}, 2021{\natexlab{b}}.

\bibitem[Lin(2022)]{xiao2022On}
Lin, X.
\newblock On the convergence rates of policy gradient methods.
\newblock \emph{arXiv preprint arXiv:2201.07443}, 2022.

\bibitem[Liu et~al.(2015)Liu, Liu, Ghavamzadeh, Mahadevan, and Petrik]{liu2015finite}
Liu, B., Liu, J., Ghavamzadeh, M., Mahadevan, S., and Petrik, M.
\newblock Finite-sample analysis of proximal gradient td algorithms.
\newblock In \emph{Proc. International Conference on Uncertainty in Artificial Intelligence (UAI)}, pp.\  504--513, 2015.

\bibitem[Ma et~al.(2020)Ma, Zhou, and Zou]{ma2020variance}
Ma, S., Zhou, Y., and Zou, S.
\newblock Variance-reduced off-policy {TDC} learning: Non-asymptotic convergence analysis.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems (NeurIPS)}, volume~33, pp.\  14796--14806, 2020.

\bibitem[Ma et~al.(2021)Ma, Chen, Zhou, and Zou]{ma2021greedy}
Ma, S., Chen, Z., Zhou, Y., and Zou, S.
\newblock Greedy-gq with variance reduction: Finite-time analysis and improved complexity.
\newblock In \emph{Proc. International Conference on Learning Representations (ICLR)}, 2021.

\bibitem[Mei et~al.(2020)Mei, Xiao, Szepesvari, and Schuurmans]{mei2020global}
Mei, J., Xiao, C., Szepesvari, C., and Schuurmans, D.
\newblock On the global convergence rates of softmax policy gradient methods.
\newblock In \emph{Proc. International Conference on Machine Learning (ICML)}, pp.\  6820--6829, 2020.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and Yoshida]{miyatospectral}
Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y.
\newblock Spectral normalization for generative adversarial networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Neyshabur(2017)]{neyshabur2017implicit}
Neyshabur, B.
\newblock Implicit regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1709.01953}, 2017.

\bibitem[Olshevsky \& Gharesifard(2022)Olshevsky and Gharesifard]{olshevsky2022small}
Olshevsky, A. and Gharesifard, B.
\newblock A small gain analysis of single timescale actor critic.
\newblock \emph{arXiv preprint arXiv:2203.02591}, 2022.

\bibitem[Peters \& Schaal(2008)Peters and Schaal]{peters2008natural}
Peters, J. and Schaal, S.
\newblock Natural actor-critic.
\newblock \emph{Neurocomputing}, 71\penalty0 (7-9):\penalty0 1180--1190, 2008.

\bibitem[Qiu et~al.(2021)Qiu, Yang, Ye, and Wang]{qiu2021finite}
Qiu, S., Yang, Z., Ye, J., and Wang, Z.
\newblock On finite-time convergence of actor-critic algorithm.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory}, 2\penalty0 (2):\penalty0 652--664, 2021.

\bibitem[Srikant \& Ying(2019)Srikant and Ying]{srikant2019finite}
Srikant, R. and Ying, L.
\newblock Finite-time error bounds for linear stochastic approximation and {TD} learning.
\newblock In \emph{Proc. Annual Conference on Learning Theory (CoLT)}, pp.\  2803--2830, Jun. 2019.

\bibitem[Sun et~al.(2019)Sun, Wang, Giannakis, Yang, and Yang]{sun2019finite}
Sun, J., Wang, G., Giannakis, G.~B., Yang, Q., and Yang, Z.
\newblock Finite-sample analysis of decentralized temporal-difference learning with linear function approximation.
\newblock \emph{arXiv preprint arXiv:1911.00934}, 2019.

\bibitem[Suttle et~al.(2023)Suttle, Bedi, Patel, Sadler, Koppel, and Manocha]{suttle2023beyond}
Suttle, W.~A., Bedi, A., Patel, B., Sadler, B.~M., Koppel, A., and Manocha, D.
\newblock Beyond exponentially fast mixing in average-reward reinforcement learning via multi-level monte carlo actor-critic.
\newblock In \emph{International Conference on Machine Learning}, pp.\  33240--33267. PMLR, 2023.

\bibitem[Sutton(1988)]{sutton1988learning}
Sutton, R.~S.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine learning}, 3\penalty0 (1):\penalty0 9--44, 1988.

\bibitem[Sutton(1995)]{sutton1995generalization}
Sutton, R.~S.
\newblock Generalization in reinforcement learning: Successful examples using sparse coarse coding.
\newblock \emph{Advances in neural information processing systems}, 8, 1995.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and Mansour]{sutton1999policy}
Sutton, R.~S., McAllester, D., Singh, S., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function approximation.
\newblock \emph{Advances in neural information processing systems}, 12, 1999.

\bibitem[Tsitsiklis \& Van~Roy(1999)Tsitsiklis and Van~Roy]{tsitsiklis1999average}
Tsitsiklis, J.~N. and Van~Roy, B.
\newblock Average cost temporal-difference learning.
\newblock \emph{Automatica}, 35\penalty0 (11):\penalty0 1799--1808, 1999.

\bibitem[Wang et~al.(2019)Wang, Cai, Yang, and Wang]{wang2019neural}
Wang, L., Cai, Q., Yang, Z., and Wang, Z.
\newblock Neural policy gradient methods: Global optimality and rates of convergence.
\newblock \emph{arXiv preprint arXiv:1909.01150}, 2019.

\bibitem[Wang \& Zou(2020)Wang and Zou]{wang2020finite}
Wang, Y. and Zou, S.
\newblock Finite-sample analysis of {Greedy-GQ} with linear function approximation under {Markovian} noise.
\newblock In \emph{Proc. International Conference on Uncertainty in Artificial Intelligence (UAI)}, volume 124, pp.\  11--20, 2020.

\bibitem[Wang et~al.(2017)Wang, Chen, Liu, Ma, and Liu]{wang2017finite}
Wang, Y., Chen, W., Liu, Y., Ma, Z.-M., and Liu, T.-Y.
\newblock Finite sample analysis of the gtd policy evaluation algorithms in {M}arkov setting.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems (NIPS)}, pp.\  5504--5513, 2017.

\bibitem[Wang et~al.(2021)Wang, Zou, and Zhou]{wang2021non}
Wang, Y., Zou, S., and Zhou, Y.
\newblock Non-asymptotic analysis for two time-scale tdc with general smooth function approximation.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 9747--9758, 2021.

\bibitem[Watkins \& Dayan(1992)Watkins and Dayan]{watkins1992q}
Watkins, C.~J. and Dayan, P.
\newblock {Q}-learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 279--292, 1992.

\bibitem[Wu et~al.(2020)Wu, Zhang, Xu, and Gu]{wu2020finite}
Wu, Y.~F., Zhang, W., Xu, P., and Gu, Q.
\newblock A finite-time analysis of two time-scale actor-critic methods.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 17617--17628, 2020.

\bibitem[Xu \& Gu(2020)Xu and Gu]{xu2020finite}
Xu, P. and Gu, Q.
\newblock A finite-time analysis of {Q}-learning with neural network function approximation.
\newblock In \emph{Proc. International Conference on Machine Learning (ICML)}, pp.\  10555--10565, 2020.

\bibitem[Xu et~al.(2019)Xu, Zou, and Liang]{xu2019two}
Xu, T., Zou, S., and Liang, Y.
\newblock Two time-scale off-policy {TD} learning: Non-asymptotic analysis over {Markovian} samples.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems (NeurIPS)}, pp.\  10633--10643, 2019.

\bibitem[Xu et~al.(2020{\natexlab{a}})Xu, Wang, and Liang]{xu2020improving}
Xu, T., Wang, Z., and Liang, Y.
\newblock Improving sample complexity bounds for (natural) actor-critic algorithms.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems (NeurIPS)}, volume~33, 2020{\natexlab{a}}.

\bibitem[Xu et~al.(2020{\natexlab{b}})Xu, Wang, and Liang]{xu2020non}
Xu, T., Wang, Z., and Liang, Y.
\newblock Non-asymptotic convergence analysis of two time-scale (natural) actor-critic algorithms.
\newblock \emph{arXiv preprint arXiv:2005.03557}, 2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Koppel, Bedi, Szepesvari, and Wang]{zhang2020variational}
Zhang, J., Koppel, A., Bedi, A.~S., Szepesvari, C., and Wang, M.
\newblock Variational policy gradient method for reinforcement learning with general utilities.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems (NeurIPS)}, volume~33, pp.\  4572--4583, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Liu, Yao, and Whiteson]{zhang2020provably}
Zhang, S., Liu, B., Yao, H., and Whiteson, S.
\newblock Provably convergent two-timescale off-policy actor-critic with function approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  11204--11213. PMLR, 2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2021)Zhang, Tachet, and Laroche]{zhang2021global}
Zhang, S., Tachet, R., and Laroche, R.
\newblock Global optimality and finite sample analysis of softmax off-policy actor critic under state distribution mismatch.
\newblock \emph{arXiv preprint arXiv:2111.02997}, 2021.

\bibitem[Zhou \& Lu(2022)Zhou and Lu]{zhou2022single}
Zhou, M. and Lu, J.
\newblock Single time-scale actor-critic method to solve the linear quadratic regulator with convergence guarantees.
\newblock \emph{arXiv preprint arXiv:2202.00048}, 2022.

\bibitem[Zou et~al.(2019)Zou, Xu, and Liang]{zou2019finite}
Zou, S., Xu, T., and Liang, Y.
\newblock Finite-sample analysis for {SARSA} with linear function approximation.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems (NeurIPS)}, pp.\  8665--8675, 2019.

\end{thebibliography}
