\begin{thebibliography}{10}

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock {\em Advances in neural information processing systems},
  25:1097--1105, 2012.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{abdel2014convolutional}
Ossama Abdel-Hamid, Abdel-rahman Mohamed, Hui Jiang, Li~Deng, Gerald Penn, and
  Dong Yu.
\newblock Convolutional neural networks for speech recognition.
\newblock {\em IEEE/ACM Transactions on audio, speech, and language
  processing}, 22(10):1533--1545, 2014.

\bibitem{conneau2016very}
Alexis Conneau, Holger Schwenk, Lo{\"\i}c Barrault, and Yann Lecun.
\newblock Very deep convolutional networks for text classification.
\newblock {\em arXiv preprint arXiv:1606.01781}, 2016.

\bibitem{karim2017lstm}
Fazle Karim, Somshubra Majumdar, Houshang Darabi, and Shun Chen.
\newblock Lstm fully convolutional networks for time series classification.
\newblock {\em IEEE access}, 6:1662--1669, 2017.

\bibitem{lindsay2020conv}
Grace~W. Lindsay.
\newblock {Convolutional Neural Networks as a Model of the Visual System: Past,
  Present, and Future}.
\newblock {\em Journal of Cognitive Neuroscience}, pages 1--15, 02 2020.

\bibitem{hubel1962receptive}
David~H Hubel and Torsten~N Wiesel.
\newblock Receptive fields, binocular interaction and functional architecture
  in the cat's visual cortex.
\newblock {\em The Journal of physiology}, 160(1):106--154, 1962.

\bibitem{fukushima1982neocognitron}
Kunihiko Fukushima and Sei Miyake.
\newblock Neocognitron: A self-organizing neural network model for a mechanism
  of visual pattern recognition.
\newblock In {\em Competition and cooperation in neural nets}, pages 267--285.
  Springer, 1982.

\bibitem{yamins2014performance}
Daniel~LK Yamins, Ha~Hong, Charles~F Cadieu, Ethan~A Solomon, Darren Seibert,
  and James~J DiCarlo.
\newblock Performance-optimized hierarchical models predict neural responses in
  higher visual cortex.
\newblock {\em Proceedings of the national academy of sciences},
  111(23):8619--8624, 2014.

\bibitem{khaligh2014deep}
Seyed-Mahdi Khaligh-Razavi and Nikolaus Kriegeskorte.
\newblock Deep supervised, but not unsupervised, models may explain it cortical
  representation.
\newblock {\em PLoS computational biology}, 10(11):e1003915, 2014.

\bibitem{schrimpf2018brain}
Martin Schrimpf, Jonas Kubilius, Ha~Hong, Najib~J. Majaj, Rishi Rajalingham,
  Elias~B. Issa, Kohitij Kar, Pouya Bashivan, Jonathan Prescott-Roy, Franziska
  Geiger, Kailyn Schmidt, Daniel L.~K. Yamins, and James~J. DiCarlo.
\newblock Brain-score: Which artificial neural network for object recognition
  is most brain-like?
\newblock {\em bioRxiv preprint}, 2018.

\bibitem{cadena2019deep}
Santiago~A Cadena, George~H Denfield, Edgar~Y Walker, Leon~A Gatys, Andreas~S
  Tolias, Matthias Bethge, and Alexander~S Ecker.
\newblock Deep convolutional models improve predictions of macaque v1 responses
  to natural images.
\newblock {\em PLoS computational biology}, 15(4):e1006897, 2019.

\bibitem{grossberg1987competitive}
Stephen Grossberg.
\newblock Competitive learning: From interactive activation to adaptive
  resonance.
\newblock {\em Cognitive science}, 11(1):23--63, 1987.

\bibitem{bartunov2018assessing}
Sergey Bartunov, Adam Santoro, Blake Richards, Luke Marris, Geoffrey~E Hinton,
  and Timothy Lillicrap.
\newblock Assessing the scalability of biologically-motivated deep learning
  algorithms and architectures.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9368--9378, 2018.

\bibitem{d2019finding}
St{\'e}phane d'Ascoli, Levent Sagun, Joan Bruna, and Giulio Biroli.
\newblock Finding the needle in the haystack with convolutions: on the benefits
  of architectural bias.
\newblock {\em arXiv preprint arXiv:1906.06766}, 2019.

\bibitem{Schrimpf2020integrative}
Martin Schrimpf, Jonas Kubilius, Michael~J Lee, N~Apurva~Ratan Murty, Robert
  Ajemian, and James~J DiCarlo.
\newblock Integrative benchmarking to advance neurally mechanistic models of
  human intelligence.
\newblock {\em Neuron}, 2020.

\bibitem{richards2019deep}
Blake~A Richards, Timothy~P Lillicrap, Philippe Beaudoin, Yoshua Bengio, Rafal
  Bogacz, Amelia Christensen, Claudia Clopath, Rui~Ponte Costa, Archy
  de~Berker, Surya Ganguli, et~al.
\newblock A deep learning framework for neuroscience.
\newblock {\em Nature neuroscience}, 22(11):1761--1770, 2019.

\bibitem{nokland2016direct}
Arild N{\o}kland.
\newblock Direct feedback alignment provides learning in deep neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1037--1045, 2016.

\bibitem{moskovitz2018feedback}
Theodore~H Moskovitz, Ashok Litwin-Kumar, and LF~Abbott.
\newblock Feedback alignment in deep convolutional networks.
\newblock {\em arXiv preprint arXiv:1812.06488}, 2018.

\bibitem{mostafa2018deep}
Hesham Mostafa, Vishwajith Ramesh, and Gert Cauwenberghs.
\newblock Deep supervised learning using local errors.
\newblock {\em Frontiers in neuroscience}, 12:608, 2018.

\bibitem{nokland2019training}
Arild N{\o}kland and Lars~Hiller Eidnes.
\newblock Training neural networks with local error signals.
\newblock {\em arXiv preprint arXiv:1901.06656}, 2019.

\bibitem{akrout2019deep}
Mohamed Akrout, Collin Wilson, Peter Humphreys, Timothy Lillicrap, and
  Douglas~B Tweed.
\newblock Deep learning without weight transport.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  974--982, 2019.

\bibitem{laborieux2020scaling}
Axel Laborieux, Maxence Ernoult, Benjamin Scellier, Yoshua Bengio, Julie
  Grollier, and Damien Querlioz.
\newblock Scaling equilibrium propagation to deep convnets by drastically
  reducing its gradient estimator bias.
\newblock {\em arXiv preprint arXiv:2006.03824}, 2020.

\bibitem{pogodin2020kernelized}
Roman Pogodin and Peter~E Latham.
\newblock Kernelized information bottleneck leads to biologically plausible
  3-factor hebbian learning in deep networks.
\newblock {\em arXiv preprint arXiv:2006.07123}, 2020.

\bibitem{neyshabur2020towards}
Behnam Neyshabur.
\newblock Towards learning convolutions from scratch.
\newblock {\em arXiv preprint arXiv:2007.13657}, 2020.

\bibitem{pmlr-v119-elsayed20a}
Gamaleldin Elsayed, Prajit Ramachandran, Jonathon Shlens, and Simon Kornblith.
\newblock Revisiting spatial invariance with low-rank local connectivity.
\newblock In Hal~Daum√© III and Aarti Singh, editors, {\em Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 2868--2879. PMLR, 13--18 Jul
  2020.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{carion2020end}
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
  Kirillov, and Sergey Zagoruyko.
\newblock End-to-end object detection with transformers.
\newblock In {\em European Conference on Computer Vision}, pages 213--229.
  Springer, 2020.

\bibitem{touvron2020training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock {\em arXiv preprint arXiv:2012.12877}, 2020.

\bibitem{zhu2020deformable}
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
\newblock Deformable detr: Deformable transformers for end-to-end object
  detection.
\newblock {\em arXiv preprint arXiv:2010.04159}, 2020.

\bibitem{tolstikhin2021mlpmixer}
Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai,
  Thomas Unterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario
  Lucic, and Alexey Dosovitskiy.
\newblock Mlp-mixer: An all-mlp architecture for vision, 2021.

\bibitem{touvron2021resmlp}
Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin
  El-Nouby, Edouard Grave, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and
  Herv{\'e} J{\'e}gou.
\newblock Resmlp: Feedforward networks for image classification with
  data-efficient training.
\newblock {\em arXiv preprint arXiv:2105.03404}, 2021.

\bibitem{liu2021pay}
Hanxiao Liu, Zihang Dai, David~R So, and Quoc~V Le.
\newblock Pay attention to mlps.
\newblock {\em arXiv preprint arXiv:2105.08050}, 2021.

\bibitem{krizhevsky2009cifar}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{le2015tiny}
Ya~Le and Xuan Yang.
\newblock Tiny imagenet visual recognition challenge.
\newblock {\em CS 231N}, 7:7, 2015.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{NEURIPS2019_9015}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems 32}, pages 8024--8035. Curran Associates,
  Inc., 2019.

\bibitem{Ott2020LearningIT}
Jordan Ott, Erik~J. Linstead, Nicholas LaHaye, and Pierre Baldi.
\newblock Learning in the machine: To share or not to share?
\newblock {\em Neural networks : the official journal of the International
  Neural Network Society}, 126:235--249, 2020.

\bibitem{Freeman2013}
Jeremy Freeman, Corey~M. Ziemba, David~J. Heeger, Eero~P. Simoncelli, and
  J.~Anthony Movshon.
\newblock A functional and perceptual signature of the second visual area in
  primates.
\newblock {\em Nature Neuroscience}, 16(7):974--981, Jul 2013.

\bibitem{marques2021multi}
Tiago Marques, Martin Schrimpf, and James~J DiCarlo.
\newblock Multi-scale hierarchical neural network models that bridge from
  single neurons in the primate primary visual cortex to object recognition
  behavior.
\newblock {\em bioRxiv}, 2021.

\bibitem{Majaj13402}
Najib~J. Majaj, Ha~Hong, Ethan~A. Solomon, and James~J. DiCarlo.
\newblock Simple learned weighted sums of inferior temporal neuronal firing
  rates accurately predict human core object recognition performance.
\newblock {\em Journal of Neuroscience}, 35(39):13402--13418, 2015.

\bibitem{Rajalingham240614}
Rishi Rajalingham, Elias~B. Issa, Pouya Bashivan, Kohitij Kar, Kailyn Schmidt,
  and James~J. DiCarlo.
\newblock Large-scale, high-resolution comparison of the core visual object
  recognition behavior of humans, monkeys, and state-of-the-art deep artificial
  neural networks.
\newblock {\em bioRxiv}, 2018.

\bibitem{Riesenhuber1999HierarchicalMO}
Maximilian Riesenhuber and Tomaso~A. Poggio.
\newblock Hierarchical models of object recognition in cortex.
\newblock {\em Nature Neuroscience}, 2:1019--1025, 1999.

\bibitem{foldiak1991learning}
Peter F{\"o}ldi{\'a}k.
\newblock Learning invariance from transformation sequences.
\newblock {\em Neural computation}, 3(2):194--200, 1991.

\bibitem{wallis1993learning}
Guy Wallis, Edmund Rolls, and Peter Foldiak.
\newblock Learning invariant responses to the natural transformations of
  objects.
\newblock In {\em Proceedings of 1993 International Conference on Neural
  Networks (IJCNN-93-Nagoya, Japan)}, volume~2, pages 1087--1090. IEEE, 1993.

\bibitem{wiskott2002slow}
Laurenz Wiskott and Terrence~J Sejnowski.
\newblock Slow feature analysis: Unsupervised learning of invariances.
\newblock {\em Neural computation}, 14(4):715--770, 2002.

\bibitem{Jha2005SleepDependentPR}
Sushil~K Jha, Brian~E. Jones, Tammi Coleman, Nick Steinmetz, Chi-Tat Law,
  Gerald~D. Griffin, Joshua~D. Hawk, Nooreen Dabbish, Valery~A. Kalatsky, and
  Marcos~G. Frank.
\newblock Sleep-dependent plasticity requires cortical activity.
\newblock {\em The Journal of Neuroscience}, 25:9266 -- 9274, 2005.

\bibitem{puentes2017linking}
Carlos Puentes-Mestril and Sara~J Aton.
\newblock Linking network activity to synaptic plasticity during sleep:
  hypotheses and recent data.
\newblock {\em Frontiers in neural circuits}, 11:61, 2017.

\bibitem{fitzpatrick1997}
William~H. Bosking, Ying Zhang, Brett Schofield, and David Fitzpatrick.
\newblock Orientation selectivity and the arrangement of horizontal connections
  in tree shrew striate cortex.
\newblock {\em J. Neurosci.}, 15, 1997.

\bibitem{gower2019sgd}
Robert~Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor
  Shulgin, and Peter Richt{\'a}rik.
\newblock Sgd: General analysis and improved rates.
\newblock In {\em International Conference on Machine Learning}, pages
  5200--5209. PMLR, 2019.

\bibitem{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034, 2015.

\end{thebibliography}
