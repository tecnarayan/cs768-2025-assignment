\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bemis and Murcko(1996)]{bemis1996properties}
Guy~W Bemis and Mark~A Murcko.
\newblock The properties of known drugs. 1. molecular frameworks.
\newblock \emph{Journal of medicinal chemistry}, 39\penalty0 (15):\penalty0
  2887--2893, 1996.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pages
  1597--1607. PMLR, 2020.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  1126--1135. PMLR, 2017.

\bibitem[Gaulton et~al.(2012)Gaulton, Bellis, Bento, Chambers, Davies, Hersey,
  Light, McGlinchey, Michalovich, Al-Lazikani, et~al.]{gaulton2012chembl}
Anna Gaulton, Louisa~J Bellis, A~Patricia Bento, Jon Chambers, Mark Davies,
  Anne Hersey, Yvonne Light, Shaun McGlinchey, David Michalovich, Bissan
  Al-Lazikani, et~al.
\newblock Chembl: a large-scale bioactivity database for drug discovery.
\newblock \emph{Nucleic acids research}, 40\penalty0 (D1):\penalty0
  D1100--D1107, 2012.

\bibitem[Hamilton et~al.(2017)Hamilton, Ying, and
  Leskovec]{hamilton2017representation}
William~L Hamilton, Rex Ying, and Jure Leskovec.
\newblock Representation learning on graphs: Methods and applications.
\newblock \emph{arXiv preprint arXiv:1709.05584}, 2017.

\bibitem[Hassani and Khasahmadi(2020)]{hassani2020contrastive}
Kaveh Hassani and Amir~Hosein Khasahmadi.
\newblock Contrastive multi-view representation learning on graphs.
\newblock In \emph{International Conference on Machine Learning}, pages
  4116--4126. PMLR, 2020.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2020moco}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross~B. Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{CVPR}, pages 9726--9735, 2020.

\bibitem[Hu et~al.(2019)Hu, Liu, Gomes, Zitnik, Liang, Pande, and
  Leskovec]{hu2019strategies}
Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande,
  and Jure Leskovec.
\newblock Strategies for pre-training graph neural networks.
\newblock \emph{arXiv preprint arXiv:1905.12265}, 2019.

\bibitem[Hu et~al.(2020)Hu, Dong, Wang, Chang, and Sun]{hu2020gpt}
Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun.
\newblock Gpt-gnn: Generative pre-training of graph neural networks.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 1857--1867, 2020.

\bibitem[Landrum(2016)]{landrum2016rdkit}
G~Landrum.
\newblock Rdkit: Open-source cheminformatics software, 2016.

\bibitem[Langley(2000)]{langley00}
P.~Langley.
\newblock Crafting papers on machine learning.
\newblock In Pat Langley, editor, \emph{Proceedings of the 17th International
  Conference on Machine Learning (ICML 2000)}, pages 1207--1216, Stanford, CA,
  2000. Morgan Kaufmann.

\bibitem[Liu et~al.(2020)Liu, Gao, and Ji]{liu2020towards}
Meng Liu, Hongyang Gao, and Shuiwang Ji.
\newblock Towards deeper graph neural networks.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 338--348, 2020.

\bibitem[Patel et~al.(2020)Patel, Ihlenfeldt, Judson, Moroz, Pevzner, Peach,
  Tarasova, and Nicklaus]{patel2020synthetically}
Hitesh Patel, Wolf Ihlenfeldt, Philip Judson, Yurii~S Moroz, Yuri Pevzner,
  Megan Peach, Nadya Tarasova, and Marc Nicklaus.
\newblock Synthetically accessible virtual inventory (savi).
\newblock 2020.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}, 2019.

\bibitem[Ramsundar et~al.(2019)Ramsundar, Eastman, Walters, and
  Pande]{ramsundar2019deep}
Bharath Ramsundar, Peter Eastman, Patrick Walters, and Vijay Pande.
\newblock \emph{Deep learning for the life sciences: applying deep learning to
  genomics, microscopy, drug discovery, and more}.
\newblock " O'Reilly Media, Inc.", 2019.

\bibitem[Rong et~al.(2020)Rong, Bian, Xu, Xie, Wei, Huang, and
  Huang]{rong2020self}
Yu~Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and
  Junzhou Huang.
\newblock Self-supervised graph transformer on large-scale molecular data.
\newblock \emph{arXiv preprint arXiv:2007.02835}, 2020.

\bibitem[Shervashidze et~al.(2011)Shervashidze, Schweitzer, Van~Leeuwen,
  Mehlhorn, and Borgwardt]{shervashidze2011weisfeiler}
Nino Shervashidze, Pascal Schweitzer, Erik~Jan Van~Leeuwen, Kurt Mehlhorn, and
  Karsten~M Borgwardt.
\newblock Weisfeiler-lehman graph kernels.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0 (9), 2011.

\bibitem[Smith(2009)]{smith2009chiral}
Silas~W Smith.
\newblock Chiral toxicology: it's the same thingâ€¦ only different.
\newblock \emph{Toxicological sciences}, 110\penalty0 (1):\penalty0 4--30,
  2009.

\bibitem[Sterling and Irwin(2015)]{sterling2015zinc}
Teague Sterling and John~J Irwin.
\newblock Zinc 15--ligand discovery for everyone.
\newblock \emph{Journal of chemical information and modeling}, 55\penalty0
  (11):\penalty0 2324--2337, 2015.

\bibitem[Veli{\v{c}}kovi{\'c} et~al.(2017)Veli{\v{c}}kovi{\'c}, Cucurull,
  Casanova, Romero, Lio, and Bengio]{velivckovic2017graph}
Petar Veli{\v{c}}kovi{\'c}, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
  Pietro Lio, and Yoshua Bengio.
\newblock Graph attention networks.
\newblock \emph{arXiv preprint arXiv:1710.10903}, 2017.

\bibitem[Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and
  Le]{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian
  Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock \emph{arXiv preprint arXiv:2109.01652}, 2021.

\bibitem[Wu et~al.(2021)Wu, Lin, Gao, Tan, Li, et~al.]{wu2021self}
Lirong Wu, Haitao Lin, Zhangyang Gao, Cheng Tan, Stan Li, et~al.
\newblock Self-supervised on graphs: Contrastive, generative, or predictive.
\newblock \emph{arXiv preprint arXiv:2105.07342}, 2021.

\bibitem[Wu et~al.(2018)Wu, Ramsundar, Feinberg, Gomes, Geniesse, Pappu,
  Leswing, and Pande]{wu2018moleculenet}
Zhenqin Wu, Bharath Ramsundar, Evan~N Feinberg, Joseph Gomes, Caleb Geniesse,
  Aneesh~S Pappu, Karl Leswing, and Vijay Pande.
\newblock Moleculenet: a benchmark for molecular machine learning.
\newblock \emph{Chemical science}, 9\penalty0 (2):\penalty0 513--530, 2018.

\bibitem[Xie et~al.(2021)Xie, Xu, Zhang, Wang, and Ji]{xie2021self}
Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and Shuiwang Ji.
\newblock Self-supervised learning of graph neural networks: A unified review.
\newblock \emph{arXiv preprint arXiv:2102.10757}, 2021.

\bibitem[Xu et~al.(2018)Xu, Hu, Leskovec, and Jegelka]{xu2018powerful}
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
\newblock How powerful are graph neural networks?
\newblock \emph{arXiv preprint arXiv:1810.00826}, 2018.

\bibitem[Ying et~al.(2021)Ying, Cai, Luo, Zheng, Ke, He, Shen, and
  Liu]{ying2021transformers}
Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di~He,
  Yanming Shen, and Tie-Yan Liu.
\newblock Do transformers really perform bad for graph representation?
\newblock \emph{arXiv preprint arXiv:2106.05234}, 2021.

\bibitem[You et~al.(2020{\natexlab{a}})You, Chen, Sui, Chen, Wang, and
  Shen]{you2020graph}
Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang
  Shen.
\newblock Graph contrastive learning with augmentations.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 5812--5823, 2020{\natexlab{a}}.

\bibitem[You et~al.(2020{\natexlab{b}})You, Chen, Wang, and Shen]{you2020does}
Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen.
\newblock When does self-supervision help graph convolutional networks?
\newblock In \emph{International Conference on Machine Learning}, pages
  10871--10880. PMLR, 2020{\natexlab{b}}.

\bibitem[Zhu et~al.(2021)Zhu, Xu, Yu, Liu, Wu, and Wang]{zhu2021graph}
Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang.
\newblock Graph contrastive learning with adaptive augmentation.
\newblock In \emph{Proceedings of the Web Conference 2021}, pages 2069--2080,
  2021.

\end{thebibliography}
