\begin{thebibliography}{10}

\bibitem{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, George Necula, Adam Paszke, Jake Vander{P}las, Skye
  Wanderman-{M}ilne, and Qiao Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.

\bibitem{gpt3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock {\em CoRR}, abs/2005.14165, 2020.

\bibitem{chihara2011introduction}
T.S. Chihara.
\newblock {\em An Introduction to Orthogonal Polynomials}.
\newblock Dover Books on Mathematics. Dover Publications, 2011.

\bibitem{sparse_trsf}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock {\em CoRR}, abs/1904.10509, 2019.

\bibitem{performer}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tam{\'{a}}s Sarl{\'{o}}s, Peter Hawkins, Jared Davis, Afroz
  Mohiuddin, Lukasz Kaiser, David Belanger, Lucy~J. Colwell, and Adrian Weller.
\newblock Rethinking attention with performers.
\newblock {\em CoRR}, abs/2009.14794, 2020.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
  Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran,
  Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob
  Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
  Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,
  Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
  Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
  Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai,
  Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
  Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
  Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
  Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
\newblock Palm: Scaling language modeling with pathways, 2022.

\bibitem{CooleyTukey}
James Cooley and John Tukey.
\newblock An algorithm for the machine calculation of complex fourier series.
\newblock {\em Mathematics of Computation}, 19(90):297--301, 1965.

\bibitem{dai2019transformer}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc~V Le, and Ruslan
  Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock {\em arXiv preprint arXiv:1901.02860}, 2019.

\bibitem{bert}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em CoRR}, abs/1810.04805, 2018.

\bibitem{fu2023hungry}
Daniel~Y. Fu, Tri Dao, Khaled~K. Saab, Armin~W. Thomas, Atri Rudra, and
  Christopher Ré.
\newblock Hungry hungry hippos: Towards language modeling with state space
  models, 2023.

\bibitem{fu2023simple}
Daniel~Y. Fu, Elliot~L. Epstein, Eric Nguyen, Armin~W. Thomas, Michael Zhang,
  Tri Dao, Atri Rudra, and Christopher Ré.
\newblock Simple hardware-efficient long convolutions for sequence modeling,
  2023.

\bibitem{gu2020hippo}
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re.
\newblock Hippo: Recurrent memory with optimal polynomial projections, 2020.

\bibitem{gu2022on}
Albert Gu, Karan Goel, Ankit Gupta, and Christopher R{\'e}.
\newblock On the parameterization and initialization of diagonal state space
  models.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{gu2022efficiently}
Albert Gu, Karan Goel, and Christopher Ré.
\newblock Efficiently modeling long sequences with structured state spaces,
  2022.

\bibitem{gu2022parameterization}
Albert Gu, Ankit Gupta, Karan Goel, and Christopher Ré.
\newblock On the parameterization and initialization of diagonal state space
  models, 2022.

\bibitem{gupta2022diagonal}
Ankit Gupta, Albert Gu, and Jonathan Berant.
\newblock Diagonal state spaces are as effective as structured state spaces,
  2022.

\bibitem{flax2020github}
Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand
  Rondepierre, Andreas Steiner, and Marc van {Z}ee.
\newblock {F}lax: A neural network library and ecosystem for {JAX}, 2023.

\bibitem{journals/ijufks/Hochreiter98}
Sepp Hochreiter.
\newblock The vanishing gradient problem during learning recurrent neural nets
  and problem solutions.
\newblock {\em International Journal of Uncertainty, Fuzziness and
  Knowledge-Based Systems}, 6(2):107--116, 1998.

\bibitem{HochSchm97}
Sepp Hochreiter and Jürgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural Computation}, 9(8):1735--1780, 1997.

\bibitem{pmlr-v162-hua22a}
Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le.
\newblock Transformer quality in linear time.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, {\em Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of {\em Proceedings
  of Machine Learning Research}, pages 9099--9117. PMLR, 17--23 Jul 2022.

\bibitem{hutchins2022block}
DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur.
\newblock Block-recurrent transformers.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{linear_trsf}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois
  Fleuret.
\newblock Transformers are {RNN}s: Fast autoregressive transformers with linear
  attention.
\newblock In Hal~Daumé III and Aarti Singh, editors, {\em Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 5156--5165. PMLR, 13--18 Jul
  2020.

\bibitem{DBLP:journals/corr/abs-1805-04623}
Urvashi Khandelwal, He~He, Peng Qi, and Dan Jurafsky.
\newblock Sharp nearby, fuzzy far away: How neural language models use context.
\newblock {\em CoRR}, abs/1805.04623, 2018.

\bibitem{KingBa15}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  San Diega, CA, USA, 2015.

\bibitem{2015-kingma}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In Yoshua Bengio and Yann LeCun, editors, {\em ICLR (Poster)}, 2015.

\bibitem{li2022the}
Conglong Li, Minjia Zhang, and Yuxiong He.
\newblock The stability-efficiency dilemma: Investigating sequence length
  warmup for training {GPT} models.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{ma2023mega}
Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig,
  Jonathan May, and Luke Zettlemoyer.
\newblock Mega: Moving average equipped gated attention, 2023.

\bibitem{mehta2023long}
Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur.
\newblock Long range language modeling via gated state spaces.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem{poli2023hyena}
Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel~Y. Fu, Tri Dao, Stephen
  Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré.
\newblock Hyena hierarchy: Towards larger convolutional language models, 2023.

\bibitem{pg19_raecompressive2019}
Jack~W. Rae, Anna Potapenko, Siddhant~M. Jayakumar, Chloe Hillier, and
  Timothy~P. Lillicrap.
\newblock Compressive transformers for long-range sequence modelling.
\newblock In {\em 8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.

\bibitem{t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em CoRR}, abs/1910.10683, 2019.

\bibitem{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em J. Mach. Learn. Res.}, 21:140:1--140:67, 2020.

\bibitem{smith2023simplified}
Jimmy T.~H. Smith, Andrew Warrington, and Scott~W. Linderman.
\newblock Simplified state space layers for sequence modeling, 2023.

\bibitem{DBLP:journals/corr/abs-2005-00581}
Sandeep Subramanian, Ronan Collobert, Marc'Aurelio Ranzato, and Y{-}Lan
  Boureau.
\newblock Multi-scale transformer language models.
\newblock {\em CoRR}, abs/2005.00581, 2020.

\bibitem{sinkhorn_trsf}
Yi~Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan.
\newblock Sparse {S}inkhorn attention.
\newblock In Hal~Daumé III and Aarti Singh, editors, {\em Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 9438--9447. PMLR, 13--18 Jul
  2020.

\bibitem{tay2020long}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
  Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock Long range arena: A benchmark for efficient transformers, 2020.

\bibitem{tay2021long}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
  Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock Long range arena : A benchmark for efficient transformers.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{lamda}
Romal Thoppilan, Daniel~De Freitas, Jamie Hall, Noam Shazeer, Apoorv
  Kulshreshtha, Heng{-}Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du,
  YaGuang Li, Hongrae Lee, Huaixiu~Steven Zheng, Amin Ghafouri, Marcelo
  Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao
  Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou,
  Chung{-}Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen~S.
  Meier{-}Hellstern, Meredith~Ringel Morris, Tulsee Doshi, Renelito~Delos
  Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran,
  Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin
  Hoffman{-}John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew
  Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray
  Kurzweil, Blaise Aguera{-}Arcas, Claire Cui, Marian Croak, Ed~H. Chi, and
  Quoc Le.
\newblock Lamda: Language models for dialog applications.
\newblock {\em CoRR}, abs/2201.08239, 2022.

\bibitem{transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem{linformer}
Sinong Wang, Belinda~Z. Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock {\em CoRR}, abs/2006.04768, 2020.

\bibitem{wu2022memorizing}
Yuhuai Wu, Markus~Norman Rabe, DeLesley Hutchins, and Christian Szegedy.
\newblock Memorizing transformers.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{zaheer2020bigbird}
Manzil Zaheer, Guru Guruganesh, Kumar~Avinava Dubey, Joshua Ainslie, Chris
  Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li~Yang,
  et~al.
\newblock Big bird: Transformers for longer sequences.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\end{thebibliography}
