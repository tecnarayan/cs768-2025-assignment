\begin{thebibliography}{10}

\bibitem{Werbos:74}
P.~J. Werbos.
\newblock {\em Beyond Regression: New Tools for Prediction and Analysis in the
  Behavioral Sciences}.
\newblock PhD thesis, Harvard University, 1974.

\bibitem{Rumelhart:86}
D.~E. Rumelhart, G.~E. Hinton, and R.~J. Williams.
\newblock Learning internal representations by error propagation.
\newblock In {\em Parallel Distributed Processing}, volume~1, pages 318--362.
  MIT Press, 1986.

\bibitem{jaderberg2017decoupled}
Max Jaderberg, Wojciech~Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex
  Graves, David Silver, and Koray Kavukcuoglu.
\newblock Decoupled neural interfaces using synthetic gradients.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1627--1635, 2017.

\bibitem{crick1989recent}
Francis Crick.
\newblock The recent excitement about neural networks.
\newblock {\em Nature}, 337(6203):129--132, 1989.

\bibitem{marblestone2016toward}
Adam~H Marblestone, Greg Wayne, and Konrad~P Kording.
\newblock Toward an integration of deep learning and neuroscience.
\newblock {\em Frontiers in computational neuroscience}, 10:94, 2016.

\bibitem{grossberg1987competitive}
Stephen Grossberg.
\newblock Competitive learning: From interactive activation to adaptive
  resonance.
\newblock {\em Cognitive science}, 11(1):23--63, 1987.

\bibitem{movellan1991contrastive}
Javier~R Movellan.
\newblock Contrastive hebbian learning in the continuous hopfield model.
\newblock In {\em Connectionist models}, pages 10--17. Elsevier, 1991.

\bibitem{o1996biologically}
Randall~C O'Reilly.
\newblock Biologically plausible error-driven learning using local activation
  differences: The generalized recirculation algorithm.
\newblock {\em Neural computation}, 8(5):895--938, 1996.

\bibitem{salakhutdinov2009deep}
Ruslan Salakhutdinov and Geoffrey Hinton.
\newblock Deep boltzmann machines.
\newblock In {\em Artificial intelligence and statistics}, pages 448--455,
  2009.

\bibitem{le1986learning}
Yann Le~Cun.
\newblock Learning process in an asymmetric threshold network.
\newblock In {\em Disordered systems and biological organization}, pages
  233--240. Springer, 1986.

\bibitem{bengio2014auto}
Yoshua Bengio.
\newblock How auto-encoders could provide credit assignment in deep networks
  via target propagation.
\newblock {\em arXiv preprint arXiv:1407.7906}, 2014.

\bibitem{lee2015difference}
Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio.
\newblock Difference target propagation.
\newblock In {\em Joint european conference on machine learning and knowledge
  discovery in databases}, pages 498--515. Springer, 2015.

\bibitem{lillicrap2016random}
Timothy~P Lillicrap, Daniel Cownden, Douglas~B Tweed, and Colin~J Akerman.
\newblock Random synaptic feedback weights support error backpropagation for
  deep learning.
\newblock {\em Nature communications}, 7(1):1--10, 2016.

\bibitem{belilovsky2019greedy}
Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon.
\newblock Greedy layerwise learning can scale to imagenet.
\newblock In {\em International Conference on Machine Learning}, pages
  583--593, 2019.

\bibitem{czarnecki2017sobolev}
Wojciech~M Czarnecki, Simon Osindero, Max Jaderberg, Grzegorz Swirszcz, and
  Razvan Pascanu.
\newblock Sobolev training for neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4278--4287, 2017.

\bibitem{nokland2019training}
Arild N{\o}kland and Lars~Hiller Eidnes.
\newblock Training neural networks with local error signals.
\newblock In {\em International Conference on Machine Learning}, pages
  4839--4850, 2019.

\bibitem{hjelm2018learning}
R~Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil
  Bachman, Adam Trischler, and Yoshua Bengio.
\newblock Learning deep representations by mutual information estimation and
  maximization.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{lowe2019putting}
Sindy L{\"o}we, Peter O'Connor, and Bastiaan Veeling.
\newblock Putting an end to end-to-end: Gradient-isolated learning of
  representations.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3033--3045, 2019.

\bibitem{nokland2016direct}
Arild N{\o}kland.
\newblock Direct feedback alignment provides learning in deep neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1037--1045, 2016.

\bibitem{bartunov2018assessing}
Sergey Bartunov, Adam Santoro, Blake Richards, Luke Marris, Geoffrey~E Hinton,
  and Timothy Lillicrap.
\newblock Assessing the scalability of biologically-motivated deep learning
  algorithms and architectures.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9368--9378, 2018.

\bibitem{lillicrap2020backpropagation}
Timothy~P Lillicrap, Adam Santoro, Luke Marris, Colin~J Akerman, and Geoffrey
  Hinton.
\newblock Backpropagation and the brain.
\newblock {\em Nature Reviews Neuroscience}, pages 1--12, 2020.

\bibitem{caporale2008spike}
Natalia Caporale and Yang Dan.
\newblock Spike timing--dependent plasticity: a hebbian learning rule.
\newblock {\em Annu. Rev. Neurosci.}, 31:25--46, 2008.

\bibitem{Lansdell2020Learning}
Benjamin~James Lansdell, Prashanth~Ravi Prakash, and Konrad~Paul Kording.
\newblock Learning to solve the credit assignment problem.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{launay2019principled}
Julien Launay, Iacopo Poli, and Florent Krzakala.
\newblock Principled training of neural networks with direct feedback
  alignment.
\newblock {\em arXiv preprint arXiv:1906.04554}, 2019.

\bibitem{crafton2019direct}
Brian Crafton, Abhinav Parihar, Evan Gebhardt, and Arijit Raychowdhury.
\newblock Direct feedback alignment with sparse connections for local learning.
\newblock {\em Frontiers in neuroscience}, 13:525, 2019.

\bibitem{liao2016important}
Qianli Liao, Joel~Z Leibo, and Tomaso Poggio.
\newblock How important is weight symmetry in backpropagation?
\newblock In {\em Thirtieth AAAI Conference on Artificial Intelligence}, 2016.

\bibitem{moskovitz2018feedback}
Theodore~H Moskovitz, Ashok Litwin-Kumar, and LF~Abbott.
\newblock Feedback alignment in deep convolutional networks.
\newblock {\em arXiv preprint arXiv:1812.06488}, 2018.

\bibitem{xiao2018biologicallyplausible}
Will Xiao, Honglin Chen, Qianli Liao, and Tomaso Poggio.
\newblock Biologically-plausible learning algorithms can scale to large
  datasets.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{kunin2020routes}
Daniel Kunin, Aran Nayebi, Javier Sagastuy-Brena, Surya Ganguli, Jonathan~M.
  Bloom, and Daniel L.~K. Yamins.
\newblock Two routes to scalable credit assignment without weight symmetry.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}, 2020.

\bibitem{akrout2019using}
Mohamed Akrout, Collin Wilson, Peter~C Humphreys, Timothy Lillicrap, and
  Douglas Tweed.
\newblock Using weight mirrors to improve feedback alignment.
\newblock {\em arXiv preprint arXiv:1904.05391}, 2019.

\bibitem{launay2020lightintheloop}
Julien Launay, Iacopo Poli, Kilian Müller, Igor Carron, Laurent Daudet,
  Florent Krzakala, and Sylvain Gigan.
\newblock Light-in-the-loop: using a photonics co-processor for scalable
  training of neural networks, 2020.

\bibitem{frenkel2020bottom}
Charlotte Frenkel.
\newblock {\em Bottom-Up and Top-Down Neuromorphic Processor Design: Unveiling
  Roads to Embedded Cognition}.
\newblock PhD thesis, UCL-Universit{\'e} Catholique de Louvain, 2020.

\bibitem{penner2017soft}
Eric Penner and Li~Zhang.
\newblock Soft 3d reconstruction for view synthesis.
\newblock {\em ACM Transactions on Graphics (TOG)}, 36(6):1--11, 2017.

\bibitem{flynn2019deepview}
John Flynn, Michael Broxton, Paul Debevec, Matthew DuVall, Graham Fyffe, Ryan
  Overbeck, Noah Snavely, and Richard Tucker.
\newblock Deepview: View synthesis with learned gradient descent.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 2367--2376, 2019.

\bibitem{mildenhall2019local}
Ben Mildenhall, Pratul~P Srinivasan, Rodrigo Ortiz-Cayon, Nima~Khademi
  Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar.
\newblock Local light field fusion: Practical view synthesis with prescriptive
  sampling guidelines.
\newblock {\em ACM Transactions on Graphics (TOG)}, 38(4):1--14, 2019.

\bibitem{sitzmann2019deepvoxels}
Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nie{\ss}ner, Gordon
  Wetzstein, and Michael Zollhofer.
\newblock Deepvoxels: Learning persistent 3d feature embeddings.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 2437--2446, 2019.

\bibitem{lombardi2019neural}
Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas
  Lehrmann, and Yaser Sheikh.
\newblock Neural volumes: Learning dynamic renderable volumes from images.
\newblock {\em ACM Transactions on Graphics (TOG)}, 38(4):65, 2019.

\bibitem{sitzmann2019scene}
Vincent Sitzmann, Michael Zollh{\"o}fer, and Gordon Wetzstein.
\newblock Scene representation networks: Continuous 3d-structure-aware neural
  scene representations.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1119--1130, 2019.

\bibitem{mildenhall2020nerf}
Ben Mildenhall, Pratul~P Srinivasan, Matthew Tancik, Jonathan~T Barron, Ravi
  Ramamoorthi, and Ren Ng.
\newblock Nerf: Representing scenes as neural radiance fields for view
  synthesis.
\newblock {\em arXiv preprint arXiv:2003.08934}, 2020.

\bibitem{sitzmann2019siren}
Vincent Sitzmann, Julien~N.P. Martel, Alexander~W. Bergman, David~B. Lindell,
  and Gordon Wetzstein.
\newblock Implicit neural representations with periodic activation functions.
\newblock In {\em Proc. NeurIPS}, 2020.

\bibitem{mcmahan2013ad}
H~Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner,
  Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, et~al.
\newblock Ad click prediction: a view from the trenches.
\newblock In {\em Proceedings of the 19th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 1222--1230, 2013.

\bibitem{rendle2010factorization}
Steffen Rendle.
\newblock Factorization machines.
\newblock In {\em 2010 IEEE International Conference on Data Mining}, pages
  995--1000. IEEE, 2010.

\bibitem{cheng2016wide}
Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
  Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et~al.
\newblock Wide \& deep learning for recommender systems.
\newblock In {\em Proceedings of the 1st workshop on deep learning for
  recommender systems}, pages 7--10, 2016.

\bibitem{guo2017deepfm}
Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He.
\newblock Deepfm: a factorization-machine based neural network for ctr
  prediction.
\newblock {\em arXiv preprint arXiv:1703.04247}, 2017.

\bibitem{wang2017deep}
Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang.
\newblock Deep \& cross network for ad click predictions.
\newblock In {\em Proceedings of the ADKDD’17}, ADKDD’17, New York, NY,
  USA, 2017. Association for Computing Machinery.

\bibitem{cheng2019adaptive}
Weiyu Cheng, Yanyan Shen, and Linpeng Huang.
\newblock Adaptive factorization network: Learning adaptive-order feature
  interactions.
\newblock In {\em Thirty-Fourth AAAI Conference on Artificial Intelligence},
  2020.

\bibitem{hines1996logarithmic}
J~Wesley Hines.
\newblock A logarithmic neural network architecture for unbounded non-linear
  function approximation.
\newblock In {\em Proceedings of International Conference on Neural Networks
  (ICNN'96)}, volume~2, pages 1245--1250. IEEE, 1996.

\bibitem{criteo}
Criteo.
\newblock Kaggle contest dataset is now available for academic use!
\newblock
  \url{http://labs.criteo.com/2014/09/kaggle-contest-dataset-now-available-academic-use/},
  2014.
\newblock accessed on the 2020-05-20.

\bibitem{dacrema2019we}
Maurizio~Ferrari Dacrema, Paolo Cremonesi, and Dietmar Jannach.
\newblock Are we really making much progress? a worrying analysis of recent
  neural recommendation approaches.
\newblock In {\em Proceedings of the 13th ACM Conference on Recommender
  Systems}, pages 101--109, 2019.

\bibitem{bronstein2017geometric}
Michael~M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre
  Vandergheynst.
\newblock Geometric deep learning: going beyond euclidean data.
\newblock {\em IEEE Signal Processing Magazine}, 34(4):18--42, 2017.

\bibitem{bruna2014spectral}
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann Lecun.
\newblock Spectral networks and locally connected networks on graphs.
\newblock In {\em International Conference on Learning Representations}, pages
  http--openreview, 2014.

\bibitem{henaff2015deep}
Mikael Henaff, Joan Bruna, and Yann LeCun.
\newblock Deep convolutional networks on graph-structured data.
\newblock {\em arXiv preprint arXiv:1506.05163}, 2015.

\bibitem{defferrard2016convolutional}
Micha{\"e}l Defferrard, Xavier Bresson, and Pierre Vandergheynst.
\newblock Convolutional neural networks on graphs with fast localized spectral
  filtering.
\newblock In {\em Advances in neural information processing systems}, pages
  3844--3852, 2016.

\bibitem{kipf2017semi}
Thomas~N. Kipf and Max Welling.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2017.

\bibitem{gori2005new}
Marco Gori, Gabriele Monfardini, and Franco Scarselli.
\newblock A new model for learning in graph domains.
\newblock In {\em Proceedings. 2005 IEEE International Joint Conference on
  Neural Networks, 2005.}, volume~2, pages 729--734. IEEE, 2005.

\bibitem{scarselli2008graph}
Franco Scarselli, Marco Gori, Ah~Chung Tsoi, Markus Hagenbuchner, and Gabriele
  Monfardini.
\newblock The graph neural network model.
\newblock {\em IEEE Transactions on Neural Networks}, 20(1):61--80, 2008.

\bibitem{li2015gated}
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel.
\newblock Gated graph sequence neural networks.
\newblock In {\em International Conference on Learning Representations}, 2016.

\bibitem{fey2018splinecnn}
Matthias Fey, Jan Eric~Lenssen, Frank Weichert, and Heinrich M{\"u}ller.
\newblock Splinecnn: Fast geometric deep learning with continuous b-spline
  kernels.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 869--877, 2018.

\bibitem{veli2018graph}
Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
  Liò, and Yoshua Bengio.
\newblock Graph attention networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{bahdanau2015neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock In {\em 3rd International Conference on Learning Representations,
  ICLR 2015}, 2015.

\bibitem{xu2018powerful}
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
\newblock How powerful are graph neural networks?
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{feyDNAConv}
Matthias Fey.
\newblock Just jump: Dynamic neighborhood aggregation in graph neural networks.
\newblock In {\em ICLR Workshop on Representation Learning on Graphs and
  Manifolds}, 2019.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem{Fey/Lenssen/2019}
Matthias Fey and Jan~E. Lenssen.
\newblock Fast graph representation learning with {PyTorch Geometric}.
\newblock In {\em ICLR Workshop on Representation Learning on Graphs and
  Manifolds}, 2019.

\bibitem{sen2008collective}
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher,
  and Tina Eliassi-Rad.
\newblock Collective classification in network data.
\newblock {\em AI magazine}, 29(3):93--93, 2008.

\bibitem{xu2018how}
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
\newblock How powerful are graph neural networks?
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{maaten2008visualizing}
Laurens van~der Maaten and Geoffrey Hinton.
\newblock Visualizing data using t-sne.
\newblock {\em Journal of machine learning research}, 9(Nov):2579--2605, 2008.

\bibitem{chan2019gpu}
David~M Chan, Roshan Rao, Forrest Huang, and John~F Canny.
\newblock Gpu accelerated t-distributed stochastic neighbor embedding.
\newblock {\em Journal of Parallel and Distributed Computing}, 131:1--13, 2019.

\bibitem{kipf2016variational}
Thomas~N Kipf and Max Welling.
\newblock Variational graph auto-encoders.
\newblock {\em NIPS Workshop on Bayesian Deep Learning}, 2016.

\bibitem{radford2018improving}
Alec Radford, Karthik Narasimhan, Time Salimans, and Ilya Sutskever.
\newblock Improving language understanding with unsupervised learning.
\newblock {\em Technical report, OpenAI}, 2018.

\bibitem{parmar2018imaget}
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer,
  Alexander Ku, and Dustin Tran.
\newblock Image transformer.
\newblock {\em ArXiv}, abs/1802.05751, 2018.

\bibitem{dhariwal2020jukebox}
Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong~Wook Kim, Alec Radford,
  and Ilya Sutskever.
\newblock Jukebox: A generative model for music.
\newblock {\em arXiv preprint arXiv:2005.00341}, 2020.

\bibitem{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

\bibitem{Shoeybi2019MegatronLMTM}
Mohammad Shoeybi, Mostofa~Ali Patwary, Raul Puri, Patrick LeGresley, Jared
  Casper, and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using
  model parallelism.
\newblock {\em ArXiv}, abs/1909.08053, 2019.

\bibitem{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em arXiv preprint arXiv:2005.14165}, 2020.

\bibitem{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock {SQ}u{AD}: 100,000+ questions for machine comprehension of text.
\newblock In {\em Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}, pages 2383--2392, Austin, Texas, November 2016.
  Association for Computational Linguistics.

\bibitem{rajpurkar2018squad2}
Pranav Rajpurkar, Robin Jia, and Percy Liang.
\newblock Know what you don{'}t know: Unanswerable questions for {SQ}u{AD}.
\newblock In {\em Proceedings of the 56th Annual Meeting of the Association for
  Computational Linguistics (Volume 2: Short Papers)}, pages 784--789,
  Melbourne, Australia, July 2018. Association for Computational Linguistics.

\bibitem{wang2019superglue}
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
  Felix Hill, Omer Levy, and Samuel Bowman.
\newblock Superglue: A stickier benchmark for general-purpose language
  understanding systems.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3261--3275, 2019.

\bibitem{commoncrawl}
The Common~Crawl Team.
\newblock {Common Crawl}.
\newblock \url{https://commoncrawl.org}, 2020.

\bibitem{howard2018universal}
Jeremy Howard and Sebastian Ruder.
\newblock Universal language model fine-tuning for text classification.
\newblock In {\em ACL}. Association for Computational Linguistics, 2018.

\bibitem{merity2017pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock {\em ArXiv}, abs/1609.07843, 2017.

\bibitem{sennrich2016bpe}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock In {\em Proceedings of the 54th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 1715--1725, Berlin,
  Germany, August 2016. Association for Computational Linguistics.

\bibitem{kingma2014adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em International Conference on Learning Representations}, 12 2014.

\bibitem{lindsay2020attention}
Grace~W Lindsay.
\newblock Attention in psychology, neuroscience, and machine learning.
\newblock {\em Frontiers in Computational Neuroscience}, 14:29, 2020.

\bibitem{bolukbasi2016man}
Tolga Bolukbasi, Kai-Wei Chang, James~Y Zou, Venkatesh Saligrama, and Adam~T
  Kalai.
\newblock Man is to computer programmer as woman is to homemaker? debiasing
  word embeddings.
\newblock In {\em Advances in neural information processing systems}, pages
  4349--4357, 2016.

\bibitem{luccioni2019morality}
Alexandra Luccioni and Yoshua Bengio.
\newblock On the morality of artificial intelligence.
\newblock {\em arXiv preprint arXiv:1912.11945}, 2019.

\bibitem{strubell2019energy}
Emma Strubell, Ananya Ganesh, and Andrew McCallum.
\newblock Energy and policy considerations for deep learning in nlp.
\newblock {\em arXiv preprint arXiv:1906.02243}, 2019.

\bibitem{tay2020synthesizer}
Yi~Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng.
\newblock Synthesizer: Rethinking self-attention in transformer models.
\newblock {\em arXiv preprint arXiv:2005.00743}, 2020.

\bibitem{liu2019variance}
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng
  Gao, and Jiawei Han.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock {\em arXiv preprint arXiv:1908.03265}, 2019.

\bibitem{raganato2020fixed}
Alessandro Raganato, Yves Scherrer, and J{\"o}rg Tiedemann.
\newblock Fixed encoder self-attention patterns in transformer-based machine
  translation.
\newblock {\em arXiv preprint arXiv:2002.10260}, 2020.

\bibitem{NEURIPS2019_9015}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems 32}, pages 8024--8035. Curran Associates,
  Inc., 2019.

\end{thebibliography}
