\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and
  Song]{allenzhu2019convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\  242--252.
  PMLR, 09--15 Jun 2019.
\newblock URL \url{https://proceedings.mlr.press/v97/allen-zhu19a.html}.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, and
  Wang]{arora2019finegrained}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\  322--332.
  PMLR, 09--15 Jun 2019{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v97/arora19a.html}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{aroraexact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc.,
  2019{\natexlab{b}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/dbc4d84bfcfe2284ba11beffb853a8c4-Paper.pdf}.

\bibitem[Atanasov et~al.(2022)Atanasov, Bordelon, and
  Pehlevan]{atanasov2022neural}
Alexander Atanasov, Blake Bordelon, and Cengiz Pehlevan.
\newblock Neural networks as kernel learners: The silent alignment effect.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=1NvflqAdoom}.

\bibitem[Baratin et~al.(2021)Baratin, George, Laurent, Devon~Hjelm, Lajoie,
  Vincent, and Lacoste-Julien]{kernelalignment}
Aristide Baratin, Thomas George, C{\'e}sar Laurent, R~Devon~Hjelm, Guillaume
  Lajoie, Pascal Vincent, and Simon Lacoste-Julien.
\newblock Implicit regularization via neural feature alignment.
\newblock In Arindam Banerjee and Kenji Fukumizu (eds.), \emph{Proceedings of
  The 24th International Conference on Artificial Intelligence and Statistics},
  volume 130 of \emph{Proceedings of Machine Learning Research}, pp.\
  2269--2277. PMLR, 13--15 Apr 2021.
\newblock URL \url{https://proceedings.mlr.press/v130/baratin21a.html}.

\bibitem[Bartlett \& Mendelson(2003)Bartlett and
  Mendelson]{bartlettmendelson2003}
Peter~L. Bartlett and Shahar Mendelson.
\newblock Rademacher and {G}aussian complexities: Risk bounds and structural
  results.
\newblock \emph{J. Mach. Learn. Res.}, 3\penalty0 (null):\penalty0 463–482,
  mar 2003.
\newblock ISSN 1532-4435.

\bibitem[Basri et~al.(2019)Basri, Jacobs, Kasten, and
  Kritchman]{basri2019convergence}
Ronen Basri, David Jacobs, Yoni Kasten, and Shira Kritchman.
\newblock The convergence rate of neural networks for learned functions of
  different frequencies.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/5ac8bb8a7d745102a978c5f8ccdb61b8-Paper.pdf}.

\bibitem[Basri et~al.(2020)Basri, Galun, Geifman, Jacobs, Kasten, and
  Kritchman]{basri2020frequency}
Ronen Basri, Meirav Galun, Amnon Geifman, David Jacobs, Yoni Kasten, and Shira
  Kritchman.
\newblock Frequency bias in neural networks for input of non-uniform density.
\newblock In Hal~Daumé III and Aarti Singh (eds.), \emph{Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  685--694. PMLR, 13--18
  Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/basri20a.html}.

\bibitem[Bowman \& Mont\'ufar(2022)Bowman and Mont\'ufar]{bowman2022implicit}
Benjamin Bowman and Guido Mont\'ufar.
\newblock Implicit bias of {MSE} gradient optimization in underparameterized
  neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=VLgmhQDVBV}.

\bibitem[Buchanan et~al.(2021)Buchanan, Gilboa, and Wright]{buchanan2021deep}
Sam Buchanan, Dar Gilboa, and John Wright.
\newblock Deep networks and the multiple manifold problem.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=O-6Pm_d_Q-}.

\bibitem[Cao et~al.(2021)Cao, Fang, Wu, Zhou, and Gu]{caounderstanding}
Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu.
\newblock Towards understanding the spectral bias of deep learning.
\newblock In Zhi-Hua Zhou (ed.), \emph{Proceedings of the Thirtieth
  International Joint Conference on Artificial Intelligence, {IJCAI-21}}, pp.\
  2205--2211. International Joint Conferences on Artificial Intelligence
  Organization, 8 2021.
\newblock \doi{10.24963/ijcai.2021/304}.
\newblock URL \url{https://doi.org/10.24963/ijcai.2021/304}.
\newblock Main Track.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang, and
  Zhai]{du2019gradient}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\
  1675--1685. PMLR, 09--15 Jun 2019{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v97/du19c.html}.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Zhai, Poczos, and
  Singh]{du2018gradient}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=S1eK3i09YQ}.

\bibitem[Dumer(2006)]{dumer_ellipsoid}
Ilya Dumer.
\newblock Covering an ellipsoid with equal balls.
\newblock \emph{J. Comb. Theory Ser. A}, 113\penalty0 (8):\penalty0
  1667–1676, nov 2006.
\newblock ISSN 0097-3165.
\newblock \doi{10.1016/j.jcta.2006.03.021}.
\newblock URL \url{https://doi.org/10.1016/j.jcta.2006.03.021}.

\bibitem[E et~al.(2020)E, Ma, and Wu]{Weinan2020ACA}
Weinan E, Chao Ma, and Lei Wu.
\newblock A comparative analysis of optimization and generalization properties
  of two-layer neural network and random feature models under gradient descent
  dynamics.
\newblock \emph{Science China Mathematics}, 63:\penalty0 1235--1258, 2020.

\bibitem[Fan \& Wang(2020)Fan and Wang]{10.5555/3495724.3496370}
Zhou Fan and Zhichao Wang.
\newblock Spectra of the conjugate kernel and neural tangent kernel for
  linear-width neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Neural
  Information Processing Systems}, NIPS'20, Red Hook, NY, USA, 2020. Curran
  Associates Inc.
\newblock ISBN 9781713829546.

\bibitem[Gordon et~al.(1987)Gordon, K{\"o}nig, and
  Sch{\"u}tt]{Gordon1987GeometricAP}
Y.~Gordon, Hermann K{\"o}nig, and Carsten Sch{\"u}tt.
\newblock Geometric and probabilistic estimates for entropy and approximation
  numbers of operators.
\newblock \emph{Journal of Approximation Theory}, 49:\penalty0 219--239, 1987.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{NIPS2017_58191d2a}
Suriya Gunasekar, Blake~E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur,
  and Nati Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett (eds.), \emph{Advances in Neural Information
  Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/file/58191d2a914c6dae66371c9dcdc91b41-Paper.pdf}.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{NEURIPS2018_0e98aeeb}
Suriya Gunasekar, Jason~D Lee, Daniel Soudry, and Nati Srebro.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett (eds.), \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/0e98aeeb54acf612b9eb4e48a269814c-Paper.pdf}.

\bibitem[Hanin \& Nica(2020)Hanin and Nica]{Hanin2020Finite}
Boris Hanin and Mihai Nica.
\newblock Finite depth and width corrections to the neural tangent kernel.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=SJgndT4KwB}.

\bibitem[Harris et~al.(2020)Harris, Millman, van~der Walt, Gommers, Virtanen,
  Cournapeau, Wieser, Taylor, Berg, Smith, Kern, Picus, Hoyer, van Kerkwijk,
  Brett, Haldane, del R{\'{i}}o, Wiebe, Peterson, G{\'{e}}rard-Marchant,
  Sheppard, Reddy, Weckesser, Abbasi, Gohlke, and Oliphant]{harris2020array}
Charles~R. Harris, K.~Jarrod Millman, St{\'{e}}fan~J. van~der Walt, Ralf
  Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor,
  Sebastian Berg, Nathaniel~J. Smith, Robert Kern, Matti Picus, Stephan Hoyer,
  Marten~H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime~Fern{\'{a}}ndez
  del R{\'{i}}o, Mark Wiebe, Pearu Peterson, Pierre G{\'{e}}rard-Marchant,
  Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph
  Gohlke, and Travis~E. Oliphant.
\newblock Array programming with {NumPy}.
\newblock \emph{Nature}, 585\penalty0 (7825):\penalty0 357--362, September
  2020.
\newblock \doi{10.1038/s41586-020-2649-2}.
\newblock URL \url{https://doi.org/10.1038/s41586-020-2649-2}.

\bibitem[Huang \& Yau(2020)Huang and Yau]{huang2019dynamics}
Jiaoyang Huang and Horng-Tzer Yau.
\newblock Dynamics of deep neural networks and neural tangent hierarchy.
\newblock In Hal~Daumé III and Aarti Singh (eds.), \emph{Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  4542--4551. PMLR,
  13--18 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/huang20l.html}.

\bibitem[Hunter(2007)]{Hunter:2007}
J.~D. Hunter.
\newblock Matplotlib: A 2d graphics environment.
\newblock \emph{Computing in Science \& Engineering}, 9\penalty0 (3):\penalty0
  90--95, 2007.
\newblock \doi{10.1109/MCSE.2007.55}.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2020neural}
Arthur Jacot, Franck Gabriel, and Clement Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett (eds.), \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf}.

\bibitem[Ji \& Telgarsky(2019)Ji and Telgarsky]{telgarskynonseparable}
Ziwei Ji and Matus Telgarsky.
\newblock The implicit bias of gradient descent on nonseparable data.
\newblock In Alina Beygelzimer and Daniel Hsu (eds.), \emph{Proceedings of the
  Thirty-Second Conference on Learning Theory}, volume~99 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1772--1798. PMLR, 25--28 Jun 2019.
\newblock URL \url{https://proceedings.mlr.press/v99/ji19a.html}.

\bibitem[Ji \& Telgarsky(2020)Ji and Telgarsky]{Ji2020Polylogarithmic}
Ziwei Ji and Matus Telgarsky.
\newblock Polylogarithmic width suffices for gradient descent to achieve
  arbitrarily small test error with shallow {ReLU} networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=HygegyrYwH}.

\bibitem[Jin \& Montúfar(2021)Jin and Montúfar]{jin2021implicit}
Hui Jin and Guido Montúfar.
\newblock Implicit bias of gradient descent for mean squared error regression
  with wide neural networks, 2021.
\newblock arXiv:2006.07356.

\bibitem[Karakida et~al.(2021)Karakida, Akaho, and
  Amari]{karakida_pathological_2021}
Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari.
\newblock Pathological {Spectra} of the {Fisher} {Information} {Metric} and
  {Its} {Variants} in {Deep} {Neural} {Networks}.
\newblock \emph{Neural Computation}, 33\penalty0 (8):\penalty0 2274--2307, July
  2021.
\newblock ISSN 0899-7667.
\newblock \doi{10.1162/neco_a_01411}.
\newblock URL \url{https://doi.org/10.1162/neco\_a\_01411}.
\newblock \_eprint:
  https://direct.mit.edu/neco/article-pdf/33/8/2274/1930880/neco\_a\_01411.pdf.

\bibitem[Kluyver et~al.(2016)Kluyver, Ragan-Kelley, P{\'e}rez, Granger,
  Bussonnier, Frederic, Kelley, Hamrick, Grout, Corlay, Ivanov, Avila, Abdalla,
  and Willing]{Kluyver2016jupyter}
Thomas Kluyver, Benjamin Ragan-Kelley, Fernando P{\'e}rez, Brian Granger,
  Matthias Bussonnier, Jonathan Frederic, Kyle Kelley, Jessica Hamrick, Jason
  Grout, Sylvain Corlay, Paul Ivanov, Dami{\'a}n Avila, Safia Abdalla, and
  Carol Willing.
\newblock Jupyter notebooks -- a publishing format for reproducible
  computational workflows.
\newblock In F.~Loizides and B.~Schmidt (eds.), \emph{Positioning and Power in
  Academic Publishing: Players, Agents and Agendas}, pp.\  87 -- 90. IOS Press,
  2016.

\bibitem[Krizhevsky(2009)]{Krizhevsky09learningmultiple}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lenetpaper}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.
\newblock \doi{10.1109/5.726791}.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{Lee2019WideNN}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/0d1a9651497a38d8b1c3871c84528bd4-Paper.pdf}.

\bibitem[Li et~al.(2020)Li, Soltanolkotabi, and Oymak]{li2019gradient}
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak.
\newblock Gradient descent with early stopping is provably robust to label
  noise for overparameterized neural networks.
\newblock In Silvia Chiappa and Roberto Calandra (eds.), \emph{Proceedings of
  the Twenty Third International Conference on Artificial Intelligence and
  Statistics}, volume 108 of \emph{Proceedings of Machine Learning Research},
  pp.\  4313--4324. PMLR, 26--28 Aug 2020.
\newblock URL \url{https://proceedings.mlr.press/v108/li20j.html}.

\bibitem[Li et~al.(2018)Li, Ma, and Zhang]{pmlr-v75-li18a}
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock In Sébastien Bubeck, Vianney Perchet, and Philippe Rigollet (eds.),
  \emph{Proceedings of the 31st Conference On Learning Theory}, volume~75 of
  \emph{Proceedings of Machine Learning Research}, pp.\  2--47. PMLR, 06--09
  Jul 2018.
\newblock URL \url{https://proceedings.mlr.press/v75/li18a.html}.

\bibitem[Liu et~al.(2020{\natexlab{a}})Liu, Zhu, and Belkin]{liubelkinarxiv}
Chaoyue Liu, Libin Zhu, and Mikhail Belkin.
\newblock On the linearity of large non-linear models: when and why the tangent
  kernel is constant.
\newblock \emph{CoRR}, abs/2010.01092, 2020{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2010.01092}.

\bibitem[Liu et~al.(2020{\natexlab{b}})Liu, Zhu, and Belkin]{liuonlinearity}
Chaoyue Liu, Libin Zhu, and Misha Belkin.
\newblock On the linearity of large non-linear models: when and why the tangent
  kernel is constant.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  15954--15964. Curran Associates, Inc., 2020{\natexlab{b}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/b7ae8fecf15b8b6c3c69eceae636d203-Paper.pdf}.

\bibitem[Luo et~al.(2020)Luo, Ma, Xu, and
  Zhang]{https://doi.org/10.48550/arxiv.2010.08153}
Tao Luo, Zheng Ma, Zhi-Qin~John Xu, and Yaoyu Zhang.
\newblock On the exact computation of linear frequency principle dynamics and
  its generalization, 2020.
\newblock URL \url{https://arxiv.org/abs/2010.08153}.

\bibitem[Nacson et~al.(2019)Nacson, Lee, Gunasekar, Srebro, and
  Soudry]{Nacson2019ConvergenceOG}
Mor~Shpigel Nacson, J.~Lee, Suriya Gunasekar, Nathan Srebro, and Daniel Soudry.
\newblock Convergence of gradient descent on separable data.
\newblock In \emph{AISTATS}, 2019.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and
  Srebro]{neyshabur2015search}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock In Yoshua Bengio and Yann LeCun (eds.), \emph{3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Workshop Track Proceedings}, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.6614}.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Tomioka, Salakhutdinov, and
  Srebro]{neyshabur2017geometry}
Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro.
\newblock Geometry of optimization and implicit regularization in deep
  learning, 2017.
\newblock arXiv:1705.03071.

\bibitem[Nguyen(2021)]{nguyenrelu}
Quynh Nguyen.
\newblock On the proof of global convergence of gradient descent for deep relu
  networks with linear widths.
\newblock In Marina Meila and Tong Zhang (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  8056--8062. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/nguyen21a.html}.

\bibitem[Nguyen \& Mondelli(2020)Nguyen and Mondelli]{qyunhpyramidal}
Quynh Nguyen and Marco Mondelli.
\newblock Global convergence of deep networks with one wide layer followed by
  pyramidal topology.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  11961--11972. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/8abfe8ac9ec214d68541fcb888c0b4c3-Paper.pdf}.

\bibitem[Oymak \& Soltanolkotabi(2020)Oymak and
  Soltanolkotabi]{oymak2019moderate}
Samet Oymak and Mahdi Soltanolkotabi.
\newblock Toward moderate overparameterization: Global convergence guarantees
  for training shallow neural networks.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  1\penalty0 (1):\penalty0 84--105, 2020.

\bibitem[Oymak et~al.(2020)Oymak, Fabian, Li, and
  Soltanolkotabi]{oymak2020generalization}
Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi.
\newblock Generalization guarantees for neural networks via harnessing the
  low-rank structure of the {J}acobian, 2020.
\newblock URL \url{https://openreview.net/forum?id=ryl5CJSFPS}.

\bibitem[Papyan(2020)]{papyantraces}
Vardan Papyan.
\newblock Traces of class/cross-class structure pervade deep learning spectra.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (252):\penalty0 1--64, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-933.html}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, K\"{o}pf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas K\"{o}pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock \emph{PyTorch: An Imperative Style, High-Performance Deep Learning
  Library}.
\newblock Curran Associates Inc., Red Hook, NY, USA, 2019.

\bibitem[Pennington \& Bahri(2017)Pennington and Bahri]{pmlr-v70-pennington17a}
Jeffrey Pennington and Yasaman Bahri.
\newblock Geometry of neural network loss surfaces via random matrix theory.
\newblock In Doina Precup and Yee~Whye Teh (eds.), \emph{Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of
  \emph{Proceedings of Machine Learning Research}, pp.\  2798--2806. PMLR,
  06--11 Aug 2017.
\newblock URL \url{https://proceedings.mlr.press/v70/pennington17a.html}.

\bibitem[Pennington \& Worah(2018)Pennington and Worah]{NEURIPS2018_18bb68e2}
Jeffrey Pennington and Pratik Worah.
\newblock The spectrum of the fisher information matrix of a
  single-hidden-layer neural network.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett (eds.), \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/18bb68e2b38e4a8ce7cf4f6b2625768c-Paper.pdf}.

\bibitem[P\'erez \& Granger(2007)P\'erez and Granger]{ipython}
Fernando P\'erez and Brian~E. Granger.
\newblock {IP}ython: a system for interactive scientific computing.
\newblock \emph{Computing in Science and Engineering}, 9\penalty0 (3):\penalty0
  21--29, May 2007.
\newblock ISSN 1521-9615.
\newblock \doi{10.1109/MCSE.2007.53}.
\newblock URL \url{https://ipython.org}.

\bibitem[Pisier(1989)]{pisier_1989}
Gilles Pisier.
\newblock \emph{The Volume of Convex Bodies and Banach Space Geometry}.
\newblock Cambridge Tracts in Mathematics. Cambridge University Press, 1989.
\newblock \doi{10.1017/CBO9780511662454}.

\bibitem[Rahaman et~al.(2019)Rahaman, Baratin, Arpit, Draxler, Lin, Hamprecht,
  Bengio, and Courville]{rahaman2019spectral}
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred
  Hamprecht, Yoshua Bengio, and Aaron Courville.
\newblock On the spectral bias of neural networks.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\
  5301--5310. PMLR, 09--15 Jun 2019.
\newblock URL \url{https://proceedings.mlr.press/v97/rahaman19a.html}.

\bibitem[Rosasco et~al.(2010)Rosasco, Belkin, and Vito]{rosasco10a}
Lorenzo Rosasco, Mikhail Belkin, and Ernesto~De Vito.
\newblock On learning with integral operators.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (30):\penalty0 905--934, 2010.
\newblock URL \url{http://jmlr.org/papers/v11/rosasco10a.html}.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{Journal of Machine Learning Research}, 19\penalty0
  (70):\penalty0 1--57, 2018.
\newblock URL \url{http://jmlr.org/papers/v19/18-188.html}.

\bibitem[Su \& Yang(2019)Su and Yang]{su2019learning}
Lili Su and Pengkun Yang.
\newblock On learning over-parameterized neural networks: A functional
  approximation perspective.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/253f7b5d921338af34da817c00f42753-Paper.pdf}.

\bibitem[Telgarsky(2021)]{mjt_dlt}
Matus Telgarsky.
\newblock Deep learning theory lecture notes.
\newblock \url{https://mjt.cs.illinois.edu/dlt/}, 2021.
\newblock Version: 2021-10-27 v0.0-e7150f2d (alpha).

\bibitem[Velikanov \& Yarotsky(2021)Velikanov and
  Yarotsky]{yarotskyasymptotics}
Maksim Velikanov and Dmitry Yarotsky.
\newblock Explicit loss asymptotics in the gradient descent training of neural
  networks.
\newblock In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann~N. Dauphin, Percy
  Liang, and Jennifer~Wortman Vaughan (eds.), \emph{Advances in Neural
  Information Processing Systems 34: Annual Conference on Neural Information
  Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual}, pp.\
  2570--2582, 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/hash/14faf969228fc18fcd4fcf59437b0c97-Abstract.html}.

\bibitem[Vershynin(2012)]{vershynin2011introduction}
Roman Vershynin.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock In \emph{Compressed Sensing}, chapter~5. Cambridge University Press,
  2012.

\bibitem[Virtanen et~al.(2020)Virtanen, Gommers, Oliphant, Haberland, Reddy,
  Cournapeau, Burovski, Peterson, Weckesser, Bright, {van der Walt}, Brett,
  Wilson, Millman, Mayorov, Nelson, Jones, Kern, Larson, Carey, Polat, Feng,
  Moore, {VanderPlas}, Laxalde, Perktold, Cimrman, Henriksen, Quintero, Harris,
  Archibald, Ribeiro, Pedregosa, {van Mulbregt}, and {SciPy 1.0
  Contributors}]{2020SciPy-NMeth}
Pauli Virtanen, Ralf Gommers, Travis~E. Oliphant, Matt Haberland, Tyler Reddy,
  David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan
  Bright, St{\'e}fan~J. {van der Walt}, Matthew Brett, Joshua Wilson, K.~Jarrod
  Millman, Nikolay Mayorov, Andrew R.~J. Nelson, Eric Jones, Robert Kern, Eric
  Larson, C~J Carey, {\.I}lhan Polat, Yu~Feng, Eric~W. Moore, Jake
  {VanderPlas}, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen,
  E.~A. Quintero, Charles~R. Harris, Anne~M. Archibald, Ant{\^o}nio~H. Ribeiro,
  Fabian Pedregosa, Paul {van Mulbregt}, and {SciPy 1.0 Contributors}.
\newblock {{SciPy} 1.0: Fundamental Algorithms for Scientific Computing in
  Python}.
\newblock \emph{Nature Methods}, 17:\penalty0 261--272, 2020.
\newblock \doi{10.1038/s41592-019-0686-2}.

\bibitem[Wainwright(2019)]{wainwright_2019}
Martin~J. Wainwright.
\newblock \emph{High-Dimensional Statistics: A Non-Asymptotic Viewpoint}.
\newblock Cambridge Series in Statistical and Probabilistic Mathematics.
  Cambridge University Press, 2019.
\newblock \doi{10.1017/9781108627771}.

\bibitem[Williams et~al.(2019)Williams, Trager, Panozzo, Silva, Zorin, and
  Bruna]{williams2019gradient}
Francis Williams, Matthew Trager, Daniele Panozzo, Claudio Silva, Denis Zorin,
  and Joan Bruna.
\newblock Gradient dynamics of shallow univariate relu networks.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/1f6419b1cbe79c71410cb320fc094775-Paper.pdf}.

\bibitem[Xu et~al.(2019)Xu, Zhang, and Xiao]{xu2019training}
Zhi-Qin~John Xu, Yaoyu Zhang, and Yanyang Xiao.
\newblock Training behavior of deep neural network in frequency domain.
\newblock In Tom Gedeon, Kok~Wai Wong, and Minho Lee (eds.), \emph{Neural
  Information Processing}, pp.\  264--274, Cham, 2019. Springer International
  Publishing.

\bibitem[Yang et~al.(2022)Yang, Ajay, and Agrawal]{yang2022overcoming}
Ge~Yang, Anurag Ajay, and Pulkit Agrawal.
\newblock Overcoming the spectral bias of neural value approximation.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=vIC-xLFuM6}.

\bibitem[Yang(2020)]{gregyangtp2}
Greg Yang.
\newblock Tensor programs {II}: Neural tangent kernel for any architecture,
  2020.
\newblock URL \url{https://arxiv.org/abs/2006.14548}.

\bibitem[Yang \& Salman(2019)Yang and
  Salman]{https://doi.org/10.48550/arxiv.1907.10599}
Greg Yang and Hadi Salman.
\newblock A fine-grained spectral perspective on neural networks, 2019.
\newblock URL \url{https://arxiv.org/abs/1907.10599}.

\bibitem[Yang et~al.(2021)Yang, Mao, and Chaudhari]{Yang2021DoesTD}
Rubing Yang, Jialin Mao, and Pratik Chaudhari.
\newblock Does the data induce capacity control in deep learning?
\newblock \emph{ArXiv}, abs/2110.14163, 2021.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=Sy8gdB9xx}.

\bibitem[Zhang et~al.(2020)Zhang, Xu, Luo, and Ma]{generalizationinducedbyinit}
Yaoyu Zhang, Zhi-Qin~John Xu, Tao Luo, and Zheng Ma.
\newblock A type of generalization error induced by initialization in deep
  neural networks.
\newblock In Jianfeng Lu and Rachel Ward (eds.), \emph{Proceedings of The First
  Mathematical and Scientific Machine Learning Conference}, volume 107 of
  \emph{Proceedings of Machine Learning Research}, pp.\  144--164. PMLR, 20--24
  Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v107/zhang20a.html}.

\bibitem[Zou \& Gu(2019)Zou and Gu]{zou2019improved}
Difan Zou and Quanquan Gu.
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf}.

\bibitem[Zou et~al.(2020)Zou, Cao, Zhou, and Gu]{zou2018stochastic}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Gradient descent optimizes over-parameterized deep {ReLU} networks.
\newblock \emph{Machine learning}, 109:\penalty0 467–492, 2020.

\end{thebibliography}
