\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achlioptas et~al.(2020)Achlioptas, Abdelreheem, Xia, Elhoseiny, and Guibas]{achlioptas2020referit3d}
Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas.
\newblock Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes.
\newblock In \emph{Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part I 16}, pp.\  422--440. Springer, 2020.

\bibitem[Azuma et~al.(2022)Azuma, Miyanishi, Kurita, and Kawanabe]{azuma2022scanqa}
Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe.
\newblock Scanqa: 3d question answering for spatial scene understanding.
\newblock In \emph{proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  19129--19139, 2022.

\bibitem[Chang et~al.(2015)Chang, Funkhouser, Guibas, Hanrahan, Huang, Li, Savarese, Savva, Song, Su, et~al.]{chang2015shapenet}
Angel~X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et~al.
\newblock Shapenet: An information-rich 3d model repository.
\newblock \emph{arXiv preprint arXiv:1512.03012}, 2015.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Chang, and Nie{\ss}ner]{chen2020scanrefer}
Dave~Zhenyu Chen, Angel~X Chang, and Matthias Nie{\ss}ner.
\newblock Scanrefer: 3d object localization in rgb-d scans using natural language.
\newblock In \emph{European conference on computer vision}, pp.\  202--221. Springer, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Xie, Vedaldi, and Zisserman]{chen2020vggsound}
Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman.
\newblock Vggsound: A large-scale audio-visual dataset.
\newblock In \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pp.\  721--725. IEEE, 2020{\natexlab{b}}.

\bibitem[Chen et~al.(2021)Chen, Xie, Afouras, Nagrani, Vedaldi, and Zisserman]{chen2021localizing}
Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman.
\newblock Localizing visual sounds the hard way.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  16867--16876, 2021.

\bibitem[Dai et~al.(2017)Dai, Chang, Savva, Halber, Funkhouser, and Nie{\ss}ner]{dai2017scannet}
Angela Dai, Angel~X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie{\ss}ner.
\newblock Scannet: Richly-annotated 3d reconstructions of indoor scenes.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  5828--5839, 2017.

\bibitem[Deitke et~al.(2023)Deitke, Schwenk, Salvador, Weihs, Michel, VanderBilt, Schmidt, Ehsani, Kembhavi, and Farhadi]{deitke2023objaverse}
Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi.
\newblock Objaverse: A universe of annotated 3d objects.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  13142--13153, 2023.

\bibitem[Elizalde et~al.(2023)Elizalde, Deshmukh, Al~Ismail, and Wang]{elizalde2023clap}
Benjamin Elizalde, Soham Deshmukh, Mahmoud Al~Ismail, and Huaming Wang.
\newblock Clap learning audio concepts from natural language supervision.
\newblock In \emph{ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pp.\  1--5. IEEE, 2023.

\bibitem[Gafni et~al.(2022)Gafni, Polyak, Ashual, Sheynin, Parikh, and Taigman]{gafni2022make}
Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman.
\newblock Make-a-scene: Scene-based text-to-image generation with human priors.
\newblock In \emph{European Conference on Computer Vision}, pp.\  89--106. Springer, 2022.

\bibitem[Gan et~al.(2022)Gan, Li, Li, Wang, Liu, Gao, et~al.]{gan2022vision}
Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et~al.
\newblock Vision-language pre-training: Basics, recent advances, and future trends.
\newblock \emph{Foundations and Trends{\textregistered} in Computer Graphics and Vision}, 14\penalty0 (3--4):\penalty0 163--352, 2022.

\bibitem[Gemmeke et~al.(2017)Gemmeke, Ellis, Freedman, Jansen, Lawrence, Moore, Plakal, and Ritter]{gemmeke2017audio}
Jort~F Gemmeke, Daniel~PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R~Channing Moore, Manoj Plakal, and Marvin Ritter.
\newblock Audio set: An ontology and human-labeled dataset for audio events.
\newblock In \emph{2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)}, pp.\  776--780. IEEE, 2017.

\bibitem[Girdhar et~al.(2023)Girdhar, El-Nouby, Liu, Singh, Alwala, Joulin, and Misra]{girdhar2023imagebind}
Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan~Vasudev Alwala, Armand Joulin, and Ishan Misra.
\newblock Imagebind: One embedding space to bind them all.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  15180--15190, 2023.

\bibitem[Gong et~al.(2022)Gong, Rouditchenko, Liu, Harwath, Karlinsky, Kuehne, and Glass]{gong2022contrastive}
Yuan Gong, Andrew Rouditchenko, Alexander~H Liu, David Harwath, Leonid Karlinsky, Hilde Kuehne, and James~R Glass.
\newblock Contrastive audio-visual masked autoencoder.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Guo et~al.(2023)Guo, Zhang, Zhu, Tang, Ma, Han, Chen, Gao, Li, Li, and Heng]{guo2023pointbind}
Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li, Hongsheng Li, and Pheng-Ann Heng.
\newblock Point-bind \& point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following, 2023.

\bibitem[Guzhov et~al.(2022)Guzhov, Raue, Hees, and Dengel]{guzhov2022audioclip}
Andrey Guzhov, Federico Raue, J{\"o}rn Hees, and Andreas Dengel.
\newblock Audioclip: Extending clip to image, text and audio.
\newblock In \emph{ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pp.\  976--980. IEEE, 2022.

\bibitem[Han et~al.(2023)Han, Zhang, Shao, Gao, Xu, Xiao, Zhang, Liu, Wen, Guo, et~al.]{han2023imagebind}
Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et~al.
\newblock Imagebind-llm: Multi-modality instruction tuning.
\newblock \emph{arXiv preprint arXiv:2309.03905}, 2023.

\bibitem[Hegde et~al.(2023)Hegde, Valanarasu, and Patel]{hegde2023clip}
Deepti Hegde, Jeya Maria~Jose Valanarasu, and Vishal~M Patel.
\newblock Clip goes 3d: Leveraging prompt tuning for language grounded 3d recognition.
\newblock \emph{arXiv preprint arXiv:2303.11313}, 2023.

\bibitem[Huang et~al.(2023{\natexlab{a}})Huang, Huang, Yang, Ren, Liu, Li, Ye, Liu, Yin, and Zhao]{huang2023make}
Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi~Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao.
\newblock Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models.
\newblock \emph{arXiv preprint arXiv:2301.12661}, 2023{\natexlab{a}}.

\bibitem[Huang et~al.(2023{\natexlab{b}})Huang, Dong, Wang, Hao, Singhal, Ma, Lv, Cui, Mohammed, Liu, et~al.]{huang2023language}
Shaohan Huang, Li~Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais~Khan Mohammed, Qiang Liu, et~al.
\newblock Language is not all you need: Aligning perception with language models.
\newblock \emph{arXiv preprint arXiv:2302.14045}, 2023{\natexlab{b}}.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and Duerig]{jia2021scaling}
Chao Jia, Yinfei Yang, Ye~Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with noisy text supervision.
\newblock In \emph{International conference on machine learning}, pp.\  4904--4916. PMLR, 2021.

\bibitem[Kim et~al.(2019)Kim, Kim, Lee, and Kim]{kim2019audiocaps}
Chris~Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim.
\newblock Audiocaps: Generating captions for audios in the wild.
\newblock In \emph{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pp.\  119--132, 2019.

\bibitem[Li et~al.(2021)Li, Selvaraju, Gotmare, Joty, Xiong, and Hoi]{li2021align}
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu~Hong Hoi.
\newblock Align before fuse: Vision and language representation learning with momentum distillation.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 9694--9705, 2021.

\bibitem[Li et~al.(2022)Li, Li, Xiong, and Hoi]{li2022blip}
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
\newblock Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  12888--12900. PMLR, 2022.

\bibitem[Liang et~al.(2022)Liang, Zhang, Kwon, Yeung, and Zou]{liang2022mind}
Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Zou.
\newblock Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning, 2022.

\bibitem[Lin et~al.(2023{\natexlab{a}})Lin, Gao, Tang, Takikawa, Zeng, Huang, Kreis, Fidler, Liu, and Lin]{lin2023magic3d}
Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin.
\newblock Magic3d: High-resolution text-to-3d content creation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  300--309, 2023{\natexlab{a}}.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll{\'a}r, and Zitnick]{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13}, pp.\  740--755. Springer, 2014.

\bibitem[Lin et~al.(2023{\natexlab{b}})Lin, Sung, Lei, Bansal, and Bertasius]{lin2023vision}
Yan-Bo Lin, Yi-Lin Sung, Jie Lei, Mohit Bansal, and Gedas Bertasius.
\newblock Vision transformers are parameter-efficient audio-visual learners.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  2299--2309, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Tian, Yuan, Liu, Mei, Kong, Wang, Wang, Wang, and Plumbley]{liu2023audioldm}
Haohe Liu, Qiao Tian, Yi~Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark~D Plumbley.
\newblock Audioldm 2: Learning holistic audio generation with self-supervised pretraining.
\newblock \emph{arXiv preprint arXiv:2308.05734}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Shi, Kuang, Zhu, Li, Han, Cai, Porikli, and Su]{liu2023openshape}
Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Xuanlin Li, Shizhong Han, Hong Cai, Fatih Porikli, and Hao Su.
\newblock Openshape: Scaling up 3d shape representation towards open-world understanding.
\newblock \emph{arXiv preprint arXiv:2305.10764}, 2023{\natexlab{b}}.

\bibitem[Mo \& Morgado(2022)Mo and Morgado]{mo2022closer}
Shentong Mo and Pedro Morgado.
\newblock A closer look at weakly-supervised audio-visual source localization.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 37524--37536, 2022.

\bibitem[Peng et~al.(2023)Peng, Wang, Dong, Hao, Huang, Ma, and Wei]{peng2023kosmos}
Zhiliang Peng, Wenhui Wang, Li~Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei.
\newblock Kosmos-2: Grounding multimodal large language models to the world.
\newblock \emph{arXiv preprint arXiv:2306.14824}, 2023.

\bibitem[Poole et~al.(2022)Poole, Jain, Barron, and Mildenhall]{poole2022dreamfusion}
Ben Poole, Ajay Jain, Jonathan~T Barron, and Ben Mildenhall.
\newblock Dreamfusion: Text-to-3d using 2d diffusion.
\newblock \emph{arXiv preprint arXiv:2209.14988}, 2022.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International conference on machine learning}, pp.\  8748--8763. PMLR, 2021.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and Chen]{ramesh2022hierarchical}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock \emph{arXiv preprint arXiv:2204.06125}, 1\penalty0 (2):\penalty0 3, 2022.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  10684--10695, 2022.

\bibitem[Ruan et~al.(2023)Ruan, Ma, Yang, He, Liu, Fu, Yuan, Jin, and Guo]{ruan2023mm}
Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas~Jing Yuan, Qin Jin, and Baining Guo.
\newblock Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  10219--10228, 2023.

\bibitem[Senocak et~al.(2018)Senocak, Oh, Kim, Yang, and Kweon]{senocak2018learning}
Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, and In~So Kweon.
\newblock Learning to localize sound source in visual scenes.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  4358--4366, 2018.

\bibitem[Su et~al.(2023)Su, Lan, Li, Xu, Wang, and Cai]{su2023pandagpt}
Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai.
\newblock Pandagpt: One model to instruction-follow them all.
\newblock \emph{arXiv preprint arXiv:2305.16355}, 2023.

\bibitem[Tang et~al.(2023)Tang, Yang, Zhu, Zeng, and Bansal]{tang2023anytoany}
Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal.
\newblock Any-to-any generation via composable diffusion, 2023.

\bibitem[Tian et~al.(2018)Tian, Shi, Li, Duan, and Xu]{tian2018audio}
Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu.
\newblock Audio-visual event localization in unconstrained videos.
\newblock In \emph{Proceedings of the European conference on computer vision (ECCV)}, pp.\  247--263, 2018.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Huang, Zhao, Zhang, and Zhao]{wang2023chat}
Zehan Wang, Haifeng Huang, Yang Zhao, Ziang Zhang, and Zhou Zhao.
\newblock Chat-3d: Data-efficiently tuning large language model for universal dialogue of 3d scenes.
\newblock \emph{arXiv preprint arXiv:2308.08769}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Zhao, Cheng, Huang, Liu, Tang, Li, Wang, Yin, Zhang, et~al.]{wang2023connecting}
Zehan Wang, Yang Zhao, Xize Cheng, Haifeng Huang, Jiageng Liu, Li~Tang, Linjun Li, Yongqi Wang, Aoxiong Yin, Ziang Zhang, et~al.
\newblock Connecting multi-modal contrastive representations.
\newblock \emph{arXiv preprint arXiv:2305.14381}, 2023{\natexlab{b}}.

\bibitem[Wu et~al.(2022)Wu, Seetharaman, Kumar, and Bello]{wu2022wav2clip}
Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, and Juan~Pablo Bello.
\newblock Wav2clip: Learning robust audio representations from clip.
\newblock In \emph{ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pp.\  4563--4567. IEEE, 2022.

\bibitem[Wu et~al.(2023)Wu, Chen, Zhang, Hui, Berg-Kirkpatrick, and Dubnov]{wu2023large}
Yusong Wu, Ke~Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov.
\newblock Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation.
\newblock In \emph{ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pp.\  1--5. IEEE, 2023.

\bibitem[Wu et~al.(2015)Wu, Song, Khosla, Yu, Zhang, Tang, and Xiao]{wu20153d}
Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao.
\newblock 3d shapenets: A deep representation for volumetric shapes.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  1912--1920, 2015.

\bibitem[Xu et~al.(2021)Xu, Ghosh, Huang, Okhonko, Aghajanyan, Metze, Zettlemoyer, and Feichtenhofer]{xu2021videoclip}
Hu~Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer.
\newblock Videoclip: Contrastive pre-training for zero-shot video-text understanding.
\newblock \emph{arXiv preprint arXiv:2109.14084}, 2021.

\bibitem[Xue et~al.(2023{\natexlab{a}})Xue, Gao, Xing, Mart{\'\i}n-Mart{\'\i}n, Wu, Xiong, Xu, Niebles, and Savarese]{xue2023ulip}
Le~Xue, Mingfei Gao, Chen Xing, Roberto Mart{\'\i}n-Mart{\'\i}n, Jiajun Wu, Caiming Xiong, Ran Xu, Juan~Carlos Niebles, and Silvio Savarese.
\newblock Ulip: Learning a unified representation of language, images, and point clouds for 3d understanding.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  1179--1189, 2023{\natexlab{a}}.

\bibitem[Xue et~al.(2023{\natexlab{b}})Xue, Yu, Zhang, Li, Mart{\'\i}n-Mart{\'\i}n, Wu, Xiong, Xu, Niebles, and Savarese]{xue2023ulip2}
Le~Xue, Ning Yu, Shu Zhang, Junnan Li, Roberto Mart{\'\i}n-Mart{\'\i}n, Jiajun Wu, Caiming Xiong, Ran Xu, Juan~Carlos Niebles, and Silvio Savarese.
\newblock Ulip-2: Towards scalable multimodal pre-training for 3d understanding.
\newblock \emph{arXiv preprint arXiv:2305.08275}, 2023{\natexlab{b}}.

\bibitem[Zhang et~al.(2023)Zhang, Li, and Bing]{zhang2023video}
Hang Zhang, Xin Li, and Lidong Bing.
\newblock Video-llama: An instruction-tuned audio-visual language model for video understanding.
\newblock \emph{arXiv preprint arXiv:2306.02858}, 2023.

\bibitem[Zhao et~al.(2018)Zhao, Gan, Rouditchenko, Vondrick, McDermott, and Torralba]{zhao2018sound}
Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, and Antonio Torralba.
\newblock The sound of pixels.
\newblock In \emph{Proceedings of the European conference on computer vision (ECCV)}, pp.\  570--586, 2018.

\bibitem[Zhao et~al.(2021)Zhao, Cai, Sheng, and Xu]{zhao20213dvg}
Lichen Zhao, Daigang Cai, Lu~Sheng, and Dong Xu.
\newblock 3dvg-transformer: Relation modeling for visual grounding on point clouds.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  2928--2937, 2021.

\bibitem[Zhao et~al.(2023)Zhao, Lin, Zhou, Huang, Feng, and Kang]{zhao2023bubogpt}
Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang.
\newblock Bubogpt: Enabling visual grounding in multi-modal llms.
\newblock \emph{arXiv preprint arXiv:2307.08581}, 2023.

\end{thebibliography}
