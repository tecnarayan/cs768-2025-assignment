\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abid et~al.(2021)Abid, Farooqi, and Zou]{abid2021persistent}
Abubakar Abid, Maheen Farooqi, and James Zou.
\newblock Persistent anti-muslim bias in large language models.
\newblock \emph{arXiv preprint arXiv:2101.05783}, 2021.

\bibitem[Amodei et~al.(2016)]{amodei2016}
Dario Amodei et~al.
\newblock Deep speech 2 end to end speech recognition in {E}nglish and
  mandarin.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  173--182, 2016.

\bibitem[Axelrod(2017)]{axelrod2017cynical}
Amittai Axelrod.
\newblock Cynical selection of language model training data.
\newblock \emph{CoRR}, abs/1709.02279, 2017.
\newblock URL \url{http://arxiv.org/abs/1709.02279}.

\bibitem[Ben-Tal et~al.(2013)Ben-Tal, den Hertog, Waegenaere, Melenberg, and
  Rennen]{bental2013robust}
Aharon Ben-Tal, Dick den Hertog, Anja~De Waegenaere, Bertrand Melenberg, and
  Gijs Rennen.
\newblock Robust solutions of optimization problems affected by uncertain
  probabilities.
\newblock \emph{Management Science}, 59:\penalty0 341--357, 2013.

\bibitem[Berant et~al.(2013)Berant, Chou, Frostig, and
  Liang]{berant2013freebase}
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang.
\newblock Semantic parsing on {F}reebase from question-answer pairs.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)},
  2013.

\bibitem[Blodgett and OConnor(2017)]{blodgett2017racial}
Su~Lin Blodgett and Brendan OConnor.
\newblock Racial disparity in natural language processing: A case study of
  social media {A}frican-{A}merican {E}nglish.
\newblock \emph{arXiv preprint arXiv:1707.00061}, 2017.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von
  Arx, Bernstein, Bohg, Bosselut, Brunskill, Brynjolfsson, Buch, Card,
  Castellon, Chatterji, Chen, Creel, Davis, Demszky, Donahue, Doumbouya,
  Durmus, Ermon, Etchemendy, Ethayarajh, Fei-Fei, Finn, Gale, Gillespie, Goel,
  Goodman, Grossman, Guha, Hashimoto, Henderson, Hewitt, Ho, Hong, Hsu, Huang,
  Icard, Jain, Jurafsky, Kalluri, Karamcheti, Keeling, Khani, Khattab, Koh,
  Krass, Krishna, Kuditipudi, Kumar, Ladhak, Lee, Lee, Leskovec, Levent, Li,
  Li, Ma, Malik, Manning, Mirchandani, Mitchell, Munyikwa, Nair, Narayan,
  Narayanan, Newman, Nie, Niebles, Nilforoshan, Nyarko, Ogut, Orr,
  Papadimitriou, Park, Piech, Portelance, Potts, Raghunathan, Reich, Ren, Rong,
  Roohani, Ruiz, Ryan, Ré, Sadigh, Sagawa, Santhanam, Shih, Srinivasan,
  Tamkin, Taori, Thomas, Tramèr, Wang, Wang, Wu, Wu, Wu, Xie, Yasunaga, You,
  Zaharia, Zhang, Zhang, Zhang, Zhang, Zheng, Zhou, and
  Liang]{bommasani2021opportunities}
Rishi Bommasani, Drew~A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon,
  Niladri Chatterji, Annie Chen, Kathleen Creel, Jared~Quincy Davis, Dorottya
  Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John
  Etchemendy, Kawin Ethayarajh, Li~Fei-Fei, Chelsea Finn, Trevor Gale, Lauren
  Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori
  Hashimoto, Peter Henderson, John Hewitt, Daniel~E. Ho, Jenny Hong, Kyle Hsu,
  Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri,
  Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang~Wei
  Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal
  Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang~Lisa Li,
  Xuechen Li, Tengyu Ma, Ali Malik, Christopher~D. Manning, Suvir Mirchandani,
  Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak
  Narayanan, Ben Newman, Allen Nie, Juan~Carlos Niebles, Hamed Nilforoshan,
  Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon~Sung Park,
  Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich,
  Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher
  Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan
  Srinivasan, Alex Tamkin, Rohan Taori, Armin~W. Thomas, Florian Tramèr,
  Rose~E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang~Michael Xie,
  Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang,
  Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Bommasani et~al.(2022)Bommasani, Creel, Kumar, Jurafsky, and
  Liang]{bommasani2022homogenization}
Rishi Bommasani, Kathleen~A. Creel, Ananya Kumar, Dan Jurafsky, and Percy
  Liang.
\newblock Picking on the same person: Does algorithmic monoculture lead to
  outcome homogenization?
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020gpt3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
  Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter,
  Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
  Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
  Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez,
  Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury,
  Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski,
  García, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov,
  Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira,
  Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei,
  Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
  A.~Rao, Parker Barnes, Yi~Tay, Noam~M. Shazeer, Vinodkumar Prabhakaran, Emily
  Reif, Nan Du, B.~Hutchinson, Reiner Pope, James Bradbury, Jacob Austin,
  M.~Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya,
  S.~Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra,
  Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, D.~Luan, Hyeontaek
  Lim, Barret Zoph, A.~Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal,
  Mark Omernick, Andrew~M. Dai, T.~S. Pillai, Marie Pellat, Aitor Lewkowycz,
  E.~Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou,
  Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason
  Wei, K.~Meier-Hellstern, D.~Eck, J.~Dean, Slav Petrov, and Noah Fiedel.
\newblock {PaLM}: Scaling language modeling with pathways.
\newblock \emph{arXiv}, 2022.

\bibitem[Coleman et~al.(2020)Coleman, Yeh, Mussmann, Mirzasoleiman, Bailis,
  Liang, Leskovec, and Zaharia]{coleman2020selection}
Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter
  Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia.
\newblock Selection via proxy: Efficient data selection for deep learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock {I}mage{N}et: A large-scale hierarchical image database.
\newblock In \emph{Computer Vision and Pattern Recognition (CVPR)}, pages
  248--255, 2009.

\bibitem[Du et~al.(2021)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu,
  Firat, Zoph, Fedus, Bosma, Zhou, Wang, Wang, Webster, Pellat, Robinson,
  Meier-Hellstern, Duke, Dixon, Zhang, Le, Wu, Chen, and Cui]{du2021glam}
Nan Du, Yanping Huang, Andrew~M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong
  Xu, M.~Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, Barret Zoph, Liam
  Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu~Emma Wang, Kellie Webster,
  Marie Pellat, Kevin Robinson, K.~Meier-Hellstern, Toju Duke, Lucas Dixon, Kun
  Zhang, Quoc~V. Le, Yonghui Wu, Zhifeng Chen, and Claire Cui.
\newblock {GLaM}: Efficient scaling of language models with mixture-of-experts.
\newblock \emph{arXiv}, 2021.

\bibitem[Duchi et~al.(2019)Duchi, Hashimoto, and
  Namkoong]{duchi2019distributionally}
John Duchi, Tatsunori Hashimoto, and Hongseok Namkoong.
\newblock Distributionally robust losses against mixture covariate shifts.
\newblock
  \url{https://cs.stanford.edu/~thashim/assets/publications/condrisk.pdf},
  2019.

\bibitem[Feng et~al.(2022)Feng, Xia, Van~Durme, and Sedoc]{feng2022automatic}
Yukun Feng, Patrick Xia, Benjamin Van~Durme, and João Sedoc.
\newblock Automatic document selection for efficient encoder pretraining, 2022.
\newblock URL \url{https://arxiv.org/abs/2210.10951}.

\bibitem[Gadre et~al.(2023)Gadre, Ilharco, Fang, Hayase, Smyrnis, Nguyen,
  Marten, Wortsman, Ghosh, Zhang, Orgad, Entezari, Daras, Pratt, Ramanujan,
  Bitton, Marathe, Mussmann, Vencu, Cherti, Krishna, Koh, Saukh, Ratner, Song,
  Hajishirzi, Farhadi, Beaumont, Oh, Dimakis, Jitsev, Carmon, Shankar, and
  Schmidt]{gadre2023datacomp}
Samir~Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios
  Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu
  Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek
  Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu,
  Mehdi Cherti, Ranjay Krishna, Pang~Wei Koh, Olga Saukh, Alexander Ratner,
  Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh,
  Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig
  Schmidt.
\newblock Datacomp: In search of the next generation of multimodal datasets.
\newblock \emph{arXiv preprint arXiv:2304.14108}, 2023.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,
  He, Thite, Nabeshima, Presser, and Leahy]{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
  Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser,
  and Connor Leahy.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{arXiv}, 2020.

\bibitem[Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and
  Smith]{gehman2020realtoxicityprompts}
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah~A Smith.
\newblock Realtoxicityprompts: Evaluating neural toxic degeneration in language
  models.
\newblock \emph{arXiv preprint arXiv:2009.11462}, 2020.

\bibitem[Hashimoto et~al.(2018)Hashimoto, Srivastava, Namkoong, and
  Liang]{hashimoto2018repeated}
Tatsunori~B. Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang.
\newblock Fairness without demographics in repeated loss minimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, de~Las~Casas, Hendricks, Welbl, Clark, Hennigan, Noland,
  Millican, van~den Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Rae,
  Vinyals, and Sifre]{hoffmann2022chinchilla}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las~Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van~den
  Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich
  Elsen, Jack~W. Rae, Oriol Vinyals, and Laurent Sifre.
\newblock An empirical analysis of compute-optimal large language model
  training.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2022.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and
  Zettlemoyer]{joshi2017triviaqa}
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer.
\newblock {TriviaQA}: A large scale distantly supervised challenge dataset for
  reading comprehension.
\newblock In \emph{Association for Computational Linguistics (ACL)}, 2017.

\bibitem[Kaushal et~al.(2019)Kaushal, Iyer, Kothawade, Mahadev, Doctor, and
  Ramakrishnan]{kaushal2019learning}
Vishal Kaushal, Rishabh Iyer, Suraj Kothawade, Rohan Mahadev, Khoshrav Doctor,
  and Ganesh Ramakrishnan.
\newblock Learning from less data: A unified data subset selection and active
  learning framework for computer vision.
\newblock \emph{IEEE/CVF Winter Conference on Applicatios of Computer Vision
  (WACV)}, 2019.

\bibitem[Killamsetty et~al.(2021{\natexlab{a}})Killamsetty, S, Ramakrishnan,
  De, and Iyer]{killamsetty2021gradmatch}
Krishnateja Killamsetty, Durga S, Ganesh Ramakrishnan, Abir De, and Rishabh
  Iyer.
\newblock {GRAD-MATCH}: Gradient matching based data subset selection for
  efficient deep model training.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2021{\natexlab{a}}.

\bibitem[Killamsetty et~al.(2021{\natexlab{b}})Killamsetty, Sivasubramanian,
  Ramakrishnan, and Iyer]{killamsetty2021glister}
Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and
  Rishabh Iyer.
\newblock Glister: Generalization based data subset selection for efficient and
  robust learning.
\newblock In \emph{Association for the Advancement of Artificial Intelligence
  (AAAI)}, 2021{\natexlab{b}}.

\bibitem[Killamsetty et~al.(2021{\natexlab{c}})Killamsetty, Zhao, Chen, and
  Iyer]{killamsetty2021retrieve}
Krishnateja Killamsetty, Xujiang Zhao, Feng Chen, and Rishabh Iyer.
\newblock Retrieve: Coreset selection for efficient and robust semi-supervised
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021{\natexlab{c}}.

\bibitem[Kingma and Ba(2015)]{kingma2015adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem[Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins,
  Parikh, Alberti, Epstein, Polosukhin, Kelcey, Devlin, Lee, Toutanova, Jones,
  Chang, Dai, Uszkoreit, Le, and Petrov]{kwiatkowski2019natural}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
  Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey,
  Jacob Devlin, Kenton Lee, Kristina~N. Toutanova, Llion Jones, Ming-Wei Chang,
  Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.
\newblock Natural questions: A benchmark for question answering research.
\newblock In \emph{Association for Computational Linguistics (ACL)}, 2019.

\bibitem[Lacoste et~al.(2019)Lacoste, Luccioni, Schmidt, and
  Dandres]{lacoste2019quantifying}
Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres.
\newblock Quantifying the carbon emissions of machine learning.
\newblock \emph{arXiv preprint arXiv:1910.09700}, 2019.

\bibitem[Ligozat et~al.(2021)Ligozat, Lef{\`{e}}vre, Bugeau, and
  Combaz]{ligozat2021unraveling}
Anne{-}Laure Ligozat, Julien Lef{\`{e}}vre, Aur{\'{e}}lie Bugeau, and Jacques
  Combaz.
\newblock Unraveling the hidden environmental impacts of {AI} solutions for
  environment.
\newblock \emph{CoRR}, abs/2110.11822, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.11822}.

\bibitem[Mindermann et~al.(2022)Mindermann, Brauner, Razzak, Sharma, Kirsch,
  Xu, Höltgen, Gomez, Morisot, Farquhar, and Gal]{mindermann2022prioritized}
Sören Mindermann, Jan Brauner, Muhammed Razzak, Mrinank Sharma, Andreas
  Kirsch, Winnie Xu, Benedikt Höltgen, Aidan~N. Gomez, Adrien Morisot,
  Sebastian Farquhar, and Yarin Gal.
\newblock Prioritized training on points that are learnable, worth learning,
  and not yet learnt.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Mirzasoleiman et~al.(2020)Mirzasoleiman, Bilmes, and
  Leskovec]{mirzasoleiman2020coresets}
Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec.
\newblock Coresets for data-efficient training of machine learning models.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Moore and Lewis(2010)]{moore2010intelligent}
Robert~C. Moore and William Lewis.
\newblock Intelligent selection of language model training data.
\newblock In \emph{Proceedings of the {ACL} 2010 Conference Short Papers},
  pages 220--224, Uppsala, Sweden, July 2010. Association for Computational
  Linguistics.
\newblock URL \url{https://aclanthology.org/P10-2041}.

\bibitem[Nadeem et~al.(2020)Nadeem, Bethke, and Reddy]{nadeem2020stereoset}
Moin Nadeem, Anna Bethke, and Siva Reddy.
\newblock Stereoset: Measuring stereotypical bias in pretrained language
  models.
\newblock \emph{arXiv preprint arXiv:2004.09456}, 2020.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovski2009robust}
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on optimization}, 19\penalty0 (4):\penalty0
  1574--1609, 2009.

\bibitem[Oren et~al.(2019)Oren, Sagawa, Hashimoto, and Liang]{oren2019drolm}
Yonatan Oren, Shiori Sagawa, Tatsunori Hashimoto, and Percy Liang.
\newblock Distributionally robust language modeling.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)},
  2019.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi,
  Pezzelle, Baroni, Boleda, and Fernandez]{paperno2016lambada}
Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan~Ngoc Pham, Raffaella
  Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez.
\newblock The {LAMBADA} dataset: Word prediction requiring a broad discourse
  context.
\newblock In \emph{Association for Computational Linguistics (ACL)}, 2016.

\bibitem[Patterson et~al.(2021)Patterson, Gonzalez, Le, Liang, Munguia,
  Rothchild, So, Texier, and Dean]{patterson2021carbon}
David~A. Patterson, Joseph Gonzalez, Quoc~V. Le, Chen Liang, Lluis{-}Miquel
  Munguia, Daniel Rothchild, David~R. So, Maud Texier, and Jeff Dean.
\newblock Carbon emissions and large neural network training.
\newblock \emph{CoRR}, abs/2104.10350, 2021.
\newblock URL \url{https://arxiv.org/abs/2104.10350}.

\bibitem[Paul et~al.(2021)Paul, Ganguli, and Dziugaite]{paul2021diet}
Mansheej Paul, Surya Ganguli, and Gintare~Karolina Dziugaite.
\newblock Deep learning on a data diet: Finding important examples early in
  training.
\newblock In \emph{Association for the Advancement of Artificial Intelligence
  (AAAI)}, 2021.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}, 2019.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang]{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock {SQuAD}: 100,000+ questions for machine comprehension of text.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)},
  2016.

\bibitem[Rajpurkar et~al.(2018)Rajpurkar, Jia, and
  Liang]{rajpurkar2018squadrun}
Pranav Rajpurkar, Robin Jia, and Percy Liang.
\newblock Know what you don't know: Unanswerable questions for {SQuAD}.
\newblock In \emph{Association for Computational Linguistics (ACL)}, 2018.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock {ImageNet} large scale visual recognition challenge.
\newblock \emph{International Journal of Computer Vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[Sagawa et~al.(2020)Sagawa, Koh, Hashimoto, and Liang]{sagawa2020group}
Shiori Sagawa, Pang~Wei Koh, Tatsunori~B. Hashimoto, and Percy Liang.
\newblock Distributionally robust neural networks for group shifts: On the
  importance of regularization for worst-case generalization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Schuhmann et~al.(2022)Schuhmann, Beaumont, Vencu, Gordon, Wightman,
  Cherti, Coombes, Katta, Mullis, Wortsman, Schramowski, Kundurthy, Crowson,
  Schmidt, Kaczmarczyk, and Jitsev]{schuhmann2022laion5b}
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross
  Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell
  Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig
  Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.
\newblock Laion-5b: An open large-scale dataset for training next generation
  image-text models.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2022.

\bibitem[Sener and Savarese(2018)]{sener2018active}
Ozan Sener and Silvio Savarese.
\newblock Active learning for convolutional neural networks: A core-set
  approach.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Shazeer and Stern(2018)]{shazeer2018adafactor}
Noam Shazeer and Mitchell Stern.
\newblock 2018.

\bibitem[Sinha et~al.(2018)Sinha, Namkoong, and Duchi]{sinha2018certifiable}
Aman Sinha, Hongseok Namkoong, and John Duchi.
\newblock Certifiable distributional robustness with principled adversarial
  training.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and Adams]{snoek12hyper}
Jasper Snoek, Hugo Larochelle, and Ryan~P. Adams.
\newblock Practical {B}ayesian optimization of machine learning algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2012.

\bibitem[Sorscher et~al.(2022)Sorscher, Geirhos, Shekhar, Ganguli, and
  Morcos]{sorscher2022beyond}
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari~S.
  Morcos.
\newblock Beyond neural scaling laws: beating power law scaling via data
  pruning.
\newblock \emph{arXiv}, 2022.

\bibitem[Strubell et~al.(2019)Strubell, Ganesh, and
  McCallum]{strubell2019energy}
Emma Strubell, Ananya Ganesh, and Andrew McCallum.
\newblock Energy and policy considerations for deep learning in {NLP}.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 3645--3650, Florence, Italy, July 2019.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P19-1355}.
\newblock URL \url{https://aclanthology.org/P19-1355}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{arXiv preprint arXiv:1706.03762}, 2017.

\bibitem[Wang et~al.(2019)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2019glue}
Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R
  Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Wang et~al.(2020)Wang, Pham, Michel, Anastasopoulos, Carbonell, and
  Neubig]{wang2020optimizing}
Xinyi Wang, Hieu Pham, Paul Michel, Antonios Anastasopoulos, Jaime Carbonell,
  and Graham Neubig.
\newblock Optimizing data usage via differentiable rewards.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Wei et~al.(2015)Wei, Iyer, and Bilmes]{wei2015submodular}
Kai Wei, Rishabh Iyer, and Jeff Bilmes.
\newblock Submodularity in data subset selection and active learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2015.

\bibitem[Xie et~al.(2023)Xie, Santurkar, Ma, and Liang]{xie2023data}
Sang~Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang.
\newblock Data selection for language models via importance resampling.
\newblock \emph{arXiv preprint arXiv:2302.03169}, 2023.

\bibitem[Zoph and Le(2016)]{zoph2016neural}
Barret Zoph and Quoc~V Le.
\newblock Neural architecture search with reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1611.01578}, 2016.

\end{thebibliography}
