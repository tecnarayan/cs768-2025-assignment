\begin{thebibliography}{23}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Castaldo et~al.(2008)Castaldo, Whaley, and
  Chronopoulos]{castaldo2008reducing}
Anthony~M Castaldo, R~Clint Whaley, and Anthony~T Chronopoulos.
\newblock Reducing floating point error in dot product using the superblock
  family of algorithms.
\newblock \emph{SIAM journal on scientific computing}, 31\penalty0
  (2):\penalty0 1156--1174, 2008.

\bibitem[Chen et~al.(2018)Chen, Choi, Gopalakrishnan, Srinivasan, and
  Venkataramani]{chen2018exploiting}
Chia-Yu Chen, Jungwook Choi, Kailash Gopalakrishnan, Viji Srinivasan, and
  Swagath Venkataramani.
\newblock Exploiting approximate computing for deep learning acceleration.
\newblock In \emph{Design, Automation \& Test in Europe Conference \&
  Exhibition (DATE)}, pages 821--826, 2018.

\bibitem[Choi et~al.(2018)Choi, Wang, Venkataramani, Chuang, Srinivasan, and
  Gopalakrishnan]{choi2018pact}
Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang,
  Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan.
\newblock Pact: Parameterized clipping activation for quantized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1805.06085}, 2018.

\bibitem[Das et~al.(2018)Das, Mellempudi, Mudigere, Kalamkar, Avancha,
  Banerjee, Sridharan, Vaidyanathan, Kaul, Georganas, et~al.]{das2018mixed}
Dipankar Das, Naveen Mellempudi, Dheevatsa Mudigere, Dhiraj Kalamkar, Sasikanth
  Avancha, Kunal Banerjee, Srinivas Sridharan, Karthik Vaidyanathan, Bharat
  Kaul, Evangelos Georganas, et~al.
\newblock Mixed precision training of convolutional neural networks using
  integer operations.
\newblock \emph{arXiv preprint arXiv:1802.00930}, 2018.

\bibitem[Fleischer et~al.(2018)Fleischer, Shukla, Ziegler, Silberman, Oh,
  Srinivasan, Choi, Mueller, Agrawal, Babinsky, et~al.]{Fleischer2018aScalable}
Bruce Fleischer, Sunil Shukla, Matthew Ziegler, Joel Silberman, Jinwook Oh,
  Vijayalakshmi Srinivasan, Jungwook Choi, Silvia Mueller, Ankur Agrawal, Tina
  Babinsky, et~al.
\newblock A scalable multi‐teraops deep learning processor core for ai
  training and inference.
\newblock In \emph{VLSI Circuits, 2018 Symposium on}. IEEE, 2018.

\bibitem[Gupta et~al.(2015)Gupta, Agrawal, Gopalakrishnan, and
  Narayanan]{gupta2015deep}
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
\newblock Deep learning with limited numerical precision.
\newblock In \emph{International Conference on Machine Learning}, pages
  1737--1746, 2015.

\bibitem[Gupta et~al.(2016)Gupta, Zhang, and Wang]{gupta2016model}
Suyog Gupta, Wei Zhang, and Fei Wang.
\newblock Model accuracy and runtime tradeoff in distributed deep learning: A
  systematic study.
\newblock In \emph{Data Mining (ICDM), 2016 IEEE 16th International Conference
  on}, pages 171--180. IEEE, 2016.

\bibitem[Harris(2016)]{nvidiavolta}
Mark Harris.
\newblock Mixed-precision programming with cuda 8, 2016.
\newblock URL
  \url{https://devblogs.nvidia.com/mixed-precision-programming-cuda-8/}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock {Deep Residual Learning for Image Recognition}.
\newblock \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 770--778, June 2016.

\bibitem[Higham(1993)]{highamSiam93}
Nicholas~J Higham.
\newblock The accuracy of floating point summation.
\newblock \emph{SIAM Journal on Scientific Computing}, 14\penalty0
  (4):\penalty0 783--799, 1993.

\bibitem[Hubara et~al.(2016)Hubara, Courbariaux, Soudry, El-Yaniv, and
  Bengio]{hubara2016binarized}
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
  Bengio.
\newblock Binarized neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  4107--4115, 2016.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock {Adam: {A} Method for Stochastic Optimization}.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2015.

\bibitem[K{\"o}ster et~al.(2017)K{\"o}ster, Webb, Wang, Nassar, Bansal,
  Constable, Elibol, Gray, Hall, Hornof, et~al.]{koster2017flexpoint}
Urs K{\"o}ster, Tristan Webb, Xin Wang, Marcel Nassar, Arjun~K Bansal, William
  Constable, Oguz Elibol, Scott Gray, Stewart Hall, Luke Hornof, et~al.
\newblock Flexpoint: An adaptive numerical format for efficient training of
  deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1742--1752, 2017.

\bibitem[Krizhevsky and Hinton(2010)]{krizhevsky2010convolutional}
Alex Krizhevsky and G~Hinton.
\newblock Convolutional deep belief networks on cifar-10.
\newblock \emph{Unpublished manuscript}, 40, 2010.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{Krizhevsky2012}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E. Hinton.
\newblock {ImageNet Classification with Deep Convolutional Neural Networks}.
\newblock In \emph{Advances in Neural Information Processing Systems 25
  (NIPS)}, pages 1097--1105, 2012.

\bibitem[Micikevicius et~al.(2017)Micikevicius, Narang, Alben, Diamos, Elsen,
  Garcia, Ginsburg, Houston, Kuchaev, Venkatesh, et~al.]{micikevicius2017mixed}
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen,
  David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaev, Ganesh
  Venkatesh, et~al.
\newblock Mixed precision training.
\newblock \emph{arXiv preprint arXiv:1710.03740}, 2017.

\bibitem[Robertazzi and Schwartz(1988)]{robertazziSiam88}
Thomas~G Robertazzi and Stuart~C Schwartz.
\newblock Best “ordering” for floating-point addition.
\newblock \emph{ACM Transactions on Mathematical Software (TOMS)}, 14\penalty0
  (1):\penalty0 101--110, 1988.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{ILSVRC15}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
  Alexander~C. Berg, and Li~Fei-Fei.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[van~den Berg et~al.(2017)van~den Berg, Ramabhadran, and
  Picheny]{van2017training}
Ewout van~den Berg, Bhuvana Ramabhadran, and Michael Picheny.
\newblock Training variance and performance evaluation of neural networks in
  speech.
\newblock In \emph{Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE
  International Conference on}, pages 2287--2291. IEEE, 2017.

\bibitem[Wu et~al.(2018)Wu, Li, Chen, and Shi]{wu2018training}
Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi.
\newblock Training and inference with integers in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1802.04680}, 2018.

\bibitem[Wu et~al.(2016)Wu, Schuster, Chen, Le, Norouzi, Macherey, Krikun, Cao,
  Gao, Macherey, et~al.]{wu2016google}
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc~V Le, Mohammad Norouzi, Wolfgang
  Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et~al.
\newblock Google's neural machine translation system: Bridging the gap between
  human and machine translation.
\newblock \emph{arXiv preprint arXiv:1609.08144}, 2016.

\bibitem[Xiong et~al.(2017)Xiong, Droppo, Huang, Seide, Seltzer, Stolcke, Yu,
  and Zweig]{xiong2017microsoft}
Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas
  Stolcke, Dong Yu, and Geoffrey Zweig.
\newblock The microsoft 2016 conversational speech recognition system.
\newblock In \emph{Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE
  International Conference on}, pages 5255--5259. IEEE, 2017.

\bibitem[Zhou et~al.(2016)Zhou, Ni, Zhou, Wen, Wu, and Zou]{zhou2016dorefa}
Shuchang Zhou, Zekun Ni, Xinyu Zhou, He~Wen, Yuxin Wu, and Yuheng Zou.
\newblock {DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with
  Low Bitwidth Gradients}.
\newblock \emph{CoRR}, abs/1606.06160, 2016.

\end{thebibliography}
