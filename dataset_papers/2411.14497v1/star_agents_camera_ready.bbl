\begin{thebibliography}{10}

\bibitem{bai2023qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,
  Wenbin Ge, Yu~Han, Fei Huang, et~al.
\newblock Qwen technical report.
\newblock {\em arXiv preprint arXiv:2309.16609}, 2023.

\bibitem{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin~Gregory Anthony, Herbie Bradley,
  Kyle O’Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit,
  USVSN~Sai Prashanth, Edward Raff, et~al.
\newblock Pythia: A suite for analyzing large language models across training
  and scaling.
\newblock In {\em International Conference on Machine Learning}, pages
  2397--2430. PMLR, 2023.

\bibitem{chang2023survey}
Yupeng Chang, Xu~Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen,
  Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi~Chang,
  Philip~S. Yu, Qiang Yang, and Xing Xie.
\newblock A survey on evaluation of large language models, 2023.

\bibitem{chen2024magdi}
Justin Chih-Yao Chen, Swarnadeep Saha, Elias Stengel-Eskin, and Mohit Bansal.
\newblock Magdi: Structured distillation of multi-agent interaction graphs
  improves reasoning in smaller language models.
\newblock {\em arXiv preprint arXiv:2402.01620}, 2024.

\bibitem{chen2023alpagasus}
Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav,
  Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et~al.
\newblock Alpagasus: Training a better alpaca with fewer data.
\newblock {\em arXiv preprint arXiv:2307.08701}, 2023.

\bibitem{chiang2023vicuna}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E Gonzalez, et~al.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt
  quality.
\newblock {\em See https://vicuna. lmsys. org (accessed 14 April 2023)},
  2(3):6, 2023.

\bibitem{conover2023free}
Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali
  Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin.
\newblock Free dolly: Introducing the world’s first truly open
  instruction-tuned llm.
\newblock {\em Company Blog of Databricks}, 2023.

\bibitem{ding2023enhancing}
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan
  Liu, Maosong Sun, and Bowen Zhou.
\newblock Enhancing chat language models by scaling high-quality instructional
  conversations.
\newblock {\em arXiv preprint arXiv:2305.14233}, 2023.

\bibitem{du-etal-2022-glm}
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and
  Jie Tang.
\newblock {GLM}: General language model pretraining with autoregressive blank
  infilling.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 320--335, Dublin,
  Ireland, May 2022. Association for Computational Linguistics.

\bibitem{guo2023evaluating}
Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Linhao Yu, Yan Liu,
  Jiaxuan Li, Bojian Xiong, Deyi Xiong, et~al.
\newblock Evaluating large language models: A comprehensive survey.
\newblock {\em arXiv preprint arXiv:2310.19736}, 2023.

\bibitem{javaheripi2023phi}
Mojan Javaheripi, S{\'e}bastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien
  Bubeck, Caio C{\'e}sar~Teodoro Mendes, Weizhu Chen, Allie Del~Giorno, Ronen
  Eldan, Sivakanth Gopi, et~al.
\newblock Phi-2: The surprising power of small language models.
\newblock {\em Microsoft Research Blog}, 2023.

\bibitem{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
  Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel,
  Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock {\em arXiv preprint arXiv:2310.06825}, 2023.

\bibitem{jiang2023lion}
Yuxin Jiang, Chunkit Chan, Mingyang Chen, and Wei Wang.
\newblock Lion: Adversarial distillation of closed-source large language model.
\newblock {\em arXiv preprint arXiv:2305.12870}, 2023.

\bibitem{khashabi-etal-2020-unifiedqa}
Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord,
  Peter Clark, and Hannaneh Hajishirzi.
\newblock {UNIFIEDQA}: Crossing format boundaries with a single {QA} system.
\newblock In {\em Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pages 1896--1907, Online, November 2020. Association for
  Computational Linguistics.

\bibitem{kopf2024openassistant}
Andreas K{\"o}pf, Yannic Kilcher, Dimitri von R{\"u}tte, Sotiris Anagnostidis,
  Zhi~Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley,
  Rich{\'a}rd Nagyfi, et~al.
\newblock Openassistant conversations-democratizing large language model
  alignment.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{li2023camel}
Guohao Li, Hasan Abed Al~Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and
  Bernard Ghanem.
\newblock Camel: Communicative agents for" mind" exploration of large scale
  language model society.
\newblock 2023.

\bibitem{li2024selective}
Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Jiuxiang Gu, and Tianyi Zhou.
\newblock Selective reflection-tuning: Student-selected data recycling for llm
  instruction-tuning.
\newblock {\em arXiv preprint arXiv:2402.10110}, 2024.

\bibitem{Li2023ReflectionTuningDR}
Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Heng Huang, Jiuxiang Gu, and
  Tianyi Zhou.
\newblock Reflection-tuning: Data recycling improves llm instruction-tuning.
\newblock {\em ArXiv}, abs/2310.11716, 2023.

\bibitem{li2024superfiltering}
Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning
  Cheng, and Tianyi Zhou.
\newblock Superfiltering: Weak-to-strong data filtering for fast
  instruction-tuning, 2024.

\bibitem{li2023quantity}
Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong
  Wang, Tianyi Zhou, and Jing Xiao.
\newblock From quantity to quality: Boosting llm performance with self-guided
  data selection for instruction tuning.
\newblock {\em arXiv preprint arXiv:2308.12032}, 2023.

\bibitem{liu2024openeval}
Chuang Liu, Linhao Yu, Jiaxuan Li, Renren Jin, Yufei Huang, Ling Shi, Junhui
  Zhang, Xinmeng Ji, Tingting Cui, Tao Liu, et~al.
\newblock Openeval: Benchmarking chinese llms across capability, alignment and
  safety.
\newblock {\em arXiv preprint arXiv:2403.12316}, 2024.

\bibitem{liu2023makes}
Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He.
\newblock What makes good data for alignment? a comprehensive study of
  automatic data selection in instruction tuning.
\newblock {\em arXiv preprint arXiv:2312.15685}, 2023.

\bibitem{Longpre2023TheFC}
S.~Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny Zhou,
  Quoc~V. Le, Barret Zoph, Jason Wei, and Adam Roberts.
\newblock The flan collection: Designing data and methods for effective
  instruction tuning.
\newblock {\em ArXiv}, abs/2301.13688, 2023.

\bibitem{lu2023instag}
Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang
  Zhou, and Jingren Zhou.
\newblock \# instag: Instruction tagging for analyzing supervised fine-tuning
  of large language models.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2023.

\bibitem{naveed2023comprehensive}
Humza Naveed, Asad~Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad
  Usman, Nick Barnes, and Ajmal Mian.
\newblock A comprehensive overview of large language models.
\newblock {\em arXiv preprint arXiv:2307.06435}, 2023.

\bibitem{nijkamp2023xgen}
Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo~Pang, Congying Xia, Chen Xing,
  Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, et~al.
\newblock Xgen-7b technical report.
\newblock {\em arXiv preprint arXiv:2309.03450}, 2023.

\bibitem{shen2023large}
Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo,
  Xinwei Wu, Yan Liu, and Deyi Xiong.
\newblock Large language model alignment: A survey.
\newblock {\em arXiv preprint arXiv:2309.15025}, 2023.

\bibitem{shen2023roleeval}
Tianhao Shen, Sun Li, Quan Tu, and Deyi Xiong.
\newblock Roleeval: A bilingual role evaluation benchmark for large language
  models.
\newblock {\em arXiv preprint arXiv:2312.16132}, 2023.

\bibitem{shu2023exploitability}
Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao, and Tom
  Goldstein.
\newblock On the exploitability of instruction tuning, 2023.

\bibitem{sun2024fuxitranyu}
Haoran Sun, Renren Jin, Shaoyang Xu, Leiyu Pan, Menglong Cui, Jiangcun Dui,
  Yikun Lei, Lei Yang, Ling Shi, Juesi Xiao, et~al.
\newblock Fuxitranyu: A multilingual large language model trained with balanced
  data.
\newblock {\em arXiv preprint arXiv:2408.06273}, 2024.

\bibitem{tang2024rethinking}
Yehui Tang, Fangcheng Liu, Yunsheng Ni, Yuchuan Tian, Zheyuan Bai, Yi-Qi Hu,
  Sichao Liu, Shangling Jui, Kai Han, and Yunhe Wang.
\newblock Rethinking optimization and architecture for tiny language models.
\newblock {\em arXiv preprint arXiv:2402.02791}, 2024.

\bibitem{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem{taori2023stanford}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model, 2023.

\bibitem{team2024gemma}
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,
  Shreya Pathak, Laurent Sifre, Morgane Rivi{\`e}re, Mihir~Sanjay Kale,
  Juliette Love, et~al.
\newblock Gemma: Open models based on gemini research and technology.
\newblock {\em arXiv preprint arXiv:2403.08295}, 2024.

\bibitem{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{wang2022self}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language models with self-generated
  instructions.
\newblock {\em arXiv preprint arXiv:2212.10560}, 2022.

\bibitem{wang-etal-2023-self-instruct}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language models with self-generated
  instructions.
\newblock In {\em Proceedings of the 61st Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 13484--13508,
  Toronto, Canada, July 2023. Association for Computational Linguistics.

\bibitem{wang-etal-2022-super}
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza
  Mirzaei, Atharva Naik, Arjun Ashok, Arut~Selvan Dhanasekaran, Anjana
  Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai,
  Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi,
  Kuntal~Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali
  Purohit, Neeraj Varshney, Phani~Rohitha Kaza, Pulkit Verma, Ravsehaj~Singh
  Puri, Rushang Karia, Savan Doshi, Shailaja~Keyur Sampat, Siddhartha Mishra,
  Sujan Reddy~A, Sumanta Patro, Tanay Dixit, and Xudong Shen.
\newblock Super-{N}atural{I}nstructions: Generalization via declarative
  instructions on 1600+ {NLP} tasks.
\newblock In {\em Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 5085--5109, Abu Dhabi, United Arab
  Emirates, December 2022. Association for Computational Linguistics.

\bibitem{wang2023pangu}
Yunhe Wang, Hanting Chen, Yehui Tang, Tianyu Guo, Kai Han, Ying Nie, Xutao
  Wang, Hailin Hu, Zheyuan Bai, Yun Wang, et~al.
\newblock Pangu-$pi$: Enhancing language model architectures via nonlinearity
  compensation.
\newblock {\em arXiv preprint arXiv:2312.17276}, 2023.

\bibitem{wei2022finetuned}
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester,
  Nan Du, Andrew~M. Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian
  Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock {\em arXiv preprint arXiv:2109.01652}, 2021.

\bibitem{xia2023sheared}
Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen.
\newblock Sheared llama: Accelerating language model pre-training via
  structured pruning.
\newblock {\em arXiv preprint arXiv:2310.06694}, 2023.

\bibitem{xia2024less}
Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi
  Chen.
\newblock Less: Selecting influential data for targeted instruction tuning.
\newblock {\em arXiv preprint arXiv:2402.04333}, 2024.

\bibitem{xu2023wizardlm}
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang
  Tao, and Daxin Jiang.
\newblock Wizardlm: Empowering large language models to follow complex
  instructions.
\newblock {\em arXiv preprint arXiv:2304.12244}, 2023.

\bibitem{xu2023baize}
Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley.
\newblock Baize: An open-source chat model with parameter-efficient tuning on
  self-chat data.
\newblock {\em arXiv preprint arXiv:2304.01196}, 2023.

\bibitem{xu2023rethinking}
Yang Xu, Yongqiang Yao, Yufan Huang, Mengnan Qi, Maoquan Wang, Bin Gu, and Neel
  Sundaresan.
\newblock Rethinking the instruction quality: Lift is what you need, 2023.

\bibitem{yan2023backdooring}
Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay
  Srinivasan, Xiang Ren, and Hongxia Jin.
\newblock Backdooring instruction-tuned large language models with virtual
  prompt injection.
\newblock In {\em NeurIPS 2023 Workshop on Backdoors in Deep Learning-The Good,
  the Bad, and the Ugly}, 2023.

\bibitem{yan2023virtual}
Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay
  Srinivasan, Xiang Ren, and Hongxia Jin.
\newblock Virtual prompt injection for instruction-tuned large language models,
  2023.

\bibitem{yang2023harnessing}
Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming
  Jiang, Bing Yin, and Xia Hu.
\newblock Harnessing the power of llms in practice: A survey on chatgpt and
  beyond, 2023.

\bibitem{ye-etal-2021-crossfit}
Qinyuan Ye, Bill~Yuchen Lin, and Xiang Ren.
\newblock {C}ross{F}it: A few-shot learning challenge for cross-task
  generalization in {NLP}.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 7163--7189, Online and Punta Cana,
  Dominican Republic, November 2021. Association for Computational Linguistics.

\bibitem{zeng2022glm}
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi
  Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et~al.
\newblock Glm-130b: An open bilingual pre-trained model.
\newblock {\em arXiv preprint arXiv:2210.02414}, 2022.

\bibitem{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock {\em arXiv preprint arXiv:2205.01068}, 2022.

\bibitem{zhao2023survey}
Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
  Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang,
  Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,
  Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.
\newblock A survey of large language models, 2023.

\bibitem{zheng2024judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
  Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\end{thebibliography}
