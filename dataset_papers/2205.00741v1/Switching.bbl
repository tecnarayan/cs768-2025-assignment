\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abernethy et~al.(2008)Abernethy, Bartlett, Rakhlin, and
  Tewari]{Minimax:Online}
Jacob Abernethy, Peter~L. Bartlett, Alexander Rakhlin, and Ambuj Tewari.
\newblock Optimal strategies and minimax lower bounds for online convex games.
\newblock In \emph{Proceedings of the 21st Annual Conference on Learning
  Theory}, pages 415--423, 2008.

\bibitem[Adamskiy et~al.(2012)Adamskiy, Koolen, Chernov, and
  Vovk]{Adamskiy2012}
Dmitry Adamskiy, Wouter~M. Koolen, Alexey Chernov, and Vladimir Vovk.
\newblock A closer look at adaptive regret.
\newblock In \emph{Proceedings of the 23rd International Conference on
  Algorithmic Learning Theory}, pages 290--304, 2012.

\bibitem[Antoniadis and Schewior(2018)]{10.1007/978-3-319-89441-6_13}
Antonios Antoniadis and Kevin Schewior.
\newblock A tight lower bound for online convex optimization with switching
  costs.
\newblock In \emph{Approximation and Online Algorithms}, pages 164--175, 2018.

\bibitem[Antoniadis et~al.(2016)Antoniadis, Barcelo, Nugent, Pruhs, Schewior,
  and Scquizzato]{CCBF:2016}
Antonios Antoniadis, Neal Barcelo, Michael Nugent, Kirk Pruhs, Kevin Schewior,
  and Michele Scquizzato.
\newblock Chasing convex bodies and functions.
\newblock In \emph{Proceedings of the 12th Latin American Symposium on
  Theoretical Informatics}, pages 68--81, 2016.

\bibitem[Argue et~al.(2019)Argue, Bubeck, Cohen, Gupta, and Lee]{Linear:CNCB}
C.J. Argue, S\'{e}bastien Bubeck, Michael~B. Cohen, Anupam Gupta, and Yin~Tat
  Lee.
\newblock A nearly-linear bound for chasing nested convex bodies.
\newblock In \emph{Proceedings of the 30th Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 117--122, 2019.

\bibitem[Argue et~al.(2020{\natexlab{a}})Argue, Gupta, and
  Guruganesh]{pmlr-v125-argue20a}
C.J. Argue, Anupam Gupta, and Guru Guruganesh.
\newblock Dimension-free bounds for chasing convex functions.
\newblock In \emph{Proceedings of 33rd Conference on Learning Theory}, pages
  219--241, 2020{\natexlab{a}}.

\bibitem[Argue et~al.(2020{\natexlab{b}})Argue, Gupta, Guruganesh, and
  Tang]{Linear:CCB}
C.J. Argue, Anupam Gupta, Guru Guruganesh, and Ziye Tang.
\newblock Chasing convex bodies with linear competitive ratio.
\newblock In \emph{Proceedings of the 31st Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 1519--1524, 2020{\natexlab{b}}.

\bibitem[Baby and Wang(2021)]{pmlr-v134-baby21a}
Dheeraj Baby and Yu-Xiang Wang.
\newblock Optimal dynamic regret in exp-concave online learning.
\newblock In \emph{Proceedings of the 34th Conference on Learning Theory},
  pages 359--409, 2021.

\bibitem[Bansal et~al.(2015)Bansal, Gupta, Krishnaswamy, Pruhs, Schewior, and
  Stein]{bansal_et_al:LIPIcs}
Nikhil Bansal, Anupam Gupta, Ravishankar Krishnaswamy, Kirk Pruhs, Kevin
  Schewior, and Cliff Stein.
\newblock A 2-competitive algorithm for online convex optimization with
  switching costs.
\newblock In \emph{Approximation, Randomization, and Combinatorial
  Optimization. Algorithms and Techniques}, pages 96--109, 2015.

\bibitem[Bansal et~al.(2018)Bansal, B\"{o}hm, Eli\'{a}{\v{s}}, Koumoutsos, and
  Umboh]{NCB:2018}
Nikhil Bansal, Martin B\"{o}hm, Marek Eli\'{a}{\v{s}}, Grigorios Koumoutsos,
  and Seeun~William Umboh.
\newblock Nested convex bodies are chaseable.
\newblock In \emph{Proceedings of the 29th Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 1253--1260, 2018.

\bibitem[Besbes et~al.(2015)Besbes, Gur, and Zeevi]{Non-Stationary}
Omar Besbes, Yonatan Gur, and Assaf Zeevi.
\newblock Non-stationary stochastic optimization.
\newblock \emph{Operations Research}, 63\penalty0 (5):\penalty0 1227--1244,
  2015.

\bibitem[Bubeck et~al.(2019)Bubeck, Lee, Li, and Sellke]{CCCB:Bubeck}
S\'{e}bastien Bubeck, Yin~Tat Lee, Yuanzhi Li, and Mark Sellke.
\newblock Competitively chasing convex bodies.
\newblock In \emph{Proceedings of the 51st Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 861--868, 2019.

\bibitem[Bubeck et~al.(2020)Bubeck, Klartag, Lee, Li, and Sellke]{Optimal:CNCB}
S\'{e}bastien Bubeck, Bo'az Klartag, Yin~Tat Lee, Yuanzhi Li, and Mark Sellke.
\newblock Chasing nested convex bodies nearly optimally.
\newblock In \emph{Proceedings of the 31st Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 1496--1508, 2020.

\bibitem[Cesa-Bianchi and Lugosi(2006)]{bianchi-2006-prediction}
Nicol\`{o} Cesa-Bianchi and G{\'a}bor Lugosi.
\newblock \emph{Prediction, Learning, and Games}.
\newblock Cambridge University Press, 2006.

\bibitem[Cesa-Bianchi and Orabona(2021)]{Online:Review:Casa}
Nicol{\`o} Cesa-Bianchi and Francesco Orabona.
\newblock Online learning algorithms.
\newblock \emph{Annual Review of Statistics and Its Application}, 8\penalty0
  (1):\penalty0 165--190, 2021.

\bibitem[Cesa-bianchi et~al.(2012)Cesa-bianchi, Gaillard, Lugosi, and
  Stoltz]{Fixed:Share:NIPS12}
Nicol\`{o} Cesa-bianchi, Pierre Gaillard, Gabor Lugosi, and Gilles Stoltz.
\newblock Mirror descent meets fixed share (and feels no regret).
\newblock In \emph{Advances in Neural Information Processing Systems 25}, pages
  980--988, 2012.

\bibitem[Chen et~al.(2018)Chen, Goel, and Wierman]{SOCO:OBD}
Niangjun Chen, Gautam Goel, and Adam Wierman.
\newblock Smoothed online convex optimization in high dimensions via online
  balanced descent.
\newblock In \emph{Proceedings of the 31st Conference on Learning Theory},
  pages 1574--1594, 2018.

\bibitem[Chiang et~al.(2012)Chiang, Yang, Lee, Mahdavi, Lu, Jin, and
  Zhu]{Gradual:COLT:12}
Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong
  Jin, and Shenghuo Zhu.
\newblock Online optimization with gradual variations.
\newblock In \emph{Proceedings of the 25th Annual Conference on Learning
  Theory}, 2012.

\bibitem[Cutkosky(2020)]{pmlr-v119-cutkosky20a}
Ashok Cutkosky.
\newblock Parameter-free, dynamic, and strongly-adaptive online learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pages 2250--2259, 2020.

\bibitem[Cutkosky and Orabona(2018)]{pmlr-v75-cutkosky18a}
Ashok Cutkosky and Francesco Orabona.
\newblock Black-box reductions for parameter-free online learning in {Banach}
  spaces.
\newblock In \emph{Proceedings of the 31st Conference On Learning Theory},
  pages 1493--1529, 2018.

\bibitem[Daniely and Mansour(2019)]{pmlr-v98-daniely19a}
Amit Daniely and Yishay Mansour.
\newblock Competitive ratio vs regret minimization: achieving the best of both
  worlds.
\newblock In \emph{Proceedings of the 30th International Conference on
  Algorithmic Learning Theory}, pages 333--368, 2019.

\bibitem[Daniely et~al.(2015)Daniely, Gonen, and
  Shalev-Shwartz]{Adaptive:ICML:15}
Amit Daniely, Alon Gonen, and Shai Shalev-Shwartz.
\newblock Strongly adaptive online learning.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning}, pages 1405--1411, 2015.

\bibitem[Even-Dar et~al.(2007)Even-Dar, Kearns, Mansour, and
  Wortman]{COLT:07:Average}
Eyal Even-Dar, Michael Kearns, Yishay Mansour, and Jennifer Wortman.
\newblock Regret to the best vs. regret to the average.
\newblock In \emph{Proceedings of the 20th Annual Conference on Learning
  Theory}, pages 233--247, 2007.

\bibitem[Freund et~al.(1997)Freund, Schapire, Singer, and
  Warmuth]{Freund:1997:UCP}
Yoav Freund, Robert~E. Schapire, Yoram Singer, and Manfred~K. Warmuth.
\newblock Using and combining predictors that specialize.
\newblock In \emph{Proceedings of the 29th Annual ACM Symposium on Theory of
  Computing}, pages 334--343, 1997.

\bibitem[Friedman and Linial(1993)]{Convex:Body}
Joel Friedman and Nathan Linial.
\newblock On convex body chasing.
\newblock \emph{Discrete \& Computational Geometry}, 9:\penalty0 293--321,
  1993.

\bibitem[Goel and Wierman(2019)]{OBD:Strongly}
Gautam Goel and Adam Wierman.
\newblock An online algorithm for smoothed regression and lqr control.
\newblock In \emph{Proceedings of the 22nd International Conference on
  Artificial Intelligence and Statistics}, pages 2504--2513, 2019.

\bibitem[Goel et~al.(2017)Goel, Chen, and Wierman]{Optimization:timescales}
Gautam Goel, Niangjun Chen, and Adam Wierman.
\newblock Thinking fast and slow: Optimization decomposition across timescales.
\newblock In \emph{Proceedings of the 56th IEEE Conference on Decision and
  Control}, pages 1291--1298, 2017.

\bibitem[Goel et~al.(2019)Goel, Lin, Sun, and Wierman]{NIPS2019_8463}
Gautam Goel, Yiheng Lin, Haoyuan Sun, and Adam Wierman.
\newblock Beyond online balanced descent: An optimal algorithm for smoothed
  online optimization.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pages
  1875--1885, 2019.

\bibitem[Gradu et~al.(2020)Gradu, Hazan, and Minasyan]{arXiv:2007.04393}
Paula Gradu, Elad Hazan, and Edgar Minasyan.
\newblock Adaptive regret for control of time-varying dynamics.
\newblock \emph{ArXiv e-prints}, arXiv:2007.04393, 2020.

\bibitem[Gy\"{o}rgy et~al.(2012)Gy\"{o}rgy, Linder, and
  Lugosi]{Track_Large_Expert}
Andr\'{a}s Gy\"{o}rgy, Tam\'{a}s Linder, and G\'{a}bor Lugosi.
\newblock Efficient tracking of large classes of experts.
\newblock \emph{IEEE Transactions on Information Theory}, 58\penalty0
  (11):\penalty0 6709--6725, 2012.

\bibitem[Hall and Willett(2013)]{Dynamic:ICML:13}
Eric~C. Hall and Rebecca~M. Willett.
\newblock Dynamical models and tracking regret in online convex programming.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning}, pages 579--587, 2013.

\bibitem[Hazan(2016)]{Intro:Online:Convex}
Elad Hazan.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends in Optimization}, 2\penalty0
  (3-4):\penalty0 157--325, 2016.

\bibitem[Hazan and Seshadhri(2007)]{Adaptive:Hazan}
Elad Hazan and C.~Seshadhri.
\newblock Adaptive algorithms for online decision problems.
\newblock \emph{Electronic Colloquium on Computational Complexity}, 88, 2007.

\bibitem[Hazan and Seshadhri(2009)]{Hazan:2009:ELA}
Elad Hazan and C.~Seshadhri.
\newblock Efficient learning algorithms for changing environments.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning}, pages 393--400, 2009.

\bibitem[Hazan et~al.(2007)Hazan, Agarwal, and Kale]{ML:Hazan:2007}
Elad Hazan, Amit Agarwal, and Satyen Kale.
\newblock Logarithmic regret algorithms for online convex optimization.
\newblock \emph{Machine Learning}, 69\penalty0 (2-3):\penalty0 169--192, 2007.

\bibitem[Herbster and Warmuth(1998)]{Herbster1998}
Mark Herbster and Manfred~K. Warmuth.
\newblock Tracking the best expert.
\newblock \emph{Machine Learning}, 32\penalty0 (2):\penalty0 151--178, 1998.

\bibitem[Jadbabaie et~al.(2015)Jadbabaie, Rakhlin, Shahrampour, and
  Sridharan]{Dynamic:AISTATS:15}
Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan.
\newblock Online optimization: Competing with dynamic comparators.
\newblock In \emph{Proceedings of the 18th International Conference on
  Artificial Intelligence and Statistics}, pages 398--406, 2015.

\bibitem[Jun et~al.(2017{\natexlab{a}})Jun, Orabona, Wright, and
  Willett]{Improved:Strongly:Adaptive}
Kwang-Sung Jun, Francesco Orabona, Stephen Wright, and Rebecca Willett.
\newblock Improved strongly adaptive online learning using coin betting.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics}, pages 943--951, 2017{\natexlab{a}}.

\bibitem[Jun et~al.(2017{\natexlab{b}})Jun, Orabona, Wright, and
  Willett]{jun2017}
Kwang-Sung Jun, Francesco Orabona, Stephen Wright, and Rebecca Willett.
\newblock Online learning for changing environments using coin betting.
\newblock \emph{Electronic Journal of Statistics}, 11\penalty0 (2):\penalty0
  5282--5310, 2017{\natexlab{b}}.

\bibitem[Kapralov and Panigrahy(2010)]{DNP:2010:Arxiv}
Michael Kapralov and Rina Panigrahy.
\newblock Prediction strategies without loss.
\newblock \emph{ArXiv e-prints}, arXiv:1008.3672, 2010.

\bibitem[Kapralov and Panigrahy(2011)]{NIPS2011_11b921ef}
Michael Kapralov and Rina Panigrahy.
\newblock Prediction strategies without loss.
\newblock In \emph{Advances in Neural Information Processing Systems 24}, pages
  828--836, 2011.

\bibitem[Kim and Giannakis(2014)]{electricity:pricing}
Seung-Jun Kim and Geogios~B. Giannakis.
\newblock Real-time electricity pricing for demand response using online convex
  optimization.
\newblock In \emph{Proceedings of the 2012 IEEE PES Conference on Innovative
  Smart Grid Technologies}, pages 1--5, 2014.

\bibitem[Kim et~al.(2015)Kim, Yue, Taylor, and
  Matthews]{Spatiotemporal:Prediction}
Taehwan Kim, Yisong Yue, Sarah Taylor, and Iain Matthews.
\newblock A decision tree framework for spatiotemporal sequence prediction.
\newblock In \emph{Proceedings of the 21th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pages 577--586, 2015.

\bibitem[Li and Li(2020)]{NEURIPS2020_a6e4f250}
Yingying Li and Na~Li.
\newblock Leveraging predictions in smoothed online convex optimization via
  gradient-based algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems 33}, pages
  14520--14531, 2020.

\bibitem[Li et~al.(2018)Li, Qu, and Li]{Prediction:Switching}
Yingying Li, Guannan Qu, and Na~Li.
\newblock Using predictions in online optimization with switching costs: A fast
  algorithm and a fundamental limit.
\newblock In \emph{the 2018 Annual American Control Conference}, pages
  3008--3013, 2018.

\bibitem[Lin et~al.(2011)Lin, Wierman, Andrew, and
  Thereska]{Dynamic:Data:Center}
Minghong Lin, Adam Wierman, Lachlan~L.H. Andrew, and Eno Thereska.
\newblock Dynamic right-sizing for power-proportional data centers.
\newblock In \emph{Proceedings of the 30th IEEE International Conference on
  Computer Communications}, pages 1098--1106, 2011.

\bibitem[Lin et~al.(2012)Lin, Liu, Wierman, and
  Andrew]{Geographical:Load:Balance}
Minghong Lin, Zhenhua Liu, Adam Wierman, and Lachlan~L.H. Andrew.
\newblock Online algorithms for geographical load balancing.
\newblock In \emph{Proceedings of the 2012 International Green Computing
  Conference}, pages 1--10, 2012.

\bibitem[Lin et~al.(2020)Lin, Goel, and Wierman]{10.1145/3379484}
Yiheng Lin, Gautam Goel, and Adam Wierman.
\newblock Online optimization with predictions and non-convex losses.
\newblock \emph{Proceedings of the ACM on Measurement and Analysis of Computing
  Systems}, 4\penalty0 (1):\penalty0 18:1--18:32, 2020.

\bibitem[Littlestone and Warmuth(1994)]{LITTLESTONE1994212}
Nick Littlestone and Manfred~K. Warmuth.
\newblock The weighted majority algorithm.
\newblock \emph{Information and Computation}, 108\penalty0 (2):\penalty0
  212--261, 1994.

\bibitem[Luo and Schapire(2015)]{pmlr-v40-Luo15}
Haipeng Luo and Robert~E. Schapire.
\newblock Achieving all with no parameters: Adanormalhedge.
\newblock In \emph{Proceedings of the 28th Conference on Learning Theory},
  pages 1286--1304, 2015.

\bibitem[Mokhtari et~al.(2016)Mokhtari, Shahrampour, Jadbabaie, and
  Ribeiro]{Dynamic:Strongly}
Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro.
\newblock Online optimization in dynamic environments: Improved regret rates
  for strongly convex problems.
\newblock In \emph{Proceedings of the 55th IEEE Conference on Decision and
  Control}, pages 7195--7201, 2016.

\bibitem[Orabona and P\'{a}l(2016)]{NIPS2016_32072254}
Francesco Orabona and D\'{a}vid P\'{a}l.
\newblock Coin betting and parameter-free online learning.
\newblock In \emph{Advances in Neural Information Processing Systems 29}, pages
  577--585, 2016.

\bibitem[Orabona et~al.(2012)Orabona, Cesa-Bianchi, and
  Gentile]{Beyond:Logarithmic}
Francesco Orabona, Nicolo Cesa-Bianchi, and Claudio Gentile.
\newblock Beyond logarithmic bounds in online learning.
\newblock In \emph{Proceedings of the 15th International Conference on
  Artificial Intelligence and Statistics}, pages 823--831, 2012.

\bibitem[Sani et~al.(2014)Sani, Neu, and Lazaric]{NIPS2014_Easy}
Amir Sani, Gergely Neu, and Alessandro Lazaric.
\newblock Exploiting easy data in online optimization.
\newblock In \emph{Advances in Neural Information Processing Systems 27}, pages
  810--818, 2014.

\bibitem[Sellke(2020)]{Optimal:CCB}
Mark Sellke.
\newblock Chasing convex bodies optimally.
\newblock In \emph{Proceedings of the 31st Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 1509--1518, 2020.

\bibitem[Shalev-Shwartz(2011)]{Online:suvery}
Shai Shalev-Shwartz.
\newblock Online learning and online convex optimization.
\newblock \emph{Foundations and Trends in Machine Learning}, 4\penalty0
  (2):\penalty0 107--194, 2011.

\bibitem[Shalev-Shwartz et~al.(2007)Shalev-Shwartz, Singer, and
  Srebro]{ICML_Pegasos}
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.
\newblock Pegasos: primal estimated sub-gradient solver for {SVM}.
\newblock In \emph{Proceedings of the 24th International Conference on Machine
  Learning}, pages 807--814, 2007.

\bibitem[Srebro et~al.(2010)Srebro, Sridharan, and Tewari]{NIPS2010_Smooth}
Nathan Srebro, Karthik Sridharan, and Ambuj Tewari.
\newblock Smoothness, low-noise and fast rates.
\newblock In \emph{Advances in Neural Information Processing Systems 23}, pages
  2199--2207, 2010.

\bibitem[Yang et~al.(2016)Yang, Zhang, Jin, and Yi]{Dynamic:2016}
Tianbao Yang, Lijun Zhang, Rong Jin, and Jinfeng Yi.
\newblock Tracking slowly moving clairvoyant: Optimal dynamic regret of online
  learning with true and noisy gradient.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning}, pages 449--457, 2016.

\bibitem[Zhang(2020)]{IJCAI:2020:Zhang}
Lijun Zhang.
\newblock Online learning in changing environments.
\newblock In \emph{Proceedings of the 29th International Joint Conference on
  Artificial Intelligence}, pages 5178--5182, 2020.
\newblock Early Career.

\bibitem[Zhang et~al.(2017)Zhang, Yang, Yi, Jin, and
  Zhou]{Dynamic:Regret:Squared}
Lijun Zhang, Tianbao Yang, Jinfeng Yi, Rong Jin, and Zhi-Hua Zhou.
\newblock Improved dynamic regret for non-degenerate functions.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pages
  732--741, 2017.

\bibitem[Zhang et~al.(2018{\natexlab{a}})Zhang, Lu, and
  Zhou]{Adaptive:Dynamic:Regret:NIPS}
Lijun Zhang, Shiyin Lu, and Zhi-Hua Zhou.
\newblock Adaptive online learning in dynamic environments.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pages
  1323--1333, 2018{\natexlab{a}}.

\bibitem[Zhang et~al.(2018{\natexlab{b}})Zhang, Yang, Jin, and
  Zhou]{Dynamic:Regret:Adaptive}
Lijun Zhang, Tianbao Yang, Rong Jin, and Zhi-Hua Zhou.
\newblock Dynamic regret of strongly adaptive methods.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, pages 5882--5891, 2018{\natexlab{b}}.

\bibitem[Zhang et~al.(2019)Zhang, Liu, and Zhou]{Adaptive:Regret:Smooth:ICML}
Lijun Zhang, Tie-Yan Liu, and Zhi-Hua Zhou.
\newblock Adaptive regret of convex and smooth functions.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, pages 7414--7423, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Lu, and Yang]{Adaptive:Dynamic:AISTATS}
Lijun Zhang, Shiyin Lu, and Tianbao Yang.
\newblock Minimizing dynamic regret and adaptive regret simultaneously.
\newblock In \emph{Proceedings of the 23rd International Conference on
  Artificial Intelligence and Statistics}, pages 309--319, 2020.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Jiang, Lu, and
  Yang]{Smoothed:Online:NeurIPS}
Lijun Zhang, Wei Jiang, Shiyin Lu, and Tianbao Yang.
\newblock Revisiting smoothed online learning.
\newblock In \emph{Advances in Neural Information Processing Systems 34}, pages
  13599--13612, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Wang, Tu, Jiang, and
  Zhou]{Dual:Adaptivity}
Lijun Zhang, Guanghui Wang, Wei-Wei Tu, Wei Jiang, and Zhi-Hua Zhou.
\newblock Dual adaptivity: A universal algorithm for minimizing the adaptive
  regret of convex functions.
\newblock In \emph{Advances in Neural Information Processing Systems 34}, pages
  24968--24980, 2021{\natexlab{b}}.

\bibitem[Zhang et~al.(2021{\natexlab{c}})Zhang, Cutkosky, and
  Paschalidis]{arXiv:2102.01623}
Zhiyu Zhang, Ashok Cutkosky, and Ioannis~Ch. Paschalidis.
\newblock Adversarial tracking control via strongly adaptive online learning
  with memory.
\newblock \emph{ArXiv e-prints}, arXiv:2102.01623, 2021{\natexlab{c}}.

\bibitem[Zhao and Zhang(2021)]{Worst:Dynamic:SSS}
Peng Zhao and Lijun Zhang.
\newblock Improved analysis for dynamic regret of strongly convex and smooth
  functions.
\newblock In \emph{Proceedings of the 3rd Annual Learning for Dynamics and
  Control Conference}, pages 48--59, 2021.

\bibitem[Zhao et~al.(2020{\natexlab{a}})Zhao, Zhang, Zhang, and
  Zhou]{Problem:Dynamic:Regret}
Peng Zhao, Yu-Jie Zhang, Lijun Zhang, and Zhi-Hua Zhou.
\newblock Dynamic regret of convex and smooth functions.
\newblock In \emph{Advances in Neural Information Processing Systems 33}, pages
  12510--12520, 2020{\natexlab{a}}.

\bibitem[Zhao et~al.(2020{\natexlab{b}})Zhao, Zhao, Zhang, Zhu, Liu, and
  Yin]{10.1145/3375788}
Yawei Zhao, Qian Zhao, Xingxing Zhang, En~Zhu, Xinwang Liu, and Jianping Yin.
\newblock Understand dynamic regret with switching cost for online decision
  making.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology},
  11\penalty0 (3), 2020{\natexlab{b}}.

\bibitem[Zinkevich(2003)]{zinkevich-2003-online}
Martin Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{Proceedings of the 20th International Conference on Machine
  Learning}, pages 928--936, 2003.

\end{thebibliography}
