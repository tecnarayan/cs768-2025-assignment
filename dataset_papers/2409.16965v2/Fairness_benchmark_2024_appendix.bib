@article{Aequitas2018,
	title={Aequitas: A Bias and Fairness Audit Toolkit},
	author={Saleiro, Pedro and Kuester, Benedict and Stevens, Abby and Anisfeld, Ari and Hinkson, Loren and London, Jesse and Ghani, Rayid}, journal={arXiv preprint arXiv:1811.05577}, year={2018}
}

@inproceedings{agarwal2018reductions,
  title={A reductions approach to fair classification},
  author={Agarwal, Alekh and Beygelzimer, Alina and Dud{\'\i}k, Miroslav and Langford, John and Wallach, Hanna},
  booktitle={International conference on machine learning},
  pages={60--69},
  year={2018},
  organization={PMLR}
}

     
 @misc{aif360_2018,
    title = "{AI Fairness} 360:  An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias",
    author = {Rachel K. E. Bellamy and Kuntal Dey and Michael Hind and
	Samuel C. Hoffman and Stephanie Houde and Kalapriya Kannan and
	Pranay Lohia and Jacquelyn Martino and Sameep Mehta and
	Aleksandra Mojsilovic and Seema Nagar and Karthikeyan Natesan Ramamurthy and
	John Richards and Diptikalyan Saha and Prasanna Sattigeri and
	Moninder Singh and Kush R. Varshney and Yunfeng Zhang},
    month = oct,
    year = {2018},
    url = {https://arxiv.org/abs/1810.01943}
}

@book{barocas-hardt-narayanan,
  title = {Fairness and Machine Learning: Limitations and Opportunities},
  author = {Solon Barocas and Moritz Hardt and Arvind Narayanan},
  publisher = {MIT Press},
  year = {2023}
}

@inproceedings{baumann2023bias,
  title={Bias on Demand: A Modelling Framework That Generates Synthetic Data With Bias},
  author={Baumann, Joachim and Castelnovo, Alessandro and Crupi, Riccardo and Inverardi, Nicole and Regoli, Daniele},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  doi={https://doi.org/10.1145/3593013.3594058},
  year={2023}
}

@inproceedings{Biswas_Rajan_2020, address={Virtual Event USA}, title={Do the machine learning models on a crowd sourced platform exhibit bias? an empirical study on model fairness}, ISBN={978-1-4503-7043-1}, url={https://dl.acm.org/doi/10.1145/3368089.3409704}, DOI={10.1145/3368089.3409704}, booktitle={Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering}, publisher={ACM}, author={Biswas, Sumon and Rajan, Hridesh}, year={2020}, month=nov, pages={642–653}, language={en} }
 
@inproceedings{Buyl_fairret_a_Framework_2024,
author = {Buyl, Maarten and Defrance, MaryBeth and De Bie, Tijl},
booktitle = {International Conference on Learning Representations},
title = {{fairret: a Framework for Differentiable Fairness Regularization Terms}},
year = {2024}
} 

@article{buyl2024inherent,
  title={Inherent limitations of ai fairness},
  author={Buyl, Maarten and De Bie, Tijl},
  journal={Communications of the ACM},
  volume={67},
  number={2},
  pages={48--55},
  year={2024},
  publisher={ACM New York, NY, USA}
}

 
@article{calmon2017optimized,
  title={Optimized pre-processing for discrimination prevention},
  author={Calmon, Flavio and Wei, Dennis and Vinzamuri, Bhanukiran and Natesan Ramamurthy, Karthikeyan and Varshney, Kush R},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{canetti2019soft,
  title={From soft classifiers to hard decisions: How fair can we be?},
  author={Canetti, Ran and Cohen, Aloni and Dikkala, Nishanth and Ramnarayan, Govind and Scheffler, Sarah and Smith, Adam},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={309--318},
  year={2019}
}


 
 @inproceedings{Cardoso_Almeida_Zaki_2019, address={Honolulu HI USA}, title={A Framework for Benchmarking Discrimination-Aware Models in Machine Learning}, ISBN={978-1-4503-6324-2}, url={https://dl.acm.org/doi/10.1145/3306618.3314262}, DOI={10.1145/3306618.3314262}, booktitle={Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society}, publisher={ACM}, author={L. Cardoso, Rodrigo and Meira Jr., Wagner and Almeida, Virgilio and J. Zaki, Mohammed}, year={2019}, month=jan, pages={437–444}, language={en} }
 
 
 @article{Caton_Haas_2024, 
 	title={Fairness in Machine Learning: A Survey}, 
	volume={56}, ISSN={0360-0300, 1557-7341}, 
	DOI={10.1145/3616865}, number={7}, 
	journal={ACM Computing Surveys}, 
	author={Caton, Simon and Haas, Christian}, 
	year={2024}, month=jul, pages={1–38}, language={en} 
	}

@article{Cortez2008,
author = {Cortez, Paulo and Silva, Alice},
year = {2008},
month = {01},
pages = {},
title = {Using data mining to predict secondary school student performance},
journal = {EUROSIS}
} 
 
 
@inproceedings{cruz2024unprocessing,
title={Unprocessing Seven Years of Algorithmic Fairness},
author={Andr{\'e} Cruz and Moritz Hardt},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=jr03SfWsBS}
}

@inproceedings{defrance2023maximal,
  title={Maximal fairness},
  author={Defrance, MaryBeth and De Bie, Tijl},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={851--880},
  year={2023}
}

@article{ding2021retiring,
  title={Retiring Adult: New Datasets for Fair Machine Learning},
  author={Ding, Frances and Hardt, Moritz and Miller, John and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@INPROCEEDINGS{Feng2015,
  author={Feng, Wei and Boukir, Samia},
  booktitle={2015 IEEE International Conference on Image Processing (ICIP)}, 
  title={Class noise removal and correction for image classification using ensemble margin}, 
  year={2015},
  volume={},
  number={},
  pages={4698-4702},
  keywords={Training;Training data;Noise measurement;Robustness;Vehicles;Bagging;Face;Class noise removal;class noise correction;ensemble margin;multiple classifier;mislabeled data identification},
  doi={10.1109/ICIP.2015.7351698}}


@article{hardt2016equality,
  title={Equality of opportunity in supervised learning},
  author={Hardt, Moritz and Price, Eric and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{hebert2018multicalibration,
  title={Multicalibration: Calibration for the (computationally-identifiable) masses},
  author={H{\'e}bert-Johnson, Ursula and Kim, Michael and Reingold, Omer and Rothblum, Guy},
  booktitle={International Conference on Machine Learning},
  pages={1939--1948},
  year={2018},
  organization={PMLR}
}



@inproceedings{holstein2019improving,
  title={Improving fairness in machine learning systems: What do industry practitioners need?},
  author={Holstein, Kenneth and Wortman Vaughan, Jennifer and Daum{\'e} III, Hal and Dudik, Miro and Wallach, Hanna},
  booktitle={Proceedings of the 2019 CHI conference on human factors in computing systems},
  pages={1--16},
  year={2019}
}


 @inproceedings{Islam_Pan_Foulds_2021, address={Virtual Event USA}, title={Can We Obtain Fairness For Free?}, ISBN={978-1-4503-8473-5}, url={https://dl.acm.org/doi/10.1145/3461702.3462614}, DOI={10.1145/3461702.3462614}, booktitle={Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society}, publisher={ACM}, author={Islam, Rashidul and Pan, Shimei and Foulds, James R.}, year={2021}, month=jul, pages={586–596}, language={en} }

@inproceedings{jiang2020wasserstein,
  title={Wasserstein fair classification},
  author={Jiang, Ray and Pacchiano, Aldo and Stepleton, Tom and Jiang, Heinrich and Chiappa, Silvia},
  booktitle={Uncertainty in artificial intelligence},
  pages={862--872},
  year={2020},
  organization={PMLR}
}


 @inproceedings{Friedler_Scheidegger_Venkatasubramanian_Choudhary_Hamilton_Roth_2019, address={New York, NY, USA}, series={FAT* ’19}, title={A comparative study of fairness-enhancing interventions in machine learning}, ISBN={978-1-4503-6125-5}, url={https://dl.acm.org/doi/10.1145/3287560.3287589}, DOI={10.1145/3287560.3287589}, booktitle={Proceedings of the Conference on Fairness, Accountability, and Transparency}, publisher={Association for Computing Machinery}, author={Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh and Choudhary, Sonam and Hamilton, Evan P. and Roth, Derek}, year={2019}, month=jan, pages={329–338}, collection={FAT* ’19} }


 
 @inproceedings{Han_Chi_Chen_Wang_Zhao_Zou_Hu_2023, title={FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods}, url={https://openreview.net/forum?id=TzAJbTClAz&referrer=%5Bthe%20profile%20of%20Xia%20Hu%5D(%2Fprofile%3Fid%3D~Xia_Hu4)},  author={Han, Xiaotian and Chi, Jianfeng and Chen, Yu and Wang, Qifan and Zhao, Han and Zou, Na and Hu, Xia}, year={2023}, month=oct, language={en} }

 @inproceedings{Hort_Zhang_Sarro_Harman_2021, address={Athens Greece}, title={Fairea: a model behaviour mutation approach to benchmarking bias mitigation methods}, ISBN={978-1-4503-8562-6}, url={https://dl.acm.org/doi/10.1145/3468264.3468565}, DOI={10.1145/3468264.3468565}, booktitle={Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering}, publisher={ACM}, author={Hort, Max and Zhang, Jie M. and Sarro, Federica and Harman, Mark}, year={2021}, month=aug, pages={994–1006}, language={en} }

@article{kamiran2012data,
  title={Data preprocessing techniques for classification without discrimination},
  author={Kamiran, Faisal and Calders, Toon},
  journal={Knowledge and information systems},
  volume={33},
  number={1},
  pages={1--33},
  year={2012},
  publisher={Springer}
}


@inproceedings{kamishima2012fairness,
  title={Fairness-aware classifier with prejudice remover regularizer},
  author={Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
  booktitle={Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2012, Bristol, UK, September 24-28, 2012. Proceedings, Part II 23},
  pages={35--50},
  year={2012},
  organization={Springer}
}

@inproceedings{kearns2019empirical,
  title={An empirical study of rich subgroup fairness for machine learning},
  author={Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={100--109},
  year={2019}
}

@article{kleinberg2016inherent,
  title={Inherent trade-offs in the fair determination of risk scores},
  author={Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
  journal={arXiv preprint arXiv:1609.05807},
  year={2016}
}



@inproceedings{krasanakis2018adaptive,
  title={Adaptive sensitive reweighting to mitigate bias in fairness-aware classification},
  author={Krasanakis, Emmanouil and Spyromitros-Xioufis, Eleftherios and Papadopoulos, Symeon and Kompatsiaris, Yiannis},
  booktitle={Proceedings of the 2018 world wide web conference},
  pages={853--862},
  year={2018}
}

@inproceedings{lenders_real_life_2023,
	address = {Tallinn Estonia},
	title = {Real-life {Performance} of {Fairness} {Interventions} - {Introducing} {A} {New} {Benchmarking} {Dataset} for {Fair} {ML}},
	isbn = {978-1-4503-9517-5},
	url = {https://dl.acm.org/doi/10.1145/3555776.3577634},
	doi = {10.1145/3555776.3577634},
	language = {en},
	urldate = {2024-04-10},
	booktitle = {Proceedings of the 38th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {ACM},
	author = {Lenders, Daphne and Calders, Toon},
	month = mar,
	year = {2023},
	pages = {350--357},
	file = {Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\6KGFE4VX\\Lenders and Calders - 2023 - Real-life Performance of Fairness Interventions - .pdf:application/pdf},
}

@inproceedings{lohaus2020too,
  title={Too relaxed to be fair},
  author={Lohaus, Michael and Perrot, Michael and Von Luxburg, Ulrike},
  booktitle={International Conference on Machine Learning},
  pages={6360--6369},
  year={2020},
  organization={PMLR}
}


@inproceedings{madras2018learning,
  title={Learning adversarially fair and transferable representations},
  author={Madras, David and Creager, Elliot and Pitassi, Toniann and Zemel, Richard},
  booktitle={International Conference on Machine Learning},
  pages={3384--3393},
  year={2018},
  organization={PMLR}
}

@article{makhlouf2020survey,
  title={Survey on causal-based machine learning fairness notions},
  author={Makhlouf, Karima and Zhioua, Sami and Palamidessi, Catuscia},
  journal={arXiv preprint arXiv:2010.09553},
  year={2020}
}


@inproceedings{menon2018cost,
  title={The cost of fairness in binary classification},
  author={Menon, Aditya Krishna and Williamson, Robert C},
  booktitle={Conference on Fairness, accountability and transparency},
  pages={107--118},
  year={2018},
  organization={PMLR}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{pedregosa2011scikit,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={the Journal of machine Learning research},
  volume={12},
  pages={2825--2830},
  year={2011},
  publisher={JMLR. org}
}



@article{starke2022fairness,
  title={Fairness perceptions of algorithmic decision-making: A systematic review of the empirical literature},
  author={Starke, Christopher and Baleis, Janine and Keller, Birte and Marcinkowski, Frank},
  journal={Big Data \& Society},
  volume={9},
  number={2},
  pages={20539517221115189},
  year={2022},
  publisher={SAGE Publications Sage UK: London, England}
}


@inproceedings{verma2018fairness,
  title={Fairness definitions explained},
  author={Verma, Sahil and Rubin, Julia},
  booktitle={Proceedings of the international workshop on software fairness},
  pages={1--7},
  year={2018}
}


@article{vzliobaite2016using,
  title={Using sensitive personal data may be necessary for avoiding discrimination in data-driven decision models},
  author={{\v{Z}}liobait{\.e}, Indr{\.e} and Custers, Bart},
  journal={Artificial Intelligence and Law},
  volume={24},
  pages={183--201},
  year={2016},
  publisher={Springer}
}

@inproceedings{weerts2023algorithmic,
  title={Algorithmic unfairness through the lens of EU non-discrimination law: Or why the law is not a decision tree},
  author={Weerts, Hilde and Xenidis, Rapha{\"e}le and Tarissan, Fabien and Olsen, Henrik Palmer and Pechenizkiy, Mykola},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={805--816},
  year={2023}
}

@article{wick2019unlocking,
  title={Unlocking fairness: a trade-off revisited},
  author={Wick, Michael and Tristan, Jean-Baptiste and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@inproceedings{woodworth2017learning,
  title={Learning non-discriminatory predictors},
  author={Woodworth, Blake and Gunasekar, Suriya and Ohannessian, Mesrob I and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={1920--1953},
  year={2017},
  organization={PMLR}
}



@article{zafar2019fairness,
  title={Fairness constraints: A flexible approach for fair classification},
  author={Zafar, Muhammad Bilal and Valera, Isabel and Gomez-Rodriguez, Manuel and Gummadi, Krishna P},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={75},
  pages={1--42},
  year={2019}
}



@inproceedings{zemel2013learning,
  title={Learning fair representations},
  author={Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
  booktitle={International conference on machine learning},
  pages={325--333},
  year={2013},
  organization={PMLR}
}

@misc{laszkiewicz_benchmarking_2024,
	title = {Benchmarking the {Fairness} of {Image} {Upsampling} {Methods}},
	url = {http://arxiv.org/abs/2401.13555},
	abstract = {Recent years have witnessed a rapid development of deep generative models for creating synthetic media, such as images and videos. While the practical applications of these models in everyday tasks are enticing, it is crucial to assess the inherent risks regarding their fairness. In this work, we introduce a comprehensive framework for benchmarking the performance and fairness of conditional generative models. We develop a set of metrics\${\textbackslash}unicode\{x2013\}\$inspired by their supervised fairness counterparts\${\textbackslash}unicode\{x2013\}\$to evaluate the models on their fairness and diversity. Focusing on the specific application of image upsampling, we create a benchmark covering a wide variety of modern upsampling methods. As part of the benchmark, we introduce UnfairFace, a subset of FairFace that replicates the racial distribution of common large-scale face datasets. Our empirical study highlights the importance of using an unbiased training set and reveals variations in how the algorithms respond to dataset imbalances. Alarmingly, we find that none of the considered methods produces statistically fair and diverse results.},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Laszkiewicz, Mike and Daunhawer, Imant and Vogt, Julia E. and Fischer, Asja and Lederer, Johannes},
	month = jan,
	year = {2024},
	note = {arXiv:2401.13555 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\X6WALX2S\\2401.html:text/html;Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\A9DJ6A5A\\Laszkiewicz et al. - 2024 - Benchmarking the Fairness of Image Upsampling Meth.pdf:application/pdf},
}

 @article{Mehrabi_Morstatter_Saxena_Lerman_Galstyan_2022, title={A Survey on Bias and Fairness in Machine Learning}, volume={54}, ISSN={0360-0300, 1557-7341}, DOI={10.1145/3457607}, number={6}, journal={ACM Computing Surveys}, author={Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram}, year={2022}, month=jul, pages={1–35}, language={en} }

@article{Weerts_Fairlearn_Assessing_and_2023,
author = {Weerts, Hilde and Dudík, Miroslav and Edgar, Richard and Jalali, Adrin and Lutz, Roman and Madaio, Michael},
journal = {Journal of Machine Learning Research},
title = {{Fairlearn: Assessing and Improving Fairness of AI Systems}},
url = {http://jmlr.org/papers/v24/23-0389.html},
volume = {24},
year = {2023}
}

@inproceedings{han_fairlib_2022,
	address = {Abu Dhabi, UAE},
	title = {{FairLib}: {A} {Unified} {Framework} for {Assessing} and {Improving} {Fairness}},
	shorttitle = {{FairLib}},
	url = {https://aclanthology.org/2022.emnlp-demos.7},
	doi = {10.18653/v1/2022.emnlp-demos.7},
	abstract = {This paper presents FairLib, an open-source python library for assessing and improving model fairness. It provides a systematic framework for quickly accessing benchmark datasets, reproducing existing debiasing baseline models, developing new methods, evaluating models with different metrics, and visualizing their results. Its modularity and extensibility enable the framework to be used for diverse types of inputs, including natural language, images, and audio. We implement 14 debiasing methods, including pre-processing,at-training-time, and post-processing approaches. The built-in metrics cover the most commonly acknowledged fairness criteria and can be further generalized and customized for fairness evaluation.},
	urldate = {2024-04-05},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Han, Xudong and Shen, Aili and Li, Yitong and Frermann, Lea and Baldwin, Timothy and Cohn, Trevor},
	editor = {Che, Wanxiang and Shutova, Ekaterina},
	month = dec,
	year = {2022},
	pages = {60--71},
	annote = {They have a small benchmark in the paper, but more to defend their implementation rather than comparing methods. 

},
	file = {Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\4IIXCRM3\\Han et al. - 2022 - FairLib A Unified Framework for Assessing and Imp.pdf:application/pdf},
}

@inproceedings{gustafson_facet_2023,
	address = {Paris, France},
	title = {{FACET}: {Fairness} in {Computer} {Vision} {Evaluation} {Benchmark}},
	isbn = {9798350307184},
	shorttitle = {{FACET}},
	url = {https://ieeexplore.ieee.org/document/10377223/},
	doi = {10.1109/ICCV51070.2023.01863},
	language = {en},
	urldate = {2024-04-05},
	booktitle = {2023 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Gustafson, Laura and Rolland, Chloe and Ravi, Nikhila and Duval, Quentin and Adcock, Aaron and Fu, Cheng-Yang and Hall, Melissa and Ross, Candace},
	month = oct,
	year = {2023},
	pages = {20313--20325},
	file = {Gustafson et al. - 2023 - FACET Fairness in Computer Vision Evaluation Benc.pdf:C\:\\Users\\Administrator\\Zotero\\storage\\UMAQBM57\\Gustafson et al. - 2023 - FACET Fairness in Computer Vision Evaluation Benc.pdf:application/pdf},
}

@misc{chalkidis_fairlex_2022,
	title = {{FairLex}: {A} {Multilingual} {Benchmark} for {Evaluating} {Fairness} in {Legal} {Text} {Processing}},
	shorttitle = {{FairLex}},
	url = {http://arxiv.org/abs/2203.07228},
	abstract = {We present a benchmark suite of four datasets for evaluating the fairness of pre-trained language models and the techniques used to fine-tune them for downstream tasks. Our benchmarks cover four jurisdictions (European Council, USA, Switzerland, and China), five languages (English, German, French, Italian and Chinese) and fairness across five attributes (gender, age, region, language, and legal area). In our experiments, we evaluate pre-trained language models using several group-robust fine-tuning techniques and show that performance group disparities are vibrant in many cases, while none of these techniques guarantee fairness, nor consistently mitigate group disparities. Furthermore, we provide a quantitative and qualitative analysis of our results, highlighting open challenges in the development of robustness methods in legal NLP.},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Chalkidis, Ilias and Pasini, Tommaso and Zhang, Sheng and Tomada, Letizia and Schwemer, Sebastian Felix and Søgaard, Anders},
	month = mar,
	year = {2022},
	note = {arXiv:2203.07228 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 9 pages, long paper at ACL 2022 proceedings},
	file = {arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\FQU9ZAL6\\2203.html:text/html;Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\NBW3L6VN\\Chalkidis et al. - 2022 - FairLex A Multilingual Benchmark for Evaluating F.pdf:application/pdf},
}

@article{reddy_benchmarking_2022,
	title = {Benchmarking bias mitigation algorithms in representation learning through fairness metrics},
	url = {https://papyrus.bib.umontreal.ca/xmlui/handle/1866/27490},
	abstract = {Le succès des modèles d’apprentissage en profondeur et leur adoption rapide dans de nombreux
domaines d’application ont soulevé d’importantes questions sur l’équité de ces modèles lorsqu’ils
sont déployés dans le monde réel. Des études récentes ont mis en évidence les biais encodés
par les algorithmes d’apprentissage des représentations et ont remis en cause la fiabilité de telles
approches pour prendre des décisions. En conséquence, il existe un intérêt croissant pour la
compréhension des sources de biais dans l’apprentissage des algorithmes et le développement de
stratégies d’atténuation des biais. L’objectif des algorithmes d’atténuation des biais est d’atténuer
l’influence des caractéristiques des données sensibles sur les décisions d’éligibilité prises. Les
caractéristiques sensibles sont des caractéristiques privées et protégées d’un ensemble de données
telles que le sexe ou la race, qui ne devraient pas affecter les décisions de sortie d’éligibilité, c’està-dire les critères qui rendent un individu qualifié ou non qualifié pour une tâche donnée, comme
l’octroi de prêts ou l’embauche. Les modèles d’atténuation des biais visent à prendre des décisions
d’éligibilité sur des échantillons d’ensembles de données sans biais envers les attributs sensibles
des données d’entrée. La difficulté des tâches d’atténuation des biais est souvent déterminée par
la distribution de l’ensemble de données, qui à son tour est fonction du déséquilibre potentiel de
l’étiquette et des caractéristiques, de la corrélation des caractéristiques potentiellement sensibles
avec d’autres caractéristiques des données, du décalage de la distribution de l’apprentissage vers
le phase de développement, etc. Sans l’évaluation des modèles d’atténuation des biais dans
diverses configurations difficiles, leurs mérites restent incertains. Par conséquent, une analyse
systématique qui comparerait différentes approches d’atténuation des biais sous la perspective de
différentes mesures d’équité pour assurer la réplication des résultats conclus est nécessaire. À
cette fin, nous proposons un cadre unifié pour comparer les approches d’atténuation des biais.
Nous évaluons différentes méthodes d’équité formées avec des réseaux de neurones profonds sur
un ensemble de données synthétiques commun et un ensemble de données du monde réel pour
obtenir de meilleures informations sur le fonctionnement de ces méthodes. En particulier, nous
formons environ 3000 modèles différents dans diverses configurations, y compris des configurations
de données déséquilibrées et corrélées, pour vérifier les limites des modèles actuels et mieux
comprendre dans quelles configurations ils sont sujets à des défaillances. Nos résultats montrent que
le biais des modèles augmente à mesure que les ensembles de données deviennent plus déséquilibrés 
ou que les attributs des ensembles de données deviennent plus corrélés, le niveau de dominance
des caractéristiques des ensembles de données sensibles corrélées a un impact sur le biais, et
les informations sensibles restent dans la représentation latente même lorsque des algorithmes
d’atténuation des biais sont appliqués. Résumant nos contributions - nous présentons un ensemble
de données, proposons diverses configurations d’évaluation difficiles et évaluons rigoureusement
les récents algorithmes prometteurs d’atténuation des biais dans un cadre commun et publions
publiquement cette référence, en espérant que la communauté des chercheurs le considérerait
comme un point d’entrée commun pour un apprentissage en profondeur équitable.},
	language = {eng},
	urldate = {2024-04-09},
	author = {Reddy, Charan},
	month = oct,
	year = {2022},
	note = {Accepted: 2023-02-09T21:00:44Z},
	file = {Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\Q833K2F3\\Reddy - 2022 - Benchmarking bias mitigation algorithms in represe.pdf:application/pdf},
}

@misc{manerba_social_2024,
	title = {Social {Bias} {Probing}: {Fairness} {Benchmarking} for {Language} {Models}},
	shorttitle = {Social {Bias} {Probing}},
	url = {http://arxiv.org/abs/2311.09090},
	abstract = {Large language models have been shown to encode a variety of social biases, which carries the risk of downstream harms. While the impact of these biases has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, offering a constrained view of the nature of societal biases within language models. In this paper, we propose an original framework for probing language models for societal biases. We collect a probing dataset to analyze language models' general associations, as well as along the axes of societal categories, identities, and stereotypes. To this end, we leverage a novel perplexity-based fairness score. We curate a large-scale benchmarking dataset addressing drawbacks and limitations of existing fairness collections, expanding to a variety of different identities and stereotypes. When comparing our methodology with prior work, we demonstrate that biases within language models are more nuanced than previously acknowledged. In agreement with recent findings, we find that larger model variants exhibit a higher degree of bias. Moreover, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Manerba, Marta Marchiori and Stańczak, Karolina and Guidotti, Riccardo and Augenstein, Isabelle},
	month = feb,
	year = {2024},
	note = {arXiv:2311.09090 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\4Q9ACI53\\2311.html:text/html;Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\3MTTIA6T\\Manerba et al. - 2024 - Social Bias Probing Fairness Benchmarking for Lan.pdf:application/pdf},
}

@inproceedings{islam_can_2021,
	address = {Virtual Event USA},
	title = {Can {We} {Obtain} {Fairness} {For} {Free}?},
	isbn = {978-1-4503-8473-5},
	url = {https://dl.acm.org/doi/10.1145/3461702.3462614},
	doi = {10.1145/3461702.3462614},
	language = {en},
	urldate = {2024-04-09},
	booktitle = {Proceedings of the 2021 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Islam, Rashidul and Pan, Shimei and Foulds, James R.},
	month = jul,
	year = {2021},
	pages = {586--596},
	file = {Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\7HI2T4R8\\Islam et al. - 2021 - Can We Obtain Fairness For Free.pdf:application/pdf},
}

@inproceedings{gohar_towards_2023,
	title = {Towards {Understanding} {Fairness} and its {Composition} in {Ensemble} {Machine} {Learning}},
	url = {https://ieeexplore.ieee.org/document/10172501},
	doi = {10.1109/ICSE48619.2023.00133},
	abstract = {Machine Learning (ML) software has been widely adopted in modern society, with reported fairness implications for minority groups based on race, sex, age, etc. Many recent works have proposed methods to measure and mitigate algorithmic bias in ML models. The existing approaches focus on single classifier-based ML models. However, real-world ML models are often composed of multiple independent or dependent learners in an ensemble (e.g., Random Forest), where the fairness composes in a non-trivial way. How does fairness compose in ensembles? What are the fairness impacts of the learners on the ultimate fairness of the ensemble? Can fair learners result in an unfair ensemble? Furthermore, studies have shown that hyperparameters influence the fairness of ML models. Ensemble hyperparameters are more complex since they affect how learners are combined in different categories of ensembles. Understanding the impact of ensemble hyperparameters on fairness will help programmers design fair ensembles. Today, we do not understand these fully for different ensemble algorithms. In this paper, we comprehensively study popular real-world ensembles: Bagging, Boosting, Stacking, and Voting. We have developed a benchmark of 168 ensemble models collected from Kaggle on four popular fairness datasets. We use existing fairness metrics to understand the composition of fairness. Our results show that ensembles can be designed to be fairer without using mitigation techniques. We also identify the interplay between fairness composition and data characteristics to guide fair ensemble design. Finally, our benchmark can be leveraged for further research on fair ensembles. To the best of our knowledge, this is one of the first and largest studies on fairness composition in ensembles yet presented in the literature.},
	urldate = {2024-04-09},
	booktitle = {2023 {IEEE}/{ACM} 45th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Gohar, Usman and Biswas, Sumon and Rajan, Hridesh},
	month = may,
	year = {2023},
	note = {ISSN: 1558-1225},
	keywords = {machine learning, fairness, Training, Machine learning algorithms, Benchmark testing, Software, ensemble, models, Software algorithms, Software measurement, Stacking},
	pages = {1533--1545},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Administrator\\Zotero\\storage\\UQGG6IQT\\10172501.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\9JJZYHCF\\Gohar et al. - 2023 - Towards Understanding Fairness and its Composition.pdf:application/pdf},
}

@inproceedings{l_cardoso_framework_2019,
	address = {Honolulu HI USA},
	title = {A {Framework} for {Benchmarking} {Discrimination}-{Aware} {Models} in {Machine} {Learning}},
	isbn = {978-1-4503-6324-2},
	url = {https://dl.acm.org/doi/10.1145/3306618.3314262},
	doi = {10.1145/3306618.3314262},
	language = {en},
	urldate = {2024-04-09},
	booktitle = {Proceedings of the 2019 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {L. Cardoso, Rodrigo and Meira Jr., Wagner and Almeida, Virgilio and J. Zaki, Mohammed},
	month = jan,
	year = {2019},
	pages = {437--444},
	file = {Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\3ELCNREK\\L. Cardoso et al. - 2019 - A Framework for Benchmarking Discrimination-Aware .pdf:application/pdf},
}

@article{fabris_algorithmic_2022,
	title = {Algorithmic fairness datasets: the story so far},
	volume = {36},
	issn = {1573-756X},
	shorttitle = {Algorithmic fairness datasets},
	url = {https://doi.org/10.1007/s10618-022-00854-z},
	doi = {10.1007/s10618-022-00854-z},
	abstract = {Data-driven algorithms are studied and deployed in diverse domains to support critical decisions, directly impacting people’s well-being. As a result, a growing community of researchers has been investigating the equity of existing algorithms and proposing novel ones, advancing the understanding of risks and opportunities of automated decision-making for historically disadvantaged populations. Progress in fair machine learning and equitable algorithm design hinges on data, which can be appropriately used only if adequately documented. Unfortunately, the algorithmic fairness community, as a whole, suffers from a collective data documentation debt caused by a lack of information on specific resources (opacity) and scatteredness of available information (sparsity). In this work, we target this data documentation debt by surveying over two hundred datasets employed in algorithmic fairness research, and producing standardized and searchable documentation for each of them. Moreover we rigorously identify the three most popular fairness datasets, namely Adult, COMPAS, and German Credit, for which we compile in-depth documentation. This unifying documentation effort supports multiple contributions. Firstly, we summarize the merits and limitations of Adult, COMPAS, and German Credit, adding to and unifying recent scholarship, calling into question their suitability as general-purpose fairness benchmarks. Secondly, we document hundreds of available alternatives, annotating their domain and supported fairness tasks, along with additional properties of interest for fairness practitioners and researchers, including their format, cardinality, and the sensitive attributes they encode. We summarize this information, zooming in on the tasks, domains, and roles of these resources. Finally, we analyze these datasets from the perspective of five important data curation topics: anonymization, consent, inclusivity, labeling of sensitive attributes, and transparency. We discuss different approaches and levels of attention to these topics, making them tangible, and distill them into a set of best practices for the curation of novel resources.},
	language = {en},
	number = {6},
	urldate = {2024-04-09},
	journal = {Data Mining and Knowledge Discovery},
	author = {Fabris, Alessandro and Messina, Stefano and Silvello, Gianmaria and Susto, Gian Antonio},
	month = nov,
	year = {2022},
	keywords = {Algorithmic fairness, Datasets, Documentation debt},
	pages = {2074--2152},
	file = {Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\4Y9BSAZB\\Fabris et al. - 2022 - Algorithmic fairness datasets the story so far.pdf:application/pdf},
}

@inproceedings{park_fairness_2022,
	address = {New York, NY, USA},
	series = {{WWW} '22},
	title = {Fairness {Audit} of {Machine} {Learning} {Models} with {Confidential} {Computing}},
	isbn = {978-1-4503-9096-5},
	url = {https://dl.acm.org/doi/10.1145/3485447.3512244},
	doi = {10.1145/3485447.3512244},
	abstract = {Algorithmic discrimination is one of the significant concerns in applying machine learning models to a real-world system. Many researchers have focused on developing fair machine learning algorithms without discrimination based on legally protected attributes. However, the existing research has barely explored various security issues that can occur while evaluating model fairness and verifying fair models. In this study, we propose a fairness audit framework that assesses the fairness of ML algorithms while addressing potential security issues such as data privacy, model secrecy, and trustworthiness. To this end, our proposed framework utilizes confidential computing and builds a chain of trust through enclave attestation primitives combined with public scrutiny and state-of-the-art software-based security techniques, enabling fair ML models to be securely certified and clients to verify a certified one. Our micro-benchmarks on various ML models and real-world datasets show the feasibility of the fairness certification implemented with Intel SGX in practice. In addition, we analyze the impact of data poisoning, which is an additional threat during data collection for fairness auditing. Based on the analysis, we illustrate the theoretical curves of fairness gap and minimal group size and the empirical results of fairness certification on poisoned datasets.},
	urldate = {2024-04-09},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
	publisher = {Association for Computing Machinery},
	author = {Park, Saerom and Kim, Seongmin and Lim, Yeon-sup},
	month = apr,
	year = {2022},
	keywords = {Fairness, Algorithmic audit, Confidential computing, Security and privacy},
	pages = {3488--3499},
	file = {Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\ITISH3MV\\Park et al. - 2022 - Fairness Audit of Machine Learning Models with Con.pdf:application/pdf},
}

@inproceedings{fabris_tackling_2022,
	address = {Arlington VA USA},
	title = {Tackling {Documentation} {Debt}: {A} {Survey} on {Algorithmic} {Fairness} {Datasets}},
	isbn = {978-1-4503-9477-2},
	shorttitle = {Tackling {Documentation} {Debt}},
	url = {https://dl.acm.org/doi/10.1145/3551624.3555286},
	doi = {10.1145/3551624.3555286},
	language = {en},
	urldate = {2024-04-09},
	booktitle = {Equity and {Access} in {Algorithms}, {Mechanisms}, and {Optimization}},
	publisher = {ACM},
	author = {Fabris, Alessandro and Messina, Stefano and Silvello, Gianmaria and Susto, Gian Antonio},
	month = oct,
	year = {2022},
	pages = {1--13},
	file = {Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\5NPXEXCA\\Fabris et al. - 2022 - Tackling Documentation Debt A Survey on Algorithm.pdf:application/pdf},
}

@inproceedings{cruz_unprocessing_2023,
	title = {Unprocessing {Seven} {Years} of {Algorithmic} {Fairness}},
	url = {https://openreview.net/forum?id=jr03SfWsBS},
	abstract = {Seven years ago, researchers proposed a postprocessing method to equalize the error rates of a model across different demographic groups. The work launched hundreds of papers purporting to improve over the postprocessing baseline. We empirically evaluate these claims through thousands of model evaluations on several tabular datasets. We find that the fairness-accuracy Pareto frontier achieved by postprocessing contains all other methods we were feasibly able to evaluate. In doing so, we address two common methodological errors that have confounded previous observations. One relates to the comparison of methods with different unconstrained base models. The other concerns methods achieving different levels of constraint relaxation. At the heart of our study is a simple idea we call unprocessing that roughly corresponds to the inverse of postprocessing. Unprocessing allows for a direct comparison of methods using different underlying models and levels of relaxation.},
	language = {en},
	urldate = {2024-04-10},
	author = {Cruz, André and Hardt, Moritz},
	month = oct,
	year = {2023},
	file = {Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\6GMCVWRG\\Cruz and Hardt - 2023 - Unprocessing Seven Years of Algorithmic Fairness.pdf:application/pdf},
}

@article{kim_fair_2023,
	title = {Fair classification by loss balancing via fairness-aware batch sampling},
	volume = {518},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231222013984},
	doi = {10.1016/j.neucom.2022.11.018},
	abstract = {Existing classification models often output discriminatory results since they learn the target attribute without addressing data imbalance with respect to the protected attributes (e.g., gender). The models tend to focus on learning toward demographic groups containing the larger number of training samples, which consequently leads to training loss discrepancy between the groups. Our work focuses on addressing the occurrence of training loss discrepancy between the groups to improve the model’s fairness. To this end, we firstly define the target-protected group using the target and protected attribute labels and observe the group-wise training loss in terms of previous fairness approaches. From the observation, we figure out that balancing the total loss across all the groups allows to mitigate fairness issue significantly, and meanwhile, only considering the sample size of each group to obtain a balanced mini-batch is not enough for mitigating fairness. Motivated by the observations, we propose a fairness-aware batch sampling scheme that adaptively updates batch sampling probability (BSP) and constructs a fairness-aware mini-batch from the model’s point of view. Our key idea is to balance the training losses via training with fairness-aware mini-batch. Through extensive experiments on two facial attribute benchmark datasets and one tabular dataset, our simple and effective sampling strategy achieves superior improvement in terms of two standard fairness metrics. We validate our algorithm with various experimental settings (e.g, multi-attribute classification, binary classification with multiple protected attributes). Moreover, we introduce a new metric for measuring the trade-off between fairness and classification performance. On this metric, our algorithm also achieves the best trade-off performance.},
	urldate = {2024-04-10},
	journal = {Neurocomputing},
	author = {Kim, Dohyung and Park, Sungho and Hwang, Sunhee and Byun, Hyeran},
	month = jan,
	year = {2023},
	keywords = {Classification, Data imbalance, Deep neural network, Fairness AI, Resampling},
	pages = {231--241},
	annote = {Introduces a FAT measure which takes the harmonic mean of the fairness-accuracy trade-off

},
	file = {ScienceDirect Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\CU4XW4H7\\S0925231222013984.html:text/html},
}

@inproceedings{chakraborty_fairway_2020,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2020},
	title = {Fairway: a way to build fair {ML} software},
	isbn = {978-1-4503-7043-1},
	shorttitle = {Fairway},
	url = {https://dl.acm.org/doi/10.1145/3368089.3409697},
	doi = {10.1145/3368089.3409697},
	abstract = {Machine learning software is increasingly being used to make decisions that affect people's lives. But sometimes, the core part of this software (the learned model), behaves in a biased manner that gives undue advantages to a specific group of people (where those groups are determined by sex, race, etc.). This "algorithmic discrimination" in the AI software systems has become a matter of serious concern in the machine learning and software engineering community. There have been works done to find "algorithmic bias" or "ethical bias" in the software system. Once the bias is detected in the AI software system, the mitigation of bias is extremely important. In this work, we a)explain how ground-truth bias in training data affects machine learning model fairness and how to find that bias in AI software,b)propose a method Fairway which combines pre-processing and in-processing approach to remove ethical bias from training data and trained model. Our results show that we can find bias and mitigate bias in a learned model, without much damaging the predictive performance of that model. We propose that (1) testing for bias and (2) bias mitigation should be a routine part of the machine learning software development life cycle. Fairway offers much support for these two purposes.},
	urldate = {2024-04-10},
	booktitle = {Proceedings of the 28th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Chakraborty, Joymallya and Majumder, Suvodeep and Yu, Zhe and Menzies, Tim},
	month = nov,
	year = {2020},
	keywords = {Bias Mitigation, Fairness Metrics, Software Fairness, In-processing, Pre-processing},
	pages = {654--665},
	annote = {Combines in-processing and pre-processing

},
	file = {Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\42RJ6L4J\\Chakraborty et al. - 2020 - Fairway a way to build fair ML software.pdf:application/pdf},
}

@article{peng_fairmask_2023,
	title = {{FairMask}: {Better} {Fairness} via {Model}-{Based} {Rebalancing} of {Protected} {Attributes}},
	volume = {49},
	issn = {1939-3520},
	shorttitle = {{FairMask}},
	url = {https://ieeexplore.ieee.org/document/9951398},
	doi = {10.1109/TSE.2022.3220713},
	number = {4},
	urldate = {2024-04-10},
	journal = {IEEE Transactions on Software Engineering},
	author = {Peng, Kewen and Chakraborty, Joymallya and Menzies, Tim},
	month = apr,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Predictive models, Data models, Software, Measurement, Software algorithms, bias mitigation, explanation, Extrapolation, Social groups, Software fairness},
	pages = {2426--2439},
	annote = {They introduce flip rate (it’s just causal discrimination, but we could add it as a fairness term and just have it to show a weakness of post-processing)
They also say that it actually comes from fairway? But I’m not finding it in de Fairway paper.
},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Administrator\\Zotero\\storage\\BHVNS698\\9951398.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\LCGYPM9U\\Peng et al. - 2023 - FairMask Better Fairness via Model-Based Rebalanc.pdf:application/pdf},
}

 @article{Bellamy_Dey_Hind_Hoffman_Houde_Kannan_Lohia_Martino_Mehta_Mojsilovic_et, 
 title={AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias}, 
 url={http://arxiv.org/abs/1810.01943}, 
 DOI={10.48550/arXiv.1810.01943}, 
 note={arXiv:1810.01943 [cs]}, 
 number={arXiv:1810.01943}, 
 publisher={arXiv}, 
 author={Bellamy, Rachel K. E. and Dey, Kuntal and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Kannan, Kalapriya and Lohia, Pranay and Martino, Jacquelyn and Mehta, Sameep and Mojsilovic, Aleksandra and Nagar, Seema and Ramamurthy, Karthikeyan Natesan and Richards, John and Saha, Diptikalyan and Sattigeri, Prasanna and Singh, Moninder and Varshney, Kush R. and Zhang, Yunfeng}, 
 year={2018}, 
 month=oct }


@inproceedings{biswas_machine_2020,
	address = {Virtual Event USA},
	title = {Do the machine learning models on a crowd sourced platform exhibit bias? an empirical study on model fairness},
	isbn = {978-1-4503-7043-1},
	shorttitle = {Do the machine learning models on a crowd sourced platform exhibit bias?},
	url = {https://dl.acm.org/doi/10.1145/3368089.3409704},
	doi = {10.1145/3368089.3409704},
	language = {en},
	urldate = {2024-04-10},
	booktitle = {Proceedings of the 28th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Biswas, Sumon and Rajan, Hridesh},
	month = nov,
	year = {2020},
	pages = {642--653},
	file = {Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\R4ERTWXY\\Biswas and Rajan - 2020 - Do the machine learning models on a crowd sourced .pdf:application/pdf},
}

@inproceedings{hort_fairea_2021,
	address = {Athens Greece},
	title = {Fairea: a model behaviour mutation approach to benchmarking bias mitigation methods},
	isbn = {978-1-4503-8562-6},
	shorttitle = {Fairea},
	url = {https://dl.acm.org/doi/10.1145/3468264.3468565},
	doi = {10.1145/3468264.3468565},
	language = {en},
	urldate = {2024-04-10},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Hort, Max and Zhang, Jie M. and Sarro, Federica and Harman, Mark},
	month = aug,
	year = {2021},
	pages = {994--1006},
	file = {Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\EZ6GA9Q7\\Hort et al. - 2021 - Fairea a model behaviour mutation approach to ben.pdf:application/pdf},
}

@inproceedings{lalor_benchmarking_2022,
	address = {Seattle, United States},
	title = {Benchmarking {Intersectional} {Biases} in {NLP}},
	url = {https://aclanthology.org/2022.naacl-main.263},
	doi = {10.18653/v1/2022.naacl-main.263},
	abstract = {There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in state-of-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race. In this work, we benchmark multiple NLP models with regards to their fairness and predictive performance across a variety of NLP tasks. In particular, we assess intersectional bias - fairness across multiple demographic dimensions. The results show that while current debiasing strategies fare well in terms of the fairness-accuracy trade-off (generally preserving predictive power in debiased models), they are unable to effectively alleviate bias in downstream tasks. Furthermore, this bias is often amplified across dimensions (i.e., intersections). We conclude by highlighting possible causes and making recommendations for future NLP debiasing research.},
	urldate = {2024-04-10},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Lalor, John and Yang, Yi and Smith, Kendall and Forsgren, Nicole and Abbasi, Ahmed},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	month = jul,
	year = {2022},
	pages = {3598--3609},
	file = {Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\4IAZLQMJ\\Lalor et al. - 2022 - Benchmarking Intersectional Biases in NLP.pdf:application/pdf},
}


@article{le_quy_survey_2022,
	title = {A survey on datasets for fairness-aware machine learning},
	volume = {12},
	issn = {1942-4795},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1452},
	doi = {10.1002/widm.1452},
	abstract = {As decision-making increasingly relies on machine learning (ML) and (big) data, the issue of fairness in data-driven artificial intelligence systems is receiving increasing attention from both research and industry. A large variety of fairness-aware ML solutions have been proposed which involve fairness-related interventions in the data, learning algorithms, and/or model outputs. However, a vital part of proposing new approaches is evaluating them empirically on benchmark datasets that represent realistic and diverse settings. Therefore, in this paper, we overview real-world datasets used for fairness-aware ML. We focus on tabular data as the most common data representation for fairness-aware ML. We start our analysis by identifying relationships between the different attributes, particularly with respect to protected attributes and class attribute, using a Bayesian network. For a deeper understanding of bias in the datasets, we investigate interesting relationships using exploratory analysis. This article is categorized under: Commercial, Legal, and Ethical Issues {\textgreater} Fairness in Data Mining Fundamental Concepts of Data and Knowledge {\textgreater} Data Concepts Technologies {\textgreater} Data Preprocessing},
	language = {en},
	number = {3},
	urldate = {2024-04-10},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Le Quy, Tai and Roy, Arjun and Iosifidis, Vasileios and Zhang, Wenbin and Ntoutsi, Eirini},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1452},
	keywords = {discrimination, bias, fairness-aware machine learning, benchmark datasets, datasets for fairness},
	pages = {e1452},
	file = {Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\YR48P2DL\\Le Quy et al. - 2022 - A survey on datasets for fairness-aware machine le.pdf:application/pdf;Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\N62N2TJ8\\widm.html:text/html},
}