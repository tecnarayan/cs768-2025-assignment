\begin{thebibliography}{78}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdal et~al.(2019)Abdal, Qin, and Wonka]{abdal2019image2stylegan}
Rameen Abdal, Yipeng Qin, and Peter Wonka.
\newblock Image2stylegan: How to embed images into the stylegan latent space?
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 4432--4441, 2019.

\bibitem[Abdal et~al.(2021)Abdal, Zhu, Mitra, and Wonka]{abdal2021styleflow}
Rameen Abdal, Peihao Zhu, Niloy~J Mitra, and Peter Wonka.
\newblock Styleflow: Attribute-conditioned exploration of stylegan-generated
  images using conditional continuous normalizing flows.
\newblock \emph{ACM Transactions on Graphics (TOG)}, 40\penalty0 (3):\penalty0
  1--21, 2021.

\bibitem[Amos(2022)]{amos2022tutorial}
Brandon Amos.
\newblock Tutorial on amortized optimization for learning to optimize over
  continuous domains.
\newblock \emph{arXiv preprint arXiv:2202.00665}, 2022.

\bibitem[Amos and Kolter(2017)]{amos2017optnet}
Brandon Amos and J.~Zico Kolter.
\newblock {OptNet}: Differentiable optimization as a layer in neural networks.
\newblock In \emph{{International Conference on Machine Learning (ICML)}},
  2017.

\bibitem[Anderson(1965)]{anderson1965iterative}
Donald~G Anderson.
\newblock Iterative procedures for nonlinear integral equations.
\newblock \emph{Journal of the ACM (JACM)}, 1965.

\bibitem[Bai et~al.(2019)Bai, Kolter, and Koltun]{bai2019deep}
Shaojie Bai, J~Zico Kolter, and Vladlen Koltun.
\newblock Deep equilibrium models.
\newblock \emph{{Neural Information Processing Systems (NeurIPS)}}, 2019.

\bibitem[Bai et~al.(2020)Bai, Koltun, and Kolter]{bai2020multiscale}
Shaojie Bai, Vladlen Koltun, and J~Zico Kolter.
\newblock Multiscale deep equilibrium models.
\newblock \emph{{Neural Information Processing Systems (NeurIPS)}}, 2020.

\bibitem[Bai et~al.(2021)Bai, Koltun, and Kolter]{bai2021stabilizing}
Shaojie Bai, Vladlen Koltun, and J~Zico Kolter.
\newblock Stabilizing equilibrium models by jacobian regularization.
\newblock \emph{arXiv preprint arXiv:2106.14342}, 2021.

\bibitem[Bai et~al.(2022{\natexlab{a}})Bai, Geng, Savani, and
  Kolter]{bai2022deep}
Shaojie Bai, Zhengyang Geng, Yash Savani, and J~Zico Kolter.
\newblock Deep equilibrium optical flow estimation.
\newblock \emph{arXiv preprint arXiv:2204.08442}, 2022{\natexlab{a}}.

\bibitem[Bai et~al.(2022{\natexlab{b}})Bai, Koltun, and Kolter]{bai2022neural}
Shaojie Bai, Vladlen Koltun, and J~Zico Kolter.
\newblock Neural deep equilibrium solvers.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{b}}.

\bibitem[Bau et~al.(2020)Bau, Strobelt, Peebles, Wulff, Zhou, Zhu, and
  Torralba]{bau2020semantic}
David Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou, Jun-Yan
  Zhu, and Antonio Torralba.
\newblock Semantic photo manipulation with a generative image prior.
\newblock \emph{arXiv preprint arXiv:2005.07727}, 2020.

\bibitem[Bora et~al.(2017)Bora, Jalal, Price, and Dimakis]{bora2017compressed}
Ashish Bora, Ajil Jalal, Eric Price, and Alexandros~G Dimakis.
\newblock Compressed sensing using generative models.
\newblock In \emph{International Conference on Machine Learning}, pages
  537--546. PMLR, 2017.

\bibitem[Broyden(1965)]{broyden1965class}
Charles~G Broyden.
\newblock A class of methods for solving nonlinear simultaneous equations.
\newblock \emph{Mathematics of computation}, 1965.

\bibitem[Chan et~al.(2021)Chan, Wang, Xu, Gu, and Loy]{chan2021glean}
Kelvin~CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen~Change Loy.
\newblock Glean: Generative latent bank for large-factor image
  super-resolution.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 14245--14254, 2021.

\bibitem[Chen et~al.(2022)Chen, Wang, Wang, Yang, and
  Lin]{chen2022optimization}
Qi~Chen, Yifei Wang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin.
\newblock Optimization-induced graph implicit nonlinear diffusion.
\newblock In \emph{International Conference on Machine Learning}, pages
  3648--3661. PMLR, 2022.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and
  Duvenaud]{chen2018neural}
Tian~Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David~K Duvenaud.
\newblock Neural ordinary differential equations.
\newblock In \emph{{Neural Information Processing Systems (NeurIPS)}}, 2018.

\bibitem[Choi et~al.(2021)Choi, Kim, Jeong, Gwon, and Yoon]{choi2021ilvr}
Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon.
\newblock Ilvr: Conditioning method for denoising diffusion probabilistic
  models.
\newblock \emph{arXiv preprint arXiv:2108.02938}, 2021.

\bibitem[Chung et~al.(2021)Chung, Sim, and Ye]{chung2021come}
Hyungjin Chung, Byeongsu Sim, and Jong~Chul Ye.
\newblock Come-closer-diffuse-faster: Accelerating conditional diffusion models
  for inverse problems through stochastic contraction.
\newblock \emph{arXiv preprint arXiv:2112.05146}, 2021.

\bibitem[Dhariwal and Nichol(2021)]{dhariwal2021diffusion}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Djolonga and Krause(2017)]{djolonga2017differentiable}
Josip Djolonga and Andreas Krause.
\newblock Differentiable learning of submodular models.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Donti et~al.(2021)Donti, Rolnick, and Kolter]{donti2021dc}
Priya~L. Donti, David Rolnick, and J~Zico Kolter.
\newblock {DC}3: A learning method for optimization with hard constraints.
\newblock In \emph{{International Conference on Learning Representations
  (ICLR)}}, 2021.

\bibitem[Dupont et~al.(2019)Dupont, Doucet, and Teh]{dupont2019augmented}
Emilien Dupont, Arnaud Doucet, and Yee~Whye Teh.
\newblock Augmented neural {ODEs}.
\newblock In \emph{{Neural Information Processing Systems (NeurIPS)}}, 2019.

\bibitem[{El Ghaoui} et~al.(2019){El Ghaoui}, {Gu}, {Travacca}, and
  {Askari}]{elghaoui2019implicit}
Laurent {El Ghaoui}, Fangda {Gu}, Bertrand {Travacca}, and Armin {Askari}.
\newblock Implicit deep learning.
\newblock \emph{arXiv:1908.06315}, 2019.

\bibitem[Falk et~al.(2019)Falk, Mai, Bensch, {\c{C}}i{\c{c}}ek, Abdulkadir,
  Marrakchi, B{\"o}hm, Deubner, J{\"a}ckel, Seiwald, et~al.]{falk2019u}
Thorsten Falk, Dominic Mai, Robert Bensch, {\"O}zg{\"u}n {\c{C}}i{\c{c}}ek,
  Ahmed Abdulkadir, Yassine Marrakchi, Anton B{\"o}hm, Jan Deubner, Zoe
  J{\"a}ckel, Katharina Seiwald, et~al.
\newblock U-net: deep learning for cell counting, detection, and morphometry.
\newblock \emph{Nature methods}, 16\penalty0 (1):\penalty0 67--70, 2019.

\bibitem[Feng and Kolter(2021)]{feng2021on}
Zhili Feng and J~Zico Kolter.
\newblock On the neural tangent kernel of equilibrium models, 2021.

\bibitem[Fung et~al.(2021)Fung, Heaton, Li, McKenzie, Osher, and
  Yin]{fung2021fixed}
Samy~Wu Fung, Howard Heaton, Qiuwei Li, Daniel McKenzie, Stanley Osher, and
  Wotao Yin.
\newblock Fixed point networks: Implicit depth models with jacobian-free
  backprop.
\newblock \emph{arXiv e-prints}, pages arXiv--2103, 2021.

\bibitem[Geng et~al.(2021{\natexlab{a}})Geng, Guo, Chen, Li, Wei, and
  Lin]{geng2021attention}
Zhengyang Geng, Meng-Hao Guo, Hongxu Chen, Xia Li, Ke~Wei, and Zhouchen Lin.
\newblock Is attention better than matrix decomposition?
\newblock In \emph{{International Conference on Learning Representations
  (ICLR)}}, 2021{\natexlab{a}}.

\bibitem[Geng et~al.(2021{\natexlab{b}})Geng, Zhang, Bai, Wang, and
  Lin]{geng2021training}
Zhengyang Geng, Xin-Yu Zhang, Shaojie Bai, Yisen Wang, and Zhouchen Lin.
\newblock On training implicit models.
\newblock \emph{{Neural Information Processing Systems (NeurIPS)}},
  2021{\natexlab{b}}.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Gu et~al.(2022)Gu, Goel, and Re]{gu2022efficiently}
Albert Gu, Karan Goel, and Christopher Re.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In \emph{{International Conference on Learning Representations
  (ICLR)}}, 2022.

\bibitem[Gu et~al.(2020)Gu, Chang, Zhu, Sojoudi, and El~Ghaoui]{gu2021ignn}
Fangda Gu, Heng Chang, Wenwu Zhu, Somayeh Sojoudi, and Laurent El~Ghaoui.
\newblock {Implicit Graph Neural Networks}.
\newblock In \emph{{Neural Information Processing Systems (NeurIPS)}}, pages
  11984--11995, 2020.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{{Neural Information Processing Systems (NeurIPS)}}, 2020.

\bibitem[Huh et~al.(2020)Huh, Zhang, Zhu, Paris, and
  Hertzmann]{huh2020transforming}
Minyoung Huh, Richard Zhang, Jun-Yan Zhu, Sylvain Paris, and Aaron Hertzmann.
\newblock Transforming and projecting images into class-conditional generative
  networks.
\newblock In \emph{European Conference on Computer Vision}, pages 17--34.
  Springer, 2020.

\bibitem[Issenhuth et~al.(2021)Issenhuth, Tanielian, Mary, and
  Picard]{issenhuth2021edibert}
Thibaut Issenhuth, Ugo Tanielian, J{\'e}r{\'e}mie Mary, and David Picard.
\newblock Edibert, a generative model for image editing.
\newblock \emph{arXiv preprint arXiv:2111.15264}, 2021.

\bibitem[Jalal et~al.(2021)Jalal, Arvinte, Daras, Price, Dimakis, and
  Tamir]{jalal2021robust}
Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros~G Dimakis,
  and Jon Tamir.
\newblock Robust compressed sensing mri with deep generative priors.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 14938--14954, 2021.

\bibitem[Kadkhodaie and Simoncelli(2020)]{kadkhodaie2020solving}
Zahra Kadkhodaie and Eero~P Simoncelli.
\newblock Solving linear inverse problems using the prior implicit in a
  denoiser.
\newblock \emph{arXiv preprint arXiv:2007.13640}, 2020.

\bibitem[Kawaguchi(2020)]{kawaguchi2020theory}
Kenji Kawaguchi.
\newblock {On the Theory of Implicit Deep Learning: Global Convergence with
  Implicit Layers}.
\newblock In \emph{{International Conference on Learning Representations
  (ICLR)}}, 2020.

\bibitem[Kawar et~al.(2021)Kawar, Vaksman, and Elad]{kawar2021snips}
Bahjat Kawar, Gregory Vaksman, and Michael Elad.
\newblock Snips: Solving noisy inverse problems stochastically.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 21757--21769, 2021.

\bibitem[Kim and Ye(2021)]{kim2021diffusionclip}
Gwanghyun Kim and Jong~Chul Ye.
\newblock Diffusionclip: Text-guided image manipulation using diffusion models.
\newblock \emph{arXiv preprint arXiv:2110.02711}, 2021.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma et~al.(2021)Kingma, Salimans, Poole, and
  Ho]{kingma2021variational}
Diederik~P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho.
\newblock Variational diffusion models.
\newblock \emph{arXiv preprint arXiv:2107.00630}, 2021.

\bibitem[Kolter et~al.(2020)Kolter, Duvenaud, and Johnson]{Kolter2020}
J.~Zico Kolter, David Duvenaud, and Matthew Johnson.
\newblock Deep implicit layers tutorial - neural {ODEs}, deep equilibirum
  models, and beyond.
\newblock \emph{Neural Information Processing Systems Tutorial}, 2020.

\bibitem[Kong and Ping(2021)]{kong2021fast}
Zhifeng Kong and Wei Ping.
\newblock On fast sampling of diffusion probabilistic models.
\newblock \emph{arXiv preprint arXiv:2106.00132}, 2021.

\bibitem[Kong et~al.(2021)Kong, Ping, Huang, Zhao, and
  Catanzaro]{kong2021diffwave}
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro.
\newblock Diffwave: A versatile diffusion model for audio synthesis.
\newblock In \emph{{International Conference on Learning Representations
  (ICLR)}}, 2021.

\bibitem[Krizhevsky(2009)]{CIFAR}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Yang, Chang, Chen, Feng, Xu, Li, and
  Chen]{li2022srdiff}
Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun Feng, Zhihai Xu, Qi~Li,
  and Yueting Chen.
\newblock Srdiff: Single image super-resolution with diffusion probabilistic
  models.
\newblock \emph{Neurocomputing}, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Wang, and Lin]{li2022cerdeq}
Mingjie Li, Yisen Wang, and Zhouchen Lin.
\newblock Cerdeq: Certifiable deep equilibrium model.
\newblock In \emph{International Conference on Machine Learning},
  2022{\natexlab{b}}.

\bibitem[Ling et~al.(2021)Ling, Kreis, Li, Kim, Torralba, and
  Fidler]{ling2021editgan}
Huan Ling, Karsten Kreis, Daiqing Li, Seung~Wook Kim, Antonio Torralba, and
  Sanja Fidler.
\newblock Editgan: High-precision semantic image editing.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 16331--16345, 2021.

\bibitem[Ling et~al.(2022)Ling, Xie, Wang, Zhang, and Lin]{ling2022global}
Zenan Ling, Xingyu Xie, Qiuhao Wang, Zongpeng Zhang, and Zhouchen Lin.
\newblock Global convergence of over-parameterized deep equilibrium models.
\newblock \emph{arXiv preprint arXiv:2205.13814}, 2022.

\bibitem[Liu et~al.(2021)Liu, Kawaguchi, Hooi, Wang, and Xiao]{liu2021eignn}
Juncheng Liu, Kenji Kawaguchi, Bryan Hooi, Yiwei Wang, and Xiaokui Xiao.
\newblock {EIGNN}: Efficient infinite-depth graph neural networks.
\newblock In \emph{{Neural Information Processing Systems (NeurIPS)}}, 2021.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{CelebA}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of International Conference on Computer Vision
  (ICCV)}, December 2015.

\bibitem[Lu et~al.(2021)Lu, Chen, Li, Wang, and Zhu]{lu2021implicit}
Cheng Lu, Jianfei Chen, Chongxuan Li, Qiuhao Wang, and Jun Zhu.
\newblock Implicit normalizing flows.
\newblock In \emph{{International Conference on Learning Representations
  (ICLR)}}, 2021.

\bibitem[Luhman and Luhman(2021)]{Luhman2021KnowledgeDI}
Eric Luhman and Troy Luhman.
\newblock Knowledge distillation in iterative generative models for improved
  sampling speed.
\newblock \emph{ArXiv}, abs/2101.02388, 2021.

\bibitem[Meng et~al.(2021)Meng, Song, Song, Wu, Zhu, and Ermon]{meng2021sdedit}
Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano
  Ermon.
\newblock Sdedit: Image synthesis and editing with stochastic differential
  equations.
\newblock \emph{arXiv preprint arXiv:2108.01073}, 2021.

\bibitem[Nichol et~al.(2021)Nichol, Dhariwal, Ramesh, Shyam, Mishkin, McGrew,
  Sutskever, and Chen]{nichol2021glide}
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,
  Bob McGrew, Ilya Sutskever, and Mark Chen.
\newblock Glide: Towards photorealistic image generation and editing with
  text-guided diffusion models.
\newblock \emph{arXiv preprint arXiv:2112.10741}, 2021.

\bibitem[Nichol and Dhariwal(2021)]{nichol2021improved}
Alexander~Quinn Nichol and Prafulla Dhariwal.
\newblock Improved denoising diffusion probabilistic models.
\newblock In \emph{International Conference on Machine Learning}, pages
  8162--8171. PMLR, 2021.

\bibitem[Nie et~al.(2022)Nie, Guo, Huang, Xiao, Vahdat, and
  Anandkumar]{nie2022diffusion}
Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Anima
  Anandkumar.
\newblock Diffusion models for adversarial purification.
\newblock \emph{arXiv preprint arXiv:2205.07460}, 2022.

\bibitem[Park et~al.(2021)Park, Choo, and Park]{park2021convergent}
Junyoung Park, Jinhyun Choo, and Jinkyoo Park.
\newblock Convergent graph solvers.
\newblock \emph{arXiv preprint arXiv:2106.01680}, 2021.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem[Perarnau et~al.(2016)Perarnau, Van De~Weijer, Raducanu, and
  {\'A}lvarez]{perarnau2016invertible}
Guim Perarnau, Joost Van De~Weijer, Bogdan Raducanu, and Jose~M {\'A}lvarez.
\newblock Invertible conditional gans for image editing.
\newblock \emph{arXiv preprint arXiv:1611.06355}, 2016.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and Chen]{dalle2}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock \emph{arXiv preprint arXiv:2204.06125}, 2022.

\bibitem[Saeedi et~al.(2018)Saeedi, Hoffman, DiVerdi, Ghandeharioun, Johnson,
  and Adams]{saeedi2018multimodal}
Ardavan Saeedi, Matthew Hoffman, Stephen DiVerdi, Asma Ghandeharioun, Matthew
  Johnson, and Ryan Adams.
\newblock Multimodal prediction and personalization of photo edits with deep
  generative models.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1309--1317. PMLR, 2018.

\bibitem[Saharia et~al.(2021)Saharia, Ho, Chan, Salimans, Fleet, and
  Norouzi]{saharia2021image}
Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David~J Fleet, and
  Mohammad Norouzi.
\newblock Image super-resolution via iterative refinement.
\newblock \emph{arXiv preprint arXiv:2104.07636}, 2021.

\bibitem[Salimans and Ho(2022)]{salimans2022progressive}
Tim Salimans and Jonathan Ho.
\newblock Progressive distillation for fast sampling of diffusion models.
\newblock In \emph{{International Conference on Learning Representations
  (ICLR)}}, 2022.

\bibitem[Sasaki et~al.(2021)Sasaki, Willcocks, and Breckon]{sasaki2021unit}
Hiroshi Sasaki, Chris~G Willcocks, and Toby~P Breckon.
\newblock Unit-ddpm: Unpaired image translation with denoising diffusion
  probabilistic models.
\newblock \emph{arXiv preprint arXiv:2104.05358}, 2021.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and
  Ganguli]{sohl2015deep}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In \emph{{International Conference on Machine Learning (ICML)}},
  2015.

\bibitem[Song et~al.(2020)Song, Meng, and Ermon]{song2020denoising}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock \emph{arXiv preprint arXiv:2010.02502}, 2020.

\bibitem[Song and Ermon(2019)]{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Song et~al.(2021{\natexlab{a}})Song, Sohl-Dickstein, Kingma, Kumar,
  Ermon, and Poole]{SDE}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock In \emph{{International Conference on Learning Representations
  (ICLR)}}, 2021{\natexlab{a}}.

\bibitem[Song et~al.(2021{\natexlab{b}})Song, Sohl-Dickstein, Kingma, Kumar,
  Ermon, and Poole]{song2021sde}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock In \emph{{International Conference on Learning Representations
  (ICLR)}}, 2021{\natexlab{b}}.

\bibitem[Wang et~al.(2019)Wang, Donti, Wilder, and Kolter]{wang2019satnet}
Po-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter.
\newblock Satnet: Bridging deep learning and logical reasoning using a
  differentiable satisfiability solver.
\newblock In \emph{{International Conference on Machine Learning (ICML)}},
  2019.

\bibitem[Wang et~al.(2020)Wang, Zhang, and Sun]{wang2020iFPN}
Tiancai Wang, Xiangyu Zhang, and Jian Sun.
\newblock {Implicit Feature Pyramid Network for Object Detection}.
\newblock \emph{arXiv preprint arXiv:2012.13563}, 2020.

\bibitem[Wei and Kolter(2022)]{wei2022certified}
Colin Wei and J~Zico Kolter.
\newblock Certified robustness for deep equilibrium models via interval bound
  propagation.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Winston and Kolter(2020)]{MON}
Ezra Winston and J.~Zico Kolter.
\newblock Monotone operator equilibrium networks.
\newblock In \emph{{Neural Information Processing Systems (NeurIPS)}}, 2020.

\bibitem[Yu et~al.(2015)Yu, Zhang, Song, Seff, and Xiao]{LSUN}
Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao.
\newblock {LSUN:} construction of a large-scale image dataset using deep
  learning with humans in the loop.
\newblock \emph{CoRR}, abs/1506.03365, 2015.

\bibitem[Zhu et~al.(2020)Zhu, Shen, Zhao, and Zhou]{zhu2020domain}
Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou.
\newblock In-domain gan inversion for real image editing.
\newblock In \emph{European conference on computer vision}, pages 592--608.
  Springer, 2020.

\bibitem[Zhu et~al.(2016)Zhu, Kr{\"a}henb{\"u}hl, Shechtman, and
  Efros]{zhu2016generative}
Jun-Yan Zhu, Philipp Kr{\"a}henb{\"u}hl, Eli Shechtman, and Alexei~A Efros.
\newblock Generative visual manipulation on the natural image manifold.
\newblock In \emph{European conference on computer vision}, pages 597--613.
  Springer, 2016.

\end{thebibliography}
