\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alaa \& van~der Schaar(2017)Alaa and van~der Schaar]{alaa2017bayesian}
Alaa, A.~M. and van~der Schaar, M.
\newblock Bayesian inference of individualized treatment effects using
  multi-task gaussian processes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3424--3432, 2017.

\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and
  Man{\'e}]{amodei2016concrete}
Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and
  Man{\'e}, D.
\newblock Concrete problems in ai safety.
\newblock \emph{arXiv preprint arXiv:1606.06565}, 2016.

\bibitem[Barber et~al.(2019{\natexlab{a}})Barber, Candes, Ramdas, and
  Tibshirani]{barber2019conformal}
Barber, R.~F., Candes, E.~J., Ramdas, A., and Tibshirani, R.~J.
\newblock Conformal prediction under covariate shift.
\newblock \emph{arXiv preprint arXiv:1904.06019}, 2019{\natexlab{a}}.

\bibitem[Barber et~al.(2019{\natexlab{b}})Barber, Candes, Ramdas, and
  Tibshirani]{barber2019predictive}
Barber, R.~F., Candes, E.~J., Ramdas, A., and Tibshirani, R.~J.
\newblock Predictive inference with the jackknife+.
\newblock \emph{arXiv preprint arXiv:1905.02928}, 2019{\natexlab{b}}.

\bibitem[Bayarri \& Berger(2004)Bayarri and Berger]{bayarri2004interplay}
Bayarri, M.~J. and Berger, J.~O.
\newblock The interplay of bayesian and frequentist analysis.
\newblock \emph{Statistical Science}, pp.\  58--80, 2004.

\bibitem[Bousquet \& Elisseeff(2002)Bousquet and
  Elisseeff]{bousquet2002stability}
Bousquet, O. and Elisseeff, A.
\newblock Stability and generalization.
\newblock \emph{Journal of machine learning research}, 2\penalty0
  (Mar):\penalty0 499--526, 2002.

\bibitem[Cohn et~al.(1996)Cohn, Ghahramani, and Jordan]{cohn1996active}
Cohn, D.~A., Ghahramani, Z., and Jordan, M.~I.
\newblock Active learning with statistical models.
\newblock \emph{Journal of artificial intelligence research}, 4:\penalty0
  129--145, 1996.

\bibitem[Cook \& Weisberg(1982)Cook and Weisberg]{cook1982residuals}
Cook, R.~D. and Weisberg, S.
\newblock \emph{Residuals and influence in regression}.
\newblock New York: Chapman and Hall, 1982.

\bibitem[Debruyne et~al.(2008)Debruyne, Hubert, and Suykens]{debruyne2008model}
Debruyne, M., Hubert, M., and Suykens, J.~A.
\newblock Model selection in kernel based regression using the influence
  function.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0
  (Oct):\penalty0 2377--2400, 2008.

\bibitem[Devroye \& Wagner(1979)Devroye and Wagner]{devroye1979distribution}
Devroye, L. and Wagner, T.
\newblock Distribution-free inequalities for the deleted and holdout error
  estimates.
\newblock \emph{IEEE Transactions on Information Theory}, 25\penalty0
  (2):\penalty0 202--207, 1979.

\bibitem[Dua \& Graff(2017)Dua and Graff]{Dua:2019}
Dua, D. and Graff, C.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Dusenberry et~al.(2019)Dusenberry, Tran, Choi, Kemp, Nixon, Jerfel,
  Heller, and Dai]{dusenberry2019analyzing}
Dusenberry, M.~W., Tran, D., Choi, E., Kemp, J., Nixon, J., Jerfel, G., Heller,
  K., and Dai, A.~M.
\newblock Analyzing the role of model uncertainty for electronic health
  records.
\newblock \emph{arXiv preprint arXiv:1906.03842}, 2019.

\bibitem[Efron(1992)]{efron1992jackknife}
Efron, B.
\newblock Jackknife-after-bootstrap standard errors and influence functions.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 54\penalty0 (1):\penalty0 83--111, 1992.

\bibitem[Fernholz(2012)]{fernholz2012mises}
Fernholz, L.~T.
\newblock \emph{Von Mises calculus for statistical functionals}, volume~19.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Gal(2016)]{gal2016uncertainty}
Gal, Y.
\newblock \emph{Uncertainty in deep learning}.
\newblock PhD thesis, PhD thesis, University of Cambridge, 2016.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{gal2016dropout}
Gal, Y. and Ghahramani, Z.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  1050--1059, 2016.

\bibitem[Giordano et~al.(2018)Giordano, Stephenson, Liu, Jordan, and
  Broderick]{giordano2018swiss}
Giordano, R., Stephenson, W., Liu, R., Jordan, M.~I., and Broderick, T.
\newblock A swiss army infinitesimal jackknife.
\newblock \emph{arXiv preprint arXiv:1806.00550}, 2018.

\bibitem[Giordano et~al.(2019)Giordano, Jordan, and
  Broderick]{giordano2019higher}
Giordano, R., Jordan, M.~I., and Broderick, T.
\newblock A higher-order swiss army infinitesimal jackknife.
\newblock \emph{arXiv preprint arXiv:1907.12116}, 2019.

\bibitem[Hampel et~al.(2011)Hampel, Ronchetti, Rousseeuw, and
  Stahel]{hampel2011robust}
Hampel, F.~R., Ronchetti, E.~M., Rousseeuw, P.~J., and Stahel, W.~A.
\newblock \emph{Robust statistics: the approach based on influence functions},
  volume 196.
\newblock John Wiley \& Sons, 2011.

\bibitem[Hern{\'a}ndez-Lobato \& Adams(2015)Hern{\'a}ndez-Lobato and
  Adams]{hernandez2015probabilistic}
Hern{\'a}ndez-Lobato, J.~M. and Adams, R.
\newblock Probabilistic backpropagation for scalable learning of bayesian
  neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  1861--1869, 2015.

\bibitem[Hron et~al.(2017)Hron, Matthews, and Ghahramani]{hron2017variational}
Hron, J., Matthews, A. G. d.~G., and Ghahramani, Z.
\newblock Variational gaussian dropout is not bayesian.
\newblock \emph{arXiv preprint arXiv:1711.02989}, 2017.

\bibitem[Huber \& Ronchetti(1981)Huber and Ronchetti]{huber1981robust}
Huber, P.~J. and Ronchetti, E.~M.
\newblock Robust statistics john wiley \& sons.
\newblock \emph{New York}, 1\penalty0 (1), 1981.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and
  Welling]{kingma2015variational}
Kingma, D.~P., Salimans, T., and Welling, M.
\newblock Variational dropout and the local reparameterization trick.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2575--2583, 2015.

\bibitem[Koh \& Liang(2017)Koh and Liang]{koh2017understanding}
Koh, P.~W. and Liang, P.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1885--1894. JMLR. org, 2017.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{lakshminarayanan2017simple}
Lakshminarayanan, B., Pritzel, A., and Blundell, C.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  6402--6413, 2017.

\bibitem[Lawless \& Fredette(2005)Lawless and Fredette]{lawless2005frequentist}
Lawless, J. and Fredette, M.
\newblock Frequentist prediction intervals and predictive distributions.
\newblock \emph{Biometrika}, 92\penalty0 (3):\penalty0 529--542, 2005.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{nature}, 521\penalty0 (7553):\penalty0 436, 2015.

\bibitem[Leonard et~al.(1992)Leonard, Kramer, and Ungar]{leonard1992neural}
Leonard, J., Kramer, M.~A., and Ungar, L.
\newblock A neural network architecture that computes its own reliability.
\newblock \emph{Computers \& chemical engineering}, 16\penalty0 (9):\penalty0
  819--835, 1992.

\bibitem[Maddox et~al.(2019)Maddox, Garipov, Izmailov, Vetrov, and
  Wilson]{maddox2019simple}
Maddox, W., Garipov, T., Izmailov, P., Vetrov, D., and Wilson, A.~G.
\newblock A simple baseline for bayesian uncertainty in deep learning.
\newblock \emph{arXiv preprint arXiv:1902.02476}, 2019.

\bibitem[Malinin \& Gales(2018)Malinin and Gales]{malinin2018predictive}
Malinin, A. and Gales, M.
\newblock Predictive uncertainty estimation via prior networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  7047--7058, 2018.

\bibitem[Mentch \& Hooker(2016)Mentch and Hooker]{mentch2016quantifying}
Mentch, L. and Hooker, G.
\newblock Quantifying uncertainty in random forests via confidence intervals
  and hypothesis tests.
\newblock \emph{The Journal of Machine Learning Research (JMLR)}, 17\penalty0
  (1):\penalty0 841--881, 2016.

\bibitem[Miller(1974)]{miller1974jackknife}
Miller, R.~G.
\newblock The jackknife-a review.
\newblock \emph{Biometrika}, 61\penalty0 (1):\penalty0 1--15, 1974.

\bibitem[Osband(2016)]{osband2016risk}
Osband, I.
\newblock Risk versus uncertainty in deep learning: Bayes, bootstrap and the
  dangers of dropout.
\newblock In \emph{NIPS Workshop on Bayesian Deep Learning}, 2016.

\bibitem[Ovadia et~al.(2019)Ovadia, Fertig, Ren, Nado, Sculley, Nowozin,
  Dillon, Lakshminarayanan, and Snoek]{ovadia2019can}
Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Nowozin, S., Dillon,
  J.~V., Lakshminarayanan, B., and Snoek, J.
\newblock Can you trust your model's uncertainty? evaluating predictive
  uncertainty under dataset shift.
\newblock \emph{arXiv preprint arXiv:1906.02530}, 2019.

\bibitem[Pearlmutter(1994)]{pearlmutter1994fast}
Pearlmutter, B.~A.
\newblock Fast exact multiplication by the hessian.
\newblock \emph{Neural computation}, 6\penalty0 (1):\penalty0 147--160, 1994.

\bibitem[Platt et~al.(1999)]{platt1999probabilistic}
Platt, J. et~al.
\newblock Probabilistic outputs for support vector machines and comparisons to
  regularized likelihood methods.
\newblock \emph{Advances in large margin classifiers}, 10\penalty0
  (3):\penalty0 61--74, 1999.

\bibitem[Ritter et~al.(2018)Ritter, Botev, and Barber]{ritter2018scalable}
Ritter, H., Botev, A., and Barber, D.
\newblock A scalable laplace approximation for neural networks.
\newblock 2018.

\bibitem[Robins et~al.(2008)Robins, Li, Tchetgen, van~der Vaart,
  et~al.]{robins2008higher}
Robins, J., Li, L., Tchetgen, E., van~der Vaart, A., et~al.
\newblock Higher order influence functions and minimax estimation of nonlinear
  functionals.
\newblock In \emph{Probability and statistics: essays in honor of David A.
  Freedman}, pp.\  335--421. Institute of Mathematical Statistics, 2008.

\bibitem[Schulam \& Saria(2019)Schulam and Saria]{schulam2019can}
Schulam, P. and Saria, S.
\newblock Can you trust this prediction? auditing pointwise reliability after
  learning.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  1022--1031, 2019.

\bibitem[Vovk et~al.(2018)Vovk, Nouretdinov, Manokhin, and
  Gammerman]{vovk2018cross}
Vovk, V., Nouretdinov, I., Manokhin, V., and Gammerman, A.
\newblock Cross-conformal predictive distributions.
\newblock In \emph{The Journal of Machine Learning Research (JMLR)}, pp.\
  37--51, 2018.

\bibitem[Wager \& Athey(2018)Wager and Athey]{wager2018estimation}
Wager, S. and Athey, S.
\newblock Estimation and inference of heterogeneous treatment effects using
  random forests.
\newblock \emph{Journal of the American Statistical Association}, 113\penalty0
  (523):\penalty0 1228--1242, 2018.

\bibitem[Wager et~al.(2014)Wager, Hastie, and Efron]{wager2014confidence}
Wager, S., Hastie, T., and Efron, B.
\newblock Confidence intervals for random forests: The jackknife and the
  infinitesimal jackknife.
\newblock \emph{The Journal of Machine Learning Research (JMLR)}, 15\penalty0
  (1):\penalty0 1625--1651, 2014.

\bibitem[Welling \& Teh(2011)Welling and Teh]{welling2011bayesian}
Welling, M. and Teh, Y.~W.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In \emph{Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pp.\  681--688, 2011.

\bibitem[White \& White(2010)White and White]{white2010interval}
White, M. and White, A.
\newblock Interval estimation for reinforcement-learning algorithms in
  continuous-state domains.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2433--2441, 2010.

\end{thebibliography}
