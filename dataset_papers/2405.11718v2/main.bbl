\begin{thebibliography}{75}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abel et~al.(2018)Abel, Arumugam, Lehnert, and Littman]{abel2018state}
Abel, D., Arumugam, D., Lehnert, L., and Littman, M.
\newblock State abstractions for lifelong reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  10--19. PMLR, 2018.

\bibitem[Achiam et~al.(2017)Achiam, Held, Tamar, and Abbeel]{achiam2017constrained}
Achiam, J., Held, D., Tamar, A., and Abbeel, P.
\newblock Constrained policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\  22--31. PMLR, 2017.

\bibitem[Altman(1999)]{altman1999constrained}
Altman, E.
\newblock \emph{Constrained Markov decision processes: stochastic modeling}.
\newblock Routledge, 1999.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong, Welinder, McGrew, Tobin, Pieter~Abbeel, and Zaremba]{andrychowicz2017hindsight}
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Pieter~Abbeel, O., and Zaremba, W.
\newblock Hindsight experience replay.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[As et~al.(2022)As, Usmanova, Curi, and Krause]{as2022constrained}
As, Y., Usmanova, I., Curi, S., and Krause, A.
\newblock Constrained policy optimization via bayesian world models.
\newblock \emph{arXiv preprint arXiv:2201.09802}, 2022.

\bibitem[Bansal et~al.(2017)Bansal, Chen, Herbert, and Tomlin]{bansal2017hamilton}
Bansal, S., Chen, M., Herbert, S., and Tomlin, C.~J.
\newblock Hamilton-jacobi reachability: A brief overview and recent advances.
\newblock In \emph{2017 IEEE 56th Annual Conference on Decision and Control (CDC)}, pp.\  2242--2253. IEEE, 2017.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and Munos]{bellemare2017distributional}
Bellemare, M.~G., Dabney, W., and Munos, R.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\  449--458. PMLR, 2017.

\bibitem[Bengio et~al.(2013)Bengio, Courville, and Vincent]{bengio2013representation}
Bengio, Y., Courville, A., and Vincent, P.
\newblock Representation learning: A review and new perspectives.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 35\penalty0 (8):\penalty0 1798--1828, 2013.

\bibitem[Berkenkamp et~al.(2017)Berkenkamp, Turchetta, Schoellig, and Krause]{berkenkamp2017safe}
Berkenkamp, F., Turchetta, M., Schoellig, A., and Krause, A.
\newblock Safe model-based reinforcement learning with stability guarantees.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Brunke et~al.(2021)Brunke, Greeff, Hall, Yuan, Zhou, Panerati, and Schoellig]{brunke2021safe}
Brunke, L., Greeff, M., Hall, A.~W., Yuan, Z., Zhou, S., Panerati, J., and Schoellig, A.~P.
\newblock Safe learning in robotics: From learning-based control to safe reinforcement learning.
\newblock \emph{Annual Review of Control, Robotics, and Autonomous Systems}, 5, 2021.

\bibitem[Cen et~al.(2024)Cen, Liu, Wang, Yao, Lam, and Zhao]{cen2024learning}
Cen, Z., Liu, Z., Wang, Z., Yao, Y., Lam, H., and Zhao, D.
\newblock Learning from sparse offline datasets via conservative density estimation.
\newblock \emph{arXiv preprint arXiv:2401.08819}, 2024.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G.
\newblock A simple framework for contrastive learning of visual representations.
\newblock In \emph{International conference on machine learning}, pp.\  1597--1607. PMLR, 2020.

\bibitem[Chen \& He(2021)Chen and He]{chen2021exploring}
Chen, X. and He, K.
\newblock Exploring simple siamese representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  15750--15758, 2021.

\bibitem[Chow et~al.(2018)Chow, Ghavamzadeh, Janson, and Pavone]{chow2018risk}
Chow, Y., Ghavamzadeh, M., Janson, L., and Pavone, M.
\newblock Risk-constrained reinforcement learning with percentile risk criteria.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0 (167):\penalty0 1--51, 2018.

\bibitem[Dalal et~al.(2018)Dalal, Dvijotham, Vecerik, Hester, Paduraru, and Tassa]{dalal2018safe}
Dalal, G., Dvijotham, K., Vecerik, M., Hester, T., Paduraru, C., and Tassa, Y.
\newblock Safe exploration in continuous action spaces.
\newblock \emph{arXiv preprint arXiv:1801.08757}, 2018.

\bibitem[Ding et~al.(2020)Ding, Zhang, Basar, and Jovanovic]{ding2020natural}
Ding, D., Zhang, K., Basar, T., and Jovanovic, M.
\newblock Natural policy gradient primal-dual method for constrained markov decision processes.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 8378--8390, 2020.

\bibitem[Ding et~al.(2023)Ding, Xu, Arief, Lin, Li, and Zhao]{ding2023survey}
Ding, W., Xu, C., Arief, M., Lin, H., Li, B., and Zhao, D.
\newblock A survey on safety-critical driving scenario generationâ€”a methodological perspective.
\newblock \emph{IEEE Transactions on Intelligent Transportation Systems}, 2023.

\bibitem[Eysenbach et~al.(2022)Eysenbach, Zhang, Levine, and Salakhutdinov]{eysenbach2022contrastive}
Eysenbach, B., Zhang, T., Levine, S., and Salakhutdinov, R.~R.
\newblock Contrastive learning as goal-conditioned reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 35603--35620, 2022.

\bibitem[Farahmand et~al.(2017)Farahmand, Barreto, and Nikovski]{farahmand2017value}
Farahmand, A.-m., Barreto, A., and Nikovski, D.
\newblock Value-aware loss function for model-based reinforcement learning.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1486--1494. PMLR, 2017.

\bibitem[Farquhar et~al.(2021)Farquhar, Baumli, Marinho, Filos, Hessel, van Hasselt, and Silver]{farquhar2021self}
Farquhar, G., Baumli, K., Marinho, Z., Filos, A., Hessel, M., van Hasselt, H.~P., and Silver, D.
\newblock Self-consistent models and values.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 1111--1125, 2021.

\bibitem[Finn et~al.(2016)Finn, Tan, Duan, Darrell, Levine, and Abbeel]{finn2016deep}
Finn, C., Tan, X.~Y., Duan, Y., Darrell, T., Levine, S., and Abbeel, P.
\newblock Deep spatial autoencoders for visuomotor learning.
\newblock In \emph{2016 IEEE International Conference on Robotics and Automation (ICRA)}, pp.\  512--519. IEEE, 2016.

\bibitem[Fisac et~al.(2019)Fisac, Lugovoy, Rubies-Royo, Ghosh, and Tomlin]{fisac2019bridging}
Fisac, J.~F., Lugovoy, N.~F., Rubies-Royo, V., Ghosh, S., and Tomlin, C.~J.
\newblock Bridging hamilton-jacobi safety analysis and reinforcement learning.
\newblock In \emph{2019 International Conference on Robotics and Automation (ICRA)}, pp.\  8550--8556. IEEE, 2019.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and Meger]{fujimoto2018addressing}
Fujimoto, S., Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International conference on machine learning}, pp.\  1587--1596. PMLR, 2018.

\bibitem[Fujimoto et~al.(2023)Fujimoto, Chang, Smith, Gu, Precup, and Meger]{fujimoto2023sale}
Fujimoto, S., Chang, W.-D., Smith, E.~J., Gu, S.~S., Precup, D., and Meger, D.
\newblock For sale: State-action representation learning for deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2306.02451}, 2023.

\bibitem[Garc{\i}a \& Fern{\'a}ndez(2015)Garc{\i}a and Fern{\'a}ndez]{garcia2015comprehensive}
Garc{\i}a, J. and Fern{\'a}ndez, F.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 16\penalty0 (1):\penalty0 1437--1480, 2015.

\bibitem[Gelada et~al.(2019)Gelada, Kumar, Buckman, Nachum, and Bellemare]{gelada2019deepmdp}
Gelada, C., Kumar, S., Buckman, J., Nachum, O., and Bellemare, M.~G.
\newblock Deepmdp: Learning continuous latent space models for representation learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  2170--2179. PMLR, 2019.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond, Buchatskaya, Doersch, Avila~Pires, Guo, Gheshlaghi~Azar, et~al.]{grill2020bootstrap}
Grill, J.-B., Strub, F., Altch{\'e}, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila~Pires, B., Guo, Z., Gheshlaghi~Azar, M., et~al.
\newblock Bootstrap your own latent-a new approach to self-supervised learning.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 21271--21284, 2020.

\bibitem[Grimm et~al.(2020)Grimm, Barreto, Singh, and Silver]{grimm2020value}
Grimm, C., Barreto, A., Singh, S., and Silver, D.
\newblock The value equivalence principle for model-based reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 5541--5552, 2020.

\bibitem[Gu et~al.(2022)Gu, Yang, Du, Chen, Walter, Wang, Yang, and Knoll]{gu2022review}
Gu, S., Yang, L., Du, Y., Chen, G., Walter, F., Wang, J., Yang, Y., and Knoll, A.
\newblock A review of safe reinforcement learning: Methods, theory and applications.
\newblock \emph{arXiv preprint arXiv:2205.10330}, 2022.

\bibitem[Hafner et~al.(2019)Hafner, Lillicrap, Fischer, Villegas, Ha, Lee, and Davidson]{hafner2019learning}
Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J.
\newblock Learning latent dynamics for planning from pixels.
\newblock In \emph{International conference on machine learning}, pp.\  2555--2565. PMLR, 2019.

\bibitem[Hafner et~al.(2020)Hafner, Lillicrap, Norouzi, and Ba]{hafner2020mastering}
Hafner, D., Lillicrap, T., Norouzi, M., and Ba, J.
\newblock Mastering atari with discrete world models.
\newblock \emph{arXiv preprint arXiv:2010.02193}, 2020.

\bibitem[Hare(2019)]{hare2019dealing}
Hare, J.
\newblock Dealing with sparse rewards in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.09281}, 2019.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2020momentum}
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  9729--9738, 2020.

\bibitem[Hessel et~al.(2018)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski, Dabney, Horgan, Piot, Azar, and Silver]{hessel2018rainbow}
Hessel, M., Modayil, J., Van~Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., and Silver, D.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~32, 2018.

\bibitem[Hessel et~al.(2021)Hessel, Danihelka, Viola, Guez, Schmitt, Sifre, Weber, Silver, and Van~Hasselt]{hessel2021muesli}
Hessel, M., Danihelka, I., Viola, F., Guez, A., Schmitt, S., Sifre, L., Weber, T., Silver, D., and Van~Hasselt, H.
\newblock Muesli: Combining improvements in policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\  4214--4226. PMLR, 2021.

\bibitem[Huang et~al.(2022)Huang, Abdolmaleki, Vezzani, Brakel, Mankowitz, Neunert, Bohez, Tassa, Heess, Riedmiller, et~al.]{huang2022constrained}
Huang, S., Abdolmaleki, A., Vezzani, G., Brakel, P., Mankowitz, D.~J., Neunert, M., Bohez, S., Tassa, Y., Heess, N., Riedmiller, M., et~al.
\newblock A constrained multi-objective reinforcement learning framework.
\newblock In \emph{Conference on Robot Learning}, pp.\  883--893. PMLR, 2022.

\bibitem[Huang et~al.(2023)Huang, Ji, Zhang, Xia, and Yang]{huang2023safe}
Huang, W., Ji, J., Zhang, B., Xia, C., and Yang, Y.
\newblock Safe dreamerv3: Safe reinforcement learning with world models.
\newblock \emph{arXiv preprint arXiv:2307.07176}, 2023.

\bibitem[Ji et~al.(2023)Ji, Zhang, Zhou, Pan, Huang, Sun, Geng, Zhong, Dai, and Yang]{ji2023safety}
Ji, J., Zhang, B., Zhou, J., Pan, X., Huang, W., Sun, R., Geng, Y., Zhong, Y., Dai, J., and Yang, Y.
\newblock Safety-gymnasium: A unified safe reinforcement learning benchmark.
\newblock \emph{arXiv preprint arXiv:2310.12567}, 2023.

\bibitem[Kaiser et~al.(2019)Kaiser, Babaeizadeh, Milos, Osinski, Campbell, Czechowski, Erhan, Finn, Kozakowski, Levine, et~al.]{kaiser2019model}
Kaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R.~H., Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., et~al.
\newblock Model-based reinforcement learning for atari.
\newblock \emph{arXiv preprint arXiv:1903.00374}, 2019.

\bibitem[Kiran et~al.(2021)Kiran, Sobh, Talpaert, Mannion, Al~Sallab, Yogamani, and P{\'e}rez]{kiran2021deep}
Kiran, B.~R., Sobh, I., Talpaert, V., Mannion, P., Al~Sallab, A.~A., Yogamani, S., and P{\'e}rez, P.
\newblock Deep reinforcement learning for autonomous driving: A survey.
\newblock \emph{IEEE Transactions on Intelligent Transportation Systems}, 23\penalty0 (6):\penalty0 4909--4926, 2021.

\bibitem[Kostrikov et~al.(2020)Kostrikov, Yarats, and Fergus]{kostrikov2020image}
Kostrikov, I., Yarats, D., and Fergus, R.
\newblock Image augmentation is all you need: Regularizing deep reinforcement learning from pixels.
\newblock \emph{arXiv preprint arXiv:2004.13649}, 2020.

\bibitem[Laskin et~al.(2020)Laskin, Srinivas, and Abbeel]{laskin2020curl}
Laskin, M., Srinivas, A., and Abbeel, P.
\newblock Curl: Contrastive unsupervised representations for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5639--5650. PMLR, 2020.

\bibitem[Lesort et~al.(2018)Lesort, D{\'\i}az-Rodr{\'\i}guez, Goudou, and Filliat]{lesort2018state}
Lesort, T., D{\'\i}az-Rodr{\'\i}guez, N., Goudou, J.-F., and Filliat, D.
\newblock State representation learning for control: An overview.
\newblock \emph{Neural Networks}, 108:\penalty0 379--392, 2018.

\bibitem[Li et~al.(2022)Li, Peng, Feng, Zhang, Xue, and Zhou]{li2022metadrive}
Li, Q., Peng, Z., Feng, L., Zhang, Q., Xue, Z., and Zhou, B.
\newblock Metadrive: Composing diverse driving scenarios for generalizable reinforcement learning.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 2022.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa, Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Lin et~al.(2024)Lin, Ding, Liu, Niu, Zhu, Niu, and Zhao]{lin2024safety}
Lin, H., Ding, W., Liu, Z., Niu, Y., Zhu, J., Niu, Y., and Zhao, D.
\newblock Safety-aware causal representation for trustworthy offline reinforcement learning in autonomous driving.
\newblock \emph{IEEE Robotics and Automation Letters}, 2024.

\bibitem[Liu et~al.(2021)Liu, Zhang, Zhao, Qin, Zhu, Li, Yu, and Liu]{liu2021return}
Liu, G., Zhang, C., Zhao, L., Qin, T., Zhu, J., Li, J., Yu, N., and Liu, T.-Y.
\newblock Return-based contrastive representation learning for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2102.10960}, 2021.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Zhu, and Zhang]{liu2022goal}
Liu, M., Zhu, M., and Zhang, W.
\newblock Goal-conditioned reinforcement learning: Problems and solutions.
\newblock \emph{arXiv preprint arXiv:2201.08299}, 2022{\natexlab{a}}.

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Cen, Isenbaev, Liu, Wu, Li, and Zhao]{liu2022constrained}
Liu, Z., Cen, Z., Isenbaev, V., Liu, W., Wu, S., Li, B., and Zhao, D.
\newblock Constrained variational policy optimization for safe reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  13644--13668. PMLR, 2022{\natexlab{b}}.

\bibitem[Liu et~al.(2022{\natexlab{c}})Liu, Guo, Cen, Zhang, Tan, Li, and Zhao]{liu2022robustness}
Liu, Z., Guo, Z., Cen, Z., Zhang, H., Tan, J., Li, B., and Zhao, D.
\newblock On the robustness of safe reinforcement learning under observational perturbations.
\newblock \emph{arXiv preprint arXiv:2205.14691}, 2022{\natexlab{c}}.

\bibitem[Liu et~al.(2023)Liu, Guo, Lin, Yao, Zhu, Cen, Hu, Yu, Zhang, Tan, et~al.]{liu2023datasets}
Liu, Z., Guo, Z., Lin, H., Yao, Y., Zhu, J., Cen, Z., Hu, H., Yu, W., Zhang, T., Tan, J., et~al.
\newblock Datasets and benchmarks for offline safe reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2306.09303}, 2023.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare, M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Oh et~al.(2017)Oh, Singh, and Lee]{oh2017value}
Oh, J., Singh, S., and Lee, H.
\newblock Value prediction network.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Oord, A. v.~d., Li, Y., and Vinyals, O.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Ray et~al.(2019)Ray, Achiam, and Amodei]{ray2019benchmarking}
Ray, A., Achiam, J., and Amodei, D.
\newblock Benchmarking safe exploration in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.01708}, 7, 2019.

\bibitem[Riedmiller et~al.(2018)Riedmiller, Hafner, Lampe, Neunert, Degrave, Wiele, Mnih, Heess, and Springenberg]{riedmiller2018learning}
Riedmiller, M., Hafner, R., Lampe, T., Neunert, M., Degrave, J., Wiele, T., Mnih, V., Heess, N., and Springenberg, J.~T.
\newblock Learning by playing solving sparse reward tasks from scratch.
\newblock In \emph{International conference on machine learning}, pp.\  4344--4353. PMLR, 2018.

\bibitem[Schrittwieser et~al.(2020)Schrittwieser, Antonoglou, Hubert, Simonyan, Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel, et~al.]{schrittwieser2020mastering}
Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., et~al.
\newblock Mastering atari, go, chess and shogi by planning with a learned model.
\newblock \emph{Nature}, 588\penalty0 (7839):\penalty0 604--609, 2020.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Schwarzer et~al.(2020)Schwarzer, Anand, Goel, Hjelm, Courville, and Bachman]{schwarzer2020data}
Schwarzer, M., Anand, A., Goel, R., Hjelm, R.~D., Courville, A., and Bachman, P.
\newblock Data-efficient reinforcement learning with self-predictive representations.
\newblock \emph{arXiv preprint arXiv:2007.05929}, 2020.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Sootla et~al.(2022)Sootla, Cowen-Rivers, Jafferjee, Wang, Mguni, Wang, and Ammar]{sootla2022saute}
Sootla, A., Cowen-Rivers, A.~I., Jafferjee, T., Wang, Z., Mguni, D.~H., Wang, J., and Ammar, H.
\newblock Saut{\'e} rl: Almost surely safe reinforcement learning using state augmentation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  20423--20443. PMLR, 2022.

\bibitem[Stooke et~al.(2020)Stooke, Achiam, and Abbeel]{stooke2020responsive}
Stooke, A., Achiam, J., and Abbeel, P.
\newblock Responsive safety in reinforcement learning by pid lagrangian methods.
\newblock In \emph{International Conference on Machine Learning}, pp.\  9133--9143. PMLR, 2020.

\bibitem[Sui et~al.(2015)Sui, Gotovos, Burdick, and Krause]{sui2015safe}
Sui, Y., Gotovos, A., Burdick, J., and Krause, A.
\newblock Safe exploration for optimization with gaussian processes.
\newblock In \emph{International conference on machine learning}, pp.\  997--1005. PMLR, 2015.

\bibitem[Tessler et~al.(2018)Tessler, Mankowitz, and Mannor]{tessler2018reward}
Tessler, C., Mankowitz, D.~J., and Mannor, S.
\newblock Reward constrained policy optimization.
\newblock \emph{arXiv preprint arXiv:1805.11074}, 2018.

\bibitem[Wachi \& Sui(2020)Wachi and Sui]{wachi2020safe}
Wachi, A. and Sui, Y.
\newblock Safe reinforcement learning in constrained markov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pp.\  9797--9806. PMLR, 2020.

\bibitem[Wachi et~al.(2018)Wachi, Sui, Yue, and Ono]{wachi2018safe}
Wachi, A., Sui, Y., Yue, Y., and Ono, M.
\newblock Safe exploration and optimization of constrained mdps using gaussian processes.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~32, 2018.

\bibitem[Wang et~al.(2023)Wang, Zhan, Jiao, Wang, Jin, Yang, Wang, Huang, and Zhu]{wang2023enforcing}
Wang, Y., Zhan, S.~S., Jiao, R., Wang, Z., Jin, W., Yang, Z., Wang, Z., Huang, C., and Zhu, Q.
\newblock Enforcing hard constraints with soft barriers: Safe reinforcement learning in unknown stochastic environments.
\newblock In \emph{International Conference on Machine Learning}, pp.\  36593--36604. PMLR, 2023.

\bibitem[Xu et~al.(2022)Xu, Liu, Huang, Ding, Cen, Li, and Zhao]{xu2022trustworthy}
Xu, M., Liu, Z., Huang, P., Ding, W., Cen, Z., Li, B., and Zhao, D.
\newblock Trustworthy reinforcement learning against intrinsic vulnerabilities: Robustness, safety, and generalizability.
\newblock \emph{arXiv preprint arXiv:2209.08025}, 2022.

\bibitem[Yang \& Nachum(2021)Yang and Nachum]{yang2021representation}
Yang, M. and Nachum, O.
\newblock Representation matters: Offline pretraining for sequential decision making.
\newblock In \emph{International Conference on Machine Learning}, pp.\  11784--11794. PMLR, 2021.

\bibitem[Yang et~al.(2020)Yang, Rosca, Narasimhan, and Ramadge]{yang2020projection}
Yang, T.-Y., Rosca, J., Narasimhan, K., and Ramadge, P.~J.
\newblock Projection-based constrained policy optimization.
\newblock \emph{arXiv preprint arXiv:2010.03152}, 2020.

\bibitem[Yao et~al.(2023)Yao, Liu, Cen, Zhu, Yu, Zhang, and Zhao]{yao2023constraint}
Yao, Y., Liu, Z., Cen, Z., Zhu, J., Yu, W., Zhang, T., and Zhao, D.
\newblock Constraint-conditioned policy optimization for versatile safe reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2310.03718}, 2023.

\bibitem[Ye et~al.(2021)Ye, Liu, Kurutach, Abbeel, and Gao]{ye2021mastering}
Ye, W., Liu, S., Kurutach, T., Abbeel, P., and Gao, Y.
\newblock Mastering atari games with limited data.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 25476--25488, 2021.

\bibitem[Yu et~al.(2022)Yu, Ma, Li, and Chen]{yu2022reachability}
Yu, D., Ma, H., Li, S., and Chen, J.
\newblock Reachability constrained reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  25636--25655. PMLR, 2022.

\bibitem[Yue et~al.(2023)Yue, Kang, Xu, Huang, and Yan]{yue2023value}
Yue, Y., Kang, B., Xu, Z., Huang, G., and Yan, S.
\newblock Value-consistent representation learning for data-efficient reinforcement learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pp.\  11069--11077, 2023.

\bibitem[Zhang et~al.(2020)Zhang, Vuong, and Ross]{zhang2020first}
Zhang, Y., Vuong, Q., and Ross, K.
\newblock First order constrained optimization in policy space.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 15338--15349, 2020.

\end{thebibliography}
