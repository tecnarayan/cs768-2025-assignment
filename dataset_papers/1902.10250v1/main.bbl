\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{pmlr-v70-arjovsky17a}
Arjovsky, M., Chintala, S., and Bottou, L.
\newblock {W}asserstein generative adversarial networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  214--223, 2017.

\bibitem[Baird(1995)]{Baird1995}
Baird, L.
\newblock {Residual Algorithms : Reinforcement Learning with Function
  Approximation}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 1995.

\bibitem[Bertsekas \& Tsitsiklis(1996)Bertsekas and Tsitsiklis]{Bertsekas96}
Bertsekas, D.~P. and Tsitsiklis, J.~N.
\newblock Neuro-dynamic programming.
\newblock \emph{Athena Scientific}, 1996.

\bibitem[Dai et~al.(2018)Dai, Shaw, Li, Xiao, He, Liu, Chen, and Song]{Dai2018}
Dai, B., Shaw, A., Li, L., Xiao, L., He, N., Liu, Z., Chen, J., and Song, L.
\newblock Sbeed: Convergent reinforcement learning with nonlinear function
  approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1133--1142, 2018.

\bibitem[Daskalakis et~al.(2018)Daskalakis, Ilyas, Syrgkanis, and
  Zeng]{daskalakis2018training}
Daskalakis, C., Ilyas, A., Syrgkanis, V., and Zeng, H.
\newblock Training {GAN}s with optimism.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=SJJySbbAZ}.

\bibitem[Ernst et~al.(2005)Ernst, Geurts, and Wehenkel]{Ernst05}
Ernst, D., Geurts, P., and Wehenkel, L.
\newblock Tree-based batch mode reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (Apr):\penalty0 503--556, 2005.

\bibitem[Farrell \& Berger(1995)Farrell and Berger]{Farrell95}
Farrell, J.~A. and Berger, T.
\newblock On the effects of the training sample density in passive learning
  control.
\newblock In \emph{American Control Conference}, 1995.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and
  Meger]{pmlr-v80-fujimoto18a}
Fujimoto, S., van Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  1587--1596, 2018.

\bibitem[Geist et~al.(2017)Geist, Piot, and Pietquin]{NIPS2017_6913}
Geist, M., Piot, B., and Pietquin, O.
\newblock Is the bellman residual a bad proxy?
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  3205--3214. 2017.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and Levine]{Haarnoja2017}
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{Haarnoja18}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock \emph{CoRR}, abs/1801.01290, 2018.
\newblock URL \url{http://arxiv.org/abs/1801.01290}.

\bibitem[Hausknecht \& Stone(2016)Hausknecht and Stone]{hausknecht2016policy}
Hausknecht, M. and Stone, P.
\newblock On-policy vs. off-policy updates for deep reinforcement learning.
\newblock In \emph{Deep Reinforcement Learning: Frontiers and Challenges,
  IJCAI}, 2016.

\bibitem[Hazan et~al.(2018)Hazan, Kakade, Singh, and Van~Soest]{hazan2018}
Hazan, E., Kakade, S.~M., Singh, K., and Van~Soest, A.
\newblock Provably efficient maximum entropy exploration.
\newblock \emph{arXiv preprint arXiv:1812.02690}, 2018.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, and Levine]{kalashnikov18}
Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E.,
  Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., and Levine, S.
\newblock Qt-opt: Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock In \emph{CoRL}, volume~87 of \emph{Proceedings of Machine Learning
  Research}, pp.\  651--673. {PMLR}, 2018.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{Lillicrap2015}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock {Continuous control with deep reinforcement learning}.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2015.

\bibitem[Lin(1992)]{lin1992replay}
Lin, L.-J.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 293--321, 1992.

\bibitem[Liu et~al.(2018)Liu, Kumaraswamy, Le, and White]{martha2018sparse}
Liu, V., Kumaraswamy, R., Le, L., and White, M.
\newblock The utility of sparse representations for control in reinforcement
  learning.
\newblock \emph{CoRR}, abs/1811.06626, 2018.
\newblock URL \url{http://arxiv.org/abs/1811.06626}.

\bibitem[Maei et~al.(2010)Maei, Szepesv{\'{a}}ri, Bhatnagar, and
  Sutton]{Maei2010}
Maei, H.~R., Szepesv{\'{a}}ri, C., Bhatnagar, S., and Sutton, R.~S.
\newblock {Toward off-policy learning control with function approximation}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2010.

\bibitem[Maillard et~al.(2010)Maillard, Munos, Lazaric, and
  Ghavamzadeh]{maillard2010finite}
Maillard, O.-A., Munos, R., Lazaric, A., and Ghavamzadeh, M.
\newblock Finite-sample analysis of bellman residual minimization.
\newblock In \emph{Asian Conference on Machine Learning (ACML)}, pp.\
  299--314, 2010.

\bibitem[Metelli et~al.(2018)Metelli, Papini, Faccio, and
  Restelli]{metelli2018nips}
Metelli, A.~M., Papini, M., Faccio, F., and Restelli, M.
\newblock Policy optimization via importance sampling.
\newblock \emph{CoRR}, abs/1809.06098, 2018.
\newblock URL \url{http://arxiv.org/abs/1809.06098}.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{Mnih2015}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., Petersen,
  S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra,
  D., Legg, S., and Hassabis, D.
\newblock {Human-level control through deep reinforcement learning}.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, feb 2015.
\newblock ISSN 0028-0836.

\bibitem[Munos(2005)]{munos2005erroravi}
Munos, R.
\newblock Error bounds for approximate value iteration.
\newblock In \emph{AAI Conference on Artificial intelligence (AAAI)}, pp.\
  1006--1011. AAAI Press, 2005.

\bibitem[Munos \& Szepesv{\'a}ri(2008)Munos and
  Szepesv{\'a}ri]{munos2008finite}
Munos, R. and Szepesv{\'a}ri, C.
\newblock Finite-time bounds for fitted value iteration.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0
  (May):\penalty0 815--857, 2008.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{munos2016safe}
Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  1054--1062, 2016.

\bibitem[Plappert et~al.(2018)Plappert, Andrychowicz, Ray, McGrew, Baker,
  Powell, Schneider, Tobin, Chociej, Welinder, Kumar, and Zaremba]{gym}
Plappert, M., Andrychowicz, M., Ray, A., McGrew, B., Baker, B., Powell, G.,
  Schneider, J., Tobin, J., Chociej, M., Welinder, P., Kumar, V., and Zaremba,
  W.
\newblock Multi-goal reinforcement learning: Challenging robotics environments
  and request for research, 2018.

\bibitem[Precup et~al.(2001)Precup, Sutton, and Dasgupta]{precup2001offpol}
Precup, D., Sutton, R.~S., and Dasgupta, S.
\newblock Off-policy temporal difference learning with function approximation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  417--424, 2001.

\bibitem[Riedmiller(2005)]{Riedmiller2005}
Riedmiller, M.
\newblock Neural fitted q iteration--first experiences with a data efficient
  neural reinforcement learning method.
\newblock In \emph{European Conference on Machine Learning}, pp.\  317--328.
  Springer, 2005.

\bibitem[Schaul et~al.(2015)Schaul, Quan, Antonoglou, and Silver]{Schaul2015}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2015.

\bibitem[Scherrer(2010)]{scherrer2010residual}
Scherrer, B.
\newblock Should one compute the temporal difference fix point or minimize the
  bellman residual? the unified oblique projection view.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  959--966, 2010.

\bibitem[Shalev-Shwartz \& Ben-David(2014)Shalev-Shwartz and
  Ben-David]{Shalev2014}
Shalev-Shwartz, S. and Ben-David, S.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Shani et~al.(2005)Shani, Heckerman, and Brafman]{shani2005recommender}
Shani, G., Heckerman, D., and Brafman, R.~I.
\newblock An mdp-based recommender system.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (Sep):\penalty0 1265--1295, 2005.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{suttonrlbook}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock Second edition, 2018.

\bibitem[Sutton et~al.(2009{\natexlab{a}})Sutton, Maei, Precup, Bhatnagar,
  Silver, Szepesv\'{a}ri, and Wiewiora]{Sutton09b}
Sutton, R.~S., Maei, H.~R., Precup, D., Bhatnagar, S., Silver, D.,
  Szepesv\'{a}ri, C., and Wiewiora, E.
\newblock Fast gradient-descent methods for temporal-difference learning with
  linear function approximation.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2009{\natexlab{a}}.

\bibitem[Sutton et~al.(2009{\natexlab{b}})Sutton, Maei, and
  Szepesv\'{a}ri]{Sutton09a}
Sutton, R.~S., Maei, H.~R., and Szepesv\'{a}ri, C.
\newblock A convergent o(n) temporal-difference algorithm for off-policy
  learning with linear function approximation.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2009{\natexlab{b}}.

\bibitem[Sutton et~al.(2016)Sutton, Mahmood, and White]{Sutton2016}
Sutton, R.~S., Mahmood, A.~R., and White, M.
\newblock An emphatic approach to the problem of off-policy temporal-difference
  learning.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2603--2631, 2016.

\bibitem[Szepesv{\'a}ri(1998)]{szepesvari1998asymptotic}
Szepesv{\'a}ri, C.
\newblock The asymptotic convergence-rate of q-learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1064--1070, 1998.

\bibitem[Tosatto et~al.(2017)Tosatto, Pirotta, D'Eramo, and
  Restelli]{tosatto2017boosted}
Tosatto, S., Pirotta, M., D'Eramo, C., and Restelli, M.
\newblock Boosted fitted q-iteration.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  3434--3443. JMLR. org, 2017.

\bibitem[Tsitsiklis \& Van~Roy(1997)Tsitsiklis and Van~Roy]{Tsitsiklis1997}
Tsitsiklis, J.~N. and Van~Roy, B.
\newblock Analysis of temporal-diffference learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  1075--1081, 1997.

\bibitem[Tuomas~Haarnoja \& Levine(2018)Tuomas~Haarnoja and
  Levine]{haarnoja2018sacapps}
Tuomas~Haarnoja, Aurick~Zhou, K. H. G. T. S. H. J. T. V. K. H. Z. A. G. P.~A.
  and Levine, S.
\newblock Soft actor-critic algorithms and applications.
\newblock Technical report, 2018.

\bibitem[Van~Hasselt et~al.(2018)Van~Hasselt, Doron, Strub, Hessel, Sonnerat,
  and Modayil]{VanHesselt2018}
Van~Hasselt, H., Doron, Y., Strub, F., Hessel, M., Sonnerat, N., and Modayil,
  J.
\newblock Deep reinforcement learning and the deadly triad.
\newblock \emph{arXiv preprint arXiv:1812.02648}, 2018.

\bibitem[Watkins \& Dayan(1992)Watkins and Dayan]{Watkins1992}
Watkins, C.~J. and Dayan, P.
\newblock Q-learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 279--292, 1992.

\bibitem[Yaz{\i}c{\i} et~al.(2019)Yaz{\i}c{\i}, Foo, Winkler, Yap, Piliouras,
  and Chandrasekhar]{yaz2018the}
Yaz{\i}c{\i}, Y., Foo, C.-S., Winkler, S., Yap, K.-H., Piliouras, G., and
  Chandrasekhar, V.
\newblock The unusual effectiveness of averaging in {GAN} training.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=SJgw_sRqFQ}.

\bibitem[Zhang \& Sutton(2017)Zhang and Sutton]{zhang2018deeper}
Zhang, S. and Sutton, R.~S.
\newblock A deeper look at experience replay.
\newblock \emph{CoRR}, abs/1712.01275, 2017.
\newblock URL \url{http://arxiv.org/abs/1712.01275}.

\end{thebibliography}
