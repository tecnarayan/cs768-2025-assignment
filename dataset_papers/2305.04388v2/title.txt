Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting