\begin{thebibliography}{10}

\bibitem{openai2024gpt4}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock {GPT-4} technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
\newblock {Flamingo}: a visual language model for few-shot learning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{claude3}
Anthropic.
\newblock The claude 3 model family: {Opus, Sonnet, Haiku}, March 2024.

\bibitem{antol2015vqa}
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C~Lawrence Zitnick, and Devi Parikh.
\newblock {VQA}: Visual question answering.
\newblock In {\em Proceedings of the IEEE international conference on computer vision}, pages 2425--2433, 2015.

\bibitem{bai2023qwenvl}
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.
\newblock {Qwen-VL}: A versatile vision-language model for understanding, localization, text reading, and beyond.
\newblock {\em arXiv preprint arXiv:2308.12966}, 2023.

\bibitem{fuyu-8b}
Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sa\u{g}nak Ta\c{s}\i{}rlar.
\newblock {Fuyu-8B}: A multimodal architecture for ai agents, 2023.

\bibitem{chen2023pali}
Xi~Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos~Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi~Tay, et~al.
\newblock {PaLI-X}: On scaling up a multilingual vision and language model.
\newblock {\em arXiv preprint arXiv:2305.18565}, 2023.

\bibitem{chen2022pali}
Xi~Chen, Xiao Wang, Soravit Changpinyo, AJ~Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish~V Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu~Karagol Ayan, Carlos~Riquelme Ruiz, Andreas~Peter Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut.
\newblock Pa{LI}: A jointly-scaled multilingual language-image model.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{chen2024far}
Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et~al.
\newblock How far are we to {GPT-4V}? closing the gap to commercial multimodal models with open-source suites.
\newblock {\em arXiv preprint arXiv:2404.16821}, 2024.

\bibitem{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023.

\bibitem{dai2023instructblip}
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.
\newblock Instruct{BLIP}: Towards general-purpose vision-language models with instruction tuning.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem{chen2023internvl}
Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu~Qiao, Dahua Lin, and Jiaqi Wang.
\newblock {InternLM-XComposer2}: Mastering free-form text-image composition and comprehension in vision-language large model.
\newblock {\em arXiv preprint arXiv:2401.16420}, 2024.

\bibitem{dong2024internlmxcomposer24khd}
Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Zhe Chen, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Kai Chen, Conghui He, Xingcheng Zhang, Jifeng Dai, Yu~Qiao, Dahua Lin, and Jiaqi Wang.
\newblock {InternLM-XComposer2-4KHD}: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd.
\newblock {\em arXiv preprint arXiv:2404.06512}, 2024.

\bibitem{dubois2024alpacafarm}
Yann Dubois, Chen~Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy~S Liang, and Tatsunori~B Hashimoto.
\newblock Alpacafarm: A simulation framework for methods that learn from human feedback.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{fu2024mme}
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu~Lin, Jinrui Yang, Xiawu Zheng, Ke~Li, Xing Sun, et~al.
\newblock {MME}: A comprehensive evaluation benchmark for multimodal large language models.
\newblock {\em arXiv preprint arXiv:2306.13394}, 2023.

\bibitem{gao2024sphinxx}
Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, et~al.
\newblock {SPHINX-X}: Scaling data and parameters for a family of multi-modal large language models.
\newblock {\em arXiv preprint arXiv:2402.05935}, 2024.

\bibitem{gao2024improving}
Tianyu Gao, Zirui Wang, Adithya Bhaskar, and Danqi Chen.
\newblock Improving language understanding from screenshots.
\newblock {\em arXiv preprint arXiv:2402.14073}, 2024.

\bibitem{gebru2021datasheets}
Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer~Wortman Vaughan, Hanna Wallach, Hal~Daum\'{e} III, and Kate Crawford.
\newblock Datasheets for datasets.
\newblock {\em Commun. ACM}, 64(12):86–92, nov 2021.

\bibitem{goyal2017making}
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
\newblock {Making the V in VQA Matter}: Elevating the role of image understanding in visual question answering.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 6904--6913, 2017.

\bibitem{hsu2021scicap}
Ting-Yao Hsu, C~Lee Giles, and Ting-Hao Huang.
\newblock {S}ci{C}ap: Generating captions for scientific figures.
\newblock In {\em Findings of the Association for Computational Linguistics: EMNLP 2021}, pages 3258--3264, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.

\bibitem{viscpm}
Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu~Han, Yankai Lin, Jiao Xue, Dahai Li, Zhiyuan Liu, and Maosong Sun.
\newblock Large multilingual models pivot zero-shot multimodal learning across languages.
\newblock {\em arXiv preprint arXiv:2308.12038}, 2023.

\bibitem{huang2023language}
Shaohan Huang, Li~Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais~Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei.
\newblock {Language Is Not All You Need}: Aligning perception with language models.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem{hudson2019gqa}
Drew~A Hudson and Christopher~D Manning.
\newblock {GQA}: A new dataset for real-world visual reasoning and compositional question answering.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 6700--6709, 2019.

\bibitem{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock {\em arXiv preprint arXiv:2310.06825}, 2023.

\bibitem{kafle2018dvqa}
Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan.
\newblock {DVQA}: Understanding data visualizations via question answering.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 5648--5656, 2018.

\bibitem{kahou2017figureqa}
Samira~Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, {\'A}kos K{\'a}d{\'a}r, Adam Trischler, and Yoshua Bengio.
\newblock {FigureQA}: An annotated figure dataset for visual reasoning.
\newblock {\em arXiv preprint arXiv:1710.07300}, 2017.

\bibitem{kembhavi2016diagram}
Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi.
\newblock A diagram is worth a dozen images.
\newblock In {\em Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV 14}, pages 235--251. Springer, 2016.

\bibitem{laurençon2023obelics}
Hugo Lauren{\c{c}}on, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander~M Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh.
\newblock {OBELICS}: An open web-scale filtered dataset of interleaved image-text documents.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2023.

\bibitem{laurenccon2024matters}
Hugo Lauren{\c{c}}on, L{\'e}o Tronchon, Matthieu Cord, and Victor Sanh.
\newblock What matters when building vision-language models?
\newblock {\em arXiv preprint arXiv:2405.02246}, 2024.

\bibitem{lee2024moai}
Byung-Kwan Lee, Beomchan Park, Chae~Won Kim, and Yong~Man Ro.
\newblock {MoAI}: Mixture of all intelligence for large language and vision models.
\newblock {\em arXiv preprint arXiv:2403.07508}, 2024.

\bibitem{li2023mimic}
Bo~Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu.
\newblock Mimic-it: Multi-modal in-context instruction tuning.
\newblock {\em arXiv preprint arXiv:2306.05425}, 2023.

\bibitem{li2023blip}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock {BLIP-2}: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock In {\em International conference on machine learning}, pages 19730--19742. PMLR, 2023.

\bibitem{li2022blip}
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
\newblock {BLIP}: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
\newblock In {\em International conference on machine learning}, pages 12888--12900. PMLR, 2022.

\bibitem{li2024multimodal}
Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi~Liu.
\newblock {Multimodal ArXiv}: A dataset for improving scientific comprehension of large vision-language models.
\newblock {\em arXiv preprint arXiv:2403.00231}, 2024.

\bibitem{li2023scigraphqa}
Shengzhi Li and Nima Tajbakhsh.
\newblock {SciGraphQA}: A large-scale synthetic multi-turn question-answering dataset for scientific graphs.
\newblock {\em arXiv preprint arXiv:2308.03349}, 2023.

\bibitem{li2024multiplechoice}
Wangyue Li, Liangzhi Li, Tong Xiang, Xiao Liu, Wei Deng, and Noa Garcia.
\newblock Can multiple-choice questions really be useful in detecting the abilities of {LLM}s?
\newblock In {\em Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)}, pages 2819--2834, Torino, Italia, May 2024. ELRA and ICCL.

\bibitem{li2024minigemini}
Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia.
\newblock {Mini-Gemini}: Mining the potential of multi-modality vision language models.
\newblock {\em arXiv:2403.18814}, 2024.

\bibitem{lin2023sphinx}
Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et~al.
\newblock {SPHINX}: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models.
\newblock {\em arXiv preprint arXiv:2311.07575}, 2023.

\bibitem{liu2024mmc}
Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu.
\newblock {MMC}: Advancing multimodal chart understanding with large-scale instruction tuning.
\newblock In {\em Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pages 1287--1310, Mexico City, Mexico, June 2024. Association for Computational Linguistics.

\bibitem{liu2023improvedllava}
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee.
\newblock Improved baselines with visual instruction tuning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 26296--26306, June 2024.

\bibitem{liu2024llavanext}
Haotian Liu, Chunyuan Li, Yuheng Li, Bo~Li, Yuanhan Zhang, Sheng Shen, and Yong~Jae Lee.
\newblock {LLaVA-NeXT}: Improved reasoning, ocr, and world knowledge, January 2024.

\bibitem{liu2023llava}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock In {\em NeurIPS}, 2023.

\bibitem{liu2024mmbench}
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo~Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et~al.
\newblock {MMBench}: Is your multi-modal model an all-around player?
\newblock {\em arXiv preprint arXiv:2307.06281}, 2023.

\bibitem{lu2024deepseekvl}
Haoyu Lu, Wen Liu, Bo~Zhang, Bingxuan Wang, Kai Dong, Bo~Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et~al.
\newblock {DeepSeek-VL}: Towards real-world vision-language understanding.
\newblock {\em arXiv preprint arXiv:2403.05525}, 2024.

\bibitem{lu2024mathvista}
Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao.
\newblock {MathVista}: Evaluating mathematical reasoning of foundation models in visual contexts.
\newblock In {\em International Conference on Learning Representations (ICLR)}, 2024.

\bibitem{lu2022learn}
Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.
\newblock {Learn to Explain}: Multimodal reasoning via thought chains for science question answering.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{marino2019ok}
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
\newblock {OK-VQA}: A visual question answering benchmark requiring external knowledge.
\newblock In {\em Proceedings of the IEEE/cvf conference on computer vision and pattern recognition}, pages 3195--3204, 2019.

\bibitem{masry2022chartqa}
Ahmed Masry, Xuan~Long Do, Jia~Qing Tan, Shafiq Joty, and Enamul Hoque.
\newblock {ChartQA}: A benchmark for question answering about charts with visual and logical reasoning.
\newblock In {\em Findings of the Association for Computational Linguistics: ACL 2022}, pages 2263--2279, 2022.

\bibitem{mathew2022infographicvqa}
Minesh Mathew, Viraj Bagal, Rub{\`e}n Tito, Dimosthenis Karatzas, Ernest Valveny, and CV~Jawahar.
\newblock {InfographicVQA}.
\newblock In {\em Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pages 1697--1706, 2022.

\bibitem{mathew2021docvqa}
Minesh Mathew, Dimosthenis Karatzas, and CV~Jawahar.
\newblock {DocVQA}: A dataset for vqa on document images.
\newblock In {\em Proceedings of the IEEE/CVF winter conference on applications of computer vision}, pages 2200--2209, 2021.

\bibitem{methani2020plotqa}
Nitesh Methani, Pritha Ganguly, Mitesh~M. Khapra, and Pratyush Kumar.
\newblock {PlotQA}: Reasoning over scientific plots.
\newblock In {\em The IEEE Winter Conference on Applications of Computer Vision (WACV)}, March 2020.

\bibitem{rekateam2024reka}
Aitor Ormazabal, Che Zheng, Cyprien de~Masson d'Autume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac Ong, et~al.
\newblock {Reka Core, Flash, and Edge}: A series of powerful multimodal language models.
\newblock {\em arXiv preprint arXiv:2404.12387}, 2024.

\bibitem{rajpurkar2018know}
Pranav Rajpurkar, Robin Jia, and Percy Liang.
\newblock Know what you don{'}t know: Unanswerable questions for {SQ}u{AD}.
\newblock In {\em Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pages 784--789, Melbourne, Australia, July 2018. Association for Computational Linguistics.

\bibitem{singh2019towards}
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu~Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.
\newblock Towards vqa models that can read.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 8317--8326, 2019.

\bibitem{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock {Gemini}: a family of highly capable multimodal models.
\newblock {\em arXiv preprint arXiv:2312.11805}, 2023.

\bibitem{LabelStudio}
Maxim Tkachenko, Mikhail Malyuk, Andrey Holmanyuk, and Nikolai Liubimov.
\newblock {Label Studio}: Data labeling software, 2020-2022.
\newblock Open source software available from https://github.com/heartexlabs/label-studio.

\bibitem{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{wang2023cogvlm}
Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji~Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et~al.
\newblock {CogVLM}: Visual expert for pretrained language models.
\newblock {\em arXiv preprint arXiv:2311.03079}, 2023.

\bibitem{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M. Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed~H. Chi, Quoc~V Le, and Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language models.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{xia2024chartx}
Renqiu Xia, Bo~Zhang, Hancheng Ye, Xiangchao Yan, Qi~Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi Yan, et~al.
\newblock {ChartX} \& {ChartVLM}: A versatile benchmark and foundation model for complicated chart reasoning.
\newblock {\em arXiv preprint arXiv:2402.12185}, 2024.

\bibitem{xu2024chartbench}
Zhengzhuo Xu, Sinan Du, Yiyan Qi, Chengjin Xu, Chun Yuan, and Jian Guo.
\newblock {ChartBench}: A benchmark for complex visual reasoning in charts.
\newblock {\em arXiv preprint arXiv:2312.15915}, 2023.

\bibitem{ye2023mplug}
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et~al.
\newblock {mPLUG-Owl}: Modularization empowers large language models with multimodality.
\newblock {\em arXiv preprint arXiv:2304.14178}, 2023.

\bibitem{young2024yi}
Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge~Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et~al.
\newblock Yi: Open foundation models by 01. ai.
\newblock {\em arXiv preprint arXiv:2403.04652}, 2024.

\bibitem{yu2023mm}
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.
\newblock {MM-Vet}: Evaluating large multimodal models for integrated capabilities.
\newblock {\em arXiv preprint arXiv:2308.02490}, 2023.

\bibitem{yue2023mmmu}
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge~Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu~Su, and Wenhu Chen.
\newblock {MMMU}: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 9556--9567, June 2024.

\bibitem{zhai2023sigmoid}
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.
\newblock Sigmoid loss for language image pre-training.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 11975--11986, 2023.

\bibitem{zhang2023llama}
Renrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu, Yu~Qiao, Hongsheng Li, and Peng Gao.
\newblock {LL}a{MA}-adapter: Efficient fine-tuning of large language models with zero-initialized attention.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{zhang2023multimodal}
Zhuosheng Zhang, Aston Zhang, Mu~Li, hai zhao, George Karypis, and Alex Smola.
\newblock Multimodal chain-of-thought reasoning in language models.
\newblock {\em Transactions on Machine Learning Research}, 2024.

\bibitem{zhou2023webarena}
Shuyan Zhou, Frank~F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig.
\newblock {WebArena}: A realistic web environment for building autonomous agents.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{zhu2023minigpt}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
\newblock {MiniGPT-4}: Enhancing vision-language understanding with advanced large language models.
\newblock {\em arXiv preprint arXiv:2304.10592}, 2023.

\end{thebibliography}
