\begin{thebibliography}{10}

\bibitem{arjevani2023lower}
Yossi Arjevani, Yair Carmon, John~C Duchi, Dylan~J Foster, Nathan Srebro, and Blake Woodworth.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock {\em Mathematical Programming}, 199(1-2):165--214, 2023.

\bibitem{attia2023sgd}
Amit Attia and Tomer Koren.
\newblock {SGD} with {AdaGrad} stepsizes: full adaptivity with high probability to unknown parameters, unbounded gradients and affine variance.
\newblock In {\em International Conference on Machine Learning}, 2023.

\bibitem{bertsekas2000gradient}
Dimitri~P Bertsekas and John~N Tsitsiklis.
\newblock Gradient convergence in gradient methods with errors.
\newblock {\em SIAM Journal on Optimization}, 10(3):627--642, 2000.

\bibitem{bottou2018optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock {\em SIAM Review}, 60(2):223--311, 2018.

\bibitem{cesa2004generalization}
Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile.
\newblock On the generalization ability of on-line learning algorithms.
\newblock {\em IEEE Transactions on Information Theory}, 50(9):2050--2057, 2004.

\bibitem{chen2018closing}
Jinghui Chen, Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu.
\newblock Closing the generalization gap of adaptive gradient methods in training deep neural networks.
\newblock {\em arXiv preprint arXiv:1806.06763}, 2018.

\bibitem{chen2018convergence}
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
\newblock On the convergence of a class of {Adam}-type algorithms for non-convex optimization.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{crawshaw2022robustness}
Michael Crawshaw, Mingrui Liu, Francesco Orabona, Wei Zhang, and Zhenxun Zhuang.
\newblock Robustness to unbounded smoothness of generalized {signSGD}.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{de2018convergence}
Soham De, Anirbit Mukherjee, and Enayat Ullah.
\newblock Convergence guarantees for {RMSP}rop and {Adam} in non-convex optimization and an empirical comparison to {N}esterov acceleration.
\newblock {\em arXiv preprint arXiv:1807.06766}, 2018.

\bibitem{defossez2020simple}
Alexandre D{\'e}fossez, Leon Bottou, Francis Bach, and Nicolas Usunier.
\newblock A simple convergence proof of {Adam} and {Adagrad}.
\newblock {\em Transactions on Machine Learning Research}, 2022.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic optimization.
\newblock {\em Journal of Machine Learning Research}, 12(7):2121--2159, 2011.

\bibitem{faw2023beyond}
Matthew Faw, Litu Rout, Constantine Caramanis, and Sanjay Shakkottai.
\newblock Beyond uniform smoothness: a stopped analysis of adaptive {SGD}.
\newblock In {\em Conference on Learning Theory}, 2023.

\bibitem{faw2022power}
Matthew Faw, Isidoros Tziotis, Constantine Caramanis, Aryan Mokhtari, Sanjay Shakkottai, and Rachel Ward.
\newblock The power of adaptivity in {SGD}: self-tuning step sizes with unbounded gradients and affine variance.
\newblock In {\em Conference on Learning Theory}, 2022.

\bibitem{fuller2009measurement}
Wayne~A Fuller.
\newblock {\em Measurement error models}.
\newblock John Wiley \& Sons, 2009.

\bibitem{ghadimi2015global}
Euhanna Ghadimi, Hamid~Reza Feyzmahdavian, and Mikael Johansson.
\newblock Global convergence of the heavy-ball method for convex optimization.
\newblock In {\em European Control Conference}, 2015.

\bibitem{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{guo2021novel}
Zhishuai Guo, Yi~Xu, Wotao Yin, Rong Jin, and Tianbao Yang.
\newblock A novel convergence analysis for algorithms of the {Adam} family.
\newblock In {\em Annual Workshop on Optimization for Machine Learning}, 2021.

\bibitem{hongrevisiting}
Yusu Hong and Junhong Lin.
\newblock Revisiting convergence of adagrad with relaxed assumptions.
\newblock In {\em The 40th Conference on Uncertainty in Artificial Intelligence}.

\bibitem{yusu2023}
Yusu Hong and Junhong Lin.
\newblock High probability convergence of {Adam} under unbounded gradients and affine variance noise.
\newblock {\em arXiv preprint arXiv:2311.02000}, 2023.

\bibitem{huang2021super}
Feihu Huang, Junyi Li, and Heng Huang.
\newblock Super-{Adam}: faster and universal framework of adaptive gradients.
\newblock In {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{khani2020feature}
Fereshte Khani and Percy Liang.
\newblock Feature noise induces loss discrepancy across groups.
\newblock In {\em International Conference on Machine Learning}, pages 5209--5219. PMLR, 2020.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: a method for stochastic optimization.
\newblock In {\em International Conference on Learning Representations}, 2015.

\bibitem{kunstner2023noise}
Frederik Kunstner, Jacques Chen, Jonathan~Wilder Lavington, and Mark Schmidt.
\newblock Noise is not the main factor behind the gap between {SGD} and {Adam} on {Transformers}, but sign descent might be.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{rakhlinconvergence}
Haochuan Li, Ali Jadbabaie, and Alexander Rakhlin.
\newblock Convergence of {Adam} under relaxed assumptions.
\newblock In {\em Advances in Neural Information Processing Systems}, 2023.

\bibitem{li2020high}
Xiaoyu Li and Francesco Orabona.
\newblock A high probability analysis of adaptive {SGD} with momentum.
\newblock In {\em Workshop on International Conference on Machine Learning}, 2020.

\bibitem{liu2019towards}
Mingrui Liu, Youssef Mroueh, Jerret Ross, Wei Zhang, Xiaodong Cui, Payel Das, and Tianbao Yang.
\newblock Towards better understanding of adaptive gradient algorithms in generative adversarial nets.
\newblock {\em arXiv preprint arXiv:1912.11940}, 2019.

\bibitem{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{nemirovski2009robust}
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock {\em SIAM Journal on Optimization}, 19(4):1574--1609, 2009.

\bibitem{polyak1964some}
Boris~T Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock {\em Ussr Computational Mathematics and Mathematical Physics}, 4(5):1--17, 1964.

\bibitem{qian2021understanding}
Jiang Qian, Yuren Wu, Bojin Zhuang, Shaojun Wang, and Jing Xiao.
\newblock Understanding gradient clipping in incremental gradient methods.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, 2021.

\bibitem{reddi2019convergence}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of {Adam} and beyond.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{reisizadeh2023variance}
Amirhossein Reisizadeh, Haochuan Li, Subhro Das, and Ali Jadbabaie.
\newblock Variance-reduced clipping for non-convex optimization.
\newblock {\em arXiv preprint arXiv:2303.00883}, 2023.

\bibitem{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock {\em Annals of Mathematical Statistics}, pages 400--407, 1951.

\bibitem{shi2020rmsprop}
Naichen Shi, Dawei Li, Mingyi Hong, and Ruoyu Sun.
\newblock {RMSProp} converges with proper hyper-parameter.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{smale2006online}
Steve Smale and Yuan Yao.
\newblock Online learning algorithms.
\newblock {\em Foundations of Computational Mathematics}, 6:145--170, 2006.

\bibitem{streeter2010lesson}
Matthew Streeter and H~Brendan McMahan.
\newblock Less regret via online conditioning.
\newblock {\em arXiv preprint arXiv:1002.4862}, 2010.

\bibitem{tieleman2012lecture}
Tijmen Tieleman and Geoffrey Hinton.
\newblock Lecture 6.5-{RMSProp}: Divide the gradient by a running average of its recent magnitude.
\newblock {\em COURSERA: Neural Networks for Machine Learning}, 2012.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in Neural Information Processing Systems}, 2017.

\bibitem{wang2023closing}
Bohan Wang, Jingwen Fu, Huishuai Zhang, Nanning Zheng, and Wei Chen.
\newblock Closing the gap between the upper bound and lower bound of {Adam's} iteration complexity.
\newblock In {\em Advances in Neural Information Processing Systems}, 2023.

\bibitem{wang2023convergence}
Bohan Wang, Huishuai Zhang, Zhiming Ma, and Wei Chen.
\newblock Convergence of {AdaGrad} for non-convex objectives: simple proofs and relaxed assumptions.
\newblock In {\em Conference on Learning Theory}, 2023.

\bibitem{wang2022provable}
Bohan Wang, Yushun Zhang, Huishuai Zhang, Qi~Meng, Zhi-Ming Ma, Tie-Yan Liu, and Wei Chen.
\newblock Provable adaptivity in {A}dam.
\newblock {\em arXiv preprint arXiv:2208.09900}, 2022.

\bibitem{wang2022divergence}
Ruiqi Wang and Diego Klabjan.
\newblock Divergence results and convergence of a variance reduced version of adam.
\newblock {\em arXiv preprint arXiv:2210.05607}, 2022.

\bibitem{ward2020adagrad}
Rachel Ward, Xiaoxia Wu, and Leon Bottou.
\newblock Adagrad stepsizes: sharp convergence over nonconvex landscapes.
\newblock {\em Journal of Machine Learning Research}, 21(1):9047--9076, 2020.

\bibitem{xie2024implicit}
Shuo Xie and Zhiyuan Li.
\newblock Implicit bias of adamw: $l_\infty$-norm constrained optimization.
\newblock In {\em International Conference on Machine Learning}, 2024.

\bibitem{xu2008robust}
Huan Xu, Constantine Caramanis, and Shie Mannor.
\newblock Robust regression and {Lasso}.
\newblock In {\em Advances in Neural Information Processing Systems}, 2008.

\bibitem{yang2018unified}
Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi~Yang.
\newblock A unified analysis of stochastic momentum methods for deep learning.
\newblock In {\em Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence}, 2018.

\bibitem{ying2006online}
Yiming Ying and D-X Zhou.
\newblock Online regularized classification algorithms.
\newblock {\em IEEE Transactions on Information Theory}, 52(11):4775--4788, 2006.

\bibitem{zaheer2018adaptive}
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, 2018.

\bibitem{zhang2020improved}
Bohang Zhang, Jikai Jin, Cong Fang, and Liwei Wang.
\newblock Improved analysis of clipping algorithms for non-convex optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{zhang2024the}
Chenyang Zhang, Difan Zou, and Yuan Cao.
\newblock The implicit bias of {Adam} on separable data.
\newblock In {\em High-dimensional Learning Dynamics 2024: The Emergence of Structure and Reasoning}, 2024.

\bibitem{zhang2020why}
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie.
\newblock Why gradient clipping accelerates training: a theoretical justification for adaptivity.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{zhang2022adam}
Yushun Zhang, Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhi-Quan Luo.
\newblock Adam can converge without any modification on update rules.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{zhao2021convergence}
Shen-Yi Zhao, Yin-Peng Xie, and Wu-Jun Li.
\newblock On the convergence and improvement of stochastic normalized gradient descent.
\newblock {\em Science China Information Sciences}, 64:1--13, 2021.

\bibitem{zhou2018convergence}
Dongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex optimization.
\newblock In {\em Annual Workshop on Optimization for Machine Learning}, 2020.

\bibitem{zhou2023win}
Pan Zhou, Xingyu Xie, and Shuicheng Yan.
\newblock Win: weight-decay-integrated {N}esterov acceleration for adaptive gradient algorithms.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{zou2021understanding}
Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu.
\newblock Understanding the generalization of adam in learning neural networks with proper regularization.
\newblock {\em arXiv preprint arXiv:2108.11371}, 2021.

\bibitem{zou2019sufficient}
Fangyu Zou, Li~Shen, Zequn Jie, Weizhong Zhang, and Wei Liu.
\newblock A sufficient condition for convergences of {A}dam and {RMSP}rop.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 2019.

\end{thebibliography}
