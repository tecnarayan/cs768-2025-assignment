\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abel et~al.(2021)Abel, Dabney, Harutyunyan, Ho, Littman, Precup, and
  Singh]{abel2021expressivity}
David Abel, Will Dabney, Anna Harutyunyan, Mark~K Ho, Michael Littman, Doina
  Precup, and Satinder Singh.
\newblock On the expressivity of {Markov} reward.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, 2021.

\bibitem[Agrawal and Goyal(2012)]{agrawal2012analysis}
Shipra Agrawal and Navin Goyal.
\newblock Analysis of thompson sampling for the multi-armed bandit problem.
\newblock In \emph{Conference on learning theory}, pages 39--1. JMLR Workshop
  and Conference Proceedings, 2012.

\bibitem[Aigner(1996)]{aigner1996searching}
Martin Aigner.
\newblock Searching with lies.
\newblock \emph{Journal of Combinatorial Theory, Series A}, 74\penalty0
  (1):\penalty0 43--56, 1996.

\bibitem[Akrour et~al.(2011)Akrour, Schoenauer, and
  Sebag]{akrour2011preference}
Riad Akrour, Marc Schoenauer, and Michele Sebag.
\newblock Preference-based policy learning.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 12--27. Springer, 2011.

\bibitem[Akrour et~al.(2012)Akrour, Schoenauer, and Sebag]{akrour2012april}
Riad Akrour, Marc Schoenauer, and Mich{\`e}le Sebag.
\newblock {APRIL}: Active preference learning-based reinforcement learning.
\newblock In \emph{Joint European conference on machine learning and knowledge
  discovery in databases}, pages 116--131. Springer, 2012.

\bibitem[Allen et~al.(2019)Allen, Shelhamer, Shin, and
  Tenenbaum]{allen2019infinite}
Kelsey Allen, Evan Shelhamer, Hanul Shin, and Joshua Tenenbaum.
\newblock Infinite mixture prototypes for few-shot learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  232--241. PMLR, 2019.

\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and
  Man{\'e}]{amodei2016concrete}
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and
  Dan Man{\'e}.
\newblock Concrete problems in ai safety.
\newblock \emph{arXiv preprint arXiv:1606.06565}, 2016.

\bibitem[Berlekamp(1968)]{berlekamp1968block}
Elwyn~R Berlekamp.
\newblock Block coding for the binary symmetric channel with noiseless,
  delayless feedback.
\newblock \emph{Error-correcting codes}, pages 61--68, 1968.

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, D{\k{e}}biak,
  Dennison, Farhi, Fischer, Hashme, Hesse, et~al.]{berner2019dota}
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys{\l}aw
  D{\k{e}}biak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme,
  Chris Hesse, et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.06680}, 2019.

\bibitem[Bradley and Terry(1952)]{bradley1952rank}
Ralph~Allan Bradley and Milton~E Terry.
\newblock Rank analysis of incomplete block designs: I. the method of paired
  comparisons.
\newblock \emph{Biometrika}, 39\penalty0 (3/4):\penalty0 324--345, 1952.

\bibitem[Brunskill and Li(2014)]{brunskill2014pac}
Emma Brunskill and Lihong Li.
\newblock Pac-inspired option discovery in lifelong reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages
  316--324. PMLR, 2014.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
  Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30, 2017.

\bibitem[Chua et~al.(2021)Chua, Lei, and Lee]{chua2021provable}
Kurtland Chua, Qi~Lei, and Jason~D Lee.
\newblock Provable hierarchy-based meta-reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2110.09507}, 2021.

\bibitem[Cicalese and Deppe(2007)]{cicalese2007perfect}
Ferdinando Cicalese and Christian Deppe.
\newblock Perfect minimally adaptive q-ary search with unreliable tests.
\newblock \emph{Journal of statistical planning and inference}, 137\penalty0
  (1):\penalty0 162--175, 2007.

\bibitem[Duan et~al.(2016)Duan, Schulman, Chen, Bartlett, Sutskever, and
  Abbeel]{duan2016rl}
Yan Duan, John Schulman, Xi~Chen, Peter~L Bartlett, Ilya Sutskever, and Pieter
  Abbeel.
\newblock {RL}{$^2$}: Fast reinforcement learning via slow reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1611.02779}, 2016.

\bibitem[Eysenbach et~al.(2019)Eysenbach, Gupta, Ibarz, and
  Levine]{eysenbach2019diversity}
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine.
\newblock Diversity is all you need: Learning skills without a reward function.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Fakoor et~al.(2020)Fakoor, Chaudhari, Soatto, and
  Smola]{fakoor2020meta}
Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, and Alexander~J Smola.
\newblock Meta-q-learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International conference on machine learning}, pages
  1126--1135. PMLR, 2017.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International Conference on Machine Learning}, pages
  2052--2062. PMLR, 2019.

\bibitem[F{\"u}rnkranz et~al.(2012)F{\"u}rnkranz, H{\"u}llermeier, Cheng, and
  Park]{furnkranz2012preference}
Johannes F{\"u}rnkranz, Eyke H{\"u}llermeier, Weiwei Cheng, and Sang-Hyeun
  Park.
\newblock Preference-based reinforcement learning: a formal framework and a
  policy iteration algorithm.
\newblock \emph{Machine learning}, 89\penalty0 (1):\penalty0 123--156, 2012.

\bibitem[Gordon et~al.(2019)Gordon, Bronskill, Bauer, Nowozin, and
  Turner]{gordon2019meta}
Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and Richard
  Turner.
\newblock Meta-learning probabilistic inference for prediction.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Guan et~al.(2021)Guan, Verma, Guo, Zhang, and
  Kambhampati]{guan2021widening}
Lin Guan, Mudit Verma, Suna~Sihang Guo, Ruohan Zhang, and Subbarao Kambhampati.
\newblock Widening the pipeline in human-guided reinforcement learning with
  explanation and context-aware data augmentation.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 21885--21897, 2021.

\bibitem[Gupta et~al.(2020)Gupta, Eysenbach, Finn, and
  Levine]{gupta2020unsupervised}
Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, and Sergey Levine.
\newblock Unsupervised meta-learning for reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pages
  1861--1870. PMLR, 2018.

\bibitem[Hausman et~al.(2018)Hausman, Springenberg, Wang, Heess, and
  Riedmiller]{hausman2018learning}
Karol Hausman, Jost~Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin
  Riedmiller.
\newblock Learning an embedding space for transferable robot skills.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Hua et~al.(2021)Hua, Wang, Jin, Li, Yan, He, and Zha]{hua2021hmrl}
Yun Hua, Xiangfeng Wang, Bo~Jin, Wenhao Li, Junchi Yan, Xiaofeng He, and
  Hongyuan Zha.
\newblock Hmrl: Hyper-meta learning for sparse reward reinforcement learning
  problem.
\newblock In \emph{Proceedings of the 27th ACM SIGKDD Conference on Knowledge
  Discovery \& Data Mining}, pages 637--645, 2021.

\bibitem[Ibarz et~al.(2018)Ibarz, Leike, Pohlen, Irving, Legg, and
  Amodei]{ibarz2018reward}
Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario
  Amodei.
\newblock Reward learning from human preferences and demonstrations in atari.
\newblock In \emph{Advances in neural information processing systems},
  volume~31, 2018.

\bibitem[Kingma and Ba(2015)]{kingma2015adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Lawler and Sarkissian(1995)]{lawler1995algorithm}
Eugene~L Lawler and Sergei Sarkissian.
\newblock An algorithm for “{Ulam}'s game” and its application to error
  correcting codes.
\newblock \emph{Information processing letters}, 56\penalty0 (2):\penalty0
  89--93, 1995.

\bibitem[Lee et~al.(2021{\natexlab{a}})Lee, Smith, Dragan, and
  Abbeel]{lee2021b}
Kimin Lee, Laura Smith, Anca Dragan, and Pieter Abbeel.
\newblock B-pref: Benchmarking preference-based reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems Datasets
  and Benchmarks Track}, 2021{\natexlab{a}}.

\bibitem[Lee et~al.(2021{\natexlab{b}})Lee, Smith, and Abbeel]{lee2021pebble}
Kimin Lee, Laura~M Smith, and Pieter Abbeel.
\newblock {PEBBLE}: Feedback-efficient interactive reinforcement learning via
  relabeling experience and unsupervised pre-training.
\newblock In \emph{International Conference on Machine Learning}, pages
  6152--6163. PMLR, 2021{\natexlab{b}}.

\bibitem[Liang et~al.(2022)Liang, Shu, Lee, and Abbeel]{liang2022reward}
Xinran Liang, Katherine Shu, Kimin Lee, and Pieter Abbeel.
\newblock Reward uncertainty for exploration in preference-based reinforcement
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Liu et~al.(2021)Liu, Raghunathan, Liang, and Finn]{liu2021decoupling}
Evan~Z Liu, Aditi Raghunathan, Percy Liang, and Chelsea Finn.
\newblock Decoupling exploration and exploitation for meta-reinforcement
  learning without sacrifices.
\newblock In \emph{International Conference on Machine Learning}, pages
  6925--6935. PMLR, 2021.

\bibitem[Liu et~al.(2020)Liu, Raghunathan, Liang, and Finn]{liu2020explore}
Evan~Zheran Liu, Aditi Raghunathan, Percy Liang, and Chelsea Finn.
\newblock Explore then execute: Adapting without rewards via factorized
  meta-reinforcement learning.
\newblock 2020.

\bibitem[Loewenstein and Prelec(1992)]{loewenstein1992anomalies}
George Loewenstein and Drazen Prelec.
\newblock Anomalies in intertemporal choice: Evidence and an interpretation.
\newblock \emph{The Quarterly Journal of Economics}, 107\penalty0 (2):\penalty0
  573--597, 1992.

\bibitem[Lynch et~al.(2020)Lynch, Khansari, Xiao, Kumar, Tompson, Levine, and
  Sermanet]{lynch2020learning}
Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey
  Levine, and Pierre Sermanet.
\newblock Learning latent plans from play.
\newblock In \emph{Conference on robot learning}, pages 1113--1132. PMLR, 2020.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{osband2013more}
Ian Osband, Daniel Russo, and Benjamin Van~Roy.
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Park et~al.(2022)Park, Seo, Shin, Lee, Abbeel, and Lee]{park2022surf}
Jongjin Park, Younggyo Seo, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin
  Lee.
\newblock {SURF}: Semi-supervised reward learning with data augmentation for
  feedback-efficient preference-based reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Pelc(1987)]{pelc1987solution}
Andrzej Pelc.
\newblock Solution of {Ulam}'s problem on searching with a lie.
\newblock \emph{Journal of Combinatorial Theory, Series A}, 44\penalty0
  (1):\penalty0 129--140, 1987.

\bibitem[Pelc(2002)]{pelc2002searching}
Andrzej Pelc.
\newblock Searching games with errors—fifty years of coping with liars.
\newblock \emph{Theoretical Computer Science}, 270\penalty0 (1-2):\penalty0
  71--109, 2002.

\bibitem[Pong et~al.(2020)Pong, Dalal, Lin, Nair, Bahl, and
  Levine]{pong2020skew}
Vitchyr Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey
  Levine.
\newblock Skew-fit: State-covering self-supervised reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  7783--7792. PMLR, 2020.

\bibitem[Prewett et~al.(2010)Prewett, Johnson, Saboe, Elliott, and
  Coovert]{prewett2010managing}
Matthew~S Prewett, Ryan~C Johnson, Kristin~N Saboe, Linda~R Elliott, and
  Michael~D Coovert.
\newblock Managing workload in human--robot interaction: A review of empirical
  studies.
\newblock \emph{Computers in Human Behavior}, 26\penalty0 (5):\penalty0
  840--856, 2010.

\bibitem[Rakelly et~al.(2019)Rakelly, Zhou, Quillen, Finn, and
  Levine]{rakelly2019efficient}
Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine.
\newblock Efficient off-policy meta-reinforcement learning via probabilistic
  context variables.
\newblock In \emph{International conference on machine learning}, pages
  5331--5340. PMLR, 2019.

\bibitem[Ren et~al.(2022)Ren, Guo, Zhou, and Peng]{ren2022learning}
Zhizhou Ren, Ruihan Guo, Yuan Zhou, and Jian Peng.
\newblock Learning long-term reward redistribution via randomized return
  decomposition.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[R{\'e}nyi(1961)]{renyi1961problem}
Alfr{\'e}d R{\'e}nyi.
\newblock On a problem of information theory.
\newblock \emph{MTA Mat. Kut. Int. Kozl. B}, 6\penalty0 (MR143666):\penalty0
  505--516, 1961.

\bibitem[R{\'e}nyi and Makkai-Bencs{\'a}th(1984)]{renyi1984diary}
Alfr{\'e}d R{\'e}nyi and Zsuzsanna Makkai-Bencs{\'a}th.
\newblock \emph{A diary on information theory}.
\newblock Akad{\'e}miai Kiad{\'o} Budapest, 1984.

\bibitem[Rivest et~al.(1980)Rivest, Meyer, Kleitman, Winklmann, and
  Spencer]{rivest1980coping}
Ronald~L. Rivest, Albert~R. Meyer, Daniel~J. Kleitman, Karl Winklmann, and Joel
  Spencer.
\newblock Coping with errors in binary search procedures.
\newblock \emph{Journal of Computer and System Sciences}, 20\penalty0
  (3):\penalty0 396--404, 1980.

\bibitem[Rothfuss et~al.(2019)Rothfuss, Lee, Clavera, Asfour, and
  Abbeel]{rothfuss2019promp}
Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel.
\newblock {ProMP}: Proximal meta-policy search.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Russo and Van~Roy(2014)]{russo2014learning}
Daniel Russo and Benjamin Van~Roy.
\newblock Learning to optimize via posterior sampling.
\newblock \emph{Mathematics of Operations Research}, 39\penalty0 (4):\penalty0
  1221--1243, 2014.

\bibitem[Rusu et~al.(2019)Rusu, Rao, Sygnowski, Vinyals, Pascanu, Osindero, and
  Hadsell]{rusu2019meta}
Andrei~A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu,
  Simon Osindero, and Raia Hadsell.
\newblock Meta-learning with latent embedding optimization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Shannon(1948)]{shannon1948mathematical}
Claude Shannon.
\newblock A mathematical theory of communication.
\newblock \emph{The Bell system technical journal}, 27\penalty0 (3):\penalty0
  379--423, 1948.

\bibitem[Shannon(1956)]{shannon1956zero}
Claude Shannon.
\newblock The zero error capacity of a noisy channel.
\newblock \emph{IRE Transactions on Information Theory}, 2\penalty0
  (3):\penalty0 8--19, 1956.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{snell2017prototypical}
Jake Snell, Kevin Swersky, and Richard Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock In \emph{Advances in neural information processing systems},
  volume~30, 2017.

\bibitem[Sorg et~al.(2010)Sorg, Lewis, and Singh]{sorg2010reward}
Jonathan Sorg, Richard~L Lewis, and Satinder Singh.
\newblock Reward design via online gradient ascent.
\newblock \emph{Advances in Neural Information Processing Systems}, 23, 2010.

\bibitem[Strens(2000)]{strens2000bayesian}
Malcolm~JA Strens.
\newblock A bayesian framework for reinforcement learning.
\newblock In \emph{Proceedings of the Seventeenth International Conference on
  Machine Learning}, pages 943--950, 2000.

\bibitem[Thompson(1933)]{thompson1933likelihood}
William~R Thompson.
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock \emph{Biometrika}, 25\penalty0 (3-4):\penalty0 285--294, 1933.

\bibitem[Ulam(1976)]{ulam1976adventures}
Stanislaw~M Ulam.
\newblock Adventures of a mathematician.
\newblock \emph{New York: Scribner}, 1976.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, Wierstra,
  et~al.]{vinyals2016matching}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et~al.
\newblock Matching networks for one shot learning.
\newblock In \emph{Advances in neural information processing systems},
  volume~29, 2016.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Oriol Vinyals, Igor Babuschkin, Wojciech~M Czarnecki, Micha{\"e}l Mathieu,
  Andrew Dudzik, Junyoung Chung, David~H Choi, Richard Powell, Timo Ewalds,
  Petko Georgiev, et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Wang et~al.(2021)Wang, Jin, Huang, Zhao, Lian, Liu, and
  Chen]{wang2021preference}
Li~Wang, Binbin Jin, Zhenya Huang, Hongke Zhao, Defu Lian, Qi~Liu, and Enhong
  Chen.
\newblock Preference-adaptive meta-learning for cold-start recommendation.
\newblock In \emph{Proceedings of the International Joint Conference on
  Artificial Intelligence (IJCAI)}, pages 1607--1614, 2021.

\bibitem[Wang et~al.(2020)Wang, Cai, Yang, and Wang]{wang2020global}
Lingxiao Wang, Qi~Cai, Zhuoran Yang, and Zhaoran Wang.
\newblock On the global optimality of model-agnostic meta-learning.
\newblock In \emph{International conference on machine learning}, pages
  9837--9846. PMLR, 2020.

\bibitem[Wilson et~al.(2012)Wilson, Fern, and Tadepalli]{wilson2012bayesian}
Aaron Wilson, Alan Fern, and Prasad Tadepalli.
\newblock A bayesian approach for policy learning from trajectory preference
  queries.
\newblock \emph{Advances in neural information processing systems}, 25, 2012.

\bibitem[Wirth et~al.(2017)Wirth, Akrour, Neumann, and
  F{\"u}rnkranz]{wirth2017survey}
Christian Wirth, Riad Akrour, Gerhard Neumann, and Johannes F{\"u}rnkranz.
\newblock A survey of preference-based reinforcement learning methods.
\newblock \emph{Journal of Machine Learning Research}, 18:\penalty0 1--46,
  2017.

\bibitem[Yu et~al.(2021)Yu, Gong, He, Zhu, Liu, Ou, and An]{yu2021personalized}
Runsheng Yu, Yu~Gong, Xu~He, Yu~Zhu, Qingwen Liu, Wenwu Ou, and Bo~An.
\newblock Personalized adaptive meta learning for cold-start user preference
  prediction.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 10772--10780, 2021.

\bibitem[Zhang et~al.(2021)Zhang, Wang, Hu, Chen, Chen, Fan, and
  Zhang]{zhang2021metacure}
Jin Zhang, Jianhao Wang, Hao Hu, Tong Chen, Yingfeng Chen, Changjie Fan, and
  Chongjie Zhang.
\newblock Metacure: Meta reinforcement learning with empowerment-driven
  exploration.
\newblock In \emph{International Conference on Machine Learning}, pages
  12600--12610. PMLR, 2021.

\bibitem[Zhang et~al.(2020)Zhang, Walshe, Liu, Guan, Muller, Whritner, Zhang,
  Hayhoe, and Ballard]{zhang2020atari}
Ruohan Zhang, Calen Walshe, Zhuode Liu, Lin Guan, Karl Muller, Jake Whritner,
  Luxin Zhang, Mary Hayhoe, and Dana Ballard.
\newblock Atari-head: Atari human eye-tracking and demonstration dataset.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~34, pages 6811--6820, 2020.

\bibitem[Zintgraf et~al.(2020)Zintgraf, Shiarlis, Igl, Schulze, Gal, Hofmann,
  and Whiteson]{zintgraf2020varibad}
Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin
  Gal, Katja Hofmann, and Shimon Whiteson.
\newblock {VariBAD}: A very good method for bayes-adaptive deep rl via
  meta-learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\end{thebibliography}
