@inproceedings{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  booktitle={ICLR},
  year={2014}
}

@inproceedings{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  booktitle={ICLR},
  year={2015}
}

@inproceedings{madry2018towards,
  title={Towards Deep Learning Models Resistant to Adversarial Attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  booktitle={ICLR},
  year={2018}
}

@inproceedings{tsipras2018robustness,
  title={Robustness may be at odds with accuracy},
  author={Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{newsome2006paragraph,
  title={Paragraph: Thwarting signature learning by training maliciously},
  author={Newsome, James and Karp, Brad and Song, Dawn},
  booktitle={International Workshop on Recent Advances in Intrusion Detection},
  year={2006},
}

@inproceedings{feng2019learning,
  title={Learning to confuse: generating training time adversarial data with auto-encoder},
  author={Feng, Ji and Cai, Qi-Zhi and Zhou, Zhi-Hua},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{huang2021unlearnable,
  title={Unlearnable Examples: Making Personal Data Unexploitable},
  author={Huang, Hanxun and Ma, Xingjun and Erfani, Sarah Monazam and Bailey, James and Wang, Yisen},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{tao2021provable,
  title={Better Safe Than Sorry: Preventing Delusive Adversaries with Adversarial Training},
  author={Tao, Lue and Feng, Lei and Yi, Jinfeng and Huang, Sheng-Jun and Chen, Songcan},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{fowl2021adversarial,
  title={Adversarial Examples Make Strong Poisons},
  author={Fowl, Liam and Goldblum, Micah and Chiang, Ping-yeh and Geiping, Jonas and Czaja, Wojtek and Goldstein, Tom},
 booktitle = {NeurIPS},
  year={2021}
}

@inproceedings{tramer2020fundamental,
  title={Fundamental tradeoffs between invariance and sensitivity to adversarial perturbations},
  author={Tram{\`e}r, Florian and Behrmann, Jens and Carlini, Nicholas and Papernot, Nicolas and Jacobsen, J{\"o}rn-Henrik},
  booktitle={ICML},
  year={2020},
}

@inproceedings{croce2021robustbench,
title={RobustBench: a standardized adversarial robustness benchmark},
author={Francesco Croce and Maksym Andriushchenko and Vikash Sehwag and Edoardo Debenedetti and Nicolas Flammarion and Mung Chiang and Prateek Mittal and Matthias Hein},
booktitle={NeurIPS Datasets and Benchmarks Track},
year={2021},
url={https://openreview.net/forum?id=SSKZPJCt7B}
}

@article{gowal2020uncovering,
  title={Uncovering the limits of adversarial training against norm-bounded adversarial examples},
  author={Gowal, Sven and Qin, Chongli and Uesato, Jonathan and Mann, Timothy and Kohli, Pushmeet},
  journal={arXiv preprint arXiv:2010.03593},
  year={2020}
}

@inproceedings{schmidt2018adversarially,
  title={Adversarially Robust Generalization Requires More Data},
  author={Schmidt, Ludwig and Santurkar, Shibani and Tsipras, Dimitris and Talwar, Kunal and Madry, Aleksander},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{zhang2020attacks,
  title={Attacks which do not kill training make adversarial learning stronger},
  author={Zhang, Jingfeng and Xu, Xilie and Han, Bo and Niu, Gang and Cui, Lizhen and Sugiyama, Masashi and Kankanhalli, Mohan},
  booktitle={ICML},
  year={2020},
}

@article{shaeiri2020towards,
  title={Towards deep learning models resistant to large perturbations},
  author={Shaeiri, Amirreza and Nobahari, Rozhin and Rohban, Mohammad Hossein},
  journal={arXiv preprint arXiv:2003.13370},
  year={2020}
}

@inproceedings{gowal2021improving,
  title={Improving robustness using generated data},
  author={Gowal, Sven and Rebuffi, Sylvestre-Alvise and Wiles, Olivia and Stimberg, Florian and Calian, Dan Andrei and Mann, Timothy A},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{athalye2018obfuscated,
  title={Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples},
  author={Athalye, Anish and Carlini, Nicholas and Wagner, David},
  booktitle={ICML},
  year={2018},
}

@inproceedings{bojarski2016end,
  title={End to end learning for self-driving cars},
  author={Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and others},
  booktitle={NeurIPS Deep Learning Symposium},
  year={2016}
}

@inproceedings{dalvi2004adversarial,
  title={Adversarial classification},
  author={Dalvi, Nilesh and Domingos, Pedro and Sanghai, Sumit and Verma, Deepak},
  booktitle={KDD},
  year={2004}
}

@article{goldblum2020dataset,
  title={Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses},
  author={Goldblum, Micah and Tsipras, Dimitris and Xie, Chulin and Chen, Xinyun and Schwarzschild, Avi and Song, Dawn and Madry, Aleksander and Li, Bo and Goldstein, Tom},
  journal={arXiv preprint arXiv:2012.10544},
  year={2020}
}

@article{biggio2018wild,
  title={Wild patterns: Ten years after the rise of adversarial machine learning},
  author={Biggio, Battista and Roli, Fabio},
  journal={Pattern Recognition},
  volume={84},
  pages={317--331},
  year={2018},
  publisher={Elsevier}
}

@article{yu2021indiscriminate,
  title={Indiscriminate Poisoning Attacks Are Shortcuts},
  author={Yu, Da and Zhang, Huishuai and Chen, Wei and Yin, Jian and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2111.00898},
  year={2021}
}

@article{nakkiran2019a,
  author = {Nakkiran, Preetum},
  title = {A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features': Adversarial Examples are Just Bugs, Too},
  journal = {Distill},
  year = {2019},
  note = {https://distill.pub/2019/advex-bugs-discussion/response-5},
  doi = {10.23915/distill.00019.5}
}

@inproceedings{biggio2013evasion,
  title={Evasion attacks against machine learning at test time},
  author={Biggio, Battista and Corona, Igino and Maiorca, Davide and Nelson, Blaine and {\v{S}}rndi{\'c}, Nedim and Laskov, Pavel and Giacinto, Giorgio and Roli, Fabio},
  booktitle={ECML-PKDD},
  year={2013},
}

@inproceedings{wong2018provable,
  title={Provable defenses against adversarial examples via the convex outer adversarial polytope},
  author={Wong, Eric and Kolter, Zico},
  booktitle={ICML},
  year={2018},
}

@inproceedings{uesato2018adversarial,
  title={Adversarial risk and the dangers of evaluating against weak attacks},
  author={Uesato, Jonathan and Oâ€™donoghue, Brendan and Kohli, Pushmeet and Oord, Aaron},
  booktitle={ICML},
  year={2018},
}

@inproceedings{salman2021unadversarial,
  title={Unadversarial Examples: Designing Objects for Robust Vision},
  author={Salman, Hadi and Ilyas, Andrew and Engstrom, Logan and Vemprala, Sai and Madry, Aleksander and Kapoor, Ashish},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{kurakin2016adversarial,
  title={Adversarial machine learning at scale},
  author={Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  booktitle={ICLR},
  year={2017}
}

@article{tao2020false,
  title={With False Friends Like These, Who Can Notice Mistakes?},
  author={Tao, Lue and Feng, Lei and Yi, Jinfeng and Chen, Songcan},
  journal={arXiv preprint arXiv:2012.14738},
  year={2020}
}

@InProceedings{pmlr-v139-yuan21b,
  title = 	 {Neural Tangent Generalization Attacks},
  author =       {Yuan, Chia-Hung and Wu, Shan-Hung},
  booktitle = 	 {ICML},
  year = 	 {2021},
}

@inproceedings{zhang2019theoretically,
  title={Theoretically principled trade-off between robustness and accuracy},
  author={Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric and El Ghaoui, Laurent and Jordan, Michael},
  booktitle={ICML},
  year={2019},
}

@inproceedings{rice2020overfitting,
  title={Overfitting in adversarially robust deep learning},
  author={Rice, Leslie and Wong, Eric and Kolter, Zico},
  booktitle={ICML},
  year={2020},
}

@inproceedings{wang2019improving,
  title={Improving adversarial robustness requires revisiting misclassified examples},
  author={Wang, Yisen and Zou, Difan and Yi, Jinfeng and Bailey, James and Ma, Xingjun and Gu, Quanquan},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={CVPR},
  year={2016}
}

@TECHREPORT{Krizhevsky09learningmultiple,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {},
    year = {2009}
}

@article{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  year={2011}
}

@article{le2015tiny,
  title={Tiny imagenet visual recognition challenge},
  author={Le, Ya and Yang, Xuan},
  journal={CS 231N},
  volume={7},
  number={7},
  pages={3},
  year={2015}
}

@inproceedings{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  booktitle={ICLR},
  year={2015}
}

@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{sandler2018mobilenetv2,
  title={Mobilenetv2: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={CVPR},
  year={2018}
}

@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@inproceedings{carlini2017towards,
  title={Towards evaluating the robustness of neural networks},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={S\&P},
  year={2017},
}

@inproceedings{croce2020reliable,
  title={Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks},
  author={Croce, Francesco and Hein, Matthias},
  booktitle={ICML},
  year={2020},
}

@inproceedings{ilyas2019adversarial,
  title={Adversarial examples are not bugs, they are features},
  author={Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{roth2019odds,
  title={The odds are odd: A statistical test for detecting adversarial examples},
  author={Roth, Kevin and Kilcher, Yannic and Hofmann, Thomas},
  booktitle={ICML},
  year={2019},
}

@article{xu2017feature,
  title={Feature squeezing: Detecting adversarial examples in deep neural networks},
  author={Xu, Weilin and Evans, David and Qi, Yanjun},
  journal={arXiv preprint arXiv:1704.01155},
  year={2017}
}

@inproceedings{papernot2016distillation,
  title={Distillation as a defense to adversarial perturbations against deep neural networks},
  author={Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
  booktitle={S\&P},
  year={2016},
}

@inproceedings{samangouei2018defense,
  title={Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models},
  author={Samangouei, Pouya and Kabkab, Maya and Chellappa, Rama},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@inproceedings{tramer2019adversarial,
  title={Adversarial Training and Robustness for Multiple Perturbations},
  author={Tramer, Florian and Boneh, Dan},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{pang2020bag,
  title={Bag of tricks for adversarial training},
  author={Pang, Tianyu and Yang, Xiao and Dong, Yinpeng and Su, Hang and Zhu, Jun},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{wu2020adversarial,
 title = {Adversarial Weight Perturbation Helps Robust Generalization},
 author = {Wu, Dongxian and Xia, Shu-Tao and Wang, Yisen},
 booktitle = {NeurIPS},
 year = {2020}
}

@inproceedings{zhang2020geometry,
  title={Geometry-aware instance-reweighted adversarial training},
  author={Zhang, Jingfeng and Zhu, Jianing and Niu, Gang and Han, Bo and Sugiyama, Masashi and Kankanhalli, Mohan},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{tack2021consistency,
title={Consistency Regularization for Adversarial Robustness},
author={Jihoon Tack and Sihyun Yu and Jongheon Jeong and Minseon Kim and Sung Ju Hwang and Jinwoo Shin},
booktitle={ICML 2021 Workshop on Adversarial Machine Learning},
year={2021},
}

@inproceedings{wang2021probabilistic,
  title={Probabilistic Margins for Instance Reweighting in Adversarial Training},
  author={Wang, Qizhou and Liu, Feng and Han, Bo and Liu, Tongliang and Gong, Chen and Niu, Gang and Zhou, Mingyuan and Sugiyama, Masashi},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{springer2021little,
  title={A Little Robustness Goes a Long Way: Leveraging Robust Features for Targeted Transfer Attacks},
  author={Springer, Jacob and Mitchell, Melanie and Kenyon, Garrett},
 booktitle = {NeurIPS},
  year={2021}
}

@inproceedings{kim2021distilling,
  title={Distilling Robust and Non-Robust Features in Adversarial Examples by Information Bottleneck},
  author={Kim, Junho and Lee, Byung-Kwan and Ro, Yong Man},
 booktitle = {NeurIPS},
  year={2021}
}

@article{carlini2019critique,
  title={A critique of the deepsec platform for security analysis of deep learning models},
  author={Carlini, Nicholas},
  journal={arXiv preprint arXiv:1905.07112},
  year={2019}
}

@article{chen2017targeted,
  title={Targeted backdoor attacks on deep learning systems using data poisoning},
  author={Chen, Xinyun and Liu, Chang and Li, Bo and Lu, Kimberly and Song, Dawn},
  journal={arXiv preprint arXiv:1712.05526},
  year={2017}
}

@article{gu2017badnets,
  title={Badnets: Identifying vulnerabilities in the machine learning model supply chain},
  author={Gu, Tianyu and Dolan-Gavitt, Brendan and Garg, Siddharth},
  journal={arXiv preprint arXiv:1708.06733},
  year={2017}
}

@inproceedings{biggio2012poisoning,
  title={Poisoning attacks against support vector machines},
  author={Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
  booktitle={ICML},
  year={2012}
}

@inproceedings{munoz2017towards,
  title={Towards poisoning of deep learning algorithms with back-gradient optimization},
  author={Mu{\~n}oz-Gonz{\'a}lez, Luis and Biggio, Battista and Demontis, Ambra and Paudice, Andrea and Wongrassamee, Vasin and Lupu, Emil C and Roli, Fabio},
  booktitle={ACM Workshop on Artificial Intelligence and Security},
  year={2017}
}

@inproceedings{xiao2015feature,
  title={Is feature selection secure against training data poisoning?},
  author={Xiao, Huang and Biggio, Battista and Brown, Gavin and Fumera, Giorgio and Eckert, Claudia and Roli, Fabio},
  booktitle={ICML},
  year={2015}
}

@inproceedings{pang2021accumulative,
  title={Accumulative Poisoning Attacks on Real-time Data},
  author={Pang, Tianyu and Yang, Xiao and Dong, Yinpeng and Su, Hang and Zhu, Jun},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{koh2017understanding,
  title={Understanding black-box predictions via influence functions},
  author={Koh, Pang Wei and Liang, Percy},
  booktitle={ICML},
  year={2017},
}

@inproceedings{shafahi2018poison,
  title={Poison frogs! targeted clean-label poisoning attacks on neural networks},
  author={Shafahi, Ali and Huang, W Ronny and Najibi, Mahyar and Suciu, Octavian and Studer, Christoph and Dumitras, Tudor and Goldstein, Tom},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{zhu2019transferable,
  title={Transferable Clean-Label Poisoning Attacks on Deep Neural Nets},
  author={Zhu, Chen and Huang, W Ronny and Li, Hengduo and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle={ICML},
  year={2019}
}

@inproceedings{geiping2020witches,
  title={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching},
  author={Geiping, Jonas and Fowl, Liam H and Huang, W Ronny and Czaja, Wojciech and Taylor, Gavin and Moeller, Michael and Goldstein, Tom},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{gao2021learning,
  title={Learning and Certification under Instance-targeted Poisoning},
  author={Gao, Ji and Karbasi, Amin and Mahmoody, Mohammad},
  booktitle={UAI},
  year={2021}
}

@article{liu2017trojaning,
  title={Trojaning attack on neural networks},
  author={Liu, Yingqi and Ma, Shiqing and Aafer, Yousra and Lee, Wen-Chuan and Zhai, Juan and Wang, Weihang and Zhang, Xiangyu},
  year={2017}
}

@inproceedings{schwarzschild2021just,
  title={Just how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks},
  author={Schwarzschild, Avi and Goldblum, Micah and Gupta, Arjun and Dickerson, John P and Goldstein, Tom},
  booktitle={ICML},
  year={2021},
}

@inproceedings{nguyen2020input,
  title={Input-Aware Dynamic Backdoor Attack},
  author={Nguyen, Tuan Anh and Tran, Anh},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{li2021invisible,
  title={Invisible backdoor attack with sample-specific triggers},
  author={Li, Yuezun and Li, Yiming and Wu, Baoyuan and Li, Longkang and He, Ran and Lyu, Siwei},
  booktitle={CVPR},
  year={2021}
}

@article{zhang2021noilin,
  title={NoiLIn: Do Noisy Labels Always Hurt Adversarial Training?},
  author={Zhang, Jingfeng and Xu, Xilie and Han, Bo and Liu, Tongliang and Niu, Gang and Cui, Lizhen and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:2105.14676},
  year={2021}
}

@inproceedings{saha2020hidden,
  title={Hidden trigger backdoor attacks},
  author={Saha, Aniruddha and Subramanya, Akshayvarun and Pirsiavash, Hamed},
  booktitle={AAAI},
  year={2020}
}

@article{turner2019label,
  title={Label-consistent backdoor attacks},
  author={Turner, Alexander and Tsipras, Dimitris and Madry, Aleksander},
  journal={arXiv preprint arXiv:1912.02771},
  year={2019}
}

@inproceedings{borgnia2021strong,
  title={Strong data augmentation sanitizes poisoning and backdoor attacks without an accuracy tradeoff},
  author={Borgnia, Eitan and Cherepanova, Valeriia and Fowl, Liam and Ghiasi, Amin and Geiping, Jonas and Goldblum, Micah and Goldstein, Tom and Gupta, Arjun},
  booktitle={ICASSP},
  year={2021},
}

@inproceedings{wu2021Adversarial,
 title = {Adversarial Neuron Pruning Purifies Backdoored Deep Models},
 author = {Wu, Dongxian and Wang, Yisen},
 booktitle = {NeurIPS},
 year = {2021}
}

@inproceedings{li2021anti,
  title={Anti-Backdoor Learning: Training Clean Models on Poisoned Data},
  author={Li, Yige and Lyu, Xixiang and Koren, Nodens and Lyu, Lingjuan and Li, Bo and Ma, Xingjun},
  booktitle={NeurIPS},
  year={2021}
}

@article{zhu2021understanding,
  title={Understanding the Interaction of Adversarial Training with Noisy Labels},
  author={Zhu, Jianing and Zhang, Jingfeng and Han, Bo and Liu, Tongliang and Niu, Gang and Yang, Hongxia and Kankanhalli, Mohan and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:2102.03482},
  year={2021}
}

@misc{
gao2022does,
title={Does Adversarial Robustness Really Imply Backdoor Vulnerability?},
author={Yinghua Gao and Dongxian Wu and Jingfeng Zhang and Shu-Tao Xia and Gang Niu and Masashi Sugiyama},
year={2022},
url={https://openreview.net/forum?id=nG4DkcHDw_}
}

@inproceedings{weng2020trade,
  title={On the Trade-off between Adversarial and Backdoor Robustness},
  author={Weng, Cheng-Hsin and Lee, Yan-Ting and Wu, Shan-Hung Brandon},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{
fu2022robust,
title={Robust Unlearnable Examples: Protecting Data Privacy Against Adversarial Learning},
author={Shaopeng Fu and Fengxiang He and Yang Liu and Li Shen and Dacheng Tao},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=baUQQPwQiAg}
}

@misc{
wang2022fooling,
title={Fooling Adversarial Training with Induction Noise},
author={Zhirui Wang and Yifei Wang and Yisen Wang},
year={2022},
url={https://openreview.net/forum?id=4o1xPXaS4X}
}

@article{liu2021going,
  title={Going Grayscale: The Road to Understanding and Improving Unlearnable Examples},
  author={Liu, Zhuoran and Zhao, Zhengyu and Kolmus, Alex and Berns, Tijn and van Laarhoven, Twan and Heskes, Tom and Larson, Martha},
  journal={arXiv preprint arXiv:2111.13244},
  year={2021}
}

@article{dobriban2020provable,
  title={Provable tradeoffs in adversarially robust classification},
  author={Dobriban, Edgar and Hassani, Hamed and Hong, David and Robey, Alexander},
  journal={arXiv preprint arXiv:2006.05161},
  year={2020}
}

@inproceedings{xu2021robust,
  title={To be robust or to be fair: Towards fairness in adversarial training},
  author={Xu, Han and Liu, Xiaorui and Li, Yaxin and Jain, Anil and Tang, Jiliang},
  booktitle={ICML},
  year={2021},
}

@article{javanmard2020precise,
  title={Precise statistical analysis of classification accuracies for adversarial training},
  author={Javanmard, Adel and Soltanolkotabi, Mahdi},
  journal={arXiv preprint arXiv:2010.11213},
  year={2020}
}

@article{shen2019tensorclog,
  title={TensorClog: An imperceptible poisoning attack on deep neural network applications},
  author={Shen, Juncheng and Zhu, Xiaolei and Ma, De},
  journal={IEEE Access},
  year={2019},
}

@article{fowl2021preventing,
  title={Preventing unauthorized use of proprietary data: Poisoning for secure dataset release},
  author={Fowl, Liam and Chiang, Ping-yeh and Goldblum, Micah and Geiping, Jonas and Bansal, Arpit and Czaja, Wojtek and Goldstein, Tom},
  journal={arXiv preprint arXiv:2103.02683},
  year={2021}
}

@inproceedings{evtimov2021disrupting,
  title={Disrupting Model Training with Adversarial Shortcuts},
  author={Evtimov, Ivan and Covert, Ian and Kusupati, Aditya and Kohno, Tadayoshi},
  booktitle={ICML 2021 Workshop},
  year={2021}
}

@inproceedings{athalye2018synthesizing,
  title={Synthesizing robust adversarial examples},
  author={Athalye, Anish and Engstrom, Logan and Ilyas, Andrew and Kwok, Kevin},
  booktitle={ICML},
  year={2018},
}

@inproceedings{su2018robustness,
  title={Is Robustness the Cost of Accuracy?--A Comprehensive Study on the Robustness of 18 Deep Image Classification Models},
  author={Su, Dong and Zhang, Huan and Chen, Hongge and Yi, Jinfeng and Chen, Pin-Yu and Gao, Yupeng},
  booktitle={ECCV},
  year={2018}
}

@article{yang2020closer,
  title={A closer look at accuracy vs. robustness},
  author={Yang, Yao-Yuan and Rashtchian, Cyrus and Zhang, Hongyang and Salakhutdinov, Russ R and Chaudhuri, Kamalika},
  journal={NeurIPS},
  year={2020}
}

@inproceedings{mehrabi2021fundamental,
  title={Fundamental tradeoffs in distributionally adversarial training},
  author={Mehrabi, Mohammad and Javanmard, Adel and Rossi, Ryan A and Rao, Anup and Mai, Tung},
  booktitle={ICML},
  year={2021},
}

@inproceedings{raghunathan2020understanding,
  title={Understanding and Mitigating the Tradeoff between Robustness and Accuracy},
  author={Raghunathan, Aditi and Xie, Sang Michael and Yang, Fanny and Duchi, John and Liang, Percy},
  booktitle={ICML},
  year={2020},
}

@article{pang2022robustness,
  title={Robustness and Accuracy Could Be Reconcilable by (Proper) Definition},
  author={Pang, Tianyu and Lin, Min and Yang, Xiao and Zhu, Jun and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2202.10103},
  year={2022}
}

@article{wu2022backdoorbench,
  title={Backdoorbench: A comprehensive benchmark of backdoor learning},
  author={Wu, Baoyuan and Chen, Hongrui and Zhang, Mingda and Zhu, Zihao and Wei, Shaokui and Yuan, Danni and Shen, Chao and Zha, Hongyuan},
  journal={arXiv preprint arXiv:2206.12654},
  year={2022}
}
