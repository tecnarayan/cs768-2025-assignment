\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arnab et~al.(2021)Arnab, Dehghani, Heigold, Sun, Lu{\v{c}}i{\'c}, and
  Schmid]{vivit}
Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lu{\v{c}}i{\'c}, M., and Schmid,
  C.
\newblock Vivit: A video vision transformer.
\newblock In \emph{ICCV}, 2021.

\bibitem[Baevski et~al.(2022)Baevski, Hsu, Xu, Babu, Gu, and Auli]{data2vec}
Baevski, A., Hsu, W.-N., Xu, Q., Babu, A., Gu, J., and Auli, M.
\newblock data2vec: A general framework for self-supervised learning in speech,
  vision and language.
\newblock In \emph{ICML}, 2022.

\bibitem[Bao et~al.(2022)Bao, Dong, and Wei]{bao2021beit}
Bao, H., Dong, L., and Wei, F.
\newblock Beit: Bert pre-training of image transformers.
\newblock \emph{ICLR}, 2022.

\bibitem[Bertasius et~al.(2021)Bertasius, Wang, and
  Torresani]{bertasius2021space}
Bertasius, G., Wang, H., and Torresani, L.
\newblock Is space-time attention all you need for video understanding?
\newblock In \emph{ICML}, 2021.

\bibitem[Bozic et~al.(2021)Bozic, Palafox, Thies, Dai, and
  Nie{\ss}ner]{bozic2021transformerfusion}
Bozic, A., Palafox, P., Thies, J., Dai, A., and Nie{\ss}ner, M.
\newblock Transformerfusion: Monocular rgb scene reconstruction using
  transformers.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Carion et~al.(2020)Carion, Massa, Synnaeve, Usunier, Kirillov, and
  Zagoruyko]{carion2020end}
Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko,
  S.
\newblock End-to-end object detection with transformers.
\newblock In \emph{ECCV}, 2020.

\bibitem[Carreira et~al.(2018)Carreira, Noland, Banki-Horvath, Hillier, and
  Zisserman]{k600}
Carreira, J., Noland, E., Banki-Horvath, A., Hillier, C., and Zisserman, A.
\newblock A short note about kinetics-600.
\newblock \emph{arXiv preprint arXiv:1808.01340}, 2018.

\bibitem[Carreira et~al.(2019)Carreira, Noland, Hillier, and Zisserman]{k700}
Carreira, J., Noland, E., Hillier, C., and Zisserman, A.
\newblock A short note on the kinetics-700 human action dataset.
\newblock \emph{arXiv preprint arXiv:1907.06987}, 2019.

\bibitem[Chen et~al.(2020)Chen, Radford, Child, Wu, Jun, Luan, and
  Sutskever]{chen2020generative}
Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I.
\newblock Generative pretraining from pixels.
\newblock In \emph{ICML}, 2020.

\bibitem[Clark et~al.(2020)Clark, Luong, Le, and Manning]{electra}
Clark, K., Luong, M.-T., Le, Q.~V., and Manning, C.~D.
\newblock {ELECTRA}: Pre-training text encoders as discriminators rather than
  generators.
\newblock In \emph{ICLR}, 2020.

\bibitem[Cubuk et~al.(2020)Cubuk, Zoph, Shlens, and Le]{randaug}
Cubuk, E.~D., Zoph, B., Shlens, J., and Le, Q.~V.
\newblock {RandAugment}: Practical automated data augmentation with a reduced
  search space.
\newblock In \emph{CVPR}, 2020.

\bibitem[Dalal \& Triggs(2005)Dalal and Triggs]{hog}
Dalal, N. and Triggs, B.
\newblock Histograms of oriented gradients for human detection.
\newblock In \emph{CVPR}, 2005.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R{\'e}]{flashattn}
Dao, T., Fu, D.~Y., Ermon, S., Rudra, A., and R{\'e}, C.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness.
\newblock \emph{arXiv preprint arXiv:2205.14135}, 2022.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{CVPR}, 2009.

\bibitem[Dong et~al.(2022)Dong, Bao, Chen, Zhang, Yu, Yuan, Chen, and
  Guo]{cswin}
Dong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., and Guo, B.
\newblock Cswin transformer: A general vision transformer backbone with
  cross-shaped windows.
\newblock In \emph{CVPR}, 2022.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{vit}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{ICLR}, 2021.

\bibitem[Duke et~al.(2021)Duke, Ahmed, Wolf, Aarabi, and
  Taylor]{duke2021sstvos}
Duke, B., Ahmed, A., Wolf, C., Aarabi, P., and Taylor, G.~W.
\newblock Sstvos: Sparse spatiotemporal transformers for video object
  segmentation.
\newblock In \emph{CVPR}, 2021.

\bibitem[El-Nouby et~al.(2021)El-Nouby, Izacard, Touvron, Laptev, Jegou, and
  Grave]{splitmask}
El-Nouby, A., Izacard, G., Touvron, H., Laptev, I., Jegou, H., and Grave, E.
\newblock Are large-scale datasets necessary for self-supervised pre-training?
\newblock \emph{arXiv preprint arXiv:2112.10740}, 2021.

\bibitem[Fan et~al.(2021)Fan, Xiong, Mangalam, Li, Yan, Malik, and
  Feichtenhofer]{mvitv1}
Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., and
  Feichtenhofer, C.
\newblock Multiscale vision transformers.
\newblock In \emph{ICCV}, 2021.

\bibitem[Feichtenhofer et~al.(2019)Feichtenhofer, Fan, Malik, and He]{slowfast}
Feichtenhofer, C., Fan, H., Malik, J., and He, K.
\newblock Slowfast networks for video recognition.
\newblock In \emph{ICCV}, 2019.

\bibitem[Feichtenhofer et~al.(2022)Feichtenhofer, Fan, Li, and He]{mae-st}
Feichtenhofer, C., Fan, H., Li, Y., and He, K.
\newblock Masked autoencoders as spatiotemporal learners.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Gao et~al.(2022)Gao, Ma, Li, Lin, Dai, and Qiao]{gao2022mcmae}
Gao, P., Ma, T., Li, H., Lin, Z., Dai, J., and Qiao, Y.
\newblock Mcmae: Masked convolution meets masked autoencoders.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Ghiasi et~al.(2021)Ghiasi, Cui, Srinivas, Qian, Lin, Cubuk, Le, and
  Zoph]{Ghiasi2021}
Ghiasi, G., Cui, Y., Srinivas, A., Qian, R., Lin, T.-Y., Cubuk, E.~D., Le,
  Q.~V., and Zoph, B.
\newblock Simple copy-paste is a strong data augmentation method for instance
  segmentation.
\newblock In \emph{CVPR}, 2021.

\bibitem[Goyal et~al.(2017{\natexlab{a}})Goyal, Dollár, Girshick, Noordhuis,
  Wesolowski, Kyrola, Tulloch, Jia, and He]{Goyal2017}
Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola,
  A., Tulloch, A., Jia, Y., and He, K.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017{\natexlab{a}}.

\bibitem[Goyal et~al.(2017{\natexlab{b}})Goyal, Ebrahimi~Kahou, Michalski,
  Materzynska, Westphal, Kim, Haenel, Fruend, Yianilos, Mueller-Freitag,
  et~al.]{goyal2017something}
Goyal, R., Ebrahimi~Kahou, S., Michalski, V., Materzynska, J., Westphal, S.,
  Kim, H., Haenel, V., Fruend, I., Yianilos, P., Mueller-Freitag, M., et~al.
\newblock The" something something" video database for learning and evaluating
  visual common sense.
\newblock In \emph{ICCV}, 2017{\natexlab{b}}.

\bibitem[Graham et~al.(2021)Graham, El-Nouby, Touvron, Stock, Joulin,
  J{\'e}gou, and Douze]{levit}
Graham, B., El-Nouby, A., Touvron, H., Stock, P., Joulin, A., J{\'e}gou, H.,
  and Douze, M.
\newblock Levit: a vision transformer in convnet's clothing for faster
  inference.
\newblock In \emph{ICCV}, 2021.

\bibitem[Gu et~al.(2018)Gu, Sun, Ross, Vondrick, Pantofaru, Li,
  Vijayanarasimhan, Toderici, Ricco, Sukthankar, et~al.]{gu2018ava}
Gu, C., Sun, C., Ross, D.~A., Vondrick, C., Pantofaru, C., Li, Y.,
  Vijayanarasimhan, S., Toderici, G., Ricco, S., Sukthankar, R., et~al.
\newblock {AVA}: A video dataset of spatio-temporally localized atomic visual
  actions.
\newblock In \emph{CVPR}, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[He et~al.(2017)He, Gkioxari, Doll{\'a}r, and Girshick]{maskrcnn}
He, K., Gkioxari, G., Doll{\'a}r, P., and Girshick, R.
\newblock {Mask R-CNN}.
\newblock In \emph{ICCV}, 2017.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'a}r, and Girshick]{mae}
He, K., Chen, X., Xie, S., Li, Y., Doll{\'a}r, P., and Girshick, R.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{CVPR}, 2022.

\bibitem[Hoffer et~al.(2020)Hoffer, Ben-Nun, Hubara, Giladi, Hoefler, and
  Soudry]{Hoffer2020}
Hoffer, E., Ben-Nun, T., Hubara, I., Giladi, N., Hoefler, T., and Soudry, D.
\newblock Augment your batch: Improving generalization through instance
  repetition.
\newblock In \emph{CVPR}, 2020.

\bibitem[Hou et~al.(2022)Hou, Sun, Chen, Xie, and Kung]{milan}
Hou, Z., Sun, F., Chen, Y.-K., Xie, Y., and Kung, S.-Y.
\newblock Milan: Masked image pretraining on language assisted representation.
\newblock \emph{arXiv preprint arXiv:2208.06049}, 2022.

\bibitem[Huang et~al.(2016)Huang, Sun, Liu, Sedra, and Weinberger]{droppath}
Huang, G., Sun, Y., Liu, Z., Sedra, D., and Weinberger, K.~Q.
\newblock Deep networks with stochastic depth.
\newblock In \emph{ECCV}, 2016.

\bibitem[Huang et~al.(2022{\natexlab{a}})Huang, You, Zheng, Wang, Qian, and
  Yamasaki]{huang2022green}
Huang, L., You, S., Zheng, M., Wang, F., Qian, C., and Yamasaki, T.
\newblock Green hierarchical vision transformer for masked image modeling.
\newblock In \emph{NeurIPS}, 2022{\natexlab{a}}.

\bibitem[Huang et~al.(2022{\natexlab{b}})Huang, Xu, Li, Baevski, Auli, Galuba,
  Metze, and Feichtenhofer]{mae-audio}
Huang, P.-Y., Xu, H., Li, J., Baevski, A., Auli, M., Galuba, W., Metze, F., and
  Feichtenhofer, C.
\newblock Masked autoencoders that listen.
\newblock \emph{NeurIPS}, 2022{\natexlab{b}}.

\bibitem[Kay et~al.(2017)Kay, Carreira, Simonyan, Zhang, Hillier,
  Vijayanarasimhan, Viola, Green, Back, Natsev, et~al.]{k400}
Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan,
  S., Viola, F., Green, T., Back, T., Natsev, P., et~al.
\newblock The kinetics human action video dataset.
\newblock \emph{arXiv preprint arXiv:1705.06950}, 2017.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{alexnet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{NeurIPS}, 2012.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 1998.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Wang, Yang, and Yang]{li2022uniform}
Li, X., Wang, W., Yang, L., and Yang, J.
\newblock Uniform masking: Enabling mae pre-training for pyramid-based vision
  transformers with locality.
\newblock \emph{arXiv preprint arXiv:2205.10063}, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Mao, Girshick, and He]{vitdet}
Li, Y., Mao, H., Girshick, R., and He, K.
\newblock Exploring plain vision transformer backbones for object detection.
\newblock In \emph{ECCV}, 2022{\natexlab{b}}.

\bibitem[Li et~al.(2022{\natexlab{c}})Li, Wu, Fan, Mangalam, Xiong, Malik, and
  Feichtenhofer]{mvitv2}
Li, Y., Wu, C.-Y., Fan, H., Mangalam, K., Xiong, B., Malik, J., and
  Feichtenhofer, C.
\newblock Mvitv2: Improved multiscale vision transformers for classification
  and detection.
\newblock In \emph{CVPR}, 2022{\natexlab{c}}.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
  Doll{\'a}r, and Zitnick]{coco}
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
  Doll{\'a}r, P., and Zitnick, C.~L.
\newblock {Microsoft COCO: Common objects in context}.
\newblock In \emph{ECCV}, 2014.

\bibitem[Lin et~al.(2017)Lin, Doll{\'a}r, Girshick, He, Hariharan, and
  Belongie]{fpn}
Lin, T.-Y., Doll{\'a}r, P., Girshick, R., He, K., Hariharan, B., and Belongie,
  S.
\newblock Feature pyramid networks for object detection.
\newblock In \emph{CVPR}, 2017.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and Guo]{swin}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In \emph{ICCV}, 2021.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{sgdr}
Loshchilov, I. and Hutter, F.
\newblock {SGDR}: Stochastic gradient descent with warm restarts.
\newblock In \emph{ICLR}, 2017.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{adamw}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock In \emph{ICLR}, 2019.

\bibitem[Misra et~al.(2021)Misra, Girdhar, and Joulin]{misra2021end}
Misra, I., Girdhar, R., and Joulin, A.
\newblock An end-to-end transformer model for 3d object detection.
\newblock In \emph{ICCV}, 2021.

\bibitem[Pathak et~al.(2016)Pathak, Krahenbuhl, Donahue, Darrell, and
  Efros]{pathak2016context}
Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., and Efros, A.~A.
\newblock Context encoders: Feature learning by inpainting.
\newblock In \emph{CVPR}, 2016.

\bibitem[Ranftl et~al.(2021)Ranftl, Bochkovskiy, and Koltun]{ranftl2021vision}
Ranftl, R., Bochkovskiy, A., and Koltun, V.
\newblock Vision transformers for dense prediction.
\newblock In \emph{CVPR}, 2021.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{JMLR}, 2014.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{inception}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{CVPR}, 2016.

\bibitem[Tong et~al.(2022)Tong, Song, Wang, and Wang]{tong2022videomae}
Tong, Z., Song, Y., Wang, J., and Wang, L.
\newblock Video{MAE}: Masked autoencoders are data-efficient learners for
  self-supervised video pre-training.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Touvron et~al.(2022)Touvron, Cord, and J{\'e}gou]{deit3}
Touvron, H., Cord, M., and J{\'e}gou, H.
\newblock Deit iii: Revenge of the vit.
\newblock \emph{ECCV}, 2022.

\bibitem[Van~Horn et~al.(2018)Van~Horn, Mac~Aodha, Song, Cui, Sun, Shepard,
  Adam, Perona, and Belongie]{van2018inaturalist}
Van~Horn, G., Mac~Aodha, O., Song, Y., Cui, Y., Sun, C., Shepard, A., Adam, H.,
  Perona, P., and Belongie, S.
\newblock The {iNaturalist} species classification and detection dataset.
\newblock In \emph{CVPR}, 2018.

\bibitem[Vincent et~al.(2010)Vincent, Larochelle, Lajoie, Bengio, Manzagol, and
  Bottou]{vincent2010stacked}
Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P.-A., and
  Bottou, L.
\newblock Stacked denoising autoencoders: Learning useful representations in a
  deep network with a local denoising criterion.
\newblock \emph{JMLR}, 2010.

\bibitem[Wang et~al.(2021)Wang, Xie, Li, Fan, Song, Liang, Lu, Luo, and
  Shao]{pvt}
Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P.,
  and Shao, L.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock In \emph{ICCV}, 2021.

\bibitem[Wei et~al.(2022)Wei, Fan, Xie, Wu, Yuille, and
  Feichtenhofer]{maskfeat}
Wei, C., Fan, H., Xie, S., Wu, C.-Y., Yuille, A., and Feichtenhofer, C.
\newblock Masked feature prediction for self-supervised visual pre-training.
\newblock In \emph{CVPR}, 2022.

\bibitem[Woo et~al.(2023)Woo, Debnath, Hu, Chen, Liu, Kweon, and
  Xie]{woo2023convnextv2}
Woo, S., Debnath, S., Hu, R., Chen, X., Liu, Z., Kweon, I.~S., and Xie, S.
\newblock Convnext v2: Co-designing and scaling convnets with masked
  autoencoders.
\newblock \emph{arXiv preprint arXiv:2301.00808}, 2023.

\bibitem[Wu et~al.(2019)Wu, Kirillov, Massa, Lo, and Girshick]{Wu2019}
Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y., and Girshick, R.
\newblock Detectron2, 2019.

\bibitem[Xie et~al.(2022)Xie, Zhang, Cao, Lin, Bao, Yao, Dai, and
  Hu]{xie2021simmim}
Xie, Z., Zhang, Z., Cao, Y., Lin, Y., Bao, J., Yao, Z., Dai, Q., and Hu, H.
\newblock Simmim: A simple framework for masked image modeling.
\newblock In \emph{CVPR}, 2022.

\bibitem[Yun et~al.(2019)Yun, Han, Oh, Chun, Choe, and Yoo]{cutmix}
Yun, S., Han, D., Oh, S.~J., Chun, S., Choe, J., and Yoo, Y.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In \emph{ICCV}, 2019.

\bibitem[Zhai et~al.(2021)Zhai, Kolesnikov, Houlsby, and Beyer]{scalingvits}
Zhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L.
\newblock Scaling vision transformers.
\newblock In \emph{CVPR}, 2021.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and Lopez-Paz]{mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{ICLR}, 2018.

\bibitem[Zhou et~al.(2014)Zhou, Lapedriza, Xiao, Torralba, and
  Oliva]{NIPS2014_3fe94a00}
Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., and Oliva, A.
\newblock Learning deep features for scene recognition using places database.
\newblock In \emph{NeurIPS}, 2014.

\end{thebibliography}
