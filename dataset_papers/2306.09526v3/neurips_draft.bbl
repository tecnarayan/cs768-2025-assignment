\begin{thebibliography}{10}

\bibitem{bansal2018chauffeurnet}
M.~Bansal, A.~Krizhevsky, and A.~Ogale.
\newblock Chauffeurnet: Learning to drive by imitating the best and
  synthesizing the worst.
\newblock In {\em Proceedings of Robotics: Science and Systems},
  FreiburgimBreisgau, Germany, June 2019.

\bibitem{bhattacharyya2022modeling}
R.~Bhattacharyya, B.~Wulfe, D.~J. Phillips, A.~Kuefler, J.~Morton,
  R.~Senanayake, and M.~J. Kochenderfer.
\newblock Modeling human driving behavior through generative adversarial
  imitation learning.
\newblock {\em IEEE Transactions on Intelligent Transportation Systems}, 2022.

\bibitem{bhattacharyya2019simulating}
R.~P. Bhattacharyya, D.~J. Phillips, C.~Liu, J.~K. Gupta, K.~Driggs-Campbell,
  and M.~J. Kochenderfer.
\newblock Simulating emergent properties of human driving behavior using
  multi-agent reward augmented imitation learning.
\newblock In {\em 2019 International Conference on Robotics and Automation
  (ICRA)}, pages 789--795. IEEE, 2019.

\bibitem{bojarski2016end}
M.~Bojarski, D.~Del~Testa, D.~Dworakowski, B.~Firner, B.~Flepp, P.~Goyal, L.~D.
  Jackel, M.~Monfort, U.~Muller, J.~Zhang, et~al.
\newblock End to end learning for self-driving cars.
\newblock {\em arXiv preprint arXiv:1604.07316}, 2016.

\bibitem{brockman2016openai}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba.
\newblock Openai gym.
\newblock {\em arXiv preprint arXiv:1606.01540}, 2016.

\bibitem{chi2023diffusion}
C.~Chi, S.~Feng, Y.~Du, Z.~Xu, E.~Cousineau, B.~Burchfiel, and S.~Song.
\newblock Diffusion policy: Visuomotor policy learning via action diffusion.
\newblock {\em arXiv preprint arXiv:2303.04137}, 2023.

\bibitem{codevilla2018end}
F.~Codevilla, M.~M{\"u}ller, A.~L{\'o}pez, V.~Koltun, and A.~Dosovitskiy.
\newblock End-to-end driving via conditional imitation learning.
\newblock In {\em 2018 IEEE international conference on robotics and automation
  (ICRA)}, pages 4693--4700. IEEE, 2018.

\bibitem{coulom2007efficient}
R.~Coulom.
\newblock Efficient selectivity and backup operators in monte-carlo tree
  search.
\newblock In {\em Computers and Games: 5th International Conference, CG 2006,
  Turin, Italy, May 29-31, 2006. Revised Papers 5}, pages 72--83. Springer,
  2007.

\bibitem{englert2017inverse}
P.~Englert, N.~A. Vien, and M.~Toussaint.
\newblock Inverse kkt: Learning cost functions of manipulation tasks from
  demonstrations.
\newblock {\em The International Journal of Robotics Research},
  36(13-14):1474--1488, 2017.

\bibitem{espinoza2022deep}
J.~L.~V. Espinoza, A.~Liniger, W.~Schwarting, D.~Rus, and L.~Van~Gool.
\newblock Deep interactive motion prediction and planning: Playing games with
  motion prediction models.
\newblock In {\em Learning for Dynamics and Control Conference}, pages
  1006--1019. PMLR, 2022.

\bibitem{ettinger2021large}
S.~Ettinger, S.~Cheng, B.~Caine, C.~Liu, H.~Zhao, S.~Pradhan, Y.~Chai, B.~Sapp,
  C.~R. Qi, Y.~Zhou, et~al.
\newblock Large scale interactive motion forecasting for autonomous driving:
  The waymo open motion dataset.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9710--9719, 2021.

\bibitem{fu2017learning}
J.~Fu, K.~Luo, and S.~Levine.
\newblock Learning robust rewards with adversarial inverse reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:1710.11248}, 2017.

\bibitem{fujimoto2021minimalist}
S.~Fujimoto and S.~S. Gu.
\newblock A minimalist approach to offline reinforcement learning.
\newblock {\em Advances in neural information processing systems},
  34:20132--20145, 2021.

\bibitem{fujimoto2018addressing}
S.~Fujimoto, H.~Hoof, and D.~Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In {\em International conference on machine learning}, pages
  1587--1596. PMLR, 2018.

\bibitem{garg2021iq}
D.~Garg, S.~Chakraborty, C.~Cundy, J.~Song, and S.~Ermon.
\newblock Iq-learn: Inverse soft-q learning for imitation.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~34, pages 4028--4039, 2021.

\bibitem{gleave2022imitation}
A.~Gleave, M.~Taufeeque, J.~Rocamonde, E.~Jenner, S.~H. Wang, S.~Toyer,
  M.~Ernestus, N.~Belrose, S.~Emmons, and S.~Russell.
\newblock imitation: Clean imitation learning implementations.
\newblock {\em arXiv preprint arXiv:2211.11972}, 2022.

\bibitem{haarnoja2017reinforcement}
T.~Haarnoja, H.~Tang, P.~Abbeel, and S.~Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In {\em International conference on machine learning}, pages
  1352--1361. PMLR, 2017.

\bibitem{haarnoja2018soft}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In {\em International conference on machine learning}, pages
  1861--1870. PMLR, 2018.

\bibitem{haarnoja2018soft2}
T.~Haarnoja, A.~Zhou, K.~Hartikainen, G.~Tucker, S.~Ha, J.~Tan, V.~Kumar,
  H.~Zhu, A.~Gupta, P.~Abbeel, et~al.
\newblock Soft actor-critic algorithms and applications.
\newblock {\em arXiv preprint arXiv:1812.05905}, 2018.

\bibitem{hansen2023idql}
P.~Hansen-Estruch, I.~Kostrikov, M.~Janner, J.~G. Kuba, and S.~Levine.
\newblock Idql: Implicit q-learning as an actor-critic method with diffusion
  policies.
\newblock {\em arXiv preprint arXiv:2304.10573}, 2023.

\bibitem{hester2018deep}
T.~Hester, M.~Vecerik, O.~Pietquin, M.~Lanctot, T.~Schaul, B.~Piot, D.~Horgan,
  J.~Quan, A.~Sendonaris, I.~Osband, et~al.
\newblock Deep q-learning from demonstrations.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem{ho2016generative}
J.~Ho and S.~Ermon.
\newblock Generative adversarial imitation learning.
\newblock In {\em Advances in neural information processing systems},
  volume~29, 2016.

\bibitem{janner2022planning}
M.~Janner, Y.~Du, J.~B. Tenenbaum, and S.~Levine.
\newblock Planning with diffusion for flexible behavior synthesis.
\newblock {\em arXiv preprint arXiv:2205.09991}, 2022.

\bibitem{jauhri2022robot}
S.~Jauhri, J.~Peters, and G.~Chalvatzaki.
\newblock Robot learning of mobile manipulation with reachability behavior
  priors.
\newblock {\em IEEE Robotics and Automation Letters}, 7(3):8399--8406, 2022.

\bibitem{judah2014imitation}
K.~Judah, A.~Fern, P.~Tadepalli, and R.~Goetschalckx.
\newblock Imitation learning with demonstrations and shaping rewards.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~28, 2014.

\bibitem{juozapaitis2019explainable}
Z.~Juozapaitis, A.~Koul, A.~Fern, M.~Erwig, and F.~Doshi-Velez.
\newblock Explainable reinforcement learning via reward decomposition.
\newblock In {\em IJCAI/ECAI Workshop on explainable artificial intelligence},
  2019.

\bibitem{kelly2019hg}
M.~Kelly, C.~Sidrane, K.~Driggs-Campbell, and M.~J. Kochenderfer.
\newblock Hg-dagger: Interactive imitation learning with human experts.
\newblock In {\em 2019 International Conference on Robotics and Automation
  (ICRA)}, pages 8077--8083. IEEE, 2019.

\bibitem{klink2021boosted}
P.~Klink, C.~D'Eramo, J.~Peters, and J.~Pajarinen.
\newblock Boosted curriculum reinforcement learning.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{kostrikov2021offline}
I.~Kostrikov, A.~Nair, and S.~Levine.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock {\em arXiv preprint arXiv:2110.06169}, 2021.

\bibitem{kumar2019stabilizing}
A.~Kumar, J.~Fu, M.~Soh, G.~Tucker, and S.~Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{kumar2020conservative}
A.~Kumar, A.~Zhou, G.~Tucker, and S.~Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:1179--1191, 2020.

\bibitem{highway-env}
E.~Leurent.
\newblock An environment for autonomous driving decision-making.
\newblock {\em GitHub repository}, 2018.

\bibitem{li2022efficient}
C.~Li, T.~Trinh, L.~Wang, C.~Liu, M.~Tomizuka, and W.~Zhan.
\newblock Efficient game-theoretic planning with prediction heuristic for
  socially-compliant autonomous driving.
\newblock {\em IEEE Robotics and Automation Letters}, 7(4):10248--10255, 2022.

\bibitem{li2022dealing}
J.~Li, C.~Tang, M.~Tomizuka, and W.~Zhan.
\newblock Dealing with the unknown: Pessimistic offline reinforcement learning.
\newblock In {\em Conference on Robot Learning}, pages 1455--1464. PMLR, 2022.

\bibitem{li2017infogail}
Y.~Li, J.~Song, and S.~Ermon.
\newblock Infogail: Interpretable imitation learning from visual
  demonstrations.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem{lu2022imitation}
Y.~Lu, J.~Fu, G.~Tucker, X.~Pan, E.~Bronstein, B.~Roelofs, B.~Sapp, B.~White,
  A.~Faust, S.~Whiteson, et~al.
\newblock Imitation is not enough: Robustifying imitation with reinforcement
  learning for challenging driving scenarios.
\newblock {\em arXiv preprint arXiv:2212.11419}, 2022.

\bibitem{mandlekar2020learning}
A.~Mandlekar, D.~Xu, R.~Mart{\'\i}n-Mart{\'\i}n, S.~Savarese, and L.~Fei-Fei.
\newblock Learning to generalize across long-horizon tasks from human
  demonstrations.
\newblock {\em arXiv preprint arXiv:2003.06085}, 2020.

\bibitem{mandlekar2020human}
A.~Mandlekar, D.~Xu, R.~Mart{\'\i}n-Mart{\'\i}n, Y.~Zhu, L.~Fei-Fei, and
  S.~Savarese.
\newblock Human-in-the-loop imitation learning using remote teleoperation.
\newblock {\em arXiv preprint arXiv:2012.06733}, 2020.

\bibitem{mandlekar2022matters}
A.~Mandlekar, D.~Xu, J.~Wong, S.~Nasiriany, C.~Wang, R.~Kulkarni, L.~Fei-Fei,
  S.~Savarese, Y.~Zhu, and R.~Mart{\'\i}n-Mart{\'\i}n.
\newblock What matters in learning from offline human demonstrations for robot
  manipulation.
\newblock In {\em Conference on Robot Learning}, pages 1678--1690. PMLR, 2022.

\bibitem{nair2020awac}
A.~Nair, A.~Gupta, M.~Dalal, and S.~Levine.
\newblock Awac: Accelerating online reinforcement learning with offline
  datasets.
\newblock {\em arXiv preprint arXiv:2006.09359}, 2020.

\bibitem{peng2022safe}
Z.~Peng, Q.~Li, C.~Liu, and B.~Zhou.
\newblock Safe driving via expert guided policy optimization.
\newblock In {\em Conference on Robot Learning}, pages 1554--1563. PMLR, 2022.

\bibitem{rl-zoo3}
A.~Raffin.
\newblock Rl baselines3 zoo.
\newblock {\em GitHub repository}, 2020.

\bibitem{stable-baselines3}
A.~Raffin, A.~Hill, A.~Gleave, A.~Kanervisto, M.~Ernestus, and N.~Dormann.
\newblock Stable-baselines3: Reliable reinforcement learning implementations.
\newblock {\em Journal of Machine Learning Research}, 22(268):1--8, 2021.

\bibitem{rajeswaran2017learning}
A.~Rajeswaran, V.~Kumar, A.~Gupta, G.~Vezzani, J.~Schulman, E.~Todorov, and
  S.~Levine.
\newblock Learning complex dexterous manipulation with deep reinforcement
  learning and demonstrations.
\newblock {\em arXiv preprint arXiv:1709.10087}, 2017.

\bibitem{rhinehartdeep}
N.~Rhinehart, R.~McAllister, and S.~Levine.
\newblock Deep imitative models for flexible inference, planning, and control.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{scheel2022urban}
O.~Scheel, L.~Bergamini, M.~Wolczyk, B.~Osi{\'n}ski, and P.~Ondruska.
\newblock Urban driver: Learning to drive from real-world demonstrations using
  policy gradients.
\newblock In {\em Conference on Robot Learning}, pages 718--728. PMLR, 2022.

\bibitem{schrittwieser2020mastering}
J.~Schrittwieser, I.~Antonoglou, T.~Hubert, K.~Simonyan, L.~Sifre, S.~Schmitt,
  A.~Guez, E.~Lockhart, D.~Hassabis, T.~Graepel, et~al.
\newblock Mastering atari, go, chess and shogi by planning with a learned
  model.
\newblock {\em Nature}, 588(7839):604--609, 2020.

\bibitem{singh2020parrot}
A.~Singh, H.~Liu, G.~Zhou, A.~Yu, N.~Rhinehart, and S.~Levine.
\newblock Parrot: Data-driven behavioral priors for reinforcement learning.
\newblock {\em arXiv preprint arXiv:2011.10024}, 2020.

\bibitem{6386109}
E.~Todorov, T.~Erez, and Y.~Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In {\em 2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pages 5026--5033, 2012.

\bibitem{tosatto2017boosted}
S.~Tosatto, M.~Pirotta, C.~d’Eramo, and M.~Restelli.
\newblock Boosted fitted q-iteration.
\newblock In {\em International Conference on Machine Learning}, pages
  3434--3443. PMLR, 2017.

\bibitem{51579}
I.~Uchendu, T.~Xiao, Y.~Lu, B.~Zhu, M.~Yan, J.~Simon, M.~Bennice, C.~K. Fu,
  C.~Ma, J.~Jiao, S.~Levine, and K.~Hausman.
\newblock Jump-start reinforcement learning.
\newblock In {\em NeurIPS 2021 Robot Learning Workshop, RSS 2022 Scaling Robot
  Learning Workshop}, 2022.

\bibitem{van2017hybrid}
H.~Van~Seijen, M.~Fatemi, J.~Romoff, R.~Laroche, T.~Barnes, and J.~Tsang.
\newblock Hybrid reward architecture for reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem{wilson2023argoverse}
B.~Wilson, W.~Qi, T.~Agarwal, J.~Lambert, J.~Singh, S.~Khandelwal, B.~Pan,
  R.~Kumar, A.~Hartnett, J.~K. Pontes, et~al.
\newblock Argoverse 2: Next generation datasets for self-driving perception and
  forecasting.
\newblock {\em arXiv preprint arXiv:2301.00493}, 2023.

\bibitem{wu2019behavior}
Y.~Wu, G.~Tucker, and O.~Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:1911.11361}, 2019.

\bibitem{xiao2019maximum}
C.~Xiao, R.~Huang, J.~Mei, D.~Schuurmans, and M.~M{\"u}ller.
\newblock Maximum entropy monte-carlo planning.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{xue2022guarded}
Z.~Xue, Z.~Peng, Q.~Li, Z.~Liu, and B.~Zhou.
\newblock Guarded policy optimization with imperfect online demonstrations.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2022.

\bibitem{ye2021mastering}
W.~Ye, S.~Liu, T.~Kurutach, P.~Abbeel, and Y.~Gao.
\newblock Mastering atari games with limited data.
\newblock {\em Advances in Neural Information Processing Systems},
  34:25476--25488, 2021.

\bibitem{zhan2019interaction}
W.~Zhan, L.~Sun, D.~Wang, H.~Shi, A.~Clausse, M.~Naumann, J.~Kummerle,
  H.~Konigshof, C.~Stiller, A.~de~La~Fortelle, et~al.
\newblock Interaction dataset: An international, adversarial and cooperative
  motion dataset in interactive driving scenarios with semantic maps.
\newblock {\em arXiv preprint arXiv:1910.03088}, 2019.

\bibitem{zhang2018deep}
T.~Zhang, Z.~McCarthy, O.~Jow, D.~Lee, X.~Chen, K.~Goldberg, and P.~Abbeel.
\newblock Deep imitation learning for complex manipulation tasks from virtual
  reality teleoperation.
\newblock In {\em 2018 IEEE International Conference on Robotics and Automation
  (ICRA)}, pages 5628--5635. IEEE, 2018.

\bibitem{ziebart2008maximum}
B.~D. Ziebart, A.~L. Maas, J.~A. Bagnell, A.~K. Dey, et~al.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In {\em Aaai}, volume~8, pages 1433--1438. Chicago, IL, USA, 2008.

\bibitem{ziegler2019fine}
D.~M. Ziegler, N.~Stiennon, J.~Wu, T.~B. Brown, A.~Radford, D.~Amodei,
  P.~Christiano, and G.~Irving.
\newblock Fine-tuning language models from human preferences.
\newblock {\em arXiv preprint arXiv:1909.08593}, 2019.

\end{thebibliography}
