\begin{thebibliography}{10}

\bibitem{allen2017katyusha}
Zeyuan Allen-Zhu.
\newblock Katyusha: The first direct acceleration of stochastic gradient
  methods.
\newblock {\em The Journal of Machine Learning Research}, 18(1):8194--8244,
  2017.

\bibitem{allen2018katyusha}
Zeyuan Allen-Zhu.
\newblock Katyusha x: Practical momentum method for stochastic sum-of-nonconvex
  optimization.
\newblock {\em arXiv preprint arXiv:1802.03866}, 2018.

\bibitem{allen2016variance}
Zeyuan Allen-Zhu and Elad Hazan.
\newblock Variance reduction for faster non-convex optimization.
\newblock In {\em International conference on machine learning}, pages
  699--707. PMLR, 2016.

\bibitem{allen2016improved}
Zeyuan Allen-Zhu and Yang Yuan.
\newblock Improved svrg for non-strongly-convex or sum-of-non-convex
  objectives.
\newblock In {\em International conference on machine learning}, pages
  1080--1089. PMLR, 2016.

\bibitem{antonakopoulos2021adaptive}
Kimon Antonakopoulos, Veronica Belmega, and Panayotis Mertikopoulos.
\newblock Adaptive extra-gradient methods for min-max optimization and games.
\newblock In {\em International Conference on Learning Representations
  {(ICLR)}}, 2021.

\bibitem{bach2019universal}
Francis Bach and Kfir~Y Levy.
\newblock A universal algorithm for variational inequalities adaptive to
  smoothness and noise.
\newblock In {\em Conference on Learning Theory}, pages 164--194. PMLR, 2019.

\bibitem{chang2011libsvm}
Chih-Chung Chang and Chih-Jen Lin.
\newblock Libsvm: a library for support vector machines.
\newblock {\em ACM transactions on intelligent systems and technology (TIST)},
  2(3):1--27, 2011.

\bibitem{Cutkosky19}
Ashok Cutkosky.
\newblock Anytime online-to-batch, optimism and acceleration.
\newblock In {\em International Conference of Machine Learning {(ICML)}},
  volume~97 of {\em Proceedings of Machine Learning Research}, pages
  1446--1454. {PMLR}, 2019.

\bibitem{cutkosky2019momentum}
Ashok Cutkosky and Francesco Orabona.
\newblock Momentum-based variance reduction in non-convex sgd.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In {\em Advances in neural information processing systems}, pages
  1646--1654, 2014.

\bibitem{dozat2016incorporating}
Timothy Dozat.
\newblock Incorporating nesterov momentum into adam.
\newblock 2016.

\bibitem{dubois2021svrg}
Benjamin Dubois-Taine, Sharan Vaswani, Reza Babanezhad, Mark Schmidt, and Simon
  Lacoste-Julien.
\newblock Svrg meets adagrad: Painless variance reduction.
\newblock {\em arXiv preprint arXiv:2102.09645}, 2021.

\bibitem{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em Journal of machine learning research}, 12(7), 2011.

\bibitem{ene2021variational}
Alina Ene and Huy~L Nguyen.
\newblock Adaptive and universal algorithms for variational inequalities with
  optimal convergence s.
\newblock {\em arXiv preprint arXiv:2010.07799}, 2021.

\bibitem{ene2021adaptive}
Alina Ene, Huy~L Nguyen, and Adrian Vladu.
\newblock Adaptive gradient methods for constrained convex optimization and
  variational inequalities.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 7314--7321, 2021.

\bibitem{fang2018spider}
Cong Fang, Chris~Junchi Li, Zhouchen Lin, and Tong Zhang.
\newblock Spider: Near-optimal non-convex optimization via stochastic path
  integrated differential estimator.
\newblock {\em arXiv preprint arXiv:1807.01695}, 2018.

\bibitem{huang2021super}
Feihu Huang, Junyi Li, and Heng Huang.
\newblock Super-adam: Faster and universal framework of adaptive gradients.
\newblock {\em arXiv preprint arXiv:2106.08208}, 2021.

\bibitem{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock {\em Advances in neural information processing systems}, 26:315--323,
  2013.

\bibitem{joulani2020simpler}
Pooria Joulani, Anant Raj, Andras Gyorgy, and Csaba Szepesv{\'a}ri.
\newblock A simpler approach to accelerated optimization: iterative averaging
  meets optimism.
\newblock In {\em International Conference on Machine Learning}, pages
  4984--4993. PMLR, 2020.

\bibitem{KavisLBC19}
Ali Kavis, Kfir~Y. Levy, Francis Bach, and Volkan Cevher.
\newblock Unixgrad: {A} universal, adaptive algorithm with optimal guarantees
  for constrained optimization.
\newblock In {\em Advances in Neural Information Processing Systems
  {(NeurIPS)}}, pages 6257--6266, 2019.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{lan2019unified}
Guanghui Lan, Zhize Li, and Yi~Zhou.
\newblock A unified variance-reduced accelerated gradient method for convex
  optimization.
\newblock {\em arXiv preprint arXiv:1905.12412}, 2019.

\bibitem{lan2018random}
Guanghui Lan and Yi~Zhou.
\newblock Random gradient extrapolation for distributed and stochastic
  optimization.
\newblock {\em SIAM Journal on Optimization}, 28(4):2753--2782, 2018.

\bibitem{levy2021storm+}
Kfir Levy, Ali Kavis, and Volkan Cevher.
\newblock Storm+: Fully adaptive sgd with recursive momentum for nonconvex
  optimization.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{Levy17}
Kfir~Y. Levy.
\newblock Online to offline conversions, universality and adaptive minibatch
  sizes.
\newblock In {\em Advances in Neural Information Processing Systems
  {(NeurIPS)}}, pages 1613--1622, 2017.

\bibitem{levy2018online}
Kfir~Y Levy, Alp Yurtsever, and Volkan Cevher.
\newblock Online adaptive methods, universality and acceleration.
\newblock In {\em Advances in Neural Information Processing Systems
  {(NeurIPS)}}, pages 6500--6509, 2018.

\bibitem{li2020almost}
Bingcong Li, Lingda Wang, and Georgios~B Giannakis.
\newblock Almost tune-free variance reduction.
\newblock In {\em International Conference on Machine Learning}, pages
  5969--5978. PMLR, 2020.

\bibitem{li2021anita}
Zhize Li.
\newblock Anita: An optimal loopless accelerated variance-reduced gradient
  method.
\newblock {\em arXiv preprint arXiv:2103.11333}, 2021.

\bibitem{lin2015universal}
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui.
\newblock A universal catalyst for first-order optimization.
\newblock {\em arXiv preprint arXiv:1506.02186}, 2015.

\bibitem{mairal2013optimization}
Julien Mairal.
\newblock Optimization with first-order surrogate functions.
\newblock In {\em International Conference on Machine Learning}, pages
  783--791. PMLR, 2013.

\bibitem{McMahanS10}
H.~Brendan McMahan and Matthew~J. Streeter.
\newblock Adaptive bound optimization for online convex optimization.
\newblock In {\em Conference on Learning Theory {(COLT)}}, pages 244--256.
  Omnipress, 2010.

\bibitem{MohriYang16}
Mehryar Mohri and Scott Yang.
\newblock Accelerating online convex optimization via adaptive prediction.
\newblock In {\em Artificial Intelligence and Statistics {(AISTATS)}}, pages
  848--856, 2016.

\bibitem{nesterov1983method}
Yurii Nesterov.
\newblock A method for unconstrained convex minimization problem with the rate
  of convergence o (1/k\^{} 2).
\newblock In {\em Doklady an ussr}, volume 269, pages 543--547, 1983.

\bibitem{nesterov2003introductory}
Yurii Nesterov.
\newblock {\em Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem{reddi2018convergence}
Sashank~J Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{roux2012stochastic}
Nicolas~Le Roux, Mark Schmidt, and Francis Bach.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock {\em arXiv preprint arXiv:1202.6258}, 2012.

\bibitem{schmidt2017minimizing}
Mark Schmidt, Nicolas Le~Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock {\em Mathematical Programming}, 162(1-2):83--112, 2017.

\bibitem{shalev2013stochastic}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock {\em Journal of Machine Learning Research}, 14(2), 2013.

\bibitem{song2020variance}
Chaobing Song, Yong Jiang, and Yi~Ma.
\newblock Variance reduction via accelerated dual averaging for finite-sum
  optimization.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{tan2016barzilai}
Conghui Tan, Shiqian Ma, Yu-Hong Dai, and Yuqiu Qian.
\newblock Barzilai-borwein step size for stochastic gradient descent.
\newblock {\em Advances in Neural Information Processing Systems}, 29:685--693,
  2016.

\bibitem{tieleman2012lecture}
Tijmen Tieleman, Geoffrey Hinton, et~al.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock {\em COURSERA: Neural networks for machine learning}, 4(2):26--31,
  2012.

\bibitem{woodworth2016tight}
Blake~E Woodworth and Nati Srebro.
\newblock Tight complexity bounds for optimizing composite objectives.
\newblock {\em Advances in neural information processing systems},
  29:3639--3647, 2016.

\bibitem{zhou2018stochastic}
Dongruo Zhou, Pan Xu, and Quanquan Gu.
\newblock Stochastic nested variance reduction for nonconvex optimization.
\newblock {\em arXiv preprint arXiv:1806.07811}, 2018.

\end{thebibliography}
