\begin{thebibliography}{79}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbeel and Ng(2004)]{abbeel2004apprenticeship}
P.~Abbeel and A.~Y. Ng.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In \emph{Proceedings of the twenty-first international conference on
  Machine learning}, page~1. ACM, 2004.

\bibitem[Abdolmaleki et~al.(2018)Abdolmaleki, Springenberg, Tassa, Munos,
  Heess, and Riedmiller]{abdolmaleki2018maximum}
A.~Abdolmaleki, J.~T. Springenberg, Y.~Tassa, R.~Munos, N.~Heess, and
  M.~Riedmiller.
\newblock Maximum a posteriori policy optimisation.
\newblock \emph{arXiv preprint arXiv:1806.06920}, 2018.

\bibitem[Abernethy and Wang(2017)]{abernethy2017frankwolfe}
J.~D. Abernethy and J.-K. Wang.
\newblock On frank-wolfe and equilibrium computation.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/file/7371364b3d72ac9a3ed8638e6f0be2c9-Paper.pdf}.

\bibitem[Achiam et~al.(2018)Achiam, Edwards, Amodei, and
  Abbeel]{achiam2018variational}
J.~Achiam, H.~Edwards, D.~Amodei, and P.~Abbeel.
\newblock Variational option discovery algorithms.
\newblock \emph{arXiv preprint arXiv:1807.10299}, 2018.

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2020optimality}
A.~Agarwal, S.~M. Kakade, J.~D. Lee, and G.~Mahajan.
\newblock Optimality and approximation with policy gradient methods in markov
  decision processes.
\newblock In \emph{Conference on Learning Theory}, pages 64--66. PMLR, 2020.

\bibitem[Agrawal and Devanur(2014)]{agrawal2014fast}
S.~Agrawal and N.~R. Devanur.
\newblock Fast algorithms for online stochastic convex programming.
\newblock In \emph{Proceedings of the twenty-sixth annual ACM-SIAM symposium on
  Discrete algorithms}, pages 1405--1424. SIAM, 2014.

\bibitem[Altman(1999)]{altman1999constrained}
E.~Altman.
\newblock \emph{Constrained Markov decision processes}, volume~7.
\newblock CRC Press, 1999.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
M.~G. Azar, I.~Osband, and R.~Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  263--272, 2017.

\bibitem[Beck and Teboulle(2003)]{beck2003mirror}
A.~Beck and M.~Teboulle.
\newblock Mirror descent and nonlinear projected subgradient methods for convex
  optimization.
\newblock \emph{Operations Research Letters}, 31:\penalty0 167--175, 2003.

\bibitem[Belogolovsky et~al.(2021)Belogolovsky, Korsunsky, Mannor, Tessler, and
  Zahavy]{belogolovsky2019inverse}
S.~Belogolovsky, P.~Korsunsky, S.~Mannor, C.~Tessler, and T.~Zahavy.
\newblock Inverse reinforcement learning in contextual mdps.
\newblock \emph{Machine Learning}, 2021.

\bibitem[Bhatnagar and Lakshmanan(2012)]{bhatnagar2012online}
S.~Bhatnagar and K.~Lakshmanan.
\newblock An online actor--critic algorithm with function approximation for
  constrained markov decision processes.
\newblock \emph{Journal of Optimization Theory and Applications}, 153\penalty0
  (3):\penalty0 688--708, 2012.

\bibitem[Borkar(2005)]{borkar2005actor}
V.~S. Borkar.
\newblock An actor-critic algorithm for constrained markov decision processes.
\newblock \emph{Systems \& control letters}, 54\penalty0 (3):\penalty0
  207--213, 2005.

\bibitem[Boyd and Vandenberghe(2004)]{boyd2004convex}
S.~Boyd and L.~Vandenberghe.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Bregman(1967)]{bregman1967relaxation}
L.~M. Bregman.
\newblock The relaxation method of finding the common point of convex sets and
  its application to the solution of problems in convex programming.
\newblock \emph{USSR computational mathematics and mathematical physics},
  7\penalty0 (3):\penalty0 200--217, 1967.

\bibitem[Cai et~al.(2020)Cai, Yang, Jin, and Wang]{cai2020provably}
Q.~Cai, Z.~Yang, C.~Jin, and Z.~Wang.
\newblock Provably efficient exploration in policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  1283--1294. PMLR, 2020.

\bibitem[Calian et~al.(2021)Calian, Mankowitz, Zahavy, Xu, Oh, Levine, and
  Mann]{calian2021balancing}
D.~A. Calian, D.~J. Mankowitz, T.~Zahavy, Z.~Xu, J.~Oh, N.~Levine, and T.~Mann.
\newblock Balancing constraints and rewards with meta-gradient d4{\{}pg{\}}.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=TQt98Ya7UMP}.

\bibitem[Dann and Brunskill(2015)]{dann2015sample}
C.~Dann and E.~Brunskill.
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2818--2826, 2015.

\bibitem[Efroni et~al.(2020)Efroni, Mannor, and Pirotta]{efroni2020exploration}
Y.~Efroni, S.~Mannor, and M.~Pirotta.
\newblock Exploration-exploitation in constrained mdps.
\newblock \emph{arXiv preprint arXiv:2003.02189}, 2020.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, Legg, and Kavukcuoglu]{impala18a}
L.~Espeholt, H.~Soyer, R.~Munos, K.~Simonyan, V.~Mnih, T.~Ward, Y.~Doron,
  V.~Firoiu, T.~Harley, I.~Dunning, S.~Legg, and K.~Kavukcuoglu.
\newblock {IMPALA}: Scalable distributed deep-{RL} with importance weighted
  actor-learner architectures.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, 2018.

\bibitem[Eysenbach et~al.(2019)Eysenbach, Gupta, Ibarz, and
  Levine]{eysenbach2018diversity}
B.~Eysenbach, A.~Gupta, J.~Ibarz, and S.~Levine.
\newblock Diversity is all you need: Learning skills without a reward function.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=SJx63jRqFm}.

\bibitem[Florensa et~al.(2016)Florensa, Duan, and
  Abbeel]{florensa2016stochastic}
C.~Florensa, Y.~Duan, and P.~Abbeel.
\newblock Stochastic neural networks for hierarchical reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Frank and Wolfe(1956)]{frank1956algorithm}
M.~Frank and P.~Wolfe.
\newblock An algorithm for quadratic programming.
\newblock \emph{Naval research logistics quarterly}, 3\penalty0 (1-2):\penalty0
  95--110, 1956.

\bibitem[Freund and Schapire(1997)]{freund1997decision}
Y.~Freund and R.~E. Schapire.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock \emph{Journal of computer and system sciences}, 55\penalty0
  (1):\penalty0 119--139, 1997.

\bibitem[Geist et~al.(2019)Geist, Scherrer, and Pietquin]{geist2019theory}
M.~Geist, B.~Scherrer, and O.~Pietquin.
\newblock A theory of regularized markov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pages
  2160--2169. PMLR, 2019.

\bibitem[Geist et~al.(2021)Geist, P{\'e}rolat, Lauri{\`e}re, Elie, Perrin,
  Bachem, Munos, and Pietquin]{geist2021concave}
M.~Geist, J.~P{\'e}rolat, M.~Lauri{\`e}re, R.~Elie, S.~Perrin, O.~Bachem,
  R.~Munos, and O.~Pietquin.
\newblock Concave utility reinforcement learning: the mean-field game
  viewpoint.
\newblock \emph{arXiv preprint arXiv:2106.03787}, 2021.

\bibitem[Gregor et~al.(2017)Gregor, Rezende, and
  Wierstra]{gregor2016variational}
K.~Gregor, D.~J. Rezende, and D.~Wierstra.
\newblock Variational intrinsic control.
\newblock \emph{International Conference on Learning Representations, Workshop
  Track}, 2017.
\newblock URL \url{https://openreview.net/forum?id=Skc-Fo4Yg}.

\bibitem[Hausman et~al.(2018)Hausman, Springenberg, Wang, Heess, and
  Riedmiller]{hausman2018learning}
K.~Hausman, J.~T. Springenberg, Z.~Wang, N.~Heess, and M.~Riedmiller.
\newblock Learning an embedding space for transferable robot skills.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=rk07ZXZRb}.

\bibitem[Hazan(2016)]{hazan2016introduction}
E.~Hazan.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends in Optimization}, 2\penalty0
  (3-4):\penalty0 157--325, 2016.

\bibitem[Hazan et~al.(2007)Hazan, Agarwal, and Kale]{hazan2007logarithmic}
E.~Hazan, A.~Agarwal, and S.~Kale.
\newblock Logarithmic regret algorithms for online convex optimization.
\newblock \emph{Machine Learning}, 69\penalty0 (2-3):\penalty0 169--192, 2007.

\bibitem[Hazan et~al.(2019)Hazan, Kakade, Singh, and
  Van~Soest]{hazan2019provably}
E.~Hazan, S.~Kakade, K.~Singh, and A.~Van~Soest.
\newblock Provably efficient maximum entropy exploration.
\newblock In \emph{International Conference on Machine Learning}, pages
  2681--2691. PMLR, 2019.

\bibitem[Ho and Ermon(2016)]{ho2016generative}
J.~Ho and S.~Ermon.
\newblock Generative adversarial imitation learning.
\newblock \emph{arXiv preprint arXiv:1606.03476}, 2016.

\bibitem[Huang et~al.(2016)Huang, Lattimore, Gy{\"o}rgy, and
  Szepesv{\'a}ri]{huang2016following}
R.~Huang, T.~Lattimore, A.~Gy{\"o}rgy, and C.~Szepesv{\'a}ri.
\newblock Following the leader and fast rates in linear prediction: Curved
  constraint sets and other regularities.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4970--4978, 2016.

\bibitem[Jaggi(2013)]{jaggi2013revisiting}
M.~Jaggi.
\newblock Revisiting frank-wolfe: Projection-free sparse convex optimization.
\newblock In \emph{Proceedings of the 30th international conference on Machine
  learning}. ACM, 2013.

\bibitem[Jaggi and Lacoste-Julien(2015)]{jaggi2015global}
M.~Jaggi and S.~Lacoste-Julien.
\newblock On the global linear convergence of frank-wolfe optimization
  variants.
\newblock \emph{Advances in Neural Information Processing Systems}, 28, 2015.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
T.~Jaksch, R.~Ortner, and P.~Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Apr):\penalty0 1563--1600, 2010.

\bibitem[Jin and Sidford(2020)]{jin2020efficiently}
Y.~Jin and A.~Sidford.
\newblock Efficiently solving mdps with stochastic mirror descent.
\newblock In \emph{International Conference on Machine Learning}, pages
  4890--4900. PMLR, 2020.

\bibitem[Jin and Sidford(2021)]{jin2021towards}
Y.~Jin and A.~Sidford.
\newblock Towards tight bounds on the sample complexity of average-reward mdps.
\newblock \emph{arXiv preprint arXiv:2106.07046}, 2021.

\bibitem[Kaufmann et~al.(2021)Kaufmann, M{\'e}nard, Domingues, Jonsson,
  Leurent, and Valko]{kaufmann2021adaptive}
E.~Kaufmann, P.~M{\'e}nard, O.~D. Domingues, A.~Jonsson, E.~Leurent, and
  M.~Valko.
\newblock Adaptive reward-free exploration.
\newblock In \emph{Algorithmic Learning Theory}, pages 865--891. PMLR, 2021.

\bibitem[Kullback(1997)]{kullback1997information}
S.~Kullback.
\newblock \emph{Information theory and statistics}.
\newblock Courier Corporation, 1997.

\bibitem[Lattimore and Hutter(2012)]{lattimore2012pac}
T.~Lattimore and M.~Hutter.
\newblock Pac bounds for discounted mdps.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pages 320--334. Springer, 2012.

\bibitem[Lee et~al.(2019)Lee, Eysenbach, Parisotto, Xing, Levine, and
  Salakhutdinov]{lee2019efficient}
L.~Lee, B.~Eysenbach, E.~Parisotto, E.~Xing, S.~Levine, and R.~Salakhutdinov.
\newblock Efficient exploration via state marginal matching.
\newblock \emph{arXiv preprint arXiv:1906.05274}, 2019.

\bibitem[Levin et~al.(2017)Levin, Peres, and Wilmer]{levin2017markov}
D.~Levin, Y.~Peres, and E.~Wilmer.
\newblock \emph{Markov Chains and Mixing Times}.
\newblock American Mathematical Society, 2017.

\bibitem[McMahan(2011)]{mcmahan2011follow}
B.~McMahan.
\newblock Follow-the-regularized-leader and mirror descent: Equivalence
  theorems and l1 regularization.
\newblock In \emph{Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pages 525--533. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[Miryoosefi et~al.(2019)Miryoosefi, Brantley, Daum{\'e}~III,
  Dud{\'\i}k, and Schapire]{miryoosefi2019reinforcement}
S.~Miryoosefi, K.~Brantley, H.~Daum{\'e}~III, M.~Dud{\'\i}k, and R.~Schapire.
\newblock Reinforcement learning with convex constraints.
\newblock \emph{arXiv preprint arXiv:1906.09323}, 2019.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Nachum et~al.(2019)Nachum, Chow, Dai, and Li]{nachum2019dualdice}
O.~Nachum, Y.~Chow, B.~Dai, and L.~Li.
\newblock Dualdice: Behavior-agnostic estimation of discounted stationary
  distribution corrections.
\newblock \emph{arXiv preprint arXiv:1906.04733}, 2019.

\bibitem[Nemirovskij and Yudin(1983)]{nemirovskij1983problem}
A.~S. Nemirovskij and D.~B. Yudin.
\newblock Problem complexity and method efficiency in optimization.
\newblock In \emph{Wiley-Interscience}, 1983.

\bibitem[O'Donoghue(2018)]{o2018variational}
B.~O'Donoghue.
\newblock Variational {B}ayesian reinforcement learning with regret bounds.
\newblock \emph{arXiv preprint arXiv:1807.09647}, 2018.

\bibitem[O'Donoghue et~al.(2020{\natexlab{a}})O'Donoghue, Lattimore, and
  Osband]{o2020stochastic}
B.~O'Donoghue, T.~Lattimore, and I.~Osband.
\newblock Stochastic matrix games with bandit feedback.
\newblock \emph{arXiv preprint arXiv:2006.05145}, 2020{\natexlab{a}}.

\bibitem[O'Donoghue et~al.(2020{\natexlab{b}})O'Donoghue, Osband, and
  Ionescu]{o2020making}
B.~O'Donoghue, I.~Osband, and C.~Ionescu.
\newblock Making sense of reinforcement learning and probabilistic inference.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{b}}.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{osband2013more}
I.~Osband, D.~Russo, and B.~Van~Roy.
\newblock ({M}ore) efficient reinforcement learning via posterior sampling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3003--3011, 2013.

\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and Roy]{osband2016deep}
I.~Osband, C.~Blundell, A.~Pritzel, and B.~V. Roy.
\newblock Deep exploration via bootstrapped dqn.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, pages 4033--4041, 2016.

\bibitem[Osband et~al.(2019)Osband, Doron, Hessel, Aslanides, Sezener, Saraiva,
  McKinney, Lattimore, Szepesvari, Singh, et~al.]{osband2019behaviour}
I.~Osband, Y.~Doron, M.~Hessel, J.~Aslanides, E.~Sezener, A.~Saraiva,
  K.~McKinney, T.~Lattimore, C.~Szepesvari, S.~Singh, et~al.
\newblock Behaviour suite for reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Osborne and Rubinstein(1994)]{osborne1994course}
M.~J. Osborne and A.~Rubinstein.
\newblock \emph{A course in game theory}.
\newblock MIT press, 1994.

\bibitem[Puterman(1984)]{puterman2014markov}
M.~L. Puterman.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 1984.

\bibitem[Rockafellar(1970)]{rockafellar1970convex}
R.~T. Rockafellar.
\newblock \emph{Convex analysis}.
\newblock Princeton university press, 1970.

\bibitem[Rosenberg and Mansour(2019)]{rosenberg2019online}
A.~Rosenberg and Y.~Mansour.
\newblock Online convex optimization in adversarial markov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pages
  5478--5486. PMLR, 2019.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
J.~Schulman, S.~Levine, P.~Abbeel, M.~Jordan, and P.~Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pages
  1889--1897. PMLR, 2015.

\bibitem[Shani et~al.(2020{\natexlab{a}})Shani, Efroni, and
  Mannor]{shani2020adaptive}
L.~Shani, Y.~Efroni, and S.~Mannor.
\newblock Adaptive trust region policy optimization: Global convergence and
  faster rates for regularized mdps.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 5668--5675, 2020{\natexlab{a}}.

\bibitem[Shani et~al.(2020{\natexlab{b}})Shani, Efroni, Rosenberg, and
  Mannor]{shani2020optimistic}
L.~Shani, Y.~Efroni, A.~Rosenberg, and S.~Mannor.
\newblock Optimistic policy optimization with bandit feedback.
\newblock In \emph{International Conference on Machine Learning}, pages
  8604--8613. PMLR, 2020{\natexlab{b}}.

\bibitem[Shani et~al.(2021)Shani, Zahavy, and Mannor]{shani2021online}
L.~Shani, T.~Zahavy, and S.~Mannor.
\newblock Online apprenticeship learning.
\newblock \emph{arXiv preprint arXiv:2102.06924}, 2021.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
D.~Silver, J.~Schrittwieser, K.~Simonyan, I.~Antonoglou, A.~Huang, A.~Guez,
  T.~Hubert, L.~Baker, M.~Lai, A.~Bolton, et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Strehl and Littman(2008)]{strehl2008analysis}
A.~L. Strehl and M.~L. Littman.
\newblock An analysis of model-based interval estimation for markov decision
  processes.
\newblock \emph{Journal of Computer and System Sciences}, 74\penalty0
  (8):\penalty0 1309--1331, 2008.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
R.~S. Sutton and A.~G. Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Syed and Schapire(2008)]{syed2008game}
U.~Syed and R.~E. Schapire.
\newblock A game-theoretic approach to apprenticeship learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  1449--1456, 2008.

\bibitem[Syed et~al.(2008)Syed, Bowling, and Schapire]{syed2008apprenticeship}
U.~Syed, M.~Bowling, and R.~E. Schapire.
\newblock Apprenticeship learning using linear programming.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pages 1032--1039. ACM, 2008.

\bibitem[Szepesv{\'a}ri(2020)]{cmdpblog}
C.~Szepesv{\'a}ri.
\newblock Constrained mdps and the reward hypothesis, 2020.
\newblock URL
  \url{https://readingsml.blogspot.com/2020/03/constrained-mdps-and-reward-hypothesis.html}.

\bibitem[Tessler et~al.(2019)Tessler, Mankowitz, and Mannor]{tessler2018reward}
C.~Tessler, D.~J. Mankowitz, and S.~Mannor.
\newblock Reward constrained policy optimization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=SkfrvsA9FX}.

\bibitem[Tirumala et~al.(2020)Tirumala, Galashov, Noh, Hasenclever, Pascanu,
  Schwarz, Desjardins, Czarnecki, Ahuja, Teh, et~al.]{tirumala2020behavior}
D.~Tirumala, A.~Galashov, H.~Noh, L.~Hasenclever, R.~Pascanu, J.~Schwarz,
  G.~Desjardins, W.~M. Czarnecki, A.~Ahuja, Y.~W. Teh, et~al.
\newblock Behavior priors for efficient reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2010.14274}, 2020.

\bibitem[Tomar et~al.(2020)Tomar, Shani, Efroni, and
  Ghavamzadeh]{tomar2020mirror}
M.~Tomar, L.~Shani, Y.~Efroni, and M.~Ghavamzadeh.
\newblock Mirror descent policy optimization.
\newblock \emph{arXiv preprint arXiv:2005.09814}, 2020.

\bibitem[Von~Neumann(1928)]{neumann1928theorie}
J.~Von~Neumann.
\newblock Zur theorie der gesellschaftsspiele.
\newblock \emph{Mathematische annalen}, 100\penalty0 (1):\penalty0 295--320,
  1928.

\bibitem[Watkins and Dayan(1992)]{watkins1992q}
C.~J. Watkins and P.~Dayan.
\newblock Q-learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 279--292, 1992.

\bibitem[Xiao et~al.(2019)Xiao, Herman, Wagner, Ziesche, Etesami, and
  Linh]{xiao2019wasserstein}
H.~Xiao, M.~Herman, J.~Wagner, S.~Ziesche, J.~Etesami, and T.~H. Linh.
\newblock Wasserstein adversarial imitation learning.
\newblock \emph{arXiv preprint arXiv:1906.08113}, 2019.

\bibitem[Yang et~al.(2020)Yang, Nachum, Dai, Li, and Schuurmans]{yang2020off}
M.~Yang, O.~Nachum, B.~Dai, L.~Li, and D.~Schuurmans.
\newblock Off-policy evaluation via the regularized lagrangian.
\newblock \emph{arXiv preprint arXiv:2007.03438}, 2020.

\bibitem[Zahavy et~al.(2020{\natexlab{a}})Zahavy, Cohen, Kaplan, and
  Mansour]{zahavy2019apprenticeship}
T.~Zahavy, A.~Cohen, H.~Kaplan, and Y.~Mansour.
\newblock Apprenticeship learning via frank-wolfe.
\newblock \emph{AAAI, 2020}, 2020{\natexlab{a}}.

\bibitem[Zahavy et~al.(2020{\natexlab{b}})Zahavy, Cohen, Kaplan, and
  Mansour]{zahavy2019average}
T.~Zahavy, A.~Cohen, H.~Kaplan, and Y.~Mansour.
\newblock Average reward reinforcement learning with unknown mixing times.
\newblock \emph{The Conference on Uncertainty in Artificial Intelligence
  (UAI)}, 2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Koppel, Bedi, Szepesvari, and
  Wang]{zhang2020variational}
J.~Zhang, A.~Koppel, A.~S. Bedi, C.~Szepesvari, and M.~Wang.
\newblock Variational policy gradient method for reinforcement learning with
  general utilities.
\newblock \emph{arXiv preprint arXiv:2007.02151}, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Wang, Ma, Xia, Yang, Li, and
  Li]{zhang2020wasserstein}
M.~Zhang, Y.~Wang, X.~Ma, L.~Xia, J.~Yang, Z.~Li, and X.~Li.
\newblock Wasserstein distance guided adversarial imitation learning with
  reward shape exploration.
\newblock In \emph{2020 IEEE 9th Data Driven Control and Learning Systems
  Conference (DDCLS)}, pages 1165--1170. IEEE, 2020{\natexlab{b}}.

\bibitem[Zinkevich(2003)]{zinkevich2003online}
M.~Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{Proceedings of the 20th international conference on machine
  learning (icml-03)}, pages 928--936, 2003.

\end{thebibliography}
