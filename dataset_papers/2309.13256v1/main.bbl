\begin{thebibliography}{10}

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock {Language Models are Few-shot Learners}.
\newblock In {\em Proceedings of Advances in Neural Information Processing
  Systems (NeurIPS)}, 2020.

\bibitem{BadPrompt_NeurIPS2022}
Xiangrui Cai, Haidong Xu, Sihan Xu, Ying Zhang, and Xiaojie Yuan.
\newblock {BadPrompt: Backdoor Attacks on Continuous Prompts}.
\newblock In {\em Proceedings of Advances in Neural Information Processing
  Systems (NeurIPS)}, 2022.

\bibitem{chen2021mitigating}
Chuanshuai Chen and Jiazhu Dai.
\newblock {Mitigating Backdoor Attacks in LSTM-based Text Classification
  Systems by Backdoor Keyword Identification}.
\newblock {\em Neurocomputing}, 452:253--262, 2021.

\bibitem{chen2021badnl}
Xiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, and Yang Zhang.
\newblock {Badnl: Backdoor Attacks against NLP Models}.
\newblock In {\em ICML 2021 Workshop on Adversarial Machine Learning}, 2021.

\bibitem{chen2017targeted}
Xinyun Chen, Chang Liu, Bo~Li, Kimberly Lu, and Dawn Song.
\newblock {Targeted Backdoor Attacks on Deep Learning Systems Using Data
  Poisoning}.
\newblock In {\em ArXiv e-prints}, 2017.

\bibitem{dai2019backdoor}
Jiazhu Dai, Chuanshuai Chen, and Yufeng Li.
\newblock {A Backdoor Attack against LSTM-based Text Classification Systems}.
\newblock {\em IEEE Access}, 7:138872--138878, 2019.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding}.
\newblock In {\em Proceedings of Annual Conference of the North American
  Chapter of the Association for Computational Linguistics (NAACL)}, 2019.

\bibitem{duppt}
Wei Du, Yichun Zhao, Boqun Li, Gongshen Liu, and Shilin Wang.
\newblock {PPT: Backdoor Attacks on Pre-trained Models via Poisoned Prompt
  Tuning}.
\newblock In {\em Proceedings of International Joint Conference on Artificial
  Intelligence (IJCAI)}, 2022.

\bibitem{gao2021making}
Tianyu Gao, Adam Fisch, and Danqi Chen.
\newblock {Making Pre-trained Language Models Better Few-shot Learners}.
\newblock In {\em Proceedings of Annual Meeting of the Association for
  Computational Linguistics (ACL)}, 2021.

\bibitem{gao2021design}
Yansong Gao, Yeonjae Kim, Bao~Gia Doan, Zhi Zhang, Gongxuan Zhang, Surya Nepal,
  Damith~C Ranasinghe, and Hyoungshick Kim.
\newblock {Design and Evaluation of a Multi-domain Trojan Detection Method on
  Deep Neural Networks}.
\newblock {\em IEEE Transactions on Dependable and Secure Computing},
  19(4):2349--2364, 2021.

\bibitem{gu2017badnets}
Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock {Badnets: Identifying Vulnerabilities in the Machine Learning Model
  Supply Chain}.
\newblock In {\em ArXiv e-prints}, 2017.

\bibitem{kurita2020weight}
Keita Kurita, Paul Michel, and Graham Neubig.
\newblock Weight poisoning attacks on pretrained models.
\newblock In {\em Proceedings of Annual Meeting of the Association for
  Computational Linguistics (ACL)}, 2020.

\bibitem{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock {The Power of Scale for Parameter-Efficient Prompt Tuning}.
\newblock In {\em Proceedings of Conference on Empirical Methods in Natural
  Language Processing (EMNLP)}, 2021.

\bibitem{li2021backdoor}
Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, and Xipeng Qiu.
\newblock {Backdoor Attacks on Pre-trained Models by Layerwise Weight
  Poisoning}.
\newblock In {\em Proceedings of Conference on Empirical Methods in Natural
  Language Processing (EMNLP)}, 2021.

\bibitem{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock {Prefix-Tuning: Optimizing Continuous Prompts for Generation}.
\newblock In {\em Proceedings of Annual Meeting of the Association for
  Computational Linguistics (ACL)}, 2021.

\bibitem{liu2021pre}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
  Graham Neubig.
\newblock {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting
  Methods in Natural Language Processing}.
\newblock In {\em ArXiv e-prints}, 2021.

\bibitem{liu2021gpt}
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and
  Jie Tang.
\newblock Gpt understands, too.
\newblock In {\em ArXiv e-prints}, 2021.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock {Roberta: A Robustly Optimized BERT Pretraining Approach}.
\newblock In {\em ArXiv e-prints}, 2019.

\bibitem{miyato2016adversarial}
Takeru Miyato, Andrew~M Dai, and Ian Goodfellow.
\newblock {Adversarial Training Methods for Semi-Supervised Text
  Classification}.
\newblock In {\em Proceedings of International Conference on Learning
  Representations (ICLR)}, 2017.

\bibitem{pan2022hidden}
Xudong Pan, Mi~Zhang, Beina Sheng, Jiaming Zhu, and Min Yang.
\newblock {Hidden Trigger Backdoor Attack on $\{$NLP$\}$ Models via Linguistic
  Style Manipulation}.
\newblock In {\em Proceedings of USENIX Security Symposium (SEC)}, 2022.

\bibitem{imc}
Ren Pang, Hua Shen, Xinyang Zhang, Shouling Ji, Yevgeniy Vorobeychik, Xiapu
  Luo, Alex Liu, and Ting Wang.
\newblock A tale of evil twins: Adversarial inputs versus poisoned models.
\newblock In {\em Proceedings of ACM Conference on Computer and Communications
  (CCS)}, 2020.

\bibitem{petroni2019language}
Fabio Petroni, Tim Rockt{\"a}schel, Sebastian Riedel, Patrick Lewis, Anton
  Bakhtin, Yuxiang Wu, and Alexander Miller.
\newblock {Language Models as Knowledge Bases?}
\newblock In {\em Proceedings of Conference on Empirical Methods in Natural
  Language Processing (EMNLP)}, 2019.

\bibitem{qi2021onion}
Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, and Maosong Sun.
\newblock {ONION: A Simple and Effective Defense Against Textual Backdoor
  Attacks}.
\newblock In {\em Proceedings of Conference on Empirical Methods in Natural
  Language Processing (EMNLP)}, 2021.

\bibitem{qi2021hidden}
Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang,
  and Maosong Sun.
\newblock {Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic
  Trigger}.
\newblock In {\em Proceedings of Annual Meeting of the Association for
  Computational Linguistics (ACL)}, 2021.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock {Language Models are Unsupervised Multitask Learners}.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, Peter~J Liu, et~al.
\newblock {Exploring the Limits of Transfer Learning with a Unified
  Text-to-text Transformer.}
\newblock {\em J. Mach. Learn. Res.}, 21(140):1--67, 2020.

\bibitem{schick2021s}
Timo Schick and Hinrich Sch{\"u}tze.
\newblock {Itâ€™s Not Just Size That Matters: Small Language Models Are Also
  Few-Shot Learners}.
\newblock In {\em Proceedings of Annual Conference of the North American
  Chapter of the Association for Computational Linguistics (NAACL)}, 2021.

\bibitem{vinyals2016matching}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et~al.
\newblock Matching networks for one shot learning.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{wang2019survey}
Wei Wang, Vincent~W Zheng, Han Yu, and Chunyan Miao.
\newblock A survey of zero-shot learning: Settings, methods, and applications.
\newblock {\em ACM Transactions on Intelligent Systems and Technology (TIST)},
  10(2):1--37, 2019.

\bibitem{wang2020generalizing}
Yaqing Wang, Quanming Yao, James~T Kwok, and Lionel~M Ni.
\newblock {Generalizing from a Few Examples: A Survey on Few-shot Learning}.
\newblock {\em ACM computing surveys (csur)}, 53(3):1--34, 2020.

\bibitem{NEURIPS2020_44feb009}
Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le.
\newblock {Unsupervised Data Augmentation for Consistency Training}.
\newblock In {\em Proceedings of Advances in Neural Information Processing
  Systems (NeurIPS)}, 2020.

\bibitem{xu2022exploring}
Lei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao, and Zhiyuan Liu.
\newblock {Exploring the Universal Vulnerability of Prompt-based Learning
  Paradigm}.
\newblock In {\em Proceedings of Annual Meeting of the Association for
  Computational Linguistics (ACL)}, 2022.

\bibitem{EP}
Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu~Sun, and Bin He.
\newblock {Be Careful about Poisoned Word Embeddings: Exploring the
  Vulnerability of the Embedding Layers in NLP Models}.
\newblock In {\em Proceedings of Annual Conference of the North American
  Chapter of the Association for Computational Linguistics (NAACL)}, 2021.

\bibitem{yang2021rap}
Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu~Sun.
\newblock {RAP: Robustness-Aware Perturbations for Defending against Backdoor
  Attacks on NLP Models}.
\newblock In {\em Proceedings of Conference on Empirical Methods in Natural
  Language Processing (EMNLP)}, 2021.

\bibitem{yang2021rethinking}
Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu~Sun.
\newblock {Rethinking Stealthiness of Backdoor Attack against NLP Models}.
\newblock In {\em Proceedings of Annual Meeting of the Association for
  Computational Linguistics (ACL)}, 2021.

\bibitem{yin-etal-2020-universal}
Wenpeng Yin, Nazneen~Fatema Rajani, Dragomir Radev, Richard Socher, and Caiming
  Xiong.
\newblock {Universal Natural Language Processing with Limited Annotations: Try
  Few-shot Textual Entailment as a Start}.
\newblock In {\em Proceedings of Conference on Empirical Methods in Natural
  Language Processing (EMNLP)}, 2020.

\bibitem{yu-etal-2020-bridging}
Haiyang Yu, Ningyu Zhang, Shumin Deng, Hongbin Ye, Wei Zhang, and Huajun Chen.
\newblock {Bridging Text and Knowledge with Multi-Prototype Embedding for
  Few-Shot Relational Triple Extraction}.
\newblock In {\em Proceedings of International Conference on Computational
  Linguistics}, 2020.

\bibitem{yu-etal-2018-diverse}
Mo~Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Saloni Potdar, Yu~Cheng, Gerald
  Tesauro, Haoyu Wang, and Bowen Zhou.
\newblock {Diverse Few-Shot Text Classification with Multiple Metrics}.
\newblock In {\em Proceedings of Annual Conference of the North American
  Chapter of the Association for Computational Linguistics (NAACL)}, 2018.

\bibitem{zhang2021differentiable}
Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei
  Huang, and Huajun Chen.
\newblock {Differentiable Prompt Makes Pre-trained Language Models Better
  Few-shot Learners}.
\newblock In {\em Proceedings of International Conference on Learning
  Representations (ICLR)}, 2021.

\bibitem{trojanlm}
Xinyang {Zhang}, Zheng {Zhang}, Shouling {Ji}, and Ting {Wang}.
\newblock Trojaning language models for fun and profit.
\newblock In {\em Proceedings of IEEE European Symposium on Security and
  Privacy (Euro S\&P)}, 2021.

\bibitem{zhang2021neural}
Zhiyuan Zhang, Xuancheng Ren, Qi~Su, Xu~Sun, and Bin He.
\newblock {Neural Network Surgery: Injecting Data Patterns into Pre-trained
  Models with Minimal Instance-wise Side Effects}.
\newblock In {\em Proceedings of Annual Conference of the North American
  Chapter of the Association for Computational Linguistics (NAACL)}, 2021.

\bibitem{zhong2021factual}
Zexuan Zhong, Dan Friedman, and Danqi Chen.
\newblock {Factual Probing Is [MASK]: Learning vs. Learning to Recall}.
\newblock In {\em Proceedings of Annual Conference of the North American
  Chapter of the Association for Computational Linguistics (NAACL)}, 2021.

\end{thebibliography}
