\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Mishchenko et~al.(2020)Mishchenko, Khaled, and
  Richt{\'{a}}rik]{mishchenko2020random}
Konstantin Mishchenko, Ahmed Khaled, and Peter Richt{\'{a}}rik.
\newblock Random reshuffling: Simple analysis with vast improvements.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Yun et~al.(2021)Yun, Sra, and Jadbabaie]{yun2021can}
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie.
\newblock Open problem: Can single-shuffle {SGD} be better than reshuffling
  {SGD} and {GD}?
\newblock In \emph{Conference on Learning Theory}, 2021.

\bibitem[De~Sa(2020{\natexlab{a}})]{de2020random}
Christopher~M De~Sa.
\newblock Random reshuffling is not always better.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 5957--5967, 2020{\natexlab{a}}.

\bibitem[Rajput et~al.(2021)Rajput, Lee, and
  Papailiopoulos]{rajput2021permutation}
Shashank Rajput, Kangwook Lee, and Dimitris Papailiopoulos.
\newblock Permutation-based sgd: Is random optimal?
\newblock \emph{arXiv preprint arXiv:2102.09718}, 2021.

\bibitem[Lu et~al.(2021{\natexlab{a}})Lu, Meng, and De~Sa]{lu2021general}
Yucheng Lu, Si~Yi Meng, and Christopher De~Sa.
\newblock {A General Analysis of Example-Selection for Stochastic Gradient
  Descent}.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.

\bibitem[Mohtashami et~al.(2022)Mohtashami, Stich, and
  Jaggi]{mohtashami2022characterizing}
Amirkeivan Mohtashami, Sebastian Stich, and Martin Jaggi.
\newblock Characterizing \& finding good data orderings for fast convergence of
  sequential gradient methods.
\newblock \emph{arXiv preprint arXiv:2202.01838}, 2022.

\bibitem[Harvey and Samadi(2014)]{harvey2014near}
Nick Harvey and Samira Samadi.
\newblock {Near-Optimal Herding}.
\newblock In \emph{Proceedings of The 27th Conference on Learning Theory},
  volume~35, pages 1165--1182, 2014.

\bibitem[Bansal and Spencer(2013)]{bansal2013deterministic}
Nikhil Bansal and Joel Spencer.
\newblock Deterministic discrepancy minimization.
\newblock \emph{Algorithmica}, 67\penalty0 (4):\penalty0 451--471, 2013.

\bibitem[Schmidt et~al.(2017)Schmidt, Roux, and Bach]{schmidt2017minimizing}
Mark Schmidt, Nicolas~Le Roux, and Francis~R. Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, 162\penalty0 (1-2):\penalty0
  83--112, 2017.

\bibitem[Needell et~al.(2014)Needell, Ward, and Srebro]{needell2014stochastic}
Deanna Needell, Rachel Ward, and Nathan Srebro.
\newblock {Stochastic Gradient Descent, Weighted Sampling, and the Randomized
  Kaczmarz algorithm}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1017--1025, 2014.

\bibitem[Lu et~al.(2021{\natexlab{b}})Lu, Park, Chen, Wang, De~Sa, and
  Foster]{lu2021variance}
Yucheng Lu, Youngsuk Park, Lifan Chen, Yuyang Wang, Christopher De~Sa, and Dean
  Foster.
\newblock Variance reduced training with stratified sampling for forecasting
  models.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, pages 7145--7155. PMLR, 2021{\natexlab{b}}.

\bibitem[Bottou(2012)]{bottou2012stochastic}
L{\'e}on Bottou.
\newblock Stochastic gradient descent tricks.
\newblock In \emph{Neural networks: Tricks of the trade}, pages 421--436.
  Springer, 2012.

\bibitem[Ying et~al.(2017)Ying, Yuan, Vlaski, and Sayed]{ying2017performance}
Bicheng Ying, Kun Yuan, Stefan Vlaski, and Ali~H. Sayed.
\newblock On the performance of random reshuffling in stochastic learning.
\newblock In \emph{2017 Information Theory and Applications Workshop (ITA)},
  pages 1--5. IEEE, 2017.

\bibitem[Bertsekas(2011)]{bertsekas2011incremental}
Dimitri~P. Bertsekas.
\newblock {Incremental Gradient, Subgradient, and Proximal Methods for Convex
  Optimization: A Survey}.
\newblock In \emph{{Optimization for Machine Learning}}. The MIT Press, 2011.

\bibitem[G{\"{u}}rb{\"{u}}zbalaban et~al.(2019)G{\"{u}}rb{\"{u}}zbalaban,
  Ozdaglar, and Parrilo]{gurbuzbalaban2019convergence}
Mert G{\"{u}}rb{\"{u}}zbalaban, Asuman~E. Ozdaglar, and Pablo~A. Parrilo.
\newblock Convergence rate of incremental gradient and incremental {Newton}
  methods.
\newblock \emph{{SIAM} Journal on Optimization}, 29\penalty0 (4):\penalty0
  2542--2565, 2019.

\bibitem[Recht and R{\'{e}}(2012)]{recht2012toward}
Benjamin Recht and Christopher R{\'{e}}.
\newblock Toward a noncommutative arithmetic-geometric mean inequality:
  Conjectures, case-studies, and consequences.
\newblock In \emph{Conference on Learning Theory}, volume~23, pages
  11.1--11.24, 2012.

\bibitem[De~Sa(2020{\natexlab{b}})]{NEURIPS2020_42299f06}
Christopher De~Sa.
\newblock Random reshuffling is not always better.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2020{\natexlab{b}}.

\bibitem[HaoChen and Sra(2019)]{haochen2019random}
Jeff~Z. HaoChen and Suvrit Sra.
\newblock Random shuffling beats {SGD} after finite epochs.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, volume~97, pages 2624--2633, 2019.

\bibitem[G{\"u}rb{\"u}zbalaban et~al.(2021)G{\"u}rb{\"u}zbalaban, Ozdaglar, and
  Parrilo]{gurbuzbalaban2021random}
Mert G{\"u}rb{\"u}zbalaban, Asu Ozdaglar, and Pablo~A Parrilo.
\newblock Why random reshuffling beats stochastic gradient descent.
\newblock \emph{Mathematical Programming}, 186\penalty0 (1):\penalty0 49--84,
  2021.

\bibitem[Bengio et~al.(2009)Bengio, Louradour, Collobert, and
  Weston]{bengio2009curriculum}
Yoshua Bengio, J{\'e}r{\^o}me Louradour, Ronan Collobert, and Jason Weston.
\newblock Curriculum learning.
\newblock In \emph{Proceedings of the 26th annual international conference on
  machine learning}, pages 41--48, 2009.

\bibitem[Graves et~al.(2017)Graves, Bellemare, Menick, Munos, and
  Kavukcuoglu]{graves2017automated}
Alex Graves, Marc~G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu.
\newblock Automated curriculum learning for neural networks.
\newblock In \emph{international conference on machine learning}, pages
  1311--1320. PMLR, 2017.

\bibitem[Matiisen et~al.(2019)Matiisen, Oliver, Cohen, and
  Schulman]{matiisen2019teacher}
Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman.
\newblock Teacher--student curriculum learning.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  31\penalty0 (9):\penalty0 3732--3740, 2019.

\bibitem[Soviany et~al.(2022)Soviany, Ionescu, Rota, and
  Sebe]{soviany2022curriculum}
Petru Soviany, Radu~Tudor Ionescu, Paolo Rota, and Nicu Sebe.
\newblock Curriculum learning: A survey.
\newblock \emph{International Journal of Computer Vision}, pages 1--40, 2022.

\bibitem[Welling(2009)]{welling2009herding}
Max Welling.
\newblock Herding dynamical weights to learn.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning}, pages 1121--1128, 2009.

\bibitem[Chelidze et~al.(2010)Chelidze, Chobanyan, Giorgobiani, and
  Kvaratskhelia]{chelidze2010greedy}
George Chelidze, Sergei Chobanyan, George Giorgobiani, and Vakhtang
  Kvaratskhelia.
\newblock Greedy algorithm fails in compact vector summation.
\newblock \emph{Bull. Georg. Natl. Acad. Sci}, 4\penalty0 (2), 2010.

\bibitem[Bansal and Garg(2017)]{bansal2017algorithmic}
Nikhil Bansal and Shashwat Garg.
\newblock Algorithmic discrepancy beyond partial coloring.
\newblock In \emph{Proceedings of the 49th Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 914--926, 2017.

\bibitem[Li and De~Sa(2019)]{li2019dimension}
Zheng Li and Christopher~M De~Sa.
\newblock Dimension-free bounds for low-precision training.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Lu and De~Sa(2020)]{lu2020moniqua}
Yucheng Lu and Christopher De~Sa.
\newblock Moniqua: Modulo quantized communication in decentralized sgd.
\newblock In \emph{International Conference on Machine Learning}, pages
  6415--6425. PMLR, 2020.

\bibitem[Spencer(1977)]{spencer1977balancing}
Joel Spencer.
\newblock Balancing games.
\newblock \emph{Journal of Combinatorial Theory, Series B}, 23\penalty0
  (1):\penalty0 68--74, 1977.

\bibitem[Aru et~al.(2016)Aru, Narayanan, Scott, and
  Venkatesan]{aru2016balancing}
Juhan Aru, Bhargav Narayanan, Alex Scott, and Ramarathnam Venkatesan.
\newblock Balancing sums of random vectors.
\newblock \emph{arXiv preprint arXiv:1610.05221}, 2016.

\bibitem[Bansal and Spencer(2020)]{bansal2020line}
Nikhil Bansal and Joel~H Spencer.
\newblock On-line balancing of random inputs.
\newblock \emph{Random Structures \& Algorithms}, 57\penalty0 (4):\penalty0
  879--891, 2020.

\bibitem[Bansal et~al.(2020)Bansal, Jiang, Singla, and Sinha]{bansal2020online}
Nikhil Bansal, Haotian Jiang, Sahil Singla, and Makrand Sinha.
\newblock Online vector balancing and geometric discrepancy.
\newblock In \emph{Proceedings of the 52nd Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 1139--1152, 2020.

\bibitem[Jiang et~al.(2019)Jiang, Kulkarni, and Singla]{jiang2019online}
Haotian Jiang, Janardhan Kulkarni, and Sahil Singla.
\newblock Online geometric discrepancy for stochastic arrivals with
  applications to envy minimization.
\newblock \emph{arXiv preprint arXiv:1910.01073}, 2019.

\bibitem[Bansal et~al.(2021)Bansal, Jiang, Meka, Singla, and
  Sinha]{bansal2021online}
Nikhil Bansal, Haotian Jiang, Raghu Meka, Sahil Singla, and Makrand Sinha.
\newblock Online discrepancy minimization for stochastic arrivals.
\newblock In \emph{Proceedings of the 2021 ACM-SIAM Symposium on Discrete
  Algorithms (SODA)}, pages 2842--2861. SIAM, 2021.

\bibitem[Matou{\v{s}}ek and Nikolov(2015)]{matouvsek2015combinatorial}
Jir{\'\i} Matou{\v{s}}ek and Aleksandar Nikolov.
\newblock Combinatorial discrepancy for boxes via the gamma\_2 norm.
\newblock In \emph{31st International Symposium on Computational Geometry (SoCG
  2015)}. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2015.

\bibitem[Chobanyan et~al.(2015)Chobanyan, Levental, and
  Salehi]{chobanyan2015maximum}
S~Chobanyan, Shlomo Levental, and Habib Salehi.
\newblock Maximum inequalities for rearrangements of summands and assignments
  of signs.
\newblock \emph{Theory of Probability \& Its Applications}, 59\penalty0
  (4):\penalty0 677--684, 2015.

\bibitem[Chobanyan(2016)]{chobanyan2016inequalities}
Sergei Chobanyan.
\newblock Inequalities on rearrangements of summands with applications in as
  convergence of functional series.
\newblock In \emph{VII International Joint Conference of Georgian Mathematical
  Union \& Georgian Mechanical Union}, page~56, 2016.

\bibitem[Alweiss et~al.(2021)Alweiss, Liu, and Sawhney]{alweiss2021discrepancy}
Ryan Alweiss, Yang~P Liu, and Mehtaab Sawhney.
\newblock Discrepancy minimization via a self-balancing walk.
\newblock In \emph{Proceedings of the 53rd Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 14--20, 2021.

\bibitem[Krizhevsky and Hinton(2010)]{krizhevsky2010convolutional}
Alex Krizhevsky and Geoff Hinton.
\newblock Convolutional deep belief networks on cifar-10.
\newblock \emph{Unpublished manuscript}, 40\penalty0 (7):\penalty0 1--9, 2010.

\bibitem[Merity et~al.(2017)Merity, Keskar, and Socher]{merity2017regularizing}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher.
\newblock Regularizing and optimizing lstm language models.
\newblock \emph{arXiv preprint arXiv:1708.02182}, 2017.

\bibitem[Turc et~al.(2019)Turc, Chang, Lee, and Toutanova]{turc2019}
Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Well-read students learn better: On the importance of pre-training
  compact models.
\newblock \emph{arXiv preprint arXiv:1908.08962v2}, 2019.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\end{thebibliography}
