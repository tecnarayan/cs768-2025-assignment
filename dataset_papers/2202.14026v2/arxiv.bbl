\begin{thebibliography}{100}

\bibitem{belkin2019reconciling}
M.~Belkin, D.~Hsu, S.~Ma, and S.~Mandal, ``Reconciling modern machine-learning
  practice and the classical bias--variance trade-off,'' {\em Proceedings of
  the National Academy of Sciences}, vol.~116, no.~32, pp.~15849--15854, 2019.

\bibitem{yang2020rethinking}
Z.~Yang, Y.~Yu, C.~You, J.~Steinhardt, and Y.~Ma, ``Rethinking bias-variance
  trade-off for generalization of neural networks,'' in {\em International
  Conference on Machine Learning}, pp.~10767--10777, PMLR, 2020.

\bibitem{neyshabur2014search}
B.~Neyshabur, R.~Tomioka, and N.~Srebro, ``In search of the real inductive
  bias: On the role of implicit regularization in deep learning,'' {\em arXiv
  preprint arXiv:1412.6614}, 2014.

\bibitem{song2020learning}
H.~Song, M.~Kim, D.~Park, Y.~Shin, and J.-G. Lee, ``Learning from noisy labels
  with deep neural networks: A survey,'' {\em arXiv preprint arXiv:2007.08199},
  2020.

\bibitem{zhang2021understanding}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals, ``Understanding deep
  learning (still) requires rethinking generalization,'' {\em Communications of
  the ACM}, vol.~64, no.~3, pp.~107--115, 2021.

\bibitem{frenay2013classification}
B.~Fr{\'e}nay and M.~Verleysen, ``Classification in the presence of label
  noise: a survey,'' {\em IEEE transactions on neural networks and learning
  systems}, vol.~25, no.~5, pp.~845--869, 2013.

\bibitem{candes2005decoding}
E.~J. Candes and T.~Tao, ``Decoding by linear programming,'' {\em IEEE
  transactions on information theory}, vol.~51, no.~12, pp.~4203--4215, 2005.

\bibitem{candes2011robust}
E.~J. Cand{\`e}s, X.~Li, Y.~Ma, and J.~Wright, ``Robust principal component
  analysis?,'' {\em Journal of the ACM (JACM)}, vol.~58, no.~3, pp.~1--37,
  2011.

\bibitem{wright2008robust}
J.~Wright, A.~Y. Yang, A.~Ganesh, S.~S. Sastry, and Y.~Ma, ``Robust face
  recognition via sparse representation,'' {\em IEEE transactions on pattern
  analysis and machine intelligence}, vol.~31, no.~2, pp.~210--227, 2008.

\bibitem{claerbout1973robust}
J.~F. Claerbout and F.~Muir, ``Robust modeling with erratic data,'' {\em
  Geophysics}, vol.~38, no.~5, pp.~826--844, 1973.

\bibitem{vaskevicius2019implicit}
T.~Vaskevicius, V.~Kanade, and P.~Rebeschini, ``Implicit regularization for
  optimal sparse recovery,'' in {\em Advances in Neural Information Processing
  Systems}, pp.~2968--2979, 2019.

\bibitem{zhao2019implicit}
P.~Zhao, Y.~Yang, and Q.-C. He, ``Implicit regularization via hadamard product
  over-parametrization in high-dimensional linear regression,'' {\em arXiv
  preprint arXiv:1903.09367}, 2019.

\bibitem{you2020robust}
C.~You, Z.~Zhu, Q.~Qu, and Y.~Ma, ``Robust recovery via implicit bias of
  discrepant learning rates for double over-parameterization,'' {\em Advances
  in Neural Information Processing Systems}, vol.~33, pp.~17733--17744, 2020.

\bibitem{jacot2018neural}
A.~Jacot, F.~Gabriel, and C.~Hongler, ``Neural tangent kernel: Convergence and
  generalization in neural networks,'' {\em arXiv preprint arXiv:1806.07572},
  2018.

\bibitem{chizat2018lazy}
L.~Chizat, E.~Oyallon, and F.~Bach, ``On lazy training in differentiable
  programming,'' {\em Advances in Neural Information Processing Systems},
  vol.~32, pp.~2937--2947, 2019.

\bibitem{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``Imagenet classification with
  deep convolutional neural networks,'' {\em Advances in neural information
  processing systems}, vol.~25, pp.~1097--1105, 2012.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton, {\em et~al.}, ``Learning multiple layers of features
  from tiny images,'' 2009.

\bibitem{NEURIPS2018_a19744e2}
B.~Han, Q.~Yao, X.~Yu, G.~Niu, M.~Xu, W.~Hu, I.~Tsang, and M.~Sugiyama,
  ``Co-teaching: Robust training of deep neural networks with extremely noisy
  labels,'' in {\em Advances in Neural Information Processing Systems}
  (S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi, and
  R.~Garnett, eds.), vol.~31, Curran Associates, Inc., 2018.

\bibitem{liu2020early}
S.~Liu, J.~Niles-Weed, N.~Razavian, and C.~Fernandez-Granda, ``Early-learning
  regularization prevents memorization of noisy labels,'' {\em Advances in
  Neural Information Processing Systems}, vol.~33, 2020.

\bibitem{xia2020robust}
X.~Xia, T.~Liu, B.~Han, C.~Gong, N.~Wang, Z.~Ge, and Y.~Chang, ``Robust
  early-learning: Hindering the memorization of noisy labels,'' in {\em
  International Conference on Learning Representations}, 2020.

\bibitem{wei2021learning}
J.~Wei, Z.~Zhu, H.~Cheng, T.~Liu, G.~Niu, and Y.~Liu, ``Learning with noisy
  labels revisited: A study using real-world human annotations,'' {\em arXiv
  preprint arXiv:2110.12088}, 2021.

\bibitem{xiao2015learning}
T.~Xiao, T.~Xia, Y.~Yang, C.~Huang, and X.~Wang, ``Learning from massive noisy
  labeled data for image classification,'' in {\em Proceedings of the IEEE
  conference on computer vision and pattern recognition}, pp.~2691--2699, 2015.

\bibitem{li2017webvision}
W.~Li, L.~Wang, W.~Li, E.~Agustsson, and L.~Van~Gool, ``Webvision database:
  Visual learning and understanding from web data,'' {\em arXiv preprint
  arXiv:1708.02862}, 2017.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in {\em Proceedings of the IEEE conference on computer vision
  and pattern recognition}, pp.~770--778, 2016.

\bibitem{patrini2017making}
G.~Patrini, A.~Rozza, A.~Krishna~Menon, R.~Nock, and L.~Qu, ``Making deep
  neural networks robust to label noise: A loss correction approach,'' in {\em
  Proceedings of the IEEE conference on computer vision and pattern
  recognition}, pp.~1944--1952, 2017.

\bibitem{zhang2018generalized}
Z.~Zhang and M.~R. Sabuncu, ``Generalized cross entropy loss for training deep
  neural networks with noisy labels,'' in {\em 32nd Conference on Neural
  Information Processing Systems (NeurIPS)}, 2018.

\bibitem{wang2019symmetric}
Y.~Wang, X.~Ma, Z.~Chen, Y.~Luo, J.~Yi, and J.~Bailey, ``Symmetric cross
  entropy for robust learning with noisy labels,'' in {\em Proceedings of the
  IEEE/CVF International Conference on Computer Vision}, pp.~322--330, 2019.

\bibitem{li2020dividemix}
J.~Li, R.~Socher, and S.~C. Hoi, ``Dividemix: Learning with noisy labels as
  semi-supervised learning,'' {\em arXiv preprint arXiv:2002.07394}, 2020.

\bibitem{cheng2021learning}
H.~Cheng, Z.~Zhu, X.~Li, Y.~Gong, X.~Sun, and Y.~Liu, ``Learning with
  instance-dependent label noise: A sample sieve approach,'' in {\em
  International Conference on Learning Representations}, 2021.

\bibitem{kalimeris2019sgd}
D.~Kalimeris, G.~Kaplun, P.~Nakkiran, B.~Edelman, T.~Yang, B.~Barak, and
  H.~Zhang, ``Sgd on neural networks learns functions of increasing
  complexity,'' {\em Advances in Neural Information Processing Systems},
  vol.~32, pp.~3496--3506, 2019.

\bibitem{ge2015escaping}
R.~Ge, F.~Huang, C.~Jin, and Y.~Yuan, ``Escaping from saddle pointsâ€”online
  stochastic gradient for tensor decomposition,'' in {\em Conference on
  learning theory}, pp.~797--842, PMLR, 2015.

\bibitem{lee2016gradient}
J.~D. Lee, M.~Simchowitz, M.~I. Jordan, and B.~Recht, ``Gradient descent only
  converges to minimizers,'' in {\em Conference on learning theory},
  pp.~1246--1257, PMLR, 2016.

\bibitem{domahidi2013ecos}
A.~Domahidi, E.~Chu, and S.~Boyd, ``{ECOS}: {A}n {SOCP} solver for embedded
  systems,'' in {\em European Control Conference (ECC)}, pp.~3071--3076, 2013.

\bibitem{diamond2016cvxpy}
S.~Diamond and S.~Boyd, ``{CVXPY}: {A} {P}ython-embedded modeling language for
  convex optimization,'' {\em Journal of Machine Learning Research}, vol.~17,
  no.~83, pp.~1--5, 2016.

\bibitem{oymak2019generalization}
S.~Oymak, Z.~Fabian, M.~Li, and M.~Soltanolkotabi, ``Generalization guarantees
  for neural networks via harnessing the low-rank structure of the jacobian,''
  {\em arXiv preprint arXiv:1906.05392}, 2019.

\bibitem{davenport2016overview}
M.~A. Davenport and J.~Romberg, ``An overview of low-rank matrix recovery from
  incomplete observations,'' {\em IEEE Journal of Selected Topics in Signal
  Processing}, vol.~10, no.~4, pp.~608--622, 2016.

\bibitem{chi2019nonconvex}
Y.~Chi, Y.~M. Lu, and Y.~Chen, ``Nonconvex optimization meets low-rank matrix
  factorization: An overview,'' {\em IEEE Transactions on Signal Processing},
  vol.~67, no.~20, pp.~5239--5269, 2019.

\bibitem{soudry2018implicit}
D.~Soudry, E.~Hoffer, M.~S. Nacson, S.~Gunasekar, and N.~Srebro, ``The implicit
  bias of gradient descent on separable data,'' {\em The Journal of Machine
  Learning Research}, vol.~19, no.~1, pp.~2822--2878, 2018.

\bibitem{gunasekar2018implicit}
S.~Gunasekar, B.~Woodworth, S.~Bhojanapalli, B.~Neyshabur, and N.~Srebro,
  ``Implicit regularization in matrix factorization,'' in {\em 2018 Information
  Theory and Applications Workshop (ITA)}, pp.~1--10, IEEE, 2018.

\bibitem{li2018algorithmic}
Y.~Li, T.~Ma, and H.~Zhang, ``Algorithmic regularization in over-parameterized
  matrix sensing and neural networks with quadratic activations,'' in {\em
  Conference On Learning Theory}, pp.~2--47, 2018.

\bibitem{oymak2019overparameterized}
S.~Oymak and M.~Soltanolkotabi, ``Overparameterized nonlinear learning:
  Gradient descent takes the shortest path?,'' in {\em International Conference
  on Machine Learning}, pp.~4951--4960, 2019.

\bibitem{arora2019implicit}
S.~Arora, N.~Cohen, W.~Hu, and Y.~Luo, ``Implicit regularization in deep matrix
  factorization,'' in {\em Advances in Neural Information Processing Systems},
  pp.~7411--7422, 2019.

\bibitem{razin2020implicit}
N.~Razin and N.~Cohen, ``Implicit regularization in deep learning may not be
  explainable by norms,'' {\em Advances in Neural Information Processing
  Systems}, vol.~33, 2020.

\bibitem{li2020towards}
Z.~Li, Y.~Luo, and K.~Lyu, ``Towards resolving the implicit bias of gradient
  descent for matrix factorization: Greedy low-rank learning,'' in {\em
  International Conference on Learning Representations}, 2020.

\bibitem{ji2020gradient}
Z.~Ji, M.~Dud{\'\i}k, R.~E. Schapire, and M.~Telgarsky, ``Gradient descent
  follows the regularization path for general losses,'' in {\em Conference on
  Learning Theory}, pp.~2109--2136, PMLR, 2020.

\bibitem{stoger2021small}
D.~St{\"o}ger and M.~Soltanolkotabi, ``Small random initialization is akin to
  spectral learning: Optimization and generalization guarantees for
  overparameterized low-rank matrix reconstruction,'' {\em Advances in Neural
  Information Processing Systems}, vol.~34, 2021.

\bibitem{jacot2021deep}
A.~Jacot, F.~Ged, F.~Gabriel, B.~Simsek, and C.~Hongler, ``Deep linear networks
  dynamics: Low-rank biases induced by initialization scale and l2
  regularization,'' {\em arXiv preprint arXiv:2106.15933}, 2021.

\bibitem{woodworth2020kernel}
B.~Woodworth, S.~Gunasekar, J.~D. Lee, E.~Moroshko, P.~Savarese, I.~Golan,
  D.~Soudry, and N.~Srebro, ``Kernel and rich regimes in overparametrized
  models,'' in {\em Conference on Learning Theory}, pp.~3635--3673, PMLR, 2020.

\bibitem{li2021implicit}
J.~Li, T.~Nguyen, C.~Hegde, and K.~W. Wong, ``Implicit sparse regularization:
  The impact of depth and early stopping,'' {\em Advances in Neural Information
  Processing Systems}, vol.~34, 2021.

\bibitem{chou2021more}
H.-H. Chou, J.~Maly, and H.~Rauhut, ``More is less: Inducing sparsity via
  overparameterization,'' {\em arXiv preprint arXiv:2112.11027}, 2021.

\bibitem{ma2021implicit}
J.~Ma and S.~Fattahi, ``Implicit regularization of sub-gradient method in
  robust matrix recovery: Don't be afraid of outliers,'' {\em arXiv preprint
  arXiv:2102.02969}, 2021.

\bibitem{ding2021rank}
L.~Ding, L.~Jiang, Y.~Chen, Q.~Qu, and Z.~Zhu, ``Rank overspecified robust
  matrix recovery: Subgradient method and exact recovery,'' {\em Advances in
  Neural Information Processing Systems}, vol.~34, 2021.

\bibitem{loshchilov2016sgdr}
I.~Loshchilov and F.~Hutter, ``Sgdr: Stochastic gradient descent with warm
  restarts,'' {\em arXiv preprint arXiv:1608.03983}, 2016.

\bibitem{szegedy2016rethinking}
C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna, ``Rethinking the
  inception architecture for computer vision,'' in {\em Proceedings of the IEEE
  conference on computer vision and pattern recognition}, pp.~2818--2826, 2016.

\bibitem{lukasik2020does}
M.~Lukasik, S.~Bhojanapalli, A.~Menon, and S.~Kumar, ``Does label smoothing
  mitigate label noise?,'' in {\em International Conference on Machine
  Learning}, pp.~6448--6458, PMLR, 2020.

\bibitem{wei2021understanding}
J.~Wei, H.~Liu, T.~Liu, G.~Niu, and Y.~Liu, ``Understanding (generalized) label
  smoothing whenlearning with noisy labels,'' {\em arXiv preprint
  arXiv:2106.04149}, 2021.

\bibitem{zhang2018mixup}
H.~Zhang, M.~Cisse, Y.~N. Dauphin, and D.~Lopez-Paz, ``mixup: Beyond empirical
  risk minimization,'' in {\em International Conference on Learning
  Representations}, 2018.

\bibitem{algan2021image}
G.~Algan and I.~Ulusoy, ``Image classification with deep learning in the
  presence of noisy labels: A survey,'' {\em Knowledge-Based Systems},
  vol.~215, p.~106771, 2021.

\bibitem{ghosh2017robust}
A.~Ghosh, H.~Kumar, and P.~Sastry, ``Robust loss functions under label noise
  for deep neural networks,'' in {\em Proceedings of the AAAI Conference on
  Artificial Intelligence}, vol.~31, 2017.

\bibitem{amid2019robust}
E.~Amid, M.~K. Warmuth, R.~Anil, and T.~Koren, ``Robust bi-tempered logistic
  loss based on bregman divergences,'' {\em Advances in Neural Information
  Processing Systems}, vol.~32, pp.~15013--15022, 2019.

\bibitem{ma2020normalized}
X.~Ma, H.~Huang, Y.~Wang, S.~Romano, S.~Erfani, and J.~Bailey, ``Normalized
  loss functions for deep learning with noisy labels,'' in {\em International
  Conference on Machine Learning}, pp.~6543--6553, PMLR, 2020.

\bibitem{yu2020learning}
Y.~Yu, K.~H.~R. Chan, C.~You, C.~Song, and Y.~Ma, ``Learning diverse and
  discriminative representations via the principle of maximal coding rate
  reduction,'' {\em Advances in Neural Information Processing Systems},
  vol.~33, pp.~9422--9434, 2020.

\bibitem{wei2021optimizing}
J.~Wei and Y.~Liu, ``When optimizing $ f $-divergence is robust with label
  noise,'' in {\em International Conference on Learning Representations}, 2021.

\bibitem{ma2022blessing}
J.~Ma and S.~Fattahi, ``Blessing of nonconvexity in deep linear models: Depth
  flattens the optimization landscape around the true solution,'' {\em arXiv
  preprint arXiv:2207.07612}, 2022.

\bibitem{menon2019can}
A.~K. Menon, A.~S. Rawat, S.~J. Reddi, and S.~Kumar, ``Can gradient clipping
  mitigate label noise?,'' in {\em International Conference on Learning
  Representations}, 2019.

\bibitem{liu2015classification}
T.~Liu and D.~Tao, ``Classification with noisy labels by importance
  reweighting,'' {\em IEEE Transactions on pattern analysis and machine
  intelligence}, vol.~38, no.~3, pp.~447--461, 2015.

\bibitem{wang2017multiclass}
R.~Wang, T.~Liu, and D.~Tao, ``Multiclass learning with partially corrupted
  labels,'' {\em IEEE transactions on neural networks and learning systems},
  vol.~29, no.~6, pp.~2568--2580, 2017.

\bibitem{chang2017active}
H.-S. Chang, E.~Learned-Miller, and A.~McCallum, ``Active bias: Training more
  accurate neural networks by emphasizing high variance samples,'' {\em
  Advances in Neural Information Processing Systems}, vol.~30, pp.~1002--1012,
  2017.

\bibitem{zhang2021dualgraph}
H.~Zhang, X.~Xing, and L.~Liu, ``Dualgraph: A graph-based method for reasoning
  about label noise,'' in {\em Proceedings of the IEEE/CVF Conference on
  Computer Vision and Pattern Recognition}, pp.~9654--9663, 2021.

\bibitem{zetterqvist2021robust}
O.~Zetterqvist, R.~J{\"o}rnsten, and J.~Jonasson, ``Robust neural network
  classification via double regularization,'' {\em arXiv preprint
  arXiv:2112.08102}, 2021.

\bibitem{chen2015webly}
X.~Chen and A.~Gupta, ``Webly supervised learning of convolutional networks,''
  in {\em Proceedings of the IEEE International Conference on Computer Vision},
  pp.~1431--1439, 2015.

\bibitem{goldberger2016training}
J.~Goldberger and E.~Ben-Reuven, ``Training deep neural-networks using a noise
  adaptation layer,'' 2017.

\bibitem{hendrycks2018using}
D.~Hendrycks, M.~Mazeika, D.~Wilson, and K.~Gimpel, ``Using trusted data to
  train deep networks on labels corrupted by severe noise,'' {\em Advances in
  Neural Information Processing Systems}, vol.~31, pp.~10456--10465, 2018.

\bibitem{xia2019anchor}
X.~Xia, T.~Liu, N.~Wang, B.~Han, C.~Gong, G.~Niu, and M.~Sugiyama, ``Are anchor
  points really indispensable in label-noise learning?,'' {\em Advances in
  Neural Information Processing Systems}, vol.~32, pp.~6838--6849, 2019.

\bibitem{zhu2021clusterability}
Z.~Zhu, Y.~Song, and Y.~Liu, ``Clusterability as an alternative to anchor
  points when learning with noisy labels,'' {\em arXiv preprint
  arXiv:2102.05291}, 2021.

\bibitem{li2021provably}
X.~Li, T.~Liu, B.~Han, G.~Niu, and M.~Sugiyama, ``Provably end-to-end
  label-noise learning without anchor points,'' {\em arXiv preprint
  arXiv:2102.02400}, 2021.

\bibitem{zhang2021learning}
Y.~Zhang, G.~Niu, and M.~Sugiyama, ``Learning noise transition matrix from only
  noisy labels via total variation regularization,'' {\em arXiv preprint
  arXiv:2102.02414}, 2021.

\bibitem{kim2021fine}
T.~Kim, J.~Ko, J.~Choi, S.-Y. Yun, {\em et~al.}, ``Fine samples for learning
  with noisy labels,'' {\em Advances in Neural Information Processing Systems},
  vol.~34, 2021.

\bibitem{ma2018dimensionality}
X.~Ma, Y.~Wang, M.~E. Houle, S.~Zhou, S.~Erfani, S.~Xia, S.~Wijewickrema, and
  J.~Bailey, ``Dimensionality-driven learning with noisy labels,'' in {\em
  International Conference on Machine Learning}, pp.~3355--3364, PMLR, 2018.

\bibitem{jiang2020beyond}
L.~Jiang, D.~Huang, M.~Liu, and W.~Yang, ``Beyond synthetic noise: Deep
  learning on controlled noisy labels,'' in {\em International Conference on
  Machine Learning}, pp.~4804--4815, PMLR, 2020.

\bibitem{reed2014training}
S.~Reed, H.~Lee, D.~Anguelov, C.~Szegedy, D.~Erhan, and A.~Rabinovich,
  ``Training deep neural networks on noisy labels with bootstrapping,'' {\em
  arXiv preprint arXiv:1412.6596}, 2014.

\bibitem{song2019selfie}
H.~Song, M.~Kim, and J.-G. Lee, ``Selfie: Refurbishing unclean samples for
  robust deep learning,'' in {\em International Conference on Machine
  Learning}, pp.~5907--5915, PMLR, 2019.

\bibitem{li2020gradient}
M.~Li, M.~Soltanolkotabi, and S.~Oymak, ``Gradient descent with early stopping
  is provably robust to label noise for overparameterized neural networks,'' in
  {\em International conference on artificial intelligence and statistics},
  pp.~4313--4324, PMLR, 2020.

\bibitem{Liu2021AdaptiveEC}
S.~Liu, K.~Liu, W.~Zhu, Y.~Shen, and C.~Fernandez-Granda, ``Adaptive
  early-learning correction for segmentation from noisy annotations,'' {\em
  ArXiv}, vol.~abs/2110.03740, 2021.

\bibitem{lin2021learning}
J.~Z. Lin and J.~Bradic, ``Learning to combat noisy labels via classification
  margins,'' {\em arXiv preprint arXiv:2102.00751}, 2021.

\bibitem{huang2020self}
L.~Huang, C.~Zhang, and H.~Zhang, ``Self-adaptive training: beyond empirical
  risk minimization,'' {\em arXiv preprint arXiv:2002.10319}, 2020.

\bibitem{zheng2020error}
S.~Zheng, P.~Wu, A.~Goswami, M.~Goswami, D.~Metaxas, and C.~Chen,
  ``Error-bounded correction of noisy labels,'' in {\em International
  Conference on Machine Learning}, pp.~11447--11457, PMLR, 2020.

\bibitem{hu2019simple}
W.~Hu, Z.~Li, and D.~Yu, ``Simple and effective regularization methods for
  training on noisily labeled data with generalization guarantee,'' in {\em
  International Conference on Learning Representations}, 2019.

\bibitem{hoefler2021sparsity}
T.~Hoefler, D.~Alistarh, T.~Ben-Nun, N.~Dryden, and A.~Peste, ``Sparsity in
  deep learning: Pruning and growth for efficient inference and training in
  neural networks,'' {\em Journal of Machine Learning Research}, vol.~22,
  no.~241, pp.~1--124, 2021.

\bibitem{liu2021we}
S.~Liu, L.~Yin, D.~C. Mocanu, and M.~Pechenizkiy, ``Do we actually need dense
  over-parameterization? in-time over-parameterization in sparse training,'' in
  {\em International Conference on Machine Learning}, pp.~6989--7000, PMLR,
  2021.

\bibitem{chen2021sparsity}
T.~Chen, Z.~Zhang, S.~Balachandra, H.~Ma, Z.~Wang, Z.~Wang, {\em et~al.},
  ``Sparsity winning twice: Better robust generalization from more efficient
  training,'' in {\em International Conference on Learning Representations},
  2021.

\bibitem{nacson2019stochastic}
M.~S. Nacson, N.~Srebro, and D.~Soudry, ``Stochastic gradient descent on
  separable data: Exact convergence with a fixed learning rate,'' in {\em The
  22nd International Conference on Artificial Intelligence and Statistics},
  pp.~3051--3059, PMLR, 2019.

\bibitem{wang2021momentum}
B.~Wang, Q.~Meng, H.~Zhang, R.~Sun, W.~Chen, and Z.-M. Ma, ``Momentum doesn't
  change the implicit bias,'' {\em arXiv preprint arXiv:2110.03891}, 2021.

\bibitem{tibshirani2021equivalences}
R.~J. Tibshirani, ``Equivalences between sparse models and neural networks,''
  2021.

\bibitem{papyan2020prevalence}
V.~Papyan, X.~Han, and D.~L. Donoho, ``Prevalence of neural collapse during the
  terminal phase of deep learning training,'' {\em Proceedings of the National
  Academy of Sciences}, vol.~117, no.~40, pp.~24652--24663, 2020.

\bibitem{han2021neural}
X.~Han, V.~Papyan, and D.~L. Donoho, ``Neural collapse under mse loss:
  Proximity to and dynamics on the central path,'' {\em arXiv preprint
  arXiv:2106.02073}, 2021.

\bibitem{zhu2021geometric}
Z.~Zhu, T.~Ding, J.~Zhou, X.~Li, C.~You, J.~Sulam, and Q.~Qu, ``A geometric
  analysis of neural collapse with unconstrained features,'' {\em Advances in
  Neural Information Processing Systems}, 2021.

\bibitem{fang2021exploring}
C.~Fang, H.~He, Q.~Long, and W.~J. Su, ``Exploring deep neural networks via
  layer-peeled model: Minority collapse in imbalanced training,'' {\em
  Proceedings of the National Academy of Sciences}, vol.~118, no.~43, 2021.

\bibitem{chan2021redunet}
K.~H.~R. Chan, Y.~Yu, C.~You, H.~Qi, J.~Wright, and Y.~Ma, ``Redunet: A
  white-box deep network from the principle of maximizing rate reduction,''
  {\em arXiv preprint arXiv:2105.10446}, 2021.

\bibitem{Song2019PrestoppingHD}
H.~Song, M.~Kim, D.~Park, and J.-G. Lee, ``Prestopping: How does early stopping
  help generalization against label noise?,'' {\em ArXiv}, vol.~abs/1911.08059,
  2019.

\bibitem{mixmatch}
D.~Berthelot, N.~Carlini, I.~Goodfellow, N.~Papernot, A.~Oliver, and C.~A.
  Raffel, ``Mixmatch: A holistic approach to semi-supervised learning,'' in
  {\em Advances in Neural Information Processing Systems} (H.~Wallach,
  H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle Alch\'{e}-Buc, E.~Fox,
  and R.~Garnett, eds.), vol.~32, Curran Associates, Inc., 2019.

\bibitem{li2019learning}
J.~Li, Y.~Wong, Q.~Zhao, and M.~S. Kankanhalli, ``Learning to learn from noisy
  labeled data,'' in {\em Proceedings of the IEEE/CVF Conference on Computer
  Vision and Pattern Recognition}, pp.~5051--5059, 2019.

\bibitem{Xie2020UnsupervisedDA}
Q.~Xie, Z.~Dai, E.~H. Hovy, M.-T. Luong, and Q.~V. Le, ``Unsupervised data
  augmentation for consistency training,'' {\em arXiv: Learning}, 2020.

\bibitem{tanaka2018joint}
D.~Tanaka, D.~Ikami, T.~Yamasaki, and K.~Aizawa, ``Joint optimization framework
  for learning with noisy labels,'' in {\em Proceedings of the IEEE Conference
  on Computer Vision and Pattern Recognition}, pp.~5552--5560, 2018.

\bibitem{cohen2009compressed}
A.~Cohen, W.~Dahmen, and R.~DeVore, ``Compressed sensing and best $k$-term
  approximation,'' {\em Journal of the American mathematical society}, vol.~22,
  no.~1, pp.~211--231, 2009.

\end{thebibliography}
