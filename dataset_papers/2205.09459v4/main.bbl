\begin{thebibliography}{10}

\bibitem{Anthony:2009}
Martin Anthony and Peter~L. Bartlett.
\newblock {\em Neural Network Learning: Theoretical Foundations}.
\newblock Cambridge University Press, New York, NY, USA, 1st edition, 2009.

\bibitem{APICELLA202114}
Andrea Apicella, Francesco Donnarumma, Francesco Isgrò, and Roberto Prevete.
\newblock A survey on modern trainable activation functions.
\newblock {\em Neural Networks}, 138:14--32, 2021.

\bibitem{Bao2019ApproximationAO}
Chenglong Bao, Qianxiao Li, Zuowei Shen, Cheng Tai, Lei Wu, and Xueshuang
  Xiang.
\newblock Approximation analysis of convolutional neural networks.
\newblock {\em Semantic Scholar e-Preprint}, page Corpus ID: 204762668, 2019.

\bibitem{barron1993}
Andrew~R. Barron.
\newblock Universal approximation bounds for superpositions of a sigmoidal
  function.
\newblock {\em IEEE Transactions on Information Theory}, 39(3):930--945, May
  1993.

\bibitem{barron2018approximation}
Andrew~R. {Barron} and Jason~M. {Klusowski}.
\newblock Approximation and estimation for high-dimensional deep learning
  networks.
\newblock {\em arXiv e-prints}, page arXiv:1809.03090, September 2018.

\bibitem{Bartlett98almostlinear}
Peter Bartlett, Vitaly Maiorov, and Ron Meir.
\newblock Almost linear {VC}-dimension bounds for piecewise polynomial
  networks.
\newblock {\em Neural Computation}, 10(8):2159–2173, 1998.

\bibitem{6697897}
Monica Bianchini and Franco Scarselli.
\newblock On the complexity of neural network classifiers: A comparison between
  shallow and deep architectures.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  25(8):1553--1565, Aug 2014.

\bibitem{doi:10.1137/18M118709X}
Helmut. B{\"o}lcskei, Philipp. Grohs, Gitta. Kutyniok, and Philipp. Petersen.
\newblock Optimal approximation with sparsely connected deep neural networks.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 1(1):8--45, 2019.

\bibitem{Wenjing}
Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao.
\newblock Efficient approximation of deep {ReLU} networks for functions on low
  dimensional manifolds.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{10.3389/fams.2018.00014}
Charles~K. Chui, Shao-Bo Lin, and Ding-Xuan Zhou.
\newblock Construction of neural networks for realization of localized deep
  learning.
\newblock {\em Frontiers in Applied Mathematics and Statistics}, 4:14, 2018.

\bibitem{Cybenko1989ApproximationBS}
George Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock {\em Mathematics of Control, Signals, and Systems}, 2:303--314, 1989.

\bibitem{2019arXiv190501208G}
R{\'e}mi Gribonval, Gitta Kutyniok, Morten Nielsen, and Felix Voigtlaender.
\newblock Approximation spaces of deep neural networks.
\newblock {\em Constructive Approximation}, 55:259--367, 2022.

\bibitem{2019arXiv190207896G}
Ingo {G{\"u}hring}, Gitta {Kutyniok}, and Philipp {Petersen}.
\newblock Error bounds for approximations with deep {ReLU} neural networks in
  ${W}^{s,p}$ norms.
\newblock {\em Analysis and Applications}, 18(05):803--859, 2020.

\bibitem{pmlr-v65-harvey17a}
Nick Harvey, Christopher Liaw, and Abbas Mehrabian.
\newblock Nearly-tight {VC}-dimension bounds for piecewise linear neural
  networks.
\newblock In Satyen Kale and Ohad Shamir, editors, {\em Proceedings of the 2017
  Conference on Learning Theory}, volume~65 of {\em Proceedings of Machine
  Learning Research}, pages 1064--1068, Amsterdam, Netherlands, 07--10 Jul
  2017. PMLR.

\bibitem{7410480}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In {\em 2015 IEEE International Conference on Computer Vision
  (ICCV)}, pages 1026--1034, 2015.

\bibitem{DBLP:journals/corr/abs-1207-0580}
Geoffrey~E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and
  Ruslan Salakhutdinov.
\newblock Improving neural networks by preventing co-adaptation of feature
  detectors.
\newblock {\em CoRR}, abs/1207.0580, 2012.

\bibitem{HORNIK1991251}
Kurt Hornik.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock {\em Neural Networks}, 4(2):251--257, 1991.

\bibitem{HORNIK1989359}
Kurt Hornik, Maxwell Stinchcombe, and Halbert White.
\newblock Multilayer feedforward networks are universal approximators.
\newblock {\em Neural Networks}, 2(5):359--366, 1989.

\bibitem{10.5555/3045118.3045167}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em Proceedings of the 32nd International Conference on
  International Conference on Machine Learning - Volume 37}, ICML'15, page
  448–456. JMLR.org, 2015.

\bibitem{jiao2021deep}
Yuling {Jiao}, Yanming {Lai}, Xiliang {Lu}, Fengru {Wang}, Jerry {Zhijian
  Yang}, and Yuanyuan {Yang}.
\newblock Deep neural networks with {ReLU-Sine-Exponential} activations break
  curse of dimensionality on h{\"o}lder class.
\newblock {\em arXiv e-prints}, page arXiv:2103.00542, February 2021.

\bibitem{Kearns}
Michael~J. Kearns and Robert~E. Schapire.
\newblock Efficient distribution-free learning of probabilistic concepts.
\newblock {\em J. Comput. Syst. Sci.}, 48(3):464--497, June 1994.

\bibitem{2019arXiv191210382L}
Qianxiao {Li}, Ting {Lin}, and Zuowei {Shen}.
\newblock Deep learning via dynamical systems: An approximation perspective.
\newblock {\em Journal of the European Mathematical Society}, to appear.

\bibitem{Liu2020On}
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng
  Gao, and Jiawei Han.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{shijun:3}
Jianfeng Lu, Zuowei Shen, Haizhao Yang, and Shijun Zhang.
\newblock Deep network approximation for smooth functions.
\newblock {\em SIAM Journal on Mathematical Analysis}, 53(5):5465--5506, 2021.

\bibitem{MO}
Hadrien Montanelli and Haizhao Yang.
\newblock Error bounds for deep {ReLU} networks using the {Kolmogorov-Arnold}
  superposition theorem.
\newblock {\em Neural Networks}, 129:1--6, 2020.

\bibitem{bandlimit}
Hadrien {Montanelli}, Haizhao {Yang}, and Qiang {Du}.
\newblock Deep {ReLU} networks overcome the curse of dimensionality for
  bandlimited functions.
\newblock {\em Journal of Computational Mathematics}, 39(6):801--815, 2021.

\bibitem{NIPS2014_5422}
Guido~F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio.
\newblock On the number of linear regions of deep neural networks.
\newblock In Z.~Ghahramani, M.~Welling, C.~Cortes, N.~D. Lawrence, and K.~Q.
  Weinberger, editors, {\em Advances in Neural Information Processing Systems
  27}, pages 2924--2932. Curran Associates, Inc., 2014.

\bibitem{Ryumei}
Ryumei Nakada and Masaaki Imaizumi.
\newblock Adaptive approximation and generalization of deep neural network with
  intrinsic dimensionality.
\newblock {\em Journal of Machine Learning Research}, 21(174):1--38, 2020.

\bibitem{PETERSEN2018296}
Philipp Petersen and Felix Voigtlaender.
\newblock Optimal approximation of piecewise smooth functions using deep {ReLU}
  neural networks.
\newblock {\em Neural Networks}, 108:296--330, 2018.

\bibitem{2006.10598}
Bryan~A. Plummer, Nikoli Dryden, Julius Frost, Torsten Hoefler, and Kate
  Saenko.
\newblock Neural parameter allocation search.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{8546022}
Suo Qiu, Xiangmin Xu, and Bolun Cai.
\newblock Frelu: Flexible rectified linear units for improving convolutional
  neural networks.
\newblock In {\em 2018 24th International Conference on Pattern Recognition
  (ICPR)}, pages 1223--1228, Los Alamitos, CA, USA, aug 2018. IEEE Computer
  Society.

\bibitem{Sakurai}
Akito Sakurai.
\newblock Tight bounds for the {VC}-dimension of piecewise polynomial networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  323--329. Neural information processing systems foundation, 1999.

\bibitem{savarese2018learning}
Pedro Savarese and Michael Maire.
\newblock Learning implicitly recurrent {CNN}s through parameter sharing.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{shijun:1}
Zuowei Shen, Haizhao Yang, and Shijun Zhang.
\newblock Nonlinear approximation via compositions.
\newblock {\em Neural Networks}, 119:74--84, 2019.

\bibitem{shijun:2}
Zuowei Shen, Haizhao Yang, and Shijun Zhang.
\newblock Deep network approximation characterized by number of neurons.
\newblock {\em Communications in Computational Physics}, 28(5):1768--1811,
  2020.

\bibitem{shijun:4}
Zuowei Shen, Haizhao Yang, and Shijun Zhang.
\newblock Deep network with approximation error being reciprocal of width to
  power of square root of depth.
\newblock {\em Neural Computation}, 33(4):1005--1036, 03 2021.

\bibitem{shijun:5}
Zuowei Shen, Haizhao Yang, and Shijun Zhang.
\newblock Neural network approximation: {T}hree hidden layers are enough.
\newblock {\em Neural Networks}, 141:160--173, 2021.

\bibitem{shijun:arbitrary:error:with:fixed:size}
Zuowei Shen, Haizhao Yang, and Shijun Zhang.
\newblock Deep network approximation: Achieving arbitrary accuracy with fixed
  number of neurons.
\newblock {\em Journal of Machine Learning Research}, 23(276):1--60, 2022.

\bibitem{shijun:intrinsic:parameters}
Zuowei Shen, Haizhao Yang, and Shijun Zhang.
\newblock Deep network approximation in terms of intrinsic parameters.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, {\em Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of {\em Proceedings
  of Machine Learning Research}, pages 19909--19934. PMLR, 17--23 Jul 2022.

\bibitem{shijun:6}
Zuowei Shen, Haizhao Yang, and Shijun Zhang.
\newblock Optimal approximation rate of {ReLU} networks in terms of width and
  depth.
\newblock {\em Journal de Mathématiques Pures et Appliquées}, 157:101--135,
  2022.

\bibitem{JMLR:v15:srivastava14a}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock {\em Journal of Machine Learning Research}, 15(56):1929--1958, 2014.

\bibitem{suzuki2018adaptivity}
Taiji Suzuki.
\newblock Adaptivity of deep {ReLU} network for learning in {Besov} and mixed
  smooth {Besov} spaces: optimal rate and curse of dimensionality.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{Trottier2017ParametricEL}
Ludovic Trottier, Philippe Gigu{\`e}re, and Brahim Chaib-draa.
\newblock Parametric exponential linear unit for deep convolutional neural
  networks.
\newblock {\em 2017 16th IEEE International Conference on Machine Learning and
  Applications (ICMLA)}, pages 207--214, 2017.

\bibitem{9879069}
Matthew Wallingford, Hao Li, Alessandro Achille, Avinash Ravichandran, Charless
  Fowlkes, Rahul Bhotika, and Stefano Soatto.
\newblock Task adaptive parameter sharing for multi-task learning.
\newblock In {\em 2022 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 7551--7560, 2022.

\bibitem{NEURIPS2020_42cd63cb}
Jiaxing Wang, Haoli Bai, Jiaxiang Wu, Xupeng Shi, Junzhou Huang, Irwin King,
  Michael Lyu, and Jian Cheng.
\newblock Revisiting parameter sharing for automatic neural channel number
  search.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 5991--6002. Curran Associates, Inc., 2020.

\bibitem{2020arXiv200902386W}
Ze~{Wang}, Xiuyuan {Cheng}, Guillermo {Sapiro}, and Qiang {Qiu}.
\newblock {ACDC}: Weight sharing in atom-coefficient decomposed convolution.
\newblock {\em arXiv e-prints}, page arXiv:2009.02386, September 2020.

\bibitem{2017arXiv170807747X}
Han {Xiao}, Kashif {Rasul}, and Roland {Vollgraf}.
\newblock {Fashion-MNIST}: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock {\em arXiv e-prints}, page arXiv:1708.07747, August 2017.

\bibitem{yarotsky2017}
Dmitry Yarotsky.
\newblock Error bounds for approximations with deep {ReLU} networks.
\newblock {\em Neural Networks}, 94:103--114, 2017.

\bibitem{yarotsky18a}
Dmitry Yarotsky.
\newblock Optimal approximation of continuous functions by very deep {ReLU}
  networks.
\newblock In S\'ebastien Bubeck, Vianney Perchet, and Philippe Rigollet,
  editors, {\em Proceedings of the 31st Conference On Learning Theory},
  volume~75 of {\em Proceedings of Machine Learning Research}, pages 639--649.
  PMLR, 06--09 Jul 2018.

\bibitem{yarotsky:2019:06}
Dmitry Yarotsky and Anton Zhevnerchuk.
\newblock The phase diagram of approximation rates for deep neural networks.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 13005--13015. Curran Associates, Inc., 2020.

\bibitem{9859706}
Lijun Zhang, Qizheng Yang, Xiao Liu, and Hui Guan.
\newblock Rethinking hard-parameter sharing in multi-domain learning.
\newblock In {\em 2022 IEEE International Conference on Multimedia and Expo
  (ICME)}, pages 01--06, 2022.

\bibitem{shijun:thesis}
Shijun Zhang.
\newblock Deep neural network approximation via function compositions.
\newblock {\em PhD Thesis, National University of Singapore}, 2020.
\newblock URL: \url{https://scholarbank.nus.edu.sg/handle/10635/186064}.

\bibitem{ZHOU2019}
Ding-Xuan Zhou.
\newblock Universality of deep convolutional neural networks.
\newblock {\em Applied and Computational Harmonic Analysis}, 48(2):787--794,
  2020.

\end{thebibliography}
