\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbas et~al.(2022)Abbas, Xiao, Chen, Chen, and
  Chen]{Abbas2022Sharp-MAML}
Abbas, M., Xiao, Q., Chen, L., Chen, P., and Chen, T.
\newblock Sharp-maml: Sharpness-aware model-agnostic meta learning.
\newblock In \emph{International Conference on Machine Learning, {ICML}}, pp.\
  10--32, 2022.

\bibitem[Acar et~al.(2021)Acar, Zhao, Navarro, Mattina, Whatmough, and
  Saligrama]{Durmus2021Federated}
Acar, D. A.~E., Zhao, Y., Navarro, R.~M., Mattina, M., Whatmough, P.~N., and
  Saligrama, V.
\newblock Federated learning based on dynamic regularization.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR}}, 2021.

\bibitem[Andriushchenko \& Flammarion(2022)Andriushchenko and
  Flammarion]{Andriushchenko2022Towards}
Andriushchenko, M. and Flammarion, N.
\newblock Towards understanding sharpness-aware minimization.
\newblock In \emph{International Conference on Machine Learning, {ICML}},
  Proceedings of Machine Learning Research, pp.\  639--668. {PMLR}, 2022.

\bibitem[Beltr{\'a}n et~al.(2022)Beltr{\'a}n, P{\'e}rez, S{\'a}nchez, Bernal,
  Bovet, P{\'e}rez, P{\'e}rez, and Celdr{\'a}n]{beltran2022decentralized}
Beltr{\'a}n, E. T.~M., P{\'e}rez, M.~Q., S{\'a}nchez, P. M.~S., Bernal, S.~L.,
  Bovet, G., P{\'e}rez, M.~G., P{\'e}rez, G.~M., and Celdr{\'a}n, A.~H.
\newblock Decentralized federated learning: Fundamentals, state-of-the-art,
  frameworks, trends, and challenges.
\newblock \emph{arXiv preprint arXiv:2211.08413}, 2022.

\bibitem[Bottou(2010)]{bottou2010large}
Bottou, L.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In \emph{Proceedings of COMPSTAT'2010}, pp.\  177--186. Springer,
  2010.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and
  Nocedal]{bottou2018optimization}
Bottou, L., Curtis, F.~E., and Nocedal, J.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{Siam Review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Caldarola et~al.(2022)Caldarola, Caputo, and
  Ciccone]{Caldarola2022Improving}
Caldarola, D., Caputo, B., and Ciccone, M.
\newblock Improving generalization in federated learning by seeking flat
  minima.
\newblock \emph{CoRR}, abs/2203.11834, 2022.

\bibitem[Chen et~al.(2021)Chen, Zhang, Shen, Zhao, and
  Luo]{chen2021communication}
Chen, C., Zhang, J., Shen, L., Zhao, P., and Luo, Z.
\newblock Communication efficient primal-dual algorithm for nonconvex nonsmooth
  distributed optimization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1594--1602. PMLR, 2021.

\bibitem[Chen \& Chao(2021)Chen and Chao]{chen2021bridging}
Chen, H.-Y. and Chao, W.-L.
\newblock On bridging generic and personalized federated learning for image
  classification.
\newblock \emph{arXiv preprint arXiv:2107.00778}, 2021.

\bibitem[Dai et~al.(2022)Dai, Shen, He, Tian, and Tao]{Rong2022DisPFL}
Dai, R., Shen, L., He, F., Tian, X., and Tao, D.
\newblock Dispfl: Towards communication-efficient personalized federated
  learning via decentralized sparse training.
\newblock In \emph{International Conference on Machine Learning, {ICML}},
  Proceedings of Machine Learning Research, pp.\  4587--4604. {PMLR}, 2022.

\bibitem[Deng et~al.(2020)Deng, Kamani, and Mahdavi]{deng2020adaptive}
Deng, Y., Kamani, M.~M., and Mahdavi, M.
\newblock Adaptive personalized federated learning.
\newblock \emph{arXiv preprint arXiv:2003.13461}, 2020.

\bibitem[Du et~al.(2021)Du, Yan, Feng, Zhou, Zhen, Goh, and
  Tan]{du2021efficient}
Du, J., Yan, H., Feng, J., Zhou, J.~T., Zhen, L., Goh, R. S.~M., and Tan, V.
\newblock Efficient sharpness-aware minimization for improved training of
  neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2021sharpnessaware}
Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013stochastic}
Ghadimi, S. and Lan, G.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Hashemi et~al.(2022)Hashemi, Acharya, Das, Vikalo, Sanghavi, and
  Dhillon]{Hashemi2022On}
Hashemi, A., Acharya, A., Das, R., Vikalo, H., Sanghavi, S., and Dhillon, I.
\newblock On the benefits of multiple gossip steps in communication-constrained
  decentralized federated learning.
\newblock \emph{IEEE Transactions on Parallel and Distributed Systems, {TPDS}},
  pp.\  2727--2739, 2022.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hsu et~al.(2019)Hsu, Qi, and Brown]{hsu2019measuring}
Hsu, T.-M.~H., Qi, H., and Brown, M.
\newblock Measuring the effects of non-identical data distribution for
  federated visual classification.
\newblock \emph{arXiv preprint arXiv:1909.06335}, 2019.

\bibitem[Huang et~al.(2021)Huang, Chu, Zhou, Wang, Liu, Pei, and
  Zhang]{huang2021personalized}
Huang, Y., Chu, L., Zhou, Z., Wang, L., Liu, J., Pei, J., and Zhang, Y.
\newblock Personalized cross-silo federated learning on non-iid data.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pp.\  7865--7873, 2021.

\bibitem[Kairouz et~al.(2021)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji,
  Bonawitz, Charles, Cormode, Cummings, et~al.]{Kairouz2021Advances}
Kairouz, P., McMahan, H.~B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A.~N.,
  Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et~al.
\newblock Advances and open problems in federated learning.
\newblock \emph{Foundations and Trends in Machine Learning}, pp.\  1--210,
  2021.

\bibitem[Kang et~al.(2022)Kang, Ye, Nie, Xiao, Deng, Wang, Xiong, Yu, and
  Niyato]{kang2022blockchain}
Kang, J., Ye, D., Nie, J., Xiao, J., Deng, X., Wang, S., Xiong, Z., Yu, R., and
  Niyato, D.
\newblock Blockchain-based federated learning for industrial metaverses:
  Incentive scheme with optimal aoi.
\newblock In \emph{2022 IEEE International Conference on Blockchain
  (Blockchain)}, pp.\  71--78. IEEE, 2022.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2020scaffold}
Karimireddy, S.~P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh,
  A.~T.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5132--5143. PMLR, 2020.

\bibitem[Koloskova et~al.(2020)Koloskova, Loizou, Boreiri, Jaggi, and
  Stich]{koloskova2020unified}
Koloskova, A., Loizou, N., Boreiri, S., Jaggi, M., and Stich, S.
\newblock A unified theory of decentralized sgd with changing topology and
  local updates.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5381--5393. PMLR, 2020.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kwon et~al.(2021)Kwon, Kim, Park, and Choi]{kwon2021asam}
Kwon, J., Kim, J., Park, H., and Choi, I.~K.
\newblock Asam: Adaptive sharpness-aware minimization for scale-invariant
  learning of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5905--5914. PMLR, 2021.

\bibitem[Lalitha et~al.(2018)Lalitha, Shekhar, Javidi, and
  Koushanfar]{lalitha2018fully}
Lalitha, A., Shekhar, S., Javidi, T., and Koushanfar, F.
\newblock Fully decentralized federated learning.
\newblock In \emph{Third workshop on Bayesian Deep Learning (NeurIPS)}, 2018.

\bibitem[Lalitha et~al.(2019)Lalitha, Kilinc, Javidi, and
  Koushanfar]{lalitha2019peer}
Lalitha, A., Kilinc, O.~C., Javidi, T., and Koushanfar, F.
\newblock Peer-to-peer federated learning on graphs.
\newblock \emph{arXiv preprint arXiv:1901.11173}, 2019.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Cen, Chen, and
  Chi]{li2020communication}
Li, B., Cen, S., Chen, Y., and Chi, Y.
\newblock Communication-efficient distributed optimization in networks with
  gradient tracking and variance reduction.
\newblock \emph{Journal of Machine Learning Research, {JMLR}}, pp.\
  180:1--180:51, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Li et~al.(2022)Li, Shao, Wei, Ding, Ma, Shi, Han, and
  Poor]{LiJunBlockchain}
Li, J., Shao, Y., Wei, K., Ding, M., Ma, C., Shi, L., Han, Z., and Poor, H.~V.
\newblock Blockchain assisted decentralized federated learning (blade-fl):
  Performance analysis and resource allocation.
\newblock \emph{IEEE Transactions on Parallel and Distributed Systems},
  33\penalty0 (10):\penalty0 2401--2415, 2022.
\newblock \doi{10.1109/TPDS.2021.3138848}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Sahu, Talwalkar, and Smith]{Li2020fl}
Li, T., Sahu, A.~K., Talwalkar, A., and Smith, V.
\newblock Federated learning: {Challenges}, methods, and future directions.
\newblock \emph{IEEE Signal Processing Magazine}, pp.\  50--60,
  2020{\natexlab{b}}.

\bibitem[Li et~al.(2020{\natexlab{c}})Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{li2020federated}
Li, T., Sahu, A.~K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V.
\newblock Federated optimization in heterogeneous networks.
\newblock \emph{Proceedings of Machine Learning and Systems}, pp.\  429--450,
  2020{\natexlab{c}}.

\bibitem[Li et~al.(2021)Li, Hu, Beirami, and Smith]{li2021ditto}
Li, T., Hu, S., Beirami, A., and Smith, V.
\newblock Ditto: Fair and robust federated learning through personalization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6357--6368. PMLR, 2021.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017can}
Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and Liu, J.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5330--5340, 2017.

\bibitem[Liu et~al.(2022)Liu, Mai, Chen, Hsieh, and You]{liu2022towards}
Liu, Y., Mai, S., Chen, X., Hsieh, C.-J., and You, Y.
\newblock Towards efficient and scalable sharpness-aware minimization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  12360--12370, 2022.

\bibitem[Mcmahan et~al.(2017)Mcmahan, Moore, Ramage, Hampson, and
  Arcas]{mcmahan2017communication}
Mcmahan, H.~B., Moore, E., Ramage, D., Hampson, S., and Arcas, B. A.~Y.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock pp.\  1273--1282, 2017.

\bibitem[Mi et~al.(2022)Mi, Shen, Ren, Zhou, Sun, Ji, and Tao]{mi2022make}
Mi, P., Shen, L., Ren, T., Zhou, Y., Sun, X., Ji, R., and Tao, D.
\newblock Make sharpness-aware minimization stronger: A sparsified perturbation
  approach.
\newblock \emph{arXiv preprint arXiv:2210.05177}, 2022.

\bibitem[Nedic \& Ozdaglar(2009)Nedic and Ozdaglar]{nedic2009distributed}
Nedic, A. and Ozdaglar, A.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 54\penalty0
  (1):\penalty0 48--61, 2009.

\bibitem[Nguyen et~al.(2022)Nguyen, Dakka, Diakiw, VerMilyea, Perugini, Hall,
  and Perugini]{nguyen2022novel}
Nguyen, T., Dakka, M., Diakiw, S., VerMilyea, M., Perugini, M., Hall, J., and
  Perugini, D.
\newblock A novel decentralized federated learning approach to train on
  globally distributed, poor quality, and protected private medical data.
\newblock \emph{Scientific Reports}, 12\penalty0 (1):\penalty0 8888, 2022.

\bibitem[Qu et~al.(2022)Qu, Li, Duan, Liu, Tang, and Lu]{Qu2022Generalized}
Qu, Z., Li, X., Duan, R., Liu, Y., Tang, B., and Lu, Z.
\newblock Generalized federated learning via sharpness aware minimization.
\newblock In \emph{International Conference on Machine Learning, {ICML}}, pp.\
  18250--18280, 2022.

\bibitem[Reddi et~al.(2021)Reddi, Charles, Zaheer, Garrett, Rush,
  Kone{\v{c}}n{\'y}, Kumar, and McMahan]{reddi2020adaptive}
Reddi, S.~J., Charles, Z., Zaheer, M., Garrett, Z., Rush, K.,
  Kone{\v{c}}n{\'y}, J., Kumar, S., and McMahan, H.~B.
\newblock Adaptive federated optimization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=LkFG3lB13U5}.

\bibitem[Roy et~al.(2019)Roy, Siddiqui, P{\"o}lsterl, Navab, and
  Wachinger]{roy2019braintorrent}
Roy, A.~G., Siddiqui, S., P{\"o}lsterl, S., Navab, N., and Wachinger, C.
\newblock Braintorrent: A peer-to-peer environment for decentralized federated
  learning.
\newblock \emph{arXiv preprint arXiv:1905.06731}, 2019.

\bibitem[Sahu et~al.(2018)Sahu, Li, Sanjabi, Zaheer, Talwalkar, and
  Smith]{sahu2018convergence}
Sahu, A.~K., Li, T., Sanjabi, M., Zaheer, M., Talwalkar, A., and Smith, V.
\newblock On the convergence of federated optimization in heterogeneous
  networks.
\newblock \emph{arXiv preprint arXiv:1812.06127}, pp.\ ~3, 2018.

\bibitem[Shi et~al.(2023)Shi, Liu, Wei, Shen, Wang, and Tao]{shi2023make}
Shi, Y., Liu, Y., Wei, K., Shen, L., Wang, X., and Tao, D.
\newblock Make landscape flatter in differentially private federated learning.
\newblock \emph{arXiv preprint arXiv:2303.11242}, 2023.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Sun et~al.(2022)Sun, Li, and Wang]{Sun2022Decentralized}
Sun, T., Li, D., and Wang, B.
\newblock Decentralized federated averaging.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2022.

\bibitem[Sun et~al.(2023)Sun, Shen, Huang, Ding, and Tao]{sun2023fedspeed}
Sun, Y., Shen, L., Huang, T., Ding, L., and Tao, D.
\newblock Fedspeed: Larger local interval, less communication round, and higher
  generalization accuracy.
\newblock \emph{arXiv preprint arXiv:2302.10429}, 2023.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Marella, and
  Anderson]{wang2022fedadmm}
Wang, H., Marella, S., and Anderson, J.
\newblock Fedadmm: A federated primal-dual algorithm allowing partial
  participation.
\newblock \emph{arXiv preprint arXiv:2203.15104}, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2021)Wang, Charles, Xu, Joshi, McMahan, Al-Shedivat,
  Andrew, Avestimehr, Daly, Data, et~al.]{wang2021field}
Wang, J., Charles, Z., Xu, Z., Joshi, G., McMahan, H.~B., Al-Shedivat, M.,
  Andrew, G., Avestimehr, S., Daly, K., Data, D., et~al.
\newblock A field guide to federated optimization.
\newblock \emph{arXiv preprint arXiv:2107.06917}, 2021.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Xu, Xu, Chen, and
  Huang]{wang2022accelerating}
Wang, L., Xu, Y., Xu, H., Chen, M., and Huang, L.
\newblock Accelerating decentralized federated learning in heterogeneous edge
  computing.
\newblock \emph{IEEE Transactions on Mobile Computing}, 2022{\natexlab{b}}.

\bibitem[Wang et~al.(2020)Wang, Su, Zhang, and Benslimane]{wang2020learning}
Wang, Y., Su, Z., Zhang, N., and Benslimane, A.
\newblock Learning in the air: Secure federated learning for uav-assisted
  crowdsensing.
\newblock \emph{IEEE Transactions on network science and engineering},
  8\penalty0 (2):\penalty0 1055--1069, 2020.

\bibitem[Warnat-Herresthal et~al.(2021)Warnat-Herresthal, Schultze, Shastry,
  Manamohan, Mukherjee, Garg, Sarveswara, H{\"a}ndler, Pickkers, Aziz,
  et~al.]{warnat2021swarm}
Warnat-Herresthal, S., Schultze, H., Shastry, K.~L., Manamohan, S., Mukherjee,
  S., Garg, V., Sarveswara, R., H{\"a}ndler, K., Pickkers, P., Aziz, N.~A.,
  et~al.
\newblock Swarm learning for decentralized and confidential clinical machine
  learning.
\newblock \emph{Nature}, pp.\  265--270, 2021.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Yang et~al.(2021)Yang, Fang, and Liu]{yang2021achieving}
Yang, H., Fang, M., and Liu, J.
\newblock Achieving linear speedup with partial worker participation in
  non-{IID} federated learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Yao et~al.(2020)Yao, Gholami, Keutzer, and Mahoney]{yao2020pyhessian}
Yao, Z., Gholami, A., Keutzer, K., and Mahoney, M.~W.
\newblock Pyhessian: Neural networks through the lens of the hessian.
\newblock In \emph{2020 IEEE international conference on big data (Big data)},
  pp.\  581--590. IEEE, 2020.

\bibitem[Ye \& Zhang(2021)Ye and Zhang]{ye2021deepca}
Ye, H. and Zhang, T.
\newblock Deepca: Decentralized exact pca with linear convergence rate.
\newblock \emph{J. Mach. Learn. Res.}, 22\penalty0 (238):\penalty0 1--27, 2021.

\bibitem[Ye et~al.(2020)Ye, Zhou, Luo, and Zhang]{ye2020decentralized}
Ye, H., Zhou, Z., Luo, L., and Zhang, T.
\newblock Decentralized accelerated proximal gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 18308--18317, 2020.

\bibitem[Yu et~al.(2019)Yu, Yang, and Zhu]{yu2019parallel}
Yu, H., Yang, S., and Zhu, S.
\newblock Parallel restarted sgd with faster convergence and less
  communication: Demystifying why model averaging works for deep learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pp.\  5693--5700, 2019.

\bibitem[Yu et~al.(2020)Yu, Hu, Min, Xu, and Mills]{yu2020proactive}
Yu, Z., Hu, J., Min, G., Xu, H., and Mills, J.
\newblock Proactive content caching for internet-of-vehicles based on
  peer-to-peer federated learning.
\newblock In \emph{2020 IEEE 26th International Conference on Parallel and
  Distributed Systems (ICPADS)}, pp.\  601--608. IEEE, 2020.

\bibitem[Yuan et~al.(2021)Yuan, Chen, Sun, Wang, Hua, Yi, Yang, and
  Liu]{yuan2021defed}
Yuan, Y., Chen, R., Sun, C., Wang, M., Hua, F., Yi, X., Yang, T., and Liu, J.
\newblock Defed: A principled decentralized and privacy-preserving federated
  learning algorithm.
\newblock \emph{arXiv preprint arXiv:2107.07171}, 2021.

\bibitem[Zhang et~al.(2020)Zhang, Sapra, Fidler, Yeung, and
  Alvarez]{zhang2020personalized}
Zhang, M., Sapra, K., Fidler, S., Yeung, S., and Alvarez, J.~M.
\newblock Personalized federated learning with first order model optimization.
\newblock \emph{arXiv preprint arXiv:2012.08565}, 2020.

\bibitem[Zhang et~al.(2022)Zhang, Fang, Liu, Yang, Liu, and Zhu]{zhang2022net}
Zhang, X., Fang, M., Liu, Z., Yang, H., Liu, J., and Zhu, Z.
\newblock Net-fleet: Achieving linear convergence speedup for fully
  decentralized federated learning with heterogeneous data.
\newblock \emph{arXiv preprint arXiv:2208.08490}, 2022.

\bibitem[Zhao et~al.(2022)Zhao, Zhang, and Hu]{Zhao2022Penalizing}
Zhao, Y., Zhang, H., and Hu, X.
\newblock Penalizing gradient norm for efficiently improving generalization in
  deep learning.
\newblock In \emph{International Conference on Machine Learning, {ICML}}, pp.\
  26982--26992. {PMLR}, 2022.

\bibitem[Zhong et~al.(2022)Zhong, Ding, Shen, Mi, Liu, Du, and
  Tao]{zhong2022improving}
Zhong, Q., Ding, L., Shen, L., Mi, P., Liu, J., Du, B., and Tao, D.
\newblock Improving sharpness-aware minimization with fisher mask for better
  generalization on language models.
\newblock \emph{arXiv preprint arXiv:2210.05497}, 2022.

\bibitem[Zhu et~al.(2022)Zhu, He, Zhang, Niu, Song, and Tao]{zhu2022topology}
Zhu, T., He, F., Zhang, L., Niu, Z., Song, M., and Tao, D.
\newblock Topology-aware generalization of decentralized sgd.
\newblock In \emph{International Conference on Machine Learning, {ICML}}, pp.\
  27479--27503. PMLR, 2022.

\end{thebibliography}
