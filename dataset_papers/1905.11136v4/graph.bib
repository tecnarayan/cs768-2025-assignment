@article{murphy2019relational,
  title={Relational Pooling for Graph Representations},
  author={Murphy, Ryan L and Srinivasan, Balasubramaniam and Rao, Vinayak and Ribeiro, Bruno},
  journal={arXiv preprint arXiv:1903.02541},
  year={2019}
}

@article{keriven2019universal,
  author    = {Nicolas Keriven and
               Gabriel Peyr{\'{e}}},
  title     = {Universal Invariant and Equivariant Graph Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1905.04943},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.04943},
  archivePrefix = {arXiv},
  eprint    = {1905.04943},
  timestamp = {Tue, 28 May 2019 12:48:08 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1905-04943},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}
@article{hornik1989multilayer,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
  publisher={Elsevier}
}
@inproceedings{lei2017deriving,
  title={Deriving neural architectures from sequence and graph kernels},
  author={Lei, Tao and Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2024--2033},
  year={2017},
  organization={JMLR. org}
}
@article{morris2019towards,
  title={Towards a practical $ k $-dimensional Weisfeiler-Leman algorithm},
  author={Morris, Christopher and Mutzel, Petra},
  journal={arXiv preprint arXiv:1904.01543},
  year={2019}
}
@inproceedings{zhang2017weisfeiler,
  title={Weisfeiler-Lehman neural machine for link prediction},
  author={Zhang, Muhan and Chen, Yixin},
  booktitle={Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={575--583},
  year={2017},
  organization={ACM}
}

@article{fey2019fast,
  title={Fast Graph Representation Learning with PyTorch Geometric},
  author={Fey, Matthias and Lenssen, Jan Eric},
  journal={arXiv preprint arXiv:1903.02428},
  year={2019}
}
@article{wu2018moleculenet,
  title={MoleculeNet: a benchmark for molecular machine learning},
  author={Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S and Leswing, Karl and Pande, Vijay},
  journal={Chemical science},
  volume={9},
  number={2},
  pages={513--530},
  year={2018},
  publisher={Royal Society of Chemistry}
}

@article{ramakrishnan2014quantum,
  title={Quantum chemistry structures and properties of 134 kilo molecules},
  author={Ramakrishnan, Raghunathan and Dral, Pavlo O and Rupp, Matthias and Von Lilienfeld, O Anatole},
  journal={Scientific data},
  volume={1},
  pages={140022},
  year={2014},
  publisher={Nature Publishing Group}
}

@incollection{NIPS2018_7951,
title = {Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction},
author = {Herzig, Roei and Raboh, Moshiko and Chechik, Gal and Berant, Jonathan and Globerson, Amir},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {7211--7221},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction.pdf}
}

@misc{kriege2019survey,
    title={A Survey on Graph Kernels},
    author={Nils M. Kriege and Fredrik D. Johansson and Christopher Morris},
    year={2019},
    eprint={1903.11835},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{hamilton2017representation,
  title={Representation learning on graphs: Methods and applications},
  author={Hamilton, William L and Ying, Rex and Leskovec, Jure},
  journal={arXiv preprint arXiv:1709.05584},
  year={2017}
}
@article{zhou2018graph,
  title={Graph neural networks: A review of methods and applications},
  author={Zhou, Jie and Cui, Ganqu and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:1812.08434},
  year={2018}
}

@article{DBLP:journals/corr/abs-1812-04202,
  author    = {Ziwei Zhang and
               Peng Cui and
               Wenwu Zhu},
  title     = {Deep Learning on Graphs: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/1812.04202},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.04202},
  archivePrefix = {arXiv},
  eprint    = {1812.04202},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1812-04202},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{wu2019comprehensive,
  title={A comprehensive survey on graph neural networks},
  author={Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S},
  journal={arXiv preprint arXiv:1901.00596},
  year={2019}
}

@inproceedings{babai2016graph,
  title={Graph isomorphism in quasipolynomial time},
  author={Babai, L{\'a}szl{\'o}},
  booktitle={Proceedings of the forty-eighth annual ACM symposium on Theory of Computing},
  pages={684--697},
  year={2016},
  organization={ACM}
}

@article{vishwanathan2010graph,
  title={Graph kernels},
  author={Vishwanathan, S Vichy N and Schraudolph, Nicol N and Kondor, Risi and Borgwardt, Karsten M},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Apr},
  pages={1201--1242},
  year={2010}
}

@inproceedings{
xu2018how,
title={How Powerful are Graph Neural Networks?},
author={Keyulu Xu and Weihua Hu and Jure Leskovec and Stefanie Jegelka},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=ryGs6iA5Km},
}

@inproceedings{maron2019universality,
  title={On the Universality of Invariant Networks},
  author={Maron, Haggai and Fetaya, Ethan and Segol, Nimrod and Lipman, Yaron},
  booktitle={International conference on machine learning},
  year={2019}
}


@article{morris2018weisfeiler,
  title={Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks},
  author={Morris, Christopher and Ritzert, Martin and Fey, Matthias and Hamilton, William L and Lenssen, Jan Eric and Rattan, Gaurav and Grohe, Martin},
  journal={arXiv preprint arXiv:1810.02244},
  year={2018}
}

@inproceedings{morris2017glocalized,
  title={Glocalized Weisfeiler-Lehman graph kernels: Global-local feature maps of graphs},
  author={Morris, Christopher and Kersting, Kristian and Mutzel, Petra},
  booktitle={2017 IEEE International Conference on Data Mining (ICDM)},
  pages={327--336},
  year={2017},
  organization={IEEE}
}

@inproceedings{rydh2007minimal,
  title={A minimal set of generators for the ring of multisymmetric functions},
  author={Rydh, David},
  booktitle={Annales de l'institut Fourier},
  volume={57},
  number={6},
  pages={1741--1769},
  year={2007}
}
@article{briand2004algebra,
  title={When is the algebra of multisymmetric polynomials generated by the elementary multisymmetric polynomials},
  author={Briand, Emmanuel},
  journal={Contributions to Algebra and Geometry},
  volume={45},
  number={2},
  pages={353--368},
  year={2004}
}

@article{ivanov2018anonymous,
	title={Anonymous Walk Embeddings},
	author={Ivanov, Sergey and Burnaev, Evgeny},
	journal={arXiv preprint arXiv:1805.11921},
	year={2018}
}
@inproceedings{verma2017hunt,
	title={Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs},
	author={Verma, Saurabh and Zhang, Zhi-Li},
	booktitle={Advances in Neural Information Processing Systems},
	pages={88--98},
	year={2017}
}

@article{yarotsky2018universal,
	title={Universal approximations of invariant maps by neural networks},
	author={Yarotsky, Dmitry},
	journal={arXiv preprint arXiv:1804.10306},
	year={2018}
}


@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}


@book{fulton2013representation,
	title={Representation theory: a first course},
	author={Fulton, William and Harris, Joe},
	volume={129},
	year={2013},
	publisher={Springer Science \& Business Media}
}
@article{qi2017pointnet,
  title={Pointnet: Deep learning on point sets for 3d classification and segmentation},
  author={Qi, Charles R and Su, Hao and Mo, Kaichun and Guibas, Leonidas J},
  journal={Proc. Computer Vision and Pattern Recognition (CVPR), IEEE},
  volume={1},
  number={2},
  pages={4},
  year={2017}
}

@inproceedings{zaheer2017deep,
title={Deep sets},
author={Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Ruslan R and Smola, Alexander J},
booktitle={Advances in Neural Information Processing Systems},
pages={3391--3401},
year={2017}
}

@inproceedings{cohen2016group,
	title={Group equivariant convolutional networks},
	author={Cohen, Taco and Welling, Max},
	booktitle={International conference on machine learning},
	pages={2990--2999},
	year={2016}
}



@inproceedings{Simonovsky2017,
	abstract = {A number of problems can be formulated as prediction on graph-structured data. In this work, we generalize the convolution operator from regular grids to arbitrary graphs while avoiding the spectral domain, which allows us to handle graphs of varying size and connectivity. To move beyond a simple diffusion, filter weights are conditioned on the specific edge labels in the neighborhood of a vertex. Together with the proper choice of graph coarsening, we explore constructing deep neural networks for graph classification. In particular, we demonstrate the generality of our formulation in point cloud classification, where we set the new state of the art, and on a graph classification dataset, where we outperform other deep learning approaches. The source code is available at https://github.com/mys007/ecc},
	archivePrefix = {arXiv},
	arxivId = {1704.02901},
	author = {Simonovsky, Martin and Komodakis, Nikos},
	booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
	doi = {10.1109/CVPR.2017.11},
	eprint = {1704.02901},
	file = {::},
	isbn = {9781538604571},
	issn = {1063-6919},
	mendeley-groups = {equivariant},
	title = {{Dynamic edge-conditioned filters in convolutional neural networks on graphs}},
	year = {2017}
}
@techreport{Orbanz,
	abstract = {The natural habitat of most Bayesian methods is data represented by exchangeable sequences of observations, for which de Finetti's theorem provides the theoretical foundation. Dirichlet process clustering, Gaussian process regression, and many other parametric and nonparametric Bayesian models fall within the remit of this framework; many problems arising in modern data analysis do not. This article provides an introduction to Bayesian models of graphs, matrices, and other data that can be modeled by random structures. We describe results in probability theory that generalize de Finetti's theorem to such data and discuss their relevance to nonparametric Bayesian modeling. With the basic ideas in place, we survey example models available in the literature; applications of such models include collaborative filtering, link prediction, and graph and network analysis. We also highlight connections to recent developments in graph theory and probability, and sketch the more general mathematical foundation of Bayesian methods for other types of data beyond sequences and arrays.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1312.7857v2},
	author = {Orbanz, Peter and Roy, Daniel M},
	eprint = {arXiv:1312.7857v2},
	file = {::},
	mendeley-groups = {equivariant},
	title = {{Bayesian Models of Graphs, Arrays and Other Exchangeable Random Structures}}
}
@inproceedings{Hartford2018,
	title={Deep Models of Interactions Across Sets},
	author={Jason S. Hartford and Devon R. Graham and Kevin Leyton-Brown and Siamak Ravanbakhsh},
	booktitle={ICML},
	year={2018}
}
@article{Ravanbakhsh2017,
	title={Equivariance through parameter-sharing},
	author={Ravanbakhsh, Siamak and Schneider, Jeff and Poczos, Barnabas},
	journal={arXiv preprint arXiv:1702.08389},
	year={2017}
}
@article{Lei2017,
	abstract = {The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects (cf. filters in CNN) via the kernels. Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training. We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art results across these applications.},
	archivePrefix = {arXiv},
	arxivId = {1705.09037},
	author = {Lei, Tao and Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
	eprint = {1705.09037},
	file = {::},
	isbn = {9781510855144},
	mendeley-groups = {equivariant},
	title = {{Deriving Neural Architectures from Sequence and Graph Kernels}},
	year = {2017}
}
@inproceedings{Zhang,
	title={An end-to-end deep learning architecture for graph classification},
	author={Zhang, Muhan and Cui, Zhicheng and Neumann, Marion and Chen, Yixin},
	booktitle={Proceedings of AAAI Conference on Artificial Inteligence},
	year={2018}
}
@article{kipf,
	title={Semi-supervised classification with graph convolutional networks},
	author={Kipf, Thomas N and Welling, Max},
	journal={arXiv preprint arXiv:1609.02907},
	year={2016}
}
@inproceedings{Atwood2015,
	title={Diffusion-convolutional neural networks},
	author={Atwood, James and Towsley, Don},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1993--2001},
	year={2016}
}
@article{Niepert2016,
	abstract = {Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.},
	archivePrefix = {arXiv},
	arxivId = {1605.05273},
	author = {Niepert, Mathias and Ahmed, Mohamed and Kutzkov, Konstantin},
	eprint = {1605.05273},
	file = {::},
	isbn = {9781510829008},
	issn = {1938-7228},
	mendeley-groups = {equivariant},
	title = {{Learning Convolutional Neural Networks for Graphs}},
	year = {2016}
}
@techreport{Axler,
	author = {Axler, S and Gehring, E W and Ribet, K A},
	file = {::},
	mendeley-groups = {equivariant},
	title = {{Graduate Texts in Mathematics 129 Readings in Mathematics Editorial Board}}
}
@inproceedings{Yanardag2015,
	abstract = {In this paper, we present Deep Graph Kernels, a unified frame-work to learn latent representations of sub-structures for graphs, inspired by latest advancements in language modeling and deep learning. Our framework leverages the dependency information be-tween sub-structures by learning their latent representations. We demonstrate instances of our framework on three popular graph kernels, namely Graphlet kernels, Weisfeiler-Lehman subtree ker-nels, and Shortest-Path graph kernels. Our experiments on several benchmark datasets show that Deep Graph Kernels achieve signif-icant improvements in classification accuracy over state-of-the-art graph kernels.},
	author = {Yanardag, Pinar and Vishwanathan, S.V.N.},
	booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '15},
	doi = {10.1145/2783258.2783417},
	file = {:Users/haggaim/Library/Application Support/Mendeley Desktop/Downloaded/Yanardag, Vishwanathan - 2015 - Deep Graph Kernels.pdf:pdf},
	isbn = {9781450336642},
	issn = {9781450336642},
	mendeley-groups = {equivariant},
	title = {{Deep Graph Kernels}},
	year = {2015}
}
@inproceedings{Agarwal2006,
	abstract = {citation 66},
	author = {Agarwal, Sameer and Branson, Kristin and Belongie, Serge},
	booktitle = {Proceedings of the 23rd international conference on Machine learning  - ICML '06},
	doi = {10.1145/1143844.1143847},
	file = {:Users/haggaim/Library/Application Support/Mendeley Desktop/Downloaded/Agarwal, Branson, Belongie - 2006 - Higher order learning with graphs.pdf:pdf},
	isbn = {1595933832},
	issn = {1595933832},
	mendeley-groups = {equivariant},
	title = {{Higher order learning with graphs}},
	year = {2006}
}

@article{Kondor2018,
	title={Covariant compositional networks for learning graphs},
	author={Kondor, Risi and Son, Hy Truong and Pan, Horace and Anderson, Brandon and Trivedi, Shubhendu},
	journal={arXiv preprint arXiv:1801.02144},
	year={2018}
}
@article{Kondor2018a,
	title={On the generalization of equivariance and convolution in neural networks to the action of compact groups},
	author={Kondor, Risi and Trivedi, Shubhendu},
	journal={arXiv preprint arXiv:1802.03690},
	year={2018}
}
@techreport{,
	file = {:Users/haggaim/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Pure and Appllad MathamatlcH.pdf:pdf},
	mendeley-groups = {equivariant},
	title = {{Pure and Appllad MathamatlcH}}
}
@article{Yarotsky2018,
	abstract = {We describe generalizations of the universal approximation theorem for neural networks to maps invariant or equivariant with respect to linear representations of groups. Our goal is to establish network-like computational models that are both invariant/equivariant and provably complete in the sense of their ability to approximate any continuous invariant/equivariant map. Our contribution is three-fold. First, in the general case of compact groups we propose a construction of a complete invariant/equivariant network using an intermediate polynomial layer. We invoke classical theorems of Hilbert and Weyl to justify and simplify this construction; in particular, we describe an explicit complete ansatz for approximation of permutation-invariant maps. Second, we consider groups of translations and prove several versions of the universal approximation theorem for convolutional networks in the limit of continuous signals on euclidean spaces. Finally, we consider 2D signal transformations equivariant with respect to the group SE(2) of rigid euclidean motions. In this case we introduce the "charge--conserving convnet" -- a convnet-like computational model based on the decomposition of the feature space into isotypic representations of SO(2). We prove this model to be a universal approximator for continuous SE(2)--equivariant signal transformations.},
	archivePrefix = {arXiv},
	arxivId = {1804.10306},
	author = {Yarotsky, Dmitry},
	eprint = {1804.10306},
	file = {:Users/haggaim/Library/Application Support/Mendeley Desktop/Downloaded/Yarotsky - 2018 - Universal approximations of invariant maps by neural networks.pdf:pdf},
	mendeley-groups = {equivariant},
	title = {{Universal approximations of invariant maps by neural networks}},
	year = {2018}
}
@inproceedings{Qi2017,
	abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
	archivePrefix = {arXiv},
	arxivId = {1612.00593},
	author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
	booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
	doi = {10.1109/CVPR.2017.16},
	eprint = {1612.00593},
	file = {:Users/haggaim/Library/Application Support/Mendeley Desktop/Downloaded/Qi et al. - 2017 - PointNet Deep learning on point sets for 3D classification and segmentation.pdf:pdf},
	isbn = {9781538604571},
	issn = {1063-6919},
	mendeley-groups = {equivariant},
	title = {{PointNet: Deep learning on point sets for 3D classification and segmentation}},
	year = {2017}
}


@article{Monti2018,
	abstract = {In recent years, there has been a surge of interest in developing deep learning methods for non-Euclidean structured data such as graphs. In this paper, we propose Dual-Primal Graph CNN, a graph convolutional architecture that alternates convolution-like operations on the graph and its dual. Our approach allows to learn both vertex- and edge features and generalizes the previous graph attention (GAT) model. We provide extensive experimental validation showing state-of-the-art results on a variety of tasks tested on established graph benchmarks, including CORA and Citeseer citation networks as well as MovieLens, Flixter, Douban and Yahoo Music graph-guided recommender systems.},
	archivePrefix = {arXiv},
	arxivId = {1806.00770},
	author = {Monti, Federico and Shchur, Oleksandr and Bojchevski, Aleksandar and Litany, Or and G{\"{u}}nnemann, Stephan and Bronstein, Michael M.},
	eprint = {1806.00770},
	file = {:Users/haggaim/Dropbox/PhD/learning{\_}on{\_}graphs{\_}via{\_}pairwise{\_}permutation{\_}invariant{\_}layers/refs/Dual-Primal Graph Convolutional Networks.pdf:pdf},
	mendeley-groups = {equivariant},
	pages = {1--11},
	title = {{Dual-Primal Graph Convolutional Networks}},
	url = {http://arxiv.org/abs/1806.00770},
	year = {2018}
}
@article{Velickovic2017,
	abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
	archivePrefix = {arXiv},
	arxivId = {1710.10903},
	author = {Veli{\v{c}}kovi{\'{c}}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`{o}}, Pietro and Bengio, Yoshua},
	eprint = {1710.10903},
	file = {:Users/haggaim/Dropbox/PhD/learning{\_}on{\_}graphs{\_}via{\_}pairwise{\_}permutation{\_}invariant{\_}layers/refs/GRAPH ATTENTION NETWORKS.pdf:pdf},
	mendeley-groups = {equivariant},
	pages = {1--12},
	title = {{Graph Attention Networks}},
	url = {http://arxiv.org/abs/1710.10903},
	year = {2017}
}
@inproceedings{Hamilton2017,
	title={Inductive representation learning on large graphs},
	author={Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1024--1034},
	year={2017}
}


@inproceedings{Defferrard2016,
	title={Convolutional neural networks on graphs with fast localized spectral filtering},
	author={Defferrard, Micha{\"e}l and Bresson, Xavier and Vandergheynst, Pierre},
	booktitle={Advances in Neural Information Processing Systems},
	pages={3844--3852},
	year={2016}
}
@article{Levie2017,
	abstract = {The rise of graph-structured data such as social networks, regulatory networks, citation graphs, and functional brain networks, in combination with resounding success of deep learning in various applications, has brought the interest in generalizing deep learning models to non-Euclidean domains. In this paper, we introduce a new spectral domain convolutional architecture for deep learning on graphs. The core ingredient of our model is a new class of parametric rational complex functions (Cayley polynomials) allowing to efficiently compute localized regular filters on graphs that specialize on frequency bands of interest. Our model scales linearly with the size of the input data for sparsely-connected graphs, can handle different constructions of Laplacian operators, and typically requires less parameters than previous models. Extensive experimental results show the superior performance of our approach on various graph learning problems.},
	archivePrefix = {arXiv},
	arxivId = {1705.07664},
	author = {Levie, Ron and Monti, Federico and Bresson, Xavier and Bronstein, Michael M.},
	doi = {10.1109/CVPR.2017.576},
	eprint = {1705.07664},
	file = {:Users/haggaim/Dropbox/PhD/learning{\_}on{\_}graphs{\_}via{\_}pairwise{\_}permutation{\_}invariant{\_}layers/refs/Cayleynets- Graph convolutional neural networks with complex rational spectral filters.pdf:pdf},
	isbn = {978-1-5386-0457-1},
	issn = {1063-6919},
	mendeley-groups = {garph NN},
	pages = {1--12},
	title = {{CayleyNets: Graph Convolutional Neural Networks with Complex Rational Spectral Filters}},
	url = {http://arxiv.org/abs/1705.07664},
	year = {2017}
}
@article{Battaglia2016,
	abstract = {Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.},
	archivePrefix = {arXiv},
	arxivId = {1612.00222},
	author = {Battaglia, Peter W. and Pascanu, Razvan and Lai, Matthew and Rezende, Danilo and Kavukcuoglu, Koray},
	eprint = {1612.00222},
	file = {:Users/haggaim/Dropbox/PhD/learning{\_}on{\_}graphs{\_}via{\_}pairwise{\_}permutation{\_}invariant{\_}layers/refs/Interaction networks for learning about objects, relations and physics..pdf:pdf},
	issn = {10495258},
	mendeley-groups = {garph NN},
	title = {{Interaction Networks for Learning about Objects, Relations and Physics}},
	url = {http://arxiv.org/abs/1612.00222},
	year = {2016}
}
@inproceedings{Duvenaud2015,
	title={Convolutional networks on graphs for learning molecular fingerprints},
	author={Duvenaud, David K and Maclaurin, Dougal and Iparraguirre, Jorge and Bombarell, Rafael and Hirzel, Timothy and Aspuru-Guzik, Al{\'a}n and Adams, Ryan P},
	booktitle={Advances in neural information processing systems},
	pages={2224--2232},
	year={2015}
}
@article{Gori2005,
	abstract = {In several applications the information is naturally represented by graphs. Traditional approaches cope with graphi-cal data structures using a preprocessing phase which transforms the graphs into a set of flat vectors. However, in this way, important topological information may be lost and the achieved results may heavily depend on the preprocessing stage. This paper presents a new neural model, called graph neural network (GNN), capable of directly processing graphs. GNNs extends recursive neural networks and can be applied on most of the practically useful kinds of graphs, including directed, undirected, labelled and cyclic graphs. A learning algorithm for GNNs is proposed and some experiments are discussed which assess the properties of the model.},
	author = {Gori, Marco and Monfardini, Gabriele and Scarselli, Franco},
	doi = {10.1109/IJCNN.2005.1555942},
	file = {:Users/haggaim/Dropbox/PhD/learning{\_}on{\_}graphs{\_}via{\_}pairwise{\_}permutation{\_}invariant{\_}layers/refs/A{\_}new{\_}Model{\_}for{\_}Learning{\_}in{\_}Graph{\_}Domains.pdf:pdf},
	isbn = {0780390482},
	journal = {Proceedings of the International Joint Conference on Neural Networks},
	mendeley-groups = {garph NN},
	number = {January},
	pages = {729--734},
	title = {{A new model for earning in raph domains}},
	volume = {2},
	year = {2005}
}
@inproceedings{Gilmer2017,
	title={Neural Message Passing for Quantum Chemistry},
	author={Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E},
	booktitle={International Conference on Machine Learning},
	pages={1263--1272},
	year={2017}
}
@article{Bruna2013,
	abstract = {Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.},
	archivePrefix = {arXiv},
	arxivId = {1312.6203},
	author = {Bruna, Joan and Zaremba, Wojciech and Szlam, Arthur and LeCun, Yann},
	eprint = {1312.6203},
	file = {:Users/haggaim/Dropbox/PhD/learning{\_}on{\_}graphs{\_}via{\_}pairwise{\_}permutation{\_}invariant{\_}layers/refs/Spectral networks and locally connected networks on graphs.pdf:pdf},
	mendeley-groups = {garph NN},
	pages = {1--14},
	title = {{Spectral Networks and Locally Connected Networks on Graphs}},
	url = {http://arxiv.org/abs/1312.6203},
	year = {2013}
}
@article{Henaff2015,
	abstract = {Deep Learning's recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities. In this paper we consider the general question of how to construct deep architectures with small learning complexity on general non-Euclidean domains, which are typically unknown and need to be estimated from the data. In particular, we develop an extension of Spectral Networks which incorporates a Graph Estimation procedure, that we test on large-scale classification problems, matching or improving over Dropout Networks with far less parameters to estimate.},
	archivePrefix = {arXiv},
	arxivId = {1506.05163},
	author = {Henaff, Mikael and Bruna, Joan and LeCun, Yann},
	eprint = {1506.05163},
	file = {:Users/haggaim/Dropbox/PhD/learning{\_}on{\_}graphs{\_}via{\_}pairwise{\_}permutation{\_}invariant{\_}layers/refs/Deep{\_}Convolutional{\_}Networks{\_}on{\_}Graph-Structured{\_}Da.pdf:pdf},
	issn = {1506.05163},
	mendeley-groups = {garph NN},
	number = {June},
	title = {{Deep Convolutional Networks on Graph-Structured Data}},
	url = {http://arxiv.org/abs/1506.05163},
	year = {2015}
}
@article{Li2015,
	abstract = {Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.},
	archivePrefix = {arXiv},
	arxivId = {1511.05493},
	author = {Li, Yujia and Tarlow, Daniel and Brockschmidt, Marc and Zemel, Richard},
	doi = {10.1103/PhysRevLett.116.082003},
	eprint = {1511.05493},
	file = {:Users/haggaim/Dropbox/PhD/learning{\_}on{\_}graphs{\_}via{\_}pairwise{\_}permutation{\_}invariant{\_}layers/refs/GATED GRAPH SEQUENCE NEURAL NETWORKS.pdf:pdf},
	issn = {10797114},
	mendeley-groups = {garph NN},
	number = {1},
	pages = {1--20},
	title = {{Gated Graph Sequence Neural Networks}},
	url = {http://arxiv.org/abs/1511.05493},
	year = {2015}
}
@article{Scarselli2009,
	author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
	doi = {10.1109/TNN.2008.2005605},
	file = {:Users/haggaim/Dropbox/PhD/learning{\_}on{\_}graphs{\_}via{\_}pairwise{\_}permutation{\_}invariant{\_}layers/refs/The graph neural network model.pdf:pdf},
	isbn = {1045-9227},
	issn = {1045-9227},
	journal = {Neural Networks, IEEE Transactions on},
	mendeley-groups = {garph NN},
	number = {1},
	pages = {61--80},
	title = {{The graph neural network model}},
	volume = {20},
	year = {2009}
}



@article{Ying2018,
	abstract = {Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10{\%} accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark data sets.},
	archivePrefix = {arXiv},
	arxivId = {1806.08804},
	author = {Ying, Rex and You, Jiaxuan and Morris, Christopher and Ren, Xiang and , William L. and Leskovec, Jure},
	doi = {10.1145/nnnnnnn.nnnnnnn},
	eprint = {1806.08804},
	file = {:Users/haggaim/Dropbox/PhD/learning{\_}on{\_}graphs{\_}via{\_}pairwise{\_}permutation{\_}invariant{\_}layers/refs/Hierarchical Graph Representation Learning with Differentiable Pooling.pdf:pdf},
	mendeley-groups = {garph NN},
	title = {{Hierarchical Graph Representation Learning with Differentiable Pooling}},
	url = {http://arxiv.org/abs/1806.08804},
	year = {2018}
}
@inproceedings{monti2017geometric,
	title={Geometric deep learning on graphs and manifolds using mixture model CNNs},
	author={Monti, Federico and Boscaini, Davide and Masci, Jonathan and Rodola, Emanuele and Svoboda, Jan and Bronstein, Michael M},
	booktitle={Proc. CVPR},
	volume={1},
	number={2},
	pages={3},
	year={2017}
}

@inproceedings{krizhevsky2012imagenet,
	title={Imagenet classification with deep convolutional neural networks},
	author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	booktitle={Advances in neural information processing systems},
	pages={1097--1105},
	year={2012}
}

@article{Welling2018,
	title={Spherical CNNs},
	author={Cohen, Taco S and Geiger, Mario and K{\"o}hler, Jonas and Welling, Max},
	journal={arXiv preprint arXiv:1801.10130},
	year={2018}
}
@article{Weiler2018,
	abstract = {We present a convolutional network that is equivariant to rigid body motions. The model uses scalar-, vector-, and tensor fields over 3D Euclidean space to represent data, and equivariant convolutions to map between such representations. These SE(3)-equivariant convolutions utilize kernels which are parameterized as a linear combination of a complete steerable kernel basis, which is derived in this paper. We prove that equivariant convolutions are the most general equivariant linear maps between fields over R{\^{}}3. Our experimental results confirm the effectiveness of 3D Steerable CNNs for the problem of amino acid propensity prediction and protein structure classification, both of which have inherent SE(3) symmetry.},
	archivePrefix = {arXiv},
	arxivId = {1807.02547},
	author = {Weiler, Maurice and Geiger, Mario and Welling, Max and Boomsma, Wouter and Cohen, Taco},
	eprint = {1807.02547},
	file = {:Users/haggaim/Dropbox/PhD/learning{\_}on{\_}graphs{\_}via{\_}pairwise{\_}permutation{\_}invariant{\_}layers/refs/3D Steerable CNNs- Learning Rotationally Equivariant Features in Volumetric Data.pdf:pdf},
	mendeley-groups = {equivariant NN},
	title = {{3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data}},
	url = {http://arxiv.org/abs/1807.02547},
	year = {2018}
}
@article{Cohen2016,
	abstract = {It has long been recognized that the invariance and equivariance properties of a representation are critically important for success in many vision tasks. In this paper we present Steerable Convolutional Neural Networks, an efficient and flexible class of equivariant convolutional networks. We show that steerable CNNs achieve state of the art results on the CIFAR image classification benchmark. The mathematical theory of steerable representations reveals a type system in which any steerable representation is a composition of elementary feature types, each one associated with a particular kind of symmetry. We show how the parameter cost of a steerable filter bank depends on the types of the input and output features, and show how to use this knowledge to construct CNNs that utilize parameters effectively.},
	archivePrefix = {arXiv},
	arxivId = {1612.08498},
	author = {Cohen, Taco S. and Welling, Max},
	eprint = {1612.08498},
	file = {:Users/haggaim/Dropbox/PhD/learning{\_}on{\_}graphs{\_}via{\_}pairwise{\_}permutation{\_}invariant{\_}layers/refs/STEERABLE CNNS.pdf:pdf},
	mendeley-groups = {equivariant NN},
	number = {1990},
	pages = {1--14},
	title = {{Steerable CNNs}},
	url = {http://arxiv.org/abs/1612.08498},
	year = {2016}
}

@incollection{tsuda2010graph,
	title={Graph classification},
	author={Tsuda, Koji and Saigo, Hiroto},
	booktitle={Managing and mining graph data},
	pages={337--363},
	year={2010},
	publisher={Springer}
}
@book{chung1997spectral,
	title={Spectral graph theory},
	author={Chung, Fan RK and Graham, Fan Chung},
	number={92},
	year={1997},
	publisher={American Mathematical Soc.}
}
@inproceedings{shervashidze2009efficient,
	title={Efficient graphlet kernels for large graph comparison},
	author={Shervashidze, Nino and Vishwanathan, SVN and Petri, Tobias and Mehlhorn, Kurt and Borgwardt, Karsten},
	booktitle={Artificial Intelligence and Statistics},
	pages={488--495},
	year={2009}
}


@article{neumann2016propagation,
	title={Propagation kernels: efficient graph kernels from propagated information},
	author={Neumann, Marion and Garnett, Roman and Bauckhage, Christian and Kersting, Kristian},
	journal={Machine Learning},
	volume={102},
	number={2},
	pages={209--245},
	year={2016},
	publisher={Springer}
}
@article{shervashidze2011weisfeiler,
	title={Weisfeiler-lehman graph kernels},
	author={Shervashidze, Nino and Schweitzer, Pascal and Leeuwen, Erik Jan van and Mehlhorn, Kurt and Borgwardt, Karsten M},
	journal={Journal of Machine Learning Research},
	volume={12},
	number={Sep},
	pages={2539--2561},
	year={2011}
}


@inproceedings{tensorflow,
	title={Tensorflow: a system for large-scale machine learning.},
	author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
	booktitle={OSDI},
	volume={16},
	pages={265--283},
	year={2016}
}
@article{lecun1989backpropagation,
	title={Backpropagation applied to handwritten zip code recognition},
	author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
	journal={Neural computation},
	volume={1},
	number={4},
	pages={541--551},
	year={1989},
	publisher={MIT Press}
}

@book{grohe2017descriptive,
  title={Descriptive complexity, canonisation, and definable graph structure theory},
  author={Grohe, Martin},
  volume={47},
  year={2017},
  publisher={Cambridge University Press}
}

@article{cai1992optimal,
  title={An optimal lower bound on the number of variables for graph identification},
  author={Cai, Jin-Yi and F{\"u}rer, Martin and Immerman, Neil},
  journal={Combinatorica},
  volume={12},
  number={4},
  pages={389--410},
  year={1992},
  publisher={Springer}
}

@inproceedings{
maron2018invariant,
title={Invariant and Equivariant Graph Networks},
author={Haggai Maron and Heli Ben-Hamu and Nadav Shamir and Yaron Lipman},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Syx72jC9tm},
}

@article{douglas2011weisfeiler,
  title={The Weisfeiler-Lehman method and graph isomorphism testing},
  author={Douglas, Brendan L},
  journal={arXiv preprint arXiv:1101.5211},
  year={2011}
}

@article{grohe2015pebble,
  title={Pebble games and linear equations},
  author={Grohe, Martin and Otto, Martin},
  journal={The Journal of Symbolic Logic},
  volume={80},
  number={3},
  pages={797--844},
  year={2015},
  publisher={Cambridge University Press}
}

@inproceedings{abadi2016tensorflow,
  title={Tensorflow: A system for large-scale machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 16)},
  pages={265--283},
  year={2016}
}