\begin{thebibliography}{10}

\bibitem{arxiv2020}
Arxiv dataset.
\newblock \url{https://www.kaggle.com/Cornell-University/arxiv}.

\bibitem{tiny2017}
Tiny {IMAGENET} visual recognition challenge.
\newblock \url{https://www.kaggle.com/c/tiny-imagenet}.

\bibitem{antoniou2017data}
Antreas Antoniou, Amos Storkey, and Harrison Edwards.
\newblock Data augmentation generative adversarial networks.
\newblock {\em arXiv preprint arXiv:1711.04340}, 2017.

\bibitem{arbel2020generalized}
Michael Arbel, Liang Zhou, and Arthur Gretton.
\newblock Generalized energy based models.
\newblock In {\em ICLR}, 2020.

\bibitem{arjovsky2019invariant}
Martin Arjovsky, L{\'e}on Bottou, Ishaan Gulrajani, and David Lopez-Paz.
\newblock Invariant risk minimization.
\newblock {\em arXiv preprint arXiv:1907.02893}, 2019.

\bibitem{bansal2018can}
Nitin Bansal, Xiaohan Chen, and Zhangyang Wang.
\newblock Can we gain more from orthogonality regularizations in training deep
  networks?
\newblock In {\em NeurIPS}, 2018.

\bibitem{botev2008efficient}
Zdravko~I Botev and Dirk~P Kroese.
\newblock An efficient algorithm for rare-event probability estimation,
  combinatorial optimization, and counting.
\newblock {\em Methodology and Computing in Applied Probability},
  10(4):471--505, 2008.

\bibitem{buhlmann2018invariance}
Peter B{\"u}hlmann.
\newblock Invariance, causality and robustness.
\newblock {\em arXiv preprint arXiv:1812.08233}, 2018.

\bibitem{byrd2019effect}
Jonathon Byrd and Zachary Lipton.
\newblock What is the effect of importance weighting in deep learning?
\newblock In {\em ICML}, 2019.

\bibitem{cao2019learning}
Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma.
\newblock Learning imbalanced datasets with label-distribution-aware margin
  loss.
\newblock In {\em NeurIPS}, 2019.

\bibitem{chawla2002smote}
Nitesh~V Chawla, Kevin~W Bowyer, Lawrence~O Hall, and W~Philip Kegelmeyer.
\newblock {SMOTE}: synthetic minority over-sampling technique.
\newblock {\em Journal of artificial intelligence research}, 16:321--357, 2002.

\bibitem{chen2021wiener}
Junya Chen, Jianfeng Feng, and Wenlian Lu.
\newblock A wiener causality defined by divergence.
\newblock {\em Neural Processing Letters}, 53(3):1773--1794, 2021.

\bibitem{chen2021simpler}
Junya Chen, Zhe Gan, Xuan Li, Qing Guo, Liqun Chen, Shuyang Gao, Tagyoung
  Chung, Yi~Xu, Belinda Zeng, Wenlian Lu, et~al.
\newblock Simpler, faster, stronger: Breaking the log-k curse on contrastive
  learners with flatnce.
\newblock {\em arXiv preprint arXiv:2107.01152}, 2021.

\bibitem{chen2021variational}
Junya Chen, Danni Lu, Zidi Xiu, Ke~Bai, Lawrence Carin, and Chenyang Tao.
\newblock Variational inference with holder bounds.
\newblock {\em arXiv preprint arXiv:2111.02947}, 2021.

\bibitem{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In {\em ICML}, 2020.

\bibitem{choromanska2013extreme}
Anna Choromanska, Alekh Agarwal, and John Langford.
\newblock Extreme multi class classification.
\newblock In {\em NIPS Workshop: eXtreme Classification, submitted}, 2013.

\bibitem{chouldechova2018frontiers}
Alexandra Chouldechova and Aaron Roth.
\newblock The frontiers of fairness in machine learning.
\newblock {\em arXiv preprint arXiv:1810.08810}, 2018.

\bibitem{cogswell2015reducing}
Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zitnick, and Dhruv Batra.
\newblock Reducing overfitting in deep networks by decorrelating
  representations.
\newblock {\em arXiv preprint arXiv:1511.06068}, 2015.

\bibitem{comon1994independent}
Pierre Comon.
\newblock Independent component analysis, a new concept?
\newblock {\em Signal processing}, 36(3):287--314, 1994.

\bibitem{cortes1995support}
Corinna Cortes and Vladimir Vapnik.
\newblock Support-vector networks.
\newblock {\em Machine learning}, 20(3):273--297, 1995.

\bibitem{cubuk2019autoaugment}
Ekin~D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V Le.
\newblock Autoaugment: Learning augmentation strategies from data.
\newblock In {\em CVPR}, 2019.

\bibitem{cui2019class}
Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie.
\newblock Class-balanced loss based on effective number of samples.
\newblock In {\em CVPR}, 2019.

\bibitem{dai2019exponential}
Bo~Dai, Zhen Liu, Hanjun Dai, Niao He, Arthur Gretton, Le~Song, and Dale
  Schuurmans.
\newblock Exponential family estimation via adversarial dynamics embedding.
\newblock In {\em NeurIPS}, pages 10979--10990, 2019.

\bibitem{dal2017credit}
Andrea Dal~Pozzolo, Giacomo Boracchi, Olivier Caelen, Cesare Alippi, and
  Gianluca Bontempi.
\newblock Credit card fraud detection: a realistic modeling and a novel
  learning strategy.
\newblock {\em IEEE transactions on neural networks and learning systems},
  29(8):3784--3797, 2017.

\bibitem{desjardins2015natural}
Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, et~al.
\newblock Natural neural networks.
\newblock In {\em NIPS}, 2015.

\bibitem{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em NAACL}, 2019.

\bibitem{dingus2016driver}
Thomas~A Dingus, Feng Guo, Suzie Lee, Jonathan~F Antin, Miguel Perez, Mindy
  Buchanan-King, and Jonathan Hankey.
\newblock Driver crash risk factors and prevalence evaluation using
  naturalistic driving data.
\newblock {\em Proceedings of the National Academy of Sciences},
  113(10):2636--2641, 2016.

\bibitem{donsker1983asymptotic}
Monroe~D Donsker and SR~Srinivasa Varadhan.
\newblock Asymptotic evaluation of certain markov process expectations for
  large time. iv.
\newblock {\em Communications on Pure and Applied Mathematics}, 36(2):183--212,
  1983.

\bibitem{drummond2003c4}
Chris Drummond, Robert~C Holte, et~al.
\newblock C4. 5, class imbalance, and cost sensitivity: why under-sampling
  beats over-sampling.
\newblock In {\em Workshop on learning from imbalanced datasets II}, volume~11,
  pages 1--8. Citeseer, 2003.

\bibitem{elkan2001foundations}
Charles Elkan.
\newblock The foundations of cost-sensitive learning.
\newblock In {\em International joint conference on artificial intelligence},
  volume~17, pages 973--978. Lawrence Erlbaum Associates Ltd, 2001.

\bibitem{fei2006one}
Li~Fei-Fei, Rob Fergus, and Pietro Perona.
\newblock One-shot learning of object categories.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  28(4):594--611, 2006.

\bibitem{fenchel1949conjugate}
Werner Fenchel.
\newblock On conjugate convex functions.
\newblock {\em Canadian Journal of Mathematics}, 1(1):73--77, 1949.

\bibitem{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock {\em arXiv preprint arXiv:1703.03400}, 2017.

\bibitem{freund1997decision}
Yoav Freund and Robert~E Schapire.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock {\em Journal of computer and system sciences}, 55(1):119--139, 1997.

\bibitem{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock In {\em ICLR}, 2015.

\bibitem{gretton2012kernel}
Arthur Gretton, Karsten~M Borgwardt, Malte~J Rasch, Bernhard Sch{\"o}lkopf, and
  Alexander Smola.
\newblock A kernel two-sample test.
\newblock {\em Journal of Machine Learning Research}, 13(Mar):723--773, 2012.

\bibitem{guo2021tight}
Qing Guo, Junya Chen, Dong Wang, Yuewei Yang, Xinwei Deng, Fan Li, Lawrence
  Carin, and Chenyang Tao.
\newblock Tight mutual information estimation with contrastive
  {Fenchel-Legendre} optimization, 2021.
\newblock [Available online; accessed 28-May-2021].

\bibitem{gutmann2012bregman}
Michael Gutmann and Jun-ichiro Hirayama.
\newblock Bregman divergence as general framework to estimate unnormalized
  statistical models.
\newblock {\em arXiv preprint arXiv:1202.3727}, 2012.

\bibitem{gutmann2010noise}
Michael Gutmann and Aapo Hyv{\"a}rinen.
\newblock Noise-contrastive estimation: A new estimation principle for
  unnormalized statistical models.
\newblock In {\em AISTATS}, pages 297--304. JMLR Workshop and Conference
  Proceedings, 2010.

\bibitem{hariharan2017low}
Bharath Hariharan and Ross Girshick.
\newblock Low-shot visual recognition by shrinking and hallucinating features.
\newblock In {\em ICCV}, pages 3018--3027, 2017.

\bibitem{he2008adasyn}
Haibo He, Yang Bai, Edwardo~A Garcia, and Shutao Li.
\newblock Adasyn: Adaptive synthetic sampling approach for imbalanced learning.
\newblock In {\em WCCI}, 2008.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, 2016.

\bibitem{heinze2018invariant}
Christina Heinze-Deml, Jonas Peters, and Nicolai Meinshausen.
\newblock Invariant causal prediction for nonlinear models.
\newblock {\em Journal of Causal Inference}, 6(2), 2018.

\bibitem{hinton2002training}
Geoffrey~E Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock {\em Neural computation}, 14(8):1771--1800, 2002.

\bibitem{huang2018decorrelated}
Lei Huang, Dawei Yang, Bo~Lang, and Jia Deng.
\newblock Decorrelated batch normalization.
\newblock In {\em CVPR}, 2018.

\bibitem{hyvarinen2000independent}
Aapo Hyv{\"a}rinen and Erkki Oja.
\newblock Independent component analysis: algorithms and applications.
\newblock {\em Neural networks}, 13(4-5):411--430, 2000.

\bibitem{hyvarinen1999nonlinear}
Aapo Hyv{\"a}rinen and Petteri Pajunen.
\newblock Nonlinear independent component analysis: Existence and uniqueness
  results.
\newblock {\em Neural networks}, 12(3):429--439, 1999.

\bibitem{hyvarinen2019nonlinear}
Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner.
\newblock Nonlinear ica using auxiliary variables and generalized contrastive
  learning.
\newblock In {\em AISTATS}, pages 859--868, 2019.

\bibitem{jia2019orthogonal}
Kui Jia, Shuai Li, Yuxin Wen, Tongliang Liu, and Dacheng Tao.
\newblock Orthogonal deep neural networks.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  2019.

\bibitem{kaggle2017planet}
I~Kaggle.
\newblock Planet: Understanding the amazon from space| kaggle.
\newblock {\em Consult{\'e} sur https://www. kaggle.
  com/c/planet-understanding-the-amazon-from-space/data (Consult{\'e} le:
  2018-01-25)}, 2017.

\bibitem{kang2019decoupling}
Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi
  Feng, and Yannis Kalantidis.
\newblock Decoupling representation and classifier for long-tailed recognition.
\newblock In {\em ICLR}, 2019.

\bibitem{kessy2018optimal}
Agnan Kessy, Alex Lewin, and Korbinian Strimmer.
\newblock Optimal whitening and decorrelation.
\newblock {\em The American Statistician}, 72(4):309--314, 2018.

\bibitem{khemakhem2020variational}
Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen.
\newblock Variational autoencoders and nonlinear {ICA}: A unifying framework.
\newblock In {\em AISTATS}, pages 2207--2217, 2020.

\bibitem{khosla2020supervised}
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
  Isola, Aaron Maschinot, Ce~Liu, and Dilip Krishnan.
\newblock Supervised contrastive learning.
\newblock In {\em NeurIPS}, volume~33, 2020.

\bibitem{kim2020m2m}
Jaehyung Kim, Jongheon Jeong, and Jinwoo Shin.
\newblock M2m: Imbalanced classification via major-to-minor translation.
\newblock In {\em CVPR}, pages 13896--13905, 2020.

\bibitem{king2001logistic}
Gary King and Langche Zeng.
\newblock Logistic regression in rare events data.
\newblock {\em Political analysis}, 9(2):137--163, 2001.

\bibitem{kotsiantis2006handling}
Sotiris Kotsiantis, Dimitris Kanellopoulos, Panayiotis Pintelas, et~al.
\newblock Handling imbalanced datasets: A review.
\newblock {\em GESTS International Transactions on Computer Science and
  Engineering}, 30(1):25--36, 2006.

\bibitem{lecun2006tutorial}
Yann LeCun, Sumit Chopra, Raia Hadsell, Marc'Aurelio Ranzato, and Fu-Jie Huang.
\newblock {\em Predicting Structured Data}, chapter A Tutorial on Energy-Based
  Learning.
\newblock MIT Press, 2006.

\bibitem{li2019gradient}
Buyu Li, Yu~Liu, and Xiaogang Wang.
\newblock Gradient harmonized single-stage detector.
\newblock In {\em AAAI}, 2019.

\bibitem{lin2017focal}
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll{\'a}r.
\newblock Focal loss for dense object detection.
\newblock In {\em ICCV}, 2017.

\bibitem{liu2019large}
Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella~X
  Yu.
\newblock Large-scale long-tailed recognition in an open world.
\newblock In {\em CVPR}, 2019.

\bibitem{lu2020reconsidering}
Danni Lu, Chenyang Tao, Junya Chen, Fan Li, Feng Guo, and Lawrence Carin.
\newblock Reconsidering generative objectives for counterfactual reasoning.
\newblock In {\em NeurIPS}, 2020.

\bibitem{machado2020rare}
JA~Tenreiro Machado and Ant{\'o}nio~M Lopes.
\newblock Rare and extreme events: the case of covid-19 pandemic.
\newblock {\em Nonlinear Dynamics}, page~1, 2020.

\bibitem{mariani2018bagan}
Giovanni Mariani, Florian Scheidegger, Roxana Istrate, Costas Bekas, and
  Cristiano Malossi.
\newblock {BAGAN}: Data augmentation with balancing {GAN}.
\newblock {\em arXiv preprint arXiv:1803.09655}, 2018.

\bibitem{menon2020long}
Aditya~Krishna Menon, Sadeep Jayasumana, Ankit~Singh Rawat, Himanshu Jain,
  Andreas Veit, and Sanjiv Kumar.
\newblock Long-tail learning via logit adjustment.
\newblock In {\em ICLR}, 2020.

\bibitem{miyato2018virtual}
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii.
\newblock Virtual adversarial training: a regularization method for supervised
  and semi-supervised learning.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  41(8):1979--1993, 2018.

\bibitem{moya1996network}
Mary~M Moya and Don~R Hush.
\newblock Network constraints and multi-objective optimization for one-class
  classification.
\newblock {\em Neural networks}, 9(3):463--474, 1996.

\bibitem{mullick2019generative}
Sankha~Subhra Mullick, Shounak Datta, and Swagatam Das.
\newblock Generative adversarial minority oversampling.
\newblock In {\em ICCV}, 2019.

\bibitem{papamakarios2017masked}
George Papamakarios, Theo Pavlakou, and Iain Murray.
\newblock Masked autoregressive flow for density estimation.
\newblock In {\em NIPS}, 2017.

\bibitem{peters2016causal}
Jonas Peters, Peter B{\"u}hlmann, and Nicolai Meinshausen.
\newblock Causal inference using invariant prediction: identification and
  confidence intervals.
\newblock {\em Journal of the Royal Statistical Society Series B (Statistical
  Methodology)}, 2016.

\bibitem{poole2019variational}
Ben Poole, Sherjil Ozair, Aaron Van Den~Oord, Alex Alemi, and George Tucker.
\newblock On variational bounds of mutual information.
\newblock In {\em ICML}. PMLR, 2019.

\bibitem{robinson2020strength}
Joshua Robinson, Stefanie Jegelka, and Suvrit Sra.
\newblock Strength from weakness: Fast learning using weak supervision.
\newblock In {\em ICML}, 2020.

\bibitem{rojas2018invariant}
Mateo Rojas-Carulla, Bernhard Sch{\"o}lkopf, Richard Turner, and Jonas Peters.
\newblock Invariant models for causal transfer learning.
\newblock {\em The Journal of Machine Learning Research}, 19(1):1309--1342,
  2018.

\bibitem{rothenhausler2018anchor}
Dominik Rothenh{\"a}usler, Nicolai Meinshausen, Peter B{\"u}hlmann, and Jonas
  Peters.
\newblock Anchor regression: heterogeneous data meets causality.
\newblock {\em arXiv preprint arXiv:1801.06229}, 2018.

\bibitem{ruff2018deep}
Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib~Ahmed
  Siddiqui, Alexander Binder, Emmanuel M{\"u}ller, and Marius Kloft.
\newblock Deep one-class classification.
\newblock In {\em ICML}, 2018.

\bibitem{scholkopf2012causal}
Bernhard Sch{\"o}lkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun
  Zhang, and Joris Mooij.
\newblock On causal and anticausal learning.
\newblock {\em arXiv preprint arXiv:1206.6471}, 2012.

\bibitem{shorten2019survey}
Connor Shorten and Taghi~M Khoshgoftaar.
\newblock A survey on image data augmentation for deep learning.
\newblock {\em Journal of Big Data}, 6(1):60, 2019.

\bibitem{siddharth2017learning}
Narayanaswamy Siddharth, Brooks Paige, Jan-Willem Van~de Meent, Alban
  Desmaison, Noah Goodman, Pushmeet Kohli, Frank Wood, and Philip Torr.
\newblock Learning disentangled representations with semi-supervised deep
  generative models.
\newblock In {\em NIPS}, 2017.

\bibitem{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em CVPR}, 2016.

\bibitem{tang2020long}
Kaihua Tang, Jianqiang Huang, and Hanwang Zhang.
\newblock Long-tailed classification by keeping the good and removing the bad
  momentum causal effect.
\newblock In {\em NeurIPS}, volume~33, 2020.

\bibitem{tao2019fenchel}
Chenyang Tao, Liqun Chen, Shuyang Dai, Junya Chen, Ke~Bai, Dong Wang, Jianfeng
  Feng, Wenlian Lu, Georgiy~V Bobashev, and Lawrence Carin.
\newblock On fenchel mini-max learning.
\newblock In {\em NeurIPS}, 2019.

\bibitem{tao2019variational}
Chenyang Tao, Shuyang Dai, Liqun Chen, Ke~Bai, Junya Chen, Chang Liu, Ruiyi
  Zhang, Georgiy Bobashev, and Lawrence~Carin Duke.
\newblock Variational annealing of gans: A langevin perspective.
\newblock In {\em ICML}, pages 6176--6185. PMLR, 2019.

\bibitem{teshima2020few}
Takeshi Teshima, Issei Sato, and Masashi Sugiyama.
\newblock Few-shot domain adaptation by causal mechanism transfer.
\newblock In {\em ICML}, 2020.

\bibitem{tokmakov2019learning}
Pavel Tokmakov, Yu-Xiong Wang, and Martial Hebert.
\newblock Learning compositional representations for few-shot recognition.
\newblock In {\em ICCV}, 2019.

\bibitem{towns2014xsede}
John Towns, Timothy Cockerill, Maytal Dahan, Ian Foster, Kelly Gaither, Andrew
  Grimshaw, Victor Hazlewood, Scott Lathrop, Dave Lifka, Gregory~D Peterson,
  et~al.
\newblock Xsede: accelerating scientific discovery.
\newblock {\em Computing in science \& engineering}, 16(5):62--74, 2014.

\bibitem{van2018inaturalist}
Grant Van~Horn, Oisin Mac~Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
  Hartwig Adam, Pietro Perona, and Serge Belongie.
\newblock The {iNaturalist} species classification and detection dataset.
\newblock In {\em CVPR}, 2018.

\bibitem{vapnik2013nature}
Vladimir Vapnik.
\newblock {\em The nature of statistical learning theory}.
\newblock Springer science \& business media, 2013.

\bibitem{vinyals2016matching}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et~al.
\newblock Matching networks for one shot learning.
\newblock In {\em NIPS}, 2016.

\bibitem{wang2020generalizing}
Yaqing Wang, Quanming Yao, James~T Kwok, and Lionel~M Ni.
\newblock Generalizing from a few examples: A survey on few-shot learning.
\newblock {\em ACM Computing Surveys}, 53(3):1--34, 2020.

\bibitem{wang2017learning}
Yu-Xiong Wang, Deva Ramanan, and Martial Hebert.
\newblock Learning to model the tail.
\newblock In {\em NIPS}, 2017.

\bibitem{xiu2020variational}
Zidi Xiu, Chenyang Tao, Michael Gao, Connor Davis, Benjamin Goldstein, and
  Ricardo Henao.
\newblock Variational disentanglement for rare event modeling.
\newblock In {\em AAAI}, 2021.

\bibitem{xiu2020survival}
Zidi Xiu, Chenyang Tao, and Ricardo Henao.
\newblock Variational learning of individual survival distributions.
\newblock In {\em ACM CHIL}, 2020.

\end{thebibliography}
