\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Schuurmans, and
  Norouzi]{agarwal2020optimistic}
Agarwal, R., Schuurmans, D., and Norouzi, M.
\newblock An optimistic perspective on offline reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Amos et~al.(2020)Amos, Stanton, Yarats, and Wilson]{amos2020model}
Amos, B., Stanton, S., Yarats, D., and Wilson, A.~G.
\newblock On the model-based stochastic value gradient for continuous
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2008.12775}, 2020.

\bibitem[Anschel et~al.(2017)Anschel, Baram, and Shimkin]{anschel2017averaged}
Anschel, O., Baram, N., and Shimkin, N.
\newblock Averaged-dqn: Variance reduction and stabilization for deep
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Audibert et~al.(2009)Audibert, Munos, and
  Szepesv{\'a}ri]{audibert2009exploration}
Audibert, J.-Y., Munos, R., and Szepesv{\'a}ri, C.
\newblock Exploration--exploitation tradeoff using variance estimates in
  multi-armed bandits.
\newblock \emph{Theoretical Computer Science}, 410\penalty0 (19):\penalty0
  1876--1902, 2009.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{auer2002finite}
Auer, P., Cesa-Bianchi, N., and Fischer, P.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock \emph{Machine learning}, 47\penalty0 (2-3):\penalty0 235--256, 2002.

\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,
  Saxton, and Munos]{bellemare2016unifying}
Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and
  Munos, R.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare2013arcade}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and
  Munos]{bellemare2017distributional}
Bellemare, M.~G., Dabney, W., and Munos, R.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Chen et~al.(2017)Chen, Sidor, Abbeel, and Schulman]{chen2017ucb}
Chen, R.~Y., Sidor, S., Abbeel, P., and Schulman, J.
\newblock Ucb exploration via q-ensembles.
\newblock \emph{arXiv preprint arXiv:1706.01502}, 2017.

\bibitem[Choi et~al.(2019)Choi, Guo, Moczulski, Oh, Wu, Norouzi, and
  Lee]{choi2018contingency}
Choi, J., Guo, Y., Moczulski, M., Oh, J., Wu, N., Norouzi, M., and Lee, H.
\newblock Contingency-aware exploration in reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Chua et~al.(2018)Chua, Calandra, McAllister, and Levine]{chua2018deep}
Chua, K., Calandra, R., McAllister, R., and Levine, S.
\newblock Deep reinforcement learning in a handful of trials using
  probabilistic dynamics models.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Efron(1982)]{efron1982jackknife}
Efron, B.
\newblock \emph{The jackknife, the bootstrap, and other resampling plans},
  volume~38.
\newblock Siam, 1982.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Van~Hoof, and
  Meger]{fujimoto2018addressing}
Fujimoto, S., Van~Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Hafner et~al.(2019)Hafner, Lillicrap, Fischer, Villegas, Ha, Lee, and
  Davidson]{hafner2018learning}
Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and
  Davidson, J.
\newblock Learning latent dynamics for planning from pixels.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Hafner et~al.(2020)Hafner, Lillicrap, Ba, and
  Norouzi]{hafner2019dream}
Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M.
\newblock Dream to control: Learning behaviors by latent imagination.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Hasselt(2010)]{hasselt2010double}
Hasselt, H.~V.
\newblock Double q-learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2010.

\bibitem[Hessel et~al.(2018)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{hessel2018rainbow}
Hessel, M., Modayil, J., Van~Hasselt, H., Schaul, T., Ostrovski, G., Dabney,
  W., Horgan, D., Piot, B., Azar, M., and Silver, D.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2018.

\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and
  Abbeel]{houthooft2016vime}
Houthooft, R., Chen, X., Duan, Y., Schulman, J., De~Turck, F., and Abbeel, P.
\newblock Vime: Variational information maximizing exploration.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Kaiser et~al.(2020)Kaiser, Babaeizadeh, Milos, Osinski, Campbell,
  Czechowski, Erhan, Finn, Kozakowski, Levine, et~al.]{kaiser2019model}
Kaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R.~H.,
  Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., et~al.
\newblock Model-based reinforcement learning for atari.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, et~al.]{kalashnikov2018qt}
Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E.,
  Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., et~al.
\newblock Qt-opt: Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock In \emph{Conference on Robot Learning}, 2018.

\bibitem[Kim et~al.(2019)Kim, Asadi, Littman, and Konidaris]{kim2019deepmellow}
Kim, S., Asadi, K., Littman, M., and Konidaris, G.
\newblock Deepmellow: removing the need for a target network in deep
  q-learning.
\newblock In \emph{International Joint Conference on Artificial Intelligence},
  2019.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Yarats, and
  Fergus]{kostrikov2020image}
Kostrikov, I., Yarats, D., and Fergus, R.
\newblock Image augmentation is all you need: Regularizing deep reinforcement
  learning from pixels.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Soh, Tucker, and
  Levine]{kumar2019stabilizing}
Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Gupta, and Levine]{kumar2020discor}
Kumar, A., Gupta, A., and Levine, S.
\newblock Discor: Corrective feedback in reinforcement learning via
  distribution correction.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Kurutach et~al.(2018)Kurutach, Clavera, Duan, Tamar, and
  Abbeel]{kurutach2018model}
Kurutach, T., Clavera, I., Duan, Y., Tamar, A., and Abbeel, P.
\newblock Model-ensemble trust-region policy optimization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{lakshminarayanan2017simple}
Lakshminarayanan, B., Pritzel, A., and Blundell, C.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Lan et~al.(2020)Lan, Pan, Fyshe, and White]{lan2020maxmin}
Lan, Q., Pan, Y., Fyshe, A., and White, M.
\newblock Maxmin q-learning: Controlling the estimation bias of q-learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Laskin et~al.(2020)Laskin, Lee, Stooke, Pinto, Abbeel, and
  Srinivas]{laskin2020reinforcement}
Laskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., and Srinivas, A.
\newblock Reinforcement learning with augmented data.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Lee et~al.(2020)Lee, Nagabandi, Abbeel, and Levine]{lee2019stochastic}
Lee, A.~X., Nagabandi, A., Abbeel, P., and Levine, S.
\newblock Stochastic latent actor-critic: Deep reinforcement learning with a
  latent variable model.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529, 2015.

\bibitem[Osband et~al.(2016{\natexlab{a}})Osband, Blundell, Pritzel, and
  Van~Roy]{osband2016deep}
Osband, I., Blundell, C., Pritzel, A., and Van~Roy, B.
\newblock Deep exploration via bootstrapped dqn.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2016{\natexlab{a}}.

\bibitem[Osband et~al.(2016{\natexlab{b}})Osband, Van~Roy, and
  Wen]{osband2016generalization}
Osband, I., Van~Roy, B., and Wen, Z.
\newblock Generalization and exploration via randomized value functions.
\newblock In \emph{International Conference on Machine Learning},
  2016{\natexlab{b}}.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and
  Darrell]{pathak2017curiosity}
Pathak, D., Agrawal, P., Efros, A.~A., and Darrell, T.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and
  Silver]{schaul2015prioritized}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Chen, and
  Abbeel]{schulman2017equivalence}
Schulman, J., Chen, X., and Abbeel, P.
\newblock Equivalence between policy gradients and soft q-learning.
\newblock \emph{arXiv preprint arXiv:1704.06440}, 2017.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
  A., Hubert, T., Baker, L., Lai, M., Bolton, A., et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{Nature}, 550\penalty0 (7676):\penalty0 354, 2017.

\bibitem[Silver et~al.(2018)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2018general}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
  Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al.
\newblock A general reinforcement learning algorithm that masters chess, shogi,
  and go through self-play.
\newblock \emph{Science}, 362\penalty0 (6419):\penalty0 1140--1144, 2018.

\bibitem[Song et~al.(2019)Song, Parr, and Carin]{song2019revisiting}
Song, Z., Parr, R., and Carin, L.
\newblock Revisiting the softmax bellman operator: New benefits and new
  perspective.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Srinivas et~al.(2018)Srinivas, Jabri, Abbeel, Levine, and
  Finn]{srinivas2018universal}
Srinivas, A., Jabri, A., Abbeel, P., Levine, S., and Finn, C.
\newblock Universal planning networks.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Srinivas et~al.(2020)Srinivas, Laskin, and Abbeel]{srinivas2020curl}
Srinivas, A., Laskin, M., and Abbeel, P.
\newblock Curl: Contrastive unsupervised representations for reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Tamar et~al.(2016)Tamar, Wu, Thomas, Levine, and
  Abbeel]{tamar2016value}
Tamar, A., Wu, Y., Thomas, G., Levine, S., and Abbeel, P.
\newblock Value iteration networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Tassa et~al.(2018)Tassa, Doron, Muldal, Erez, Li, Casas, Budden,
  Abdolmaleki, Merel, Lefrancq, et~al.]{tassa2018deepmind}
Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d.~L., Budden,
  D., Abdolmaleki, A., Merel, J., Lefrancq, A., et~al.
\newblock Deepmind control suite.
\newblock \emph{arXiv preprint arXiv:1801.00690}, 2018.

\bibitem[Tobin et~al.(2017)Tobin, Fong, Ray, Schneider, Zaremba, and
  Abbeel]{tobin2017domain}
Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel, P.
\newblock Domain randomization for transferring deep neural networks from
  simulation to the real world.
\newblock In \emph{International Conference on Intelligent Robots and Systems},
  2017.

\bibitem[Torabi et~al.(2018)Torabi, Warnell, and Stone]{torabi2018behavioral}
Torabi, F., Warnell, G., and Stone, P.
\newblock Behavioral cloning from observation.
\newblock In \emph{International Joint Conferences on Artificial Intelligence
  Organization}, 2018.

\bibitem[Van~Hasselt et~al.(2016)Van~Hasselt, Guez, and Silver]{van2016deep}
Van~Hasselt, H., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2016.

\bibitem[van Hasselt et~al.(2019)van Hasselt, Hessel, and
  Aslanides]{van2019use}
van Hasselt, H.~P., Hessel, M., and Aslanides, J.
\newblock When to use parametric models in reinforcement learning?
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Wang \& Ba(2020)Wang and Ba]{wang2019exploring}
Wang, T. and Ba, J.
\newblock Exploring model-based planning with policy networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Wang et~al.(2019)Wang, Bao, Clavera, Hoang, Wen, Langlois, Zhang,
  Zhang, Abbeel, and Ba]{wang2019benchmarking}
Wang, T., Bao, X., Clavera, I., Hoang, J., Wen, Y., Langlois, E., Zhang, S.,
  Zhang, G., Abbeel, P., and Ba, J.
\newblock Benchmarking model-based reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1907.02057}, 2019.

\bibitem[Wiering \& Van~Hasselt(2008)Wiering and
  Van~Hasselt]{wiering2008ensemble}
Wiering, M.~A. and Van~Hasselt, H.
\newblock Ensemble algorithms in reinforcement learning.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics},
  38\penalty0 (4):\penalty0 930--936, 2008.

\bibitem[Yarats et~al.(2019)Yarats, Zhang, Kostrikov, Amos, Pineau, and
  Fergus]{yarats2019improving}
Yarats, D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J., and Fergus, R.
\newblock Improving sample efficiency in model-free reinforcement learning from
  images.
\newblock \emph{arXiv preprint arXiv:1910.01741}, 2019.

\bibitem[Ziebart(2010)]{ziebart2010modeling}
Ziebart, B.~D.
\newblock Modeling purposeful adaptive behavior with the principle of maximum
  causal entropy.
\newblock 2010.

\end{thebibliography}
