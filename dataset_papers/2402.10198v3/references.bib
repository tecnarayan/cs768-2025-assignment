% RevIN
@inproceedings{kim2021reversible,
  title     = {Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift},
  author    = {Kim, Taesung and 
               Kim, Jinhee and 
               Tae, Yunwon and 
               Park, Cheonbok and 
               Choi, Jang-Ho and 
               Choo, Jaegul},
  booktitle = {International Conference on Learning Representations},
  year      = {2021},
  url       = {https://openreview.net/forum?id=cGDAkQo1C0p}
}

% PatchTST
@inproceedings{nie2023patchtst,
title={A Time Series is Worth 64 Words:  Long-term Forecasting with Transformers},
author={Yuqi Nie and Nam H Nguyen and Phanwadee Sinthong and Jayant Kalagnanam},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Jbdc0vTOcol}
}

% TSMixer
@article{chen2023tsmixer,
title={{TSM}ixer: An All-{MLP} Architecture for Time Series Forecasting},
author={Si-An Chen and Chun-Liang Li and Sercan O Arik and Nathanael Christian Yoder and Tomas Pfister},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=wbpxTuXgm0},
note={}
}

% attention is all you need
@inproceedings{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

% Are transformers effective
@inproceedings{zeng2022effective,
  title={Are Transformers Effective for Time Series Forecasting?},
  author={Ailing Zeng and Muxi Chen and Lei Zhang and Qiang Xu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2023}
}

% FedFormer
@inproceedings{zhou2022fedformer,
  title={{FEDformer}: Frequency enhanced decomposed transformer for long-term series forecasting},
  author={Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Wang, Xue and Sun, Liang and Jin, Rong},
  booktitle={Proc. 39th International Conference on Machine Learning (ICML 2022)},
  location = {Baltimore, Maryland},
  pages={},
  year={2022}
}

% AutoFormer
@inproceedings{wu2021autoformer,
  title={Autoformer: Decomposition Transformers with {Auto-Correlation} for Long-Term Series Forecasting},
  author={Haixu Wu and Jiehui Xu and Jianmin Wang and Mingsheng Long},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

% Informer
@inproceedings{haoyi2021informer,
  author    = {Haoyi Zhou and
               Shanghang Zhang and
               Jieqi Peng and
               Shuai Zhang and
               Jianxin Li and
               Hui Xiong and
               Wancai Zhang},
  title     = {Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting},
  booktitle = {The Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI} 2021, Virtual Conference},
  volume    = {35},
  pages     = {11106--11115},
  publisher = {{AAAI} Press},
  year      = {2021},
}

% ViT
@inproceedings{dosovitskiy2021vit,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

% Bert
@misc{devlin2018bert,
  abstract = {We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations by jointly conditioning on both left and right
context in all layers. As a result, the pre-trained BERT representations can be
fine-tuned with just one additional output layer to create state-of-the-art
models for a wide range of tasks, such as question answering and language
inference, without substantial task-specific architecture modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI
accuracy to 86.7 (5.6% absolute improvement) and the SQuAD v1.1 question
answering Test F1 to 93.2 (1.5% absolute improvement), outperforming human
performance by 2.0%.},
  added-at = {2019-02-05T23:35:51.000+0100},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  biburl = {https://www.bibsonomy.org/bibtex/210c860e3f390c6fbfd78a3b91ab9b0af/albinzehe},
  description = {[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  interhash = {a74f4c3853d3f0340e75546639134e91},
  intrahash = {10c860e3f390c6fbfd78a3b91ab9b0af},
  keywords = {bert elmo embeddings kallimachos nlp proposal-knowledge wordembeddings},
  timestamp = {2020-07-28T14:17:24.000+0200},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding},
  url = {http://arxiv.org/abs/1810.04805},
  year = 2018
}


% GPT
@article{radford2018improving,
  added-at = {2020-07-14T16:37:42.000+0200},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/273ced32c0d4588eb95b6986dc2c8147c/jonaskaiser},
  interhash = {5c343ed9a31ac52fd17a898f72af228f},
  intrahash = {73ced32c0d4588eb95b6986dc2c8147c},
  keywords = {final thema:transformer},
  timestamp = {2020-07-14T16:49:42.000+0200},
  title = {Improving language understanding by generative pre-training},
journal= {2018 OpenAI Tech Report},
  year = 2018
}

% GPT4
@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% Vit outperforms resnets with SAM
@inproceedings{chen2022vitwithsam,
title={When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations},
author={Xiangning Chen and Cho-Jui Hsieh and Boqing Gong},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=LtKcMgGOeLt}
}
#Transformers learn to implement preconditioned gradient
@inproceedings{
ahn2023transformers,
title={Transformers learn to implement preconditioned gradient descent for in-context learning},
author={Kwangjun Ahn and Xiang Cheng and Hadi Daneshmand and Suvrit Sra},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=LziniAXEI9}
}

% SAM
@inproceedings{foret2021sharpnessaware,
title={Sharpness-aware Minimization for Efficiently Improving Generalization},
author={Pierre Foret and Ariel Kleiner and Hossein Mobahi and Behnam Neyshabur},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=6Tm1mposlrM}
}

% Generalization gap and sharp minima
@inproceedings{keskar2017sharpminima,
title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
author={Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=H1oyRlYgg}
}

% nonvacuous generalization bounds for DNNs
@inproceedings{dziugaite2017nonvacuous,
        title = {Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data},
       author = {Gintare Karolina Dziugaite and Daniel M. Roy},
         year = {2017},
    booktitle = {Proceedings of the 33rd Annual Conference on Uncertainty in Artificial Intelligence (UAI)},
archivePrefix = {arXiv},
       eprint = {1703.11008},
}

% fantastic generalization bounds
@inproceedings{jiang2020fantastic,
title={Fantastic Generalization Measures and Where to Find Them},
author={Yiding Jiang and Behnam Neyshabur and Hossein Mobahi and Dilip Krishnan and Samy Bengio},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJgIPJBFvH}
}

% Entropy-SGD
@inproceedings{chaudhari2017entropysgd,
title={Entropy-{SGD}: Biasing Gradient Descent Into Wide Valleys},
author={Pratik Chaudhari and Anna Choromanska and Stefano Soatto and Yann LeCun and Carlo Baldassi and Christian Borgs and Jennifer Chayes and Levent Sagun and Riccardo Zecchina},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=B1YfAfcgl}
}

% Rocket
@article{dempster2020rocket,
   title={ROCKET: exceptionally fast and accurate time series classification using random convolutional kernels},
   volume={34},
   ISSN={1573-756X},
   url={http://dx.doi.org/10.1007/s10618-020-00701-z},
   DOI={10.1007/s10618-020-00701-z},
   number={5},
   journal={Data Mining and Knowledge Discovery},
   publisher={Springer Science and Business Media LLC},
   author={Dempster, Angus and Petitjean, François and Webb, Geoffrey I.},
   year={2020},
   month=jul, pages={1454–1495} }

% Visualization of loss landscapes
@inproceedings{li2018visualloss,
  title={Visualizing the Loss Landscape of Neural Nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle={Neural Information Processing Systems},
  year={2018}
}

% NTK
@inproceedings{jacot2018NTK,
 author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf},
 volume = {31},
 year = {2018}
}

% PL condition for overparameterized models Belkin
@article{liu2022pl,
title = {Loss landscapes and optimization in over-parameterized non-linear systems and neural networks},
journal = {Applied and Computational Harmonic Analysis},
volume = {59},
pages = {85-116},
year = {2022},
note = {Special Issue on Harmonic Analysis and Machine Learning},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2021.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S106352032100110X},
author = {Chaoyue Liu and Libin Zhu and Mikhail Belkin},
keywords = {Deep learning, Non-linear optimization, Over-parameterized models, PL condition},
abstract = {The success of deep learning is due, to a large extent, to the remarkable effectiveness of gradient-based optimization methods applied to large neural networks. The purpose of this work is to propose a modern view and a general mathematical framework for loss landscapes and efficient optimization in over-parameterized machine learning models and systems of non-linear equations, a setting that includes over-parameterized deep neural networks. Our starting observation is that optimization landscapes corresponding to such systems are generally not convex, even locally around a global minimum, a condition we call essential non-convexity. We argue that instead they satisfy PL⁎, a variant of the Polyak-Łojasiewicz condition [32], [25] on most (but not all) of the parameter space, which guarantees both the existence of solutions and efficient optimization by (stochastic) gradient descent (SGD/GD). The PL⁎ condition of these systems is closely related to the condition number of the tangent kernel associated to a non-linear system showing how a PL⁎-based non-linear theory parallels classical analyses of over-parameterized linear equations. We show that wide neural networks satisfy the PL⁎ condition, which explains the (S)GD convergence to a global minimum. Finally we propose a relaxation of the PL⁎ condition applicable to “almost” over-parameterized systems.}
}

% when and why the tangent kernel is constant Belkin
@inproceedings{liu2020when,
 author = {Liu, Chaoyue and Zhu, Libin and Belkin, Misha},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15954--15964},
 publisher = {Curran Associates, Inc.},
 title = {On the linearity of large non-linear models: when and why the tangent kernel is constant},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/b7ae8fecf15b8b6c3c69eceae636d203-Paper.pdf},
 volume = {33},
 year = {2020}
}

% Condition number kappa as trainability
@misc{xiao2020disantangling,
title={Disentangling Trainability and Generalization in Deep Learning},
author={Lechao Xiao and Jeffrey Pennington and Sam Schoenholz},
year={2020},
url={https://openreview.net/forum?id=Bkx1mxSKvB}
}

@inproceedings{chen2021neural,
title={Neural Architecture Search on ImageNet in Four {\{}GPU{\}} Hours: A Theoretically Inspired Perspective},
author={Wuyang Chen and Xinyu Gong and Zhangyang Wang},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=Cnon5ezMHtu}
}

% Theoretical analyzes of SAAM: worst case and average directions
@inproceedings{wen2023how,
title={How Sharpness-Aware Minimization Minimizes Sharpness?},
author={Kaiyue Wen and Tengyu Ma and Zhiyuan Li},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=5spDgWmpY6x}
}

% SGD
@article{nesterov1983sgd,
  title={A method for solving the convex programming problem with convergence rate $O(1/k^2)$},
  author={Yurii Nesterov},
  journal={Proceedings of the USSR Academy of Sciences},
  year={1983},
  volume={269},
  pages={543-547},
  url={https://api.semanticscholar.org/CorpusID:145918791}
}

% Adam
@InProceedings{KingBa15,
  author    = {Kingma, Diederik and Ba, Jimmy},
  booktitle = {International Conference on Learning Representations (ICLR)},
  title     = {Adam: A Method for Stochastic Optimization},
  year      = {2015},
  address   = {San Diega, CA, USA},
  optmonth  = {12},
}

% Understanding transformers difficult to train
@inproceedings{liu2020understanding,
  title={Understanding the Difficulty of Training Transformers},
  author = {Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)},
  year={2020}
}

% mimetic initialization
@inproceedings{trockman2023mimetic,
author = {Trockman, Asher and Kolter, J. Zico},
title = {Mimetic Initialization of Self-Attention Layers},
year = {2023},
publisher = {JMLR.org},
abstract = {It is notoriously difficult to train Transformers on small datasets; typically, large pre-trained models are instead used as the starting point. We explore the weights of such pre-trained Transformers (particularly for vision) to attempt to find reasons for this discrepancy. Surprisingly, we find that simply initializing the weights of self-attention layers so that they "look" more like their pre-trained counterparts allows us to train vanilla Transformers faster and to higher final accuracies, particularly on vision tasks such as CIFAR-10 and ImageNet classification, where we see gains in accuracy of over 5\% and 4\%, respectively. Our initialization scheme is closed form, learning-free, and very simple: we set the product of the query and key weights to be approximately the identity, and the product of the value and projection weights to approximately the negative identity. As this mimics the patterns we saw in pre-trained Transformers, we call the technique mimetic initialization.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1435},
numpages = {13},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

% Adam outperforms SGD on language models
@inproceedings{yadav2023why,
title={Why Adam Outperforms Gradient Descent on Language Models: A Heavy-Tailed Class Imbalance Problem},
author={Robin Yadav and Frederik Kunstner and Mark Schmidt and Alberto Bietti},
booktitle={OPT 2023: Optimization for Machine Learning},
year={2023},
url={https://openreview.net/forum?id=iqLUnznmxC}
}

% Apple entropy collapse
@InProceedings{zhai2023collapse,
  title = 	 {Stabilizing Transformer Training by Preventing Attention Entropy Collapse},
  author =       {Zhai, Shuangfei and Likhomanenko, Tatiana and Littwin, Etai and Busbridge, Dan and Ramapuram, Jason and Zhang, Yizhe and Gu, Jiatao and Susskind, Joshua M.},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {40770--40803},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/zhai23a/zhai23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/zhai23a.html},
}

#Rank Collapse
@inproceedings{anagnostidis2022signal,
title={Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse},
author={Sotiris Anagnostidis and Luca Biggio and Lorenzo Noci and Antonio Orvieto and Sidak Pal Singh and Aurelien Lucchi},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=FxVH7iToXS}
}

#On the Convergence of Encoder-only Shallow Transformers
@inproceedings{
wu2023on,
title={On the Convergence of Encoder-only Shallow Transformers},
author={Yongtao Wu and Fanghui Liu and Grigorios Chrysos and Volkan Cevher},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=8ZveVHfmIE}
}

#ResNeSt : channel-wise attention
@InProceedings{Zhang_2022_CVPR,
    author    = {Zhang, Hang and Wu, Chongruo and Zhang, Zhongyue and Zhu, Yi and Lin, Haibin and Zhang, Zhi and Sun, Yue and He, Tong and Mueller, Jonas and Manmatha, R. and Li, Mu and Smola, Alexander},
    title     = {ResNeSt: Split-Attention Networks},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2022},
    pages     = {2736-2746}
}


% Llama
@misc{touvron2023llama,
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from
7B to 65B parameters. We train our models on trillions of tokens, and show that
it is possible to train state-of-the-art models using publicly available
datasets exclusively, without resorting to proprietary and inaccessible
datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,
and LLaMA-65B is competitive with the best models, Chinchilla-70B and
PaLM-540B. We release all our models to the research community.},
  added-at = {2023-03-29T15:33:41.000+0200},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  biburl = {https://www.bibsonomy.org/bibtex/2731b422f753eccd5c5d65fbde4856c6d/jil},
  description = {[2302.13971] LLaMA: Open and Efficient Foundation Language Models},
  interhash = {03a85d2a0612b9704acf6884edbe60aa},
  intrahash = {731b422f753eccd5c5d65fbde4856c6d},
  keywords = {facebook llm meta model pretrained},
  note = {cite arxiv:2302.13971},
  timestamp = {2023-03-29T15:34:56.000+0200},
  title = {LLaMA: Open and Efficient Foundation Language Models},
  url = {http://arxiv.org/abs/2302.13971},
  year = 2023
}

% DINO
@inproceedings{caron2021emerging,
  title={Emerging Properties in Self-Supervised Vision Transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J\'egou, Herv\'e  and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Proceedings of the International Conference on Computer Vision (ICCV)},
  year={2021}
}

% ImageNet
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

% NarrativeXL
@inproceedings{moskvichev2023narrativexl,
title={Narrative{XL}: a Large-scale Dataset for Long-Term Memory Models},
author={Arsenii Kirillovich Moskvichev and Ky-Vinh Mai},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
url={https://openreview.net/forum?id=3QibSyz6Qt}
}

% LLM optimal training compute
@inproceedings{hoffmann2022an,
title={An empirical analysis of compute-optimal large language model training},
author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katherine Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Oriol Vinyals and Jack William Rae and Laurent Sifre},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=iBBcRUlOAPR}
}

% Scaling law of language models
@misc{kaplan2020scaling,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Efficient viT and attention distillation
@InProceedings{touvron2021efficient,
  title = 	 {Training data-efficient image transformers and distillation through attention},
  author =       {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {10347--10357},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/touvron21a.html}
}

% Electricity dataset
@misc{electricity,
  author = {UCI},
  title = {Electricity dataset},
  url = {https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014},
  urldate = {2024-01-13},
  year= {2015}
}

% Exchange rate dataset
@inproceedings{lai2018modeling,
author = {Lai, Guokun and Chang, Wei-Cheng and Yang, Yiming and Liu, Hanxiao},
title = {Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks},
year = {2018},
isbn = {9781450356572},
booktitle = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210006},
doi = {10.1145/3209978.3210006},
pages = {95–104},
numpages = {10},
keywords = {autoregressive models, neural network, multivariate time series},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

% Traffic dataset
@misc{traffic,
  author = {{California Department of Transportation}},
  title = {Traffic dataset},
  url = {https://pems.dot.ca.gov/},
  urldate = {2024-01-13},
year= {2021}
}

% Weather dataset
@misc{weather,
  author = {{Max Planck Institute}},
  title = {Weather dataset},
  url = {https://www.bgc-jena.mpg.de/wetter/},
  urldate = {2024-01-13},
  year= {2021}
}

% LogTrans
@inproceedings{li2019logtrans,
 author = {Li, Shiyang and Jin, Xiaoyong and Xuan, Yao and Zhou, Xiyou and Chen, Wenhu and Wang, Yu-Xiang and Yan, Xifeng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf},
 volume = {32},
 year = {2019}
}

% Pyraformer
@inproceedings{liu2022pyraformer,
title={Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting},
author={Shizhan Liu and Hang Yu and Cong Liao and Jianguo Li and Weiyao Lin and Alex X. Liu and Schahram Dustdar},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=0EXmFzUn5I}
}

% Triformer
@inproceedings{cirstea2022triformer,
  title     = {Triformer: Triangular, Variable-Specific Attentions for Long Sequence Multivariate Time Series Forecasting},
  author    = {Cirstea, Razvan-Gabriel and Guo, Chenjuan and Yang, Bin and Kieu, Tung and Dong, Xuanyi and Pan, Shirui},
  booktitle = {Proceedings of the Thirty-First International Joint Conference on
               Artificial Intelligence, {IJCAI-22}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Lud De Raedt},
  pages     = {1994--2001},
  year      = {2022},
  month     = {7},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2022/277},
  url       = {https://doi.org/10.24963/ijcai.2022/277},
}

% Reformer
@inproceedings{kitaev2020reformer,
title={Reformer: The Efficient Transformer},
author={Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgNKkHtvB}
}

% Transformers are universal approximations of sequence-to-sequence functions
@inproceedings{yun2020universal,
title={Are Transformers universal approximators of sequence-to-sequence functions?},
author={Chulhee Yun and Srinadh Bhojanapalli and Ankit Singh Rawat and Sashank Reddi and Sanjiv Kumar},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=ByxRM0Ntvr}
}

% attention is NOT all you need: rank of attention
@InProceedings{dong2021attentionrank,
  title = 	 {Attention is not all you need: pure attention loses rank doubly exponentially with depth},
  author =       {Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2793--2803},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/dong21a/dong21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/dong21a.html},
  abstract = 	 {Attention-based architectures have become ubiquitous in machine learning. Yet, our understanding of the reasons for their effectiveness remains limited. This work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms—or paths—each involving the operation of a sequence of attention heads across layers. Using this path decomposition, we prove that self-attention possesses a strong inductive bias towards "token uniformity". Specifically, without skip connections or multi-layer perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and MLPs stop the output from degeneration. Our experiments verify the convergence results on standard transformer architectures.}
}

% Survey time series forecasting
@Article{casolaro2023survey,
AUTHOR = {Casolaro, Angelo and Capone, Vincenzo and Iannuzzo, Gennaro and Camastra, Francesco},
TITLE = {Deep Learning for Time Series Forecasting: Advances and Open Problems},
JOURNAL = {Information},
VOLUME = {14},
YEAR = {2023},
NUMBER = {11},
ARTICLE-NUMBER = {598},
URL = {https://www.mdpi.com/2078-2489/14/11/598},
ISSN = {2078-2489},
DOI = {10.3390/info14110598}
}

%n Stock prices forecasting
@Article{sonkavde2023stock,
AUTHOR = {Sonkavde, Gaurang and Dharrao, Deepak Sudhakar and Bongale, Anupkumar M. and Deokate, Sarika T. and Doreswamy, Deepak and Bhat, Subraya Krishna},
TITLE = {Forecasting Stock Market Prices Using Machine Learning and Deep Learning Models: A Systematic Review, Performance Analysis and Discussion of Implications},
JOURNAL = {International Journal of Financial Studies},
VOLUME = {11},
YEAR = {2023},
NUMBER = {3},
ARTICLE-NUMBER = {94},
URL = {https://www.mdpi.com/2227-7072/11/3/94},
ISSN = {2227-7072},
DOI = {10.3390/ijfs11030094}
}

% Forecasting on electrocardiogram
@Article{cepulionis2016electro,
author={{\v{C}}epulionis, Paulius
and Luko{\v{s}}evi{\v{c}}i{\={u}}t{\.{e}}, Kristina},
title={Electrocardiogram time series forecasting and optimization using ant colony optimization algorithm},
journal={Mathematical Models in Engineering},
year={2016},
month={Jun},
day={30},
publisher={JVE International Ltd.},
volume={2},
number={1},
pages={69-77},
issn={2351-5279},
url={https://www.extrica.com/article/17229}
}

% LSTM
@Article{hoch1997lstm,
  author      = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  number      = {8},
  pages       = {1735--1780},
  volume      = {9},
  optdoi      = {10.1162/neco.1997.9.8.1735},
  opteprint   = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
  opturl      = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
}

% GRU
@inproceedings{cho2014gru,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}

% Recurrent dep state space for forecasting
@inproceedings{rangapuram2018deep,
author = {Rangapuram, Syama Sundar and Seeger, Matthias W and Gasthaus, Jan and Stella, Lorenzo and Wang, Yuyang and Januschowski, Tim},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep State Space Models for Time Series Forecasting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/5cf68969fb67aa6082363a6d4e6468e2-Paper.pdf},
 volume = {31},
 year = {2018}
}

% DeepAR: recurrent for forecasting
@article{salinas2020deepar,
title = {DeepAR: Probabilistic forecasting with autoregressive recurrent networks},
journal = {International Journal of Forecasting},
volume = {36},
number = {3},
pages = {1181-1191},
year = {2020},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2019.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169207019301888},
author = {David Salinas and Valentin Flunkert and Jan Gasthaus and Tim Januschowski},
keywords = {Probabilistic forecasting, Neural networks, Deep learning, Big data, Demand forecasting},
abstract = {Probabilistic forecasting, i.e., estimating a time series’ future probability distribution given its past, is a key enabler for optimizing business processes. In retail businesses, for example, probabilistic demand forecasts are crucial for having the right inventory available at the right time and in the right place. This paper proposes DeepAR, a methodology for producing accurate probabilistic forecasts, based on training an autoregressive recurrent neural network model on a large number of related time series. We demonstrate how the application of deep learning techniques to forecasting can overcome many of the challenges that are faced by widely-used classical approaches to the problem. By means of extensive empirical evaluations on several real-world forecasting datasets, we show that our methodology produces more accurate forecasts than other state-of-the-art methods, while requiring minimal manual work.}
}

% Sinkformer
@InProceedings{sander2022sinkformer,
  title = 	 { Sinkformers: Transformers with Doubly Stochastic Attention },
  author =       {Sander, Michael E. and Ablin, Pierre and Blondel, Mathieu and Peyr\'e, Gabriel},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3515--3530},
  year = 	 {2022},
  editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  volume = 	 {151},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28--30 Mar},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v151/sander22a/sander22a.pdf},
  url = 	 {https://proceedings.mlr.press/v151/sander22a.html},
  abstract = 	 { Attention based models such as Transformers involve pairwise interactions between data points, modeled with a learnable attention matrix. Importantly, this attention matrix is normalized with the SoftMax operator, which makes it row-wise stochastic. In this paper, we propose instead to use Sinkhorn’s algorithm to make attention matrices doubly stochastic. We call the resulting model a Sinkformer. We show that the row-wise stochastic attention matrices in classical Transformers get close to doubly stochastic matrices as the number of epochs increases, justifying the use of Sinkhorn normalization as an informative prior. On the theoretical side, we show that, unlike the SoftMax operation, this normalization makes it possible to understand the iterations of self-attention modules as a discretized gradient-flow for the Wasserstein metric. We also show in the infinite number of samples limit that, when rescaling both attention matrices and depth, Sinkformers operate a heat diffusion. On the experimental side, we show that Sinkformers enhance model accuracy in vision and natural language processing tasks. In particular, on 3D shapes classification, Sinkformers lead to a significant improvement. }
}

% lipschitz constant of attention
@InProceedings{kim2021lipschitz,
  title = 	 {The Lipschitz Constant of Self-Attention},
  author =       {Kim, Hyunjik and Papamakarios, George and Mnih, Andriy},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {5562--5571},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/kim21i/kim21i.pdf},
  url = 	 {https://proceedings.mlr.press/v139/kim21i.html},
  abstract = 	 {Lipschitz constants of neural networks have been explored in various contexts in deep learning, such as provable adversarial robustness, estimating Wasserstein distance, stabilising training of GANs, and formulating invertible neural networks. Such works have focused on bounding the Lipschitz constant of fully connected or convolutional networks, composed of linear maps and pointwise non-linearities. In this paper, we investigate the Lipschitz constant of self-attention, a non-linear neural network module widely used in sequence modelling. We prove that the standard dot-product self-attention is not Lipschitz for unbounded input domain, and propose an alternative L2 self-attention that is Lipschitz. We derive an upper bound on the Lipschitz constant of L2 self-attention and provide empirical evidence for its asymptotic tightness. To demonstrate the practical relevance of our theoretical work, we formulate invertible self-attention and use it in a Transformer-based architecture for a character-level language modelling task.}
}

% glorot initialization
@InProceedings{glorot2010initialization,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

% Cosine annealing (SGDR)
@inproceedings{loshchilov2017sgdr,
title={{SGDR}: Stochastic Gradient Descent with Warm Restarts},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=Skq89Scxx}
}

% Linear attention is maybe all you need
@misc{ahn2023linear,
      title={Linear attention is (maybe) all you need (to understand transformer optimization)}, 
      author={Kwangjun Ahn and Xiang Cheng and Minhak Song and Chulhee Yun and Ali Jadbabaie and Suvrit Sra},
      year={2023},
      eprint={2310.01082},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Why adam converges faster than SGD for transformers
@inproceedings{pan2022toward,
title={Toward Understanding Why Adam Converges Faster Than {SGD} for Transformers},
author={Yan Pan and Yuanzhi Li},
booktitle={OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)},
year={2022},
url={https://openreview.net/forum?id=Sf1NlV2r6PO}
}

% Why are adaptive methods good for attention models
@inproceedings{zhang2020adaptattention,
 author = {Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {15383--15393},
 publisher = {Curran Associates, Inc.},
 title = {Why are Adaptive Methods Good for Attention Models?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/b05b57f6add810d3b7490866d74c0053-Paper.pdf},
 volume = {33},
 year = {2020}
}

% AdamW
@inproceedings{loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

% Forecasting and Control (ARIMA)
@Article{box1974forecasting,
  author={G. E. P. Box and G. M. Jenkins and J. F. MacGregor},
  title={{Some Recent Advances in Forecasting and Control}},
  journal={Journal of the Royal Statistical Society Series C},
  year=1974,
  volume={23},
  number={2},
  pages={158-179},
  month={June},
  keywords={},
  doi={10.2307/2346997},
  url={https://ideas.repec.org/a/bla/jorssc/v23y1974i2p158-179.html}
}

@book{box1990arima,
author = {Box, George Edward Pelham and Jenkins, Gwilym},
title = {Time Series Analysis, Forecasting and Control},
year = {1990},
isbn = {0816211043},
publisher = {Holden-Day, Inc.},
address = {USA}
}

% Classic tools
@article{sorjamaa2007methodology,
title = {Methodology for long-term prediction of time series},
journal = {Neurocomputing},
volume = {70},
number = {16},
pages = {2861-2869},
year = {2007},
note = {Neural Network Applications in Electrical Engineering Selected papers from the 3rd International Work-Conference on Artificial Neural Networks (IWANN 2005)},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2006.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S0925231207001610},
author = {Antti Sorjamaa and Jin Hao and Nima Reyhani and Yongnan Ji and Amaury Lendasse},
keywords = {Time series prediction, Input selection, -Nearest neighbors, Mutual information, Nonparametric noise estimation, Recursive prediction, Direct prediction, Least squares support vector machines},
}

% Forecasting with maths and hamiltonian
@InProceedings{chen2021hamiltonian,
  title = 	 {Data-driven Prediction of General Hamiltonian Dynamics via Learning Exactly-Symplectic Maps},
  author =       {Chen, Renyi and Tao, Molei},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {1717--1727},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/chen21r/chen21r.pdf},
  url = 	 {https://proceedings.mlr.press/v139/chen21r.html},
}

% RNN-CNN forecasters
@inproceedings{fan2017multistep,
author = {Fan, Chenyou and Zhang, Yuze and Pan, Yi and Li, Xiaoyue and Zhang, Chi and Yuan, Rong and Wu, Di and Wang, Wensheng and Pei, Jian and Huang, Heng},
title = {Multi-Horizon Time Series Forecasting with Temporal Attention Learning},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330662},
doi = {10.1145/3292500.3330662},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {2527–2535},
numpages = {9},
keywords = {supply chain, sales forecasting, recurrent neural networks, machine learning, business modeling},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

% LSTNet
@inproceedings{lai2018lstnet,
author = {Lai, Guokun and Chang, Wei-Cheng and Yang, Yiming and Liu, Hanxiao},
title = {Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210006},
doi = {10.1145/3209978.3210006},
booktitle = {The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
pages = {95–104},
numpages = {10},
keywords = {autoregressive models, multivariate time series, neural network},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

% Dual-stage attention and RNN
@inproceedings{qin2017dualstage,
author = {Qin, Yao and Song, Dongjin and Cheng, Haifeng and Cheng, Wei and Jiang, Guofei and Cottrell, Garrison W.},
title = {A dual-stage attention-based recurrent neural network for time series prediction},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2627–2633},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

% TCN
@inproceedings{sen2019tcn,
author = {Sen, Rajat and Yu, Hsiang-Fu and Dhillon, Inderjit},
title = {Think globally, act locally: a deep neural network approach to high-dimensional time series forecasting},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {435},
numpages = {10}
}
% MLP-Mixer
@inproceedings{tolstikhin2021mlpmixer,
title={{MLP}-Mixer: An all-{MLP} Architecture for Vision},
author={Ilya Tolstikhin and Neil Houlsby and Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Thomas Unterthiner and Jessica Yung and Andreas Peter Steiner and Daniel Keysers and Jakob Uszkoreit and Mario Lucic and Alexey Dosovitskiy},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=EI2KOXKdnP}
}

% Sequence-to-to-sequence
@inproceedings{sutsekever2014seqtoseq,
 author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sequence to Sequence Learning with Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf},
 volume = {27},
 year = {2014}
}

% Batch norm and rank collapse
@inproceedings{daneshmand2020bncollapse,
author = {Daneshmand, Hadi and Kohler, Jonas and Bach, Francis and Hofmann, Thomas and Lucchi, Aurelien},
title = {Batch normalization provably avoids rank collapse for randomly initialised deep networks},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1544},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

% Simpler approach to matrix completion
@article{rechet2011matrixcompletion,
author = {Recht, Benjamin},
title = {A Simpler Approach to Matrix Completion},
year = {2011},
issue_date = {2/1/2011},
publisher = {JMLR.org},
volume = {12},
number = {null},
issn = {1532-4435},
abstract = {This paper provides the best bounds to date on the number of randomly sampled entries required to reconstruct an unknown low-rank matrix. These results improve on prior work by Cand\`{e}s and Recht (2009), Cand\`{e}s and Tao (2009), and Keshavan et al. (2009). The reconstruction is accomplished by minimizing the nuclear norm, or sum of the singular values, of the hidden matrix subject to agreement with the provided entries. If the underlying matrix satisfies a certain incoherence condition, then the number of entries required is equal to a quadratic logarithmic factor times the number of parameters in the singular value decomposition. The proof of this assertion is short, self contained, and uses very elementary analysis. The novel techniques herein are based on recent work in quantum information theory.},
journal = {J. Mach. Learn. Res.},
month = {dec},
pages = {3413–3430},
numpages = {18}
}

% Guaranteed Minimum-Rank Solutions
@article{recht2010guaranteed,
author = {Recht, Benjamin and Fazel, Maryam and Parrilo, Pablo A.},
title = {Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization},
journal = {SIAM Review},
volume = {52},
number = {3},
pages = {471-501},
year = {2010},
doi = {10.1137/070697835},
URL = {https://doi.org/10.1137/070697835},
eprint = {https://doi.org/10.1137/070697835}
,
}

% Exact matrix completion via convex optimization
@article{candes2012matrixcompletion,
    author = {Cand\`{e}s, Emmanuel and Recht, Benjamin},
    title = {Exact matrix completion via convex optimization},
    year = {2012},
    issue_date = {June 2012},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {55},
    number = {6},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/2184319.2184343},
    doi = {10.1145/2184319.2184343},
    journal = {Commun. ACM},
    month = {jun},
    pages = {111–119},
    numpages = {9}
}

% Channel-wise attention
@inproceedings{zamir2021Restormer,
    title={Restormer: Efficient Transformer for High-Resolution Image Restoration}, 
    author={Syed Waqas Zamir and Aditya Arora and Salman Khan and Munawar Hayat 
            and Fahad Shahbaz Khan and Ming-Hsuan Yang},
    booktitle={CVPR},
    year={2022}
}

% Signal propagation of transformers without shortcuts
@inproceedings{he2023deepshortcut,
    title={Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation},
    author={Bobby He and James Martens and Guodong Zhang and Aleksandar Botev and Andrew Brock and Samuel L Smith and Yee Whye Teh},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=NPrsUQgMjKK}
}

% How Sharpness-Aware Minimization Minimizes Sharpness
@inproceedings{wen2023howsharp,
    title={How Sharpness-Aware Minimization Minimizes Sharpness?},
    author={Kaiyue Wen and Tengyu Ma and Zhiyuan Li},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=5spDgWmpY6x}
}

@book{Horn_Johnson_1991,
    place={Cambridge},
    title={Topics in Matrix Analysis},
    publisher={Cambridge University Press},
    author={Horn, Roger A. and Johnson, Charles R.},
    year={1991}
} 

% Python
@book{van1995python,
  title={Python reference manual},
  author={Van Rossum, Guido and Drake Jr, Fred L},
  year={1995},
  publisher={Centrum voor Wiskunde en Informatica Amsterdam}
}

% Pytorch
@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.}
}

% Matplotlib
@Article{hunter2007matplotlib,
  Author    = {Hunter, J. D.},
  Title     = {Matplotlib: A 2D graphics environment},
  Journal   = {Computing in Science \& Engineering},
  Volume    = {9},
  Number    = {3},
  Pages     = {90--95},
  abstract  = {Matplotlib is a 2D graphics package used for Python for
  application development, interactive scripting, and publication-quality
  image generation across user interfaces and operating systems.},
  publisher = {IEEE COMPUTER SOC},
  doi       = {10.1109/MCSE.2007.55},
  year      = 2007
}

% sklearn 
@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

% TensorFlow
@misc{tensorflow2015-whitepaper,
title={{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={http://tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dan~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

% MOIRAI
@misc{woo2024unified,
      title={Unified Training of Universal Time Series Forecasting Transformers}, 
      author={Gerald Woo and Chenghao Liu and Akshat Kumar and Caiming Xiong and Silvio Savarese and Doyen Sahoo},
      year={2024},
      eprint={2402.02592},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% iTransformer
@inproceedings{liu2024itransformer,
title={iTransformer: Inverted Transformers Are Effective for Time Series Forecasting},
author={Yong Liu and Tengge Hu and Haoran Zhang and Haixu Wu and Shiyu Wang and Lintao Ma and Mingsheng Long},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=JePfAI8fah}
}