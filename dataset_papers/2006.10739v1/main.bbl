\begin{thebibliography}{10}

\bibitem{div2k_Agustsson_2017}
Eirikur Agustsson and Radu Timofte.
\newblock {NTIRE} 2017 challenge on single image super-resolution: Dataset and
  study.
\newblock {\em CVPR Workshops}, 2017.

\bibitem{arora2019finegrained}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock {\em ICML}, 2019.

\bibitem{basri2020frequency}
Ronen Basri, Meirav Galun, Amnon Geifman, David Jacobs, Yoni Kasten, and Shira
  Kritchman.
\newblock Frequency bias in neural networks for input of non-uniform density.
\newblock {\em arXiv preprint arXiv:2003.04560}, 2020.

\bibitem{basri2019convergence}
Ronen Basri, David Jacobs, Yoni Kasten, and Shira Kritchman.
\newblock The convergence rate of neural networks for learned functions of
  different frequencies.
\newblock {\em NeurIPS}, 2019.

\bibitem{bietti2019inductive}
Alberto Bietti and Julien Mairal.
\newblock On the inductive bias of neural tangent kernels.
\newblock {\em NeurIPS}, 2019.

\bibitem{bordelon2020spectrum}
Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan.
\newblock Spectrum dependent learning curves in kernel regression and wide
  neural networks.
\newblock {\em arXiv preprint arXiv:2002.02561}, 2020.

\bibitem{bracewell}
R.~N. Bracewell.
\newblock Strip integration in radio astronomy.
\newblock {\em Australian Journal of Physics}, 1956.

\bibitem{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, and Skye Wanderman-Milne.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock \url{http://github.com/google/jax}.

\bibitem{implicitfields}
Zhiqin Chen and Hao Zhang.
\newblock Learning implicit fields for generative shape modeling.
\newblock {\em CVPR}, 2019.

\bibitem{neuralarticulated}
Boyang Deng, JP~Lewis, Timothy Jeruzalski, Gerard Pons-Moll, Geoffrey Hinton,
  Mohammad Norouzi, and Andrea Tagliasacchi.
\newblock Neural articulated shape approximation.
\newblock {\em arXiv preprint arXiv:1912.03207}, 2019.

\bibitem{du19}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock {\em ICLR}, 2019.

\bibitem{learningshape}
Kyle Genova, Forrester Cole, Aaron~Sarna Daniel~Vlasic, William~T. Freeman, and
  Thomas Funkhouser.
\newblock Learning shape templates with structured implicit functions.
\newblock {\em ICCV}, 2019.

\bibitem{localdeep}
Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, and Thomas Funkhouser.
\newblock Local deep implicit functions for {3D} shape.
\newblock {\em CVPR}, 2020.

\bibitem{heckel2020compressive}
Reinhard Heckel and Mahdi Soltanolkotabi.
\newblock Compressive sensing with un-trained neural networks: Gradient descent
  finds the smoothest approximation.
\newblock {\em arXiv preprint arXiv:2005.03991}, 2020.

\bibitem{henzler2020neuraltexture}
Philipp Henzler, Niloy~J Mitra, and Tobias Ritschel.
\newblock Learning a neural 3d texture space from 2d exemplars.
\newblock {\em CVPR}, 2020.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock {N}eural {T}angent {K}ernel: {C}onvergence and generalization in
  neural networks.
\newblock {\em NeurIPS}, 2018.

\bibitem{localimplicit}
Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nie{\ss}ner,
  and Thomas Funkhouser.
\newblock Local implicit grid representations for {3D} scenes.
\newblock {\em CVPR}, 2020.

\bibitem{kazemi2019time2vec}
Seyed~Mehran Kazemi, Rishab Goel, Sepehr Eghbali, Janahan Ramanan, Jaspreet
  Sahota, Sanjay Thakur, Stella Wu, Cathal Smyth, Pascal Poupart, and Marcus
  Brubaker.
\newblock Time2vec: Learning a vector representation of time.
\newblock {\em arXiv preprint arXiv:1907.05321}, 2019.

\bibitem{adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em ICLR}, 2015.

\bibitem{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock {\em NeurIPS}, 2019.

\bibitem{atlas}
Sook-Lei Liew, Julia~M. Anglin, Nick~W. Banks, Matt Sondag, Kaori~L. Ito, Kim,
  et~al.
\newblock A large, open source dataset of stroke anatomical brain images and
  manual lesion segmentations.
\newblock {\em Scientific Data}, 2018.

\bibitem{dist}
Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc Pollefeys, and Zhaopeng
  Cui.
\newblock Dist: Rendering deep implicit signed distance function with
  differentiable sphere tracing.
\newblock {\em CVPR}, 2020.

\bibitem{implicitwithout3d}
Shichen Liu, Shunsuke Saito, Weikai Chen, and Hao Li.
\newblock Learning to infer implicit surfaces without {3D} supervision.
\newblock {\em NeurIPS}, 2019.

\bibitem{occupancynet}
Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and
  Andreas Geiger.
\newblock Occupancy networks: Learning {3D} reconstruction in function space.
\newblock {\em CVPR}, 2019.

\bibitem{trimesh}
{Michael Dawson-Haggerty et al.}
\newblock trimesh, 2019.
\newblock \url{https://trimsh.org/}.

\bibitem{implicitlayers}
Mateusz Michalkiewicz, Jhony~K Pontes, Dominic Jack, Mahsa Baktashmotlagh, and
  Anders Eriksson.
\newblock Implicit surface representations as layers in neural networks.
\newblock {\em ICCV}, 2019.

\bibitem{mildenhall2020nerf}
Ben Mildenhall, Pratul~P Srinivasan, Matthew Tancik, Jonathan~T. Barron, Ravi
  Ramamoorthi, and Ren Ng.
\newblock Ne{RF}: {R}epresenting scenes as neural radiance fields for view
  synthesis.
\newblock {\em arXiv preprint arXiv:2003.08934}, 2020.

\bibitem{nguyen2015deep}
Anh Nguyen, Jason Yosinski, and Jeff Clune.
\newblock Deep neural networks are easily fooled: High confidence predictions
  for unrecognizable images.
\newblock {\em CVPR}, 2015.

\bibitem{diffvolumetric}
Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger.
\newblock Differentiable volumetric rendering: Learning implicit {3D}
  representations without {3D} supervision.
\newblock {\em CVPR}, 2020.

\bibitem{neuraltangents2020}
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander~A. Alemi, Jascha
  Sohl-Dickstein, and Samuel~S. Schoenholz.
\newblock Neural tangents: Fast and easy infinite neural networks in {P}ython.
\newblock {\em ICLR}, 2020.

\bibitem{texturefields}
Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, and Andreas
  Geiger.
\newblock Texture fields: Learning texture representations in function space.
\newblock {\em ICCV}, 2019.

\bibitem{deepsdf}
Jeong~Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven
  Lovegrove.
\newblock Deep{SDF}: Learning continuous signed distance functions for shape
  representation.
\newblock {\em CVPR}, 2019.

\bibitem{rahaman2018spectral}
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred~A.
  Hamprecht, Yoshua Bengio, and Aaron Courville.
\newblock On the spectral bias of neural networks.
\newblock {\em ICML}, 2019.

\bibitem{rahimi2007}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock {\em NeurIPS}, 2007.

\bibitem{pifu}
Shunsuke Saito, , Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa,
  and Hao Li.
\newblock {PIFu}: Pixel-aligned implicit function for high-resolution clothed
  human digitization.
\newblock {\em ICCV}, 2019.

\bibitem{shepp}
Lawrence~A. Shepp and Benjamin~F. Logan.
\newblock The {F}ourier reconstruction of a head section.
\newblock {\em IEEE Transactions on nuclear science}, 1974.

\bibitem{srn}
Vincent Sitzmann, Michael Zollhoefer, and Gordon Wetzstein.
\newblock Scene representation networks: Continuous {3D}-structure-aware neural
  scene representations.
\newblock {\em NeurIPS}, 2019.

\bibitem{stanley2007compositional}
Kenneth~O. Stanley.
\newblock Compositional pattern producing networks: A novel abstraction of
  development.
\newblock {\em Genetic Programming and Evolvable Machines}, 2007.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em NeurIPS}, 2017.

\bibitem{wainwright_2019}
Martin~J. Wainwright.
\newblock {\em Reproducing Kernel Hilbert Spaces}, page 383â€“415.
\newblock Cambridge Series in Statistical and Probabilistic Mathematics.
  Cambridge University Press, 2019.

\bibitem{embree}
Ingo Wald, Sven Woop, Carsten Benthin, Gregory~S Johnson, and Manfred Ernst.
\newblock Embree: a kernel framework for efficient {CPU} ray tracing.
\newblock {\em ACM Transactions on Graphics (TOG)}, 2014.

\bibitem{xu2019selfattention}
Da~Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan.
\newblock Self-attention with functional time representation learning.
\newblock {\em NeurIPS}, 2019.

\bibitem{yang2019fine}
Greg Yang and Hadi Salman.
\newblock A fine-grained spectral perspective on neural networks.
\newblock {\em arXiv preprint arXiv:1907.10599}, 2019.

\bibitem{Zhong2020Reconstructing}
Ellen~D. Zhong, Tristan Bepler, Joseph~H. Davis, and Bonnie Berger.
\newblock Reconstructing continuous distributions of 3{D} protein structure
  from cryo-{EM} images.
\newblock {\em ICLR}, 2020.

\end{thebibliography}
