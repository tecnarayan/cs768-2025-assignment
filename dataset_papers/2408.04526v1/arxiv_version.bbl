\begin{thebibliography}{}

\bibitem[Abbasi-yadkori et~al., 2011]{abbas2011improved}
Abbasi-yadkori, Y., P\'{a}l, D., and Szepesv\'{a}ri, C. (2011).
\newblock Improved algorithms for linear stochastic bandits.
\newblock In Shawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., and Weinberger, K., editors, {\em Advances in Neural Information Processing Systems}, volume~24. Curran Associates, Inc.

\bibitem[Agarwal et~al., 2022]{agarwal2022voql}
Agarwal, A., Jin, Y., and Zhang, T. (2022).
\newblock Vo$q$l: Towards optimal regret in model-free rl with nonlinear function approximation.

\bibitem[Amortila et~al., 2024]{amortila2024harnessing}
Amortila, P., Foster, D.~J., Jiang, N., Sekhari, A., and Xie, T. (2024).
\newblock Harnessing density ratios for online reinforcement learning.

\bibitem[Azar et~al., 2017]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R. (2017).
\newblock Minimax regret bounds for reinforcement learning.
\newblock In {\em International conference on machine learning}, pages 263--272. PMLR.

\bibitem[Ball et~al., 2023]{ball2023efficient}
Ball, P.~J., Smith, L., Kostrikov, I., and Levine, S. (2023).
\newblock Efficient online reinforcement learning with offline data.
\newblock In {\em International Conference on Machine Learning}, pages 1577--1594. PMLR.

\bibitem[Du et~al., 2019]{du2019good}
Du, S.~S., Kakade, S.~M., Wang, R., and Yang, L.~F. (2019).
\newblock Is a good representation sufficient for sample efficient reinforcement learning?
\newblock {\em arXiv preprint arXiv:1910.03016}.

\bibitem[Duan and Wang, 2020]{duan2020minimaxoptimal}
Duan, Y. and Wang, M. (2020).
\newblock Minimax-optimal off-policy evaluation with linear function approximation.

\bibitem[Fan et~al., 2020]{fan2020theoretical}
Fan, J., Wang, Z., Xie, Y., and Yang, Z. (2020).
\newblock A theoretical analysis of deep q-learning.

\bibitem[He et~al., 2023]{he2023nearly}
He, J., Zhao, H., Zhou, D., and Gu, Q. (2023).
\newblock Nearly minimax optimal reinforcement learning for linear markov decision processes.

\bibitem[Hu et~al., 2023]{hu2023nearlynot}
Hu, P., Chen, Y., and Huang, L. (2023).
\newblock Nearly minimax optimal reinforcement learning with linear function approximation.

\bibitem[Jin et~al., 2018]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I. (2018).
\newblock Is q-learning provably efficient?
\newblock {\em Advances in neural information processing systems}, 31.

\bibitem[Jin et~al., 2021a]{jin2021bellman}
Jin, C., Liu, Q., and Miryoosefi, S. (2021a).
\newblock Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms.

\bibitem[Jin et~al., 2019]{jin2019provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I. (2019).
\newblock Provably efficient reinforcement learning with linear function approximation.

\bibitem[Jin et~al., 2021b]{jin2021pessimism}
Jin, Y., Yang, Z., and Wang, Z. (2021b).
\newblock Is pessimism provably efficient for offline rl?
\newblock In {\em International Conference on Machine Learning}, pages 5084--5096. PMLR.

\bibitem[Kausik et~al., 2024]{kausik2024leveragingofflinedatalinear}
Kausik, C., Tan, K., and Tewari, A. (2024).
\newblock Leveraging offline data in linear latent bandits.

\bibitem[Kearns and Singh, 2002]{kearns2002near}
Kearns, M. and Singh, S. (2002).
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock {\em Machine learning}, 49:209--232.

\bibitem[Lange et~al., 2012]{lange2012batch}
Lange, S., Gabel, T., and Riedmiller, M. (2012).
\newblock Batch reinforcement learning.
\newblock In {\em Reinforcement learning: State-of-the-art}, pages 45--73. Springer.

\bibitem[Lattimore et~al., 2020]{lattimore2020learning}
Lattimore, T., Szepesvari, C., and Weisz, G. (2020).
\newblock Learning with good feature representations in bandits and in rl with a generative model.

\bibitem[Levine et~al., 2020]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J. (2020).
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on open problems.

\bibitem[Li et~al., 2021]{li2021sample}
Li, G., Chen, Y., Chi, Y., Gu, Y., and Wei, Y. (2021).
\newblock Sample-efficient reinforcement learning is feasible for linearly realizable mdps with limited revisiting.
\newblock {\em Advances in Neural Information Processing Systems}, 34:16671--16685.

\bibitem[Li et~al., 2024]{li2024settling}
Li, G., Shi, L., Chen, Y., Chi, Y., and Wei, Y. (2024).
\newblock Settling the sample complexity of model-based offline reinforcement learning.
\newblock {\em The Annals of Statistics}, 52(1):233--260.

\bibitem[Li et~al., 2023a]{li2023minimaxoptimal}
Li, G., Yan, Y., Chen, Y., and Fan, J. (2023a).
\newblock Minimax-optimal reward-agnostic exploration in reinforcement learning.

\bibitem[Li et~al., 2023b]{li2023reward}
Li, G., Zhan, W., Lee, J.~D., Chi, Y., and Chen, Y. (2023b).
\newblock Reward-agnostic fine-tuning: Provable statistical benefits of hybrid reinforcement learning.
\newblock {\em arXiv preprint arXiv:2305.10282}.

\bibitem[Min et~al., 2021]{min2021variance}
Min, Y., Wang, T., Zhou, D., and Gu, Q. (2021).
\newblock Variance-aware off-policy evaluation with linear function approximation.
\newblock {\em Advances in neural information processing systems}, 34:7598--7610.

\bibitem[Munos and Szepesv{{\'a}}ri, 2008]{munos2008finite}
Munos, R. and Szepesv{{\'a}}ri, C. (2008).
\newblock Finite-time bounds for fitted value iteration.
\newblock {\em Journal of Machine Learning Research}, 9(27):815--857.

\bibitem[Nahum-Shani et~al., 2017]{NahumShani2017JustinTimeAI}
Nahum-Shani, I., Smith, S.~N., Spring, B.~J., Collins, L.~M., Witkiewitz, K.~A., Tewari, A., and Murphy, S.~A. (2017).
\newblock Just-in-time adaptive interventions (jitais) in mobile health: Key components and design principles for ongoing health behavior support.
\newblock {\em Annals of Behavioral Medicine: A Publication of the Society of Behavioral Medicine}, 52:446 -- 462.

\bibitem[Nair et~al., 2020]{nair2020awac}
Nair, A., Gupta, A., Dalal, M., and Levine, S. (2020).
\newblock Awac: Accelerating online reinforcement learning with offline datasets.
\newblock {\em arXiv preprint arXiv:2006.09359}.

\bibitem[Nakamoto et~al., 2023]{nakamoto2023calql}
Nakamoto, M., Zhai, Y., Singh, A., Mark, M.~S., Ma, Y., Finn, C., Kumar, A., and Levine, S. (2023).
\newblock Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning.

\bibitem[Qiao and Wang, 2022]{qiao2022near}
Qiao, D. and Wang, Y.-X. (2022).
\newblock Near-optimal deployment efficiency in reward-free reinforcement learning with linear function approximation.
\newblock {\em arXiv preprint arXiv:2210.00701}.

\bibitem[Song et~al., 2023]{song2023hybrid}
Song, Y., Zhou, Y., Sekhari, A., Bagnell, J.~A., Krishnamurthy, A., and Sun, W. (2023).
\newblock Hybrid rl: Using both offline and online data can make rl efficient.

\bibitem[Sutton and Barto, 2018]{sutton1998}
Sutton, R.~S. and Barto, A.~G. (2018).
\newblock {\em Reinforcement Learning: An Introduction}.
\newblock The MIT Press, second edition.

\bibitem[Tan and Xu, 2024]{tan2024natural}
Tan, K. and Xu, Z. (2024).
\newblock A natural extension to online algorithms for hybrid rl with limited coverage.

\bibitem[Vecerik et~al., 2017]{vecerik2017leveraging}
Vecerik, M., Hester, T., Scholz, J., Wang, F., Pietquin, O., Piot, B., Heess, N., Roth{\"o}rl, T., Lampe, T., and Riedmiller, M. (2017).
\newblock Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards.
\newblock {\em arXiv preprint arXiv:1707.08817}.

\bibitem[Wagenmaker et~al., 2022]{wagenmaker2022firstorder}
Wagenmaker, A., Chen, Y., Simchowitz, M., Du, S.~S., and Jamieson, K. (2022).
\newblock First-order regret in reinforcement learning with linear function approximation: A robust estimation approach.

\bibitem[Wagenmaker and Jamieson, 2023]{wagenmaker2023instancedependent}
Wagenmaker, A. and Jamieson, K. (2023).
\newblock Instance-dependent near-optimal policy identification in linear mdps via online experiment design.

\bibitem[Wagenmaker and Pacchiano, 2023]{wagenmaker2023leveraging}
Wagenmaker, A. and Pacchiano, A. (2023).
\newblock Leveraging offline data in online reinforcement learning.

\bibitem[Xie et~al., 2023]{xie2023bellmanconsistent}
Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A. (2023).
\newblock Bellman-consistent pessimism for offline reinforcement learning.

\bibitem[Xie et~al., 2022a]{xie2022role}
Xie, T., Foster, D.~J., Bai, Y., Jiang, N., and Kakade, S.~M. (2022a).
\newblock The role of coverage in online reinforcement learning.
\newblock {\em arXiv preprint arXiv:2210.04157}.

\bibitem[Xie et~al., 2022b]{xie2022policy}
Xie, T., Jiang, N., Wang, H., Xiong, C., and Bai, Y. (2022b).
\newblock Policy finetuning: Bridging sample-efficient offline and online reinforcement learning.

\bibitem[Xiong et~al., 2023]{xiong2023nearly}
Xiong, W., Zhong, H., Shi, C., Shen, C., Wang, L., and Zhang, T. (2023).
\newblock Nearly minimax optimal offline reinforcement learning with linear function approximation: Single-agent mdp and markov game.

\bibitem[Yang and Wang, 2019]{yang2019sampleoptimal}
Yang, L.~F. and Wang, M. (2019).
\newblock Sample-optimal parametric q-learning using linearly additive features.

\bibitem[Yin et~al., 2022]{yin2022nearoptimal}
Yin, M., Duan, Y., Wang, M., and Wang, Y.-X. (2022).
\newblock Near-optimal offline reinforcement learning with linear representation: Leveraging variance information with pessimism.

\bibitem[Zanette et~al., 2021]{zanette2021provable}
Zanette, A., Wainwright, M.~J., and Brunskill, E. (2021).
\newblock Provable benefits of actor-critic methods for offline reinforcement learning.

\bibitem[Zhan et~al., 2022]{zhan2022offline}
Zhan, W., Huang, B., Huang, A., Jiang, N., and Lee, J.~D. (2022).
\newblock Offline reinforcement learning with realizability and single-policy concentrability.

\bibitem[Zhang et~al., 2023]{zhang2023settling}
Zhang, Z., Chen, Y., Lee, J.~D., and Du, S.~S. (2023).
\newblock Settling the sample complexity of online reinforcement learning.
\newblock {\em arXiv preprint arXiv:2307.13586}.

\bibitem[Zhou and Gu, 2022]{zhou2022computationally}
Zhou, D. and Gu, Q. (2022).
\newblock Computationally efficient horizon-free reinforcement learning for linear mixture mdps.

\bibitem[Zhou et~al., 2021]{zhou2021nearly}
Zhou, D., Gu, Q., and Szepesvari, C. (2021).
\newblock Nearly minimax optimal reinforcement learning for linear mixture markov decision processes.

\bibitem[Zhou et~al., 2023]{zhou2023offline}
Zhou, Y., Sekhari, A., Song, Y., and Sun, W. (2023).
\newblock Offline data enhanced on-policy policy gradient with provable guarantees.

\end{thebibliography}
