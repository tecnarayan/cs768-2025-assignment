\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbeel \& Ng(2004)Abbeel and Ng]{abbeel2004apprenticeship}
Abbeel, P. and Ng, A.~Y.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In \emph{International Conference on Machine learning}, 2004.

\bibitem[Agarwal et~al.(2019)Agarwal, Jiang, Kakade, and Sun]{agarwal2019reinforcement}
Agarwal, A., Jiang, N., Kakade, S.~M., and Sun, W.
\newblock Reinforcement learning: Theory and algorithms.
\newblock \emph{CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep}, 32:\penalty0 96, 2019.

\bibitem[Bai \& Bilmes(2018)Bai and Bilmes]{bai2018greed}
Bai, W. and Bilmes, J.
\newblock Greed is still good: Maximizing monotone {S}ubmodular+{S}upermodular ({BP}) functions.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th International Conference on Machine Learning}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\  304--313. PMLR, 10--15 Jul 2018.

\bibitem[Barakat et~al.(2023)Barakat, Fatkhullin, and He]{barakat2023reinforcement}
Barakat, A., Fatkhullin, I., and He, N.
\newblock Reinforcement learning with general utilities: Simpler variance reduction and large state-action space.
\newblock \emph{arXiv preprint arXiv:2306.01854}, 2023.

\bibitem[Bellman(1966)]{bellman1966dynamic}
Bellman, R.
\newblock Dynamic programming.
\newblock \emph{Science}, 153\penalty0 (3731):\penalty0 34--37, 1966.

\bibitem[Billionnet \& Minoux(1985)Billionnet and Minoux]{billionnet1985maximizing}
Billionnet, A. and Minoux, M.
\newblock Maximizing a supermodular pseudoboolean function: A polynomial algorithm for supermodular cubic functions.
\newblock \emph{Discrete Applied Mathematics}, 12\penalty0 (1):\penalty0 1--11, 1985.

\bibitem[Bilmes(2022)]{bilmes2022submodularity}
Bilmes, J.
\newblock Submodularity in machine learning and artificial intelligence.
\newblock \emph{arXiv preprint arXiv:2202.00132}, 2022.

\bibitem[Blum et~al.(2007)Blum, Chawla, Karger, Lane, Meyerson, and Minkoff]{blum2007approximation}
Blum, A., Chawla, S., Karger, D.~R., Lane, T., Meyerson, A., and Minkoff, M.
\newblock Approximation algorithms for orienteering and discounted-reward tsp.
\newblock \emph{SIAM Journal on Computing}, 37\penalty0 (2):\penalty0 653--670, 2007.

\bibitem[Brafman et~al.(2018)Brafman, De~Giacomo, and Patrizi]{brafman2018ltlf}
Brafman, R., De~Giacomo, G., and Patrizi, F.
\newblock Ltlf/ldlf non-markovian rewards.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~32, 2018.

\bibitem[Brantley et~al.(2020)Brantley, Dudik, Lykouris, Miryoosefi, Simchowitz, Slivkins, and Sun]{brantley2020constrained}
Brantley, K., Dudik, M., Lykouris, T., Miryoosefi, S., Simchowitz, M., Slivkins, A., and Sun, W.
\newblock Constrained episodic reinforcement learning in concave-convex and knapsack settings.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Campos et~al.(2020)Campos, Trott, Xiong, Socher, Gir{\'o}-i Nieto, and Torres]{campos2020explore}
Campos, V., Trott, A., Xiong, C., Socher, R., Gir{\'o}-i Nieto, X., and Torres, J.
\newblock Explore, discover and learn: Unsupervised discovery of state-covering skills.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Chekuri \& Pal(2005)Chekuri and Pal]{chekuri_rg}
Chekuri, C. and Pal, M.
\newblock A recursive greedy algorithm for walks in directed graphs.
\newblock In \emph{46th Annual IEEE Symposium on Foundations of Computer Science (FOCS'05)}, pp.\  245--253, 2005.
\newblock \doi{10.1109/SFCS.2005.9}.

\bibitem[Chen et~al.(2017)Chen, Krause, and Karbasi]{Chen2017InteractiveSB}
Chen, L., Krause, A., and Karbasi, A.
\newblock Interactive submodular bandit.
\newblock In \emph{NIPS}, 2017.

\bibitem[Conforti \& Cornuéjols(1984)Conforti and Cornuéjols]{CONFORTI1984251}
Conforti, M. and Cornuéjols, G.
\newblock Submodular set functions, matroids and the greedy algorithm: Tight worst-case bounds and some generalizations of the rado-edmonds theorem.
\newblock \emph{Discrete Applied Mathematics}, 7\penalty0 (3):\penalty0 251--274, 1984.
\newblock ISSN 0166-218X.

\bibitem[Das \& Kempe(2011)Das and Kempe]{das2011submodular}
Das, A. and Kempe, D.
\newblock Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection.
\newblock \emph{arXiv preprint arXiv:1102.3975}, 2011.

\bibitem[De~Giacomo \& Vardi(2013)De~Giacomo and Vardi]{de2013linear}
De~Giacomo, G. and Vardi, M.~Y.
\newblock Linear temporal logic and linear dynamic logic on finite traces.
\newblock In \emph{IJCAI'13 Proceedings of the Twenty-Third international joint conference on Artificial Intelligence}, pp.\  854--860. Association for Computing Machinery, 2013.

\bibitem[Eysenbach et~al.(2018)Eysenbach, Gupta, Ibarz, and Levine]{eysenbach2018diversity}
Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S.
\newblock Diversity is all you need: Learning skills without a reward function.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Gabillon et~al.(2013)Gabillon, Kveton, Wen, Eriksson, and Muthukrishnan]{gabillon2013adaptive}
Gabillon, V., Kveton, B., Wen, Z., Eriksson, B., and Muthukrishnan, S.
\newblock Adaptive submodular maximization in bandit setting.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Gallo \& Simeone(1989)Gallo and Simeone]{gallo1989supermodular}
Gallo, G. and Simeone, B.
\newblock On the supermodular knapsack problem.
\newblock \emph{Mathematical Programming}, 45:\penalty0 295--309, 1989.

\bibitem[Gaon \& Brafman(2020)Gaon and Brafman]{gaon2020reinforcement}
Gaon, M. and Brafman, R.
\newblock Reinforcement learning with non-markovian rewards.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~34, pp.\  3980--3987, 2020.

\bibitem[Garc{\i}a \& Fern{\'a}ndez(2015)Garc{\i}a and Fern{\'a}ndez]{garcia2015comprehensive}
Garc{\i}a, J. and Fern{\'a}ndez, F.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 16\penalty0 (1):\penalty0 1437--1480, 2015.

\bibitem[Geist et~al.(2022)Geist, P{\'e}rolat, Lauri{\`e}re, Elie, Perrin, Bachem, Munos, and Pietquin]{geist2021concave}
Geist, M., P{\'e}rolat, J., Lauri{\`e}re, M., Elie, R., Perrin, S., Bachem, O., Munos, R., and Pietquin, O.
\newblock Concave utility reinforcement learning: The mean-field game viewpoint.
\newblock In \emph{International Conference on Autonomous Agents and Multiagent Systems}, 2022.

\bibitem[Ghasemipour et~al.(2020)Ghasemipour, Zemel, and Gu]{ghasemipour2020divergence}
Ghasemipour, S. K.~S., Zemel, R., and Gu, S.
\newblock A divergence minimization perspective on imitation learning methods.
\newblock In \emph{Conference on Robot Learning}, 2020.

\bibitem[Hazan et~al.(2019)Hazan, Kakade, Singh, and Van~Soest]{hazan2019maxent}
Hazan, E., Kakade, S., Singh, K., and Van~Soest, A.
\newblock Provably efficient maximum entropy exploration.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[He et~al.(2022)He, Jiang, Zhang, Shao, and Ji]{he2022wasserstein}
He, S., Jiang, Y., Zhang, H., Shao, J., and Ji, X.
\newblock Wasserstein unsupervised reinforcement learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2022.

\bibitem[Iwata(2008)]{iwata2008submodular}
Iwata, S.
\newblock Submodular function minimization.
\newblock \emph{Mathematical Programming}, 112:\penalty0 45--64, 2008.

\bibitem[Iyer \& Bilmes(2012{\natexlab{a}})Iyer and Bilmes]{iyer2012algorithms}
Iyer, R. and Bilmes, J.
\newblock Algorithms for approximate minimization of the difference between submodular functions, with applications.
\newblock \emph{arXiv preprint arXiv:1207.0560}, 2012{\natexlab{a}}.

\bibitem[Iyer \& Bilmes(2015)Iyer and Bilmes]{iyer2015polyhedral}
Iyer, R. and Bilmes, J.
\newblock Polyhedral aspects of submodularity, convexity and concavity.
\newblock \emph{arXiv preprint arXiv:1506.07329}, 2015.

\bibitem[Iyer \& Bilmes(2012{\natexlab{b}})Iyer and Bilmes]{iyer2012submodular}
Iyer, R. and Bilmes, J.~A.
\newblock Submodular-bregman and the lov{\'a}sz-bregman divergences with applications.
\newblock \emph{Advances in Neural Information Processing Systems}, 25, 2012{\natexlab{b}}.

\bibitem[Iyer et~al.(2013)Iyer, Jegelka, and Bilmes]{iyer2013fast}
Iyer, R., Jegelka, S., and Bilmes, J.
\newblock Fast semidifferential-based submodular function optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\  855--863. PMLR, 2013.

\bibitem[Ji et~al.(2019)Ji, Xu, Li, Wang, and Zhang]{ji2019stochastic}
Ji, S., Xu, D., Li, M., Wang, Y., and Zhang, D.
\newblock Stochastic greedy algorithm is still good: maximizing submodular+ supermodular functions.
\newblock In \emph{World Congress on Global Optimization}, pp.\  488--497. Springer, 2019.

\bibitem[Krause \& Golovin(2014)Krause and Golovin]{krause2014submodular}
Krause, A. and Golovin, D.
\newblock Submodular function maximization.
\newblock \emph{Tractability}, 3:\penalty0 71--104, 2014.

\bibitem[Krause \& Guestrin(2011)Krause and Guestrin]{krause2011submodularity}
Krause, A. and Guestrin, C.
\newblock Submodularity and its applications in optimized information gathering.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology (TIST)}, 2\penalty0 (4):\penalty0 1--20, 2011.

\bibitem[Lee et~al.(2019)Lee, Eysenbach, Parisotto, Xing, Levine, and Salakhutdinov]{state_marginal}
Lee, L., Eysenbach, B., Parisotto, E., Xing, E., Levine, S., and Salakhutdinov, R.
\newblock Efficient exploration via state marginal matching.
\newblock \emph{arXiv preprint arXiv:1906.05274}, 2019.

\bibitem[Liu \& Abbeel(2021)Liu and Abbeel]{liu2021behavior}
Liu, H. and Abbeel, P.
\newblock Behavior from the void: Unsupervised active pre-training.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Lov{\'a}sz(1983)]{lovasz1983submodular}
Lov{\'a}sz, L.
\newblock Submodular functions and convexity.
\newblock \emph{Mathematical Programming The State of the Art: Bonn 1982}, pp.\  235--257, 1983.

\bibitem[Mutn\'{y} et~al.(2023)Mutn\'{y}, Janik, and Krause]{Mutny2023}
Mutn\'{y}, M., Janik, T., and Krause, A.
\newblock Active exploration via experiment design in markov chains.
\newblock In \emph{Proceedings of the 26th International Conference on Artificial Intelligence and Statistics (AISTATS)}, 2023.

\bibitem[Mutny et~al.(2023)Mutny, Janik, and Krause]{mutny2023active}
Mutny, M., Janik, T., and Krause, A.
\newblock Active exploration via experiment design in markov chains.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  7349--7374. PMLR, 2023.

\bibitem[Mutti et~al.(2022{\natexlab{a}})Mutti, De~Santi, De~Bartolomeis, and Restelli]{mutti2022challenging}
Mutti, M., De~Santi, R., De~Bartolomeis, P., and Restelli, M.
\newblock Challenging common assumptions in convex reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 4489--4502, 2022{\natexlab{a}}.

\bibitem[Mutti et~al.(2022{\natexlab{b}})Mutti, De~Santi, and Restelli]{mutti2022importance}
Mutti, M., De~Santi, R., and Restelli, M.
\newblock The importance of non-markovianity in maximum state entropy exploration.
\newblock In \emph{International Conference on Machine Learning}, 2022{\natexlab{b}}.

\bibitem[Mutti et~al.(2023)Mutti, De~Santi, De~Bartolomeis, and Restelli]{mutti2023convex}
Mutti, M., De~Santi, R., De~Bartolomeis, P., and Restelli, M.
\newblock Convex reinforcement learning in finite trials.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (250):\penalty0 1--42, 2023.

\bibitem[Narasimhan \& Bilmes(2012)Narasimhan and Bilmes]{narasimhan2012submodular}
Narasimhan, M. and Bilmes, J.~A.
\newblock A submodular-supermodular procedure with applications to discriminative structure learning.
\newblock \emph{arXiv preprint arXiv:1207.1404}, 2012.

\bibitem[Prajapat et~al.(2022)Prajapat, Turchetta, Zeilinger, and Krause]{near_optimal_safe_cov}
Prajapat, M., Turchetta, M., Zeilinger, M., and Krause, A.
\newblock Near-optimal multi-agent learning for safe coverage control.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~35, pp.\  14998--15012. Curran Associates, Inc., 2022.

\bibitem[Prajapat et~al.(2023)Prajapat, Mutn{\`y}, Zeilinger, and Krause]{prajapat2023submodular}
Prajapat, M., Mutn{\`y}, M., Zeilinger, M.~N., and Krause, A.
\newblock Submodular reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2307.13372}, 2023.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Qin et~al.(2021)Qin, Chen, and Fan]{qin2021density}
Qin, Z., Chen, Y., and Fan, C.
\newblock Density constrained reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Rasmussen et~al.(2006)Rasmussen, Williams, et~al.]{rasmussen2006gaussian}
Rasmussen, C.~E., Williams, C.~K., et~al.
\newblock \emph{Gaussian processes for machine learning}, volume~1.
\newblock Springer, 2006.

\bibitem[Tarbouriech \& Lazaric(2019)Tarbouriech and Lazaric]{tarbouriech2019active}
Tarbouriech, J. and Lazaric, A.
\newblock Active exploration in markov decision processes.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence and Statistics}, pp.\  974--982. PMLR, 2019.

\bibitem[Tarbouriech et~al.(2020)Tarbouriech, Shekhar, Pirotta, Ghavamzadeh, and Lazaric]{tarbouriech2020active}
Tarbouriech, J., Shekhar, S., Pirotta, M., Ghavamzadeh, M., and Lazaric, A.
\newblock Active model estimation in markov decision processes.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, pp.\  1019--1028. PMLR, 2020.

\bibitem[Thiede et~al.(2022)Thiede, Krenn, Nigam, and Aspuru-Guzik]{thiede2022curiosity}
Thiede, L.~A., Krenn, M., Nigam, A., and Aspuru-Guzik, A.
\newblock Curiosity in exploring chemical spaces: intrinsic rewards for molecular reinforcement learning.
\newblock \emph{Machine Learning: Science and Technology}, 3\penalty0 (3):\penalty0 035008, 2022.

\bibitem[Wang et~al.(2020)Wang, Zhang, Chaplot, Garagi{\'c}, and Salakhutdinov]{wang2020planning}
Wang, R., Zhang, H., Chaplot, D.~S., Garagi{\'c}, D., and Salakhutdinov, R.
\newblock Planning with submodular objective functions.
\newblock \emph{arXiv preprint arXiv:2010.11863}, 2020.

\bibitem[Yue \& Guestrin(2011)Yue and Guestrin]{yue2011linear}
Yue, Y. and Guestrin, C.
\newblock Linear submodular bandits and their application to diversified retrieval.
\newblock \emph{Advances in Neural Information Processing Systems}, 24, 2011.

\bibitem[Zahavy et~al.(2021)Zahavy, O'Donoghue, Desjardins, and Singh]{zahavy2021reward}
Zahavy, T., O'Donoghue, B., Desjardins, G., and Singh, S.
\newblock Reward is enough for convex mdps.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 25746--25759, 2021.

\bibitem[Zhang et~al.(2020)Zhang, Koppel, Bedi, Szepesvari, and Wang]{zhang2020variational}
Zhang, J., Koppel, A., Bedi, A.~S., Szepesvari, C., and Wang, M.
\newblock Variational policy gradient method for reinforcement learning with general utilities.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 4572--4583, 2020.

\end{thebibliography}
