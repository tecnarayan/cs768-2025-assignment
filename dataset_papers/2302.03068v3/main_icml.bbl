 \newcommand*{\annalstat}{Annals of Statistics} \newcommand*{\jasa}{Journal of
  the American Statistical Association (JASA)} \newcommand*{\tacl}{Transactions
  of the Association for Computational Linguistics (TACL)}
  \newcommand*{\coling}{International Conference on Computational Linguistics
  (COLING)} \newcommand*{\acl}{Association for Computational Linguistics (ACL)}
  \newcommand*{\naacl}{North American Association for Computational Linguistics
  (NAACL)} \newcommand*{\aclijcnlp}{Association for Computational Linguistics
  and International Joint Conference on Natural Language Processing
  (ACL-IJCNLP)} \newcommand*{\emnlpconll}{Empirical Methods in Natural Language
  Processing and Computational Natural Language Learning (EMNLP/CoNLL)}
  \newcommand*{\emnlpijnlp}{Empirical Methods in Natural Language Processing
  and International Joint Conference on Natural Language Processing
  (EMNLP-IJCNLP)} \newcommand*{\emnlp}{Empirical Methods in Natural Language
  Processing} \newcommand*{\hltnaacl}{Human Language Technology and North
  American Association for Computational Linguistics (HLT/NAACL)}
  \newcommand*{\eacl}{European Association for Computational Linguistics
  (EACL)}  \newcommand*{\icml}{International Conference on Machine Learning
  (ICML)} \newcommand*{\neurips}{Advances in Neural Information Processing
  Systems (NeurIPS)} \newcommand*{\nips}{Advances in Neural Information
  Processing Systems} \newcommand*{\iclr}{International Conference on Learning
  Representations (ICLR)} \newcommand*{\iclrworkshop}{International Conference
  on Learning Representations Workshop (ICLR)} \newcommand*{\jmlr}{Journal of
  Machine Learning Research (JMLR)} \newcommand*{\fatml}{Conference on
  Fairness, Accountability, and Transparency} \newcommand*{\aistats}{Artificial
  Intelligence and Statistics (AISTATS)} \newcommand*{\cvpr}{Conference on
  Computer Vision and Pattern Recognition (CVPR)}
  \newcommand*{\iccv}{International Conference on Computer Vision (ICCV)}
  \newcommand*{\icpr}{International Conference on Pattern Recognition (ICPR)}
  \newcommand*{\eccv}{European Conference on Computer Vision (ECCV)}
  \newcommand*{\uai}{Conference on Uncertainty in Artificial Intelligence
  (UAI)}  \newcommand*{\ecai}{European Conference on Artificial Intelligence}
  \newcommand*{\aaai}{AAAI Conference on Artificial Intelligence}
  \newcommand*{\packdd}{Pacific-Asia Conference on Knowledge Discovery and Data
  Mining} \newcommand*{\kdd}{International Conference on Knowledge Discovery
  and Data Mining (KDD)} \newcommand*{\neurcom}{Neural Computation}
  \newcommand*{\msml}{Mathematical and Scientific Machine Learning Conference
  (MSML)} \newcommand*{\ijcnn}{International Joint Conference on Neural
  Networks (IJCNN)} \newcommand*{\ieeesigproc}{IEEE Transactions on Signal
  Processing} \newcommand*{\ieeeec}{IEEE Transactions on Electronic Computers}
  \newcommand*{\procieee}{Proceedings of the IEEE}
  \newcommand*{\pnas}{Proceedings of the National Academy of Sciences}
  \newcommand*{\chiconf}{Conference on Human Factors in Computing Systems
  (CHI)} \newcommand*{\ieeecp}{IEEE Symposium on Security and Privacy (SP)}
  \newcommand*{\stoc}{Symposium on Theory of Computing (STOC)}
  \newcommand*{\pods}{Symposium on Principles of Database Systems (PODS)}
  \newcommand*{\colt}{Conference on Learning Theory (COLT)}
  \newcommand*{\www}{The World Wide Web Conference (WWW)}
  \newcommand*{\soda}{Symposium on Discrete Algorithms (SODA)}
  \newcommand*{\focs}{Symposium on Foundations of Computer Science (FOCS)}
  \newcommand*{\acm}{Communications of the Association for Computing Machinery
  (ACM)} \newcommand*{\ieeeaccess}{IEEE Access}
  \newcommand*{\ijcv}{International Journal of Computer Vision (IJCV)}
  \newcommand*{\ieeetpami}{IEEE Transactions on Pattern Analysis and Machine
  Intelligence} \newcommand*{\ieeetit}{IEEE Transactions on Information Theory}
  \newcommand*{\alt}{Conference on Algorithmic Learning Theory (ALT)}
  \newcommand*{\cocoon}{International Computing and Combinatorics Conference
  (COCOON)} \newcommand*{\arxiv}[1]{arXiv preprint arXiv:#1}
\begin{thebibliography}{94}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Appalaraju et~al.(2020)Appalaraju, Zhu, Xie, and
  Feh{\'e}rv{\'a}ri]{appalaraju_towards_2020}
Appalaraju, S., Zhu, Y., Xie, Y., and Feh{\'e}rv{\'a}ri, I.
\newblock Towards good practices in self-supervised representation learning.
\newblock \emph{\arxiv{2012.00868}}, 2020.

\bibitem[Asano et~al.(2020)Asano, Rupprecht, and
  Vedaldi]{asano_self-labelling_2020}
Asano, Y.~M., Rupprecht, C., and Vedaldi, A.
\newblock Self-labelling via simultaneous clustering and representation
  learning.
\newblock In \emph{\iclr}, 2020.

\bibitem[Assran et~al.(2022)Assran, Caron, Misra, Bojanowski, Bordes, Vincent,
  Joulin, R., and Ballas]{assran_masked_2022}
Assran, M., Caron, M., Misra, I., Bojanowski, P., Bordes, F., Vincent, P.,
  Joulin, A., R., M., and Ballas, N.
\newblock Masked siamese networks for label-efficient learning.
\newblock In \emph{\eccv}, 2022.

\bibitem[Bachman et~al.(2019)Bachman, Hjelm, and
  Buchwalter]{bachman_learning_2019}
Bachman, P., Hjelm, R.~D., and Buchwalter, W.
\newblock Learning representations by maximizing mutual information across
  views.
\newblock In \emph{\neurips}, 2019.

\bibitem[Bansal et~al.(2021)Bansal, Kaplun, and Barak]{bansal_for_2021}
Bansal, Y., Kaplun, G., and Barak, B.
\newblock For self-supervised learning, rationality implies generalization,
  provably.
\newblock In \emph{\iclr}, 2021.

\bibitem[Bao et~al.(2022)Bao, Dong, Piao, and Wei]{bao_beit_2022}
Bao, H., Dong, L., Piao, S., and Wei, F.
\newblock Beit: {BERT} pre-training of image transformers.
\newblock In \emph{\iclr}, 2022.

\bibitem[Bardes et~al.(2022{\natexlab{a}})Bardes, Ponce, and
  LeCun]{bardes_vicreg_2022}
Bardes, A., Ponce, J., and LeCun, Y.
\newblock {VICR}eg: Variance-invariance-covariance regularization for
  self-supervised learning.
\newblock In \emph{\iclr}, 2022{\natexlab{a}}.

\bibitem[Bardes et~al.(2022{\natexlab{b}})Bardes, Ponce, and
  LeCun]{bardes_vicregl_2022}
Bardes, A., Ponce, J., and LeCun, Y.
\newblock {VICR}egl: Self-supervised learning of local visual features.
\newblock In \emph{\neurips}, 2022{\natexlab{b}}.

\bibitem[Barron(1994)]{barron_approximation_1994}
Barron, A.~R.
\newblock Approximation and estimation bounds for artificial neural networks.
\newblock \emph{Machine Learning}, 14:\penalty0 115--133, 1994.

\bibitem[Belkin et~al.(2019)Belkin, Hsu, Ma, and
  Mandal]{belkin_reconciling_2019}
Belkin, M., Hsu, D., Ma, S., and Mandal, S.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock \emph{\pnas}, 116:\penalty0 15849--15854, 2019.

\bibitem[Bergstra et~al.(2011)Bergstra, Bardenet, Bengio, and
  K{\'e}gl]{bergstra2011algorithms}
Bergstra, J., Bardenet, R., Bengio, Y., and K{\'e}gl, B.
\newblock Algorithms for hyper-parameter optimization.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Bottou \& Bousquet(2007)Bottou and Bousquet]{bottou_tradeoffs_2007}
Bottou, L. and Bousquet, O.
\newblock The tradeoffs of large scale learning.
\newblock In \emph{\neurips}, 2007.

\bibitem[Bousquet et~al.(2022)Bousquet, Daniely, Kaplan, Mansour, Moran, and
  Stemmer]{bousquet_monotone_2022}
Bousquet, O.~J., Daniely, A., Kaplan, H., Mansour, Y., Moran, S., and Stemmer,
  U.
\newblock Monotone learning.
\newblock In \emph{\colt}, 2022.

\bibitem[Caron et~al.(2018)Caron, Bojanowski, Joulin, and
  Douze]{caron_deep_2018}
Caron, M., Bojanowski, P., Joulin, A., and Douze, M.
\newblock Deep clustering for unsupervised learning of visual features.
\newblock In \emph{\eccv}, 2018.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{caron_unsupervised_2020}
Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock In \emph{\neurips}, 2020.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J{'e}gou, Mairal, Bojanowski,
  and Joulin]{caron_emerging_2021}
Caron, M., Touvron, H., Misra, I., J{'e}gou, H., Mairal, J., Bojanowski, P.,
  and Joulin, A.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In \emph{\iccv}, 2021.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Gan, Li, Guo, Chen, Gao, Chung,
  Xu, Zeng, Lu, Li, Carin, and Tao]{chen_simpler_2021}
Chen, J., Gan, Z., Li, X., Guo, Q., Chen, L., Gao, S., Chung, T., Xu, Y., Zeng,
  B., Lu, W., Li, F., Carin, L., and Tao, C.
\newblock Simpler, faster, stronger: Breaking the log-k curse on contrastive
  learners with flatnce.
\newblock \emph{\arxiv{2107.01152}}, 2021{\natexlab{a}}.

\bibitem[Chen \& Guestrin(2016)Chen and Guestrin]{chen_xgboost_2016}
Chen, T. and Guestrin, C.
\newblock Xgboost: A scalable tree boosting system.
\newblock In \emph{SIGKDD}, pp.\  785--794, 2016.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Kornblith, Norouzi, and
  Hinton]{chen_simple_2020}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{\icml}, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Kornblith, Swersky, Norouzi, and
  Hinton]{chen_big_2020}
Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G.
\newblock Big self-supervised models are strong semi-supervised learners.
\newblock In \emph{\neurips}, 2020{\natexlab{b}}.

\bibitem[Chen et~al.(2020{\natexlab{c}})Chen, Fan, Girshick, and
  He]{chen_improved_2020}
Chen, X., Fan, H., Girshick, R., and He, K.
\newblock Improved baselines with momentum contrastive learning.
\newblock \emph{\arxiv{2003.04297}}, 2020{\natexlab{c}}.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Xie, and He]{chen_empirical_2021}
Chen, X., Xie, S., and He, K.
\newblock An empirical study of training self-supervised vision transformers.
\newblock In \emph{\iccv}, 2021{\natexlab{b}}.

\bibitem[Cherti et~al.(2022)Cherti, Beaumont, Wightman, Wortsman, Ilharco,
  Gordon, Schuhmann, Schmidt, and Jitsev]{cherti_reproducible_2022}
Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C.,
  Schuhmann, C., Schmidt, L., and Jitsev, J.
\newblock Reproducible scaling laws for contrastive language-image learning.
\newblock \emph{\arxiv{2212.07143}}, 2022.

\bibitem[Dar et~al.(2021)Dar, Muthukumar, and Baraniuk]{dar_farewell_2021}
Dar, Y., Muthukumar, V., and Baraniuk, R.~G.
\newblock A farewell to the bias-variance tradeoff? an overview of the theory
  of overparameterized machine learning.
\newblock \emph{\arxiv{2109.02355}}, 2021.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin_bert_2019}
Devlin, J., Chang, M., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{\naacl}, pp.\  4171--4186, Minneapolis, Minnesota, June
  2019. Association for Computational Linguistics.

\bibitem[Doersch et~al.(2015)Doersch, Gupta, and
  Efros]{doersch_unsupervised_2015}
Doersch, C., Gupta, A., and Efros, A.
\newblock Unsupervised visual representation learning by context prediction.
\newblock In \emph{\iccv}, 2015.

\bibitem[Domingos(2000)]{pedro_unified_2000}
Domingos, P.
\newblock A unified bias-variance decomposition and its applications.
\newblock In \emph{\icml}, 2000.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy_image_2021}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{\iclr}, 2021.

\bibitem[Dubois et~al.(2020)Dubois, Kiela, Schwab, and
  Vedantam]{dubois_learning_2020}
Dubois, Y., Kiela, D., Schwab, D.~J., and Vedantam, R.
\newblock Learning optimal representations with the decodable information
  bottleneck.
\newblock In \emph{\neurips}, 2020.

\bibitem[Dubois et~al.(2021)Dubois, Bloem-Reddy, Ullrich, and
  Maddison]{dubois_lossy_2021}
Dubois, Y., Bloem-Reddy, B., Ullrich, K., and Maddison, C.~J.
\newblock Lossy compression for lossless prediction.
\newblock \emph{\neurips}, 2021.

\bibitem[Dubois et~al.(2022)Dubois, Hashimoto, Ermon, and
  Liang]{dubois_improving_2022}
Dubois, Y., Hashimoto, T., Ermon, S., and Liang, P.
\newblock Improving self-supervised learning by characterizing idealized
  representations.
\newblock In \emph{\neurips}, 2022.

\bibitem[Ericsson et~al.(2021)Ericsson, Gouk, and
  Hospedales]{ericsson_why_2021}
Ericsson, L., Gouk, H., and Hospedales, T.~M.
\newblock Why do self-supervised models transfer? investigating the impact of
  invariance on downstream tasks.
\newblock \emph{\arxiv{abs/2111.11398}}, 2021.

\bibitem[Federici et~al.(2020)Federici, Dutta, Forr{\'{e}}, Kushman, and
  Akata]{federici_learning_2020}
Federici, M., Dutta, A., Forr{\'{e}}, P., Kushman, N., and Akata, Z.
\newblock Learning robust representations via multi-view information
  bottleneck.
\newblock In \emph{\iclr}, 2020.

\bibitem[Foster et~al.(2021)Foster, Pukdee, and
  Rainforth]{foster_improving_2021}
Foster, A., Pukdee, R., and Rainforth, T.
\newblock Improving transformation invariance in contrastive representation
  learning.
\newblock In \emph{\iclr}, 2021.

\bibitem[Geman et~al.(1992)Geman, Bienenstock, and Doursat]{geman_neural_1992}
Geman, S., Bienenstock, E., and Doursat, R.
\newblock Neural networks and the bias/variance dilemma.
\newblock \emph{Neural computation}, 4\penalty0 (1):\penalty0 1--58, 1992.

\bibitem[Gidaris et~al.(2018)Gidaris, Singh, and
  Komodakis]{gidaris_unsupervised_2018}
Gidaris, S., Singh, P., and Komodakis, N.
\newblock Unsupervised visual representation learning by context prediction.
\newblock In \emph{\iclr}, 2018.

\bibitem[Goyal et~al.(2019)Goyal, Mahajan, Gupta, and
  Misra]{goyal_scaling_2019}
Goyal, P., Mahajan, D., Gupta, A., and Misra, I.
\newblock Scaling and benchmarking self-supervised visual representation
  learning.
\newblock In \emph{\iccv}, 2019.

\bibitem[Goyal et~al.(2021)Goyal, Duval, Reizenstein, Leavitt, Xu, Lefaudeux,
  Singh, Reis, Caron, Bojanowski, Joulin, and Misra]{goyal_vissl_2021}
Goyal, P., Duval, Q., Reizenstein, J., Leavitt, M., Xu, M., Lefaudeux, B.,
  Singh, M., Reis, V., Caron, M., Bojanowski, P., Joulin, A., and Misra, I.
\newblock {VISSL}, 2021.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'{e}}, Tallec, Richemond,
  Buchatskaya, Doersch, Pires, Guo, Azar, Piot, Kavukcuoglu, Munos, and
  Valko]{grill_bootstrap_2020}
Grill, J.~B., Strub, F., Altch{\'{e}}, F., Tallec, C., Richemond, P.~H.,
  Buchatskaya, E., Doersch, C., Pires, B.~A., Guo, Z., Azar, M.~G., Piot, B.,
  Kavukcuoglu, K., Munos, R., and Valko, M.
\newblock Bootstrap {Your} {Own} {Latent} - a new approach to self-supervised
  learning.
\newblock In \emph{\neurips}, 2020.

\bibitem[Gupta et~al.(2022)Gupta, Ajanthan, Hengel, and
  Gould]{gupta_understanding_2022}
Gupta, K., Ajanthan, T., Hengel, A. v.~d., and Gould, S.
\newblock Understanding and improving the role of projection head in
  self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2212.11491}, 2022.

\bibitem[HaoChen et~al.(2021)HaoChen, Wei, Gaidon, and
  Ma]{haochen_provable_2021}
HaoChen, J.~Z., Wei, C., Gaidon, A., and Ma, T.
\newblock Provable guarantees for self-supervised deep learning with spectral
  contrastive loss.
\newblock In \emph{\neurips}, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he_deep_2016}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{\cvpr}, 2016.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he_momentum_2020}
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{\cvpr}, 2020.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'{a}}r, and
  Girshick]{he_masked_2022}
He, K., Chen, X., Xie, S., Li, Y., Doll{\'{a}}r, P., and Girshick, R.~B.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{\cvpr}, 2022.

\bibitem[Hua et~al.(2021)Hua, Wang, Xue, Ren, Wang, and Zhao]{hua_on_2021}
Hua, T., Wang, W., Xue, Z., Ren, S., Wang, Y., and Zhao, H.
\newblock On feature decorrelation in self-supervised learning.
\newblock In \emph{\iccv}, 2021.

\bibitem[Jing et~al.(2022)Jing, Vincent, LeCun, and
  Tian]{jing_understanding_2022}
Jing, L., Vincent, P., LeCun, Y., and Tian, Y.
\newblock Understanding dimensional collapse in contrastive self-supervised
  learning.
\newblock In \emph{\iclr}, 2022.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan_scaling_2020}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T., Chess, B., Child, R.,
  Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kohavi \& Wolpert(1996)Kohavi and Wolpert]{kohavi_bias_1996}
Kohavi, R. and Wolpert, D.~H.
\newblock Bias plus variance decomposition for zero-one loss functions.
\newblock In \emph{\icml}, 1996.

\bibitem[Lundberg \& Lee(2017)Lundberg and Lee]{lundberg_unified_2017}
Lundberg, S.~M. and Lee, S.
\newblock A unified approach to interpreting model predictions.
\newblock In \emph{\neurips}, 2017.

\bibitem[Miao et~al.(2022)Miao, Mathieu, Dubois, Rainforth, Teh, Foster, and
  Kim]{miao_learning_2022}
Miao, N., Mathieu, E., Dubois, Y., Rainforth, T., Teh, Y.~W., Foster, A., and
  Kim, H.
\newblock Instance-specific augmentation: Capturing local invariances.
\newblock \emph{\arxiv{2206.00051}}, 2022.

\bibitem[Misra \& van~der Maaten(2020)Misra and van~der
  Maaten]{misra_self-supervised_2020}
Misra, I. and van~der Maaten, L.
\newblock Self-supervised learning of pretext-invariant representations.
\newblock In \emph{\cvpr}, 2020.

\bibitem[Mitrovic et~al.(2021)Mitrovic, McWilliams, Walker, Buesing, and
  Blundell]{mitrovic_representation_2021}
Mitrovic, J., McWilliams, B., Walker, J., Buesing, L., and Blundell, C.
\newblock Representation learning via invariant causal mechanisms.
\newblock In \emph{\iclr}, 2021.

\bibitem[MMSelfSup(2021)]{mmselfsup2021}
MMSelfSup.
\newblock {MMSelfSup}: Openmmlab self-supervised learning toolbox and
  benchmark.
\newblock \url{https://github.com/open-mmlab/mmselfsup}, 2021.

\bibitem[Mukherjee et~al.(2006)Mukherjee, Niyogi, Poggio, and
  Rifkin]{mukherjee_learning_2006}
Mukherjee, S., Niyogi, P., Poggio, T., and Rifkin, R.
\newblock Learning theory: stability is sufficient for generalization and
  necessary and sufficient for consistency of empirical risk minimization.
\newblock \emph{Advances in Computational Mathematics}, 25:\penalty0 161--193,
  2006.

\bibitem[Nakkiran et~al.(2020)Nakkiran, Kaplun, Bansal, Yang, Barak, and
  Sutskever]{nakkiran_deep_2020}
Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., and Sutskever, I.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock In \emph{\iclr}, 2020.

\bibitem[Neal(2019)]{neal_on_2019}
Neal, B.
\newblock On the bias-variance tradeoff: Textbooks need an update.
\newblock \emph{\arxiv{1912.08286}}, 2019.

\bibitem[Neal et~al.(2018)Neal, Mittal, Baratin, Tantia, Scicluna,
  Lacoste{-}Julien, and Mitliagkas]{neal_modern_2018}
Neal, B., Mittal, S., Baratin, A., Tantia, V., Scicluna, M., Lacoste{-}Julien,
  S., and Mitliagkas, I.
\newblock A modern take on the bias-variance tradeoff in neural networks.
\newblock \emph{\arxiv{1810.08591}}, 2018.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and
  Srebro]{neyshabur_in_2015}
Neyshabur, B., Tomioka, R., and Srebro, N.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock In \emph{iclrworkshop}, 2015.

\bibitem[Noroozi \& Favaro(2016)Noroozi and Favaro]{noroozi_unsupervised_2016}
Noroozi, M. and Favaro, P.
\newblock Unsupervised learning of visual representations by solving jigsaw
  puzzles.
\newblock In \emph{\eccv}, 2016.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{paszke_pytorch_2019}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{\neurips}, 2019.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,
  Cournapeau, Brucher, Perrot, and Duchesnay]{pedregosa_scikit-learn_2011}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,
  O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J.,
  Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 12, 2011.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, Krueger, and
  Sutskever]{radford_learning_2021}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{\icml}, 2021.

\bibitem[Rosenfeld et~al.(2020)Rosenfeld, Rosenfeld, and
  Belinkov]{rosenfeld_constructive_2020}
Rosenfeld, J., Rosenfeld, A., and Belinkov, Y.
\newblock A constructive prediction of the generalization error across scales.
\newblock In \emph{\iclr}, 2020.

\bibitem[Rosenfeld(2021)]{rosenfeld_scaling_2021}
Rosenfeld, J.~S.
\newblock \emph{Scaling laws for deep learning}.
\newblock PhD thesis, Massachusetts Institute of Technology, 2021.

\bibitem[Ruan et~al.(2022)Ruan, Dubois, and Maddison]{ruan_optimal_2022}
Ruan, Y., Dubois, Y., and Maddison, C.~J.
\newblock Optimal representations for covariate shift.
\newblock In \emph{\iclr}, 2022.

\bibitem[Santurkar et~al.(2022)Santurkar, Dubois, Taori, Liang, and
  Hashimoto]{santurkar2022caption}
Santurkar, S., Dubois, Y., Taori, R., Liang, P., and Hashimoto, T.
\newblock Is a caption worth a thousand images? a controlled study for
  representation learning.
\newblock \emph{arXiv preprint arXiv:2207.07635}, 2022.

\bibitem[Saunshi et~al.(2019)Saunshi, Plevrakis, Arora, Khodak, and
  Khandeparkar]{saunshi_theoretical_2019}
Saunshi, N., Plevrakis, O., Arora, S., Khodak, M., and Khandeparkar, H.
\newblock A theoretical analysis of contrastive unsupervised representation
  learning.
\newblock In \emph{\icml}, 2019.

\bibitem[Saunshi et~al.(2022)Saunshi, Ash, Goel, Misra, Zhang, Arora, Kakade,
  and Krishnamurthy]{saunshi_understanding_2022}
Saunshi, N., Ash, J.~T., Goel, S., Misra, D., Zhang, C., Arora, S., Kakade,
  S.~M., and Krishnamurthy, A.
\newblock Understanding contrastive learning requires incorporating inductive
  biases.
\newblock In \emph{\icml}, 2022.

\bibitem[Shalev-Shwartz \& Ben-David(2014)Shalev-Shwartz and
  Ben-David]{shalevshwartz_understanding_2014}
Shalev-Shwartz, S. and Ben-David, S.
\newblock \emph{Understanding Machine Learning: From Theory to Algorithms}.
\newblock Cambridge University Press, 2014.

\bibitem[Tian et~al.(2020{\natexlab{a}})Tian, Krishnan, and
  Isola]{tian_contrastive_2020}
Tian, Y., Krishnan, D., and Isola, P.
\newblock Contrastive multiview coding.
\newblock In \emph{\eccv}, 2020{\natexlab{a}}.

\bibitem[Tian et~al.(2020{\natexlab{b}})Tian, Sun, Poole, Krishnan, Schmid, and
  Isola]{tian_what_2020}
Tian, Y., Sun, C., Poole, B., Krishnan, D., Schmid, C., and Isola, P.
\newblock What makes for good views for contrastive learning?
\newblock In \emph{\neurips}, 2020{\natexlab{b}}.

\bibitem[Tosh et~al.(2021)Tosh, Krishnamurthy, and Hsu]{tosh_contrastive_2021}
Tosh, C., Krishnamurthy, A., and Hsu, D.
\newblock Contrastive learning, multi-view redundancy, and linear models.
\newblock In \emph{\alt}, 2021.

\bibitem[Tsai et~al.(2021)Tsai, Wu, Salakhutdinov, and
  Morency]{tsai_self-supervised_2021}
Tsai, Y.~H., Wu, Y., Salakhutdinov, R.~R., and Morency, L.
\newblock Self-supervised learning from a multi-view perspective.
\newblock In \emph{\iclr}, 2021.

\bibitem[Valentini \& Dietterich(2004)Valentini and
  Dietterich]{valentini_bias_2004}
Valentini, G. and Dietterich, T.~G.
\newblock Bias-variance analysis of support vector machines for the development
  of svm-based ensemble methods.
\newblock \emph{\jmlr}, 5:\penalty0 725--775, 2004.

\bibitem[van~den Oord et~al.(2019)van~den Oord, Li, and
  Vinyals]{oord_representation_2019}
van~den Oord, A., Li, Y., and Vinyals, O.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{\arxiv{2110.02796}}, 2019.

\bibitem[Vapnik(2000)]{vapnik_nature_2000}
Vapnik, V.~N.
\newblock \emph{The Nature of Statistical Learning Theory}.
\newblock Springer-Verlag, 2000.

\bibitem[Vapnik \& Chervonenkis(1971)Vapnik and Chervonenkis]{vapnik_on_1971}
Vapnik, V.~N. and Chervonenkis, A.~Y.
\newblock On uniform convergence of the frequencies of events to their
  probabilities.
\newblock \emph{Teoriya Veroyatnostei i ee Primeneniya}, 16\penalty0
  (2):\penalty0 264--279, 1971.

\bibitem[Viering et~al.(2019)Viering, Mey, and Loog]{viering_open_2017}
Viering, T., Mey, A., and Loog, M.
\newblock Open problem: Monotonicity of learning.
\newblock In \emph{\colt}, 2019.

\bibitem[Wang \& Liu(2021)Wang and Liu]{wang_understanding_2021}
Wang, F. and Liu, H.
\newblock Understanding the behaviour of contrastive loss.
\newblock In \emph{\cvpr}, 2021.

\bibitem[Wang \& Isola(2020)Wang and Isola]{wang_understanding_2020}
Wang, T. and Isola, P.
\newblock Understanding contrastive representation learning through alignment
  and uniformity on the hypersphere.
\newblock In \emph{\icml}, 2020.

\bibitem[Wang et~al.(2021)Wang, Zhang, Shen, Kong, and Li]{wang_dense_2021}
Wang, X., Zhang, R., Shen, C., Kong, T., and Li, L.
\newblock Dense contrastive learning for self-supervised visual pre-training.
\newblock In \emph{\cvpr}, 2021.

\bibitem[Wang et~al.(2022)Wang, Zhang, Wang, Yang, and Lin]{wang_chaos_2022}
Wang, Y., Zhang, Y., Wang, Y., Yang, J., and Lin, Z.
\newblock Chaos is a ladder: A new theoretical understanding of contrastive
  learning via augmentation overlap.
\newblock In \emph{\iclr}, 2022.

\bibitem[Wu et~al.(2021)Wu, Zhuang, Mosse, Yamins, and Goodman]{wu_on_2021}
Wu, M., Zhuang, C., Mosse, M., Yamins, D. L.~K., and Goodman, N.~D.
\newblock On mutual information in contrastive learning for visual
  representations.
\newblock \emph{\arxiv{2005.13149}}, 2021.

\bibitem[Wu et~al.(2020)Wu, Guo, Chen, Liang, Jha, and
  Chalasani]{wu_representation_2020}
Wu, X., Guo, Y., Chen, J., Liang, Y., Jha, S., and Chalasani, P.
\newblock Representation bayesian risk decompositions and multi-source domain
  adaptation.
\newblock \emph{\arxiv{2004.10390}}, 2020.

\bibitem[Wu et~al.(2018)Wu, Xiong, Yu, and Lin]{wu_unsupervised_2018}
Wu, Z., Xiong, Y., Yu, S.~X., and Lin, D.
\newblock Unsupervised feature learning via non-parametric instance
  discrimination.
\newblock In \emph{\cvpr}, 2018.

\bibitem[Xu et~al.(2020)Xu, Zhao, Song, Stewart, and Ermon]{xu_theory_2020}
Xu, Y., Zhao, S., Song, J., Stewart, R., and Ermon, S.
\newblock A theory of usable information under computational constraints.
\newblock In \emph{\iclr}, 2020.

\bibitem[Yan et~al.(2020)Yan, Misra, Gupta, Ghadiyaram, and
  Mahajan]{yan_clusterfit_2020}
Yan, X., Misra, I., Gupta, A., Ghadiyaram, D., and Mahajan, D.
\newblock {ClusterFit}: Improving generalization of visual representations.
\newblock In \emph{\cvpr}, 2020.

\bibitem[Yang et~al.(2020)Yang, Yu, You, Steinhardt, and
  Ma]{yang_rethinking_2020}
Yang, Z., Yu, Y., You, C., Steinhardt, J., and Ma, Y.
\newblock Rethinking bias-variance trade-off for generalization of neural
  networks.
\newblock In \emph{\icml}, 2020.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and
  Deny]{zbontar_barlow_2021}
Zbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S.
\newblock Barlow {Twins}: Self-supervised learning via redundancy reduction.
\newblock In \emph{\icml}, 2021.

\bibitem[Zhan et~al.(2020)Zhan, Xie, Liu, Ong, and Loy]{zhan_online_2020}
Zhan, X., Xie, J., Liu, Z., Ong, Y.-S., and Loy, C.~C.
\newblock Online deep clustering for unsupervised representation learning.
\newblock In \emph{\cvpr}, 2020.

\bibitem[Zhiliang et~al.(2022)Zhiliang, Li, Bao, Ye, and
  Wei]{zhiliang_beit_2022}
Zhiliang, P., Li, D., Bao, H., Ye, Q., and Wei, F.
\newblock Beit v2: Masked image modeling with vector-quantized visual
  tokenizers.
\newblock \emph{\arxiv{2208.06366}}, 2022.

\bibitem[Zhou et~al.(2021)Zhou, Wei, Wang, Shen, Xie, Yuille, and
  Kong]{zhou_ibot_2021}
Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A.~L., and Kong, T.
\newblock ibot: Image {BERT} pre-training with online tokenizer.
\newblock \emph{\arxiv{2111.07832}}, 2021.

\bibitem[Zhou et~al.(2022{\natexlab{a}})Zhou, Zhou, Si, Yu, Ng, and
  Yan]{zhou_mugs_2022}
Zhou, P., Zhou, Y., Si, C., Yu, W., Ng, T.~K., and Yan, S.
\newblock Mugs: {A} multi-granular self-supervised learning framework.
\newblock \emph{\arxiv{2203.14415}}, 2022{\natexlab{a}}.

\bibitem[Zhou et~al.(2022{\natexlab{b}})Zhou, Wu, Wang, and
  He]{zhou_adversarial_2022}
Zhou, Y., Wu, J., Wang, H., and He, J.
\newblock Adversarial robustness through bias variance decomposition: A new
  perspective for federated learning.
\newblock In \emph{Conference on Information and Knowledge Management (CIKM)},
  2022{\natexlab{b}}.

\end{thebibliography}
