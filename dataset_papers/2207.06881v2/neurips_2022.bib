@inproceedings{adam,
  author={Diederik P. Kingma and Jimmy Ba},
  title={Adam: A Method for Stochastic Optimization},
  year={2015},
  cdate={1420070400000},
  url={http://arxiv.org/abs/1412.6980},
  booktitle={ICLR (Poster)}
}

@inproceedings{NIPS2016_9f44e956,
 author = {Ba, Jimmy and Hinton, Geoffrey E and Mnih, Volodymyr and Leibo, Joel Z and Ionescu, Catalin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Using Fast Weights to Attend to the Recent Past},
 url = {https://proceedings.neurips.cc/paper/2016/file/9f44e956e3a2b7b5598c625fcc802c36-Paper.pdf},
 volume = {29},
 year = {2016}
}



@inproceedings{lester-etal-2021-power,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.243",
    doi = "10.18653/v1/2021.emnlp-main.243",
    pages = "3045--3059"
}

@inproceedings{li-liang-2021-prefix,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.353",
    doi = "10.18653/v1/2021.acl-long.353",
    pages = "4582--4597"
}

@article{ju2021staircase,
  title={Staircase Attention for Recurrent Processing of Sequences},
  author={Ju, Da and Roller, Stephen and Sukhbaatar, Sainbayar and Weston, Jason},
  journal={arXiv preprint arXiv:2106.04279},
  year={2021}
}

@INPROCEEDINGS{speech-transformer,
  author={Dong, Linhao and Xu, Shuang and Xu, Bo},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition}, 
  year={2018},
  volume={},
  number={},
  pages={5884-5888},
  doi={10.1109/ICASSP.2018.8462506}}


@InProceedings{dalle,
  title = 	 {Zero-Shot Text-to-Image Generation},
  author =       {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8821--8831},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/ramesh21a.html},
}



@inproceedings{
dosovitskiy2021vit,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@article{fan2020feedback-transformer,
  title={Addressing some limitations of transformers with feedback memory},
  author={Fan, Angela and Lavril, Thibaut and Grave, Edouard and Joulin, Armand and Sukhbaatar, Sainbayar},
  journal={arXiv preprint arXiv:2002.09402},
  year={2020}
}

@inproceedings{dehghani2018universal-transformer,
title={Universal Transformers},
author={Mostafa Dehghani and Stephan Gouws and Oriol Vinyals and Jakob Uszkoreit and Lukasz Kaiser},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HyzdRiR9Y7},
}

@article{mcculloch1943logical,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={The bulletin of mathematical biophysics},
  volume={5},
  number={4},
  pages={115--133},
  year={1943},
  publisher={Springer}
}

@article{stephen1956kleene,
  title={Kleene. Representation of events in nerve nets and finite automata},
  author={Stephen, C},
  journal={Automata studies},
  year={1956},
  publisher={Princeton University Press}
}

@article{werbos1990backpropagation,
  title={Backpropagation through time: what it does and how to do it},
  author={Werbos, Paul J},
  journal={Proceedings of the IEEE},
  volume={78},
  number={10},
  pages={1550--1560},
  year={1990},
  publisher={IEEE}
}


@article{jaegle2021perceiverio,
  title={Perceiver IO: A general architecture for structured inputs \& outputs},
  author={Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and others},
  journal={arXiv preprint arXiv:2107.14795},
  year={2021}
}

@inproceedings{schlag2021fastweights,
  title={Linear transformers are secretly fast weight programmers},
  author={Schlag, Imanol and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  booktitle={International Conference on Machine Learning},
  pages={9355--9366},
  year={2021},
  organization={PMLR}
}

@misc{enwik8,
  author = {Mahoney, Matt},
  title = {Large text compression benchmark},
  year = 2006,
  url = "http://www.mattmahoney.net/dc/text.html",
}

@inproceedings{ding-etal-2021-ernie-doc,
    title = "{ERNIE}-{D}oc: A Retrospective Long-Document Modeling Transformer",
    author = "Ding, SiYu  and
      Shang, Junyuan  and
      Wang, Shuohuan  and
      Sun, Yu  and
      Tian, Hao  and
      Wu, Hua  and
      Wang, Haifeng",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.227",
    doi = "10.18653/v1/2021.acl-long.227",
    pages = "2914--2927",
}

@inproceedings{lei-etal-2020-mart,
    title = "{MART}: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning",
    author = "Lei, Jie  and
      Wang, Liwei  and
      Shen, Yelong  and
      Yu, Dong  and
      Berg, Tamara  and
      Bansal, Mohit",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.233",
    doi = "10.18653/v1/2020.acl-main.233",
    pages = "2603--2614",
}

@article{wu2020memformer,
  title={Memformer: The Memory-Augmented Transformer},
  author={Wu, Qingyang and Lan, Zhenzhong and Gu, Jing and Yu, Zhou},
  journal={arXiv preprint arXiv:2010.06891},
  year={2020}
}

@inproceedings{cho2014gru,
    title = "On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Bahdanau, Dzmitry  and
      Bengio, Yoshua},
    booktitle = "Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-4012",
    doi = "10.3115/v1/W14-4012",
    pages = "103--111",
}

@article{gupta2020gmat,
  title={GMAT: Global memory augmentation for transformers},
  author={Gupta, Ankit and Berant, Jonathan},
  journal={arXiv preprint arXiv:2006.03274},
  year={2020}
}

@article{martins2021infinity,
  title={$\infty$-former: Infinite Memory Transformer},
  author={Martins, Pedro Henrique and Marinho, Zita and Martins, Andr{\'e} FT},
  journal={arXiv preprint arXiv:2109.00301},
  year={2021}
}

@inproceedings{merity2017wikitext,
  author    = {Stephen Merity and
               Caiming Xiong and
               James Bradbury and
               Richard Socher},
  title     = {Pointer Sentinel Mixture Models},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=Byj72udxe},
  timestamp = {Thu, 25 Jul 2019 14:25:57 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/MerityX0S17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{
tay2021synthesizer,
title={Synthesizer: Rethinking Self-Attention for Transformer Models},
author={Yi Tay and Dara Bahri and Donald Metzler and Da-Cheng Juan and Zhe Zhao and Che Zheng},
year={2021},
url={https://openreview.net/forum?id=H-SPvQtMwm}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@inproceedings{peng2021random-feature-attention,
title={Random Feature Attention},
author={Hao Peng and Nikolaos Pappas and Dani Yogatama and Roy Schwartz and Noah Smith and Lingpeng Kong},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=QtTKTdVrFBB}
}

@article{choromanski2020performer,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}

@article{burtsev2020memory-transformer,
  title={Memory transformer},
  author={Burtsev, Mikhail S and Kuratov, Yuri and Peganov, Anton and Sapunov, Grigory V},
  journal={arXiv preprint arXiv:2006.11527},
  year={2020}
}

@article{10.1162/neco.1997.9.8.1735,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
journal = {Neural Comput.},
month = nov,
pages = {1735–1780},
numpages = {46}
}

@misc{graves2014neural,
    title={Neural Turing Machines},
    author={Alex Graves and Greg Wayne and Ivo Danihelka},
    year={2014},
    eprint={1410.5401},
    archivePrefix={arXiv},
    primaryClass={cs.NE}
}

@misc{weston2014memory,
    title={Memory Networks},
    author={Jason Weston and Sumit Chopra and Antoine Bordes},
    year={2014},
    eprint={1410.3916},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
@misc{sukhbaatar2015endtoend,
    title={End-To-End Memory Networks},
    author={Sainbayar Sukhbaatar and Arthur Szlam and Jason Weston and Rob Fergus},
    year={2015},
    eprint={1503.08895},
    archivePrefix={arXiv},
    primaryClass={cs.NE}
}
@article{graves2016hybrid,
  abstract = {Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external read–write memory.},
  added-at = {2017-09-19T10:02:58.000+0200},
  author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwińska, Agnieszka and Colmenarejo, Sergio Gómez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adrià Puigdomènech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/21e067f9c481da30d7d743b9301d98b2e/hotho},
  description = {Hybrid computing using a neural network with dynamic external memory : Nature : Nature Research},
  interhash = {6a12ea752581b1895eca3716668df9fc},
  intrahash = {1e067f9c481da30d7d743b9301d98b2e},
  issn = {00280836},
  journal = {Nature},
  keywords = {access deep direct learning memory network neural rnn toread},
  month = oct,
  number = 7626,
  pages = {471--476},
  publisher = {Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  timestamp = {2017-09-19T10:02:58.000+0200},
  title = {Hybrid computing using a neural network with dynamic external memory},
  url = {http://dx.doi.org/10.1038/nature20101},
  volume = 538,
  year = 2016
}

@misc{rae2016scaling,
    title={Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes},
    author={Jack W Rae and Jonathan J Hunt and Tim Harley and Ivo Danihelka and Andrew Senior and Greg Wayne and Alex Graves and Timothy P Lillicrap},
    year={2016},
    eprint={1610.09027},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{arm2015inferring,
    title={Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets},
    author={Armand Joulin and Tomas Mikolov},
    year={2015},
    eprint={1503.01007},
    archivePrefix={arXiv},
    primaryClass={cs.NE}
}

@misc{grefenstette2015learning,
    title={Learning to Transduce with Unbounded Memory},
    author={Edward Grefenstette and Karl Moritz Hermann and Mustafa Suleyman and Phil Blunsom},
    year={2015},
    eprint={1506.02516},
    archivePrefix={arXiv},
    primaryClass={cs.NE}
}

@misc{lample2019large,
    title={Large Memory Layers with Product Keys},
    author={Guillaume Lample and Alexandre Sablayrolles and Marc'Aurelio Ranzato and Ludovic Denoyer and Hervé Jégou},
    year={2019},
    eprint={1907.05242},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{lei2020mart,
    title={MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning},
    author={Jie Lei and Liwei Wang and Yelong Shen and Dong Yu and Tamara L. Berg and Mohit Bansal},
    year={2020},
    eprint={2005.05402},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{child2019generating,
    title={Generating Long Sequences with Sparse Transformers},
    author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
    year={2019},
    eprint={1904.10509},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{kitaev2020reformer,
    title={Reformer: The Efficient Transformer},
    author={Nikita Kitaev and Łukasz Kaiser and Anselm Levskaya},
    year={2020},
    eprint={2001.04451},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{sukhbaatar2019adaptive,
    title={Adaptive Attention Span in Transformers},
    author={Sainbayar Sukhbaatar and Edouard Grave and Piotr Bojanowski and Armand Joulin},
    year={2019},
    eprint={1905.07799},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{dai2019transformerxl,
    title={Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context},
    author={Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
    year={2019},
    eprint={1901.02860},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{rae2019compressive,
    title={Compressive Transformers for Long-Range Sequence Modelling},
    author={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Timothy P. Lillicrap},
    year={2019},
    eprint={1911.05507},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{ainslie2020encoding,
    title={ETC: Encoding Long and Structured Data in Transformers},
    author={Joshua Ainslie and Santiago Ontanon and Chris Alberti and Philip Pham and Anirudh Ravula and Sumit Sanghai},
    year={2020},
    eprint={2004.08483},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{guo2019startransformer,
    title={Star-Transformer},
    author={Qipeng Guo and Xipeng Qiu and Pengfei Liu and Yunfan Shao and Xiangyang Xue and Zheng Zhang},
    year={2019},
    eprint={1902.09113},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{brown2020language,
    title={Language Models are Few-Shot Learners},
    author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
    year={2020},
    eprint={2005.14165},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{watkins1992,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}
@inproceedings{chen2017double,
  title={Double-task deep q-learning with multiple views},
  author={Chen, Jun and Bai, Tingzhu and Huang, Xiangsheng and Guo, Xian and Yang, Jianing and Yao, Yuxing},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision Workshops},
  pages={1050--1058},
  year={2017}
}
@book{powell2007approximate,
  title={Approximate Dynamic Programming: Solving the curses of dimensionality},
  author={Powell, Warren B},
  volume={703},
  year={2007},
  publisher={John Wiley \& Sons}
}
@inproceedings{li2019multi,
  title={Multi-View Reinforcement Learning},
  author={Li, Minne and Wu, Lisheng and Jun, WANG and Ammar, Haitham Bou},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1418--1429},
  year={2019}
}
@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529},
  year={2015},
  publisher={Nature Publishing Group}
}
@article{kaelbling1998planning,
  title={Planning and acting in partially observable stochastic domains},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Cassandra, Anthony R},
  journal={Artificial intelligence},
  volume={101},
  number={1-2},
  pages={99--134},
  year={1998},
  publisher={Elsevier}
}
@inbook{gibson1966senses,
  title={The senses considered as perceptual systems.},
  author={Gibson, James Jerome},
  chapter={1},
  pages={1-5},
  year={1966},
  publisher={Houghton Mifflin}
}
@article{bossens2019learning,
  title={Learning to learn with active adaptive perception},
  author={Bossens, DM and Townsend, Nicholas C and Sobey, AJ},
  journal={Neural Networks},
  volume={115},
  pages={30--49},
  year={2019},
  publisher={Elsevier}
}
@misc{brockman2016openai,
    title={OpenAI Gym},
    author={Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
    year={2016},
    eprint={1606.01540},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@inproceedings{akrour2012april,
  title={April: Active preference learning-based reinforcement learning},
  author={Akrour, Riad and Schoenauer, Marc and Sebag, Mich{\`e}le},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={116--131},
  year={2012},
  organization={Springer}
}
@article{dietterich2000hierarchical,
  title={Hierarchical reinforcement learning with the MAXQ value function decomposition},
  author={Dietterich, Thomas G},
  journal={Journal of artificial intelligence research},
  volume={13},
  pages={227--303},
  year={2000}
}
%model-based RL
@inproceedings{sutton1990,
  title={Integrated architectures for learning, planning, and reacting based on approximating dynamic programming},
  author={Sutton, Richard S},
  booktitle={Machine learning proceedings 1990},
  pages={216--224},
  year={1990},
  publisher={Elsevier}
}

@inproceedings{kumar2016optimal,
  title={Optimal control with learned local models: Application to dexterous manipulation},
  author={Kumar, Vikash and Todorov, Emanuel and Levine, Sergey},
  booktitle={2016 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={378--383},
  year={2016},
  organization={IEEE}
}
@inproceedings{levine2013guided,
  title={Guided policy search},
  author={Levine, Sergey and Koltun, Vladlen},
  booktitle={International Conference on Machine Learning},
  pages={1--9},
  year={2013}
}
@inproceedings{deisenroth2011pilco,
  title={PILCO: A model-based and data-efficient approach to policy search},
  author={Deisenroth, Marc and Rasmussen, Carl E},
  booktitle={Proceedings of the 28th International Conference on machine learning (ICML-11)},
  pages={465--472},
  year={2011}
}
@inproceedings{gal2016improving,
  title={Improving PILCO with Bayesian neural network dynamics models},
  author={Gal, Yarin and McAllister, Rowan and Rasmussen, Carl Edward},
  booktitle={Data-Efficient Machine Learning workshop, ICML},
  volume={4},
  pages={34},
  year={2016}
}
%active RL
@article{Settles2012,
    title={Active learning},
  author={Settles, B.},
  journal={ Synthesis Lectures on Artificial Intelligence and Machine Learning},
  pages={1--114},
  year={2012},
  publisher={Elsevier}
}

@inproceedings{epshteyn2008active,
  title={Active reinforcement learning},
  author={Epshteyn, Arkady and Vogel, Adam and DeJong, Gerald},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={296--303},
  year={2008}
}
@inproceedings{krueger2016active,
  title={Active reinforcement learning: Observing rewards at a cost},
  author={Krueger, David and Leike, Jan and Evans, Owain and Salvatier, John},
  booktitle={Future of Interactive Learning Machines, NIPS Workshop},
  year={2016}
}

@inproceedings{lopes2009active,
  title={Active learning for reward estimation in inverse reinforcement learning},
  author={Lopes, Manuel and Melo, Francisco and Montesano, Luis},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={31--46},
  year={2009},
  organization={Springer}
}
@inproceedings{doshi2008reinforcement,
  title={Reinforcement learning with limited reinforcement: Using Bayes risk for active learning in POMDPs},
  author={Doshi, Finale and Pineau, Joelle and Roy, Nicholas},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={256--263},
  year={2008}
}
@article{schulze2018active,
  title={Active reinforcement learning with monte-carlo tree search},
  author={Schulze, Sebastian and Evans, Owain},
  journal={arXiv preprint arXiv:1803.04926},
  year={2018}
}
@article{thalken2005synthesis,
  title={Synthesis for semiconductor device design},
  author={Thalken, Jason and Haas, Stephan and Levi, AFJ},
  journal={Journal of applied physics},
  volume={98},
  number={4},
  pages={044508},
  year={2005},
  publisher={American Institute of Physics}
}
@article{zaheer2020big,
  title={Big Bird: Transformers for Longer Sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={arXiv preprint arXiv:2007.14062},
  year={2020}
}
@misc{wang2020linformer,
      title={Linformer: Self-Attention with Linear Complexity}, 
      author={Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
      year={2020},
      eprint={2006.04768},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{wolf2020transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations},
  pages={38--45},
  year={2020}
}

@article{ding2020ernie,
  title={ERNIE-Doc: A retrospective long-document modeling transformer},
  author={Ding, Siyu and Shang, Junyuan and Wang, Shuohuan and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},
  journal={arXiv preprint arXiv:2012.15688},
  year={2020}
}
@article{xu2021contrastive,
  title={Contrastive Document Representation Learning with Graph Attention Networks},
  author={Xu, Peng and Chen, Xinchi and Ma, Xiaofei and Huang, Zhiheng and Xiang, Bing},
  journal={arXiv preprint arXiv:2110.10778},
  year={2021}
}
@article{liu2022ernie,
  title={ERNIE-SPARSE: Learning Hierarchical Efficient Transformer Through Regularized Self-Attention},
  author={Liu, Yang and Liu, Jiaxiang and Chen, Li and Lu, Yuxiang and Feng, Shikun and Feng, Zhida and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},
  journal={arXiv preprint arXiv:2203.12276},
  year={2022}
}
@article{xiong2021simple,
  title={Simple Local Attentions Remain Competitive for Long-Context Tasks},
  author={Xiong, Wenhan and O{\u{g}}uz, Barlas and Gupta, Anchit and Chen, Xilun and Liskovich, Diana and Levy, Omer and Yih, Wen-tau and Mehdad, Yashar},
  journal={arXiv preprint arXiv:2112.07210},
  year={2021}
}
@misc{katharopoulos2020transformers,
      title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention}, 
      author={Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and François Fleuret},
      year={2020},
      eprint={2006.16236},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{kiesel2019semeval,
  title={Semeval-2019 task 4: Hyperpartisan news detection},
  author={Kiesel, Johannes and Mestre, Maria and Shukla, Rishabh and Vincent, Emmanuel and Adineh, Payam and Corney, David and Stein, Benno and Potthast, Martin},
  booktitle={Proceedings of the 13th International Workshop on Semantic Evaluation},
  pages={829--839},
  year={2019}
}
@article{gulcehre2016dynamic,
  title={Dynamic neural turing machine with soft and hard addressing schemes},
  author={Gulcehre, Caglar and Chandar, Sarath and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1607.00036},
  year={2016}
}

@article{gulcehre2017memory,
  title={Memory augmented neural networks with wormhole connections},
  author={Gulcehre, Caglar and Chandar, Sarath and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1701.08718},
  year={2017}
}


@inproceedings{meng2018context,
  title={Context-aware neural model for temporal information extraction},
  author={Meng, Yuanliang and Rumshisky, Anna},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={527--536},
  year={2018}
}



@article{KobayashiKuribayashiEtAl_2020_Attention_Module_is_Not_Only_Weight_Analyzing_Transformers_with_Vector_Norms,
  ids = {KobayashiKuribayashiEtAl\_2020\_Attention\_Module\_is\_Not\_Only\_Weight\_Analyzing\_Transformers\_with\_Vector\_Normsa},
  title = {Attention {{Module}} Is {{Not Only}} a {{Weight}}: {{Analyzing Transformers}} with {{Vector Norms}}},
  shorttitle = {Attention {{Module}} Is {{Not Only}} a {{Weight}}},
  author = {Kobayashi, Goro and Kuribayashi, Tatsuki and Yokoi, Sho and Inui, Kentaro},
  year = {2020},
  url = {http://arxiv.org/abs/2004.10102},
  archivePrefix = {arXiv},
  journal = {arXiv:2004.10102 [cs]},
  primaryClass = {cs}
}


@InProceedings{bojar-EtAl:2014:W14-33,
  author    = {Bojar, Ondrej  and  Buck, Christian  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Leveling, Johannes  and  Monz, Christof  and  Pecina, Pavel  and  Post, Matt  and  Saint-Amand, Herve  and  Soricut, Radu  and  Specia, Lucia  and  Tamchyna, Ale
{s} },
  title     = {Findings of the 2014 Workshop on Statistical Machine Translation},
  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
  month     = {June},
  year      = {2014},
  address   = {Baltimore, Maryland, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {12--58},
  url       = {http://www.aclweb.org/anthology/W/W14/W14-3302}
}


@article{kasai2020parallel,
  title={Parallel Machine Translation with Disentangled Context Transformer},
  author={Kasai, Jungo and Cross, James and Ghazvininejad, Marjan and Gu, Jiatao},
  journal={arXiv preprint arXiv:2001.05136},
  year={2020}
}


@inproceedings{levy2014neural,
  title={Neural word embedding as implicit matrix factorization},
  author={Levy, Omer and Goldberg, Yoav},
  booktitle={Advances in neural information processing systems},
  pages={2177--2185},
  year={2014}
}


@inproceedings{kong2019mutual,
  title={A Mutual Information Maximization Perspective of Language Representation Learning},
  author={Kong, Lingpeng and de Masson d'Autume, Cyprien and Yu, Lei and Ling, Wang and Dai, Zihang and Yogatama, Dani},
  booktitle={International Conference on Learning Representations},
  url = {https://openreview.net/forum?id=Syx79eBKwr&noteId=Syx79eBKwr},
  year={2019}
}


@inproceedings{da2019cracking,
  title = {Cracking the {{Contextual Commonsense Code}}: {{Understanding Commonsense Reasoning Aptitude}} of {{Deep Contextual Representations}}},
  shorttitle = {Cracking the {{Contextual Commonsense Code}}},
  booktitle = {Proceedings of the {{First Workshop}} on {{Commonsense Inference}} in {{Natural Language Processing}}},
  author = {Da, Jeff and Kasai, Jungo},
  year = {2019},
  pages = {1--12},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-6001},
  url = {https://www.aclweb.org/anthology/D19-6001}
}




@inproceedings{kim2020pre,
  title={Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction},
  author={Kim, Taeuk and Choi, Jihun and Edmiston, Daniel and Lee, Sang-goo},
  url = {https://arxiv.org/abs/2002.00737},
  booktitle = {{{ICLR}} 2020},
  year = {2020}
}


@inproceedings{vanSchijndel2019quantity,
  title = {Quantity Doesn't Buy Quality Syntax with Neural Language Models},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {{van Schijndel}, Marten and Mueller, Aaron and Linzen, Tal},
  year = {2019},
  pages = {5831--5837},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1592},
  url = {https://www.aclweb.org/anthology/D19-1592}
}





@inproceedings{vilares2020parsing,
  title={Parsing as Pretraining},
  author={Vilares, David and Strzyz, Michalina and S{\o}gaard, Anders and G{\'o}mez-Rodr{\'\i}guez, Carlos},
  booktitle={Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)},
  url = {http://arxiv.org/abs/2002.01685},
  year={2020}
}

@article{rosa2019inducing,
  title={Inducing syntactic trees from {{BERT}} representations},
  author={Rosa, Rudolf and Mare{\v{c}}ek, David},
  journal={arXiv preprint arXiv:1906.11511},
  url = {http://arxiv.org/abs/1906.11511},
  year={2019}
}



@inproceedings{JinJinEtAl_2020_Is_BERT_Really_Robust_Strong_Baseline_for_Natural_Language_Attack_on_Text_Classification_and_Entailment,
  title = {Is {{BERT Really Robust}}? {{A Strong Baseline}} for {{Natural Language Attack}} on {{Text Classification}} and {{Entailment}}},
  shorttitle = {Is {{BERT Really Robust}}?},
  booktitle = {{{AAAI}} 2020},
  author = {Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
  year = {2020},
  month = jan,
  url = {http://arxiv.org/abs/1907.11932},
  urldate = {2020-03-16},
  archivePrefix = {arXiv},
  eprint = {1907.11932},
  eprinttype = {arxiv}
}


@inproceedings{WangYuEtAl_2019_Improving_Pre-Trained_Multilingual_Model_with_Vocabulary_Expansion,
  title = {Improving {{Pre}}-{{Trained Multilingual Model}} with {{Vocabulary Expansion}}},
  booktitle = {Proceedings of the 23rd {{Conference}} on {{Computational Natural Language Learning}} ({{CoNLL}})},
  author = {Wang, Hai and Yu, Dian and Sun, Kai and Chen, Jianshu and Yu, Dong},
  year = {2019},
  month = nov,
  pages = {316--327},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/K19-1030},
  url = {https://www.aclweb.org/anthology/K19-1030},
  urldate = {2020-03-16}
}


@inproceedings{KitaevCaoEtAl_2019_Multilingual_Constituency_Parsing_with_Self-Attention_and_Pre-Training,
  title = {Multilingual {{Constituency Parsing}} with {{Self}}-{{Attention}} and {{Pre}}-{{Training}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Kitaev, Nikita and Cao, Steven and Klein, Dan},
  year = {2019},
  month = jul,
  pages = {3499--3505},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1340},
  url = {https://www.aclweb.org/anthology/P19-1340},
  urldate = {2020-03-16}
}




@inproceedings{WangYuEtAl_2019_Improving_Pre-Trained_Multilingual_Model_with_Vocabulary_Expansion,
  title = {Improving {{Pre}}-{{Trained Multilingual Model}} with {{Vocabulary Expansion}}},
  booktitle = {Proceedings of the 23rd {{Conference}} on {{Computational Natural Language Learning}} ({{CoNLL}})},
  author = {Wang, Hai and Yu, Dian and Sun, Kai and Chen, Jianshu and Yu, Dong},
  year = {2019},
  month = nov,
  pages = {316--327},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/K19-1030},
  url = {https://www.aclweb.org/anthology/K19-1030},
  urldate = {2020-03-16}
}




@article{SiWangEtAl_2019_What_does_BERT_Learn_from_Multiple-Choice_Reading_Comprehension_Datasets,
  title = {What Does {{BERT Learn}} from {{Multiple}}-{{Choice Reading Comprehension Datasets}}?},
  url = {https://arxiv.org/abs/1910.12391},
  author = {Si, Chenglei and Wang, Shuohang and Kan, Min-Yen and Jiang, Jing},
  year = {2019},
  month = oct,
  urldate = {2020-03-16},
  archivePrefix = {arXiv},
  eprint = {1910.12391},
  eprinttype = {arxiv},
  journal = {arXiv:1910.12391 [cs]},
  primaryClass = {cs}
}


@article{TalmorElazarEtAl_2019_oLMpics_-_On_what_Language_Model_Pre-training_Captures,
  title = {{{oLMpics}} -- {{On}} What {{Language Model Pre}}-{{Training}} {{Captures}}},
  author = {Talmor, Alon and Elazar, Yanai and Goldberg, Yoav and Berant, Jonathan},
  year = {2019},
  month = dec,
  url = {http://arxiv.org/abs/1912.13283},
  urldate = {2020-03-16},
  archivePrefix = {arXiv},
  eprint = {1912.13283},
  eprinttype = {arxiv},
  journal = {arXiv:1912.13283 [cs]},
  primaryClass = {cs}
}




@inproceedings{KrishnaTomarEtAl_2020_Thieves_on_Sesame_Street_Model_Extraction_of_BERT-based_APIs,
  title = {Thieves on {{Sesame Street}}! {{Model Extraction}} of {{BERT}}-{{Based}} {{APIs}}},
  booktitle = {{{ICLR}} 2020},
  author = {Krishna, Kalpesh and Tomar, Gaurav Singh and Parikh, Ankur P. and Papernot, Nicolas and Iyyer, Mohit},
  year = {2020},
  month = jan,
  url = {http://arxiv.org/abs/1910.12366},
  urldate = {2020-03-16},
  archivePrefix = {arXiv},
  eprint = {1910.12366},
  eprinttype = {arxiv}
}




@article{YogatamadAutumeEtAl_2019_Learning_and_Evaluating_General_Linguistic_Intelligencea,
  title = {Learning and {{Evaluating General Linguistic Intelligence}}},
  author = {Yogatama, Dani and {d'Autume}, Cyprien de Masson and Connor, Jerome and Kocisky, Tomas and Chrzanowski, Mike and Kong, Lingpeng and Lazaridou, Angeliki and Ling, Wang and Yu, Lei and Dyer, Chris and Blunsom, Phil},
  year = {2019},
  month = jan,
  url = {http://arxiv.org/abs/1901.11373},
  urldate = {2020-03-16},
  archivePrefix = {arXiv},
  eprint = {1901.11373},
  eprinttype = {arxiv},
  journal = {arXiv:1901.11373 [cs, stat]},
  primaryClass = {cs, stat}
}




@article{ConneauKhandelwalEtAl_2019_Unsupervised_Cross-lingual_Representation_Learning_at_Scale,
  title = {Unsupervised {{Cross}}-{{Lingual}} {{Representation Learning}} at {{Scale}}},
  author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = nov,
  url = {http://arxiv.org/abs/1911.02116},
  urldate = {2020-03-16},
  archivePrefix = {arXiv},
  eprint = {1911.02116},
  eprinttype = {arxiv},
  journal = {arXiv:1911.02116 [cs]},
  primaryClass = {cs}
}


@inproceedings{NivenKao_2019_Probing_Neural_Network_Comprehension_of_Natural_Language_Argumentsa,
  title = {Probing {{Neural Network Comprehension}} of {{Natural Language Arguments}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Niven, Timothy and Kao, Hung-Yu},
  year = {2019},
  month = jul,
  pages = {4658--4664},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1459},
  url = {https://www.aclweb.org/anthology/P19-1459},
  urldate = {2020-03-16}
}




@article{DodgeIlharcoEtAl_2020_Fine-Tuning_Pretrained_Language_Models_Weight_Initializations_Data_Orders_and_Early_Stopping,
  title = {{{Fine}}-{{Tuning}} {{Pretrained Language Models}}: {{Weight Initializations}}, {{Data Orders}}, and {{Early Stopping}}},
  shorttitle = {Fine-{{Tuning Pretrained Language Models}}},
  author = {Dodge, Jesse and Ilharco, Gabriel and Schwartz, Roy and Farhadi, Ali and Hajishirzi, Hannaneh and Smith, Noah},
  year = {2020},
  url = {http://arxiv.org/abs/2002.06305},
  archivePrefix = {arXiv},
  journal = {arXiv:2002.06305 [cs]},
  primaryClass = {cs}
}


@article{RichardsonSabharwal_2019_What_Does_My_QA_Model_Know_Devising_Controlled_Probes_using_Expert_Knowledge,
  title = {What {{Does My QA Model Know}}? {{Devising Controlled Probes}} Using {{Expert Knowledge}}},
  shorttitle = {What {{Does My QA Model Know}}?},
  author = {Richardson, Kyle and Sabharwal, Ashish},
  year = {2019},
  url = {http://arxiv.org/abs/1912.13337},
  archivePrefix = {arXiv},
  journal = {arXiv:1912.13337 [cs]},
  primaryClass = {cs}
}

@inproceedings{SugawaraStenetorpEtAl_2020_Assessing_Benchmarking_Capacity_of_Machine_Reading_Comprehension_Datasets,
  title = {Assessing the {{Benchmarking Capacity}} of {{Machine Reading Comprehension Datasets}}},
  booktitle = {{{AAAI}}},
  author = {Sugawara, Saku and Stenetorp, Pontus and Inui, Kentaro and Aizawa, Akiko},
  year = {2020},
  url = {http://arxiv.org/abs/1911.09241},
  archivePrefix = {arXiv}
}


@inproceedings{RogersKovalevaEtAl_2020_Getting_Closer_to_AI_Complete_Question_Answering_Set_of_Prerequisite_Real_Tasks,
  title = {Getting {{Closer}} to {{AI Complete Question Answering}}: {{A Set}} of {{Prerequisite Real Tasks}}},
  booktitle = {{{AAAI}}},
  author = {Rogers, Anna and Kovaleva, Olga and Downey, Matthew and Rumshisky, Anna},
  year = {2020},
  pages = {11},
  url = {https://aaai.org/Papers/AAAI/2020GB/AAAI-RogersA.7778.pdf}
}





@article{Crane_2018_Questionable_Answers_in_Question_Answering_Research_Reproducibility_and_Variability_of_Published_Results,
  title = {Questionable {{Answers}} in {{Question Answering Research}}: {{Reproducibility}} and {{Variability}} of {{Published Results}}},
  shorttitle = {Questionable {{Answers}} in {{Question Answering Research}}},
  author = {Crane, Matt},
  year = {2018},
  volume = {6},
  pages = {241--252},
  doi = {10.1162/tacl_a_00018},
  url = {https://aclweb.org/anthology/papers/Q/Q18/Q18-1018/},
  journal = {Transactions of the Association for Computational Linguistics}
}


@inproceedings{Petrov_2010_Products_of_Random_Latent_Variable_Grammars,
  title = {Products of {{Random Latent Variable Grammars}}},
  booktitle = {Human {{Language Technologies}}: {{The}} 2010 {{Annual Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Petrov, Slav},
  year = {2010},
  pages = {19--27},
  publisher = {{Association for Computational Linguistics}},
  address = {{Los Angeles, California}},
  url = {https://www.aclweb.org/anthology/N10-1003}
}



@inproceedings{BrunnerLiuEtAl_2020_On_Identifiability_in_Transformers,
  title = {On {{Identifiability}} in {{Transformers}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Brunner, Gino and Liu, Yang and Pascual, Damian and Richter, Oliver and Ciaramita, Massimiliano and Wattenhofer, Roger},
  year = {2020},
  url = {https://openreview.net/forum?id=BJg1f6EFDB}
}


@article{BaoDongEtAl_2020_UniLMv2_Pseudo-Masked_Language_Models_for_Unified_Language_Model_Pre-Training,
  title = {{{UniLMv2}}: {{Pseudo}}-{{Masked Language Models}} for {{Unified Language Model}} {{Pre}}-{{Training}}},
  shorttitle = {{{UniLMv2}}},
  author = {Bao, Hangbo and Dong, Li and Wei, Furu and Wang, Wenhui and Yang, Nan and Liu, Xiaodong and Wang, Yu and Piao, Songhao and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  year = {2020},
  url = {http://arxiv.org/abs/2002.12804},
  archivePrefix = {arXiv},
  journal = {arXiv:2002.12804 [cs]},
  primaryClass = {cs}
}




@inproceedings{SongTanEtAl_2019_MASS_Masked_Sequence_to_Sequence_Pre-training_for_Language_Generation,
  title = {{{MASS}}: {{Masked Sequence}} to {{Sequence}} {{Pre}}-{{Training}} for {{Language Generation}}},
  shorttitle = {{{MASS}}},
  booktitle = {{{ICML}}},
  author = {Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  year = {2019},
  url = {http://arxiv.org/abs/1905.02450},
  archivePrefix = {arXiv}
}

@unpublished{RadfordNarasimhanEtAl_2018_Improving_language_understanding_by_generative_pre-training,
  title = {Improving Language Understanding by Generative Pre-Training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year = {2018},
  url = {https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf}
}


@article{PhangFevryEtAl_2019_Sentence_Encoders_on_STILTs_Supplementary_Training_on_Intermediate_Labeled-data_Tasks,
  title = {Sentence {{Encoders}} on {{STILTs}}: {{Supplementary Training}} on {{Intermediate}} {{Labeled}}-{{Data}} {{Tasks}}},
  shorttitle = {Sentence {{Encoders}} on {{STILTs}}},
  author = {Phang, Jason and F{\'e}vry, Thibault and Bowman, Samuel R.},
  year = {2019},
  url = {http://arxiv.org/abs/1811.01088},
  archivePrefix = {arXiv},
  journal = {arXiv:1811.01088 [cs]},
  primaryClass = {cs}
}

@article{model:t5,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified}} {{Text}}-{{to}}-{{Text Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = {2019},
  url = {http://arxiv.org/abs/1910.10683},
  archivePrefix = {arXiv},
  journal = {arXiv:1910.10683 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{SuCheng_2019_SesameBERT_Attention_for_Anywhere,
  title = {{{SesameBERT}}: {{Attention}} for {{Anywhere}}},
  shorttitle = {{{SesameBERT}}},
  author = {Su, Ta-Chun and Cheng, Hsiang-Chih},
  year = {2019},
  url = {http://arxiv.org/abs/1910.03176},
  archivePrefix = {arXiv},
  journal = {arXiv:1910.03176 [cs]},
  primaryClass = {cs}
}


@inproceedings{ClinchantJungEtAl_2019_On_use_of_BERT_for_Neural_Machine_Translation,
  title = {On the Use of {{BERT}} for {{Neural Machine Translation}}},
  booktitle = {Proceedings of the 3rd {{Workshop}} on {{Neural Generation}} and {{Translation}}},
  author = {Clinchant, Stephane and Jung, Kweon Woo and Nikoulina, Vassilina},
  year = {2019},
  pages = {108--117},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong}},
  doi = {10.18653/v1/D19-5611},
  url = {https://www.aclweb.org/anthology/D19-5611}
}


@book{Schutze_1996_Empirical_Base_of_Linguistics_Grammaticality_Judgments_and_Linguistic_Methodology,
  title = {The {{Empirical Base}} of {{Linguistics}}: {{Grammaticality Judgments}} and {{Linguistic Methodology}}},
  shorttitle = {The {{Empirical Base}} of {{Linguistics}}},
  author = {Schutze, Carson T.},
  year = {1996},
  publisher = {{University of Chicago Press}},
  isbn = {978-0-226-74154-3},
  language = {en}
}




@article{model:xlnet,
  title = {{{XLNet}}: {{Generalized Autoregressive Pretraining}} for {{Language Understanding}}},
  shorttitle = {{{XLNet}}},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
  year = {2019},
  month = jun,
  url = {http://arxiv.org/abs/1906.08237},
  urldate = {2019-07-01},
  archivePrefix = {arXiv},
  eprint = {1906.08237},
  eprinttype = {arxiv},
  journal = {arXiv:1906.08237 [cs]},
  primaryClass = {cs}
}


@article{BelinkovGlass_2019_Analysis_Methods_in_Neural_Language_Processing_Survey,
  title = {Analysis {{Methods}} in {{Neural Language Processing}}: {{A Survey}}},
  shorttitle = {Analysis {{Methods}} in {{Neural Language Processing}}},
  author = {Belinkov, Yonatan and Glass, James},
  year = {2019},
  month = mar,
  volume = {7},
  pages = {49--72},
  doi = {10.1162/tacl_a_00254},
  url = {https://doi.org/10.1162/tacl_a_00254},
  urldate = {2019-07-04},
  journal = {Transactions of the Association for Computational Linguistics}
}




@inproceedings{ZhangBowman_2018_Language_Modeling_Teaches_You_More_than_Translation_Does_Lessons_Learned_Through_Auxiliary_Syntactic_Task_Analysis,
  title = {Language {{Modeling Teaches You More}} than {{Translation Does}}: {{Lessons Learned Through Auxiliary Syntactic Task Analysis}}},
  shorttitle = {Language {{Modeling Teaches You More}} than {{Translation Does}}},
  booktitle = {Proceedings of the 2018 {{EMNLP Workshop BlackboxNLP}}: {{Analyzing}} and {{Interpreting Neural Networks}} for {{NLP}}},
  author = {Zhang, Kelly and Bowman, Samuel},
  year = {2018},
  month = nov,
  pages = {359--361},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/W18-5448},
  url = {https://www.aclweb.org/anthology/W18-5448},
  urldate = {2020-01-28}
}


@inproceedings{HowardRuder_2018_Universal_Language_Model_Fine-tuning_for_Text_Classification,
  title = {Universal {{Language Model}} {{Fine}}-{{Tuning}} for {{Text Classification}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Howard, Jeremy and Ruder, Sebastian},
  year = {2018},
  month = jul,
  pages = {328--339},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1031},
  url = {https://www.aclweb.org/anthology/P18-1031},
  urldate = {2020-01-28}
}


@inproceedings{MikolovSutskeverEtAl_2013_Distributed_representations_of_words_and_phrases_and_their_compositionality,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26 ({{NIPS}} 2013)},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S. and Dean, Jeff},
  year = {2013},
  pages = {3111--3119},
  url = {http://papers.nips.cc/paper/5021-di},
  urldate = {2015-07-24}
}




@inproceedings{PetersRuderEtAl_2019_To_Tune_or_Not_to_Tune_Adapting_Pretrained_Representations_to_Diverse_Tasks,
  title = {To {{Tune}} or {{Not}} to {{Tune}}? {{Adapting Pretrained Representations}} to {{Diverse Tasks}}},
  shorttitle = {To {{Tune}} or {{Not}} to {{Tune}}?},
  booktitle = {Proceedings of the 4th {{Workshop}} on {{Representation Learning}} for {{NLP}} ({{RepL4NLP}}-2019)},
  author = {Peters, Matthew E. and Ruder, Sebastian and Smith, Noah A.},
  year = {2019},
  month = aug,
  pages = {7--14},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/W19-4302},
  url = {https://www.aclweb.org/anthology/W19-4302},
  urldate = {2020-01-28}
}


@article{LampleConneau_2019_Cross-lingual_Language_Model_Pretraining,
  title = {{{Cross}}-{{Lingual}} {{Language Model Pretraining}}},
  author = {Lample, Guillaume and Conneau, Alexis},
  year = {2019},
  month = jan,
  url = {http://arxiv.org/abs/1901.07291},
  urldate = {2020-01-28},
  archivePrefix = {arXiv},
  eprint = {1901.07291},
  eprinttype = {arxiv},
  journal = {arXiv:1901.07291 [cs]},
  primaryClass = {cs}
}


@inproceedings{JainWallace_2019_Attention_is_not_Explanation,
  title = {Attention Is Not {{Explanation}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Jain, Sarthak and Wallace, Byron C.},
  year = {2019},
  month = jun,
  pages = {3543--3556},
  url = {https://aclweb.org/anthology/papers/N/N19/N19-1357/},
  urldate = {2019-07-07},
  language = {en-us}
}

@article{SerranoSmith_2019_Is_Attention_Interpretable,
  title = {Is {{Attention Interpretable}}?},
  author = {Serrano, Sofia and Smith, Noah A.},
  year = {2019},
  month = jun,
  url = {http://arxiv.org/abs/1906.03731},
  urldate = {2019-06-14},
  archivePrefix = {arXiv},
  eprint = {1906.03731},
  eprinttype = {arxiv},
  journal = {arXiv:1906.03731 [cs]},
  primaryClass = {cs}
}

@inproceedings{WiegreffePinter_2019_Attention_is_not_not_Explanation,
  title = {Attention Is Not Not {{Explanation}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Wiegreffe, Sarah and Pinter, Yuval},
  year = {2019},
  month = nov,
  pages = {11--20},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1002},
  url = {https://www.aclweb.org/anthology/D19-1002},
  urldate = {2020-01-14}
}




@inproceedings{WuLvEtAl_2019_Conditional_BERT_Contextual_Augmentation,
  title = {Conditional {{BERT Contextual Augmentation}}},
  booktitle = {{{ICCS}} 2019: {{Computational Science}} \textendash{} {{ICCS}} 2019},
  author = {Wu, Xing and Lv, Shangwen and Zang, Liangjun and Han, Jizhong and Hu, Songlin},
  year = {2019},
  pages = {84--95},
  publisher = Springer,
  url = {https://link.springer.com/chapter/10.1007/978-3-030-22747-0_7},
  urldate = {2019-02-19},
  archivePrefix = {arXiv},
  eprint = {1812.06705},
  eprinttype = {arxiv}
}


@inproceedings{RaganatoTiedemann_2018_Analysis_of_Encoder_Representations_in_Transformer-Based_Machine_Translation,
  title = {An {{Analysis}} of {{Encoder Representations}} in {{Transformer}}-{{Based}} {{Machine Translation}}},
  booktitle = {Proceedings of the 2018 {{EMNLP Workshop BlackboxNLP}}: {{Analyzing}} and {{Interpreting Neural Networks}} for {{NLP}}},
  author = {Raganato, Alessandro and Tiedemann, J{\"o}rg},
  year = {2018},
  month = nov,
  pages = {287--297},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/W18-5431},
  url = {https://www.aclweb.org/anthology/W18-5431},
  urldate = {2019-10-24}
}




@inproceedings{RogersDrozdEtAl_2017_Too_Many_Problems_of_Analogical_Reasoning_with_Word_Vectors,
  title = {The ({{Too Many}}) {{Problems}} of {{Analogical Reasoning}} with {{Word Vectors}}},
  booktitle = {Proceedings of the 6th {{Joint Conference}} on {{Lexical}} and {{Computational Semantics}} (* {{SEM}} 2017)},
  author = {Rogers, Anna and Drozd, Aleksandr and Li, Bofang},
  year = {2017},
  pages = {135--148},
  url = {http://www.aclweb.org/anthology/S17-1017}
}


@inproceedings{DrozdGladkovaEtAl_2016_Word_embeddings_analogies_and_machine_learning_beyond_king_man_woman_queen,
  title = {Word Embeddings, Analogies, and Machine Learning: Beyond King - Man + Woman = Queen},
  shorttitle = {Word {{Embeddings}}, {{Analogies}}, and {{Machine Learning}}},
  booktitle = {Proceedings of {{COLING}} 2016, the 26th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  author = {Drozd, Aleksandr and Gladkova, Anna and Matsuoka, Satoshi},
  year = {2016},
  pages = {3519--3530},
  address = {{Osaka, Japan, December 11-17}},
  url = {https://www.aclweb.org/anthology/C/C16/C16-1332.pdf}
}

@article{ArtetxeRuderEtAl_2019_On_Cross-lingual_Transferability_of_Monolingual_Representations,
  title = {On the {{Cross}}-{{lingual}} {{Transferability}} of {{Monolingual}} {{Representations}}},
  author = {Artetxe, Mikel and Ruder, Sebastian and Yogatama, Dani},
  year = {2019},
  url = {http://arxiv.org/abs/1910.11856},
  archivePrefix = {arXiv},
  journal = {arXiv:1911.03310 [cs]},
  primaryClass = {cs}
}

@article{LibovickyRosaEtAl_2019_How_Language-Neutral_is_Multilingual_BERT,
  title = {How {{Language}}-{{Neutral}} Is {{Multilingual BERT}}?},
  author = {Libovicky, Jindrich and Rosa, Rudolf and Fraser, Alexander},
  year = {2019},
  url = {http://arxiv.org/abs/1911.03310},
  archivePrefix = {arXiv},
  journal = {arXiv:1911.03310 [cs]},
  primaryClass = {cs}
}

@inproceedings{SinghMcCannEtAl_2019_BERT_is_Not_Interlingua_and_Bias_of_Tokenization,
  title = {{{BERT}} Is {{Not}} an {{Interlingua}} and the {{Bias}} of {{Tokenization}}},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Deep Learning Approaches}} for {{Low}}-{{Resource NLP}} ({{DeepLo}} 2019)},
  author = {Singh, Jasdeep and McCann, Bryan and Socher, Richard and Xiong, Caiming},
  year = {2019},
  pages = {47--55},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-6106},
  url = {https://www.aclweb.org/anthology/D19-6106}
}

@inproceedings{TenneyXiaEtAl_2019_What_do_you_learn_from_context_Probing_for_sentence_structure_in_contextualized_word_representations,
  title = {What Do You Learn from Context? {{Probing}} for Sentence Structure in Contextualized Word Representations},
  shorttitle = {What Do You Learn from Context?},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R. Thomas and Kim, Najoung and Durme, Benjamin Van and Bowman, Samuel R. and Das, Dipanjan and Pavlick, Ellie},
  year = {2019},
  url = {https://openreview.net/forum?id=SJzSgnRcKX},
  urldate = {2020-01-21}
}

@inproceedings{McCoyLinzenEtAl_2019_RNNs_implicitly_implement_tensor-product_representations,
  title = {{{RNNs}} Implicitly Implement Tensor-Product Representations},
  booktitle = {International Conference on Learning Representations},
  author = {McCoy, R. Thomas and Linzen, Tal and Dunbar, Ewan and Smolensky, Paul},
  year = {2019},
  url = {https://openreview.net/forum?id=BJx0sjC5FX}
}


@article{model:tinyBERT,
  title = {{{TinyBERT}}: {{Distilling BERT}} for {{Natural Language Understanding}}},
  shorttitle = {{{TinyBERT}}},
  author = {Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  year = {2019},
  journal = {arXiv preprint arXiv:1909.10351}
}

@inproceedings{model:distilBERT,
  title = {{{DistilBERT}}, a Distilled Version of {{BERT}}: Smaller, Faster, Cheaper and Lighter},
  shorttitle = {{{DistilBERT}}, a Distilled Version of {{BERT}}},
  booktitle = {5th {{Workshop}} on {{Energy Efficient Machine Learning}} and {{Cognitive Computing}} - {{NeurIPS}} 2019},
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  year = {2019},
  month = oct,
  url = {http://arxiv.org/abs/1910.01108},
  urldate = {2019-10-18},
  archivePrefix = {arXiv},
  eprint = {1910.01108},
  eprinttype = {arxiv}
}

@inproceedings{model:8bitbert,
  title = {{{Q8BERT}}: {{Quantized 8Bit BERT}}},
  shorttitle = {{{Q8BERT}}},
  booktitle = {5th {{Workshop}} on {{Energy Efficient Machine Learning}} and {{Cognitive Computing}} ({{NeurIPS}} 2019)},
  author = {Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe},
  year = {2019},
  month = oct,
  url = {http://arxiv.org/abs/1910.06188},
  urldate = {2019-11-05},
  archivePrefix = {arXiv},
  eprint = {1910.06188},
  eprinttype = {arxiv}
}


@inproceedings{WuFanEtAl_2019_Pay_Less_Attention_with_Lightweight_and_Dynamic_Convolutions,
  title = {Pay {{Less Attention}} with {{Lightweight}} and {{Dynamic Convolutions}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Wu, Felix and Fan, Angela and Baevski, Alexei and Dauphin, Yann and Auli, Michael},
  year = {2019},
  url = {https://openreview.net/forum?id=SkVhlh09tX}
}


@inproceedings{ZhangHanEtAl_2019_ERNIE_Enhanced_Language_Representation_with_Informative_Entities,
  title = {{{ERNIE}}: {{Enhanced Language Representation}} with {{Informative Entities}}},
  shorttitle = {{{ERNIE}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  year = {2019},
  month = jul,
  pages = {1441--1451},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1139},
  url = {https://www.aclweb.org/anthology/P19-1139},
  urldate = {2020-01-31}
}


@article{SunWangEtAl_2019_ERNIE_Enhanced_Representation_through_Knowledge_Integration,
  title = {{{ERNIE}}: {{Enhanced Representation}} through {{Knowledge Integration}}},
  shorttitle = {{{ERNIE}}},
  author = {Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Chen, Xuyi and Zhang, Han and Tian, Xin and Zhu, Danxiang and Tian, Hao and Wu, Hua},
  year = {2019},
  url = {http://arxiv.org/abs/1904.09223},
  archivePrefix = {arXiv},
  journal = {arXiv:1904.09223 [cs]},
  primaryClass = {cs}
}




@inproceedings{DavisonFeldmanEtAl_2019_Commonsense_Knowledge_Mining_from_Pretrained_Models,
  title = {Commonsense {{Knowledge Mining}} from {{Pretrained Models}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Davison, Joe and Feldman, Joshua and Rush, Alexander},
  year = {2019},
  month = nov,
  pages = {1173--1178},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1109},
  url = {https://www.aclweb.org/anthology/D19-1109},
  urldate = {2019-11-05}
}


@article{WolfDebutEtAl_2020_HuggingFaces_Transformers_State-of-the-art_Natural_Language_Processing,
  title = {{{HuggingFace}}'s {{Transformers}}: {{State}}-{{of}}-{{the}}-{{Art}} {{Natural Language Processing}}},
  shorttitle = {{{HuggingFace}}'s {{Transformers}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and Brew, Jamie},
  year = {2020},
  month = feb,
  url = {http://arxiv.org/abs/1910.03771},
  urldate = {2020-03-16},
  archivePrefix = {arXiv},
  eprint = {1910.03771},
  eprinttype = {arxiv},
  journal = {arXiv:1910.03771 [cs]},
  primaryClass = {cs}
}


@inproceedings{ClarkLuongEtAl_2020_ELECTRA_Pre-training_Text_Encoders_as_Discriminators_Rather_Than_Generators,
  title = {{{ELECTRA}}: {{Pre}}-{{Training}} {{Text Encoders}} as {{Discriminators Rather Than Generators}}},
  shorttitle = {{{ELECTRA}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
  year = {2020},
  url = {https://openreview.net/forum?id=r1xMH1BtvB},
  urldate = {2020-03-16}
}


@article{WuConneauEtAl_2019_Emerging_Cross-lingual_Structure_in_Pretrained_Language_Models,
  title = {Emerging {{Cross}}-{{Lingual}} {{Structure}} in {{Pretrained Language Models}}},
  author = {Wu, Shijie and Conneau, Alexis and Li, Haoran and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = nov,
  url = {http://arxiv.org/abs/1911.01464},
  urldate = {2020-03-16},
  archivePrefix = {arXiv},
  eprint = {1911.01464},
  eprinttype = {arxiv},
  journal = {arXiv:1911.01464 [cs]},
  primaryClass = {cs}
}


@inproceedings{LoSimard_2019_Fully_Unsupervised_Crosslingual_Semantic_Textual_Similarity_Metric_Based_on_BERT_for_Identifying_Parallel_Data,
  title = {Fully {{Unsupervised Crosslingual Semantic Textual Similarity Metric Based}} on {{BERT}} for {{Identifying Parallel Data}}},
  booktitle = {Proceedings of the 23rd {{Conference}} on {{Computational Natural Language Learning}} ({{CoNLL}})},
  author = {Lo, Chi-kiu and Simard, Michel},
  year = {2019},
  month = nov,
  pages = {206--215},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/K19-1020},
  url = {https://www.aclweb.org/anthology/K19-1020},
  urldate = {2020-03-16}
}


@article{NozzaBianchiEtAl_2020_What_MASK_Making_Sense_of_Language-Specific_BERT_Models,
  title = {What the [{{MASK}}]? {{Making Sense}} of {{Language}}-{{Specific}} {{BERT Models}}},
  shorttitle = {What the [{{MASK}}]?},
  author = {Nozza, Debora and Bianchi, Federico and Hovy, Dirk},
  year = {2020},
  month = mar,
  url = {http://arxiv.org/abs/2003.02912},
  urldate = {2020-03-16},
  archivePrefix = {arXiv},
  eprint = {2003.02912},
  eprinttype = {arxiv},
  journal = {arXiv:2003.02912 [cs]},
  primaryClass = {cs}
}


@article{ChiDongEtAl_2019_Can_Monolingual_Pretrained_Models_Help_Cross-Lingual_Classification,
  title = {Can {{Monolingual Pretrained Models Help}} {{Cross}}-{{Lingual Classification}}?},
  author = {Chi, Zewen and Dong, Li and Wei, Furu and Mao, Xian-Ling and Huang, Heyan},
  year = {2019},
  month = nov,
  url = {http://arxiv.org/abs/1911.03913},
  urldate = {2020-03-16},
  archivePrefix = {arXiv},
  eprint = {1911.03913},
  eprinttype = {arxiv},
  journal = {arXiv:1911.03913 [cs]},
  primaryClass = {cs}
}



@article{WangYuEtAl_2019_Improving_Pre-Trained_Multilingual_Models_with_Vocabulary_Expansion,
  title = {Improving {{Pre}}-{{Trained Multilingual Models}} with {{Vocabulary Expansion}}},
  author = {Wang, Hai and Yu, Dian and Sun, Kai and Chen, Janshu and Yu, Dong},
  year = {2019},
  month = sep,
  url = {http://arxiv.org/abs/1909.12440},
  urldate = {2020-03-16},
  archivePrefix = {arXiv},
  eprint = {1909.12440},
  eprinttype = {arxiv},
  journal = {arXiv:1909.12440 [cs]},
  primaryClass = {cs}
}




@inproceedings{TranBisazza_2019_Zero-shot_Dependency_Parsing_with_Pre-trained_Multilingual_Sentence_Representations,
  title = {Zero-Shot {{Dependency Parsing}} with {{Pre}}-{{Trained}} {{Multilingual Sentence Representations}}},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Deep Learning Approaches}} for {{Low}}-{{Resource NLP}} ({{DeepLo}} 2019)},
  author = {Tran, Ke and Bisazza, Arianna},
  year = {2019},
  month = nov,
  pages = {281--288},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-6132},
  url = {https://www.aclweb.org/anthology/D19-6132},
  urldate = {2020-03-16}
}






@inproceedings{KondratyukStraka_2019_75_Languages_1_Model_Parsing_Universal_Dependencies_Universally,
  title = {75 {{Languages}}, 1 {{Model}}: {{Parsing Universal Dependencies Universally}}},
  shorttitle = {75 {{Languages}}, 1 {{Model}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Kondratyuk, Dan and Straka, Milan},
  year = {2019},
  month = nov,
  pages = {2779--2795},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1279},
  url = {https://www.aclweb.org/anthology/D19-1279},
  urldate = {2020-03-16}
}




@inproceedings{SticklandMurray_2019_BERT_and_PALs_Projected_Attention_Layers_for_Efficient_Adaptation_in_Multi-Task_Learning,
  title = {{{BERT}} and {{PALs}}: {{Projected Attention Layers}} for {{Efficient Adaptation}} in {{Multi}}-{{Task}} {{Learning}}},
  shorttitle = {{{BERT}} and {{PALs}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Stickland, Asa Cooper and Murray, Iain},
  year = {2019},
  month = may,
  pages = {5986--5995},
  issn = {1938-7228},
  url = {http://proceedings.mlr.press/v97/stickland19a.html},
  urldate = {2020-03-16},
  chapter = {Machine Learning},
  language = {en}
}





@inproceedings{AraseTsujii_2019_Transfer_Fine-Tuning_BERT_Case_Study,
  title = {Transfer {{Fine}}-{{Tuning}}: {{A {{BERT}} Case Study}}},
  shorttitle = {Transfer {{Fine}}-{{Tuning}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Arase, Yuki and Tsujii, Jun'ichi},
  year = {2019},
  month = nov,
  pages = {5393--5404},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1542},
  url = {https://www.aclweb.org/anthology/D19-1542},
  urldate = {2020-03-16}
}




@article{ChengXuEtAl_2019_Symmetric_Regularization_based_BERT_for_Pair-wise_Semantic_Reasoning,
  title = {Symmetric {{Regularization}} Based {{BERT}} for {{Pair}}-{{Wise}} {{Semantic Reasoning}}},
  author = {Cheng, Xingyi and Xu, Weidi and Chen, Kunlong and Wang, Wei and Bi, Bin and Yan, Ming and Wu, Chen and Si, Luo and Chu, Wei and Wang, Taifeng},
  year = {2019},
  month = sep,
  url = {http://arxiv.org/abs/1909.03405},
  urldate = {2020-03-16},
  archivePrefix = {arXiv},
  eprint = {1909.03405},
  eprinttype = {arxiv},
  journal = {arXiv:1909.03405 [cs]},
  primaryClass = {cs}
}



@inproceedings{BaevskiEdunovEtAl_2019_Cloze-driven_Pretraining_of_Self-attention_Networks,
  title = {Cloze-Driven {{Pretraining}} of {{Self}}-{{Attention}} {{Networks}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Baevski, Alexei and Edunov, Sergey and Liu, Yinhan and Zettlemoyer, Luke and Auli, Michael},
  year = {2019},
  month = nov,
  pages = {5360--5369},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1539},
  url = {https://www.aclweb.org/anthology/D19-1539},
  urldate = {2020-03-16}
}





@inproceedings{PetroniRocktaschelEtAl_2019_Language_Models_as_Knowledge_Bases,
  title = {Language {{Models}} as {{Knowledge Bases}}?},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Petroni, Fabio and Rockt{\"a}schel, Tim and Riedel, Sebastian and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander},
  year = {2019},
  month = nov,
  pages = {2463--2473},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1250},
  url = {https://www.aclweb.org/anthology/D19-1250},
  urldate = {2019-12-10}
}

@inproceedings{CaoKitaevEtAl_2019_Multilingual_Alignment_of_Contextual_Word_Representations,
  title = {Multilingual {{Alignment}} of {{Contextual Word Representations}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Cao, Steven and Kitaev, Nikita and Klein, Dan},
  year = {2019},
  month = sep,
  url = {https://openreview.net/forum?id=r1xCMyBtPS},
  urldate = {2020-01-31}
}


@article{CuiCheEtAl_2019_Pre-Training_with_Whole_Word_Masking_for_Chinese_BERT,
  title = {{{Pre}}-{{Training}} with {{Whole Word Masking}} for {{Chinese BERT}}},
  author = {Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Yang, Ziqing and Wang, Shijin and Hu, Guoping},
  year = {2019},
  month = oct,
  url = {http://arxiv.org/abs/1906.08101},
  urldate = {2020-03-16},
  archivePrefix = {arXiv},
  eprint = {1906.08101},
  eprinttype = {arxiv},
  journal = {arXiv:1906.08101 [cs]},
  primaryClass = {cs}
}




@article{Tran_2020_From_English_To_Foreign_Languages_Transferring_Pre-trained_Language_Models,
  title = {From {{English To Foreign Languages}}: {{Transferring}} {{Pre}}-{{Trained}} {{Language Models}}},
  shorttitle = {From {{English To Foreign Languages}}},
  author = {Tran, Ke},
  year = {2020},
  month = feb,
  url = {http://arxiv.org/abs/2002.07306},
  urldate = {2020-03-16},
  archivePrefix = {arXiv},
  eprint = {2002.07306},
  eprinttype = {arxiv},
  journal = {arXiv:2002.07306 [cs]},
  primaryClass = {cs}
}





@inproceedings{HuangLiangEtAl_2019_Unicoder_Universal_Language_Encoder_by_Pre-training_with_Multiple_Cross-lingual_Tasks,
  title = {Unicoder: {{A Universal Language Encoder}} by {{Pre}}-Training with {{Multiple}} {{Cross}}-{{Lingual}} {{Tasks}}},
  shorttitle = {Unicoder},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Huang, Haoyang and Liang, Yaobo and Duan, Nan and Gong, Ming and Shou, Linjun and Jiang, Daxin and Zhou, Ming},
  year = {2019},
  month = nov,
  pages = {2485--2494},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1252},
  url = {https://www.aclweb.org/anthology/D19-1252},
  urldate = {2020-01-31}
}


@inproceedings{RonnqvistKanervaEtAl_2019_Is_Multilingual_BERT_Fluent_in_Language_Generation,
  title = {Is {{Multilingual {{BERT}} Fluent}} in {{Language Generation}}?},
  booktitle = {Proceedings of the {{First NLPL Workshop}} on {{Deep Learning}} for {{Natural Language Processing}}},
  author = {R{\"o}nnqvist, Samuel and Kanerva, Jenna and Salakoski, Tapio and Ginter, Filip},
  year = {2019},
  month = sep,
  pages = {29--36},
  publisher = {{Link{\"o}ping University Electronic Press}},
  address = {{Turku, Finland}},
  url = {https://www.aclweb.org/anthology/W19-6204},
  urldate = {2020-01-30}
}


@inproceedings{Vig_2019_Multiscale_Visualization_of_Attention_in_Transformer_Model,
  title = {A {{Multiscale Visualization}} of {{Attention}} in the {{Transformer Model}}},
  booktitle = {{{ACL}} 2019},
  author = {Vig, Jesse},
  year = {2019},
  month = jun,
  url = {http://arxiv.org/abs/1906.05714},
  urldate = {2019-07-26},
  archivePrefix = {arXiv},
  eprint = {1906.05714},
  eprinttype = {arxiv}
}



@article{JoshiChenEtAl_2020_SpanBERT_Improving_Pre-training_by_Representing_and_Predicting_Spans,
  title = {{{SpanBERT}}: {{Improving}} {{Pre}}-{{Training}} by {{Representing}} and {{Predicting Spans}}},
  shorttitle = {{{SpanBERT}}},
  author = {Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S. and Zettlemoyer, Luke and Levy, Omer},
  year = {2020},
  month = jan,
  volume = {8},
  pages = {64--77},
  publisher = {{MIT Press}},
  doi = {10.1162/tacl_a_00300},
  url = {https://doi.org/10.1162/tacl_a_00300},
  urldate = {2020-03-16},
  journal = {Transactions of the Association for Computational Linguistics}
}








@inproceedings{VigBelinkov_2019_Analyzing_Structure_of_Attention_in_Transformer_Language_Model,
  title = {Analyzing the {{Structure}} of {{Attention}} in a {{Transformer Language Model}}},
  booktitle = {Proceedings of the 2019 {{ACL Workshop BlackboxNLP}}: {{Analyzing}} and {{Interpreting Neural Networks}} for {{NLP}}},
  author = {Vig, Jesse and Belinkov, Yonatan},
  year = {2019},
  pages = {63--76},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/W19-4808},
  url = {https://www.aclweb.org/anthology/W19-4808}
}


@article{YangZhao_2019_Deepening_Hidden_Representations_from_Pre-trained_Language_Models_for_Natural_Language_Understanding,
  title = {Deepening {{Hidden Representations}} from {{Pre}}-{{Trained}} {{Language Models}} for {{Natural Language Understanding}}},
  author = {Yang, Junjie and Zhao, Hai},
  year = {2019},
  url = {http://arxiv.org/abs/1911.01940},
  archivePrefix = {arXiv},
  journal = {arXiv:1911.01940 [cs]},
  primaryClass = {cs}
}


@inproceedings{StrubellGaneshEtAl_2019_Energy_and_Policy_Considerations_for_Deep_Learning_in_NLP,
  title = {Energy and {{Policy Considerations}} for {{Deep Learning}} in {{NLP}}},
  booktitle = {{{ACL}} 2019},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  year = {2019},
  url = {http://arxiv.org/abs/1906.02243},
  archivePrefix = {arXiv}
}


@article{SchwartzDodgeEtAl_2019_Green_AI,
  title = {Green {{AI}}},
  author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
  year = {2019},
  url = {http://arxiv.org/abs/1907.10597},
  archivePrefix = {arXiv},
  journal = {arXiv:1907.10597 [cs, stat]},
  primaryClass = {cs, stat}
}





@article{Vig_2019_Visualizing_Attention_in_Transformer-Based_Language_Representation_Models,
  title = {Visualizing {{Attention}} in {{Transformer}}-{{Based}} {{Language Representation Models}}},
  author = {Vig, Jesse},
  year = {2019},
  url = {http://arxiv.org/abs/1904.02679},
  archivePrefix = {arXiv},
  journal = {arXiv:1904.02679 [cs, stat]},
  primaryClass = {cs, stat}
}


@inproceedings{Vig_2019_Multiscale_Visualization_of_Attention_in_Transformer_Model,
  title = {A {{Multiscale Visualization}} of {{Attention}} in the {{Transformer Model}}},
  booktitle = {{{ACL}} 2019},
  author = {Vig, Jesse},
  year = {2019},
  url = {http://arxiv.org/abs/1906.05714},
  archivePrefix = {arXiv}
}

@article{HooverStrobeltEtAl_2019_exBERT_Visual_Analysis_Tool_to_Explore_Learned_Representations_in_Transformers_Models,
  title = {{{exBERT}}: {{A Visual Analysis Tool}} to {{Explore Learned Representations}} in {{Transformers Models}}},
  shorttitle = {{{exBERT}}},
  author = {Hoover, Benjamin and Strobelt, Hendrik and Gehrmann, Sebastian},
  year = {2019},
  url = {http://arxiv.org/abs/1910.05276},
  archivePrefix = {arXiv},
  journal = {arXiv:1910.05276 [cs]},
  primaryClass = {cs}
}





@inproceedings{model:albert,
  title = {{{ALBERT}}: {{A Lite BERT}} for {{Self}}-{{Supervised}} {{Learning}} of {{Language Representations}}},
  shorttitle = {{{ALBERT}}},
  booktitle = ICLR,
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  year = {2020},
  url = {https://openreview.net/forum?id=H1eA7AEtvS},
  urldate = {2019-09-27}
}


@article{KumarChoudharyEtAl_2020_Data_Augmentation_using_Pre-trained_Transformer_Models,
  title = {Data {{Augmentation}} Using {{Pre}}-{{Trained}} {{Transformer Models}}},
  author = {Kumar, Varun and Choudhary, Ashutosh and Cho, Eunah},
  year = {2020},
  month = mar,
  url = {http://arxiv.org/abs/2003.02245},
  urldate = {2020-03-16},
  archivePrefix = {arXiv},
  eprint = {2003.02245},
  eprinttype = {arxiv},
  journal = {arXiv:2003.02245 [cs]},
  primaryClass = {cs}
}

@article{LevineLenzEtAl_2019_SenseBERT_Driving_Some_Sense_into_BERT,
  title = {{{SenseBERT}}: {{Driving Some Sense}} into {{BERT}}},
  shorttitle = {{{SenseBERT}}},
  author = {Levine, Yoav and Lenz, Barak and Dagan, Or and Padnos, Dan and Sharir, Or and {Shalev-Shwartz}, Shai and Shashua, Amnon and Shoham, Yoav},
  year = {2019},
  month = aug,
  url = {http://arxiv.org/abs/1908.05646},
  urldate = {2020-03-16},
  archivePrefix = {arXiv},
  eprint = {1908.05646},
  eprinttype = {arxiv},
  journal = {arXiv:1908.05646 [cs]},
  primaryClass = {cs}
}

@article{ShenQuachEtAl_2020_Blank_Language_Models,
  title = {Blank {{Language Models}}},
  author = {Shen, Tianxiao and Quach, Victor and Barzilay, Regina and Jaakkola, Tommi},
  year = {2020},
  month = feb,
  url = {http://arxiv.org/abs/2002.03079},
  urldate = {2020-03-16},
  archivePrefix = {arXiv},
  eprint = {2002.03079},
  eprinttype = {arxiv},
  journal = {arXiv:2002.03079 [cs]},
  primaryClass = {cs}
}

@article{SundararamanSubramanianEtAl_2019_Syntax-Infused_Transformer_and_BERT_models_for_Machine_Translation_and_Natural_Language_Understanding,
  title = {{{Syntax}}-{{Infused Transformer}} and {{BERT}} Models for {{Machine Translation}} and {{Natural Language Understanding}}},
  author = {Sundararaman, Dhanasekar and Subramanian, Vivek and Wang, Guoyin and Si, Shijing and Shen, Dinghan and Wang, Dong and Carin, Lawrence},
  year = {2019},
  month = nov,
  url = {http://arxiv.org/abs/1911.06156},
  urldate = {2020-03-16},
  archivePrefix = {arXiv},
  eprint = {1911.06156},
  eprinttype = {arxiv},
  journal = {arXiv:1911.06156 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{ZhangZhangEtAl_2020_Graph-Bert_Only_Attention_is_Needed_for_Learning_Graph_Representations,
  title = {{{Graph}}-{{Bert}}: {{Only Attention}} Is {{Needed}} for {{Learning Graph Representations}}},
  shorttitle = {Graph-{{Bert}}},
  author = {Zhang, Jiawei and Zhang, Haopeng and Xia, Congying and Sun, Li},
  year = {2020},
  month = jan,
  url = {http://arxiv.org/abs/2001.05140},
  urldate = {2020-03-16},
  archivePrefix = {arXiv},
  eprint = {2001.05140},
  eprinttype = {arxiv},
  journal = {arXiv:2001.05140 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{ZhuChengEtAl_2019_FreeLB_Enhanced_Adversarial_Training_for_Natural_Language_Understanding,
  title = {{{FreeLB}}: {{Enhanced Adversarial Training}} for {{Natural Language Understanding}}},
  shorttitle = {{{FreeLB}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Zhu, Chen and Cheng, Yu and Gan, Zhe and Sun, Siqi and Goldstein, Tom and Liu, Jingjing},
  year = {2019},
  month = sep,
  url = {https://openreview.net/forum?id=BygzbyHFvB},
  urldate = {2020-03-16}
}



@article{model_bart,
  title = {{{BART}}: {{Denoising Sequence}}-to-{{Sequence Pre}}-{{Training}} for {{Natural Language Generation}}, {{Translation}}, and {{Comprehension}}},
  shorttitle = {{{BART}}},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  year = {2019},
  month = oct,
  url = {http://arxiv.org/abs/1910.13461},
  urldate = {2019-11-05},
  archivePrefix = {arXiv},
  eprint = {1910.13461},
  eprinttype = {arxiv},
  journal = {arXiv:1910.13461 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{model:roberta,
  ids = {LiuOttEtAl\_2019\_RoBERTa\_Robustly\_Optimized\_BERT\_Pretraining\_Approach,LiuOttEtAl\_2019\_RoBERTa\_Robustly\_Optimized\_BERT\_Pretraining\_Approacha},
  title = {{{RoBERTa}}: {{A Robustly Optimized {{BERT}} Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  url = {http://arxiv.org/abs/1907.11692},
  urldate = {2019-08-14},
  archivePrefix = {arXiv},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  journal = {arXiv:1907.11692 [cs]},
  primaryClass = {cs}
}


@article{ZhuChengEtAl_2019_FreeLB_Enhanced_Adversarial_Training_for_Language_Understanding,
  title = {{{FreeLB}}: {{Enhanced Adversarial Training}} for {{Language Understanding}}},
  shorttitle = {{{FreeLB}}},
  author = {Zhu, Chen and Cheng, Yu and Gan, Zhe and Sun, Siqi and Goldstein, Tom and Liu, Jingjing},
  year = {2019},
  month = oct,
  url = {http://arxiv.org/abs/1909.11764},
  urldate = {2020-01-31},
  archivePrefix = {arXiv},
  eprint = {1909.11764},
  eprinttype = {arxiv},
  journal = {arXiv:1909.11764 [cs]},
  primaryClass = {cs}
}

@inproceedings{ZhangWuEtAl_2020_Semantics-aware_BERT_for_Language_Understanding,
  title = {Semantics-Aware {{BERT}} for {{Language Understanding}}},
  booktitle = {{{AAAI}} 2020},
  author = {Zhang, Zhuosheng and Wu, Yuwei and Zhao, Hai and Li, Zuchao and Zhang, Shuailiang and Zhou, Xi and Zhou, Xiang},
  year = {2020},
  url = {http://arxiv.org/abs/1909.02209},
  urldate = {2020-01-31},
  archivePrefix = {arXiv},
  eprint = {1909.02209},
  eprinttype = {arxiv}
}


@article{HoulsbyGiurgiuEtAl_2019_Parameter-Efficient_Transfer_Learning_for_NLP,
  title = {{{Parameter}}-{{Efficient Transfer Learning}} for {{NLP}}},
  author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and {de Laroussilhe}, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  year = {2019},
  month = jun,
  url = {http://arxiv.org/abs/1902.00751},
  urldate = {2020-01-31},
  archivePrefix = {arXiv},
  eprint = {1902.00751},
  eprinttype = {arxiv},
  journal = {arXiv:1902.00751 [cs, stat]},
  primaryClass = {cs, stat}
}


@article{model:ernie,
  title = {{{ERNIE}} 2.0: {{A Continual}} {{Pre}}-{{Training}} {{Framework}} for {{Language Understanding}}},
  shorttitle = {{{ERNIE}} 2.0},
  author = {Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Tian, Hao and Wu, Hua and Wang, Haifeng},
  year = {2019},
  month = jul,
  url = {http://arxiv.org/abs/1907.12412},
  urldate = {2019-07-30},
  archivePrefix = {arXiv},
  eprint = {1907.12412},
  eprinttype = {arxiv},
  journal = {arXiv:1907.12412 [cs]},
  primaryClass = {cs}
}



@inproceedings{MikolovYihEtAl_2013_Linguistic_Regularities_in_Continuous_Space_Word_Representations,
  title = {Linguistic {{Regularities}} in {{Continuous Space Word Representations}}.},
  booktitle = {Proceedings of {{NAACL}}-{{HLT}} 2013},
  author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  year = {2013},
  pages = {746--751},
  address = {{Atlanta, Georgia, 9\textendash{}14 June 2013}},
  url = {https://www.aclweb.org/anthology/N13-1090}
}


@article{WangBiEtAl_2019_StructBERT_Incorporating_Language_Structures_into_Pre-training_for_Deep_Language_Understanding,
  title = {{{StructBERT}}: {{Incorporating Language Structures}} into {{Pre}}-{{Training}} for {{Deep Language Understanding}}},
  shorttitle = {{{StructBERT}}},
  author = {Wang, Wei and Bi, Bin and Yan, Ming and Wu, Chen and Bao, Zuyi and Peng, Liwei and Si, Luo},
  year = {2019},
  month = aug,
  url = {http://arxiv.org/abs/1908.04577},
  urldate = {2019-08-20},
  archivePrefix = {arXiv},
  eprint = {1908.04577},
  eprinttype = {arxiv},
  journal = {arXiv:1908.04577 [cs]},
  primaryClass = {cs}
}



@book{Goldberg_2006_Constructions_at_Work_The_Nature_of_Generalization_in_Language,
  title = {Constructions at {{Work}}: {{The Nature}} of {{Generalization}} in {{Language}}},
  author = {Goldberg, Adele},
  year = {2006},
  publisher = {{Oxford University Press, USA}},
  isbn = {0-19-926852-5}
}






@inproceedings{tenney2019bert,
  title={{{BERT Rediscovers the Classical NLP Pipeline}}},
  author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4593--4601},
  year={2019},
  url={https://www.aclweb.org/anthology/P19-1452.pdf}
}


@article{BouraouiCamacho-ColladosEtAl_2019_Inducing_Relational_Knowledge_from_BERT,
  title = {Inducing {{Relational Knowledge}} from {{BERT}}},
  author = {Bouraoui, Zied and {Camacho-Collados}, Jose and Schockaert, Steven},
  year = {2019},
  url = {http://arxiv.org/abs/1911.12753},
  archivePrefix = {arXiv},
  journal = {arXiv:1911.12753 [cs]},
  primaryClass = {cs}
}


@article{JiangXuEtAl_2019_How_Can_We_Know_What_Language_Models_Know,
  title = {How {{Can We Know What Language Models Know}}?},
  author = {Jiang, Zhengbao and Xu, Frank F. and Araki, Jun and Neubig, Graham},
  year = {2019},
  month = nov,
  url = {http://arxiv.org/abs/1911.12543},
  urldate = {2020-03-16},
  archivePrefix = {arXiv},
  eprint = {1911.12543},
  eprinttype = {arxiv},
  journal = {arXiv:1911.12543 [cs]},
  primaryClass = {cs}
}


@inproceedings{PetersNeumannEtAl_2019_Knowledge_Enhanced_Contextual_Word_Representations,
  title = {Knowledge {{Enhanced Contextual Word Representations}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Peters, Matthew E. and Neumann, Mark and Logan, Robert and Schwartz, Roy and Joshi, Vidur and Singh, Sameer and Smith, Noah A.},
  year = {2019},
  pages = {43--54},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  url = {https://www.aclweb.org/anthology/D19-1005}
}




@inproceedings{BakerFillmoreEtAl_1998_Berkeley_Framenet_project,
  title = {The {{Berkeley Framenet}} Project},
  booktitle = {Proceedings of the 17th International Conference on {{Computational Linguistics}}},
  author = {Baker, Collin F. and Fillmore, Charles J. and Lowe, John B.},
  year = {1998},
  url={https://www.aclweb.org/anthology/P98-1013/},
  volume = {1},
  pages = {86--90},
  publisher = {{Association for Computational Linguistics}}
}


@inproceedings{KovalevaRomanovEtAl_2019_Revealing_Dark_Secrets_of_BERT,
  title = {Revealing the {{Dark Secrets}} of {{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and Rumshisky, Anna},
  year = {2019},
  pages = {4356--4365},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1445},
  url = {https://www.aclweb.org/anthology/D19-1445}
}




@article{Ettinger_2019_What_BERT_is_not_Lessons_from_new_suite_of_psycholinguistic_diagnostics_for_language_models,
  title = {What {{BERT}} Is Not: {{Lessons}} from a New Suite of Psycholinguistic Diagnostics for Language Models},
  shorttitle = {What {{BERT}} Is Not},
  author = {Ettinger, Allyson},
  year = {2019},
  url = {http://arxiv.org/abs/1907.13528},
  archivePrefix = {arXiv},
  journal = {arXiv:1907.13528 [cs]},
  primaryClass = {cs}
}


@inproceedings{WallaceFengEtAl_2019_Universal_Adversarial_Triggers_for_Attacking_and_Analyzing_NLP,
  title = {Universal {{Adversarial Triggers}} for {{Attacking}} and {{Analyzing NLP}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Wallace, Eric and Feng, Shi and Kandpal, Nikhil and Gardner, Matt and Singh, Sameer},
  year = {2019},
  pages = {2153--2162},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1221},
  url = {https://www.aclweb.org/anthology/D19-1221}
}




@book{Schutze_1996_Empirical_Base_of_Linguistics_Grammaticality_Judgments_and_Linguistic_Methodology,
  title = {The {{Empirical Base}} of {{Linguistics}}: {{Grammaticality Judgments}} and {{Linguistic Methodology}}},
  shorttitle = {The {{Empirical Base}} of {{Linguistics}}},
  author = {Schutze, Carson T.},
  year = {1996},
  publisher = {{University of Chicago Press}},
  isbn = {978-0-226-74154-3}
}


@article{mickus2019you,
  title={What do you mean, {{BERT}}? Assessing {{BERT}} as a Distributional Semantics Model},
  author={Mickus, Timothee and Paperno, Denis and Constant, Mathieu and van Deemeter, Kees},
  journal={arXiv preprint arXiv:1911.05758},
  url = {http://arxiv.org/abs/1911.05758},
  year={2019}
}

@article{goldberg2019assessing,
  title={Assessing {{BERT's}} Syntactic Abilities},
  author={Goldberg, Yoav},
  journal={arXiv preprint arXiv:1901.05287},
  url = {http://arxiv.org/abs/1901.05287},
  year={2019}
}


@inproceedings{hewitt2019structural,
  title = {A {{Structural Probe}} for {{Finding Syntax}} in {{Word Representations}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Hewitt, John and Manning, Christopher D.},
  year = {2019},
  pages = {4129--4138},
  url = {https://aclweb.org/anthology/papers/N/N19/N19-1419/}
}





@inproceedings{jawahar2019does,
  title={What does {{BERT}} learn about the structure of language?},
  author={Jawahar, Ganesh and Sagot, Beno{\^\i}t and Seddah, Djam{\'e} and Unicomb, Samuel and I{\~n}iguez, Gerardo and Karsai, M{\'a}rton and L{\'e}o, Yannick and Karsai, M{\'a}rton and Sarraute, Carlos and Fleury, {\'E}ric and others},
  booktitle={57th Annual Meeting of the Association for Computational Linguistics (ACL), Florence, Italy},
  url = {https://www.aclweb.org/anthology/P19-1356.pdf},
  year={2019}
}

@article{reimers2019sentence,
  title={Sentence-bert: Sentence embeddings using siamese bert-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}


@article{michel2019sixteen,
  title = {Are {{Sixteen Heads Really Better}} than {{One}}?},
  author = {Michel, Paul and Levy, Omer and Neubig, Graham},
  year = {2019},
  url = {http://papers.nips.cc/paper/9551-are-sixteen-heads-really-better-than-one},
  archivePrefix = {arXiv},
  journal = {Advances in Neural Information Processing Systems 32 (NIPS 2019)}
}




@article{voita2019analyzing,
  title={{{Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned}}},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  journal={arXiv preprint arXiv:1905.09418},
  year={2019},
  url={https://www.aclweb.org/anthology/P19-1580/}
}



@inproceedings{BrunnerLiuEtAl_2019_On_Identifiability_in_Transformers,
  title = {On {{Identifiability}} in {{Transformers}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Brunner, Gino and Liu, Yang and Pascual, Damian and Richter, Oliver and Ciaramita, Massimiliano and Wattenhofer, Roger},
  year = {2020},
  url = {https://openreview.net/forum?id=BJg1f6EFDB}
}


@inproceedings{GargVuEtAl_2020_TANDA_Transfer_and_Adapt_Pre-Trained_Transformer_Models_for_Answer_Sentence_Selection,
  title = {{{TANDA}}: {{Transfer}} and {{Adapt}} {{Pre}}-{{Trained}} {{Transformer Models}} for {{Answer Sentence Selection}}},
  shorttitle = {{{TANDA}}},
  booktitle = {{{AAAI}}},
  author = {Garg, Siddhant and Vu, Thuy and Moschitti, Alessandro},
  year = {2020},
  url = {http://arxiv.org/abs/1911.04118},
  archivePrefix = {arXiv}
}


@article{htut2019attention,
  title={Do Attention Heads in {{BERT}} Track Syntactic Dependencies?},
  author={Htut, Phu Mon and Phang, Jason and Bordia, Shikha and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1911.12246},
  url = {http://arxiv.org/abs/1911.12246},
  year={2019}
}


@inproceedings{liu2019linguistic,
  title = {Linguistic {{Knowledge}} and {{Transferability}} of {{Contextual Representations}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Liu, Nelson F. and Gardner, Matt and Belinkov, Yonatan and Peters, Matthew E. and Smith, Noah A.},
  year = {2019},
  pages = {1073--1094},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  url = {https://www.aclweb.org/anthology/N19-1112/},
  archivePrefix = {arXiv}
}




@article{poerner2019bert,
  title={{{BERT}} is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA},
  author={Poerner, Nina and Waltinger, Ulli and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:1911.03681},
  url = {http://arxiv.org/abs/1911.03681},
  year={2019}
}

@article{zhou2019evaluating,
  title={Evaluating Commonsense in Pre-trained Language Models},
  author={Zhou, Xuhui and Zhang, Yue and Cui, Leyang and Huang, Dandan},
  journal={arXiv preprint arXiv:1911.11931},
  year={2019}
}


@inproceedings{clark2019does,
  title = {What {{Does BERT Look}} at? {{An Analysis}} of {{BERT}}'s {{Attention}}},
  shorttitle = {What {{Does BERT Look}} At?},
  booktitle = {Proceedings of the 2019 {{ACL Workshop BlackboxNLP}}: {{Analyzing}} and {{Interpreting Neural Networks}} for {{NLP}}},
  author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  year = {2019},
  pages = {276--286},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/W19-4828},
  url = {https://www.aclweb.org/anthology/W19-4828}
}



@inproceedings{warstadt2019investigating,
  title={{{Investigating BERT's Knowledge of Language: Five Analysis Methods with NPIs}}},
  author={Warstadt, Alex and Cao, Yu and Grosu, Ioana and Peng, Wei and Blix, Hagen and Nie, Yining and Alsop, Anna and Bordia, Shikha and Liu, Haokun and Parrish, Alicia and others},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={2870--2880},
  year={2019},
  url={https://www.aclweb.org/anthology/D19-1286/}
}

@article{bouraoui2019inducing,
  title={Inducing Relational Knowledge from {{BERT}}},
  author={Bouraoui, Zied and Camacho-Collados, Jose and Schockaert, Steven},
  journal={arXiv preprint arXiv:1911.12753},
  year={2019}
}


@inproceedings{devlin2019bert,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  pages = {4171--4186},
  url = {https://aclweb.org/anthology/papers/N/N19/N19-1423/}
}

@inproceedings{burtsev2018deeppavlov,
  title={Deeppavlov: Open-source library for dialogue systems},
  author={Burtsev, Mikhail and Seliverstov, Alexander and Airapetyan, Rafael and Arkhipov, Mikhail and Baymurzina, Dilyara and Bushkov, Nickolay and Gureenkova, Olga and Khakhulin, Taras and Kuratov, Yurii and Kuznetsov, Denis and others},
  booktitle={Proceedings of ACL 2018, System Demonstrations},
  pages={122--127},
  year={2018}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}


@inproceedings{vaswani2017attention,
  title={{{Attention is All you Need}}},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017},
  url={http://papers.nips.cc/paper/7181-attention-is-all-you-need}
}

@article{wang2019cross,
  title={{{Cross-Lingual Ability of Multilingual BERT: An Empirical Study}}},
  author={Wang, Zihan and Mayhew, Stephen and Roth, Dan and others},
  journal={arXiv preprint arXiv:1912.07840},
  year={2019},
  url={https://arxiv.org/abs/1912.07840}
}

@article{si2019does,
  title={What does {{BERT}} Learn from Multiple-Choice Reading Comprehension Datasets?},
  author={Si, Chenglei and Wang, Shuohang and Kan, Min-Yen and Jiang, Jing},
  journal={arXiv preprint arXiv:1910.12391},
  year={2019}
}

@inproceedings{van2019does,
  title={{{How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations}}},
  author={van Aken, Betty and Winter, Benjamin and L{\"o}ser, Alexander and Gers, Felix A},
  booktitle={Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
  pages={1823--1832},
  year={2019},
  url={https://dl.acm.org/doi/abs/10.1145/3357384.3358028}
}

@article{arkhangelskaia2019whatcha,
  title={{{Whatcha lookin'at? DeepLIFTing BERT's Attention in Question Answering}}},
  author={Arkhangelskaia, Ekaterina and Dutta, Sourav},
  journal={arXiv preprint arXiv:1910.06431},
  year={2019},
  url={https://arxiv.org/abs/1910.06431}
}

@article{wiedemann2019does,
  title={{{Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings}}},
  author={Wiedemann, Gregor and Remus, Steffen and Chawla, Avi and Biemann, Chris},
  journal={arXiv preprint arXiv:1909.10430},
  year={2019},
  url={https://arxiv.org/abs/1909.10430}
}


@inproceedings{richardson2019probing,
  title = {Probing {{Natural Language Inference Models}} through {{Semantic Fragments}}},
  booktitle = {{{AAAI}} 2020},
  author = {Richardson, Kyle and Hu, Hai and Moss, Lawrence S. and Sabharwal, Ashish},
  year = {2020},
  url = {http://arxiv.org/abs/1909.07521},
  archivePrefix = {arXiv}
}




@article{wallace2019nlp,
  title={{{Do NLP Models Know Numbers? Probing Numeracy in Embeddings}}},
  author={Wallace, Eric and Wang, Yizhong and Li, Sujian and Singh, Sameer and Gardner, Matt},
  journal={arXiv preprint arXiv:1909.07940},
  year={2019},
  url={https://arxiv.org/abs/1909.07940}
}

@article{ettinger2019bert,
  title={What {{BERT}} is not: Lessons from a new suite of psycholinguistic diagnostics for language models},
  author={Ettinger, Allyson},
  journal={arXiv preprint arXiv:1907.13528},
  year={2019}
}

@article{bacon2019does,
  title={{{Does BERT agree? Evaluating knowledge of structure dependence through agreement relations}}},
  author={Bacon, Geoff and Regier, Terry},
  journal={arXiv preprint arXiv:1908.09892},
  year={2019},
  url={https://arxiv.org/abs/1908.09892}
}

@inproceedings{lin2019open,
  title={{{Open Sesame: Getting inside BERT's Linguistic Knowledge}}},
  author={Lin, Yongjie and Tan, Yi Chern and Frank, Robert},
  booktitle={Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={241--253},
  year={2019},
  url={https://www.aclweb.org/anthology/W19-4825/}
}

@inproceedings{toneva2019interpreting,
  title={Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)},
  author={Toneva, Mariya and Wehbe, Leila},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14928--14938},
  year={2019}
}


@inproceedings{mccoy2019right,
  title = {Right for the {{Wrong Reasons}}: {{Diagnosing Syntactic Heuristics}} in {{Natural Language Inference}}},
  shorttitle = {Right for the {{Wrong Reasons}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {McCoy, Tom and Pavlick, Ellie and Linzen, Tal},
  year = {2019},
  pages = {3428--3448},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1334},
  url = {https://www.aclweb.org/anthology/P19-1334}
}





@inproceedings{ZhouZhangEtAl_2020_Evaluating_Commonsense_in_Pre-trained_Language_Models,
  title = {Evaluating {{Commonsense}} in {{Pre}}-{{Trained}} {{Language Models}}},
  booktitle = {{{AAAI}} 2020},
  author = {Zhou, Xuhui and Zhang, Yue and Cui, Leyang and Huang, Dandan},
  year = {2020},
  url = {https://arxiv.org/abs/1911.11931}
}


@inproceedings{zellers2019hellaswag,
  title={{{HellaSwag: Can a Machine Really Finish Your Sentence?}}},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4791--4800},
  year={2019},
  url={https://www.aclweb.org/anthology/P19-1472/}
}

@inproceedings{reif2019visualizing,
  title={Visualizing and Measuring the Geometry of {{BERT}}},
  author={Reif, Emily and Yuan, Ann and Wattenberg, Martin and Viegas, Fernanda B and Coenen, Andy and Pearce, Adam and Kim, Been},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8592--8600},
  year={2019}
}


@inproceedings{ethayarajh2019contextual,
  title = {How {{Contextual}} Are {{Contextualized Word Representations}}? {{Comparing}} the {{Geometry}} of {{BERT}}, {{ELMo}}, and {{GPT}}-2 {{Embeddings}}},
  shorttitle = {How {{Contextual}} Are {{Contextualized Word Representations}}?},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Ethayarajh, Kawin},
  year = {2019},
  pages = {55--65},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  url = {https://www.aclweb.org/anthology/D19-1006}
}





@inproceedings{pires2019multilingual,
  ids = {PiresSchlingerEtAl\_2019\_How\_Multilingual\_is\_Multilingual\_BERT},
  title = {How {{Multilingual}} Is {{Multilingual BERT}}?},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Pires, Telmo and Schlinger, Eva and Garrette, Dan},
  year = {2019},
  pages = {4996--5001},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1493},
  url = {https://www.aclweb.org/anthology/P19-1493}
}




@article{wu2019beto,
  title={{{Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT}}},
  author={Wu, Shijie and Dredze, Mark},
  journal={arXiv preprint arXiv:1904.09077},
  year={2019},
  url={https://arxiv.org/abs/1904.09077}
}

@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}



@inproceedings{hao2019visualizing,
  title = {Visualizing and {{Understanding}} the {{Effectiveness}} of {{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Hao, Yaru and Dong, Li and Wei, Furu and Xu, Ke},
  year = {2019},
  pages = {4143--4152},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1424},
  url = {https://www.aclweb.org/anthology/D19-1424/},
  archivePrefix = {arXiv}
}




@article{liu2019roberta,
  title={{{RoBERTa}}: A robustly optimized {{BERT}} pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{wu2016google,
  title={{{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}}},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016},
  url={https://arxiv.org/abs/1609.08144}
}

@inproceedings{zhu2015aligning,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}

@article{you2019large,
  title={{{Large Batch Optimization for Deep Learning: Training BERT in 76 Minutes}}},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  volume={1},
  number={5},
  year={2019},
  url={https://openreview.net/forum?id=Syx4wnEtvH}
}

@inproceedings{yang2019xlnet,
  title={{{XLNet}}: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={5754--5764},
  year={2019}
}

@article{zhou2019improving,
  title={{{Improving BERT Fine-tuning with Embedding Normalization}}},
  author={Zhou, Wenxuan and Du, Junyi and Ren, Xiang},
  journal={arXiv preprint arXiv:1911.03918},
  year={2019},
  url={https://arxiv.org/abs/1911.03918}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of {{BERT}}: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{jiao2019tinybert,
  title={{{TinyBERT: Distilling BERT for natural language understanding}}},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  journal={arXiv preprint arXiv:1909.10351},
  year={2019},
  url={https://arxiv.org/abs/1909.10351}
}


@inproceedings{lan2019albert,
  title = {{{ALBERT}}: {{A Lite BERT}} for {{Self}}-Supervised {{Learning}} of {{Language Representations}}},
  shorttitle = {{{ALBERT}}},
  booktitle = {{{ICLR}} 2020},
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  year = {2020},
  url = {https://openreview.net/forum?id=H1eA7AEtvS}
}




@article{tang2019distilling,
  title={{{Distilling Task-Specific Knowledge from BERT into Simple Neural Networks}}},
  author={Tang, Raphael and Lu, Yao and Liu, Linqing and Mou, Lili and Vechtomova, Olga and Lin, Jimmy},
  journal={arXiv preprint arXiv:1903.12136},
  year={2019},
  url={https://arxiv.org/abs/1903.12136}
}

@inproceedings{sun2019patient,
  title={{{Patient Knowledge Distillation for BERT Model Compression}}},
  author={Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={4314--4323},
  year={2019},
  url={https://www.aclweb.org/anthology/D19-1441/}
}

@article{zhao2019extreme,
  title={{{Extreme Language Model Compression with Optimal Subwords and Shared Projections}}},
  author={Zhao, Sanqiang and Gupta, Raghav and Song, Yang and Zhou, Denny},
  journal={arXiv preprint arXiv:1909.11687},
  year={2019},
  url={https://arxiv.org/abs/1909.11687}
}


@inproceedings{WangSinghEtAl_2018_GLUE_A_Multi-Task_Benchmark_and_Analysis_Platform_for_Natural_Language_Understanding,
  title = {{{GLUE}}: {{A Multi}}-{{Task}} {{Benchmark}} and {{Analysis Platform}} for {{Natural Language Understanding}}},
  shorttitle = {{{GLUE}}},
  booktitle = {Proceedings of the 2018 {{EMNLP Workshop BlackboxNLP}}: {{Analyzing}} and {{Interpreting Neural Networks}} for {{NLP}}},
  author = {Wang, Alex and Singh, Amapreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  year = {2018},
  pages = {353--355},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  url = {http://aclweb.org/anthology/W18-5446},
  archivePrefix = {arXiv}
}





@inproceedings{ForbesHoltzmanEtAl_Do_Neural_Language_Representations_Learn_Physical_Commonsense,
  title = {Do {{Neural Language Representations Learn Physical Commonsense}}?},
  booktitle = {Proceedings of the 41st {{Annual Conference}} of the {{Cognitive Science Society}} ({{CogSci}} 2019)},
  author = {Forbes, Maxwell and Holtzman, Ari and Choi, Yejin},
  url = {https://arxiv.org/pdf/1908.02899.pdf},
  year = {2019},
  pages = {7},
}




@article{aguilar2019knowledge,
  title={{{Knowledge Distillation from Internal Representations}}},
  author={Aguilar, Gustavo and Ling, Yuan and Zhang, Yu and Yao, Benjamin and Fan, Xing and Guo, Edward},
  journal={arXiv preprint arXiv:1910.03723},
  year={2019},
  url = {https://arxiv.org/abs/1910.03723}
}

@article{shen2019q,
  title={{{Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT}}},
  author={Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1909.05840},
  year={2019},
  url={https://arxiv.org/abs/1909.05840}
}

@article{zafrir2019q8bert,
  title={{{Q8BERT: Quantized 8bit BERT}}},
  author={Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe},
  journal={arXiv preprint arXiv:1910.06188},
  year={2019},
  url={https://arxiv.org/abs/1910.06188}
}

@inproceedings{gong2019efficient,
  title={Efficient training of {{BERT}} by progressively stacking},
  author={Gong, Linyuan and He, Di and Li, Zhuohan and Qin, Tao and Wang, Liwei and Liu, Tieyan},
  booktitle={International Conference on Machine Learning},
  url = {http://proceedings.mlr.press/v97/gong19a/gong19a.pdf},
  pages={2337--2346},
  year={2019}
}

@article{sunmobilebert,
  title={{{MobileBERT: Task-Agnostic Compression of BERT for Resource Limited Devices}}},
  author={Sun, Zhiqing and Yu, Hongkun and Song, Xiaodan and Liu, Renjie and Yang, Yiming and Zhou, Denny},
  url={https://www.cs.cmu.edu/~zhiqings/files/MobileBERT.pdf}
}

@article{turc2019well,
  title={{{Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation}}},
  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1908.08962},
  year={2019},
  url={https://arxiv.org/abs/1908.08962}
}

@article{xu2020bert,
  title={{{BERT-of-Theseus: Compressing BERT by Progressive Module Replacing}}},
  author={Xu, Canwen and Zhou, Wangchunshu and Ge, Tao and Wei, Furu and Zhou, Ming},
  journal={arXiv preprint arXiv:2002.02925},
  year={2020},
  url={https://arxiv.org/abs/2002.02925}
}

@article{tsai2019small,
  title={{{Small and Practical BERT Models for Sequence Labeling}}},
  author={Tsai, Henry and Riesa, Jason and Johnson, Melvin and Arivazhagan, Naveen and Li, Xin and Archer, Amelia},
  journal={arXiv preprint arXiv:1909.00100},
  year={2019},
  url={https://arxiv.org/abs/1909.00100}
}


@inproceedings{hinton2015distilling,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  booktitle = {Deep {{Learning}} and {{Representation Learning Workshop}}: {{NIPS}} 2014},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  year = {2014},
  url = {http://www.dlworkshop.org/accepted-papers},
  archivePrefix = {arXiv}
}




@article{kao2020further,
  title={Further Boosting {{BERT}}-based Models by Duplicating Existing Layers: Some Intriguing Phenomena inside {{BERT}}},
  author={Kao, Wei-Tsung and Wu, Tsung-Han and Chi, Po-Han and Hsieh, Chun-Cheng and Lee, Hung-Yi},
  url = {https://arxiv.org/abs/2001.09309},
  journal={arXiv preprint arXiv:2001.09309},
  year={2020}
}

@article{conneau2019unsupervised,
  title={Unsupervised cross-lingual representation learning at scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1911.02116},
  year={2019}
}

@article{roberts2020much,
  title={{{How Much Knowledge Can You Pack Into the Parameters of a Language Model?}}},
  author={Roberts, Adam and Raffel, Colin and Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.08910},
  year={2020},
  url={https://arxiv.org/abs/2002.08910}
}

@article{mccarley2019pruning,
  title={{{Pruning a BERT-based Question Answering Model}}},
  author={McCarley, JS},
  journal={arXiv preprint arXiv:1910.06360},
  year={2019},
  url={https://arxiv.org/abs/1910.06360}
}

@article{tian2019waldorf,
  title={{{WaLDORf: Wasteless Language-model Distillation On Reading-comprehension}}},
  author={Tian, James Yi and Kreuzer, Alexander P and Chen, Pai-Hung and Will, Hans-Martin},
  journal={arXiv preprint arXiv:1912.06638},
  year={2019},
  url={https://arxiv.org/abs/1912.06638}
}

@article{goyal2020power,
  title={PoWER-BERT: Accelerating {{BERT}} inference for Classification Tasks},
  author={Goyal, Saurabh and Choudhary, Anamitra Roy and Chakaravarthy, Venkatesan and ManishRaje, Saurabh and Sabharwal, Yogish and Verma, Ashish},
  url = {https://arxiv.org/abs/2001.08950},
  journal={arXiv preprint arXiv:2001.08950},
  year={2020}
}

@article{gordon2020compressing,
  title={Compressing {{BERT}}: Studying the Effects of Weight Pruning on Transfer Learning},
  author={Gordon, Mitchell A and Duh, Kevin and Andrews, Nicholas},
  url = {https://arxiv.org/abs/2002.08307},
  journal={arXiv preprint arXiv:2002.08307},
  year={2020}
}

@article{wang2020minilm,
  title={{{MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers}}},
  author={Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
  journal={arXiv preprint arXiv:2002.10957},
  year={2020},
  url={https://arxiv.org/abs/2002.10957}
}

@article{ganesh2020compressing,
  title={Compressing Large-Scale Transformer-Based Models: A Case Study on {{BERT}}},
  author={Ganesh, Prakhar and Chen, Yao and Lou, Xin and Khan, Mohammad Ali and Yang, Yin and Chen, Deming and Winslett, Marianne and Sajjad, Hassan and Nakov, Preslav},
  url = {https://arxiv.org/abs/2002.11985},
  journal={arXiv preprint arXiv:2002.11985},
  year={2020}
}

@article{li2020train,
  title={Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers},
  author={Li, Zhuohan and Wallace, Eric and Shen, Sheng and Lin, Kevin and Keutzer, Kurt and Klein, Dan and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2002.11794},
  url = {https://arxiv.org/abs/2002.11794},
  year={2020}
}


@misc{turing-nlg,
  title={Turing-{{NLG}}: A 17-billion-parameter Language Model by Microsoft},
  author={Microsoft},
  url={https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/},
  year={2020}
}

@article{jiang2019smart,
  title={{{SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization}}},
  author={Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Zhao, Tuo},
  journal={arXiv preprint arXiv:1911.03437},
  year={2019},
  url={https://arxiv.org/abs/1911.03437}
}

@article{lee2019mixout,
  title={Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models},
  author={Lee, Cheolhyoung and Cho, Kyunghyun and Kang, Wanmo},
  journal={arXiv preprint arXiv:1909.11299},
  url = {https://arxiv.org/abs/1909.11299},
  year={2019}
}

@article{schmidt2020bert,
  title={{{BERT as a Teacher: Contextual Embeddings for Sequence-Level Reward}}},
  author={Schmidt, Florian and Hofmann, Thomas},
  journal={arXiv preprint arXiv:2003.02738},
  year={2020},
  url={https://arxiv.org/abs/2003.02738}
}


@article{AssenmacherHeumann_2020_On_comparability_of_Pre-trained_Language_Models,
  title = {On the Comparability of {{Pre}}-{{Trained}} {{Language Models}}},
  author = {A{\ss}enmacher, Matthias and Heumann, Christian},
  year = {2020},
  url = {http://arxiv.org/abs/2001.00781},
  archivePrefix = {arXiv},
  journal = {arXiv:2001.00781 [cs, stat]},
  primaryClass = {cs, stat}
}


@inproceedings{voita2019bottom,
  title={{{The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives}}},
  author={Voita, Elena and Sennrich, Rico and Titov, Ivan},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={4387--4397},
  year={2019},
  url={https://arxiv.org/abs/1909.01380}
}

@article{baan2019understanding,
  title={{{Understanding Multi-Head Attention in Abstractive Summarization}}},
  author={Baan, Joris and ter Hoeve, Maartje and van der Wees, Marlies and Schuth, Anne and de Rijke, Maarten},
  journal={arXiv preprint arXiv:1911.03898},
  year={2019},
  url={https://arxiv.org/abs/1911.03898}
}

@book{em:86,
  editor  = "Engelmore, Robert and Morgan, Anthony",
  title   = "Blackboard Systems",
  year    = 1986,
  address = "Reading, Mass.",
  publisher = "Addison-Wesley",
}

@inproceedings{c:83,
  author  = "Clancey, William J.",
  year    = 1983,
  title   = "{Communication, Simulation, and Intelligent
Agents: Implications of Personal Intelligent Machines
for Medical Education}",
  booktitle="Proceedings of the Eighth International Joint Conference on Artificial Intelligence {(IJCAI-83)}", 
  pages   = "556-560",
  address = "Menlo Park, Calif",
  publisher = "{IJCAI Organization}",
}
@inproceedings{c:84,
  author  = "Clancey, William J.",
  year    = 1984,
  title   = "{Classification Problem Solving}",
  booktitle = "Proceedings of the Fourth National 
              Conference on Artificial Intelligence",
  pages   = "45-54",
  address = "Menlo Park, Calif.",
  publisher="AAAI Press",
}
@article{r:80,
  author = {Robinson, Arthur L.},
  title = {New Ways to Make Microcircuits Smaller},
  volume = {208},
  number = {4447},
  pages = {1019--1022},
  year = {1980},
  doi = {10.1126/science.208.4447.1019},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075},
  URL = {https://science.sciencemag.org/content/208/4447/1019},
  eprint = {https://science.sciencemag.org/content/208/4447/1019.full.pdf},
  journal = {Science},
}
@article{r:80x,
  author  = "Robinson, Arthur L.",
  year    = 1980,
  title   = "{New Ways to Make Microcircuits Smaller---Duplicate Entry}",
  journal = "Science",
  volume  =  208,
  pages   = "1019-1026",
}
@article{hcr:83,
title = {Strategic explanations for a diagnostic consultation system},
journal = {International Journal of Man-Machine Studies},
volume = {20},
number = {1},
pages = {3-19},
year = {1984},
issn = {0020-7373},
doi = {https://doi.org/10.1016/S0020-7373(84)80003-6},
url = {https://www.sciencedirect.com/science/article/pii/S0020737384800036},
author = {Diane Warner Hasling and William J. Clancey and Glenn Rennels},
abstract = {This article examines the problem of automatte explanation of reasoning, especially as it relates to expert systems. By explanation we mean the ability of a program to discuss what it is doing in some understandable way. We first present a general framework in which to view explanation and review some of the research done in this area. We then focus on the explanation system for NEOMYCIN, a medical consultation program. A consultation program interactively helps a user to solve a problem. Our goal is to have NEOMYCIN explain its problem-solving strategies. An explanation of strategy describes the plan the program is using to reach a solution. Such an explanation is usually concrete, referring to aspects of the current problem situation. Abstract explanations articulate a general principle, which can be applied in different situations; such explanations are useful in teaching and in explaining by analogy. We describe the aspects of NEOMYCIN that make abstract strategic explanations possible—the representation of strategic knowledge explicitly and separately from domain knowledge— and demonstrate how this representation can be used to generate explanations.}
}
@article{hcrt:83,
  author  = "Hasling, Diane Warner and Clancey, William J. and Rennels, Glenn R. and Test, Thomas",
  year    = 1983,
  title   = "{Strategic Explanations in Consultation---Duplicate}",
  journal = "The International Journal of Man-Machine Studies",
  volume  = 20,
  number  = 1,
  pages   = "3-19",
}
@techreport{r:86,
  author  = "Rice, James",
  year    = 1986,
  title   = "{Poligon: A System for Parallel Problem Solving}",
  type    = "Technical Report", 
  number  = "KSL-86-19", 
  institution = "Dept.\ of Computer Science, Stanford Univ.",
}
@phdthesis{c:79,
  author  = "Clancey, William J.",
  year    = 1979,
  title   = "{Transfer of Rule-Based Expertise
through a Tutorial Dialogue}",
  type    = "{Ph.D.} diss.",
  school  = "Dept.\ of Computer Science, Stanford Univ.",
  address = "Stanford, Calif.",
}
@unpublished{c:21,
  author  = "Clancey, William J.",
  title   = "{The Engineering of Qualitative Models}",
  year    = 2021,
  note    = "Forthcoming",
}
@misc{c:22,
      title={Crime and punishment in scientific research}, 
      author={Mathieu Bouville},
      year={2008},
      eprint={0803.4058},
      archivePrefix={arXiv},
      primaryClass={physics.soc-ph}
}
@misc{c:23,
  title        = "Pluto: The 'Other' Red Planet",
  author       = "{NASA}",
  howpublished = "\url{https://www.nasa.gov/nh/pluto-the-other-red-planet}",
  year         = 2015,
  note         = "Accessed: 2018-12-06"
}

@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}
