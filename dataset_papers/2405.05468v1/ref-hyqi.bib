% books
@book{vapnik1995learning,
    author = {Vapnik, Vladimir N.},
    title = {The Nature of Statistical Learning Theory},
    year = {1995},
    isbn = {0387945598},
    publisher = {Springer-Verlag},
    address = {Berlin, Heidelberg}
}

@book{quinonero2022dataset,
  title={Dataset Shift in Machine Learning},
  author={Quinonero-Candela, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D},
  year={2022},
  publisher={MIT Press}
}


@article{taori2020measuring,
  title={Measuring robustness to natural distribution shifts in image classification},
  author={Taori, Rohan and Dave, Achal and Shankar, Vaishaal and Carlini, Nicholas and Recht, Benjamin and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18583--18599},
  year={2020}
}

@inproceedings{koh2021wilds,
  title={Wilds: A benchmark of in-the-wild distribution shifts},
  author={Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and others},
  booktitle={International Conference on Machine Learning},
  pages={5637--5664},
  year={2021},
  organization={PMLR}
}

@incollection{kuhn2019wasserstein,
  title={Wasserstein distributionally robust optimization: Theory and applications in machine learning},
  author={Kuhn, Daniel and Esfahani, Peyman Mohajerin and Nguyen, Viet Anh and Shafieezadeh-Abadeh, Soroosh},
  booktitle={Operations research \& management science in the age of analytics},
  pages={130--166},
  year={2019},
  publisher={Informs}
}



@article{kumar2023policy,
  title={Policy Gradient for s-Rectangular Robust Markov Decision Processes},
  author={Kumar, Navdeep and Derman, Esther and Geist, Matthieu and Levy, Kfir and Mannor, Shie},
  journal={arXiv preprint arXiv:2301.13589},
  year={2023}
}

@inproceedings{wang2022policy,
  title={Policy gradient method for robust reinforcement learning},
  author={Wang, Yue and Zou, Shaofeng},
  booktitle={International Conference on Machine Learning},
  pages={23484--23526},
  year={2022},
}


@inproceedings{grand2021scalable,
  title={Scalable first-order methods for robust MDPs},
  author={Grand-Cl{\'e}ment, Julien and Kroer, Christian},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={12086--12094},
  year={2021}
}


@article{wang2022convergence,
  title={On the Convergence of Policy Gradient in Robust MDPs},
  author={Wang, Qiuhao and Ho, Chin Pang and Petrik, Marek},
  journal={arXiv preprint arXiv:2212.10439},
  year={2022}
}

@book{boucheron2013concentration,
  title={Concentration Inequalities: A Nonasymptotic Theory of Independence},
  author={Boucheron, S. and Lugosi, G. and Massart, P.},
  isbn={9780199535255},
  lccn={2012277339},
  url={https://books.google.com/books?id=koNqWRluhP0C},
  year={2013},
  publisher={OUP Oxford}
}

@book{habib1998probabilistic,
  title={Probabilistic Methods for Algorithmic Discrete Mathematics},
  author={Habib, M. and McDiarmid, C. and Ramirez-Alfonsin, J. and Reed, B. and Graham, R.L. and Korte, B. and Lov{\'a}sz, L. and Wigderson, A. and Ziegler, G.M.},
  isbn={9783540646228},
  lccn={98036217},
  series={Algorithms and Combinatorics},
  url={https://books.google.com/books?id=Vx7FJy5JlcEC},
  year={1998},
  publisher={Springer}
}

@article{chen2020distributionally,
  title={Distributionally robust learning},
  author={Chen, Ruidi and Paschalidis, Ioannis Ch and others},
  journal={Foundations and Trends{\textregistered} in Optimization},
  volume={4},
  number={1-2},
  pages={1--243},
  year={2020},
  publisher={Now Publishers, Inc.}
}

@book{basu2011statistical,
  title={Statistical Inference: The Minimum Distance Approach},
  author={Basu, A. and Shioya, H. and Park, C.},
  isbn={9781420099669},
  lccn={2011021886},
  series={Chapman \& Hall/CRC Monographs on Statistics \& Applied Probability},
  url={https://books.google.com/books?id=C-lOIgDp5\_0C},
  year={2011},
  publisher={CRC Press}
}

@article{gibbs2002choosing,
  title={On choosing and bounding probability metrics},
  author={Gibbs, Alison L and Su, Francis Edward},
  journal={International statistical review},
  volume={70},
  number={3},
  pages={419--435},
  year={2002},
  publisher={Wiley Online Library}
}


% conference


@InProceedings{xu-panaganti-2023samplecomplexity,
  title = 	 {Improved Sample Complexity Bounds for  Distributionally Robust Reinforcement Learning },
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  author =       {Xu$^{*}$, Zaiyan and Panaganti$^{*}$, Kishan and Kalathil, Dileep},
  publisher = 	 {Conference on Artificial Intelligence and Statistics},
  year = 	 {2023}
}



% DRO
@article{shapiro-2017-dro,
  title={Distributionally robust stochastic programming},
  author={Shapiro, Alexander},
  journal={SIAM Journal on Optimization},
  volume={27},
  number={4},
  pages={2258--2275},
  year={2017},
  publisher={SIAM}
}

@article{blanchet2019dro, 
  title={Robust Wasserstein profile inference and applications to machine learning}, 
  volume={56},
  DOI={10.1017/jpr.2019.49},
  number={3},
  journal={Journal of Applied Probability},
  publisher={Cambridge University Press},
  author={Blanchet, Jose and Kang, Yang and Murthy, Karthyek}, 
  year={2019}, 
  pages={830–857}
}


@article{gao-2022-distributionally,
  title={Distributionally robust stochastic optimization with Wasserstein distance},
  author={Gao, Rui and Kleywegt, Anton},
  journal={Mathematics of Operations Research},
  year={2022},
  publisher={INFORMS}
}

@inproceedings{moses2011further,
  title={Further results on geometric properties of a family of relative entropies},
  author={Moses, Ashok Kumar and Sundaresan, Rajesh},
  booktitle={2011 IEEE International Symposium on Information Theory Proceedings},
  pages={1940--1944},
  year={2011},
}

@article{cover1991information,
  title={Information theory and the stock market},
  author={Cover, Thomas M and Thomas, Joy A},
  journal={Elements of Information Theory. Wiley Inc., New York},
  pages={543--556},
  year={1991}
}

@book{bertsekas2019reinforcement,
  title={Reinforcement learning and optimal control},
  author={Bertsekas, Dimitri},
  year={2019},
  publisher={Athena Scientific}
}


@article{namkoong2016stochastic,
  title={Stochastic gradient methods for distributionally robust optimization with f-divergences},
  author={Namkoong, Hongseok and Duchi, John C},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}


@article{duchi2018learning,
  title={Learning models with uniform performance via distributionally robust optimization},
  author={Duchi, John and Namkoong, Hongseok},
  journal={arXiv preprint arXiv:1810.08750},
  year={2018}
}

@article{shapiro2017distributionally,
  title={Distributionally robust stochastic programming},
  author={Shapiro, Alexander},
  journal={SIAM Journal on Optimization},
  volume={27},
  number={4},
  pages={2258--2275},
  year={2017},
  publisher={SIAM}
}

@article{bertsimas2018dro,
author = {Bertsimas, Dimitris and Gupta, Vishal and Kallus, Nathan},
title = {Data-Driven Robust Optimization},
year = {2018},
issue_date = {February  2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {167},
number = {2},
issn = {0025-5610},
url = {https://doi.org/10.1007/s10107-017-1125-8},
doi = {10.1007/s10107-017-1125-8},
abstract = {The last decade witnessed an explosion in the availability of data for operations research applications. Motivated by this growing availability, we propose a novel schema for utilizing data to design uncertainty sets for robust optimization using statistical hypothesis tests. The approach is flexible and widely applicable, and robust optimization problems built from our new sets are computationally tractable, both theoretically and practically. Furthermore, optimal solutions to these problems enjoy a strong, finite-sample probabilistic guarantee whenever the constraints and objective function are concave in the uncertainty. We describe concrete procedures for choosing an appropriate set for a given application and applying our approach to multiple uncertain constraints. Computational evidence in portfolio management and queueing confirm that our data-driven sets significantly outperform traditional robust optimization techniques whenever data are available.},
journal = {Math. Program.},
month = {feb},
pages = {235–292},
numpages = {58},
keywords = {80M50 (Optimization: Operations research, Robust optimization, Data-driven optimization, 62H15 (Multivariate Analysis: Hypothesis Testing), Chance-constraints, Hypothesis testing, mathematical programming)}
}


@InProceedings{hu2018drsl,
  title = 	 {Does Distributionally Robust Supervised Learning Give Robust Classifiers?},
  author =       {Hu, Weihua and Niu, Gang and Sato, Issei and Sugiyama, Masashi},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2029--2037},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/hu18a/hu18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/hu18a.html},
  abstract = 	 {Distributionally Robust Supervised Learning (DRSL) is necessary for building reliable machine learning systems. When machine learning is deployed in the real world, its performance can be significantly degraded because test data may follow a different distribution from training data. DRSL with f-divergences explicitly considers the worst-case distribution shift by minimizing the adversarially reweighted training loss. In this paper, we analyze this DRSL, focusing on the classification scenario. Since the DRSL is explicitly formulated for a distribution shift scenario, we naturally expect it to give a robust classifier that can aggressively handle shifted distributions. However, surprisingly, we prove that the DRSL just ends up giving a classifier that exactly fits the given training distribution, which is too pessimistic. This pessimism comes from two sources: the particular losses used in classification and the fact that the variety of distributions to which the DRSL tries to be robust is too wide. Motivated by our analysis, we propose simple DRSL that overcomes this pessimism and empirically demonstrate its effectiveness.}
}



% article
@phdthesis {kakade-thesis,
	title = {On the Sample Complexity of Reinforcement Learning},
	year = {2003},
	type = {PhD thesis},
	abstract = {
	This thesis is a detailed investigation into the following question: how much data must an agent collect in order to perform "reinforcement learning" successfully? This question is analogous to the classical issue of the sample complexity in supervised learning, but is harder because of the increased realism of the reinforcement learning setting. This thesis summarizes recent sample complexity results in the reinforcement learning literature and builds on these results to provide novel algorithms with strong performance guarantees. We focus on a variety of reasonable performance criteria and sampling models by which agents may access the environment. For instance, in a policy search setting, we consider the problem of how much simulated experience is required to reliably choose a "good" policy among a restricted class of policies II (as in Kearns, Mansour, and Ng 2000]). In a more online setting, we consider the case in which an agent is placed in an environment and must follow one unbroken chain of experience with no access to "offline" simulation (as in Kearns and Singh 1998]). We build on the sample based algorithms suggested by Kearns, Mansour, and Ng 2000]. Their sample complexity bounds have no dependence on the size of the state space, an exponential dependence on the planning horizon time, and linear dependence on the complexity of II. We suggest novel algorithms with more restricted guarantees whose sample complexities are again independent of the size of the state space and depend linearly on the complexity of the policy class II, but have only a polynomial dependence on the horizon time. We pay particular attention to the tradeoffs made by such algorithms.
    },
	author = {Sham M. Kakade},
	school = {University of College London}
}

@article{panaganti-rfqi,
  title={Robust Reinforcement Learning using Offline Data},
  author= {Panaganti, Kishan and Xu, Zaiyan and Kalathil, Dileep and Ghavamzadeh, Mohammad},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}


@InProceedings{wong2018adversarial,
  title = 	 {Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope},
  author =       {Wong, Eric and Kolter, Zico},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5286--5295},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/wong18a/wong18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/wong18a.html},
  abstract = 	 {We propose a method to learn deep ReLU-based classifiers that are provably robust against norm-bounded adversarial perturbations on the training data. For previously unseen examples, the approach is guaranteed to detect all adversarial examples, though it may flag some non-adversarial examples as well. The basic idea is to consider a convex outer approximation of the set of activations reachable through a norm-bounded perturbation, and we develop a robust optimization procedure that minimizes the worst case loss over this outer region (via a linear program). Crucially, we show that the dual problem to this linear program can be represented itself as a deep network similar to the backpropagation network, leading to very efficient optimization approaches that produce guaranteed bounds on the robust loss. The end result is that by executing a few more forward and backward passes through a slightly modified version of the original network (though possibly with much larger batch sizes), we can learn a classifier that is provably robust to any norm-bounded adversarial attack. We illustrate the approach on a number of tasks to train classifiers with robust adversarial guarantees (e.g. for MNIST, we produce a convolutional classifier that provably has less than 5.8% test error for any adversarial attack with bounded $\ell_\infty$ norm less than $\epsilon = 0.1$).}
}



@InProceedings{cohan2019randomsmoothing,
  title = 	 {Certified Adversarial Robustness via Randomized Smoothing},
  author =       {Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1310--1320},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/cohen19c/cohen19c.pdf},
  url = 	 {https://proceedings.mlr.press/v97/cohen19c.html},
  abstract = 	 {We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the L2 norm. While this "randomized smoothing" technique has been proposed before in the literature, we are the first to provide a tight analysis, which establishes a close connection between L2 robustness and Gaussian noise. We use the technique to train an ImageNet classifier with e.g. a certified top-1 accuracy of 49% under adversarial perturbations with L2 norm less than 0.5 (=127/255). Smoothing is the only approach to certifiably robust classification which has been shown feasible on full-resolution ImageNet. On smaller-scale datasets where competing approaches to certified L2 robustness are viable, smoothing delivers higher certified accuracies. The empirical success of the approach suggests that provable methods based on randomization at prediction time are a promising direction for future research into adversarially robust classification.}
}


@article {csisz-divergence,
    AUTHOR = {Csisz\'{a}r, Imre},
     TITLE = {Eine informationstheoretische {U}ngleichung und ihre
              {A}nwendung auf den {B}eweis der {E}rgodizit\"{a}t von
              {M}arkoffschen {K}etten},
   JOURNAL = {Magyar Tud. Akad. Mat. Kutat\'{o} Int. K\"{o}zl.},
  FJOURNAL = {A Magyar Tudom\'{a}nyos Akad\'{e}mia. Matematikai Kutat\'{o} Int\'{e}zet\'{e}nek
              K\"{o}zlem\'{e}nyei},
    VOLUME = {8},
      YEAR = {1963},
     PAGES = {85--108},
      ISSN = {0541-9514},
   MRCLASS = {60.65},
  MRNUMBER = {164374},
MRREVIEWER = {S. Kullback},
}

@misc{yang-2022,
  doi = {10.48550/ARXIV.2105.03863},
  url = {https://arxiv.org/abs/2105.03863},
  author = {Yang, Wenhao and Zhang, Liangyu and Zhang, Zhihua},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Towards Theoretical Understandings of Robust Markov Decision Processes: Sample Complexity and Asymptotics},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{shi2022distributionally,
  title={Distributionally Robust Model-Based Offline Reinforcement Learning with Near-Optimal Sample Complexity},
  author={Shi, Laixi and Chi, Yuejie},
  journal={arXiv preprint arXiv:2208.05767},
  year={2022}
}

@misc{duchi2022distributionally,
      title={Distributionally Robust Losses for Latent Covariate Mixtures}, 
      author={John Duchi and Tatsunori Hashimoto and Hongseok Namkoong},
      year={2022},
      eprint={2007.13982},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
  
@article{dann2015sample,
  title={Sample complexity of episodic fixed-horizon reinforcement learning},
  author={Dann, Christoph and Brunskill, Emma},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  year={2015}
}


@inproceedings{MaurerP09,
  author    = {Andreas Maurer and
               Massimiliano Pontil},
  title     = {Empirical Bernstein Bounds and Sample-Variance Penalization},
  booktitle = {{COLT} 2009 - The 22nd Conference on Learning Theory, Montreal, Quebec,
               Canada, June 18-21, 2009},
  year      = {2009},
}

% Simulation

@ARTICLE{scipy20,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@INPROCEEDINGS{emanuel2012mujoco,
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
  title={MuJoCo: A physics engine for model-based control}, 
  year={2012},
  volume={},
  number={},
  pages={5026-5033},
  doi={10.1109/IROS.2012.6386109}
}

@misc{antonin2020rlzoo3,
  author = {Raffin, Antonin},
  title = {RL Baselines3 Zoo},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/DLR-RM/rl-baselines3-zoo}},
}

@article{kraft1988software,
  title={A software package for sequential quadratic programming},
  author={Kraft, Dieter},
  journal={Forschungsbericht- Deutsche Forschungs- und Versuchsanstalt fur Luft- und Raumfahrt},
  year={1988}
}


% Robust Tabular setting


@inproceedings{zhou2021finite,
  title={Finite-Sample Regret Bound for Distributionally Robust Offline Tabular Reinforcement Learning},
  author={Zhou, Zhengqing and Bai, Qinxun and Zhou, Zhengyuan and Qiu, Linhai and Blanchet, Jose and Glynn, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3331--3339},
  year={2021},
}



%% Meta learning + LFA
@inproceedings{wang2020global,
  title={On the global optimality of model-agnostic meta-learning},
  author={Wang, Lingxiao and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={9837--9846},
  year={2020},
}

@inproceedings{kong2020meta,
  title={Meta-learning for mixed linear regression},
  author={Kong, Weihao and Somani, Raghav and Song, Zhao and Kakade, Sham and Oh, Sewoong},
  booktitle={International Conference on Machine Learning},
  pages={5394--5404},
  year={2020},
  organization={PMLR}
}

%% Imitation Learning + LFA
@inproceedings{arora2020provable,
  title={Provable representation learning for imitation learning via bi-level optimization},
  author={Arora, Sanjeev and Du, Simon and Kakade, Sham and Luo, Yuping and Saunshi, Nikunj},
  booktitle={International Conference on Machine Learning},
  pages={367--376},
  year={2020},
  organization={PMLR}
}

@inproceedings{abbeel2004apprenticeship,
  title={Apprenticeship learning via inverse reinforcement learning},
  author={Abbeel, Pieter and Ng, Andrew Y},
  booktitle={International Conference on Machine Learning},
  pages={1},
  year={2004}
}

@inproceedings{ziebart2008irl,
author = {Ziebart, Brian D. and Maas, Andrew and Bagnell, J. Andrew and Dey, Anind K.},
title = {Maximum Entropy Inverse Reinforcement Learning},
year = {2008},
isbn = {9781577353683},
publisher = {AAAI Press},
abstract = {Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods.We develop our technique in the context of modeling real-world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories.},
booktitle = {Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 3},
pages = {1433–1438},
numpages = {6},
location = {Chicago, Illinois},
series = {AAAI'08}
}

@inproceedings{ng2000irl,
author = {Ng, Andrew Y. and Russell, Stuart J.},
title = {Algorithms for Inverse Reinforcement Learning},
year = {2000},
isbn = {1558607072},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Seventeenth International Conference on Machine Learning},
pages = {663–670},
numpages = {8},
series = {ICML '00}
}

@inproceedings{bain1995bc,
  title={A Framework for Behavioural Cloning},
  author={Michael Bain and Claude Sammut},
  booktitle={Machine Intelligence 15},
  year={1995}
}

@inproceedings{finn2016irl,
author = {Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
title = {Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization},
year = {2016},
publisher = {JMLR.org},
abstract = {Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {49–58},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@inproceedings{xu2022receding,
title={Receding Horizon Inverse Reinforcement Learning},
author={Yiqing Xu and Wei Gao and David Hsu},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=CgkjJaKBvkX}
}








%% Offline RL + LFA


@article{kumar2020conservative,
  title={Conservative q-learning for offline reinforcement learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1179--1191},
  year={2020}
}

@inproceedings{buckman2020importance,
  title={The Importance of Pessimism in Fixed-Dataset Policy Optimization},
  author={Buckman, Jacob and Gelada, Carles and Bellemare, Marc G},
  booktitle={International Conference on Learning Representations},
  year={2020}
}



@inproceedings{yang2020reinforcement,
  title={Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound},
  author={Yang, Lin and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={10746--10756},
  year={2020},
  organization={PMLR}
}


@inproceedings{
	wang2021what,
	title={What are the Statistical Limits of Offline {\{}RL{\}} with Linear Function Approximation?},
	author={Ruosong Wang and Dean Foster and Sham M. Kakade},
	booktitle={International Conference on Learning Representations},
	year={2021},
	url={https://openreview.net/forum?id=30EvkP2aQLD}
}

@inproceedings{duan2020minimax,
	title={Minimax-optimal off-policy evaluation with linear function approximation},
	author={Duan, Yaqi and Jia, Zeyu and Wang, Mengdi},
	booktitle={International Conference on Machine Learning},
	pages={2701--2709},
	year={2020},
}



@inproceedings{pinto2017robust,
  title={Robust adversarial reinforcement learning},
  author={Pinto, Lerrel and Davidson, James and Sukthankar, Rahul and Gupta, Abhinav},
  booktitle={International Conference on Machine Learning},
  pages={2817--2826},
  year={2017},
}




@article{antos2008learning,
  title={Learning near-optimal policies with {B}ellman-residual minimization based fitted policy iteration and a single sample path},
  author={Antos, Andr{\'a}s and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  journal={Machine Learning},
  volume={71},
  number={1},
  pages={89--129},
  year={2008},
  publisher={Springer}
}


@article{liao2022batch,
  title={Batch policy learning in average reward Markov decision processes},
  author={Liao, Peng and Qi, Zhengling and Wan, Runzhe and Klasnja, Predrag and Murphy, Susan A},
  journal={The Annals of Statistics},
  volume={50},
  number={6},
  pages={3364--3387},
  year={2022},
  publisher={Institute of Mathematical Statistics}
}

@article{munos2007performance,
  title={Performance bounds in l\_p-norm for approximate value iteration},
  author={Munos, R{\'e}mi},
  journal={SIAM journal on control and optimization},
  volume={46},
  number={2},
  pages={541--561},
  year={2007},
  publisher={SIAM}
} 

@article{yang2019reinforcement,
  title={Reinforcement Learning in Feature Space: Matrix Bandit, Kernels, and Regret Bound},
  author={Yang, Lin F and Wang, Mengdi},
  journal={arXiv preprint arXiv:1905.10389},
  year={2019}
}



@inproceedings{paternain2018learning,
  title={Learning policies for {M}arkov decision processes in continuous spaces},
  author={Paternain, Santiago and Bazerque, Juan Andr{\'e}s and Small, Austin and Ribeiro, Alejandro},
  booktitle={2018 IEEE Conference on Decision and Control (CDC)},
  pages={4751--4758},
  year={2018},
  organization={IEEE}
}

@inproceedings{behzadian2019fast,
  title={Fast Feature Selection for Linear Value Function Approximation},
  author={Behzadian, Bahram and Gharatappeh, Soheil and Petrik, Marek},
  booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
  volume={29},
  number={1},
  pages={601--609},
  year={2019}
}

@article{chuchro2017game,
  title={Game playing with deep q-learning using openai gym},
  author={Chuchro, Robert and Gupta, Deepak},
  journal={Semantic Scholar},
  year={2017}
}


@inproceedings{lillicrapHPHETS15,
  author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
  title={Continuous control with deep reinforcement learning.},
  year={2016},
  booktitle={ICLR (Poster)}
}

@book{borkar2009stochastic,
  title={Stochastic approximation: a dynamical systems viewpoint},
  author={Borkar, Vivek S},
  volume={48},
  year={2009},
  publisher={Springer}
}

@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2137--2143},
  year={2020}
}

@inproceedings{jia2020model,
  title={Model-Based Reinforcement Learning with Value-Targeted Regression},
  author={Jia, Zeyu and Yang, Lin and Szepesvari, Csaba and Wang, Mengdi},
  year={2020}, pages = {666--686}, booktitle = {Proceedings of Machine Learning Research}
}



@inproceedings{yu2020mopo,
  title={MOPO: Model-based Offline Policy Optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

@inproceedings{kidambi2020morel,
  title={MOReL: Model-Based Offline Reinforcement Learning},
  author={Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}


@article{munos08a,
  author  = {R{{\'e}}mi Munos and Csaba Szepesv{{\'a}}ri},
  title   = {Finite-Time Bounds for Fitted Value Iteration},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {27},
  pages   = {815-857}
}

@inproceedings{liu2015finite,
  title={Finite-Sample Analysis of Proximal Gradient TD Algorithms.},
  author={Liu, Bo and Liu, Ji and Ghavamzadeh, Mohammad and Mahadevan, Sridhar and Petrik, Marek},
  booktitle={UAI},
  pages={504--513},
  year={2015},
  organization={Citeseer}
}

@inproceedings{kumar2019stabilizing,
  title={Stabilizing off-policy q-learning via bootstrapping error reduction},
  author={Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11784--11794},
  year={2019}
}


@article{deramo2020mushroomrl,
      title={MushroomRL: Simplifying Reinforcement Learning Research},
      author={D'Eramo, Carlo and Tateo, Davide and Bonarini, Andrea and Restelli, Marcello and Peters, Jan},
      journal={arXiv preprint arXiv:2001.01102},
      year={2020},
      url={https://github.com/MushroomRL/mushroom-rl}
}

@article{nedic2003least,
  title={Least squares policy evaluation algorithms with linear function approximation},
  author={Nedi{\'c}, A and Bertsekas, Dimitri P},
  journal={Discrete Event Dynamic Systems},
  volume={13},
  number={1-2},
  pages={79--110},
  year={2003},
  publisher={Springer}
}

@inproceedings{tsitsiklis1997analysis,
  title={Analysis of temporal-difference learning with function approximation},
  author={Tsitsiklis, John N and Van Roy, Benjamin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1075--1081},
  year={1997}
}




@article{wiesemann2013robust,
  title={Robust {M}arkov decision processes},
  author={Wiesemann, Wolfram and Kuhn, Daniel and Rustem, Ber{\c{c}}},
  journal={Mathematics of Operations Research},
  volume={38},
  number={1},
  pages={153--183},
  year={2013},
  publisher={INFORMS}
}

@inproceedings{xu2010distributionally,
  title={Distributionally robust {M}arkov decision processes},
  author={Xu, Huan and Mannor, Shie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2505--2513},
  year={2010}
}

@article{yu2015distributionally,
  title={Distributionally robust counterpart in {M}arkov decision processes},
  author={Yu, Pengqian and Xu, Huan},
  journal={IEEE Transactions on Automatic Control},
  volume={61},
  number={9},
  pages={2538--2543},
  year={2015},
  publisher={IEEE}
}


@inproceedings{tamar2014scaling,
  title={Scaling up robust MDPs using function approximation},
  author={Tamar, Aviv and Mannor, Shie and Xu, Huan},
  booktitle={International Conference on Machine Learning},
  pages={181--189},
  year={2014}
}


@inproceedings{lim2013reinforcement,
  title={Reinforcement learning in robust {M}arkov decision processes},
  author={Lim, Shiau Hong and Xu, Huan and Mannor, Shie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={701--709},
  year={2013}
}

@inproceedings{roy2017reinforcement,
  title={Reinforcement learning under model mismatch},
  author={Roy, Aurko and Xu, Huan and Pokutta, Sebastian},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3043--3052},
  year={2017}
}


@article{kaufman2013robust,
  title={Robust modified policy iteration},
  author={Kaufman, David L and Schaefer, Andrew J},
  journal={INFORMS Journal on Computing},
  volume={25},
  number={3},
  pages={396--410},
  year={2013},
  publisher={INFORMS}
}


@article{bertsekas2011approximate,
  title={Approximate policy iteration: A survey and some new methods},
  author={Bertsekas, Dimitri P},
  journal={Journal of Control Theory and Applications},
  volume={9},
  number={3},
  pages={310--335},
  year={2011},
  publisher={Springer}
}


@inproceedings{lim2019kernel,
  title={Kernel-based reinforcement learning in robust {M}arkov decision processes},
  author={Lim, Shiau Hong and Autef, Arnaud},
  booktitle={International Conference on Machine Learning},
  pages={3973--3981},
  year={2019}
}

@inproceedings{derman2018soft,
  title={Soft-robust actor-critic policy-gradient},
  author={Derman, Esther and Mankowitz, Daniel J and Mann, Timothy A and Mannor, Shie},
  booktitle={AUAI press for Association for Uncertainty in Artificial Intelligence},
  pages={208--218},
  year={2018}
}

@inproceedings{tessler2019action,
  title={Action Robust Reinforcement Learning and Applications in Continuous Control},
  author={Tessler, Chen and Efroni, Yonathan and Mannor, Shie},
  booktitle={International Conference on Machine Learning},
  pages={6215--6224},
  year={2019}
}

@inproceedings{
Mankowitz2020Robust,
title={Robust Reinforcement Learning for Continuous Control with Model Misspecification},
author={Daniel J. Mankowitz and Nir Levine and Rae Jeong and Abbas Abdolmaleki and Jost Tobias Springenberg and Yuanyuan Shi and Jackie Kay and Todd Hester and Timothy Mann and Martin Riedmiller},
booktitle={International Conference on Learning Representations},
year={2020}
}



@book{BerBook12,
  title={Dynamic programming and optimal control, Vol - 2},
  author={Bertsekas, Dimitri P},
  year={2012},
  publisher={Athena scientific Belmont, MA}
}

@book{Puterman05,
  title={{M}arkov Decision Processes: Discrete Stochastic Dynamic Programming},
  author={Puterman, Martin L},
  year={2005},
  publisher={John Wiley \& Sons, Inc., NJ}
}


@article{bertsekas2009projected,
  title={Projected equation methods for approximate solution of large linear systems},
  author={Bertsekas, Dimitri P and Yu, Huizhen},
  journal={Journal of Computational and Applied Mathematics},
  volume={227},
  number={1},
  pages={27--50},
  year={2009},
  publisher={Elsevier}
}

@article{lagoudakis2003least,
  title={Least-squares policy iteration},
  author={Lagoudakis, Michail G and Parr, Ronald},
  journal={Journal of Machine Learning Research},
  volume={4},
  number={Dec},
  pages={1107--1149},
  year={2003}
}

@inproceedings{munos2003error,
  title={Error bounds for approximate policy iteration},
  author={Munos, R{\'e}mi},
  booktitle={ICML},
  volume={3},
  pages={560--567},
  year={2003}
}

@article{lazaric2012finite,
  title={Finite-sample analysis of least-squares policy iteration},
  author={Lazaric, Alessandro and Ghavamzadeh, Mohammad and Munos, R{\'e}mi},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={Oct},
  pages={3041--3074},
  year={2012}
}


@article{russel2019beyond,
  title={Beyond Confidence Regions: Tight Bayesian Ambiguity Sets for Robust MDPs},
  author={Russel, Reazul Hasan and Petrik, Marek},
  journal={Advances in Neural Information Processing Systems},
  year={2019}
}

@article{mannor2016robust,
  title={Robust MDPs with k-rectangular uncertainty},
  author={Mannor, Shie and Mebel, Ofir and Xu, Huan},
  journal={Mathematics of Operations Research},
  volume={41},
  number={4},
  pages={1484--1509},
  year={2016},
  publisher={INFORMS}
}

@inproceedings{derman2020bayesian,
  title={A bayesian approach to robust reinforcement learning},
  author={Derman, Esther and Mankowitz, Daniel and Mann, Timothy and Mannor, Shie},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={648--658},
  year={2020},
}

@inproceedings{abdolmaleki2018maximum,
  title={Maximum a Posteriori Policy Optimisation},
  author={Abdolmaleki, Abbas and Springenberg, Jost Tobias and Tassa, Yuval and Munos, Remi and Heess, Nicolas and Riedmiller, Martin},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@InProceedings{panaganti2020robust,
  title = 	 {Robust Reinforcement Learning using Least Squares Policy Iteration with Provable Performance Guarantees},
  author =       {Panaganti, Kishan  and Kalathil, Dileep},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  pages = 	 {511--520},
  year = 	 {2021},
}


@inproceedings{petrik2014raam,
  title={RAAM: The Benefits of Robustness in Approximating Aggregated MDPs in Reinforcement Learning.},
  author={Petrik, Marek and Subramanian, Dharmashankar},
  booktitle={NIPS},
  pages={1979--1987},
  year={2014}
}




@book{dullerud2013course,
  title={A course in robust control theory: a convex approach},
  author={Dullerud, Geir E and Paganini, Fernando},
  volume={36},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@book{zhou1996robust,
  title={Robust and optimal control},
  author={Zhou, Kemin and Doyle, John Comstock and Glover, Keith and others},
  volume={40},
  year={1996},
  publisher={Prentice hall New Jersey}
}



@inproceedings{zhang2020policy,
  author    = {Kaiqing Zhang and
               Bin Hu and
               Tamer Basar},
  title     = {Policy Optimization for {H}\({}_{\mbox{2}}\) Linear Control with {H}\({}_{\mbox{{\(\infty\)}}}\)
               Robustness Guarantee: Implicit Regularization and Global Convergence},
  booktitle = {Proceedings of the 2nd Annual Conference on Learning for Dynamics
               and Control, {L4DC} 2020, Online Event, Berkeley, CA, USA, 11-12 June
               2020},
  series    = {Proceedings of Machine Learning Research},
  volume    = {120},
  pages     = {179--190},
  year      = {2020}
}

@article{zhang2020stability,
  title={On the stability and convergence of robust adversarial reinforcement learning: A case study on linear quadratic systems},
  author={Zhang, Kaiqing and Hu, Bin and Basar, Tamer},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{borkar2002q,
  title={Q-learning for risk-sensitive control},
  author={Borkar, Vivek S},
  journal={Mathematics of operations research},
  volume={27},
  number={2},
  pages={294--311},
  year={2002},
  publisher={INFORMS}
}







%%%%%% GENERATIVE MODEL SAMPLE COMPLEXITY PAPERS

@article{singh1994upper,
  title={An upper bound on the loss from approximate optimal-value functions},
  author={Singh, Satinder P and Yee, Richard C},
  journal={Machine Learning},
  volume={16},
  number={3},
  pages={227--233},
  year={1994},
  publisher={Springer}
}


@article{AzarMK13,
  author    = {Mohammad Gheshlaghi Azar and
               R{\'{e}}mi Munos and
               Hilbert J. Kappen},
  title     = {Minimax {PAC} bounds on the sample complexity of reinforcement learning
               with a generative model},
  journal   = {Mach. Learn.},
  volume    = {91},
  number    = {3},
  pages     = {325--349},
  year      = {2013},
  url       = {https://doi.org/10.1007/s10994-013-5368-1},
  doi       = {10.1007/s10994-013-5368-1},
  timestamp = {Mon, 02 Mar 2020 16:28:55 +0100},
  biburl    = {https://dblp.org/rec/journals/ml/AzarMK13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
  }
  


@article{haskell2016empirical,
  title={Empirical dynamic programming},
  author={Haskell, William B and Jain, Rahul and Kalathil, Dileep},
  journal={Mathematics of Operations Research},
  volume={41},
  number={2},
  pages={402--429},
  year={2016},
}




@article{kalathil2021empirical,
  title={Empirical {Q}-{V}alue {I}teration},
  author={Kalathil, Dileep and Borkar, Vivek S and Jain, Rahul},
  journal={Stochastic Systems},
  volume={11},
  number={1},
  pages={1--18},
  year={2021},
}


@inproceedings{sidford2018near,
  title={Near-optimal time and sample complexities for solving Markov decision processes with a generative model},
  author={Sidford, Aaron and Wang, Mengdi and Wu, Xian and Yang, Lin F and Ye, Yinyu},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={5192--5202},
  year={2018}
}

@inproceedings{agarwal2020model,
  title={Model-based reinforcement learning with a generative model is minimax optimal},
  author={Agarwal, Alekh and Kakade, Sham and Yang, Lin F},
  booktitle={Conference on Learning Theory},
  pages={67--83},
  year={2020},
}

@inproceedings{li2020breaking,
 author = {Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {12861--12872},
 title = {Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model},
 volume = {33},
 year = {2020}
}

@article{li2021breaking,
  title={Breaking the sample complexity barrier to regret-optimal model-free reinforcement learning},
  author={Li, Gen and Shi, Laixi and Chen, Yuxin and Gu, Yuantao and Chi, Yuejie},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17762--17776},
  year={2021}
}

@article{li2022minimax,
  title={Minimax-optimal multi-agent RL in zero-sum Markov games with a generative model},
  author={Li, Gen and Chi, Yuejie and Wei, Yuting and Chen, Yuxin},
  journal={arXiv preprint arXiv:2208.10458},
  year={2022}
}




%%% CLASSIC BOOKS

@book{vershynin2018high,
  title={High-Dimensional Probability: An Introduction with Applications in Data Science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge University press}
}




%%%% Introduction

@article{guastella2020learning,
  title={Learning-based methods of perception and navigation for ground vehicles in unstructured environments: A review},
  author={Guastella, Dario Calogero and Muscato, Giovanni},
  journal={Sensors},
  volume={21},
  number={1},
  pages={73},
  year={2020},
  publisher={MDPI}
}

@article{sunderhauf2018limits,
  title={The limits and potentials of deep learning for robotics},
  author={S{\"u}nderhauf, Niko and Brock, Oliver and Scheirer, Walter and Hadsell, Raia and Fox, Dieter and Leitner, J{\"u}rgen and Upcroft, Ben and Abbeel, Pieter and Burgard, Wolfram and Milford, Michael and others},
  journal={The International journal of robotics research},
  volume={37},
  number={4-5},
  pages={405--420},
  year={2018},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{tobin2017domain,
  title={Domain randomization for transferring deep neural networks from simulation to the real world},
  author={Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle={2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)},
  pages={23--30},
  year={2017},
}



@inproceedings{peng2018sim,
  title={Sim-to-real transfer of robotic control with dynamics randomization},
  author={Peng, Xue Bin and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle={2018 IEEE international conference on robotics and automation (ICRA)},
  pages={3803--3810},
  year={2018},
  organization={IEEE}
}


@article{weng2019DR,
  title   = "Domain Randomization for Sim2Real Transfer",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2019",
  url     = "https://lilianweng.github.io/posts/2019-05-05-domain-randomization/"
}

@article{wang2021online,
  title={Online robust reinforcement learning with model uncertainty},
  author={Wang, Yue and Zou, Shaofeng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={7193--7206},
  year={2021}
}

@article{choi2009reinforcement,
  title={Reinforcement learning and savings behavior},
  author={Choi, James J and Laibson, David and Madrian, Brigitte C and Metrick, Andrew},
  journal={The Journal of finance},
  volume={64},
  number={6},
  pages={2515--2534},
  year={2009},
  publisher={Wiley Online Library}
}

@inproceedings{schulman2013finding,
  title={Finding locally optimal, collision-free trajectories with sequential convex optimization.},
  author={Schulman, John and Ho, Jonathan and Lee, Alex X and Awwal, Ibrahim and Bradlow, Henry and Abbeel, Pieter},
  booktitle={Robotics: science and systems},
  volume={9},
  pages={1--10},
  year={2013},
  organization={Citeseer}
}


@inproceedings{xie2020q,
  title={Q* approximation schemes for batch reinforcement learning: A theoretical comparison},
  author={Xie, Tengyang and Jiang, Nan},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  pages={550--559},
  year={2020},
}

@incollection{lange2012batch,
  title={Batch reinforcement learning},
  author={Lange, Sascha and Gabel, Thomas and Riedmiller, Martin},
  booktitle={Reinforcement learning},
  pages={45--73},
  year={2012},
  publisher={Springer}
}

@inproceedings{chen2019information,
  title={Information-theoretic considerations in batch reinforcement learning},
  author={Chen, Jinglin and Jiang, Nan},
  booktitle={International Conference on Machine Learning},
  pages={1042--1051},
  year={2019},
}

@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@inproceedings{
buckman2021the,
title={The Importance of Pessimism in Fixed-Dataset Policy Optimization},
author={Jacob Buckman and Carles Gelada and Marc G Bellemare},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=E3Ys6a1NTGT}
}

@inproceedings{lykouris2021corruption,
  title={Corruption-robust exploration in episodic reinforcement learning},
  author={Lykouris, Thodoris and Simchowitz, Max and Slivkins, Alex and Sun, Wen},
  booktitle={Conference on Learning Theory},
  pages={3242--3245},
  year={2021}
}


@inproceedings{si2020distributionally,
  title={Distributionally robust policy evaluation and learning in offline contextual bandits},
  author={Si, Nian and Zhang, Fan and Zhou, Zhengyuan and Blanchet, Jose},
  booktitle={International Conference on Machine Learning},
  pages={8884--8894},
  year={2020},
 }
 
 @article{vinitsky2020robust,
  title={Robust reinforcement learning using adversarial populations},
  author={Vinitsky, Eugene and Du, Yuqing and Parvate, Kanaad and Jang, Kathy and Abbeel, Pieter and Bayen, Alexandre},
  journal={arXiv preprint arXiv:2008.01825},
  year={2020}
}

@inproceedings{zhang2020robust,
  title={Robust Reinforcement Learning on State Observations with Learned Optimal Adversary},
  author={Zhang, Huan and Chen, Hongge and Boning, Duane S and Hsieh, Cho-Jui},
  booktitle={International Conference on Learning Representations},
  year={2020}
}




@article{yang2019reinforcement,
  title={Reinforcement Learning in Feature Space: Matrix Bandit, Kernels, and Regret Bound},
  author={Yang, Lin F and Wang, Mengdi},
  journal={arXiv preprint arXiv:1905.10389},
  year={2019}
}



@inproceedings{paternain2018learning,
  title={Learning policies for {M}arkov decision processes in continuous spaces},
  author={Paternain, Santiago and Bazerque, Juan Andr{\'e}s and Small, Austin and Ribeiro, Alejandro},
  booktitle={2018 IEEE Conference on Decision and Control (CDC)},
  pages={4751--4758},
  year={2018},
  organization={IEEE}
}

@inproceedings{behzadian2019fast,
  title={Fast Feature Selection for Linear Value Function Approximation},
  author={Behzadian, Bahram and Gharatappeh, Soheil and Petrik, Marek},
  booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
  volume={29},
  number={1},
  pages={601--609},
  year={2019}
}

@article{chuchro2017game,
  title={Game playing with deep q-learning using openai gym},
  author={Chuchro, Robert and Gupta, Deepak},
  journal={Semantic Scholar},
  year={2017}
}



@inproceedings{jia2020model,
  title={Model-Based Reinforcement Learning with Value-Targeted Regression},
  author={Jia, Zeyu and Yang, Lin and Szepesvari, Csaba and Wang, Mengdi},
  year={2020}, pages = {666--686}, booktitle = {Proceedings of Machine Learning Research}
}


@inproceedings{kidambi2020morel,
  title={MOReL: Model-Based Offline Reinforcement Learning},
  author={Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}


@inproceedings{liu2015finite,
  title={Finite-Sample Analysis of Proximal Gradient TD Algorithms.},
  author={Liu, Bo and Liu, Ji and Ghavamzadeh, Mohammad and Mahadevan, Sridhar and Petrik, Marek},
  booktitle={UAI},
  pages={504--513},
  year={2015},
  organization={Citeseer}
}



@article{bertsekas1996temporal,
  title={Temporal {D}ifferences-{B}ased {P}olicy {I}teration and {A}pplications in {N}euro-{D}ynamic {P}rogramming},
  author={Bertsekas, Dimitri P and Ioffe, Sergey},
  journal={Lab. for Info. and Decision Systems Report LIDS-P-2349, MIT, Cambridge, MA},
  volume={14},
  year={1996},
  publisher={Citeseer}
}



@article{iyengar2005robust,
  title={Robust dynamic programming},
  author={Iyengar, Garud N},
  journal={Mathematics of Operations Research},
  volume={30},
  number={2},
  pages={257--280},
  year={2005},
  publisher={INFORMS}
}


@article{nilim2005robust,
  title={Robust control of {M}arkov decision processes with uncertain transition matrices},
  author={Nilim, Arnab and El Ghaoui, Laurent},
  journal={Operations Research},
  volume={53},
  number={5},
  pages={780--798},
  year={2005},
  publisher={INFORMS}
}

@inproceedings{pomerleau1988il,
 author = {Pomerleau, Dean A.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {ALVINN: An Autonomous Land Vehicle in a Neural Network},
 url = {https://proceedings.neurips.cc/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf},
 volume = {1},
 year = {1988}
}



%%%%%%%%.    RFQI


%% Books

@book{rockafellar2009variational,
  title={Variational analysis},
  author={Rockafellar, R Tyrrell and Wets, Roger J-B},
  volume={317},
  year={2009},
  publisher={Springer Science \& Business Media}
}






%%%%%%% Introduction 

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@article{yang2021generalized,
  title={Generalized out-of-distribution detection: A survey},
  author={Yang, Jingkang and Zhou, Kaiyang and Li, Yixuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2110.11334},
  year={2021}
} 

@article{robey2020model,
  title={Model-based robust deep learning: Generalizing to natural, out-of-distribution data},
  author={Robey, Alexander and Hassani, Hamed and Pappas, George J},
  journal={arXiv preprint arXiv:2005.10247},
  year={2020}
}

@article{esfahani2015data,
  title={Data-driven distributionally robust optimization using the Wasserstein metric: Performance guarantees and tractable reformulations},
  author={Esfahani, Peyman Mohajerin and Kuhn, Daniel},
  journal={arXiv preprint arXiv:1505.05116},
  year={2015}
}


@inproceedings{zhang2021towards,
  author    = {Siyuan Zhang and
               Nan Jiang},
  title     = {Towards Hyperparameter-free Policy Selection for Offline Reinforcement
               Learning},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {12864--12875},
  year      = {2021},
}

@article{cheng2022adversarially,
  title={Adversarially Trained Actor Critic for Offline Reinforcement Learning},
  author={Cheng, Ching-An and Xie, Tengyang and Jiang, Nan and Agarwal, Alekh},
  journal={arXiv preprint arXiv:2202.02446},
  year={2022}
}

@article{wang2023distributionally,
  title={Achieving Minimax Optimal Sample Complexity of Offline Reinforcement Learning: A DRO-Based Approach},
  author={Wang, Yue and Hu, Yuting and Xiong, Jinjun and Zou, Shaofeng},
  journal={arXiv preprint arXiv:2305.13289v2},
  year={2023}
}

@article{panaganti2023bridging,
  title={Bridging Distributionally Robust Learning and Offline RL: An Approach to Mitigate Distribution Shift and Partial Data Coverage},
  author={Panaganti, Kishan and Xu, Zaiyan and Kalathil, Dileep and Ghavamzadeh, Mohammad},
  journal={arXiv preprint arXiv:2310.18434},
  year={2023}
}


@inproceedings{precup2000eligibility,
  author    = {Doina Precup and
               Richard S. Sutton and
               Satinder Singh},
  title     = {Eligibility Traces for Off-Policy Policy Evaluation},
  booktitle = {Proceedings of the Seventeenth International Conference on Machine
               Learning},
  pages     = {759--766},
  year      = {2000},
}

@inproceedings{gordon1995stable,
  author    = {Geoffrey J. Gordon},
  editor    = {Armand Prieditis and
               Stuart Russell},
  title     = {Stable Function Approximation in Dynamic Programming},
  booktitle = {Machine Learning, Proceedings of the Twelfth International Conference
               on Machine Learning, 1995},
  pages     = {261--268},
  year      = {1995},
}




@article{ernst2005tree,
  title={Tree-based batch mode reinforcement learning},
  author={Ernst, Damien and Geurts, Pierre and Wehenkel, Louis},
  journal={Journal of Machine Learning Research},
  volume={6},
  pages={503--556},
  year={2005},
  publisher={Microtome Publishing}
}



@inproceedings{szepesvari2005finite,
  title={Finite time bounds for sampling based fitted value iteration},
  author={Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  booktitle={Proceedings of the 22nd international conference on Machine learning},
  pages={880--887},
  year={2005}
}


@article{farahmand2010error,
  title={Error propagation for approximate policy and value iteration},
  author={Farahmand, Amir-massoud and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  journal={Advances in Neural Information Processing Systems},
  volume={23},
  year={2010}
}






@inproceedings{liu2020provably,
  author    = {Yao Liu and Adith Swaminathan and  Alekh Agarwal and               Emma Brunskill},
  title     = {Provably Good Batch Off-Policy Reinforcement Learning Without Great
               Exploration},
  booktitle = {Neural Information Processing Systems},
  year      = {2020}
}



@article{xie2021bellman,
  title={Bellman-consistent pessimism for offline reinforcement learning},
  author={Xie, Tengyang and Cheng, Ching-An and Jiang, Nan and Mineiro, Paul and Agarwal, Alekh},
  journal={Advances in neural information processing systems},
  volume={34},
  year={2021}
}


@inproceedings{fujimoto2019off,
  title={Off-policy deep reinforcement learning without exploration},
  author={Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle={International Conference on Machine Learning},
  pages={2052--2062},
  year={2019},
}



@article{fujimoto2021minimalist,
  title={A minimalist approach to offline reinforcement learning},
  author={Fujimoto, Scott and Gu, Shixiang Shane},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={20132--20145},
  year={2021}
}

@inproceedings{kostrikov2021offline,
  title={Offline reinforcement learning with fisher divergence critic regularization},
  author={Kostrikov, Ilya and Fergus, Rob and Tompson, Jonathan and Nachum, Ofir},
  booktitle={International Conference on Machine Learning},
  pages={5774--5783},
  year={2021},
}








%%%%    Robust RL papers


@InProceedings{panaganti22a,
  title = 	 { Sample Complexity of Robust Reinforcement Learning with a Generative Model },
  author =       {Panaganti, Kishan and Kalathil, Dileep},
  booktitle = 	 {International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages = 	 {9582--9602},
  year = 	 {2022},
  url = 	 {https://proceedings.mlr.press/v151/panaganti22a.html},
}




%% Misc. classic papers

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{huber1965robust,
  title={A robust version of the probability ratio test},
  author={Huber, Peter J},
  journal={The Annals of Mathematical Statistics},
  pages={1753--1758},
  year={1965},
  publisher={JSTOR}
}


@article{csiszar1967information,
  title={Information-type measures of difference of probability distributions and indirect observation},
  author={Csisz{\'a}r, Imre},
  journal={studia scientiarum Mathematicarum Hungarica},
  volume={2},
  pages={229--318},
  year={1967}
}

@article{kis2014reptile,
author = {Kis, Anna and Huber, Ludwig and Wilkinson, Anna},
year = {2014},
month = {09},
pages = {},
title = {Social learning by imitation in a reptile (Pogona vitticeps)},
volume = {18},
journal = {Animal cognition},
doi = {10.1007/s10071-014-0803-7}
}


%% Robust FQI


@article{agarwal2019reinforcement,
  title={Reinforcement learning: Theory and algorithms},
  author={Agarwal, Alekh and Jiang, Nan and Kakade, Sham M and Sun, Wen},
  journal={CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep},
  year={2019}
}



% Simulation

@misc{fu2020d4rl,
    title={D4RL: Datasets for Deep Data-Driven Reinforcement Learning},
    author={Justin Fu and Aviral Kumar and Ofir Nachum and George Tucker and Sergey Levine},
    year={2020},
    eprint={2004.07219},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}


@misc{rl-zoo3,
  author = {Raffin, Antonin},
  title = {RL Baselines3 Zoo},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/DLR-RM/rl-baselines3-zoo}},
}

@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
}

@inproceedings{fujimoto2018addressing,
  title={Addressing Function Approximation Error in Actor-Critic Methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International Conference on Machine Learning},
  pages={1582--1591},
  year={2018}
}


% Robust Tabular setting

@book{dudley2002real,
  title={Real analysis and {P}robability},
  author={Dudley, Richard M},
  year={2002},
  publisher={Cambridge University Press}
}



@article{huang2022robust,
  title={Robust Reinforcement Learning as a Stackelberg Game via Adaptively-Regularized Adversarial Training},
  author={Huang, Peide and Xu, Mengdi and Fang, Fei and Zhao, Ding},
  journal={arXiv preprint arXiv:2202.09514},
  year={2022}
}

@inproceedings{zhang2021provably,
  title={Provably Efficient Actor-Critic for Risk-Sensitive and Robust Adversarial RL: A Linear-Quadratic Case},
  author={Zhang, Yufeng and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2764--2772},
  year={2021},
}




@article{yang2019reinforcement,
  title={Reinforcement Learning in Feature Space: Matrix Bandit, Kernels, and Regret Bound},
  author={Yang, Lin F and Wang, Mengdi},
  journal={arXiv preprint arXiv:1905.10389},
  year={2019}
}



@inproceedings{paternain2018learning,
  title={Learning policies for {M}arkov decision processes in continuous spaces},
  author={Paternain, Santiago and Bazerque, Juan Andr{\'e}s and Small, Austin and Ribeiro, Alejandro},
  booktitle={2018 IEEE Conference on Decision and Control (CDC)},
  pages={4751--4758},
  year={2018},
  organization={IEEE}
}

@inproceedings{behzadian2019fast,
  title={Fast Feature Selection for Linear Value Function Approximation},
  author={Behzadian, Bahram and Gharatappeh, Soheil and Petrik, Marek},
  booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
  volume={29},
  number={1},
  pages={601--609},
  year={2019}
}

@article{chuchro2017game,
  title={Game playing with deep q-learning using openai gym},
  author={Chuchro, Robert and Gupta, Deepak},
  journal={Semantic Scholar},
  year={2017}
}




@inproceedings{jia2020model,
  title={Model-Based Reinforcement Learning with Value-Targeted Regression},
  author={Jia, Zeyu and Yang, Lin and Szepesvari, Csaba and Wang, Mengdi},
  year={2020}, pages = {666--686}, booktitle = {Proceedings of Machine Learning Research}
}


@inproceedings{kidambi2020morel,
  title={MOReL: Model-Based Offline Reinforcement Learning},
  author={Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}


@inproceedings{liu2015finite,
  title={Finite-Sample Analysis of Proximal Gradient TD Algorithms.},
  author={Liu, Bo and Liu, Ji and Ghavamzadeh, Mohammad and Mahadevan, Sridhar and Petrik, Marek},
  booktitle={UAI},
  pages={504--513},
  year={2015},
  organization={Citeseer}
}




@inproceedings{ho2018fast,
  title={Fast Bellman updates for robust MDPs},
  author={Ho, Chin Pang and Petrik, Marek and Wiesemann, Wolfram},
  booktitle={International Conference on Machine Learning},
  pages={1979--1988},
  year={2018},
  organization={PMLR}
}

@inproceedings{ho2022robust,
  title={Robust $\phi$-Divergence MDPs.},
  author={Ho, Chin Pang and Petrik, Marek and Wiesemann, Wolfram},
  booktitle={NeurIPS},
  year={2022}
}




@inproceedings{abdolmaleki2018maximum,
  title={Maximum a Posteriori Policy Optimisation},
  author={Abdolmaleki, Abbas and Springenberg, Jost Tobias and Tassa, Yuval and Munos, Remi and Heess, Nicolas and Riedmiller, Martin},
  booktitle={International Conference on Learning Representations},
  year={2018}
}








@article{prashanth2016variance,
  author    = {L. A. Prashanth and
               Mohammad Ghavamzadeh},
  title     = {Variance-constrained actor-critic algorithms for discounted and average
               reward MDPs},
  journal   = {Mach. Learn.},
  volume    = {105},
  number    = {3},
  pages     = {367--417},
  year      = {2016},
}

@inproceedings{fei2021exponential,
  author    = {Yingjie Fei and
               Zhuoran Yang and
               Yudong Chen and
               Zhaoran Wang},
  title     = {Exponential Bellman Equation and Improved Regret Bounds for Risk-Sensitive
               Reinforcement Learning},
  booktitle = {Annual Conference
               on Neural Information Processing Systems 2021},
  pages     = {20436--20446},
  year      = {2021},
}


@techreport{van2014probability,
  title={Probability in {H}igh {D}imension},
  author={van Handel, Ramon},
  year={2014},
  institution={Princeton University NJ}
}


@article{muller1997integral,
  title={Integral probability metrics and their generating classes of functions},
  author={M{\"u}ller, Alfred},
  journal={Advances in applied probability},
  volume={29},
  number={2},
  pages={429--443},
  year={1997},
  publisher={Cambridge University Press}
}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%       Future research citations


@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@inproceedings{kakade2001natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  booktitle={Advances in neural information processing systems},
  volume={14},
  pages={1531--1538},
  year={2001}
}

@article{agarwal2021theory,
  title={On the theory of policy gradient methods: Optimality, approximation, and distribution shift},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={98},
  pages={1--76},
  year={2021}
}


@InProceedings{wang22policygradient,
  title = 	 {Policy Gradient Method For Robust Reinforcement Learning},
  author =       {Wang, Yue and Zou, Shaofeng},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {23484--23526},
  year = 	 {2022},
  volume = 	 {162},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  url = 	 {https://proceedings.mlr.press/v162/wang22at.html},
}

@article{li2022first,
  title={First-order Policy Optimization for Robust Markov Decision Process},
  author={Li, Yan and Zhao, Tuo and Lan, Guanghui},
  journal={arXiv preprint arXiv:2209.10579},
  year={2022}
}

@inproceedings{brantley2019disagreement,
  title={Disagreement-regularized imitation learning},
  author={Brantley, Kiante and Sun, Wen and Henaff, Mikael},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{rajaraman2020toward,
  title={Toward the fundamental limits of imitation learning},
  author={Rajaraman, Nived and Yang, Lin and Jiao, Jiantao and Ramchandran, Kannan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2914--2924},
  year={2020}
}


@inproceedings{ross2011reduction,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={627--635},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

@inproceedings{ross2010efficient,
  title={Efficient reductions for imitation learning},
  author={Ross, St{\'e}phane and Bagnell, Drew},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={661--668},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}


@article{ho2016generative,
  title={Generative adversarial imitation learning},
  author={Ho, Jonathan and Ermon, Stefano},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{xu2020error,
  title={Error bounds of imitating policies and environments},
  author={Xu, Tian and Li, Ziniu and Yu, Yang},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15737--15749},
  year={2020}
}

@article{schweizer1992mean,
  title={Mean-variance hedging for general claims},
  author={Schweizer, Martin},
  journal={The annals of applied probability},
  pages={171--179},
  year={1992},
  publisher={JSTOR}
}

@article{fang2019survey,
  title={Survey of imitation learning for robotic manipulation},
  author={Fang, Bin and Jia, Shidong and Guo, Di and Xu, Muhua and Wen, Shuhuan and Sun, Fuchun},
  journal={International Journal of Intelligent Robotics and Applications},
  volume={3},
  number={4},
  pages={362--369},
  year={2019},
  publisher={Springer}
}

@article{rajaraman2021provably,
  title={Provably breaking the quadratic error compounding barrier in imitation learning, optimally},
  author={Rajaraman, Nived and Han, Yanjun and Yang, Lin F and Ramchandran, Kannan and Jiao, Jiantao},
  journal={arXiv preprint arXiv:2102.12948},
  year={2021}
}



%%%%%% Intro first paragraph general ML/RL



@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{silver2018general,
  title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={Science},
  volume={362},
  number={6419},
  pages={1140--1144},
  year={2018},
  publisher={American Association for the Advancement of Science}
}


@article{mirhoseini2021graph,
  title={A graph placement methodology for fast chip design},
  author={Mirhoseini, Azalia and Goldie, Anna and Yazgan, Mustafa and Jiang, Joe Wenjie and Songhori, Ebrahim and Wang, Shen and Lee, Young-Joon and Johnson, Eric and Pathak, Omkar and Nazi, Azade and others},
  journal={Nature},
  volume={594},
  number={7862},
  pages={207--212},
  year={2021},
  publisher={Nature Publishing Group}
}


@article{akkaya2019solving,
  title={Solving rubik's cube with a robot hand},
  author={Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and others},
  journal={arXiv preprint arXiv:1910.07113},
  year={2019}
}



@article{levine2016end,
  title={End-to-end training of deep visuomotor policies},
  author={Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1334--1373},
  year={2016},
  publisher={JMLR. org}
}




@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M and Nasrabadi, Nasser M},
  volume={4},
  number={4},
  year={2006},
  publisher={Springer}
}


@book{meyn2022control,
  title={Control Systems and Reinforcement Learning},
  author={Meyn, Sean},
  year={2022},
  publisher={Cambridge University Press}
}

@misc{ucb-robotics,
  title={{B}erkeley {A}rtificial {I}ntelligence {R}esearch},
  url={https://bair.berkeley.edu/software.html},
}

@INPROCEEDINGS{Kumar_ROBEL, 

     AUTHOR = {Michael Ahn AND Henry Zhu AND Kristian Hartikainen AND Hugo Ponte AND Abhishek Gupta AND Sergey Levine AND Vikash Kumar}, 

     TITLE = "{ROBEL: RObotics BEnchmarks for Learning with low-cost robots}", 

     BOOKTITLE = {Conference on Robot Learning (CoRL)}, 

     YEAR = {2019}, }
     
     
@article{muzero, 
year = {2020}, 
title = {{Mastering Atari, Go, chess and shogi by planning with a learned model}}, 
author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David}, 
journal = {Nature}, 
issn = {0028-0836}, 
doi = {10.1038/s41586-020-03051-4}, 
pages = {604--609}, 
number = {7839}, 
volume = {588}
}

@article{dreamer,
  title={Dream to Control: Learning Behaviors by Latent Imagination},
  author={Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  journal={arXiv preprint arXiv:1912.01603},
  year={2019}
}

@misc{ibm-watson,
  title={{IBM} {W}atson {H}ealth},
  url={https://www.ibm.com/watson-health},
}

@article{dulac2021challenges,
  title={Challenges of real-world reinforcement learning: definitions, benchmarks and analysis},
  author={Dulac-Arnold, Gabriel and Levine, Nir and Mankowitz, Daniel J and Li, Jerry and Paduraru, Cosmin and Gowal, Sven and Hester, Todd},
  journal={Machine Learning},
  volume={110},
  number={9},
  pages={2419--2468},
  year={2021},
  publisher={Springer}
}

@article{berkenkamp2017safe,
  title={Safe model-based reinforcement learning with stability guarantees},
  author={Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela and Krause, Andreas},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{lutjens2020certified,
  title={Certified adversarial robustness for deep reinforcement learning},
  author={L{\"u}tjens, Bj{\"o}rn and Everett, Michael and How, Jonathan P},
  booktitle={Conference on Robot Learning},
  pages={1328--1337},
  year={2020},
  organization={PMLR}
}

@article{arulkumaran2017deep,
  title={Deep reinforcement learning: A brief survey},
  author={Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  journal={IEEE Signal Processing Magazine},
  volume={34},
  number={6},
  pages={26--38},
  year={2017},
  publisher={IEEE}
}

@article{kiran2021deep,
  title={Deep reinforcement learning for autonomous driving: A survey},
  author={Kiran, B Ravi and Sobh, Ibrahim and Talpaert, Victor and Mannion, Patrick and Al Sallab, Ahmad A and Yogamani, Senthil and P{\'e}rez, Patrick},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  year={2021},
  publisher={IEEE}
}

@article{singh2021reinforcement,
  title={Reinforcement learning in robotic applications: a comprehensive survey},
  author={Singh, Bharat and Kumar, Rajesh and Singh, Vinay Pratap},
  journal={Artificial Intelligence Review},
  pages={1--46},
  year={2021},
  publisher={Springer}
}

@article{ghosh2021generalization,
  title={Why generalization in rl is difficult: Epistemic pomdps and implicit partial observability},
  author={Ghosh, Dibya and Rahme, Jad and Kumar, Aviral and Zhang, Amy and Adams, Ryan P and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={25502--25515},
  year={2021}
}

@inproceedings{da2020uncertainty,
  title={Uncertainty-aware action advising for deep reinforcement learning agents},
  author={Da Silva, Felipe Leno and Hernandez-Leal, Pablo and Kartal, Bilal and Taylor, Matthew E},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={04},
  pages={5792--5799},
  year={2020}
}

@inproceedings{lockwood2022review,
  title={A Review of Uncertainty for Deep Reinforcement Learning},
  author={Lockwood, Owen and Si, Mei},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  volume={18},
  number={1},
  pages={155--162},
  year={2022}
}

@ARTICLE{sim2real-1,
  author={Salvato, Erica and Fenu, Gianfranco and Medvet, Eric and Pellegrino, Felice Andrea},
  journal={IEEE Access}, 
  title={Crossing the Reality Gap: A Survey on Sim-to-Real Transferability of Robot Controllers in Reinforcement Learning}, 
  year={2021},
  volume={9},
  number={},
  pages={153171-153187},
  doi={10.1109/ACCESS.2021.3126658}}
  
  @inproceedings{sim2real-2,
  title={Retinagan: An object-aware approach to sim-to-real transfer},
  author={Ho, Daniel and Rao, Kanishka and Xu, Zhuo and Jang, Eric and Khansari, Mohi and Bai, Yunfei},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={10920--10926},
  year={2021},
  organization={IEEE}
}

@misc{sim2real-industry-1,
  title={{C}losing the {S}imulation-to-{R}eality Gap for {D}eep {R}obotic {L}earning},
  url={https://ai.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html},
}

@misc{sim2real-industry-2,
  title={{C}losing the {S}im2{R}eal {G}ap with {NVIDIA} {I}saac Sim and {NVIDIA} {I}saac Replicator},
  url={https://developer.nvidia.com/blog/closing-the-sim2real-gap-with-nvidia-isaac-sim-and-nvidia-isaac-replicator/},
}

@misc{sim2real-acad-1,
  title={{C}losing the {R}eality {G}ap in {S}im2{R}eal Transfer for Robotics},
  url={https://sim2real.github.io/},
}

@misc{sim2real-acad-2,
  title={A Framework for Curriculum Schema Transfer from Low-Fidelity to High-Fidelity Environments},
  author={Shukla, Yash and Sinpov, Jivko}
}

@article{sim2real-acad-3,
  title={Toward Real-World Implementation of Deep Reinforcement Learning for Vision-Based Autonomous Drone Navigation with Mission},
  author={Navardi, Mozhgan and Dixit, Prakhar and Manjunath, Tejaswini and Waytowich, Nicholas R and Mohsenin, Tinoosh and Oates, Tim},
  journal={UMBC Student Collection},
  year={2022}
}


@article{dasgupta2021off,
  title={Off-Policy Evaluation Using Information Borrowing and Context-Based Switching},
  author={Dasgupta, Sutanoy and Niu, Yabo and Panaganti, Kishan and Kalathil, Dileep and Pati, Debdeep and Mallick, Bani},
  journal={arXiv preprint arXiv:2112.09865},
  year={2021}
}

@article{panaganti2020bounded,
  title={Bounded Regret for Finitely Parameterized Multi-Armed Bandits},
  author={Panaganti, Kishan and Kalathil, Dileep},
  journal={IEEE Control Systems Letters},
  volume={5},
  number={3},
  pages={1073--1078},
  year={2020},
  publisher={IEEE}
}

@article{maghakian2022interaction,
  title={Interaction-Grounded Learning for Recommender Systems},
  author={Maghakian, Jessica and Panaganti, Kishan and Mineiro, Paul and Saran, Akanksha and Tan, Cheng},
  journal={Online Recommender Systems and User Modeling ACM RecSys Workshop},
  year={2022}
}

@article{maghakian2022prl,
  title={Personalized Reward Learning with Interaction-Grounded Learning ({IGL})},
  author={Maghakian, Jessica and Mineiro, Paul and Panaganti, Kishan and Rucker, Mark and Saran, Akanksha and Tan, Cheng},
  journal={arXiv preprint arXiv:2211.15823},
  year={2022}
}

@article{pirotta2015policy,
  title={Policy gradient in lipschitz markov decision processes},
  author={Pirotta, Matteo and Restelli, Marcello and Bascetta, Luca},
  journal={Machine Learning},
  volume={100},
  number={2},
  pages={255--283},
  year={2015},
  publisher={Springer}
}

@article{chang2021mitigating,
  title={Mitigating Covariate Shift in Imitation Learning via Offline Data With Partial Coverage},
  author={Chang, Jonathan and Uehara, Masatoshi and Sreenivas, Dhruv and Kidambi, Rahul and Sun, Wen},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={965--979},
  year={2021}
}

@inproceedings{mei2020global,
  title={On the global convergence rates of softmax policy gradient methods},
  author={Mei, Jincheng and Xiao, Chenjun and Szepesvari, Csaba and Schuurmans, Dale},
  booktitle={International Conference on Machine Learning},
  pages={6820--6829},
  year={2020},
  organization={PMLR}
}

@article{o2020making,
  title={Making sense of reinforcement learning and probabilistic inference},
  author={O'Donoghue, Brendan and Osband, Ian and Ionescu, Catalin},
  journal={arXiv preprint arXiv:2001.00805},
  year={2020}
}

@article{nachum2017bridging,
  title={Bridging the gap between value and policy based reinforcement learning},
  author={Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{chow2018path,
  title={Path consistency learning in tsallis entropy regularized mdps},
  author={Chow, Yinlam and Nachum, Ofir and Ghavamzadeh, Mohammad},
  booktitle={International conference on machine learning},
  pages={979--988},
  year={2018},
  organization={PMLR}
}


@misc{isaac-sim2real-2021,
title={Closing the Sim2Real Gap with NVIDIA Isaac Sim and NVIDIA Isaac Replicator},
author={NVIDIA Corporation},
year={2021},
url={https://rb.gy/6xcwgi/}
}

@article{power-system-expert,
AUTHOR = {Meinecke, Steffen and Thurner, Leon and Braun, Martin},
TITLE = {Review of Steady-State Electric Power Distribution System Datasets},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {18},
ARTICLE-NUMBER = {4826},
URL = {https://www.mdpi.com/1996-1073/13/18/4826},
ISSN = {1996-1073},
DOI = {10.3390/en13184826}
}

@article{waymo-expert,
  title={Large Scale Interactive Motion Forecasting for Autonomous Driving : The
  Waymo Open Motion Dataset},
  author={Scott Ettinger and Shuyang Cheng and Benjamin Caine and Chenxi Liu and Hang Zhao and Sabeek Pradhan and Yuning Chai and Ben Sapp and Charles Qi and Yin Zhou and Zoey Yang and Aurelien Chouard and Pei Sun and Jiquan Ngiam and Vijay Vasudevan and Alexander McCauley and Jonathon Shlens and Dragomir Anguelov},
  journal={arXiv},
  year={2021}
}

@misc{healthcare-expert,
title={10 Best Healthcare Data Sets \& Examples},
author={Maxwell Travers},
year={2021},
url={https://www.cprime.com/resources/blog/10-best-healthcare-data-sets-examples/}
}

@inproceedings{eysenbach2022maximum,
  title={Maximum Entropy RL (Provably) Solves Some Robust RL Problems},
  author={Eysenbach, Benjamin and Levine, Sergey},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{bashiri2021distributionally,
  title={Distributionally Robust Imitation Learning},
  author={Bashiri, Mohammad Ali and Ziebart, Brian and Zhang, Xinhua},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={24404--24417},
  year={2021}
}



%% Model-free offline RL papers

@article{jiang2020minimax,
  title={Minimax value interval for off-policy evaluation and policy optimization},
  author={Jiang, Nan and Huang, Jiawei},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2747--2758},
  year={2020}
}

@article{foster2022offline,
  title={Offline reinforcement learning: Fundamental barriers for value function approximation},
  author={Foster, Dylan J and Krishnamurthy, Akshay and Simchi-Levi, David and Xu, Yunzong},
  journal={arXiv preprint arXiv:2111.10919},
  year={2022}
}

@article{rashidinejad2022optimal,
  title={Optimal conservative offline rl with general function approximation via augmented lagrangian},
  author={Rashidinejad, Paria and Zhu, Hanlin and Yang, Kunhe and Russell, Stuart and Jiao, Jiantao},
  journal={arXiv preprint arXiv:2211.00716},
  year={2022}
}

@inproceedings{shi2022pessimistic,
  title={Pessimistic q-learning for offline reinforcement learning: Towards optimal sample complexity},
  author={Shi, Laixi and Li, Gen and Wei, Yuting and Chen, Yuxin and Chi, Yuejie},
  booktitle={International Conference on Machine Learning},
  pages={19967--20025},
  year={2022},
  organization={PMLR}
}

@article{shi2023curious,
  title={The curious price of distributional robustness in reinforcement learning with a generative model},
  author={Shi, Laixi and Li, Gen and Wei, Yuting and Chen, Yuxin and Geist, Matthieu and Chi, Yuejie},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{zanette2022bellman,
  title={Bellman residual orthogonalization for offline reinforcement learning},
  author={Zanette, Andrea and Wainwright, Martin J},
  journal={arXiv preprint arXiv:2203.12786},
  year={2022}
}

@inproceedings{zhan2022offline,
  title={Offline reinforcement learning with realizability and single-policy concentrability},
  author={Zhan, Wenhao and Huang, Baihe and Huang, Audrey and Jiang, Nan and Lee, Jason},
  booktitle={Conference on Learning Theory},
  pages={2730--2775},
  year={2022},
  organization={PMLR}
}

@article{ozdaglar2022revisiting,
  title={Revisiting the Linear-Programming Framework for Offline RL with General Function Approximation},
  author={Ozdaglar, Asuman and Pattathil, Sarath and Zhang, Jiawei and Zhang, Kaiqing},
  journal={arXiv preprint arXiv:2212.13861},
  year={2022}
}

@article{zhu2023importance,
  title={Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning},
  author={Zhu, Hanlin and Rashidinejad, Paria and Jiao, Jiantao},
  journal={arXiv preprint arXiv:2301.12714},
  year={2023}
}

@inproceedings{xie2021batch,
  title={Batch value-function approximation with only realizability},
  author={Xie, Tengyang and Jiang, Nan},
  booktitle={International Conference on Machine Learning},
  pages={11404--11413},
  year={2021},
  organization={PMLR}
}

@article{xie2022armor,
  title={ARMOR: A Model-based Framework for Improving Arbitrary Baseline Policies with Offline Data},
  author={Xie, Tengyang and Bhardwaj, Mohak and Jiang, Nan and Cheng, Ching-An},
  journal={arXiv preprint arXiv:2211.04538},
  year={2022}
}




%% Offline RL 

@article{cabi2019scaling,
  title={Scaling data-driven robotics with reward sketching and batch reinforcement learning},
  author={Cabi, Serkan and Colmenarejo, Sergio G{\'o}mez and Novikov, Alexander and Konyushkova, Ksenia and Reed, Scott and Jeong, Rae and Zolna, Konrad and Aytar, Yusuf and Budden, David and Vecerik, Mel and others},
  journal={arXiv preprint arXiv:1909.12200},
  year={2019}
}

@article{li2019aads,
  title={AADS: Augmented autonomous driving simulation using data-driven algorithms},
  author={Li, Wei and Pan, CW and Zhang, Rong and Ren, JP and Ma, YX and Fang, Jin and Yan, FL and Geng, QC and Huang, XY and Gong, HJ and others},
  journal={Science robotics},
  volume={4},
  number={28},
  pages={eaaw0863},
  year={2019},
  publisher={American Association for the Advancement of Science}
}

@article{amini2020learning,
  title={Learning robust control policies for end-to-end autonomous driving from data-driven simulation},
  author={Amini, Alexander and Gilitschenski, Igor and Phillips, Jacob and Moseyko, Julia and Banerjee, Rohan and Karaman, Sertac and Rus, Daniela},
  journal={IEEE Robotics and Automation Letters},
  volume={5},
  number={2},
  pages={1143--1150},
  year={2020},
  publisher={IEEE}
}

@article{hu2016data,
  title={Data driven analytics for personalized healthcare},
  author={Hu, Jianying and Perer, Adam and Wang, Fei},
  journal={Healthcare Information Management Systems: Cases, Strategies, and Solutions},
  pages={529--554},
  year={2016},
  publisher={Springer}
}

@article{dang2020data,
  title={Data-driven structural health monitoring using feature fusion and hybrid deep learning},
  author={Dang, Hung V and Tran-Ngoc, Hoa and Nguyen, Tung V and Bui-Tien, Thanh and De Roeck, Guido and Nguyen, Huan X},
  journal={IEEE Transactions on Automation Science and Engineering},
  volume={18},
  number={4},
  pages={2087--2103},
  year={2020},
  publisher={IEEE}
}

@article{li2022settling,
  title={Settling the sample complexity of model-based offline reinforcement learning},
  author={Li, Gen and Shi, Laixi and Chen, Yuxin and Chi, Yuejie and Wei, Yuting},
  journal={arXiv preprint arXiv:2204.05275},
  year={2022}
}



@inproceedings{jin2021pessimism,
  title={Is pessimism provably efficient for offline rl?},
  author={Jin, Ying and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={5084--5096},
  year={2021},
  organization={PMLR}
}

@article{gabbianelli2023offline,
  title={Offline Primal-Dual Reinforcement Learning for Linear MDPs},
  author={Gabbianelli, Germano and Neu, Gergely and Okolo, Nneka and Papini, Matteo},
  journal={arXiv preprint arXiv:2305.12944},
  year={2023}
}

@inproceedings{xiong2022nearly,
  title={Nearly Minimax Optimal Offline Reinforcement Learning with Linear Function Approximation: Single-Agent MDP and Markov Game},
  author={Xiong, Wei and Zhong, Han and Shi, Chengshuai and Shen, Cong and Wang, Liwei and Zhang, Tong},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}




@article{rashidinejad2022bridging,
  title={Bridging offline reinforcement learning and imitation learning: A tale of pessimism},
  author={Rashidinejad, Paria and Zhu, Banghua and Ma, Cong and Jiao, Jiantao and Russell, Stuart},
  journal={IEEE Transactions on Information Theory},
  volume={68},
  number={12},
  pages={8156--8196},
  year={2022},
  publisher={IEEE}
}



@inproceedings{uehara2021pessimistic,
  title={Pessimistic Model-based Offline Reinforcement Learning under Partial Coverage},
  author={Uehara, Masatoshi and Sun, Wen},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{yin2022near,
  title={Near-optimal offline reinforcement learning with linear representation: Leveraging variance information with pessimism},
  author={Yin, Ming and Duan, Yaqi and Wang, Mengdi and Wang, Yu-Xiang},
  journal={arXiv preprint arXiv:2203.05804},
  year={2022}
}

@article{ma2022distributionally,
  title={Distributionally robust offline reinforcement learning with linear function approximation},
  author={Ma, Xiaoteng and Liang, Zhipeng and Xia, Li and Zhang, Jiheng and Blanchet, Jose and Liu, Mingwen and Zhao, Qianchuan and Zhou, Zhengyuan},
  journal={arXiv preprint arXiv:2209.06620},
  year={2022}
}

@article{sriperumbudur2009integral,
  title={On integral probability metrics,$\backslash$phi-divergences and binary classification},
  author={Sriperumbudur, Bharath K and Fukumizu, Kenji and Gretton, Arthur and Sch{\"o}lkopf, Bernhard and Lanckriet, Gert RG},
  journal={arXiv preprint arXiv:0901.2698},
  year={2009}
}


@InProceedings{cheng2022adversarially,
  title = 	 {Adversarially Trained Actor Critic for Offline Reinforcement Learning},
  author =       {Cheng, Ching-An and Xie, Tengyang and Jiang, Nan and Agarwal, Alekh},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {3852--3878},
  year = 	 {2022},
}



@inproceedings{precup2000eligibility,
  author    = {Doina Precup and
               Richard S. Sutton and
               Satinder Singh},
  title     = {Eligibility Traces for Off-Policy Policy Evaluation},
  booktitle = {Proceedings of the Seventeenth International Conference on Machine
               Learning},
  pages     = {759--766},
  year      = {2000},
}

@article{canonne2020short,
  title={A short note on learning discrete distributions},
  author={Canonne, Cl{\'e}ment L},
  journal={arXiv preprint arXiv:2002.11457},
  year={2020}
}

@article{massart1990tight,
  title={The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality},
  author={Massart, Pascal},
  journal={The annals of Probability},
  pages={1269--1283},
  year={1990},
  publisher={JSTOR}
}

@article{kumar2022efficient,
  title={Efficient policy iteration for robust markov decision processes via regularization},
  author={Kumar, Navdeep and Levy, Kfir and Wang, Kaixin and Mannor, Shie},
  journal={arXiv preprint arXiv:2205.14327},
  year={2022}
}

@book{cormen2022introduction,
  title={Introduction to algorithms},
  author={Cormen, Thomas H and Leiserson, Charles E and Rivest, Ronald L and Stein, Clifford},
  year={2022},
  publisher={MIT press}
}

@inproceedings{bhattacharyya2021near,
  title={Near-optimal learning of tree-structured distributions by Chow-Liu},
  author={Bhattacharyya, Arnab and Gayen, Sutanu and Price, Eric and Vinodchandran, NV},
  booktitle={Proceedings of the 53rd annual acm SIGACT symposium on theory of computing},
  pages={147--160},
  year={2021}
}

@article{arora2023near,
  title={Near-Optimal Degree Testing for Bayes Nets},
  author={Arora, Vipul and Bhattacharyya, Arnab and Canonne, Cl{\'e}ment L and Yang, Joy Qiping},
  journal={arXiv preprint arXiv:2304.06733},
  year={2023}
}

@article{fournier2015rate,
  title={On the rate of convergence in Wasserstein distance of the empirical measure},
  author={Fournier, Nicolas and Guillin, Arnaud},
  journal={Probability theory and related fields},
  volume={162},
  number={3-4},
  pages={707--738},
  year={2015},
  publisher={Springer}
}

@article{lei2020convergence,
author = {Jing Lei},
title = {{Convergence and concentration of empirical measures under Wasserstein distance in unbounded functional spaces}},
volume = {26},
journal = {Bernoulli},
number = {1},
publisher = {Bernoulli Society for Mathematical Statistics and Probability},
pages = {767 -- 798},
year = {2020},
}

@book{villani2009optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric and others},
  volume={338},
  year={2009},
  publisher={Springer}
}

@inproceedings{zhang2022corruption,
  title={Corruption-robust offline reinforcement learning},
  author={Zhang, Xuezhou and Chen, Yiding and Zhu, Xiaojin and Sun, Wen},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={5757--5773},
  year={2022},
  organization={PMLR}
}

@article{diamond2016cvxpy,
  title={CVXPY: A Python-embedded modeling language for convex optimization},
  author={Diamond, Steven and Boyd, Stephen},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={2909--2913},
  year={2016},
  publisher={JMLR.org}
}


%% New citations for 2024 RFQI paper

@article{yang2023avoiding,
  title={Avoiding model estimation in robust markov decision processes with a generative model},
  author={Yang, Wenhao and Wang, Han and Kozuno, Tadashi and Jordan, Scott M and Zhang, Zhihua},
  journal={arXiv preprint arXiv:2302.01248},
  year={2023}
}

@article{zhang2023regularized,
  title={Regularized Robust MDPs and Risk-Sensitive MDPs: Equivalence, Policy Gradient, and Sample Complexity},
  author={Zhang, Runyu and Hu, Yang and Li, Na},
  journal={arXiv preprint arXiv:2306.11626},
  year={2023}
}

@article{bruns2023robust,
  title={Robust Fitted-Q-Evaluation and Iteration under Sequentially Exogenous Unobserved Confounders},
  author={Bruns-Smith, David and Zhou, Angela},
  journal={arXiv preprint arXiv:2302.00662},
  year={2023}
}

@inproceedings{song2023hybrid,
  title={Hybrid RL: Using both offline and online data can make RL efficient},
  author={Song, Yuda and Zhou, Yifei and Sekhari, Ayush and Bagnell, Drew and Krishnamurthy, Akshay and Sun, Wen},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}

@article{sinha2017certifiable,
  title={Certifiable Distributional Robustness with Principled Adversarial Training. CoRR, abs/1710.10571},
  author={Sinha, Aman and Namkoong, Hongseok and Duchi, John C},
  journal={arXiv preprint arXiv:1710.10571},
  year={2017}
}

@article{levy2020large,
  title={Large-scale methods for distributionally robust optimization},
  author={Levy, Daniel and Carmon, Yair and Duchi, John C and Sidford, Aaron},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={8847--8860},
  year={2020}
}

@article{jin2021non,
  title={Non-convex distributionally robust optimization: Non-asymptotic analysis},
  author={Jin, Jikai and Zhang, Bohang and Wang, Haiyang and Wang, Liwei},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2771--2782},
  year={2021}
}

@article{van2015fast,
  title={Fast rates in statistical and online learning},
  author={Van Erven, Tim and Grunwald, Peter and Mehta, Nishant A and Reid, Mark and Williamson, Robert and others},
  year={2015},
    journal={JMLR},
  publisher={MIT Press}
}

@article{jin2021bellman,
  title={Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms},
  author={Jin, Chi and Liu, Qinghua and Miryoosefi, Sobhan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={13406--13418},
  year={2021}
}

@inproceedings{du2021bilinear,
  title={Bilinear classes: A structural framework for provable generalization in rl},
  author={Du, Simon and Kakade, Sham and Lee, Jason and Lovett, Shachar and Mahajan, Gaurav and Sun, Wen and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={2826--2836},
  year={2021},
}

@article{botvinick2019reinforcement,
  title={Reinforcement learning, fast and slow},
  author={Botvinick, Matthew and Ritter, Sam and Wang, Jane X and Kurth-Nelson, Zeb and Blundell, Charles and Hassabis, Demis},
  journal={Trends in cognitive sciences},
  volume={23},
  number={5},
  pages={408--422},
  year={2019},
  publisher={Elsevier}
}

@article{lesort2020continual,
  title={Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges},
  author={Lesort, Timoth{\'e}e and Lomonaco, Vincenzo and Stoian, Andrei and Maltoni, Davide and Filliat, David and D{\'\i}az-Rodr{\'\i}guez, Natalia},
  journal={Information fusion},
  volume={58},
  pages={52--68},
  year={2020},
  publisher={Elsevier}
}

@article{liang2023single,
  title={Single-Trajectory Distributionally Robust Reinforcement Learning},
  author={Liang, Zhipeng and Ma, Xiaoteng and Blanchet, Jose and Zhang, Jiheng and Zhou, Zhengyuan},
  journal={arXiv preprint arXiv:2301.11721},
  year={2023}
}

@inproceedings{liu2022distributionally,
  title={Distributionally Robust $ Q $-Learning},
  author={Liu, Zijian and Bai, Qinxun and Blanchet, Jose and Dong, Perry and Xu, Wei and Zhou, Zhengqing and Zhou, Zhengyuan},
  booktitle={International Conference on Machine Learning},
  pages={13623--13643},
  year={2022},
}

@article{wang2023sample,
  title={Sample Complexity of Variance-reduced Distributionally Robust Q-learning},
  author={Wang, Shengbo and Si, Nian and Blanchet, Jose and Zhou, Zhengyuan},
  journal={arXiv preprint arXiv:2305.18420},
  year={2023}
}

@inproceedings{wang2023finite,
  title={A finite sample complexity bound for distributionally robust q-learning},
  author={Wang, Shengbo and Si, Nian and Blanchet, Jose and Zhou, Zhengyuan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3370--3398},
  year={2023},
}

@inproceedings{zhou2023natural,
  title={Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation},
  author={Zhou, Ruida and Liu, Tao and Cheng, Min and Kalathil, Dileep and Kumar, Panganamala and Tian, Chao},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}


@article{fawzi2022discovering,
  title={Discovering faster matrix multiplication algorithms with reinforcement learning},
  author={Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and R Ruiz, Francisco J and Schrittwieser, Julian and Swirszcz, Grzegorz and others},
  journal={Nature},
  volume={610},
  number={7930},
  pages={47--53},
  year={2022},
  publisher={Nature Publishing Group}
}

@inproceedings{schmidt2015depth,
  title={Depth-based tracking with physical constraints for robot manipulation},
  author={Schmidt, Tanner and Hertkorn, Katharina and Newcombe, Richard and Marton, Zoltan and Suppa, Michael and Fox, Dieter},
  booktitle={2015 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={119--126},
  year={2015},
}

@inproceedings{shah2018airsim,
  title={Airsim: High-fidelity visual and physical simulation for autonomous vehicles},
  author={Shah, Shital and Dey, Debadeepta and Lovett, Chris and Kapoor, Ashish},
  booktitle={Field and Service Robotics: Results of the 11th International Conference},
  pages={621--635},
  year={2018},
  organization={Springer}
}


@article{maraun2016bias,
  title={Bias correcting climate change simulations-a critical review},
  author={Maraun, Douglas},
  journal={Current Climate Change Reports},
  volume={2},
  pages={211--220},
  year={2016},
  publisher={Springer}
}

@article{chen1996design,
  title={Design of unknown input observers and robust fault detection filters},
  author={Chen, Jie and Patton, Ron J and Zhang, Hong-Yue},
  journal={International Journal of control},
  volume={63},
  number={1},
  pages={85--105},
  year={1996},
  publisher={Taylor \& Francis}
}

@inproceedings{pioch2009adversarial,
  title={Adversarial intent modeling using embedded simulation and temporal Bayesian knowledge bases},
  author={Pioch, Nicholas J and Melhuish, James and Seidel, Andy and Santos Jr, Eugene and Li, Deqing and Gorniak, Mark},
  booktitle={Modeling and Simulation for Military Operations IV},
  volume={7348},
  pages={115--126},
  year={2009},
}

@article{scherrer2015approximate,
  title={Approximate modified policy iteration and its application to the game of Tetris.},
  author={Scherrer, Bruno and Ghavamzadeh, Mohammad and Gabillon, Victor and Lesner, Boris and Geist, Matthieu},
  journal={J. Mach. Learn. Res.},
  volume={16},
  number={49},
  pages={1629--1676},
  year={2015}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
}


----

@inproceedings{huang2023reinforcement,
  title={Reinforcement learning in low-rank mdps with density features},
  author={Huang, Audrey and Chen, Jinglin and Jiang, Nan},
  booktitle={International Conference on Machine Learning},
  pages={13710--13752},
  year={2023},
}


@inproceedings{panaganti2021sample,
  title={Sample complexity of model-based robust reinforcement learning},
  author={Panaganti, Kishan and Kalathil, Dileep},
  booktitle={2021 60th IEEE Conference on Decision and Control (CDC)},
  pages={2240--2245},
  year={2021},
}

@phdthesis{panaganti2023thesis,
  title={Robust Reinforcement Learning: Theory and Algorithms},
  author={Panaganti, Kishan},
  year={2023},
  school={Texas A\&M University}
}

@inproceedings{panaganti2023distributionally,
  title={Distributionally Robust Behavioral Cloning for Robust Imitation Learning},
  author={Panaganti, Kishan and Xu, Zaiyan and Kalathil, Dileep and Ghavamzadeh, Mohammad},
  booktitle={2023 62nd IEEE Conference on Decision and Control (CDC)},
  pages={1342--1347},
  year={2023},
}

@article{chen2022finite,
  title={Finite-sample analysis of off-policy natural actor--critic with linear function approximation},
  author={Chen, Zaiwei and Khodadadian, Sajad and Maguluri, Siva Theja},
  journal={IEEE Control Systems Letters},
  volume={6},
  pages={2611--2616},
  year={2022},
  publisher={IEEE}
}

@article{blanchet2023double,
  title={Double pessimism is provably efficient for distributionally robust offline reinforcement learning: Generic algorithm and robust partial coverage},
  author={Blanchet, Jose and Lu, Miao and Zhang, Tong and Zhong, Han},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}