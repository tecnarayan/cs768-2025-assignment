\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bai et~al.(2018)Bai, Kolter, and Koltun]{bai2018empirical}
Bai, S., Kolter, J.~Z., and Koltun, V.
\newblock An empirical evaluation of generic convolutional and recurrent
  networks for sequence modeling.
\newblock \emph{arXiv preprint arXiv:1803.01271}, 2018.

\bibitem[Chen et~al.(2016)Chen, Zhu, Ling, Wei, Jiang, and
  Inkpen]{chen2016enhanced}
Chen, Q., Zhu, X., Ling, Z., Wei, S., Jiang, H., and Inkpen, D.
\newblock Enhanced lstm for natural language inference.
\newblock \emph{arXiv preprint arXiv:1609.06038}, 2016.

\bibitem[Coates et~al.(2011)Coates, Ng, and Lee]{coates2011analysis}
Coates, A., Ng, A., and Lee, H.
\newblock An analysis of single-layer networks in unsupervised feature
  learning.
\newblock In \emph{Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pp.\  215--223. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai2019transformer}
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q.~V., and Salakhutdinov, R.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock \emph{arXiv preprint arXiv:1901.02860}, 2019.

\bibitem[DeGroot \& Fienberg(1983)DeGroot and Fienberg]{degroot1983comparison}
DeGroot, M.~H. and Fienberg, S.~E.
\newblock The comparison and evaluation of forecasters.
\newblock \emph{Journal of the Royal Statistical Society: Series D (The
  Statistician)}, 32\penalty0 (1-2):\penalty0 12--22, 1983.

\bibitem[Demirkaya et~al.(2020)Demirkaya, Chen, and
  Oymak]{demirkaya2020exploring}
Demirkaya, A., Chen, J., and Oymak, S.
\newblock Exploring the role of loss functions in multiclass classification.
\newblock In \emph{2020 54th annual conference on information sciences and
  systems (ciss)}, pp.\  1--5. IEEE, 2020.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Elsayed et~al.(2018)Elsayed, Krishnan, Mobahi, Regan, and
  Bengio]{elsayed2018large}
Elsayed, G., Krishnan, D., Mobahi, H., Regan, K., and Bengio, S.
\newblock Large margin deep networks for classification.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Fern{\'a}ndez-Delgado et~al.(2014)Fern{\'a}ndez-Delgado, Cernadas,
  Barro, and Amorim]{fernandez2014we}
Fern{\'a}ndez-Delgado, M., Cernadas, E., Barro, S., and Amorim, D.
\newblock Do we need hundreds of classifiers to solve real world classification
  problems?
\newblock \emph{The journal of machine learning research}, 15\penalty0
  (1):\penalty0 3133--3181, 2014.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
\newblock On calibration of modern neural networks.
\newblock In \emph{International conference on machine learning}, pp.\
  1321--1330. PMLR, 2017.

\bibitem[Han et~al.(2021)Han, Papyan, and Donoho]{han2021neural}
Han, X., Papyan, V., and Donoho, D.~L.
\newblock Neural collapse under mse loss: Proximity to and dynamics on the
  central path.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock arXiv preprint arXiv:2106.02073.

\bibitem[He \& Lin(2016)He and Lin]{he2016pairwise}
He, H. and Lin, J.
\newblock Pairwise word interaction modeling with deep neural networks for
  semantic similarity measurement.
\newblock In \emph{Proceedings of the 2016 conference of the north American
  chapter of the Association for Computational Linguistics: human language
  technologies}, pp.\  937--948, 2016.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hui \& Belkin(2020)Hui and Belkin]{hui2020evaluation}
Hui, L. and Belkin, M.
\newblock Evaluation of neural architectures trained with square loss vs
  cross-entropy in classification tasks.
\newblock \emph{arXiv preprint arXiv:2006.07322}, 2020.

\bibitem[Kim et~al.(2017)Kim, Hori, and Watanabe]{kim2017joint}
Kim, S., Hori, T., and Watanabe, S.
\newblock Joint ctc-attention based end-to-end speech recognition using
  multi-task learning.
\newblock In \emph{2017 IEEE international conference on acoustics, speech and
  signal processing (ICASSP)}, pp.\  4835--4839. IEEE, 2017.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lin et~al.(2017)Lin, Goyal, Girshick, He, and
  Doll{\'a}r]{lin2017focal}
Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Doll{\'a}r, P.
\newblock Focal loss for dense object detection.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  2980--2988, 2017.

\bibitem[Liu et~al.(2022)Liu, Ben~Ayed, Galdran, and Dolz]{liu2022devil}
Liu, B., Ben~Ayed, I., Galdran, A., and Dolz, J.
\newblock The devil is in the margin: Margin-based label smoothing for network
  calibration.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  80--88, 2022.

\bibitem[Moritz et~al.(2019)Moritz, Hori, and Le~Roux]{moritz2019triggered}
Moritz, N., Hori, T., and Le~Roux, J.
\newblock Triggered attention for end-to-end speech recognition.
\newblock In \emph{ICASSP 2019-2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pp.\  5666--5670. IEEE, 2019.

\bibitem[Mukhoti et~al.(2020)Mukhoti, Kulharia, Sanyal, Golodetz, Torr, and
  Dokania]{mukhoti2020calibrating}
Mukhoti, J., Kulharia, V., Sanyal, A., Golodetz, S., Torr, P., and Dokania, P.
\newblock Calibrating deep neural networks using focal loss.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15288--15299, 2020.

\bibitem[M{\"u}ller et~al.(2019)M{\"u}ller, Kornblith, and
  Hinton]{muller2019does}
M{\"u}ller, R., Kornblith, S., and Hinton, G.~E.
\newblock When does label smoothing help?
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Naeini et~al.(2015)Naeini, Cooper, and
  Hauskrecht]{naeini2015obtaining}
Naeini, M.~P., Cooper, G., and Hauskrecht, M.
\newblock Obtaining well calibrated probabilities using bayesian binning.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~29, 2015.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A.~Y.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Niculescu-Mizil \& Caruana(2005)Niculescu-Mizil and
  Caruana]{niculescu2005predicting}
Niculescu-Mizil, A. and Caruana, R.
\newblock Predicting good probabilities with supervised learning.
\newblock In \emph{Proceedings of the 22nd international conference on Machine
  learning}, pp.\  625--632, 2005.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{Pap20a}
Papyan, V., Han, X.~Y., and Donoho, D.~L.
\newblock Prevalence of neural collapse during the terminal phase of deep
  learning training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (40):\penalty0 24652--24663, 2020.
\newblock \doi{10.1073/pnas.2015509117}.
\newblock URL \url{https://www.pnas.org/doi/abs/10.1073/pnas.2015509117}.

\bibitem[Platt et~al.(1999)]{platt1999probabilistic}
Platt, J. et~al.
\newblock Probabilistic outputs for support vector machines and comparisons to
  regularized likelihood methods.
\newblock \emph{Advances in large margin classifiers}, 10\penalty0
  (3):\penalty0 61--74, 1999.

\bibitem[Que \& Belkin(2016)Que and Belkin]{que2016back}
Que, Q. and Belkin, M.
\newblock Back to the future: Radial basis function networks revisited.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  1375--1383.
  PMLR, 2016.

\bibitem[Rifkin(2002)]{rifkin2002everything}
Rifkin, R.~M.
\newblock \emph{Everything old is new again: a fresh look at historical
  approaches in machine learning}.
\newblock PhD thesis, MaSSachuSettS InStitute of Technology, 2002.

\bibitem[Sangari \& Sethares(2015)Sangari and Sethares]{sangari2015convergence}
Sangari, A. and Sethares, W.
\newblock Convergence analysis of two loss functions in soft-max regression.
\newblock \emph{IEEE Transactions on Signal Processing}, 64\penalty0
  (5):\penalty0 1280--1288, 2015.

\bibitem[Sun(2019)]{sun2019optimization}
Sun, R.
\newblock Optimization for deep learning: theory and algorithms.
\newblock \emph{arXiv preprint arXiv:1912.08957}, 2019.

\bibitem[Tan \& Le(2019)Tan and Le]{tan2019efficientnet}
Tan, M. and Le, Q.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In \emph{International conference on machine learning}, pp.\
  6105--6114. PMLR, 2019.

\bibitem[Towns et~al.(2014)Towns, Cockerill, Dahan, Foster, Gaither, Grimshaw,
  Hazlewood, Lathrop, Lifka, Peterson, et~al.]{towns2014xsede}
Towns, J., Cockerill, T., Dahan, M., Foster, I., Gaither, K., Grimshaw, A.,
  Hazlewood, V., Lathrop, S., Lifka, D., Peterson, G.~D., et~al.
\newblock Xsede: Accelerating scientific discovery computing in science \&
  engineering, 16 (5): 62--74, sep 2014.
\newblock \emph{URL https://doi. org/10.1109/mcse}, 128, 2014.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Watanabe et~al.(2018)Watanabe, Hori, Karita, Hayashi, Nishitoba, Unno,
  Soplin, Heymann, Wiesner, Chen, et~al.]{watanabe2018espnet}
Watanabe, S., Hori, T., Karita, S., Hayashi, T., Nishitoba, J., Unno, Y.,
  Soplin, N. E.~Y., Heymann, J., Wiesner, M., Chen, N., et~al.
\newblock Espnet: End-to-end speech processing toolkit.
\newblock \emph{arXiv preprint arXiv:1804.00015}, 2018.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\end{thebibliography}
