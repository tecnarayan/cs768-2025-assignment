\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ajay et~al.(2023)Ajay, Han, Du, Li, Gupta, Jaakkola, Tenenbaum, Kaelbling, Srivastava, and Agrawal]{ajay2023compositional}
Ajay, A., Han, S., Du, Y., Li, S., Gupta, A., Jaakkola, T.~S., Tenenbaum, J.~B., Kaelbling, L.~P., Srivastava, A., and Agrawal, P.
\newblock Compositional foundation models for hierarchical planning.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Akrour et~al.(2011)Akrour, Schoenauer, and Sebag]{akrour2011preference}
Akrour, R., Schoenauer, M., and Sebag, M.
\newblock Preference-based policy learning.
\newblock In \emph{Proceedings of the 2011th European Conference on Machine Learning and Knowledge Discovery in Databases-Volume Part I}, pp.\  12--27, 2011.

\bibitem[Bhateja et~al.(2023)Bhateja, Guo, Ghosh, Singh, Tomar, Vuong, Chebotar, Levine, and Kumar]{bhateja2023robotic}
Bhateja, C.~A., Guo, D., Ghosh, D., Singh, A., Tomar, M., Vuong, Q., Chebotar, Y., Levine, S., and Kumar, A.
\newblock Robotic offline rl from internet videos via value-function pre-training.
\newblock In \emph{NeurIPS 2023 Foundation Models for Decision Making Workshop}, 2023.

\bibitem[Black et~al.(2023)Black, Nakamoto, Atreya, Walke, Finn, Kumar, and Levine]{black2023zero}
Black, K., Nakamoto, M., Atreya, P., Walke, H., Finn, C., Kumar, A., and Levine, S.
\newblock Zero-shot robotic manipulation with pre-trained image-editing diffusion models.
\newblock In \emph{NeurIPS 2023 Workshop on Goal-Conditioned Reinforcement Learning}, 2023.

\bibitem[Bradley \& Terry(1952)Bradley and Terry]{bradley1952rank}
Bradley, R.~A. and Terry, M.~E.
\newblock Rank analysis of incomplete block designs: I. the method of paired comparisons.
\newblock \emph{Biometrika}, 39\penalty0 (3/4):\penalty0 324--345, 1952.

\bibitem[Brohan et~al.(2022)Brohan, Brown, Carbajal, Chebotar, Dabis, Finn, Gopalakrishnan, Hausman, Herzog, Hsu, et~al.]{rt1}
Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., et~al.
\newblock Rt-1: Robotics transformer for real-world control at scale.
\newblock \emph{arXiv preprint arXiv:2212.06817}, 2022.

\bibitem[Collaboration et~al.(2023)Collaboration, Padalkar, Pooley, Jain, Bewley, Herzog, Irpan, Khazatsky, Rai, Singh, Brohan, Raffin, Wahid, Burgess-Limerick, Kim, Schölkopf, Ichter, Lu, Xu, Finn, Xu, Chi, Huang, Chan, Pan, Fu, Devin, Driess, Pathak, Shah, Büchler, Kalashnikov, Sadigh, Johns, Ceola, Xia, Stulp, Zhou, Sukhatme, Salhotra, Yan, Schiavi, Su, Fang, Shi, Amor, Christensen, Furuta, Walke, Fang, Mordatch, Radosavovic, Leal, Liang, Kim, Schneider, Hsu, Bohg, Bingham, Wu, Wu, Luo, Gu, Tan, Oh, Malik, Tompson, Yang, Lim, Silvério, Han, Rao, Pertsch, Hausman, Go, Gopalakrishnan, Goldberg, Byrne, Oslund, Kawaharazuka, Zhang, Majd, Rana, Srinivasan, Chen, Pinto, Tan, Ott, Lee, Tomizuka, Du, Ahn, Zhang, Ding, Srirama, Sharma, Kim, Kanazawa, Hansen, Heess, Joshi, Suenderhauf, Palo, Shafiullah, Mees, Kroemer, Sanketi, Wohlhart, Xu, Sermanet, Sundaresan, Vuong, Rafailov, Tian, Doshi, Martín-Martín, Mendonca, Shah, Hoque, Julian, Bustamante, Kirmani, Levine, Moore, Bahl, Dass, Song, Xu, Haldar, Adebola,
  Guist, Nasiriany, Schaal, Welker, Tian, Dasari, Belkhale, Osa, Harada, Matsushima, Xiao, Yu, Ding, Davchev, Zhao, Armstrong, Darrell, Jain, Vanhoucke, Zhan, Zhou, Burgard, Chen, Wang, Zhu, Li, Lu, Chebotar, Zhou, Zhu, Xu, Wang, Bisk, Cho, Lee, Cui, hua Wu, Tang, Zhu, Li, Iwasawa, Matsuo, Xu, and Cui]{open_x_embodiment_rt_x_2023}
Collaboration, O. X.-E., Padalkar, A., Pooley, A., Jain, A., Bewley, A., Herzog, A., Irpan, A., Khazatsky, A., Rai, A., Singh, A., Brohan, A., Raffin, A., Wahid, A., Burgess-Limerick, B., Kim, B., Schölkopf, B., Ichter, B., Lu, C., Xu, C., Finn, C., Xu, C., Chi, C., Huang, C., Chan, C., Pan, C., Fu, C., Devin, C., Driess, D., Pathak, D., Shah, D., Büchler, D., Kalashnikov, D., Sadigh, D., Johns, E., Ceola, F., Xia, F., Stulp, F., Zhou, G., Sukhatme, G.~S., Salhotra, G., Yan, G., Schiavi, G., Su, H., Fang, H.-S., Shi, H., Amor, H.~B., Christensen, H.~I., Furuta, H., Walke, H., Fang, H., Mordatch, I., Radosavovic, I., Leal, I., Liang, J., Kim, J., Schneider, J., Hsu, J., Bohg, J., Bingham, J., Wu, J., Wu, J., Luo, J., Gu, J., Tan, J., Oh, J., Malik, J., Tompson, J., Yang, J., Lim, J.~J., Silvério, J., Han, J., Rao, K., Pertsch, K., Hausman, K., Go, K., Gopalakrishnan, K., Goldberg, K., Byrne, K., Oslund, K., Kawaharazuka, K., Zhang, K., Majd, K., Rana, K., Srinivasan, K., Chen, L.~Y., Pinto, L., Tan, L.,
  Ott, L., Lee, L., Tomizuka, M., Du, M., Ahn, M., Zhang, M., Ding, M., Srirama, M.~K., Sharma, M., Kim, M.~J., Kanazawa, N., Hansen, N., Heess, N., Joshi, N.~J., Suenderhauf, N., Palo, N.~D., Shafiullah, N. M.~M., Mees, O., Kroemer, O., Sanketi, P.~R., Wohlhart, P., Xu, P., Sermanet, P., Sundaresan, P., Vuong, Q., Rafailov, R., Tian, R., Doshi, R., Martín-Martín, R., Mendonca, R., Shah, R., Hoque, R., Julian, R., Bustamante, S., Kirmani, S., Levine, S., Moore, S., Bahl, S., Dass, S., Song, S., Xu, S., Haldar, S., Adebola, S., Guist, S., Nasiriany, S., Schaal, S., Welker, S., Tian, S., Dasari, S., Belkhale, S., Osa, T., Harada, T., Matsushima, T., Xiao, T., Yu, T., Ding, T., Davchev, T., Zhao, T.~Z., Armstrong, T., Darrell, T., Jain, V., Vanhoucke, V., Zhan, W., Zhou, W., Burgard, W., Chen, X., Wang, X., Zhu, X., Li, X., Lu, Y., Chebotar, Y., Zhou, Y., Zhu, Y., Xu, Y., Wang, Y., Bisk, Y., Cho, Y., Lee, Y., Cui, Y., hua Wu, Y., Tang, Y., Zhu, Y., Li, Y., Iwasawa, Y., Matsuo, Y., Xu, Z., and Cui, Z.~J.
\newblock Open {X-E}mbodiment: Robotic learning datasets and {RT-X} models.
\newblock \url{https://arxiv.org/abs/2310.08864}, 2023.

\bibitem[Cui et~al.(2022)Cui, Niekum, Gupta, Kumar, and Rajeswaran]{cui2022can}
Cui, Y., Niekum, S., Gupta, A., Kumar, V., and Rajeswaran, A.
\newblock Can foundation models perform zero-shot task specification for robot manipulation?
\newblock In \emph{Learning for Dynamics and Control Conference}, pp.\  893--905. PMLR, 2022.

\bibitem[Damen et~al.(2018)Damen, Doughty, Farinella, Fidler, Furnari, Kazakos, Moltisanti, Munro, Perrett, Price, et~al.]{damen2018scaling}
Damen, D., Doughty, H., Farinella, G.~M., Fidler, S., Furnari, A., Kazakos, E., Moltisanti, D., Munro, J., Perrett, T., Price, W., et~al.
\newblock Scaling egocentric vision: The epic-kitchens dataset.
\newblock In \emph{Proceedings of the European conference on computer vision (ECCV)}, pp.\  720--736, 2018.

\bibitem[Du et~al.(2023{\natexlab{a}})Du, Yang, Dai, Dai, Nachum, Tenenbaum, Schuurmans, and Abbeel]{du2023learning}
Du, Y., Yang, M., Dai, B., Dai, H., Nachum, O., Tenenbaum, J.~B., Schuurmans, D., and Abbeel, P.
\newblock Learning universal policies via text-guided video generation.
\newblock \emph{arXiv preprint arXiv:2302.00111}, 2023{\natexlab{a}}.

\bibitem[Du et~al.(2023{\natexlab{b}})Du, Yang, Florence, Xia, Wahid, Ichter, Sermanet, Yu, Abbeel, Tenenbaum, et~al.]{du2023video}
Du, Y., Yang, M., Florence, P., Xia, F., Wahid, A., Ichter, B., Sermanet, P., Yu, T., Abbeel, P., Tenenbaum, J.~B., et~al.
\newblock Video language planning.
\newblock \emph{arXiv preprint arXiv:2310.10625}, 2023{\natexlab{b}}.

\bibitem[Goyal et~al.(2017)Goyal, Ebrahimi~Kahou, Michalski, Materzynska, Westphal, Kim, Haenel, Fruend, Yianilos, Mueller-Freitag, et~al.]{goyal2017something}
Goyal, R., Ebrahimi~Kahou, S., Michalski, V., Materzynska, J., Westphal, S., Kim, H., Haenel, V., Fruend, I., Yianilos, P., Mueller-Freitag, M., et~al.
\newblock The" something something" video database for learning and evaluating visual common sense.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pp.\  5842--5850, 2017.

\bibitem[Grauman et~al.(2022)Grauman, Westbury, Byrne, Chavis, Furnari, Girdhar, Hamburger, Jiang, Liu, Liu, et~al.]{grauman2022ego4d}
Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu, X., et~al.
\newblock Ego4d: Around the world in 3,000 hours of egocentric video.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  18995--19012, 2022.

\bibitem[Gupta et~al.(2019)Gupta, Kumar, Lynch, Levine, and Hausman]{gupta2019relay}
Gupta, A., Kumar, V., Lynch, C., Levine, S., and Hausman, K.
\newblock Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.11956}, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  770--778, 2016.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'a}r, and Girshick]{he2022masked}
He, K., Chen, X., Xie, S., Li, Y., Doll{\'a}r, P., and Girshick, R.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  16000--16009, 2022.

\bibitem[Hu et~al.(2023)Hu, Li, Zhan, Jia, and Zhang]{hu2023query}
Hu, X., Li, J., Zhan, X., Jia, Q.-S., and Zhang, Y.-Q.
\newblock Query-policy misalignment in preference-based reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2305.17400}, 2023.

\bibitem[Jang et~al.(2022)Jang, Irpan, Khansari, Kappler, Ebert, Lynch, Levine, and Finn]{jang2022bc}
Jang, E., Irpan, A., Khansari, M., Kappler, D., Ebert, F., Lynch, C., Levine, S., and Finn, C.
\newblock Bc-z: Zero-shot task generalization with robotic imitation learning.
\newblock In \emph{Conference on Robot Learning}, pp.\  991--1002. PMLR, 2022.

\bibitem[Karamcheti et~al.(2023)Karamcheti, Nair, Chen, Kollar, Finn, Sadigh, and Liang]{karamcheti2023language}
Karamcheti, S., Nair, S., Chen, A.~S., Kollar, T., Finn, C., Sadigh, D., and Liang, P.
\newblock Language-driven representation learning for robotics.
\newblock \emph{arXiv preprint arXiv:2302.12766}, 2023.

\bibitem[Khandelwal et~al.(2022)Khandelwal, Weihs, Mottaghi, and Kembhavi]{khandelwal2022simple}
Khandelwal, A., Weihs, L., Mottaghi, R., and Kembhavi, A.
\newblock Simple but effective: Clip embeddings for embodied ai.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  14829--14838, 2022.

\bibitem[Korbar et~al.(2019)Korbar, Tran, and Torresani]{korbar2019scsampler}
Korbar, B., Tran, D., and Torresani, L.
\newblock Scsampler: Sampling salient clips from video for efficient action recognition.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  6232--6242, 2019.

\bibitem[Kumar et~al.(2022)Kumar, Singh, Ebert, Nakamoto, Yang, Finn, and Levine]{kumar2022pre}
Kumar, A., Singh, A., Ebert, F., Nakamoto, M., Yang, Y., Finn, C., and Levine, S.
\newblock Pre-training for robots: Offline rl enables learning new tasks from a handful of trials.
\newblock \emph{arXiv preprint arXiv:2210.05178}, 2022.

\bibitem[Laskin et~al.(2020{\natexlab{a}})Laskin, Lee, Stooke, Pinto, Abbeel, and Srinivas]{laskin2020reinforcement}
Laskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., and Srinivas, A.
\newblock Reinforcement learning with augmented data.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 19884--19895, 2020{\natexlab{a}}.

\bibitem[Laskin et~al.(2020{\natexlab{b}})Laskin, Srinivas, and Abbeel]{laskin2020curl}
Laskin, M., Srinivas, A., and Abbeel, P.
\newblock Curl: Contrastive unsupervised representations for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5639--5650. PMLR, 2020{\natexlab{b}}.

\bibitem[Lee et~al.(2021)Lee, Smith, and Abbeel]{lee2021pebble}
Lee, K., Smith, L.~M., and Abbeel, P.
\newblock Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training.
\newblock In \emph{International Conference on Machine Learning}, pp.\  6152--6163. PMLR, 2021.

\bibitem[Li et~al.(2023)Li, Hu, Xu, Liu, Zhan, Jia, and Zhang]{li2023mind}
Li, J., Hu, X., Xu, H., Liu, J., Zhan, X., Jia, Q.-S., and Zhang, Y.-Q.
\newblock Mind the gap: Offline policy optimization for imperfect rewards.
\newblock \emph{arXiv preprint arXiv:2302.01667}, 2023.

\bibitem[Li et~al.(2019)Li, Wei, and Ma]{li2019towards}
Li, Y., Wei, C., and Ma, T.
\newblock Towards explaining the regularization effect of initial large learning rate in training neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Li et~al.(2022)Li, Gao, Yang, Xu, and Wu]{li2022phasic}
Li, Y., Gao, T., Yang, J., Xu, H., and Wu, Y.
\newblock Phasic self-imitative reduction for sparse-reward goal-conditioned reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  12765--12781. PMLR, 2022.

\bibitem[Liu et~al.(2022)Liu, Williams, Jacobson, Fidler, and Litany]{liu2022learning}
Liu, H.-T.~D., Williams, F., Jacobson, A., Fidler, S., and Litany, O.
\newblock Learning smooth neural functions via lipschitz regularization.
\newblock In \emph{ACM SIGGRAPH 2022 Conference Proceedings}, pp.\  1--13, 2022.

\bibitem[Liu et~al.(2023)Liu, Huang, Zheng, Liu, and Li]{liu2023mixmae}
Liu, J., Huang, X., Zheng, J., Liu, Y., and Li, H.
\newblock Mixmae: Mixed and masked autoencoder for efficient pretraining of hierarchical vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  6252--6261, 2023.

\bibitem[Ma et~al.(2022)Ma, Yan, Jayaraman, and Bastani]{ma2022offline}
Ma, J.~Y., Yan, J., Jayaraman, D., and Bastani, O.
\newblock Offline goal-conditioned reinforcement learning via $ f $-advantage regression.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 310--323, 2022.

\bibitem[Ma et~al.(2023{\natexlab{a}})Ma, Kumar, Zhang, Bastani, and Jayaraman]{liv}
Ma, Y.~J., Kumar, V., Zhang, A., Bastani, O., and Jayaraman, D.
\newblock {LIV}: Language-image representations and rewards for robotic control.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), \emph{Proceedings of the 40th International Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  23301--23320. PMLR, 23--29 Jul 2023{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v202/ma23b.html}.

\bibitem[Ma et~al.(2023{\natexlab{b}})Ma, Sodhani, Jayaraman, Bastani, Kumar, and Zhang]{vip}
Ma, Y.~J., Sodhani, S., Jayaraman, D., Bastani, O., Kumar, V., and Zhang, A.
\newblock Vip: Towards universal visual reward and representation via value-implicit pre-training.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{b}}.

\bibitem[Mees et~al.(2022)Mees, Hermann, and Burgard]{mees2022matters}
Mees, O., Hermann, L., and Burgard, W.
\newblock What matters in language conditioned robotic imitation learning over unstructured data.
\newblock \emph{IEEE Robotics and Automation Letters}, 7\penalty0 (4):\penalty0 11205--11212, 2022.

\bibitem[Mendonca et~al.(2023)Mendonca, Bahl, and Pathak]{mendonca2023structured}
Mendonca, R., Bahl, S., and Pathak, D.
\newblock Structured world models from human videos.
\newblock \emph{arXiv preprint arXiv:2308.10901}, 2023.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and Yoshida]{miyato2018spectral}
Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y.
\newblock Spectral normalization for generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1802.05957}, 2018.

\bibitem[Myers et~al.(2023)Myers, He, Fang, Walke, Hansen-Estruch, Cheng, Jalobeanu, Kolobov, Dragan, and Levine]{myers2023goal}
Myers, V., He, A.~W., Fang, K., Walke, H.~R., Hansen-Estruch, P., Cheng, C.-A., Jalobeanu, M., Kolobov, A., Dragan, A., and Levine, S.
\newblock Goal representations for instruction following: A semi-supervised language interface to control.
\newblock In \emph{Conference on Robot Learning}, pp.\  3894--3908. PMLR, 2023.

\bibitem[Nair et~al.(2022)Nair, Mitchell, Chen, Savarese, Finn, et~al.]{nair2022learning}
Nair, S., Mitchell, E., Chen, K., Savarese, S., Finn, C., et~al.
\newblock Learning language-conditioned robot behavior from offline data and crowd-sourced annotation.
\newblock In \emph{Conference on Robot Learning}, pp.\  1303--1315. PMLR, 2022.

\bibitem[Nair et~al.(2023)Nair, Rajeswaran, Kumar, Finn, and Gupta]{r3m}
Nair, S., Rajeswaran, A., Kumar, V., Finn, C., and Gupta, A.
\newblock R3m: A universal visual representation for robot manipulation.
\newblock In Liu, K., Kulic, D., and Ichnowski, J. (eds.), \emph{Proceedings of The 6th Conference on Robot Learning}, volume 205 of \emph{Proceedings of Machine Learning Research}, pp.\  892--909. PMLR, 14--18 Dec 2023.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{ng1999policy}
Ng, A.~Y., Harada, D., and Russell, S.
\newblock Policy invariance under reward transformations: Theory and application to reward shaping.
\newblock In \emph{Icml}, volume~99, pp.\  278--287. Citeseer, 1999.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Oord, A. v.~d., Li, Y., and Vinyals, O.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Oprea et~al.(2020)Oprea, Martinez-Gonzalez, Garcia-Garcia, Castro-Vargas, Orts-Escolano, Garcia-Rodriguez, and Argyros]{oprea2020review}
Oprea, S., Martinez-Gonzalez, P., Garcia-Garcia, A., Castro-Vargas, J.~A., Orts-Escolano, S., Garcia-Rodriguez, J., and Argyros, A.
\newblock A review on deep learning techniques for video prediction.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 44\penalty0 (6):\penalty0 2806--2826, 2020.

\bibitem[Parisi et~al.(2022)Parisi, Rajeswaran, Purushwalkam, and Gupta]{parisi2022unsurprising}
Parisi, S., Rajeswaran, A., Purushwalkam, S., and Gupta, A.
\newblock The unsurprising effectiveness of pre-trained vision models for control.
\newblock In \emph{International Conference on Machine Learning}, pp.\  17359--17371. PMLR, 2022.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Radford et~al.(2021{\natexlab{a}})Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{clip}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International conference on machine learning}, pp.\  8748--8763. PMLR, 2021{\natexlab{a}}.

\bibitem[Radford et~al.(2021{\natexlab{b}})Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International conference on machine learning}, pp.\  8748--8763. PMLR, 2021{\natexlab{b}}.

\bibitem[Radosavovic et~al.(2023)Radosavovic, Xiao, James, Abbeel, Malik, and Darrell]{radosavovic2023real}
Radosavovic, I., Xiao, T., James, S., Abbeel, P., Malik, J., and Darrell, T.
\newblock Real-world robot learning with masked visual pre-training.
\newblock In \emph{Conference on Robot Learning}, pp.\  416--426. PMLR, 2023.

\bibitem[Reed et~al.(2022)Reed, Zolna, Parisotto, Colmenarejo, Novikov, Barth-Maron, Gimenez, Sulsky, Kay, Springenberg, et~al.]{reed2022generalist}
Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S.~G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J.~T., et~al.
\newblock A generalist agent.
\newblock \emph{arXiv preprint arXiv:2205.06175}, 2022.

\bibitem[Reid et~al.(2022)Reid, Yamada, and Gu]{reid2022can}
Reid, M., Yamada, Y., and Gu, S.~S.
\newblock Can wikipedia help offline reinforcement learning?
\newblock \emph{arXiv preprint arXiv:2201.12122}, 2022.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{sanh2019distilbert}
Sanh, V., Debut, L., Chaumond, J., and Wolf, T.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.
\newblock \emph{arXiv preprint arXiv:1910.01108}, 2019.

\bibitem[Seo et~al.(2022)Seo, Lee, James, and Abbeel]{seo2022reinforcement}
Seo, Y., Lee, K., James, S.~L., and Abbeel, P.
\newblock Reinforcement learning with action-free pre-training from videos.
\newblock In \emph{International Conference on Machine Learning}, pp.\  19561--19579. PMLR, 2022.

\bibitem[Sermanet et~al.(2018)Sermanet, Lynch, Chebotar, Hsu, Jang, Schaal, Levine, and Brain]{sermanet2018time}
Sermanet, P., Lynch, C., Chebotar, Y., Hsu, J., Jang, E., Schaal, S., Levine, S., and Brain, G.
\newblock Time-contrastive networks: Self-supervised learning from video.
\newblock In \emph{2018 IEEE international conference on robotics and automation (ICRA)}, pp.\  1134--1141. IEEE, 2018.

\bibitem[Shah et~al.(2023)Shah, Mart{\'\i}n-Mart{\'\i}n, and Zhu]{shah2023mutex}
Shah, R., Mart{\'\i}n-Mart{\'\i}n, R., and Zhu, Y.
\newblock Mutex: Learning unified policies from multimodal task specifications.
\newblock In \emph{7th Annual Conference on Robot Learning}, 2023.

\bibitem[Shah \& Kumar(2021)Shah and Kumar]{shah2021rrl}
Shah, R.~M. and Kumar, V.
\newblock Rrl: Resnet as representation for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  9465--9476. PMLR, 2021.

\bibitem[Shi et~al.(2013)Shi, Petriu, and Laganiere]{shi2013sampling}
Shi, F., Petriu, E., and Laganiere, R.
\newblock Sampling strategies for real-time action recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  2595--2602, 2013.

\bibitem[Shridhar et~al.(2023)Shridhar, Manuelli, and Fox]{shridhar2023perceiver}
Shridhar, M., Manuelli, L., and Fox, D.
\newblock Perceiver-actor: A multi-task transformer for robotic manipulation.
\newblock In \emph{Conference on Robot Learning}, pp.\  785--799. PMLR, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Virmaux \& Scaman(2018)Virmaux and Scaman]{virmaux2018lipschitz}
Virmaux, A. and Scaman, K.
\newblock Lipschitz regularity of deep neural networks: analysis and efficient estimation.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Walke et~al.(2023)Walke, Black, Zhao, Vuong, Zheng, Hansen-Estruch, He, Myers, Kim, Du, et~al.]{walke2023bridgedata}
Walke, H.~R., Black, K., Zhao, T.~Z., Vuong, Q., Zheng, C., Hansen-Estruch, P., He, A.~W., Myers, V., Kim, M.~J., Du, M., et~al.
\newblock Bridgedata v2: A dataset for robot learning at scale.
\newblock In \emph{Conference on Robot Learning}, pp.\  1723--1736. PMLR, 2023.

\bibitem[Wang et~al.(2023)Wang, Cheng, Zhan, Li, Song, and Liu]{wang2023openchat}
Wang, G., Cheng, S., Zhan, X., Li, X., Song, S., and Liu, Y.
\newblock Openchat: Advancing open-source language models with mixed-quality data.
\newblock \emph{arXiv preprint arXiv:2309.11235}, 2023.

\bibitem[Wang et~al.(2018)Wang, Xiong, Wang, Qiao, Lin, Tang, and Van~Gool]{wang2018temporal}
Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., and Van~Gool, L.
\newblock Temporal segment networks for action recognition in videos.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 41\penalty0 (11):\penalty0 2740--2755, 2018.

\bibitem[Williams et~al.(2017)Williams, Aldrich, and Theodorou]{williams2017model}
Williams, G., Aldrich, A., and Theodorou, E.~A.
\newblock Model predictive path integral control: From theory to parallel computation.
\newblock \emph{Journal of Guidance, Control, and Dynamics}, 40\penalty0 (2):\penalty0 344--357, 2017.

\bibitem[Xiao et~al.(2022)Xiao, Radosavovic, Darrell, and Malik]{xiao2022masked}
Xiao, T., Radosavovic, I., Darrell, T., and Malik, J.
\newblock Masked visual pre-training for motor control.
\newblock \emph{arXiv preprint arXiv:2203.06173}, 2022.

\bibitem[Yarats et~al.(2021)Yarats, Fergus, and Kostrikov]{yarats2021image}
Yarats, D., Fergus, R., and Kostrikov, I.
\newblock Image augmentation is all you need: Regularizing deep reinforcement learning from pixels.
\newblock In \emph{9th International Conference on Learning Representations, ICLR 2021}, 2021.

\bibitem[Zakharov et~al.(2022)Zakharov, Guo, and Fountas]{zakharov2022long}
Zakharov, A., Guo, Q., and Fountas, Z.
\newblock Long-horizon video prediction using a dynamic latent hierarchy.
\newblock \emph{arXiv preprint arXiv:2212.14376}, 2022.

\bibitem[Zhang et~al.(2021)Zhang, McAllister, Calandra, Gal, and Levine]{zhang2021learning}
Zhang, A., McAllister, R.~T., Calandra, R., Gal, Y., and Levine, S.
\newblock Learning invariant representations for reinforcement learning without reconstruction.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=-2FCwDKRREu}.

\bibitem[Zhang et~al.(2023)Zhang, Zhao, Chen, Mi, Zhu, and Geng]{zhang2023closer}
Zhang, Y., Zhao, J., Chen, Z., Mi, S., Zhu, H., and Geng, X.
\newblock A closer look at video sampling for sequential action recognition.
\newblock \emph{IEEE Transactions on Circuits and Systems for Video Technology}, 2023.

\bibitem[Zhi et~al.(2021)Zhi, Tong, Wang, and Wu]{zhi2021mgsampler}
Zhi, Y., Tong, Z., Wang, L., and Wu, G.
\newblock Mgsampler: An explainable sampling strategy for video action recognition.
\newblock In \emph{Proceedings of the IEEE/CVF International conference on Computer Vision}, pp.\  1513--1522, 2021.

\bibitem[Zitkovich et~al.(2023)Zitkovich, Yu, Xu, Xu, Xiao, Xia, Wu, Wohlhart, Welker, Wahid, et~al.]{rt2}
Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., et~al.
\newblock Rt-2: Vision-language-action models transfer web knowledge to robotic control.
\newblock In \emph{Conference on Robot Learning}, pp.\  2165--2183. PMLR, 2023.

\end{thebibliography}
