\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Baevski et~al.(2020)Baevski, Zhou, Mohamed, and
  Auli]{baevski2020wav2vec}
Baevski, A., Zhou, Y., Mohamed, A., and Auli, M.
\newblock wav2vec 2.0: A framework for self-supervised learning of speech
  representations.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  12435--12446. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf}.

\bibitem[Beylkin \& Mohlenkamp(2002)Beylkin and
  Mohlenkamp]{beylkin2002numerical}
Beylkin, G. and Mohlenkamp, M.~J.
\newblock Numerical operator calculus in higher dimensions.
\newblock \emph{Proceedings of the National Academy of Sciences}, 99\penalty0
  (16):\penalty0 10246--10251, 2002.

\bibitem[Beylkin et~al.(2009)Beylkin, Garcke, and
  Mohlenkamp]{beylkin2009multivariate}
Beylkin, G., Garcke, J., and Mohlenkamp, M.~J.
\newblock Multivariate regression and machine learning with sums of separable
  functions.
\newblock \emph{SIAM Journal on Scientific Computing}, 31\penalty0
  (3):\penalty0 1840--1857, 2009.

\bibitem[Bhojanapalli et~al.(2020)Bhojanapalli, Yun, Rawat, Reddi, and
  Kumar]{bhojanapalli2020low}
Bhojanapalli, S., Yun, C., Rawat, A.~S., Reddi, S., and Kumar, S.
\newblock Low-rank bottleneck in multi-head attention models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  864--873. PMLR, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{GPT3}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
  D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
  S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
  I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  1876--1900. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Brunner et~al.(2020)Brunner, Liu, Pascual, Richter, Ciaramita, and
  Wattenhofer]{brunner2020identifiability}
Brunner, G., Liu, Y., Pascual, D., Richter, O., Ciaramita, M., and Wattenhofer,
  R.
\newblock On identifiability in transformers.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=BJg1f6EFDB}.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel,
  Srinivas, and Mordatch]{chen2021decisiontransformer}
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P.,
  Srinivas, A., and Mordatch, I.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{arXiv preprint arXiv:2106.01345}, 2021.

\bibitem[Chen et~al.(2020)Chen, Radford, Child, Wu, Jun, Luan, and
  Sutskever]{chen2020generative}
Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I.
\newblock Generative pretraining from pixels.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  1691--1703. PMLR,
  13--18 Jul 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/chen20s.html}.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019sparsetransformer}
Child, R., Gray, S., Radford, A., and Sutskever, I.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{URL https://openai.com/blog/sparse-transformers}, 2019.

\bibitem[Cohen \& Shashua(2017)Cohen and Shashua]{cohen2017inductive}
Cohen, N. and Shashua, A.
\newblock Inductive bias of deep convolutional networks through pooling
  geometry.
\newblock In \emph{5th International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Cohen et~al.(2017)Cohen, Sharir, Levine, Tamari, Yakira, and
  Shashua]{cohen2017analysis}
Cohen, N., Sharir, O., Levine, Y., Tamari, R., Yakira, D., and Shashua, A.
\newblock Analysis and design of convolutional networks via hierarchical tensor
  decompositions.
\newblock \emph{arXiv preprint arXiv:1705.02302}, 2017.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M., Lee, K., and Toutanova, K.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In Burstein, J., Doran, C., and Solorio, T. (eds.), \emph{Proceedings
  of the 2019 Conference of the North American Chapter of the Association for
  Computational Linguistics: Human Language Technologies, {NAACL-HLT} 2019,
  Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)}, pp.\
   4171--4186. Association for Computational Linguistics, 2019.
\newblock \doi{10.18653/v1/n19-1423}.
\newblock URL \url{https://doi.org/10.18653/v1/n19-1423}.

\bibitem[Dhariwal et~al.(2020)Dhariwal, Jun, Payne, Kim, Radford, and
  Sutskever]{dhariwal2020jukebox}
Dhariwal, P., Jun, H., Payne, C., Kim, J.~W., Radford, A., and Sutskever, I.
\newblock Jukebox: A generative model for music.
\newblock \emph{arXiv preprint arXiv:2005.00341}, 2020.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2021an}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[Hackbusch(2006)]{hackbusch2006efficient}
Hackbusch, W.
\newblock On the efficient evaluation of coalescence integrals in population
  balance models.
\newblock \emph{Computing}, 78\penalty0 (2):\penalty0 145--159, 2006.

\bibitem[Harrison et~al.(2003)Harrison, Fann, Yanai, and
  Beylkin]{harrison2003multiresolution}
Harrison, R.~J., Fann, G.~I., Yanai, T., and Beylkin, G.
\newblock Multiresolution quantum chemistry in multiwavelet bases.
\newblock In \emph{Computational Science-ICCS 2003}, pp.\  103--110. Springer,
  2003.

\bibitem[Henighan et~al.(2020)Henighan, Kaplan, Katz, Chen, Hesse, Jackson,
  Jun, Brown, Dhariwal, Gray, et~al.]{henighan2020scaling}
Henighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, J., Jun, H.,
  Brown, T.~B., Dhariwal, P., Gray, S., et~al.
\newblock Scaling laws for autoregressive generative modeling.
\newblock \emph{arXiv preprint arXiv:2010.14701}, 2020.

\bibitem[Jain \& Wallace(2019)Jain and Wallace]{AttentionIsNotExplanation}
Jain, S. and Wallace, B.~C.
\newblock {A}ttention is not {E}xplanation.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pp.\  3543--3556,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1357}.
\newblock URL \url{https://www.aclweb.org/anthology/N19-1357}.

\bibitem[Jun et~al.(2020)Jun, Child, Chen, Schulman, Ramesh, Radford, and
  Sutskever]{child2020distributionaugmentation}
Jun, H., Child, R., Chen, M., Schulman, J., Ramesh, A., Radford, A., and
  Sutskever, I.
\newblock Distribution augmentation for generative modeling.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  5006--5019. PMLR,
  13--18 Jul 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/jun20a.html}.

\bibitem[Kudo \& Richardson(2018)Kudo and
  Richardson]{kudo-richardson-2018-sentencepiece}
Kudo, T. and Richardson, J.
\newblock {S}entence{P}iece: A simple and language independent subword
  tokenizer and detokenizer for neural text processing.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pp.\  66--71, Brussels,
  Belgium, November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-2012}.
\newblock URL \url{https://www.aclweb.org/anthology/D18-2012}.

\bibitem[Lan et~al.(2020)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut]{Lan2020ALBERT}
Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=H1eA7AEtvS}.

\bibitem[Levine et~al.(2018{\natexlab{a}})Levine, Sharir, Ziv, and
  Shashua]{levine2018benefits}
Levine, Y., Sharir, O., Ziv, A., and Shashua, A.
\newblock {Benefits of depth for long-term memory of recurrent networks}.
\newblock \emph{(ICLR 2018) International Conference on Learning
  Representations workshop}, 2018{\natexlab{a}}.

\bibitem[Levine et~al.(2018{\natexlab{b}})Levine, Yakira, Cohen, and
  Shashua]{levine2018deep}
Levine, Y., Yakira, D., Cohen, N., and Shashua, A.
\newblock Deep learning and quantum entanglement: Fundamental connections with
  implications to network design.
\newblock In \emph{International Conference on Learning Representations},
  2018{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=SywXXwJAb}.

\bibitem[Levine et~al.(2019)Levine, Sharir, Cohen, and
  Shashua]{levine2019quantum}
Levine, Y., Sharir, O., Cohen, N., and Shashua, A.
\newblock Quantum entanglement in deep learning architectures.
\newblock \emph{Physical review letters}, 122\penalty0 (6):\penalty0 065301,
  2019.

\bibitem[Levine et~al.(2020)Levine, Wies, Sharir, Bata, and
  Shashua]{levine2020limits}
Levine, Y., Wies, N., Sharir, O., Bata, H., and Shashua, A.
\newblock Limits to depth efficiencies of self-attention.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  22640--22651. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/ff4dfdf5904e920ce52b48c1cef97829-Paper.pdf}.

\bibitem[Levine et~al.(2021)Levine, Lenz, Lieber, Abend, Leyton-Brown,
  Tennenholtz, and Shoham]{levine2021pmimasking}
Levine, Y., Lenz, B., Lieber, O., Abend, O., Leyton-Brown, K., Tennenholtz, M.,
  and Shoham, Y.
\newblock {\{}PMI{\}}-masking: Principled masking of correlated spans.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=3Aoft6NWFej}.

\bibitem[Press \& Wolf(2017)Press and Wolf]{press-wolf-2017-using}
Press, O. and Wolf, L.
\newblock Using the output embedding to improve language models.
\newblock In \emph{Proceedings of the 15th Conference of the {E}uropean Chapter
  of the Association for Computational Linguistics: Volume 2, Short Papers},
  pp.\  157--163, Valencia, Spain, April 2017. Association for Computational
  Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/E17-2025}.

\bibitem[Press et~al.(2020)Press, Smith, and Levy]{press2019improving}
Press, O., Smith, N.~A., and Levy, O.
\newblock Improving transformer models by reordering their sublayers.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  2996--3005, Online, July 2020.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.270}.
\newblock URL \url{https://www.aclweb.org/anthology/2020.acl-main.270}.

\bibitem[Pruthi et~al.(2020)Pruthi, Gupta, Dhingra, Neubig, and
  Lipton]{pruthi2019learning}
Pruthi, D., Gupta, M., Dhingra, B., Neubig, G., and Lipton, Z.~C.
\newblock Learning to deceive with attention-based explanations.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  4782--4793, Online, July 2020.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.432}.
\newblock URL \url{https://www.aclweb.org/anthology/2020.acl-main.432}.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{radford2018improving}
Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (140):\penalty0 1--67, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Rao et~al.(2021)Rao, Liu, Verkuil, Meier, Canny, Abbeel, Sercu, and
  Rives]{rao2021msa}
Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.~F., Abbeel, P., Sercu, T.,
  and Rives, A.
\newblock Msa transformer.
\newblock \emph{bioRxiv}, 2021.

\bibitem[Richter \& Wattenhofer(2020)Richter and
  Wattenhofer]{richter2020normalized}
Richter, O. and Wattenhofer, R.
\newblock Normalized attention without probability cage.
\newblock \emph{arXiv preprint arXiv:2005.09561}, 2020.

\bibitem[Rives et~al.(2019)Rives, Meier, Sercu, Goyal, Lin, Liu, Guo, Ott,
  Zitnick, Ma, and Fergus]{rives2019biological}
Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M.,
  Zitnick, C.~L., Ma, J., and Fergus, R.
\newblock Biological structure and function emerge from scaling unsupervised
  learning to 250 million protein sequences.
\newblock \emph{bioRxiv}, 2019.
\newblock \doi{10.1101/622803}.
\newblock URL \url{https://www.biorxiv.org/content/10.1101/622803v4}.

\bibitem[Saxton et~al.(2019)Saxton, Grefenstette, Hill, and
  Kohli]{saxton2018analysing}
Saxton, D., Grefenstette, E., Hill, F., and Kohli, P.
\newblock Analysing mathematical reasoning abilities of neural models.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=H1gR5iR5FX}.

\bibitem[{Schuster} \& {Nakajima}(2012){Schuster} and {Nakajima}]{wordpiece}
{Schuster}, M. and {Nakajima}, K.
\newblock Japanese and korean voice search.
\newblock In \emph{2012 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pp.\  5149--5152, 2012.
\newblock \doi{10.1109/ICASSP.2012.6289079}.

\bibitem[Sennrich et~al.(2016)Sennrich, Haddow, and
  Birch]{sennrich-etal-2016-neural}
Sennrich, R., Haddow, B., and Birch, A.
\newblock Neural machine translation of rare words with subword units.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  1715--1725,
  Berlin, Germany, August 2016. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P16-1162}.
\newblock URL \url{https://www.aclweb.org/anthology/P16-1162}.

\bibitem[Sharir et~al.(2020)Sharir, Levine, Wies, Carleo, and
  Shashua]{sharir2020deep}
Sharir, O., Levine, Y., Wies, N., Carleo, G., and Shashua, A.
\newblock Deep autoregressive models for the efficient variational simulation
  of many-body quantum systems.
\newblock \emph{Physical review letters}, 124\penalty0 (2):\penalty0 020503,
  2020.

\bibitem[Shazeer(2020)]{craffel2021t511}
Shazeer, N.
\newblock Experimental t5 pre-trained model checkpoints.
\newblock
  \url{https://github.com/google-research/text-to-text-transfer-transformer/blob/master/released_checkpoints.md},
  4 2020.
\newblock (Accessed on 06/06/2021).

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017transformer}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H.~M., Fergus,
  R., Vishwanathan, S. V.~N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 30: Annual Conference on Neural Information
  Processing Systems 2017, 4-9 December 2017, Long Beach, CA, {USA}}, pp.\
  5998--6008, 2017.
\newblock URL \url{http://papers.nips.cc/paper/7181-attention-is-all-you-need}.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Wang et~al.(2017)Wang, Sun, Li, Zhang, and Xu]{wang2017RaptorX}
Wang, S., Sun, S., Li, Z., Zhang, R., and Xu, J.
\newblock Accurate de novo prediction of protein contact map by ultra-deep
  learning model.
\newblock \emph{PLOS Computational Biology}, 13\penalty0 (1):\penalty0 1--34,
  01 2017.
\newblock \doi{10.1371/journal.pcbi.1005324}.
\newblock URL \url{https://doi.org/10.1371/journal.pcbi.1005324}.

\bibitem[Weissenborn et~al.(2020)Weissenborn, Täckström, and
  Uszkoreit]{Weissenborn2020Scaling}
Weissenborn, D., Täckström, O., and Uszkoreit, J.
\newblock Scaling autoregressive video models.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rJgsskrFwH}.

\bibitem[Xue et~al.(2021{\natexlab{a}})Xue, Barua, Constant, Al-Rfou, Narang,
  Kale, Roberts, and Raffel]{xue2021byt5}
Xue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S., Kale, M., Roberts,
  A., and Raffel, C.
\newblock Byt5: Towards a token-free future with pre-trained byte-to-byte
  models.
\newblock \emph{arXiv preprint arXiv:2105.13626}, 2021{\natexlab{a}}.

\bibitem[Xue et~al.(2021{\natexlab{b}})Xue, Constant, Roberts, Kale, Al-Rfou,
  Siddhant, Barua, and Raffel]{xue-etal-2021-mt5}
Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua,
  A., and Raffel, C.
\newblock m{T}5: A massively multilingual pre-trained text-to-text transformer.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pp.\  483--498, Online, June 2021{\natexlab{b}}. Association
  for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/2021.naacl-main.41}.

\bibitem[Yang et~al.(2018)Yang, Dai, Salakhutdinov, and
  Cohen]{yang2018breaking}
Yang, Z., Dai, Z., Salakhutdinov, R., and Cohen, W.~W.
\newblock Breaking the softmax bottleneck: A high-rank {RNN} language model.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=HkwZSG-CZ}.

\end{thebibliography}
