


@Misc{wolf20,
  author =    {Thomas Wolf},
  title =     {Huggingface},
  howpublished = {http://huggingface.co},
  year =      2020}

@inproceedings{pennington_glove_2014,
	address = {Doha, Qatar},
	title = {Glove: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {Glove},
	url = {http://aclweb.org/anthology/D14-1162},
	doi = {10.3115/v1/D14-1162},
	abstract = {Recent methods for learning vector space representations of words have succeeded in capturing ﬁne-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efﬁciently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
	language = {en},
	urldate = {2020-12-10},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	year = {2014},
	pages = {1532--1543},
	annote = {MAIN POINTS 
 
* Create fixed semantical embeddings for the words of a lexicon
 
* Use a global log-bilinear regression model that combines: global matrix factorization and local context window
 
* Leverages statistical information by training only on the non-zero elements in a word-word co-occurrence matrix},
	file = {Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:/Users/alexpsq/Zotero/storage/JSPKB7DT/Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:application/pdf},
}

@article{schaefer_atlas,
    title= {Local-Global parcellation of the human cerebral cortex from intrinsic functional connectivity MRI},
    url = {http://people.csail.mit.edu/ythomas/publications/2018LocalGlobal-CerebCor.pdf},
    author = {Schaefer, A. and Kong, R. and Gordon, E.M. and Laumann, T.O. and Zuo, X.N. and Holmes, A.J. and Eickhoff, S.B. and Yeo, B.T.T.},
    journal = {Cerebral Cortex},
    volume=29,
    pages={3095--3114},
    year = {2018},
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {MAIN POINTS
 
* Introduce the transformers architecture
 
* Layer-wise structure with only attention mechanisms (without recurrence or convolutions) in encoder and decoder layer stacks
 
* Parallelizable computations
 
* 1 embedding layer and then X transformer layer},
	file = {Vaswani et al. - 2017 - Attention Is All You Need.pdf:/Users/alexpsq/Zotero/storage/NZQNWH35/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf},
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2020-12-10},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	annote = {MAIN POINTS
 
* Stack of encoder layers based on transformer architecture.
 
* Trained on 3B words with Mask Language Modeling and Next Sentence Prediciton tasks.
 
* 1 embedding layer and X encoder layers
 
* Bi-directional model},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/GBLRMMN9/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/GZFJIEMF/1810.html:text/html},
}




@Misc{radford20,
  author =    {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  title =     {GPT-2},
  howpublished = {http://github.com/openai/gpt-2},
  year =      2020}

@article{radford_language_nodate,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
  year = {2019},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	pages = {24},
	annote = {MAIN POINTS
 
* Stack of decoder layers, based on transformer architecture.
 
* Trained on 40Gb of data on a language modeling task (= next word prediction)
 
* 1 embedding layer and X decoder layers
 
* Auto-regressive model},
	file = {Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:/Users/alexpsq/Zotero/storage/BFCJFPQT/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:application/pdf},
}

@article{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1907.11692 [cs]},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.11692},
	keywords = {Computer Science - Computation and Language},
	annote = {MAIN POINTS
 
* BERT trained longer, on more data, with dynamic masking, full-sentences without NSP loss, large mini-batches and a larger byte-level BPE
 
* Hyperparameter choice has significant result on final result},
	file = {Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:/Users/alexpsq/Zotero/storage/TJ7HHJUM/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:application/pdf},
}

@article{yang_xlnet_2020,
	title = {{XLNet}: {Generalized} {Autoregressive} {Pretraining} for {Language} {Understanding}},
	shorttitle = {{XLNet}},
	url = {http://arxiv.org/abs/1906.08237},
	abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-ﬁnetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.1.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1906.08237 [cs]},
	author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
	month = jan,
	year = {2020},
	note = {arXiv: 1906.08237},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {MAIN POINTS (TO CORRECT + DETAILS)
 
* Generalized autoregressive pretraining method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order
 
* Overcome the limitation of BERT thanks to its auto-regressive formluation},
	file = {Yang et al. - 2020 - XLNet Generalized Autoregressive Pretraining for .pdf:/Users/alexpsq/Zotero/storage/P3KGNPNR/Yang et al. - 2020 - XLNet Generalized Autoregressive Pretraining for .pdf:application/pdf},
}

@article{lewis_bart_2019,
	title = {{BART}: {Denoising} {Sequence}-to-{Sequence} {Pre}-training for {Natural} {Language} {Generation}, {Translation}, and {Comprehension}},
	shorttitle = {{BART}},
	url = {http://arxiv.org/abs/1910.13461},
	abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, ﬁnding the best performance by both randomly shufﬂing the order of the original sentences and using a novel in-ﬁlling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when ﬁne tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most inﬂuence end-task performance.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1910.13461 [cs, stat]},
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.13461},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {MAIN POINTS
 
* Denoising autoencoder for pretraining sequence to sequence models
 
* Combines a bidirectional encoder with an autoregressive decoder.
 
* No feed-forward network in it.
 
* Trained by corrupting text with an arbitrary noising function and learning a model to reconstruct the signal
 
* works best when both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token.
 
* Particularly effective when fine-tuned for text generation but works well for comprehension tasks too.},
	file = {Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf:/Users/alexpsq/Zotero/storage/8AGET7ZT/Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf:application/pdf},
}

@article{lan_albert_2020,
	title = {{ALBERT}: {A} {Lite} {BERT} for {Self}-supervised {Learning} of {Language} {Representations}},
	shorttitle = {{ALBERT}},
	url = {http://arxiv.org/abs/1909.11942},
	abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameterreduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1909.11942 [cs]},
	author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
	month = feb,
	year = {2020},
	note = {arXiv: 1909.11942},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {MAIN POINTS
 
* Clip layer weights of BERT
 
* Factorization of the embedding parameters
 
* Good performance on benchmark datasets
 
* Lighter model
 
* Trained with MLM and Sentence Order Prediction (SOP)},
	file = {Lan et al. - 2020 - ALBERT A Lite BERT for Self-supervised Learning o.pdf:/Users/alexpsq/Zotero/storage/E92NVKFC/Lan et al. - 2020 - ALBERT A Lite BERT for Self-supervised Learning o.pdf:application/pdf},
}

@article{goldstein_thinking_nodate,
	title = {Thinking ahead: prediction in context as a keystone of language in humans and machines},
	abstract = {Departing from classical rule-based linguistic models, advances in deep learning have led to the development of a new family of self-supervised deep language models (DLMs). These models are trained using a simple self-supervised autoregressive objective, which aims to predict the next word in the context of preceding words in real-life corpora. After training, autoregressive DLMs are able to generate new “context-aware” sentences with appropriate syntax and convincing semantics and pragmatics. Here we provide empirical evidence for the deep connection between autoregressive DLMs and the human language faculty using a 30-min spoken narrative and electrocorticographic (ECoG) recordings. Behaviorally, we demonstrate that humans have a remarkable capacity for word prediction in natural contexts, and that, given a sufficient context window, DLMs can attain human-level prediction performance. Next, we leverage DLM embeddings to demonstrate that many electrodes spontaneously predict the meaning of upcoming words, even hundreds of milliseconds before they are perceived. Finally, we demonstrate that contextual embeddings derived from autoregressive DLMs capture neural representations of the unique, context-specific meaning of words in the narrative. Our findings suggest that deep language models provide an important step toward creating a biologically feasible computational framework for generative language.},
	language = {en},
	author = {Goldstein, Ariel and Zada, Zaid and Buchnik, Eliav and Schain, Mariano and Price, Amy and Nastase, Samuel A and Feder, Amir and Emanuel, Dotan and Cohen, Alon and Jansen, Aren and Gazula, Harshvardhan and Choe, Gina and Rao, Aditi and Kim, Catherine and Casto, Colton and Flinker, Adeen and Devore, Sasha and Doyle, Werner and Friedman, Daniel and Dugan, Patricia and Hassidim, Avinatan and Brenner, Michael and Matias, Yossi and Norman, Kenneth A and Devinsky, Orrin and Hasson, Uri},
	pages = {40},
	file = {Goldstein et al. - Thinking ahead prediction in context as a keyston.pdf:/Users/alexpsq/Zotero/storage/M2HZWI9T/Goldstein et al. - Thinking ahead prediction in context as a keyston.pdf:application/pdf},
}

@article{le_flaubert_2020,
	title = {{FlauBERT}: {Unsupervised} {Language} {Model} {Pre}-training for {French}},
	shorttitle = {{FlauBERT}},
	url = {http://arxiv.org/abs/1912.05372},
	abstract = {Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efﬁcient way to pre-train continuous word representations that can be ﬁne-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientiﬁc Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classiﬁcation, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT as well as a uniﬁed evaluation protocol for the downstream tasks, called FLUE (French Language Understanding Evaluation), are shared to the research community for further reproducible experiments in French NLP.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1912.05372 [cs]},
	author = {Le, Hang and Vial, Loïc and Frej, Jibril and Segonne, Vincent and Coavoux, Maximin and Lecouteux, Benjamin and Allauzen, Alexandre and Crabbé, Benoît and Besacier, Laurent and Schwab, Didier},
	month = mar,
	year = {2020},
	note = {arXiv: 1912.05372},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {MAIN POINTS
 
* BERT trained on french data
* Trained on 71Gb of data},
	file = {Le et al. - 2020 - FlauBERT Unsupervised Language Model Pre-training.pdf:/Users/alexpsq/Zotero/storage/IAVZQJ6L/Le et al. - 2020 - FlauBERT Unsupervised Language Model Pre-training.pdf:application/pdf},
}

@misc{noauthor_long_nodate,
	title = {{LONG} {SHORT}-{TERM} {MEMORY}.pdf},
	annote = {MAIN POINTS
 
* LSTM are RNN granted with gating mechanisms that enable them to learn long-range dependencies
 
* Each units has multiple gates: in, out, forget, cell, hidden.},
	file = {LONG SHORT-TERM MEMORY.pdf:/Users/alexpsq/Zotero/storage/SWLQDVZ7/LONG SHORT-TERM MEMORY.pdf:application/pdf},
}

@book{poldrack_handbook_2011,
	address = {Cambridge},
	title = {Handbook of {Functional} {MRI} {Data} {Analysis}},
	isbn = {978-0-511-89502-9},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511895029},
	language = {en},
	urldate = {2020-12-10},
	publisher = {Cambridge University Press},
	author = {Poldrack, Russell A. and Nichols, Thomas and Mumford, Jeanette},
	year = {2011},
	doi = {10.1017/CBO9780511895029},
	file = {Poldrack et al. - 2011 - Handbook of Functional MRI Data Analysis.pdf:/Users/alexpsq/Zotero/storage/3MWDZCNL/Poldrack et al. - 2011 - Handbook of Functional MRI Data Analysis.pdf:application/pdf},
}

@article{hewitt_structural_nodate,
	title = {A {Structural} {Probe} for {Finding} {Syntax} in {Word} {Representations}},
	abstract = {Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network’s word representation space. The probe identiﬁes a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models’ vector geometry.},
	language = {en},
	author = {Hewitt, John and Manning, Christopher D},
	pages = {10},
	file = {Hewitt et Manning - A Structural Probe for Finding Syntax in Word Repr.pdf:/Users/alexpsq/Zotero/storage/LWCI6JXX/Hewitt et Manning - A Structural Probe for Finding Syntax in Word Repr.pdf:application/pdf},
}

@article{kell_task-optimized_2018,
	title = {A {Task}-{Optimized} {Neural} {Network} {Replicates} {Human} {Auditory} {Behavior}, {Predicts} {Brain} {Responses}, and {Reveals} a {Cortical} {Processing} {Hierarchy}},
	volume = {98},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627318302502},
	doi = {10.1016/j.neuron.2018.03.044},
	abstract = {A core goal of auditory neuroscience is to build quantitative models that predict cortical responses to natural sounds. Reasoning that a complete model of auditory cortex must solve ecologically relevant tasks, we optimized hierarchical neural networks for speech and music recognition. The best-performing network contained separate music and speech pathways following early shared processing, potentially replicating human cortical organization. The network performed both tasks as well as humans and exhibited human-like errors despite not being optimized to do so, suggesting common constraints on network and human performance. The network predicted fMRI voxel responses substantially better than traditional spectrotemporal ﬁlter models throughout auditory cortex. It also provided a quantitative signature of cortical representational hierarchy—primary and non-primary responses were best predicted by intermediate and late network layers, respectively. The results suggest that task optimization provides a powerful set of tools for modeling sensory systems.},
	language = {en},
	number = {3},
	urldate = {2020-12-10},
	journal = {Neuron},
	author = {Kell, Alexander J.E. and Yamins, Daniel L.K. and Shook, Erica N. and Norman-Haignere, Sam V. and McDermott, Josh H.},
	month = may,
	year = {2018},
	pages = {630--644.e16},
	file = {Kell et al. - 2018 - A Task-Optimized Neural Network Replicates Human A.pdf:/Users/alexpsq/Zotero/storage/NJG3UCXB/Kell et al. - 2018 - A Task-Optimized Neural Network Replicates Human A.pdf:application/pdf},
}

@article{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for ﬁrst-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efﬁcient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the inﬁnity norm.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {Kingma et Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:/Users/alexpsq/Zotero/storage/ZCE77TFI/Kingma et Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf},
}

@article{amunts_architectonic_2015,
	title = {Architectonic {Mapping} of the {Human} {Brain} beyond {Brodmann}},
	volume = {88},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627315010727},
	doi = {10.1016/j.neuron.2015.12.001},
	language = {en},
	number = {6},
	urldate = {2020-12-10},
	journal = {Neuron},
	author = {Amunts, Katrin and Zilles, Karl},
	month = dec,
	year = {2015},
	pages = {1086--1107},
	file = {Amunts et Zilles - 2015 - Architectonic Mapping of the Human Brain beyond Br.pdf:/Users/alexpsq/Zotero/storage/ZRD3KXXM/Amunts et Zilles - 2015 - Architectonic Mapping of the Human Brain beyond Br.pdf:application/pdf},
}

@article{goldberg_assessing_2019,
	title = {Assessing {BERT}'s {Syntactic} {Abilities}},
	url = {http://arxiv.org/abs/1901.05287},
	abstract = {I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) “coloreless green ideas” subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inﬂection; and (3) manually crafted stimuli for subject-verb agreement and reﬂexive anaphora phenomena. The BERT model performs remarkably well on all cases.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1901.05287 [cs]},
	author = {Goldberg, Yoav},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.05287},
	keywords = {Computer Science - Computation and Language},
	file = {Goldberg - 2019 - Assessing BERT's Syntactic Abilities.pdf:/Users/alexpsq/Zotero/storage/TUG2SQNP/Goldberg - 2019 - Assessing BERT's Syntactic Abilities.pdf:application/pdf},
}

@article{tenney_bert_2019,
	title = {{BERT} {Rediscovers} the {Classical} {NLP} {Pipeline}},
	url = {http://arxiv.org/abs/1905.05950},
	abstract = {Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We ﬁnd that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lowerlevel decisions on the basis of disambiguating information from higher-level representations.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1905.05950 [cs]},
	author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
	month = aug,
	year = {2019},
	note = {arXiv: 1905.05950},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Presented at ACL 2019},
	file = {Tenney et al. - 2019 - BERT Rediscovers the Classical NLP Pipeline.pdf:/Users/alexpsq/Zotero/storage/PWZ5NHQE/Tenney et al. - 2019 - BERT Rediscovers the Classical NLP Pipeline.pdf:application/pdf},
}

@article{naselaris_bayesian_2009,
	title = {Bayesian {Reconstruction} of {Natural} {Images} from {Human} {Brain} {Activity}},
	volume = {63},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627309006850},
	doi = {10.1016/j.neuron.2009.09.006},
	abstract = {Recent studies have used fMRI signals from early visual areas to reconstruct simple geometric patterns. Here, we demonstrate a new Bayesian decoder that uses fMRI signals from early and anterior visual areas to reconstruct complex natural images. Our decoder combines three elements: a structural encoding model that characterizes responses in early visual areas, a semantic encoding model that characterizes responses in anterior visual areas, and prior information about the structure and semantic content of natural images. By combining all these elements, the decoder produces reconstructions that accurately reﬂect both the spatial structure and semantic category of the objects contained in the observed natural image. Our results show that prior information has a substantial effect on the quality of natural image reconstructions. We also demonstrate that much of the variance in the responses of anterior visual areas to complex natural images is explained by the semantic category of the image alone.},
	language = {en},
	number = {6},
	urldate = {2020-12-10},
	journal = {Neuron},
	author = {Naselaris, Thomas and Prenger, Ryan J. and Kay, Kendrick N. and Oliver, Michael and Gallant, Jack L.},
	month = sep,
	year = {2009},
	pages = {902--915},
	file = {Naselaris et al. - 2009 - Bayesian Reconstruction of Natural Images from Hum.pdf:/Users/alexpsq/Zotero/storage/FTPPQ2JT/Naselaris et al. - 2009 - Bayesian Reconstruction of Natural Images from Hum.pdf:application/pdf},
}

@article{duncan_consistency_2009,
	title = {Consistency and variability in functional localisers},
	volume = {46},
	issn = {10538119},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811909002353},
	doi = {10.1016/j.neuroimage.2009.03.014},
	abstract = {A critical assumption underlying the use of functional localiser scans is that the voxels identiﬁed as the functional region-of-interest (fROI) are essentially the same as those activated by the main experimental manipulation. Intra-subject variability in the location of the fROI violates this assumption, reducing the sensitivity of the analysis and biasing the results. Here we investigated consistency and variability in fROIs in a set of 45 volunteers. They performed two functional localiser scans to identify word- and object-sensitive regions of ventral and lateral occipito-temporal cortex, respectively. In the main analyses, fROIs were deﬁned as the category-selective voxels in each region and consistency was measured as the spatial overlap between scans. Consistency was greatest when minimally selective thresholds were used to deﬁne “active” voxels (p b 0.05 uncorrected), revealing that approximately 65\% of the voxels were commonly activated by both scans. In contrast, highly selective thresholds (p b 10− 4 to 10−6) yielded the lowest consistency values with less than 25\% overlap of the voxels active in both scans. In other words, intra-subject variability was surprisingly high, with between one third and three quarters of the voxels in a given fROI not corresponding to those activated in the main task. This level of variability stands in striking contrast to the consistency seen in retinotopically-deﬁned areas and has important implications for designing robust but efﬁcient functional localiser scans.},
	language = {en},
	number = {4},
	urldate = {2020-12-10},
	journal = {NeuroImage},
	author = {Duncan, Keith J. and Pattamadilok, Chotiga and Knierim, Iris and Devlin, Joseph T.},
	month = jul,
	year = {2009},
	pages = {1018--1026},
	file = {Duncan et al. - 2009 - Consistency and variability in functional localise.pdf:/Users/alexpsq/Zotero/storage/229ZN6Y3/Duncan et al. - 2009 - Consistency and variability in functional localise.pdf:application/pdf},
}

@inproceedings{lecun_convolutional_2010,
	address = {Paris, France},
	title = {Convolutional networks and applications in vision},
	isbn = {978-1-4244-5308-5},
	url = {http://ieeexplore.ieee.org/document/5537907/},
	doi = {10.1109/ISCAS.2010.5537907},
	abstract = {Intelligent tasks, such as visual perception, auditory perception, and language understanding require the construction of good internal representations of the world (or ”features”), which must be invariant to irrelevant variations of the input while, preserving relevant information. A major question for Machine Learning is how to learn such good features automatically. Convolutional Networks (ConvNets) are a biologicallyinspired trainable architecture that can learn invariant features. Each stage in a ConvNets is composed of a ﬁlter bank, some non-linearities, and feature pooling layers. With multiple stages, a ConvNet can learn multi-level hierarchies of features. While ConvNets have been successfully deployed in many commercial applications from OCR to video surveillance, they require large amounts of labeled training samples. We describe new unsupervised learning algorithms, and new non-linear stages that allow ConvNets to be trained with very few labeled samples. Applications to visual object recognition and vision navigation for off-road mobile robots are described.},
	language = {en},
	urldate = {2020-12-10},
	booktitle = {Proceedings of 2010 {IEEE} {International} {Symposium} on {Circuits} and {Systems}},
	publisher = {IEEE},
	author = {LeCun, Yann and Kavukcuoglu, Koray and Farabet, Clement},
	month = may,
	year = {2010},
	pages = {253--256},
	file = {LeCun et al. - 2010 - Convolutional networks and applications in vision.pdf:/Users/alexpsq/Zotero/storage/57XLAR33/LeCun et al. - 2010 - Convolutional networks and applications in vision.pdf:application/pdf},
}

@article{lin_dcnn-gan_2019,
	title = {{DCNN}-{GAN}: {Reconstructing} {Realistic} {Image} from {fMRI}},
	shorttitle = {{DCNN}-{GAN}},
	url = {http://arxiv.org/abs/1901.07368},
	abstract = {Visualizing the perceptual content by analyzing human functional magnetic resonance imaging (fMRI) has been an active research area. However, due to its high dimensionality, complex dimensional structure, and small number of samples available, reconstructing realistic images from fMRI remains challenging. Recently with the development of convolutional neural network (CNN) and generative adversarial network (GAN), mapping multi-voxel fMRI data to complex, realistic images has been made possible. In this paper, we propose a model, DCNN-GAN, by combining a reconstruction network and GAN. We utilize the CNN for hierarchical feature extraction and the DCNN-GAN to reconstruct more realistic images. Extensive experiments have been conducted, showing that our method outperforms previous works, regarding reconstruction quality and computational cost.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1901.07368 [cs, eess]},
	author = {Lin, Yunfeng and Li, Jiangbei and Wang, Hanjing},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.07368},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Lin et al. - 2019 - DCNN-GAN Reconstructing Realistic Image from fMRI.pdf:/Users/alexpsq/Zotero/storage/4TDBRWIZ/Lin et al. - 2019 - DCNN-GAN Reconstructing Realistic Image from fMRI.pdf:application/pdf},
}

@article{pedregosa_data-driven_2015,
	title = {Data-driven {HRF} estimation for encoding and decoding models},
	volume = {104},
	issn = {10538119},
	url = {http://arxiv.org/abs/1402.7015},
	doi = {10.1016/j.neuroimage.2014.09.060},
	abstract = {Despite the common usage of a canonical, data-independent, hemodynamic response function (HRF), it is known that the shape of the HRF varies across brain regions and subjects. This suggests that a data-driven estimation of this function could lead to more statistical power when modeling BOLD fMRI data. However, unconstrained estimation of the HRF can yield highly unstable results when the number of free parameters is large. We develop a method for the joint estimation of activation and HRF by means of a rank constraint, forcing the estimated HRF to be equal across events or experimental conditions, yet permitting it to diﬀer across voxels. Model estimation leads to an optimization problem that we propose to solve with an eﬃcient quasi-Newton method, exploiting fast gradient computations. This model, called GLM with Rank-1 constraint (R1-GLM), can be extended to the setting of GLM with separate designs which has been shown to improve decoding accuracy in brain activity decoding experiments. We compare 10 diﬀerent HRF modeling methods in terms of encoding and decoding score on two diﬀerent datasets. Our results show that the R1-GLM model outperforms competing methods in both encoding and decoding settings, positioning it as an attractive method both from the points of view of accuracy and computational eﬃciency.},
	language = {en},
	urldate = {2020-12-10},
	journal = {NeuroImage},
	author = {Pedregosa, Fabian and Eickenberg, Michael and Ciuciu, Philippe and Thirion, Bertrand and Gramfort, Alexandre},
	month = jan,
	year = {2015},
	note = {arXiv: 1402.7015},
	keywords = {Computer Science - Machine Learning, Computer Science - Computational Engineering, Finance, and Science},
	pages = {209--220},
	annote = {Comment: appears in NeuroImage (2015)},
	file = {Pedregosa et al. - 2015 - Data-driven HRF estimation for encoding and decodi.pdf:/Users/alexpsq/Zotero/storage/35NE7EQM/Pedregosa et al. - 2015 - Data-driven HRF estimation for encoding and decodi.pdf:application/pdf},
}

@article{blevins_deep_2018,
	title = {Deep {RNNs} {Encode} {Soft} {Hierarchical} {Syntax}},
	url = {http://arxiv.org/abs/1805.04218},
	abstract = {We present a set of experiments to demonstrate that deep recurrent neural networks (RNNs) learn internal representations that capture soft hierarchical notions of syntax from highly varied supervision. We consider four syntax tasks at different depths of the parse tree; for each word, we predict its part of speech as well as the ﬁrst (parent), second (grandparent) and third level (great-grandparent) constituent labels that appear above it. These predictions are made from representations produced at different depths in networks that are pretrained with one of four objectives: dependency parsing, semantic role labeling, machine translation, or language modeling. In every case, we ﬁnd a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the models encode signiﬁcant amounts of syntax even in the absence of an explicit syntactic training supervision.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1805.04218 [cs]},
	author = {Blevins, Terra and Levy, Omer and Zettlemoyer, Luke},
	month = may,
	year = {2018},
	note = {arXiv: 1805.04218},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted to ACL 2018},
	file = {Blevins et al. - 2018 - Deep RNNs Encode Soft Hierarchical Syntax.pdf:/Users/alexpsq/Zotero/storage/QT7UR85J/Blevins et al. - 2018 - Deep RNNs Encode Soft Hierarchical Syntax.pdf:application/pdf},
}

@article{khaligh-razavi_deep_2014,
	title = {Deep {Supervised}, but {Not} {Unsupervised}, {Models} {May} {Explain} {IT} {Cortical} {Representation}},
	volume = {10},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1003915},
	doi = {10.1371/journal.pcbi.1003915},
	abstract = {Inferior temporal (IT) cortex in human and nonhuman primates serves visual object recognition. Computational objectvision models, although continually improving, do not yet reach human performance. It is unclear to what extent the internal representations of computational models can explain the IT representation. Here we investigate a wide range of computational model representations (37 in total), testing their categorization performance and their ability to account for the IT representational geometry. The models include well-known neuroscientific object-recognition models (e.g. HMAX, VisNet) along with several models from computer vision (e.g. SIFT, GIST, self-similarity features, and a deep convolutional neural network). We compared the representational dissimilarity matrices (RDMs) of the model representations with the RDMs obtained from human IT (measured with fMRI) and monkey IT (measured with cell recording) for the same set of stimuli (not used in training the models). Better performing models were more similar to IT in that they showed greater clustering of representational patterns by category. In addition, better performing models also more strongly resembled IT in terms of their within-category representational dissimilarities. Representational geometries were significantly correlated between IT and many of the models. However, the categorical clustering observed in IT was largely unexplained by the unsupervised models. The deep convolutional network, which was trained by supervision with over a million categorylabeled images, reached the highest categorization performance and also best explained IT, although it did not fully explain the IT data. Combining the features of this model with appropriate weights and adding linear combinations that maximize the margin between animate and inanimate objects and between faces and other objects yielded a representation that fully explained our IT data. Overall, our results suggest that explaining IT requires computational features trained through supervised learning to emphasize the behaviorally important categorical divisions prominently reflected in IT.},
	language = {en},
	number = {11},
	urldate = {2020-12-10},
	journal = {PLoS Computational Biology},
	author = {Khaligh-Razavi, Seyed-Mahdi and Kriegeskorte, Nikolaus},
	editor = {Diedrichsen, Jörn},
	month = nov,
	year = {2014},
	pages = {e1003915},
	file = {Khaligh-Razavi et Kriegeskorte - 2014 - Deep Supervised, but Not Unsupervised, Models May .pdf:/Users/alexpsq/Zotero/storage/TBSUSNSI/Khaligh-Razavi et Kriegeskorte - 2014 - Deep Supervised, but Not Unsupervised, Models May .pdf:application/pdf},
}

@inproceedings{peters_deep_2018,
	address = {New Orleans, Louisiana},
	title = {Deep {Contextualized} {Word} {Representations}},
	url = {http://aclweb.org/anthology/N18-1202},
	doi = {10.18653/v1/N18-1202},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and signiﬁcantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	language = {en},
	urldate = {2020-12-10},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of           the {Association} for {Computational} {Linguistics}: {Human} {Language}           {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	year = {2018},
	pages = {2227--2237},
	file = {Peters et al. - 2018 - Deep Contextualized Word Representations.pdf:/Users/alexpsq/Zotero/storage/HWHSI4VJ/Peters et al. - 2018 - Deep Contextualized Word Representations.pdf:application/pdf},
}


@article{ethayarajh_how_2019,
	title = {How {Contextual} are {Contextualized} {Word} {Representations}? {Comparing} the {Geometry} of {BERT}, {ELMo}, and {GPT}-2 {Embeddings}},
	shorttitle = {How {Contextual} are {Contextualized} {Word} {Representations}?},
	url = {http://arxiv.org/abs/1909.00512},
	abstract = {Replacing static word embeddings with contextualized word representations has yielded signiﬁcant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there inﬁnitely many context-speciﬁc representations for each word, or are words essentially assigned one of a ﬁnite number of word-sense representations? For one, we ﬁnd that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-speciﬁc representations, much like how upper layers of LSTMs produce more task-speciﬁc representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5\% of the variance in a word’s contextualized representations can be explained by a static embedding for that word, providing some justiﬁcation for the success of contextualized representations.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1909.00512 [cs]},
	author = {Ethayarajh, Kawin},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.00512},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted to EMNLP 2019},
	file = {Ethayarajh - 2019 - How Contextual are Contextualized Word Representat.pdf:/Users/alexpsq/Zotero/storage/5K5EPGRS/Ethayarajh - 2019 - How Contextual are Contextualized Word Representat.pdf:application/pdf},
}

@article{authors_deep_nodate,
	title = {Deep {Recurrent} {Encoder}: {A} {Scalable} {End}-to-{End} {Network}  to {Predict} {Non}-{Linear}, {Variable} and {High}-{Dimensional} {Brain} {Responses}},
	language = {en},
	author = {Authors, Anonymous},
	pages = {8},
	file = {Authors - Deep Recurrent Encoder A Scalable End-to-End Netw.pdf:/Users/alexpsq/Zotero/storage/53DR9ARQ/Authors - Deep Recurrent Encoder A Scalable End-to-End Netw.pdf:application/pdf},
}

@article{kay_identifying_2008,
	title = {Identifying natural images from human brain activity},
	volume = {452},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature06713},
	doi = {10.1038/nature06713},
	language = {en},
	number = {7185},
	urldate = {2020-12-10},
	journal = {Nature},
	author = {Kay, Kendrick N. and Naselaris, Thomas and Prenger, Ryan J. and Gallant, Jack L.},
	month = mar,
	year = {2008},
	pages = {352--355},
	file = {Kay et al. - 2008 - Identifying natural images from human brain activi.pdf:/Users/alexpsq/Zotero/storage/2AWFGBWF/Kay et al. - 2008 - Identifying natural images from human brain activi.pdf:application/pdf},
}

@article{schwartz_inducing_2019,
	title = {Inducing brain-relevant bias in natural language processing models},
	url = {http://arxiv.org/abs/1911.03268},
	abstract = {Progress in natural language processing (NLP) models that estimate representations of word sequences has recently been leveraged to improve the understanding of language processing in the brain. However, these models have not been speciﬁcally designed to capture the way the brain represents language meaning. We hypothesize that ﬁne-tuning these models to predict recordings of brain activity of people reading text will lead to representations that encode more brain-activity-relevant language information. We demonstrate that a version of BERT, a recently introduced and powerful language model, can improve the prediction of brain activity after ﬁne-tuning. We show that the relationship between language and brain activity learned by BERT during this ﬁne-tuning transfers across multiple participants. We also show that, for some participants, the ﬁne-tuned representations learned from both magnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI) are better for predicting fMRI than the representations learned from fMRI alone, indicating that the learned representations capture brain-activity-relevant information that is not simply an artifact of the modality. While changes to language representations help the model predict brain activity, they also do not harm the model’s ability to perform downstream NLP tasks. Our ﬁndings are notable for research on language understanding in the brain.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1911.03268 [cs, q-bio]},
	author = {Schwartz, Dan and Toneva, Mariya and Wehbe, Leila},
	month = oct,
	year = {2019},
	note = {arXiv: 1911.03268},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
	annote = {MAIN POINTS
 
* Fine-tuned BERT model on brain data from people reading texts
 
* Increase model ability to predict brain performance
 
* Transfer from participant learning onto other participants
 
* No loss on NLP benchmarck tasks
 },
	file = {Schwartz et al. - 2019 - Inducing brain-relevant bias in natural language p.pdf:/Users/alexpsq/Zotero/storage/5XXZM7L8/Schwartz et al. - 2019 - Inducing brain-relevant bias in natural language p.pdf:application/pdf},
}

@article{kriegeskorte_information-based_2006,
	title = {Information-based functional brain mapping},
	volume = {103},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0600244103},
	doi = {10.1073/pnas.0600244103},
	language = {en},
	number = {10},
	urldate = {2020-12-10},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kriegeskorte, N. and Goebel, R. and Bandettini, P.},
	month = mar,
	year = {2006},
	pages = {3863--3868},
	file = {Kriegeskorte et al. - 2006 - Information-based functional brain mapping.pdf:/Users/alexpsq/Zotero/storage/ZSL5HSAR/Kriegeskorte et al. - 2006 - Information-based functional brain mapping.pdf:application/pdf},
}

@article{toneva_interpreting_nodate,
	title = {Interpreting and improving natural-language processing (in machines) with  natural language-processing (in the brain)},
	abstract = {Neural networks models for NLP are typically implemented without the explicit encoding of language rules and yet they are able to break one performance record after another. This has generated a lot of research interest in interpreting the representations learned by these networks. We propose here a novel interpretation approach that relies on the only processing system we have that does understand language: the human brain. We use brain imaging recordings of subjects reading complex natural text to interpret word and sequence embeddings from 4 recent NLP models - ELMo, USE, BERT and Transformer-XL. We study how their representations differ across layer depth, context length, and attention type. Our results reveal differences in the context-related representations across these models. Further, in the transformer models, we ﬁnd an interaction between layer depth and context length, and between layer depth and attention type. We ﬁnally hypothesize that altering BERT to better align with brain recordings would enable it to also better understand language. Probing the altered BERT using syntactic NLP tasks reveals that the model with increased brain-alignment outperforms the original model. Cognitive neuroscientists have already begun using NLP networks to study the brain, and this work closes the loop to allow the interaction between NLP and cognitive neuroscience to be a true cross-pollination.},
	language = {en},
	author = {Toneva, Mariya and Wehbe, Leila},
	pages = {11},
	annote = {MAIN POINTS
 
* Compare ELMo, BERT, USE and T-XL to brain data (MEG and fMRI).
 
* fMRI and MEG: text presented one word at a time (respectively 8 and 3 subjects).
 
* Find that middle layers of BERT and T-XL perform the best for contexts longer than 15 words.
 
* Find that T-XL's performance doesn't degrade as context is increased, unlike the other models.
 
* Find that using uniform attention in early layers of BERT leads to better prediction of brain activity.
 
* Show that when BERT is altered to better align with brain activity by removing the pretrained attention in the shallow layers, it performs better at NLP tasks that probe its syntactic understanding.},
	file = {Toneva et Wehbe - Interpreting and improving natural-language proces.pdf:/Users/alexpsq/Zotero/storage/TCQ7GG7B/Toneva et Wehbe - Interpreting and improving natural-language proces.pdf:application/pdf},
}

@article{thirion_inverse_2006,
	title = {Inverse retinotopy: {Inferring} the visual content of images from brain activation patterns},
	volume = {33},
	issn = {10538119},
	shorttitle = {Inverse retinotopy},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811906007373},
	doi = {10.1016/j.neuroimage.2006.06.062},
	language = {en},
	number = {4},
	urldate = {2020-12-10},
	journal = {NeuroImage},
	author = {Thirion, Bertrand and Duchesnay, Edouard and Hubbard, Edward and Dubois, Jessica and Poline, Jean-Baptiste and Lebihan, Denis and Dehaene, Stanislas},
	month = dec,
	year = {2006},
	pages = {1104--1116},
	file = {Thirion et al. - 2006 - Inverse retinotopy Inferring the visual content o.pdf:/Users/alexpsq/Zotero/storage/VWY8I7FX/Thirion et al. - 2006 - Inverse retinotopy Inferring the visual content o.pdf:application/pdf},
}

@article{radford_improving_nodate,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	language = {en},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	pages = {12},
	file = {Radford et al. - Improving Language Understanding by Generative Pre.pdf:/Users/alexpsq/Zotero/storage/R5P6VV5G/Radford et al. - Improving Language Understanding by Generative Pre.pdf:application/pdf},
}

@article{gauthier_linking_2019,
	title = {Linking artificial and human neural representations of language},
	url = {http://arxiv.org/abs/1910.01244},
	abstract = {What information from an act of sentence understanding is robustly represented in the human brain? We investigate this question by comparing sentence encoding models on a brain decoding task, where the sentence that an experimental participant has seen must be predicted from the fMRI signal evoked by the sentence. We take a pre-trained BERT architecture as a baseline sentence encoding model and ﬁne-tune it on a variety of natural language understanding (NLU) tasks, asking which lead to improvements in brain-decoding performance. We ﬁnd that none of the sentence encoding tasks tested yield signiﬁcant increases in brain decoding performance. Through further task ablations and representational analyses, we ﬁnd that tasks which produce syntax-light representations yield signiﬁcant improvements in brain decoding performance. Our results constrain the space of NLU models that could best account for human neural representations of language, but also suggest limits on the possibility of decoding ﬁne-grained syntactic information from fMRI human neuroimaging.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1910.01244 [cs, q-bio]},
	author = {Gauthier, Jon and Levy, Roger},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.01244},
	keywords = {Computer Science - Computation and Language, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: EMNLP 2019},
	file = {Gauthier et Levy - 2019 - Linking artificial and human neural representation.pdf:/Users/alexpsq/Zotero/storage/YSSXMX4I/Gauthier et Levy - 2019 - Linking artificial and human neural representation.pdf:application/pdf},
}

@article{makin_machine_2020,
	title = {Machine translation of cortical activity to text with an encoder–decoder framework},
	volume = {23},
	issn = {1097-6256, 1546-1726},
	url = {http://www.nature.com/articles/s41593-020-0608-8},
	doi = {10.1038/s41593-020-0608-8},
	language = {en},
	number = {4},
	urldate = {2020-12-10},
	journal = {Nature Neuroscience},
	author = {Makin, Joseph G. and Moses, David A. and Chang, Edward F.},
	month = apr,
	year = {2020},
	pages = {575--582},
	file = {Makin et al. - 2020 - Machine translation of cortical activity to text w.pdf:/Users/alexpsq/Zotero/storage/RIIPIDRB/Makin et al. - 2020 - Machine translation of cortical activity to text w.pdf:application/pdf},
}

@article{mashour_conscious_2020,
	title = {Conscious {Processing} and the {Global} {Neuronal} {Workspace} {Hypothesis}},
	volume = {105},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627320300520},
	doi = {10.1016/j.neuron.2020.01.026},
	language = {en},
	number = {5},
	urldate = {2020-12-10},
	journal = {Neuron},
	author = {Mashour, George A. and Roelfsema, Pieter and Changeux, Jean-Pierre and Dehaene, Stanislas},
	month = mar,
	year = {2020},
	pages = {776--798},
	file = {Mashour et al. - 2020 - Conscious Processing and the Global Neuronal Works.pdf:/Users/alexpsq/Zotero/storage/EA8PAKUL/Mashour et al. - 2020 - Conscious Processing and the Global Neuronal Works.pdf:application/pdf},
}

@article{pervaiz_optimising_2020,
	title = {Optimising network modelling methods for {fMRI}},
	volume = {211},
	issn = {10538119},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811920300914},
	doi = {10.1016/j.neuroimage.2020.116604},
	abstract = {A major goal of neuroimaging studies is to develop predictive models to analyse the relationship between whole brain functional connectivity patterns and behavioural traits. However, there is no single widelyaccepted standard pipeline for analyzing functional connectivity. The common procedure for designing functional connectivity based predictive models entails three main steps: parcellating the brain, estimating the interaction between deﬁned parcels, and lastly, using these integrated associations between brain parcels as features fed to a classiﬁer for predicting non-imaging variables e.g., behavioural traits, demographics, emotional measures, etc. There are also additional considerations when using correlation-based measures of functional connectivity, resulting in three supplementary steps: utilising Riemannian geometry tangent space parameterization to preserve the geometry of functional connectivity; penalizing the connectivity estimates with shrinkage approaches to handle challenges related to short time-series (and noisy) data; and removing confounding variables from brain-behaviour data. These six steps are contingent on each-other, and to optimise a general framework one should ideally examine these various methods simultaneously. In this paper, we investigated strengths and short-comings, both independently and jointly, of the following measures: parcellation techniques of four kinds (categorized further depending upon number of parcels), ﬁve measures of functional connectivity, the decision of staying in the ambient space of connectivity matrices or in tangent space, the choice of applying shrinkage estimators, six alternative techniques for handling confounds and ﬁnally four novel classiﬁers/predictors. For performance evaluation, we have selected two of the largest datasets, UK Biobank and the Human Connectome Project resting state fMRI data, and have run more than 9000 diﬀerent pipeline variants on a total of ∼14000 individuals to determine the optimum pipeline. For independent performance validation, we have run some best-performing pipeline variants on ABIDE and ACPI datasets (∼1000 subjects) to evaluate the generalisability of proposed network modelling methods.},
	language = {en},
	urldate = {2020-12-10},
	journal = {NeuroImage},
	author = {Pervaiz, Usama and Vidaurre, Diego and Woolrich, Mark W. and Smith, Stephen M.},
	month = may,
	year = {2020},
	pages = {116604},
	file = {Pervaiz et al. - 2020 - Optimising network modelling methods for fMRI.pdf:/Users/alexpsq/Zotero/storage/KKQTS9MS/Pervaiz et al. - 2020 - Optimising network modelling methods for fMRI.pdf:application/pdf},
}

@inproceedings{richard_optimizing_2018,
	address = {Philadelphia, Pennsylvania, USA},
	title = {Optimizing deep video representation to match brain activity},
	url = {https://ccneuro.org/2018/Papers/ViewPapers.asp?PaperNum=1071},
	doi = {10.32470/CCN.2018.1071-0},
	abstract = {The comparison of observed brain activity with the statistics generated by artiﬁcial intelligence systems is useful to probe brain functional organization under ecological conditions. Here we study fMRI activity in ten subjects watching color natural movies and compute deep representations of these movies with an architecture that relies on optical ﬂow and image content. The association of activity in visual areas with the different layers of the deep architecture displays complexity-related contrasts across visual areas and reveals a striking foveal/peripheral dichotomy.},
	language = {en},
	urldate = {2020-12-10},
	booktitle = {2018 {Conference} on {Cognitive} {Computational} {Neuroscience}},
	publisher = {Cognitive Computational Neuroscience},
	author = {Richard, Hugo and Pinho, Ana Luisa and Thirion, Bertrand and Charpiat, Guillaume},
	year = {2018},
	file = {Richard et al. - 2018 - Optimizing deep video representation to match brai.pdf:/Users/alexpsq/Zotero/storage/G4DZSK2W/Richard et al. - 2018 - Optimizing deep video representation to match brai.pdf:application/pdf},
}

@article{haak_population_2012,
	title = {Population {Receptive} {Field} {Dynamics} in {Human} {Visual} {Cortex}},
	volume = {7},
	abstract = {Seminal work in the early nineties revealed that the visual receptive field of neurons in cat primary visual cortex can change in location and size when artificial scotomas are applied. Recent work now suggests that these single neuron receptive field dynamics also pertain to the neuronal population receptive field (pRF) that can be measured in humans with functional magnetic resonance imaging (fMRI). To examine this further, we estimated the pRF in twelve healthy participants while masking the central portion of the visual field. We found that the pRF changes in location and size for two differently sized artificial scotomas, and that these pRF dynamics are most likely due to a combination of the neuronal receptive field position and size scatter as well as modulatory feedback signals from extrastriate visual areas.},
	language = {en},
	number = {5},
	journal = {PLoS ONE},
	author = {Haak, Koen V and Cornelissen, Frans W and Morland, Antony B},
	year = {2012},
	pages = {8},
	file = {Haak et al. - 2012 - Population Receptive Field Dynamics in Human Visua.pdf:/Users/alexpsq/Zotero/storage/P9S9K2P2/Haak et al. - 2012 - Population Receptive Field Dynamics in Human Visua.pdf:application/pdf},
}

@article{serre_robust_2007,
	title = {Robust {Object} {Recognition} with {Cortex}-{Like} {Mechanisms}},
	volume = {29},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/4069258/},
	doi = {10.1109/TPAMI.2007.56},
	abstract = {We introduce a new general framework for the recognition of complex visual scenes, which is motivated by biology: We describe a hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation. We demonstrate the strength of the approach on a range of recognition tasks: From invariant single object recognition in clutter to multiclass categorization problems and complex scene understanding tasks that rely on the recognition of both shape-based as well as texture-based objects. Given the biological constraints that the system had to satisfy, the approach performs surprisingly well: It has the capability of learning from only a few training examples and competes with state-of-the-art systems. We also discuss the existence of a universal, redundant dictionary of features that could handle the recognition of most object categories. In addition to its relevance for computer vision, the success of this approach suggests a plausibility proof for a class of feedforward models of object recognition in cortex.},
	language = {en},
	number = {3},
	urldate = {2020-12-10},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Serre, Thomas and Wolf, Lior and Bileschi, Stanley and Riesenhuber, Maximilian and Poggio, Tomaso},
	month = mar,
	year = {2007},
	pages = {411--426},
	file = {Serre et al. - 2007 - Robust Object Recognition with Cortex-Like Mechani.pdf:/Users/alexpsq/Zotero/storage/SSZECBIQ/Serre et al. - 2007 - Robust Object Recognition with Cortex-Like Mechani.pdf:application/pdf},
}

@article{eickenberg_seeing_2017,
	title = {Seeing it all: {Convolutional} network layers map the function of the human visual system},
	volume = {152},
	issn = {10538119},
	shorttitle = {Seeing it all},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811916305481},
	doi = {10.1016/j.neuroimage.2016.10.001},
	abstract = {Convolutional networks used for computer vision represent candidate models for the computations performed in mammalian visual systems. We use them as a detailed model of human brain activity during the viewing of natural images by constructing predictive models based on their diﬀerent layers and BOLD fMRI activations. Analyzing the predictive performance across layers yields characteristic ﬁngerprints for each visual brain region: early visual areas are better described by lower level convolutional net layers and later visual areas by higher level net layers, exhibiting a progression across ventral and dorsal streams. Our predictive model generalizes beyond brain responses to natural images. We illustrate this on two experiments, namely retinotopy and faceplace oppositions, by synthesizing brain activity and performing classical brain mapping upon it. The synthesis recovers the activations observed in the corresponding fMRI studies, showing that this deep encoding model captures representations of brain function that are universal across experimental paradigms.},
	language = {en},
	urldate = {2020-12-10},
	journal = {NeuroImage},
	author = {Eickenberg, Michael and Gramfort, Alexandre and Varoquaux, Gaël and Thirion, Bertrand},
	month = may,
	year = {2017},
	pages = {184--194},
	file = {Eickenberg et al. - 2017 - Seeing it all Convolutional network layers map th.pdf:/Users/alexpsq/Zotero/storage/CA3G5KRR/Eickenberg et al. - 2017 - Seeing it all Convolutional network layers map th.pdf:application/pdf},
}

@inproceedings{kudo_sentencepiece_2018,
	address = {Brussels, Belgium},
	title = {{SentencePiece}: {A} simple and language independent subword tokenizer and detokenizer for {Neural} {Text} {Processing}},
	shorttitle = {{SentencePiece}},
	url = {http://aclweb.org/anthology/D18-2012},
	doi = {10.18653/v1/D18-2012},
	abstract = {This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and ﬁnd that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various conﬁgurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.},
	language = {en},
	urldate = {2020-12-10},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Kudo, Taku and Richardson, John},
	year = {2018},
	pages = {66--71},
	file = {Kudo et Richardson - 2018 - SentencePiece A simple and language independent s.pdf:/Users/alexpsq/Zotero/storage/I9KM6PNL/Kudo et Richardson - 2018 - SentencePiece A simple and language independent s.pdf:application/pdf},
}

@article{poline_general_2012,
	title = {The general linear model and {fMRI}: {Does} love last forever?},
	abstract = {In this review, we ﬁrst set out the general linear model (GLM) for the non technical reader, as a tool able to do both linear regression and ANOVA within the same ﬂexible framework. We present a short history of its development in the fMRI community, and describe some interesting examples of its early use. We offer a few warnings, as the GLM relies on assumptions that may not hold in all situations. We conclude with a few wishes for the future of fMRI analyses, with or without the GLM. The appendix develops some aspects of use of contrasts for testing for the more technical reader.},
	language = {en},
	author = {Poline, Jean-Baptiste and Brett, Matthew},
	year = {2012},
	pages = {10},
	file = {Poline et Brett - 2012 - The general linear model and fMRI Does love last .pdf:/Users/alexpsq/Zotero/storage/CW23EZ7E/Poline et Brett - 2012 - The general linear model and fMRI Does love last .pdf:application/pdf},
}

@article{dai_transformer-xl_2019,
	title = {Transformer-{XL}: {Attentive} {Language} {Models} {Beyond} a {Fixed}-{Length} {Context}},
	shorttitle = {Transformer-{XL}},
	url = {http://arxiv.org/abs/1901.02860},
	abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a ﬁxed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a ﬁxed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, TransformerXL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-ofthe-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without ﬁnetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorﬂow and PyTorch1.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1901.02860 [cs, stat]},
	author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	note = {arXiv: 1901.02860},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ACL 2019 long paper. Code and pretrained models are available at https://github.com/kimiyoung/transformer-xl},
	file = {Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf:/Users/alexpsq/Zotero/storage/429FX83L/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf:application/pdf},
}

@article{coenen_visualizing_2019,
	title = {Visualizing and {Measuring} the {Geometry} of {BERT}},
	url = {http://arxiv.org/abs/1906.02715},
	abstract = {Transformer architectures show signiﬁcant promise for natural language processing. Given that a single pretrained model can be ﬁne-tuned to perform well on many different tasks, these networks appear to extract generally useful linguistic features. How do such networks represent this information internally? This paper describes qualitative and quantitative investigations of one particularly effective model, BERT. At a high level, linguistic features seem to be represented in separate semantic and syntactic subspaces. We ﬁnd evidence of a ﬁne-grained geometric representation of word senses. We also present empirical descriptions of syntactic representations in both attention matrices and individual word embeddings, as well as a mathematical argument to explain the geometry of these representations.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1906.02715 [cs, stat]},
	author = {Coenen, Andy and Reif, Emily and Yuan, Ann and Kim, Been and Pearce, Adam and Viégas, Fernanda and Wattenberg, Martin},
	month = oct,
	year = {2019},
	note = {arXiv: 1906.02715},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 8 pages, 5 figures},
	file = {Coenen et al. - 2019 - Visualizing and Measuring the Geometry of BERT.pdf:/Users/alexpsq/Zotero/storage/JP56CMQU/Coenen et al. - 2019 - Visualizing and Measuring the Geometry of BERT.pdf:application/pdf},
}

@article{zeiler_visualizing_2013,
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1311.2901},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classiﬁcation performance on the ImageNet benchmark (Krizhevsky et al., 2012). However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classiﬁer. Used in a diagnostic role, these visualizations allow us to ﬁnd model architectures that outperform Krizhevsky et al. on the ImageNet classiﬁcation benchmark. We also perform an ablation study to discover the performance contribution from diﬀerent model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classiﬁer is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1311.2901 [cs]},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	month = nov,
	year = {2013},
	note = {arXiv: 1311.2901},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zeiler et Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:/Users/alexpsq/Zotero/storage/C58EWWBX/Zeiler et Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:application/pdf},
}

@inproceedings{jawahar_what_2019,
	address = {Florence, Italy},
	title = {What {Does} {BERT} {Learn} about the {Structure} of {Language}?},
	url = {https://www.aclweb.org/anthology/P19-1356},
	doi = {10.18653/v1/P19-1356},
	abstract = {BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. We ﬁrst show that BERT’s phrasal representation captures phrase-level information in the lower layers. We also show that BERT’s intermediate layers encode a rich hierarchy of linguistic information, with surface features at the bottom, syntactic features in the middle and semantic features at the top. BERT turns out to require deeper layers when long-distance dependency information is required, e.g. to track subjectverb agreement. Finally, we show that BERT representations capture linguistic information in a compositional way that mimics classical, tree-like structures.},
	language = {en},
	urldate = {2020-12-10},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Jawahar, Ganesh and Sagot, Benoît and Seddah, Djamé},
	year = {2019},
	pages = {3651--3657},
	file = {Jawahar et al. - 2019 - What Does BERT Learn about the Structure of Langua.pdf:/Users/alexpsq/Zotero/storage/4BCX53U4/Jawahar et al. - 2019 - What Does BERT Learn about the Structure of Langua.pdf:application/pdf},
}

@inproceedings{clark_what_2019,
	address = {Florence, Italy},
	title = {What {Does} {BERT} {Look} at? {An} {Analysis} of {BERT}’s {Attention}},
	shorttitle = {What {Does} {BERT} {Look} at?},
	url = {https://www.aclweb.org/anthology/W19-4828},
	doi = {10.18653/v1/W19-4828},
	abstract = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classiﬁers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT’s attention heads exhibit patterns such as attending to delimiter tokens, speciﬁc positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we ﬁnd heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classiﬁer and use it to further demonstrate that substantial syntactic information is captured in BERT’s attention.},
	language = {en},
	urldate = {2020-12-10},
	booktitle = {Proceedings of the 2019 {ACL} {Workshop} {BlackboxNLP}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
	year = {2019},
	pages = {276--286},
	file = {Clark et al. - 2019 - What Does BERT Look at An Analysis of BERT’s Atte.pdf:/Users/alexpsq/Zotero/storage/HGPBB3G6/Clark et al. - 2019 - What Does BERT Look at An Analysis of BERT’s Atte.pdf:application/pdf},
}

@article{tai_sing_lee_image_1996,
	title = {Image representation using {2D} {Gabor} wavelets},
	volume = {18},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/541406/},
	doi = {10.1109/34.541406},
	abstract = {This paper extends to two dimensions the frame criterion developed by Daubechies for one-dimensional wavelets, and it computes the frame bounds for the particular case of 2D Gabor wavelets. Completeness criteria for 2D Gabor image representations are important because of their increasing role in many computer vision applications and also in modeling biological vision, since recent neurophysiological evidence from the visual cortex of mammalian brains suggests that the filter response profiles of the main class of linearly-responding cortical neurons (called simple cells) are best modeled as a family of self-similar 2D Gabor wavelets. We therefore derive the conditions under which a set of continuous 2D Gabor wavelets will provide a complete representation of any image, and we also find self-similar wavelet parameterizations which allow stable reconstruction by summation as though the wavelets formed an orthonormal basis. Approximating a “tight frame” generates redundancy which allows low-resolution neural responses to represent high-resolution images, as we illustrate by image reconstructions with severely quantized 2D Gabor coefficients.},
	language = {en},
	number = {10},
	urldate = {2020-12-10},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {{Tai Sing Lee}},
	month = oct,
	year = {1996},
	pages = {959--971},
	file = {Tai Sing Lee - 1996 - Image representation using 2D Gabor wavelets.pdf:/Users/alexpsq/Zotero/storage/7MBXSYPJ/Tai Sing Lee - 1996 - Image representation using 2D Gabor wavelets.pdf:application/pdf},
}


@article{schrimpf_artificial_2020,
author = {Martin Schrimpf  and Idan Asher Blank  and Greta Tuckute  and Carina Kauf  and Eghbal A. Hosseini  and Nancy Kanwisher  and Joshua B. Tenenbaum  and Evelina Fedorenko },
title = {The neural architecture of language: Integrative modeling converges on predictive processing},
journal = {Proceedings of the National Academy of Sciences},
volume = {118},
number = {45},
pages = {e2105646118},
year = {2021},
doi = {10.1073/pnas.2105646118},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2105646118},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2105646118},
abstract = {Language is a quintessentially human ability. Research has long probed the functional architecture of language in the mind and brain using diverse neuroimaging, behavioral, and computational modeling approaches. However, adequate neurally-mechanistic accounts of how meaning might be extracted from language are sorely lacking. Here, we report a first step toward addressing this gap by connecting recent artificial neural networks from machine learning to human recordings during language processing. We find that the most powerful models predict neural and behavioral responses across different datasets up to noise levels. Models that perform better at predicting the next word in a sequence also better predict brain measurementsâproviding computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the brain. The neuroscience of perception has recently been revolutionized with an integrative modeling approach in which computation, brain function, and behavior are linked across many datasets and many computational models. By revealing trends across models, this approach yields novel insights into cognitive and neural mechanisms in the target domain. We here present a systematic study taking this approach to higher-level cognition: human language processing, our speciesâ signature cognitive skill. We find that the most powerful âtransformerâ models predict nearly 100\% of explainable variance in neural responses to sentences and generalize across different datasets and imaging modalities (functional MRI and electrocorticography). Modelsâ neural fits (âbrain scoreâ) and fits to behavioral responses are both strongly correlated with model accuracy on the next-word prediction task (but not other language tasks). Model architecture appears to substantially contribute to neural fit. These results provide computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the human brain.}}


@article{barachant_multiclass_2012,
	title = {Multiclass {Brain}–{Computer} {Interface} {Classification} by {Riemannian} {Geometry}},
	volume = {59},
	issn = {0018-9294, 1558-2531},
	url = {http://ieeexplore.ieee.org/document/6046114/},
	doi = {10.1109/TBME.2011.2172210},
	abstract = {This paper presents a new classiﬁcation framework for Brain Computer Interface (BCI) based on motor imagery. This framework involves the concept of Riemannian geometry in the manifold of covariance matrices. The main idea is to use spatial covariance matrices as EEG signal descriptors and to rely on Riemannian geometry to directly classify these matrices using the topology of the manifold of Symmetric and Positive Deﬁnite (SPD) matrices. This framework allows to extract the spatial information contained in EEG signals without using spatial ﬁltering. Two methods are proposed and compared with a reference method (multi-class Common Spatial Pattern (CSP) and Linear Discriminant Analysis (LDA)) on the multi-class dataset IIa from the BCI competition IV. The ﬁrst method, named Minimum Distance to Riemanian Mean (MDRM), is an implementation of the Minimum Distance to Mean (MDM) classiﬁcation algorithm using Riemannian distance and Riemannian mean. This simple method shows comparable results with the reference method. The second method, named Tangent Space LDA (TSLDA), maps the covariance matrices onto the Riemannian tangent space where matrices can be vectorized and treated as Euclidean objects. Then, a variable selection procedure is applied in order to decrease dimensionality and a classiﬁcation by LDA is performed. This latter method outperforms the reference method increasing the mean classiﬁcation accuracy from 65.1\% to 70.2\%.},
	language = {en},
	number = {4},
	urldate = {2020-12-10},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Barachant, A. and Bonnet, S. and Congedo, M. and Jutten, C.},
	month = apr,
	year = {2012},
	pages = {920--928},
	file = {Barachant et al. - 2012 - Multiclass Brain–Computer Interface Classification.pdf:/Users/alexpsq/Zotero/storage/KRG8QU9V/Barachant et al. - 2012 - Multiclass Brain–Computer Interface Classification.pdf:application/pdf},
}

@article{caucheteux_language_2020,
	title = {{LANGUAGE} {PROCESSING} {IN} {BRAINS} {AND} {DEEP} {NEURAL} {NETWORKS}: {COMPUTATIONAL} {CONVERGENCE} {AND} {ITS} {LIMITS}},
	language = {en},
	author = {Caucheteux, Charlotte and King, Jean-Rémi},
	year = {2020},
	pages = {14},
	annote = {MAIN POINTS
 
*},
	file = {Caucheteux et King - 2020 - LANGUAGE PROCESSING IN BRAINS AND DEEP NEURAL NETW.pdf:/Users/alexpsq/Zotero/storage/G3Z3RRDU/Caucheteux et King - 2020 - LANGUAGE PROCESSING IN BRAINS AND DEEP NEURAL NETW.pdf:application/pdf},
}

@article{richard_fast_2019,
	title = {Fast shared response model for {fMRI} data},
	url = {http://arxiv.org/abs/1909.12537},
	abstract = {The shared response model provides a simple but eﬀective framework to analyse fMRI data of subjects exposed to naturalistic stimuli. However when the number of subjects or runs is large, ﬁtting the model requires a large amount of memory and computational power, which limits its use in practice. In this work, we introduce the FastSRM algorithm that relies on an intermediate atlas-based representation. It provides considerable speed-up in time and memory usage, hence it allows easy and fast large-scale analysis of naturalistic-stimulus fMRI data. Using four diﬀerent datasets, we show that our method matches the performance of the original SRM algorithm while being about 5x faster and 20x to 40x more memory eﬃcient. Based on this contribution, we use FastSRM to predict age from movie watching data on the CamCAN sample. Besides delivering accurate predictions (mean absolute error of 7.5 years), FastSRM extracts topographic patterns that are predictive of age, demonstrating that brain activity during free perception reﬂects age.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1909.12537 [cs, eess, q-bio]},
	author = {Richard, Hugo and Martin, Lucas and Pinho, Ana Luısa and Pillow, Jonathan and Thirion, Bertrand},
	month = dec,
	year = {2019},
	note = {arXiv: 1909.12537},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Quantitative Biology - Neurons and Cognition},
	file = {Richard et al. - 2019 - Fast shared response model for fMRI data.pdf:/Users/alexpsq/Zotero/storage/VN7BLRPQ/Richard et al. - 2019 - Fast shared response model for fMRI data.pdf:application/pdf},
}

@article{mcinnes_umap_2020,
	title = {{UMAP}: {Uniform} {Manifold} {Approximation} and {Projection} for {Dimension} {Reduction}},
	shorttitle = {{UMAP}},
	url = {http://arxiv.org/abs/1802.03426},
	abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. e result is a practical scalable algorithm that applies to real world data. e UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1802.03426 [cs, stat]},
	author = {McInnes, Leland and Healy, John and Melville, James},
	month = sep,
	year = {2020},
	note = {arXiv: 1802.03426},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computational Geometry},
	annote = {Comment: Reference implementation available at http://github.com/lmcinnes/umap},
	file = {McInnes et al. - 2020 - UMAP Uniform Manifold Approximation and Projectio.pdf:/Users/alexpsq/Zotero/storage/M5XB2EYG/McInnes et al. - 2020 - UMAP Uniform Manifold Approximation and Projectio.pdf:application/pdf},
}

@article{fedorenko_neural_2016,
	title = {Neural correlate of the construction of sentence meaning},
	volume = {113},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1612132113},
	doi = {10.1073/pnas.1612132113},
	abstract = {The neural processes that underlie your ability to read and understand this sentence are unknown. Sentence comprehension occurs very rapidly, and can only be understood at a mechanistic level by discovering the precise sequence of underlying computational and neural events. However, we have no continuous and online neural measure of sentence processing with high spatial and temporal resolution. Here we report just such a measure: intracranial recordings from the surface of the human brain show that neural activity, indexed by γ-power, increases monotonically over the course of a sentence as people read it. This steady increase in activity is absent when people read and remember nonword-lists, despite the higher cognitive demand entailed, ruling out accounts in terms of generic attention, working memory, and cognitive load. Response increases are lower for sentence structure without meaning (“Jabberwocky” sentences) and word meaning without sentence structure (word-lists), showing that this effect is not explained by responses to syntax or word meaning alone. Instead, the full effect is found only for sentences, implicating compositional processes of sentence understanding, a striking and unique feature of human language not shared with animal communication systems. This work opens up new avenues for investigating the sequence of neural events that underlie the construction of linguistic meaning.},
	language = {en},
	number = {41},
	urldate = {2020-12-10},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Fedorenko, Evelina and Scott, Terri L. and Brunner, Peter and Coon, William G. and Pritchett, Brianna and Schalk, Gerwin and Kanwisher, Nancy},
	month = oct,
	year = {2016},
	pages = {E6256--E6262},
	file = {Fedorenko et al. - 2016 - Neural correlate of the construction of sentence m.pdf:/Users/alexpsq/Zotero/storage/MXEL9HLR/Fedorenko et al. - 2016 - Neural correlate of the construction of sentence m.pdf:application/pdf},
}

@article{schick_exploiting_2020,
	title = {Exploiting {Cloze} {Questions} for {Few} {Shot} {Text} {Classification} and {Natural} {Language} {Inference}},
	url = {http://arxiv.org/abs/2001.07676},
	abstract = {Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with "task descriptions" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, regular supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms both supervised training and unsupervised approaches in low-resource settings by a large margin.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:2001.07676 [cs]},
	author = {Schick, Timo and Schütze, Hinrich},
	month = apr,
	year = {2020},
	note = {arXiv: 2001.07676},
	keywords = {Computer Science - Computation and Language},
	file = {Schick et Schütze - 2020 - Exploiting Cloze Questions for Few Shot Text Class.pdf:/Users/alexpsq/Zotero/storage/3SM4Z737/Schick et Schütze - 2020 - Exploiting Cloze Questions for Few Shot Text Class.pdf:application/pdf},
}

@article{ghorbani_neuron_2020,
	title = {Neuron {Shapley}: {Discovering} the {Responsible} {Neurons}},
	shorttitle = {Neuron {Shapley}},
	url = {http://arxiv.org/abs/2002.09815},
	abstract = {We develop Neuron Shapley as a new framework to quantify the contribution of individual neurons to the prediction and performance of a deep network. By accounting for interactions across neurons, Neuron Shapley is more effective in identifying important ﬁlters compared to common approaches based on activation patterns. Interestingly, removing just 30 ﬁlters with the highest Shapley scores effectively destroys the prediction accuracy of Inception-v3 on ImageNet. Visualization of these few critical ﬁlters provides insights into how the network functions. Neuron Shapley is a ﬂexible framework and can be applied to identify responsible neurons in many tasks. We illustrate additional applications of identifying ﬁlters that are responsible for biased prediction in facial recognition and ﬁlters that are vulnerable to adversarial attacks. Removing these ﬁlters is a quick way to repair models. Computing exact Shapley values is computationally infeasible and therefore sampling-based approximations are used in practice. We introduce a new multi-armed bandit algorithm that is able to efﬁciently detect neurons with the largest Shapley value orders of magnitude faster than existing Shapley value approximation methods.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:2002.09815 [cs, stat]},
	author = {Ghorbani, Amirata and Zou, James},
	month = nov,
	year = {2020},
	note = {arXiv: 2002.09815},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {Ghorbani et Zou - 2020 - Neuron Shapley Discovering the Responsible Neuron.pdf:/Users/alexpsq/Zotero/storage/B2BBADWF/Ghorbani et Zou - 2020 - Neuron Shapley Discovering the Responsible Neuron.pdf:application/pdf},
}

@article{huth_natural_2016,
	title = {Natural speech reveals the semantic maps that tile human cerebral cortex},
	volume = {532},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature17637},
	doi = {10.1038/nature17637},
	language = {en},
	number = {7600},
	urldate = {2020-12-10},
	journal = {Nature},
	author = {Huth, Alexander G. and de Heer, Wendy A. and Griffiths, Thomas L. and Theunissen, Frédéric E. and Gallant, Jack L.},
	month = apr,
	year = {2016},
	pages = {453--458},
	file = {Huth et al. - 2016 - Natural speech reveals the semantic maps that tile.pdf:/Users/alexpsq/Zotero/storage/TZFH6PLS/Huth et al. - 2016 - Natural speech reveals the semantic maps that tile.pdf:application/pdf},
}

@article{reddy_syntactic_nodate,
	title = {Syntactic representations in the human brain: beyond effort-based metrics},
	abstract = {We are far from having a complete mechanistic understanding of the brain computations involved in language processing and of the role that syntax plays in those computations. Most language studies do not computationally model syntactic structure, and most studies that do model syntactic processing use eﬀort-based metrics. These metrics capture the eﬀort needed to process the syntactic information given by every word [9, 10, 25]. They can reveal where in the brain syntactic processing occurs, but not what features of syntax are processed by diﬀerent brain regions. Here, we move beyond eﬀort-based metrics and propose explicit features capturing the syntactic structure that is incrementally built while a sentence is being read. Using these features and functional Magnetic Resonance Imaging (fMRI) recordings of participants reading a natural text, we study the brain representation of syntax. We ﬁnd that our syntactic structure-based features are better than eﬀort-based metrics at predicting brain activity in various parts of the language system. We show evidence of the brain representation of complex syntactic information such as phrase and clause structures. We see that regions well-predicted by syntactic features are distributed in the language system and are not distinguishable from those processing semantics. Our results call for a shift in the approach used for studying syntactic processing.},
	language = {en},
	author = {Reddy, Aniketh Janardhan and Wehbe, Leila},
	pages = {17},
	file = {Reddy et Wehbe - Syntactic representations in the human brain beyo.pdf:/Users/alexpsq/Zotero/storage/H9JXXRU6/Reddy et Wehbe - Syntactic representations in the human brain beyo.pdf:application/pdf},
}

@article{grodzinsky_neuroimaging_2006,
	title = {Neuroimaging of syntax and syntactic processing},
	volume = {16},
	issn = {09594388},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438806000328},
	doi = {10.1016/j.conb.2006.03.007},
	language = {en},
	number = {2},
	urldate = {2020-12-10},
	journal = {Current Opinion in Neurobiology},
	author = {Grodzinsky, Yosef and Friederici, Angela D},
	month = apr,
	year = {2006},
	pages = {240--246},
	file = {Grodzinsky et Friederici - 2006 - Neuroimaging of syntax and syntactic processing.pdf:/Users/alexpsq/Zotero/storage/MLAY9D3C/Grodzinsky et Friederici - 2006 - Neuroimaging of syntax and syntactic processing.pdf:application/pdf},
}

@article{yi_encoding_2019,
	title = {The {Encoding} of {Speech} {Sounds} in the {Superior} {Temporal} {Gyrus}},
	volume = {102},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627319303800},
	doi = {10.1016/j.neuron.2019.04.023},
	language = {en},
	number = {6},
	urldate = {2020-12-10},
	journal = {Neuron},
	author = {Yi, Han Gyol and Leonard, Matthew K. and Chang, Edward F.},
	month = jun,
	year = {2019},
	pages = {1096--1110},
	file = {Yi et al. - 2019 - The Encoding of Speech Sounds in the Superior Temp.pdf:/Users/alexpsq/Zotero/storage/MUUTEG56/Yi et al. - 2019 - The Encoding of Speech Sounds in the Superior Temp.pdf:application/pdf},
}

@article{kriegeskorte_representational_2008,
	title = {Representational similarity analysis – connecting the branches of systems neuroscience},
	issn = {16625137},
	url = {http://journal.frontiersin.org/article/10.3389/neuro.06.004.2008/abstract},
	doi = {10.3389/neuro.06.004.2008},
	abstract = {A fundamental challenge for systems neuroscience is to quantitatively relate its three major branches of research: brain-activity measurement, behavioral measurement, and computational modeling. Using measured brain-activity patterns to evaluate computational network models is complicated by the need to deﬁne the correspondency between the units of the model and the channels of the brain-activity data, e.g., single-cell recordings or voxels from functional magnetic resonance imaging (fMRI). Similar correspondency problems complicate relating activity patterns between different modalities of brain-activity measurement (e.g., fMRI and invasive or scalp electrophysiology), and between subjects and species. In order to bridge these divides, we suggest abstracting from the activity patterns themselves and computing representational dissimilarity matrices (RDMs), which characterize the information carried by a given representation in a brain or model. Building on a rich psychological and mathematical literature on similarity analysis, we propose a new experimental and data-analytical framework called representational similarity analysis (RSA), in which multi-channel measures of neural activity are quantitatively related to each other and to computational theory and behavior by comparing RDMs. We demonstrate RSA by relating representations of visual objects as measured with fMRI in early visual cortex and the fusiform face area to computational models spanning a wide range of complexities.The RDMs are simultaneously related via second-level application of multidimensional scaling and tested using randomization and bootstrap techniques. We discuss the broad potential of RSA, including novel approaches to experimental design, and argue that these ideas, which have deep roots in psychology and neuroscience, will allow the integrated quantitative analysis of data from all three branches, thus contributing to a more uniﬁed systems neuroscience.},
	language = {en},
	urldate = {2020-12-10},
	journal = {Frontiers in Systems Neuroscience},
	author = {Kriegeskorte, Nikolaus},
	year = {2008},
	file = {Kriegeskorte - 2008 - Representational similarity analysis – connecting .pdf:/Users/alexpsq/Zotero/storage/XX98RBGK/Kriegeskorte - 2008 - Representational similarity analysis – connecting .pdf:application/pdf},
}

@article{chyzhyk_confounds_nodate,
	title = {Confounds in predictive models: removing or controlling their eﬀects},
	abstract = {With increasing data sizes and easy availability of computational methods, neurosciences rely more and more on predictive modeling with machine learning, eg to extract imaging biomarkers of pathologies. Yet, a successful prediction may be driven not by brain feature speciﬁc to the question of interest –eg the pathology– but rather capture an undesirable confounding eﬀect correlated with the outcome. For instance, patients tend to move more in the scanner than typical controls. Imaging biomarkers of a pathology may be an expensive way of measuring head motion. Here we study how methods to control for confounds in statistical analysis can be adapted to predictive modeling settings. We consider the strategy of deconfounding, removing the eﬀect of the confounding variable, used in classic statistics. We also show how cross-validation, the method used to measure prediction accuracy, can be adapted to account for confounds and measure prediction accuracy independently of the confounds. To guide understanding and practical recommendations, we apply various strategies to assess predictive models in the presence of confounds on simulated data and population brain imaging settings. Theoretical and empirical studies show that deconfounding approaches must be adapted to decouple modeling the eﬀect of confounds on the train set and removing it on all the data. Cross-validation crafted to isolate confounding eﬀects give an additional very useful information: the prediction accuracy in the absence of confounding eﬀect.},
	language = {en},
	author = {Chyzhyk, Darya and Varoquaux, Gael and Milham, Michael and Thirion, Bertrand},
	pages = {12},
	file = {Chyzhyk et al. - Confounds in predictive models removing or contro.pdf:/Users/alexpsq/Zotero/storage/IMKMUN4I/Chyzhyk et al. - Confounds in predictive models removing or contro.pdf:application/pdf},
}

@article{nickel_poincare_nodate,
	title = {Poincaré {Embeddings} for {Learning} {Hierarchical} {Representations}},
	abstract = {Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, state-of-the-art embedding methods typically do not account for latent hierarchical structures which are characteristic for many complex symbolic datasets. In this work, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space – or more precisely into an n-dimensional Poincaré ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We present an efﬁcient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincaré embeddings can outperform Euclidean embeddings signiﬁcantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.},
	language = {en},
	author = {Nickel, Maximillian and Kiela, Douwe},
	pages = {10},
	file = {Nickel et Kiela - Poincaré Embeddings for Learning Hierarchical Repr.pdf:/Users/alexpsq/Zotero/storage/D8NZ532E/Nickel et Kiela - Poincaré Embeddings for Learning Hierarchical Repr.pdf:application/pdf},
}

@article{pascanu_number_2014,
	title = {On the number of response regions of deep feed forward networks with piece-wise linear activations},
	url = {http://arxiv.org/abs/1312.6098},
	abstract = {This paper explores the complexity of deep feedforward networks with linear presynaptic couplings and rectiﬁed linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piecewise linear functions based on computational geometry. We look at a deep rectiﬁer multi-layer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime, when the number of inputs stays constant, if the shallow model has kn hidden units and n0 inputs, then the number of linear regions is O(kn0 nn0 ). For a k layer model with n hidden units on each layer it is Ω( n/n0 k−1 nn0 ). The number n/n0 k−1 grows faster than kn0 when n tends to inﬁnity or when k tends to inﬁnity and n ≥ 2n0. Additionally, even when k is small, if we restrict n to be 2n0, we can show that a deep model has considerably more linear regions that a shallow one. We consider this as a ﬁrst step towards understanding the complexity of these models and speciﬁcally towards providing suitable mathematical tools for future analysis.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1312.6098 [cs]},
	author = {Pascanu, Razvan and Montufar, Guido and Bengio, Yoshua},
	month = feb,
	year = {2014},
	note = {arXiv: 1312.6098},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 17 pages, 9 figures},
	file = {Pascanu et al. - 2014 - On the number of response regions of deep feed for.pdf:/Users/alexpsq/Zotero/storage/GC829WA6/Pascanu et al. - 2014 - On the number of response regions of deep feed for.pdf:application/pdf},
}

@article{montufar_number_2014,
	title = {On the {Number} of {Linear} {Regions} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1402.1869},
	abstract = {We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer’s input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network’s depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not speciﬁc to a single family of models, and as an example, we employ it for rectiﬁer and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1402.1869 [cs, stat]},
	author = {Montúfar, Guido and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1402.1869},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Montúfar et al. - 2014 - On the Number of Linear Regions of Deep Neural Net.pdf:/Users/alexpsq/Zotero/storage/RBQIBDDU/Montúfar et al. - 2014 - On the Number of Linear Regions of Deep Neural Net.pdf:application/pdf},
}

@article{goodfellow_maxout_2013,
	title = {Maxout {Networks}},
	url = {http://arxiv.org/abs/1302.4389},
	abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We deﬁne a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout’s fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classiﬁcation performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR100, and SVHN.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1302.4389 [cs, stat]},
	author = {Goodfellow, Ian J. and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
	month = sep,
	year = {2013},
	note = {arXiv: 1302.4389},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: This is the version of the paper that appears in ICML 2013},
	file = {Goodfellow et al. - 2013 - Maxout Networks.pdf:/Users/alexpsq/Zotero/storage/HFRRQ9EA/Goodfellow et al. - 2013 - Maxout Networks.pdf:application/pdf},
}

@article{gauthier_linking_2019-1,
	title = {Linking artificial and human neural representations of language},
	url = {http://arxiv.org/abs/1910.01244},
	abstract = {What information from an act of sentence understanding is robustly represented in the human brain? We investigate this question by comparing sentence encoding models on a brain decoding task, where the sentence that an experimental participant has seen must be predicted from the fMRI signal evoked by the sentence. We take a pre-trained BERT architecture as a baseline sentence encoding model and ﬁne-tune it on a variety of natural language understanding (NLU) tasks, asking which lead to improvements in brain-decoding performance. We ﬁnd that none of the sentence encoding tasks tested yield signiﬁcant increases in brain decoding performance. Through further task ablations and representational analyses, we ﬁnd that tasks which produce syntax-light representations yield signiﬁcant improvements in brain decoding performance. Our results constrain the space of NLU models that could best account for human neural representations of language, but also suggest limits on the possibility of decoding ﬁne-grained syntactic information from fMRI human neuroimaging.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:1910.01244 [cs, q-bio]},
	author = {Gauthier, Jon and Levy, Roger},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.01244},
	keywords = {Computer Science - Computation and Language, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: EMNLP 2019},
	file = {Gauthier et Levy - 2019 - Linking artificial and human neural representation.pdf:/Users/alexpsq/Zotero/storage/A9WP4PRU/Gauthier et Levy - 2019 - Linking artificial and human neural representation.pdf:application/pdf},
}

@techreport{oganian_speech_2018,
	type = {preprint},
	title = {A speech envelope landmark for syllable encoding in human superior temporal gyrus},
	url = {http://biorxiv.org/lookup/doi/10.1101/388280},
	abstract = {Listeners use the slow amplitude modulations of speech, known as the envelope, to segment continuous speech into syllables. However, the underlying neural computations are heavily debated. We used high-density intracranial cortical recordings while participants listened to natural and synthesized control speech stimuli to determine how the envelope is represented in the human superior temporal gyrus (STG), a critical auditory brain area for speech processing. We found that the STG does not encode the instantaneous, moment-by-moment amplitude envelope of speech. Rather, a zone of the middle STG detects discrete acoustic onset edges, defined by local maxima in the rate-of-change of the envelope. Acoustic analysis demonstrated that acoustic onset edges reliably cue the information-rich transition between the consonantonset and vowel-nucleus of syllables. Furthermore, the steepness of the acoustic edge cued whether a syllable was stressed. Synthesized amplitude-modulated tone stimuli showed that steeper edges elicited monotonically greater cortical responses, confirming the encoding of relative but not absolute amplitude. Overall, encoding of the timing and magnitude of acoustic onset edges in STG underlies our perception of the syllabic rhythm of speech.},
	language = {en},
	urldate = {2020-12-10},
	institution = {Neuroscience},
	author = {Oganian, Yulia and Chang, Edward F.},
	month = aug,
	year = {2018},
	doi = {10.1101/388280},
	file = {Oganian et Chang - 2018 - A speech envelope landmark for syllable encoding i.pdf:/Users/alexpsq/Zotero/storage/98G7REAN/Oganian et Chang - 2018 - A speech envelope landmark for syllable encoding i.pdf:application/pdf},
}

@article{zoph_learning_2018,
	title = {Learning {Transferable} {Architectures} for {Scalable} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1707.07012},
	abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "NASNet search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4\% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7\% top-1 and 96.2\% top-5 on ImageNet. Our model is 1.2\% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28\% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74\% top-1 accuracy, which is 3.1\% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0\% achieving 43.1\% mAP on the COCO dataset.},
	urldate = {2020-12-11},
	journal = {arXiv:1707.07012 [cs, stat]},
	author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
	month = apr,
	year = {2018},
	note = {arXiv: 1707.07012},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/VP3TWNXW/Zoph et al. - 2018 - Learning Transferable Architectures for Scalable I.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/NHTVB8LG/1707.html:text/html},
}

@article{brock_smash_2017,
	title = {{SMASH}: {One}-{Shot} {Model} {Architecture} {Search} through {HyperNetworks}},
	shorttitle = {{SMASH}},
	url = {http://arxiv.org/abs/1708.05344},
	abstract = {Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks. Our code is available at https://github.com/ajbrock/SMASH},
	urldate = {2020-12-11},
	journal = {arXiv:1708.05344 [cs]},
	author = {Brock, Andrew and Lim, Theodore and Ritchie, J. M. and Weston, Nick},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.05344},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/RJTEW2MX/Brock et al. - 2017 - SMASH One-Shot Model Architecture Search through .pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/9EQAGHDH/1708.html:text/html},
}

@article{zoph_neural_2017,
	title = {Neural {Architecture} {Search} with {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1611.01578},
	abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
	urldate = {2020-12-11},
	journal = {arXiv:1611.01578 [cs]},
	author = {Zoph, Barret and Le, Quoc V.},
	month = feb,
	year = {2017},
	note = {arXiv: 1611.01578},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/FBQAI52L/Zoph et Le - 2017 - Neural Architecture Search with Reinforcement Lear.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/UUPR9YIZ/1611.html:text/html},
}

@article{real_regularized_2019,
	title = {Regularized {Evolution} for {Image} {Classifier} {Architecture} {Search}},
	url = {http://arxiv.org/abs/1802.01548},
	abstract = {The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier---AmoebaNet-A---that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-the-art 83.9\% / 96.6\% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.},
	urldate = {2020-12-11},
	journal = {arXiv:1802.01548 [cs]},
	author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.},
	month = feb,
	year = {2019},
	note = {arXiv: 1802.01548},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Computer Science - Distributed, Parallel, and Cluster Computing, I.2.6, I.5.1, I.5.2},
	annote = {Comment: Accepted for publication at AAAI 2019, the Thirty-Third AAAI Conference on Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/GDTNL6VL/Real et al. - 2019 - Regularized Evolution for Image Classifier Archite.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/PAI66A78/1802.html:text/html},
}

@article{stanley_designing_2019,
	title = {Designing neural networks through neuroevolution},
	volume = {1},
	copyright = {2019 Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-018-0006-z},
	doi = {10.1038/s42256-018-0006-z},
	abstract = {Much of recent machine learning has focused on deep learning, in which neural network weights are trained through variants of stochastic gradient descent. An alternative approach comes from the field of neuroevolution, which harnesses evolutionary algorithms to optimize neural networks, inspired by the fact that natural brains themselves are the products of an evolutionary process. Neuroevolution enables important capabilities that are typically unavailable to gradient-based approaches, including learning neural network building blocks (for example activation functions), hyperparameters, architectures and even the algorithms for learning themselves. Neuroevolution also differs from deep learning (and deep reinforcement learning) by maintaining a population of solutions during search, enabling extreme exploration and massive parallelization. Finally, because neuroevolution research has (until recently) developed largely in isolation from gradient-based neural network research, it has developed many unique and effective techniques that should be effective in other machine learning areas too. This Review looks at several key aspects of modern neuroevolution, including large-scale computing, the benefits of novelty and diversity, the power of indirect encoding, and the field’s contributions to meta-learning and architecture search. Our hope is to inspire renewed interest in the field as it meets the potential of the increasing computation available today, to highlight how many of its ideas can provide an exciting resource for inspiration and hybridization to the deep learning, deep reinforcement learning and machine learning communities, and to explain how neuroevolution could prove to be a critical tool in the long-term pursuit of artificial general intelligence.},
	language = {en},
	number = {1},
	urldate = {2020-12-11},
	journal = {Nature Machine Intelligence},
	author = {Stanley, Kenneth O. and Clune, Jeff and Lehman, Joel and Miikkulainen, Risto},
	month = jan,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {24--35},
	file = {Full Text PDF:/Users/alexpsq/Zotero/storage/VZ4C6RUK/Stanley et al. - 2019 - Designing neural networks through neuroevolution.pdf:application/pdf;Snapshot:/Users/alexpsq/Zotero/storage/P9NTFXNB/s42256-018-0006-z.html:text/html},
}

@article{ren_benchmarking_2020,
	title = {Benchmarking deep inverse models over time, and the neural-adjoint method},
	url = {http://arxiv.org/abs/2009.12919},
	abstract = {We consider the task of solving generic inverse problems, where one wishes to determine the hidden parameters of a natural system that will give rise to a particular set of measurements. Recently many new approaches based upon deep learning have arisen generating impressive results. We conceptualize these models as different schemes for efficiently, but randomly, exploring the space of possible inverse solutions. As a result, the accuracy of each approach should be evaluated as a function of time rather than a single estimated solution, as is often done now. Using this metric, we compare several state-of-the-art inverse modeling approaches on four benchmark tasks: two existing tasks, one simple task for visualization and one new task from metamaterial design. Finally, inspired by our conception of the inverse problem, we explore a solution that uses a deep learning model to approximate the forward model, and then uses backpropagation to search for good inverse solutions. This approach, termed the neural-adjoint, achieves the best performance in many scenarios.},
	urldate = {2020-12-11},
	journal = {arXiv:2009.12919 [cs, eess, stat]},
	author = {Ren, Simiao and Padilla, Willie and Malof, Jordan},
	month = oct,
	year = {2020},
	note = {arXiv: 2009.12919},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
	annote = {Comment: Accepted to NeurIPS 2020, but not the final version},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/SCS6NIIZ/Ren et al. - 2020 - Benchmarking deep inverse models over time, and th.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/9WMURLTT/2009.html:text/html},
}

@article{eliasmith_large-scale_2012,
	title = {A {Large}-{Scale} {Model} of the {Functioning} {Brain}},
	volume = {338},
	language = {en},
	author = {Eliasmith, Chris and Stewart, Terrence C and Choo, Xuan and Bekolay, Trevor and DeWolf, Travis and Tang, Yichuan and Rasmussen, Daniel},
	year = {2012},
	pages = {5},
	file = {Eliasmith et al. - 2012 - A Large-Scale Model of the Functioning Brain.pdf:/Users/alexpsq/Zotero/storage/IIBYCDJ3/Eliasmith et al. - 2012 - A Large-Scale Model of the Functioning Brain.pdf:application/pdf},
}

@article{summerfield_how_2018,
	title = {How to build a brain from scratch},
	language = {en},
	author = {Summerfield, Christopher},
	year = {2018},
	pages = {211},
	file = {Summerfield - 2018 - How to build a brain from scratch.pdf:/Users/alexpsq/Zotero/storage/VMRHJ3SV/Summerfield - 2018 - How to build a brain from scratch.pdf:application/pdf},
}

@article{roziere_evolgan_2020,
	title = {{EvolGAN}: {Evolutionary} {Generative} {Adversarial} {Networks}},
	shorttitle = {{EvolGAN}},
	url = {http://arxiv.org/abs/2009.13311},
	abstract = {We propose to use a quality estimator and evolutionary methods to search the latent space of generative adversarial networks trained on small, difficult datasets, or both. The new method leads to the generation of significantly higher quality images while preserving the original generator's diversity. Human raters preferred an image from the new version with frequency 83.7pc for Cats, 74pc for FashionGen, 70.4pc for Horses, and 69.2pc for Artworks, and minor improvements for the already excellent GANs for faces. This approach applies to any quality scorer and GAN generator.},
	urldate = {2020-12-11},
	journal = {arXiv:2009.13311 [cs]},
	author = {Roziere, Baptiste and Teytaud, Fabien and Hosu, Vlad and Lin, Hanhe and Rapin, Jeremy and Zameshina, Mariia and Teytaud, Olivier},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.13311},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: accepted ACCV oral},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/WS8DX2ZA/Roziere et al. - 2020 - EvolGAN Evolutionary Generative Adversarial Netwo.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/9YHFE367/2009.html:text/html},
}

@article{arjovsky_towards_2017,
	title = {Towards {Principled} {Methods} for {Training} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1701.04862},
	abstract = {The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.},
	urldate = {2020-12-11},
	journal = {arXiv:1701.04862 [cs, stat]},
	author = {Arjovsky, Martin and Bottou, Léon},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.04862},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/SRMRFTDX/Arjovsky et Bottou - 2017 - Towards Principled Methods for Training Generative.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/57U89759/1701.html:text/html},
}

@article{mirza_conditional_2014,
	title = {Conditional {Generative} {Adversarial} {Nets}},
	url = {http://arxiv.org/abs/1411.1784},
	abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
	urldate = {2020-12-11},
	journal = {arXiv:1411.1784 [cs, stat]},
	author = {Mirza, Mehdi and Osindero, Simon},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.1784},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/3HBQSCFD/Mirza et Osindero - 2014 - Conditional Generative Adversarial Nets.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/Z2R8HGQ2/1411.html:text/html},
}

@article{rogers_primer_2020,
	title = {A {Primer} in {BERTology}: {What} we know about how {BERT} works},
	shorttitle = {A {Primer} in {BERTology}},
	url = {http://arxiv.org/abs/2002.12327},
	abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.},
	urldate = {2020-12-11},
	journal = {arXiv:2002.12327 [cs]},
	author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
	month = nov,
	year = {2020},
	note = {arXiv: 2002.12327},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted to TACL. Please note that the multilingual BERT section is only available in version 1},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/TRLCFMC3/Rogers et al. - 2020 - A Primer in BERTology What we know about how BERT.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/9HPCMTIW/2002.html:text/html},
}

@book{quinonero-candela_dataset_2009,
	address = {Cambridge, Mass},
	series = {Neural information processing series},
	title = {Dataset shift in machine learning},
	isbn = {978-0-262-17005-5},
	language = {en},
	publisher = {MIT Press},
	editor = {Quiñonero-Candela, Joaquin},
	year = {2009},
	note = {OCLC: ocn227205909},
	keywords = {Machine learning},
	file = {Quiñonero-Candela - 2009 - Dataset shift in machine learning.pdf:/Users/alexpsq/Zotero/storage/V5PREY3P/Quiñonero-Candela - 2009 - Dataset shift in machine learning.pdf:application/pdf},
}

@article{laforgue_autoencoding_2020,
	title = {Autoencoding any {Data} through {Kernel} {Autoencoders}},
	url = {http://arxiv.org/abs/1805.11028},
	abstract = {This paper investigates a novel algorithmic approach to data representation based on kernel methods. Assuming that the observations lie in a Hilbert space X, the introduced Kernel Autoencoder (KAE) is the composition of mappings from vector-valued Reproducing Kernel Hilbert Spaces (vv-RKHSs) that minimizes the expected reconstruction error. Beyond a first extension of the autoencoding scheme to possibly infinite dimensional Hilbert spaces, KAE further allows to autoencode any kind of data by choosing X to be itself a RKHS. A theoretical analysis of the model is carried out, providing a generalization bound, and shedding light on its connection with Kernel Principal Component Analysis. The proposed algorithms are then detailed at length: they crucially rely on the form taken by the minimizers, revealed by a dedicated Representer Theorem. Finally, numerical experiments on both simulated data and real labeled graphs (molecules) provide empirical evidence of the KAE performances.},
	urldate = {2020-12-11},
	journal = {arXiv:1805.11028 [cs, stat]},
	author = {Laforgue, Pierre and Clémençon, Stephan and d'Alché-Buc, Florence},
	month = dec,
	year = {2020},
	note = {arXiv: 1805.11028},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/6PWWJVK8/Laforgue et al. - 2020 - Autoencoding any Data through Kernel Autoencoders.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/KG4CJUB5/1805.html:text/html},
}

@article{chen_deepfacedrawing_2020,
	title = {{DeepFaceDrawing}: deep generation of face images from sketches},
	volume = {39},
	issn = {0730-0301, 1557-7368},
	shorttitle = {{DeepFaceDrawing}},
	url = {https://dl.acm.org/doi/10.1145/3386569.3392386},
	doi = {10.1145/3386569.3392386},
	language = {en},
	number = {4},
	urldate = {2020-12-11},
	journal = {ACM Transactions on Graphics},
	author = {Chen, Shu-Yu and Su, Wanchao and Gao, Lin and Xia, Shihong and Fu, Hongbo},
	month = jul,
	year = {2020},
	file = {Chen et al. - 2020 - DeepFaceDrawing deep generation of face images fr.pdf:/Users/alexpsq/Zotero/storage/ZUXWN3TM/Chen et al. - 2020 - DeepFaceDrawing deep generation of face images fr.pdf:application/pdf},
}

@article{dhamdhere_how_2018,
	title = {How {Important} {Is} a {Neuron}?},
	url = {http://arxiv.org/abs/1805.12233},
	abstract = {The problem of attributing a deep network's prediction to its {\textbackslash}emph\{input/base\} features is well-studied. We introduce the notion of {\textbackslash}emph\{conductance\} to extend the notion of attribution to the understanding the importance of {\textbackslash}emph\{hidden\} units. Informally, the conductance of a hidden unit of a deep network is the {\textbackslash}emph\{flow\} of attribution via this hidden unit. We use conductance to understand the importance of a hidden unit to the prediction for a specific input, or over a set of inputs. We evaluate the effectiveness of conductance in multiple ways, including theoretical properties, ablation studies, and a feature selection task. The empirical evaluations are done using the Inception network over ImageNet data, and a sentiment analysis network over reviews. In both cases, we demonstrate the effectiveness of conductance in identifying interesting insights about the internal workings of these networks.},
	urldate = {2020-12-11},
	journal = {arXiv:1805.12233 [cs, stat]},
	author = {Dhamdhere, Kedar and Sundararajan, Mukund and Yan, Qiqi},
	month = may,
	year = {2018},
	note = {arXiv: 1805.12233},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: under submission},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/27YF66BP/Dhamdhere et al. - 2018 - How Important Is a Neuron.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/4VK4R6CV/1805.html:text/html},
}

@article{douglas_similarity_2020,
	title = {On the {Similarity} of {Deep} {Learning} {Representations} {Across} {Didactic} and {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/2002.06816},
	abstract = {The increasing use of deep neural networks (DNNs) has motivated a parallel endeavor: the design of adversaries that profit from successful misclassifications. However, not all adversarial examples are crafted for malicious purposes. For example, real world systems often contain physical, temporal, and sampling variability across instrumentation. Adversarial examples in the wild may inadvertently prove deleterious for accurate predictive modeling. Conversely, naturally occurring covariance of image features may serve didactic purposes. Here, we studied the stability of deep learning representations for neuroimaging classification across didactic and adversarial conditions characteristic of MRI acquisition variability. We show that representational similarity and performance vary according to the frequency of adversarial examples in the input space.},
	urldate = {2020-12-11},
	journal = {arXiv:2002.06816 [cs, eess, q-bio]},
	author = {Douglas, Pamela K. and Farahani, Farzad Vasheghani},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.06816},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Quantitative Biology - Neurons and Cognition, I.2.4},
	annote = {Comment: 2 figures},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/I85A585Z/Douglas et Farahani - 2020 - On the Similarity of Deep Learning Representations.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/RQUZN9DN/2002.html:text/html},
}

@article{papamakarios_normalizing_2019,
	title = {Normalizing {Flows} for {Probabilistic} {Modeling} and {Inference}},
	url = {http://arxiv.org/abs/1912.02762},
	abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
	urldate = {2020-12-11},
	journal = {arXiv:1912.02762 [cs, stat]},
	author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.02762},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Review article. 60 pages, 4 figures},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/8SEVFHBM/Papamakarios et al. - 2019 - Normalizing Flows for Probabilistic Modeling and I.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/T6YR9TGJ/1912.html:text/html},
}

@article{lu_influence_2020,
	title = {Influence {Paths} for {Characterizing} {Subject}-{Verb} {Number} {Agreement} in {LSTM} {Language} {Models}},
	url = {http://arxiv.org/abs/2005.01190},
	abstract = {LSTM-based recurrent neural networks are the state-of-the-art for many natural language processing (NLP) tasks. Despite their performance, it is unclear whether, or how, LSTMs learn structural features of natural languages such as subject-verb number agreement in English. Lacking this understanding, the generality of LSTM performance on this task and their suitability for related tasks remains uncertain. Further, errors cannot be properly attributed to a lack of structural capability, training data omissions, or other exceptional faults. We introduce *influence paths*, a causal account of structural properties as carried by paths across gates and neurons of a recurrent neural network. The approach refines the notion of influence (the subject's grammatical number has influence on the grammatical number of the subsequent verb) into a set of gate or neuron-level paths. The set localizes and segments the concept (e.g., subject-verb agreement), its constituent elements (e.g., the subject), and related or interfering elements (e.g., attractors). We exemplify the methodology on a widely-studied multi-layer LSTM language model, demonstrating its accounting for subject-verb number agreement. The results offer both a finer and a more complete view of an LSTM's handling of this structural aspect of the English language than prior results based on diagnostic classifiers and ablation.},
	urldate = {2020-12-11},
	journal = {arXiv:2005.01190 [cs]},
	author = {Lu, Kaiji and Mardziel, Piotr and Leino, Klas and Fedrikson, Matt and Datta, Anupam},
	month = may,
	year = {2020},
	note = {arXiv: 2005.01190},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ACL 2020},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/4S3RXFZN/Lu et al. - 2020 - Influence Paths for Characterizing Subject-Verb Nu.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/G7RN2JDE/2005.html:text/html},
}

@inproceedings{kovaleva_revealing_2019,
	address = {Hong Kong, China},
	title = {Revealing the {Dark} {Secrets} of {BERT}},
	url = {https://www.aclweb.org/anthology/D19-1445},
	doi = {10.18653/v1/D19-1445},
	abstract = {BERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success. In the current work, we focus on the interpretation of self-attention, which is one of the fundamental underlying components of BERT. Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERT's heads. Our findings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization. While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks. We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned BERT models.},
	urldate = {2020-12-11},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and Rumshisky, Anna},
	month = nov,
	year = {2019},
	pages = {4365--4374},
	file = {Full Text PDF:/Users/alexpsq/Zotero/storage/3MQQ5CLE/Kovaleva et al. - 2019 - Revealing the Dark Secrets of BERT.pdf:application/pdf},
}

@article{peters_dissecting_2018,
	title = {Dissecting {Contextual} {Word} {Embeddings}: {Architecture} and {Representation}},
	shorttitle = {Dissecting {Contextual} {Word} {Embeddings}},
	url = {http://arxiv.org/abs/1808.08949},
	abstract = {Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.},
	urldate = {2020-12-11},
	journal = {arXiv:1808.08949 [cs]},
	author = {Peters, Matthew E. and Neumann, Mark and Zettlemoyer, Luke and Yih, Wen-tau},
	month = sep,
	year = {2018},
	note = {arXiv: 1808.08949},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: EMNLP 2018},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/VC9AZ3A2/Peters et al. - 2018 - Dissecting Contextual Word Embeddings Architectur.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/8QBYZPVR/1808.html:text/html},
}

@article{karras_analyzing_2020,
	title = {Analyzing and {Improving} the {Image} {Quality} of {StyleGAN}},
	url = {http://arxiv.org/abs/1912.04958},
	abstract = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
	urldate = {2020-12-11},
	journal = {arXiv:1912.04958 [cs, eess, stat]},
	author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
	month = mar,
	year = {2020},
	note = {arXiv: 1912.04958},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/9H4GSSW3/Karras et al. - 2020 - Analyzing and Improving the Image Quality of Style.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/K7A4PWD5/1912.html:text/html},
}

@article{radford_unsupervised_2016,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	urldate = {2020-12-11},
	journal = {arXiv:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = jan,
	year = {2016},
	note = {arXiv: 1511.06434},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Under review as a conference paper at ICLR 2016},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/TH4CGSEZ/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/6YDHNWVN/1511.html:text/html},
}

@article{conneau_what_2018,
	title = {What you can cram into a single vector: {Probing} sentence embeddings for linguistic properties},
	shorttitle = {What you can cram into a single vector},
	url = {http://arxiv.org/abs/1805.01070},
	abstract = {Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. "Downstream" tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.},
	urldate = {2020-12-11},
	journal = {arXiv:1805.01070 [cs]},
	author = {Conneau, Alexis and Kruszewski, German and Lample, Guillaume and Barrault, Loïc and Baroni, Marco},
	month = jul,
	year = {2018},
	note = {arXiv: 1805.01070},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ACL 2018},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/HM24QN5B/Conneau et al. - 2018 - What you can cram into a single vector Probing se.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/6HAZTQ6T/1805.html:text/html},
}

@inproceedings{peters_dissecting_2018-1,
	address = {Brussels, Belgium},
	title = {Dissecting {Contextual} {Word} {Embeddings}: {Architecture} and {Representation}},
	shorttitle = {Dissecting {Contextual} {Word} {Embeddings}},
	url = {https://www.aclweb.org/anthology/D18-1179},
	doi = {10.18653/v1/D18-1179},
	abstract = {Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.},
	urldate = {2020-12-11},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Peters, Matthew and Neumann, Mark and Zettlemoyer, Luke and Yih, Wen-tau},
	month = oct,
	year = {2018},
	pages = {1499--1509},
	file = {Full Text PDF:/Users/alexpsq/Zotero/storage/BG7V8HET/Peters et al. - 2018 - Dissecting Contextual Word Embeddings Architectur.pdf:application/pdf},
}

@article{slavakis_riemannian-geometry-based_2017,
	title = {Riemannian-geometry-based modeling and clustering of network-wide non-stationary time series: {The} brain-network case},
	shorttitle = {Riemannian-geometry-based modeling and clustering of network-wide non-stationary time series},
	url = {http://arxiv.org/abs/1701.07767},
	abstract = {This paper advocates Riemannian multi-manifold modeling in the context of network-wide non-stationary time-series analysis. Time-series data, collected sequentially over time and across a network, yield features which are viewed as points in or close to a union of multiple submanifolds of a Riemannian manifold, and distinguishing disparate time series amounts to clustering multiple Riemannian submanifolds. To support the claim that exploiting the latent Riemannian geometry behind many statistical features of time series is beneficial to learning from network data, this paper focuses on brain networks and puts forth two feature-generation schemes for network-wide dynamic time series. The first is motivated by Granger-causality arguments and uses an auto-regressive moving average model to map low-rank linear vector subspaces, spanned by column vectors of appropriately defined observability matrices, to points into the Grassmann manifold. The second utilizes (non-linear) dependencies among network nodes by introducing kernel-based partial correlations to generate points in the manifold of positive-definite matrices. Capitilizing on recently developed research on clustering Riemannian submanifolds, an algorithm is provided for distinguishing time series based on their geometrical properties, revealed within Riemannian feature spaces. Extensive numerical tests demonstrate that the proposed framework outperforms classical and state-of-the-art techniques in clustering brain-network states/structures hidden beneath synthetic fMRI time series and brain-activity signals generated from real brain-network structural connectivity matrices.},
	urldate = {2020-12-11},
	journal = {arXiv:1701.07767 [cs, stat]},
	author = {Slavakis, Konstantinos and Salsabilian, Shiva and Wack, David S. and Muldoon, Sarah F. and Baidoo-Williams, Henry E. and Vettel, Jean M. and Cieslak, Matthew and Grafton, Scott T.},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.07767},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/DYVKECMC/Slavakis et al. - 2017 - Riemannian-geometry-based modeling and clustering .pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/L8H9HAJ9/1701.html:text/html},
}

@inproceedings{congedo_closed-form_2017,
	address = {Jeju Island, South Korea},
	title = {A {Closed}-{Form} {Unsupervised} {Geometry}-{Aware} {Dimensionality} {Reduction} {Method} in the {Riemannian} {Manifold} of {SPD} {Matrices}},
	url = {https://hal.archives-ouvertes.fr/hal-01563153},
	abstract = {Riemannian geometry has been found accurate and robust for classifying multidimensional data, for instance, in brain-computer interfaces based on electroencephalography. Given a number of data points on the manifold of symmetric positive-definite matrices, it is often of interest to embed these points in a manifold of smaller dimension. This is necessary for large dimensions in order to preserve accuracy and useful in general to speed up computations. Geometry-aware methods try to accomplish this task while respecting as much as possible the geometry of the original data points. We provide a closed-form solution for this problem in a fully unsupervised setting. Through the analysis of three brain-computer interface data bases we show that our method allows substantial dimensionality reduction without affecting the classification accuracy.},
	urldate = {2020-12-11},
	booktitle = {39th {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society}, {Jeju} {Island} ({EMBC}'17)},
	publisher = {IEEE},
	author = {Congedo, Marco and Coelho Rodrigues, Pedro Luiz and Bouchard, Florent and Barachant, Alexandre and Jutten, Christian},
	month = jul,
	year = {2017},
	note = {Backup Publisher: IEEE},
	keywords = {Dimensionality Reduction, Manifold Learning, Riemannian Geometry},
	pages = {3198--3201},
	file = {HAL PDF Full Text:/Users/alexpsq/Zotero/storage/V2YIF7TJ/Congedo et al. - 2017 - A Closed-Form Unsupervised Geometry-Aware Dimensio.pdf:application/pdf},
}

@techreport{barachant_riemannian_2017,
	type = {preprint},
	title = {Riemannian {Geometry} {Boosts} {Representational} {Similarity} {Analyses} of {Dense} {Neural} {Time} {Series}},
	url = {http://biorxiv.org/lookup/doi/10.1101/232710},
	abstract = {Representational similarity analysis (RSA) is a popular technique to estimate the structure of mental representations from neuroimaging data. However, RSA can be difficult to estimate for neural time series, where mental representations may be distributed in a highly dimensional space. Here, we show that RSA can be efficiently estimated from dense neural time series using Riemannian geometry. Using a public magnetoencephalography dataset, we decoded 24 classes from the brain evoked responses to 720 visual stimuli. RSA estimated from the confusion matrices of a standard regularized logistic regression achieved an average decoding accuracy of 23\% (chance=4\%). Our approach based on spatial filtering and Riemannian geometry nearly doubled this score with an average 42\% decoding accuracy. Finally, our results revealed how RSA becomes ill-conceived when it derives from confusion matrices of highly accurate multivariate pattern classifications. Instead, we propose to directly estimate RSA from Riemannian metrics without fitting a multivariate pattern classifier. Overall, our approach, based on Riemannian geometry provides a principled and efficient basis to study the structure of mental representations from highly dimensional neural time series.},
	language = {en},
	urldate = {2020-12-11},
	institution = {Neuroscience},
	author = {Barachant, Alexandre and King, Jean-Rémi},
	month = dec,
	year = {2017},
	doi = {10.1101/232710},
	file = {Barachant et King - 2017 - Riemannian Geometry Boosts Representational Simila.pdf:/Users/alexpsq/Zotero/storage/CPKVTMUL/Barachant et King - 2017 - Riemannian Geometry Boosts Representational Simila.pdf:application/pdf},
}

@article{gwilliams_neural_nodate,
	title = {Neural dynamics of phoneme sequencing in real speech jointly encode order and invariant content},
	abstract = {Listeners experience speech as a sequence of discrete words. However, the real input is a continuously varying acoustic signal that blends words and phonemes into one another. Here we recorded two-hour magnetoencephalograms from 21 subjects listening to stories, in order to investigate how the brain concurrently solves three competing demands: 1) processing overlapping acoustic-phonetic information while 2) keeping track of the relative order of phonemic units and 3) maintaining individuated phonetic information until successful word recognition. We show that the human brain transforms speech input, roughly at the rate of phoneme duration, along a temporally-deﬁned representational trajectory. These representations, absent from the acoustic signal, are active earlier when phonemes are predictable than when they are surprising, and are sustained until lexical ambiguity is resolved. The results reveal how phoneme sequences in natural speech are represented and how they interface with stored lexical items.},
	language = {en},
	author = {Gwilliams, Laura and King, Jean-Remi and Marantz, Alec and Poeppel, David},
	pages = {15},
	file = {Gwilliams et al. - Neural dynamics of phoneme sequencing in real spee.pdf:/Users/alexpsq/Zotero/storage/FK7KCM2H/Gwilliams et al. - Neural dynamics of phoneme sequencing in real spee.pdf:application/pdf},
}

@article{jat_relating_2019,
	title = {Relating {Simple} {Sentence} {Representations} in {Deep} {Neural} {Networks} and the {Brain}},
	url = {http://arxiv.org/abs/1906.11861},
	abstract = {What is the relationship between sentence representations learned by deep recurrent models against those encoded by the brain? Is there any correspondence between hidden layers of these recurrent models and brain regions when processing sentences? Can these deep models be used to synthesize brain data which can then be utilized in other extrinsic tasks? We investigate these questions using sentences with simple syntax and semantics (e.g., The bone was eaten by the dog.). We consider multiple neural network architectures, including recently proposed ELMo and BERT. We use magnetoencephalography (MEG) brain recording data collected from human subjects when they were reading these simple sentences. Overall, we find that BERT's activations correlate the best with MEG brain data. We also find that the deep network representation can be used to generate brain data from new sentences to augment existing brain data. To the best of our knowledge, this is the first work showing that the MEG brain recording when reading a word in a sentence can be used to distinguish earlier words in the sentence. Our exploration is also the first to use deep neural network representations to generate synthetic brain data and to show that it helps in improving subsequent stimuli decoding task accuracy.},
	urldate = {2020-12-11},
	journal = {arXiv:1906.11861 [cs]},
	author = {Jat, Sharmistha and Tang, Hao and Talukdar, Partha and Mitchell, Tom},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.11861},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Association for Computational Linguistics (ACL) 2019},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/PICSG8XE/Jat et al. - 2019 - Relating Simple Sentence Representations in Deep N.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/5BT7E3CA/1906.html:text/html},
}

@article{tenney_what_2019,
	title = {{WHAT} {DO} {YOU} {LEARN} {FROM} {CONTEXT}? {PROBING} {FOR} {SENTENCE} {STRUCTURE} {IN} {CONTEXTUALIZED} {WORD} {REPRESENTATIONS}},
	abstract = {Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We ﬁnd that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.},
	language = {en},
	author = {Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R Thomas and Kim, Najoung and Durme, Benjamin Van and Bowman, Samuel R and Das, Dipanjan and Pavlick, Ellie},
	year = {2019},
	pages = {17},
	file = {Tenney et al. - 2019 - WHAT DO YOU LEARN FROM CONTEXT PROBING FOR SENTEN.pdf:/Users/alexpsq/Zotero/storage/8IKQZJQ3/Tenney et al. - 2019 - WHAT DO YOU LEARN FROM CONTEXT PROBING FOR SENTEN.pdf:application/pdf},
}

@article{brock_large_2019,
	title = {{LARGE} {SCALE} {GAN} {TRAINING} {FOR} {HIGH} {FIDELITY} {NATURAL} {IMAGE} {SYNTHESIS}},
	abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities speciﬁc to such scale. We ﬁnd that applying orthogonal regularization to the generator renders it amenable to a simple “truncation trick,” allowing ﬁne control over the trade-off between sample ﬁdelity and variety by reducing the variance of the Generator’s input. Our modiﬁcations lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128×128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Fre´chet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.65.},
	language = {en},
	author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
	year = {2019},
	pages = {35},
	file = {Brock et al. - 2019 - LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL.pdf:/Users/alexpsq/Zotero/storage/QV4TT96G/Brock et al. - 2019 - LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL.pdf:application/pdf},
}

@article{yogatama_learning_2019,
	title = {Learning and {Evaluating} {General} {Linguistic} {Intelligence}},
	url = {http://arxiv.org/abs/1901.11373},
	abstract = {We define general linguistic intelligence as the ability to reuse previously acquired knowledge about a language's lexicon, syntax, semantics, and pragmatic conventions to adapt to new tasks quickly. Using this definition, we analyze state-of-the-art natural language understanding models and conduct an extensive empirical investigation to evaluate them against these criteria through a series of experiments that assess the task-independence of the knowledge being acquired by the learning process. In addition to task performance, we propose a new evaluation metric based on an online encoding of the test data that quantifies how quickly an existing agent (model) learns a new task. Our results show that while the field has made impressive progress in terms of model architectures that generalize to many tasks, these models still require a lot of in-domain training examples (e.g., for fine tuning, training task-specific modules), and are prone to catastrophic forgetting. Moreover, we find that far from solving general tasks (e.g., document question answering), our models are overfitting to the quirks of particular datasets (e.g., SQuAD). We discuss missing components and conjecture on how to make progress toward general linguistic intelligence.},
	urldate = {2020-12-11},
	journal = {arXiv:1901.11373 [cs, stat]},
	author = {Yogatama, Dani and d'Autume, Cyprien de Masson and Connor, Jerome and Kocisky, Tomas and Chrzanowski, Mike and Kong, Lingpeng and Lazaridou, Angeliki and Ling, Wang and Yu, Lei and Dyer, Chris and Blunsom, Phil},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.11373},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/KD2T3CSV/Yogatama et al. - 2019 - Learning and Evaluating General Linguistic Intelli.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/3N4JZFV6/1901.html:text/html},
}

@article{chollet_measure_2019,
	title = {On the {Measure} of {Intelligence}},
	url = {http://arxiv.org/abs/1911.01547},
	abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
	urldate = {2020-12-11},
	journal = {arXiv:1911.01547 [cs]},
	author = {Chollet, François},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.01547},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/SNF5CECX/Chollet - 2019 - On the Measure of Intelligence.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/CR6P9A4H/1911.html:text/html},
}

@article{brunner_identifiability_2020,
	title = {{ON} {IDENTIFIABILITY} {IN} {TRANSFORMERS}},
	abstract = {In this paper we delve deep in the Transformer architecture by investigating two of its core components: self-attention and contextual embeddings. In particular, we study the identiﬁability of attention weights and token embeddings, and the aggregation of context into hidden tokens. We show that, for sequences longer than the attention head dimension, attention weights are not identiﬁable. We propose effective attention as a complementary tool for improving explanatory interpretations based on attention. Furthermore, we show that input tokens retain to a large degree their identity across the model. We also ﬁnd evidence suggesting that identity information is mainly encoded in the angle of the embeddings and gradually decreases with depth. Finally, we demonstrate strong mixing of input information in the generation of contextual embeddings by means of a novel quantiﬁcation method based on gradient attribution. Overall, we show that self-attention distributions are not directly interpretable and present tools to better understand and further investigate Transformer models.},
	language = {en},
	author = {Brunner, Gino and Liu, Yang and Pascual, Damian and Richter, Oliver and Ciaramita, Massimiliano and Wattenhofer, Roger},
	year = {2020},
	pages = {35},
	file = {Brunner et al. - 2020 - ON IDENTIFIABILITY IN TRANSFORMERS.pdf:/Users/alexpsq/Zotero/storage/9J89N8R3/Brunner et al. - 2020 - ON IDENTIFIABILITY IN TRANSFORMERS.pdf:application/pdf},
}

@article{cordonnier_relationship_2020,
	title = {{ON} {THE} {RELATIONSHIP} {BETWEEN} {SELF}-{ATTENTION} {AND} {CONVOLUTIONAL} {LAYERS}},
	abstract = {Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Speciﬁcally, we prove that a multi-head self-attention layer with sufﬁcient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available1.},
	language = {en},
	author = {Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
	year = {2020},
	pages = {18},
	file = {Cordonnier et al. - 2020 - ON THE RELATIONSHIP BETWEEN SELF-ATTENTION AND CON.pdf:/Users/alexpsq/Zotero/storage/FVUADQJR/Cordonnier et al. - 2020 - ON THE RELATIONSHIP BETWEEN SELF-ATTENTION AND CON.pdf:application/pdf},
}

@article{parmar_image_nodate,
	title = {Image {Transformer}},
	abstract = {Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the selfattention mechanism to attend to local neighborhoods we signiﬁcantly increase the size of images the model can process in practice, despite maintaining signiﬁcantly larger receptive ﬁelds per layer than typical convolutional neural networks. While conceptually simple, our generative models signiﬁcantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magniﬁcation ratio, applying an encoder-decoder conﬁguration of our architecture. In a human evaluation study, we ﬁnd that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.},
	language = {en},
	author = {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Łukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
	pages = {10},
	file = {Parmar et al. - Image Transformer.pdf:/Users/alexpsq/Zotero/storage/MLGERP2H/Parmar et al. - Image Transformer.pdf:application/pdf},
}

@book{noauthor_invited_1999,
	title = {Invited review {Event}-related {EEG}/{MEG} synchronization and desynchronization: basic principles},
	shorttitle = {Invited review {Event}-related {EEG}/{MEG} synchronization and desynchronization},
	abstract = {An internally or externally paced event results not only in the generation of an event-related potential (ERP) but also in a change in the ongoing EEG/MEG in form of an event-related desynchronization (ERD) or event-related synchronization (ERS). The ERP on the one side and the ERD/ERS on the other side are different responses of neuronal structures in the brain. While the former is phase-locked, the latter is not phase-locked to the event. The most important difference between both phenomena is that the ERD/ERS is highly frequency band-speci®c, whereby either the same or different locations on the scalp can display ERD and ERS simultaneously. Quanti®cation of ERD/ERS in},
	year = {1999},
	file = {Citeseer - Snapshot:/Users/alexpsq/Zotero/storage/ZW8CG3DR/summary.html:text/html;Citeseer - Full Text PDF:/Users/alexpsq/Zotero/storage/3K7U933H/1999 - Invited review Event-related EEGMEG synchronizati.pdf:application/pdf},
}

@article{congedo_riemannian_2017,
	title = {Riemannian geometry for {EEG}-based brain-computer interfaces; a primer and a review},
	volume = {4},
	url = {https://hal.archives-ouvertes.fr/hal-01570120},
	doi = {10.1080/2326263X.2017.1297192},
	abstract = {Despite its short history, the use of Riemannian geometry in brain-computer interface (BCI) decoding is currently attracting increasing attention, due to accumulating documentation of its simplicity, accuracy, robustness and transfer learning capabilities, including the winning score obtained in five recent international predictive modeling BCI data competitions. The Riemannian framework is sharp from a mathematical perspective, yet in practice it is simple, both algorithmically and computationally. This allows the conception of online decoding machines suiting real-world operation in adverse conditions. We provide here a review on the use of Riemannian geometry for BCI and a primer on the classification frameworks based on it. While the theoretical research on Riemannian geometry is technical, our aim here is to show the appeal of the framework on an intuitive geometrical ground. In particular, we provide a rationale for its robustness and transfer learning capabilities and we elucidate the link between a simple Riemannian classifier and a state-of-the-art spatial filtering approach. We conclude by reporting details on the construction of data points to be manipulated in the Riemannian framework in the context of BCI and by providing links to available open-source Matlab and Python code libraries for designing BCI decoders.},
	number = {3},
	urldate = {2020-12-11},
	journal = {Brain-Computer Interfaces},
	author = {Congedo, Marco and Barachant, Alexandre and Bhatia, Rajendra},
	year = {2017},
	note = {Publisher: Taylor \& Francis},
	keywords = {Riemannian Geometry, Brain-Computer Interface BCI, Brain-Computer interfaces, Center of mass},
	pages = {155--174},
	file = {HAL PDF Full Text:/Users/alexpsq/Zotero/storage/NXCFGPD5/Congedo et al. - 2017 - Riemannian geometry for EEG-based brain-computer i.pdf:application/pdf},
}

@article{giraldo_plant_2014,
	title = {Plant nanobionics approach to augment photosynthesis and biochemical sensing},
	volume = {13},
	issn = {1476-1122, 1476-4660},
	url = {http://www.nature.com/articles/nmat3890},
	doi = {10.1038/nmat3890},
	language = {en},
	number = {4},
	urldate = {2020-12-11},
	journal = {Nature Materials},
	author = {Giraldo, Juan Pablo and Landry, Markita P. and Faltermeier, Sean M. and McNicholas, Thomas P. and Iverson, Nicole M. and Boghossian, Ardemis A. and Reuel, Nigel F. and Hilmer, Andrew J. and Sen, Fatih and Brew, Jacqueline A. and Strano, Michael S.},
	month = apr,
	year = {2014},
	pages = {400--408},
	file = {Giraldo et al. - 2014 - Plant nanobionics approach to augment photosynthes.pdf:/Users/alexpsq/Zotero/storage/NHLT9VG5/Giraldo et al. - 2014 - Plant nanobionics approach to augment photosynthes.pdf:application/pdf},
}

@article{turc_well-read_2019,
	title = {Well-{Read} {Students} {Learn} {Better}: {On} the {Importance} of {Pre}-training {Compact} {Models}},
	shorttitle = {Well-{Read} {Students} {Learn} {Better}},
	url = {http://arxiv.org/abs/1908.08962},
	abstract = {Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to down-stream tasks, several model compression techniques on pre-trained language representations have been proposed (Sun et al., 2019; Sanh, 2019). However, surprisingly, the simple baseline of just pre-training and fine-tuning compact models has been overlooked. In this paper, we first show that pre-training remains important in the context of smaller architectures, and fine-tuning pre-trained compact models can be competitive to more elaborate methods proposed in concurrent work. Starting with pre-trained compact models, we then explore transferring task knowledge from large fine-tuned models through standard knowledge distillation. The resulting simple, yet effective and general algorithm, Pre-trained Distillation, brings further improvements. Through extensive experiments, we more generally explore the interaction between pre-training and distillation under two variables that have been under-studied: model size and properties of unlabeled task data. One surprising observation is that they have a compound effect even when sequentially applied on the same data. To accelerate future research, we will make our 24 pre-trained miniature BERT models publicly available.},
	urldate = {2020-12-11},
	journal = {arXiv:1908.08962 [cs]},
	author = {Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = sep,
	year = {2019},
	note = {arXiv: 1908.08962},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Added comparison to concurrent work},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/H6E3PX6Y/Turc et al. - 2019 - Well-Read Students Learn Better On the Importance.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/8NCVWWR8/1908.html:text/html},
}

@article{haxby_hyperalignment_2020,
	title = {Hyperalignment: {Modeling} shared information encoded in idiosyncratic cortical topographies},
	volume = {9},
	issn = {2050-084X},
	shorttitle = {Hyperalignment},
	url = {https://doi.org/10.7554/eLife.56601},
	doi = {10.7554/eLife.56601},
	abstract = {Information that is shared across brains is encoded in idiosyncratic fine-scale functional topographies. Hyperalignment captures shared information by projecting pattern vectors for neural responses and connectivities into a common, high-dimensional information space, rather than by aligning topographies in a canonical anatomical space. Individual transformation matrices project information from individual anatomical spaces into the common model information space, preserving the geometry of pairwise dissimilarities between pattern vectors, and model cortical topography as mixtures of overlapping, individual-specific topographic basis functions, rather than as contiguous functional areas. The fundamental property of brain function that is preserved across brains is information content, rather than the functional properties of local features that support that content. In this Perspective, we present the conceptual framework that motivates hyperalignment, its computational underpinnings for joint modeling of a common information space and idiosyncratic cortical topographies, and discuss implications for understanding the structure of cortical functional architecture.},
	urldate = {2020-12-11},
	journal = {eLife},
	author = {Haxby, James V and Guntupalli, J Swaroop and Nastase, Samuel A and Feilong, Ma},
	editor = {Baker, Chris I and de Lange, Floris P},
	month = jun,
	year = {2020},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {cortex, functional connectivity, hyperalignment, individual differences, population response, topography},
	pages = {e56601},
	file = {Full Text PDF:/Users/alexpsq/Zotero/storage/6SA9TJZV/Haxby et al. - 2020 - Hyperalignment Modeling shared information encode.pdf:application/pdf},
}

@article{noauthor_lack_nodate,
	title = {Lack of selectivity for syntax relative to word meanings throughout the language network},
	language = {en},
	pages = {52},
	file = {Lack of selectivity for syntax relative to word me.pdf:/Users/alexpsq/Zotero/storage/96XGUGXQ/Lack of selectivity for syntax relative to word me.pdf:application/pdf},
}

@article{marek_towards_2020,
	title = {Towards {Reproducible} {Brain}-{Wide} {Association} {Studies}},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/2020.08.21.257758v1},
	doi = {10.1101/2020.08.21.257758},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}Magnetic resonance imaging (MRI) continues to drive many important neuroscientific advances. However, progress in uncovering reproducible associations between individual differences in brain structure/function and behavioral phenotypes (e.g., cognition, mental health) may have been undermined by typical neuroimaging sample sizes (median N=25)$^{\textrm{1,2}}$. Leveraging the Adolescent Brain Cognitive Development (ABCD) Study$^{\textrm{3}}$ (N=11,878), we estimated the effect sizes and reproducibility of these brain-wide associations studies (BWAS) as a function of sample size. The very largest, replicable brain-wide associations for univariate and multivariate methods were r=0.14 and r=0.34, respectively. In smaller samples, typical for brain-wide association studies (BWAS), irreproducible, inflated effect sizes were ubiquitous, no matter the method (univariate, multivariate). Until sample sizes started to approach consortium-levels, BWAS were underpowered and statistical errors assured. Multiple factors contribute to replication failures$^{\textrm{4–6}}$; here, we show that the pairing of small brain-behavioral phenotype effect sizes with sampling variability is a key element in wide-spread BWAS replication failure. Brain-behavioral phenotype associations stabilize and become more reproducible with sample sizes of N⪆2,000. While investigator-initiated brain-behavior research continues to generate hypotheses and propel innovation, large consortia are needed to usher in a new era of reproducible human brain-wide association studies.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2020-12-11},
	journal = {bioRxiv},
	author = {Marek, Scott and Tervo-Clemmens, Brenden and Calabro, Finnegan J. and Montez, David F. and Kay, Benjamin P. and Hatoum, Alexander S. and Donohue, Meghan Rose and Foran, William and Miller, Ryland L. and Feczko, Eric and Miranda-Dominguez, Oscar and Graham, Alice M. and Earl, Eric A. and Perrone, Anders J. and Cordova, Michaela and Doyle, Olivia and Moore, Lucille A. and Conan, Greg and Uriarte, Johnny and Snider, Kathy and Tam, Angela and Chen, Jianzhong and Newbold, Dillan J. and Zheng, Annie and Seider, Nicole A. and Van, Andrew N. and Laumann, Timothy O. and Thompson, Wesley K. and Greene, Deanna J. and Petersen, Steven E. and Nichols, Thomas E. and Yeo, B. T. Thomas and Barch, Deanna M. and Garavan, Hugh and Luna, Beatriz and Fair, Damien A. and Dosenbach, Nico U. F.},
	month = aug,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	pages = {2020.08.21.257758},
	file = {Snapshot:/Users/alexpsq/Zotero/storage/IG2JKQFH/2020.08.21.257758v1.html:text/html;Full Text PDF:/Users/alexpsq/Zotero/storage/ISVDGKMM/Marek et al. - 2020 - Towards Reproducible Brain-Wide Association Studie.pdf:application/pdf;Snapshot:/Users/alexpsq/Zotero/storage/HLC3QVMC/2020.08.21.257758v1.html:text/html},
}

@misc{noauthor_4_nodate,
	title = {4. {Machine} {Learning} for {Molecules} - {Deep} {Learning} for the {Life} {Sciences} [{Book}]},
	url = {https://www.oreilly.com/library/view/deep-learning-for/9781492039822/ch04.html},
	abstract = {Chapter 4. Machine Learning for Molecules This chapter covers the basics of performing machine learning on molecular data. Before we dive into the chapter, it might help for us to … - Selection from Deep Learning for the Life Sciences [Book]},
	language = {en},
	urldate = {2020-12-11},
	file = {Snapshot:/Users/alexpsq/Zotero/storage/45UT9YEK/ch04.html:text/html},
}

@article{friederici_brain_2011,
	title = {The {Brain} {Basis} of {Language} {Processing}: {From} {Structure} to {Function}},
	volume = {91},
	language = {en},
	journal = {Physiol Rev},
	author = {Friederici, Angela D},
	year = {2011},
	pages = {36},
	file = {Friederici - 2011 - The Brain Basis of Language Processing From Struc.pdf:/Users/alexpsq/Zotero/storage/RIXYD53L/Friederici - 2011 - The Brain Basis of Language Processing From Struc.pdf:application/pdf},
}

@article{dorent_learning_2021,
	title = {Learning joint segmentation of tissues and brain lesions from task-specific hetero-modal domain-shifted datasets},
	volume = {67},
	issn = {1361-8415},
	url = {http://www.sciencedirect.com/science/article/pii/S1361841520302267},
	doi = {10.1016/j.media.2020.101862},
	abstract = {Brain tissue segmentation from multimodal MRI is a key building block of many neuroimaging analysis pipelines. Established tissue segmentation approaches have, however, not been developed to cope with large anatomical changes resulting from pathology, such as white matter lesions or tumours, and often fail in these cases. In the meantime, with the advent of deep neural networks (DNNs), segmentation of brain lesions has matured significantly. However, few existing approaches allow for the joint segmentation of normal tissue and brain lesions. Developing a DNN for such a joint task is currently hampered by the fact that annotated datasets typically address only one specific task and rely on task-specific imaging protocols including a task-specific set of imaging modalities. In this work, we propose a novel approach to build a joint tissue and lesion segmentation model from aggregated task-specific hetero-modal domain-shifted and partially-annotated datasets. Starting from a variational formulation of the joint problem, we show how the expected risk can be decomposed and optimised empirically. We exploit an upper bound of the risk to deal with heterogeneous imaging modalities across datasets. To deal with potential domain shift, we integrated and tested three conventional techniques based on data augmentation, adversarial learning and pseudo-healthy generation. For each individual task, our joint approach reaches comparable performance to task-specific and fully-supervised models. The proposed framework is assessed on two different types of brain lesions: White matter lesions and gliomas. In the latter case, lacking a joint ground-truth for quantitative assessment purposes, we propose and use a novel clinically-relevant qualitative assessment methodology.},
	language = {en},
	urldate = {2020-12-11},
	journal = {Medical Image Analysis},
	author = {Dorent, Reuben and Booth, Thomas and Li, Wenqi and Sudre, Carole H. and Kafiabadi, Sina and Cardoso, Jorge and Ourselin, Sebastien and Vercauteren, Tom},
	month = jan,
	year = {2021},
	keywords = {Domain adaptation, Joint learning, Multi-Modal, Multi-Task learning},
	pages = {101862},
	file = {ScienceDirect Full Text PDF:/Users/alexpsq/Zotero/storage/IADUMPFC/Dorent et al. - 2021 - Learning joint segmentation of tissues and brain l.pdf:application/pdf;ScienceDirect Snapshot:/Users/alexpsq/Zotero/storage/KIU8SCM5/S1361841520302267.html:text/html},
}

@article{wu_stochastic_nodate,
	title = {Stochastic {Normalizing} {Flows}},
	abstract = {The sampling of probability distributions speciﬁed up to a normalization constant is an important problem in both machine learning and statistical mechanics. While classical stochastic sampling methods such as Markov Chain Monte Carlo (MCMC) or Langevin Dynamics (LD) can suffer from slow mixing times there is a growing interest in using normalizing ﬂows in order to learn the transformation of a simple prior distribution to the given target distribution. Here we propose a generalized and combined approach to sample target densities: Stochastic Normalizing Flows (SNF) – an arbitrary sequence of deterministic invertible functions and stochastic sampling blocks. We show that stochasticity overcomes expressivity limitations of normalizing ﬂows resulting from the invertibility constraint, whereas trainable transformations between sampling steps improve efﬁciency of pure MCMC/LD along the ﬂow. By invoking ideas from non-equilibrium statistical mechanics we derive an efﬁcient training procedure by which both the sampler’s and the ﬂow’s parameters can be optimized end-to-end, and by which we can compute exact importance weights without having to marginalize out the randomness of the stochastic blocks. We illustrate the representational power, sampling efﬁciency and asymptotic correctness of SNFs on several benchmarks including applications to sampling molecular systems in equilibrium.},
	language = {en},
	author = {Wu, Hao and Köhler, Jonas and Noé, Frank},
	pages = {12},
	file = {Wu et al. - Stochastic Normalizing Flows.pdf:/Users/alexpsq/Zotero/storage/GZFH2UL7/Wu et al. - Stochastic Normalizing Flows.pdf:application/pdf},
}

@article{elam_connectome_nodate,
	title = {Connectome {Workbench} v1.0 {Tutorial}},
	language = {en},
	author = {Elam, Jennifer},
	pages = {41},
}

@misc{noauthor_neural_2020,
	title = {Neural {Architecture} {Search}},
	url = {https://lilianweng.github.io/2020/08/06/neural-architecture-search.html},
	abstract = {Neural Architecture Search (NAS) automates network architecture engineering. It aims to learn a network topology that can achieve best performance on a certain task. By dissecting the methods for NAS into three components: search space, search algorithm and child model evolution strategy, this post reviews many interesting ideas for better,...},
	language = {en},
	urldate = {2020-12-11},
	journal = {Lil'Log},
	month = aug,
	year = {2020},
	file = {Snapshot:/Users/alexpsq/Zotero/storage/XMY5UHKS/neural-architecture-search.html:text/html},
}

@article{matuschak_how_2019,
	title = {How the quantum search algorithm works},
	url = {https://quantum.country/search},
	abstract = {An explanation of how the quantum search algorithm works, \& introduction to the design of quantum algorithms},
	urldate = {2020-12-11},
	author = {Matuschak, Andy and Nielsen, Michael},
	year = {2019},
	file = {Snapshot:/Users/alexpsq/Zotero/storage/L4I2HDFP/search.html:text/html},
}

@inproceedings{szwarcman_quantum-inspired_2019,
	title = {Quantum-{Inspired} {Neural} {Architecture} {Search}},
	doi = {10.1109/IJCNN.2019.8852453},
	abstract = {Deep neural networks have gained attention in the last decade as significant progress has been made in a variety of tasks thanks to these new architectures. Most of the time, hand-designed networks are responsible for this incredible success. However, this engineering process demands considerable time and expert knowledge, which leads to an increasing interest in automating the design of deep architectures. Several new algorithms have been proposed to address the neural architecture search problem, but many of them require significant computational resources. Quantum-inspired evolutionary algorithms (QIEA) have their roots on quantum computing principles and present promising results in respect to faster convergence. In this work, we propose Q-NAS (Quantum-inspired Neural Architecture Search): a quantum-inspired algorithm to search for deep neural architectures by assembling substructures and optimizing some numerical hyperparameters. We present the first results applying Q-NAS on the CIFAR-10 dataset using only 20 K80 GPUs for about 50 hours. The obtained networks are relatively small (less than 20 layers) compared to other state-of-the-art models and achieve promising accuracies with considerably less computational cost than other NAS algorithms.},
	booktitle = {2019 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Szwarcman, D. and Civitarese, D. and Vellasco, M.},
	month = jul,
	year = {2019},
	note = {ISSN: 2161-4407},
	keywords = {Computer architecture, deep learning, Deep learning, deep neural architectures, deep neural networks, evolutionary computation, Evolutionary computation, expert knowledge, expert systems, hand-designed networks, Neural architecture search, neural architecture search problem, neural nets, Probability density function, Q-NAS, quantum computing, Quantum computing, quantum computing principles, quantum inspired algorithms, quantum-inspired evolutionary algorithms, quantum-inspired neural architecture search, search problems, Sociology, Statistics},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:/Users/alexpsq/Zotero/storage/NMGG97UQ/8852453.html:text/html},
}

@inproceedings{szwarcman_quantum-inspired_2019-1,
	address = {Budapest, Hungary},
	title = {Quantum-{Inspired} {Neural} {Architecture} {Search}},
	isbn = {978-1-72811-985-4},
	url = {https://ieeexplore.ieee.org/document/8852453/},
	doi = {10.1109/IJCNN.2019.8852453},
	language = {en},
	urldate = {2020-12-11},
	booktitle = {2019 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Szwarcman, Daniela and Civitarese, Daniel and Vellasco, Marley},
	month = jul,
	year = {2019},
	pages = {1--8},
	file = {Szwarcman et al. - 2019 - Quantum-Inspired Neural Architecture Search.pdf:/Users/alexpsq/Zotero/storage/PDHW3ZQY/Szwarcman et al. - 2019 - Quantum-Inspired Neural Architecture Search.pdf:application/pdf},
}

@article{zhang_differentiable_2020,
	title = {Differentiable {Quantum} {Architecture} {Search}},
	url = {http://arxiv.org/abs/2010.08561},
	abstract = {Quantum architecture search (QAS) is the process of automating architecture engineering of quantum circuits. It has been desired to construct a powerful and general QAS platform which can significantly accelerate current efforts to identify quantum advantages of error-prone and depth-limited quantum circuits in the NISQ era. Hereby, we propose a general framework of differentiable quantum architecture search (DQAS), which enables automated designs of quantum circuits in an end-to-end differentiable fashion. We present several examples of circuit design problems to demonstrate the power of DQAS. For instance, unitary operations are decomposed into quantum gates, noisy circuits are re-designed to improve accuracy, and circuit layouts for quantum approximation optimization algorithm are automatically discovered and upgraded for combinatorial optimization problems. These results not only manifest the vast potential of DQAS being an essential tool for the NISQ application developments, but also present an interesting research topic from the theoretical perspective as it draws inspirations from the newly emerging interdisciplinary paradigms of differentiable programming, probabilistic programming, and quantum programming.},
	urldate = {2020-12-11},
	journal = {arXiv:2010.08561 [quant-ph]},
	author = {Zhang, Shi-Xin and Hsieh, Chang-Yu and Zhang, Shengyu and Yao, Hong},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.08561},
	keywords = {Quantum Physics},
	annote = {Comment: 8.1 pages + supplemental materials, 6 figures},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/M2A8UFSL/Zhang et al. - 2020 - Differentiable Quantum Architecture Search.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/D94NCYUA/2010.html:text/html},
}

@inproceedings{ye_quantum-inspired_2020,
	title = {Quantum-{Inspired} {Evolutionary} {Algorithm} for {Convolutional} {Neural} {Networks} {Architecture} {Search}},
	doi = {10.1109/CEC48606.2020.9185727},
	abstract = {Convolutional neural networks (CNN) are widely used and effective deep learning methods for image classification tasks. But the architecture of CNN such as LetNet and AlexNet were designed elaborately by experts because designing the neural networks is time-consuming and requires expert knowledge. This paper proposed a quantum-inspired evolutionary algorithm to search the neural architectures. First, we encode CNNs into quantum chromosomes and distinguish these chromosomes from the Convolutional Layer, Pooling Layer, Fully-connected Layer and Disabled Layer with its range. Second, quantum chromosomes are updated by applying quantum gates and find the best individual with quantum genetic algorithm. Third, we can predict the network performance after a few steps of stochastic gradient descent by means of evaluation estimate strategy so that we can stop training the bad networks early, which can speed up evolutionary process. The proposed algorithm is examined and compared with some state-of-art methods for image classification in three benchmark datasets. The experimental results prove the proposed algorithm can search a strong classifier robustly. In addition, it performs better than the general evolutionary algorithm. More importantly, with the help of evaluation estimate strategy, it is substantially faster than the algorithms without evaluation estimate strategy which means we can take less time to search a good network for the given task.},
	booktitle = {2020 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	author = {Ye, W. and Liu, R. and Li, Y. and Jiao, L.},
	month = jul,
	year = {2020},
	keywords = {Computer architecture, evolutionary computation, Evolutionary computation, expert knowledge, neural nets, Quantum computing, bad networks, Biological cells, CNN, convolution, Convolutional Layer, Convolutional neural networks, Convolutional neural networks architecture search, Disabled Layer, effective deep learning methods, evaluation estimate, evaluation estimate strategy, evolutionary process, Fully-connected Layer, general evolutionary algorithm, genetic algorithm, genetic algorithms, good network, gradient methods, image classification, image classification tasks, learning (artificial intelligence), network performance, neural architecture search, neural architectures, Pooling Layer, quantum chromosomes, quantum gates, quantum genetic algorithm, quantum-inspired, quantum-inspired evolutionary algorithm, Task analysis},
	pages = {1--8},
	file = {Ye et al. - 2020 - Quantum-Inspired Evolutionary Algorithm for Convol.pdf:/Users/alexpsq/Zotero/storage/L85FKIYV/Ye et al. - 2020 - Quantum-Inspired Evolutionary Algorithm for Convol.pdf:application/pdf},
}

@article{cichy_similarity-based_2016,
	title = {Similarity-{Based} {Fusion} of {MEG} and {fMRI} {Reveals} {Spatio}-{Temporal} {Dynamics} in {Human} {Cortex} {During} {Visual} {Object} {Recognition}},
	volume = {26},
	issn = {1460-2199},
	doi = {10.1093/cercor/bhw135},
	abstract = {Every human cognitive function, such as visual object recognition, is realized in a complex spatio-temporal activity pattern in the brain. Current brain imaging techniques in isolation cannot resolve the brain's spatio-temporal dynamics, because they provide either high spatial or temporal resolution but not both. To overcome this limitation, we developed an integration approach that uses representational similarities to combine measurements of magnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI) to yield a spatially and temporally integrated characterization of neuronal activation. Applying this approach to 2 independent MEG-fMRI data sets, we observed that neural activity first emerged in the occipital pole at 50-80 ms, before spreading rapidly and progressively in the anterior direction along the ventral and dorsal visual streams. Further region-of-interest analyses established that dorsal and ventral regions showed MEG-fMRI correspondence in representations later than early visual cortex. Together, these results provide a novel and comprehensive, spatio-temporally resolved view of the rapid neural dynamics during the first few hundred milliseconds of object vision. They further demonstrate the feasibility of spatially unbiased representational similarity-based fusion of MEG and fMRI, promising new insights into how the brain computes complex cognitive functions.},
	language = {eng},
	number = {8},
	journal = {Cerebral Cortex (New York, N.Y.: 1991)},
	author = {Cichy, Radoslaw Martin and Pantazis, Dimitrios and Oliva, Aude},
	year = {2016},
	pmid = {27235099},
	pmcid = {PMC4961022},
	keywords = {Adult, Brain Mapping, Cerebral Cortex, Feasibility Studies, Female, fMRI, Humans, Magnetic Resonance Imaging, Magnetoencephalography, Male, MEG, Multimodal Imaging, multimodal integration, Neuropsychological Tests, Pattern Recognition, Visual, Recognition, Psychology, representational similarity analysis, Signal Processing, Computer-Assisted, visual object recognition, Visual Pathways},
	pages = {3563--3579},
	file = {Texte intégral:/Users/alexpsq/Zotero/storage/GYCB4QEI/Cichy et al. - 2016 - Similarity-Based Fusion of MEG and fMRI Reveals Sp.pdf:application/pdf},
}

@book{chawla_proceedings_2017,
	address = {Philadelphia, PA},
	title = {Proceedings of the 2017 {SIAM} {International} {Conference} on {Data} {Mining}},
	isbn = {978-1-61197-497-3},
	url = {https://epubs.siam.org/doi/book/10.1137/1.9781611974973},
	abstract = {How close can we zoom in to observe brain activity? Our understanding is limited by the resolution of imaging modalities that exhibit good spatial but poor temporal resolution, or vice-versa. In this paper, we propose BRAINZOOM, an efﬁcient imaging algorithm that crossleverages multi-modal brain signals. BRAINZOOM (a) constructs high resolution brain images from multi-modal signals, (b) is scalable, and (c) is ﬂexible in that it can easily incorporate various priors on the brain activities, such as sparsity, low rank, or smoothness. We carefully formulate the problem to tackle nonlinearity in the measurements (via variable splitting) and auto-scale between different modal signals, and judiciously design an inexact alternating optimization-based algorithmic framework to handle the problem with provable convergence guarantees. Our experiments using a popular realistic brain signal simulator to generate fMRI and MEG demonstrate that high spatio-temporal resolution brain imaging is possible from these two modalities. The experiments also suggest that smoothness seems to be the best prior, among several we tried.},
	language = {en},
	urldate = {2020-12-11},
	publisher = {Society for Industrial and Applied Mathematics},
	editor = {Chawla, Nitesh and Wang, Wei},
	month = jun,
	year = {2017},
	doi = {10.1137/1.9781611974973},
	file = {Chawla et Wang - 2017 - Proceedings of the 2017 SIAM International Confere.pdf:/Users/alexpsq/Zotero/storage/NWIXDYRV/Chawla et Wang - 2017 - Proceedings of the 2017 SIAM International Confere.pdf:application/pdf},
}

@article{zhang_simple_nodate,
	title = {Simple and {Scalable} {Sparse} k-means {Clustering} via {Feature} {Ranking}},
	abstract = {Clustering, a fundamental activity in unsupervised learning, is notoriously difﬁcult when the feature space is high-dimensional. Fortunately, in many realistic scenarios, only a handful of features may be relevant in distinguishing clusters. This has motivated the development of sparse clustering techniques that typically rely on k-means within outer algorithms of high computational complexity. Current techniques also require careful tuning of shrinkage parameters, further limiting their scalability. In this paper, we propose a novel framework for sparse k-means clustering that is intuitive, simple to implement, and competitive with state-of-theart algorithms. We show that our algorithm enjoys consistency and convergence guarantees. Our core method readily generalizes to several task-speciﬁc algorithms such as clustering on subsets of attributes and in partially observed data settings. We showcase these contributions thoroughly via simulated experiments and real data benchmarks, including a case study on protein expression in trisomic mice.},
	language = {en},
	author = {Zhang, Zhiyue and Lange, Kenneth and Xu, Jason},
	pages = {13},
	file = {Zhang et al. - Simple and Scalable Sparse k-means Clustering via .pdf:/Users/alexpsq/Zotero/storage/NZ3JIY5E/Zhang et al. - Simple and Scalable Sparse k-means Clustering via .pdf:application/pdf},
}


@inproceedings{hale_finding_2018,
	address = {Melbourne, Australia},
	title = {Finding syntax in human encephalography with beam search},
	url = {https://www.aclweb.org/anthology/P18-1254},
	doi = {10.18653/v1/P18-1254},
	abstract = {Recurrent neural network grammars (RNNGs) are generative models of (tree , string ) pairs that rely on neural networks to evaluate derivational choices. Parsing with them using beam search yields a variety of incremental complexity metrics such as word surprisal and parser action count. When used as regressors against human electrophysiological responses to naturalistic text, they derive two amplitude effects: an early peak and a P600-like later peak. By contrast, a non-syntactic neural language model yields no reliable effects. Model comparisons attribute the early peak to syntactic composition within the RNNG. This pattern of results recommends the RNNG+beam search combination as a mechanistic model of the syntactic processing that occurs during normal human language comprehension.},
	urldate = {2020-12-17},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hale, John and Dyer, Chris and Kuncoro, Adhiguna and Brennan, Jonathan},
	month = jul,
	year = {2018},
	pages = {2727--2736},
	file = {Full Text PDF:/Users/alexpsq/Zotero/storage/L6E8H29K/Hale et al. - 2018 - Finding syntax in human encephalography with beam .pdf:application/pdf},
}

@article{kell_task-optimized_2018-1,
	title = {A {Task}-{Optimized} {Neural} {Network} {Replicates} {Human} {Auditory} {Behavior}, {Predicts} {Brain} {Responses}, and {Reveals} a {Cortical} {Processing} {Hierarchy}},
	volume = {98},
	issn = {0896-6273},
	url = {http://www.sciencedirect.com/science/article/pii/S0896627318302502},
	doi = {10.1016/j.neuron.2018.03.044},
	abstract = {A core goal of auditory neuroscience is to build quantitative models that predict cortical responses to natural sounds. Reasoning that a complete model of auditory cortex must solve ecologically relevant tasks, we optimized hierarchical neural networks for speech and music recognition. The best-performing network contained separate music and speech pathways following early shared processing, potentially replicating human cortical organization. The network performed both tasks as well as humans and exhibited human-like errors despite not being optimized to do so, suggesting common constraints on network and human performance. The network predicted fMRI voxel responses substantially better than traditional spectrotemporal filter models throughout auditory cortex. It also provided a quantitative signature of cortical representational hierarchy—primary and non-primary responses were best predicted by intermediate and late network layers, respectively. The results suggest that task optimization provides a powerful set of tools for modeling sensory systems.},
	language = {en},
	number = {3},
	urldate = {2020-12-17},
	journal = {Neuron},
	author = {Kell, Alexander J. E. and Yamins, Daniel L. K. and Shook, Erica N. and Norman-Haignere, Sam V. and McDermott, Josh H.},
	month = may,
	year = {2018},
	keywords = {deep learning, fMRI, auditory cortex, convolutional neural network, deep neural network, encoding models, hierarchy, human auditory cortex, natural sounds, word recognition},
	pages = {630--644.e16},
	file = {ScienceDirect Snapshot:/Users/alexpsq/Zotero/storage/FZYWT5UQ/S0896627318302502.html:text/html;ScienceDirect Full Text PDF:/Users/alexpsq/Zotero/storage/VDEJ7GZG/Kell et al. - 2018 - A Task-Optimized Neural Network Replicates Human A.pdf:application/pdf;ScienceDirect Snapshot:/Users/alexpsq/Zotero/storage/MIM7P28B/S0896627318302502.html:text/html},
}

@article{yamins_performance-optimized_2014,
	title = {Performance-optimized hierarchical models predict neural responses in higher visual cortex},
	volume = {111},
	copyright = {©  . Freely available online through the PNAS open access option.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/111/23/8619},
	doi = {10.1073/pnas.1403112111},
	abstract = {The ventral visual stream underlies key human visual object recognition abilities. However, neural encoding in the higher areas of the ventral stream remains poorly understood. Here, we describe a modeling approach that yields a quantitatively accurate model of inferior temporal (IT) cortex, the highest ventral cortical area. Using high-throughput computational techniques, we discovered that, within a class of biologically plausible hierarchical neural network models, there is a strong correlation between a model’s categorization performance and its ability to predict individual IT neural unit response data. To pursue this idea, we then identified a high-performing neural network that matches human performance on a range of recognition tasks. Critically, even though we did not constrain this model to match neural data, its top output layer turns out to be highly predictive of IT spiking responses to complex naturalistic images at both the single site and population levels. Moreover, the model’s intermediate layers are highly predictive of neural responses in the V4 cortex, a midlevel visual area that provides the dominant cortical input to IT. These results show that performance optimization—applied in a biologically appropriate model class—can be used to build quantitative predictive models of neural processing.},
	language = {en},
	number = {23},
	urldate = {2020-12-18},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Yamins, Daniel L. K. and Hong, Ha and Cadieu, Charles F. and Solomon, Ethan A. and Seibert, Darren and DiCarlo, James J.},
	month = jun,
	year = {2014},
	pmid = {24812127},
	note = {Publisher: National Academy of Sciences
Section: Biological Sciences},
	keywords = {array electrophysiology, computational neuroscience, computer vision},
	pages = {8619--8624},
	file = {Full Text PDF:/Users/alexpsq/Zotero/storage/YCGQ7SRQ/Yamins et al. - 2014 - Performance-optimized hierarchical models predict .pdf:application/pdf;Snapshot:/Users/alexpsq/Zotero/storage/UXH9LFVE/8619.html:text/html},
}

@article{schrittwieser_mastering_2020,
	title = {Mastering {Atari}, {Go}, chess and shogi by planning with a learned model},
	volume = {588},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-03051-4},
	doi = {10.1038/s41586-020-03051-4},
	abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3—the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4—the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi—canonical environments for high-performance planning—the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game.},
	language = {en},
	number = {7839},
	urldate = {2021-01-01},
	journal = {Nature},
	author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
	month = dec,
	year = {2020},
	note = {Number: 7839
Publisher: Nature Publishing Group},
	pages = {604--609},
	file = {Full Text PDF:/Users/alexpsq/Zotero/storage/TZU3S3FN/Schrittwieser et al. - 2020 - Mastering Atari, Go, chess and shogi by planning w.pdf:application/pdf;Snapshot:/Users/alexpsq/Zotero/storage/ZRNR6FQ9/s41586-020-03051-4.html:text/html},
}

@article{grosenick_interpretable_2013,
	title = {Interpretable whole-brain prediction analysis with {GraphNet}},
	volume = {72},
	issn = {1095-9572},
	doi = {10.1016/j.neuroimage.2012.12.062},
	abstract = {Multivariate machine learning methods are increasingly used to analyze neuroimaging data, often replacing more traditional "mass univariate" techniques that fit data one voxel at a time. In the functional magnetic resonance imaging (fMRI) literature, this has led to broad application of "off-the-shelf" classification and regression methods. These generic approaches allow investigators to use ready-made algorithms to accurately decode perceptual, cognitive, or behavioral states from distributed patterns of neural activity. However, when applied to correlated whole-brain fMRI data these methods suffer from coefficient instability, are sensitive to outliers, and yield dense solutions that are hard to interpret without arbitrary thresholding. Here, we develop variants of the Graph-constrained Elastic-Net (GraphNet), a fast, whole-brain regression and classification method developed for spatially and temporally correlated data that automatically yields interpretable coefficient maps (Grosenick et al., 2009b). GraphNet methods yield sparse but structured solutions by combining structured graph constraints (based on knowledge about coefficient smoothness or connectivity) with a global sparsity-inducing prior that automatically selects important variables. Because GraphNet methods can efficiently fit regression or classification models to whole-brain, multiple time-point data sets and enhance classification accuracy relative to volume-of-interest (VOI) approaches, they eliminate the need for inherently biased VOI analyses and allow whole-brain fitting without the multiple comparison problems that plague mass univariate and roaming VOI ("searchlight") methods. As fMRI data are unlikely to be normally distributed, we (1) extend GraphNet to include robust loss functions that confer insensitivity to outliers, (2) equip them with "adaptive" penalties that asymptotically guarantee correct variable selection, and (3) develop a novel sparse structured Support Vector GraphNet classifier (SVGN). When applied to previously published data (Knutson et al., 2007), these efficient whole-brain methods significantly improved classification accuracy over previously reported VOI-based analyses on the same data (Grosenick et al., 2008; Knutson et al., 2007) while discovering task-related regions not documented in the original VOI approach. Critically, GraphNet estimates fit to the Knutson et al. (2007) data generalize well to out-of-sample data collected more than three years later on the same task but with different subjects and stimuli (Karmarkar et al., submitted for publication). By enabling robust and efficient selection of important voxels from whole-brain data taken over multiple time points ({\textgreater}100,000 "features"), these methods enable data-driven selection of brain areas that accurately predict single-trial behavior within and across individuals.},
	language = {eng},
	journal = {NeuroImage},
	author = {Grosenick, Logan and Klingenberg, Brad and Katovich, Kiefer and Knutson, Brian and Taylor, Jonathan E.},
	month = may,
	year = {2013},
	pmid = {23298747},
	keywords = {Brain Mapping, Humans, Magnetic Resonance Imaging, Algorithms, Artificial Intelligence, Brain, Image Interpretation, Computer-Assisted},
	pages = {304--321},
}

@article{schrittwieser_mastering_2020-1,
	title = {Mastering {Atari}, {Go}, chess and shogi by planning with a learned model},
	volume = {588},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-03051-4},
	doi = {10.1038/s41586-020-03051-4},
	abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3—the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4—the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi—canonical environments for high-performance planning—the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game.},
	language = {en},
	number = {7839},
	urldate = {2021-01-09},
	journal = {Nature},
	author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
	month = dec,
	year = {2020},
	note = {Number: 7839
Publisher: Nature Publishing Group},
	pages = {604--609},
	file = {Full Text PDF:/Users/alexpsq/Zotero/storage/HFMYYHS9/Schrittwieser et al. - 2020 - Mastering Atari, Go, chess and shogi by planning w.pdf:application/pdf;Snapshot:/Users/alexpsq/Zotero/storage/AXC668GV/s41586-020-03051-4.html:text/html},
}

@article{feder_graphs_nodate,
	title = {Graphs in {Quantum} {Information} {Theory}},
	language = {en},
	author = {Feder, David},
	pages = {19},
	file = {Feder - Graphs in Quantum Information Theory.pdf:/Users/alexpsq/Zotero/storage/KMLGV6FA/Feder - Graphs in Quantum Information Theory.pdf:application/pdf},
}

@article{zou_stylized_2020,
	title = {Stylized {Neural} {Painting}},
	url = {http://arxiv.org/abs/2011.08114},
	abstract = {This paper proposes an image-to-painting translation method that generates vivid and realistic painting artworks with controllable styles. Different from previous image-to-image translation methods that formulate the translation as pixel-wise prediction, we deal with such an artistic creation process in a vectorized environment and produce a sequence of physically meaningful stroke parameters that can be further used for rendering. Since a typical vector render is not differentiable, we design a novel neural renderer which imitates the behavior of the vector renderer and then frame the stroke prediction as a parameter searching process that maximizes the similarity between the input and the rendering output. We explored the zero-gradient problem on parameter searching and propose to solve this problem from an optimal transportation perspective. We also show that previous neural renderers have a parameter coupling problem and we re-design the rendering network with a rasterization network and a shading network that better handles the disentanglement of shape and color. Experiments show that the paintings generated by our method have a high degree of fidelity in both global appearance and local textures. Our method can be also jointly optimized with neural style transfer that further transfers visual style from other images. Our code and animated results are available at {\textbackslash}url\{https://jiupinjia.github.io/neuralpainter/\}.},
	urldate = {2021-01-09},
	journal = {arXiv:2011.08114 [cs]},
	author = {Zou, Zhengxia and Shi, Tianyang and Qiu, Shuang and Yuan, Yi and Shi, Zhenwei},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.08114},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/EXWYP283/Zou et al. - 2020 - Stylized Neural Painting.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/AIAR2UXN/2011.html:text/html},
}

@article{karras_training_2020,
	title = {Training {Generative} {Adversarial} {Networks} with {Limited} {Data}},
	url = {http://arxiv.org/abs/2006.06676},
	abstract = {Training generative adversarial networks (GAN) using too little data typically leads to discriminator overfitting, causing training to diverge. We propose an adaptive discriminator augmentation mechanism that significantly stabilizes training in limited data regimes. The approach does not require changes to loss functions or network architectures, and is applicable both when training from scratch and when fine-tuning an existing GAN on another dataset. We demonstrate, on several datasets, that good results are now possible using only a few thousand training images, often matching StyleGAN2 results with an order of magnitude fewer images. We expect this to open up new application domains for GANs. We also find that the widely used CIFAR-10 is, in fact, a limited data benchmark, and improve the record FID from 5.59 to 2.42.},
	urldate = {2021-01-09},
	journal = {arXiv:2006.06676 [cs, stat]},
	author = {Karras, Tero and Aittala, Miika and Hellsten, Janne and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.06676},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/P79ERKN9/Karras et al. - 2020 - Training Generative Adversarial Networks with Limi.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/NFI9W45T/2006.html:text/html},
}

@article{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2021-01-09},
	journal = {arXiv:2005.14165 [cs]},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv: 2005.14165},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/2S3A7CWR/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/WT4TMJ4X/2005.html:text/html},
}

@article{chen_generative_nodate,
	title = {Generative {Pretraining} from {Pixels}},
	abstract = {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we ﬁnd that a GPT-2 scale model learns strong image representations as measured by linear probing, ﬁne-tuning, and low-data classiﬁcation. On CIFAR-10, we achieve 96.3\% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0\% accuracy with full ﬁnetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0\% top-1 accuracy on a linear probe of our features.},
	language = {en},
	author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeff and Jun, Heewoo and Dhariwal, Prafulla and Luan, David and Sutskever, Ilya},
	pages = {12},
	file = {Chen et al. - Generative Pretraining from Pixels.pdf:/Users/alexpsq/Zotero/storage/G2TLH9B8/Chen et al. - Generative Pretraining from Pixels.pdf:application/pdf},
}

@inproceedings{ye_quantum-inspired_2020-1,
	address = {Glasgow, United Kingdom},
	title = {Quantum-{Inspired} {Evolutionary} {Algorithm} for {Convolutional} {Neural} {Networks} {Architecture} {Search}},
	isbn = {978-1-72816-929-3},
	url = {https://ieeexplore.ieee.org/document/9185727/},
	doi = {10.1109/CEC48606.2020.9185727},
	urldate = {2021-01-11},
	booktitle = {2020 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	publisher = {IEEE},
	author = {Ye, Weiliang and Liu, Ruijiao and Li, Yangyang and Jiao, Licheng},
	month = jul,
	year = {2020},
	pages = {1--8},
}

@article{pinho_subject-specific_nodate,
	title = {Subject-specific segregation of functional territories based on deep phenotyping},
	volume = {n/a},
	issn = {1097-0193},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hbm.25189},
	doi = {https://doi.org/10.1002/hbm.25189},
	abstract = {Functional magnetic resonance imaging (fMRI) has opened the possibility to investigate how brain activity is modulated by behavior. Most studies so far are bound to one single task, in which functional responses to a handful of contrasts are analyzed and reported as a group average brain map. Contrariwise, recent data-collection efforts have started to target a systematic spatial representation of multiple mental functions. In this paper, we leverage the Individual Brain Charting (IBC) dataset—a high-resolution task-fMRI dataset acquired in a fixed environment—in order to study the feasibility of individual mapping. First, we verify that the IBC brain maps reproduce those obtained from previous, large-scale datasets using the same tasks. Second, we confirm that the elementary spatial components, inferred across all tasks, are consistently mapped within and, to a lesser extent, across participants. Third, we demonstrate the relevance of the topographic information of the individual contrast maps, showing that contrasts from one task can be predicted by contrasts from other tasks. At last, we showcase the benefit of contrast accumulation for the fine functional characterization of brain regions within a prespecified network. To this end, we analyze the cognitive profile of functional territories pertaining to the language network and prove that these profiles generalize across participants.},
	language = {en},
	number = {n/a},
	urldate = {2021-01-11},
	journal = {Human Brain Mapping},
	author = {Pinho, Ana Luísa and Amadon, Alexis and Fabre, Murielle and Dohmatob, Elvis and Denghien, Isabelle and Torre, Juan Jesús and Ginisty, Chantal and Becuwe‐Desmidt, Séverine and Roger, Séverine and Laurier, Laurence and Joly‐Testault, Véronique and Médiouni‐Cloarec, Gaëlle and Doublé, Christine and Martins, Bernadette and Pinel, Philippe and Eger, Evelyn and Varoquaux, Gaël and Pallier, Christophe and Dehaene, Stanislas and Hertz‐Pannier, Lucie and Thirion, Bertrand},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hbm.25189},
	keywords = {atlases, brain imaging, cognitive function, data set, functional magnetic resonance imaging},
	file = {Snapshot:/Users/alexpsq/Zotero/storage/JDE4II85/hbm.html:text/html;Full Text PDF:/Users/alexpsq/Zotero/storage/VMPXQIY5/Pinho et al. - Subject-specific segregation of functional territo.pdf:application/pdf},
}

@article{la_rocca_revisiting_2021,
	title = {Revisiting {Functional} {Connectivity} for {Infraslow} {Scale}-{Free} {Brain} {Dynamics} {Using} {Complex} {Wavelets}},
	volume = {11},
	issn = {1664-042X},
	url = {https://www.frontiersin.org/articles/10.3389/fphys.2020.578537/full?&utm_source=Email_to_authors_&utm_medium=Email&utm_content=T1_11.5e1_author&utm_campaign=Email_publication&field=&journalName=Frontiers_in_Physiology&id=578537},
	doi = {10.3389/fphys.2020.578537},
	abstract = {The analysis of human brain functional networks is achieved by computing functional connectivity indices reflecting phase coupling and interactions between remote brain regions. In magneto- and electroencephalography, the most often used functional connectivity indices are constructed on Fourier-based cross spectral estimation applied to specific fast and band limited oscillatory regimes. Recently, infraslow arrhythmic fluctuations (below the 1Hz) were recognized as playing a leading role in spontaneous brain activity. The present work aims to define fractal connectivity, extending the assessment of functional connectivity to the infraslow arrhythmic or scale-free temporal dynamics of M/EEG-quantified brain activity. Instead of being based on Fourier analysis, new Imaginary Coherence and weighted Phase Lag indices are constructed from complex-wavelet representations. Their performance are, first, assessed on synthetic data, by means of Monte-Carlo simulations, and compared favorably against the classical Fourier-based indices. These new fractal connectivity indices are, second, applied to MEG data collected on 36 individuals, both at rest and during the learning of a visual motion discrimination task. They demonstrate a higher statistical sensitivity, compared to their Fourier counterparts, in capturing significant and relevant functional interactions in the infraslow regime, and modulations form rest to task. Notably, the consistent overall increase in fractal connectivity from rest to task, correlated with a change in temporal dynamics, as well as with improved performance in task completion, suggests that W-complex-wavelet weighted Phase Lag index is the sole index able to capture brain plasticity in the infraslow scale-free regime.},
	language = {English},
	urldate = {2021-01-11},
	journal = {Frontiers in Physiology},
	author = {La Rocca, Daria and Wendt, Herwig and van Wassenhove, Virginie and Ciuciu, Philippe and Abry, Patrice},
	year = {2021},
	note = {Publisher: Frontiers},
	keywords = {functional connectivity, Arrhythmic, complex-wavelet, Fractal connectivity, Human brain temporal dynamics, Infraslow, MEG data, phase coupling, scale-free},
	file = {Full Text PDF:/Users/alexpsq/Zotero/storage/N5WSLQEB/La Rocca et al. - 2021 - Revisiting Functional Connectivity for Infraslow S.pdf:application/pdf},
}

@article{richard_fast_2019-1,
	title = {Fast shared response model for {fMRI} data},
	url = {http://arxiv.org/abs/1909.12537},
	abstract = {The shared response model provides a simple but effective framework to analyse fMRI data of subjects exposed to naturalistic stimuli. However when the number of subjects or runs is large, fitting the model requires a large amount of memory and computational power, which limits its use in practice. In this work, we introduce the FastSRM algorithm that relies on an intermediate atlas-based representation. It provides considerable speed-up in time and memory usage, hence it allows easy and fast large-scale analysis of naturalistic-stimulus fMRI data. Using four different datasets, we show that our method matches the performance of the original SRM algorithm while being about 5x faster and 20x to 40x more memory efficient. Based on this contribution, we use FastSRM to predict age from movie watching data on the CamCAN sample. Besides delivering accurate predictions (mean absolute error of 7.5 years), FastSRM extracts topographic patterns that are predictive of age, demonstrating that brain activity during free perception reflects age.},
	urldate = {2021-01-27},
	journal = {arXiv:1909.12537 [cs, eess, q-bio]},
	author = {Richard, Hugo and Martin, Lucas and Pinho, Ana Luısa and Pillow, Jonathan and Thirion, Bertrand},
	month = dec,
	year = {2019},
	note = {arXiv: 1909.12537},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:/Users/alexpsq/Zotero/storage/WITLWDWY/Richard et al. - 2019 - Fast shared response model for fMRI data.pdf:application/pdf;arXiv.org Snapshot:/Users/alexpsq/Zotero/storage/FN2CJ6JG/1909.html:text/html},
}

@inproceedings{caucheteux:emnlp2021,
  TITLE = {{Model-based analysis of brain activity reveals the hierarchy of language in 305 subjects}},
  AUTHOR = {Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-R{\'e}mi},
  URL = {https://hal.archives-ouvertes.fr/hal-03361430},
  BOOKTITLE = {{EMNLP 2021 - Conference on Empirical Methods in Natural Language Processing}},
  ADDRESS = {Dominican Republic},
  YEAR = {2021},
  MONTH = Nov,
  PDF = {https://hal.archives-ouvertes.fr/hal-03361430/file/Narratives_Replication_Lerner_et_al.pdf},
  HAL_ID = {hal-03361430},
  HAL_VERSION = {v1},
}
@inproceedings{caucheteux:icml2021,
  TITLE = {{Disentangling Syntax and Semantics in the Brain with Deep Networks}},
  AUTHOR = {Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-Remi},
  URL = {https://hal.archives-ouvertes.fr/hal-03361421},
  BOOKTITLE = {{ICML 2021 - 38th International Conference on Machine Learning}},
  ADDRESS = {France},
  YEAR = {2021},
  MONTH = Jul,
  PDF = {https://hal.archives-ouvertes.fr/hal-03361421/file/Narratives_Syntax_semantics.pdf},
  HAL_ID = {hal-03361421},
  HAL_VERSION = {v1},
}
@inproceedings{toneva:neurips2020,
 author = {Toneva, Mariya and Stretcu, Otilia and Poczos, Barnabas and Wehbe, Leila and Mitchell, Tom M},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {5284--5295},
 publisher = {Curran Associates, Inc.},
 title = {Modeling Task Effects on Meaning Representation in the Brain via Zero-Shot MEG Prediction},
 url = {https://proceedings.neurips.cc/paper/2020/file/38a8e18d75e95ca619af8df0da1417f2-Paper.pdf},
 volume = {33},
 year = {2020}
}
@misc{toneva:neurips2019,
  abstract = {Neural networks models for NLP are typically implemented without the explicit
encoding of language rules and yet they are able to break one performance
record after another. This has generated a lot of research interest in
interpreting the representations learned by these networks. We propose here a
novel interpretation approach that relies on the only processing system we have
that does understand language: the human brain. We use brain imaging recordings
of subjects reading complex natural text to interpret word and sequence
embeddings from 4 recent NLP models - ELMo, USE, BERT and Transformer-XL. We
study how their representations differ across layer depth, context length, and
attention type. Our results reveal differences in the context-related
representations across these models. Further, in the transformer models, we
find an interaction between layer depth and context length, and between layer
depth and attention type. We finally hypothesize that altering BERT to better
align with brain recordings would enable it to also better understand language.
Probing the altered BERT using syntactic NLP tasks reveals that the model with
increased brain-alignment outperforms the original model. Cognitive
neuroscientists have already begun using NLP networks to study the brain, and
this work closes the loop to allow the interaction between NLP and cognitive
neuroscience to be a true cross-pollination.},
  added-at = {2021-06-05T16:26:58.000+0200},
  author = {Toneva, Mariya and Wehbe, Leila},
  biburl = {https://www.bibsonomy.org/bibtex/28d1d70a1fb9555e69fb5ad229df5d3bd/kvdberg},
  description = {[1905.11833v4] Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)},
  interhash = {adb6a065d70ca95109d93e7741cb5c70},
  intrahash = {8d1d70a1fb9555e69fb5ad229df5d3bd},
  keywords = {NLP attention},
  timestamp = {2021-06-05T17:05:25.000+0200},
  title = {Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)},
  url = {http://arxiv.org/abs/1905.11833},
  year = 2019
}
@article {Goldstein:2020,
	author = {Goldstein, Ariel and Zada, Zaid and Buchnik, Eliav and Schain, Mariano and Price, Amy and Aubrey, Bobbi and Nastase, Samuel A. and Feder, Amir and Emanuel, Dotan and Cohen, Alon and Jansen, Aren and Gazula, Harshvardhan and Choe, Gina and Rao, Aditi and Kim, Se Catherine and Casto, Colton and Fanda, Lora and Doyle, Werner and Friedman, Daniel and Dugan, Patricia and Melloni, Lucia and Reichart, Roi and Devore, Sasha and Flinker, Adeen and Hasenfratz, Liat and Levy, Omer and Hassidim, Avinatan and Brenner, Michael and Matias, Yossi and Norman, Kenneth A. and Devinsky, Orrin and Hasson, Uri},
	title = {Thinking ahead: spontaneous prediction in context as a keystone of language in humans and machines},
	elocation-id = {2020.12.02.403477},
	year = {2021},
	doi = {10.1101/2020.12.02.403477},
	publisher = {Cold Spring Harbor Laboratory},
	URL = {https://www.biorxiv.org/content/early/2021/09/30/2020.12.02.403477},
	eprint = {https://www.biorxiv.org/content/early/2021/09/30/2020.12.02.403477.full.pdf},
	journal = {bioRxiv}
}

@article{Elman:1990,
  added-at = {2009-06-26T15:25:19.000+0200},
  annote = {cited by Tani, Dynamical systems, representation of time in connectionist
	models, reread?},
  author = {Elman, Jeffrey L.},
  biburl = {https://www.bibsonomy.org/bibtex/27399042378ca115d0951674538ded4b8/butz},
  description = {diverse cognitive systems bib},
  interhash = {8770242c4eff4016d2408ab338039b7c},
  intrahash = {7399042378ca115d0951674538ded4b8},
  journal = {Cognitive Science},
  keywords = {imported},
  pages = {179-211},
  timestamp = {2009-06-26T15:25:28.000+0200},
  title = {Finding structure in time},
  volume = 14,
  year = 1990
}

﻿@Article{Saxe2021,
author={Saxe, Andrew
and Nelli, Stephanie
and Summerfield, Christopher},
title={If deep learning is the answer, what is the question?},
journal={Nature Reviews Neuroscience},
year={2021},
month={Jan},
day={01},
volume={22},
number={1},
pages={55-67},
abstract={Neuroscience research is undergoing a minor revolution. Recent advances in machine learning and artificial intelligence research have opened up new ways of thinking about neural computation. Many researchers are excited by the possibility that deep neural networks may offer theories of perception, cognition and action for biological brains. This approach has the potential to radically reshape our approach to understanding neural systems, because the computations performed by deep networks are learned from experience, and not endowed by the researcher. If so, how can neuroscientists use deep networks to model and understand biological brains? What is the outlook for neuroscientists who seek to characterize computations or neural codes, or who wish to understand perception, attention, memory and executive functions? In this Perspective, our goal is to offer a road map for systems neuroscience research in the age of deep learning. We discuss the conceptual and methodological challenges of comparing behaviour, learning dynamics and neural representations in artificial and biological systems, and we highlight new research questions that have emerged for neuroscience as a direct consequence of recent advances in machine learning.},
issn={1471-0048},
doi={10.1038/s41583-020-00395-8},
url={https://doi.org/10.1038/s41583-020-00395-8}
}

@inproceedings{Chen2015,
 author = {Chen, Po-Hsuan (Cameron) and Chen, Janice and Yeshurun, Yaara and Hasson, Uri and Haxby, James and Ramadge, Peter J},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Reduced-Dimension fMRI Shared Response Model},
 url = {https://proceedings.neurips.cc/paper/2015/file/b3967a0e938dc2a6340e258630febd5a-Paper.pdf},
 volume = {28},
 year = {2015}
}

@article {Caucheteux:2021c,
	author = {Caucheteux, Charlotte and King, Jean-R{\'e}mi},
	title = {Language processing in brains and deep neural networks: computational convergence and its limits},
	elocation-id = {2020.07.03.186288},
	year = {2021},
	doi = {10.1101/2020.07.03.186288},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Deep Learning has recently led to major advances in natural language processing. Do these models process sentences similarly to humans, and is this similarity driven by specific principles? Using a variety of artificial neural networks, trained on image classification, word embedding, or language modeling, we evaluate whether their architectural and functional properties lead them to generate activations linearly comparable to those of 102 human brains measured with functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG). We show that image, word and contextualized word embeddings separate the hierarchical levels of language processing in the brain. Critically, we compare 3,600 embeddings in their ability to linearly map onto these brain responses. The results show that (1) the position of the layer in the network and (2) the ability of the network to accurately predict words from context are the main factors responsible for the emergence of brain-like representations in artificial neural networks. Together, these results show how perceptual, lexical and compositional representations precisely unfold within each cortical region and contribute to uncovering the governing principles of language processing in brains and algorithms.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2021/01/14/2020.07.03.186288},
	eprint = {https://www.biorxiv.org/content/early/2021/01/14/2020.07.03.186288.full.pdf},
	journal = {bioRxiv}
}