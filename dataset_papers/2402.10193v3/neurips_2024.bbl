\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Beeching et~al.(2023)Beeching, Fourrier, Habib, Han, Lambert, Rajani, Sanseviero, Tunstall, and Wolf]{open-llm-leaderboard}
Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf.
\newblock Open llm leaderboard.
\newblock \url{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}, 2023.

\bibitem[Biderman et~al.(2024)Biderman, Ortiz, Portes, Paul, Greengard, Jennings, King, Havens, Chiley, Frankle, Blakeney, and Cunningham]{biderman2024lora}
Dan Biderman, Jose~Gonzalez Ortiz, Jacob Portes, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, Cody Blakeney, and John~P. Cunningham.
\newblock Lora learns less and forgets less, 2024.

\bibitem[Cai et~al.(2024)Cai, Li, Geng, Peng, Lee, Chen, and Dao]{cai2024medusa}
Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason~D. Lee, Deming Chen, and Tri Dao.
\newblock Medusa: Simple llm inference acceleration framework with multiple decoding heads.
\newblock \emph{arXiv preprint arXiv: 2401.10774}, 2024.

\bibitem[Chee et~al.(2023)Chee, Cai, Kuleshov, and De~Sa]{chee2023quip}
Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De~Sa.
\newblock Quip: 2-bit quantization of large language models with guarantees.
\newblock \emph{arXiv preprint arXiv:2307.13304}, 2023.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Borgeaud, Irving, Lespiau, Sifre, and Jumper]{chen2023accelerating}
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper.
\newblock Accelerating large language model decoding with speculative sampling.
\newblock February 2023{\natexlab{a}}.
\newblock \doi{10.48550/ARXIV.2302.01318}.

\bibitem[Chen et~al.(2022)Chen, Liu, Meng, and Liang]{chen2022revisiting}
Guanzheng Chen, Fangyu Liu, Zaiqiao Meng, and Shangsong Liang.
\newblock Revisiting parameter-efficient tuning: Are we really there yet?, 2022.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Ye, Wu, Zhuo, Ceze, and Krishnamurthy]{punica}
Lequn Chen, Zihao Ye, Yongji Wu, Danyang Zhuo, Luis Ceze, and Arvind Krishnamurthy.
\newblock Punica: Multi-tenant lora serving, 2023{\natexlab{b}}.

\bibitem[Chen et~al.(2023{\natexlab{c}})Chen, Wong, Chen, and Tian]{chen2023extending}
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian.
\newblock Extending context window of large language models via positional interpolation, 2023{\natexlab{c}}.

\bibitem[Chen et~al.(2023{\natexlab{d}})Chen, Qian, Tang, Lai, Liu, Han, and Jia]{chen2023longlora}
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.
\newblock Longlora: Efficient fine-tuning of long-context large language models, 2023{\natexlab{d}}.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang, Zhuang, Gonzalez, Stoica, and Xing]{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023.
\newblock URL \url{https://lmsys.org/blog/2023-03-30-vicuna/}.

\bibitem[Christiano et~al.(2023)Christiano, Leike, Brown, Martic, Legg, and Amodei]{christiano2023deep}
Paul Christiano, Jan Leike, Tom~B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.
\newblock Deep reinforcement learning from human preferences, 2023.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and Zettlemoyer]{dettmers2022llm}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock Llm. int8 (): 8-bit matrix multiplication for transformers at scale.
\newblock \emph{arXiv preprint arXiv:2208.07339}, 2022.

\bibitem[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{dettmers2023qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock \emph{arXiv preprint arXiv:2305.14314}, 2023.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186, 2019.

\bibitem[Ding et~al.(2023)Ding, Chen, Xu, Qin, Zheng, Hu, Liu, Sun, and Zhou]{ding2023enhancing}
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou.
\newblock Enhancing chat language models by scaling high-quality instructional conversations, 2023.

\bibitem[Frantar and Alistarh(2023)]{frantar2023sparsegpt}
Elias Frantar and Dan Alistarh.
\newblock Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023.

\bibitem[Frantar et~al.(2022)Frantar, Ashkboos, Hoefler, and Alistarh]{frantar2022gptq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock Gptq: Accurate post-training quantization for generative pre-trained transformers.
\newblock \emph{arXiv preprint arXiv:2210.17323}, 2022.

\bibitem[Gao et~al.(2023)Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster, Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds, Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and Zou]{eval-harness}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le~Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
\newblock A framework for few-shot language model evaluation, 12 2023.
\newblock URL \url{https://zenodo.org/records/10256836}.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han2015learning}
Song Han, Jeff Pool, John Tran, and William~J. Dally.
\newblock Learning both weights and connections for efficient neural networks, 2015.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{han2016deep}
Song Han, Huizi Mao, and William~J. Dally.
\newblock Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding, 2016.

\bibitem[Hartford(2023)]{Hartford2023}
Eric Hartford.
\newblock Cognitivecomputations/dolphin-2.2.1-mistral-7b, hugging face, 2023.
\newblock URL \url{https://huggingface.co/cognitivecomputations/dolphin-2.2.1-mistral-7b}.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone, De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019parameter}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In \emph{International Conference on Machine Learning}, pages 2790--2799. PMLR, 2019.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, and Chen]{hu2021lora}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{ICLR}, 2021.

\bibitem[Isik et~al.(2023)Isik, Kumbong, Ning, Yao, Koyejo, and Zhang]{isik2023gptzip}
Berivan Isik, Hermann Kumbong, Wanyi Ning, Xiaozhe Yao, Sanmi Koyejo, and Ce~Zhang.
\newblock {GPT}-zip: Deep compression of finetuned large language models.
\newblock In \emph{Workshop on Efficient Systems for Foundation Models @ ICML2023}, 2023.
\newblock URL \url{https://openreview.net/forum?id=hO0c2tG2xL}.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, de~las Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock, Scao, Lavril, Wang, Lacroix, and Sayed]{jiang2023mistral}
Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio~Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed.
\newblock Mistral 7b, 2023.

\bibitem[Jin et~al.(2023)Jin, Ren, Preotiuc-Pietro, and Cheng]{jin2023dataless}
Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng.
\newblock Dataless knowledge fusion by merging weights of language models, 2023.

\bibitem[Kim et~al.(2023)Kim, Hooper, Gholami, Dong, Li, Shen, Mahoney, and Keutzer]{kim2023squeezellm}
Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael~W Mahoney, and Kurt Keutzer.
\newblock Squeezellm: Dense-and-sparse quantization.
\newblock \emph{arXiv preprint arXiv:2306.07629}, 2023.

\bibitem[Kingma and Ba(2017)]{kingma2017adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization, 2017.

\bibitem[LeCun et~al.(1989)LeCun, Denker, and Solla]{lecun1989obd}
Yann LeCun, John Denker, and Sara Solla.
\newblock Optimal brain damage.
\newblock In D.~Touretzky, editor, \emph{Advances in Neural Information Processing Systems}, volume~2. Morgan-Kaufmann, 1989.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf}.

\bibitem[Leviathan et~al.(2022)Leviathan, Kalman, and Matias]{leviathan2022fast}
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
\newblock Fast inference from transformers via speculative decoding.
\newblock November 2022.
\newblock \doi{10.48550/ARXIV.2211.17192}.

\bibitem[Lin et~al.(2023)Lin, Tang, Tang, Yang, Dang, and Han]{lin2023awq}
Ji~Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.
\newblock Awq: Activation-aware weight quantization for llm compression and acceleration.
\newblock \emph{arXiv preprint arXiv:2306.00978}, 2023.

\bibitem[Lin et~al.(2022)Lin, Hilton, and Evans]{lin2022truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods, 2022.

\bibitem[Ma et~al.(2024)Ma, Wang, Ma, Wang, Wang, Huang, Dong, Wang, Xue, and Wei]{ma2024era}
Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li~Dong, Ruiping Wang, Jilong Xue, and Furu Wei.
\newblock The era of 1-bit llms: All large language models are in 1.58 bits, 2024.

\bibitem[Mishra et~al.(2021)Mishra, Latorre, Pool, Stosic, Stosic, Venkatesh, Yu, and Micikevicius]{mishra2021accelerating}
Asit Mishra, Jorge~Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius.
\newblock Accelerating sparse deep neural networks.
\newblock \emph{arXiv preprint arXiv: 2104.08378}, 2021.

\bibitem[Mukherjee et~al.(2023)Mukherjee, Mitra, Jawahar, Agarwal, Palangi, and Awadallah]{mukherjee2023orca}
Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah.
\newblock Orca: Progressive learning from complex explanation traces of gpt-4, 2023.

\bibitem[Niederfahrenhorst et~al.(2023)Niederfahrenhorst, Hakhamaneshi, and Ahmad]{Anyscale2023}
Artur Niederfahrenhorst, Kourosh Hakhamaneshi, and Rehaan Ahmad.
\newblock Fine-tuning llms: In-depth analysis with llama-2, Sep 2023.
\newblock URL \url{https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{arXiv preprint arXiv:2203.02155}, 2022.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi, Pezzelle, Baroni, Boleda, and Fernández]{paperno2016lambada}
Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan~Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández.
\newblock The lambada dataset: Word prediction requiring a broad discourse context, 2016.

\bibitem[Press et~al.(2022)Press, Smith, and Lewis]{press2022train}
Ofir Press, Noah~A. Smith, and Mike Lewis.
\newblock Train short, test long: Attention with linear biases enables input length extrapolation, 2022.

\bibitem[Qiu et~al.(2023)Qiu, Li, Sun, Peng, Shi, Zhang, Dong, Lam, Lo, Xiao, Yuan, Wang, Xu, and Lo]{healthinfo}
Jianing Qiu, Lin Li, Jiankai Sun, Jiachuan Peng, Peilun Shi, Ruiyang Zhang, Yinzhao Dong, Kyle Lam, Frank P.-W. Lo, Bo~Xiao, Wu~Yuan, Ningli Wang, Dong Xu, and Benny Lo.
\newblock Large ai models in health informatics: Applications, challenges, and the future.
\newblock \emph{IEEE Journal of Biomedical and Health Informatics}, 27\penalty0 (12):\penalty0 6074--6087, 2023.
\newblock \doi{10.1109/JBHI.2023.3316750}.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and Sutskever]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Raffel et~al.(2023)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2023exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer, 2023.

\bibitem[Rebuffi et~al.(2017)Rebuffi, Bilen, and Vedaldi]{rebuffi2017learning}
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
\newblock Learning multiple visual domains with residual adapters.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Ryu et~al.(2023)Ryu, Seo, and Yoo]{ryu2023efficient}
Simo Ryu, Seunghyun Seo, and Jaejun Yoo.
\newblock Efficient storage of fine-tuned models via low-rank approximation of weight residuals, 2023.

\bibitem[Sakaguchi et~al.(2019)Sakaguchi, Bras, Bhagavatula, and Choi]{sakaguchi2019winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale, 2019.

\bibitem[Sheng et~al.(2023)Sheng, Cao, Li, Hooper, Lee, Yang, Chou, Zhu, Zheng, Keutzer, Gonzalez, and Stoica]{sheng2023slora}
Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, Joseph~E. Gonzalez, and Ion Stoica.
\newblock S-lora: Serving thousands of concurrent lora adapters.
\newblock \emph{arXiv preprint arXiv:2311.03285}, 2023.

\bibitem[Suzgun et~al.(2022)Suzgun, Scales, Sch{\"a}rli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou, et~al.]{suzgun2022challenging}
Mirac Suzgun, Nathan Scales, Nathanael Sch{\"a}rli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, et~al.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve them.
\newblock \emph{arXiv preprint arXiv:2210.09261}, 2022.

\bibitem[Team.(2023)]{mosaicmpt}
MosaicML~NLP Team.
\newblock Introducing mpt-7b: A new standard for open-source, commercially usable llms., 2023.
\newblock URL \url{https://www.databricks.com/blog/mpt-7b}.

\bibitem[Team(2023)]{xwin-lm}
Xwin-LM Team.
\newblock Xwin-lm, 9 2023.
\newblock URL \url{https://github.com/Xwin-LM/Xwin-LM}.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Tseng et~al.(2024)Tseng, Chee, Sun, Kuleshov, and Sa]{tseng2024quip}
Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher~De Sa.
\newblock Quip\#: Even better llm quantization with hadamard incoherence and lattice codebooks, 2024.

\bibitem[Tunstall et~al.(2023)Tunstall, Beeching, Lambert, Rajani, Rasul, Belkada, Huang, von Werra, Fourrier, Habib, Sarrazin, Sanseviero, Rush, and Wolf]{tunstall2023zephyr}
Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander~M. Rush, and Thomas Wolf.
\newblock Zephyr: Direct distillation of lm alignment, 2023.

\bibitem[Upstage(2023)]{Solar70B}
Upstage.
\newblock Upstage/solar-0-70b-16bit · hugging face, 2023.
\newblock URL \url{https://huggingface.co/upstage/SOLAR-0-70b-16bit}.

\bibitem[Wang et~al.(2023)Wang, Cheng, Zhan, Li, Song, and Liu]{wang2023openchat}
Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu.
\newblock Openchat: Advancing open-source language models with mixed-quality data, 2023.

\bibitem[Wang et~al.(2024)Wang, Ma, Cao, Zhang, Xue, Shi, Zheng, Miao, Yang, Cao, Yang, and Yang]{ladder-osdi24}
Lei Wang, Lingxiao Ma, Shijie Cao, Quanlu Zhang, Jilong Xue, Yining Shi, Ningxin Zheng, Ziming Miao, Fan Yang, Ting Cao, Yuqing Yang, and Mao Yang.
\newblock Ladder: Enabling efficient low-precision deep learning computing through hardware-aware tensor transformation.
\newblock In \emph{18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)}, pages 307--323, Santa Clara, CA, July 2024. USENIX Association.
\newblock ISBN 978-1-939133-40-3.
\newblock URL \url{https://www.usenix.org/conference/osdi24/presentation/wang-lei}.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Gadre, Roelofs, Gontijo-Lopes, Morcos, Namkoong, Farhadi, Carmon, Kornblith, and Schmidt]{wortsman2022model}
Mitchell Wortsman, Gabriel Ilharco, Samir~Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari~S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt.
\newblock Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time, 2022.

\bibitem[Xiao et~al.(2023)Xiao, Lin, Seznec, Wu, Demouth, and Han]{xiao2023smoothquant}
Guangxuan Xiao, Ji~Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.
\newblock Smoothquant: Accurate and efficient post-training quantization for large language models.
\newblock In \emph{International Conference on Machine Learning}, pages 38087--38099. PMLR, 2023.

\bibitem[Xu et~al.(2024)Xu, Du, Niyato, Kang, Xiong, Mao, Han, Jamalipour, Kim, Shen, Leung, and Poor]{edgecloud}
Minrui Xu, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu Han, Abbas Jamalipour, Dong~In Kim, Xuemin Shen, Victor C.~M. Leung, and H.~Vincent Poor.
\newblock Unleashing the power of edge-cloud generative ai in mobile networks: A survey of aigc services.
\newblock \emph{IEEE Communications Surveys \& Tutorials}, pages 1--1, 2024.
\newblock \doi{10.1109/COMST.2024.3353265}.

\bibitem[Yadav et~al.(2023)Yadav, Choshen, Raffel, and Bansal]{yadav2023compeft}
Prateek Yadav, Leshem Choshen, Colin Raffel, and Mohit Bansal.
\newblock Compeft: Compression for communicating parameter efficient updates via sparsification and quantization, 2023.

\bibitem[Yao and Klimovic(2023)]{yao2023deltazip}
Xiaozhe Yao and Ana Klimovic.
\newblock Deltazip: Multi-tenant language model serving via delta compression.
\newblock \emph{arXiv preprint arXiv:2312.05215}, 2023.

\bibitem[Yu et~al.(2023)Yu, Yu, Yu, Huang, and Li]{yu2023language}
Le~Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li.
\newblock Language models are super mario: Absorbing abilities from homologous models as a free lunch.
\newblock \emph{arXiv preprint arXiv:2311.03099}, 2023.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?, 2019.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, Zhang, Gonzalez, and Stoica]{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric.~P Xing, Hao Zhang, Joseph~E. Gonzalez, and Ion Stoica.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

\bibitem[Zhu and Gupta(2017)]{zhu2017prune}
Michael Zhu and Suyog Gupta.
\newblock To prune, or not to prune: exploring the efficacy of pruning for model compression.
\newblock \emph{International Conference on Learning Representations}, 2017.

\end{thebibliography}
