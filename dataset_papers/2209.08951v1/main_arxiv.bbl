\newcommand{\etalchar}[1]{$^{#1}$}
\providecommand{\bysame}{\leavevmode\hbox to3em{\hrulefill}\thinspace}
\providecommand{\MR}{\relax\ifhmode\unskip\space\fi MR }
% \MRhref is called by the amsart/book/proc definition of \MR.
\providecommand{\MRhref}[2]{%
  \href{http://www.ams.org/mathscinet-getitem?mr=#1}{#2}
}
\providecommand{\href}[2]{#2}
\begin{thebibliography}{DSBDSS15}

\bibitem[AKL21]{amir21a}
Idan Amir, Tomer Koren, and Roi Livni, \emph{{SGD Generalizes Better Than GD
  (And Regularization Doesn't Help)}}, Conference on Learning Theory, 2021.

\bibitem[Ant05]{antos05}
Andr{\'a}s Antos, \emph{{Improved Minimax Bounds on the Test and Training
  Distortion of Empirically Designed Vector Quantizers}}, IEEE Transactions on
  Information Theory (2005).

\bibitem[BB94]{bottou94}
Leon Bottou and Yoshua Bengio, \emph{{Convergence Properties of the K-Means
  Algorithms}}, Advances in Neural Information Processing Systems, 1994.

\bibitem[BE02]{bousquet02}
Olivier Bousquet and Andr{\'e} Elisseeff, \emph{{Stability and
  Generalization}}, Journal of Machine Learning Research (2002).

\bibitem[BFGT20]{bassily20}
Raef Bassily, Vitaly Feldman, Crist{\'o}bal Guzm{\'a}n, and Kunal Talwar,
  \emph{Stability of stochastic gradient descent on nonsmooth convex losses},
  Advances in Neural Information Processing Systems, 2020.

\bibitem[BKZ20]{bousquet20}
Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy, \emph{{Sharper
  Bounds for Uniformly Stable Algorithms}}, Conference on Learning Theory,
  2020.

\bibitem[CDE{\etalchar{+}}21]{camuto21}
Alexander Camuto, George Deligiannidis, Murat~A. Erdogdu, Mert
  G{\"u}rb{\"u}zbalaban, Umut {\c{S}}im{\c{s}}ekli, and Lingjiong Zhu,
  \emph{{Fractal Structure and Generalization Properties of Stochastic
  Optimization Algorithms}}, Advances in Neural Information Processing Systems,
  2021.

\bibitem[CMR13]{cortes13}
Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh, \emph{{Multi-Class
  Classification with Maximum Margin Multiple Kernel}}, International
  Conference on Machine Learning, 2013.

\bibitem[DDB20]{dieuleveut20}
Aymeric Dieuleveut, Alain Durmus, and Francis Bach, \emph{{Bridging the Gap
  between Constant Step Size Stochastic Gradient Descent and Markov Chains}},
  The Annals of Statistics (2020).

\bibitem[DN21]{doikov21}
Nikita Doikov and Yurii Nesterov, \emph{Minimizing uniformly convex functions
  by cubic regularization of newton method}, Journal of Optimization Theory and
  Applications (2021).

\bibitem[DSBDSS15]{daniely15}
Amit Daniely, Sivan Sabato, Shai Ben-David, and Shai Shalev-Shwartz,
  \emph{{Multiclass Learnability and the ERM Principle}}, Journal of Machine
  Learning Research (2015).

\bibitem[EH21]{erdogdu2021convergence}
Murat~A Erdogdu and Rasa Hosseinzadeh, \emph{On the convergence of langevin
  monte carlo: The interplay between tail growth and smoothness}, Conference on
  Learning Theory, PMLR, 2021, pp.~1776--1822.

\bibitem[EMS18]{erdogdu2018global}
Murat~A Erdogdu, Lester Mackey, and Ohad Shamir, \emph{Global non-convex
  optimization with discretized diffusions}, Advances in Neural Information
  Processing Systems \textbf{31} (2018).

\bibitem[Fal14]{falconer14}
Kenneth Falconer, \emph{Fractal geometry: mathematical foundations and
  applications}, Wiley, 2014.

\bibitem[Fel16]{feldman16}
Vitaly Feldman, \emph{{Generalization of ERM in Stochastic Convex Optimization:
  The Dimension Strikes Back}}, Advances in Neural Information Processing
  Systems (2016).

\bibitem[FO21]{farnia21}
Farzan Farnia and Asuman Ozdaglar, \emph{Train simultaneously, generalize
  better: {S}tability of gradient-based minimax learners}, International
  Conference on Machine Learning, 2021.

\bibitem[FV19]{feldman19}
Vitaly Feldman and Jan Vondrak, \emph{High probability generalization bounds
  for uniformly stable algorithms with nearly optimal rate}, Conference on
  Learning Theory, 2019.

\bibitem[Gue02]{guermeur02}
Yann Guermeur, \emph{{Combining Discriminant Models with New Multi-Class
  SVMs}}, Pattern Analysis \& Applications (2002).

\bibitem[Hoe63]{hoeffding63}
Wassily Hoeffding, \emph{{Probability Inequalities for Sums of Bounded Random
  Variables}}, Journal of the American Statistical Association (1963).

\bibitem[HRS15]{hardt15}
Moritz Hardt, Benjamin Recht, and Yoram Singer, \emph{{Train faster, generalize
  better: Stability of stochastic gradient descent}}, arXiv preprint
  arXiv:1509.01240 (2015).

\bibitem[JKZ{\etalchar{+}}12]{jenssen12}
Robert Jenssen, Marius Kloft, Alexander Zien, S{\"o}ren Sonnenburg, and
  Klaus-Robert M{\"u}ller, \emph{{A Scatter-Based Prototype Framework and
  Multi-Class Extension of Support Vector Machines}}, PloS one (2012).

\bibitem[KWS22]{kozachkov22}
Leo Kozachkov, Patrick~M. Wensing, and Jean-Jacques Slotine,
  \emph{{Generalization in Supervised Learning Through Riemannian
  Contraction}}, arXiv preprint arXiv:2201.06656 (2022).

\bibitem[LDBK15]{lei15}
Yunwen Lei, {\"U}r{\"u}n Dogan, Alexander Binder, and Marius Kloft,
  \emph{{Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to
  Novel Algorithms}}, Advances in Neural Information Processing Systems, 2015.

\bibitem[LDZK19]{lei19}
Yunwen Lei, {\"U}r{\"u}n Dogan, Ding-Xuan Zhou, and Marius Kloft,
  \emph{Data-dependent generalization bounds for multi-class classification},
  IEEE Transactions on Information Theory (2019).

\bibitem[Lev13]{levrard13}
Cl{\'e}ment Levrard, \emph{Fast rates for empirical vector quantization},
  Electronic Journal of Statistics (2013).

\bibitem[LL21]{li21}
Shaojie Li and Yong Liu, \emph{{Sharper Generalization Bounds for Clustering}},
  International Conference on Machine Learning, 2021.

\bibitem[LLQ20]{li19}
Jian Li, Xuanyuan Luo, and Mingda Qiao, \emph{{On Generalization Error Bounds
  of Noisy Gradient Methods for Non-Convex Learning}}, International Conference
  on Learning Representations, 2020.

\bibitem[LLY21]{lei21b}
Yunwen Lei, Mingrui Liu, and Yiming Ying, \emph{{Generalization Guarantee of
  SGD for Pairwise Learning}}, International Conference on Artificial
  Intelligence and Statistics, 2021.

\bibitem[Lon17]{london17}
Ben London, \emph{{A PAC-Bayesian analysis of randomized learning with
  application to stochastic gradient descent}}, Advances in Neural Information
  Processing Systems (2017).

\bibitem[LY20]{lei20}
Yunwen Lei and Yiming Ying, \emph{{Fine-Grained Analysis of Stability and
  Generalization for Stochastic Gradient Descent}}, International Conference on
  Machine Learning, 2020.

\bibitem[Mac03]{mackay03}
David~J.C. MacKay, \emph{{Information Theory, Inference and Learning
  Algorithms}}, Cambridge University Press, 2003.

\bibitem[Mil19]{milne18}
Tristan Milne, \emph{{Piecewise Strong Convexity of Neural Networks}}, Advances
  in Neural Information Processing Systems, 2019.

\bibitem[Nes15]{nesterov15}
Yu~Nesterov, \emph{Universal gradient methods for convex optimization
  problems}, Mathematical Programming (2015).

\bibitem[PLYS21]{park21}
Sejun Park, Jaeho Lee, Chulhee Yun, and Jinwoo Shin, \emph{{Provable
  Memorization via Deep Neural Networks using Sub-linear Parameters}},
  Conference on Learning Theory, 2021.

\bibitem[Pri12]{prince12}
Simon~J.D. Prince, \emph{Computer vision: models, learning, and inference},
  Cambridge University Press, 2012.

\bibitem[RRT17]{raginsky17}
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky, \emph{{Non-convex
  learning via Stochastic Gradient Langevin Dynamics: a nonasymptotic
  analysis}}, Conference on Learning Theory, 2017.

\bibitem[Scu10]{sculley10}
David Sculley, \emph{{Web-Scale K-Means Clustering}}, International Conference
  on World Wide Web, 2010.

\bibitem[SHN{\etalchar{+}}18]{soudry18}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro, \emph{{The Implicit Bias of Gradient Descent on Separable Data}}, The
  Journal of Machine Learning Research (2018).

\bibitem[SSBD14]{shalev14}
Shai Shalev-Shwartz and Shai Ben-David, \emph{{Understanding Machine Learning:
  From Theory to Algorithms}}, Cambridge university press, 2014.

\bibitem[{\c{S}}SDE20]{csimcsekli20}
Umut {\c{S}}im{\c{s}}ekli, Ozan Sener, George Deligiannidis, and Murat~A.
  Erdogdu, \emph{{Hausdorff Dimension, Heavy Tails, and Generalization in
  Neural Networks}}, Advances in Neural Information Processing Systems, 2020.

\bibitem[SSSSS09]{shalev09}
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan,
  \emph{{Stochastic Convex Optimization}}, Conference on Learning Theory, 2009.

\bibitem[TM16]{tang16}
Cheng Tang and Claire Monteleoni, \emph{{On Lloydâ€™s Algorithm: New
  Theoretical Insights for Clustering in Practice}}, International Conference
  on Artificial Intelligence and Statistics, 2016.

\bibitem[TM17]{tang17}
\bysame, \emph{{Convergence Rate of Stochastic k-means}}, International
  Conference on Artificial Intelligence and Statistics, 2017.

\bibitem[TTJC15]{thorpe15}
Matthew Thorpe, Florian Theil, Adam~M Johansen, and Neil Cade,
  \emph{{Convergence of the k-Means Minimization Problem using
  $\Gamma$-Convergence}}, SIAM Journal on Applied Mathematics (2015).

\bibitem[Ver18]{vershynin18}
Roman Vershynin, \emph{{High-dimensional probability: An introduction with
  applications in data science}}, Cambridge University Press, 2018.

\bibitem[YBVE21]{yu21}
Lu~Yu, Krishnakumar Balasubramanian, Stanislav Volgushev, and Murat~A. Erdogdu,
  \emph{{An Analysis of Constant Step Size SGD in the Non-convex Regime:
  Asymptotic Normality and Bias}}, Advances in Neural Information Processing
  Systems, 2021.

\bibitem[YKM21]{yun21}
Chulhee Yun, Shankar Krishnan, and Hossein Mobahi, \emph{{A Unifying View on
  Implicit Bias in Training Linear Neural Networks}}, International Conference
  on Learning Representations, 2021.

\bibitem[ZBH{\etalchar{+}}21]{zhang21b}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals,
  \emph{Understanding deep learning (still) requires rethinking
  generalization}, Communications of the ACM (2021).

\bibitem[Zha04]{zhang04}
Tong Zhang, \emph{{Statistical Analysis of Some Multi-Category Large Margin
  Classification Methods}}, Journal of Machine Learning Research (2004).

\end{thebibliography}
