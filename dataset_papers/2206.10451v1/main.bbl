\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achille et~al.(2019)Achille, Rovere, and Soatto]{achille2019critical}
Achille, A., Rovere, M., and Soatto, S.
\newblock Critical learning periods in deep neural networks, 2019.

\bibitem[Akbik et~al.(2019)Akbik, Bergmann, Blythe, Rasul, Schweter, and
  Vollgraf]{akbik2019flair}
Akbik, A., Bergmann, T., Blythe, D., Rasul, K., Schweter, S., and Vollgraf, R.
\newblock Flair: An easy-to-use framework for state-of-the-art nlp.
\newblock In \emph{{NAACL} 2019, 2019 Annual Conference of the North American
  Chapter of the Association for Computational Linguistics (Demonstrations)},
  2019.

\bibitem[Bellec et~al.(2018)Bellec, Kappel, Maass, and
  Legenstein]{bellec2018deep}
Bellec, G., Kappel, D., Maass, W., and Legenstein, R.
\newblock Deep rewiring: Training very sparse deep networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{openaigym}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym, 2016.

\bibitem[Bulu\c{c} et~al.(2009)Bulu\c{c}, Fineman, Frigo, Gilbert, and
  Leiserson]{10.1145/1583991.1584053}
Bulu\c{c}, A., Fineman, J.~T., Frigo, M., Gilbert, J.~R., and Leiserson, C.~E.
\newblock Parallel sparse matrix-vector and matrix-transpose-vector
  multiplication using compressed sparse blocks.
\newblock In \emph{Proceedings of the Twenty-First Annual Symposium on
  Parallelism in Algorithms and Architectures}, SPAA '09, 2009.

\bibitem[Chen et~al.(2020)Chen, Frankle, Chang, Liu, Zhang, Wang, and
  Carbin]{chen2020lottery}
Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang, Z., and Carbin, M.
\newblock The lottery ticket hypothesis for pre-trained bert networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[de~Jorge et~al.(2020)de~Jorge, Sanyal, Behl, Torr, Rogez, and
  Dokania]{dejorge2020progressive}
de~Jorge, P., Sanyal, A., Behl, H.~S., Torr, P.~H., Rogez, G., and Dokania,
  P.~K.
\newblock Progressive skeletonization: Trimming more fat from a network at
  initialization.
\newblock \emph{arXiv preprint arXiv:2006.09081}, 2020.

\bibitem[de~Jorge et~al.(2021)de~Jorge, Sanyal, Behl, Torr, Rogez, and
  Dokania]{jorge2021progressive}
de~Jorge, P., Sanyal, A., Behl, H., Torr, P., Rogez, G., and Dokania, P.~K.
\newblock Progressive skeletonization: Trimming more fat from a network at
  initialization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{5206848}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE Conference on Computer Vision and Pattern
  Recognition}, 2009.

\bibitem[Dettmers \& Zettlemoyer(2019)Dettmers and
  Zettlemoyer]{dettmers2020sparse}
Dettmers, T. and Zettlemoyer, L.
\newblock Sparse networks from scratch: Faster training without losing
  performance.
\newblock \emph{arXiv preprint arXiv:1907.04840}, 2019.

\bibitem[{Ding} et~al.(2019){Ding}, {Ding}, {Guo}, and {Han}]{ding}
{Ding}, X., {Ding}, G., {Guo}, Y., and {Han}, J.
\newblock Centripetal sgd for pruning very deep convolutional networks with
  complicated structure.
\newblock In \emph{2019 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2019.

\bibitem[Evci et~al.(2020)Evci, Gale, Menick, Castro, and
  Elsen]{evci2020rigging}
Evci, U., Gale, T., Menick, J., Castro, P.~S., and Elsen, E.
\newblock Rigging the lottery: Making all tickets winners.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, Proceedings of Machine Learning Research, 2020.

\bibitem[Frankle \& Carbin(2019)Frankle and Carbin]{frankle2018the}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Frankle et~al.(2020{\natexlab{a}})Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020linear}
Frankle, J., Dziugaite, G.~K., Roy, D., and Carbin, M.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, Proceedings of Machine Learning Research, 2020{\natexlab{a}}.

\bibitem[Frankle et~al.(2020{\natexlab{b}})Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020pruning}
Frankle, J., Dziugaite, G.~K., Roy, D.~M., and Carbin, M.
\newblock Pruning neural networks at initialization: Why are we missing the
  mark?
\newblock \emph{arXiv preprint arXiv:2009.08576}, 2020{\natexlab{b}}.

\bibitem[Frankle et~al.(2020{\natexlab{c}})Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020stabilizing}
Frankle, J., Dziugaite, G.~K., Roy, D.~M., and Carbin, M.
\newblock Stabilizing the lottery ticket hypothesis, 2020{\natexlab{c}}.

\bibitem[Ghorbani et~al.(2020)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{ghorbani2020linearized}
Ghorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A.
\newblock Linearized two-layers neural networks in high dimension, 2020.

\bibitem[Goldblum et~al.(2020)Goldblum, Geiping, Schwarzschild, Moeller, and
  Goldstein]{goldblum2020truth}
Goldblum, M., Geiping, J., Schwarzschild, A., Moeller, M., and Goldstein, T.
\newblock Truth or backpropaganda? an empirical investigation of deep learning
  theory, 2020.

\bibitem[Gur-Ari et~al.(2018)Gur-Ari, Roberts, and Dyer]{gur-ari2019gradient}
Gur-Ari, G., Roberts, D.~A., and Dyer, E.
\newblock Gradient descent happens in a tiny subspace.
\newblock \emph{arXiv preprint arXiv:1812.04754}, 2018.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han}
Han, S., Pool, J., Tran, J., and Dally, W.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2015.

\bibitem[{Hassibi} et~al.(1993){Hassibi}, {Stork}, and {Wolff}]{hassibi}
{Hassibi}, B., {Stork}, D.~G., and {Wolff}, G.~J.
\newblock Optimal brain surgeon and general network pruning.
\newblock In \emph{IEEE International Conference on Neural Networks}, 1993.

\bibitem[He et~al.(2019)He, Liu, Wang, Hu, and Yang]{He_2019_CVPR}
He, Y., Liu, P., Wang, Z., Hu, Z., and Yang, Y.
\newblock Filter pruning via geometric median for deep convolutional neural
  networks acceleration.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, June 2019.

\bibitem[He et~al.(2020)He, Dong, Kang, Fu, Yan, and Yang]{8816678}
He, Y., Dong, X., Kang, G., Fu, Y., Yan, C., and Yang, Y.
\newblock Asymptotic soft filter pruning for deep convolutional neural
  networks.
\newblock \emph{IEEE Transactions on Cybernetics}, 50\penalty0 (8):\penalty0
  3594--3604, 2020.
\newblock \doi{10.1109/TCYB.2019.2933477}.

\bibitem[ichi Amari(2020)]{amari2020target}
ichi Amari, S.
\newblock Any target function exists in a neighborhood of any sufficiently wide
  random network: A geometrical perspective, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Hongler, and Gabriel]{jacot2018}
Jacot, A., Hongler, C., and Gabriel, F.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{adamoptimizer}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR (Poster)}, 2015.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and Hinton]{Krizhevsky09}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Master's thesis, Department of Computer Science, University of
  Toronto}, 2009.

\bibitem[Laina et~al.(2016)Laina, Rupprecht, Belagiannis, Tombari, and
  Navab]{laina2016deeper}
Laina, I., Rupprecht, C., Belagiannis, V., Tombari, F., and Navab, N.
\newblock Deeper depth prediction with fully convolutional residual networks.
\newblock In \emph{3D Vision (3DV), 2016 Fourth International Conference on}.
  IEEE, 2016.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{lecun}
LeCun, Y., Denker, J.~S., and Solla, S.~A.
\newblock Optimal brain damage.
\newblock In \emph{Advances in Neural Information Processing Systems 2}. 1990.

\bibitem[Lee et~al.(2019)Lee, Ajanthan, and Torr]{lee2018snip}
Lee, N., Ajanthan, T., and Torr, P.
\newblock {SNIP}: Single-shot network pruning based on connection sensitivity.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Li et~al.(2016)Li, Kadav, Durdanovic, Samet, and Graf]{li2016pruning}
Li, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H.~P.
\newblock Pruning filters for efficient convnets.
\newblock 2016.

\bibitem[Liu et~al.(2017)Liu, Li, Shen, Huang, Yan, and Zhang]{liu2017learning}
Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., and Zhang, C.
\newblock Learning efficient convolutional networks through network slimming,
  2017.

\bibitem[Louizos et~al.(2018)Louizos, Welling, and Kingma]{louizos2018learning}
Louizos, C., Welling, M., and Kingma, D.~P.
\newblock Learning sparse neural networks through $l_0$ regularization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Lubana \& Dick(2021)Lubana and
  Dick]{lubana2021gradient-flow-framework}
Lubana, E.~S. and Dick, R.
\newblock A gradient flow framework for analyzing network pruning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Lym et~al.(2019)Lym, Choukse, Zangeneh, Wen, Sanghavi, and
  Erez]{10.1145/3295500.3356156}
Lym, S., Choukse, E., Zangeneh, S., Wen, W., Sanghavi, S., and Erez, M.
\newblock Prunetrain: Fast neural network training by dynamic sparse model
  reconfiguration.
\newblock In \emph{Proceedings of the International Conference for High
  Performance Computing, Networking, Storage and Analysis}, SC '19, New York,
  NY, USA, 2019. Association for Computing Machinery.
\newblock ISBN 9781450362290.
\newblock \doi{10.1145/3295500.3356156}.
\newblock URL \url{https://doi.org/10.1145/3295500.3356156}.

\bibitem[Marcus et~al.(1993)Marcus, Marcinkiewicz, and Santorini]{PTBDATASET}
Marcus, M.~P., Marcinkiewicz, M.~A., and Santorini, B.
\newblock Building a large annotated corpus of english: The penn treebank.
\newblock \emph{Comput. Linguist.}, 1993.
\newblock ISSN 0891-2017.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and
  Socher]{Merity2017PointerSM}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer sentinel mixture models.
\newblock \emph{ArXiv}, 2017.

\bibitem[Mocanu et~al.(2018)Mocanu, Mocanu, Stone, Nguyen, Gibescu, and
  Liotta]{mocanu2018scalable}
Mocanu, D.~C., Mocanu, E., Stone, P., Nguyen, P.~H., Gibescu, M., and Liotta,
  A.
\newblock Scalable training of artificial neural networks with adaptive sparse
  connectivity inspired by network science.
\newblock \emph{Nature communications}, 2018.

\bibitem[Molchanov et~al.(2017)Molchanov, Tyree, Karras, Aila, and
  Kautz]{molchanov}
Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}, 2017.

\bibitem[Mostafa \& Wang(2019)Mostafa and Wang]{mostafa2019parameter}
Mostafa, H. and Wang, X.
\newblock Parameter efficient training of deep convolutional neural networks by
  dynamic sparse reparameterization.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, Proceedings of Machine Learning Research, 2019.

\bibitem[Nathan~Silberman \& Fergus(2012)Nathan~Silberman and
  Fergus]{Silberman:ECCV12}
Nathan~Silberman, Derek~Hoiem, P.~K. and Fergus, R.
\newblock Indoor segmentation and support inference from rgbd images.
\newblock In \emph{ECCV}, 2012.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{NEURIPS2019_9015}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems 32}. 2019.

\bibitem[Schmidt et~al.(2021)Schmidt, Goyal, Joshi, Feld, Conell, Laskaris,
  Blank, Wilson, Friedler, and Luccioni]{codecarbon}
Schmidt, V., Goyal, K., Joshi, A., Feld, B., Conell, L., Laskaris, N., Blank,
  D., Wilson, J., Friedler, S., and Luccioni, S.
\newblock {CodeCarbon: Estimate and Track Carbon Emissions from Machine
  Learning Computing}.
\newblock 2021.

\bibitem[Srinivas \& Babu(2016)Srinivas and Babu]{srinivas2016generalized}
Srinivas, S. and Babu, R.~V.
\newblock Generalized dropout.
\newblock \emph{arXiv preprint arXiv:1611.06791}, 2016.

\bibitem[Su et~al.(2020)Su, Chen, Cai, Wu, Gao, Wang, and
  Lee]{su2020sanitychecking}
Su, J., Chen, Y., Cai, T., Wu, T., Gao, R., Wang, L., and Lee, J.~D.
\newblock Sanity-checking pruning methods: Random tickets can win the jackpot.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Sun(2019)]{sun2019optimization}
Sun, R.
\newblock Optimization for deep learning: theory and algorithms, 2019.

\bibitem[Tanaka et~al.(2020)Tanaka, Kunin, Yamins, and
  Ganguli]{tanaka2020pruning}
Tanaka, H., Kunin, D., Yamins, D.~L., and Ganguli, S.
\newblock Pruning neural networks without any data by iteratively conserving
  synaptic flow.
\newblock \emph{arXiv preprint arXiv:2006.05467}, 2020.

\bibitem[Verdenius et~al.(2020)Verdenius, Stol, and
  Forr{\'e}]{verdenius2020pruning}
Verdenius, S., Stol, M., and Forr{\'e}, P.
\newblock Pruning via iterative ranking of sensitivity statistics.
\newblock \emph{arXiv preprint arXiv:2006.00896}, 2020.

\bibitem[Wang et~al.(2019)Wang, Grosse, Fidler, and Zhang]{wangchaoqi}
Wang, C., Grosse, R.~B., Fidler, S., and Zhang, G.
\newblock Eigendamage: Structured pruning in the kronecker-factored eigenbasis.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  Proceedings of Machine Learning Research, 2019.

\bibitem[Wang et~al.(2020)Wang, Zhang, and Grosse]{Wang2020Picking}
Wang, C., Zhang, G., and Grosse, R.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Woodworth et~al.(2020)Woodworth, Gunasekar, Lee, Moroshko, Savarese,
  Golan, Soudry, and Srebro]{woodworth2020regime}
Woodworth, B., Gunasekar, S., Lee, J.~D., Moroshko, E., Savarese, P., Golan,
  I., Soudry, D., and Srebro, N.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In \emph{Proceedings of Thirty Third Conference on Learning Theory},
  2020.

\bibitem[You et~al.(2020)You, Li, Xu, Fu, Wang, Chen, Baraniuk, Wang, and
  Lin]{You2020Drawing}
You, H., Li, C., Xu, P., Fu, Y., Wang, Y., Chen, X., Baraniuk, R.~G., Wang, Z.,
  and Lin, Y.
\newblock Drawing early-bird tickets: Toward more efficient training of deep
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[You et~al.(2019)You, Yan, Ye, Ma, and Wang]{you2019gate}
You, Z., Yan, K., Ye, J., Ma, M., and Wang, P.
\newblock Gate decorator: Global filter pruning method for accelerating deep
  convolutional neural networks, 2019.

\bibitem[Zhou et~al.(2021)Zhou, Ma, Zhu, Liu, Zhang, Yuan, Sun, and
  Li]{zhou_learning_2021}
Zhou, A., Ma, Y., Zhu, J., Liu, J., Zhang, Z., Yuan, K., Sun, W., and Li, H.
\newblock Learning {N}:{M} {Fine}-grained {Structured} {Sparse} {Neural}
  {Networks} {From} {Scratch}.
\newblock \emph{9th International Conference on Learning Representations,
  \{ICLR\} 2021}, pp.\ ~15, 2021.

\bibitem[Zhou et~al.(2019)Zhou, Lan, Liu, and Yosinski]{NEURIPS2019_1113d7a7}
Zhou, H., Lan, J., Liu, R., and Yosinski, J.
\newblock Deconstructing lottery tickets: Zeros, signs, and the supermask.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\end{thebibliography}
