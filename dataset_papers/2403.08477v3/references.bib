@STRING{NeurIPS = "Advances in Neural Information Processing Systems"}
@STRING{ICML = "International Conference on Machine Learning"}
@STRING{ICLR = "International Conference on Learning Representations"}
@STRING{CVPR = "IEEE Conference on Computer Vision and Pattern Recognition"}
@STRING{CVPRW = "IEEE Conference on Computer Vision and Pattern Recognition Workshops"}
@STRING{ICCV = "IEEE International Conference on Computer Vision"}
@STRING{ECCV = "European Conference on Computer Vision"}
@STRING{BMVC = "British Machine Vision Conference"}
@STRING{WACV = "IEEE Winter Conference on Applications of Computer Vision"}
@STRING{ACL = "Annual Conference of the Association for Computational Linguistics"}
@STRING{EMNLP = "Conference on Empirical Methods in Natural Language Processing"}
@STRING{NAACL = "Annual Conference of the North American Chapter of the Association for Computational Linguistics"}
@STRING{AAAI = "AAAI Conference on Artificial Intelligence"}
@STRING{IJCAI = "International Joint Conferences on Artificial Intelligence"}
@STRING{SIGKDD = "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"}
@STRING{WWW = "International World Wide Web Conferences"}
@STRING{CORL = "Conference on Robot Learning"}
@STRING{TMLR = "Transactions on Machine Learning Research"}
@STRING{JMLR = "Journal of Machine Learning Research"}
@STRING{ICASSP = "IEEE International Conference on Acoustics, Speech and Signal Processing"}


@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{
Triantafillou2020Meta-Dataset,
title={Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples},
author={Eleni Triantafillou and Tyler Zhu and Vincent Dumoulin and Pascal Lamblin and Utku Evci and Kelvin Xu and Ross Goroshin and Carles Gelada and Kevin Swersky and Pierre-Antoine Manzagol and Hugo Larochelle},
booktitle=ICLR,
year={2020},
urlll={https://openreview.net/forum?id=rkgAGAVKPr}
}

@inproceedings{doersch_crosstransformers_2020,
	title = {{CrossTransformers}: spatially-aware few-shot transfer},
	volume = {33},
	urlll = {https://proceedings.neurips.cc/paper_files/paper/2020/file/fa28c6cdf8dd6f41a657c3d7caa5c709-Paper.pdf},
	booktitle = NeurIPS,
	publisher = {Curran Associates, Inc.},
	author = {Doersch, Carl and Gupta, Ankush and Zisserman, Andrew},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {21981--21993},
}

 
@inproceedings{peng2019moment,
  title={Moment matching for multi-source domain adaptation},
  author={Peng, Xingchao and Bai, Qinxun and Xia, Xide and Huang, Zijun and Saenko, Kate and Wang, Bo},
  booktitle=ICCV,
  pages={1406--1415},
  year={2019}
}

@inproceedings{bragg2021flex,
      title={FLEX: Unifying Evaluation for Few-Shot NLP},
      author={Jonathan Bragg and Arman Cohan and Kyle Lo and Iz Beltagy},
      year={2021},
      booktitle=NeurIPS,
}

@inproceedings{hu2022pmf,
                author = {Hu, Shell Xu
                          and Li, Da
                          and St\"uhmer, Jan
                          and Kim, Minyoung
                          and Hospedales, Timothy M.},
                title = {Pushing the Limits of Simple Pipelines for Few-Shot Learning:
                         External Data and Fine-Tuning Make a Difference},
                booktitle =CVPR,
                year={2022}
            }

@inproceedings{
hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle=ICLR,
year={2022},
urlll={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@inproceedings{caron2021emerging,
  title={Emerging Properties in Self-Supervised Vision Transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J\'egou, Herv\'e  and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle=ICCV,
  year={2021}
}

@inproceedings{caccia2020online,
  title={Online fast adaptation and knowledge accumulation: a new approach to continual learning},
  author={Caccia, Massimo and Rodriguez, Pau and Ostapenko, Oleksiy and Normandin, Fabrice and Lin, Min and Caccia, Lucas and Laradji, Issam and Rish, Irina and Lacoste, Alexandre and Vazquez, David and others},
  booktitle=NeurIPS,
  year={2020}
}

@inproceedings{gupta2020look,
  title={Look-ahead meta learning for continual learning},
  author={Gupta, Gunshi and Yadav, Karmesh and Paull, Liam},
  booktitle=NeurIPS,
  year={2020}
}

@inproceedings{chen2023forgetting,
  title={Is forgetting less a good inductive bias for forward transfer?},
  author={Chen, Jiefeng and Nguyen, Timothy and Gorur, Dilan and Chaudhry, Arslan},
  booktitle=ICLR,
  year={2023}
}

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle=ICML,
  year={2017},
}

@inproceedings{perez2018film,
  title={Film: Visual reasoning with a general conditioning layer},
  author={Perez, Ethan and Strub, Florian and De Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  year={2018}
}

@article{raghu2019rapid,
  title={Rapid learning or feature reuse? towards understanding the effectiveness of maml},
  author={Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1909.09157},
  year={2019}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{oh2020boil,
  title={Boil: Towards representation change for few-shot learning},
  author={Oh, Jaehoon and Yoo, Hyungjun and Kim, ChangHwan and Yun, Se-Young},
  journal={arXiv preprint arXiv:2008.08882},
  year={2020}
}

@article{requeima2019fast,
  title={Fast and flexible multi-task classification using conditional neural adaptive processes},
  author={Requeima, James and Gordon, Jonathan and Bronskill, John and Nowozin, Sebastian and Turner, Richard E},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@inproceedings{triantafillou2021learning,
  title={Learning a universal template for few-shot dataset generalization},
  author={Triantafillou, Eleni and Larochelle, Hugo and Zemel, Richard and Dumoulin, Vincent},
  booktitle={International Conference on Machine Learning},
  pages={10424--10433},
  year={2021},
  organization={PMLR}
}

@article{dupont2022data,
  title={From data to functa: Your data point is a function and you can treat it like one},
  author={Dupont, Emilien and Kim, Hyunjik and Eslami, SM and Rezende, Danilo and Rosenbaum, Dan},
  journal={arXiv preprint arXiv:2201.12204},
  year={2022}
}

@article{shysheya2022fit,
  title={Fit: Parameter efficient few-shot transfer learning for personalized and federated image classification},
  author={Shysheya, Aliaksandra and Bronskill, John and Patacchiola, Massimiliano and Nowozin, Sebastian and Turner, Richard E},
  journal={arXiv preprint arXiv:2206.08671},
  year={2022}
}

@article{schwarz2022meta,
  title={Meta-learning sparse compression networks},
  author={Schwarz, Jonathan Richard and Teh, Yee Whye},
  journal={arXiv preprint arXiv:2205.08957},
  year={2022}
}

@article{lee2021meta,
  title={Meta-learning sparse implicit neural representations},
  author={Lee, Jaeho and Tack, Jihoon and Lee, Namhoon and Shin, Jinwoo},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={11769--11780},
  year={2021}
}

@article{schwarz2023modality,
  title={Modality-Agnostic Variational Compression of Implicit Neural Representations},
  author={Schwarz, Jonathan Richard and Tack, Jihoon and Teh, Yee Whye and Lee, Jaeho and Shin, Jinwoo},
  journal={arXiv preprint arXiv:2301.09479},
  year={2023}
}




@inproceedings{
louizos2018learning,
title={Learning Sparse Neural Networks through $L_0$ Regularization},
author={Christos Louizos and Max Welling and Diederik P. Kingma},
booktitle=ICLR,
year={2018},
urlll={https://openreview.net/forum?id=H1Y8hhg0b},
}

@article{von2021learning,
  title={Learning where to learn: Gradient sparsity in meta and continual learning},
  author={Von Oswald, Johannes and Zhao, Dominic and Kobayashi, Seijin and Schug, Simon and Caccia, Massimo and Zucchet, Nicolas and Sacramento, Jo{\~a}o},
  journal=NeurIPS,
  year={2021}
}

@article{he2019task,
  title={Task agnostic continual learning via meta learning},
  author={He, Xu and Sygnowski, Jakub and Galashov, Alexandre and Rusu, Andrei A and Teh, Yee Whye and Pascanu, Razvan},
  journal={arXiv preprint arXiv:1906.05201},
  year={2019}
}

@article{zeno2018task,
  title={Task agnostic continual learning using online variational bayes},
  author={Zeno, Chen and Golan, Itay and Hoffer, Elad and Soudry, Daniel},
  journal={arXiv preprint arXiv:1803.10123},
  year={2018}
}

@inproceedings{zintgraf2019fast,
  title={Caml: Fast context adaptation via meta-learning},
  author={Zintgraf, Luisa M and Shiarlis, Kyriacos and Kurin, Vitaly and Hofmann, Katja and Whiteson, Shimon},
  booktitle=ICML,
  year={2019}
}

@inproceedings{lee2018gradient,
  title={Gradient-based meta-learning with learned layerwise metric and subspace},
  author={Lee, Yoonho and Choi, Seungjin},
  booktitle=ICML,
  year={2018}
}

@inproceedings{shysheya2023fit,
  title={Fit: Parameter efficient few-shot transfer learning for personalized and federated image classification},
  author={Shysheya, Aliaksandra and Bronskill, John and Patacchiola, Massimiliano and Nowozin, Sebastian and Turner, Richard E},
  booktitle=ICLR,
  year={2023}
}

@misc{basustrong2023,
	title = {Strong {Baselines} for {Parameter} {Efficient} {Few}-{Shot} {Fine}-tuning},
	urlll = {http://arxiv.org/abs/2304.01917},
	doi = {10.48550/arXiv.2304.01917},
	abstract = {Few-shot classification (FSC) entails learning novel classes given only a few examples per class after a pre-training (or meta-training) phase on a set of base classes. Recent works have shown that simply fine-tuning a pre-trained Vision Transformer (ViT) on new test classes is a strong approach for FSC. Fine-tuning ViTs, however, is expensive in time, compute and storage. This has motivated the design of parameter efficient fine-tuning (PEFT) methods which fine-tune only a fraction of the Transformer's parameters. While these methods have shown promise, inconsistencies in experimental conditions make it difficult to disentangle their advantage from other experimental factors including the feature extractor architecture, pre-trained initialization and fine-tuning algorithm, amongst others. In our paper, we conduct a large-scale, experimentally consistent, empirical analysis to study PEFTs for few-shot image classification. Through a battery of over 1.8k controlled experiments on large-scale few-shot benchmarks including Meta-Dataset (MD) and ORBIT, we uncover novel insights on PEFTs that cast light on their efficacy in fine-tuning ViTs for few-shot classification. Through our controlled empirical study, we have two main findings: (i) Fine-tuning just the LayerNorm parameters (which we call LN-Tune) during few-shot adaptation is an extremely strong baseline across ViTs pre-trained with both self-supervised and supervised objectives, (ii) For self-supervised ViTs, we find that simply learning a set of scaling parameters for each attention matrix (which we call AttnScale) along with a domain-residual adapter (DRA) module leads to state-of-the-art performance (while being \${\textbackslash}sim{\textbackslash}!\$ 9\${\textbackslash}times\$ more parameter-efficient) on MD. Our extensive empirical findings set strong baselines and call for rethinking the current design of PEFT methods for FSC.},
	urllldate = {2024-01-08},
	publisher = {arXiv},
	author = {Basu, Samyadeep and Massiceti, Daniela and Hu, Shell Xu and Feizi, Soheil},
	month = apr,
	year = {2023},
	note = {arXiv:2304.01917 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:H\:\\ZoteroStorage\\storage\\8CQLYRCL\\Basu et al. - 2023 - Strong Baselines for Parameter Efficient Few-Shot .pdf:application/pdf},
}


@inproceedings{
    chen2023secure,
    title={Secure Out-of-Distribution Task Generalization with Energy-Based Models},
    author={Shengzhuang Chen and Long-Kai Huang and Jonathan Richard Schwarz and Yilun Du and Ying Wei},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    urlll={https://openreview.net/forum?id=tt7bQnTdRm}
}

@inproceedings{li2022TaskSpecificAdapter,
    author    = {Li, Wei-Hong and Liu, Xialei and Bilen, Hakan},
    title     = {Cross-domain Few-shot Learning with Task-specific Adapters},
    booktitle = {IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022}
}

@inproceedings{
jang2017categorical,
title={Categorical Reparameterization with Gumbel-Softmax},
author={Eric Jang and Shixiang Gu and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2017},
urlll={https://openreview.net/forum?id=rkE3y85ee}
}

@inproceedings{
flennerhag2022bootstrapped,
title={Bootstrapped Meta-Learning},
author={Sebastian Flennerhag and Yannick Schroecker and Tom Zahavy and Hado van Hasselt and David Silver and Satinder Singh},
booktitle={International Conference on Learning Representations},
year={2022},
urlll={https://openreview.net/forum?id=b-ny3x071E5}
}

@inproceedings{
tack2022selfimproving,
title={Meta-Learning with Self-Improving Momentum Target},
author={Jihoon Tack and Jongjin Park and Hankook Lee and Jaeho Lee and Jinwoo Shin},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
urlll={https://openreview.net/forum?id=FCNMbF_TsKm}
}

@inproceedings{
gallego-posada2022controlled,
title={Controlled Sparsity via Constrained Optimization or: How I Learned to Stop Tuning Penalties and Love Constraints},
author={Jose Gallego-Posada and Juan Ramirez and Akram Erraqabi and Yoshua Bengio and Simon Lacoste-Julien},
booktitle={Thirty-Sixth Conference on Neural Information Processing Systems},
year={2022},
urlll={https://openreview.net/forum?id=XUvSYc6TqDF}
}

@inproceedings{snell_prototypical_2017,
	title = {Prototypical {Networks} for {Few}-shot {Learning}},
	volume = {30},
	urlll = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html},
	abstract = {We propose Prototypical Networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend Prototypical Networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.},
	urllldate = {2024-01-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Snell, Jake and Swersky, Kevin and Zemel, Richard},
	year = {2017},
	file = {Full Text PDF:H\:\\ZoteroStorage\\storage\\2H8APPUR\\Snell et al. - 2017 - Prototypical Networks for Few-shot Learning.pdf:application/pdf},
}

@inproceedings{
shazeer2017,
title={ Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
author={Noam Shazeer and *Azalia Mirhoseini and *Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
booktitle={International Conference on Learning Representations},
year={2017},
urlll={https://openreview.net/forum?id=B1ckMDqlg}
}

@inproceedings{
lepikhin2021gshard,
title={{\{}GS{\}}hard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
author={Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen},
booktitle={International Conference on Learning Representations},
year={2021},
urlll={https://openreview.net/forum?id=qrwe7XHTmYb}
}

@inproceedings{Riquelme2021ScalingVW,
  title={Scaling Vision with Sparse Mixture of Experts},
  author={Carlos Riquelme and Joan Puigcerver and Basil Mustafa and Maxim Neumann and Rodolphe Jenatton and Andr{\'e} Susano Pinto and Daniel Keysers and Neil Houlsby},
  booktitle={Neural Information Processing Systems},
  year={2021},
  urlll={https://api.semanticscholar.org/CorpusID:235417196}
}

@article{Muqeeth2023SoftMO,
  title={Soft Merging of Experts with Adaptive Routing},
  author={Mohammed Muqeeth and Haokun Liu and Colin Raffel},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.03745},
  urlll={https://api.semanticscholar.org/CorpusID:259088823}
}

@inproceedings{Roller2021HashLF,
  title={Hash Layers For Large Sparse Models},
  author={Stephen Roller and Sainbayar Sukhbaatar and Arthur Szlam and Jason Weston},
  booktitle={Neural Information Processing Systems},
  year={2021},
  urlll={https://api.semanticscholar.org/CorpusID:235367626}
}

@article{Puigcerver2023FromST,
  title={From Sparse to Soft Mixtures of Experts},
  author={Joan Puigcerver and Carlos Riquelme and Basil Mustafa and Neil Houlsby},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.00951},
  urlll={https://api.semanticscholar.org/CorpusID:260378993}
}

@inproceedings{
zuo2022taming,
title={Taming Sparsely Activated Transformer with Stochastic Experts},
author={Simiao Zuo and Xiaodong Liu and Jian Jiao and Young Jin Kim and Hany Hassan and Ruofei Zhang and Jianfeng Gao and Tuo Zhao},
booktitle={International Conference on Learning Representations},
year={2022},
urlll={https://openreview.net/forum?id=B72HXs80q4}
}

@inproceedings{
zhou2022mixtureofexperts,
title={Mixture-of-Experts with Expert Choice Routing},
author={Yanqi Zhou and Tao Lei and Hanxiao Liu and Nan Du and Yanping Huang and Vincent Y Zhao and Andrew M. Dai and Zhifeng Chen and Quoc V Le and James Laudon},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
urlll={https://openreview.net/forum?id=jdJo1HIVinI}
}

@inproceedings{
mustafa2022multimodal,
title={Multimodal Contrastive Learning with {LIM}oE: the Language-Image Mixture of Experts},
author={Basil Mustafa and Carlos Riquelme Ruiz and Joan Puigcerver and Rodolphe Jenatton and Neil Houlsby},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
urlll={https://openreview.net/forum?id=Qy1D9JyMBg0}
}

@inproceedings{dai-etal-2022-stablemoe,
    title = "{S}table{M}o{E}: Stable Routing Strategy for Mixture of Experts",
    author = "Dai, Damai  and
      Dong, Li  and
      Ma, Shuming  and
      Zheng, Bo  and
      Sui, Zhifang  and
      Chang, Baobao  and
      Wei, Furu",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    urlll = "https://aclanthology.org/2022.acl-long.489",
    doi = "10.18653/v1/2022.acl-long.489",
    pages = "7085--7095",
    abstract = "The Mixture-of-Experts (MoE) technique can scale up the model size of Transformers with an affordable computational overhead. We point out that existing learning-to-route MoE methods suffer from the routing fluctuation issue, i.e., the target expert of the same input may change along with training, but only one expert will be activated for the input during inference. The routing fluctuation tends to harm sample efficiency because the same input updates different experts but only one is finally used. In this paper, we propose StableMoE with two training stages to address the routing fluctuation problem. In the first training stage, we learn a balanced and cohesive routing strategy and distill it into a lightweight router decoupled from the backbone model. In the second training stage, we utilize the distilled router to determine the token-to-expert assignment and freeze it for a stable routing strategy. We validate our method on language modeling and multilingual machine translation. The results show that StableMoE outperforms existing MoE methods in terms of both convergence speed and performance.",
}

@inproceedings{Lewis2021BASELS,
  title={BASE Layers: Simplifying Training of Large, Sparse Models},
  author={Mike Lewis and Shruti Bhosale and Tim Dettmers and Naman Goyal and Luke Zettlemoyer},
  booktitle={International Conference on Machine Learning},
  year={2021},
  urlll={https://api.semanticscholar.org/CorpusID:232428341}
}


@inproceedings{
liu2023sparsityconstrained,
title={Sparsity-Constrained Optimal Transport},
author={Tianlin Liu and Joan Puigcerver and Mathieu Blondel},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
urlll={https://openreview.net/forum?id=yHY9NbQJ5BP}
}

@article{Fedus2021SwitchTS,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={William Fedus and Barret Zoph and Noam M. Shazeer},
  journal={J. Mach. Learn. Res.},
  year={2021},
  volume={23},
  pages={120:1-120:39},
  urlll={https://api.semanticscholar.org/CorpusID:231573431}
}

@inproceedings{
grant2018recasting,
title={Recasting Gradient-Based Meta-Learning as Hierarchical Bayes},
author={Erin Grant and Chelsea Finn and Sergey Levine and Trevor Darrell and Thomas Griffiths},
booktitle={International Conference on Learning Representations},
year={2018},
urlll={https://openreview.net/forum?id=BJ_UL-k0b},
}

@article{Li2017MetaSGDLT,
  title={Meta-SGD: Learning to Learn Quickly for Few Shot Learning},
  author={Zhenguo Li and Fengwei Zhou and Fei Chen and Hang Li},
  journal={ArXiv},
  year={2017},
  volume={abs/1707.09835},
  urlll={https://api.semanticscholar.org/CorpusID:25316837}
}

@article{Hospedales2020MetaLearningIN,
  title={Meta-Learning in Neural Networks: A Survey},
  author={Timothy M. Hospedales and Antreas Antoniou and Paul Micaelli and Amos J. Storkey},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2020},
  volume={44},
  pages={5149-5169},
  urlll={https://api.semanticscholar.org/CorpusID:215744839}
}

@article{Bateni2019ImprovedFV,
  title={Improved Few-Shot Visual Classification},
  author={Peyman Bateni and Raghav Goyal and Vaden Masrani and Frank D. Wood and Leonid Sigal},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={14481-14490},
  urlll={https://api.semanticscholar.org/CorpusID:208910869}
}

@INPROCEEDINGS{triM,
  author={Liu, Yanbin and Lee, Juho and Zhu, Linchao and Chen, Ling and Shi, Humphrey and Yang, Yi},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={A Multi-Mode Modulator for Multi-Domain Few-Shot Classification}, 
  year={2021},
  volume={},
  number={},
  pages={8433-8442},
  keywords={Training;Extrapolation;Computer vision;Correlation;Computational modeling;Modulation;Information sharing;Transfer/Low-shot/Semi/Unsupervised Learning},
  doi={10.1109/ICCV48922.2021.00834}}


@article{Wei2021FinetunedLM,
  title={Finetuned Language Models Are Zero-Shot Learners},
  author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.01652},
  urlll={https://api.semanticscholar.org/CorpusID:237416585}
}

@inproceedings{Zhong2021AdaptingLM,
  title={Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections},
  author={Ruiqi Zhong and Kristy Lee and Zheng Zhang and Dan Klein},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2021},
  urlll={https://api.semanticscholar.org/CorpusID:237304362}
}

@inproceedings{Gao2021MakingPL,
  title={Making Pre-trained Language Models Better Few-shot Learners},
  author={Tianyu Gao and Adam Fisch and Danqi Chen},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2021},
  urlll={https://api.semanticscholar.org/CorpusID:229923710}
}

@article{Min2021MetaICLLT,
  title={MetaICL: Learning to Learn In Context},
  author={Sewon Min and Mike Lewis and Luke Zettlemoyer and Hannaneh Hajishirzi},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.15943},
  urlll={https://api.semanticscholar.org/CorpusID:240288835}
}

@article{Chen2021MetalearningVL,
  title={Meta-learning via Language Model In-context Tuning},
  author={Yanda Chen and Ruiqi Zhong and Sheng Zha and George Karypis and He He},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.07814},
  urlll={https://api.semanticscholar.org/CorpusID:239009828}
}

@article{Wortsman2021RobustFO,
  title={Robust fine-tuning of zero-shot models},
  author={Mitchell Wortsman and Gabriel Ilharco and Mike Li and Jong Wook Kim and Hannaneh Hajishirzi and Ali Farhadi and Hongseok Namkoong and Ludwig Schmidt},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={7949-7961},
  urlll={https://api.semanticscholar.org/CorpusID:237420687}
}

@inproceedings{Panigrahi2023TaskSpecificSL,
  title={Task-Specific Skill Localization in Fine-tuned Language Models},
  author={Abhishek Panigrahi and Nikunj Saunshi and Haoyu Zhao and Sanjeev Arora},
  booktitle={International Conference on Machine Learning},
  year={2023},
  urlll={https://api.semanticscholar.org/CorpusID:256826987}
}

@article{Ilharco2022PatchingOM,
  title={Patching open-vocabulary models by interpolating weights},
  author={Gabriel Ilharco and Mitchell Wortsman and Samir Yitzhak Gadre and Shuran Song and Hannaneh Hajishirzi and Simon Kornblith and Ali Farhadi and Ludwig Schmidt},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.05592},
  urlll={https://api.semanticscholar.org/CorpusID:251493208}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{chen2022understanding,
  title={Understanding benign overfitting in gradient-based meta learning},
  author={Chen, Lisha and Lu, Songtao and Chen, Tianyi},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={19887--19899},
  year={2022}
}

@article{yao2021meta,
  title={Meta-learning with an adaptive task scheduler},
  author={Yao, Huaxiu and Wang, Yu and Wei, Ying and Zhao, Peilin and Mahdavi, Mehrdad and Lian, Defu and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={7497--7509},
  year={2021}
}

@book{thrun2012learning,
  title={Learning to learn},
  author={Thrun, Sebastian and Pratt, Lorien},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{panigrahi2023task,
  title={Task-Specific Skill Localization in Fine-tuned Language Models},
  author={Panigrahi, Abhishek and Saunshi, Nikunj and Zhao, Haoyu and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2302.06600},
  year={2023}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{yu2023scaling,
  title={Scaling autoregressive multi-modal models: Pretraining and instruction tuning},
  author={Yu, Lili and Shi, Bowen and Pasunuru, Ramakanth and Muller, Benjamin and Golovneva, Olga and Wang, Tianlu and Babu, Arun and Tang, Binh and Karrer, Brian and Sheynin, Shelly and others},
  journal={arXiv preprint arXiv:2309.02591},
  year={2023}
}

@article{liu2023towards,
  title={Towards graph foundation models: A survey and beyond},
  author={Liu, Jiawei and Yang, Cheng and Lu, Zhiyuan and Chen, Junze and Li, Yibo and Zhang, Mengmei and Bai, Ting and Fang, Yuan and Sun, Lichao and Yu, Philip S and others},
  journal={arXiv preprint arXiv:2310.11829},
  year={2023}
}

@inproceedings{yeh2023toward,
  title={Toward a foundation model for time series data},
  author={Yeh, Chin-Chia Michael and Dai, Xin and Chen, Huiyuan and Zheng, Yan and Fan, Yujie and Der, Audrey and Lai, Vivian and Zhuang, Zhongfang and Wang, Junpeng and Wang, Liang and others},
  booktitle={Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
  pages={4400--4404},
  year={2023}
}

@article{hong2023spectralgpt,
  title={Spectralgpt: Spectral foundation model},
  author={Hong, Danfeng and Zhang, Bing and Li, Xuyang and Li, Yuxuan and Li, Chenyu and Yao, Jing and Yokoya, Naoto and Li, Hao and Jia, Xiuping and Plaza, Antonio and others},
  journal={arXiv preprint arXiv:2311.07113},
  year={2023}
}

@article{zhuang2020comprehensive,
  title={A comprehensive survey on transfer learning},
  author={Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  journal={Proceedings of the IEEE},
  volume={109},
  number={1},
  pages={43--76},
  year={2020},
  publisher={IEEE}
}

@article{zhang2023instruction,
  title={Instruction tuning for large language models: A survey},
  author={Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and others},
  journal={arXiv preprint arXiv:2308.10792},
  year={2023}
}

@article{lin2023evolutionary,
  title={Evolutionary-scale prediction of atomic-level protein structure with a language model},
  author={Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Smetanin, Nikita and Verkuil, Robert and Kabeli, Ori and Shmueli, Yaniv and others},
  journal={Science},
  volume={379},
  number={6637},
  pages={1123--1130},
  year={2023},
  publisher={American Association for the Advancement of Science}
}

@article{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{Dosovitskiy2020AnII,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.11929},
  urlll={https://api.semanticscholar.org/CorpusID:225039882}
}

@inproceedings{
ilharco2023editing,
title={Editing models with task arithmetic},
author={Gabriel Ilharco and Marco Tulio Ribeiro and Mitchell Wortsman and Ludwig Schmidt and Hannaneh Hajishirzi and Ali Farhadi},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
urlll={https://openreview.net/forum?id=6t0Kwf8-jrj}
}

@article{Matena2021MergingMW,
  title={Merging Models with Fisher-Weighted Averaging},
  author={Michael Matena and Colin Raffel},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.09832},
  urlll={https://api.semanticscholar.org/CorpusID:244345933}
}

@article{Triantafillou2021LearningAU,
  title={Learning a Universal Template for Few-shot Dataset Generalization},
  author={Eleni Triantafillou and H. Larochelle and Richard S. Zemel and Vincent Dumoulin},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.07029},
  urlll={https://api.semanticscholar.org/CorpusID:234741836}
}

@article{Lee2022SurgicalFI,
  title={Surgical Fine-Tuning Improves Adaptation to Distribution Shifts},
  author={Yoonho Lee and Annie S. Chen and Fahim Tajwar and Ananya Kumar and Huaxiu Yao and Percy Liang and Chelsea Finn},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.11466},
  urlll={https://api.semanticscholar.org/CorpusID:253018859}
}

@article{tack2024learning,
  title={Learning Large-scale Neural Fields via Context Pruned Meta-Learning},
  author={Tack, Jihoon and Kim, Subin and Yu, Sihyun and Lee, Jaeho and Shin, Jinwoo and Schwarz, Jonathan Richard},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{Yu2020GradientSF,
  title={Gradient Surgery for Multi-Task Learning},
  author={Tianhe Yu and Saurabh Kumar and Abhishek Gupta and Sergey Levine and Karol Hausman and Chelsea Finn},
  journal={ArXiv},
  year={2020},
  volume={abs/2001.06782},
  urlll={https://api.semanticscholar.org/CorpusID:210839011}
}

@article{Wang2020GradientVI,
  title={Gradient Vaccine: Investigating and Improving Multi-task Optimization in Massively Multilingual Models},
  author={Zirui Wang and Yulia Tsvetkov and Orhan Firat and Yuan Cao},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.05874},
  urlll={https://api.semanticscholar.org/CorpusID:222291168}
}


@inproceedings{
eustratiadis2024neural,
title={Neural Fine-Tuning Search for Few-Shot Learning},
author={Panagiotis Eustratiadis and {\L}ukasz Dudziak and Da Li and Timothy Hospedales},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
urlll={https://openreview.net/forum?id=T7YV5UZKBc}
}