\begin{thebibliography}{10}

\bibitem{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock {\em arXiv preprint arXiv:1409.0473}, 2014.

\bibitem{bakhtin2019real}
Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc'Aurelio Ranzato, and
  Arthur Szlam.
\newblock Real or fake? learning to discriminate machine from human generated
  text.
\newblock {\em arXiv preprint arXiv:1906.03351}, 2019.

\bibitem{bojar2016results}
Ond{\v{r}}ej Bojar, Yvette Graham, Amir Kamran, and Milo{\v{s}} Stanojevi{\'c}.
\newblock Results of the wmt16 metrics shared task.
\newblock In {\em Proceedings of the First Conference on Machine Translation:
  Volume 2, Shared Task Papers}, pages 199--231, 2016.

\bibitem{bondielli2019survey}
Alessandro Bondielli and Francesco Marcelloni.
\newblock A survey on fake news and rumour detection techniques.
\newblock {\em Information Sciences}, 497:38--55, 2019.

\bibitem{brown2020language}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
  Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter,
  Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
  Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
  Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners, 2020.

\bibitem{cettolo2014report}
Mauro Cettolo, Jan Niehues, Sebastian St{\"u}ker, Luisa Bentivogli, and
  Marcello Federico.
\newblock Report on the 11th iwslt evaluation campaign, iwslt 2014.
\newblock In {\em Proceedings of the International Workshop on Spoken Language
  Translation, Hanoi, Vietnam}, volume~57, 2014.

\bibitem{charniak2005coarse}
Eugene Charniak and Mark Johnson.
\newblock Coarse-to-fine n-best parsing and maxent discriminative reranking.
\newblock In {\em Proceedings of the 43rd annual meeting on association for
  computational linguistics}, pages 173--180. Association for Computational
  Linguistics, 2005.

\bibitem{charniak2006multilevel}
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph Austerweil, David Ellis,
  Isaac Haxton, Catherine Hill, R~Shrivaths, Jeremy Moore, Michael Pozar,
  et~al.
\newblock Multilevel coarse-to-fine pcfg parsing.
\newblock In {\em Proceedings of the main conference on Human Language
  Technology Conference of the North American Chapter of the Association of
  Computational Linguistics}, pages 168--175. Association for Computational
  Linguistics, 2006.

\bibitem{faris2017partisanship}
Robert Faris, Hal Roberts, Bruce Etling, Nikki Bourassa, Ethan Zuckerman, and
  Yochai Benkler.
\newblock Partisanship, propaganda, and disinformation: Online media and the
  2016 us presidential election.
\newblock {\em Berkman Klein Center Research Publication}, 6, 2017.

\bibitem{gehrmann2019gltr}
Sebastian Gehrmann, Hendrik Strobelt, and Alexander~M Rush.
\newblock Gltr: Statistical detection and visualization of generated text.
\newblock {\em arXiv preprint arXiv:1906.04043}, 2019.

\bibitem{ghazvininejad2019constant}
Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer.
\newblock Constant-time machine translation with conditional masked language
  models.
\newblock {\em arXiv preprint arXiv:1904.09324}, 2019.

\bibitem{ghazvininejad2020semi}
Marjan Ghazvininejad, Omer Levy, and Luke Zettlemoyer.
\newblock Semi-autoregressive training improves mask-predict decoding.
\newblock {\em arXiv preprint arXiv:2001.08785}, 2020.

\bibitem{gu2017non}
Jiatao Gu, James Bradbury, Caiming Xiong, Victor~OK Li, and Richard Socher.
\newblock Non-autoregressive neural machine translation.
\newblock {\em arXiv preprint arXiv:1711.02281}, 2017.

\bibitem{gu2019insertion}
Jiatao Gu, Qi~Liu, and Kyunghyun Cho.
\newblock Insertion-based decoding with automatically inferred generation
  order.
\newblock {\em Transactions of the Association for Computational Linguistics},
  7:661--676, 2019.

\bibitem{gu2019levenshtein}
Jiatao Gu, Changhan Wang, and Junbo Zhao.
\newblock Levenshtein transformer.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  11179--11189, 2019.

\bibitem{guo2019non}
Junliang Guo, Xu~Tan, Di~He, Tao Qin, Linli Xu, and Tie-Yan Liu.
\newblock Non-autoregressive neural machine translation with enhanced decoder
  input.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 3723--3730, 2019.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem{kim2016sequence}
Yoon Kim and Alexander~M Rush.
\newblock Sequence-level knowledge distillation.
\newblock {\em arXiv preprint arXiv:1606.07947}, 2016.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{Kingma2014}
Diederik~P. Kingma and Max Welling.
\newblock {A}uto-{E}ncoding {V}ariational {B}ayes.
\newblock In {\em Proceedings of ICLR}, 2014.

\bibitem{kryscinski2019evaluating}
Wojciech Kry{\'s}ci{\'n}ski, Bryan McCann, Caiming Xiong, and Richard Socher.
\newblock Evaluating the factual consistency of abstractive text summarization.
\newblock {\em arXiv}, pages arXiv--1910, 2019.

\bibitem{kudo2018sentencepiece}
Taku Kudo and John Richardson.
\newblock Sentencepiece: A simple and language independent subword tokenizer
  and detokenizer for neural text processing.
\newblock {\em arXiv preprint arXiv:1808.06226}, 2018.

\bibitem{lafferty2001conditional}
John Lafferty, Andrew McCallum, and Fernando~CN Pereira.
\newblock Conditional random fields: Probabilistic models for segmenting and
  labeling sequence data.
\newblock 2001.

\bibitem{lee2018deterministic}
Jason Lee, Elman Mansimov, and Kyunghyun Cho.
\newblock Deterministic non-autoregressive neural sequence modeling by
  iterative refinement.
\newblock {\em arXiv preprint arXiv:1802.06901}, 2018.

\bibitem{libovicky2018end}
Jind{\v{r}}ich Libovick{\`y} and Jind{\v{r}}ich Helcl.
\newblock End-to-end non-autoregressive neural machine translation with
  connectionist temporal classification.
\newblock {\em arXiv preprint arXiv:1811.04719}, 2018.

\bibitem{luong2015effective}
Minh-Thang Luong, Hieu Pham, and Christopher~D Manning.
\newblock Effective approaches to attention-based neural machine translation.
\newblock {\em arXiv preprint arXiv:1508.04025}, 2015.

\bibitem{ma2019flowseq}
Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neubig, and Eduard Hovy.
\newblock Flowseq: Non-autoregressive conditional sequence generation with
  generative flow.
\newblock {\em arXiv preprint arXiv:1909.02480}, 2019.

\bibitem{machavcek2014results}
Matou{\v{s}} Mach{\'a}{\v{c}}ek and Ond{\v{r}}ej Bojar.
\newblock Results of the wmt14 metrics shared task.
\newblock In {\em Proceedings of the Ninth Workshop on Statistical Machine
  Translation}, pages 293--301, 2014.

\bibitem{mansimov2019generalized}
Elman Mansimov, Alex Wang, and Kyunghyun Cho.
\newblock A generalized framework of sequence generation with application to
  undirected sequence models.
\newblock {\em arXiv preprint arXiv:1905.12790}, 2019.

\bibitem{Mnih2014}
Andriy Mnih and Karol Gregor.
\newblock {N}eural {V}ariational {I}nference and {L}earning in {B}elief
  {N}etworks.
\newblock In {\em Proceedings of ICML}, 2014.

\bibitem{muller2019does}
Rafael M{\"u}ller, Simon Kornblith, and Geoffrey~E Hinton.
\newblock When does label smoothing help?
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4696--4705, 2019.

\bibitem{nissan2017digital}
Ephraim Nissan.
\newblock Digital technologies and artificial intelligenceâ€™s present and
  foreseeable impact on lawyering, judging, policing and law enforcement.
\newblock {\em Ai \& Society}, 32(3):441--464, 2017.

\bibitem{ott2019fairseq}
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
  David Grangier, and Michael Auli.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock {\em arXiv preprint arXiv:1904.01038}, 2019.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8024--8035, 2019.

\bibitem{press2016using}
Ofir Press and Lior Wolf.
\newblock Using the output embedding to improve language models.
\newblock {\em arXiv preprint arXiv:1608.05859}, 2016.

\bibitem{rehmcracking}
Georg Rehm.
\newblock Cracking the language barrier for a multilingual europe.

\bibitem{Rezende2014}
Danilo~Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
\newblock {S}tochastic {B}ackpropagation and {A}pproximate {I}nference in
  {D}eep {G}enerative {M}odels.
\newblock In {\em Proceedings of ICML}, 2014.

\bibitem{rush2020torch}
Alexander~M Rush.
\newblock Torch-struct: Deep structured prediction library.
\newblock {\em arXiv preprint arXiv:2002.00876}, 2020.

\bibitem{rush2015neural}
Alexander~M Rush, Sumit Chopra, and Jason Weston.
\newblock A neural attention model for abstractive sentence summarization.
\newblock {\em arXiv preprint arXiv:1509.00685}, 2015.

\bibitem{rush2012vine}
Alexander~M Rush and Slav Petrov.
\newblock Vine pruning for efficient multi-pass dependency parsing.
\newblock In {\em Proceedings of the 2012 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 498--507. Association for Computational Linguistics,
  2012.

\bibitem{saharia2020non}
Chitwan Saharia, William Chan, Saurabh Saxena, and Mohammad Norouzi.
\newblock Non-autoregressive machine translation with latent alignments.
\newblock {\em arXiv preprint arXiv:2004.07437}, 2020.

\bibitem{sarkka2019temporal}
Simo S{\"a}rkk{\"a} and {\'A}ngel~F Garc{\'\i}a-Fern{\'a}ndez.
\newblock Temporal parallelization of bayesian filters and smoothers.
\newblock {\em arXiv preprint arXiv:1905.13002}, 2019.

\bibitem{schuster2019we}
Tal Schuster, Roei Schuster, Darsh~J Shah, and Regina Barzilay.
\newblock Are we safe yet? the limitations of distributional features for fake
  news detection.
\newblock {\em arXiv preprint arXiv:1908.09805}, 2019.

\bibitem{see2017get}
Abigail See, Peter~J Liu, and Christopher~D Manning.
\newblock Get to the point: Summarization with pointer-generator networks.
\newblock {\em arXiv preprint arXiv:1704.04368}, 2017.

\bibitem{sennrich2015neural}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock {\em arXiv preprint arXiv:1508.07909}, 2015.

\bibitem{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
  and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using
  gpu model parallelism.
\newblock {\em arXiv preprint arXiv:1909.08053}, 2019.

\bibitem{solaiman2019release}
Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss,
  Jeff Wu, Alec Radford, and Jasmine Wang.
\newblock Release strategies and the social impacts of language models.
\newblock {\em arXiv preprint arXiv:1908.09203}, 2019.

\bibitem{stern2019insertion}
Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit.
\newblock Insertion transformer: Flexible sequence generation via insertion
  operations.
\newblock {\em arXiv preprint arXiv:1902.03249}, 2019.

\bibitem{stern2018blockwise}
Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit.
\newblock Blockwise parallel decoding for deep autoregressive models.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10086--10095, 2018.

\bibitem{sun2019fast}
Zhiqing Sun, Zhuohan Li, Haoqing Wang, Di~He, Zi~Lin, and Zhihong Deng.
\newblock Fast structured decoding for sequence models.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3011--3020, 2019.

\bibitem{sundermeyer2012lstm}
Martin Sundermeyer, Ralf Schl{\"u}ter, and Hermann Ney.
\newblock Lstm neural networks for language modeling.
\newblock In {\em Thirteenth annual conference of the international speech
  communication association}, 2012.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem{wang2018semi}
Chunqi Wang, Ji~Zhang, and Haiqing Chen.
\newblock Semi-autoregressive neural machine translation.
\newblock {\em arXiv preprint arXiv:1808.08583}, 2018.

\bibitem{wang2019non}
Yiren Wang, Fei Tian, Di~He, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu.
\newblock Non-autoregressive machine translation with auxiliary regularization.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 5377--5384, 2019.

\bibitem{wardle2017information}
Claire Wardle and Hossein Derakhshan.
\newblock Information disorder: Toward an interdisciplinary framework for
  research and policy making.
\newblock {\em Council of Europe report}, 27, 2017.

\bibitem{wei2019imitation}
Bingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang Lin, and Xu~Sun.
\newblock Imitation learning for non-autoregressive neural machine translation.
\newblock {\em arXiv preprint arXiv:1906.02041}, 2019.

\bibitem{weiss2010structured}
David Weiss and Benjamin Taskar.
\newblock Structured prediction cascades.
\newblock In {\em Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, pages 916--923, 2010.

\bibitem{welleck2019neural}
Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and
  Jason Weston.
\newblock Neural text generation with unlikelihood training.
\newblock {\em arXiv preprint arXiv:1908.04319}, 2019.

\bibitem{wiseman2017challenges}
Sam Wiseman, Stuart~M Shieber, and Alexander~M Rush.
\newblock Challenges in data-to-document generation.
\newblock {\em arXiv preprint arXiv:1707.08052}, 2017.

\bibitem{xu2015show}
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan
  Salakhudinov, Rich Zemel, and Yoshua Bengio.
\newblock Show, attend and tell: Neural image caption generation with visual
  attention.
\newblock In {\em International conference on machine learning}, pages
  2048--2057, 2015.

\bibitem{zellers2019defending}
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi,
  Franziska Roesner, and Yejin Choi.
\newblock Defending against neural fake news.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9051--9062, 2019.

\bibitem{zhang2018speeding}
Wen Zhang, Liang Huang, Yang Feng, Lei Shen, and Qun Liu.
\newblock Speeding up neural machine translation decoding by cube pruning.
\newblock {\em arXiv preprint arXiv:1809.02992}, 2018.

\bibitem{zhang2020pointer}
Yizhe Zhang, Guoyin Wang, Chunyuan Li, Zhe Gan, Chris Brockett, and Bill Dolan.
\newblock Pointer: Constrained text generation via insertion-based generative
  pre-training.
\newblock {\em arXiv preprint arXiv:2005.00558}, 2020.

\bibitem{zhou2019understanding}
Chunting Zhou, Graham Neubig, and Jiatao Gu.
\newblock Understanding knowledge distillation in non-autoregressive machine
  translation.
\newblock {\em arXiv preprint arXiv:1911.02727}, 2019.

\bibitem{zhou2020improving}
Jiawei Zhou and Phillip Keung.
\newblock Improving non-autoregressive neural machine translation with
  monolingual data.
\newblock {\em arXiv preprint arXiv:2005.00932}, 2020.

\bibitem{ziegler2019latent}
Zachary~M Ziegler and Alexander~M Rush.
\newblock Latent normalizing flows for discrete sequences.
\newblock {\em arXiv preprint arXiv:1901.10548}, 2019.

\end{thebibliography}
