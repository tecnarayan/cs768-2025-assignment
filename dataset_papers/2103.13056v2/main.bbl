\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, Freund, and
  Schapire]{auer2002nonstochastic}
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert~E Schapire.
\newblock The nonstochastic multiarmed bandit problem.
\newblock \emph{SIAM journal on computing}, 32\penalty0 (1):\penalty0 48--77,
  2002.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 263--272. JMLR. org, 2017.

\bibitem[Bertsekas and Tsitsiklis(1991)]{bertsekas1991analysis}
Dimitri~P Bertsekas and John~N Tsitsiklis.
\newblock An analysis of stochastic shortest path problems.
\newblock \emph{Mathematics of Operations Research}, 16\penalty0 (3):\penalty0
  580--595, 1991.

\bibitem[Cai et~al.(2020)Cai, Yang, Jin, and Wang]{cai2019provably}
Qi~Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang.
\newblock Provably efficient exploration in policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  1283--1294. PMLR, 2020.

\bibitem[Chen and Luo(2021)]{chen2021finding}
Liyu Chen and Haipeng Luo.
\newblock Finding the stochastic shortest path with low regret: The adversarial
  cost and unknown transition case.
\newblock \emph{arXiv preprint arXiv:2102.05284}, 2021.

\bibitem[Chen et~al.(2020)Chen, Luo, and Wei]{chen2020minimax}
Liyu Chen, Haipeng Luo, and Chen-Yu Wei.
\newblock Minimax regret for stochastic shortest path with adversarial costs
  and known transition.
\newblock \emph{arXiv preprint arXiv:2012.04053}, 2020.

\bibitem[Dann et~al.(2019)Dann, Li, Wei, and Brunskill]{dann2019policy}
Christoph Dann, Lihong Li, Wei Wei, and Emma Brunskill.
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1507--1516. PMLR, 2019.

\bibitem[Efroni et~al.(2019)Efroni, Merlis, Ghavamzadeh, and
  Mannor]{efroni2019tight}
Yonathan Efroni, Nadav Merlis, Mohammad Ghavamzadeh, and Shie Mannor.
\newblock Tight regret bounds for model-based reinforcement learning with
  greedy policies.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
  d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett, editors, \emph{Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019,
  Vancouver, BC, Canada}, pages 12203--12213, 2019.

\bibitem[Efroni et~al.(2020)Efroni, Merlis, and
  Mannor]{efroni2020reinforcement}
Yonathan Efroni, Nadav Merlis, and Shie Mannor.
\newblock Reinforcement learning with trajectory feedback.
\newblock \emph{arXiv preprint arXiv:2008.06036}, 2020.

\bibitem[Efroni et~al.(2021)Efroni, Merlis, Saha, and
  Mannor]{efroni2021confidence}
Yonathan Efroni, Nadav Merlis, Aadirupa Saha, and Shie Mannor.
\newblock Confidence-budget matching for sequential budgeted learning.
\newblock \emph{arXiv preprint arXiv:2102.03400}, 2021.

\bibitem[Fruit et~al.(2018)Fruit, Pirotta, Lazaric, and
  Ortner]{fruit2018efficient}
Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Ronald Ortner.
\newblock Efficient bias-span-constrained exploration-exploitation in
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1802.04020}, 2018.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (4), 2010.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is q-learning provably efficient?
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4863--4873, 2018.

\bibitem[Jin et~al.(2020{\natexlab{a}})Jin, Jin, Luo, Sra, and
  Yu]{jin2019learning}
Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, and Tiancheng Yu.
\newblock Learning adversarial markov decision processes with bandit feedback
  and unknown transition.
\newblock In \emph{International Conference on Machine Learning}, pages
  4860--4869. PMLR, 2020{\natexlab{a}}.

\bibitem[Jin et~al.(2020{\natexlab{b}})Jin, Yang, Wang, and
  Jordan]{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pages 2137--2143,
  2020{\natexlab{b}}.

\bibitem[Jin and Luo(2020)]{jin2020simultaneously}
Tiancheng Jin and Haipeng Luo.
\newblock Simultaneously learning stochastic and adversarial episodic mdps with
  known transition.
\newblock \emph{Advances in neural information processing systems}, 2020.

\bibitem[Lancewicki et~al.(2020)Lancewicki, Rosenberg, and
  Mansour]{lancewicki2020learning}
Tal Lancewicki, Aviv Rosenberg, and Yishay Mansour.
\newblock Learning adversarial markov decision processes with delayed feedback.
\newblock \emph{arXiv preprint arXiv:2012.14843}, 2020.

\bibitem[Lee et~al.(2020)Lee, Luo, Wei, and Zhang]{lee2020bias}
Chung-Wei Lee, Haipeng Luo, Chen-Yu Wei, and Mengxiao Zhang.
\newblock Bias no more: high-probability data-dependent regret bounds for
  adversarial bandits and mdps.
\newblock \emph{Advances in neural information processing systems}, 2020.

\bibitem[Neu et~al.(2010)Neu, Gy{\"{o}}rgy, and
  Szepesv{\'{a}}ri]{neu2010loopfree}
Gergely Neu, Andr{\'{a}}s Gy{\"{o}}rgy, and Csaba Szepesv{\'{a}}ri.
\newblock The online loop-free stochastic shortest-path problem.
\newblock In \emph{{COLT} 2010 - The 23rd Conference on Learning Theory, Haifa,
  Israel, June 27-29, 2010}, pages 231--243, 2010.

\bibitem[Neu et~al.(2012)Neu, Gyorgy, and Szepesv{\'a}ri]{neu2012adversarial}
Gergely Neu, Andras Gyorgy, and Csaba Szepesv{\'a}ri.
\newblock The adversarial stochastic shortest path problem with unknown
  transition probabilities.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 805--813,
  2012.

\bibitem[Rosenberg and Mansour(2019{\natexlab{a}})]{rosenberg2019bandit}
Aviv Rosenberg and Yishay Mansour.
\newblock Online stochastic shortest path with bandit feedback and unknown
  transition function.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2209--2218, 2019{\natexlab{a}}.

\bibitem[Rosenberg and Mansour(2019{\natexlab{b}})]{rosenberg2019full}
Aviv Rosenberg and Yishay Mansour.
\newblock Online convex optimization in adversarial markov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pages
  5478--5486, 2019{\natexlab{b}}.

\bibitem[Rosenberg and Mansour(2020)]{rosenberg2020stochastic}
Aviv Rosenberg and Yishay Mansour.
\newblock Stochastic shortest path with adversarially changing costs, 2020.

\bibitem[Rosenberg et~al.(2020)Rosenberg, Cohen, Mansour, and
  Kaplan]{rosenberg2020near}
Aviv Rosenberg, Alon Cohen, Yishay Mansour, and Haim Kaplan.
\newblock Near-optimal regret bounds for stochastic shortest path.
\newblock In \emph{International Conference on Machine Learning}, pages
  8210--8219. PMLR, 2020.

\bibitem[Shani et~al.(2020)Shani, Efroni, Rosenberg, and
  Mannor]{efroni2020optimistic}
Lior Shani, Yonathan Efroni, Aviv Rosenberg, and Shie Mannor.
\newblock Optimistic policy optimization with bandit feedback.
\newblock In \emph{International Conference on Machine Learning}, pages
  8604--8613. PMLR, 2020.

\bibitem[Simchowitz and Jamieson(2019)]{simchowitz2019non}
Max Simchowitz and Kevin~G Jamieson.
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1153--1162, 2019.

\bibitem[Tarbouriech et~al.(2020)Tarbouriech, Garcelon, Valko, Pirotta, and
  Lazaric]{tarbouriech2019noregret}
Jean Tarbouriech, Evrard Garcelon, Michal Valko, Matteo Pirotta, and Alessandro
  Lazaric.
\newblock No-regret exploration in goal-oriented reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Tarbouriech et~al.(2021)Tarbouriech, Zhou, Du, Pirotta, Valko, and
  Lazaric]{tarbouriech2021stochastic}
Jean Tarbouriech, Runlong Zhou, Simon~S Du, Matteo Pirotta, Michal Valko, and
  Alessandro Lazaric.
\newblock Stochastic shortest path: Minimax, parameter-free and towards
  horizon-free regret.
\newblock \emph{arXiv preprint arXiv:2104.11186}, 2021.

\bibitem[Yang and Wang(2019)]{yang2019sample}
Lin~F Yang and Mengdi Wang.
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock \emph{arXiv preprint arXiv:1902.04779}, 2019.

\bibitem[Zanette and Brunskill(2019)]{zanette2019tighter}
Andrea Zanette and Emma Brunskill.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \emph{International Conference on Machine Learning}, pages
  7304--7312, 2019.

\bibitem[Zanette et~al.(2020{\natexlab{a}})Zanette, Brandfonbrener, Brunskill,
  Pirotta, and Lazaric]{zanette2020frequentist}
Andrea Zanette, David Brandfonbrener, Emma Brunskill, Matteo Pirotta, and
  Alessandro Lazaric.
\newblock Frequentist regret bounds for randomized least-squares value
  iteration.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1954--1964, 2020{\natexlab{a}}.

\bibitem[Zanette et~al.(2020{\natexlab{b}})Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020learning}
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock \emph{arXiv preprint arXiv:2003.00153}, 2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2020)Zhang, Ji, and Du]{zhang2020reinforcement}
Zihan Zhang, Xiangyang Ji, and Simon~S Du.
\newblock Is reinforcement learning more difficult than bandits? a near-optimal
  algorithm escaping the curse of horizon.
\newblock \emph{arXiv preprint arXiv:2009.13503}, 2020.

\bibitem[Zimin and Neu(2013)]{zimin2013online}
Alexander Zimin and Gergely Neu.
\newblock Online learning in episodic markovian decision processes by relative
  entropy policy search.
\newblock In \emph{Advances in Neural Information Processing Systems 26: 27th
  Annual Conference on Neural Information Processing Systems 2013. Proceedings
  of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States},
  pages 1583--1591, 2013.

\end{thebibliography}
