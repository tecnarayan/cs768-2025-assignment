\begin{thebibliography}{91}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjovsky et~al.(2016)Arjovsky, Shah, and Bengio]{arjovsky2016}
M.~Arjovsky, A.~Shah, and Y.~Bengio.
\newblock Unitary evolution recurrent neural networks.
\newblock In \emph{International conference on machine learning}, pages 1120--1128. PMLR, 2016.

\bibitem[Arribas(2018)]{arribas2018derivatives}
I.~P. Arribas.
\newblock Derivatives pricing using signature payoffs.
\newblock \emph{arXiv preprint arXiv:1809.09466}, 2018.

\bibitem[Bagnall et~al.(2018)Bagnall, Dau, Lines, Flynn, Large, Bostrom, Southam, and Keogh]{bagnall2018uea}
A.~Bagnall, H.~A. Dau, J.~Lines, M.~Flynn, J.~Large, A.~Bostrom, P.~Southam, and E.~Keogh.
\newblock The uea multivariate time series classification archive, 2018.
\newblock \emph{arXiv preprint arXiv:1811.00075}, 2018.

\bibitem[Barbero et~al.(2024)Barbero, Banino, Kapturowski, Kumaran, Ara{\'u}jo, Vitvitskyi, Pascanu, and Veli{\v{c}}kovi{\'c}]{barbero2024transformers}
F.~Barbero, A.~Banino, S.~Kapturowski, D.~Kumaran, J.~G. Ara{\'u}jo, A.~Vitvitskyi, R.~Pascanu, and P.~Veli{\v{c}}kovi{\'c}.
\newblock Transformers need glasses! information over-squashing in language tasks.
\newblock \emph{arXiv preprint arXiv:2406.04267}, 2024.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
I.~Beltagy, M.~E. Peters, and A.~Cohan.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Bilo\v{s} et~al.(2021)Bilo\v{s}, Sommer, Rangapuram, Januschowski, and G\"{u}nnemann]{bilos2021neural}
M.~Bilo\v{s}, J.~Sommer, S.~S. Rangapuram, T.~Januschowski, and S.~G\"{u}nnemann.
\newblock Neural flows: Efficient alternative to neural odes.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W. Vaughan, editors, \emph{Advances in Neural Information Processing Systems}, volume~34, pages 21325--21337. Curran Associates, Inc., 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2021/file/b21f9f98829dea9a48fd8aaddc1f159d-Paper.pdf}.

\bibitem[Calvo-Ordonez et~al.(2023)Calvo-Ordonez, Huang, Zhang, Yang, Schonlieb, and Aviles-Rivero]{calvo2023beyond}
S.~Calvo-Ordonez, J.~Huang, L.~Zhang, G.~Yang, C.-B. Schonlieb, and A.~I. Aviles-Rivero.
\newblock Beyond u: Making diffusion models faster \& lighter.
\newblock \emph{arXiv preprint arXiv:2310.20092}, 2023.

\bibitem[Cartea et~al.(2015)Cartea, Jaimungal, and Penalva]{cartea2015algorithmic}
{\'A}.~Cartea, S.~Jaimungal, and J.~Penalva.
\newblock \emph{Algorithmic and high-frequency trading}.
\newblock Cambridge University Press, 2015.

\bibitem[Cartea et~al.(2023)Cartea, Duran-Martin, and S{\'a}nchez-Betancourt]{cartea2023detecting}
{\'A}.~Cartea, G.~Duran-Martin, and L.~S{\'a}nchez-Betancourt.
\newblock Detecting toxic flow.
\newblock \emph{arXiv preprint arXiv:2312.05827}, 2023.

\bibitem[Chang et~al.(2018)Chang, Chen, Haber, and Chi]{chang2019}
B.~Chang, M.~Chen, E.~Haber, and E.~H. Chi.
\newblock Antisymmetricrnn: A dynamical system view on recurrent neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Chang et~al.(2023)Chang, Dur{\`a}n-Mart{\'\i}n, Shestopaloff, Jones, and Murphy]{chang2023low}
P.~Chang, G.~Dur{\`a}n-Mart{\'\i}n, A.~Y. Shestopaloff, M.~Jones, and K.~Murphy.
\newblock Low-rank extended kalman filtering for online learning of neural networks from streaming data.
\newblock \emph{arXiv preprint arXiv:2305.19535}, 2023.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and Duvenaud]{chen2018neural}
R.~T. Chen, Y.~Rubanova, J.~Bettencourt, and D.~K. Duvenaud.
\newblock Neural ordinary differential equations.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Chen et~al.(2023)Chen, Ren, Wang, Fang, Sun, and Li]{chen2023contiformer}
Y.~Chen, K.~Ren, Y.~Wang, Y.~Fang, W.~Sun, and D.~Li.
\newblock Contiformer: Continuous-time transformer for irregular time series modeling.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and Sutskever]{child2019generating}
R.~Child, S.~Gray, A.~Radford, and I.~Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Cho et~al.(2014)Cho, Van~Merri{\"e}nboer, Gulcehre, Bahdanau, Bougares, Schwenk, and Bengio]{cho2014learning}
K.~Cho, B.~Van~Merri{\"e}nboer, C.~Gulcehre, D.~Bahdanau, F.~Bougares, H.~Schwenk, and Y.~Bengio.
\newblock Learning phrase representations using rnn encoder-decoder for statistical machine translation.
\newblock \emph{arXiv preprint arXiv:1406.1078}, 2014.

\bibitem[Choromanski et~al.(2020)Choromanski, Likhosherstov, Dohan, Song, Gane, Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, et~al.]{choromanski2020rethinking}
K.~M. Choromanski, V.~Likhosherstov, D.~Dohan, X.~Song, A.~Gane, T.~Sarlos, P.~Hawkins, J.~Q. Davis, A.~Mohiuddin, L.~Kaiser, et~al.
\newblock Rethinking attention with performers.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Cini et~al.(2024)Cini, Marisca, Zambon, and Alippi]{cini2024taming}
A.~Cini, I.~Marisca, D.~Zambon, and C.~Alippi.
\newblock Taming local effects in graph-based spatiotemporal forecasting.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Cirone et~al.(2024)Cirone, Orvieto, Walker, Salvi, and Lyons]{cirone2024theoretical}
N.~M. Cirone, A.~Orvieto, B.~Walker, C.~Salvi, and T.~Lyons.
\newblock Theoretical foundations of deep selective state-space models.
\newblock \emph{arXiv preprint arXiv:2402.19047}, 2024.

\bibitem[Compagnoni et~al.(2023)Compagnoni, Scampicchio, Biggio, Orvieto, Hofmann, and Teichmann]{compagnoni2023effectiveness}
E.~M. Compagnoni, A.~Scampicchio, L.~Biggio, A.~Orvieto, T.~Hofmann, and J.~Teichmann.
\newblock On the effectiveness of randomized signatures as reservoir for learning rough dynamics.
\newblock In \emph{2023 International Joint Conference on Neural Networks (IJCNN)}, pages 1--8. IEEE, 2023.

\bibitem[Corsi(2009)]{corsi2009simple}
F.~Corsi.
\newblock A simple approximate long-memory model of realized volatility.
\newblock \emph{Journal of Financial Econometrics}, 7\penalty0 (2):\penalty0 174--196, 2009.

\bibitem[Cuchiero et~al.(2021)Cuchiero, Gonon, Grigoryeva, Ortega, and Teichmann]{cuchiero2021discrete}
C.~Cuchiero, L.~Gonon, L.~Grigoryeva, J.-P. Ortega, and J.~Teichmann.
\newblock Discrete-time signatures and randomness in reservoir computing.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems}, 33\penalty0 (11):\penalty0 6321--6330, 2021.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R{\'e}]{dao2022flashattention}
T.~Dao, D.~Fu, S.~Ermon, A.~Rudra, and C.~R{\'e}.
\newblock Flashattention: Fast and memory-efficient exact attention with io-awareness.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 16344--16359, 2022.

\bibitem[de~Oc{\'a}riz~Borde et~al.(2023)de~Oc{\'a}riz~Borde, Arroyo, and Posner]{de2023projections}
H.~S. de~Oc{\'a}riz~Borde, A.~Arroyo, and I.~Posner.
\newblock Projections of model spaces for latent graph inference.
\newblock In \emph{ICLR 2023 Workshop on Physics for Machine Learning}, 2023.

\bibitem[Dupont et~al.(2019)Dupont, Doucet, and Teh]{dupont2019augmented}
E.~Dupont, A.~Doucet, and Y.~W. Teh.
\newblock Augmented neural odes.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Erichson et~al.(2020)Erichson, Azencot, Queiruga, Hodgkinson, and Mahoney]{erichson2021}
N.~B. Erichson, O.~Azencot, A.~Queiruga, L.~Hodgkinson, and M.~W. Mahoney.
\newblock Lipschitz recurrent neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Feng et~al.(2023)Feng, Li, Jiang, and Ying]{feng2023diffuser}
A.~Feng, I.~Li, Y.~Jiang, and R.~Ying.
\newblock Diffuser: efficient transformers with multi-hop attention diffusion for long sequences.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pages 12772--12780, 2023.

\bibitem[Fermanian(2021)]{fermanian2021embedding}
A.~Fermanian.
\newblock Embedding and learning with signatures.
\newblock \emph{Computational Statistics \& Data Analysis}, 157:\penalty0 107148, 2021.

\bibitem[Fleming et~al.(2018)Fleming, Sheldon, Fagan, Leimgruber, Mueller, Nandintsetseg, Noonan, Olson, Setyawan, Sianipar, et~al.]{fleming2018correcting}
C.~Fleming, D.~Sheldon, W.~Fagan, P.~Leimgruber, T.~Mueller, D.~Nandintsetseg, M.~Noonan, K.~Olson, E.~Setyawan, A.~Sianipar, et~al.
\newblock Correcting for missing and irregular data in home-range estimation.
\newblock \emph{Ecological Applications}, 28\penalty0 (4):\penalty0 1003--1010, 2018.

\bibitem[Fons et~al.(2022)Fons, Sztrajman, El-Laham, Iosifidis, and Vyetrenko]{fons2022hypertime}
E.~Fons, A.~Sztrajman, Y.~El-Laham, A.~Iosifidis, and S.~Vyetrenko.
\newblock Hypertime: Implicit neural representation for time series.
\newblock \emph{arXiv preprint arXiv:2208.05836}, 2022.

\bibitem[Funahashi and Nakamura(1993)]{funahashi1993approximation}
K.-i. Funahashi and Y.~Nakamura.
\newblock Approximation of dynamical systems by continuous time recurrent neural networks.
\newblock \emph{Neural networks}, 6\penalty0 (6):\penalty0 801--806, 1993.

\bibitem[Gu and Dao(2023)]{gu2023}
A.~Gu and T.~Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock \emph{arXiv preprint arXiv:2312.00752}, 2023.

\bibitem[Gu et~al.(2021)Gu, Goel, and Re]{gu2021}
A.~Gu, K.~Goel, and C.~Re.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Hambly and Lyons(2010)]{hambly2010uniqueness}
B.~Hambly and T.~Lyons.
\newblock Uniqueness for the signature of a path of bounded variation and the reduced path group.
\newblock \emph{Annals of Mathematics}, pages 109--167, 2010.

\bibitem[Hasani et~al.(2021)Hasani, Lechner, Amini, Rus, and Grosu]{hasani2021liquid}
R.~Hasani, M.~Lechner, A.~Amini, D.~Rus, and R.~Grosu.
\newblock Liquid time-constant networks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, pages 7657--7666, 2021.

\bibitem[Hausdorff and Peng(1996)]{hausdorff1996multiscaled}
J.~M. Hausdorff and C.-K. Peng.
\newblock Multiscaled randomness: A possible source of 1/f noise in biology.
\newblock \emph{Physical review E}, 54\penalty0 (2):\penalty0 2154, 1996.

\bibitem[Hautsch(2004)]{hautsch2004modelling}
N.~Hautsch.
\newblock \emph{Modelling irregularly spaced financial data: theory and practice of dynamic duration models}.
\newblock Springer Science \& Business Media, 2004.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778, 2016.

\bibitem[Henaff et~al.(2016)Henaff, Szlam, and LeCun]{Henaff2016}
M.~Henaff, A.~Szlam, and Y.~LeCun.
\newblock Recurrent orthogonal networks and long-memory tasks.
\newblock In \emph{International Conference on Machine Learning}, pages 2034--2042. PMLR, 2016.

\bibitem[H{\"o}glund et~al.(2023)H{\"o}glund, Ferrucci, Hern{\'a}ndez, Gonzalez, Salvi, S{\'a}nchez-Betancourt, and Zhang]{hoglund2023neural}
M.~H{\"o}glund, E.~Ferrucci, C.~Hern{\'a}ndez, A.~M. Gonzalez, C.~Salvi, L.~S{\'a}nchez-Betancourt, and Y.~Zhang.
\newblock A neural rde approach for continuous-time non-markovian stochastic control problems.
\newblock In \emph{ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems}, 2023.

\bibitem[Holt et~al.(2022)Holt, Qian, and van~der Schaar]{holt2022neural}
S.~I. Holt, Z.~Qian, and M.~van~der Schaar.
\newblock Neural laplace: Learning diverse classes of differential equations in the laplace domain.
\newblock In \emph{International Conference on Machine Learning}, pages 8811--8832. PMLR, 2022.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and Fleuret]{katharopoulos2020transformers}
A.~Katharopoulos, A.~Vyas, N.~Pappas, and F.~Fleuret.
\newblock Transformers are rnns: Fast autoregressive transformers with linear attention.
\newblock In \emph{International conference on machine learning}, pages 5156--5165. PMLR, 2020.

\bibitem[Keller et~al.(2023)Keller, Muller, Sejnowski, and Welling]{keller2023}
T.~A. Keller, L.~Muller, T.~Sejnowski, and M.~Welling.
\newblock Traveling waves encode the recent past and enhance sequence learning.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[Kidger and Lyons(2020)]{kidger2020signatory}
P.~Kidger and T.~Lyons.
\newblock Signatory: differentiable computations of the signature and logsignature transforms, on both cpu and gpu.
\newblock \emph{arXiv preprint arXiv:2001.00706}, 2020.

\bibitem[Kidger et~al.(2019)Kidger, Bonnier, Perez~Arribas, Salvi, and Lyons]{kidger2018deep}
P.~Kidger, P.~Bonnier, I.~Perez~Arribas, C.~Salvi, and T.~Lyons.
\newblock Deep signature transforms.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/d2cdf047a6674cef251d56544a3cf029-Paper.pdf}.

\bibitem[Kidger et~al.(2020)Kidger, Morrill, Foster, and Lyons]{kidger2020neural}
P.~Kidger, J.~Morrill, J.~Foster, and T.~Lyons.
\newblock Neural controlled differential equations for irregular time series.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 6696--6707, 2020.

\bibitem[Lechner and Hasani(2020)]{lechner2020learning}
M.~Lechner and R.~Hasani.
\newblock Learning long-term dependencies in irregularly-sampled time series.
\newblock \emph{arXiv preprint arXiv:2006.04418}, 2020.

\bibitem[Lemercier et~al.(2021)Lemercier, Salvi, Cass, Bonilla, Damoulas, and Lyons]{lemercier2021siggpde}
M.~Lemercier, C.~Salvi, T.~Cass, E.~V. Bonilla, T.~Damoulas, and T.~J. Lyons.
\newblock Siggpde: Scaling sparse gaussian processes on sequential data.
\newblock In \emph{International Conference on Machine Learning}, pages 6233--6242. PMLR, 2021.

\bibitem[Lezcano-Casado and Mart{\i}nez-Rubio(2019)]{lezcano2019}
M.~Lezcano-Casado and D.~Mart{\i}nez-Rubio.
\newblock Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group.
\newblock In \emph{International Conference on Machine Learning}, pages 3794--3803. PMLR, 2019.

\bibitem[Li et~al.(2019)Li, Jin, Xuan, Zhou, Chen, Wang, and Yan]{li2019enhancing}
S.~Li, X.~Jin, Y.~Xuan, X.~Zhou, W.~Chen, Y.-X. Wang, and X.~Yan.
\newblock Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Li et~al.(2020)Li, Kovachki, Azizzadenesheli, Bhattacharya, Stuart, Anandkumar, et~al.]{li2020fourier}
Z.~Li, N.~B. Kovachki, K.~Azizzadenesheli, K.~Bhattacharya, A.~Stuart, A.~Anandkumar, et~al.
\newblock Fourier neural operator for parametric partial differential equations.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Lyons et~al.(2007)Lyons, Caruana, and L{\'e}vy]{lyons2007differential}
T.~J. Lyons, M.~Caruana, and T.~L{\'e}vy.
\newblock \emph{Differential equations driven by rough paths}.
\newblock Springer, 2007.

\bibitem[Melnychuk et~al.(2022)Melnychuk, Frauen, and Feuerriegel]{melnychuk2022causal}
V.~Melnychuk, D.~Frauen, and S.~Feuerriegel.
\newblock Causal transformer for estimating counterfactual outcomes.
\newblock In \emph{International Conference on Machine Learning}, pages 15293--15329. PMLR, 2022.

\bibitem[Morariu-Patrichi and Pakkanen(2022)]{morariu2022state}
M.~Morariu-Patrichi and M.~S. Pakkanen.
\newblock State-dependent hawkes processes and their application to limit order book modelling.
\newblock \emph{Quantitative Finance}, 22\penalty0 (3):\penalty0 563--583, 2022.

\bibitem[Moreno-Pino and Zohren(2022)]{moreno2022deepvol}
F.~Moreno-Pino and S.~Zohren.
\newblock Deepvol: Volatility forecasting from high-frequency data with dilated causal convolutions.
\newblock \emph{arXiv preprint arXiv:2210.04797}, 2022.

\bibitem[Moreno-Pino et~al.(2023)Moreno-Pino, Olmos, and Art{\'e}s-Rodr{\'\i}guez]{moreno2023deep}
F.~Moreno-Pino, P.~M. Olmos, and A.~Art{\'e}s-Rodr{\'\i}guez.
\newblock Deep autoregressive models with spectral attention.
\newblock \emph{Pattern Recognition}, 133:\penalty0 109014, 2023.

\bibitem[Moreno-Pino et~al.(2024)Moreno-Pino, Arroyo, Waldon, Dong, and Cartea]{moreno2024rough}
F.~Moreno-Pino, {\'A}.~Arroyo, H.~Waldon, X.~Dong, and {\'A}.~Cartea.
\newblock Rough transformers for continuous and efficient time-series modelling.
\newblock \emph{arXiv preprint arXiv:2403.10288}, 2024.

\bibitem[Morrill et~al.(2021)Morrill, Salvi, Kidger, and Foster]{morrill2021neural}
J.~Morrill, C.~Salvi, P.~Kidger, and J.~Foster.
\newblock Neural rough differential equations for long time series.
\newblock In \emph{International Conference on Machine Learning}, pages 7829--7838. PMLR, 2021.

\bibitem[Nguyen and Grover(2022)]{nguyen2022transformer}
T.~Nguyen and A.~Grover.
\newblock Transformer neural processes: Uncertainty-aware meta learning via sequence modeling.
\newblock In \emph{International Conference on Machine Learning}, pages 16569--16594. PMLR, 2022.

\bibitem[Norcliffe et~al.(2020{\natexlab{a}})Norcliffe, Bodnar, Day, Moss, and Li{\`o}]{norcliffe2020neural}
A.~Norcliffe, C.~Bodnar, B.~Day, J.~Moss, and P.~Li{\`o}.
\newblock Neural ode processes.
\newblock In \emph{International Conference on Learning Representations}, 2020{\natexlab{a}}.

\bibitem[Norcliffe et~al.(2020{\natexlab{b}})Norcliffe, Bodnar, Day, Simidjievski, and Li{\`o}]{norcliffe2020second}
A.~Norcliffe, C.~Bodnar, B.~Day, N.~Simidjievski, and P.~Li{\`o}.
\newblock On second order behaviour in augmented neural odes.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 5911--5921, 2020{\natexlab{b}}.

\bibitem[Oh et~al.()Oh, Lim, and Kim]{ohstable}
Y.~Oh, D.~Lim, and S.~Kim.
\newblock Stable neural stochastic differential equations in analyzing irregular time series data.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[Orvieto et~al.(2023)Orvieto, Smith, Gu, Fernando, Gulcehre, Pascanu, and De]{orvieto2023}
A.~Orvieto, S.~L. Smith, A.~Gu, A.~Fernando, C.~Gulcehre, R.~Pascanu, and S.~De.
\newblock Resurrecting recurrent neural networks for long sequences.
\newblock In \emph{International Conference on Machine Learning}, pages 26670--26698. PMLR, 2023.

\bibitem[Park et~al.(2023)Park, Choi, Yoon, Kang, et~al.]{park2023learning}
Y.~Park, J.~Choi, C.~Yoon, M.~Kang, et~al.
\newblock Learning pde solution operator for continuous modeling of time-series.
\newblock \emph{arXiv preprint arXiv:2302.00854}, 2023.

\bibitem[Perez~Arribas et~al.(2018)Perez~Arribas, Goodwin, Geddes, Lyons, and Saunders]{perez2018signature}
I.~Perez~Arribas, G.~M. Goodwin, J.~R. Geddes, T.~Lyons, and K.~E. Saunders.
\newblock A signature-based machine learning model for distinguishing bipolar disorder and borderline personality disorder.
\newblock \emph{Translational psychiatry}, 8\penalty0 (1):\penalty0 274, 2018.

\bibitem[Perveen et~al.(2020)Perveen, Shahbaz, Saba, Keshavjee, Rehman, and Guergachi]{perveen2020handling}
S.~Perveen, M.~Shahbaz, T.~Saba, K.~Keshavjee, A.~Rehman, and A.~Guergachi.
\newblock Handling irregularly sampled longitudinal data and prognostic modeling of diabetes using machine learning technique.
\newblock \emph{IEEE Access}, 8:\penalty0 21875--21885, 2020.

\bibitem[Ratcliff et~al.(2016)Ratcliff, Smith, Brown, and McKoon]{ratcliff2016diffusion}
R.~Ratcliff, P.~L. Smith, S.~D. Brown, and G.~McKoon.
\newblock Diffusion decision model: Current issues and history.
\newblock \emph{Trends in cognitive sciences}, 20\penalty0 (4):\penalty0 260--281, 2016.

\bibitem[Reizenstein(2017)]{reizenstein2017calculation}
J.~Reizenstein.
\newblock Calculation of iterated-integral signatures and log signatures.
\newblock \emph{arXiv preprint arXiv:1712.02757}, 2017.

\bibitem[Reizenstein and Graham(2018)]{reizenstein2018iisignature}
J.~Reizenstein and B.~Graham.
\newblock The iisignature library: efficient calculation of iterated-integral signatures and log signatures.
\newblock \emph{arXiv preprint arXiv:1802.08252}, 2018.

\bibitem[Romero et~al.()Romero, Kuzina, Bekkers, Tomczak, and Hoogendoorn]{romerockconv}
D.~W. Romero, A.~Kuzina, E.~J. Bekkers, J.~M. Tomczak, and M.~Hoogendoorn.
\newblock Ckconv: Continuous kernel convolution for sequential data.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[Rubanova et~al.(2019)Rubanova, Chen, and Duvenaud]{rubanova2019latent}
Y.~Rubanova, R.~T. Chen, and D.~K. Duvenaud.
\newblock Latent ordinary differential equations for irregularly-sampled time series.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Rusch and Mishra(2020)]{rusch2021a}
T.~K. Rusch and S.~Mishra.
\newblock Coupled oscillatory recurrent neural network (cornn): An accurate and (gradient) stable architecture for learning long time dependencies.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Rusch and Mishra(2021)]{rusch2021b}
T.~K. Rusch and S.~Mishra.
\newblock Unicornn: A recurrent model for learning very long time dependencies.
\newblock In \emph{International Conference on Machine Learning}, pages 9168--9178. PMLR, 2021.

\bibitem[Rusch et~al.(2021)Rusch, Mishra, Erichson, and Mahoney]{rusch2022}
T.~K. Rusch, S.~Mishra, N.~B. Erichson, and M.~W. Mahoney.
\newblock Long expressive memory for sequence modeling.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Rusch et~al.(2022)Rusch, Chamberlain, Rowbottom, Mishra, and Bronstein]{rusch2022graph}
T.~K. Rusch, B.~Chamberlain, J.~Rowbottom, S.~Mishra, and M.~Bronstein.
\newblock Graph-coupled oscillator networks.
\newblock In \emph{International Conference on Machine Learning}, pages 18888--18909. PMLR, 2022.

\bibitem[Rusch et~al.(2023)Rusch, Bronstein, and Mishra]{rusch2023survey}
T.~K. Rusch, M.~M. Bronstein, and S.~Mishra.
\newblock A survey on oversmoothing in graph neural networks.
\newblock \emph{arXiv preprint arXiv:2303.10993}, 2023.

\bibitem[S{\'a}ez~de Oc{\'a}riz~Borde et~al.(2024)S{\'a}ez~de Oc{\'a}riz~Borde, Arroyo, Morales, Posner, and Dong]{saez2024neural}
H.~S{\'a}ez~de Oc{\'a}riz~Borde, A.~Arroyo, I.~Morales, I.~Posner, and X.~Dong.
\newblock Neural latent geometry search: product manifold inference via gromov-hausdorff-informed bayesian optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Salvi et~al.(2021)Salvi, Lemercier, Liu, Horvath, Damoulas, and Lyons]{salvi2021higher}
C.~Salvi, M.~Lemercier, C.~Liu, B.~Horvath, T.~Damoulas, and T.~Lyons.
\newblock Higher order kernel mean embeddings to capture filtrations of stochastic processes.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 16635--16647, 2021.

\bibitem[Schirmer et~al.(2022)Schirmer, Eltayeb, Lessmann, and Rudolph]{schirmer2022modeling}
M.~Schirmer, M.~Eltayeb, S.~Lessmann, and M.~Rudolph.
\newblock Modeling irregular time series with continuous recurrent units.
\newblock In \emph{International conference on machine learning}, pages 19388--19405. PMLR, 2022.

\bibitem[Seedat et~al.(2022)Seedat, Imrie, Bellot, Qian, and van~der Schaar]{seedat2022continuous}
N.~Seedat, F.~Imrie, A.~Bellot, Z.~Qian, and M.~van~der Schaar.
\newblock Continuous-time modeling of counterfactual outcomes using neural controlled differential equations.
\newblock In \emph{International Conference on Machine Learning}, pages 19497--19521. PMLR, 2022.

\bibitem[Sitzmann et~al.(2020)Sitzmann, Martel, Bergman, Lindell, and Wetzstein]{sitzmann2020implicit}
V.~Sitzmann, J.~Martel, A.~Bergman, D.~Lindell, and G.~Wetzstein.
\newblock Implicit neural representations with periodic activation functions.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 7462--7473, 2020.

\bibitem[Smith et~al.()Smith, Warrington, and Linderman]{smithsimplified}
J.~T. Smith, A.~Warrington, and S.~Linderman.
\newblock Simplified state space layers for sequence modeling.
\newblock In \emph{The Eleventh International Conference on Learning Representations}.

\bibitem[Tallec and Ollivier(2018)]{tallec2018}
C.~Tallec and Y.~Ollivier.
\newblock Can recurrent neural networks warp time?
\newblock \emph{arXiv preprint arXiv:1804.11188}, 2018.

\bibitem[Tan et~al.(2020)Tan, Bergmeir, Petitjean, and Webb]{tan2020monash}
C.~W. Tan, C.~Bergmeir, F.~Petitjean, and G.~I. Webb.
\newblock Monash university, uea, ucr time series extrinsic regression archive.
\newblock \emph{arXiv preprint arXiv:2006.10996}, 2020.

\bibitem[Tong et~al.(2023)Tong, Nguyen-Tang, Lee, Tran, and Choi]{tong2023sigformer}
A.~Tong, T.~Nguyen-Tang, D.~Lee, T.~M. Tran, and J.~Choi.
\newblock Sigformer: Signature transformers for deep hedging.
\newblock In \emph{Proceedings of the Fourth ACM International Conference on AI in Finance}, pages 124--132, 2023.

\bibitem[Vahid et~al.(2020)Vahid, M{\"u}ckschel, Stober, Stock, and Beste]{vahid2020applying}
A.~Vahid, M.~M{\"u}ckschel, S.~Stober, A.-K. Stock, and C.~Beste.
\newblock Applying deep learning to single-trial eeg data provides evidence for complementary theories on action control.
\newblock \emph{Communications biology}, 3\penalty0 (1):\penalty0 112, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Walker et~al.(2024)Walker, McLeod, Qin, Cheng, Li, and Lyons]{walker2024log}
B.~Walker, A.~D. McLeod, T.~Qin, Y.~Cheng, H.~Li, and T.~Lyons.
\newblock Log neural controlled differential equations: The lie brackets make a difference.
\newblock \emph{arXiv preprint arXiv:2402.18512}, 2024.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
S.~Wang, B.~Z. Li, M.~Khabsa, H.~Fang, and H.~Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Yoon et~al.(2019)Yoon, Jarrett, and Van~der Schaar]{yoon2019time}
J.~Yoon, D.~Jarrett, and M.~Van~der Schaar.
\newblock Time-series generative adversarial networks.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti, Ontanon, Pham, Ravula, Wang, Yang, et~al.]{zaheer2020big}
M.~Zaheer, G.~Guruganesh, K.~A. Dubey, J.~Ainslie, C.~Alberti, S.~Ontanon, P.~Pham, A.~Ravula, Q.~Wang, L.~Yang, et~al.
\newblock Big bird: Transformers for longer sequences.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 17283--17297, 2020.

\bibitem[Zeng et~al.(2023)Zeng, Chen, Zhang, and Xu]{zeng2023transformers}
A.~Zeng, M.~Chen, L.~Zhang, and Q.~Xu.
\newblock Are transformers effective for time series forecasting?
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~37, pages 11121--11128, 2023.

\end{thebibliography}
