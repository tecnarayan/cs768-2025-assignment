\begin{thebibliography}{43}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Aharoni et~al.(2023)Aharoni, Adir, Baruch, Drucker, Ezov, Farkash, Greenberg, Masalha, Moshkowich, Murik et~al.}]{helayers}
Ehud Aharoni, Allon Adir, Moran Baruch, Nir Drucker, Gilad Ezov, Ariel Farkash, Lev Greenberg, Ramy Masalha, Guy Moshkowich, Dov Murik, et~al. 2023.
\newblock \href {https://doi.org/10.56553/popets-2023-0020} {{HElayers: A tile tensors framework for large neural networks on encrypted data}}.
\newblock \emph{PoPETs}.

\bibitem[{Ao and Boddeti(2023)}]{ao2023autofhe}
Wei Ao and Vishnu~Naresh Boddeti. 2023.
\newblock Autofhe: Automated adaption of cnns for efficient evaluation over fhe.
\newblock \emph{arXiv preprint arXiv:2310.08012}.

\bibitem[{Baruch et~al.(2023)Baruch, Drucker, Ezov, Kushnir, Lerner, Soceanu, and Zimerman}]{baruch2023sensitive}
Moran Baruch, Nir Drucker, Gilad Ezov, Eyal Kushnir, Jenny Lerner, Omri Soceanu, and Itamar Zimerman. 2023.
\newblock Sensitive tuning of large scale cnns for e2e secure prediction using homomorphic encryption.
\newblock \emph{arXiv preprint arXiv:2304.14836}.

\bibitem[{Baruch et~al.(2022)Baruch, Drucker, Greenberg, and Moshkowich}]{baruch2021fighting}
Moran Baruch, Nir Drucker, Lev Greenberg, and Guy Moshkowich. 2022.
\newblock \href {https://doi.org/10.1007/978-3-031-16815-4\_29} {{A Methodology for Training Homomorphic Encryption Friendly Neural Networks}}.
\newblock In \emph{Applied Cryptography and Network Security Workshops}, pages 536--553, Cham. Springer International Publishing.

\bibitem[{Chen et~al.(2022)Chen, Bao, Huang, Dong, Jiao, Jiang, Zhou, Li, and Wei}]{TransformerPPML1}
Tianyu Chen, Hangbo Bao, Shaohan Huang, Li~Dong, Binxing Jiao, Daxin Jiang, Haoyi Zhou, Jianxin Li, and Furu Wei. 2022.
\newblock The-x: Privacy-preserving transformer inference with homomorphic encryption.
\newblock \emph{arXiv preprint arXiv:2206.00216}.

\bibitem[{Cheon et~al.(2017)Cheon, Kim, Kim, and Song}]{ckks2017}
Jung~Hee Cheon, Andrey Kim, Miran Kim, and Yongsoo Song. 2017.
\newblock \href {https://doi.org/10.1007/978-3-319-70694-8\_15} {Homomorphic encryption for arithmetic of approximate numbers}.
\newblock In \emph{International Conference on the Theory and Application of Cryptology and Information Security}, pages 409--437. Springer.

\bibitem[{Chrysos et~al.(2020)Chrysos, Moschoglou, Bouritsas, Panagakis, Deng, and Zafeiriou}]{chrysos2020p}
Grigorios~G Chrysos, Stylianos Moschoglou, Giorgos Bouritsas, Yannis Panagakis, Jiankang Deng, and Stefanos Zafeiriou. 2020.
\newblock P-nets: Deep polynomial neural networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 7325--7335.

\bibitem[{Dathathri et~al.(2019)Dathathri, Saarikivi, Chen, Laine, Lauter, Maleki, Musuvathi, and Mytkowicz}]{chet_compiler}
Roshan Dathathri, Olli Saarikivi, Hao Chen, Kim Laine, Kristin Lauter, Saeed Maleki, Madanlal Musuvathi, and Todd Mytkowicz. 2019.
\newblock \href {https://doi.org/10.1145/3314221.3314628} {Chet: An optimizing compiler for fully-homomorphic neural-network inferencing}.
\newblock In \emph{Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation}, PLDI 2019, page 142–156, New York, NY, USA.

\bibitem[{Ding et~al.(2023)Ding, Guo, Guan, Liu, Huo, Guan, and Zhang}]{TransformerPPML3}
Yuanchao Ding, Hua Guo, Yewei Guan, Weixin Liu, Jiarong Huo, Zhenyu Guan, and Xiyong Zhang. 2023.
\newblock East: Efficient and accurate secure transformer framework for inference.
\newblock \emph{arXiv preprint arXiv:2308.09923}.

\bibitem[{Egidi et~al.(2020)Egidi, Fatone, and Misici}]{RI2}
Nadaniela Egidi, Lorella Fatone, and Luciano Misici. 2020.
\newblock \href {https://doi.org/10.1007/978-3-030-39081-5\_7} {{A New Remez-Type Algorithm for Best Polynomial Approximation}}.
\newblock In \emph{Numerical Computations: Theory and Algorithms}, pages 56--69, Cham. Springer International Publishing.

\bibitem[{Gentry(2009)}]{Gentry2009}
Craig Gentry. 2009.
\newblock \href {https://crypto.stanford.edu/craig/craig-thesis.pdf} {\emph{A fully homomorphic encryption scheme}}.
\newblock Ph.D. thesis, Stanford University, Palo Alto, CA.

\bibitem[{Gilad~Bachrach et~al.(2016)Gilad~Bachrach, Dowlin, Laine, Lauter, Naehrig, and Wernsing}]{CryptoNets2016}
Ran Gilad~Bachrach, Nathan Dowlin, Kim Laine, Kristin Lauter, Michael Naehrig, and John Wernsing. 2016.
\newblock \href {http://proceedings.mlr.press/v48/gilad-bachrach16.pdf} {Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy}.
\newblock In \emph{International Conference on Machine Learning}, pages 201--210.

\bibitem[{Gottemukkula(2020)}]{gottemukkula2020polynomial}
Vikas Gottemukkula. 2020.
\newblock Polynomial activation functions.

\bibitem[{Goyal et~al.(2020)Goyal, Goyal, and Lall}]{goyal2020improved}
Mohit Goyal, Rajan Goyal, and Brejesh Lall. 2020.
\newblock Improved polynomial neural networks with normalised activations.
\newblock In \emph{2020 International Joint Conference on Neural Networks (IJCNN)}, pages 1--8. IEEE.

\bibitem[{Gupta et~al.(2023)Gupta, Jawalkar, Mukherjee, Chandran, Gupta, Panwar, and Sharma}]{TransformerPPML6}
Kanav Gupta, Neha Jawalkar, Ananta Mukherjee, Nishanth Chandran, Divya Gupta, Ashish Panwar, and Rahul Sharma. 2023.
\newblock Sigma: Secure gpt inference with function secret sharing.
\newblock \emph{Cryptology ePrint Archive}.

\bibitem[{Hao et~al.(2022)Hao, Li, Chen, Xing, Xu, and Zhang}]{TransformerPPML9}
Meng Hao, Hongwei Li, Hanxiao Chen, Pengzhi Xing, Guowen Xu, and Tianwei Zhang. 2022.
\newblock Iron: Private inference on transformers.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:15718--15731.

\bibitem[{He et~al.(2016)He, Zhang, Ren, and Sun}]{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778.

\bibitem[{Hesamifard et~al.(2017)Hesamifard, Takabi, and Ghasemi}]{cryptoDL}
Ehsan Hesamifard, Hassan Takabi, and Mehdi Ghasemi. 2017.
\newblock \href {https://doi.org/10.48550/ARXIV.1711.05189} {Cryptodl: Deep neural networks over encrypted data}.

\bibitem[{Hua et~al.(2022)Hua, Dai, Liu, and Le}]{hua2022transformer}
Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. 2022.
\newblock Transformer quality in linear time.
\newblock In \emph{International Conference on Machine Learning}, pages 9099--9117. PMLR.

\bibitem[{Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton}]{alexnet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. 2012.
\newblock \href {https://doi.org/10.1145/3065386} {Imagenet classification with deep convolutional neural networks}.
\newblock \emph{Neural Information Processing Systems}, 25.

\bibitem[{Lee et~al.(2022{\natexlab{a}})Lee, Lee, Lee, Kim, Kim, No, and Choi}]{pmlr-v162-lee22e}
Eunsang Lee, Joon-Woo Lee, Junghyun Lee, Young-Sik Kim, Yongjune Kim, Jong-Seon No, and Woosuk Choi. 2022{\natexlab{a}}.
\newblock \href {https://proceedings.mlr.press/v162/lee22e.html} {Low-complexity deep convolutional neural networks on fully homomorphic encryption using multiplexed parallel convolutions}.
\newblock In \emph{Proceedings of the 39th International Conference on Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning Research}, pages 12403--12422. PMLR.

\bibitem[{Lee et~al.(2022{\natexlab{b}})Lee, Lee, Lee, Kim, Kim, No, and Choi}]{lee2022low}
Eunsang Lee, Joon-Woo Lee, Junghyun Lee, Young-Sik Kim, Yongjune Kim, Jong-Seon No, and Woosuk Choi. 2022{\natexlab{b}}.
\newblock Low-complexity deep convolutional neural networks on fully homomorphic encryption using multiplexed parallel convolutions.
\newblock In \emph{International Conference on Machine Learning}, pages 12403--12422. PMLR.

\bibitem[{Lee et~al.(2022{\natexlab{c}})Lee, Kim, Park, Hwang, and Cheon}]{lee2022privacy}
Garam Lee, Minsoo Kim, Jai~Hyun Park, Seung-won Hwang, and Jung~Hee Cheon. 2022{\natexlab{c}}.
\newblock Privacy-preserving text classification on bert embeddings with homomorphic encryption.
\newblock \emph{arXiv preprint arXiv:2210.02574}.

\bibitem[{Lee et~al.(2022{\natexlab{d}})Lee, Kang, Lee, Choi, Eom, Deryabin, Lee, Lee, Yoo, Kim, and No}]{relu1}
Joon-Woo Lee, Hyungchul Kang, Yongwoo Lee, Woosuk Choi, Jieun Eom, Maxim Deryabin, Eunsang Lee, Junghyun Lee, Donghoon Yoo, Young-Sik Kim, and Jong-Seon No. 2022{\natexlab{d}}.
\newblock \href {https://doi.org/10.1109/ACCESS.2022.3159694} {Privacy-preserving machine learning with fully homomorphic encryption for deep neural network}.
\newblock \emph{IEEE Access}, 10:30039--30054.

\bibitem[{Lee et~al.(2021)Lee, Lee, Lee, Kim, Kim, and No}]{lee2021precise}
Junghyun Lee, Eunsang Lee, Joon-Woo Lee, Yongjune Kim, Young-Sik Kim, and Jong-Seon No. 2021.
\newblock \href {https://arxiv.org/abs/2105.10879} {Precise approximation of convolutional neural networks for homomorphically encrypted data}.
\newblock \emph{arXiv preprint arXiv:2105.10879}.

\bibitem[{Lee et~al.(2023)Lee, Lee, Kim, Shin, and Lee}]{lee2023hetal}
Seewoo Lee, Garam Lee, Jung~Woo Kim, Junbum Shin, and Mun-Kyu Lee. 2023.
\newblock Hetal: Efficient privacy-preserving transfer learning with homomorphic encryption.

\bibitem[{Liang et~al.(2023)Liang, Wang, Zhang, Xu, and Zhang}]{TransformerPPML5}
Zi~Liang, Pinghui Wang, Ruofei Zhang, Nuo Xu, and Shuo Zhang. 2023.
\newblock Merge: Fast private text generation.
\newblock \emph{arXiv preprint arXiv:2305.15769}.

\bibitem[{Liu and Liu(2023)}]{TransformerPPML4}
Xuanqi Liu and Zhuotao Liu. 2023.
\newblock Llms can understand encrypted prompt: Towards privacy-computing friendly transformers.
\newblock \emph{arXiv preprint arXiv:2305.18396}.

\bibitem[{Liu et~al.(2022)Liu, Mao, Wu, Feichtenhofer, Darrell, and Xie}]{convnext1}
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. 2022.
\newblock A convnet for the 2020s.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 11976--11986.

\bibitem[{Ma et~al.(2022)Ma, Zhou, Kong, He, Gui, Neubig, May, and Zettlemoyer}]{ma2022mega}
Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. 2022.
\newblock Mega: moving average equipped gated attention.
\newblock \emph{arXiv preprint arXiv:2209.10655}.

\bibitem[{Mohassel and Zhang(2017)}]{SecureML}
Payman Mohassel and Yupeng Zhang. 2017.
\newblock \href {https://doi.org/10.1109/SP.2017.12} {Secureml: A system for scalable privacy-preserving machine learning}.
\newblock In \emph{2017 IEEE Symposium on Security and Privacy (SP)}, pages 19--38.

\bibitem[{Pach{\'o}n and Trefethen(2009)}]{RI1}
Ricardo Pach{\'o}n and Lloyd~N Trefethen. 2009.
\newblock \href {https://doi.org/https://doi.org/10.1007/s10543-009-0240-1} {Barycentric-remez algorithms for best polynomial approximation in the chebfun system}.
\newblock \emph{BIT Numerical Mathematics}, 49(4):721--741.

\bibitem[{Panda(2022)}]{panda2022polynomial}
Samanvaya Panda. 2022.
\newblock Polynomial approximation of inverse sqrt function for fhe.
\newblock In \emph{International Symposium on Cyber Security, Cryptology, and Machine Learning}, pages 366--376. Springer.

\bibitem[{Raphson(1702)}]{raphson1702analysis}
Joseph Raphson. 1702.
\newblock \emph{Analysis aequationum universalis}.
\newblock Typis TB prostant venales apud A. and I. Churchill.

\bibitem[{Remez(1934)}]{Remez}
Eugene~Y Remez. 1934.
\newblock Sur la d{\'e}termination des polyn{\^o}mes d’approximation de degr{\'e} donn{\'e}e.
\newblock \emph{Comm. Soc. Math. Kharkov}, 10(196):41--63.

\bibitem[{So et~al.(2021)So, Ma{\'n}ke, Liu, Dai, Shazeer, and Le}]{so2021primer}
David~R So, Wojciech Ma{\'n}ke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc~V Le. 2021.
\newblock Primer: Searching for efficient transformers for language modeling.
\newblock \emph{arXiv preprint arXiv:2109.08668}.

\bibitem[{Takabi et~al.(2019)Takabi, Podschwadt, Druce, Wu, and Procopio}]{relu2}
Daniel Takabi, Robert Podschwadt, Jeff Druce, Curt Wu, and Kevin Procopio. 2019.
\newblock \href {https://10.48550/ARXIV.1911.11377} {Privacy preserving neural network inference on encrypted data with gpus}.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30.

\bibitem[{Wang et~al.(2022)Wang, Wu, and Huang}]{wang2022understanding}
Jiaxi Wang, Ji~Wu, and Lei Huang. 2022.
\newblock Understanding the failure of batch normalization for transformers in nlp.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:37617--37630.

\bibitem[{Yao et~al.(2021)Yao, Cao, Lin, Liu, Zhang, and Hu}]{yao2021leveraging}
Zhuliang Yao, Yue Cao, Yutong Lin, Ze~Liu, Zheng Zhang, and Han Hu. 2021.
\newblock Leveraging batch normalization for vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 413--422.

\bibitem[{Zeng et~al.(2023)Zeng, Li, Xiong, Tong, Lu, Tan, Wang, and Huang}]{TransformerPPML8}
Wenxuan Zeng, Meng Li, Wenjie Xiong, Tong Tong, Wen-jie Lu, Jin Tan, Runsheng Wang, and Ru~Huang. 2023.
\newblock Mpcvit: Searching for accurate and efficient mpc-friendly vision transformer with heterogeneous attention.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 5052--5063.

\bibitem[{Zheng et~al.(2023)Zheng, Lou, and Jiang}]{TransformerPPML7}
Mengxin Zheng, Qian Lou, and Lei Jiang. 2023.
\newblock Primer: Fast private transformer inference on encrypted data.
\newblock \emph{arXiv preprint arXiv:2303.13679}.

\bibitem[{Zhou et~al.(2019)Zhou, Qian, Lu, Duan, Huang, and Shao}]{zhou2019polynomial}
Jun Zhou, Huimin Qian, Xinbiao Lu, Zhaoxia Duan, Haoqian Huang, and Zhen Shao. 2019.
\newblock Polynomial activation neural networks: Modeling, stability analysis and coverage bp-training.
\newblock \emph{Neurocomputing}, 359:227--240.

\end{thebibliography}
