\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and
  Zheng]{tensorflow2015-whitepaper}
Mart\'{\i}n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
  Craig Citro, Greg~S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,
  Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
  Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
  Levenberg, Dan Man\'{e}, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,
  Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar,
  Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\'{e}gas, Oriol
  Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang
  Zheng.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock URL \url{https://www.tensorflow.org/}.

\bibitem[Allen-Zhu(2017)]{katyusha}
Zeyuan Allen-Zhu.
\newblock Katyusha: The first direct acceleration of stochastic gradient
  methods.
\newblock In \emph{Proceedings of the 49th Annual ACM SIGACT Symposium on
  Theory of Computing}, STOC 2017, 2017.

\bibitem[Allen-Zhu and Hazan(2016)]{pmlr-v48-allen-zhua16}
Zeyuan Allen-Zhu and Elad Hazan.
\newblock Variance reduction for faster non-convex optimization.
\newblock In \emph{Proceedings of The 33rd International Conference on Machine
  Learning}, 2016.

\bibitem[Balamurugan and Bach(2016)]{bach-saddle}
P.~Balamurugan and Francis Bach.
\newblock Stochastic variance reduction methods for saddle-point problems.
\newblock \emph{Advances in Neural Information Processing Systems 29
  (NIPS2016)}, 2016.

\bibitem[Bietti and Mairal(2017)]{online-miso-mairal}
Alberto Bietti and Julien Mairal.
\newblock Stochastic optimization with variance reduction for infinite datasets
  with finite sum structure.
\newblock In \emph{Advances in Neural Information Processing Systems 30 (NIPS
  2017)}, 2017.

\bibitem[Clevert et~al.(2016)Clevert, Unterthiner, and Hochreiter]{elu}
Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter.
\newblock Fast and accurate deep network learning by exponential linear units
  (elus).
\newblock In \emph{International conference on learning representations 2016
  (ICLR 2016)}. 2016.

\bibitem[Defazio(2016)]{adefazio-nips2016}
Aaron Defazio.
\newblock A simple practical accelerated method for finite sums.
\newblock \emph{Advances in Neural Information Processing Systems 29 (NIPS
  2016)}, 2016.

\bibitem[Defazio et~al.(2014{\natexlab{a}})Defazio, Bach, and
  Lacoste-Julien]{adefazio-nips2014}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock \emph{Advances in Neural Information Processing Systems 27 (NIPS
  2014)}, 2014{\natexlab{a}}.

\bibitem[Defazio et~al.(2014{\natexlab{b}})Defazio, Caetano, and
  Domke]{adefazio-icml2014}
Aaron Defazio, Tiberio Caetano, and Justin Domke.
\newblock Finito: A faster, permutable incremental gradient method for big data
  problems.
\newblock \emph{The 31st International Conference on Machine Learning (ICML
  2014)}, 2014{\natexlab{b}}.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{adagrad}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Frostig et~al.(2015)Frostig, Ge, Kakade, and Sidford]{streaming-svrg}
Roy Frostig, Rong Ge, Sham~M. Kakade, and Aaron Sidford.
\newblock Competing with the empirical risk minimizer in a single pass.
\newblock In \emph{Proceedings of The 28th Conference on Learning Theory},
  2015.

\bibitem[Goyal et~al.(2017)Goyal, Dollár, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{imagenet1h}
Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski,
  Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock 06 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Huang et~al.(2017)Huang, Liu, van~der Maaten, and
  Weinberger]{densenet}
Gao Huang, Zhuang Liu, Laurens van~der Maaten, and Kilian~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, July 2017.

\bibitem[Ioffe and Szegedy(2015)]{batchnorm}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning}, 2015.

\bibitem[Johnson and Zhang(2013)]{svrg}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock \emph{Advances in Neural Information Processing Systems 26
  (NIPS2013)}, 2013.

\bibitem[Kidambi et~al.(2018)Kidambi, Netrapalli, Jain, and
  Kakade]{insuff_momentum}
Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham~M. Kakade.
\newblock On the insufficiency of existing momentum schemes for stochastic
  optimization.
\newblock \emph{International Conference on Learning Representations (ICLR),
  2018, Vancouver, Canada}, 2018.

\bibitem[Kingma and Ba(2014)]{adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Lecun et~al.(1998)Lecun, Bottou, Bengio, and Haffner]{lenet}
Y.~Lecun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 1998.

\bibitem[Lei and Jordan(2017)]{scsg}
Lihua Lei and Michael Jordan.
\newblock {Less than a Single Pass: Stochastically Controlled Stochastic
  Gradient}.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics}, 2017.

\bibitem[Lei et~al.(2017)Lei, Ju, Chen, and Jordan]{nonconvex-scsg}
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael~I Jordan.
\newblock Non-convex finite-sum optimization via scsg methods.
\newblock In \emph{Advances in Neural Information Processing Systems 30}. 2017.

\bibitem[Lin et~al.(2015)Lin, Mairal, and Harchaoui]{catalyst}
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui.
\newblock A universal catalyst for first-order optimization.
\newblock In \emph{Advances in Neural Information Processing Systems 28}. 2015.

\bibitem[Mairal(2014)]{miso2}
Julien Mairal.
\newblock Incremental majorization-minimization optimization with application
  to large-scale machine learning.
\newblock Technical report, INRIA Grenoble Rh\^{o}ne-Alpes / LJK Laboratoire
  Jean Kuntzmann, 2014.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{sarah}
Lam~M. Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak{\'a}{\v{c}}.
\newblock Sarah: A novel method for machine learning problems using stochastic
  recursive gradient.
\newblock \emph{Proceedings of the 34th International Conference on Machine
  Learning (ICML2017)}, 2017.

\bibitem[Nguyen et~al.(2019)Nguyen, van Dijk, Phan, Nguyen, Weng, and
  Kalagnanam]{sarahplusplus}
Lam~M. Nguyen, Marten van Dijk, Dzung~T. Phan, Phuong~Ha Nguyen, Tsui-Wei Weng,
  and Jayant~R. Kalagnanam.
\newblock Finite-sum smooth optimization with sarah.
\newblock 2019.

\bibitem[Paquette et~al.(2018)Paquette, Lin, Drusvyatskiy, Mairal, and
  Harchaoui]{pmlr-v84-paquette18a}
Courtney Paquette, Hongzhou Lin, Dmitriy Drusvyatskiy, Julien Mairal, and Zaid
  Harchaoui.
\newblock Catalyst for gradient-based nonconvex optimization.
\newblock In Amos Storkey and Fernando Perez-Cruz, editors, \emph{Proceedings
  of the Twenty-First International Conference on Artificial Intelligence and
  Statistics}, volume~84 of \emph{Proceedings of Machine Learning Research},
  pages 613--622, Playa Blanca, Lanzarote, Canary Islands, 09--11 Apr 2018.
  PMLR.
\newblock URL \url{http://proceedings.mlr.press/v84/paquette18a.html}.

\bibitem[Reddi et~al.(2016)Reddi, Hefny, Sra, P\'{o}cz\'{o}s, and
  Smola]{reddi-nonconvex-svrg}
Sashank~J. Reddi, Ahmed Hefny, Suvrit Sra, Barnab\'{a}s P\'{o}cz\'{o}s, and
  Alex Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In \emph{Proceedings of the 33rd International Conference on
  International Conference on Machine Learning - Volume 48}, ICML'16, 2016.

\bibitem[Schmidt et~al.(2017)Schmidt, Roux, and Bach]{SAG}
Mark Schmidt, Nicolas~Le Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{F. Math. Program.}, 2017.

\bibitem[Shalev-Shwartz and Zhang(2013)]{SDCA}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock \emph{Journal of Machine Learning Research}, 14, 2013.

\bibitem[Shalev-Shwartz and Zhang(2014)]{accel-sdca}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Accelerated proximal stochastic dual coordinate ascent for
  regularized loss minimization.
\newblock \emph{Proceedings of the 31st International Conference on Machine
  Learning}, 2014.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research}, 2014.

\end{thebibliography}
