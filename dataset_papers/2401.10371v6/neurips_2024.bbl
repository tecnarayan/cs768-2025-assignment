\begin{thebibliography}{10}

\bibitem{sekhari2021remember}
A.~Sekhari, J.~Acharya, G.~Kamath, and A.~T. Suresh, ``Remember what you want to forget: Algorithms for machine unlearning,'' {\em Advances in Neural Information Processing Systems}, vol.~34, pp.~18075--18086, 2021.

\bibitem{carlini2019secret}
N.~Carlini, C.~Liu, {\'U}.~Erlingsson, J.~Kos, and D.~Song, ``The secret sharer: Evaluating and testing unintended memorization in neural networks,'' in {\em 28th USENIX Security Symposium (USENIX Security 19)}, pp.~267--284, 2019.

\bibitem{cao2015towards}
Y.~Cao and J.~Yang, ``Towards making systems forget with machine unlearning,'' in {\em 2015 IEEE symposium on security and privacy}, pp.~463--480, IEEE, 2015.

\bibitem{bourtoule2021machine}
L.~Bourtoule, V.~Chandrasekaran, C.~A. Choquette-Choo, H.~Jia, A.~Travers, B.~Zhang, D.~Lie, and N.~Papernot, ``Machine unlearning,'' in {\em 2021 IEEE Symposium on Security and Privacy (SP)}, pp.~141--159, IEEE, 2021.

\bibitem{ullah2021machine}
E.~Ullah, T.~Mai, A.~Rao, R.~A. Rossi, and R.~Arora, ``Machine unlearning via algorithmic stability,'' in {\em Conference on Learning Theory}, pp.~4126--4142, PMLR, 2021.

\bibitem{ullah2023adaptive}
E.~Ullah and R.~Arora, ``From adaptive query release to machine unlearning,'' in {\em International Conference on Machine Learning}, pp.~34642--34667, PMLR, 2023.

\bibitem{guo2020certified}
C.~Guo, T.~Goldstein, A.~Hannun, and L.~Van Der~Maaten, ``Certified data removal from machine learning models,'' in {\em International Conference on Machine Learning}, pp.~3832--3842, PMLR, 2020.

\bibitem{neel2021descent}
S.~Neel, A.~Roth, and S.~Sharifi-Malvajerdi, ``Descent-to-delete: Gradient-based methods for machine unlearning,'' in {\em Algorithmic Learning Theory}, pp.~931--962, PMLR, 2021.

\bibitem{gupta2021adaptive}
V.~Gupta, C.~Jung, S.~Neel, A.~Roth, S.~Sharifi-Malvajerdi, and C.~Waites, ``Adaptive machine unlearning,'' {\em Advances in Neural Information Processing Systems}, vol.~34, pp.~16319--16330, 2021.

\bibitem{chien2022efficient}
E.~Chien, C.~Pan, and O.~Milenkovic, ``Efficient model updates for approximate unlearning of graph-structured data,'' in {\em The Eleventh International Conference on Learning Representations}, 2022.

\bibitem{dwork2006calibrating}
C.~Dwork, F.~McSherry, K.~Nissim, and A.~Smith, ``Calibrating noise to sensitivity in private data analysis,'' in {\em Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3}, pp.~265--284, Springer, 2006.

\bibitem{chourasia2021differential}
R.~Chourasia, J.~Ye, and R.~Shokri, ``Differential privacy dynamics of langevin diffusion and noisy gradient descent,'' {\em Advances in Neural Information Processing Systems}, vol.~34, pp.~14771--14781, 2021.

\bibitem{abadi2016deep}
M.~Abadi, A.~Chu, I.~Goodfellow, H.~B. McMahan, I.~Mironov, K.~Talwar, and L.~Zhang, ``Deep learning with differential privacy,'' in {\em Proceedings of the 2016 ACM SIGSAC conference on computer and communications security}, pp.~308--318, 2016.

\bibitem{kairouz2015composition}
P.~Kairouz, S.~Oh, and P.~Viswanath, ``The composition theorem for differential privacy,'' in {\em International conference on machine learning}, pp.~1376--1385, PMLR, 2015.

\bibitem{vempala2019rapid}
S.~Vempala and A.~Wibisono, ``Rapid convergence of the unadjusted langevin algorithm: Isoperimetry suffices,'' {\em Advances in neural information processing systems}, vol.~32, 2019.

\bibitem{ma2019sampling}
Y.-A. Ma, Y.~Chen, C.~Jin, N.~Flammarion, and M.~I. Jordan, ``Sampling can be faster than optimization,'' {\em Proceedings of the National Academy of Sciences}, vol.~116, no.~42, pp.~20881--20885, 2019.

\bibitem{lamperski2021projected}
A.~Lamperski, ``Projected stochastic gradient langevin algorithms for constrained sampling and non-convex learning,'' in {\em Conference on Learning Theory}, pp.~2891--2937, PMLR, 2021.

\bibitem{ye2022differentially}
J.~Ye and R.~Shokri, ``Differentially private learning needs hidden state (or much faster convergence),'' {\em Advances in Neural Information Processing Systems}, vol.~35, pp.~703--715, 2022.

\bibitem{altschuler2022privacy}
J.~Altschuler and K.~Talwar, ``Privacy of noisy stochastic gradient descent: More iterations without more privacy loss,'' {\em Advances in Neural Information Processing Systems}, vol.~35, pp.~3788--3800, 2022.

\bibitem{gross1975logarithmic}
L.~Gross, ``Logarithmic sobolev inequalities,'' {\em American Journal of Mathematics}, vol.~97, no.~4, pp.~1061--1083, 1975.

\bibitem{chen2021dimension}
H.-B. Chen, S.~Chewi, and J.~Niles-Weed, ``Dimension-free log-sobolev inequalities for mixture distributions,'' {\em Journal of Functional Analysis}, vol.~281, no.~11, p.~109236, 2021.

\bibitem{mironov2017renyi}
I.~Mironov, ``R{\'e}nyi differential privacy,'' in {\em 2017 IEEE 30th computer security foundations symposium (CSF)}, pp.~263--275, IEEE, 2017.

\bibitem{chourasia2023forget}
R.~Chourasia and N.~Shah, ``Forget unlearning: Towards true data-deletion in machine learning,'' in {\em International Conference on Machine Learning}, pp.~6028--6073, PMLR, 2023.

\bibitem{ganesh2020faster}
A.~Ganesh and K.~Talwar, ``Faster differentially private samplers via r{\'e}nyi divergence analysis of discretized langevin mcmc,'' {\em Advances in Neural Information Processing Systems}, vol.~33, pp.~7222--7233, 2020.

\bibitem{ryffel2022differential}
T.~Ryffel, F.~Bach, and D.~Pointcheval, ``Differential privacy guarantees for stochastic gradient langevin dynamics,'' {\em arXiv preprint arXiv:2201.11980}, 2022.

\bibitem{chewi2023logconcave}
{Chewi, Sinho}, ``{Log-Concave Sampling}.'' \url{https://chewisinho.github.io/main.pdf}, 2023.
\newblock Online; accessed September 29, 2023.

\bibitem{feldman2018privacy}
V.~Feldman, I.~Mironov, K.~Talwar, and A.~Thakurta, ``Privacy amplification by iteration,'' in {\em 2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS)}, pp.~521--532, IEEE, 2018.

\bibitem{dalalyan2012sparse}
A.~S. Dalalyan and A.~B. Tsybakov, ``Sparse regression learning by aggregation and langevin monte-carlo,'' {\em Journal of Computer and System Sciences}, vol.~78, no.~5, pp.~1423--1443, 2012.

\bibitem{durmus2017nonasymptotic}
A.~Durmus and E.~Moulines, ``Nonasymptotic convergence analysis for the unadjusted langevin algorithm,'' 2017.

\bibitem{erdogdu2022convergence}
M.~A. Erdogdu, R.~Hosseinzadeh, and S.~Zhang, ``Convergence of langevin monte carlo in chi-squared and r{\'e}nyi divergence,'' in {\em International Conference on Artificial Intelligence and Statistics}, pp.~8151--8175, PMLR, 2022.

\bibitem{mousavi2023towards}
A.~Mousavi-Hosseini, T.~Farghly, Y.~He, K.~Balasubramanian, and M.~A. Erdogdu, ``Towards a complete analysis of langevin monte carlo: Beyond poincar$\backslash$'e inequality,'' {\em arXiv preprint arXiv:2303.03589}, 2023.

\bibitem{altschuler2022resolving}
J.~M. Altschuler and K.~Talwar, ``Resolving the mixing time of the langevin algorithm to its stationary distribution for log-concave sampling,'' {\em arXiv preprint arXiv:2210.08448}, 2022.

\bibitem{meyn2012markov}
S.~P. Meyn and R.~L. Tweedie, {\em Markov chains and stochastic stability}.
\newblock Springer Science \& Business Media, 2012.

\bibitem{deng2012mnist}
L.~Deng, ``The mnist database of handwritten digit images for machine learning research,'' {\em IEEE Signal Processing Magazine}, vol.~29, no.~6, pp.~141--142, 2012.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky {\em et~al.}, ``Learning multiple layers of features from tiny images,'' 2009.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image recognition,'' in {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.~770--778, 2016.

\bibitem{misc_adult_2}
B.~Becker and R.~Kohavi, ``{Adult}.'' UCI Machine Learning Repository, 1996.
\newblock {DOI}: https://doi.org/10.24432/C5XW20.

\bibitem{chien2024stochastic}
E.~Chien, H.~Wang, Z.~Chen, and P.~Li, ``Stochastic gradient langevin unlearning,'' {\em Advances in Neural Information Processing Systems}, 2024.

\bibitem{hastings1970monte}
W.~K. Hastings, ``Monte carlo sampling methods using markov chains and their applications,'' 1970.

\bibitem{neal2011mcmc}
R.~M. Neal {\em et~al.}, ``Mcmc using hamiltonian dynamics,'' {\em Handbook of markov chain monte carlo}, vol.~2, no.~11, p.~2, 2011.

\bibitem{aerni2024evaluations}
M.~Aerni, J.~Zhang, and F.~Tram{\`e}r, ``Evaluations of machine learning privacy defenses are misleading,'' {\em arXiv preprint arXiv:2404.17399}, 2024.

\bibitem{nguyen2020variational}
Q.~P. Nguyen, B.~K.~H. Low, and P.~Jaillet, ``Variational bayesian unlearning,'' {\em Advances in Neural Information Processing Systems}, vol.~33, pp.~16025--16036, 2020.

\bibitem{nguyen2022markov}
Q.~P. Nguyen, R.~Oikawa, D.~M. Divakaran, M.~C. Chan, and B.~K.~H. Low, ``Markov chain monte carlo-based machine unlearning: Unlearning what needs to be forgotten,'' in {\em Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security}, pp.~351--363, 2022.

\bibitem{rawat2022challenges}
A.~Rawat, J.~Requeima, W.~Bruinsma, and R.~Turner, ``Challenges and pitfalls of bayesian unlearning,'' {\em arXiv preprint arXiv:2207.03227}, 2022.

\bibitem{fu2021bayesian}
S.~Fu, F.~He, Y.~Xu, and D.~Tao, ``Bayesian inference forgetting,'' {\em arXiv preprint arXiv:2101.06417}, 2021.

\bibitem{gronwall1919note}
T.~H. Gronwall, ``Note on the derivatives with respect to a parameter of the solutions of a system of differential equations,'' {\em Annals of Mathematics}, pp.~292--296, 1919.

\bibitem{hardt2016train}
M.~Hardt, B.~Recht, and Y.~Singer, ``Train faster, generalize better: Stability of stochastic gradient descent,'' in {\em International conference on machine learning}, pp.~1225--1234, PMLR, 2016.

\bibitem{torchvision}
T.~maintainers and contributors, ``Torchvision: Pytorch's computer vision library.'' \url{https://github.com/pytorch/vision}, 2016.

\bibitem{PyTorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen, Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~Kopf, E.~Yang, Z.~DeVito, M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang, J.~Bai, and S.~Chintala, ``Pytorch: An imperative style, high-performance deep learning library,'' in {\em Advances in Neural Information Processing Systems 32}, pp.~8024--8035, Curran Associates, Inc., 2019.

\bibitem{paszke2017automatic}
A.~Paszke, S.~Gross, S.~Chintala, G.~Chanan, E.~Yang, Z.~DeVito, Z.~Lin, A.~Desmaison, L.~Antiga, and A.~Lerer, ``Automatic differentiation in pytorch,'' 2017.

\bibitem{harris2020array}
C.~R. Harris, K.~J. Millman, S.~J. van~der Walt, R.~Gommers, P.~Virtanen, D.~Cournapeau, E.~Wieser, J.~Taylor, S.~Berg, N.~J. Smith, R.~Kern, M.~Picus, S.~Hoyer, M.~H. van Kerkwijk, M.~Brett, A.~Haldane, J.~F. del R{\'{i}}o, M.~Wiebe, P.~Peterson, P.~G{\'{e}}rard-Marchant, K.~Sheppard, T.~Reddy, W.~Weckesser, H.~Abbasi, C.~Gohlke, and T.~E. Oliphant, ``Array programming with {NumPy},'' 2020.

\bibitem{kairouz2021practical}
P.~Kairouz, B.~McMahan, S.~Song, O.~Thakkar, A.~Thakurta, and Z.~Xu, ``Practical and private (deep) learning without sampling or shuffling,'' in {\em International Conference on Machine Learning}, pp.~5213--5225, PMLR, 2021.

\end{thebibliography}
