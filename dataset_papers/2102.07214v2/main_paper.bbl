\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alistarh \& Korhonen(2020)Alistarh and Korhonen]{alistarh2020improved}
Alistarh, D. and Korhonen, J.~H.
\newblock Improved communication lower bounds for distributed optimisation.
\newblock \emph{arXiv preprint arXiv:2010.08222}, 2020.

\bibitem[Alistarh et~al.(2016)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2016qsgd}
Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock \emph{arXiv preprint arXiv:1610.02132}, 2016.

\bibitem[Alistarh et~al.(2018)Alistarh, Hoefler, Johansson, Khirirat,
  Konstantinov, and Renggli]{alistarh2018convergence}
Alistarh, D., Hoefler, T., Johansson, M., Khirirat, S., Konstantinov, N., and
  Renggli, C.
\newblock The convergence of sparsified gradient methods.
\newblock \emph{arXiv preprint arXiv:1809.10505}, 2018.

\bibitem[Arjevani \& Shamir(2015)Arjevani and Shamir]{NIPS2015_5731}
Arjevani, Y. and Shamir, O.
\newblock Communication complexity of distributed convex learning and
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems 28 (NIPS
  2015)}, pp.\  1756--1764, 2015.

\bibitem[Ben-Nun \& Hoefler(2019)Ben-Nun and Hoefler]{ben2019demystifying}
Ben-Nun, T. and Hoefler, T.
\newblock Demystifying parallel and distributed deep learning: An in-depth
  concurrency analysis.
\newblock \emph{ACM Computing Surveys (CSUR)}, 52\penalty0 (4):\penalty0 1--43,
  2019.

\bibitem[Chang \& Lin(2011)Chang and Lin]{LibSVM}
Chang, C.-C. and Lin, C.-J.
\newblock {LIBSVM}: A library for support vector machines.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology},
  2:\penalty0 27:1--27:27, 2011.
\newblock Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}.

\bibitem[Chen(2019)]{chen2019gradient}
Chen, Y.
\newblock Gradient methods for unconstrained problems.
\newblock
  \url{http://www.princeton.edu/~yc5/ele522_optimization/lectures/grad_descent_unconstrained.pdf},
  2019.
\newblock Princeton University, Fall 2019.

\bibitem[Davies et~al.(2021)Davies, Gurunanthan, Moshrefi, Ashkboos, and
  Alistarh]{davies2021new}
Davies, P., Gurunanthan, V., Moshrefi, N., Ashkboos, S., and Alistarh, D.
\newblock New bounds for distributed mean estimation and variance reduction.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=t86MwoUCCNe}.

\bibitem[Ghadikolaei \& Magn{\'u}sson(2020)Ghadikolaei and
  Magn{\'u}sson]{ghadikolaei2020communication}
Ghadikolaei, H.~S. and Magn{\'u}sson, S.
\newblock Communication-efficient variance-reduced stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:2003.04686}, 2020.

\bibitem[Hendrikx et~al.(2020)Hendrikx, Xiao, Bubeck, Bach, and
  Massoulie]{hendrikx2020statistically}
Hendrikx, H., Xiao, L., Bubeck, S., Bach, F., and Massoulie, L.
\newblock Statistically preconditioned accelerated gradient method for
  distributed optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4203--4227. PMLR, 2020.

\bibitem[Islamov et~al.(2021)Islamov, Qian, and
  Richt{\'a}rik]{islamov2021distributed}
Islamov, R., Qian, X., and Richt{\'a}rik, P.
\newblock Distributed second order methods with fast rates and compressed
  communication.
\newblock \emph{arXiv preprint arXiv:2102.07158. Accepted to ICML 2021.}, 2021.

\bibitem[Jaggi et~al.(2014)Jaggi, Smith, Tak{\'a}{\v{c}}, Terhorst, Krishnan,
  Hofmann, and Jordan]{jaggi2014communication}
Jaggi, M., Smith, V., Tak{\'a}{\v{c}}, M., Terhorst, J., Krishnan, S., Hofmann,
  T., and Jordan, M.~I.
\newblock Communication-efficient distributed dual coordinate ascent.
\newblock \emph{arXiv preprint arXiv:1409.1458}, 2014.

\bibitem[Jordan et~al.(2018)Jordan, Lee, and Yang]{jordan2018communication}
Jordan, M.~I., Lee, J.~D., and Yang, Y.
\newblock Communication-efficient distributed statistical inference.
\newblock \emph{Journal of the American Statistical Association}, 2018.

\bibitem[Khirirat et~al.(2018)Khirirat, Feyzmahdavian, and
  Johansson]{khirirat2018distributed}
Khirirat, S., Feyzmahdavian, H.~R., and Johansson, M.
\newblock Distributed learning with compressed gradients.
\newblock \emph{arXiv preprint arXiv:1806.06573}, 2018.

\bibitem[Li et~al.(2014)Li, Andersen, Park, Smola, Ahmed, Josifovski, Long,
  Shekita, and Su]{PS}
Li, M., Andersen, D.~G., Park, J.~W., Smola, A.~J., Ahmed, A., Josifovski, V.,
  Long, J., Shekita, E.~J., and Su, B.-Y.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In \emph{Proc.\ 11th {USENIX} Symposium on Operating Systems Design
  and Implementation ({OSDI} 2014)}, pp.\  583--598, 2014.

\bibitem[Magn{\'u}sson et~al.(2020)Magn{\'u}sson, Shokri-Ghadikolaei, and
  Li]{magnusson2020maintaining}
Magn{\'u}sson, S., Shokri-Ghadikolaei, H., and Li, N.
\newblock On maintaining linear convergence of distributed learning and
  optimization under limited communication.
\newblock \emph{IEEE Transactions on Signal Processing}, 68:\penalty0
  6101--6116, 2020.

\bibitem[Mendler-D{\"u}nner \& Lucchi(2020)Mendler-D{\"u}nner and
  Lucchi]{mendler2020randomized}
Mendler-D{\"u}nner, C. and Lucchi, A.
\newblock Randomized block-diagonal preconditioning for parallel learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6841--6851. PMLR, 2020.

\bibitem[Nesterov \& Polyak(2006)Nesterov and Polyak]{RePEc:cor:louvrp:1927}
Nesterov, Y. and Polyak, B.
\newblock Cubic regularization of newton method and its global performance.
\newblock LIDAM Reprints CORE 1927, Université catholique de Louvain, Center
  for Operations Research and Econometrics (CORE), 2006.
\newblock URL \url{https://EconPapers.repec.org/RePEc:cor:louvrp:1927}.

\bibitem[Nguyen et~al.(2018)Nguyen, Nguyen, Dijk, Richt{\'a}rik, Scheinberg,
  and Tak{\'a}c]{nguyen2018sgd}
Nguyen, L., Nguyen, P.~H., Dijk, M., Richt{\'a}rik, P., Scheinberg, K., and
  Tak{\'a}c, M.
\newblock Sgd and hogwild! convergence without the bounded gradients
  assumption.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3750--3758. PMLR, 2018.

\bibitem[Niu et~al.(2011)Niu, Recht, R{\'e}, and Wright]{niu2011hogwild}
Niu, F., Recht, B., R{\'e}, C., and Wright, S.~J.
\newblock Hogwild!: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock \emph{arXiv preprint arXiv:1106.5730}, 2011.

\bibitem[Ramezani{-}Kebrya et~al.(2019)Ramezani{-}Kebrya, Faghri, and
  Roy]{NUQSGD}
Ramezani{-}Kebrya, A., Faghri, F., and Roy, D.~M.
\newblock {NUQSGD:} improved communication efficiency for data-parallel {SGD}
  via nonuniform quantization.
\newblock \emph{CoRR}, abs/1908.06077, 2019.
\newblock URL \url{http://arxiv.org/abs/1908.06077}.

\bibitem[Reddi et~al.(2016)Reddi, Kone{\v{c}}n{\`y}, Richt{\'a}rik,
  P{\'o}cz{\'o}s, and Smola]{reddi2016aide}
Reddi, S.~J., Kone{\v{c}}n{\`y}, J., Richt{\'a}rik, P., P{\'o}cz{\'o}s, B., and
  Smola, A.
\newblock Aide: Fast and communication efficient distributed optimization.
\newblock \emph{arXiv preprint arXiv:1608.06879}, 2016.

\bibitem[Safaryan et~al.(2021)Safaryan, Islamov, Qian, and
  Richtárik]{safaryan2021fednl}
Safaryan, M., Islamov, R., Qian, X., and Richtárik, P.
\newblock Fednl: Making newton-type methods applicable to federated learning,
  2021.

\bibitem[Scaman et~al.(2017)Scaman, Bach, Bubeck, Lee, and
  Massouli{\'e}]{scaman2017optimal}
Scaman, K., Bach, F., Bubeck, S., Lee, Y.~T., and Massouli{\'e}, L.
\newblock Optimal algorithms for smooth and strongly convex distributed
  optimization in networks.
\newblock In \emph{international conference on machine learning}, pp.\
  3027--3036. PMLR, 2017.

\bibitem[Shamir(2014)]{NIPS2014_5386}
Shamir, O.
\newblock Fundamental limits of online and distributed algorithms for
  statistical learning and estimation.
\newblock In \emph{Advances in Neural Information Processing Systems 27 (NIPS
  2014)}, pp.\  163--171, 2014.

\bibitem[Shamir et~al.(2014)Shamir, Srebro, and Zhang]{shamir2014communication}
Shamir, O., Srebro, N., and Zhang, T.
\newblock Communication-efficient distributed optimization using an approximate
  newton-type method.
\newblock In \emph{International conference on machine learning}, pp.\
  1000--1008. PMLR, 2014.

\bibitem[Suresh et~al.(2017)Suresh, Yu, Kumar, and McMahan]{pmlr-v70-suresh17a}
Suresh, A.~T., Yu, F.~X., Kumar, S., and McMahan, H.~B.
\newblock Distributed mean estimation with limited communication.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pp.\  3329--3337, 2017.

\bibitem[{Tsitsiklis} \& {Luo}(1986){Tsitsiklis} and {Luo}]{4048825}
{Tsitsiklis}, J.~N. and {Luo}, Z.
\newblock Communication complexity of convex optimization.
\newblock In \emph{1986 25th IEEE Conference on Decision and Control}, pp.\
  608--611, 1986.
\newblock \doi{10.1109/CDC.1986.267379}.

\bibitem[Vempala et~al.(2020)Vempala, Wang, and
  Woodruff]{vempala2020communication}
Vempala, S.~S., Wang, R., and Woodruff, D.~P.
\newblock The communication complexity of optimization.
\newblock In \emph{Proceedings of the Fourteenth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pp.\  1733--1752. SIAM, 2020.

\bibitem[Wang et~al.(2018)Wang, Roosta, Xu, and Mahoney]{wang2018giant}
Wang, S., Roosta, F., Xu, P., and Mahoney, M.~W.
\newblock Giant: Globally improved approximate newton method for distributed
  optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  31:\penalty0 2332--2342, 2018.

\bibitem[Ye \& Abbe(2018)Ye and Abbe]{ye2018communication}
Ye, M. and Abbe, E.
\newblock Communication-computation efficient gradient coding.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5610--5619. PMLR, 2018.

\bibitem[Zhang et~al.(2020)Zhang, You, and Ba{\c{s}}ar]{zhang2020distributed}
Zhang, J., You, K., and Ba{\c{s}}ar, T.
\newblock Distributed adaptive newton methods with globally superlinear
  convergence.
\newblock \emph{arXiv preprint arXiv:2002.07378}, 2020.

\bibitem[Zhang \& Lin(2015)Zhang and Lin]{zhang2015disco}
Zhang, Y. and Lin, X.
\newblock Disco: Distributed optimization for self-concordant empirical loss.
\newblock In \emph{International conference on machine learning}, pp.\
  362--370. PMLR, 2015.

\bibitem[Zhang et~al.(2013)Zhang, Duchi, Jordan, and Wainwright]{NIPS2013_4902}
Zhang, Y., Duchi, J., Jordan, M.~I., and Wainwright, M.~J.
\newblock Information-theoretic lower bounds for distributed statistical
  estimation with communication constraints.
\newblock In \emph{Advances in Neural Information Processing Systems 26 (NIPS
  2013)}, pp.\  2328--2336, 2013.

\end{thebibliography}
