\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahn et~al.(2020)Ahn, Yun, and Sra]{ahn_sgd_2020}
Ahn, K., Yun, C., and Sra, S.
\newblock {{SGD}} with shuffling: Optimal rates without component convexity and
  large epoch requirements.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~33, pp.\  17526--17535. {Curran Associates, Inc.}, 2020.

\bibitem[Bassily et~al.(2014)Bassily, Smith, and
  Thakurta]{bassily_private_2014}
Bassily, R., Smith, A., and Thakurta, A.
\newblock Private empirical risk minimization: Efficient algorithms and tight
  error bounds.
\newblock In \emph{Proceedings of the {{IEEE Annual Symposium}} on
  {{Foundations}} of {{Computer Science}}}, {{FOCS}} '14, pp.\  464--473,
  {USA}, October 2014. {IEEE Computer Society}.

\bibitem[Bietti \& Mairal(2017)Bietti and Mairal]{bietti_stochastic_2017}
Bietti, A. and Mairal, J.
\newblock Stochastic optimization with variance reduction for infinite datasets
  with finite sum structure.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~30, pp.\  1623--1633. {Curran Associates, Inc.}, 2017.

\bibitem[Bottou(1999)]{bottou_online_1999}
Bottou, L.
\newblock On-line learning and stochastic approximations.
\newblock In \emph{On-{{Line Learning}} in {{Neural Networks}}}, pp.\  9--42.
  {Cambridge University Press}, 1 edition, January 1999.

\bibitem[Bottou(2009)]{bottou_curiously_2009}
Bottou, L.
\newblock Curiously fast convergence of some stochastic gradient descent
  algorithms.
\newblock Unpublished open problem at the {{International Symposium}} on
  {{Statistical Learning}} and {{Data Sciences}} ({{SLDS}}), 2009.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and
  Nocedal]{bottou_optimization_2018}
Bottou, L., Curtis, F.~E., and Nocedal, J.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{SIAM Review}, 60\penalty0 (2):\penalty0 223--311, January 2018.

\bibitem[Cha et~al.(2023)Cha, Lee, and Yun]{cha_tighter_2023}
Cha, J., Lee, J., and Yun, C.
\newblock Tighter lower bounds for shuffling {{SGD}}: Random permutations and
  beyond.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine
  Learning}}}, volume 202 of \emph{{{PMLR}}}, pp.\  3855--3912. {JMLR}, July
  2023.

\bibitem[Csiba \& Richt{\'a}rik(2018)Csiba and
  Richt{\'a}rik]{csiba_importance_2018}
Csiba, D. and Richt{\'a}rik, P.
\newblock Importance sampling for minibatches.
\newblock \emph{Journal of Machine Learning Research}, 19\penalty0
  (27):\penalty0 1--21, 2018.

\bibitem[Dai et~al.(2014)Dai, Xie, He, Liang, Raj, Balcan, and
  Song]{dai_scalable_2014}
Dai, B., Xie, B., He, N., Liang, Y., Raj, A., Balcan, M.-F.~F., and Song, L.
\newblock Scalable kernel methods via doubly stochastic gradients.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~27, pp.\  3041--3049. {Curran Associates, Inc.}, 2014.

\bibitem[Domke(2019)]{domke_provable_2019}
Domke, J.
\newblock Provable gradient variance guarantees for black-box variational
  inference.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~32, pp.\  329--338. {Curran Associates, Inc.}, 2019.

\bibitem[Domke(2020)]{domke_provable_2020}
Domke, J.
\newblock Provable smoothness guarantees for black-box variational inference.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, volume 119 of \emph{{{PMLR}}}, pp.\  2587--2596. {JMLR}, July
  2020.

\bibitem[Domke et~al.(2023)Domke, Gower, and Garrigos]{domke_provable_2023}
Domke, J., Gower, R., and Garrigos, G.
\newblock Provable convergence guarantees for black-box variational inference.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~36, pp.\  66289--66327. {Curran Associates, Inc.}, 2023.

\bibitem[Duchi et~al.(2012)Duchi, Bartlett, and
  Wainwright]{duchi_randomized_2012}
Duchi, J.~C., Bartlett, P.~L., and Wainwright, M.~J.
\newblock Randomized smoothing for stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0
  674--701, January 2012.

\bibitem[Garrigos \& Gower(2023)Garrigos and Gower]{garrigos_handbook_2023}
Garrigos, G. and Gower, R.~M.
\newblock Handbook of convergence theorems for (stochastic) gradient methods.
\newblock Preprint arXiv:2301.11235, {arXiv}, February 2023.

\bibitem[Gorbunov et~al.(2020)Gorbunov, Hanzely, and
  Richtarik]{gorbunov_unified_2020}
Gorbunov, E., Hanzely, F., and Richtarik, P.
\newblock A unified theory of {{SGD}}: {{Variance}} reduction, sampling,
  quantization and coordinate descent.
\newblock In \emph{Proceedings of the {{International Conference}} on
  {{Artificial Intelligence}} and {{Statistics}}}, volume 108 of
  \emph{{{PMLR}}}, pp.\  680--690. {JMLR}, June 2020.

\bibitem[Gower et~al.(2021{\natexlab{a}})Gower, Sebbouh, and
  Loizou]{gower_sgd_2021}
Gower, R., Sebbouh, O., and Loizou, N.
\newblock {{SGD}} for structured nonconvex functions: {{Learning}} rates,
  minibatching and interpolation.
\newblock In \emph{Proceedings of the {{International Conference}} on
  {{Artificial Intelligence}} and {{Statistics}}}, volume 130 of
  \emph{{{PMLR}}}, pp.\  1315--1323. {JMLR}, March 2021{\natexlab{a}}.

\bibitem[Gower et~al.(2019)Gower, Loizou, Qian, Sailanbayev, Shulgin, and
  Richt{\'a}rik]{gower_sgd_2019}
Gower, R.~M., Loizou, N., Qian, X., Sailanbayev, A., Shulgin, E., and
  Richt{\'a}rik, P.
\newblock {{SGD}}: {{General}} analysis and improved rates.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, volume~97 of \emph{{{PMLR}}}, pp.\  5200--5209. {JMLR}, June 2019.

\bibitem[Gower et~al.(2020)Gower, Schmidt, Bach, and
  Richtarik]{gower_variancereduced_2020}
Gower, R.~M., Schmidt, M., Bach, F., and Richtarik, P.
\newblock Variance-reduced methods for machine learning.
\newblock \emph{Proceedings of the IEEE}, 108\penalty0 (11):\penalty0
  1968--1983, November 2020.

\bibitem[Gower et~al.(2021{\natexlab{b}})Gower, Richt{\'a}rik, and
  Bach]{gower_stochastic_2021}
Gower, R.~M., Richt{\'a}rik, P., and Bach, F.
\newblock Stochastic quasi-gradient methods: {{Variance}} reduction via
  {{Jacobian}} sketching.
\newblock \emph{Mathematical Programming}, 188\penalty0 (1):\penalty0 135--192,
  July 2021{\natexlab{b}}.

\bibitem[Guminov et~al.(2023)Guminov, Gasnikov, and
  Kuruzov]{guminov_accelerated_2023}
Guminov, S., Gasnikov, A., and Kuruzov, I.
\newblock Accelerated methods for weakly-quasi-convex optimization problems.
\newblock \emph{Computational Management Science}, 20\penalty0 (1):\penalty0
  36, December 2023.

\bibitem[G{\"u}rb{\"u}zbalaban et~al.(2021)G{\"u}rb{\"u}zbalaban, Ozdaglar, and
  Parrilo]{gurbuzbalaban_why_2021}
G{\"u}rb{\"u}zbalaban, M., Ozdaglar, A., and Parrilo, P.~A.
\newblock Why random reshuffling beats stochastic gradient descent.
\newblock \emph{Mathematical Programming}, 186\penalty0 (1):\penalty0 49--84,
  March 2021.

\bibitem[Haochen \& Sra(2019)Haochen and Sra]{haochen_random_2019}
Haochen, J. and Sra, S.
\newblock Random shuffling beats {{SGD}} after finite epochs.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine
  Learning}}}, volume~97 of \emph{{{PMLR}}}, pp.\  2624--2633. {JMLR}, May
  2019.

\bibitem[Hinder et~al.(2020)Hinder, Sidford, and
  Sohoni]{hinder_nearoptimal_2020}
Hinder, O., Sidford, A., and Sohoni, N.
\newblock Near-optimal methods for minimizing star-convex functions and beyond.
\newblock In \emph{Proceedings of {{Conference}} on {{Learning Theory}}},
  volume 125 of \emph{{{PMLR}}}, pp.\  1894--1938. {JMLR}, July 2020.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho_denoising_2020}
Ho, J., Jain, A., and Abbeel, P.
\newblock Denoising diffusion probabilistic models.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~33, pp.\  6840--6851. {Curran Associates, Inc.}, 2020.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{johnson_accelerating_2013}
Johnson, R. and Zhang, T.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~26, pp.\  315--323. {Curran Associates, Inc.}, 2013.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{karimi_linear_2016}
Karimi, H., Nutini, J., and Schmidt, M.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the {{Polyak-{\L}ojasiewicz}} condition.
\newblock In \emph{Machine {{Learning}} and {{Knowledge Discovery}} in
  {{Databases}}}, Lecture {{Notes}} in {{Computer Science}}, pp.\  795--811,
  {Cham}, 2016. {Springer International Publishing}.

\bibitem[Khaled \& Richt{\'a}rik(2023)Khaled and
  Richt{\'a}rik]{khaled_better_2023}
Khaled, A. and Richt{\'a}rik, P.
\newblock Better theory for {{SGD}} in the nonconvex world.
\newblock \emph{Transactions of Machine Learning Research}, 2023.

\bibitem[Kim et~al.(2023)Kim, Oh, Wu, Ma, and Gardner]{kim_convergence_2023}
Kim, K., Oh, J., Wu, K., Ma, Y., and Gardner, J.~R.
\newblock On the convergence of black-box variational inference.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~36, pp.\  44615--44657, {New Orleans, LA, USA}, December 2023. {Curran
  Associates Inc.}

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{kingma_autoencoding_2014}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational {{Bayes}}.
\newblock In \emph{Proceedings of the {{International Conference}} on
  {{Learning Representations}}}, {Banff, AB, Canada}, April 2014.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and
  Welling]{kingma_variational_2015}
Kingma, D.~P., Salimans, T., and Welling, M.
\newblock Variational dropout and the local reparameterization trick.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~28, pp.\  2575--2583. {Curran Associates, Inc.}, 2015.

\bibitem[Kucukelbir et~al.(2017)Kucukelbir, Tran, Ranganath, Gelman, and
  Blei]{kucukelbir_automatic_2017}
Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., and Blei, D.~M.
\newblock Automatic differentiation variational inference.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (14):\penalty0 1--45, 2017.

\bibitem[Kulunchakov \& Mairal(2020)Kulunchakov and
  Mairal]{kulunchakov_estimate_2020}
Kulunchakov, A. and Mairal, J.
\newblock Estimate sequences for stochastic composite optimization: Variance
  reduction, acceleration, and robustness to noise.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (155):\penalty0 1--52, 2020.

\bibitem[Liu et~al.(2021)Liu, Li, Wei, Zhou, and Zhao]{liu_noisy_2021a}
Liu, T., Li, Y., Wei, S., Zhou, E., and Zhao, T.
\newblock Noisy gradient descent converges to flat minima for nonconvex matrix
  factorization.
\newblock In \emph{Proceedings of the {{International Conference}} on
  {{Artificial Intelligence}} and {{Statistics}}}, volume 130 of
  \emph{{{PMLR}}}, pp.\  1891--1899. {JMLR}, March 2021.

\bibitem[Ma et~al.(2018)Ma, Bassily, and Belkin]{ma_power_2018}
Ma, S., Bassily, R., and Belkin, M.
\newblock The power of interpolation: {{Understanding}} the effectiveness of
  {{SGD}} in modern over-parametrized learning.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine
  Learning}}}, volume~80 of \emph{{{PMLR}}}, pp.\  3325--3334. {JMLR}, July
  2018.

\bibitem[Mishchenko et~al.(2020)Mishchenko, Khaled, and
  Richtarik]{mishchenko_random_2020}
Mishchenko, K., Khaled, A., and Richtarik, P.
\newblock Random reshuffling: Simple analysis with vast improvements.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~33, pp.\  17309--17320. {Curran Associates, Inc.}, 2020.

\bibitem[Mohamed et~al.(2020)Mohamed, Rosca, Figurnov, and
  Mnih]{mohamed_monte_2020}
Mohamed, S., Rosca, M., Figurnov, M., and Mnih, A.
\newblock Monte {{Carlo}} gradient estimation in machine learning.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (132):\penalty0 1--62, 2020.

\bibitem[Moulines \& Bach(2011)Moulines and Bach]{moulines_nonasymptotic_2011}
Moulines, E. and Bach, F.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~24, pp.\  451--459. {Curran Associates, Inc.}, 2011.

\bibitem[Needell \& Ward(2017)Needell and Ward]{needell_batched_2017}
Needell, D. and Ward, R.
\newblock Batched stochastic gradient descent with weighted sampling.
\newblock In \emph{Approximation {{Theory XV}}: {{San Antonio}} 2016}, Springer
  {{Proceedings}} in {{Mathematics}} \& {{Statistics}}, pp.\  279--306, {Cham},
  2017. {Springer International Publishing}.

\bibitem[Needell et~al.(2016)Needell, Srebro, and
  Ward]{needell_stochastic_2016}
Needell, D., Srebro, N., and Ward, R.
\newblock Stochastic gradient descent, weighted sampling, and the randomized
  {{Kaczmarz}} algorithm.
\newblock \emph{Mathematical Programming}, 155\penalty0 (1):\penalty0 549--573,
  January 2016.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovski_robust_2009}
Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on Optimization}, 19\penalty0 (4):\penalty0
  1574--1609, January 2009.

\bibitem[Nesterov \& Polyak(2006)Nesterov and Polyak]{nesterov_cubic_2006}
Nesterov, Y. and Polyak, B.
\newblock Cubic regularization of {{Newton}} method and its global performance.
\newblock \emph{Mathematical Programming}, 108\penalty0 (1):\penalty0 177--205,
  August 2006.

\bibitem[Nesterov(2005)]{nesterov_smooth_2005}
Nesterov, {\relax Yu}.
\newblock Smooth minimization of non-smooth functions.
\newblock \emph{Mathematical Programming}, 103\penalty0 (1):\penalty0 127--152,
  May 2005.

\bibitem[Nguyen et~al.(2018)Nguyen, Nguyen, {van Dijk}, Richtarik, Scheinberg,
  and Takac]{nguyen_sgd_2018}
Nguyen, L., Nguyen, P.~H., {van Dijk}, M., Richtarik, P., Scheinberg, K., and
  Takac, M.
\newblock {{SGD}} and {{Hogwild}}! {{Convergence}} without the bounded
  gradients assumption.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, volume~80 of \emph{{{PMLR}}}, pp.\  3750--3758. {JMLR}, July 2018.

\bibitem[Nguyen et~al.(2021)Nguyen, {Tran-Dinh}, Phan, Nguyen, and
  Van~Dijk]{nguyen_unified_2021}
Nguyen, L.~M., {Tran-Dinh}, Q., Phan, D.~T., Nguyen, P.~H., and Van~Dijk, M.
\newblock A unified convergence analysis for shuffling-type gradient methods.
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0
  (1):\penalty0 207:9397--207:9440, January 2021.

\bibitem[Orvieto et~al.(2023)Orvieto, Raj, Kersting, and
  Bach]{orvieto_explicit_2023}
Orvieto, A., Raj, A., Kersting, H., and Bach, F.
\newblock Explicit regularization in overparametrized models via noise
  injection.
\newblock In \emph{Proceedings of the {{International Conference}} on
  {{Artificial Intelligence}} and {{Statistics}}}, volume 206 of
  \emph{{{PMLR}}}, pp.\  7265--7287. {JMLR}, April 2023.

\bibitem[Polyak \& {d Aleksandr Borisovich}(1990)Polyak and {d Aleksandr
  Borisovich}]{polyak_optimal_1990}
Polyak, B.~T. and {d Aleksandr Borisovich}, T.
\newblock Optimal order of accuracy of search algorithms in stochastic
  optimization.
\newblock \emph{Problemy Peredachi Informatsii}, 26\penalty0 (2):\penalty0
  45--53, 1990.

\bibitem[Ranganath et~al.(2014)Ranganath, Gerrish, and
  Blei]{ranganath_black_2014}
Ranganath, R., Gerrish, S., and Blei, D.
\newblock Black box variational inference.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics}, volume~33 of \emph{{{PMLR}}}, pp.\  814--822.
  {JMLR}, April 2014.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende_stochastic_2014}
Rezende, D.~J., Mohamed, S., and Wierstra, D.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine
  Learning}}}, volume~32 of \emph{{{PMLR}}}, pp.\  1278--1286. {JMLR}, June
  2014.

\bibitem[Richt{\'a}rik \& Tak{\'a}{\v c}(2016)Richt{\'a}rik and Tak{\'a}{\v
  c}]{richtarik_parallel_2016}
Richt{\'a}rik, P. and Tak{\'a}{\v c}, M.
\newblock Parallel coordinate descent methods for big data optimization.
\newblock \emph{Mathematical Programming}, 156\penalty0 (1-2):\penalty0
  433--484, March 2016.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{robbins_stochastic_1951}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, 22\penalty0
  (3):\penalty0 400--407, September 1951.

\bibitem[Safran \& Shamir(2020)Safran and Shamir]{safran_how_2020}
Safran, I. and Shamir, O.
\newblock How good is {{SGD}} with random shuffling?
\newblock In \emph{Proceedings of {{Conference}} on {{Learning Theory}}},
  volume 125 of \emph{{{PMLR}}}, pp.\  3250--3284. {JMLR}, July 2020.

\bibitem[Safran \& Shamir(2021)Safran and Shamir]{safran_random_2021}
Safran, I. and Shamir, O.
\newblock Random shuffling beats {{SGD}} only after many epochs on
  ill-conditioned problems.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~34, pp.\  15151--15161. {Curran Associates, Inc.}, 2021.

\bibitem[Schmidt \& Roux(2013)Schmidt and Roux]{schmidt_fast_2013}
Schmidt, M. and Roux, N.~L.
\newblock Fast convergence of stochastic gradient descent under a strong growth
  condition.
\newblock {{arXiv}} Preprint arXiv:1308.6370, {arXiv}, August 2013.

\bibitem[{Shalev-Shwartz} et~al.(2009){Shalev-Shwartz}, Shamir, Srebro, and
  Sridharan]{shalev-shwartz_stochastic_2009}
{Shalev-Shwartz}, S., Shamir, O., Srebro, N., and Sridharan, K.
\newblock Stochastic convex optimization.
\newblock In \emph{Proceedings of the Conference on {{Computational}} Learning
  Theory}, June 2009.

\bibitem[{Shalev-Shwartz} et~al.(2011){Shalev-Shwartz}, Singer, Srebro, and
  Cotter]{shalev-shwartz_pegasos_2011}
{Shalev-Shwartz}, S., Singer, Y., Srebro, N., and Cotter, A.
\newblock Pegasos: Primal estimated sub-gradient solver for {{SVM}}.
\newblock \emph{Mathematical Programming}, 127\penalty0 (1):\penalty0 3--30,
  March 2011.

\bibitem[Shi et~al.(2021)Shi, Gu, Li, Deng, and Huang]{shi_triply_2021}
Shi, W., Gu, B., Li, X., Deng, C., and Huang, H.
\newblock Triply stochastic gradient method for large-scale nonlinear similar
  unlabeled classification.
\newblock \emph{Machine Learning}, 110\penalty0 (8):\penalty0 2005--2033,
  August 2021.

\bibitem[{Sohl-Dickstein} et~al.(2015){Sohl-Dickstein}, Weiss, Maheswaranathan,
  and Ganguli]{sohl-dickstein_deep_2015}
{Sohl-Dickstein}, J., Weiss, E., Maheswaranathan, N., and Ganguli, S.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine
  Learning}}}, volume~37 of \emph{{{PMLR}}}, pp.\  2256--2265. {JMLR}, June
  2015.

\bibitem[Song et~al.(2013)Song, Chaudhuri, and Sarwate]{song_stochastic_2013}
Song, S., Chaudhuri, K., and Sarwate, A.~D.
\newblock Stochastic gradient descent with differentially private updates.
\newblock In \emph{Proceedings of the {{IEEE Global Conference}} on {{Signal}}
  and {{Information Processing}}}, pp.\  245--248, {Austin, TX, USA}, December
  2013. {IEEE}.

\bibitem[Song \& Ermon(2019)Song and Ermon]{song_generative_2019}
Song, Y. and Ermon, S.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~32, pp.\  11918--11930. {Curran Associates, Inc.}, 2019.

\bibitem[Stich(2019)]{stich_unified_2019}
Stich, S.~U.
\newblock Unified optimal analysis of the (stochastic) gradient method.
\newblock Preprint arXiv:1907.04232, {arXiv}, December 2019.

\bibitem[Titsias \& {L{\'a}zaro-Gredilla}(2014)Titsias and
  {L{\'a}zaro-Gredilla}]{titsias_doubly_2014}
Titsias, M. and {L{\'a}zaro-Gredilla}, M.
\newblock Doubly stochastic variational {{Bayes}} for non-conjugate inference.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine
  Learning}}}, volume~32 of \emph{{{PMLR}}}, pp.\  1971--1979. {JMLR}, June
  2014.

\bibitem[Vapnik(1991)]{vapnik_principles_1991}
Vapnik, V.
\newblock Principles of risk minimization for learning theory.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~4, pp.\  831--838. {Morgan-Kaufmann}, 1991.

\bibitem[Vaswani et~al.(2019)Vaswani, Bach, and Schmidt]{vaswani_fast_2019}
Vaswani, S., Bach, F., and Schmidt, M.
\newblock Fast and faster convergence of {{SGD}} for over-parameterized models
  and an accelerated perceptron.
\newblock In \emph{Proceedings of the {{International Conference}} on
  {{Artificial Intelligence}} and {{Statistics}}}, volume~89 of
  \emph{{{PMLR}}}, pp.\  1195--1204. {JMLR}, April 2019.

\bibitem[Wright \& Recht(2021)Wright and Recht]{wright_optimization_2021}
Wright, S.~J. and Recht, B.
\newblock \emph{Optimization for Data Analysis}.
\newblock {Cambridge University Press}, {New York}, 2021.

\bibitem[Xie et~al.(2015)Xie, Liang, and Song]{xie_scale_2015}
Xie, B., Liang, Y., and Song, L.
\newblock Scale up nonlinear component analysis with doubly stochastic
  gradients.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~28, pp.\  2341--2349. {Curran Associates, Inc.}, 2015.

\bibitem[Xu et~al.(2019)Xu, Quiroz, Kohn, and Sisson]{xu_variance_2019}
Xu, M., Quiroz, M., Kohn, R., and Sisson, S.~A.
\newblock Variance reduction properties of the reparameterization trick.
\newblock In \emph{Proceedings of the {{International Conference}} on
  {{Artificial Intelligence}} and {{Statistics}}}, volume~89 of
  \emph{{{PMLR}}}, pp.\  2711--2720. {JMLR}, April 2019.

\bibitem[Zheng \& Kwok(2018)Zheng and Kwok]{zheng_lightweight_2018}
Zheng, S. and Kwok, J. T.-Y.
\newblock Lightweight stochastic optimization for minimizing finite sums with
  infinite data.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine
  Learning}}}, volume~80 of \emph{{{PMLR}}}, pp.\  5932--5940. {JMLR}, July
  2018.

\end{thebibliography}
