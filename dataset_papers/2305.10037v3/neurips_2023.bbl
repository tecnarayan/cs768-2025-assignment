\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Huang et~al.(2022)Huang, Abbeel, Pathak, and
  Mordatch]{huang2022language}
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
\newblock Language models as zero-shot planners: Extracting actionable
  knowledge for embodied agents.
\newblock In \emph{International Conference on Machine Learning}, pages
  9118--9147. PMLR, 2022.

\bibitem[Andreas(2022)]{andreas-2022-language}
Jacob Andreas.
\newblock Language models as agent models.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2022}, pages 5769--5779, Abu Dhabi, United Arab Emirates, December
  2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.findings-emnlp.423}.

\bibitem[Adhikari et~al.(2020)Adhikari, Yuan, C{\^o}t{\'e}, Zelinka, Rondeau,
  Laroche, Poupart, Tang, Trischler, and Hamilton]{adhikari2020learning}
Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre C{\^o}t{\'e}, Mikul{\'a}{\v{s}}
  Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang,
  Adam Trischler, and Will Hamilton.
\newblock Learning dynamic belief graphs to generalize on text-based games.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3045--3057, 2020.

\bibitem[Ammanabrolu and Riedl(2021)]{ammanabrolu2021learning}
Prithviraj Ammanabrolu and Mark Riedl.
\newblock Learning knowledge graph-based world models of textual environments.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan,
  editors, \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=o24k_XfIe6_}.

\bibitem[Tandon et~al.(2019)Tandon, Dalvi, Sakaguchi, Clark, and
  Bosselut]{tandon-etal-2019-wiqa}
Niket Tandon, Bhavana Dalvi, Keisuke Sakaguchi, Peter Clark, and Antoine
  Bosselut.
\newblock {WIQA}: A dataset for {``}what if...{''} reasoning over procedural
  text.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 6076--6085, Hong Kong,
  China, November 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-1629}.
\newblock URL \url{https://aclanthology.org/D19-1629}.

\bibitem[Madaan et~al.(2022)Madaan, Zhou, Alon, Yang, and
  Neubig]{madaan-etal-2022-language}
Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig.
\newblock Language models of code are few-shot commonsense learners.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 1384--1403, Abu Dhabi, United Arab
  Emirates, December 2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.90}.

\bibitem[Creswell et~al.(2023)Creswell, Shanahan, and
  Higgins]{creswell2023selectioninference}
Antonia Creswell, Murray Shanahan, and Irina Higgins.
\newblock Selection-inference: Exploiting large language models for
  interpretable logical reasoning.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=3Pf3Wg6o-A4}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 27730--27744, 2022.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz,
  Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
  Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg,
  et~al.
\newblock Sparks of artificial general intelligence: Early experiments with
  gpt-4.
\newblock \emph{arXiv preprint arXiv:2303.12712}, 2023.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, brian ichter, Xia, Chi,
  Le, and Zhou]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia,
  Ed~H. Chi, Quoc~V Le, and Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language
  models.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=_VjQlMeSB_J}.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and
  Iwasawa]{kojima2022large}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
  Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=e2TBb5y0yFf}.

\bibitem[Zhou et~al.(2023)Zhou, Sch{\"a}rli, Hou, Wei, Scales, Wang,
  Schuurmans, Cui, Bousquet, Le, and Chi]{zhou2023leasttomost}
Denny Zhou, Nathanael Sch{\"a}rli, Le~Hou, Jason Wei, Nathan Scales, Xuezhi
  Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc~V Le, and Ed~H.
  Chi.
\newblock Least-to-most prompting enables complex reasoning in large language
  models.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=WZH7099tgfM}.

\bibitem[Wang et~al.(2023)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery,
  and Zhou]{wang2023selfconsistency}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc~V Le, Ed~H. Chi, Sharan Narang,
  Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language
  models.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=1PL1NIMMrw}.

\bibitem[Patel and Pavlick(2022)]{patel2022mapping}
Roma Patel and Ellie Pavlick.
\newblock Mapping language models to grounded conceptual spaces.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=gJcEM8sxHK}.

\bibitem[Patel et~al.(2021)Patel, Bhattamishra, and Goyal]{patel-etal-2021-nlp}
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
\newblock Are {NLP} models really able to solve simple math word problems?
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 2080--2094, Online, June 2021. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.168}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.168}.

\bibitem[Sun et~al.(2023)Sun, Wang, Tay, Yang, and
  Zhou]{sun2023recitationaugmented}
Zhiqing Sun, Xuezhi Wang, Yi~Tay, Yiming Yang, and Denny Zhou.
\newblock Recitation-augmented language models.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=-cqvvvb-NkI}.

\bibitem[Valmeekam et~al.(2022)Valmeekam, Olmo, Sreedharan, and
  Kambhampati]{valmeekam2022large}
Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati.
\newblock Large language models still can't plan (a benchmark for {LLM}s on
  planning and reasoning about change).
\newblock In \emph{NeurIPS 2022 Foundation Models for Decision Making
  Workshop}, 2022.
\newblock URL \url{https://openreview.net/forum?id=wUU-7XTL5XO}.

\bibitem[Sclar et~al.(2023)Sclar, Kumar, West, Suhr, Choi, and
  Tsvetkov]{sclar-etal-2023-minding}
Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, and Yulia
  Tsvetkov.
\newblock Minding language models{'} (lack of) theory of mind: A plug-and-play
  multi-character belief tracker.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 13960--13980,
  Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.780}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.780}.

\bibitem[Madaan et~al.(2021)Madaan, Rajagopal, Tandon, Yang, and
  Hovy]{madaan-etal-2021-give}
Aman Madaan, Dheeraj Rajagopal, Niket Tandon, Yiming Yang, and Eduard Hovy.
\newblock Could you give me a hint ? generating inference graphs for defeasible
  reasoning.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL-IJCNLP 2021}, pages 5138--5147, Online, August 2021. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2021.findings-acl.456}.
\newblock URL \url{https://aclanthology.org/2021.findings-acl.456}.

\bibitem[Saha et~al.(2021)Saha, Yadav, Bauer, and
  Bansal]{saha-etal-2021-explagraphs}
Swarnadeep Saha, Prateek Yadav, Lisa Bauer, and Mohit Bansal.
\newblock {E}xpla{G}raphs: An explanation graph generation task for structured
  commonsense reasoning.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 7716--7740, Online and Punta Cana,
  Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.609}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.609}.

\bibitem[Yu et~al.(2022)Yu, Min, Zettlemoyer, and Hajishirzi]{yu2022crepe}
Xinyan~Velocity Yu, Sewon Min, Luke Zettlemoyer, and Hannaneh Hajishirzi.
\newblock Crepe: Open-domain question answering with false presuppositions.
\newblock \emph{arXiv preprint arXiv:2211.17257}, 2022.
\newblock URL \url{https://arxiv.org/abs/2211.17257}.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Bosselut, Yasunaga, Ren, Liang,
  Manning, and Leskovec]{zhang2022greaselm}
Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang,
  Christopher~D Manning, and Jure Leskovec.
\newblock Grease{LM}: Graph {REAS}oning enhanced language models.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=41e9o6cQPj}.

\bibitem[He et~al.(2021)He, Cho, and Glass]{he2021empirical}
Tianxing He, Kyunghyun Cho, and James Glass.
\newblock An empirical study on few-shot knowledge probing for pretrained
  language models, 2021.

\bibitem[Chen et~al.(2023)Chen, Mao, Li, Jin, Wen, Wei, Wang, Yin, Fan, Liu,
  et~al.]{chen2023exploring}
Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang
  Wang, Dawei Yin, Wenqi Fan, Hui Liu, et~al.
\newblock Exploring the potential of large language models (llms) in learning
  on graphs.
\newblock \emph{arXiv preprint arXiv:2307.03393}, 2023.

\bibitem[Ling et~al.(2017)Ling, Yogatama, Dyer, and
  Blunsom]{ling-etal-2017-program}
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom.
\newblock Program induction by rationale generation: Learning to solve and
  explain algebraic word problems.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 158--167,
  Vancouver, Canada, July 2017. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P17-1015}.
\newblock URL \url{https://aclanthology.org/P17-1015}.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[He-Yueya et~al.(2023)He-Yueya, Poesia, Wang, and
  Goodman]{he2023solving}
Joy He-Yueya, Gabriel Poesia, Rose~E Wang, and Noah~D Goodman.
\newblock Solving math word problems by combining language models with symbolic
  solvers.
\newblock \emph{arXiv preprint arXiv:2304.09102}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Shi et~al.(2023)Shi, Chen, Misra, Scales, Dohan, Chi, Sch{\"a}rli, and
  Zhou]{shi2023large}
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed~H Chi,
  Nathanael Sch{\"a}rli, and Denny Zhou.
\newblock Large language models can be easily distracted by irrelevant context.
\newblock In \emph{International Conference on Machine Learning}, pages
  31210--31227. PMLR, 2023.

\bibitem[Welleck et~al.(2021)Welleck, Liu, Bras, Hajishirzi, Choi, and
  Cho]{welleck2021naturalproofs}
Sean Welleck, Jiacheng Liu, Ronan~Le Bras, Hannaneh Hajishirzi, Yejin Choi, and
  Kyunghyun Cho.
\newblock Naturalproofs: Mathematical theorem proving in natural language.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 1)}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Jvxa8adr3iY}.

\bibitem[Welleck et~al.(2022)Welleck, Liu, Lu, Hajishirzi, and
  Choi]{welleck2022naturalprover}
Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi.
\newblock Naturalprover: Grounded mathematical proof generation with language
  models.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=rhdfTOiXBng}.

\bibitem[Srivastava et~al.(2023)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch,
  Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2023beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar
  Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a}
  Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the
  capabilities of language models.
\newblock \emph{Transactions on Machine Learning Research}, 2023.

\bibitem[Talmor et~al.(2019)Talmor, Herzig, Lourie, and
  Berant]{talmor-etal-2019-commonsenseqa}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.
\newblock {C}ommonsense{QA}: A question answering challenge targeting
  commonsense knowledge.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4149--4158,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1421}.
\newblock URL \url{https://aclanthology.org/N19-1421}.

\bibitem[Geva et~al.(2021)Geva, Khashabi, Segal, Khot, Roth, and
  Berant]{geva2021strategyqa}
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan
  Berant.
\newblock {Did Aristotle Use a Laptop? A Question Answering Benchmark with
  Implicit Reasoning Strategies}.
\newblock \emph{Transactions of the Association for Computational Linguistics
  (TACL)}, 2021.

\bibitem[Zhou et~al.(2022)Zhou, Nova, Larochelle, Courville, Neyshabur, and
  Sedghi]{zhou2022teaching}
Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur,
  and Hanie Sedghi.
\newblock Teaching algorithmic reasoning via in-context learning.
\newblock \emph{arXiv preprint arXiv:2211.09066}, 2022.

\bibitem[Lee and Kim(2023)]{lee-kim-2023-recursion}
Soochan Lee and Gunhee Kim.
\newblock Recursion of thought: A divide-and-conquer approach to multi-context
  reasoning with language models.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL 2023}, pages 623--658, Toronto, Canada, July 2023. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2023.findings-acl.40}.
\newblock URL \url{https://aclanthology.org/2023.findings-acl.40}.

\bibitem[Zelikman et~al.(2023)Zelikman, Huang, Poesia, Goodman, and
  Haber]{zelikman2023parsel}
Eric Zelikman, Qian Huang, Gabriel Poesia, Noah~D Goodman, and Nick Haber.
\newblock Parsel: A (de-) compositional framework for algorithmic reasoning
  with language models.
\newblock \emph{arXiv preprint arXiv:2212.10561}, 2023.

\bibitem[Liu et~al.(2023)Liu, Lu, Chen, Jiang, Svyatkovskiy, Fu, Sundaresan,
  and Duan]{liu-etal-2023-code}
Chenxiao Liu, Shuai Lu, Weizhu Chen, Daxin Jiang, Alexey Svyatkovskiy, Shengyu
  Fu, Neel Sundaresan, and Nan Duan.
\newblock Code execution with pre-trained language models.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL 2023}, pages 4984--4999, Toronto, Canada, July 2023. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2023.findings-acl.308}.
\newblock URL \url{https://aclanthology.org/2023.findings-acl.308}.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Roller, Goyal, Artetxe, Chen,
  Chen, Dewan, Diab, Li, Lin, et~al.]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022{\natexlab{b}}.

\end{thebibliography}
