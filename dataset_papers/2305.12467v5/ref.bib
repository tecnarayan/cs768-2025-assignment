@article{vardi2022margin,
  title={On margin maximization in linear and relu networks},
  author={Vardi, Gal and Shamir, Ohad and Srebro, Nati},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={37024--37036},
  year={2022}
}
@article{mulayoff2021implicit,
  title={The implicit bias of minima stability: A view from function space},
  author={Mulayoff, Rotem and Michaeli, Tomer and Soudry, Daniel},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17749--17761},
  year={2021}
}
@article{vardi2023implicit,
  title={On the implicit bias in deep-learning algorithms},
  author={Vardi, Gal},
  journal={Communications of the ACM},
  volume={66},
  number={6},
  pages={86--93},
  year={2023},
  publisher={ACM New York, NY, USA}
}
@inproceedings{keskar2016large,
  title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  booktitle={International Conference on Learning Representations},
  year={2016}
}
@article{li2021happens,
  title={What Happens after SGD Reaches Zero Loss?--A Mathematical Framework},
  author={Li, Zhiyuan and Wang, Tianhao and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2110.06914},
  year={2021}
}
@inproceedings{blanc2020implicit,
  title={Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process},
  author={Blanc, Guy and Gupta, Neha and Valiant, Gregory and Valiant, Paul},
  booktitle={Conference on learning theory},
  pages={483--513},
  year={2020},
  organization={PMLR}
}
@InProceedings{wu2023implicitstability,
  title =    {The Implicit Regularization of Dynamical Stability in Stochastic Gradient Descent},
  author =       {Wu, Lei and Su, Weijie J},
  booktitle =    {Proceedings of the 40th International Conference on Machine Learning},
  pages =    {37656--37684},
  year =   {2023},
  volume =   {202},
  series =   {Proceedings of Machine Learning Research},
  publisher =    {PMLR}
}
@article{ma2021linear,
  title={On linear stability of sgd and input-smoothness of neural networks},
  author={Ma, Chao and Ying, Lexing},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16805--16817},
  year={2021}
}
@article{brutzkus2017sgd,
  title={SGD learns over-parameterized networks that provably generalize on linearly separable data},
  author={Brutzkus, Alon and Globerson, Amir and Malach, Eran and Shalev-Shwartz, Shai},
  journal={arXiv preprint arXiv:1710.10174},
  year={2017}
}
@inproceedings{chizat2020implicit,
  title={Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Conference on Learning Theory},
  pages={1305--1338},
  year={2020},
  organization={PMLR}
}
@article{safran2022effective,
  title={On the effective number of linear regions in shallow univariate ReLU networks: Convergence guarantees and implicit bias},
  author={Safran, Itay and Vardi, Gal and Lee, Jason D},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={32667--32679},
  year={2022}
}
@inproceedings{thomas2020interplay,
	author = {Thomas, Valentin and Pedregosa, Fabian and Merri{\"e}nboer, Bart and Manzagol, Pierre-Antoine and Bengio, Yoshua and Le Roux, Nicolas},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	organization = {PMLR},
	pages = {3503--3513},
	title = {On the interplay between noise and curvature and its effect on optimization and generalization},
	year = {2020}}

@inproceedings{ziyin2021minibatch,
	author = {Liu Ziyin and Kangqiao Liu and Takashi Mori and Masahito Ueda},
	booktitle = {International Conference on Learning Representations},
	title = {Strength of Minibatch Noise in {SGD}},
	year = {2022}}

@inproceedings{liu2021noise,
	author = {Liu, Kangqiao and Ziyin, Liu and Ueda, Masahito},
	booktitle = {International Conference on Machine Learning},
	organization = {PMLR},
	pages = {7045--7056},
	title = {Noise and fluctuation of finite learning rate stochastic gradient descent},
	year = {2021}}

@article{feng2021inverse,
	author = {Feng, Yu and Tu, Yuhai},
	journal = {Proceedings of the National Academy of Sciences},
	number = {9},
	publisher = {National Acad Sciences},
	title = {The inverse variance--flatness relation in stochastic gradient descent is critical for finding flat minima},
	volume = {118},
	year = {2021}}
@article{wojtowytsch2023stochastic,
  title={Stochastic gradient descent with noise of machine learning type Part I: Discrete time analysis},
  author={Wojtowytsch, Stephan},
  journal={Journal of Nonlinear Science},
  volume={33},
  number={3},
  pages={45},
  year={2023},
  publisher={Springer}
}
@article{wang2023noise,
  title={The Noise Geometry of Stochastic Gradient Descent: A Quantitative and Analytical Characterization},
  author={Wang, Mingze and Wu, Lei},
  journal={arXiv preprint arXiv:2310.00692},
  year={2023}
}
@inproceedings{zhu2019anisotropic,
	author = {Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
	booktitle = {International Conference on Machine Learning},
	organization = {PMLR},
	pages = {7654--7663},
	title = {The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Sharp Minima and Regularization Effects},
	year = {2019}}
@inproceedings{saxe2022neural,
  title={The neural race reduction: Dynamics of abstraction in gated networks},
  author={Saxe, Andrew and Sodhani, Shagun and Lewallen, Sam Jay},
  booktitle={International Conference on Machine Learning},
  pages={19287--19309},
  year={2022},
  organization={PMLR}
}
@article{atanasov2021neural,
  title={Neural networks as kernel learners: The silent alignment effect},
  author={Atanasov, Alexander and Bordelon, Blake and Pehlevan, Cengiz},
  journal={International Conference on Learning Representations},
  year={2022}
}
@article{jastrzkebski2018relation,
  title={On the relation between the sharpest directions of DNN loss and the SGD step length},
  author={Jastrz{\k{e}}bski, Stanis{\l}aw and Kenton, Zachary and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
journal={International Conference on Learning Representations},
  year={2019}
}
@article{dutta2013approximate,
  title={Approximate KKT points and a proximity measure for termination},
  author={Dutta, Joydeep and Deb, Kalyanmoy and Tulshyan, Rupesh and Arora, Ramnik},
  journal={Journal of Global Optimization},
  volume={56},
  number={4},
  pages={1463--1499},
  year={2013},
  publisher={Springer}
}
@book{clarke2008nonsmooth,
  title={Nonsmooth analysis and control theory},
  author={Clarke, Francis H and Ledyaev, Yuri S and Stern, Ronald J and Wolenski, Peter R},
  volume={178},
  year={2008},
  publisher={Springer Science \& Business Media}
}
@article{ji2019polylogarithmic,
  title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow relu networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1909.12292},
  year={2019}
}
@article{han2021neural,
  title={Neural collapse under mse loss: Proximity to and dynamics on the central path},
  author={Han, XY and Papyan, Vardan and Donoho, David L},
  journal={arXiv preprint arXiv:2106.02073},
  year={2021}
}
@article{zhu2021geometric,
  title={A geometric analysis of neural collapse with unconstrained features},
  author={Zhu, Zhihui and Ding, Tianyu and Zhou, Jinxin and Li, Xiao and You, Chong and Sulam, Jeremias and Qu, Qing},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={29820--29834},
  year={2021}
}
@article{fang2021exploring,
  title={Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training},
  author={Fang, Cong and He, Hangfeng and Long, Qi and Su, Weijie J},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={43},
  pages={e2103091118},
  year={2021},
  publisher={National Acad Sciences}
}
@article{papyan2020prevalence,
  title={Prevalence of neural collapse during the terminal phase of deep learning training},
  author={Papyan, Vardan and Han, XY and Donoho, David L},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={40},
  pages={24652--24663},
  year={2020},
  publisher={National Acad Sciences}
}
@article{chen2023phase,
  title={Phase Diagram of Initial Condensation for Two-layer Neural Networks},
  author={Chen, Zhengan and Li, Yuqing and Luo, Tao and Zhou, Zhangchen and Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:2303.06561},
  year={2023}
}
@article{zhou2022empirical,
  title={Empirical phase diagram for three-layer neural networks with infinite width},
  author={Zhou, Hanxu and Qixuan, Zhou and Jin, Zhenyuan and Luo, Tao and Zhang, Yaoyu and Xu, Zhi-Qin},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={26021--26033},
  year={2022}
}
@inproceedings{arpit2017closer,
  title={A closer look at memorization in deep networks},
  author={Arpit, Devansh and Jastrz{\k{e}}bski, Stanis{\l}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
  booktitle={International conference on machine learning},
  pages={233--242},
  year={2017},
  organization={PMLR}
}
@inproceedings{rahaman2019spectral,
  title={On the spectral bias of neural networks},
  author={Rahaman, Nasim xd Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={5301--5310},
  year={2019},
  organization={PMLR}
}
@article{zhang2021embedding,
author = {Zhang, Yaoyu and Li, Yuqing and Zhang, Zhongwang and Luo, Tao and John Xu, Zhi-Qin},
title = {Embedding Principle: A Hierarchical Structure of Loss Landscape of Deep Neural Networks},
journal = {Journal of Machine Learning},
year = {2022},
volume = {1},
number = {1},
pages = {60--113},
issn = {2790-2048}
}
@article{abbe2023sgd,
  title={SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics},
  author={Abbe, Emmanuel and Boix-Adsera, Enric and Misiakiewicz, Theodor},
  journal={arXiv preprint arXiv:2302.11055},
  year={2023}
}
@inproceedings{abbe2022merged,
  title={The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks},
  author={Abbe, Emmanuel and Adsera, Enric Boix and Misiakiewicz, Theodor},
  booktitle={Conference on Learning Theory},
  pages={4782--4887},
  year={2022},
  organization={PMLR}
}
@inproceedings{abbe2022initial,
  title={An initial alignment between neural network and target is needed for gradient descent to learn},
  author={Abbe, Emmanuel and Cornacchia, Elisabetta and Hazla, Jan and Marquis, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={33--52},
  year={2022},
  organization={PMLR}
}
@article{ma2022beyond,
  title={Beyond the Quadratic Approximation: the Multiscale Structure of Neural Network Loss Landscapes},
  author={Ma, Chao and Kunin, Daniel and Wu, Lei and Ying, Lexing},
  journal={arXiv preprint arXiv:2204.11326},
  year={2022}
}
@article{damian2022self,
  title={Self-stabilization: The implicit bias of gradient descent at the edge of stability},
  author={Damian, Alex and Nichani, Eshaan and Lee, Jason D},
  journal={arXiv preprint arXiv:2209.15594},
  year={2022}
}
@article{zhu2022understanding,
  title={Understanding Edge-of-Stability Training Dynamics with a Minimalist Example},
  author={Zhu, Xingyu and Wang, Zixuan and Wang, Xiang and Zhou, Mo and Ge, Rong},
  journal={arXiv preprint arXiv:2210.03294},
  year={2022}
}
@article{li2022analyzing,
  title={Analyzing sharpness along gd trajectory: Progressive sharpening and edge of stability},
  author={Li, Zhouzi and Wang, Zixuan and Li, Jian},
  journal={arXiv preprint arXiv:2207.12678},
  year={2022}
}
@article{ahn2022learning,
  title={Learning threshold neurons via the" edge of stability"},
  author={Ahn, Kwangjun and Bubeck, S{\'e}bastien and Chewi, Sinho and Lee, Yin Tat and Suarez, Felipe and Zhang, Yi},
  journal={arXiv preprint arXiv:2212.07469},
  year={2022}
}
@article{ma2022multiscale,
  title={The multiscale structure of neural network loss functions: The effect on optimization and origin},
  author={Ma, Chao and Wu, Lei and Ying, Lexing},
  journal={arXiv preprint arXiv:2204.11326},
  year={2022}
}
@inproceedings{ahn2022understanding,
  title={Understanding the unstable convergence of gradient descent},
  author={Ahn, Kwangjun and Zhang, Jingzhao and Sra, Suvrit},
  booktitle={International Conference on Machine Learning},
  pages={247--257},
  year={2022},
  organization={PMLR}
}
@inproceedings{mei2019mean,
  title={Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Conference on Learning Theory},
  pages={2388--2464},
  year={2019},
  organization={PMLR}
}
@article{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@inproceedings{woodworth2020kernel,
  title={Kernel and rich regimes in overparametrized models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={3635--3673},
  year={2020},
  organization={PMLR}
}
@article{zhou2022towards,
  title={Towards understanding the condensation of neural networks at initial training},
  author={Zhou, Hanxu and Qixuan, Zhou and Luo, Tao and Zhang, Yaoyu and Xu, Zhi-Qin},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2184--2196},
  year={2022}
}
@article{luo2021phase,
  title={Phase diagram for two-layer relu neural networks at infinite-width limit},
  author={Luo, Tao and Xu, Zhi-Qin John and Ma, Zheng and Zhang, Yaoyu},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={3327--3373},
  year={2021},
  publisher={JMLRORG}
}
@article{xu2019frequency,
  title={Frequency principle: Fourier analysis sheds light on deep neural networks},
  author={Xu, Zhi-Qin John and Zhang, Yaoyu and Luo, Tao and Xiao, Yanyang and Ma, Zheng},
  journal={arXiv preprint arXiv:1901.06523},
  year={2019}
}
@article{nakkiran2019sgd,
  title={Sgd on neural networks learns functions of increasing complexity},
  author={Nakkiran, Preetum and Kaplun, Gal and Kalimeris, Dimitris and Yang, Tristan and Edelman, Benjamin L and Zhang, Fred and Barak, Boaz},
  journal={arXiv preprint arXiv:1905.11604},
  year={2019}
}
@article{pesme2023saddle,
  title={Saddle-to-Saddle Dynamics in Diagonal Linear Networks},
  author={Pesme, Scott and Flammarion, Nicolas},
  journal={arXiv preprint arXiv:2304.00488},
  year={2023}
}
@article{jacot2021saddle,
  title={Saddle-to-saddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity},
  author={Jacot, Arthur and Ged, Fran{\c{c}}ois and {\c{S}}im{\c{s}}ek, Berfin and Hongler, Cl{\'e}ment and Gabriel, Franck},
  journal={arXiv preprint arXiv:2106.15933},
  year={2021}
}
@article{zou2018stochastic,
  title={Stochastic gradient descent optimizes over-parameterized deep relu networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:1811.08888},
  year={2018}
}
@inproceedings{allen2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}
@article{maennel2018gradient,
  title={Gradient descent quantizes relu network features},
  author={Maennel, Hartmut and Bousquet, Olivier and Gelly, Sylvain},
  journal={arXiv preprint arXiv:1803.08367},
  year={2018}
}
@inproceedings{phuong2021inductive,
  title={The inductive bias of relu networks on orthogonally separable data},
  author={Phuong, Mary and Lampert, Christoph H},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@article{wang2022early,
  title={Early Stage Convergence and Global Convergence of Training Mildly Parameterized Neural Networks},
  author={Wang, Mingze and Ma, Chao},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}
@article{boursier2022gradient,
  title={Gradient flow dynamics of shallow relu networks for square loss and orthogonal inputs},
  author={Boursier, Etienne and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}
@article{bolte2010characterizations,
  title={Characterizations of {\L}ojasiewicz inequalities: subgradient flows, talweg, convexity},
  author={Bolte, J{\'e}r{\^o}me and Daniilidis, Aris and Ley, Olivier and Mazet, Laurent},
  journal={Transactions of the American Mathematical Society},
  volume={362},
  number={6},
  pages={3319--3363},
  year={2010}
}
@article{wu2022does,
  title={When does sgd favor flat minima? a quantitative characterization via linear stability},
  author={Wu, Lei and Wang, Mingze and Su, Weijie},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}
@inproceedings{nacson2022implicit,
  title={Implicit bias of the step size in linear diagonal neural networks},
  author={Nacson, Mor Shpigel and Ravichandran, Kavya and Srebro, Nathan and Soudry, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={16270--16295},
  year={2022},
  organization={PMLR}
}
@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@article{kunin2022asymmetric,
  title={The Asymmetric Maximum Margin Bias of Quasi-Homogeneous Neural Networks},
  author={Kunin, Daniel and Yamamura, Atsushi and Ma, Chao and Ganguli, Surya},
  journal={International Conference on Learning Representations},
  year={2023}
}
@inproceedings{nacson2019lexicographic,
  title={Lexicographic and depth-sensitive margins in homogeneous and non-homogeneous deep models},
  author={Nacson, Mor Shpigel and Gunasekar, Suriya and Lee, Jason and Srebro, Nathan and Soudry, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={4683--4692},
  year={2019},
  organization={PMLR}
}
@article{ji2020directional,
  title={Directional convergence and alignment in deep learning},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17176--17186},
  year={2020}
}
@article{gunasekar2018implicit,
  title={Implicit bias of gradient descent on linear convolutional networks},
  author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{ji2018gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1810.02032},
  year={2018}
}
@article{sun2022mirror,
  title={Mirror descent maximizes generalized margin and can be implemented efficiently},
  author={Sun, Haoyuan and Ahn, Kwangjun and Thrampoulidis, Christos and Azizan, Navid},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}
@article{wang2021momentum,
  title={Momentum Doesn't Change The Implicit Bias},
  author={Wang, Bohan and Meng, Qi and Zhang, Huishuai and Sun, Ruoyu and Chen, Wei and Ma, Zhi-Ming},
  journal={Advances in Neural Information Processing Systems},
  year={2022},
}
@inproceedings{nacson2019stochastic,
  title={Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate},
  author={Nacson, Mor Shpigel and Srebro, Nathan and Soudry, Daniel},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={3051--3059},
  year={2019},
  organization={PMLR}
}
@inproceedings{gunasekar2018characterizing,
  title={Characterizing implicit bias in terms of optimization geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={1832--1841},
  year={2018},
  organization={PMLR}
}
@inproceedings{ji2020gradient,
  title={Gradient descent follows the regularization path for general losses},
  author={Ji, Ziwei and Dud{\'\i}k, Miroslav and Schapire, Robert E and Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={2109--2136},
  year={2020},
  organization={PMLR}
}
@inproceedings{ji2021fast,
  title={Fast margin maximization via dual acceleration},
  author={Ji, Ziwei and Srebro, Nathan and Telgarsky, Matus},
  booktitle={International Conference on Machine Learning},
  pages={4860--4869},
  year={2021},
  organization={PMLR}
}
@inproceedings{ji2021characterizing,
  title={Characterizing the implicit bias via a primal-dual analysis},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={Algorithmic Learning Theory},
  pages={772--804},
  year={2021},
  organization={PMLR}
}
@article{ji2018risk,
  title={Risk and parameter convergence of logistic regression},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1803.07300},
  year={2018}
}
@inproceedings{nacson2019convergence,
  title={Convergence of gradient descent on separable data},
  author={Nacson, Mor Shpigel and Lee, Jason and Gunasekar, Suriya and Savarese, Pedro Henrique Pamplona and Srebro, Nathan and Soudry, Daniel},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={3420--3428},
  year={2019},
  organization={PMLR}
}
@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018},
  publisher={JMLR. org}
}
@book{filippov2013differential,
  title={Differential equations with discontinuous righthand sides: control systems},
  author={Filippov, Aleksei Fedorovich},
  volume={18},
  year={2013},
  publisher={Springer Science \& Business Media}
}
@article{lyu2021gradient,
  title={Gradient Descent on Two-layer Nets: Margin Maximization and Simplicity Bias},
  author={Lyu, Kaifeng and Li, Zhiyuan and Wang, Runzhe and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{lyu2019gradient,
  title={Gradient descent maximizes the margin of homogeneous neural networks},
  author={Lyu, Kaifeng and Li, Jian},
  journal={arXiv preprint arXiv:1906.05890},
  year={2019}
}
@article{luo2019theory,
  title={Theory of the frequency principle for general deep neural networks},
  author={Luo, Tao and Ma, Zheng and Xu, Zhi-Qin John and Zhang, Yaoyu},
  journal={arXiv preprint arXiv:1906.09235},
  year={2019}
}
@article{li2020curse,
  title={On the curse of memory in recurrent neural networks: Approximation and optimization analysis},
  author={Li, Zhong and Han, Jiequn and Li, Qianxiao and others},
  journal={arXiv preprint arXiv:2009.07799},
  year={2020}
}
@article{li2022approximation,
  title={Approximation and Optimization Theory for Linear Continuous-Time Recurrent Neural Networks},
  author={Li, Zhong and Han, Jiequn and Weinan, E and Li, Qianxiao},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={42},
  pages={1--85},
  year={2022},
  publisher={Microtome Publishing}
}
@article{gur2018gradient,
  title={Gradient descent happens in a tiny subspace},
  author={Gur-Ari, Guy and Roberts, Daniel A and Dyer, Ethan},
  journal={arXiv preprint arXiv:1812.04754},
  year={2018}
}
@article{chatterji2021doesA,
  title={When does gradient descent with logistic loss find interpolating two-layer networks?},
  author={Chatterji, Niladri S and Long, Philip M and Bartlett, Peter L},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={159},
  pages={1--48},
  year={2021}
}
@inproceedings{chatterji2021doesB,
  title={When does gradient descent with logistic loss interpolate using deep networks with smoothed ReLU activations?},
  author={Chatterji, Niladri S and Long, Philip M and Bartlett, Peter},
  booktitle={Conference on Learning Theory},
  pages={927--1027},
  year={2021},
  organization={PMLR}
}
@inproceedings{zhou2021local,
  title={A local convergence theory for mildly over-parameterized two-layer neural network},
  author={Zhou, Mo and Ge, Rong and Jin, Chi},
  booktitle={Conference on Learning Theory},
  pages={4577--4632},
  year={2021},
  organization={PMLR}
}
@inproceedings{safran2018spurious,
  title={Spurious local minima are common in two-layer relu neural networks},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={International conference on machine learning},
  pages={4433--4441},
  year={2018},
  organization={PMLR}
}
@inproceedings{safran2021effects,
  title={The effects of mild over-parameterization on the optimization landscape of shallow relu neural networks},
  author={Safran, Itay M and Yehudai, Gilad and Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={3889--3934},
  year={2021},
  organization={PMLR}
}
@article{cho2009kernel,
  title={Kernel methods for deep learning},
  author={Cho, Youngmin and Saul, Lawrence},
  journal={Advances in neural information processing systems},
  volume={22},
  year={2009}
}
@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}
@book{mohri2018foundations,
  title={Foundations of machine learning},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2018},
  publisher={MIT press}
}
@inproceedings{NEURIPS2018_6651526b,
 author = {Wu, Lei and Ma, Chao and E, Weinan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {How SGD Selects the Global Minima in Over-parameterized Learning: A Dynamical Stability Perspective},
 url = {https://proceedings.neurips.cc/paper/2018/file/6651526b6fb8f29a00507de6a49ce30f-Paper.pdf},
 volume = {31},
 year = {2018}
}
@article{cohen2021gradient,
  title={Gradient descent on neural networks typically occurs at the edge of stability},
  author={Cohen, Jeremy M and Kaur, Simran and Li, Yuanzhi and Kolter, J Zico and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:2103.00065},
  year={2021}
}
@article{bassily2020stability,
  title={Stability of stochastic gradient descent on nonsmooth convex losses},
  author={Bassily, Raef and Feldman, Vitaly and Guzm{\'a}n, Crist{\'o}bal and Talwar, Kunal},
  journal={arXiv preprint arXiv:2006.06914},
  year={2020}
}
@inproceedings{bousquet2020sharper,
  title={Sharper bounds for uniformly stable algorithms},
  author={Bousquet, Olivier and Klochkov, Yegor and Zhivotovskiy, Nikita},
  booktitle={Conference on Learning Theory},
  pages={610--626},
  year={2020},
  organization={PMLR}
}
@article{10.1214/aos/1176344196,
author = {W. H. Rogers and T. J. Wagner},
title = {{A Finite Sample Distribution-Free Performance Bound for Local Discrimination Rules}},
volume = {6},
journal = {The Annals of Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {506 -- 514},
keywords = {deleted estimate, discrimination, distribution-free bound, error rate estimate, finite sample bound, local inference, nearest neighbor rules},
year = {1978},
doi = {10.1214/aos/1176344196},
URL = {https://doi.org/10.1214/aos/1176344196}
}
@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}@article{zhu2018anisotropic,
  title={The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  journal={arXiv preprint arXiv:1803.00195},
  year={2018}
}@article{bartlett2017spectrally,
  title={Spectrally-normalized margin bounds for neural networks},
  author={Bartlett, Peter and Foster, Dylan J and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1706.08498},
  year={2017}
}@article{ma2020towards,
  title={Towards a Mathematical Understanding of Neural Network-Based Machine Learning: what we know and what we don't},
  author={Ma, Chao and Wojtowytsch, Stephan and Wu, Lei and others},
  journal={arXiv preprint arXiv:2009.10713},
  year={2020}
}@inproceedings{li2017stochastic,
  title={Stochastic modified equations and adaptive stochastic gradient algorithms},
  author={Li, Qianxiao and Tai, Cheng and E, Weinan},
  booktitle={International Conference on Machine Learning},
  pages={2101--2110},
  year={2017},
  organization={PMLR}
}@article{jastrzkebski2017three,
  title={Three factors influencing minima in sgd},
  author={Jastrz{\k{e}}bski, Stanis{\l}aw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal={arXiv preprint arXiv:1711.04623},
  year={2017}
}@article{li2019stochastic,
  title={Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations},
  author={Li, Qianxiao and Tai, Cheng and E, Weinan},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={1474--1520},
  year={2019},
  publisher={JMLR. org}
}@article{bottou1991stochastic,
  title={Stochastic gradient learning in neural networks},
  author={Bottou, L{\'e}on and others},
  journal={Proceedings of Neuro-N{\i}mes},
  volume={91},
  number={8},
  pages={12},
  year={1991},
  publisher={Nimes}
}@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}@article{wu2019global,
  title={Global convergence of gradient descent for deep linear residual networks},
  author={Wu, Lei and Wang, Qingcan and Ma, Chao},
  journal={arXiv preprint arXiv:1911.00645},
  year={2019}
}@article{asadi2018chaining,
  title={Chaining mutual information and tightening generalization bounds},
  author={Asadi, Amir R and Abbe, Emmanuel and Verd{\'u}, Sergio},
  journal={arXiv preprint arXiv:1806.03803},
  year={2018}
}@article{li2017visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  journal={arXiv preprint arXiv:1712.09913},
  year={2017}
}@article{sun2020global,
  title={The global landscape of neural networks: An overview},
  author={Sun, Ruoyu and Li, Dawei and Liang, Shiyu and Ding, Tian and Srikant, Rayadurgam},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={5},
  pages={95--108},
  year={2020},
  publisher={IEEE}
}@inproceedings{arora2018stronger,
  title={Stronger generalization bounds for deep nets via a compression approach},
  author={Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  booktitle={International Conference on Machine Learning},
  pages={254--263},
  year={2018},
  organization={PMLR}
}@inproceedings{raginsky2017non,
  title={Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis},
  author={Raginsky, Maxim and Rakhlin, Alexander and Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={1674--1703},
  year={2017},
  organization={PMLR}
}@inproceedings{welling2011bayesian,
  title={Bayesian learning via stochastic gradient Langevin dynamics},
  author={Welling, Max and Teh, Yee W},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={681--688},
  year={2011},
  organization={Citeseer}
}@inproceedings{zhang2017hitting,
  title={A hitting time analysis of stochastic gradient langevin dynamics},
  author={Zhang, Yuchen and Liang, Percy and Charikar, Moses},
  booktitle={Conference on Learning Theory},
  pages={1980--2022},
  year={2017},
  organization={PMLR}
}@article{chen2018stability,
  title={Stability and convergence trade-off of iterative optimization algorithms},
  author={Chen, Yuansi and Jin, Chi and Yu, Bin},
  journal={arXiv preprint arXiv:1804.01619},
  year={2018}
}@inproceedings{bousquet2020sharper,
  title={Sharper bounds for uniformly stable algorithms},
  author={Bousquet, Olivier and Klochkov, Yegor and Zhivotovskiy, Nikita},
  booktitle={Conference on Learning Theory},
  pages={610--626},
  year={2020},
  organization={PMLR}
}@article{vapnik1994measuring,
  title={Measuring the VC-dimension of a learning machine},
  author={Vapnik, Vladimir and Levin, Esther and Le Cun, Yann},
  journal={Neural computation},
  volume={6},
  number={5},
  pages={851--876},
  year={1994},
  publisher={MIT Press}
}@article{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nathan},
  journal={arXiv preprint arXiv:1706.08947},
  year={2017}
}@article{kraskov2004estimating,
  title={Estimating mutual information},
  author={Kraskov, Alexander and St{\"o}gbauer, Harald and Grassberger, Peter},
  journal={Physical review E},
  volume={69},
  number={6},
  pages={066138},
  year={2004},
  publisher={APS}
}@inproceedings{steinke2020reasoning,
  title={Reasoning about generalization via conditional mutual information},
  author={Steinke, Thomas and Zakynthinou, Lydia},
  booktitle={Conference on Learning Theory},
  pages={3437--3452},
  year={2020},
  organization={PMLR}
}@article{haghifam2020sharpened,
  title={Sharpened generalization bounds based on conditional mutual information and an application to noisy, iterative algorithms},
  author={Haghifam, Mahdi and Negrea, Jeffrey and Khisti, Ashish and Roy, Daniel M and Dziugaite, Gintare Karolina},
  journal={arXiv preprint arXiv:2004.12983},
  year={2020}
}@article{xu2017information,
  title={Information-theoretic analysis of generalization capability of learning algorithms},
  author={Xu, Aolin and Raginsky, Maxim},
  journal={arXiv preprint arXiv:1705.07809},
  year={2017}
}@article{bu2020tightening,
  title={Tightening Mutual Information-Based Bounds on Generalization Error},
  author={Bu, Yuheng and Zou, Shaofeng and Veeravalli, Venugopal V},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={1},
  number={1},
  pages={121--130},
  year={2020},
  publisher={IEEE}
}
@inproceedings{tu2019understanding,
  title={Understanding generalization in recurrent neural networks},
  author={Tu, Zhuozhuo and He, Fengxiang and Tao, Dacheng},
  booktitle={International Conference on Learning Representations},
  year={2019}
}@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:1806.07572},
  year={2018}
}@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}@inproceedings{hardt2016train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1225--1234},
  year={2016},
  organization={PMLR}
}
@article{hoffer2017train,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  journal={arXiv preprint arXiv:1705.08741},
  year={2017}
}
@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International Conference on Machine Learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}@article{allen2019learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  journal={Advances in neural information processing systems},
  year={2019}
}
@article{weinan2020comparative,
  title={A comparative analysis of optimization and generalization properties oftwo-layer neural network and random feature models under gradient descent dynamics},
  author={Weinan, E and Chao, Ma and Lei, Wu},
  journal={Science China Mathematics},
  volume={63},
  number={7},
  pages={1235},
  year={2020},
  publisher={Science China Press}
}
@inproceedings{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019},
  organization={PMLR}
}
@article{shwartz2017opening,
  title={Opening the black box of deep neural networks via information},
  author={Shwartz-Ziv, Ravid and Tishby, Naftali},
  journal={arXiv preprint arXiv:1703.00810},
  year={2017}
}
@inproceedings{liang2019fisher,
  title={Fisher-rao metric, geometry, and complexity of neural networks},
  author={Liang, Tengyuan and Poggio, Tomaso and Rakhlin, Alexander and Stokes, James},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={888--896},
  year={2019},
  organization={PMLR}
}
@article{wu2018sgd,
  title={How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective},
  author={Wu, Lei and Ma, Chao and E, Weinan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  pages={8279--8288},
  year={2018}
}@article{weinan2021barron,
  title={The Barron Space and the Flow-Induced Function Spaces for Neural Network Models},
  author={Weinan, E and Ma, Chao and Wu, Lei},
  journal={Constructive Approximation},
  pages={1--38},
  year={2021},
  publisher={Springer}
}@article{zhou2020towards,
  title={Towards theoretically understanding why sgd generalizes better than adam in deep learning},
  author={Zhou, Pan and Feng, Jiashi and Ma, Chao and Xiong, Caiming and Hoi, Steven and others},
  journal={arXiv preprint arXiv:2010.05627},
  year={2020}
}@article{li2020complexity,
  title={Complexity measures for neural networks with general activation functions using path-based norms},
  author={Li, Zhong and Ma, Chao and Wu, Lei},
  journal={arXiv preprint arXiv:2009.06132},
  year={2020}
}@article{ma2019priori,
  title={A priori estimates of the population risk for residual networks},
  author={Ma, Chao and Wang, Qingcan and others},
  journal={arXiv preprint arXiv:1903.02154},
  year={2019}
}@article{ma2018priori,
  title={A priori estimates of the population risk for two-layer neural networks},
  author={Ma, Chao and Wu, Lei and others},
  journal={arXiv preprint arXiv:1810.06397},
  year={2018}
}@article{fehrman2020convergence,
  title={Convergence rates for the stochastic gradient descent method for non-convex objective functions},
  author={Fehrman, Benjamin and Gess, Benjamin and Jentzen, Arnulf},
  journal={Journal of Machine Learning Research},
  volume={21},
  year={2020},
  publisher={MICROTOME PUBL}
  }
@inproceedings{wu2020noisy,
  title={On the noisy gradient descent that generalizes as sgd},
  author={Wu, Jingfeng and Hu, Wenqing and Xiong, Haoyi and Huan, Jun and Braverman, Vladimir and Zhu, Zhanxing},
  booktitle={International Conference on Machine Learning},
  pages={10367--10376},
  year={2020},
  organization={PMLR}
}
@article{ma2021sobolev,
  title={The Sobolev Regularization Effect of Stochastic Gradient Descent},
  author={Ma, Chao and Ying, Lexing},
  journal={arXiv preprint arXiv:2105.13462},
  year={2021}
}
@article{jentzen2021proof,
  title={A proof of convergence for the gradient descent optimization method with random initializations in the training of neural networks with ReLU activation for piecewise linear target functions},
  author={Jentzen, Arnulf and Riekert, Adrian},
  journal={arXiv preprint arXiv:2108.04620},
  year={2021}
}
@article{mustafa2021fine,
  title={Fine-grained Generalization Analysis of Structured Output Prediction},
  author={Mustafa, Waleed and Lei, Yunwen and Ledent, Antoine and Kloft, Marius},
  journal={arXiv preprint arXiv:2106.00115},
  year={2021}
}
@article{li2018tighter,
  title={On tighter generalization bound for deep neural networks: Cnns, resnets, and beyond},
  author={Li, Xingguo and Lu, Junwei and Wang, Zhaoran and Haupt, Jarvis and Zhao, Tuo},
  journal={arXiv preprint arXiv:1806.05159},
  year={2018}
}
@article{ledent2019norm,
  title={Norm-based generalisation bounds for multi-class convolutional neural networks},
  author={Ledent, Antoine and Mustafa, Waleed and Lei, Yunwen and Kloft, Marius},
  journal={arXiv preprint arXiv:1905.12430},
  year={2019},
  publisher={Technical report}
}
@inproceedings{zhou2018understanding,
  title={Understanding generalization and optimization performance of deep CNNs},
  author={Zhou, Pan and Feng, Jiashi},
  booktitle={International Conference on Machine Learning},
  pages={5960--5969},
  year={2018},
  organization={PMLR}
}
@article{long2019generalization,
  title={Generalization bounds for deep convolutional neural networks},
  author={Long, Philip M and Sedghi, Hanie},
  journal={arXiv preprint arXiv:1905.12600},
  year={2019}
}
@inproceedings{liang2019fisher,
  title={Fisher-rao metric, geometry, and complexity of neural networks},
  author={Liang, Tengyuan and Poggio, Tomaso and Rakhlin, Alexander and Stokes, James},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={888--896},
  year={2019},
  organization={PMLR}
}
@inproceedings{golowich2018size,
  title={Size-independent sample complexity of neural networks},
  author={Golowich, Noah and Rakhlin, Alexander and Shamir, Ohad},
  booktitle={Conference On Learning Theory},
  pages={297--299},
  year={2018},
  organization={PMLR}
}
@inproceedings{mou2018generalization,
  title={Generalization bounds of sgld for non-convex learning: Two theoretical viewpoints},
  author={Mou, Wenlong and Wang, Liwei and Zhai, Xiyu and Zheng, Kai},
  booktitle={Conference on Learning Theory},
  pages={605--638},
  year={2018},
  organization={PMLR}
}
@article{weinan2021barron,
  title={The Barron Space and the Flow-Induced Function Spaces for Neural Network Models},
  author={Weinan, E and Ma, Chao and Wu, Lei},
  journal={Constructive Approximation},
  pages={1--38},
  year={2021},
  publisher={Springer}
}
@article{asadi2020chaining,
  title={Chaining Meets Chain Rule: Multilevel Entropic Regularization and Training of Neural Networks.},
  author={Asadi, Amir R and Abbe, Emmanuel},
  journal={J. Mach. Learn. Res.},
  volume={21},
  pages={139--1},
  year={2020}
}
@article{xu2017information,
  title={Information-theoretic analysis of generalization capability of learning algorithms},
  author={Xu, Aolin and Raginsky, Maxim},
  journal={arXiv preprint arXiv:1705.07809},
  year={2017}
}
@article{bu2020tightening,
  title={Tightening Mutual Information-Based Bounds on Generalization Error},
  author={Bu, Yuheng and Zou, Shaofeng and Veeravalli, Venugopal V},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={1},
  number={1},
  pages={121--130},
  year={2020},
  publisher={IEEE}
}
@inproceedings{pensia2018generalization,
  title={Generalization error bounds for noisy, iterative algorithms},
  author={Pensia, Ankit and Jog, Varun and Loh, Po-Ling},
  booktitle={2018 IEEE International Symposium on Information Theory (ISIT)},
  pages={546--550},
  year={2018},
  organization={IEEE}
}
@inproceedings{fang2019sharp,
  title={Sharp analysis for nonconvex sgd escaping from saddle points},
  author={Fang, Cong and Lin, Zhouchen and Zhang, Tong},
  booktitle={Conference on Learning Theory},
  pages={1192--1234},
  year={2019},
  organization={PMLR}
}
@article{zhou2018convergence,
  title={On the convergence of adaptive gradient methods for nonconvex optimization},
  author={Zhou, Dongruo and Chen, Jinghui and Cao, Yuan and Tang, Yiqi and Yang, Ziyan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1808.05671},
  year={2018}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{reddi2019convergence,
  title={On the convergence of adam and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1904.09237},
  year={2019}
}
@inproceedings{ge2015escaping,
  title={Escaping from saddle points—online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on learning theory},
  pages={797--842},
  year={2015},
  organization={PMLR}
}
@inproceedings{kleinberg2018alternative,
  title={An alternative view: When does SGD escape local minima?},
  author={Kleinberg, Bobby and Li, Yuanzhi and Yuan, Yang},
  booktitle={International Conference on Machine Learning},
  pages={2698--2707},
  year={2018},
  organization={PMLR}
}
@article{ma2020qualitative,
  title={A qualitative study of the dynamic behavior of adaptive gradient algorithms},
  author={Ma, Chao and Wu, Lei and others},
  journal={arXiv preprint arXiv:2009.06125},
  year={2020}
}
@article{allen2017katyusha,
  title={Katyusha: The first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={8194--8244},
  year={2017},
  publisher={JMLR. org}
}
@article{li2020accelerated,
author = {Li, Huan and Fang, Cong and Lin, Zhouchen},
year = {2020},
month = {07},
pages = {1-16},
title = {Accelerated First-Order Optimization Algorithms for Machine Learning},
volume = {PP},
journal = {Proceedings of the IEEE},
doi = {10.1109/JPROC.2020.3007634}
}
@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}
@article{cai2019gram,
  title={Gram-Gauss-Newton Method: Learning Overparameterized Neural Networks for Regression Problems},
  author={Cai, Tianle and Gao, Ruiqi and Hou, Jikai and Chen, Siyu and Wang, Dong and He, Di and Zhang, Zhihua and Wang, Liwei},
  journal={arXiv preprint arXiv:1905.11675},
  year={2019}
}
@article{zhang2019fast,
  title={Fast convergence of natural gradient descent for overparameterized neural networks},
  author={Zhang, Guodong and Martens, James and Grosse, Roger},
  journal={arXiv preprint arXiv:1905.10961},
  year={2019}
}
@article{fang2018spider,
  title={Spider: Near-optimal non-convex optimization via stochastic path integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  journal={arXiv preprint arXiv:1807.01695},
  year={2018}
}
@article{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={26},
  pages={315--323},
  year={2013},
  publisher={Citeseer}
}
@inproceedings{zou2019sufficient,
  title={A sufficient condition for convergences of adam and rmsprop},
  author={Zou, Fangyu and Shen, Li and Jie, Zequn and Zhang, Weizhong and Liu, Wei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11127--11135},
  year={2019}
}
@article{barakat2019convergence,
  title={Convergence analysis of a momentum algorithm with adaptive step size for non convex optimization},
  author={Barakat, Anas and Bianchi, Pascal},
  journal={arXiv preprint arXiv:1911.07596},
  year={2019}
}
@inproceedings{staib2019escaping,
  title={Escaping saddle points with adaptive gradient methods},
  author={Staib, Matthew and Reddi, Sashank and Kale, Satyen and Kumar, Sanjiv and Sra, Suvrit},
  booktitle={International Conference on Machine Learning},
  pages={5956--5965},
  year={2019},
  organization={PMLR}
}
@article{zhou2020towards,
  title={Towards Theoretically Understanding Why SGD Generalizes Better Than ADAM in Deep Learning},
  author={Zhou, Pan and Feng, Jiashi and Ma, Chao and Xiong, Caiming and HOI, Steven and others},
  journal={arXiv preprint arXiv:2010.05627},
  year={2020}
}
@article{zhu2018anisotropic,
  title={The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  journal={arXiv preprint arXiv:1803.00195},
  year={2018}
}
