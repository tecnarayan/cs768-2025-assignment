\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bengio et~al.(2003)Bengio, Ducharme, Vincent, and
  Janvin]{bengio2003neural}
Yoshua Bengio, R{\'e}jean Ducharme, Pascal Vincent, and Christian Janvin.
\newblock A neural probabilistic language model.
\newblock \emph{The journal of machine learning research}, 3:\penalty0
  1137--1155, 2003.

\bibitem[Elman(1990)]{Elman1990FindingSI}
J.~Elman.
\newblock Finding structure in time.
\newblock \emph{Cogn. Sci.}, 14:\penalty0 179--211, 1990.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Mikolov et~al.(2010)Mikolov, Karafi{\'a}t, Burget, {\v{C}}ernock{\`y},
  and Khudanpur]{mikolov2010recurrent}
Tom{\'a}{\v{s}} Mikolov, Martin Karafi{\'a}t, Luk{\'a}{\v{s}} Burget, Jan
  {\v{C}}ernock{\`y}, and Sanjeev Khudanpur.
\newblock Recurrent neural network based language model.
\newblock In \emph{Eleventh annual conference of the international speech
  communication association}, 2010.

\bibitem[Bahdanau et~al.(2015)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock In \emph{{ICLR}}, 2015.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem[Sukhbaatar et~al.(2015)Sukhbaatar, Szlam, Weston, and
  Fergus]{sukhbaatar2015end}
Sainbayar Sukhbaatar, Arthur~D. Szlam, J.~Weston, and R.~Fergus.
\newblock End-to-end memory networks.
\newblock In \emph{NIPS}, 2015.

\bibitem[Dehghani et~al.(2018)Dehghani, Gouws, Vinyals, Uszkoreit, and
  Kaiser]{dehghani2018universal}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and {\L}ukasz
  Kaiser.
\newblock Universal transformers.
\newblock \emph{arXiv preprint arXiv:1807.03819}, 2018.

\bibitem[Lan et~al.(2019)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut]{lan2019albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock \emph{arXiv preprint arXiv:1909.11942}, 2019.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Press et~al.(2019)Press, Smith, and Levy]{press2019improving}
Ofir Press, Noah~A Smith, and Omer Levy.
\newblock Improving transformer models by reordering their sublayers.
\newblock \emph{arXiv preprint arXiv:1911.03864}, 2019.

\bibitem[Lu et~al.(2019)Lu, Li, He, Sun, Dong, Qin, Wang, and
  Liu]{lu2019understanding}
Yiping Lu, Zhuohan Li, Di~He, Zhiqing Sun, Bin Dong, Tao Qin, Liwei Wang, and
  Tie-Yan Liu.
\newblock Understanding and improving transformer from a multi-particle dynamic
  system point of view.
\newblock \emph{arXiv preprint arXiv:1906.02762}, 2019.

\bibitem[Chen et~al.(2018)Chen, Firat, Bapna, Johnson, Macherey, Foster, Jones,
  Parmar, Schuster, Chen, et~al.]{chen2018best}
Mia~Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey,
  George Foster, Llion Jones, Niki Parmar, Mike Schuster, Zhifeng Chen, et~al.
\newblock The best of both worlds: Combining recent advances in neural machine
  translation.
\newblock \emph{arXiv preprint arXiv:1804.09849}, 2018.

\bibitem[Hao et~al.(2019)Hao, Wang, Yang, Wang, Zhang, and Tu]{hao2019modeling}
Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng Tu.
\newblock Modeling recurrence for transformer.
\newblock \emph{arXiv preprint arXiv:1904.03092}, 2019.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai2019transformer}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime~G. Carbonell, Quoc~Viet Le, and
  Ruslan Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock In \emph{{ACL} {(1)}}, pages 2978--2988. Association for
  Computational Linguistics, 2019.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Kitaev et~al.(2019)Kitaev, Kaiser, and Levskaya]{kitaev2019reformer}
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
Iz~Beltagy, Matthew~E Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Fan et~al.(2020)Fan, Lavril, Grave, Joulin, and
  Sukhbaatar]{fan2020addressing}
Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar
  Sukhbaatar.
\newblock Addressing some limitations of transformers with feedback memory.
\newblock \emph{arXiv preprint arXiv:2002.09402}, 2020.

\bibitem[Fedus et~al.(2021)Fedus, Zoph, and Shazeer]{fedus2021switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock \emph{arXiv preprint arXiv:2101.03961}, 2021.

\bibitem[Lewis et~al.(2021)Lewis, Bhosale, Dettmers, Goyal, and
  Zettlemoyer]{lewis2021base}
Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer.
\newblock Base layers: Simplifying training of large, sparse models.
\newblock \emph{arXiv preprint arXiv:2103.16716}, 2021.

\bibitem[Al-Rfou et~al.(2019)Al-Rfou, Choe, Constant, Guo, and
  Jones]{al2018character}
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones.
\newblock Character-level language modeling with deeper self-attention.
\newblock In \emph{Proceedings of the 33rd {AAAI} Conference on Artificial
  Intelligence}, 2019.

\bibitem[Shaw et~al.(2018)Shaw, Uszkoreit, and Vaswani]{shaw2018self}
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
\newblock Self-attention with relative position representations.
\newblock In \emph{{NAACL-HLT} {(2)}}, 2018.

\bibitem[Mahoney(2011)]{mahoney2011large}
Matt Mahoney.
\newblock Large text compression benchmark.
\newblock \emph{URL: http://www. mattmahoney. net/text/text. html}, 2011.

\bibitem[Sukhbaatar et~al.(2019)Sukhbaatar, Grave, Bojanowski, and
  Joulin]{sukhbaatar2019adaptive}
Sainbayar Sukhbaatar, {\'E}douard Grave, Piotr Bojanowski, and Armand Joulin.
\newblock Adaptive attention span in transformers.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 331--335, 2019.

\bibitem[Yang et~al.(2018)Yang, Yuan, Cer, Kong, Constant, Pilar, Ge, Sung,
  Strope, and Kurzweil]{reddit_use}
Yinfei Yang, Steve Yuan, Daniel Cer, Sheng-yi Kong, Noah Constant, Petr Pilar,
  Heming Ge, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil.
\newblock Learning semantic textual similarity from conversations.
\newblock In \emph{Proceedings of The Third Workshop on Representation Learning
  for {NLP}}, pages 164--174, Melbourne, Australia, July 2018. Association for
  Computational Linguistics.

\bibitem[Mazar{\'e} et~al.(2018)Mazar{\'e}, Humeau, Raison, and
  Bordes]{mazare2018trainingmillions}
Pierre-Emmanuel Mazar{\'e}, Samuel Humeau, Martin Raison, and Antoine Bordes.
\newblock Training millions of personalized dialogue agents.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 2775--2779, Brussels, Belgium,
  October-November 2018. Association for Computational Linguistics.

\bibitem[Keskar et~al.(2019)Keskar, McCann, Varshney, Xiong, and
  Socher]{keskar2019ctrl}
Nitish~Shirish Keskar, Bryan McCann, Lav~R Varshney, Caiming Xiong, and Richard
  Socher.
\newblock {CTRL}: A conditional transformer language model for controllable
  generation.
\newblock \emph{arXiv preprint arXiv:1909.05858}, 2019.

\bibitem[Shuster et~al.(2019)Shuster, Ju, Roller, Dinan, Boureau, and
  Weston]{shuster2019dialogue}
Kurt Shuster, Da~Ju, Stephen Roller, Emily Dinan, Y-Lan Boureau, and Jason
  Weston.
\newblock The dialogue dodecathlon: Open-domain knowledge and image grounded
  conversational agents, 2019.

\bibitem[Humeau et~al.(2019)Humeau, Shuster, Lachaux, and
  Weston]{humeau2019polyencoder}
Samuel Humeau, Kurt Shuster, Marie{-}Anne Lachaux, and Jason Weston.
\newblock Poly-encoders: Architectures and pre-training strategies for fast and
  accurate multi-sentence scoring.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2019.

\bibitem[Baumgartner et~al.(2020)Baumgartner, Zannettou, Keegan, Squire, and
  Blackburn]{baumgartner2020pushshift}
Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy
  Blackburn.
\newblock The pushshift reddit dataset.
\newblock \emph{arXiv preprint arXiv:2001.08435}, 2020.

\bibitem[Roller et~al.(2020)Roller, Dinan, Goyal, Ju, Williamson, Liu, Xu, Ott,
  Shuster, Smith, et~al.]{roller2020recipes}
Stephen Roller, Emily Dinan, Naman Goyal, Da~Ju, Mary Williamson, Yinhan Liu,
  Jing Xu, Myle Ott, Kurt Shuster, Eric~M Smith, et~al.
\newblock Recipes for building an open-domain chatbot.
\newblock \emph{arXiv preprint arXiv:2004.13637}, 2020.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Conneau et~al.(2019)Conneau, Khandelwal, Goyal, Chaudhary, Wenzek,
  Guzm{\'a}n, Grave, Ott, Zettlemoyer, and Stoyanov]{conneau2019unsupervised}
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
  Wenzek, Francisco Guzm{\'a}n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and
  Veselin Stoyanov.
\newblock Unsupervised cross-lingual representation learning at scale.
\newblock \emph{arXiv preprint arXiv:1911.02116}, 2019.

\end{thebibliography}
