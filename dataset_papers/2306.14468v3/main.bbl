\begin{thebibliography}{}

\bibitem[Abbasi-Yadkori et~al., 2011]{abbasi2011improved}
Abbasi-Yadkori, Y., P{\'a}l, D., and Szepesv{\'a}ri, C. (2011).
\newblock Improved algorithms for linear stochastic bandits.
\newblock {\em Advances in neural information processing systems}, 24.

\bibitem[Agarwal et~al., 2014]{agarwal2014taming}
Agarwal, A., Hsu, D., Kale, S., Langford, J., Li, L., and Schapire, R. (2014).
\newblock Taming the monster: A fast and simple algorithm for contextual
  bandits.
\newblock In {\em International Conference on Machine Learning}, pages
  1638--1646. PMLR.

\bibitem[Agarwal and Zhang, 2022]{agarwal2022model}
Agarwal, A. and Zhang, T. (2022).
\newblock Model-based rl with optimistic posterior sampling: Structural
  conditions and sample complexity.
\newblock {\em arXiv preprint arXiv:2206.07659}.

\bibitem[Akkaya et~al., 2019]{akkaya2019solving}
Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., Petron, A.,
  Paino, A., Plappert, M., Powell, G., Ribas, R., et~al. (2019).
\newblock Solving rubik's cube with a robot hand.
\newblock {\em arXiv preprint arXiv:1910.07113}.

\bibitem[Ayoub et~al., 2020]{ayoub2020model}
Ayoub, A., Jia, Z., Szepesvari, C., Wang, M., and Yang, L. (2020).
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In {\em International Conference on Machine Learning}, pages
  463--474. PMLR.

\bibitem[Bai et~al., 2019]{baiyu2019lowswitchcosttabular}
Bai, Y., Xie, T., Jiang, N., and Wang, Y.-X. (2019).
\newblock Provably efficient q-learning with low switching cost.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Bradtke, 1992]{bradtke1992reinforcement}
Bradtke, S. (1992).
\newblock Reinforcement learning applied to linear quadratic regulation.
\newblock {\em Advances in neural information processing systems}, 5.

\bibitem[Chen et~al., 2019]{chen2019top}
Chen, M., Beutel, A., Covington, P., Jain, S., Belletti, F., and Chi, E.~H.
  (2019).
\newblock Top-k off-policy correction for a reinforce recommender system.
\newblock In {\em Proceedings of the Twelfth ACM International Conference on
  Web Search and Data Mining}, pages 456--464.

\bibitem[Chen et~al., 2022]{chen2022abc}
Chen, Z., Li, C.~J., Yuan, A., Gu, Q., and Jordan, M.~I. (2022).
\newblock A general framework for sample-efficient function approximation in
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:2209.15634}.

\bibitem[Cui et~al., 2023]{cui2023breaking}
Cui, Q., Zhang, K., and Du, S.~S. (2023).
\newblock Breaking the curse of multiagents in a large state space: Rl in
  markov games with independent linear function approximation.
\newblock {\em arXiv preprint arXiv:2302.03673}.

\bibitem[Dani et~al., 2008]{dani2008stochastic}
Dani, V., Hayes, T.~P., and Kakade, S.~M. (2008).
\newblock Stochastic linear optimization under bandit feedback.

\bibitem[Ding et~al., 2022]{ding2022independent}
Ding, D., Wei, C.-Y., Zhang, K., and Jovanovic, M. (2022).
\newblock Independent policy gradient for large-scale markov potential games:
  Sharper rates, function approximation, and game-agnostic convergence.
\newblock In {\em International Conference on Machine Learning}, pages
  5166--5220. PMLR.

\bibitem[Du et~al., 2021]{du2021bilinear}
Du, S., Kakade, S., Lee, J., Lovett, S., Mahajan, G., Sun, W., and Wang, R.
  (2021).
\newblock Bilinear classes: A structural framework for provable generalization
  in rl.
\newblock In {\em International Conference on Machine Learning}, pages
  2826--2836. PMLR.

\bibitem[Foster et~al., 2023]{foster2023complexity}
Foster, D.~J., Foster, D.~P., Golowich, N., and Rakhlin, A. (2023).
\newblock On the complexity of multi-agent decision making: From learning in
  games to partial monitoring.
\newblock {\em arXiv preprint arXiv:2305.00684}.

\bibitem[Foster et~al., 2021]{foster2021statistical}
Foster, D.~J., Kakade, S.~M., Qian, J., and Rakhlin, A. (2021).
\newblock The statistical complexity of interactive decision making.
\newblock {\em arXiv preprint arXiv:2112.13487}.

\bibitem[Gao et~al., 2021]{gao2021lowcostlinear}
Gao, M., Xie, T., Du, S.~S., and Yang, L.~F. (2021).
\newblock A provably efficient algorithm for linear markov decision process
  with low switching cost.
\newblock {\em arXiv preprint arXiv:2101.00494}.

\bibitem[Gao et~al., 2019]{gao2019batched}
Gao, Z., Han, Y., Ren, Z., and Zhou, Z. (2019).
\newblock Batched multi-armed bandits problem.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Gu et~al., 2021]{gu2021batched}
Gu, Q., Karbasi, A., Khosravi, K., Mirrokni, V., and Zhou, D. (2021).
\newblock Batched neural bandits.
\newblock {\em arXiv preprint arXiv:2102.13028}.

\bibitem[Han et~al., 2020]{han2020sequential}
Han, Y., Zhou, Z., Zhou, Z., Blanchet, J., Glynn, P.~W., and Ye, Y. (2020).
\newblock Sequential batch learning in finite-action linear contextual bandits.
\newblock {\em arXiv preprint arXiv:2004.06321}.

\bibitem[Huang et~al., 2021]{huang2021markovgamegeneral}
Huang, B., Lee, J.~D., Wang, Z., and Yang, Z. (2021).
\newblock Towards general function approximation in zero-sum markov games.
\newblock {\em arXiv preprint arXiv:2107.14702}.

\bibitem[Ishfaq et~al., 2021]{ishfaq2021randomized}
Ishfaq, H., Cui, Q., Nguyen, V., Ayoub, A., Yang, Z., Wang, Z., Precup, D., and
  Yang, L. (2021).
\newblock Randomized exploration in reinforcement learning with general value
  function approximation.
\newblock In {\em International Conference on Machine Learning}, pages
  4607--4616. PMLR.

\bibitem[Jiang et~al., 2017]{jiang@2017}
Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R.~E.
  (2017).
\newblock Contextual decision processes with low {B}ellman rank are
  {PAC}-learnable.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning}, volume~70 of {\em Proceedings of Machine Learning Research}, pages
  1704--1713. PMLR.

\bibitem[Jin et~al., 2021]{jin2021bellman}
Jin, C., Liu, Q., and Miryoosefi, S. (2021).
\newblock Bellman eluder dimension: New rich classes of rl problems, and
  sample-efficient algorithms.
\newblock {\em Advances in Neural Information Processing Systems}, 34.

\bibitem[Jin et~al., 2022]{jin2022power}
Jin, C., Liu, Q., and Yu, T. (2022).
\newblock The power of exploiter: Provable multi-agent rl in large state
  spaces.
\newblock In {\em International Conference on Machine Learning}, pages
  10251--10279. PMLR.

\bibitem[Jin et~al., 2020]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I. (2020).
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In {\em Conference on Learning Theory}, pages 2137--2143. PMLR.

\bibitem[Kakade et~al., 2020]{kakade2020information}
Kakade, S., Krishnamurthy, A., Lowrey, K., Ohnishi, M., and Sun, W. (2020).
\newblock Information theoretic regret bounds for online nonlinear control.
\newblock {\em Advances in Neural Information Processing Systems},
  33:15312--15325.

\bibitem[Kong et~al., 2021]{kong2021eluderlowcost}
Kong, D., Salakhutdinov, R., Wang, R., and Yang, L.~F. (2021).
\newblock Online sub-sampling for reinforcement learning with general function
  approximation.
\newblock {\em arXiv preprint arXiv:2106.07203}.

\bibitem[Li, 2017]{li2017deep}
Li, Y. (2017).
\newblock Deep reinforcement learning: An overview.
\newblock {\em arXiv preprint arXiv:1701.07274}.

\bibitem[Liu et~al., 2022a]{liu2022partially}
Liu, Q., Chung, A., Szepesv{\'a}ri, C., and Jin, C. (2022a).
\newblock When is partially observable reinforcement learning not scary?
\newblock {\em arXiv preprint arXiv:2204.08967}.

\bibitem[Liu et~al., 2022b]{liu2022optimistic}
Liu, Q., Netrapalli, P., Szepesvari, C., and Jin, C. (2022b).
\newblock Optimistic mle--a generic model-based algorithm for partially
  observable sequential decision making.
\newblock {\em arXiv preprint arXiv:2209.14997}.

\bibitem[Liu et~al., 2023]{liu2023objective}
Liu, Z., Lu, M., Xiong, W., Zhong, H., Hu, H., Zhang, S., Zheng, S., Yang, Z.,
  and Wang, Z. (2023).
\newblock One objective to rule them all: A maximization objective fusing
  estimation and planning for exploration.

\bibitem[Mirhoseini et~al., 2017]{mirhoseini2017device}
Mirhoseini, A., Pham, H., Le, Q.~V., Steiner, B., Larsen, R., Zhou, Y., Kumar,
  N., Norouzi, M., Bengio, S., and Dean, J. (2017).
\newblock Device placement optimization with reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  2430--2439. PMLR.

\bibitem[Perchet et~al., 2016]{perchet2016batched}
Perchet, V., Rigollet, P., Chassang, S., and Snowberg, E. (2016).
\newblock Batched bandit problems.
\newblock {\em The Annals of Statistics}, pages 660--681.

\bibitem[Qiao et~al., 2023]{qiao2023logarithmic}
Qiao, D., Yin, M., and Wang, Y.-X. (2023).
\newblock Logarithmic switching cost in reinforcement learning beyond linear
  mdps.
\newblock {\em arXiv preprint arXiv:2302.12456}.

\bibitem[Qiu et~al., 2021]{qiu2021reward}
Qiu, S., Ye, J., Wang, Z., and Yang, Z. (2021).
\newblock On reward-free rl with kernel and neural function approximations:
  Single-agent mdp and markov game.
\newblock In {\em International Conference on Machine Learning}, pages
  8737--8747. PMLR.

\bibitem[Russo and Van~Roy, 2013]{russo2013eluder}
Russo, D. and Van~Roy, B. (2013).
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock {\em Advances in Neural Information Processing Systems}, 26.

\bibitem[Silver et~al., 2016]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al. (2016).
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em nature}, 529(7587):484--489.

\bibitem[Sun et~al., 2019]{sun2019model}
Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A., and Langford, J. (2019).
\newblock Model-based rl in contextual decision processes: Pac bounds and
  exponential improvements over model-free approaches.
\newblock In {\em Conference on learning theory}, pages 2898--2933. PMLR.

\bibitem[Velegkas et~al., 2022]{zhuoranyang2022lowcosteluder}
Velegkas, G., Yang, Z., and Karbasi, A. (2022).
\newblock Reinforcement learning with logarithmic regret and policy switches.
\newblock {\em Advances in Neural Information Processing Systems},
  35:36040--36053.

\bibitem[Vinyals et~al., 2019]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al. (2019).
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock {\em Nature}, 575(7782):350--354.

\bibitem[Wang et~al., 2020]{wang2020reinforcement}
Wang, R., Salakhutdinov, R.~R., and Yang, L. (2020).
\newblock Reinforcement learning with general value function approximation:
  Provably efficient approach via bounded eluder dimension.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6123--6135.

\bibitem[Wang et~al., 2021]{quanquangu2021linearlowcost}
Wang, T., Zhou, D., and Gu, Q. (2021).
\newblock Provably efficient reinforcement learning with linear function
  approximation under adaptivity constraints.
\newblock {\em Advances in Neural Information Processing Systems},
  34:13524--13536.

\bibitem[Wang et~al., 2023]{wang2023breaking}
Wang, Y., Liu, Q., Bai, Y., and Jin, C. (2023).
\newblock Breaking the curse of multiagency: Provably efficient decentralized
  multi-agent rl with function approximation.
\newblock {\em arXiv preprint arXiv:2302.06606}.

\bibitem[Xie et~al., 2020]{xie2020learning}
Xie, Q., Chen, Y., Wang, Z., and Yang, Z. (2020).
\newblock Learning zero-sum simultaneous-move markov games using function
  approximation and correlated equilibrium.
\newblock In {\em Conference on learning theory}, pages 3674--3682. PMLR.

\bibitem[Yu et~al., 2021]{yu2021reinforcement}
Yu, C., Liu, J., Nemati, S., and Yin, G. (2021).
\newblock Reinforcement learning in healthcare: A survey.
\newblock {\em ACM Computing Surveys (CSUR)}, 55(1):1--36.

\bibitem[Zanette et~al., 2020]{zanette2020learning}
Zanette, A., Lazaric, A., Kochenderfer, M., and Brunskill, E. (2020).
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In {\em International Conference on Machine Learning}, pages
  10978--10989. PMLR.

\bibitem[Zhan et~al., 2022]{zhan2022pac}
Zhan, W., Uehara, M., Sun, W., and Lee, J.~D. (2022).
\newblock Pac reinforcement learning for predictive state representations.
\newblock {\em arXiv preprint arXiv:2207.05738}.

\bibitem[Zhang et~al., 2020]{zhang2020lowswitchcosttabular}
Zhang, Z., Zhou, Y., and Ji, X. (2020).
\newblock Almost optimal model-free reinforcement learningvia
  reference-advantage decomposition.
\newblock {\em Advances in Neural Information Processing Systems},
  33:15198--15207.

\bibitem[Zhao et~al., 2022]{zhao2022provably}
Zhao, Y., Tian, Y., Lee, J., and Du, S. (2022).
\newblock Provably efficient policy optimization for two-player zero-sum markov
  games.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2736--2761. PMLR.

\bibitem[Zhong et~al., 2022]{zhong2022gec}
Zhong, H., Xiong, W., Zheng, S., Wang, L., Wang, Z., Yang, Z., and Zhang, T.
  (2022).
\newblock A posterior sampling framework for interactive decision making.
\newblock {\em arXiv preprint arXiv:2211.01962}.

\bibitem[Zhong et~al., 2021]{zhong2021can}
Zhong, H., Yang, Z., Wang, Z., and Jordan, M.~I. (2021).
\newblock Can reinforcement learning find stackelberg-nash equilibria in
  general-sum markov games with myopic followers?
\newblock {\em arXiv preprint arXiv:2112.13521}.

\bibitem[Zou et~al., 2020]{zou2020pseudo}
Zou, L., Xia, L., Du, P., Zhang, Z., Bai, T., Liu, W., Nie, J.-Y., and Yin, D.
  (2020).
\newblock Pseudo dyna-q: A reinforcement learning framework for interactive
  recommendation.
\newblock In {\em Proceedings of the 13th International Conference on Web
  Search and Data Mining}, pages 816--824.

\end{thebibliography}
