\begin{thebibliography}{100}

\bibitem{aamari_levrard2019}
Eddie Aamari and Cl{\'e}ment Levrard.
\newblock {Nonasymptotic rates for manifold, tangent space and curvature
  estimation}.
\newblock {\em The Annals of Statistics}, 47(1):177 -- 204, 2019.

\bibitem{alacaoglu2022stochastic}
Ahmet Alacaoglu and Yura Malitsky.
\newblock Stochastic variance reduction for variational inequality methods.
\newblock In {\em Conference on Learning Theory}, pages 778--816. PMLR, 2022.

\bibitem{aybat2019universally}
Necdet~Serhat Aybat, Alireza Fallah, Mert Gurbuzbalaban, and Asuman Ozdaglar.
\newblock A universally optimal multistage accelerated stochastic gradient
  method.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{bach2011optimization}
Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski.
\newblock Optimization with sparsity-inducing penalties.
\newblock {\em arXiv preprint arXiv:1108.0775}, 2011.

\bibitem{bach2008convex}
Francis Bach, Julien Mairal, and Jean Ponce.
\newblock Convex sparse matrix factorizations.
\newblock {\em arXiv preprint arXiv:0812.1869}, 2008.

\bibitem{beck2017first}
Amir Beck.
\newblock {\em First-order methods in optimization}.
\newblock Society for Industrial and Applied Mathematics (SIAM), 2017.

\bibitem{BenTal2009:book}
Aharon Ben-Tal, Laurent~El Ghaoui, and Arkadi Nemirovski.
\newblock {\em Robust Optimization}.
\newblock Princeton University Press, 2009.

\bibitem{beznosikov2023stochastic}
Aleksandr Beznosikov, Eduard Gorbunov, Hugo Berard, and Nicolas Loizou.
\newblock Stochastic gradient descent-ascent: Unified theory and new efficient
  methods.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 172--235. PMLR, 2023.

\bibitem{beznosikov2020distributed}
Aleksandr Beznosikov, Valentin Samokhin, and Alexander Gasnikov.
\newblock Distributed saddle-point problems: Lower bounds, optimal and robust
  algorithms.
\newblock {\em arXiv preprint arXiv:2010.13112}, 2020.

\bibitem{bhandari2018finite}
Jalaj Bhandari, Daniel Russo, and Raghav Singal.
\newblock A finite time analysis of temporal difference learning with linear
  function approximation.
\newblock In {\em Conference on learning theory}, pages 1691--1692. PMLR, 2018.

\bibitem{chambolle2011first}
Antonin Chambolle and Thomas Pock.
\newblock A first-order primal-dual algorithm for convex problems with
  applications to imaging.
\newblock {\em Journal of mathematical imaging and vision}, 40(1):120--145,
  2011.

\bibitem{chavdarova2019reducing}
Tatjana Chavdarova, Gauthier Gidel, Fran{\c{c}}ois Fleuret, and Simon
  Lacoste-Julien.
\newblock Reducing noise in gan training with variance reduced extragradient.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{chen2020convergence}
You-Lin Chen, Sen Na, and Mladen Kolar.
\newblock Convergence analysis of accelerated stochastic gradient descent under
  the growth condition.
\newblock {\em arXiv preprint arXiv:2006.06782}, 2020.

\bibitem{cotter2011better}
Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan.
\newblock Better mini-batch algorithms via accelerated gradient methods.
\newblock {\em Advances in neural information processing systems}, 24, 2011.

\bibitem{daskalakis2017training}
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng.
\newblock Training gans with optimism.
\newblock {\em arXiv preprint arXiv:1711.00141}, 2017.

\bibitem{devolder2011stochastic}
Olivier Devolder et~al.
\newblock Stochastic first order methods in smooth convex optimization.
\newblock Technical report, CORE, 2011.

\bibitem{dieuleveut2017harder}
Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach.
\newblock Harder, better, faster, stronger convergence rates for least-squares
  regression.
\newblock {\em The Journal of Machine Learning Research}, 18(1):3520--3570,
  2017.

\bibitem{dimakis07}
Alexandros~G. Dimakis, Soummya Kar, Jos√© M.~F. Moura, Michael~G. Rabbat, and
  Anna Scaglione.
\newblock Gossip algorithms for distributed signal processing.
\newblock {\em Proceedings of the IEEE}, 98(11):1847--1864, 2010.

\bibitem{doan23}
Thinh~T. Doan.
\newblock Finite-time analysis of markov gradient descent.
\newblock {\em IEEE Transactions on Automatic Control}, 68(4):2140--2153, 2023.

\bibitem{doan2020convergence}
Thinh~T Doan, Lam~M Nguyen, Nhan~H Pham, and Justin Romberg.
\newblock Convergence rates of accelerated markov gradient descent with
  applications in reinforcement learning.
\newblock {\em arXiv preprint arXiv:2002.02873}, 2020.

\bibitem{dorfman2022adapting}
Ron Dorfman and Kfir~Yehuda Levy.
\newblock Adapting to mixing time in stochastic optimization with markovian
  data.
\newblock In {\em International Conference on Machine Learning}, pages
  5429--5446. PMLR, 2022.

\bibitem{douc:moulines:priouret:soulier:2018}
R.~Douc, E.~Moulines, P.~Priouret, and P.~Soulier.
\newblock {\em Markov chains}.
\newblock Springer Series in Operations Research and Financial Engineering.
  Springer, 2018.

\bibitem{duchi2012ergodic}
John~C Duchi, Alekh Agarwal, Mikael Johansson, and Michael~I Jordan.
\newblock Ergodic mirror descent.
\newblock {\em SIAM Journal on Optimization}, 22(4):1549--1578, 2012.

\bibitem{moulines23Rosenthal}
Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov, and Marina
  Sheshukova.
\newblock Rosenthal-type inequalities for linear statistics of markov chains.
\newblock {\em arXiv preprint arXiv:2303.05838}, 2023.

\bibitem{durmus2021stability}
Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov, and Hoi-To Wai.
\newblock On the stability of random matrix product with markovian noise:
  Application to linear stochastic approximation and td learning.
\newblock In {\em Conference on Learning Theory}, pages 1711--1752. PMLR, 2021.

\bibitem{dvurechensky2016stochastic}
Pavel Dvurechensky and Alexander Gasnikov.
\newblock Stochastic intermediate gradient method for convex problems with
  stochastic inexact oracle.
\newblock {\em Journal of Optimization Theory and Applications}, 171:121--145,
  2016.

\bibitem{esser2010general}
Ernie Esser, Xiaoqun Zhang, and Tony~F Chan.
\newblock A general framework for a class of first order primal-dual algorithms
  for convex optimization in imaging science.
\newblock {\em SIAM Journal on Imaging Sciences}, 3(4):1015--1046, 2010.

\bibitem{even2023stochastic}
Mathieu Even.
\newblock Stochastic gradient descent under {M}arkovian sampling schemes.
\newblock {\em arXiv preprint arXiv:2302.14428}, 2023.

\bibitem{facchinei2007finite}
F.~Facchinei and J.S. Pang.
\newblock {\em Finite-Dimensional Variational Inequalities and Complementarity
  Problems}.
\newblock Springer Series in Operations Research and Financial Engineering.
  Springer New York, 2007.

\bibitem{gasnikov2018universal}
Alexander~Vladimirovich Gasnikov and Yu~E Nesterov.
\newblock Universal method for stochastic composite optimization problems.
\newblock {\em Computational Mathematics and Mathematical Physics}, 58:48--64,
  2018.

\bibitem{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{ghadimi2016mini}
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang.
\newblock Mini-batch stochastic approximation methods for nonconvex stochastic
  composite optimization.
\newblock {\em Mathematical Programming}, 155(1-2):267--305, 2016.

\bibitem{gidel2018variational}
Gauthier Gidel, Hugo Berard, Ga{\"e}tan Vignoud, Pascal Vincent, and Simon
  Lacoste-Julien.
\newblock A variational inequality perspective on generative adversarial
  networks.
\newblock {\em arXiv preprint arXiv:1802.10551}, 2018.

\bibitem{giles08}
Michael~B. Giles.
\newblock Multilevel monte carlo path simulation.
\newblock {\em Operations Research}, 56(3):607--617, 2008.

\bibitem{glynn2014exact}
Peter~W Glynn and Chang-han Rhee.
\newblock Exact estimation for markov chain equilibrium expectations.
\newblock {\em Journal of Applied Probability}, 51(A):377--389, 2014.

\bibitem{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2014.

\bibitem{GoodBengCour16}
Ian~J. Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock {\em Deep Learning}.
\newblock MIT Press, Cambridge, MA, USA, 2016.
\newblock \url{http://www.deeplearningbook.org}.

\bibitem{gorbunov2022stochastic}
Eduard Gorbunov, Hugo Berard, Gauthier Gidel, and Nicolas Loizou.
\newblock Stochastic extragradient: General analysis and improved rates.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 7865--7901. PMLR, 2022.

\bibitem{gorbunov2021near}
Eduard Gorbunov, Marina Danilova, Innokentiy Shibaev, Pavel Dvurechensky, and
  Alexander Gasnikov.
\newblock Near-optimal high probability complexity bounds for non-smooth
  stochastic optimization with heavy-tailed noise.
\newblock {\em arXiv preprint arXiv:2106.05958}, 2021.

\bibitem{han2021lower}
Yuze Han, Guangzeng Xie, and Zhihua Zhang.
\newblock Lower complexity bounds of finite-sum optimization problems: The
  results and construction.
\newblock {\em arXiv preprint arXiv:2103.08280}, 2021.

\bibitem{HarkerVIsurvey1990}
P.~T. Harker and J.-S. Pang.
\newblock Finite-dimensional variational inequality and nonlinear
  complementarity problems: a survey of theory, algorithms and applications.
\newblock {\em Mathematical programming}, 1990.

\bibitem{hsieh2019convergence}
Yu-Guan Hsieh, Franck Iutzeler, J{\'e}r{\^o}me Malick, and Panayotis
  Mertikopoulos.
\newblock On the convergence of single-call stochastic extra-gradient methods.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{hsieh2020explore}
Yu-Guan Hsieh, Franck Iutzeler, J{\'e}r{\^o}me Malick, and Panayotis
  Mertikopoulos.
\newblock Explore aggressively, update conservatively: Stochastic extragradient
  methods with variable stepsize scaling.
\newblock {\em Advances in Neural Information Processing Systems},
  33:16223--16234, 2020.

\bibitem{hu2009accelerated}
Chonghai Hu, Weike Pan, and James Kwok.
\newblock Accelerated gradient methods for stochastic optimization and online
  learning.
\newblock {\em Advances in Neural Information Processing Systems}, 22, 2009.

\bibitem{doi:10.1137/15M1031953}
A.~N. Iusem, A.~Jofr\'{e}, R.~I. Oliveira, and P.~Thompson.
\newblock Extragradient method with variance reduction for stochastic
  variational inequalities.
\newblock {\em SIAM Journal on Optimization}, 27(2):686--724, 2017.

\bibitem{iusem2019incremental}
Alfredo~N Iusem, Alejandro Jofr{\'e}, and Philip Thompson.
\newblock Incremental constraint projection methods for monotone stochastic
  variational inequalities.
\newblock {\em Mathematics of Operations Research}, 44(1):236--263, 2019.

\bibitem{jain2018accelerating}
Prateek Jain, Sham~M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron
  Sidford.
\newblock Accelerating stochastic gradient descent for least squares
  regression.
\newblock In {\em Conference On Learning Theory}, pages 545--604. PMLR, 2018.

\bibitem{JMLR:v18:16-595}
Prateek Jain, Sham~M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron
  Sidford.
\newblock Parallelizing stochastic gradient descent for least squares
  regression: Mini-batching, averaging, and model misspecification.
\newblock {\em Journal of Machine Learning Research}, 18(223):1--42, 2018.

\bibitem{4610024}
Houyuan Jiang and Huifu Xu.
\newblock Stochastic approximation approaches to the stochastic variational
  inequality problem.
\newblock {\em IEEE Transactions on Automatic Control}, 53(6):1462--1475, 2008.

\bibitem{Jin2020:mdp}
Yujia Jin and Aaron Sidford.
\newblock Efficiently solving {MDP}s with stochastic mirror descent.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning (ICML)}, volume 119, pages 4890--4900. PMLR, 2020.

\bibitem{Thorsten}
Thorsten Joachims.
\newblock A support vector method for multivariate performance measures.
\newblock pages 377--384, 01 2005.

\bibitem{juditsky2011solving}
Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel.
\newblock Solving variational inequalities with stochastic mirror-prox
  algorithm.
\newblock {\em Stochastic Systems}, 1(1):17--58, 2011.

\bibitem{kannan2019optimal}
Aswin Kannan and Uday~V Shanbhag.
\newblock Optimal stochastic extragradient schemes for pseudomonotone
  stochastic variational inequality problems and their variants.
\newblock {\em Computational Optimization and Applications}, 74(3):779--820,
  2019.

\bibitem{kidambi2018insufficiency}
Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham Kakade.
\newblock On the insufficiency of existing momentum schemes for stochastic
  optimization.
\newblock In {\em 2018 Information Theory and Applications Workshop (ITA)},
  pages 1--9. IEEE, 2018.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{koloskova2023shuffle}
Anastasia Koloskova, Nikita Doikov, Sebastian~U Stich, and Martin Jaggi.
\newblock Shuffle {SGD} is always better than {SGD}: Improved analysis of {SGD}
  with arbitrary data orders.
\newblock {\em arXiv preprint arXiv:2305.19259}, 2023.

\bibitem{korpelevich1976extragradient}
G.~M. Korpelevich.
\newblock The extragradient method for finding saddle points and other
  problems.
\newblock {\em Matecon}, 12:35--49, 1977.

\bibitem{lan2012optimal}
Guanghui Lan.
\newblock An optimal method for stochastic composite optimization.
\newblock {\em Mathematical Programming}, 133(1-2):365--397, 2012.

\bibitem{lan20}
Guanghui Lan.
\newblock {\em First-order and Stochastic Optimization Methods for Machine
  Learning}.
\newblock 01 2020.

\bibitem{pmlr-v89-liang19b}
Tengyuan Liang and James Stokes.
\newblock Interaction matters: A note on non-asymptotic local convergence of
  generative adversarial networks.
\newblock In Kamalika Chaudhuri and Masashi Sugiyama, editors, {\em Proceedings
  of the Twenty-Second International Conference on Artificial Intelligence and
  Statistics}, volume~89 of {\em Proceedings of Machine Learning Research},
  pages 907--915. PMLR, 16--18 Apr 2019.

\bibitem{lin2014smoothing}
Qihang Lin, Xi~Chen, and Javier Pena.
\newblock A smoothing stochastic gradient method for composite optimization.
\newblock {\em Optimization Methods and Software}, 29(6):1281--1301, 2014.

\bibitem{liu2018accelerating}
Chaoyue Liu and Mikhail Belkin.
\newblock Accelerating sgd with momentum for over-parameterized learning.
\newblock {\em arXiv preprint arXiv:1810.13395}, 2018.

\bibitem{lopes_sayed07}
Cassio~G. Lopes and Ali~H. Sayed.
\newblock Incremental adaptive strategies over distributed networks.
\newblock {\em IEEE Transactions on Signal Processing}, 55(8):4064--4077, 2007.

\bibitem{Madry2017:adv}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2018.

\bibitem{mao20}
Xianghui Mao, Kun Yuan, Yubin Hu, Yuantao Gu, Ali~H. Sayed, and Wotao Yin.
\newblock Walkman: A communication-efficient random-walk algorithm for
  decentralized optimization.
\newblock {\em IEEE Transactions on Signal Processing}, 68:2513--2528, 2020.

\bibitem{mertikopoulos2018optimistic}
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay
  Chandrasekhar, and Georgios Piliouras.
\newblock Optimistic mirror descent in saddle-point problems: Going the extra
  (gradient) mile.
\newblock {\em arXiv preprint arXiv:1807.02629}, 2018.

\bibitem{mishchenko2020random}
Konstantin Mishchenko, Ahmed Khaled, and Peter Richt{\'a}rik.
\newblock Random reshuffling: Simple analysis with vast improvements.
\newblock {\em Advances in Neural Information Processing Systems},
  33:17309--17320, 2020.

\bibitem{mishchenko2020revisiting}
Konstantin Mishchenko, Dmitry Kovalev, Egor Shulgin, Peter Richt{\'a}rik, and
  Yura Malitsky.
\newblock Revisiting stochastic extragradient.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 4573--4582. PMLR, 2020.

\bibitem{deeprl}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A. Rusu, Joel Veness,
  Marc~G. Bellemare, Alex Graves, Martin Riedmiller, Andreas~K. Fidjeland,
  Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis
  Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and
  Demis Hassabis.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529--533, 2015.

\bibitem{bachmoulines2011}
Eric Moulines and Francis Bach.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock In J.~Shawe-Taylor, R.~Zemel, P.~Bartlett, F.~Pereira, and K.Q.
  Weinberger, editors, {\em Advances in Neural Information Processing Systems},
  volume~24. Curran Associates, Inc., 2011.

\bibitem{nagaraj2020least}
Dheeraj Nagaraj, Xian Wu, Guy Bresler, Prateek Jain, and Praneeth Netrapalli.
\newblock Least squares regression with markovian data: Fundamental limits and
  algorithms.
\newblock {\em Advances in neural information processing systems},
  33:16666--16676, 2020.

\bibitem{nemirovski2004prox}
Arkadi Nemirovski.
\newblock Prox-method with rate of convergence {O(1/t)} for variational
  inequalities with lipschitz continuous monotone operators and smooth
  convex-concave saddle point problems.
\newblock {\em SIAM Journal on Optimization}, 15(1):229--251, 2004.

\bibitem{nesterov2005smooth}
Yu~Nesterov.
\newblock Smooth minimization of non-smooth functions.
\newblock {\em Mathematical programming}, 103(1):127--152, 2005.

\bibitem{doi:10.1137/100802001}
Yu. Nesterov.
\newblock Efficiency of coordinate descent methods on huge-scale optimization
  problems.
\newblock {\em SIAM Journal on Optimization}, 22(2):341--362, 2012.

\bibitem{nesterov_accelerated}
Yu.~E. Nesterov.
\newblock A method for solving the convex programming problem with convergence
  rate {$O(1/k^{2})$}.
\newblock {\em Dokl. Akad. Nauk SSSR}, 269(3):543--547, 1983.

\bibitem{nesterov2003introductory}
Yurii Nesterov.
\newblock {\em Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem{nesterov2007dual}
Yurii Nesterov.
\newblock Dual extrapolation and its applications to solving variational
  inequalities and related problems.
\newblock {\em Mathematical Programming}, 109(2):319--344, 2007.

\bibitem{NeumannGameTheory1944}
J.~Von Neumann and O.~Morgenstern.
\newblock {\em Theory of games and economic behavior}.
\newblock Princeton University Press, 1944.

\bibitem{Omidshafiei2017:rl}
Shayegan Omidshafiei, Jason Pazis, Christopher Amato, Jonathan~P. How, and John
  Vian.
\newblock Deep decentralized multi-task multi-agent reinforcement learning
  under partial observability.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning (ICML)}, volume~70, pages 2681--2690. PMLR, 2017.

\bibitem{palaniappan2016stochastic}
Balamurugan Palaniappan and Francis Bach.
\newblock Stochastic variance reduction methods for saddle-point problems.
\newblock {\em Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem{paulin_spectral}
Daniel Paulin.
\newblock {Concentration inequalities for Markov chains by Marton couplings and
  spectral methods}.
\newblock {\em Electronic Journal of Probability}, 20(none):1 -- 32, 2015.

\bibitem{peng2020training}
Wei Peng, Yu-Hong Dai, Hui Zhang, and Lizhi Cheng.
\newblock Training gans with centripetal acceleration.
\newblock {\em Optimization Methods and Software}, 35(5):955--973, 2020.

\bibitem{POLYAK1963864}
B.T. Polyak.
\newblock Gradient methods for the minimisation of functionals.
\newblock {\em USSR Computational Mathematics and Mathematical Physics},
  3(4):864--878, 1963.

\bibitem{10.1214/aoms/1177729586}
Herbert Robbins and Sutton Monro.
\newblock {A Stochastic Approximation Method}.
\newblock {\em The Annals of Mathematical Statistics}, 22(3):400 -- 407, 1951.

\bibitem{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In {\em International conference on machine learning}, pages
  1889--1897. PMLR, 2015.

\bibitem{scutari2010vi}
Gesualdo Scutari, Daniel Palomar, Francisco Facchinei, and Jong-shi Pang.
\newblock Convex optimization, game theory, and variational inequality theory.
\newblock {\em Signal Processing Magazine, IEEE}, 27:35 -- 49, 06 2010.

\bibitem{srikant2019finite}
Rayadurgam Srikant and Lei Ying.
\newblock Finite-time error bounds for linear stochastic approximation andtd
  learning.
\newblock In {\em Conference on Learning Theory}, pages 2803--2830. PMLR, 2019.

\bibitem{stich2019unified}
Sebastian~U Stich.
\newblock Unified optimal analysis of the (stochastic) gradient method.
\newblock {\em arXiv preprint arXiv:1907.04232}, 2019.

\bibitem{pmlr-v162-sun22b}
Tao Sun, Dongsheng Li, and Bao Wang.
\newblock Adaptive {R}andom {W}alk {G}radient {D}escent for {D}ecentralized
  {O}ptimization.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, {\em Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of {\em Proceedings
  of Machine Learning Research}, pages 20790--20809. PMLR, 17--23 Jul 2022.

\bibitem{sun2018markov}
Tao Sun, Yuejiao Sun, and Wotao Yin.
\newblock On {M}arkov chain gradient descent.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{pmlr-v28-sutskever13}
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In Sanjoy Dasgupta and David McAllester, editors, {\em Proceedings of
  the 30th International Conference on Machine Learning}, volume~28 of {\em
  Proceedings of Machine Learning Research}, pages 1139--1147, Atlanta,
  Georgia, USA, 17--19 Jun 2013. PMLR.

\bibitem{sutton1988learning}
Richard~S Sutton.
\newblock Learning to predict by the methods of temporal differences.
\newblock {\em Machine learning}, 3:9--44, 1988.

\bibitem{Sutton1998}
Richard~S. Sutton and Andrew~G. Barto.
\newblock {\em Reinforcement Learning: An Introduction}.
\newblock The MIT Press, second edition, 2018.

\bibitem{taylor2019stochastic}
Adrien Taylor and Francis Bach.
\newblock Stochastic first-order methods: non-asymptotic and computer-aided
  analyses via potential functions.
\newblock In {\em Conference on Learning Theory}, pages 2934--2992. PMLR, 2019.

\bibitem{doi:10.1137/S0363012998338806}
Paul Tseng.
\newblock A modified forward-backward splitting method for maximal monotone
  mappings.
\newblock {\em SIAM Journal on Control and Optimization}, 38(2):431--446, 2000.

\bibitem{van2000asymptotic}
Aad~W Van~der Vaart.
\newblock {\em Asymptotic statistics}, volume~3.
\newblock Cambridge university press, 2000.

\bibitem{vaswani2019fast}
Sharan Vaswani, Francis Bach, and Mark Schmidt.
\newblock Fast and faster convergence of {SGD} for over-parameterized models
  and an accelerated perceptron.
\newblock In {\em The 22nd international conference on artificial intelligence
  and statistics}, pages 1195--1204. PMLR, 2019.

\bibitem{DBLP:journals/corr/abs-1905-09997}
Sharan Vaswani, Aaron Mishkin, Issam~H. Laradji, Mark Schmidt, Gauthier Gidel,
  and Simon Lacoste{-}Julien.
\newblock Painless stochastic gradient: Interpolation, line-search, and
  convergence rates.
\newblock {\em CoRR}, abs/1905.09997, 2019.

\bibitem{wang2022stability}
Puyu Wang, Yunwen Lei, Yiming Ying, and Ding-Xuan Zhou.
\newblock Stability and generalization for markov chain stochastic gradient
  methods.
\newblock {\em arXiv preprint arXiv:2209.08005}, 2022.

\bibitem{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine learning}, 8:229--256, 1992.

\bibitem{wolfer2019estimating}
Geoffrey Wolfer and Aryeh Kontorovich.
\newblock Estimating the mixing time of ergodic markov chains.
\newblock In {\em Conference on Learning Theory}, pages 3120--3159. PMLR, 2019.

\bibitem{woodworth2021even}
Blake~E Woodworth and Nathan Srebro.
\newblock An even more optimal stochastic optimization algorithm: minibatching
  and interpolation learning.
\newblock {\em Advances in Neural Information Processing Systems},
  34:7333--7345, 2021.

\bibitem{NIPS2004_64036755}
Linli Xu, James Neufeld, Bryce Larson, and Dale Schuurmans.
\newblock Maximum margin clustering.
\newblock In L.~Saul, Y.~Weiss, and L.~Bottou, editors, {\em Advances in Neural
  Information Processing Systems}, volume~17. MIT Press, 2005.

\bibitem{NEURIPS2020_0cc6928e}
Junchi Yang, Negar Kiyavash, and Niao He.
\newblock Global convergence and variance reduction for a class of
  nonconvex-nonconcave minimax problems.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 1153--1165. Curran Associates, Inc., 2020.

\bibitem{Yu1997}
Bin Yu.
\newblock {\em Assouad, Fano, and Le Cam}, pages 423--435.
\newblock Springer New York, New York, NY, 1997.

\bibitem{zhang2019lower}
Junyu Zhang, Mingyi Hong, and Shuzhong Zhang.
\newblock On lower iteration complexity bounds for the saddle point problems.
\newblock {\em arXiv preprint arXiv:1912.07481}, 2019.

\end{thebibliography}
