@article{SCINet,
  title={Scinet: Time series modeling and forecasting with sample convolution and interaction},
  author={Liu, Minhao and Zeng, Ailing and Chen, Muxi and Xu, Zhijian and Lai, Qiuxia and Ma, Lingna and Xu, Qiang},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={5816--5828},
  year={2022}
}

@inproceedings{LSTNet,
  title={Modeling long-and short-term temporal patterns with deep neural networks},
  author={Lai, Guokun and Chang, Wei-Cheng and Yang, Yiming and Liu, Hanxiao},
  booktitle={The 41st international ACM SIGIR conference on research \& development in information retrieval},
  pages={95--104},
  year={2018}
}

@article{EnergyData,
  title={Data driven prediction models of energy use of appliances in a low-energy house},
  author={Candanedo, Luis M and Feldheim, V{\'e}ronique and Deramaix, Dominique},
  journal={Energy and buildings},
  volume={140},
  pages={81--97},
  year={2017},
  publisher={Elsevier}
}

@article{Timesnet,
  title={TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis},
  author={Wu, Haixu and Hu, Tengge and Liu, Yong and Zhou, Hang and Wang, Jianmin and Long, Mingsheng},
  journal={ICLR},
  year={2023}
}

@article{Unet,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  journal={Medical Image Computing and Computer-Assisted Intervention--MICCAI 2015: 18th International Conference},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@article{Pyraformer,
  title={Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting},
  author={Liu, Shizhan and Yu, Hang and Liao, Cong and Li, Jianguo and Lin, Weiyao and Liu, Alex X and Dustdar, Schahram},
  journal={International conference on learning representations},
  year={2021}
}

@article{DSTP-RNN,
  title={DSTP-RNN: A dual-stage two-phase attention-based recurrent neural network for long-term and multivariate time series prediction},
  author={Liu, Yeqi and Gong, Chuanyang and Yang, Ling and Chen, Yingyi},
  journal={Expert Systems with Applications},
  volume={143},
  pages={113082},
  year={2020},
  publisher={Elsevier}
}

@article{Stationary,
  title={Non-stationary Transformers: Rethinking the Stationarity in Time Series Forecasting},
  author={Yong Liu and Haixu Wu and Jianmin Wang and Mingsheng Long},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{DLinear,
  title={Are transformers effective for time series forecasting?},
  author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={37},
  pages={11121--11128},
  year={2023}
}

@article{Nbeats,
  title={N-{BEATS}: Neural basis expansion analysis for interpretable time series forecasting},
  author={Oreshkin, Boris N and Carpov, Dmitri and Chapados, Nicolas and Bengio, Yoshua},
  journal={ICLR},
  year={2019}
}

@article{LayerNorm,
 author = {Ba, Jimmy Lei  and  Kiros, Jamie Ryan  and  Hinton, Geoffrey E.},
 journal = {https://arxiv.org/pdf/1607.06450.pdf},
 title = {Layer Normalization},
 year = {2016}
}

@article{Pytorch,
  title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author={Adam Paszke and S. Gross and Francisco Massa and A. Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Z. Lin and N. Gimelshein and L. Antiga and Alban Desmaison and Andreas K{\"o}pf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
  journal={NeurIPS},
  year={2019}
}

@article{Nhits,
  title={N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting},
  author={Challu, Cristian and Olivares, Kin G and Oreshkin, Boris N and Garza, Federico and Mergenthaler, Max and Dubrawski, Artur},
  journal={arXiv preprint arXiv:2201.12886},
  year={2022}
}

@article{salinas2019high,
  title={High-dimensional multivariate forecasting with low-rank gaussian copula processes},
  author={Salinas, David and Bohlke-Schneider, Michael and Callot, Laurent and Medico, Roberto and Gasthaus, Jan},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{RNN1,
  title={Deep state space models for time series forecasting},
  author={Syama Sundar Rangapuram and Matthias W Seeger and Jan Gasthaus and Lorenzo Stella and Yuyang Wang and Tim Januschowski},
  journal={NeurIPS},
  year={2018}
}

@article{LSTMnetwork,
  title={LSTM network: a deep learning approach for short-term traffic forecast},
  author={Zhao, Zheng and Chen, Weihai and Wu, Xingming and Chen, Peter CY and Liu, Jingmeng},
  journal={IET Intelligent Transport Systems},
  volume={11},
  number={2},
  pages={68--75},
  year={2017},
  publisher={Wiley Online Library}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={NeurIPS},
  year={2020}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={ICLR},
  year={2021}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{lee2019set,
  title={Set transformer: A framework for attention-based permutation-invariant neural networks},
  author={Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
  booktitle={International conference on machine learning},
  pages={3744--3753},
  year={2019},
  organization={PMLR}
}

@article{bai1803empirical,
  title={An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal={arXiv preprint arXiv:1803.01271},
  volume={2},
  year={2018}
}

@article{das2023long,
  title={Long-term Forecasting with TiDE: Time-series Dense Encoder},
  author={Das, Abhimanyu and Kong, Weihao and Leach, Andrew and Sen, Rajat and Yu, Rose},
  journal={arXiv preprint arXiv:2304.08424},
  year={2023}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}


@article{kim2021reversible,
  title={Reversible instance normalization for accurate time-series forecasting against distribution shift},
  author={Kim, Taesung and Kim, Jinhee and Tae, Yunwon and Park, Cheonbok and Choi, Jang-Ho and Choo, Jaegul},
  journal={ICLR},
  year={2021}
}

@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}

@article{li2023revisiting,
  title={Revisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping},
  author={Li, Zhe and Qi, Shiyi and Li, Yiduo and Xu, Zenglin},
  journal={arXiv preprint arXiv:2305.10721},
  year={2023}
}

@article{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={ICLR},
  year={2020}
}

@inproceedings{liu2021pyraformer,
  title={Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting},
  author={Liu, Shizhan and Yu, Hang and Liao, Cong and Li, Jianguo and Lin, Weiyao and Liu, Alex X and Dustdar, Schahram},
  booktitle={International conference on learning representations},
  year={2021}
}

@article{wu2022flowformer,
  title={Flowformer: Linearizing transformers with conservation flows},
  author={Wu, Haixu and Wu, Jialong and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  journal={ICML},
  year={2022}
}

@article{Kornblith2019SimilarityON,
  title={Similarity of Neural Network Representations Revisited},
  author={Simon Kornblith and Mohammad Norouzi and Honglak Lee and Geoffrey E. Hinton},
  journal={ICML},
  year={2019},
}

@article{dong2023simmtm,
  title={SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling},
  author={Dong, Jiaxiang and Wu, Haixu and Zhang, Haoran and Zhang, Li and Wang, Jianmin and Long, Mingsheng},
  journal={arXiv preprint arXiv:2302.00861},
  year={2023}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={NeurIPS},
  year={2022}
}

@article{salinas2020deepar,
  title={DeepAR: Probabilistic forecasting with autoregressive recurrent networks},
  author={Salinas, David and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim},
  journal={International Journal of Forecasting},
  volume={36},
  number={3},
  pages={1181--1191},
  year={2020},
  publisher={Elsevier}
}

@article{tolstikhin2021mlp,
  title={Mlp-mixer: An all-mlp architecture for vision},
  author={Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and others},
  journal={NeurIPS},
  year={2021}
}

@article{han2023capacity,
  title={The Capacity and Robustness Trade-off: Revisiting the Channel Independent Strategy for Multivariate Time Series Forecasting},
  author={Han, Lu and Ye, Han-Jia and Zhan, De-Chuan},
  journal={arXiv preprint arXiv:2304.05206},
  year={2023}
}

@article{liu2023koopa,
  title={Koopa: Learning Non-stationary Time Series Dynamics with Koopman Predictors},
  author={Liu, Yong and Li, Chenyu and Wang, Jianmin and Long, Mingsheng},
  journal={arXiv preprint arXiv:2305.18803},
  year={2023}
}

@article{box1968some,
  title={Some recent advances in forecasting and control},
  author={Box, George EP and Jenkins, Gwilym M},
  journal={Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  volume={17},
  number={2},
  pages={91--109},
  year={1968},
  publisher={JSTOR}
}

@book{hyndman2018forecasting,
  title={Forecasting: principles and practice},
  author={Hyndman, Rob J and Athanasopoulos, George},
  year={2018},
  publisher={OTexts}
}

@article{GaussianProcess,
  title={Gaussian process priors with uncertain inputs application to multiple-step ahead time series forecasting},
  author={Girard, Agathe and Rasmussen, Carl and Candela, Joaquin Q and Murray-Smith, Roderick},
  journal={Advances in neural information processing systems},
  volume={15},
  year={2002}
}
@article{friedman2001greedy,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  pages={1189--1232},
  year={2001},
  publisher={JSTOR}
}

@article{SVM,
  title={Financial time series forecasting using support vector machines},
  author={Kim, Kyoung-jae},
  journal={Neurocomputing},
  volume={55},
  number={1-2},
  pages={307--319},
  year={2003},
  publisher={Elsevier}
}

@article{ahmed2010empirical,
  title={An empirical comparison of machine learning models for time series forecasting},
  author={Ahmed, Nesreen K and Atiya, Amir F and Gayar, Neamat El and El-Shishiny, Hisham},
  journal={Econometric reviews},
  volume={29},
  number={5-6},
  pages={594--621},
  year={2010},
  publisher={Taylor \& Francis}
}

@inproceedings{lai2018modeling,
  title={Modeling long-and short-term temporal patterns with deep neural networks},
  author={Lai, Guokun and Chang, Wei-Cheng and Yang, Yiming and Liu, Hanxiao},
  booktitle={The 41st international ACM SIGIR conference on research \& development in information retrieval},
  pages={95--104},
  year={2018}
}

@article{sagheer2019time,
  title={Time series forecasting of petroleum production using deep LSTM recurrent networks},
  author={Sagheer, Alaa and Kotb, Mostafa},
  journal={Neurocomputing},
  volume={323},
  pages={203--213},
  year={2019},
  publisher={Elsevier}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}

@article{wan2019multivariate,
  title={Multivariate temporal convolutional network: A deep neural networks approach for multivariate time series forecasting},
  author={Wan, Renzhuo and Mei, Shuping and Wang, Jun and Liu, Min and Yang, Fan},
  journal={Electronics},
  volume={8},
  number={8},
  pages={876},
  year={2019},
  publisher={MDPI}
}

@article{hewage2020temporal,
  title={Temporal convolutional neural (TCN) network for an effective weather forecasting using time-series data from the local weather station},
  author={Hewage, Pradeep and Behera, Ardhendu and Trovati, Marcello and Pereira, Ella and Ghahremani, Morteza and Palmieri, Francesco and Liu, Yonghuai},
  journal={Soft Computing},
  volume={24},
  pages={16453--16482},
  year={2020},
  publisher={Springer}
}

@article{LogSparseTransformer,
  title={Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting},
  author={Li, Shiyang and Jin, Xiaoyong and Xuan, Yao and Zhou, Xiyou and Chen, Wenhu and Wang, Yu-Xiang and Yan, Xifeng},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{Reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@inproceedings{Informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  pages={11106--11115},
  year={2021}
}

@inproceedings{Fedformer,
  title={Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting},
  author={Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Wang, Xue and Sun, Liang and Jin, Rong},
  booktitle={International conference on machine learning},
  pages={27268--27286},
  year={2022},
  organization={PMLR}
}

@inproceedings{Crossformer,
  title={Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting},
  author={Zhang, Yunhao and Yan, Junchi},
  booktitle={The eleventh international conference on learning representations},
  year={2022}
}

@article{Autoformer,
  title={Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting},
  author={Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={22419--22430},
  year={2021}
}

@article{FreTS,
  title={Frequency-domain MLPs are more effective learners in time series forecasting},
  author={Yi, Kun and Zhang, Qi and Fan, Wei and Wang, Shoujin and Wang, Pengyang and He, Hui and An, Ning and Lian, Defu and Cao, Longbing and Niu, Zhendong},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{TSMixer,
  title={Tsmixer: Lightweight mlp-mixer model for multivariate time series forecasting},
  author={Ekambaram, Vijay and Jati, Arindam and Nguyen, Nam and Sinthong, Phanwadee and Kalagnanam, Jayant},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={459--469},
  year={2023}
}

@article{TiDE,
  title={Long-term forecasting with tide: Time-series dense encoder},
  author={Das, Abhimanyu and Kong, Weihao and Leach, Andrew and Mathur, Shaan and Sen, Rajat and Yu, Rose},
  journal={arXiv preprint arXiv:2304.08424},
  year={2023}
}

@article{RLinear,
  title={Revisiting long-term time series forecasting: An investigation on linear mapping},
  author={Li, Zhe and Qi, Shiyi and Li, Yiduo and Xu, Zenglin},
  journal={arXiv preprint arXiv:2305.10721},
  year={2023}
}

@article{iTransformer,
  title={itransformer: Inverted transformers are effective for time series forecasting},
  author={Liu, Yong and Hu, Tengge and Zhang, Haoran and Wu, Haixu and Wang, Shiyu and Ma, Lintao and Long, Mingsheng},
  journal={arXiv preprint arXiv:2310.06625},
  year={2023}
}

@article{Transformer,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{PatchTST,
  title={A time series is worth 64 words: Long-term forecasting with transformers},
  author={Nie, Yuqi and Nguyen, Nam H and Sinthong, Phanwadee and Kalagnanam, Jayant},
  journal={arXiv preprint arXiv:2211.14730},
  year={2022}
}

@inproceedings{boser1992training,
  title={A training algorithm for optimal margin classifiers},
  author={Boser, Bernhard E and Guyon, Isabelle M and Vapnik, Vladimir N},
  booktitle={Proceedings of the fifth annual workshop on Computational learning theory},
  pages={144--152},
  year={1992}
}

@inproceedings{ho1995random,
  title={Random decision forests},
  author={Ho, Tin Kam},
  booktitle={Proceedings of 3rd international conference on document analysis and recognition},
  volume={1},
  pages={278--282},
  year={1995},
  organization={IEEE}
}

@article{hinton2006fast,
  title={A fast learning algorithm for deep belief nets},
  author={Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  journal={Neural computation},
  volume={18},
  number={7},
  pages={1527--1554},
  year={2006},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@book{rosenblatt1957perceptron,
  title={The perceptron, a perceiving and recognizing automaton Project Para},
  author={Rosenblatt, Frank},
  year={1957},
  publisher={Cornell Aeronautical Laboratory}
}

@article{hu1964adaptive,
  title={An adaptive data processing system for weather forecasting},
  author={Hu, MJC and Root, Halbert E},
  journal={Journal of Applied Meteorology and Climatology},
  volume={3},
  number={5},
  pages={513--523},
  year={1964}
}

@article{arima,
  title={Time series analysis using autoregressive integrated moving average (ARIMA) models},
  author={Nelson, Brian K},
  journal={Academic emergency medicine},
  volume={5},
  number={7},
  pages={739--744},
  year={1998},
  publisher={Wiley Online Library}
}

@inproceedings{WaveForM,
  title={WaveForM: Graph enhanced wavelet learning for long sequence forecasting of multivariate time series},
  author={Yang, Fuhao and Li, Xin and Wang, Min and Zang, Hongyu and Pang, Wei and Wang, Mingzhong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  pages={10754--10761},
  year={2023}
}

@article{FreDo,
  title={Fredo: frequency domain-based long-term time series forecasting},
  author={Sun, Fan-Keng and Boning, Duane S},
  journal={arXiv preprint arXiv:2205.12301},
  year={2022}
}

@article{liu2020dstp,
  title={DSTP-RNN: A dual-stage two-phase attention-based recurrent neural network for long-term and multivariate time series prediction},
  author={Liu, Yeqi and Gong, Chuanyang and Yang, Ling and Chen, Yingyi},
  journal={Expert Systems with Applications},
  volume={143},
  pages={113082},
  year={2020},
  publisher={Elsevier}
}

@article{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{hsu2019measuring,
  title={Measuring the effects of non-identical data distribution for federated visual classification},
  author={Hsu, Tzu-Ming Harry and Qi, Hang and Brown, Matthew},
  journal={arXiv preprint arXiv:1909.06335},
  year={2019}
}

@article{PyTroch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{Adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@ARTICLE{9446858,
  author={Song, Donghwan and Chung Baek, Adrian Matias and Kim, Namhun},
  journal={IEEE Access}, 
  title={Forecasting Stock Market Indices Using Padding-Based Fourier Transform Denoising and Time Series Deep Learning Models}, 
  year={2021},
  volume={9},
  number={},
  pages={83786-83796},
  keywords={Logic gates;Predictive models;Noise reduction;Time series analysis;Biological system modeling;Mathematical model;Indexes;Deep learning;denoising framework;Fourier transform;stock index prediction;time series},
  doi={10.1109/ACCESS.2021.3086537}}

@INPROCEEDINGS{9770652,
  author={Kahraman, A. and Yang, G. and Hou, P.},
  booktitle={20th International Workshop on Large-Scale Integration of Wind Power into Power Systems as well as on Transmission Networks for Offshore Wind Power Plants (WIW 2021)}, 
  title={Wind power forecasting using LSTM incorporating fourier transformation based denoising technique}, 
  year={2021},
  volume={2021},
  number={},
  pages={94-98},
  keywords={},
  doi={10.1049/icp.2021.2604}}

@inproceedings{
FITS,
title={{FITS}: Modeling Time Series with \$10k\$ Parameters},
author={Zhijian Xu and Ailing Zeng and Qiang Xu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=bWcnvZ3qMb}
}

@book{bloomfield2004fourier,
  title={Fourier analysis of time series: an introduction},
  author={Bloomfield, Peter},
  year={2004},
  publisher={John Wiley \& Sons}
}

@article{pandiyan2020analysis,
  title={Analysis of time, frequency and time-frequency domain features from acoustic emissions during Laser Powder-Bed fusion process},
  author={Pandiyan, Vigneashwara and Drissi-Daoudi, Rita and Shevchik, Sergey and Masinelli, Giulio and Log{\'e}, Roland and Wasmer, Kilian},
  journal={Procedia CIRP},
  volume={94},
  pages={392--397},
  year={2020},
  publisher={Elsevier}
}

@article{wen2022transformers,
  title={Transformers in time series: A survey},
  author={Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
  journal={arXiv preprint arXiv:2202.07125},
  year={2022}
}

@article{rezaei2021stock,
  title={Stock price prediction using deep learning and frequency decomposition},
  author={Rezaei, Hadi and Faaljou, Hamidreza and Mansourfar, Gholamreza},
  journal={Expert Systems with Applications},
  volume={169},
  pages={114332},
  year={2021},
  publisher={Elsevier}
}

@article{GELU,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@article{RELU,
  title={Deep learning using rectified linear units (relu)},
  author={Agarap, Abien Fred},
  journal={arXiv preprint arXiv:1803.08375},
  year={2018}
}

@inproceedings{RevIN,
  title={Reversible instance normalization for accurate time-series forecasting against distribution shift},
  author={Kim, Taesung and Kim, Jinhee and Tae, Yunwon and Park, Cheonbok and Choi, Jang-Ho and Choo, Jaegul},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{bptt,
  title={Backpropagation through time: what it does and how to do it},
  author={Werbos, Paul J},
  journal={Proceedings of the IEEE},
  volume={78},
  number={10},
  pages={1550--1560},
  year={1990},
  publisher={IEEE}
}

@article{krapf2018power,
  title={Power spectral density of a single Brownian trajectory: what one can and cannot learn from it},
  author={Krapf, Diego and Marinari, Enzo and Metzler, Ralf and Oshanin, Gleb and Xu, Xinran and Squarcini, Alessio},
  journal={New Journal of Physics},
  volume={20},
  number={2},
  pages={023029},
  year={2018},
  publisher={IOP Publishing}
}

@misc{wiki:BrownianNoise,
  author = "MickOhrberg",
  title = "Brownian noise --- {Wikipedia}{,} The Free Encyclopedia",
  year = "2024",
  url = "https://en.wikipedia.org/wiki/Brownian_noise",
  note = "[Online; accessed 9-April-2024]"
}

@article{moreno2023deep,
  title={Deep autoregressive models with spectral attention},
  author={Moreno-Pino, Fernando and Olmos, Pablo M and Art{\'e}s-Rodr{\'\i}guez, Antonio},
  journal={Pattern Recognition},
  volume={133},
  pages={109014},
  year={2023},
  publisher={Elsevier}
}