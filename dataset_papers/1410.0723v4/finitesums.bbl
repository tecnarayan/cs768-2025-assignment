\begin{thebibliography}{15}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2012)Agarwal, Negahban, and
  Wainwright]{AgarwalNeWa2012}
Alekh Agarwal, Sahand Negahban, and Martin~J Wainwright.
\newblock Fast global convergence of gradient methods for high-dimensional
  statistical recovery.
\newblock \emph{The Annals of Statistics}, 40\penalty0 (5):\penalty0
  2452--2482, 2012.

\bibitem[{Arjevani}(2014)]{Arjevani2014}
Y.~{Arjevani}.
\newblock {On Lower and Upper Bounds in Smooth Strongly Convex Optimization - A
  Unified Approach via Linear Iterative Methods}.
\newblock \emph{ArXiv e-prints}, 2014.

\bibitem[Bertsekas(2012)]{bertsekas-2010}
Dimitri~P. Bertsekas.
\newblock Incremental gradient, subgradient, and proximal methods for convex
  optimization: A survey.
\newblock In S.~Sra, S.~Nowozin, and S.~J. Wright, editors, \emph{Optimization
  for Machine Learning}, pages 85--119. MIT Press, 2012.
\newblock Extended version: LIDS report LIDS-P2848, MIT, 2010.

\bibitem[Bottou and Bousquet(2008)]{bottou-bousquet-2008}
L\'{e}on Bottou and Olivier Bousquet.
\newblock The tradeoffs of large scale learning.
\newblock In J.C. Platt, D.~Koller, Y.~Singer, and S.~Roweis, editors,
  \emph{Advances in Neural Information Processing Systems}, volume~20, pages
  161--168. NIPS Foundation (http://books.nips.cc), 2008.
\newblock URL \url{http://leon.bottou.org/papers/bottou-bousquet-2008}.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and Lacoste-Julien]{Defazio2014}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2014.

\bibitem[Johnson and Zhang(2013)]{JohnsonZh2013}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In C.J.C. Burges, L.~Bottou, M.~Welling, Z.~Ghahramani, and K.Q.
  Weinberger, editors, \emph{Advances in Neural Information Processing Systems
  26}, pages 315--323. 2013.

\bibitem[{Le Roux} et~al.(2012){Le Roux}, Schmidt, and Bach]{RouxScBa2012}
Nicolas {Le Roux}, Mark Schmidt, and Francis Bach.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock In F.~Pereira, C.J.C. Burges, L.~Bottou, and K.Q. Weinberger,
  editors, \emph{Advances in Neural Information Processing Systems 25}, pages
  2663--2671. 2012.

\bibitem[Nemirovsky and Yudin(1983)]{nemirovsky-yudin-1983}
Arkadi Nemirovsky and David~B. Yudin.
\newblock \emph{Problem Complexity and Method Efficiency in Optimization}.
\newblock Interscience Series in Discrete Mathematics. Wiley, 1983.

\bibitem[Nesterov(2004)]{nesterov-2004}
Yurii Nesterov.
\newblock \emph{Introductory Lectures on Convex Optimization}.
\newblock Kluwer Academic Publisher, 2004.

\bibitem[Nesterov(2007)]{Nesterov2007}
Yurii Nesterov.
\newblock Gradient methods for minimizing composite objective function, 2007.

\bibitem[Schmidt et~al.(2013)Schmidt, {Le Roux}, and Bach]{SchmidtRoBa2013}
Mark Schmidt, Nicolas {Le Roux}, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{arXiv preprint arXiv:1309.2388}, 2013.

\bibitem[Shalev-Shwartz and Zhang(2013)]{Shalev-ShwartzZh2013}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss.
\newblock \emph{The Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 567--599, 2013.

\bibitem[Shalev{-}Shwartz and Zhang(2014)]{Shalev-ShwartzZh2014}
Shai Shalev{-}Shwartz and Tong Zhang.
\newblock Accelerated proximal stochastic dual coordinate ascent for
  regularized loss minimization.
\newblock In \emph{Proceedings of the 31th International Conference on Machine
  Learning, {ICML} 2014, Beijing, China, 21-26 June 2014}, pages 64--72, 2014.

\bibitem[Vershynin(2012)]{Vershynin2012}
Roman Vershynin.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock In Yonina~C. Eldar and Gitta Kutyniok, editors, \emph{Compressed
  Sensing}, pages 210--268. Cambridge University Press, 2012.

\bibitem[Zhang and Xiao(2014)]{ZhangXi2014}
Yuchen Zhang and Lin Xiao.
\newblock Stochastic primal-dual coordinate method for regularized empirical
  risk minimization.
\newblock Technical Report MSR-TR-2014-123, September 2014.

\end{thebibliography}
