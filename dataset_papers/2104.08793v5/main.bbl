\begin{thebibliography}{10}

\bibitem{andreas2017learning}
Jacob Andreas, Dan Klein, and Sergey Levine.
\newblock Learning with latent language.
\newblock {\em arXiv preprint arXiv:1711.00482}, 2017.

\bibitem{bastings2020elephant}
Jasmijn Bastings and Katja Filippova.
\newblock The elephant in the interpretability room: Why use attention as
  explanation when we have saliency methods?
\newblock {\em arXiv preprint arXiv:2010.05607}, 2020.

\bibitem{bastings2019interpretable}
Joost Bastings, Wilker Aziz, and Ivan Titov.
\newblock Interpretable neural predictions with differentiable binary
  variables.
\newblock {\em arXiv preprint arXiv:1905.08160}, 2019.

\bibitem{battaglia2018relational}
Peter~W Battaglia, Jessica~B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez,
  Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam
  Santoro, Ryan Faulkner, et~al.
\newblock Relational inductive biases, deep learning, and graph networks.
\newblock {\em arXiv preprint arXiv:1806.01261}, 2018.

\bibitem{bosselut2019dynamic}
Antoine Bosselut and Yejin Choi.
\newblock Dynamic knowledge graph construction for zero-shot commonsense
  question answering.
\newblock {\em arXiv preprint arXiv:1911.03876}, 2019.

\bibitem{chen2019codah}
Michael Chen, Mike D{'}Arcy, Alisa Liu, Jared Fernandez, and Doug Downey.
\newblock {CODAH}: An adversarially-authored question answering dataset for
  common sense.
\newblock In {\em Proceedings of the 3rd Workshop on Evaluating Vector Space
  Representations for {NLP}}, pages 63--69, Minneapolis, USA, June 2019.
  Association for Computational Linguistics.

\bibitem{chen2017neural}
Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Diana Inkpen, and Si~Wei.
\newblock Neural natural language inference models enhanced with external
  knowledge.
\newblock {\em arXiv preprint arXiv:1711.04289}, 2017.

\bibitem{co2018guiding}
John~D Co-Reyes, Abhishek Gupta, Suvansh Sanjeev, Nick Altieri, Jacob Andreas,
  John DeNero, Pieter Abbeel, and Sergey Levine.
\newblock Guiding policies with language via meta-learning.
\newblock {\em arXiv preprint arXiv:1811.07882}, 2018.

\bibitem{davis2015commonsense}
Ernest Davis and Gary Marcus.
\newblock Commonsense reasoning and commonsense knowledge in artificial
  intelligence.
\newblock {\em Communications of the ACM}, 58(9):92--103, 2015.

\bibitem{denil2014extraction}
Misha Denil, Alban Demiraj, and Nando De~Freitas.
\newblock Extraction of salient sentences from labelled documents.
\newblock {\em arXiv preprint arXiv:1412.6815}, 2014.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of NAACL)}, pages 4171--4186, Minneapolis,
  Minnesota, June 2019. Association for Computational Linguistics.

\bibitem{deyoung2019eraser}
Jay DeYoung, Sarthak Jain, Nazneen~Fatema Rajani, Eric Lehman, Caiming Xiong,
  Richard Socher, and Byron~C Wallace.
\newblock Eraser: A benchmark to evaluate rationalized nlp models.
\newblock {\em arXiv preprint arXiv:1911.03429}, 2019.

\bibitem{feng2020scalable}
Yanlin Feng, Xinyue Chen, Bill~Yuchen Lin, Peifeng Wang, Jun Yan, and Xiang
  Ren.
\newblock Scalable multi-hop relational reasoning for knowledge-aware question
  answering.
\newblock {\em arXiv preprint arXiv:2005.00646}, 2020.

\bibitem{ghaeini2018interpreting}
Reza Ghaeini, Xiaoli~Z Fern, and Prasad Tadepalli.
\newblock Interpreting recurrent and attention-based neural models: a case
  study on natural language inference.
\newblock {\em arXiv preprint arXiv:1808.03894}, 2018.

\bibitem{gunning2018machine}
David Gunning.
\newblock Machine common sense concept paper.
\newblock {\em arXiv preprint arXiv:1810.07528}, 2018.

\bibitem{hase2021can}
Peter Hase and Mohit Bansal.
\newblock When can models learn from explanations? a formal framework for
  understanding the roles of explanation data.
\newblock {\em arXiv preprint arXiv:2102.02201}, 2021.

\bibitem{hase2020leakage}
Peter Hase, Shiyue Zhang, Harry Xie, and Mohit Bansal.
\newblock Leakage-adjusted simulatability: Can models generate non-trivial
  explanations of their behavior in natural language?
\newblock {\em arXiv preprint arXiv:2010.04119}, 2020.

\bibitem{honnibal-johnson:2015:EMNLP}
Matthew Honnibal and Mark Johnson.
\newblock An improved non-monotonic transition system for dependency parsing.
\newblock In {\em Proceedings of the 2015 Conference on Empirical Methods in
  Natural Language Processing}, pages 1373--1378, Lisbon, Portugal, September
  2015. Association for Computational Linguistics.

\bibitem{huang2020graphlime}
Qiang Huang, Makoto Yamada, Yuan Tian, Dinesh Singh, Dawei Yin, and Yi~Chang.
\newblock Graphlime: Local interpretable model explanations for graph neural
  networks.
\newblock {\em arXiv preprint arXiv:2001.06216}, 2020.

\bibitem{jain2019attention}
Sarthak Jain and Byron~C Wallace.
\newblock Attention is not explanation.
\newblock {\em arXiv preprint arXiv:1902.10186}, 2019.

\bibitem{jain2020learning}
Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, and Byron~C Wallace.
\newblock Learning to faithfully rationalize by construction.
\newblock {\em arXiv preprint arXiv:2005.00115}, 2020.

\bibitem{kadar2017representation}
Akos K{\'a}d{\'a}r, Grzegorz Chrupa{\l}a, and Afra Alishahi.
\newblock Representation of linguistic form and function in recurrent neural
  networks.
\newblock {\em Computational Linguistics}, 43(4):761--780, 2017.

\bibitem{khot2020qasc}
Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal.
\newblock {QASC:} {A} dataset for question answering via sentence composition.
\newblock In {\em The Thirty-Fourth {AAAI} Conference on Artificial
  Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of
  Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium
  on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York,
  NY, USA, February 7-12, 2020}, pages 8082--8090. {AAAI} Press, 2020.

\bibitem{lan2019albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock {\em arXiv preprint arXiv:1909.11942}, 2019.

\bibitem{lee2017interactive}
Jaesong Lee, Joong-Hwi Shin, and Jun-Seok Kim.
\newblock Interactive visualization and manipulation of attention-based neural
  machine translation.
\newblock In {\em Proceedings of the 2017 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 121--126, 2017.

\bibitem{lee2018graph}
John~Boaz Lee, Ryan Rossi, and Xiangnan Kong.
\newblock Graph classification using structural attention.
\newblock In {\em Proceedings of the 24th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 1666--1674, 2018.

\bibitem{lee2019self}
Junhyun Lee, Inyeop Lee, and Jaewoo Kang.
\newblock Self-attention graph pooling.
\newblock In {\em International Conference on Machine Learning}, pages
  3734--3743. PMLR, 2019.

\bibitem{lei2016rationalizing}
Tao Lei, Regina Barzilay, and Tommi Jaakkola.
\newblock Rationalizing neural predictions.
\newblock {\em arXiv preprint arXiv:1606.04155}, 2016.

\bibitem{li2015visualizing}
Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.
\newblock Visualizing and understanding neural models in nlp.
\newblock {\em arXiv preprint arXiv:1506.01066}, 2015.

\bibitem{li2016understanding}
Jiwei Li, Will Monroe, and Dan Jurafsky.
\newblock Understanding neural networks through representation erasure.
\newblock {\em arXiv preprint arXiv:1612.08220}, 2016.

\bibitem{lin2019kagnet}
Bill~Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren.
\newblock {K}ag{N}et: Knowledge-aware graph networks for commonsense reasoning.
\newblock In {\em Proceedings of EMNLP-IJCNLP}, pages 2829--2839, Hong Kong,
  China, November 2019. Association for Computational Linguistics.

\bibitem{lin-etal-2020-commongen}
Bill~Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula,
  Yejin Choi, and Xiang Ren.
\newblock {C}ommon{G}en: A constrained text generation challenge for generative
  commonsense reasoning.
\newblock In {\em Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pages 1823--1840, Online, November 2020. Association for
  Computational Linguistics.

\bibitem{liu2020kg}
Ye~Liu, Yao Wan, Lifang He, Hao Peng, and Philip~S Yu.
\newblock Kg-bart: Knowledge graph-augmented bart for generative commonsense
  reasoning.
\newblock {\em arXiv preprint arXiv:2009.12677}, 2020.

\bibitem{liu2020commonsense}
Ye~Liu, Tao Yang, Zeyu You, Wei Fan, and Philip~S Yu.
\newblock Commonsense evidence generation and injection in reading
  comprehension.
\newblock {\em arXiv preprint arXiv:2005.05240}, 2020.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{lv2020graph}
Shangwen Lv, Daya Guo, Jingjing Xu, Duyu Tang, Nan Duan, Ming Gong, Linjun
  Shou, Daxin Jiang, Guihong Cao, and Songlin Hu.
\newblock Graph-based reasoning over heterogeneous external knowledge for
  commonsense question answering.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 8449--8456, 2020.

\bibitem{ma2019towards}
Kaixin Ma, Jonathan Francis, Quanyang Lu, Eric Nyberg, and Alessandro
  Oltramari.
\newblock Towards generalizable neuro-symbolic systems for commonsense question
  answering.
\newblock In {\em Proceedings of the First Workshop on Commonsense Inference in
  Natural Language Processing}, pages 22--32, Hong Kong, China, November 2019.
  Association for Computational Linguistics.

\bibitem{marcus2018deep}
Gary Marcus.
\newblock Deep learning: A critical appraisal.
\newblock {\em arXiv preprint arXiv:1801.00631}, 2018.

\bibitem{mihaylov2018can}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book
  question answering.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 2381--2391, Brussels, Belgium,
  October-November 2018. Association for Computational Linguistics.

\bibitem{mohankumar2020towards}
Akash~Kumar Mohankumar, Preksha Nema, Sharan Narasimhan, Mitesh~M Khapra,
  Balaji~Vasan Srinivasan, and Balaraman Ravindran.
\newblock Towards transparent and explainable attention models.
\newblock {\em arXiv preprint arXiv:2004.14243}, 2020.

\bibitem{narang2020wt5}
Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and
  Karishma Malkan.
\newblock Wt5?! training text-to-text models to explain their predictions.
\newblock {\em arXiv preprint arXiv:2004.14546}, 2020.

\bibitem{poerner2018evaluating}
Nina Poerner, Benjamin Roth, and Hinrich Sch{\"u}tze.
\newblock Evaluating neural network explanation methods using hybrid documents
  and morphological agreement.
\newblock {\em arXiv preprint arXiv:1801.06422}, 2018.

\bibitem{pruthi2020evaluating}
Danish Pruthi, Bhuwan Dhingra, Livio~Baldini Soares, Michael Collins, Zachary~C
  Lipton, Graham Neubig, and William~W Cohen.
\newblock Evaluating explanations: How much do explanations from the teacher
  aid students?
\newblock {\em arXiv preprint arXiv:2012.00893}, 2020.

\bibitem{rajani2019explain}
Nazneen~Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher.
\newblock Explain yourself! leveraging language models for commonsense
  reasoning.
\newblock {\em arXiv preprint arXiv:1906.02361}, 2019.

\bibitem{raman2020learning}
Mrigank Raman, Aaron Chan, Siddhant Agarwal, Peifeng Wang, Hansen Wang,
  Sungchul Kim, Ryan Rossi, Handong Zhao, Nedim Lipka, and Xiang Ren.
\newblock Learning to deceive knowledge graph augmented models via targeted
  perturbation.
\newblock {\em arXiv preprint arXiv:2010.12872}, 2020.

\bibitem{santoro2017simple}
Adam Santoro, David Raposo, David~G Barrett, Mateusz Malinowski, Razvan
  Pascanu, Peter Battaglia, and Timothy Lillicrap.
\newblock A simple neural network module for relational reasoning.
\newblock In {\em Advances in neural information processing systems}, pages
  4967--4976, 2017.

\bibitem{schlichtkrull2018modeling}
Michael Schlichtkrull, Thomas~N Kipf, Peter Bloem, Rianne Van Den~Berg, Ivan
  Titov, and Max Welling.
\newblock Modeling relational data with graph convolutional networks.
\newblock In {\em European Semantic Web Conference}, pages 593--607. Springer,
  2018.

\bibitem{serrano2019attention}
Sofia Serrano and Noah~A Smith.
\newblock Is attention interpretable?
\newblock {\em arXiv preprint arXiv:1906.03731}, 2019.

\bibitem{speer2017conceptnet}
Robyn Speer, Joshua Chin, and Catherine Havasi.
\newblock Conceptnet 5.5: an open multilingual graph of general knowledge.
\newblock In {\em Proceedings of AAAI}, pages 4444--4451, 2017.

\bibitem{strout2019human}
Julia Strout, Ye~Zhang, and Raymond~J Mooney.
\newblock Do human rationales improve machine explanations?
\newblock {\em arXiv preprint arXiv:1905.13714}, 2019.

\bibitem{sundararajan2017axiomatic}
Mukund Sundararajan, Ankur Taly, and Qiqi Yan.
\newblock Axiomatic attribution for deep networks.
\newblock In {\em International Conference on Machine Learning}, pages
  3319--3328. PMLR, 2017.

\bibitem{talmor2019commonsenseqa}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.
\newblock {C}ommonsense{QA}: A question answering challenge targeting
  commonsense knowledge.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4149--4158,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

\bibitem{tutek2020staying}
Martin Tutek and Jan {\v{S}}najder.
\newblock Staying true to your word:(how) can attention become explanation?
\newblock {\em arXiv preprint arXiv:2005.09379}, 2020.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em arXiv preprint arXiv:1706.03762}, 2017.

\bibitem{velivckovic2017graph}
Petar Veli{\v{c}}kovi{\'c}, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
  Pietro Lio, and Yoshua Bengio.
\newblock Graph attention networks.
\newblock {\em arXiv preprint arXiv:1710.10903}, 2017.

\bibitem{wang2020connecting}
Peifeng Wang, Nanyun Peng, Pedro Szekely, and Xiang Ren.
\newblock Connecting the dots: A knowledgeable path generator for commonsense
  question answering.
\newblock {\em arXiv preprint arXiv:2005.00691}, 2020.

\bibitem{wang2019improving}
Xiaoyan Wang, Pavan Kapanipathi, Ryan Musa, Mo~Yu, Kartik Talamadupula, Ibrahim
  Abdelaziz, Maria Chang, Achille Fokoue, Bassem Makni, Nicholas Mattei, et~al.
\newblock Improving natural language inference using external knowledge in the
  science questions domain.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 7208--7215, 2019.

\bibitem{wiegreffe2020measuring}
Sarah Wiegreffe, Ana Marasovic, and Noah~A Smith.
\newblock Measuring association between labels and free-text rationales.
\newblock {\em arXiv preprint arXiv:2010.12762}, 2020.

\bibitem{wiegreffe2019attention}
Sarah Wiegreffe and Yuval Pinter.
\newblock Attention is not not explanation.
\newblock {\em arXiv preprint arXiv:1908.04626}, 2019.

\bibitem{yan2020learning}
Jun Yan, Mrigank Raman, Aaron Chan, Tianyu Zhang, Ryan Rossi, Handong Zhao,
  Sungchul Kim, Nedim Lipka, and Xiang Ren.
\newblock Learning contextualized knowledge structures for commonsense
  reasoning.
\newblock {\em arXiv preprint arXiv:2010.12873}, 2020.

\bibitem{yasunaga2021qa}
Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure
  Leskovec.
\newblock Qa-gnn: Reasoning with language models and knowledge graphs for
  question answering.
\newblock {\em arXiv preprint arXiv:2104.06378}, 2021.

\bibitem{ying2019gnnexplainer}
Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec.
\newblock Gnnexplainer: Generating explanations for graph neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  9244--9255, 2019.

\bibitem{zellers2018swag}
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi.
\newblock Swag: A large-scale adversarial dataset for grounded commonsense
  inference.
\newblock {\em arXiv preprint arXiv:1808.05326}, 2018.

\bibitem{zhao2020lirex}
Xinyan Zhao and VG~Vydiswaran.
\newblock Lirex: Augmenting language inference with relevant explanation.
\newblock {\em arXiv preprint arXiv:2012.09157}, 2020.

\bibitem{zhou2018commonsense}
Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao, Jingfang Xu, and Xiaoyan Zhu.
\newblock Commonsense knowledge aware conversation generation with graph
  attention.
\newblock In {\em IJCAI}, pages 4623--4629, 2018.

\bibitem{zhou2020towards}
Wangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan
  Xiong, and Jian Tang.
\newblock Towards interpretable natural language understanding with
  explanations as latent variables.
\newblock {\em arXiv preprint arXiv:2011.05268}, 2020.

\end{thebibliography}
