\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori and Neu(2014)]{abbasi2014online}
Yasin Abbasi-Yadkori and Gergely Neu.
\newblock Online learning in mdps with side information.
\newblock \emph{arXiv preprint arXiv:1406.6812}, 2014.

\bibitem[Afsar et~al.(2021)Afsar, Crump, and Far]{afsar2021reinforcement}
M~Mehdi Afsar, Trafford Crump, and Behrouz Far.
\newblock Reinforcement learning based recommender systems: A survey.
\newblock \emph{arXiv preprint arXiv:2101.06286}, 2021.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{auer2002finite}
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock \emph{Machine learning}, 47\penalty0 (2):\penalty0 235--256, 2002.

\bibitem[Avner et~al.(2012)Avner, Mannor, and Shamir]{avner2012decoupling}
Orly Avner, Shie Mannor, and Ohad Shamir.
\newblock Decoupling exploration and exploitation in multi-armed bandits.
\newblock \emph{arXiv preprint arXiv:1205.2874}, 2012.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  263--272. PMLR, 2017.

\bibitem[Buckman et~al.(2020)Buckman, Gelada, and
  Bellemare]{buckman2020importance}
Jacob Buckman, Carles Gelada, and Marc~G Bellemare.
\newblock The importance of pessimism in fixed-dataset policy optimization.
\newblock \emph{arXiv preprint arXiv:2009.06799}, 2020.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Christoph Dann, Tor Lattimore, and Emma Brunskill.
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning, 2017.

\bibitem[Dann et~al.(2021)Dann, Marinov, Mohri, and Zimmert]{dann2021beyond}
Christoph Dann, Teodor~Vanislavov Marinov, Mehryar Mohri, and Julian Zimmert.
\newblock Beyond value-function gaps: Improved instance-dependent regret bounds
  for episodic reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Domingues et~al.(2021)Domingues, M{\'e}nard, Kaufmann, and
  Valko]{domingues2021episodic}
Omar~Darwiche Domingues, Pierre M{\'e}nard, Emilie Kaufmann, and Michal Valko.
\newblock Episodic reinforcement learning in finite mdps: Minimax lower bounds
  revisited.
\newblock In \emph{Algorithmic Learning Theory}, pages 578--598. PMLR, 2021.

\bibitem[Fujimoto and Gu(2021)]{fujimoto2021minimalist}
Scott Fujimoto and Shixiang~Shane Gu.
\newblock A minimalist approach to offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2106.06860}, 2021.

\bibitem[He et~al.(2021)He, Zhou, and Gu]{he2021logarithmic}
Jiafan He, Dongruo Zhou, and Quanquan Gu.
\newblock Logarithmic regret for reinforcement learning with linear function
  approximation.
\newblock In \emph{International Conference on Machine Learning}, pages
  4171--4180. PMLR, 2021.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (4), 2010.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is q-learning provably efficient?
\newblock \emph{arXiv preprint arXiv:1807.03765}, 2018.

\bibitem[Jin et~al.(2021)Jin, Yang, and Wang]{jin2021pessimism}
Ying Jin, Zhuoran Yang, and Zhaoran Wang.
\newblock Is pessimism provably efficient for offline rl?
\newblock In \emph{International Conference on Machine Learning}, pages
  5084--5096. PMLR, 2021.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.04779}, 2020.

\bibitem[Lattimore and Szepesv{\'a}ri(2020)]{lattimore2020bandit}
Tor Lattimore and Csaba Szepesv{\'a}ri.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Lipsky and Sharp(2001)]{lipsky2001idea}
Martin~S Lipsky and Lisa~K Sharp.
\newblock From idea to market: the drug approval process.
\newblock \emph{The Journal of the American Board of Family Practice},
  14\penalty0 (5):\penalty0 362--367, 2001.

\bibitem[Liu et~al.(2018)Liu, Li, Tang, and Zhou]{liu2018breaking}
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou.
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Liu et~al.(2020)Liu, Swaminathan, Agarwal, and
  Brunskill]{liu2020provably}
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
\newblock Provably good batch reinforcement learning without great exploration.
\newblock \emph{arXiv preprint arXiv:2007.08202}, 2020.

\bibitem[Modi et~al.(2018)Modi, Jiang, Singh, and Tewari]{modi2018markov}
Aditya Modi, Nan Jiang, Satinder Singh, and Ambuj Tewari.
\newblock Markov decision processes with continuous side information.
\newblock In \emph{Algorithmic Learning Theory}, pages 597--618. PMLR, 2018.

\bibitem[Papini et~al.(2021)Papini, Tirinzoni, Pacchiano, Restelli, Lazaric,
  and Pirotta]{papini2021reinforcement}
Matteo Papini, Andrea Tirinzoni, Aldo Pacchiano, Marcello Restelli, Alessandro
  Lazaric, and Matteo Pirotta.
\newblock Reinforcement learning in linear mdps: Constant regret and
  representation selection.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Rouyer and Seldin(2020)]{rouyer2020tsallis}
Chlo{\'e} Rouyer and Yevgeny Seldin.
\newblock Tsallis-inf for decoupled exploration and exploitation in multi-armed
  bandits.
\newblock In \emph{Conference on Learning Theory}, pages 3227--3249. PMLR,
  2020.

\bibitem[Simchowitz and Jamieson(2019)]{simchowitz2019non}
Max Simchowitz and Kevin~G Jamieson.
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 1153--1162, 2019.

\bibitem[Slivkins(2019)]{slivkins2019introduction}
Aleksandrs Slivkins.
\newblock Introduction to multi-armed bandits.
\newblock \emph{arXiv preprint arXiv:1904.07272}, 2019.

\bibitem[Uehara and Sun(2022)]{uehara2022pessimistic}
Masatoshi Uehara and Wen Sun.
\newblock Pessimistic model-based offline reinforcement learning under partial
  coverage.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=tyrJsbKAe6}.

\bibitem[Xie et~al.(2021)Xie, Cheng, Jiang, Mineiro, and
  Agarwal]{xie2021bellman}
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal.
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2106.06926}, 2021.

\bibitem[Xu et~al.(2021)Xu, Ma, and Du]{xu2021fine}
Haike Xu, Tengyu Ma, and Simon~S Du.
\newblock Fine-grained gap-dependent bounds for tabular mdps via adaptive
  multi-step bootstrap.
\newblock \emph{arXiv preprint arXiv:2102.04692}, 2021.

\bibitem[Yang et~al.(2021)Yang, Yang, and Du]{yang2021q}
Kunhe Yang, Lin Yang, and Simon Du.
\newblock Q-learning with logarithmic regret.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1576--1584. PMLR, 2021.

\bibitem[Yin and Wang(2021)]{yin2021towards}
Ming Yin and Yu-Xiang Wang.
\newblock Towards instance-optimal offline reinforcement learning with
  pessimism.
\newblock In \emph{Thirty-Fifth Conference on Neural Information Processing
  Systems}, 2021.

\bibitem[Yu et~al.(2021)Yu, Liu, Nemati, and Yin]{yu2021reinforcement}
Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin.
\newblock Reinforcement learning in healthcare: A survey.
\newblock \emph{ACM Computing Surveys (CSUR)}, 55\penalty0 (1):\penalty0 1--36,
  2021.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and
  Ma]{yu2020mopo}
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James~Y Zou, Sergey
  Levine, Chelsea Finn, and Tengyu Ma.
\newblock Mopo: Model-based offline policy optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 14129--14142, 2020.

\bibitem[Zanette and Brunskill(2019)]{zanette2019tighter}
Andrea Zanette and Emma Brunskill.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \emph{International Conference on Machine Learning}, pages
  7304--7312. PMLR, 2019.

\end{thebibliography}
