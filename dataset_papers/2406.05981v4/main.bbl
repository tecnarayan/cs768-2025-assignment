\begin{thebibliography}{76}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Afshar et~al.(2018)Afshar, Perros, Papalexakis, et~al.]{afshar2018copa}
Ardavan Afshar, Ioakeim Perros, Evangelos~E Papalexakis, et~al.
\newblock {COPA: Constrained PARAFAC2 for sparse \& large datasets}.
\newblock In \emph{CIKM}, 2018.

\bibitem[AI(2024)]{llama3}
Meta AI.
\newblock {LLaMA 3}.
\newblock \url{https://github.com/meta-llama/llama3}, 2024.

\bibitem[Anil et~al.(2023)Anil, Borgeaud, et~al.]{team2023gemini}
Rohan Anil, Sebastian Borgeaud, et~al.
\newblock {Gemini: A Family of Highly Capable Multimodal Models}.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Boratko et~al.(2018)Boratko, Padigela, Mikkilineni, et~al.]{boratko2018systematic}
Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, et~al.
\newblock {A Systematic Classification of Knowledge, Reasoning, and Context within the ARC Dataset}.
\newblock \emph{arXiv preprint arXiv:1806.00358}, 2018.

\bibitem[Brito et~al.(2014)Brito, Rabuske, Fernandes, et~al.]{brito2014quaternary}
Diogo Brito, Taimur~G Rabuske, Jorge~R Fernandes, et~al.
\newblock {Quaternary logic lookup table in standard CMOS}.
\newblock \emph{TVLSI}, 2014.

\bibitem[Chee et~al.(2024)Chee, Cai, Kuleshov, et~al.]{chee2024quip}
Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, et~al.
\newblock {QuIP: 2-Bit Quantization of Large Language Models With Guarantees}.
\newblock \emph{NeurIPS}, 2024.

\bibitem[Chen et~al.(2020)Chen, Wang, Xu, et~al.]{chen2020addernet}
Hanting Chen, Yunhe Wang, Chunjing Xu, et~al.
\newblock {AdderNet: Do We Really Need Multiplications in Deep Learning?}
\newblock In \emph{CVPR}, 2020.

\bibitem[Chen et~al.(2016)Chen, Krishna, Emer, et~al.]{chen2016eyeriss}
Yu-Hsin Chen, Tushar Krishna, Joel~S Emer, et~al.
\newblock {Eyeriss: An Energy-efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks}.
\newblock \emph{JSSCC}, 2016.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, et~al.]{clark2019boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, et~al.
\newblock {BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions}.
\newblock \emph{arXiv preprint arXiv:1905.10044}, 2019.

\bibitem[Courbariaux et~al.(2016)Courbariaux, Hubara, Soudry, et~al.]{courbariaux2016binarized}
Matthieu Courbariaux, Itay Hubara, Daniel Soudry, et~al.
\newblock {Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1}.
\newblock \emph{arXiv preprint arXiv:1602.02830}, 2016.

\bibitem[Dagan et~al.(2022)Dagan, Roth, Zanzotto, et~al.]{dagan2022recognizing}
Ido Dagan, Dan Roth, Fabio Zanzotto, et~al.
\newblock \emph{{Recognizing Textual Entailment: Models and Applications}}.
\newblock Springer Nature, 2022.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, et~al.]{dao2022flashattention}
Tri Dao, Dan Fu, Stefano Ermon, et~al.
\newblock {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Darvish~Rouhani et~al.(2020)Darvish~Rouhani, Lo, Zhao, Liu, Fowers, Ovtcharov, Vinogradsky, Massengill, Yang, Bittner, et~al.]{darvish2020pushing}
Bita Darvish~Rouhani, Daniel Lo, Ritchie Zhao, Ming Liu, Jeremy Fowers, Kalin Ovtcharov, Anna Vinogradsky, Sarah Massengill, Lita Yang, Ray Bittner, et~al.
\newblock Pushing the limits of narrow precision inferencing at cloud scale with microsoft floating point.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 10271--10281, 2020.

\bibitem[Dettmers and Zettlemoyer(2023)]{dettmers2023case}
Tim Dettmers and Luke Zettlemoyer.
\newblock {The Case for 4-bit Precision: k-bit Inference Scaling Laws}.
\newblock In \emph{ICML}, 2023.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, et~al.]{dettmers2022gpt3}
Tim Dettmers, Mike Lewis, Younes Belkada, et~al.
\newblock {GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale}.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Elhoushi et~al.(2021)Elhoushi, Chen, Shafiq, et~al.]{elhoushi2021deepshift}
Mostafa Elhoushi, Zihao Chen, Farhan Shafiq, et~al.
\newblock {DeepShift: Towards Multiplication-Less Neural Networks}.
\newblock In \emph{CVPR}, 2021.

\bibitem[Frantar and Alistarh(2022)]{frantar2022optimal}
Elias Frantar and Dan Alistarh.
\newblock {Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning}.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Frantar et~al.(2022)Frantar, Ashkboos, Hoefler, et~al.]{frantar2022optq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, et~al.
\newblock {OPTQ: Accurate Quantization for Generative Pre-trained Transformers}.
\newblock In \emph{ICLR}, 2022.

\bibitem[Gholami et~al.(2024)Gholami, Yao, Kim, et~al.]{flops_memory_summary}
Amir Gholami, Zhewei Yao, Sehoon Kim, et~al.
\newblock {AI and Memory Wall}.
\newblock \emph{IEEE Micro Journal}, 2024.

\bibitem[Gromov et~al.(2024)Gromov, Tirumala, Shapourian, et~al.]{gromov2024unreasonable}
Andrey Gromov, Kushal Tirumala, Hassan Shapourian, et~al.
\newblock {The Unreasonable Ineffectiveness of the Deeper Layers}.
\newblock \emph{arXiv preprint arXiv:2403.17887}, 2024.

\bibitem[Guo et~al.(2017)Guo, Yao, Zhao, et~al.]{guo2017network}
Yiwen Guo, Anbang Yao, Hao Zhao, et~al.
\newblock {Network Sketching: Exploiting Binary Structure in Deep CNNs}.
\newblock In \emph{CVPR}, 2017.

\bibitem[Gwee et~al.(2008)Gwee, Chang, Shi, et~al.]{gwee2008low}
Bah-Hwee Gwee, Joseph~S Chang, Yiqiong Shi, et~al.
\newblock {A Low-Voltage Micropower Asynchronous Multiplier With Shift–Add Multiplication Approach}.
\newblock \emph{IEEE Transactions on Circuits and Systems I: Regular Papers}, 2008.

\bibitem[Han et~al.(2016)Han, Liu, Mao, et~al.]{han2016eie}
Song Han, Xingyu Liu, Huizi Mao, et~al.
\newblock {EIE: Efficient Inference Engine on Compressed Deep Neural Network}.
\newblock \emph{ACM SIGARCH Computer Architecture News}, 2016.

\bibitem[Harma et~al.(2024)Harma, Chakraborty, Kostenok, Mishin, Ha, Falsafi, Jaggi, Liu, Oh, Subramanian, and Yazdanbakhsh]{qs-theory}
Simla~Burcu Harma, Ayan Chakraborty, Elizaveta Kostenok, Danila Mishin, Dongho Ha, Babak Falsafi, Martin Jaggi, Ming Liu, Yunho Oh, Suvinay Subramanian, and Amir Yazdanbakhsh.
\newblock {Effective Interplay between Sparsity and Quantization: From Theory to Practice}.
\newblock \emph{arXiv preprint arXiv:2405.20935}, 2024.

\bibitem[Hassibi et~al.(1993)Hassibi, Stork, and Wolff]{hassibi1993optimal}
Babak Hassibi, David~G Stork, and Gregory~J Wolff.
\newblock {Optimal Brain Surgeon and General Network Pruning}.
\newblock In \emph{IEEE international conference on neural networks}, 1993.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Horowitz(2014)]{horowitz20141}
Mark Horowitz.
\newblock {1.1 Computing's Energy Problem (and what we can do about it)}.
\newblock In \emph{ISSCC}, 2014.

\bibitem[Huang et~al.(2024)Huang, Liu, Qin, et~al.]{huang2024billm}
Wei Huang, Yangdong Liu, Haotong Qin, et~al.
\newblock {BiLLM: Pushing the Limit of Post-Training Quantization for LLMs}.
\newblock \emph{arXiv preprint arXiv:2402.04291}, 2024.

\bibitem[Jeon et~al.(2020)Jeon, Park, Kwon, et~al.]{jeon2020biqgemm}
Yongkweon Jeon, Baeseong Park, Se~Jung Kwon, et~al.
\newblock {BiQGEMM: Matrix Multiplication with Lookup Table For Binary-Coding-based Quantized DNNs}.
\newblock In \emph{SC}, 2020.

\bibitem[Jeon et~al.(2022)Jeon, Lee, Cho, et~al.]{biq}
Yongkweon Jeon, Chungman Lee, Eulrang Cho, et~al.
\newblock {Mr.BiQ: Post-Training Non-Uniform Quantization based on Minimizing the Reconstruction Error}.
\newblock In \emph{CVPR}, 2022.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, et~al.]{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, et~al.
\newblock {Mistral 7B}.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Juefei-Xu et~al.(2017)Juefei-Xu, Naresh~Boddeti, and Savvides]{juefei2017local}
Felix Juefei-Xu, Vishnu Naresh~Boddeti, and Marios Savvides.
\newblock {Local Binary Convolutional Neural Networks}.
\newblock In \emph{CVPR}, 2017.

\bibitem[Kwon et~al.(2021)Kwon, Lee, Jeon, et~al.]{kwon2020post}
Se~Jung Kwon, Dongsoo Lee, Yongkweon Jeon, et~al.
\newblock {Post-Training Weighted Quantization of Neural Networks for Language Models}.
\newblock \url{https://openreview.net/forum?id=2Id6XxTjz7c}, 2021.

\bibitem[Lee et~al.(2023)Lee, Kim, Kwon, and Lee]{lee2023flexround}
Jung~Hyun Lee, Jeonghoon Kim, Se~Jung Kwon, and Dongsoo Lee.
\newblock Flexround: Learnable rounding based on element-wise division for post-training quantization.
\newblock In \emph{International Conference on Machine Learning}, pages 18913--18939. PMLR, 2023.

\bibitem[Lee et~al.(2024)Lee, Kim, Yang, Kwon, Yang, Yoo, and Lee]{lee2024lrq}
Jung~Hyun Lee, Jeonghoon Kim, June~Yong Yang, Se~Jung Kwon, Eunho Yang, Kang~Min Yoo, and Dongsoo Lee.
\newblock Lrq: Optimizing post-training quantization for large language models by learning low-rank weight-scaling matrices.
\newblock \emph{arXiv preprint arXiv:2407.11534}, 2024.

\bibitem[Li et~al.(2023)Li, Liu, Yang, et~al.]{li2023denseshift}
Xinlin Li, Bang Liu, Rui~Heng Yang, et~al.
\newblock {DenseShift: Towards Accurate and Efficient Low-Bit Power-of-Two Quantization}.
\newblock In \emph{ICCV}, 2023.

\bibitem[Li et~al.(2019)Li, Dong, and Wang]{li2019additive}
Yuhang Li, Xin Dong, and Wei Wang.
\newblock {Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks}.
\newblock \emph{arXiv preprint arXiv:1909.13144}, 2019.

\bibitem[Lin et~al.(2023)Lin, Tang, Tang, et~al.]{lin2023awq}
Ji~Lin, Jiaming Tang, Haotian Tang, et~al.
\newblock {AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration}.
\newblock \emph{arXiv preprint arXiv:2306.00978}, 2023.

\bibitem[Liu et~al.(2023)Liu, Oguz, Zhao, et~al.]{liu2023llm}
Zechun Liu, Barlas Oguz, Changsheng Zhao, et~al.
\newblock {LLM-QAT: Data-free Quantization Aware Training for Large Language Models}.
\newblock \emph{arXiv preprint arXiv:2305.17888}, 2023.

\bibitem[Ma et~al.(2023)Ma, Fang, and Wang]{ma2023llm}
Xinyin Ma, Gongfan Fang, and Xinchao Wang.
\newblock {LLM-Pruner: On the Structural Pruning of Large Language Models}.
\newblock \emph{NeurIPS}, 2023.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, et~al.]{merity2017pointer}
Stephen Merity, Caiming Xiong, James Bradbury, et~al.
\newblock {Pointer Sentinel Mixture Models}.
\newblock In \emph{ICLR}, 2017.

\bibitem[Mesnard et~al.(2024)Mesnard, Hardin, et~al.]{team2024gemma}
Thomas Mesnard, Cassidy Hardin, et~al.
\newblock {Gemma: Open Models Based on Gemini Research and Technology}.
\newblock \emph{arXiv preprint arXiv:2403.08295}, 2024.

\bibitem[Mostafazadeh et~al.(2017)Mostafazadeh, Roth, Louis, et~al.]{mostafazadeh2017lsdsem}
Nasrin Mostafazadeh, Michael Roth, Annie Louis, et~al.
\newblock {LSDSem 2017 Shared Task: The Story Cloze Test}.
\newblock In \emph{Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics}, 2017.

\bibitem[Mozaffari et~al.(2024)Mozaffari, Yazdanbakhsh, Zhang, and Mehri~Dahnavi]{slope}
Mohammad Mozaffari, Amir Yazdanbakhsh, Zhao Zhang, and Maryam Mehri~Dahnavi.
\newblock {SLoPe: Double-Pruned Sparse Plus Lazy Low-rank Adapter Pretraining of LLMs}.
\newblock \emph{arXiv preprint arXiv:2405.16325}, 2024.

\bibitem[{NVIDIA Corporation}(2020)]{nvidia2020a100}
{NVIDIA Corporation}.
\newblock {NVIDIA A100 Tensor Core GPU}.
\newblock \url{https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf}, 2020.
\newblock Datasheet.

\bibitem[{OpenAI}(2023)]{chatgpt}
{OpenAI}.
\newblock {ChatGPT: Language Model for Dialogue Generation}.
\newblock \url{https://www.openai.com/chatgpt/}, 2023.
\newblock Website.

\bibitem[OpenAI(2023)]{gpt4}
OpenAI.
\newblock {GPT-4 Technical Report}.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Park et~al.(2022)Park, Park, Kim, et~al.]{park2022lut}
Gunho Park, Baeseong Park, Minsub Kim, et~al.
\newblock {LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models}.
\newblock \emph{arXiv preprint arXiv:2206.09557}, 2022.

\bibitem[Scao et~al.(2022)Scao, Fan, Akiki, et~al.]{le2023bloom}
Teven~Le Scao, Angela Fan, Christopher Akiki, et~al.
\newblock {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model}.
\newblock \emph{arXiv preprint arXiv:2211.05100}, 2022.

\bibitem[Sentieys(2021)]{sentieys2021approximate}
Olivier Sentieys.
\newblock {Approximate Computing for DNN}.
\newblock In \emph{CSW 2021-HiPEAC Computing Systems Week}, 2021.

\bibitem[Shao et~al.(2023)Shao, Chen, Zhang, Xu, Zhao, Li, Zhang, Gao, Qiao, and Luo]{shao2023omniquant}
Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu~Qiao, and Ping Luo.
\newblock Omniquant: Omnidirectionally calibrated quantization for large language models.
\newblock \emph{arXiv preprint arXiv:2308.13137}, 2023.

\bibitem[Shen et~al.(2024)Shen, Kong, Yang, et~al.]{shen2024edgeqat}
Xuan Shen, Zhenglun Kong, Changdi Yang, et~al.
\newblock {EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge}.
\newblock \emph{arXiv preprint arXiv:2402.10787}, 2024.

\bibitem[Shi et~al.(2022)Shi, You, Zhao, Wang, and Lin]{shi2022nasa}
Huihong Shi, Haoran You, Yang Zhao, Zhongfeng Wang, and Yingyan Lin.
\newblock Nasa: Neural architecture search and acceleration for hardware inspired hybrid networks.
\newblock In \emph{Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design}, pages 1--9, 2022.

\bibitem[Shu et~al.(2021)Shu, Wang, Chen, et~al.]{shu2021adder}
Han Shu, Jiahao Wang, Hanting Chen, et~al.
\newblock {Adder Attention for Vision Transformer}.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Sun et~al.(2023)Sun, Liu, Bair, et~al.]{sun2023simple}
Mingjie Sun, Zhuang Liu, Anna Bair, et~al.
\newblock {A Simple and Effective Pruning Approach for Large Language Models}.
\newblock \emph{arXiv preprint arXiv:2306.11695}, 2023.

\bibitem[Tata and Patel(2003)]{tata2003piqa}
Sandeep Tata and Jignesh~M Patel.
\newblock {PiQA: An Algebra for Querying Protein Data Sets}.
\newblock In \emph{SSDBM}, 2003.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, et~al.
\newblock {LLaMA: Open and Efficient Foundation Language Models}.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, et~al.]{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, et~al.
\newblock {Llama 2: Open Foundation and Fine-Tuned Chat Models}.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Waisberg et~al.(2023)Waisberg, Ong, Masalkhi, et~al.]{bard}
Ethan Waisberg, Joshua Ong, Mouayad Masalkhi, et~al.
\newblock {Google’s AI chatbot “Bard”: A Side-by-Side Comparison with ChatGPT and its Utilization in Ophthalmology}.
\newblock \emph{Eye}, 2023.

\bibitem[Wang et~al.(2022)Wang, Zhao, Tang, et~al.]{wang2022shift}
Guangting Wang, Yucheng Zhao, Chuanxin Tang, et~al.
\newblock {When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism}.
\newblock In \emph{AAAI}, 2022.

\bibitem[Wang et~al.(2021)Wang, Huang, Han, et~al.]{adder_hardware}
Yunhe Wang, Mingqiang Huang, Kai Han, et~al.
\newblock {AdderNet and its Minimalist Hardware Design for Energy-Efficient Artificial Intelligence}.
\newblock \emph{arXiv preprint arXiv:2101.10015}, 2021.

\bibitem[Wu et~al.(2018)Wu, Wan, Yue, et~al.]{wu2018shift}
Bichen Wu, Alvin Wan, Xiangyu Yue, et~al.
\newblock {Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions}.
\newblock In \emph{CVPR}, 2018.

\bibitem[Xiao et~al.(2023)Xiao, Lin, Seznec, et~al.]{xiao2023smoothquant}
Guangxuan Xiao, Ji~Lin, Mickael Seznec, et~al.
\newblock {SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models}.
\newblock In \emph{ICML}, 2023.

\bibitem[Xu et~al.(2018)Xu, Yao, Lin, et~al.]{xu2018alternating}
Chen Xu, Jianqiang Yao, Zhouchen Lin, et~al.
\newblock {Alternating Multi-bit Quantization for Recurrent Neural Networks}.
\newblock \emph{arXiv preprint arXiv:1802.00150}, 2018.

\bibitem[Xu et~al.(2020)Xu, Xu, Chen, et~al.]{adder_distillation}
Yixing Xu, Chang Xu, Xinghao Chen, et~al.
\newblock {Kernel Based Progressive Distillation for Adder Neural Networks}.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Xue and Liu(1986)]{xue1986adaptive}
Ping Xue and Bede Liu.
\newblock {Adaptive Equalizer Based on a Power-Of-Two-Quantized-LMF Algorithm}.
\newblock \emph{IEEE transactions on acoustics, speech, and signal processing}, 1986.

\bibitem[Yang et~al.(2023)Yang, Wang, Shen, et~al.]{yang2023gated}
Songlin Yang, Bailin Wang, Yikang Shen, et~al.
\newblock {Gated Linear Attention Transformers with Hardware-Efficient Training}.
\newblock \emph{arXiv preprint arXiv:2312.06635}, 2023.

\bibitem[Yao et~al.(2022)Yao, Yazdani~Aminabadi, Zhang, et~al.]{yao2022zeroquant}
Zhewei Yao, Reza Yazdani~Aminabadi, Minjia Zhang, et~al.
\newblock {ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers}.
\newblock \emph{NeurIPS}, 2022.

\bibitem[You et~al.(2020)You, Chen, Zhang, et~al.]{you2020shiftaddnet}
Haoran You, Xiaohan Chen, Yongan Zhang, et~al.
\newblock {ShiftAddNet: A Hardware-Inspired Deep Network}.
\newblock \emph{NeurIPS}, 2020.

\bibitem[You et~al.(2022)You, Li, Huihong, et~al.]{you2022shiftaddnas}
Haoran You, Baopu Li, Shi Huihong, et~al.
\newblock {ShiftAddNAS: Hardware-Inspired Search for More Accurate and Efficient Neural Networks}.
\newblock In \emph{ICLR}, 2022.

\bibitem[You et~al.(2024{\natexlab{a}})You, Fu, Wang, et~al.]{you2024linear}
Haoran You, Yichao Fu, Zheng Wang, et~al.
\newblock {When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models}.
\newblock In \emph{ICML}, 2024{\natexlab{a}}.

\bibitem[You et~al.(2024{\natexlab{b}})You, Shi, Guo, et~al.]{you2024shiftaddvit}
Haoran You, Huihong Shi, Yipin Guo, et~al.
\newblock {ShiftAddViT: Mixture of multiplication primitives towards efficient vision transformer}.
\newblock \emph{NeurIPS}, 2024{\natexlab{b}}.

\bibitem[Yuan et~al.(2024)Yuan, Shang, Zhou, Dong, Xue, Wu, Li, Gu, Lee, Yan, et~al.]{yuan2024llm}
Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong~Jae Lee, Yan Yan, et~al.
\newblock Llm inference unveiled: Survey and roofline model insights.
\newblock \emph{arXiv preprint arXiv:2402.16363}, 2024.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, et~al.]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, et~al.
\newblock {OPT: Open Pre-trained Transformer Language Models}.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem[Zhao et~al.(2020)Zhao, Li, Wang, et~al.]{zhao2020dnn}
Yang Zhao, Chaojian Li, Yue Wang, et~al.
\newblock {DNN-Chip Predictor: An Analytical Performance Predictor for DNN Accelerators with Various Dataflows and Hardware Architectures}.
\newblock In \emph{ICASSP}, 2020.

\bibitem[Zhu et~al.(2024)Zhu, Zhang, Sifferman, Sheaves, Wang, Richmond, Zhou, and Eshraghian]{zhu2024scalable}
Rui-Jie Zhu, Yu~Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, and Jason~K Eshraghian.
\newblock Scalable matmul-free language modeling.
\newblock \emph{arXiv preprint arXiv:2406.02528}, 2024.

\end{thebibliography}
