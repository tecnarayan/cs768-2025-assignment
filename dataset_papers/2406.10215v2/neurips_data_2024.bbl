\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Frank(2023{\natexlab{a}})]{frankBridgingDataGap2023}
Michael~C. Frank.
\newblock Bridging the data gap between children and large language models.
\newblock \emph{Trends in Cognitive Sciences}, 27\penalty0 (11):\penalty0 990--992, 2023{\natexlab{a}}.
\newblock ISSN 1364-6613.
\newblock \doi{10.1016/j.tics.2023.08.007}.

\bibitem[Warstadt et~al.(2023)Warstadt, Mueller, Choshen, Wilcox, Zhuang, Ciro, Mosquera, Paranjabe, Williams, Linzen, and Cotterell]{warstadtFindingsBabyLMChallenge2023}
Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe, Adina Williams, Tal Linzen, and Ryan Cotterell.
\newblock Findings of the {{BabyLM Challenge}}: {{Sample-Efficient Pretraining}} on {{Developmentally Plausible Corpora}}.
\newblock In Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe, Adina Williams, Tal Linzen, and Ryan Cotterell, editors, \emph{Proceedings of the {{BabyLM Challenge}} at the 27th {{Conference}} on {{Computational Natural Language Learning}}}, pages 1--34. Association for Computational Linguistics, 2023.
\newblock \doi{10.18653/v1/2023.conll-babylm.1}.

\bibitem[Huebner et~al.(2021)Huebner, Sulem, Cynthia, and Roth]{huebnerBabyBERTaLearningMore2021}
Philip~A. Huebner, Elior Sulem, Fisher Cynthia, and Dan Roth.
\newblock {{BabyBERTa}}: {{Learning More Grammar With Small-Scale Child-Directed Language}}.
\newblock In Arianna Bisazza and Omri Abend, editors, \emph{Proceedings of the 25th {{Conference}} on {{Computational Natural Language Learning}}}, pages 624--646. Association for Computational Linguistics, 2021.
\newblock \doi{10.18653/v1/2021.conll-1.49}.

\bibitem[Vong et~al.(2024)Vong, Wang, Orhan, and Lake]{vongGroundedLanguageAcquisition2024}
Wai~Keen Vong, Wentao Wang, A.~Emin Orhan, and Brenden~M. Lake.
\newblock Grounded language acquisition through the eyes and ears of a single child.
\newblock \emph{Science}, 383\penalty0 (6682):\penalty0 504--511, 2024.
\newblock \doi{10.1126/science.adi1374}.

\bibitem[Wang et~al.(2023)Wang, Vong, Kim, and Lake]{wangFindingStructureOne2023}
Wentao Wang, Wai~Keen Vong, Najoung Kim, and Brenden~M. Lake.
\newblock Finding {{Structure}} in {{One Child}}'s {{Linguistic Experience}}.
\newblock \emph{Cognitive Science}, 47\penalty0 (6):\penalty0 e13305, 2023.
\newblock ISSN 1551-6709.
\newblock \doi{10.1111/cogs.13305}.

\bibitem[Borovsky(2022)]{borovskyLexicosemanticStructureVocabulary2022}
Arielle Borovsky.
\newblock Lexico-semantic structure in vocabulary and its links to lexical processing in toddlerhood and language outcomes at age three.
\newblock \emph{Developmental Psychology}, 58\penalty0 (4):\penalty0 607--630, 2022.
\newblock ISSN 1939-0599.
\newblock \doi{10.1037/dev0001291}.

\bibitem[Brinchmann et~al.(2019)Brinchmann, Braeken, and Lyster]{brinchmannThereDirectRelation2019}
Ellen~Irén Brinchmann, Johan Braeken, and Solveig-Alma~Halaas Lyster.
\newblock Is there a direct relation between the development of vocabulary and grammar?
\newblock \emph{Developmental Science}, 22\penalty0 (1):\penalty0 e12709, 2019.
\newblock ISSN 1467-7687.
\newblock \doi{10.1111/desc.12709}.

\bibitem[Justice et~al.(2018)Justice, Cain, Jiang, Logan, Jia, Jiang, Logan, and Jia]{justiceModelingNatureGrammar2018}
Laura Justice, Kate Cain, Hui Jiang, Jessica Logan, Rongfang Jia, Hui Jiang, Jessica~A. Logan, and Rongfang Jia.
\newblock Modeling the {{Nature}} of {{Grammar}} and {{Vocabulary Trajectories From Prekindergarten}} to {{Third Grade}}.
\newblock \emph{Journal of Speech, Language, and Hearing Research}, 61\penalty0 (4):\penalty0 910--923, 2018.
\newblock \doi{10.1044/2018_JSLHR-L-17-0090}.

\bibitem[Keith and Nicoladis(2013)]{keithRoleWithinlanguageVocabulary2013}
Margaux Keith and Elena Nicoladis.
\newblock The role of within-language vocabulary size in children's semantic development: Evidence from bilingual children.
\newblock \emph{Journal of Child Language}, 40\penalty0 (4):\penalty0 873--884, 2013.
\newblock ISSN 0305-0009, 1469-7602.
\newblock \doi{10.1017/S0305000912000268}.

\bibitem[Ricketts et~al.(2021)Ricketts, Dawson, and Davies]{rickettsHiddenDepthsNew2021}
Jessie Ricketts, Nicola Dawson, and Robert Davies.
\newblock The hidden depths of new word knowledge: {{Using}} graded measures of orthographic and semantic learning to measure vocabulary acquisition.
\newblock \emph{Learning and Instruction}, 74:\penalty0 101468, 2021.
\newblock ISSN 0959-4752.
\newblock \doi{10.1016/j.learninstruc.2021.101468}.

\bibitem[Frank(2023{\natexlab{b}})]{frank2023baby}
Michael~C Frank.
\newblock Baby steps in evaluating the capacities of large language models.
\newblock \emph{Nature Reviews Psychology}, 2\penalty0 (8):\penalty0 451--452, 2023{\natexlab{b}}.

\bibitem[Ivanova(2023)]{ivanovaRunningCognitiveEvaluations2023}
Anna~A. Ivanova.
\newblock Running cognitive evaluations on large language models: {{The}} do's and the don'ts, 2023.

\bibitem[Hu and Frank(2024)]{hu2024auxiliary}
Jennifer Hu and Michael~C Frank.
\newblock Auxiliary task demands mask the capabilities of smaller language models.
\newblock \emph{arXiv preprint arXiv:2404.02418}, 2024.

\bibitem[Cherti et~al.(2023)Cherti, Beaumont, Wightman, Wortsman, Ilharco, Gordon, Schuhmann, Schmidt, and Jitsev]{chertiReproducibleScalingLaws2023}
Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev.
\newblock Reproducible {{Scaling Laws}} for {{Contrastive Language-Image Learning}}.
\newblock In \emph{2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})}, pages 2818--2829. IEEE, 2023.
\newblock ISBN 9798350301298.
\newblock \doi{10.1109/CVPR52729.2023.00276}.

\bibitem[Baltrušaitis et~al.(2019)Baltrušaitis, Ahuja, and Morency]{Baltrušaitis2019}
Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency.
\newblock Multimodal machine learning: A survey and taxonomy.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 41\penalty0 (2):\penalty0 423--443, 2019.
\newblock \doi{10.1109/TPAMI.2018.2798607}.

\bibitem[Johnson et~al.(2016)Johnson, Hariharan, van~der Maaten, Fei-Fei, Zitnick, and Girshick]{johnson2016clevrdiagnosticdatasetcompositional}
Justin Johnson, Bharath Hariharan, Laurens van~der Maaten, Li~Fei-Fei, C.~Lawrence Zitnick, and Ross Girshick.
\newblock Clevr: A diagnostic dataset for compositional language and elementary visual reasoning, 2016.
\newblock URL \url{https://arxiv.org/abs/1612.06890}.

\bibitem[Bugliarello et~al.(2022)Bugliarello, Liu, Pfeiffer, Reddy, Elliott, Ponti, and Vuli{\'c}]{Bugliarello2022IGLUE}
Emanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, Siva Reddy, Desmond Elliott, Edoardo~Maria Ponti, and Ivan Vuli{\'c}.
\newblock {IGLUE}: A benchmark for transfer learning across modalities, tasks, and languages.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, \emph{Proceedings of the 39th International Conference on Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning Research}, pages 2370--2392. PMLR, 17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/bugliarello22a.html}.

\bibitem[Yin et~al.(2023)Yin, Wang, Cao, Shi, Liu, Li, Sheng, Bai, Huang, Wang, Shao, and Ouyang]{yin2023lamm}
Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Lu~Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang, Jing Shao, and Wanli Ouyang.
\newblock Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark, 2023.

\bibitem[Fu et~al.(2024)Fu, Chen, Shen, Qin, Zhang, Lin, Yang, Zheng, Li, Sun, Wu, and Ji]{fu2024mme}
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu~Lin, Jinrui Yang, Xiawu Zheng, Ke~Li, Xing Sun, Yunsheng Wu, and Rongrong Ji.
\newblock Mme: A comprehensive evaluation benchmark for multimodal large language models, 2024.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Duan, Zhang, Li, Zhang, Zhao, Yuan, Wang, He, Liu, Chen, and Lin]{liu2024mmbench}
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo~Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin.
\newblock Mmbench: Is your multi-modal model an all-around player?, 2024{\natexlab{a}}.

\bibitem[Patraucean et~al.(2023)Patraucean, Smaira, Gupta, Recasens, Markeeva, Banarse, Koppula, heyward, Malinowski, Yang, Doersch, Matejovicova, Sulsky, Miech, Fr\'{e}chette, Klimczak, Koster, Zhang, Winkler, Aytar, Osindero, Damen, Zisserman, and Carreira]{Patraucean2023perception}
Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, joseph heyward, Mateusz Malinowski, Yi~Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine Miech, Alexandre Fr\'{e}chette, Hanna Klimczak, Raphael Koster, Junlin Zhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, and Joao Carreira.
\newblock Perception test: A diagnostic benchmark for multimodal video models.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, \emph{Advances in Neural Information Processing Systems}, volume~36, pages 42748--42761. Curran Associates, Inc., 2023.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2023/file/8540fba4abdc7f9f7a7b1cc6cd60e409-Paper-Datasets_and_Benchmarks.pdf}.

\bibitem[Su et~al.(2021)Su, Duan, Cui, Ji, Wu, Luo, Liu, Zhong, Bharti, and Sacheti]{Su2021GEM}
Lin Su, Nan Duan, Edward Cui, Lei Ji, Chenfei Wu, Huaishao Luo, Yongfei Liu, Ming Zhong, Taroon Bharti, and Arun Sacheti.
\newblock {GEM:} {A} general evaluation benchmark for multimodal tasks.
\newblock \emph{CoRR}, abs/2106.09889, 2021.
\newblock URL \url{https://arxiv.org/abs/2106.09889}.

\bibitem[Liang et~al.(2021)Liang, Lyu, Fan, Wu, Cheng, Wu, Chen, Wu, Lee, Zhu, Salakhutdinov, and Morency]{Liang2023multibench}
Paul~Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Chen, Peter Wu, Michelle~A. Lee, Yuke Zhu, Ruslan Salakhutdinov, and Louis{-}Philippe Morency.
\newblock Multibench: Multiscale benchmarks for multimodal representation learning.
\newblock \emph{CoRR}, abs/2107.07502, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.07502}.

\bibitem[Kosoy et~al.(2023)Kosoy, Reagan, Lai, Gopnik, and Cobb]{kosoyComparingMachinesChildren2023}
Eliza Kosoy, Emily~Rose Reagan, Leslie Lai, Alison Gopnik, and Danielle~Krettek Cobb.
\newblock Comparing {{Machines}} and {{Children}}: {{Using Developmental Psychology Experiments}} to {{Assess}} the {{Strengths}} and {{Weaknesses}} of {{LaMDA Responses}}.
\newblock In \emph{Proceedings of the {{First Workshop}} on {{AI}} Meets {{Moral Philosophy}} and {{Moral Psychology}} at {{NeurIPS}} 2023}, 2023.
\newblock \doi{10.48550/arXiv.2305.11243}.

\bibitem[Weihs et~al.(2022)Weihs, Yuile, Baillargeon, Fisher, Marcus, Mottaghi, and Kembhavi]{weihs2022InfLevel}
Luca Weihs, Amanda~Rose Yuile, Ren\'{e}e Baillargeon, Cynthia Fisher, Gary Marcus, Roozbeh Mottaghi, and Aniruddha Kembhavi.
\newblock Benchmarking progress to infant-level physical reasoning in ai.
\newblock \emph{TMLR}, 2022.

\bibitem[Jiang et~al.(2023)Jiang, Xu, Xin, Liang, Peng, Zhang, and Zhu]{jiangMEWLFewshotMultimodal2023}
Guangyuan Jiang, Manjie Xu, Shiji Xin, Wei Liang, Yujia Peng, Chi Zhang, and Yixin Zhu.
\newblock {{MEWL}}: {{Few-shot}} multimodal word learning with referential uncertainty.
\newblock In \emph{Proceedings of the 40th {{International Conference}} on {{Machine Learning}}}, pages 15144--15169. PMLR, 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/jiang23i.html}.

\bibitem[Sheybani et~al.(2024)Sheybani, Smith, Tiganj, Maini, and Dendukuri]{sheybaniModelVsBabyDevelopmentallyMotivated2024}
Saber Sheybani, Linda~B. Smith, Zoran Tiganj, Sahaj~Singh Maini, and Aravind Dendukuri.
\newblock {{ModelVsBaby}}: A {{Developmentally Motivated Benchmark}} of {{Out-of-Distribution Object Recognition}}, 2024.

\bibitem[Li et~al.(2023)Li, Wang, Wang, Ge, Ge, and Shan]{li2023seedbench}
Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan.
\newblock Seed-bench: Benchmarking multimodal llms with generative comprehension, 2023.

\bibitem[Zhang et~al.(2023)Zhang, Aljunied, Gao, Chia, and Bing]{Zhang2023}
Wenxuan Zhang, Mahani Aljunied, Chang Gao, Yew~Ken Chia, and Lidong Bing.
\newblock M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, \emph{Advances in Neural Information Processing Systems}, volume~36, pages 5484--5505. Curran Associates, Inc., 2023.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2023/file/117c5c8622b0d539f74f6d1fb082a2e9-Paper-Datasets_and_Benchmarks.pdf}.

\bibitem[Chang and Bergen(2022)]{changWordAcquisitionNeural2022}
Tyler~A. Chang and Benjamin~K. Bergen.
\newblock Word {{Acquisition}} in {{Neural Language Models}}.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 10:\penalty0 1--16, 2022.
\newblock ISSN 2307-387X.
\newblock \doi{10.1162/tacl_a_00444}.

\bibitem[Evanson et~al.(2023)Evanson, Lakretz, and King]{evansonLanguageAcquisitionChildren2023}
Linnea Evanson, Yair Lakretz, and Jean~Rémi King.
\newblock Language acquisition: Do children and language models follow similar learning stages?
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, \emph{Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2023}, pages 12205--12218. Association for Computational Linguistics, 2023.
\newblock \doi{10.18653/v1/2023.findings-acl.773}.

\bibitem[Fernald et~al.(2008)Fernald, Zangl, Portillo, and Marchman]{fernaldUsingEyeMovements2008}
Anne~E. Fernald, Renate Zangl, Ana~Luz Portillo, and Virginia~A. Marchman.
\newblock \emph{Using Eye Movements to Monitor Spoken Language Comprehension by Infants and Young Children: {{Looking}} While Listening}.
\newblock Language {{Acquisition}} and {{Language Disorders}}. John Benjamins Publishing Company, 2008.
\newblock ISBN 978-90-272-5304-0 978-90-272-5305-7 978-90-272-9150-9.
\newblock \doi{10.1075/lald.44.06fer}.

\bibitem[Adams et~al.(2018)Adams, Marchman, Loi, Ashland, Fernald, and Feldman]{adamsCaregiverTalkMedical2018}
Katherine~A. Adams, Virginia~A. Marchman, Elizabeth~C. Loi, Melanie~D. Ashland, Anne Fernald, and Heidi~M. Feldman.
\newblock Caregiver {{Talk}} and {{Medical Risk}} as {{Predictors}} of {{Language Outcomes}} in {{Full Term}} and {{Preterm Toddlers}}.
\newblock \emph{Child Development}, 89\penalty0 (5):\penalty0 1674--1690, 2018.
\newblock ISSN 1467-8624.
\newblock \doi{10.1111/cdev.12818}.

\bibitem[Frank et~al.(2016)Frank, Sugarman, Horowitz, Lewis, and Yurovsky]{frankUsingTabletsCollect2016a}
Michael~C. Frank, Elise Sugarman, Alexandra~C. Horowitz, Molly~L. Lewis, and Daniel Yurovsky.
\newblock Using {{Tablets}} to {{Collect Data From Young Children}}.
\newblock \emph{Journal of Cognition and Development}, 17\penalty0 (1):\penalty0 1--17, 2016.
\newblock ISSN 1524-8372.
\newblock \doi{10.1080/15248372.2015.1061528}.

\bibitem[Donnelly and Kidd(2021)]{donnellyOnsetNeighborhoodDensity2021}
Seamus Donnelly and Evan Kidd.
\newblock Onset {{Neighborhood Density Slows Lexical Access}} in {{High Vocabulary}} 30-{{Month Olds}}.
\newblock \emph{Cognitive Science}, 45\penalty0 (9):\penalty0 e13022, 2021.
\newblock ISSN 1551-6709.
\newblock \doi{10.1111/cogs.13022}.

\bibitem[Long et~al.(2024{\natexlab{a}})Long, Ma, Silverman, Yeatman, and Frank]{long2024vss}
Bria Long, Wanjing~Anya Ma, Rebecca~D. Silverman, Jason~D. Yeatman, and Michael~C. Frank.
\newblock Developmental changes in the precision of visual concept knowledge.
\newblock In \emph{Annual Meeting of the Vision Sciences Society}, 2024{\natexlab{a}}.

\bibitem[Bishop(1989)]{bishopTestReceptionGrammar1989}
Dorothy V.~M. Bishop.
\newblock \emph{Test for the Reception of Grammar ({{TROG}})}.
\newblock Medical Research Council, 2nd ed edition, 1989.

\bibitem[Silverman and Yeatman(2023)]{silvermanAcceleratingLiteracyDigital2023}
Rebecca~D. Silverman and Jason~D. Yeatman.
\newblock Accelerating literacy with digital technology, 2023.

\bibitem[Thrush et~al.(2022)Thrush, Jiang, Bartolo, Singh, Williams, Kiela, and Ross]{thrushWinogroundProbingVision2022}
Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross.
\newblock Winoground: {{Probing Vision}} and {{Language Models}} for {{Visio-Linguistic Compositionality}}.
\newblock In \emph{2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})}, pages 5228--5238, 2022.
\newblock \doi{10.1109/CVPR52688.2022.00517}.

\bibitem[Diwan et~al.(2022)Diwan, Berry, Choi, Harwath, and Mahowald]{diwanWhyWinogroundHard2022}
Anuj Diwan, Layne Berry, Eunsol Choi, David Harwath, and Kyle Mahowald.
\newblock Why is {{Winoground Hard}}? {{Investigating Failures}} in {{Visuolinguistic Compositionality}}.
\newblock In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, \emph{Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}}, pages 2236--2250. Association for Computational Linguistics, 2022.
\newblock \doi{10.18653/v1/2022.emnlp-main.143}.

\bibitem[Entwisle(1966)]{entwisleWordAssociationsYoung1966}
Doris~R. Entwisle.
\newblock \emph{Word Associations of Young Children / by {{Doris R}}. {{Entwisle}}.}
\newblock Johns Hopkins Press, 1966.

\bibitem[Nelson et~al.(1998)Nelson, McEvoy, and Schreiber]{nelsonUniversitySouthFlorida1998}
Douglas~L. Nelson, Cathy~L. McEvoy, and Thomas~A. Schreiber.
\newblock The {{University}} of {{South Florida}} word association, rhyme, and word fragment norms., 1998.
\newblock URL \url{http://w3.usf.edu/FreeAssociation/}.

\bibitem[Spriet et~al.(2022)Spriet, Abassi, Hochmann, and Papeo]{sprietVisualObjectCategorization2022}
Céline Spriet, Etienne Abassi, Jean-Rémy Hochmann, and Liuba Papeo.
\newblock Visual object categorization in infancy.
\newblock \emph{Proceedings of the National Academy of Sciences}, 119\penalty0 (8):\penalty0 e2105866119, 2022.
\newblock \doi{10.1073/pnas.2105866119}.

\bibitem[Hebart et~al.(2019)Hebart, Dickter, Kidder, Kwok, Corriveau, Wicklin, and Baker]{hebartTHINGSDatabase8542019}
Martin~N. Hebart, Adam~H. Dickter, Alexis Kidder, Wan~Y. Kwok, Anna Corriveau, Caitlin~Van Wicklin, and Chris~I. Baker.
\newblock {{THINGS}}: {{A}} database of 1,854 object concepts and more than 26,000 naturalistic object images.
\newblock \emph{PLOS ONE}, 14\penalty0 (10):\penalty0 e0223792, 2019.
\newblock ISSN 1932-6203.
\newblock \doi{10.1371/journal.pone.0223792}.

\bibitem[Hebart et~al.(2023)Hebart, Contier, Teichmann, Rockter, Zheng, Kidder, Corriveau, Vaziri-Pashkam, and Baker]{hebartTHINGSdataMultimodalCollection2023}
Martin~N Hebart, Oliver Contier, Lina Teichmann, Adam~H Rockter, Charles~Y Zheng, Alexis Kidder, Anna Corriveau, Maryam Vaziri-Pashkam, and Chris~I Baker.
\newblock {{THINGS-data}}, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior.
\newblock \emph{eLife}, 12:\penalty0 e82580, 2023.
\newblock ISSN 2050-084X.
\newblock \doi{10.7554/eLife.82580}.

\bibitem[Zheng et~al.(2018)Zheng, Pereira, Baker, and Hebart]{zhengRevealingInterpretableObject2018}
Charles~Y. Zheng, Francisco Pereira, Chris~I. Baker, and Martin~N. Hebart.
\newblock Revealing interpretable object representations from human behavior.
\newblock In \emph{Proceedings of the 7th {{International Conference}} on {{Learning Representations}}}, 2018.
\newblock \doi{10.48550/arXiv.1901.02915}.

\bibitem[Kriegeskorte et~al.(2008)Kriegeskorte, Mur, and Bandettini]{kriegeskorteRepresentationalSimilarityAnalysis2008}
Nikolaus Kriegeskorte, Marieke Mur, and Peter Bandettini.
\newblock Representational similarity analysis - connecting the branches of systems neuroscience.
\newblock \emph{Frontiers in Systems Neuroscience}, 2, 2008.
\newblock ISSN 1662-5137.
\newblock \doi{10.3389/neuro.06.004.2008}.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, Krueger, and Sutskever]{radfordLearningTransferableVisual2021}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.
\newblock Learning {{Transferable Visual Models From Natural Language Supervision}}, 2021.

\bibitem[Kim et~al.(2021)Kim, Son, and Kim]{kimViLTVisionandLanguageTransformer2021}
Wonjae Kim, Bokyung Son, and Ildoo Kim.
\newblock {{ViLT}}: {{Vision-and-Language Transformer Without Convolution}} or {{Region Supervision}}, 2021.

\bibitem[Singh et~al.(2022)Singh, Hu, Goswami, Couairon, Galuba, Rohrbach, and Kiela]{singhFLAVAFoundationalLanguage2022}
Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela.
\newblock {{FLAVA}}: {{A Foundational Language And Vision Alignment Model}}, 2022.

\bibitem[Li et~al.(2022)Li, Li, Xiong, and Hoi]{liBLIPBootstrappingLanguageImage2022}
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
\newblock {{BLIP}}: {{Bootstrapping Language-Image Pre-training}} for {{Unified Vision-Language Understanding}} and {{Generation}}, 2022.

\bibitem[Xu et~al.(2024)Xu, Wu, Rosenman, Lal, Che, and Duan]{xuBridgeTowerBuildingBridges2024}
Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, and Nan Duan.
\newblock {{BridgeTower}}: {{Building Bridges Between Encoders}} in {{Vision-Language Representation Learning}}, 2024.

\bibitem[Beaumont(2022)]{beaumontLargeScaleOpenCLIP2022}
Romain Beaumont.
\newblock Large scale {{openCLIP}}: {{L}}/14, {{H}}/14 and g/14 trained on {{LAION-2B}}, 2022.
\newblock URL \url{https://laion.ai/blog/large-openclip}.

\bibitem[Zhai et~al.(2023)Zhai, Mustafa, Kolesnikov, and Beyer]{zhaiSigmoidlosslanguageimage2023}
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.
\newblock Sigmoid loss for language image pre-training, 2023.
\newblock URL \url{https://arxiv.org/abs/2303.15343}.

\bibitem[Goodman and Frank(2016)]{goodmanPragmaticLanguageInterpretation2016}
Noah~D. Goodman and Michael~C. Frank.
\newblock Pragmatic {{Language Interpretation}} as {{Probabilistic Inference}}.
\newblock \emph{Trends in Cognitive Sciences}, 20\penalty0 (11):\penalty0 818--829, 2016.
\newblock ISSN 1364-6613.
\newblock \doi{10.1016/j.tics.2016.08.005}.

\bibitem[Kidd and Garcia(2022)]{kiddHowDiverseChild2022}
Evan Kidd and Rowena Garcia.
\newblock How diverse is child language acquisition research?
\newblock \emph{First Language}, 42\penalty0 (6):\penalty0 703--735, 2022.
\newblock ISSN 0142-7237.
\newblock \doi{10.1177/01427237211066405}.

\bibitem[Ganea et~al.(2008)Ganea, Pickard, and DeLoache]{ganeaTransferPictureBooks2008}
Patricia~A. Ganea, Megan~Bloom Pickard, and Judy~S. DeLoache.
\newblock Transfer between {{Picture Books}} and the {{Real World}} by {{Very Young Children}}.
\newblock \emph{Journal of Cognition and Development}, 9\penalty0 (1):\penalty0 46--66, 2008.
\newblock ISSN 1524-8372.
\newblock \doi{10.1080/15248370701836592}.

\bibitem[Simcock and DeLoache(2006)]{simcockGetPictureEffects2006}
Gabrielle Simcock and Judy DeLoache.
\newblock Get the picture? {{The}} effects of iconicity on toddlers' reenactment from picture books.
\newblock \emph{Developmental Psychology}, 42\penalty0 (6):\penalty0 1352--1357, 2006.
\newblock ISSN 1939-0599, 0012-1649.
\newblock \doi{10.1037/0012-1649.42.6.1352}.

\bibitem[Tare et~al.(2010)Tare, Chiong, Ganea, and DeLoache]{tareLessMoreHow2010}
Medha Tare, Cynthia Chiong, Patricia Ganea, and Judy DeLoache.
\newblock Less is more: {{How}} manipulative features affect children's learning from picture books.
\newblock \emph{Journal of Applied Developmental Psychology}, 31\penalty0 (5):\penalty0 395--400, 2010.
\newblock ISSN 0193-3973.
\newblock \doi{10.1016/j.appdev.2010.06.005}.

\bibitem[Long et~al.(2024{\natexlab{b}})Long, Xiang, Stojanov, Sparks, Yin, Keene, Tan, Feng, Zhuang, Marchman, Yamins, and Frank]{longBabyViewDatasetHighresolution2024}
Bria Long, Violet Xiang, Stefan Stojanov, Robert~Z. Sparks, Zi~Yin, Grace~E. Keene, Alvin W.~M. Tan, Steven~Y. Feng, Chengxu Zhuang, Virginia~A. Marchman, Daniel L.~K. Yamins, and Michael~C. Frank.
\newblock The {{BabyView}} dataset: {{High-resolution}} egocentric videos of infants' and young children's everyday experiences, 2024{\natexlab{b}}.

\bibitem[Ruan et~al.(2024)Ruan, Maddison, and Hashimoto]{ruanObservationalScalingLaws2024}
Yangjun Ruan, Chris~J. Maddison, and Tatsunori Hashimoto.
\newblock Observational {{Scaling Laws}} and the {{Predictability}} of {{Language Model Performance}}, 2024.

\bibitem[Wang(2024)]{wangCalibrationDeepLearning2024}
Cheng Wang.
\newblock Calibration in {{Deep Learning}}: {{A Survey}} of the {{State-of-the-Art}}, 2024.

\bibitem[Frank(2023{\natexlab{c}})]{frankOpenlyAccessibleLLMs2023}
Michael~C. Frank.
\newblock Openly accessible {{LLMs}} can help us to understand human cognition.
\newblock \emph{Nature Human Behaviour}, 7\penalty0 (11):\penalty0 1825--1827, 2023{\natexlab{c}}.
\newblock ISSN 2397-3374.
\newblock \doi{10.1038/s41562-023-01732-4}.

\bibitem[Frank(2024)]{frank2024LEVANTE}
Michael~C Frank.
\newblock Learning variability network exchange (levante): A global framework for measuring children’s learning variability through collaborative data sharing, Sep 2024.
\newblock URL \url{osf.io/preprints/psyarxiv/namx2}.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll{\'a}r, and Zitnick]{linMicrosoftCOCOCommon2014}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'a}r, and C.~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, \emph{Computer Vision -- ECCV 2014}, pages 740--755, Cham, 2014. Springer International Publishing.
\newblock ISBN 978-3-319-10602-1.

\bibitem[Schuhmann et~al.(2022)Schuhmann, Beaumont, Vencu, Gordon, Wightman, Cherti, Coombes, Katta, Mullis, Wortsman, Schramowski, Kundurthy, Crowson, Schmidt, Kaczmarczyk, and Jitsev]{schuhmannLAIONOpenLarge2022}
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.
\newblock Laion-5b: An open large-scale dataset for training next generation image-text models.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh, editors, \emph{Advances in Neural Information Processing Systems}, volume~35, pages 25278--25294. Curran Associates, Inc., 2022.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/a1859debfb3b59d094f3504d5ebb6c25-Paper-Datasets_and_Benchmarks.pdf}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Li, Li, Li, Zhang, Shen, and Lee]{liuLlavanextImprovedReasoning2024}
Haotian Liu, Chunyuan Li, Yuheng Li, Bo~Li, Yuanhan Zhang, Sheng Shen, and Yong~Jae Lee.
\newblock Llava-next: Improved reasoning, ocr, and world knowledge, January 2024{\natexlab{b}}.
\newblock URL \url{https://llava-vl.github.io/blog/2024-01-30-llava-next/}.

\bibitem[Liu et~al.(2023)Liu, Li, Wu, and Lee]{liuVisualInstructionTuning2023}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning, 2023.

\bibitem[Zhou et~al.(2024)Zhou, Hu, Weng, Jia, Luo, Liu, Wu, and Huang]{zhouTinyllavaFrameworkSmallscale2024}
Baichuan Zhou, Ying Hu, Xi~Weng, Junlong Jia, Jie Luo, Xien Liu, Ji~Wu, and Lei Huang.
\newblock Tinyllava: A framework of small-scale large multimodal models, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.14289}.

\bibitem[Peng et~al.(2023)Peng, Wang, Dong, Hao, Huang, Ma, and Wei]{peng2023kosmos2groundingmultimodallarge}
Zhiliang Peng, Wenhui Wang, Li~Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei.
\newblock Kosmos-2: Grounding multimodal large language models to the world, 2023.
\newblock URL \url{https://arxiv.org/abs/2306.14824}.

\bibitem[moondream AI~team(2024)]{moondream2}
moondream AI~team.
\newblock moondream2, 2024.
\newblock URL \url{https://github.com/vikhyat/moondream}.

\bibitem[Wang et~al.(2024)Wang, Lv, Yu, Hong, Qi, Wang, Ji, Yang, Zhao, Song, Xu, Xu, Li, Dong, Ding, and Tang]{wang2024cogvlmvisualexpertpretrained}
Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji~Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang.
\newblock Cogvlm: Visual expert for pretrained language models, 2024.
\newblock URL \url{https://arxiv.org/abs/2311.03079}.

\end{thebibliography}
