\begin{thebibliography}{10}

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ~R Salakhutdinov,
  and Quoc~V Le.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{lewis2019bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock {\em arXiv preprint arXiv:1910.13461}, 2019.

\bibitem{han2015deepcompress}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock {\em arXiv preprint arXiv:1510.00149}, 2015.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{zoph2016nas}
Barret Zoph and Quoc~V Le.
\newblock Neural architecture search with reinforcement learning.
\newblock {\em arXiv preprint arXiv:1611.01578}, 2016.

\bibitem{2021-iccv-oqa}
Mingzhu Shen, Feng Liang, Ruihao Gong, Yuhang Li, Chuming Li, Chen Lin, Fengwei
  Yu, Junjie Yan, and Wanli Ouyang.
\newblock Once quantization-aware training: High performance extremely low-bit
  architecture search.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, pages 5340--5349, October 2021.

\bibitem{2019-iccv-dsq}
Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin,
  Fengwei Yu, and Junjie Yan.
\newblock Differentiable soft quantization: Bridging full-precision and low-bit
  neural networks.
\newblock In {\em The IEEE International Conference on Computer Vision (ICCV)},
  October 2019.

\bibitem{esser2019learned}
Steven~K Esser, Jeffrey~L McKinstry, Deepika Bablani, Rathinakumar Appuswamy,
  and Dharmendra~S Modha.
\newblock Learned step size quantization.
\newblock {\em arXiv preprint arXiv:1902.08153}, 2019.

\bibitem{bhalgat2020lsq+}
Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak.
\newblock Lsq+: Improving low-bit quantization through learnable offsets and
  better initialization.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 696--697, 2020.

\bibitem{2020-cvpr-int8training}
Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li,
  Xiuqi Yang, and Junjie Yan.
\newblock Towards unified int8 training for convolutional neural network.
\newblock In {\em The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2020.

\bibitem{hubara2021accurate}
Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry.
\newblock Accurate post training quantization with small calibration sets.
\newblock In {\em International Conference on Machine Learning}, pages
  4466--4475. PMLR, 2021.

\bibitem{nahshan2019lapq}
Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner,
  Alex~M Bronstein, and Avi Mendelson.
\newblock Loss aware post-training quantization.
\newblock {\em arXiv preprint arXiv:1911.07190}, 2019.

\bibitem{cai2020zeroq}
Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael~W Mahoney, and Kurt
  Keutzer.
\newblock Zeroq: A novel zero shot quantization framework.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 13169--13178, 2020.

\bibitem{2021-cvpr-dsg}
Xiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao Gong, Qinghua Yan, Renshuai Tao,
  Yuhang Li, Fengwei Yu, and Xianglong Liu.
\newblock Diversifying sample generation for accurate data-free quantization.
\newblock In {\em The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2021.

\bibitem{nagel2020adaround}
Markus Nagel, Rana~Ali Amjad, Mart Van~Baalen, Christos Louizos, and Tijmen
  Blankevoort.
\newblock Up or down? adaptive rounding for post-training quantization.
\newblock In {\em International Conference on Machine Learning}, pages
  7197--7206. PMLR, 2020.

\bibitem{2021-iclr-brecq}
Yuhang Li, Ruihao Gong, Xu~Tan, Yang Yang, Peng Hu, Qi~Zhang, Fengwei Yu, Wei
  Wang, and Shi Gu.
\newblock Brecq: Pushing the limit of post-training quantization by block
  reconstruction.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{2022-iclr-qdrop}
Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Fengwei Yu.
\newblock Qdrop: Randomly dropping quantization for extremely low-bit
  post-training quantization.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{zafrir2019q8bert}
Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
\newblock Q8bert: Quantized 8bit bert.
\newblock In {\em 2019 Fifth Workshop on Energy Efficient Machine Learning and
  Cognitive Computing-NeurIPS Edition (EMC2-NIPS)}, pages 36--39. IEEE, 2019.

\bibitem{shen2020q}
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
  Michael~W Mahoney, and Kurt Keutzer.
\newblock Q-bert: Hessian based ultra low precision quantization of bert.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 8815--8821, 2020.

\bibitem{zhang2020ternarybert}
Wei Zhang, Lu~Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu.
\newblock Ternarybert: Distillation-aware ultra-low bit bert.
\newblock {\em arXiv preprint arXiv:2009.12812}, 2020.

\bibitem{bai2020binarybert}
Haoli Bai, Wei Zhang, Lu~Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu,
  Michael Lyu, and Irwin King.
\newblock Binarybert: Pushing the limit of bert quantization.
\newblock {\em arXiv preprint arXiv:2012.15701}, 2020.

\bibitem{kim2021bert}
Sehoon Kim, Amir Gholami, Zhewei Yao, Michael~W Mahoney, and Kurt Keutzer.
\newblock I-bert: Integer-only bert quantization.
\newblock In {\em International conference on machine learning}, pages
  5506--5518. PMLR, 2021.

\bibitem{bondarenko2021understanding}
Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.
\newblock Understanding and overcoming the challenges of efficient transformer
  quantization.
\newblock {\em arXiv preprint arXiv:2109.12948}, 2021.

\bibitem{luo2020positional}
Ziyang Luo, Artur Kulmizev, and Xiaoxi Mao.
\newblock Positional artefacts propagate through masked language model
  embeddings.
\newblock {\em arXiv preprint arXiv:2011.04393}, 2020.

\bibitem{choukroun2019low}
Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev.
\newblock Low-bit quantization of neural networks for efficient inference.
\newblock In {\em 2019 IEEE/CVF International Conference on Computer Vision
  Workshop (ICCVW)}, pages 3009--3018. IEEE, 2019.

\bibitem{mckinstry2019discovering}
Jeffrey~L. McKinstry, Steven~K. Esser, Rathinakumar Appuswamy, Deepika Bablani,
  John~V. Arthur, Izzet~B. Yildiz, and Dharmendra~S. Modha.
\newblock Discovering low-precision networks close to full-precision networks
  for efficient embedded inference, 2019.

\bibitem{dai2019transformer}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc~V Le, and Ruslan
  Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock {\em arXiv preprint arXiv:1901.02860}, 2019.

\bibitem{dehghani2018universal}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and {\L}ukasz
  Kaiser.
\newblock Universal transformers.
\newblock {\em arXiv preprint arXiv:1807.03819}, 2018.

\bibitem{clark2020electra}
Kevin Clark, Minh-Thang Luong, Quoc~V Le, and Christopher~D Manning.
\newblock Electra: Pre-training text encoders as discriminators rather than
  generators.
\newblock {\em arXiv preprint arXiv:2003.10555}, 2020.

\bibitem{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R
  Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock {\em arXiv preprint arXiv:1804.07461}, 2018.

\bibitem{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock {\em arXiv preprint arXiv:1606.05250}, 2016.

\bibitem{rajpurkar-etal-2018-know}
Pranav Rajpurkar, Robin Jia, and Percy Liang.
\newblock Know what you don{'}t know: Unanswerable questions for {SQ}u{AD}.
\newblock In {\em Proceedings of the 56th Annual Meeting of the Association for
  Computational Linguistics (Volume 2: Short Papers)}, pages 784--789.
  Association for Computational Linguistics, 2018.

\bibitem{narayan-etal-2018-dont}
Shashi Narayan, Shay~B. Cohen, and Mirella Lapata.
\newblock Don{'}t give me the details, just the summary! topic-aware
  convolutional neural networks for extreme summarization.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 1797--1807. Association for Computational
  Linguistics, 2018.

\bibitem{nallapati-etal-2016-abstractive}
Ramesh Nallapati, Bowen Zhou, C{\'{\i}}cero~Nogueira dos Santos, {\c{C}}aglar
  G{\"{u}}l{\c{c}}ehre, and Bing Xiang.
\newblock Abstractive text summarization using sequence-to-sequence {RNN}s and
  beyond.
\newblock In {\em Proceedings of The 20th {SIGNLL} Conference on Computational
  Natural Language Learning}, pages 280--290. Association for Computational
  Linguistics, 2016.

\bibitem{FasterTransformer}
NVIDIA.
\newblock Faster transformer.
\newblock \url{https://github.com/NVIDIA/FasterTransformer}, 2022.

\bibitem{google-whitepaper}
Raghuraman Krishnamoorthi.
\newblock Quantizing deep convolutional networks for efficient inference: A
  whitepaper.
\newblock {\em arXiv preprint arXiv:1806.08342}, 2018.

\bibitem{wu2020easyquant}
Di~Wu, Qi~Tang, Yongle Zhao, Ming Zhang, Ying Fu, and Debing Zhang.
\newblock Easyquant: Post-training quantization via scale optimization.
\newblock {\em arXiv preprint arXiv:2006.16669}, 2020.

\bibitem{choi2018pact}
Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang,
  Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan.
\newblock Pact: Parameterized clipping activation for quantized neural
  networks.
\newblock {\em arXiv preprint arXiv:1805.06085}, 2018.

\bibitem{NNCFTransformer}
Intel.
\newblock Nncf.
\newblock
  \url{https://github.com/openvinotoolkit/nncf/tree/develop/third_party_integration/huggingface_transformers},
  2022.

\bibitem{kure}
Brian Chmiel, Ron Banner, Gil Shomron, Yury Nahshan, Alex Bronstein, Uri
  Weiser, et~al.
\newblock Robust quantization: One model to rule them all.
\newblock {\em Advances in neural information processing systems},
  33:5308--5317, 2020.

\bibitem{bert_busters}
Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and Anna Rumshisky.
\newblock Bert busters: Outlier dimensions that disrupt transformers.
\newblock {\em arXiv preprint arXiv:2105.06990}, 2021.

\bibitem{token_frequency}
Giovanni Puccetti, Anna Rogers, Aleksandr Drozd, and Felice Dell'Orletta.
\newblock Outliers dimensions that disrupt transformers are driven by
  frequency.
\newblock {\em arXiv preprint arXiv:2205.11380}, 2022.

\bibitem{wu2020integer}
Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius.
\newblock Integer quantization for deep learning inference: Principles and
  empirical evaluation.
\newblock {\em arXiv preprint arXiv:2004.09602}, 2020.

\bibitem{fan2020training}
Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, R{\'e}mi Gribonval,
  Herve Jegou, and Armand Joulin.
\newblock Training with quantization noise for extreme model compression.
\newblock {\em arXiv preprint arXiv:2004.07320}, 2020.

\bibitem{tao2022compression}
Chaofan Tao, Lu~Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, and
  Ngai Wong.
\newblock Compression of generative pre-trained language models via
  quantization.
\newblock {\em arXiv preprint arXiv:2203.10705}, 2022.

\end{thebibliography}
