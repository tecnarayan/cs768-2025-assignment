\begin{thebibliography}{10}

\bibitem{gadre2023datacomp}
Samir~Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios
  Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu
  Zhang, et~al.
\newblock Datacomp: In search of the next generation of multimodal datasets.
\newblock {\em arXiv preprint arXiv:2304.14108}, 2023.

\bibitem{fang2023data}
Alex Fang, Albin~Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev,
  and Vaishaal Shankar.
\newblock Data filtering networks.
\newblock {\em arXiv preprint arXiv:2309.17425}, 2023.

\bibitem{kim2024hype}
Wonjae Kim, Sanghyuk Chun, Taekyung Kim, Dongyoon Han, and Sangdoo Yun.
\newblock Hype: Hyperbolic entailment filtering for underspecified images and
  texts.
\newblock {\em arXiv preprint arXiv:2404.17507}, 2024.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International conference on machine learning}, pages
  8748--8763. PMLR, 2021.

\bibitem{schuhmann2022laion}
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross
  Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell
  Wortsman, et~al.
\newblock Laion-5b: An open large-scale dataset for training next generation
  image-text models.
\newblock {\em Advances in Neural Information Processing Systems},
  35:25278--25294, 2022.

\bibitem{cherti2023reproducible}
Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel
  Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev.
\newblock Reproducible scaling laws for contrastive language-image learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 2818--2829, 2023.

\bibitem{bai2023qwen}
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang
  Lin, Chang Zhou, and Jingren Zhou.
\newblock Qwen-vl: A frontier large vision-language model with versatile
  abilities.
\newblock {\em arXiv preprint arXiv:2308.12966}, 2023.

\bibitem{vo2024automatic}
Huy~V Vo, Vasil Khalidov, Timoth{\'e}e Darcet, Th{\'e}o Moutakanni, Nikita
  Smetanin, Marc Szafraniec, Hugo Touvron, Camille Couprie, Maxime Oquab,
  Armand Joulin, et~al.
\newblock Automatic data curation for self-supervised learning: A
  clustering-based approach.
\newblock {\em arXiv preprint arXiv:2405.15613}, 2024.

\bibitem{huang2024multimodal}
Tzu-Heng Huang, Changho Shin, Sui~Jiet Tay, Dyah Adila, and Frederic Sala.
\newblock Multimodal data curation via object detection and filter ensembles.
\newblock {\em arXiv preprint arXiv:2401.12225}, 2024.

\bibitem{xie2023data}
Sang~Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy~S Liang.
\newblock Data selection for language models via importance resampling.
\newblock {\em Advances in Neural Information Processing Systems},
  36:34201--34227, 2023.

\bibitem{abbas2024effective}
Amro Abbas, Evgenia Rusak, Kushal Tirumala, Wieland Brendel, Kamalika
  Chaudhuri, and Ari~S Morcos.
\newblock Effective pruning of web-scale datasets based on complexity of
  concept clusters.
\newblock {\em arXiv preprint arXiv:2401.04578}, 2024.

\bibitem{maini2023t}
Pratyush Maini, Sachin Goyal, Zachary~C Lipton, J~Zico Kolter, and Aditi
  Raghunathan.
\newblock T-mars: Improving visual representations by circumventing text
  feature learning.
\newblock {\em arXiv preprint arXiv:2307.03132}, 2023.

\bibitem{chen2021fast}
Zhe Chen, Jiahao Wang, Wenhai Wang, Guo Chen, Enze Xie, Ping Luo, and Tong Lu.
\newblock Fast: Faster arbitrarily-shaped text detector with minimalist kernel
  representation, 2021.

\bibitem{yu2023devil}
Haichao Yu, Yu~Tian, Sateesh Kumar, Linjie Yang, and Heng Wang.
\newblock The devil is in the details: A deep dive into the rabbit hole of data
  filtering.
\newblock {\em arXiv preprint arXiv:2309.15954}, 2023.

\bibitem{joulin2016bag}
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov.
\newblock Bag of tricks for efficient text classification.
\newblock {\em arXiv preprint arXiv:1607.01759}, 2016.

\bibitem{li2023blip}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image
  encoders and large language models.
\newblock In {\em International conference on machine learning}, pages
  19730--19742. PMLR, 2023.

\bibitem{nguyen2023improving}
Thao Nguyen, Samir~Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig
  Schmidt.
\newblock Improving multimodal datasets with image captioning.
\newblock {\em arXiv preprint arXiv:2307.10350}, 2023.

\bibitem{maharana2023d2}
Adyasha Maharana, Prateek Yadav, and Mohit Bansal.
\newblock D2 pruning: Message passing for balancing diversity and difficulty in
  data pruning.
\newblock {\em arXiv preprint arXiv:2310.07931}, 2023.

\bibitem{mahmoud2023sieve}
Anas Mahmoud, Mostafa Elhoushi, Amro Abbas, Yu~Yang, Newsha Ardalani, Hugh
  Leather, and Ari Morcos.
\newblock Sieve: Multimodal dataset pruning using image captioning models.
\newblock {\em arXiv preprint arXiv:2310.02110}, 2023.

\bibitem{xia2024less}
Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi
  Chen.
\newblock Less: Selecting influential data for targeted instruction tuning.
\newblock {\em arXiv preprint arXiv:2402.04333}, 2024.

\bibitem{Xia_2023_combatting}
Xiaobo Xia, Bo~Han, Yibing Zhan, Jun Yu, Mingming Gong, Chen Gong, and
  Tongliang Liu.
\newblock Combating noisy labels with sample selection by mining
  high-discrepancy examples.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, pages 1833--1843, October 2023.

\bibitem{pmlr-v162-mindermann22a-prioritized}
S{\"o}ren Mindermann, Jan~M Brauner, Muhammed~T Razzak, Mrinank Sharma, Andreas
  Kirsch, Winnie Xu, Benedikt H{\"o}ltgen, Aidan~N Gomez, Adrien Morisot,
  Sebastian Farquhar, and Yarin Gal.
\newblock Prioritized training on points that are learnable, worth learning,
  and not yet learnt.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, {\em Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of {\em Proceedings
  of Machine Learning Research}, pages 15630--15649. PMLR, 17--23 Jul 2022.

\bibitem{shen2021much}
Sheng Shen, Liunian~Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei
  Chang, Zhewei Yao, and Kurt Keutzer.
\newblock How much can clip benefit vision-and-language tasks?
\newblock {\em arXiv preprint arXiv:2107.06383}, 2021.

\bibitem{zeng2022multi}
Yan Zeng, Xinsong Zhang, and Hang Li.
\newblock Multi-grained vision language pre-training: Aligning texts with
  visual concepts.
\newblock In {\em International Conference on Machine Learning}, pages
  25994--26009. PMLR, 2022.

\bibitem{yamada2022lemons}
Yutaro Yamada, Yingtian Tang, and Ilker Yildirim.
\newblock When are lemons purple? the concept association bias of clip.
\newblock {\em arXiv preprint arXiv:2212.12043}, 2022.

\bibitem{pmlr-v238-joshi24a}
Siddharth Joshi, Arnav Jain, Ali Payani, and Baharan Mirzasoleiman.
\newblock Data-efficient contrastive language-image pretraining: Prioritizing
  data quality over quantity.
\newblock In Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li, editors, {\em
  Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics}, volume 238 of {\em Proceedings of Machine Learning
  Research}, pages 1000--1008. PMLR, 02--04 May 2024.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{wang2019learning}
Haohan Wang, Songwei Ge, Zachary Lipton, and Eric~P Xing.
\newblock Learning robust global representations by penalizing local predictive
  power.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{pmlr-v97-recht19a}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do {I}mage{N}et classifiers generalize to {I}mage{N}et?
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, {\em
  Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of {\em Proceedings of Machine Learning Research}, pages
  5389--5400. PMLR, 09--15 Jun 2019.

\bibitem{hendrycks2021natural}
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.
\newblock Natural adversarial examples.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 15262--15271, 2021.

\bibitem{hendrycks2021many}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan
  Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et~al.
\newblock The many faces of robustness: A critical analysis of
  out-of-distribution generalization.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 8340--8349, 2021.

\bibitem{zhai2019large}
Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos
  Riquelme, Mario Lucic, Josip Djolonga, Andre~Susano Pinto, Maxim Neumann,
  Alexey Dosovitskiy, et~al.
\newblock A large-scale study of representation learning with the visual task
  adaptation benchmark.
\newblock {\em arXiv preprint arXiv:1910.04867}, 2019.

\bibitem{koh2021wilds}
Pang~Wei Koh, Shiori Sagawa, Henrik Marklund, Sang~Michael Xie, Marvin Zhang,
  Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard~Lanas Phillips,
  Irena Gao, et~al.
\newblock Wilds: A benchmark of in-the-wild distribution shifts.
\newblock In {\em International Conference on Machine Learning}, pages
  5637--5664. PMLR, 2021.

\bibitem{sagawa2021extending}
Shiori Sagawa, Pang~Wei Koh, Tony Lee, Irena Gao, Sang~Michael Xie, Kendrick
  Shen, Ananya Kumar, Weihua Hu, Michihiro Yasunaga, Henrik Marklund, et~al.
\newblock Extending the wilds benchmark for unsupervised adaptation.
\newblock {\em arXiv preprint arXiv:2112.05090}, 2021.

\bibitem{young-etal-2014-image}
Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.
\newblock From image descriptions to visual denotations: New similarity metrics
  for semantic inference over event descriptions.
\newblock {\em Transactions of the Association for Computational Linguistics},
  2:67--78, 2014.

\bibitem{chen2015microsoft}
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr
  Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco captions: Data collection and evaluation server.
\newblock {\em arXiv preprint arXiv:1504.00325}, 2015.

\bibitem{bitton2022winogavil}
Yonatan Bitton, Nitzan Bitton~Guetta, Ron Yosef, Yuval Elovici, Mohit Bansal,
  Gabriel Stanovsky, and Roy Schwartz.
\newblock Winogavil: Gamified association benchmark to challenge
  vision-and-language models.
\newblock {\em Advances in Neural Information Processing Systems},
  35:26549--26564, 2022.

\bibitem{hessel2021clipscore}
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan~Le Bras, and Yejin Choi.
\newblock Clipscore: A reference-free evaluation metric for image captioning.
\newblock {\em arXiv preprint arXiv:2104.08718}, 2021.

\bibitem{changpinyo2021conceptual12}
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.
\newblock Conceptual 12m: Pushing web-scale image-text pre-training to
  recognize long-tail visual concepts.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 3558--3568, 2021.

\bibitem{sharma2018conceptual3}
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset
  for automatic image captioning.
\newblock In Iryna Gurevych and Yusuke Miyao, editors, {\em Proceedings of the
  56th Annual Meeting of the Association for Computational Linguistics (Volume
  1: Long Papers)}, pages 2556--2565, Melbourne, Australia, July 2018.
  Association for Computational Linguistics.

\bibitem{nguyen2022quality}
Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, and Ludwig
  Schmidt.
\newblock Quality not quantity: On the interaction between dataset design and
  robustness of clip.
\newblock {\em Advances in Neural Information Processing Systems},
  35:21455--21469, 2022.

\bibitem{wang2024finetuned}
Weizhi Wang, Khalil Mrini, Linjie Yang, Sateesh Kumar, Yu~Tian, Xifeng Yan, and
  Heng Wang.
\newblock Finetuned multimodal language models are high-quality image-text data
  filters.
\newblock {\em arXiv preprint arXiv:2403.02677}, 2024.

\bibitem{liu2023improved}
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee.
\newblock Improved baselines with visual instruction tuning.
\newblock {\em arXiv preprint arXiv:2310.03744}, 2023.

\bibitem{chiang2023vicuna}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E Gonzalez, et~al.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt
  quality.
\newblock {\em See https://vicuna. lmsys. org (accessed 14 April 2023)},
  2(3):6, 2023.

\bibitem{desai2023hyperbolic}
Karan Desai, Maximilian Nickel, Tanmay Rajpurohit, Justin Johnson, and
  Shanmukha~Ramakrishna Vedantam.
\newblock Hyperbolic image-text representations.
\newblock In {\em International Conference on Machine Learning}, pages
  7694--7731. PMLR, 2023.

\bibitem{nakada2023understanding}
Ryumei Nakada, Halil~Ibrahim Gulluk, Zhun Deng, Wenlong Ji, James Zou, and
  Linjun Zhang.
\newblock Understanding multimodal contrastive learning and incorporating
  unpaired data.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 4348--4380. PMLR, 2023.

\bibitem{golub2013matrix}
Gene~H Golub and Charles~F Van~Loan.
\newblock {\em Matrix computations}.
\newblock JHU press, 2013.

\bibitem{wainwright2019high}
Martin~J Wainwright.
\newblock {\em High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge university press, 2019.

\bibitem{johnson2019billion}
Jeff Johnson, Matthijs Douze, and Herv{\'e} J{\'e}gou.
\newblock Billion-scale similarity search with gpus.
\newblock {\em IEEE Transactions on Big Data}, 7(3):535--547, 2019.

\bibitem{nguyen2024improving}
Thao Nguyen, Samir~Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig
  Schmidt.
\newblock Improving multimodal datasets with image captioning.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{goyal2024scaling}
Sachin Goyal, Pratyush Maini, Zachary~C Lipton, Aditi Raghunathan, and J~Zico
  Kolter.
\newblock Scaling laws for data filtering--data curation cannot be compute
  agnostic.
\newblock {\em arXiv preprint arXiv:2404.07177}, 2024.

\end{thebibliography}
