\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahn et~al.(2022)Ahn, Zhang, and Sra]{ahn2022understanding}
Kwangjun Ahn, Jingzhao Zhang, and Suvrit Sra.
\newblock Understanding the unstable convergence of gradient descent.
\newblock \emph{arXiv preprint arXiv:2204.01050}, 2022.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  322--332. PMLR, 2019.

\bibitem[Arora et~al.(2022)Arora, Li, and Panigrahi]{arora2022understanding}
Sanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi.
\newblock Understanding gradient descent on edge of stability in deep learning.
\newblock \emph{arXiv preprint arXiv:2205.09745}, 2022.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and
  Nocedal]{bottou2018optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{Siam Review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2019lazy}
Lenaic Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Cohen et~al.(2021)Cohen, Kaur, Li, Kolter, and
  Talwalkar]{cohen2021gradient}
Jeremy~M Cohen, Simran Kaur, Yuanzhi Li, J~Zico Kolter, and Ameet Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of
  stability.
\newblock \emph{arXiv preprint arXiv:2103.00065}, 2021.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International conference on machine learning}, pages
  1675--1685. PMLR, 2019.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018.

\bibitem[Foret et~al.(2020)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2020sharpness}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock \emph{arXiv preprint arXiv:2010.01412}, 2020.

\bibitem[Golmant et~al.()Golmant, Yao, Gholami, Mahoney, and
  Gonzalez]{golmantpytorchhessian}
Noah Golmant, Zhewei Yao, Amir Gholami, Michael Mahoney, and Joseph Gonzalez.
\newblock pytorchhessian-eigentings: efficient pytorch hessian
  eigendecomposition, october 2018.
\newblock \emph{URL https://github.
  com/noahgolmant/pytorch-hessian-eigenthings}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Hu et~al.(2020)Hu, Xiao, Adlam, and Pennington]{hu2020surprising}
Wei Hu, Lechao Xiao, Ben Adlam, and Jeffrey Pennington.
\newblock The surprising simplicity of the early-time learning dynamics of
  neural networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 17116--17128, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Jastrzebski et~al.(2020)Jastrzebski, Szymczak, Fort, Arpit, Tabor,
  Cho, and Geras]{jastrzebski2020break}
Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek
  Tabor, Kyunghyun Cho, and Krzysztof Geras.
\newblock The break-even point on optimization trajectories of deep neural
  networks.
\newblock \emph{arXiv preprint arXiv:2002.09572}, 2020.

\bibitem[Karakida et~al.(2019{\natexlab{a}})Karakida, Akaho, and
  Amari]{karakida2019normalization}
Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari.
\newblock The normalization method for alleviating pathological sharpness in
  wide neural networks.
\newblock \emph{Advances in neural information processing systems}, 32,
  2019{\natexlab{a}}.

\bibitem[Karakida et~al.(2019{\natexlab{b}})Karakida, Akaho, and
  Amari]{karakida2019pathological}
Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari.
\newblock Pathological spectra of the fisher information metric and its
  variants in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1910.05992}, 2019{\natexlab{b}}.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Lewkowycz et~al.(2020)Lewkowycz, Bahri, Dyer, Sohl-Dickstein, and
  Gur-Ari]{lewkowycz2020large}
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy
  Gur-Ari.
\newblock The large learning rate phase of deep learning: the catapult
  mechanism.
\newblock \emph{arXiv preprint arXiv:2003.02218}, 2020.

\bibitem[Li et~al.(2021)Li, Wang, and Arora]{li2021happens}
Zhiyuan Li, Tianhao Wang, and Sanjeev Arora.
\newblock What happens after sgd reaches zero loss?--a mathematical framework.
\newblock \emph{arXiv preprint arXiv:2110.06914}, 2021.

\bibitem[Lyu et~al.(2022)Lyu, Li, and Arora]{lyu2022understanding}
Kaifeng Lyu, Zhiyuan Li, and Sanjeev Arora.
\newblock Understanding the generalization benefit of normalization layers:
  Sharpness reduction.
\newblock \emph{arXiv preprint arXiv:2206.07085}, 2022.

\bibitem[Ma et~al.(2022)Ma, Wu, and Ying]{ma2022multiscale}
Chao Ma, Lei Wu, and Lexing Ying.
\newblock The multiscale structure of neural network loss functions: The effect
  on optimization and origin.
\newblock \emph{arXiv preprint arXiv:2204.11326}, 2022.

\bibitem[Martens(2016)]{martens2016second}
James Martens.
\newblock \emph{Second-order optimization for neural networks}.
\newblock University of Toronto (Canada), 2016.

\bibitem[Mulayoff et~al.(2021)Mulayoff, Michaeli, and
  Soudry]{mulayoff2021implicit}
Rotem Mulayoff, Tomer Michaeli, and Daniel Soudry.
\newblock The implicit bias of minima stability: A view from function space.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Papyan(2018)]{papyan2018full}
Vardan Papyan.
\newblock The full spectrum of deepnet hessians at scale: Dynamics with sgd
  training and sample size.
\newblock \emph{arXiv preprint arXiv:1811.07062}, 2018.

\bibitem[Papyan(2019)]{papyan2019measurements}
Vardan Papyan.
\newblock Measurements of three-level hierarchical structure in the outliers in
  the spectrum of deepnet hessians.
\newblock \emph{arXiv preprint arXiv:1901.08244}, 2019.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Sagun et~al.(2016)Sagun, Bottou, and LeCun]{sagun2016eigenvalues}
Levent Sagun, Leon Bottou, and Yann LeCun.
\newblock Eigenvalues of the hessian in deep learning: Singularity and beyond.
\newblock \emph{arXiv preprint arXiv:1611.07476}, 2016.

\bibitem[Sagun et~al.(2017)Sagun, Evci, Guney, Dauphin, and
  Bottou]{sagun2017empirical}
Levent Sagun, Utku Evci, V~Ugur Guney, Yann Dauphin, and Leon Bottou.
\newblock Empirical analysis of the hessian of over-parametrized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1706.04454}, 2017.

\bibitem[Wu et~al.(2018)Wu, Ma, et~al.]{wu2018sgd}
Lei Wu, Chao Ma, et~al.
\newblock How sgd selects the global minima in over-parameterized learning: A
  dynamical stability perspective.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Wu et~al.(2020)Wu, Zhu, Wu, Wang, and Ge]{wu2020dissecting}
Yikai Wu, Xingyu Zhu, Chenwei Wu, Annie Wang, and Rong Ge.
\newblock Dissecting hessian: Understanding common structure of hessian in
  neural networks.
\newblock \emph{arXiv preprint arXiv:2010.04261}, 2020.

\bibitem[Xing et~al.(2018)Xing, Arpit, Tsirigotis, and Bengio]{xing2018walk}
Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio.
\newblock A walk with sgd.
\newblock \emph{arXiv preprint arXiv:1802.08770}, 2018.

\end{thebibliography}
