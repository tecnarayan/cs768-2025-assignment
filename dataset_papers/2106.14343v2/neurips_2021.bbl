\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu(2018)]{allen2018natasha}
Allen-Zhu, Z.
\newblock Natasha 2: Faster non-convex optimization than sgd.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2675--2686, 2018.

\bibitem[Arjevani et~al.(2019)Arjevani, Carmon, Duchi, Foster, Srebro, and
  Woodworth]{arjevani2019lower}
Arjevani, Y., Carmon, Y., Duchi, J.~C., Foster, D.~J., Srebro, N., and
  Woodworth, B.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1912.02365}, 2019.

\bibitem[Arjevani et~al.(2020)Arjevani, Carmon, Duchi, Foster, Sekhari, and
  Sridharan]{arjevani2020second}
Arjevani, Y., Carmon, Y., Duchi, J.~C., Foster, D.~J., Sekhari, A., and
  Sridharan, K.
\newblock Second-order information in non-convex stochastic optimization: Power
  and limitations.
\newblock In \emph{Conference on Learning Theory}, pp.\  242--299, 2020.

\bibitem[Arnold et~al.(2019)Arnold, Manzagol, Babanezhad, Mitliagkas, and
  Roux]{arnold2019reducing}
Arnold, S.~M., Manzagol, P.-A., Babanezhad, R., Mitliagkas, I., and Roux, N.~L.
\newblock Reducing the variance in online optimization by transporting past
  gradients.
\newblock \emph{arXiv preprint arXiv:1906.03532}, 2019.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
  D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
  S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
  I., and Amodei, D.
\newblock Language models are few-shot learners, 2020.

\bibitem[Bubeck et~al.(2013)Bubeck, Cesa-Bianchi, and
  Lugosi]{bubeck2013bandits}
Bubeck, S., Cesa-Bianchi, N., and Lugosi, G.
\newblock Bandits with heavy tail.
\newblock \emph{IEEE Transactions on Information Theory}, 59\penalty0
  (11):\penalty0 7711--7717, 2013.

\bibitem[Chen et~al.(2018)Chen, Firat, Bapna, Johnson, Macherey, Foster, Jones,
  Schuster, Shazeer, Parmar, and et~al.]{Chen_2018_nmt_warmup}
Chen, M.~X., Firat, O., Bapna, A., Johnson, M., Macherey, W., Foster, G.,
  Jones, L., Schuster, M., Shazeer, N., Parmar, N., and et~al.
\newblock The best of both worlds: Combining recent advances in neural machine
  translation.
\newblock \emph{Proceedings of the 56th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, 2018.
\newblock \doi{10.18653/v1/p18-1008}.
\newblock URL \url{http://dx.doi.org/10.18653/v1/P18-1008}.

\bibitem[Cutkosky(2018)]{cutkosky2018algorithms}
Cutkosky, A.
\newblock \emph{Algorithms and Lower Bounds for Parameter-free Online
  Learning}.
\newblock PhD thesis, Stanford University, 2018.

\bibitem[Cutkosky \& Mehta(2020)Cutkosky and Mehta]{cutkosky2020momentum}
Cutkosky, A. and Mehta, H.
\newblock Momentum improves normalized sgd.
\newblock \emph{arXiv preprint arXiv:2002.03305}, 2020.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin-etal-2019-bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pp.\  4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://www.aclweb.org/anthology/N19-1423}.

\bibitem[Duchi et~al.(2010)Duchi, Hazan, and Singer]{duchi10adagrad}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2010.

\bibitem[Fang et~al.(2019)Fang, Lin, and Zhang]{fang2019sharp}
Fang, C., Lin, Z., and Zhang, T.
\newblock Sharp analysis for nonconvex sgd escaping from saddle points.
\newblock In \emph{Conference on Learning Theory}, pp.\  1192--1234, 2019.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013stochastic}
Ghadimi, S. and Lan, G.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Gorbunov et~al.(2020)Gorbunov, Danilova, and
  Gasnikov]{gorbunov2020stochastic}
Gorbunov, E., Danilova, M., and Gasnikov, A.
\newblock Stochastic optimization with heavy-tailed noise via accelerated
  gradient clipping.
\newblock In \emph{Neural Information Processing Systems}, 2020.

\bibitem[Goyal et~al.(2017)Goyal, Dollár, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola,
  A., Tulloch, A., Jia, Y., and He, K.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour, 2017.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{He2015DeepRL}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  770--778, 2015.

\bibitem[Howard et~al.(2018)Howard, Ramdas, McAuliffe, and
  Sekhon]{howard2018time}
Howard, S.~R., Ramdas, A., McAuliffe, J., and Sekhon, J.
\newblock Time-uniform, nonparametric, nonasymptotic confidence sequences.
\newblock \emph{arXiv preprint arXiv:1810.08240}, 2018.

\bibitem[Huang et~al.(2020)Huang, Perez, Ba, and Volkovs]{pmlr-v119-huang20f}
Huang, X.~S., Perez, F., Ba, J., and Volkovs, M.
\newblock Improving transformer optimization through better initialization.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  4475--4483. PMLR,
  13--18 Jul 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/huang20f.html}.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Li \& Orabona(2019)Li and Orabona]{li2019convergence}
Li, X. and Orabona, F.
\newblock On the convergence of stochastic gradient descent with adaptive
  stepsizes.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  983--992. PMLR, 2019.

\bibitem[Li \& Orabona(2020)Li and Orabona]{li2020high}
Li, X. and Orabona, F.
\newblock A high probability analysis of adaptive sgd with momentum.
\newblock \emph{arXiv preprint arXiv:2007.14294}, 2020.

\bibitem[Liu et~al.(2020)Liu, Jiang, He, Chen, Liu, Gao, and Han]{Liu2020On}
Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkgz2aEKDr}.

\bibitem[Lugosi \& Mendelson(2019)Lugosi and Mendelson]{lugosi2019mean}
Lugosi, G. and Mendelson, S.
\newblock Mean estimation and regression under heavy-tailed distributions: A
  survey.
\newblock \emph{Foundations of Computational Mathematics}, 19\penalty0
  (5):\penalty0 1145--1190, 2019.

\bibitem[Minsker(2011)]{minsker2011some}
Minsker, S.
\newblock On some extensions of bernstein's inequality for self-adjoint
  operators.
\newblock \emph{arXiv preprint arXiv:1112.5448}, 2011.

\bibitem[Nguyen \& Salazar(2019)Nguyen and Salazar]{Nguyen2019TransformersWT}
Nguyen, T.~Q. and Salazar, J.
\newblock Transformers without tears: Improving the normalization of
  self-attention.
\newblock \emph{ArXiv}, abs/1910.05895, 2019.

\bibitem[Pascanu et~al.(2012)Pascanu, Mikolov, and
  Bengio]{pascanu2012understanding}
Pascanu, R., Mikolov, T., and Bengio, Y.
\newblock Understanding the exploding gradient problem.
\newblock \emph{CoRR, abs/1211.5063}, 2\penalty0 (417):\penalty0 1, 2012.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{JMLR:v21:20-074:T5-paper}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (140):\penalty0 1--67, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{reddi2018on}
Reddi, S.~J., Kale, S., and Kumar, S.
\newblock On the convergence of adam and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Scaman \& Malherbe(2020)Scaman and Malherbe]{scaman2020robustness}
Scaman, K. and Malherbe, C.
\newblock Robustness analysis of non-convex stochastic gradient descent using
  biased expectations.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[{\c{S}}im{\c{s}}ekli et~al.(2019){\c{S}}im{\c{s}}ekli,
  G{\"u}rb{\"u}zbalaban, Nguyen, Richard, and Sagun]{simcsekli2019heavy}
{\c{S}}im{\c{s}}ekli, U., G{\"u}rb{\"u}zbalaban, M., Nguyen, T.~H., Richard,
  G., and Sagun, L.
\newblock On the heavy-tailed theory of stochastic gradient descent for deep
  neural networks.
\newblock \emph{arXiv preprint arXiv:1912.00018}, 2019.

\bibitem[Simsekli et~al.(2019)Simsekli, Sagun, and
  Gurbuzbalaban]{simsekli2019tail}
Simsekli, U., Sagun, L., and Gurbuzbalaban, M.
\newblock A tail-index analysis of stochastic gradient noise in deep neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5827--5837. PMLR, 2019.

\bibitem[Tripuraneni et~al.(2018)Tripuraneni, Stern, Jin, Regier, and
  Jordan]{tripuraneni2018stochastic}
Tripuraneni, N., Stern, M., Jin, C., Regier, J., and Jordan, M.~I.
\newblock Stochastic cubic regularization for fast nonconvex optimization.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2899--2908, 2018.

\bibitem[Tropp(2011)]{tropp2011freedman}
Tropp, J.
\newblock Freedman's inequality for matrix martingales.
\newblock \emph{Electronic Communications in Probability}, 16:\penalty0
  262--270, 2011.

\bibitem[Tropp(2015)]{tropp2015introduction}
Tropp, J.~A.
\newblock An introduction to matrix concentration inequalities.
\newblock \emph{arXiv preprint arXiv:1501.01571}, 2015.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in Neural Information Processing Systems},
  30:\penalty0 5998--6008, 2017.

\bibitem[Wang et~al.(2021)Wang, G{\"u}rb{\"u}zbalaban, Zhu,
  {\c{S}}im{\c{s}}ekli, and Erdogdu]{wang2021convergence}
Wang, H., G{\"u}rb{\"u}zbalaban, M., Zhu, L., {\c{S}}im{\c{s}}ekli, U., and
  Erdogdu, M.~A.
\newblock Convergence rates of stochastic gradient descent under infinite noise
  variance.
\newblock \emph{arXiv preprint arXiv:2102.10346}, 2021.

\bibitem[Wang et~al.(2019)Wang, Li, Xiao, Zhu, Li, Wong, and Chao]{Wang_2019}
Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D.~F., and Chao, L.~S.
\newblock Learning deep transformer models for machine translation.
\newblock \emph{Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, 2019.
\newblock \doi{10.18653/v1/p19-1176}.
\newblock URL \url{http://dx.doi.org/10.18653/v1/P19-1176}.

\bibitem[Ward et~al.(2019)Ward, Wu, and Bottou]{ward2019adagrad}
Ward, R., Wu, X., and Bottou, L.
\newblock Adagrad stepsizes: Sharp convergence over nonconvex landscapes.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6677--6686. PMLR, 2019.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{you2017large}
You, Y., Gitman, I., and Ginsburg, B.
\newblock Large batch training of convolutional networks.
\newblock \emph{arXiv preprint arXiv:1708.03888}, 2017.

\bibitem[You et~al.(2020)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song,
  Demmel, Keutzer, and Hsieh]{You2020LargeLAMB}
You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X.,
  Demmel, J., Keutzer, K., and Hsieh, C.-J.
\newblock Large batch optimization for deep learning: Training bert in 76
  minutes.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=Syx4wnEtvH}.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Jin, Fang, and
  Wang]{zhang2020improved}
Zhang, B., Jin, J., Fang, C., and Wang, L.
\newblock Improved analysis of clipping algorithms for non-convex optimization.
\newblock \emph{arXiv preprint arXiv:2010.02519}, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2019)Zhang, He, Sra, and Jadbabaie]{zhang2019gradient}
Zhang, J., He, T., Sra, S., and Jadbabaie, A.
\newblock Why gradient clipping accelerates training: A theoretical
  justification for adaptivity.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Karimireddy, Veit, Kim, Reddi,
  Kumar, and Sra]{zhang2020adaptive}
Zhang, J., Karimireddy, S.~P., Veit, A., Kim, S., Reddi, S., Kumar, S., and
  Sra, S.
\newblock Why are adaptive methods good for attention models?
\newblock \emph{Advances in Neural Information Processing Systems}, 33,
  2020{\natexlab{b}}.

\end{thebibliography}
