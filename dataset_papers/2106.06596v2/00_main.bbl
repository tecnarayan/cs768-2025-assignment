\begin{thebibliography}{}

\bibitem[Adlam et~al., 2020]{adlam2020cold}
Adlam, B., Snoek, J., and Smith, S.~L. (2020).
\newblock Cold posteriors and aleatoric uncertainty.
\newblock {\em arXiv preprint arXiv:2008.00029}.

\bibitem[Aitchison, 2020]{aitchison2020statistical}
Aitchison, L. (2020).
\newblock A statistical theory of cold posteriors in deep neural networks.
\newblock {\em arXiv preprint arXiv:2008.05912}.

\bibitem[Atanov et~al., 2018]{atanov2018deep}
Atanov, A., Ashukha, A., Struminsky, K., Vetrov, D., and Welling, M. (2018).
\newblock The deep weight prior.
\newblock {\em arXiv preprint arXiv:1810.06943}.

\bibitem[Barber and Bishop, 1998]{barber1998ensemblelearning}
Barber, D. and Bishop, C.~M. (1998).
\newblock Ensemble learning for multi-layer networks.
\newblock In {\em Advances in neural information processing systems}, pages
  395--401.

\bibitem[Bhattacharya et~al., 2019]{bhattacharya2019fractionalposteriors}
Bhattacharya, A., Pati, D., Yang, Y., et~al. (2019).
\newblock Bayesian fractional posteriors.
\newblock {\em The Annals of Statistics}, 47(1):39--66.

\bibitem[Blundell et~al., 2015]{blundell2015weightuncertainty}
Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. (2015).
\newblock Weight uncertainty in neural network.
\newblock 37:1613--1622.

\bibitem[Chen et~al., 2016]{chen2016bridging}
Chen, C., Carlson, D., Gan, Z., Li, C., and Carin, L. (2016).
\newblock Bridging the gap between stochastic gradient mcmc and stochastic
  optimization.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1051--1060.
  PMLR.

\bibitem[Chen et~al., 2014]{chen2014stochastichmc}
Chen, T., Fox, E., and Guestrin, C. (2014).
\newblock Stochastic gradient {Hamiltonian Monte Carlo}.
\newblock In {\em International Conference on Machine Learning}, pages
  1683--1691.

\bibitem[Dayan et~al., 1995]{dayan1995helmholtz}
Dayan, P., Hinton, G.~E., Neal, R.~M., and Zemel, R.~S. (1995).
\newblock The helmholtz machine.
\newblock {\em Neural computation}, 7(5):889--904.

\bibitem[Fortuin et~al., 2021]{fortuin2021bayesian}
Fortuin, V., Garriga-Alonso, A., Wenzel, F., R{\"a}tsch, G., Turner, R.,
  van~der Wilk, M., and Aitchison, L. (2021).
\newblock Bayesian neural network priors revisited.
\newblock {\em arXiv preprint arXiv:2102.06571}.

\bibitem[Gr{\"u}nwald, 2011]{grunwald2011safe}
Gr{\"u}nwald, P. (2011).
\newblock Safe learning: bridging the gap between bayes, mdl and statistical
  learning theory via empirical convexity.
\newblock In {\em Proceedings of the 24th Annual Conference on Learning
  Theory}, pages 397--420. JMLR Workshop and Conference Proceedings.

\bibitem[Gr{\"u}nwald, 2012]{grunwald2012safe}
Gr{\"u}nwald, P. (2012).
\newblock The safe bayesian.
\newblock In {\em International Conference on Algorithmic Learning Theory},
  pages 169--183. Springer.

\bibitem[Gr{\"u}nwald et~al., 2017]{grunwald2017inconsistency}
Gr{\"u}nwald, P., Van~Ommen, T., et~al. (2017).
\newblock Inconsistency of bayesian inference for misspecified linear models,
  and a proposal for repairing it.
\newblock {\em Bayesian Analysis}, 12(4):1069--1103.

\bibitem[Guo et~al., 2017]{guo2017calibration}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q. (2017).
\newblock On calibration of modern neural networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1321--1330. JMLR. org.

\bibitem[He et~al., 2016]{resnet2016}
He, K., Zhang, X., Ren, S., and Sun, J. (2016).
\newblock Deep residual learning for image recognition.
\newblock In {\em In IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 770--778.

\bibitem[Hinton and Van~Camp, 1993]{hinton1993mdl}
Hinton, G. and Van~Camp, D. (1993).
\newblock Keeping neural networks simple by minimizing the description length
  of the weights.
\newblock In {\em in Proc. of the 6th Ann. ACM Conf. on Computational Learning
  Theory}.

\bibitem[Izmailov et~al., 2021]{izmailov2021bayesian}
Izmailov, P., Vikram, S., Hoffman, M.~D., and Wilson, A.~G. (2021).
\newblock What are bayesian neural network posteriors really like?
\newblock {\em arXiv preprint arXiv:2104.14421}.

\bibitem[Jansen, 2013]{jansen2013misspecification}
Jansen, L. (2013).
\newblock Robust {Bayesian} inference under model misspecification.
\newblock Master thesis.

\bibitem[Kaplan et~al., 2020]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R.,
  Gray, S., Radford, A., Wu, J., and Amodei, D. (2020).
\newblock Scaling laws for neural language models.
\newblock {\em arXiv preprint arXiv:2001.08361}.

\bibitem[Kingma and Ba, 2014]{kingma2014adam}
Kingma, D.~P. and Ba, J. (2014).
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}.

\bibitem[Krizhevsky and Hinton, 2009]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G. (2009).
\newblock Learning multiple layers of features from tiny images.

\bibitem[Leimkuhler et~al., 2019]{leimkuhler2019partitionedlangevin}
Leimkuhler, B., Matthews, C., and Vlaar, T. (2019).
\newblock Partitioned integrators for thermodynamic parameterization of neural
  networks.
\newblock {\em arXiv preprint arXiv:1908.11843}.

\bibitem[Li et~al., 2016]{li2016preconditionedsgld}
Li, C., Chen, C., Carlson, D., and Carin, L. (2016).
\newblock Preconditioned stochastic gradient {Langevin} dynamics for deep
  neural networks.
\newblock In {\em Thirtieth AAAI Conference on Artificial Intelligence}.

\bibitem[Ma et~al., 2015]{ma2015complete}
Ma, Y.-A., Chen, T., and Fox, E. (2015).
\newblock A complete recipe for stochastic gradient {MCMC}.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2917--2925.

\bibitem[MacKay, 1992]{mackay1992practical}
MacKay, D.~J. (1992).
\newblock A practical bayesian framework for backpropagation networks.
\newblock {\em Neural computation}, 4(3):448--472.

\bibitem[MacKay et~al., 1995]{mackay1995ensemblelearning}
MacKay, D.~J. et~al. (1995).
\newblock Ensemble learning and evidence maximization.
\newblock In {\em Proc. Nips}, volume~10, page 4083. Citeseer.

\bibitem[Neal, 1995]{neal1995bayesianneuralnetworks}
Neal, R.~M. (1995).
\newblock {\em Bayesian learning for neural networks}.
\newblock PhD thesis, University of Toronto.

\bibitem[Netzer et~al., 2011]{netzer2011reading}
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A.~Y. (2011).
\newblock Reading digits in natural images with unsupervised feature learning.

\bibitem[Ovadia et~al., 2019]{ovadia2019modeluncertainty}
Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Nowozin, S., Dillon,
  J.~V., Lakshminarayanan, B., and Snoek, J. (2019).
\newblock Can you trust your model's uncertainty? evaluating predictive
  uncertainty under dataset shift.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS
  2019)}.

\bibitem[Peterson et~al., 2019]{peterson2019human}
Peterson, J.~C., Battleday, R.~M., Griffiths, T.~L., and Russakovsky, O.
  (2019).
\newblock Human uncertainty makes classification more robust.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9617--9626.

\bibitem[Quionero-Candela et~al., 2009]{quionero2009dataset}
Quionero-Candela, J., Sugiyama, M., Schwaighofer, A., and Lawrence, N.~D.
  (2009).
\newblock {\em Dataset shift in machine learning}.
\newblock The MIT Press.

\bibitem[Sutskever et~al., 2013]{sutskever2013momentum}
Sutskever, I., Martens, J., Dahl, G., and Hinton, G. (2013).
\newblock On the importance of initialization and momentum in deep learning.
\newblock In {\em International conference on machine learning}, pages
  1139--1147.

\bibitem[Welling and Teh, 2011]{welling2011bayesian}
Welling, M. and Teh, Y.~W. (2011).
\newblock Bayesian learning via stochastic gradient {Langevin} dynamics.
\newblock In {\em Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 681--688.

\bibitem[Wenzel et~al., 2020]{wenzel2020good}
Wenzel, F., Roth, K., Veeling, B., Swiatkowski, J., Tran, L., Mandt, S., Snoek,
  J., Salimans, T., Jenatton, R., and Nowozin, S. (2020).
\newblock How good is the bayes posterior in deep neural networks really?
\newblock In {\em International Conference on Machine Learning}, pages
  10248--10259. PMLR.

\bibitem[Wilk et~al., 2018]{wilk2018learning}
Wilk, M. v.~d., Bauer, M., John, S., and Hensman, J. (2018).
\newblock Learning invariances using the marginal likelihood.
\newblock In {\em Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 9960--9970.

\bibitem[Wilson, 2019]{wilson2019bayesian}
Wilson, A.~G. (2019).
\newblock The case for {B}ayesian deep learning.
\newblock {\em NYU Courant Technical Report}.
\newblock Accessible at \url{https://cims.nyu.edu/~andrewgw/caseforbdl.pdf}.

\bibitem[Wilson and Izmailov, 2020]{wilson2020bayesianperspective}
Wilson, A.~G. and Izmailov, P. (2020).
\newblock Bayesian deep learning and a probabilistic perspective of
  generalization.
\newblock {\em arXiv preprint arXiv:2002.08791}.

\bibitem[Zeno et~al., 2020]{zeno2020cold}
Zeno, C., Golan, I., Pakman, A., and Soudry, D. (2020).
\newblock Why cold posteriors? on the suboptimal generalization of optimal
  bayes estimates.
\newblock {\em 3rd Symposium on Advances in Approximate Bayesian Inference}.

\bibitem[Zhang et~al., 2020]{zhang2019cyclicalsgmcmc}
Zhang, R., Li, C., Zhang, J., Chen, C., and Wilson, A.~G. (2020).
\newblock Cyclical stochastic gradient {MCMC} for {Bayesian} deep learning.
\newblock In {\em International Conference on Learning Representations (ICLR
  2020)}.

\end{thebibliography}
