\begin{thebibliography}{}

\bibitem[Allen-Zhu and Li, 2019]{allen2019can}
Allen-Zhu, Z. and Li, Y. (2019).
\newblock What can resnet learn efficiently, going beyond kernels?
\newblock {\em arXiv preprint arXiv:1905.10337}.

\bibitem[Allen-Zhu et~al., 2018]{allen2018learning}
Allen-Zhu, Z., Li, Y., and Liang, Y. (2018).
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock {\em arXiv preprint arXiv:1811.04918}.

\bibitem[Andoni et~al., 2014]{andoni2014learning}
Andoni, A., Panigrahy, R., Valiant, G., and Zhang, L. (2014).
\newblock Learning polynomials with neural networks.
\newblock In {\em International conference on machine learning}, pages
  1908--1916.

\bibitem[Arora et~al., 2019a]{arora2019exact}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R., and Wang, R. (2019a).
\newblock On exact computation with an infinitely wide neural net.
\newblock {\em arXiv preprint arXiv:1904.11955}.

\bibitem[Arora et~al., 2019b]{arora2019fine}
Arora, S., Du, S.~S., Hu, W., Li, Z., and Wang, R. (2019b).
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock {\em arXiv preprint arXiv:1901.08584}.

\bibitem[Bai et~al., 2020]{bai2020taylorized}
Bai, Y., Krause, B., Wang, H., Xiong, C., and Socher, R. (2020).
\newblock Taylorized training: Towards better approximation of neural network
  training at finite width.
\newblock {\em arXiv preprint arXiv:2002.04010}.

\bibitem[Bai and Lee, 2019]{bai2019beyond}
Bai, Y. and Lee, J.~D. (2019).
\newblock Beyond linearization: On quadratic and higher-order approximation of
  wide neural networks.
\newblock {\em arXiv preprint arXiv:1910.01619}.

\bibitem[Carbery and Wright, 2001]{carbery2001distributional}
Carbery, A. and Wright, J. (2001).
\newblock Distributional and $l^q$ norm inequalities for polynomials over
  convex bodies in $r^n$.
\newblock {\em Mathematical research letters}, 8(3):233--248.

\bibitem[Cardoso, 1991]{cardoso1991super}
Cardoso, J.-F. (1991).
\newblock Super-symmetric decomposition of the fourth-order cumulant tensor.
  blind identification of more sources than sensors.
\newblock In {\em International Conference on Acoustics, Speech, \& Signal
  Processing, Icassp}.

\bibitem[Chizat and Bach, 2018a]{chizat2018note}
Chizat, L. and Bach, F. (2018a).
\newblock A note on lazy training in supervised differentiable programming.
\newblock {\em arXiv preprint arXiv:1812.07956}, 8.

\bibitem[Chizat and Bach, 2018b]{chizat2018global}
Chizat, L. and Bach, F. (2018b).
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock In {\em Advances in neural information processing systems}, pages
  3040--3050.

\bibitem[Daniely, 2019]{daniely2019neural}
Daniely, A. (2019).
\newblock Neural networks learning and memorization with (almost) no
  over-parameterization.
\newblock {\em arXiv preprint arXiv:1911.09873}.

\bibitem[Dasgupta and Gupta, 2003]{dasgupta2003elementary}
Dasgupta, S. and Gupta, A. (2003).
\newblock An elementary proof of a theorem of johnson and lindenstrauss.
\newblock {\em Random Structures \& Algorithms}, 22(1):60--65.

\bibitem[Du et~al., 2018a]{du2018gradientb}
Du, S.~S., Lee, J.~D., Li, H., Wang, L., and Zhai, X. (2018a).
\newblock Gradient descent finds global minima of deep neural networks.
\newblock {\em arXiv preprint arXiv:1811.03804}.

\bibitem[Du et~al., 2018b]{du2018gradient}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A. (2018b).
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock {\em arXiv preprint arXiv:1810.02054}.

\bibitem[Dyer and Gur-Ari, 2019]{dyer2019asymptotics}
Dyer, E. and Gur-Ari, G. (2019).
\newblock Asymptotics of wide networks from feynman diagrams.
\newblock {\em arXiv preprint arXiv:1909.11304}.

\bibitem[Ge et~al., 2015]{ge2015escaping}
Ge, R., Huang, F., Jin, C., and Yuan, Y. (2015).
\newblock Escaping from saddle points—online stochastic gradient for tensor
  decomposition.
\newblock In {\em Proceedings of The 28th Conference on Learning Theory}, pages
  797--842.

\bibitem[Ge et~al., 2016]{ge2016matrix}
Ge, R., Lee, J.~D., and Ma, T. (2016).
\newblock Matrix completion has no spurious local minimum.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2973--2981.

\bibitem[Ge et~al., 2018]{ge2017learning}
Ge, R., Lee, J.~D., and Ma, T. (2018).
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Ghorbani et~al., 2019]{ghorbani2019limitations}
Ghorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A. (2019).
\newblock Limitations of lazy training of two-layers neural networks.
\newblock {\em arXiv preprint arXiv:1906.08899}.

\bibitem[Harshman, 1970]{harshman1970foundations}
Harshman, R. (1970).
\newblock Foundations of the parafac procedure: Model and conditions for an
  explanatory factor analysis.
\newblock {\em Technical Report UCLA Working Papers in Phonetics 16, University
  of California, Los Angeles, Los Angeles, CA}.

\bibitem[Hillar and Lim, 2013]{hillar2013most}
Hillar, C.~J. and Lim, L.-H. (2013).
\newblock Most tensor problems are np-hard.
\newblock {\em Journal of the ACM (JACM)}, 60(6):1--39.

\bibitem[Huang and Yau, 2019]{huang2019dynamics}
Huang, J. and Yau, H.-T. (2019).
\newblock Dynamics of deep neural networks and neural tangent hierarchy.
\newblock {\em arXiv preprint arXiv:1909.08156}.

\bibitem[Jacot et~al., 2018]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C. (2018).
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  8571--8580.

\bibitem[Lee et~al., 2019]{lee2019wide}
Lee, J., Xiao, L., Schoenholz, S.~S., Bahri, Y., Sohl-Dickstein, J., and
  Pennington, J. (2019).
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock {\em arXiv preprint arXiv:1902.06720}.

\bibitem[Livni et~al., 2013]{livni2013algorithm}
Livni, R., Shalev-Shwartz, S., and Shamir, O. (2013).
\newblock An algorithm for training polynomial networks.
\newblock {\em arXiv preprint arXiv:1304.7045}.

\bibitem[Livni et~al., 2014]{livni2014computational}
Livni, R., Shalev-Shwartz, S., and Shamir, O. (2014).
\newblock On the computational efficiency of training neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  855--863.

\bibitem[Ma et~al., 2016]{ma2016polynomial}
Ma, T., Shi, J., and Steurer, D. (2016).
\newblock Polynomial-time tensor decompositions with sum-of-squares.
\newblock In {\em 2016 IEEE 57th Annual Symposium on Foundations of Computer
  Science (FOCS)}, pages 438--446. IEEE.

\bibitem[Mei et~al., 2018]{mei2018mean}
Mei, S., Montanari, A., and Nguyen, P.-M. (2018).
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock {\em Proceedings of the National Academy of Sciences},
  115(33):E7665--E7671.

\bibitem[Oymak and Soltanolkotabi, 2020]{oymak2020towards}
Oymak, S. and Soltanolkotabi, M. (2020).
\newblock Towards moderate overparameterization: global convergence guarantees
  for training shallow neural networks.
\newblock {\em IEEE Journal on Selected Areas in Information Theory}.

\bibitem[Rotskoff and Vanden-Eijnden, 2018]{rotskoff2018neural}
Rotskoff, G.~M. and Vanden-Eijnden, E. (2018).
\newblock Neural networks as interacting particle systems: Asymptotic convexity
  of the loss landscape and universal scaling of the approximation error.
\newblock {\em arXiv preprint arXiv:1805.00915}.

\bibitem[Sirignano and Spiliopoulos, 2018]{sirignano2018mean}
Sirignano, J. and Spiliopoulos, K. (2018).
\newblock Mean field analysis of neural networks.
\newblock {\em arXiv preprint arXiv:1805.01053}.

\bibitem[Vershynin, 2018]{vershynin2018high}
Vershynin, R. (2018).
\newblock {\em High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge University Press.

\bibitem[Vignat and Bhatnagar, 2008]{vignat2008extension}
Vignat, C. and Bhatnagar, S. (2008).
\newblock An extension of wick’s theorem.
\newblock {\em Statistics \& probability letters}, 78(15):2404--2407.

\bibitem[Wei et~al., 2018]{wei2018margin}
Wei, C., Lee, J.~D., Liu, Q., and Ma, T. (2018).
\newblock On the margin theory of feedforward neural networks.
\newblock {\em arXiv preprint arXiv:1810.05369}.

\bibitem[Wei et~al., 2019]{wei2019regularization}
Wei, C., Lee~Jason, D., Liu, Q., and Ma, T. (2019).
\newblock Regularization matters: Generalization and optimization of neural
  nets vs their induced kernel.
\newblock {\em arXiv preprint arXiv:1810.05369}.

\bibitem[Woodworth et~al., 2020]{woodworth2020kernel}
Woodworth, B., Gunasekar, S., Lee, J.~D., Moroshko, E., Savarese, P., Golan,
  I., Soudry, D., and Srebro, N. (2020).
\newblock Kernel and rich regimes in overparametrized models.
\newblock {\em arXiv preprint arXiv:2002.09277}.

\bibitem[Yehudai and Shamir, 2019]{yehudai2019power}
Yehudai, G. and Shamir, O. (2019).
\newblock On the power and limitations of random features for understanding
  neural networks.
\newblock {\em arXiv preprint arXiv:1904.00687}.

\bibitem[Zou and Gu, 2019]{zou2019improved}
Zou, D. and Gu, Q. (2019).
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2053--2062.

\end{thebibliography}
