\begin{thebibliography}{19}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal and Jia(2017)]{agrawal2017}
Shipra Agrawal and Randy Jia.
\newblock Posterior sampling for reinforcement learning: worst-case regret
  bounds.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, 2017.

\bibitem[Auer and Ortner(2007)]{ucrl}
Peter Auer and Ronald Ortner.
\newblock Logarithmic online regret bounds for undiscounted reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems 19}, 2007.

\bibitem[Auer et~al.(2009)Auer, Jaksch, and Ortner]{auer2009near}
Peter Auer, Thomas Jaksch, and Ronald Ortner.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems 22}, 2009.

\bibitem[Bartlett and Tewari(2009)]{regal}
Peter~L. Bartlett and Ambuj Tewari.
\newblock {REGAL}: A regularization based algorithm for reinforcement learning
  in weakly communicating {MDPs}.
\newblock In \emph{Proceedings of the 25th Conference on Uncertainty in
  Artificial Intelligence}, 2009.

\bibitem[Burnetas and Katehakis(1997)]{burnetas1997optimal}
Apostolos~N. Burnetas and Michael~N. Katehakis.
\newblock Optimal adaptive policies for {Markov} decision processes.
\newblock \emph{Mathematics of Operations Research}, 22\penalty0 (1):\penalty0
  222--255, 1997.

\bibitem[Combes and Proutiere(2014)]{Combes2014}
Richard Combes and Alexandre Proutiere.
\newblock Unimodal bandits: Regret lower bounds and optimal algorithms.
\newblock In \emph{Proceedings of the 31st International Conference on Machine
  Learning}, 2014.

\bibitem[Combes et~al.(2017)Combes, Magureanu, and Proutiere]{combesnips17}
Richard Combes, Stefan Magureanu, and Alexandre Proutiere.
\newblock Minimal exploration in structured stochastic bandits.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, 2017.

\bibitem[Filippi et~al.(2010)Filippi, Capp{\'e}, and Garivier]{filippi}
Sarah Filippi, Olivier Capp{\'e}, and Aur{\'e}lien Garivier.
\newblock Optimism in reinforcement learning and {Kullback-Leibler} divergence.
\newblock In \emph{48th Annual Allerton Conference on Communication, Control,
  and Computing}, 2010.

\bibitem[Garivier et~al.(Jun. 2018)Garivier, Ménard, and Stoltz]{garivier193}
Aurélien Garivier, Pierre Ménard, and Gilles Stoltz.
\newblock Explore first, exploit next: The true shape of regret in bandit
  problems.
\newblock \emph{Mathematics of Operations Research}, Jun. 2018.

\bibitem[Graves and Lai(1997)]{graves1997}
Todd~L. Graves and Tze~Leung Lai.
\newblock Asymptotically efficient adaptive choice of control laws in
  controlled {Markov} chains.
\newblock \emph{SIAM J. Control and Optimization}, 35\penalty0 (3):\penalty0
  715--743, 1997.

\bibitem[Kaufmann et~al.(2016)Kaufmann, Capp{\'e}, and
  Garivier]{kaufmann2016complexity}
Emilie Kaufmann, Olivier Capp{\'e}, and Aurélien Garivier.
\newblock On the complexity of best-arm identification in multi-armed bandit
  models.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1--42, 2016.

\bibitem[Lakshmanan et~al.(2015)Lakshmanan, Ortner, and
  Ryabko]{lakshmanan2015improved}
Kailasam Lakshmanan, Ronald Ortner, and Daniil Ryabko.
\newblock Improved regret bounds for undiscounted continuous reinforcement
  learning.
\newblock In \emph{32nd International Conference on Machine Learning}, 2015.

\bibitem[Magureanu et~al.(2014)Magureanu, Combes, and Proutiere]{magureanu2014}
Stefan Magureanu, Richard Combes, and Alexandre Proutiere.
\newblock Lipschitz bandits: Regret lower bounds and optimal algorithms.
\newblock In \emph{Conference on Learning Theory}, 2014.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{mnih015}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A. Rusu, Joel Veness,
  Marc~G. Bellemare, Alex Graves, Martin Riedmiller, Andreas~K. Fidjeland,
  Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis
  Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and
  Demis Hassabis.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518:\penalty0 529--533, 2015.

\bibitem[Ortner and Ryabko(2012)]{ortner2012online}
Ronald Ortner and Daniil Ryabko.
\newblock Online regret bounds for undiscounted continuous reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems 25}, 2012.

\bibitem[Osband and Van~Roy(2014)]{eluder}
Ian Osband and Benjamin Van~Roy.
\newblock Model-based reinforcement learning and the {Eluder} dimension.
\newblock In \emph{Advances in Neural Information Processing Systems 27}, 2014.

\bibitem[Osband and Van~Roy(2016)]{osband2016lower}
Ian Osband and Benjamin Van~Roy.
\newblock On lower bounds for regret in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1608.02732}, 2016.

\bibitem[Puterman(1994)]{puterman1994markov}
M.~L. Puterman.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 1994.

\bibitem[Tewari and Bartlett(2008)]{bartlett08}
Ambuj Tewari and Peter~L. Bartlett.
\newblock Optimistic linear programming gives logarithmic regret for
  irreducible {MDP}s.
\newblock In \emph{Advances in Neural Information Processing Systems 20}, 2008.

\end{thebibliography}
