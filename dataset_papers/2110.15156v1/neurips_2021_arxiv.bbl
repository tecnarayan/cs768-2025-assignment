\begin{thebibliography}{10}

\bibitem{dosovitskiy2021an}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em ICLR}, 2021.

\bibitem{beal2020toward}
Josh Beal, Eric Kim, Eric Tzeng, Dong~Huk Park, Andrew Zhai, and Dmitry
  Kislyuk.
\newblock Toward transformer-based object detection.
\newblock {\em arXiv preprint arXiv:2012.09958}, 2020.

\bibitem{SETR}
Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang,
  Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip~H.S. Torr, and Li~Zhang.
\newblock Rethinking semantic segmentation from a sequence-to-sequence
  perspective with transformers.
\newblock In {\em CVPR}, 2021.

\bibitem{arnab2021vivit}
Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu{\v{c}}i{\'c},
  and Cordelia Schmid.
\newblock Vivit: A video vision transformer.
\newblock {\em arXiv preprint arXiv:2103.15691}, 2021.

\bibitem{nyquist1928certain}
Harry Nyquist.
\newblock Certain topics in telegraph transmission theory.
\newblock {\em Transactions of the American Institute of Electrical Engineers},
  1928.

\bibitem{gonzalez2002digital}
Rafael~C Gonzalez, Richard~E Woods, et~al.
\newblock Digital image processing, 2002.

\bibitem{yuan2021tokens}
Li~Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis~EH
  Tay, Jiashi Feng, and Shuicheng Yan.
\newblock Tokens-to-token vit: Training vision transformers from scratch on
  imagenet.
\newblock {\em arXiv preprint arXiv:2101.11986}, 2021.

\bibitem{caron2021emerging}
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv{\'e} J{\'e}gou, Julien Mairal,
  Piotr Bojanowski, and Armand Joulin.
\newblock Emerging properties in self-supervised vision transformers.
\newblock {\em arXiv preprint arXiv:2104.14294}, 2021.

\bibitem{touvron2020training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock {\em arXiv preprint arXiv:2012.12877}, 2020.

\bibitem{zhang2019making}
Richard Zhang.
\newblock Making convolutional networks shift-invariant again.
\newblock In {\em ICML}, 2019.

\bibitem{dong2011sparsity}
Weisheng Dong, Xin Li, Lei Zhang, and Guangming Shi.
\newblock Sparsity-based image denoising via dictionary learning and structural
  clustering.
\newblock In {\em CVPR}, 2011.

\bibitem{zou2020delving}
Xueyan Zou, Fanyi Xiao, Zhiding Yu, and Yong~Jae Lee.
\newblock Delving deeper into anti-aliasing in convnets.
\newblock In {\em BMVC}, 2020.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em NIPS}, 2017.

\bibitem{cubuk2018autoaugment}
Ekin~D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V Le.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock {\em arXiv preprint arXiv:1805.09501}, 2018.

\bibitem{cubuk2020randaugment}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In {\em CVPR Workshops}, 2020.

\bibitem{carion2020end}
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
  Kirillov, and Sergey Zagoruyko.
\newblock End-to-end object detection with transformers.
\newblock In {\em ECCV}, 2020.

\bibitem{zhu2020deformable}
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
\newblock Deformable detr: Deformable transformers for end-to-end object
  detection.
\newblock In {\em ICLR}, 2021.

\bibitem{wang2020end}
Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen,
  and Huaxia Xia.
\newblock End-to-end video instance segmentation with transformers.
\newblock In {\em CVPR}, 2021.

\bibitem{zhu2021unified}
Fangrui Zhu, Yi~Zhu, Li~Zhang, Chongruo Wu, Yanwei Fu, and Mu~Li.
\newblock A unified efficient pyramid transformer for semantic segmentation.
\newblock In {\em ICCV}, 2021.

\bibitem{zhao2020point}
Hengshuang Zhao, Li~Jiang, Jiaya Jia, Philip Torr, and Vladlen Koltun.
\newblock Point transformer.
\newblock {\em arXiv preprint arXiv:2012.09164}, 2020.

\bibitem{ramesh2021zero}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock {\em arXiv preprint arXiv:2102.12092}, 2021.

\bibitem{esser2020taming}
Patrick Esser, Robin Rombach, and Bj{\"o}rn Ommer.
\newblock Taming transformers for high-resolution image synthesis.
\newblock In {\em CVPR}, 2021.

\bibitem{chen2020generative}
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and
  Ilya Sutskever.
\newblock Generative pretraining from pixels.
\newblock In {\em ICML}, 2020.

\bibitem{zhang2021vidtr}
Yanyi Zhang, Xinyu Li, Chunhui Liu, Bing Shuai, Yi~Zhu, Biagio Brattoli, Hao
  Chen, Ivan Marsic, and Joseph Tighe.
\newblock Vidtr: Video transformer without convolutions.
\newblock In {\em ICCV}, 2021.

\bibitem{chu2021conditional}
Xiangxiang Chu, Zhi Tian, Bo~Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and
  Chunhua Shen.
\newblock Conditional positional encodings for vision transformers.
\newblock {\em arXiv preprint arXiv:2102.10882}, 2021.

\bibitem{xu2021coscale}
Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu.
\newblock Co-scale conv-attentional image transformers, 2021.

\bibitem{chen2021crossvit}
Chun-Fu Chen, Quanfu Fan, and Rameswar Panda.
\newblock Crossvit: Cross-attention multi-scale vision transformer for image
  classification.
\newblock {\em arXiv preprint arXiv:2103.14899}, 2021.

\bibitem{zhang2021multi}
Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu~Yuan, Lei Zhang, and
  Jianfeng Gao.
\newblock Multi-scale vision longformer: A new vision transformer for
  high-resolution image encoding.
\newblock {\em arXiv preprint arXiv:2103.15358}, 2021.

\bibitem{yuan2021incorporating}
Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu.
\newblock Incorporating convolution designs into visual transformers.
\newblock {\em arXiv preprint arXiv:2103.11816}, 2021.

\bibitem{wu2021cvt}
Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu~Yuan, and Lei
  Zhang.
\newblock Cvt: Introducing convolutions to vision transformers.
\newblock {\em arXiv preprint arXiv:2103.15808}, 2021.

\bibitem{li2021localvit}
Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van~Gool.
\newblock Localvit: Bringing locality to vision transformers.
\newblock {\em arXiv preprint arXiv:2104.05707}, 2021.

\bibitem{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock {\em arXiv preprint arXiv:2103.14030}, 2021.

\bibitem{wang2021pyramid}
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong
  Lu, Ping Luo, and Ling Shao.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock {\em arXiv preprint arXiv:2102.12122}, 2021.

\bibitem{choromanski2021rethinking}
Krzysztof~Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared~Quincy Davis, Afroz
  Mohiuddin, Lukasz Kaiser, David~Benjamin Belanger, Lucy~J Colwell, and Adrian
  Weller.
\newblock Rethinking attention with performers.
\newblock In {\em ICLR}, 2021.

\bibitem{tolstikhin2021mlp}
Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai,
  Thomas Unterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario
  Lucic, et~al.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock {\em arXiv preprint arXiv:2105.01601}, 2021.

\bibitem{lecun1990handwritten}
Yann LeCun, Bernhard~E Boser, John~S Denker, Donnie Henderson, Richard~E
  Howard, Wayne~E Hubbard, and Lawrence~D Jackel.
\newblock Handwritten digit recognition with a back-propagation network.
\newblock In {\em NIPS}, 1990.

\bibitem{vasconcelos2020effective}
Cristina Vasconcelos, Hugo Larochelle, Vincent Dumoulin, Nicolas~Le Roux, and
  Ross Goroshin.
\newblock An effective anti-aliasing approach for residual networks.
\newblock {\em arXiv preprint arXiv:2011.10675}, 2020.

\bibitem{sinha2020curriculum}
Samarth Sinha, Animesh Garg, and Hugo Larochelle.
\newblock Curriculum by smoothing.
\newblock {\em NeurIPS}, 2020.

\bibitem{oppenheim2001discrete}
Alan~V Oppenheim, John~R Buck, and Ronald~W Schafer.
\newblock {\em Discrete-time signal processing. Vol. 2}.
\newblock Upper Saddle River, NJ: Prentice Hall, 2001.

\bibitem{jimenez2011filtering}
Jorge Jimenez, Diego Gutierrez, Jason Yang, Alexander Reshetov, Pete
  Demoreuille, Tobias Berghoff, Cedric Perthuis, Henry Yu, Morgan McGuire,
  Timothy Lottes, et~al.
\newblock Filtering approaches for real-time anti-aliasing.
\newblock {\em SIGGRAPH}, 2011.

\bibitem{lee2021fnet}
James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon.
\newblock Fnet: Mixing tokens with fourier transforms.
\newblock {\em arXiv preprint arXiv:2105.03824}, 2021.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{getreuer2018blade}
Pascal Getreuer, Ignacio Garcia-Dorado, John Isidoro, Sungjoon Choi, Frank Ong,
  and Peyman Milanfar.
\newblock Blade: Filter learning for general purpose computational photography.
\newblock In {\em ICCP}, 2018.

\bibitem{loper2015smpl}
Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael~J
  Black.
\newblock Smpl: A skinned multi-person linear model.
\newblock {\em TOG}, 2015.

\bibitem{li2019learning}
Yawei Li, Shuhang Gu, Luc~Van Gool, and Radu Timofte.
\newblock Learning filter basis for convolutional neural network compression.
\newblock In {\em CVPR}, 2019.

\bibitem{jaderberg2014speeding}
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman.
\newblock Speeding up convolutional neural networks with low rank expansions.
\newblock {\em arXiv preprint arXiv:1405.3866}, 2014.

\bibitem{korein1983temporal}
Jonathan Korein and Norman Badler.
\newblock Temporal anti-aliasing in computer generated animation.
\newblock In {\em SIGGRAPH}, 1983.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em ICML}, 2015.

\bibitem{NEURIPS2018_905056c1}
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry.
\newblock How does batch normalization help optimization?
\newblock In {\em NIPS}, 2018.

\bibitem{code2021swin}
Microsoft.
\newblock Swin transformer.
\newblock \url{https://github.com/microsoft/Swin-Transformer}, 2021.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em CVPR}, 2009.

\bibitem{bhojanapalli2021understanding}
Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas
  Unterthiner, and Andreas Veit.
\newblock Understanding robustness of transformers for image classification.
\newblock {\em arXiv preprint arXiv:2103.14586}, 2021.

\bibitem{shi2020robustness}
Zhouxing Shi, Huan Zhang, Kai-Wei Chang, Minlie Huang, and Cho-Jui Hsieh.
\newblock Robustness verification for transformers.
\newblock In {\em ICLR}, 2020.

\bibitem{code2021deit}
Facebook Research.
\newblock Deit.
\newblock \url{https://github.com/facebookresearch/deit}, 2021.

\bibitem{code2021coat}
Weijian Xu.
\newblock Coat.
\newblock \url{https://github.com/mlpc-ucsd/CoaT}, 2021.

\bibitem{code2021T2T}
YITU.
\newblock T2t vit.
\newblock \url{https://github.com/yitu-opensource/T2T-ViT}, 2021.

\bibitem{rw2019timm}
Ross Wightman.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em arXiv preprint arXiv:1912.01703}, 2019.

\bibitem{sun2017revisiting}
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta.
\newblock Revisiting unreasonable effectiveness of data in deep learning era.
\newblock In {\em ICCV}, 2017.

\bibitem{hendrycks2019robustness}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock {\em ICLR}, 2019.

\bibitem{shao2021adversarial}
Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, and Cho-Jui Hsieh.
\newblock On the adversarial robustness of visual transformers.
\newblock {\em arXiv preprint arXiv:2103.15670}, 2021.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, 2016.

\bibitem{yan2015hd}
Zhicheng Yan, Hao Zhang, Robinson Piramuthu, Vignesh Jagadeesh, Dennis DeCoste,
  Wei Di, and Yizhou Yu.
\newblock Hd-cnn: hierarchical deep convolutional neural networks for large
  scale visual recognition.
\newblock In {\em ICCV}, 2015.

\bibitem{tabernik2018spatially}
Domen Tabernik, Matej Kristan, and Ale{\v{s}} Leonardis.
\newblock Spatially-adaptive filter units for deep neural networks.
\newblock In {\em CVPR}, 2018.

\bibitem{kyprianidis2008image}
Jan~Eric Kyprianidis and J{\"u}rgen D{\"o}llner.
\newblock Image abstraction by structure adaptive filtering.
\newblock In {\em TPCG}, 2008.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em ECCV}, 2014.

\bibitem{he2017mask}
Kaiming He, Georgia Gkioxari, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Mask r-cnn.
\newblock In {\em ICCV}, 2017.

\bibitem{cai2018cascade}
Zhaowei Cai and Nuno Vasconcelos.
\newblock Cascade r-cnn: Delving into high quality object detection.
\newblock In {\em CVPR}, 2018.

\bibitem{chen2019mmdetection}
Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu~Xiong, Xiaoxiao Li,
  Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, et~al.
\newblock Mmdetection: Open mmlab detection toolbox and benchmark.
\newblock {\em arXiv preprint arXiv:1906.07155}, 2019.

\bibitem{zhou2019semantic}
Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso,
  and Antonio Torralba.
\newblock Semantic understanding of scenes through the ade20k dataset.
\newblock {\em IJCV}, 2019.

\bibitem{xiao2018unified}
Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun.
\newblock Unified perceptual parsing for scene understanding.
\newblock In {\em ECCV}, 2018.

\bibitem{mmseg2020}
MMSegmentation Contributors.
\newblock {MMSegmentation}: Openmmlab semantic segmentation toolbox and
  benchmark.
\newblock \url{https://github.com/open-mmlab/mmsegmentation}, 2020.

\end{thebibliography}
