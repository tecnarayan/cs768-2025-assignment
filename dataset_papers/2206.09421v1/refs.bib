@inproceedings{bartlett2007sample,
  title={Sample complexity of policy search with known dynamics},
  author={Bartlett, Peter L and Tewari, Ambuj},
  booktitle={Advances in Neural Information Processing Systems},
  pages={97--104},
  year={2007}
}

@article{devidze2021explicable,
  title={Explicable Reward Design for Reinforcement Learning Agents},
  author={Devidze, Rati and Radanovic, Goran and Kamalaruban, Parameswaran and Singla, Adish},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{marthi2007automatic,
  title={Automatic shaping and decomposition of reward functions},
  author={Marthi, Bhaskara},
  booktitle={Proceedings of the 24th International Conference on Machine learning},
  pages={601--608},
  year={2007}
}

@article{abel2021expressivity,
  title={On the Expressivity of Markov Reward},
  author={Abel, David and Dabney, Will and Harutyunyan, Anna and Ho, Mark K and Littman, Michael and Precup, Doina and Singh, Satinder},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{grzes2017reward,
  title={Reward shaping in episodic reinforcement learning},
  author={Grzes, Marek},
  year={2017},
  publisher={ACM}
}


@article{hu2020learning,
  title={Learning to utilize shaping rewards: A new approach of reward shaping},
  author={Hu, Yujing and Wang, Weixun and Jia, Hangtian and Wang, Yixiang and Chen, Yingfeng and Hao, Jianye and Wu, Feng and Fan, Changjie},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15931--15941},
  year={2020}
}

@article{icarte2022reward,
  title={Reward machines: Exploiting reward function structure in reinforcement learning},
  author={Icarte, Rodrigo Toro and Klassen, Toryn Q and Valenzano, Richard and McIlraith, Sheila A},
  journal={Journal of Artificial Intelligence Research},
  volume={73},
  pages={173--208},
  year={2022}
}

@incollection{mataric1994reward,
  title={Reward functions for accelerated learning},
  author={Mataric, Maja J},
  booktitle={Machine learning proceedings 1994},
  pages={181--189},
  year={1994},
  publisher={Elsevier}
}
@article{liu2018simple,
  title={When simple exploration is sample efficient: Identifying sufficient conditions for random exploration to yield pac rl algorithms},
  author={Liu, Yao and Brunskill, Emma},
  journal={arXiv preprint arXiv:1805.09045},
  year={2018}
}

@article{rakhlin2015online,
  title={Online nonparametric regression with general loss functions},
  author={Rakhlin, Alexander and Sridharan, Karthik},
  journal={arXiv preprint arXiv:1501.06598},
  year={2015}
}

@article{uehara2021pessimistic,
  title={Pessimistic Model-based Offline Reinforcement Learning under Partial Coverage},
  author={Uehara, Masatoshi and Sun, Wen},
  journal={arXiv preprint arXiv:2107.06226},
  year={2021}
}

@article{zhang2021feel,
  title={Feel-Good Thompson Sampling for Contextual Bandits and Reinforcement Learning},
  author={Zhang, Tong},
  journal={arXiv preprint arXiv:2110.00871},
  year={2021}
}

@article{brafman2002r,
  title={R-max-a general polynomial time algorithm for near-optimal reinforcement learning},
  author={Brafman, Ronen I and Tennenholtz, Moshe},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Oct},
  pages={213--231},
  year={2002}
}

@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}

@article{dann2018oracle,
  title={On oracle-efficient PAC RL with rich observations},
  author={Dann, Christoph and Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
  journal={arXiv preprint arXiv:1803.00606},
  year={2018}
}

@inproceedings{azizzadenesheli2016open,
  title={Open problem: Approximate planning of pomdps in the class of memoryless policies},
  author={Azizzadenesheli, Kamyar and Lazaric, Alessandro and Anandkumar, Animashree},
  booktitle={Conference on Learning Theory},
  pages={1639--1642},
  year={2016},
  organization={PMLR}
}


@inproceedings{dann2015sample,
  title={Sample complexity of episodic fixed-horizon reinforcement learning},
  author={Dann, Christoph and Brunskill, Emma},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2818--2826},
  year={2015}
}

@article{allman2009identifiability,
  title={Identifiability of parameters in latent structure models with many observed variables},
  author={Allman, Elizabeth S and Matias, Catherine and Rhodes, John A and others},
  journal={The Annals of Statistics},
  volume={37},
  number={6A},
  pages={3099--3132},
  year={2009},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{peshkin2001bounds,
  title={Bounds on sample size for policy evaluation in Markov environments},
  author={Peshkin, Leonid and Mukherjee, Sayan},
  booktitle={International Conference on Computational Learning Theory},
  pages={616--629},
  year={2001},
  organization={Springer}
}

@article{jaksch2010near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Apr},
  pages={1563--1600},
  year={2010}
}

@inproceedings{rosenberg2019online,
  title={Online Stochastic Shortest Path with Bandit Feedback and Unknown Transition Function},
  author={Rosenberg, Aviv and Mansour, Yishay},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2209--2218},
  year={2019}
}

@article{du2019good,
  title={Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?},
  author={Du, Simon S and Kakade, Sham M and Wang, Ruosong and Yang, Lin F},
  journal={arXiv preprint arXiv:1910.03016},
  year={2019}
}

@inproceedings{simchowitz2020naive,
  title={Naive exploration is optimal for online lqr},
  author={Simchowitz, Max and Foster, Dylan},
  booktitle={International Conference on Machine Learning},
  pages={8937--8948},
  year={2020},
  organization={PMLR}
}

@inproceedings{kong2020sublinear,
  title={Sublinear optimal policy value estimation in contextual bandits},
  author={Kong, Weihao and Brunskill, Emma and Valiant, Gregory},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4377--4387},
  year={2020},
  organization={PMLR}
}

@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={263--272},
  year={2017},
  organization={PMLR}
}

@article{hazan2018provably,
  title={Provably efficient maximum entropy exploration},
  author={Hazan, Elad and Kakade, Sham M and Singh, Karan and Van Soest, Abby},
  journal={arXiv preprint arXiv:1812.02690},
  year={2018}
}

@article{jin2019learning,
  title={Learning Adversarial Markov Decision Processes with Bandit Feedback and Unknown Transition},
  author={Jin, Tiancheng and Luo, Haipeng},
  journal={arXiv preprint arXiv:1912.01192},
  year={2019}
}

@article{jin2020reward,
  title={Reward-Free Exploration for Reinforcement Learning},
  author={Jin, Chi and Krishnamurthy, Akshay and Simchowitz, Max and Yu, Tiancheng},
  journal={arXiv preprint arXiv:2002.02794},
  year={2020}
}


@inproceedings{jiang2017contextual,
  title={Contextual decision processes with low Bellman rank are PAC-learnable},
  author={Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1704--1713},
  year={2017},
  organization={JMLR. org}
}

@article{sun2018model,
  title={Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches},
  author={Sun, Wen and Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
  journal={arXiv preprint arXiv:1811.08540},
  year={2018}
}


@inproceedings{osband2014model,
  title={Model-based reinforcement learning and the {E}luder dimension},
  author={Osband, Ian and Van Roy, Benjamin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1466--1474},
  year={2014}
}

@article{schrittwieser2019mastering,
  title={Mastering atari, go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={arXiv preprint arXiv:1911.08265},
  year={2019}
}

@article{uehara2019minimax,
  title={Minimax Weight and Q-Function Learning for Off-Policy Evaluation},
  author={Uehara, Masatoshi and Jiang, Nan},
  journal={arXiv preprint arXiv:1910.12809},
  year={2019}
}

@article{rakhlin2015sequential,
  title={Sequential probability assignment with binary alphabets and large classes of experts},
  author={Rakhlin, Alexander and Sridharan, Karthik},
  journal={arXiv preprint arXiv:1501.07340},
  year={2015}
}


@article{acharya2016courseL3,
  title={An Algorithmic and Information-Theoretic Toolbox for Massive Data, Lecture 3},
  author={Jayadev Acharya},
  journal={Lecture Notes},
  year={2016}, 
  url=https://people.ece.cornell.edu/acharya/teaching/ece6980f16/scribing/01-sep-16-issa.pdf,
}

@article{dai2016learning,
  title={Learning from conditional distributions via dual embeddings},
  author={Dai, Bo and He, Niao and Pan, Yunpeng and Boots, Byron and Song, Le},
  journal={arXiv preprint arXiv:1607.04579},
  year={2016}
}

@article{kaelbling1996reinforcement,
  title={Reinforcement learning: A survey},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  journal={Journal of artificial intelligence research},
  volume={4},
  pages={237--285},
  year={1996}
}


@inproceedings{krishnamurthy2016pac,
  title={PAC reinforcement learning with rich observations},
  author={Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1840--1848},
  year={2016}
}

@article{kearns2002near,
  title={Near-optimal reinforcement learning in polynomial time},
  author={Kearns, Michael and Singh, Satinder},
  journal={Machine learning},
  volume={49},
  number={2-3},
  pages={209--232},
  year={2002},
  publisher={Springer}
}

@inproceedings{kearns2000approximate,
  title={Approximate planning in large POMDPs via reusable trajectories},
  author={Kearns, Michael J and Mansour, Yishay and Ng, Andrew Y},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1001--1007},
  year={2000}
}

@article{rosenberg2019online,
  title={Online Convex Optimization in Adversarial Markov Decision Processes},
  author={Rosenberg, Aviv and Mansour, Yishay},
  journal={arXiv preprint arXiv:1905.07773},
  year={2019}
}


@incollection{kalai2003efficient,
  title={Efficient algorithms for online decision problems},
  author={Kalai, Adam and Vempala, Santosh},
  booktitle={Learning Theory and Kernel Machines},
  pages={26--40},
  year={2003},
  publisher={Springer}
}

@article{arora2012multiplicative,
  title={The multiplicative weights update method: a meta-algorithm and applications},
  author={Arora, Sanjeev and Hazan, Elad and Kale, Satyen},
  journal={Theory of Computing},
  volume={8},
  number={1},
  pages={121--164},
  year={2012},
  publisher={Theory of Computing Exchange}
}

@article{arora2012online,
  title={Online bandit learning against an adaptive adversary: from regret to policy regret},
  author={Arora, Raman and Dekel, Ofer and Tewari, Ambuj},
  journal={arXiv preprint arXiv:1206.6400},
  year={2012}
}

@article{wang2020long,
  title={Is Long Horizon Reinforcement Learning More Difficult Than Short Horizon Reinforcement Learning?},
  author={Wang, Ruosong and Du, Simon S and Yang, Lin F and Kakade, Sham M},
  journal={arXiv preprint arXiv:2005.00527},
  year={2020}
}

@article{segercrantz1992improving,
  title={Improving the Cayley-Hamilton equation for low-rank transformations},
  author={Segercrantz, J},
  journal={The American mathematical monthly},
  volume={99},
  number={1},
  pages={42--44},
  year={1992},
  publisher={Taylor \& Francis}
}

@article{safran2019depth,
  title={Depth Separations in Neural Networks: What is Actually Being Separated?},
  author={Safran, Itay and Eldan, Ronen and Shamir, Ohad},
  journal={arXiv preprint arXiv:1904.06984},
  year={2019}
}

@article{agarwal2020flambe,
  title={Flambe: Structural complexity and representation learning of low rank mdps},
  author={Agarwal, Alekh and Kakade, Sham and Krishnamurthy, Akshay and Sun, Wen},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{wang2020reinforcement,
  title={Reinforcement Learning with General Value Function Approximation: Provably Efficient Approach via Bounded {E}luder Dimension},
  author={Wang, Ruosong and Salakhutdinov, Russ R and Yang, Lin},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{foster2020instance,
  title={Instance-dependent complexity of contextual bandits and reinforcement learning: A disagreement-based perspective},
  author={Foster, Dylan J and Rakhlin, Alexander and Simchi-Levi, David and Xu, Yunzong},
  journal={arXiv preprint arXiv:2010.03104},
  year={2020}
}



@article{even2003learning,
  title={Learning Rates for {Q}-learning.},
  author={Even-Dar, Eyal and Mansour, Yishay},
  journal={Journal of machine learning Research},
  volume={5},
  number={1},
  year={2003}
}

@inproceedings{ng1999policy,
  title={Policy invariance under reward transformations: Theory and application to reward shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  booktitle={Icml},
  volume={99},
  pages={278--287},
  year={1999}
}

@article{dai2019maximum,
  title={Maximum expected hitting cost of a markov decision process and informativeness of rewards},
  author={Dai, Falcon Z and Walter, Matthew R},
  journal={arXiv preprint arXiv:1907.02114},
  year={2019}
}

@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={In Proc. 19th International Conference on Machine Learning},
  year={2002},
  organization={Citeseer}
}

@article{jin2021bellman,
  title={Bellman {E}luder dimension: New rich classes of RL problems, and sample-efficient algorithms},
  author={Jin, Chi and Liu, Qinghua and Miryoosefi, Sobhan},
  journal={arXiv preprint arXiv:2102.00815},
  year={2021}
}

@inproceedings{jiang2016structural,
  title={On Structural Properties of MDPs that Bound Loss Due to Shallow Planning.},
  author={Jiang, Nan and Singh, Satinder P and Tewari, Ambuj},
  booktitle={IJCAI},
  pages={1640--1647},
  year={2016}
}

@article{dann2021provably,
  title={A Provably Efficient Model-Free Posterior Sampling Method for Episodic Reinforcement Learning},
  author={Dann, Christoph and Mohri, Mehryar and Zhang, Tong and Zimmert, Julian},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{du2021bilinear,
  title={Bilinear classes: A structural framework for provable generalization in rl},
  author={Du, Simon S and Kakade, Sham M and Lee, Jason D and Lovett, Shachar and Mahajan, Gaurav and Sun, Wen and Wang, Ruosong},
  journal={arXiv preprint arXiv:2103.10897},
  year={2021}
}

@article{burnetas1996optimal,
  title={Optimal adaptive policies for sequential allocation problems},
  author={Burnetas, Apostolos N and Katehakis, Michael N},
  journal={Advances in Applied Mathematics},
  volume={17},
  number={2},
  pages={122--142},
  year={1996},
  publisher={Elsevier}
}

@article{garivier2019explore,
  title={Explore first, exploit next: The true shape of regret in bandit problems},
  author={Garivier, Aur{\'e}lien and M{\'e}nard, Pierre and Stoltz, Gilles},
  journal={Mathematics of Operations Research},
  volume={44},
  number={2},
  pages={377--399},
  year={2019},
  publisher={INFORMS}
}

@article{he2020logarithmic,
  title={Logarithmic regret for reinforcement learning with linear function approximation},
  author={He, Jiafan and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:2011.11566},
  year={2020}
}

@article{lykouris2019corruption,
  title={Corruption robust exploration in episodic reinforcement learning},
  author={Lykouris, Thodoris and Simchowitz, Max and Slivkins, Aleksandrs and Sun, Wen},
  journal={arXiv preprint arXiv:1911.08689},
  year={2019}
}

@article{du2020agnostic,
  title={Agnostic {Q}-learning with function approximation in deterministic systems: Tight bounds on approximation error and sample complexity},
  author={Du, Simon S and Lee, Jason D and Mahajan, Gaurav and Wang, Ruosong},
  journal={arXiv preprint arXiv:2002.07125},
  year={2020}
}

@article{jin2020simultaneously,
  title={Simultaneously Learning Stochastic and Adversarial Episodic {MDP}s with Known Transition},
  author={Jin, Tiancheng and Luo, Haipeng},
  journal={arXiv preprint arXiv:2006.05606},
  year={2020}
}


@article{basu2017strong,
  title={Strong duality and sensitivity analysis in semi-infinite linear programming},
  author={Basu, Amitabh and Martin, Kipp and Ryan, Christopher Thomas},
  journal={Mathematical Programming},
  volume={161},
  number={1-2},
  pages={451--485},
  year={2017},
  publisher={Springer}
}

@article{yang2020q,
  title={{$Q$}-learning with Logarithmic Regret},
  author={Yang, Kunhe and Yang, Lin F and Du, Simon S},
  journal={arXiv preprint arXiv:2006.09118},
  year={2020}
}

@article{jonsson2020planning,
  title={Planning in markov decision processes with gap-dependent sample complexity},
  author={Jonsson, Anders and Kaufmann, Emilie and M{\'e}nard, Pierre and Domingues, Omar Darwiche and Leurent, Edouard and Valko, Michal},
  journal={arXiv preprint arXiv:2006.05879},
  year={2020}
}

@article{basu2015projection,
  title={Projection: a unified approach to semi-infinite linear programs and duality in convex programming},
  author={Basu, Amitabh and Martin, Kipp and Ryan, Christopher Thomas},
  journal={Mathematics of Operations Research},
  volume={40},
  number={1},
  pages={146--170},
  year={2015},
  publisher={INFORMS}
}

@inproceedings{ok2018exploration,
  title={Exploration in structured reinforcement learning},
  author={Ok, Jungseul and Proutiere, Alexandre and Tranos, Damianos},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8874--8882},
  year={2018}
}

@inproceedings{tewari2008optimistic,
  title={Optimistic linear programming gives logarithmic regret for irreducible {MDP}s},
  author={Tewari, Ambuj and Bartlett, Peter L},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1505--1512},
  year={2008}
}

@inproceedings{combes2017minimal,
  title={Minimal exploration in structured stochastic bandits},
  author={Combes, Richard and Magureanu, Stefan and Proutiere, Alexandre},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1763--1771},
  year={2017}
}

@article{jin2019learning,
  title={Learning adversarial markov decision processes with bandit feedback and unknown transition},
  author={Jin, Chi and Jin, Tiancheng and Luo, Haipeng and Sra, Suvrit and Yu, Tiancheng},
  journal={arXiv preprint arXiv:1912.01192},
  year={2019}
}

@article{lai1985asymptotically,
  title={Asymptotically efficient adaptive allocation rules},
  author={Lai, Tze Leung and Robbins, Herbert},
  journal={Advances in applied mathematics},
  volume={6},
  number={1},
  pages={4--22},
  year={1985},
  publisher={Academic Press}
}

@article{graves1997asymptotically,
  title={Asymptotically efficient adaptive choice of control laws incontrolled markov chains},
  author={Graves, Todd L and Lai, Tze Leung},
  journal={SIAM journal on control and optimization},
  volume={35},
  number={3},
  pages={715--743},
  year={1997},
  publisher={SIAM}
}

@article{arora2012multiplicative,
  title={The multiplicative weights update method: a meta-algorithm and applications},
  author={Arora, Sanjeev and Hazan, Elad and Kale, Satyen},
  journal={Theory of Computing},
  volume={8},
  number={1},
  pages={121--164},
  year={2012},
  publisher={Theory of Computing Exchange}
}

@inproceedings{lattimore2017end,
  title={The end of optimism? an asymptotic analysis of finite-armed linear bandits},
  author={Lattimore, Tor and Szepesvari, Csaba},
  booktitle={Artificial Intelligence and Statistics},
  pages={728--737},
  year={2017},
  organization={PMLR}
}


@article{dann2021beyond,
  title={Beyond value-function gaps: Improved instance-dependent regret bounds for episodic reinforcement learning},
  author={Dann, Christoph and Marinov, Teodor Vanislavov and Mohri, Mehryar and Zimmert, Julian},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


@article{bartlett2012regal,
  title={REGAL: A regularization based algorithm for reinforcement learning in weakly communicating {MDP}s},
  author={Bartlett, Peter L and Tewari, Ambuj},
  journal={arXiv preprint arXiv:1205.2661},
  year={2012}
}


@article{simchowitz2019non,
  title={Non-asymptotic gap-dependent regret bounds for tabular MDPs},
  author={Simchowitz, Max and Jamieson, Kevin G},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={1153--1162},
  year={2019}
}

@article{wagenmaker2021beyond,
  title={Beyond No Regret: Instance-Dependent PAC Reinforcement Learning},
  author={Wagenmaker, Andrew and Simchowitz, Max and Jamieson, Kevin},
  journal={arXiv preprint arXiv:2108.02717},
  year={2021}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{agrawal2017optimistic,
  title={Optimistic posterior sampling for reinforcement learning: worst-case regret bounds},
  author={Agrawal, Shipra and Jia, Randy},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1184--1194},
  year={2017}
}

@inproceedings{auer2007logarithmic,
  title={Logarithmic online regret bounds for undiscounted reinforcement learning},
  author={Auer, Peter and Ortner, Ronald},
  booktitle={Advances in Neural Information Processing Systems},
  pages={49--56},
  year={2007}
}

@inproceedings{filippi2010optimism,
  title={Optimism in reinforcement learning and {K}ullback-{L}eibler divergence},
  author={Filippi, Sarah and Capp{\'e}, Olivier and Garivier, Aur{\'e}lien},
  booktitle={2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={115--122},
  year={2010},
  organization={IEEE}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@inproceedings{zimin2013online,
  title={Online learning in episodic Markovian decision processes by relative entropy policy search},
  author={Zimin, Alexander and Neu, Gergely},
  booktitle={Advances in neural information processing systems},
  pages={1583--1591},
  year={2013}
}

@inproceedings{jiang2018open,
  title={Open problem: The dependence of sample complexity lower bounds on planning horizon},
  author={Jiang, Nan and Agarwal, Alekh},
  booktitle={Conference On Learning Theory},
  pages={3395--3398},
  year={2018}
}
@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2137--2143},
  year={2020}
}

@article{lattimore2015pareto,
  title={The pareto regret frontier for bandits},
  author={Lattimore, Tor},
  journal={arXiv preprint arXiv:1511.00048},
  year={2015}
}

@inproceedings{dekel2014bandits,
  title={Bandits with switching costs: T 2/3 regret},
  author={Dekel, Ofer and Ding, Jian and Koren, Tomer and Peres, Yuval},
  booktitle={Proceedings of the forty-sixth annual ACM symposium on Theory of computing},
  pages={459--467},
  year={2014}
}

@article{Freedman1975,
    title={On tail probabilities for martingales},
    author={Freedman, David A},
    journal={the Annals of Probability},
    pages={100--118},
    year={1975},
    publisher={JSTOR}
}

@article{maurer2009empirical,
  title={Empirical Bernstein bounds and sample variance penalization},
  author={Maurer, Andreas and Pontil, Massimiliano},
  journal={arXiv preprint arXiv:0907.3740},
  year={2009}
}

@article{xu2021fine,
  title={Fine-Grained Gap-Dependent Bounds for Tabular MDPs via Adaptive Multi-Step Bootstrap},
  author={Xu, Haike and Ma, Tengyu and Du, Simon S},
  journal={arXiv preprint arXiv:2102.04692},
  year={2021}
}

@inproceedings{dann2019policy,
  title={Policy certificates: Towards accountable reinforcement learning},
  author={Dann, Christoph and Li, Lihong and Wei, Wei and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={1507--1516},
  year={2019},
  organization={PMLR}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@article{dabney2020temporally,
  title={Temporally-Extended epsilon-Greedy Exploration},
  author={Dabney, Will and Ostrovski, Georg and Barreto, Andr{\'e}},
  journal={arXiv preprint arXiv:2006.01782},
  year={2020}
}

@article{kalashnikov2018qt,
  title={Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation},
  author={Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and others},
  journal={arXiv preprint arXiv:1806.10293},
  year={2018}
}

@article{osband2019deep,
  title={Deep Exploration via Randomized Value Functions.},
  author={Osband, Ian and Van Roy, Benjamin and Russo, Daniel J and Wen, Zheng and others},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={124},
  pages={1--62},
  year={2019}
}

@inproceedings{mania2019certainty,
  title={Certainty equivalence is efficient for linear quadratic control},
  author={Mania, Horia and Tu, Stephen and Recht, Benjamin},
  booktitle={Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  pages={10154--10164},
  year={2019}
}

@inproceedings{misra2020kinematic,
  title={Kinematic state abstraction and provably efficient rich-observation reinforcement learning},
  author={Misra, Dipendra and Henaff, Mikael and Krishnamurthy, Akshay and Langford, John},
  booktitle={International conference on machine learning},
  pages={6961--6971},
  year={2020},
  organization={PMLR}
}

@inproceedings{zanette2020learning,
  title={Learning near optimal policies with low inherent bellman error},
  author={Zanette, Andrea and Lazaric, Alessandro and Kochenderfer, Mykel and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={10978--10989},
  year={2020},
  organization={PMLR}
}

@article{yang2020function,
  title={On function approximation in reinforcement learning: Optimism in the face of large state spaces},
  author={Yang, Zhuoran and Jin, Chi and Wang, Zhaoran and Wang, Mengdi and Jordan, Michael I},
  journal={arXiv preprint arXiv:2011.04622},
  year={2020}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{munos2008finite,
  title={Finite-Time Bounds for Fitted Value Iteration.},
  author={Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={5},
  year={2008}
}

@article{antos2008learning,
  title={Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path},
  author={Antos, Andr{\'a}s and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  journal={Machine Learning},
  volume={71},
  number={1},
  pages={89--129},
  year={2008},
  publisher={Springer}
}
@article{antos2007fitted,
  title={Fitted Q-iteration in continuous action-space MDPs},
  author={Antos, Andr{\'a}s and Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  year={2007}
}

@article{ernst2005tree,
  title={Tree-based batch mode reinforcement learning},
  author={Ernst, Damien and Geurts, Pierre and Wehenkel, Louis},
  journal={Journal of Machine Learning Research},
  volume={6},
  pages={503--556},
  year={2005},
  publisher={Microtome Publishing}
}

@inproceedings{laud2003influence,
  title={The influence of reward on the speed of reinforcement learning: An analysis of shaping},
  author={Laud, Adam and DeJong, Gerald},
  booktitle={Proceedings of the 20th International Conference on Machine Learning (ICML-03)},
  pages={440--447},
  year={2003}
}

@article{howard2021time,
  title={Time-uniform, nonparametric, nonasymptotic confidence sequences},
  author={Howard, Steven R and Ramdas, Aaditya and McAuliffe, Jon and Sekhon, Jasjeet},
  journal={The Annals of Statistics},
  volume={49},
  number={2},
  pages={1055--1080},
  year={2021},
  publisher={Institute of Mathematical Statistics}
}