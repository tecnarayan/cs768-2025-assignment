\begin{thebibliography}{10}

\bibitem{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  263--272, 2017.

\bibitem{bauerle2014more}
Nicole B{\"a}uerle and Ulrich Rieder.
\newblock More risk-sensitive {M}arkov decision processes.
\newblock {\em Mathematics of Operations Research}, 39(1):105--120, 2014.

\bibitem{bellemare2017distributional}
Marc~G. Bellemare, Will Dabney, and R{\'e}mi Munos.
\newblock A distributional perspective on reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  449--458. PMLR, 2017.

\bibitem{borkar2001sensitivity}
Vivek~S. Borkar.
\newblock A sensitivity formula for risk-sensitive cost and the actor-critic
  algorithm.
\newblock {\em Systems \& Control Letters}, 44(5):339--346, 2001.

\bibitem{borkar2002q}
Vivek~S. Borkar.
\newblock Q-learning for risk-sensitive control.
\newblock {\em Mathematics of Operations Research}, 27(2):294--311, 2002.

\bibitem{borkar2010learning}
Vivek~S. Borkar.
\newblock Learning algorithms for risk-sensitive control.
\newblock In {\em Proceedings of the 19th International Symposium on
  Mathematical Theory of Networks and Systems--MTNS}, pages 55--60, 2010.

\bibitem{borkar2002risk}
Vivek~S. Borkar and Sean~P. Meyn.
\newblock Risk-sensitive optimal control for {M}arkov decision processes with
  monotone cost.
\newblock {\em Mathematics of Operations Research}, 27(1):192--209, 2002.

\bibitem{cavazos2011discounted}
Rolando Cavazos-Cadena and Daniel Hern{\'a}ndez-Hern{\'a}ndez.
\newblock Discounted approximations for risk-sensitive average criteria in
  {M}arkov decision chains with finite state space.
\newblock {\em Mathematics of Operations Research}, 36(1):133--146, 2011.

\bibitem{chen2020multiple}
Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi.
\newblock Multiple descent: Design your own generalization curve.
\newblock In {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{chen2021infinite}
Lin Chen, Bruno Scherrer, and Peter~L Bartlett.
\newblock Infinite-horizon offline reinforcement learning with linear function
  approximation: Curse of dimensionality and algorithm.
\newblock {\em arXiv preprint arXiv:2103.09847}, 2021.

\bibitem{chen2020deep}
Lin Chen and Sheng Xu.
\newblock Deep neural tangent kernel and laplace kernel have the same rkhs.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{chen2019minimax}
Lin Chen, Qian Yu, Hannah Lawrence, and Amin Karbasi.
\newblock Minimax regret of switching-constrained online convex optimization:
  No phase transition.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{coraluppi1999chapter}
Stefano~P. Coraluppi and Steven~I. Marcus.
\newblock Risk-sensitive, minimax, and mixed risk-neutral/minimax control of
  {M}arkov decision processes.
\newblock In {\em Stochastic Analysis, Control, Optimization and Applications},
  pages 21--40. Springer, 1999.

\bibitem{dabney2018implicit}
Will Dabney, Georg Ostrovski, David Silver, and R{\'e}mi Munos.
\newblock Implicit quantile networks for distributional reinforcement learning.
\newblock In {\em International conference on machine learning}, pages
  1096--1105. PMLR, 2018.

\bibitem{dabney2018distributional}
Will Dabney, Mark Rowland, Marc Bellemare, and R{\'e}mi Munos.
\newblock Distributional reinforcement learning with quantile regression.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem{di1999risk}
Giovanni~B. Di~Masi and Lukasz Stettner.
\newblock Risk-sensitive control of discrete-time {M}arkov processes with
  infinite horizon.
\newblock {\em SIAM Journal on Control and Optimization}, 38(1):61--78, 1999.

\bibitem{di2000infinite}
Giovanni~B. Di~Masi and Lukasz Stettner.
\newblock Infinite horizon risk sensitive control of discrete time {M}arkov
  processes with small risk.
\newblock {\em Systems \& Control Letters}, 40(1):15--20, 2000.

\bibitem{di2007infinite}
Giovanni~B. Di~Masi and {\L}ukasz Stettner.
\newblock Infinite horizon risk sensitive control of discrete time {M}arkov
  processes under minorization property.
\newblock {\em SIAM Journal on Control and Optimization}, 46(1):231--252, 2007.

\bibitem{farahmand2019value}
Amir-massoud Farahmand.
\newblock Value function in frequency domain and the characteristic value
  iteration algorithm.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{fei2020risk}
Yingjie Fei, Zhuoran Yang, Yudong Chen, Zhaoran Wang, and Qiaomin Xie.
\newblock Risk-sensitive reinforcement learning: Near-optimal risk-sample
  tradeoff in regret.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{fei2021risk}
Yingjie Fei, Zhuoran Yang, and Zhaoran Wang.
\newblock Risk-sensitive reinforcement learning with function approximation: A
  debiasing approach.
\newblock In {\em International Conference on Machine Learning}, pages
  3198--3207. PMLR, 2021.

\bibitem{fleming1995risk}
Wendell~H Fleming and William~M McEneaney.
\newblock Risk-sensitive control on an infinite time horizon.
\newblock {\em SIAM Journal on Control and Optimization}, 33(6):1881--1915,
  1995.

\bibitem{hernandez1996risk}
Daniel Hern{\'a}ndez-Hern{\'a}ndez and Steven~I. Marcus.
\newblock Risk sensitive control of {M}arkov processes in countable state
  space.
\newblock {\em Systems \& Control Letters}, 29(3):147--155, 1996.

\bibitem{howard1972risk}
Ronald~A. Howard and James~E. Matheson.
\newblock Risk-sensitive {M}arkov decision processes.
\newblock {\em Management Science}, 18(7):356--369, 1972.

\bibitem{huang2020stochastic}
Wenjie Huang and William~B Haskell.
\newblock Stochastic approximation for risk-aware markov decision processes.
\newblock {\em IEEE Transactions on Automatic Control}, 66(3):1314--1320, 2020.

\bibitem{jaskiewicz2007average}
Anna Ja{\'s}kiewicz.
\newblock Average optimality for risk-sensitive control with general state
  space.
\newblock {\em The Annals of Applied Probability}, 17(2):654--675, 2007.

\bibitem{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I. Jordan.
\newblock Is {Q}-learning provably efficient?
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4863--4873, 2018.

\bibitem{jin2019provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I. Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock {\em arXiv preprint arXiv:1907.05388}, 2019.

\bibitem{leibo2018psychlab}
Joel~Z Leibo, Cyprien de~Masson d'Autume, Daniel Zoran, David Amos, Charles
  Beattie, Keith Anderson, Antonio~Garc{\'\i}a Casta{\~n}eda, Manuel Sanchez,
  Simon Green, Audrunas Gruslys, et~al.
\newblock Psychlab: a psychology laboratory for deep reinforcement learning
  agents.
\newblock {\em arXiv preprint arXiv:1801.08116}, 2018.

\bibitem{ling2019landscape}
Shuyang Ling, Ruitu Xu, and Afonso~S Bandeira.
\newblock On the landscape of synchronization networks: A perspective from
  nonconvex optimization.
\newblock {\em SIAM Journal on Optimization}, 29(3):1879--1907, 2019.

\bibitem{marcus1997risk}
Steven~I. Marcus, Emmanual Fern{\'a}ndez-Gaucherand, Daniel
  Hern{\'a}ndez-Hernandez, Stefano Coraluppi, and Pedram Fard.
\newblock Risk sensitive {M}arkov decision processes.
\newblock In {\em Systems and Control in the Twenty-first Century}, pages
  263--279. Springer, 1997.

\bibitem{mavrin2019distributional}
Borislav Mavrin, Hengshuai Yao, Linglong Kong, Kaiwen Wu, and Yaoliang Yu.
\newblock Distributional reinforcement learning for efficient exploration.
\newblock In {\em International Conference on Machine Learning}, pages
  4424--4434. PMLR, 2019.

\bibitem{mihatsch2002risk}
Oliver Mihatsch and Ralph Neuneier.
\newblock Risk-sensitive reinforcement learning.
\newblock {\em Machine Learning}, 49(2-3):267--290, 2002.

\bibitem{morimura2010nonparametric}
Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and
  Toshiyuki Tanaka.
\newblock Nonparametric return distribution approximation for reinforcement
  learning.
\newblock In {\em International Conference on Machine Learning}, 2010.

\bibitem{morimura2012parametric}
Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and
  Toshiyuki Tanaka.
\newblock Parametric return density estimation for reinforcement learning.
\newblock {\em arXiv preprint arXiv:1203.3497}, 2012.

\bibitem{niv2012neural}
Yael Niv, Jeffrey~A. Edlund, Peter Dayan, and John~P. O'Doherty.
\newblock Neural prediction errors reveal a risk-sensitive
  reinforcement-learning process in the human brain.
\newblock {\em Journal of Neuroscience}, 32(2):551--562, 2012.

\bibitem{osogami2012robustness}
Takayuki Osogami.
\newblock Robustness and risk-sensitivity in {M}arkov decision processes.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  233--241, 2012.

\bibitem{patek2001terminating}
Stephen~D. Patek.
\newblock On terminating {M}arkov decision processes with a risk-averse
  objective function.
\newblock {\em Automatica}, 37(9):1379--1386, 2001.

\bibitem{rowland2018analysis}
Mark Rowland, Marc Bellemare, Will Dabney, R{\'e}mi Munos, and Yee~Whye Teh.
\newblock An analysis of categorical distributional reinforcement learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 29--37. PMLR, 2018.

\bibitem{shen2013risk}
Yun Shen, Wilhelm Stannat, and Klaus Obermayer.
\newblock Risk-sensitive {M}arkov control processes.
\newblock {\em SIAM Journal on Control and Optimization}, 51(5):3652--3672,
  2013.

\bibitem{shen2014risk}
Yun Shen, Michael~J. Tobia, Tobias Sommer, and Klaus Obermayer.
\newblock Risk-sensitive reinforcement learning.
\newblock {\em Neural Computation}, 26(7):1298--1328, 2014.

\bibitem{song2021convergence}
Ganlin Song, Ruitu Xu, and John Lafferty.
\newblock Convergence and alignment of gradient descentwith random back
  propagation weights.
\newblock {\em arXiv preprint arXiv:2106.06044}, 2021.

\bibitem{whittle1990risk}
Peter Whittle.
\newblock {\em Risk-sensitive {O}ptimal {C}ontrol}, volume~20.
\newblock Wiley New York, 1990.

\bibitem{xu2021meta}
Ruitu Xu, Lin Chen, and Amin Karbasi.
\newblock Meta learning in the continuous time limit.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 3052--3060. PMLR, 2021.

\bibitem{yang2019fully}
Derek Yang, Li~Zhao, Zichuan Lin, Tao Qin, Jiang Bian, and Tie-Yan Liu.
\newblock Fully parameterized quantile function for distributional
  reinforcement learning.
\newblock {\em Advances in neural information processing systems},
  32:6193--6202, 2019.

\end{thebibliography}
