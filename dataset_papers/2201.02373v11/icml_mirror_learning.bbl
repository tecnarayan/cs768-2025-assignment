\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjona-Medina et~al.(2018)Arjona-Medina, Gillhofer, Widrich,
  Unterthiner, Brandstetter, and Hochreiter]{arjona2018rudder}
Arjona-Medina, J.~A., Gillhofer, M., Widrich, M., Unterthiner, T.,
  Brandstetter, J., and Hochreiter, S.
\newblock Rudder: Return decomposition for delayed rewards.
\newblock \emph{arXiv preprint arXiv:1806.07857}, 2018.

\bibitem[Ausubel \& Deneckere(1993)Ausubel and
  Deneckere]{ausubel1993generalized}
Ausubel, L.~M. and Deneckere, R.~J.
\newblock A generalized theorem of the maximum.
\newblock \emph{Economic Theory}, 3\penalty0 (1):\penalty0 99--107, 1993.

\bibitem[Baxter \& Bartlett(2001)Baxter and Bartlett]{baxter2001infinite}
Baxter, J. and Bartlett, P.~L.
\newblock Infinite-horizon policy-gradient estimation.
\newblock \emph{Journal of Artificial Intelligence Research}, 15:\penalty0
  319--350, 2001.

\bibitem[Beck \& Teboulle(2003)Beck and Teboulle]{beck2003mirror}
Beck, A. and Teboulle, M.
\newblock Mirror descent and nonlinear projected subgradient methods for convex
  optimization.
\newblock \emph{Operations Research Letters}, 31\penalty0 (3):\penalty0
  167--175, 2003.

\bibitem[Bellman(1957)]{bellman1957markov}
Bellman, R.
\newblock A markov decision process. journal of mathematical mechanics.
\newblock 1957.

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, Debiak, Dennison,
  Farhi, Fischer, Hashme, Hesse, et~al.]{berner2019dota}
Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C.,
  Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.06680}, 2019.

\bibitem[Chu et~al.(2019)Chu, Blanchet, and Glynn]{chu2019probability}
Chu, C., Blanchet, J., and Glynn, P.
\newblock Probability functional descent: A unifying perspective on gans,
  variational inference, and reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1213--1222. PMLR, 2019.

\bibitem[Duan et~al.(2016)Duan, Chen, Houthooft, Schulman, and
  Abbeel]{duan2016benchmarking}
Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In \emph{International conference on machine learning}, pp.\
  1329--1338. PMLR, 2016.

\bibitem[Engstrom et~al.(2020)Engstrom, Ilyas, Santurkar, Tsipras, Janoos,
  Rudolph, and Madry]{engstrom2020implementation}
Engstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Janoos, F., Rudolph, L.,
  and Madry, A.
\newblock Implementation matters in deep policy gradients: A case study on ppo
  and trpo.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Fukushima \& Miyake(1982)Fukushima and
  Miyake]{fukushima1982neocognitron}
Fukushima, K. and Miyake, S.
\newblock Neocognitron: A self-organizing neural network model for a mechanism
  of visual pattern recognition.
\newblock In \emph{Competition and cooperation in neural nets}, pp.\  267--285.
  Springer, 1982.

\bibitem[Gateaux(1922)]{gateaux1922diverses}
Gateaux, R.
\newblock Sur diverses questions de calcul fonctionnel.
\newblock \emph{Bulletin de la Soci{\'e}t{\'e} Math{\'e}matique de France},
  50:\penalty0 1--37, 1922.

\bibitem[Hamilton \& Nashed(1982)Hamilton and Nashed]{hamilton1982global}
Hamilton, E. and Nashed, M.
\newblock Global and local variational derivatives and integral representations
  of g{\^a}teaux differentials.
\newblock \emph{Journal of Functional Analysis}, 49\penalty0 (1):\penalty0
  128--144, 1982.

\bibitem[Henderson et~al.(2018)Henderson, Islam, Bachman, Pineau, Precup, and
  Meger]{henderson2018deep}
Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D.
\newblock Deep reinforcement learning that matters.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~32, 2018.

\bibitem[Hsu et~al.(2020)Hsu, Mendler-D{\"u}nner, and Hardt]{hsu2020revisiting}
Hsu, C. C.-Y., Mendler-D{\"u}nner, C., and Hardt, M.
\newblock Revisiting design choices in proximal policy optimization.
\newblock \emph{arXiv preprint arXiv:2009.10897}, 2020.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{kakade2002approximately}
Kakade, S. and Langford, J.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{In Proc. 19th International Conference on Machine Learning}.
  Citeseer, 2002.

\bibitem[Kuba et~al.(2021)Kuba, Chen, Wen, Wen, Sun, Wang, and
  Yang]{kuba2021trust}
Kuba, J.~G., Chen, R., Wen, M., Wen, Y., Sun, F., Wang, J., and Yang, Y.
\newblock Trust region policy optimisation in multi-agent reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2109.11251}, 2021.

\bibitem[Lan(2021)]{lan2021policy}
Lan, G.
\newblock Policy mirror descent for reinforcement learning: Linear convergence,
  new sampling complexity, and generalized problem classes.
\newblock \emph{arXiv preprint arXiv:2102.00135}, 2021.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Liu et~al.(2019)Liu, Cai, Yang, and Wang]{liu2019neural}
Liu, B., Cai, Q., Yang, Z., and Wang, Z.
\newblock Neural proximal/trust region policy optimization attains globally
  optimal policy.
\newblock \emph{arXiv preprint arXiv:1906.10306}, 2019.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\
  1928--1937. PMLR, 2016.

\bibitem[Nemirovskij \& Yudin(1983)Nemirovskij and
  Yudin]{nemirovskij1983problem}
Nemirovskij, A.~S. and Yudin, D.~B.
\newblock Problem complexity and method efficiency in optimization.
\newblock 1983.

\bibitem[Neu et~al.(2017)Neu, Jonsson, and G{\'{o}}mez]{neu2017unified}
Neu, G., Jonsson, A., and G{\'{o}}mez, V.
\newblock A unified view of entropy-regularized markov decision processes.
\newblock \emph{CoRR}, abs/1705.07798, 2017.
\newblock URL \url{http://arxiv.org/abs/1705.07798}.

\bibitem[Queeney et~al.(2021)Queeney, Paschalidis, and
  Cassandras]{queeney2021generalized}
Queeney, J., Paschalidis, I., and Cassandras, C.
\newblock Generalized proximal policy optimization with sample reuse.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{trpo}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1889--1897. PMLR, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{ppo}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{ArXiv}, abs/1707.06347, 2017.

\bibitem[Shani et~al.(2020)Shani, Efroni, and Mannor]{shani2020adaptive}
Shani, L., Efroni, Y., and Mannor, S.
\newblock Adaptive trust region policy optimization: Global convergence and
  faster rates for regularized mdps.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pp.\  5668--5675, 2020.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{silver2014deterministic}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{International conference on machine learning}, pp.\
  387--395. PMLR, 2014.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock 2018.

\bibitem[Sutton et~al.(2000)Sutton, Mcallester, Singh, and
  Mansour]{sutton:nips12}
Sutton, R.~S., Mcallester, D., Singh, S., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems 12},
  volume~12, pp.\  1057--1063. MIT Press, 2000.

\bibitem[Tomar et~al.(2020)Tomar, Shani, Efroni, and
  Ghavamzadeh]{tomar2020mirror}
Tomar, M., Shani, L., Efroni, Y., and Ghavamzadeh, M.
\newblock Mirror descent policy optimization.
\newblock \emph{arXiv preprint arXiv:2005.09814}, 2020.

\bibitem[Van~Hasselt et~al.(2016)Van~Hasselt, Guez, and Silver]{van2016deep}
Van~Hasselt, H., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~30, 2016.

\bibitem[Vaswani et~al.(2021)Vaswani, Bachem, Totaro, Mueller, Geist, Machado,
  Castro, and Roux]{vaswani2021functional}
Vaswani, S., Bachem, O., Totaro, S., Mueller, R., Geist, M., Machado, M.~C.,
  Castro, P.~S., and Roux, N.~L.
\newblock A functional mirror ascent view of policy gradient methods with
  function approximation.
\newblock \emph{arXiv preprint arXiv:2108.05828}, 2021.

\bibitem[Wang et~al.(2020)Wang, He, and Tan]{wang2020truly}
Wang, Y., He, H., and Tan, X.
\newblock Truly proximal policy optimization.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  113--122.
  PMLR, 2020.

\bibitem[Watkins \& Dayan(1992)Watkins and Dayan]{Watkins1992}
Watkins, C. J. C.~H. and Dayan, P.
\newblock Q-learning.
\newblock \emph{Machine Learning}, 8\penalty0 (3):\penalty0 279--292, May 1992.
\newblock ISSN 1573-0565.
\newblock \doi{10.1007/BF00992698}.
\newblock URL \url{https://doi.org/10.1007/BF00992698}.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 229--256, 1992.

\bibitem[Zhan et~al.(2021)Zhan, Cen, Huang, Chen, Lee, and Chi]{zhan2021policy}
Zhan, W., Cen, S., Huang, B., Chen, Y., Lee, J.~D., and Chi, Y.
\newblock Policy mirror descent for regularized reinforcement learning: A
  generalized framework with linear convergence.
\newblock \emph{arXiv preprint arXiv:2105.11066}, 2021.

\bibitem[Zhao et~al.(2011)Zhao, Hachiya, Niu, and Sugiyama]{sugiyama}
Zhao, T., Hachiya, H., Niu, G., and Sugiyama, M.
\newblock Analysis and improvement of policy gradient estimation.
\newblock In \emph{NIPS}, pp.\  262--270. Citeseer, 2011.

\end{thebibliography}
