\begin{thebibliography}{10}

\bibitem{amodei2016deep}
Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan
  Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, et~al.
\newblock Deep speech 2: End-to-end speech recognition in english and mandarin.
\newblock In {\em International Conference on Machine Learning}, pages
  173--182, 2016.

\bibitem{amodei2016concrete}
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and
  Dan Man{\'e}.
\newblock Concrete problems in ai safety.
\newblock {\em arXiv preprint arXiv:1606.06565}, 2016.

\bibitem{andrews2016transfer}
Jerone~TA Andrews, Thomas Tanay, Edward~J Morton, and Lewis~D Griffin.
\newblock Transfer representation-learning for anomaly detection.
\newblock ICML, 2016.

\bibitem{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock {\em arXiv preprint arXiv:1409.0473}, 2014.

\bibitem{chelba2013one}
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi~Ge, Thorsten Brants, Phillipp
  Koehn, and Tony Robinson.
\newblock One billion word benchmark for measuring progress in statistical
  language modeling.
\newblock {\em arXiv preprint arXiv:1312.3005}, 2013.

\bibitem{cisse2017houdini}
Moustapha~M Cisse, Yossi Adi, Natalia Neverova, and Joseph Keshet.
\newblock Houdini: Fooling deep structured visual and speech recognition models
  with adversarial examples.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6980--6990, 2017.

\bibitem{imagenet_cvpr09}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In {\em CVPR09}, 2009.

\bibitem{frome2013devise}
Andrea Frome, Greg~S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas
  Mikolov, et~al.
\newblock Devise: A deep visual-semantic embedding model.
\newblock In {\em Advances in neural information processing systems}, pages
  2121--2129, 2013.

\bibitem{fuchs2017spoken}
Tzeviya Fuchs and Joseph Keshet.
\newblock Spoken term detection automatically adjusted for a given threshold.
\newblock {\em IEEE Journal of Selected Topics in Signal Processing},
  11(8):1310--1317, 2017.

\bibitem{gal2016dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In {\em international conference on machine learning}, pages
  1050--1059, 2016.

\bibitem{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock {\em arXiv preprint arXiv:1412.6572}, 2014.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hendrycks2016baseline}
Dan Hendrycks and Kevin Gimpel.
\newblock A baseline for detecting misclassified and out-of-distribution
  examples in neural networks.
\newblock {\em arXiv preprint arXiv:1610.02136}, 2016.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{huang2017densely}
Gao Huang, Zhuang Liu, Kilian~Q Weinberger, and Laurens van~der Maaten.
\newblock Densely connected convolutional networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, volume~1, page~3, 2017.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kreuk2018fooling}
Felix Kreuk, Yossi Adi, Moustapha Cisse, and Joseph Keshet.
\newblock Fooling end-to-end speaker verification by adversarial examples.
\newblock {\em arXiv preprint arXiv:1801.03339}, 2018.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{lakshminarayanan2017simple}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6405--6416, 2017.

\bibitem{lapidoth2017foundation}
Amos Lapidoth.
\newblock {\em A foundation in digital communication}.
\newblock Cambridge University Press, 2017.

\bibitem{leacock1998combining}
Claudia Leacock and Martin Chodorow.
\newblock Combining local context and wordnet similarity for word sense
  identification.
\newblock {\em WordNet: An electronic lexical database}, 49(2):265--283, 1998.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{lee2017training}
Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin.
\newblock Training confidence-calibrated classifiers for detecting
  out-of-distribution samples.
\newblock {\em arXiv preprint arXiv:1711.09325}, 2017.

\bibitem{lee2015m}
Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall, and Dhruv
  Batra.
\newblock Why m heads are better than one: Training a diverse ensemble of deep
  networks.
\newblock {\em arXiv preprint arXiv:1511.06314}, 2015.

\bibitem{liangenhancing}
Shiyu Liang, Yixuan Li, and R~Srikant.
\newblock Enhancing the reliability of out-of-distribution image detection in
  neural networks.

\bibitem{littwin2016multiverse}
Etai Littwin and Lior Wolf.
\newblock The multiverse loss for robust transfer learning.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3957--3966, 2016.

\bibitem{mackay1992bayesian}
David~JC MacKay.
\newblock {\em Bayesian methods for adaptive models}.
\newblock PhD thesis, California Institute of Technology, 1992.

\bibitem{mikolov2018advances}
Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand
  Joulin.
\newblock Advances in pre-training distributed word representations.
\newblock In {\em Proceedings of the International Conference on Language
  Resources and Evaluation (LREC 2018)}, 2018.

\bibitem{mikolov2013distributed}
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg~S Corrado, and Jeff Dean.
\newblock Distributed representations of words and phrases and their
  compositionality.
\newblock In {\em Advances in neural information processing systems}, pages
  3111--3119, 2013.

\bibitem{naaman2017learning}
Einat Naaman, Yossi Adi, and Joseph Keshet.
\newblock Learning similarity function for pronunciation variations.
\newblock In {\em Proc. of Interspeech}, 2017.

\bibitem{neal2012bayesian}
Radford~M Neal.
\newblock {\em Bayesian learning for neural networks}, volume 118.
\newblock Springer Science \& Business Media, 2012.

\bibitem{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In {\em NIPS workshop on deep learning and unsupervised feature
  learning}, volume 2011, page~5, 2011.

\bibitem{nguyen2015deep}
Anh Nguyen, Jason Yosinski, and Jeff Clune.
\newblock Deep neural networks are easily fooled: High confidence predictions
  for unrecognizable images.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 427--436, 2015.

\bibitem{parker2011english}
Robert Parker, David Graff, Junbo Kong, Ke~Chen, and Kazuaki Maeda.
\newblock English gigaword fifth edition, linguistic data consortium.
\newblock {\em Google Scholar}, 2011.

\bibitem{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock In {\em NIPS-W}, 2017.

\bibitem{pennington2014glove}
Jeffrey Pennington, Richard Socher, and Christopher Manning.
\newblock Glove: Global vectors for word representation.
\newblock In {\em Proceedings of the 2014 conference on empirical methods in
  natural language processing (EMNLP)}, pages 1532--1543, 2014.

\bibitem{schlegl2017unsupervised}
Thomas Schlegl, Philipp Seeb{\"o}ck, Sebastian~M Waldstein, Ursula
  Schmidt-Erfurth, and Georg Langs.
\newblock Unsupervised anomaly detection with generative adversarial networks
  to guide marker discovery.
\newblock In {\em International Conference on Information Processing in Medical
  Imaging}, pages 146--157. Springer, 2017.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{taigman2015web}
Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, and Lior Wolf.
\newblock Web-scale training for face identification.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2746--2754, 2015.

\bibitem{theis2015note}
Lucas Theis, A{\"a}ron van~den Oord, and Matthias Bethge.
\newblock A note on the evaluation of generative models.
\newblock {\em ICLR}, 2015.

\bibitem{wu1994verbs}
Zhibiao Wu and Martha Palmer.
\newblock Verbs semantics and lexical selection.
\newblock In {\em Proceedings of the 32nd annual meeting on Association for
  Computational Linguistics}, pages 133--138. Association for Computational
  Linguistics, 1994.

\bibitem{yu15lsun}
Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao.
\newblock Lsun: Construction of a large-scale image dataset using deep learning
  with humans in the loop.
\newblock {\em arXiv preprint arXiv:1506.03365}, 2015.

\bibitem{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock {\em arXiv preprint arXiv:1611.03530}, 2016.

\bibitem{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock {\em arXiv preprint arXiv:1710.09412}, 2017.

\end{thebibliography}
