\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aljundi et~al.(2018)Aljundi, Babiloni, Elhoseiny, Rohrbach, and
  Tuytelaars]{aljundi2017memory}
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and
  Tinne Tuytelaars.
\newblock Memory aware synapses: Learning what (not) to forget.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 139--154, 2018.

\bibitem[Bengio et~al.(2013)Bengio, Courville, and
  Vincent]{bengio2013representation}
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
\newblock Representation learning: A review and new perspectives.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 35\penalty0 (8):\penalty0 1798--1828, 2013.

\bibitem[Buzzega et~al.(2020)Buzzega, Boschini, Porrello, Abati, and
  Calderara]{buzzega2020dark}
Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone
  Calderara.
\newblock Dark experience for general continual learning: a strong, simple
  baseline.
\newblock In \emph{34th Conference on Neural Information Processing Systems
  (NeurIPS 2020)}, 2020.

\bibitem[Carlucci et~al.(2019)Carlucci, D'Innocente, Bucci, Caputo, and
  Tommasi]{carlucci2019domain}
Fabio~M Carlucci, Antonio D'Innocente, Silvia Bucci, Barbara Caputo, and
  Tatiana Tommasi.
\newblock Domain generalization by solving jigsaw puzzles.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 2229--2238, 2019.

\bibitem[Chaudhry et~al.(2018)Chaudhry, Dokania, Ajanthan, and
  Torr]{chaudhry2018riemannian}
Arslan Chaudhry, Puneet~K Dokania, Thalaiyasingam Ajanthan, and Philip~HS Torr.
\newblock Riemannian walk for incremental learning: Understanding forgetting
  and intransigence.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 532--547, 2018.

\bibitem[Chaudhry et~al.(2019{\natexlab{a}})Chaudhry, Ranzato, Rohrbach, and
  Elhoseiny]{chaudhry2019agem}
Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny.
\newblock Efficient lifelong learning with a-gem.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019{\natexlab{a}}.

\bibitem[Chaudhry et~al.(2019{\natexlab{b}})Chaudhry, Rohrbach, Elhoseiny,
  Ajanthan, Dokania, Torr, and Ranzato]{chaudhry2019tiny}
Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan,
  Puneet~K Dokania, Philip~HS Torr, and Marc'Aurelio Ranzato.
\newblock On tiny episodic memories in continual learning.
\newblock \emph{arXiv preprint arXiv:1902.10486}, 2019{\natexlab{b}}.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pages
  1597--1607. PMLR, 2020.

\bibitem[Delange et~al.(2021)Delange, Aljundi, Masana, Parisot, Jia, Leonardis,
  Slabaugh, and Tuytelaars]{delange2021continual}
Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu~Jia, Ales
  Leonardis, Greg Slabaugh, and Tinne Tuytelaars.
\newblock A continual learning survey: Defying forgetting in classification
  tasks.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2021.

\bibitem[Diethe et~al.(2019)Diethe, Borchert, Thereska, Balle, and
  Lawrence]{diethe2019continual}
Tom Diethe, Tom Borchert, Eno Thereska, Borja Balle, and Neil Lawrence.
\newblock Continual learning in practice.
\newblock \emph{arXiv preprint arXiv:1903.05202}, 2019.

\bibitem[Douglas et~al.(1995)Douglas, Koch, Mahowald, Martin, and
  Suarez]{douglas1995recurrent}
Rodney~J Douglas, Christof Koch, Misha Mahowald, KA~Martin, and Humbert~H
  Suarez.
\newblock Recurrent excitation in neocortical circuits.
\newblock \emph{Science}, 269\penalty0 (5226):\penalty0 981--985, 1995.

\bibitem[Dumoulin et~al.(2018)Dumoulin, Perez, Schucher, Strub, Vries,
  Courville, and Bengio]{dumoulin2018feature-wise}
Vincent Dumoulin, Ethan Perez, Nathan Schucher, Florian Strub, Harm~de Vries,
  Aaron Courville, and Yoshua Bengio.
\newblock Feature-wise transformations.
\newblock \emph{Distill}, 2018.
\newblock \doi{10.23915/distill.00011}.
\newblock https://distill.pub/2018/feature-wise-transformations.

\bibitem[Erhan et~al.(2010)Erhan, Courville, Bengio, and
  Vincent]{erhan2010does}
Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent.
\newblock Why does unsupervised pre-training help deep learning?
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 201--208. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem[Fernando et~al.(2017)Fernando, Banarse, Blundell, Zwols, Ha, Rusu,
  Pritzel, and Wierstra]{fernando2017pathnet}
Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha,
  Andrei~A Rusu, Alexander Pritzel, and Daan Wierstra.
\newblock Pathnet: Evolution channels gradient descent in super neural
  networks.
\newblock \emph{arXiv preprint arXiv:1701.08734}, 2017.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1126--1135. JMLR. org, 2017.

\bibitem[French(1999)]{french1999catastrophic}
Robert~M French.
\newblock Catastrophic forgetting in connectionist networks.
\newblock \emph{Trends in cognitive sciences}, 3\penalty0 (4):\penalty0
  128--135, 1999.

\bibitem[Gepperth and Karaoguz(2016)]{gepperth2016bio}
Alexander Gepperth and Cem Karaoguz.
\newblock A bio-inspired incremental learning architecture for applied
  perceptual problems.
\newblock \emph{Cognitive Computation}, 8\penalty0 (5):\penalty0 924--934,
  2016.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond,
  Buchatskaya, Doersch, Pires, Guo, Azar, et~al.]{grill2020bootstrap}
Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, Corentin Tallec,
  Pierre~H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo~Avila Pires,
  Zhaohan~Daniel Guo, Mohammad~Gheshlaghi Azar, et~al.
\newblock Bootstrap your own latent: A new approach to self-supervised
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2020momentum}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 9729--9738, 2020.

\bibitem[He et~al.(2019)He, Sygnowski, Galashov, Rusu, Teh, and
  Pascanu]{he2019task}
Xu~He, Jakub Sygnowski, Alexandre Galashov, Andrei~A Rusu, Yee~Whye Teh, and
  Razvan Pascanu.
\newblock Task agnostic continual learning via meta learning.
\newblock \emph{arXiv preprint arXiv:1906.05201}, 2019.

\bibitem[Javed and Shafait(2018)]{javed2018revisiting}
Khurram Javed and Faisal Shafait.
\newblock Revisiting distillation and incremental classifier learning.
\newblock In \emph{Asian conference on computer vision}, pages 3--17. Springer,
  2018.

\bibitem[Javed and White(2019)]{javed2019meta}
Khurram Javed and Martha White.
\newblock Meta-learning representations for continual learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1818--1828, 2019.

\bibitem[Kemker et~al.(2018)Kemker, McClure, Abitino, Hayes, and
  Kanan]{kemker2018measuring}
Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher
  Kanan.
\newblock Measuring catastrophic forgetting in neural networks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 2017.

\bibitem[Kumaran et~al.(2016)Kumaran, Hassabis, and
  McClelland]{kumaran2016learning}
Dharshan Kumaran, Demis Hassabis, and James~L McClelland.
\newblock What learning systems do intelligent agents need? complementary
  learning systems theory updated.
\newblock \emph{Trends in cognitive sciences}, 20\penalty0 (7):\penalty0
  512--534, 2016.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock \emph{nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Li et~al.(2019)Li, Zhou, Wu, Socher, and Xiong]{learn2grow}
Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong.
\newblock Learn to grow: A continual structure learning framework for
  overcoming catastrophic forgetting.
\newblock In \emph{International Conference on Machine Learning}, pages
  3925--3934, 2019.

\bibitem[Lin(1992)]{lin1992self}
Long-Ji Lin.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 293--321, 1992.

\bibitem[Liu et~al.(2020)Liu, Su, Liu, Schiele, and Sun]{liu2020mnemonics}
Yaoyao Liu, Yuting Su, An-An Liu, Bernt Schiele, and Qianru Sun.
\newblock Mnemonics training: Multi-class incremental learning without
  forgetting.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 12245--12254, 2020.

\bibitem[Lomonaco and Maltoni(2017)]{core50}
Vincenzo Lomonaco and Davide Maltoni.
\newblock Core50: a new dataset and benchmark for continuous object
  recognition.
\newblock In \emph{Proceedings of the 1st Annual Conference on Robot Learning},
  Proceedings of Machine Learning Research, pages 17--26. PMLR, 2017.

\bibitem[Lopez-Paz and Ranzato(2017)]{lopez2017gradient}
David Lopez-Paz and Marc'Aurelio Ranzato.
\newblock Gradient episodic memory for continual learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6467--6476, 2017.

\bibitem[McClelland et~al.(1995)McClelland, McNaughton, and
  O'Reilly]{mcclelland1995there}
James~L McClelland, Bruce~L McNaughton, and Randall~C O'Reilly.
\newblock Why there are complementary learning systems in the hippocampus and
  neocortex: insights from the successes and failures of connectionist models
  of learning and memory.
\newblock \emph{Psychological review}, 102\penalty0 (3):\penalty0 419, 1995.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Parisi et~al.(2018)Parisi, Tani, Weber, and
  Wermter]{parisi2018lifelong}
German~I Parisi, Jun Tani, Cornelius Weber, and Stefan Wermter.
\newblock Lifelong learning of spatiotemporal representations with dual-memory
  recurrent self-organization.
\newblock \emph{Frontiers in neurorobotics}, 12:\penalty0 78, 2018.

\bibitem[Parisi et~al.(2019)Parisi, Kemker, Part, Kanan, and
  Wermter]{parisi2019continual}
German~I Parisi, Ronald Kemker, Jose~L Part, Christopher Kanan, and Stefan
  Wermter.
\newblock Continual lifelong learning with neural networks: A review.
\newblock \emph{Neural Networks}, 2019.

\bibitem[Perez et~al.(2018)Perez, Strub, De~Vries, Dumoulin, and
  Courville]{perez2018film}
Ethan Perez, Florian Strub, Harm De~Vries, Vincent Dumoulin, and Aaron
  Courville.
\newblock Film: Visual reasoning with a general conditioning layer.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Pham et~al.(2021)Pham, Liu, Sahoo, and Hoi]{pham2021contextual}
Quang Pham, Chenghao Liu, Doyen Sahoo, and Steven~CH Hoi.
\newblock Contextual transformation networks for online continual learning.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2021.

\bibitem[Rao et~al.(2019)Rao, Visin, Rusu, Pascanu, Teh, and
  Hadsell]{rao2019continual}
Dushyant Rao, Francesco Visin, Andrei Rusu, Razvan Pascanu, Yee~Whye Teh, and
  Raia Hadsell.
\newblock Continual unsupervised representation learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Rebuffi et~al.(2017{\natexlab{a}})Rebuffi, Bilen, and
  Vedaldi]{rebuffi2017learning}
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
\newblock Learning multiple visual domains with residual adapters.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  506--516, 2017{\natexlab{a}}.

\bibitem[Rebuffi et~al.(2017{\natexlab{b}})Rebuffi, Kolesnikov, Sperl, and
  Lampert]{rebuffi2017icarl}
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph~H
  Lampert.
\newblock icarl: Incremental classifier and representation learning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 2001--2010, 2017{\natexlab{b}}.

\bibitem[Requeima et~al.(2019)Requeima, Gordon, Bronskill, Nowozin, and
  Turner]{requeima2019fast}
James Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, and
  Richard~E Turner.
\newblock Fast and flexible multi-task classification using conditional neural
  adaptive processes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  7959--7970, 2019.

\bibitem[Riemer et~al.(2019)Riemer, Cases, Ajemian, Liu, Rish, Tu, and
  Tesauro]{riemer2018learning}
Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu,
  and Gerald Tesauro.
\newblock Learning to learn without forgetting by maximizing transfer and
  minimizing interference.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019.

\bibitem[Ritter et~al.(2018)Ritter, Botev, and Barber]{ritter2018online}
Hippolyt Ritter, Aleksandar Botev, and David Barber.
\newblock Online structured laplace approximations for overcoming catastrophic
  forgetting.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3738--3748, 2018.

\bibitem[Rusu et~al.(2016)Rusu, Rabinowitz, Desjardins, Soyer, Kirkpatrick,
  Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}
Andrei~A Rusu, Neil~C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
  Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
\newblock Progressive neural networks.
\newblock \emph{arXiv preprint arXiv:1606.04671}, 2016.

\bibitem[Serra et~al.(2018)Serra, Suris, Miron, and Karatzoglou]{hat-cl}
Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou.
\newblock Overcoming catastrophic forgetting with hard attention to the task.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning-Volume 80}, pages 4548--4557. JMLR. org, 2018.

\bibitem[van~de Ven and Tolias(2018)]{van2018generative}
Gido~M van~de Ven and Andreas~S Tolias.
\newblock Generative replay with feedback connections as a general strategy for
  continual learning.
\newblock \emph{arXiv preprint arXiv:1809.10635}, 2018.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, Wierstra,
  et~al.]{vinyals2016matching}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et~al.
\newblock Matching networks for one shot learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  3630--3638, 2016.

\bibitem[Vitter(1985)]{vitter1985random}
Jeffrey~S Vitter.
\newblock Random sampling with a reservoir.
\newblock \emph{ACM Transactions on Mathematical Software (TOMS)}, 11\penalty0
  (1):\penalty0 37--57, 1985.

\bibitem[von Oswald et~al.(2020)von Oswald, Henning, Sacramento, and
  Grewe]{von2019continual}
Johannes von Oswald, Christian Henning, Jo{\~a}o Sacramento, and Benjamin~F
  Grewe.
\newblock Continual learning with hypernetworks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2020.

\bibitem[Yoon et~al.(2018)Yoon, Yang, Lee, and Hwang]{yoon2018lifelong}
Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung~Ju Hwang.
\newblock Lifelong learning with dynamically expandable networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{you2017large}
Yang You, Igor Gitman, and Boris Ginsburg.
\newblock Large batch training of convolutional networks.
\newblock \emph{arXiv preprint arXiv:1708.03888}, 2017.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and
  Deny]{zbontar2021barlow}
Jure Zbontar, Li~Jing, Ishan Misra, Yann LeCun, and St{\'e}phane Deny.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock \emph{arXiv preprint arXiv:2103.03230}, 2021.

\bibitem[Zenke et~al.(2017)Zenke, Poole, and Ganguli]{zenke2017continual}
Friedemann Zenke, Ben Poole, and Surya Ganguli.
\newblock Continual learning through synaptic intelligence.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 3987--3995. JMLR. org, 2017.

\bibitem[Zhang et~al.(2019)Zhang, Lucas, Ba, and Hinton]{zhang2019lookahead}
Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey~E Hinton.
\newblock Lookahead optimizer: k steps forward, 1 step back.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\end{thebibliography}
