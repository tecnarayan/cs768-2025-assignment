\begin{thebibliography}{65}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
  Ghemawat, S., Irving, G., Isard, M., et~al.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \emph{12th $\{$USENIX$\}$ Symposium on Operating Systems Design
  and Implementation ($\{$OSDI$\}$ 16)}, pp.\  265--283, 2016.

\bibitem[Achille \& Soatto(2018)Achille and Soatto]{achille2018information}
Achille, A. and Soatto, S.
\newblock Information dropout: Learning optimal representations through noisy
  computation.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 40\penalty0 (12):\penalty0 2897--2905, 2018.

\bibitem[Alemi et~al.(2017)Alemi, Poole, Fischer, Dillon, Saurous, and
  Murphy]{alemi2017fixing}
Alemi, A.~A., Poole, B., Fischer, I., Dillon, J.~V., Saurous, R.~A., and
  Murphy, K.
\newblock Fixing a broken elbo.
\newblock \emph{arXiv preprint arXiv:1711.00464}, 2017.

\bibitem[Amari(1998)]{amari1998natural}
Amari, S.-I.
\newblock Natural gradient works efficiently in learning.
\newblock \emph{Neural computation}, 10\penalty0 (2):\penalty0 251--276, 1998.

\bibitem[Arvanitidis et~al.(2017)Arvanitidis, Hansen, and
  Hauberg]{arvanitidis2017latent}
Arvanitidis, G., Hansen, L.~K., and Hauberg, S.
\newblock Latent space oddity: on the curvature of deep generative models.
\newblock \emph{arXiv preprint arXiv:1710.11379}, 2017.

\bibitem[Babaeizadeh et~al.(2018)Babaeizadeh, Finn, Erhan, Campbell, and
  Levine]{babaeizadeh2018stochastic}
Babaeizadeh, M., Finn, C., Erhan, D., Campbell, R.~H., and Levine, S.
\newblock Stochastic variational video prediction.
\newblock 2018.

\bibitem[Barron(2019)]{barron2019general}
Barron, J.~T.
\newblock A general and adaptive robust loss function.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  4331--4339, 2019.

\bibitem[Bozkurt et~al.(2019)Bozkurt, Esmaeili, Tristan, Brooks, Dy, and van~de
  Meent]{bozkurt2019rate}
Bozkurt, A., Esmaeili, B., Tristan, J.-B., Brooks, D.~H., Dy, J.~G., and van~de
  Meent, J.-W.
\newblock Rate-regularization and generalization in vaes.
\newblock \emph{arXiv preprint arXiv:1911.04594}, 2019.

\bibitem[Castrejon et~al.(2019)Castrejon, Ballas, and
  Courville]{castrejon2019improved}
Castrejon, L., Ballas, N., and Courville, A.
\newblock Improved conditional vrnns for video prediction.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  7608--7617, 2019.

\bibitem[Chen et~al.(2016)Chen, Kingma, Salimans, Duan, Dhariwal, Schulman,
  Sutskever, and Abbeel]{chen2016variational}
Chen, X., Kingma, D.~P., Salimans, T., Duan, Y., Dhariwal, P., Schulman, J.,
  Sutskever, I., and Abbeel, P.
\newblock Variational lossy autoencoder.
\newblock \emph{arXiv preprint arXiv:1611.02731}, 2016.

\bibitem[Chua et~al.(2018)Chua, Calandra, McAllister, and Levine]{chua2018deep}
Chua, K., Calandra, R., McAllister, R., and Levine, S.
\newblock Deep reinforcement learning in a handful of trials using
  probabilistic dynamics models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4754--4765, 2018.

\bibitem[Chung et~al.(2015)Chung, Kastner, Dinh, Goel, Courville, and
  Bengio]{chung2015recurrent}
Chung, J., Kastner, K., Dinh, L., Goel, K., Courville, A.~C., and Bengio, Y.
\newblock A recurrent latent variable model for sequential data.
\newblock 2015.

\bibitem[Dai \& Wipf(2019)Dai and Wipf]{dai2019diagnosing}
Dai, B. and Wipf, D.
\newblock Diagnosing and enhancing vae models.
\newblock \emph{arXiv preprint arXiv:1903.05789}, 2019.

\bibitem[Dawid(1982)]{dawid1982well}
Dawid, A.~P.
\newblock The well-calibrated bayesian.
\newblock \emph{Journal of the American Statistical Association}, 77\penalty0
  (379):\penalty0 605--610, 1982.

\bibitem[DeGroot \& Fienberg(1983)DeGroot and Fienberg]{degroot1983comparison}
DeGroot, M.~H. and Fienberg, S.~E.
\newblock The comparison and evaluation of forecasters.
\newblock \emph{Journal of the Royal Statistical Society: Series D (The
  Statistician)}, 32\penalty0 (1-2):\penalty0 12--22, 1983.

\bibitem[{Denton} \& {Fergus}(2018){Denton} and {Fergus}]{denton2018stochastic}
{Denton}, E. and {Fergus}, R.
\newblock Stochastic video generation with a learned prior.
\newblock 2018.

\bibitem[Dhariwal et~al.(2020)Dhariwal, Jun, Payne, Kim, Radford, and
  Sutskever]{dhariwal2020jukebox}
Dhariwal, P., Jun, H., Payne, C., Kim, J.~W., Radford, A., and Sutskever, I.
\newblock Jukebox: A generative model for music.
\newblock \emph{arXiv preprint arXiv:[TODO]}, 2020.

\bibitem[Edwards \& Storkey(2016)Edwards and Storkey]{edwards2016towards}
Edwards, H. and Storkey, A.
\newblock Towards a neural statistician.
\newblock \emph{arXiv preprint arXiv:1606.02185}, 2016.

\bibitem[Engel et~al.(2017)Engel, Hoffman, and Roberts]{engel2017latent}
Engel, J., Hoffman, M., and Roberts, A.
\newblock Latent constraints: Learning to generate conditionally from
  unconditional generative models.
\newblock \emph{arXiv preprint arXiv:1711.05772}, 2017.

\bibitem[Finn \& Levine(2017)Finn and Levine]{finn2017deep}
Finn, C. and Levine, S.
\newblock Deep visual foresight for planning robot motion.
\newblock 2017.

\bibitem[Ghosh et~al.(2019)Ghosh, Sajjadi, Vergari, Black, and
  Sch{\"o}lkopf]{ghosh2019variational}
Ghosh, P., Sajjadi, M.~S., Vergari, A., Black, M., and Sch{\"o}lkopf, B.
\newblock From variational to deterministic autoencoders.
\newblock \emph{arXiv preprint arXiv:1903.12436}, 2019.

\bibitem[Gregor et~al.(2015)Gregor, Danihelka, Graves, Rezende, and
  Wierstra]{gregor2015draw}
Gregor, K., Danihelka, I., Graves, A., Rezende, D.~J., and Wierstra, D.
\newblock Draw: A recurrent neural network for image generation.
\newblock \emph{arXiv preprint arXiv:1502.04623}, 2015.

\bibitem[Gregor et~al.(2016)Gregor, Besse, Rezende, Danihelka, and
  Wierstra]{gregor2016towards}
Gregor, K., Besse, F., Rezende, D.~J., Danihelka, I., and Wierstra, D.
\newblock Towards conceptual compression.
\newblock 2016.

\bibitem[Gulrajani et~al.(2016)Gulrajani, Kumar, Ahmed, Taiga, Visin, Vazquez,
  and Courville]{gulrajani2016pixelvae}
Gulrajani, I., Kumar, K., Ahmed, F., Taiga, A.~A., Visin, F., Vazquez, D., and
  Courville, A.
\newblock Pixelvae: A latent variable model for natural images.
\newblock \emph{arXiv preprint arXiv:1611.05013}, 2016.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
\newblock On calibration of modern neural networks.
\newblock \emph{arXiv preprint arXiv:1706.04599}, 2017.

\bibitem[Hafner et~al.(2019{\natexlab{a}})Hafner, Lillicrap, Ba, and
  Norouzi]{hafner2019dream}
Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M.
\newblock Dream to control: Learning behaviors by latent imagination.
\newblock \emph{arXiv preprint arXiv:1912.01603}, 2019{\natexlab{a}}.

\bibitem[Hafner et~al.(2019{\natexlab{b}})Hafner, Lillicrap, Fischer, Villegas,
  Ha, Lee, and Davidson]{hafner2018learning}
Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and
  Davidson, J.
\newblock Learning latent dynamics for planning from pixels.
\newblock 2019{\natexlab{b}}.

\bibitem[Henaff et~al.(2019)Henaff, Canziani, and LeCun]{henaff2019model}
Henaff, M., Canziani, A., and LeCun, Y.
\newblock Model-predictive policy learning with uncertainty regularization for
  driving in dense traffic.
\newblock \emph{arXiv preprint arXiv:1901.02705}, 2019.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{heusel2017gans}
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  6626--6637, 2017.

\bibitem[Higgins et~al.(2017)Higgins, Matthey, Pal, Burgess, Glorot, Botvinick,
  Mohamed, and Lerchner]{higgins2017beta}
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M.,
  Mohamed, S., and Lerchner, A.
\newblock beta-{VAE}: Learning basic visual concepts with a constrained
  variational framework.
\newblock 2017.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Ho, J., Jain, A., and Abbeel, P.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Jordan et~al.(1999)Jordan, Ghahramani, Jaakkola, and
  Saul]{jordan1999introduction}
Jordan, M.~I., Ghahramani, Z., Jaakkola, T.~S., and Saul, L.~K.
\newblock An introduction to variational methods for graphical models.
\newblock \emph{Machine learning}, 37\penalty0 (2):\penalty0 183--233, 1999.

\bibitem[Kendall \& Gal(2017)Kendall and Gal]{kendall2017uncertainties}
Kendall, A. and Gal, Y.
\newblock What uncertainties do we need in bayesian deep learning for computer
  vision?
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5574--5584, 2017.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock 2015.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{kingma2014auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational {B}ayes.
\newblock 2014.

\bibitem[Kingma et~al.(2016)Kingma, Salimans, Jozefowicz, Chen, Sutskever, and
  Welling]{kingma2016improved}
Kingma, D.~P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., and
  Welling, M.
\newblock Improved variational inference with inverse autoregressive flow.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  4743--4751, 2016.

\bibitem[Kohl et~al.(2018)Kohl, Romera-Paredes, Meyer, De~Fauw, Ledsam,
  Maier-Hein, Eslami, Rezende, and Ronneberger]{kohl2018probabilistic}
Kohl, S., Romera-Paredes, B., Meyer, C., De~Fauw, J., Ledsam, J.~R.,
  Maier-Hein, K., Eslami, S.~A., Rezende, D.~J., and Ronneberger, O.
\newblock A probabilistic u-net for segmentation of ambiguous images.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6965--6975, 2018.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lee et~al.(2018)Lee, Zhang, Ebert, Abbeel, Finn, and
  Levine]{lee2018savp}
Lee, A.~X., Zhang, R., Ebert, F., Abbeel, P., Finn, C., and Levine, S.
\newblock Stochastic adversarial video prediction.
\newblock \emph{arXiv:1804.01523}, abs/1804.01523, 2018.

\bibitem[Lee et~al.(2019)Lee, Nagabandi, Abbeel, and Levine]{lee2019stochastic}
Lee, A.~X., Nagabandi, A., Abbeel, P., and Levine, S.
\newblock Stochastic latent actor-critic: Deep reinforcement learning with a
  latent variable model.
\newblock \emph{arXiv preprint arXiv:1907.00953}, 2019.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015faceattributes}
Liu, Z., Luo, P., Wang, X., and Tang, X.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of International Conference on Computer Vision
  (ICCV)}, December 2015.

\bibitem[Lucas et~al.(2019)Lucas, Tucker, Grosse, and Norouzi]{lucas2019don}
Lucas, J., Tucker, G., Grosse, R.~B., and Norouzi, M.
\newblock Don't blame the elbo! a linear vae perspective on posterior collapse.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9403--9413, 2019.

\bibitem[Maal{\o}e et~al.(2019)Maal{\o}e, Fraccaro, Li{\'e}vin, and
  Winther]{maaloe2019biva}
Maal{\o}e, L., Fraccaro, M., Li{\'e}vin, V., and Winther, O.
\newblock Biva: A very deep hierarchy of latent variables for generative
  modeling.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  6548--6558, 2019.

\bibitem[Mattei \& Frellsen(2018)Mattei and Frellsen]{mattei2018leveraging}
Mattei, P.-A. and Frellsen, J.
\newblock Leveraging the exact likelihood of deep latent variable models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3855--3866, 2018.

\bibitem[Neal \& Hinton(1998)Neal and Hinton]{neal1998view}
Neal, R.~M. and Hinton, G.~E.
\newblock A view of the em algorithm that justifies incremental, sparse, and
  other variants.
\newblock In \emph{Learning in graphical models}, pp.\  355--368. Springer,
  1998.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A.~Y.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8024--8035, 2019.

\bibitem[Pavlakos et~al.(2019)Pavlakos, Choutas, Ghorbani, Bolkart, Osman,
  Tzionas, and Black]{pavlakos2019expressive}
Pavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A.~A., Tzionas,
  D., and Black, M.~J.
\newblock Expressive body capture: 3d hands, face, and body from a single
  image.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  10975--10985, 2019.

\bibitem[Peng et~al.(2018)Peng, Kanazawa, Toyer, Abbeel, and
  Levine]{peng2018variational}
Peng, X.~B., Kanazawa, A., Toyer, S., Abbeel, P., and Levine, S.
\newblock Variational discriminator bottleneck: Improving imitation learning,
  inverse rl, and gans by constraining information flow.
\newblock \emph{arXiv preprint arXiv:1810.00821}, 2018.

\bibitem[Peters \& Schaal(2008)Peters and Schaal]{peters2008reinforcement}
Peters, J. and Schaal, S.
\newblock Reinforcement learning of motor skills with policy gradients.
\newblock \emph{Neural networks}, 21\penalty0 (4):\penalty0 682--697, 2008.

\bibitem[Pong et~al.(2019)Pong, Dalal, Lin, Nair, Bahl, and
  Levine]{pong2019skew}
Pong, V.~H., Dalal, M., Lin, S., Nair, A., Bahl, S., and Levine, S.
\newblock Skew-fit: State-covering self-supervised reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1903.03698}, 2019.

\bibitem[Rezende \& Viola(2018)Rezende and Viola]{rezende2018taming}
Rezende, D.~J. and Viola, F.
\newblock Taming vaes.
\newblock \emph{arXiv preprint arXiv:1810.00597}, 2018.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Rezende, D.~J., Mohamed, S., and Wierstra, D.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock 2014.

\bibitem[Rolfe(2016)]{rolfe2016discrete}
Rolfe, J.~T.
\newblock Discrete variational autoencoders.
\newblock \emph{arXiv preprint arXiv:1609.02200}, 2016.

\bibitem[Rosca et~al.(2018)Rosca, Lakshminarayanan, and
  Mohamed]{rosca2018distribution}
Rosca, M., Lakshminarayanan, B., and Mohamed, S.
\newblock Distribution matching in variational inference.
\newblock \emph{arXiv preprint arXiv:1802.06847}, 2018.

\bibitem[Salimans et~al.(2017)Salimans, Karpathy, Chen, and
  Kingma]{salimans2017pixelcnn++}
Salimans, T., Karpathy, A., Chen, X., and Kingma, D.~P.
\newblock Pixelcnn++: Improving the pixelcnn with discretized logistic mixture
  likelihood and other modifications.
\newblock \emph{arXiv preprint arXiv:1701.05517}, 2017.

\bibitem[Shu et~al.(2018)Shu, Bui, Zhao, Kochenderfer, and
  Ermon]{shu2018amortized}
Shu, R., Bui, H.~H., Zhao, S., Kochenderfer, M.~J., and Ermon, S.
\newblock Amortized inference regularization.
\newblock \emph{arXiv preprint arXiv:1805.08913}, 2018.

\bibitem[Sohn et~al.(2015)Sohn, Lee, and Yan]{sohn2015learning}
Sohn, K., Lee, H., and Yan, X.
\newblock Learning structured output representation using deep conditional
  generative models.
\newblock 2015.

\bibitem[S{\o}nderby et~al.(2016)S{\o}nderby, Raiko, Maal{\o}e, S{\o}nderby,
  and Winther]{sonderby2016ladder}
S{\o}nderby, C.~K., Raiko, T., Maal{\o}e, L., S{\o}nderby, S.~K., and Winther,
  O.
\newblock Ladder variational autoencoders.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3738--3746, 2016.

\bibitem[Stirn \& Knowles(2020)Stirn and Knowles]{stirn2020variational}
Stirn, A. and Knowles, D.~A.
\newblock Variational variance: Simple and reliable predictive variance
  parameterization.
\newblock \emph{arXiv preprint arXiv:2006.04910}, 2020.

\bibitem[Takahashi et~al.(2018)Takahashi, Iwata, Yamanaka, Yamada, and
  Yagi]{takahashi2018student}
Takahashi, H., Iwata, T., Yamanaka, Y., Yamada, M., and Yagi, S.
\newblock Student-t variational autoencoder for robust density estimation.
\newblock In \emph{IJCAI}, pp.\  2696--2702, 2018.

\bibitem[Theis et~al.(2016)Theis, Oord, and Bethge]{theis2015note}
Theis, L., Oord, A. v.~d., and Bethge, M.
\newblock A note on the evaluation of generative models.
\newblock \emph{ICLR}, 2016.

\bibitem[Watter et~al.(2015)Watter, Springenberg, Boedecker, and
  Riedmiller]{watter2015embed}
Watter, M., Springenberg, J., Boedecker, J., and Riedmiller, M.
\newblock Embed to control: A locally linear latent dynamics model for control
  from raw images.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2746--2754, 2015.

\bibitem[Zhao et~al.(2017)Zhao, Song, and Ermon]{zhao2017infovae}
Zhao, S., Song, J., and Ermon, S.
\newblock Infovae: Information maximizing variational autoencoders.
\newblock \emph{arXiv preprint arXiv:1706.02262}, 2017.

\bibitem[Zhu et~al.(2017)Zhu, Zhang, Pathak, Darrell, Efros, Wang, and
  Shechtman]{zhu2017toward}
Zhu, J.-Y., Zhang, R., Pathak, D., Darrell, T., Efros, A.~A., Wang, O., and
  Shechtman, E.
\newblock Toward multimodal image-to-image translation.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  465--476, 2017.

\end{thebibliography}
