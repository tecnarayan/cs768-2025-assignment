\begin{thebibliography}{62}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Azimi et~al.(2010)Azimi, Fern, and Fern]{azimi2010batch}
J.~Azimi, A.~Fern, and X.Z. Fern.
\newblock Batch {B}ayesian optimization via simulation matching.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2010.

\bibitem[Bach(2013)]{bach2013learning}
F.~Bach.
\newblock Learning with submodular functions: A convex optimization
  perspective.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  6\penalty0 (2-3), 2013.

\bibitem[Bansal et~al.(2017)Bansal, Calandra, Xiao, Levine, and
  Tomlin]{bansal2017goal}
S.~Bansal, R.~Calandra, T.~Xiao, S.~Levine, and C.J. Tomlin.
\newblock Goal-driven dynamics learning via {B}ayesian optimization.
\newblock \emph{arXiv preprint arXiv:1703.09260}, 2017.

\bibitem[Bergstra and Bengio(2012)]{bergstra12}
J.~Bergstra and Y.~Bengio.
\newblock Random search for hyper-parameter optimization.
\newblock \emph{Journal of Machine Learning Research}, 2012.

\bibitem[Bochner(1959)]{bochner1959lectures}
S.~Bochner.
\newblock \emph{Lectures on {F}ourier Integrals}.
\newblock Number~42. Princeton University Press, 1959.

\bibitem[Bousquet and Bottou(2008)]{bousquet2008tradeoffs}
O.~Bousquet and L.~Bottou.
\newblock The tradeoffs of large scale learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2008.

\bibitem[Calandra et~al.(2016)Calandra, Seyfarth, Peters, and
  Deisenroth]{calandra2016bayesian}
R.~Calandra, A.~Seyfarth, J.~Peters, and M.P. Deisenroth.
\newblock Bayesian optimization for learning gaits under uncertainty.
\newblock \emph{Annals of Mathematics and Artificial Intelligence}, 76\penalty0
  (1-2), 2016.

\bibitem[Cao(1985)]{cao1985convergence}
X.~Cao.
\newblock Convergence of parameter sensitivity estimates in a stochastic
  experiment.
\newblock \emph{IEEE Transactions on Automatic Control}, 30\penalty0 (9), 1985.

\bibitem[Chen and Krause()]{chen2013near}
Y.~Chen and A.~Krause.
\newblock Near-optimal batch mode active learning and adaptive submodular
  optimization.

\bibitem[Chevalier and Ginsbourger(2013)]{chevalier2013fast}
C.~Chevalier and D.~Ginsbourger.
\newblock Fast computation of the multi-points expected improvement with
  applications in batch selection.
\newblock In \emph{International Conference on Learning and Intelligent
  Optimization}, 2013.

\bibitem[Christian(2007)]{robert2007bayesian}
R.~Christian.
\newblock \emph{The {B}ayesian choice: from decision-theoretic foundations to
  computational implementation}.
\newblock Springer Science \& Business Media, 2007.

\bibitem[Contal et~al.(2013)Contal, Buffoni, Robicquet, and
  Vayatis]{contal2013parallel}
E.~Contal, D.~Buffoni, A.~Robicquet, and N.~Vayatis.
\newblock Parallel \uppercase{G}aussian process optimization with upper
  confidence bound and pure exploration.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, 2013.

\bibitem[Cunningham et~al.(2011)Cunningham, Hennig, and
  Lacoste-Julien]{cunningham2011epmgp}
J.P. Cunningham, P.~Hennig, and S.~Lacoste-Julien.
\newblock Gaussian probabilities and expectation propagation.
\newblock \emph{arXiv preprint arXiv:1111.6832}, 2011.

\bibitem[DeGroot(2005)]{degroot2005optimal}
M.H. DeGroot.
\newblock \emph{Optimal statistical decisions}, volume~82.
\newblock John Wiley \& Sons, 2005.

\bibitem[Desautels et~al.(2014)Desautels, Krause, and
  Burdick]{desautels2014parallelizing}
T.~Desautels, A.~Krause, and J.W. Burdick.
\newblock Parallelizing exploration-exploitation tradeoffs in
  \uppercase{G}aussian process bandit optimization.
\newblock \emph{Journal of Machine Learning Research}, 2014.

\bibitem[Falkner et~al.(2018)Falkner, Klein, and Hutter]{falkner-icml-18}
S.~Falkner, A.~Klein, and F.~Hutter.
\newblock {BOHB}: Robust and efficient hyperparameter optimization at scale.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Frazier and Wang(2016)]{frazier2016bayesian}
P.I. Frazier and J.~Wang.
\newblock Bayesian optimization for materials design.
\newblock In \emph{Information Science for Materials Discovery and Design}.
  2016.

\bibitem[Gassmann et~al.(2002)Gassmann, De{\'a}k, and Sz{\'a}ntai]{gassmann02}
H.I. Gassmann, I.~De{\'a}k, and T.~Sz{\'a}ntai.
\newblock Computing multivariate normal probabilities: A new look.
\newblock \emph{Journal of Computational and Graphical Statistics}, 11\penalty0
  (4), 2002.

\bibitem[Gelbart et~al.(2014)Gelbart, Snoek, and Adams]{gelbart2014bayesian}
M.A. Gelbart, J.~Snoek, and R.P. Adams.
\newblock Bayesian optimization with unknown constraints.
\newblock \emph{arXiv preprint arXiv:1403.5607}, 2014.

\bibitem[Genz(1992)]{genz92numerical}
A.~Genz.
\newblock Numerical computation of multivariate normal probabilities.
\newblock \emph{Journal of Computational and Graphical Statistics}, 1992.

\bibitem[Genz(2004)]{genz2004numerical}
A.~Genz.
\newblock Numerical computation of rectangular bivariate and trivariate normal
  and t probabilities.
\newblock \emph{Statistics and Computing}, 14\penalty0 (3), 2004.

\bibitem[Ginsbourger et~al.(2010)Ginsbourger, Le~Riche, and
  Carraro]{ginsbourger2010kriging}
D.~Ginsbourger, R.~Le~Riche, and L.~Carraro.
\newblock \emph{Kriging is well-suited to parallelize optimization}, chapter~6.
\newblock Springer, 2010.

\bibitem[Ginsbourger et~al.(2011)Ginsbourger, Janusevskis, and
  Le~Riche]{ginsbourger2011dealing}
D.~Ginsbourger, J.~Janusevskis, and R.~Le~Riche.
\newblock Dealing with asynchronicity in parallel {G}aussian process based
  global optimization.
\newblock In \emph{International Conference of the ERCIM WG on Computing \&
  Statistics}, 2011.

\bibitem[Glasserman(1988)]{glasserman1988performance}
P.~Glasserman.
\newblock Performance continuity and differentiability in {M}onte {C}arlo
  optimization.
\newblock In \emph{Simulation Conference Proceedings, 1988 Winter}. IEEE, 1988.

\bibitem[Gradshteyn and Ryzhik(2014)]{gradshteyn2014table}
I.S. Gradshteyn and I.M. Ryzhik.
\newblock \emph{Table of integrals, series, and products}.
\newblock Academic press, 2014.

\bibitem[Hansen(2016)]{hansen2016cma}
N.~Hansen.
\newblock The {CMA} evolution strategy: A tutorial.
\newblock \emph{arXiv preprint arXiv:1604.00772}, 2016.

\bibitem[Hennig and Schuler(2012)]{hennig-jmlr12a}
P.~Hennig and C.~Schuler.
\newblock Entropy search for information-efficient global optimization.
\newblock \emph{Journal of Machine Learning Research}, 2012.

\bibitem[Hern\'{a}ndez-Lobato et~al.(2014)Hern\'{a}ndez-Lobato, Hoffman, and
  Ghahramani]{hernandez-nips14}
J.~Hern\'{a}ndez-Lobato, M.~Hoffman, and Z.~Ghahramani.
\newblock Predictive entropy search for efficient global optimization of
  black-box functions.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2014.

\bibitem[Hutter et~al.(2011)Hutter, Hoos, and
  Leyton-Brown]{hutter2011sequential}
F.~Hutter, H.H. Hoos, and K.~Leyton-Brown.
\newblock Sequential model-based optimization for general algorithm
  configuration.
\newblock In \emph{International Conference on Learning and Intelligent
  Optimization}. Springer, 2011.

\bibitem[Jamieson and Talwalkar(2016)]{jamieson2016non}
K.~Jamieson and A.~Talwalkar.
\newblock Non-stochastic best arm identification and hyperparameter
  optimization.
\newblock In \emph{Artificial Intelligence and Statistics}, 2016.

\bibitem[Jang et~al.(2016)Jang, Gu, and Poole]{jang2016categorical}
E.~Jang, S.~Gu, and B.~Poole.
\newblock Categorical reparameterization with
  \uppercase{G}umbel-\uppercase{S}oftmax.
\newblock \emph{arXiv preprint arXiv:1611.01144}, 2016.

\bibitem[Jones et~al.(1998)Jones, Schonlau, and Welch]{jones-jgo98a}
D.~Jones, M.~Schonlau, and W.~Welch.
\newblock Efficient global optimization of expensive black box functions.
\newblock \emph{Journal of Global Optimization}, 13:\penalty0 455--492, 1998.

\bibitem[Jones et~al.(1993)Jones, Perttunen, and
  Stuckman]{jones1993lipschitzian}
D.R. Jones, C.D. Perttunen, and B.E. Stuckman.
\newblock Lipschitzian optimization without the \uppercase{L}ipschitz constant.
\newblock \emph{Journal of Optimization Theory and Applications}, 1993.

\bibitem[Karnin et~al.(2013)Karnin, Koren, and Somekh]{karnin2013almost}
Z.~Karnin, T.~Koren, and O.~Somekh.
\newblock Almost optimal exploration in multi-armed bandits.
\newblock In \emph{International Conference on Machine Learning}, 2013.

\bibitem[Kathuria et~al.(2016)Kathuria, Deshpande, and
  Kohli]{kathuria2016batched}
T.~Kathuria, A.~Deshpande, and P.~Kohli.
\newblock Batched {G}aussian process bandit optimization via determinantal
  point processes.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma and Welling(2014)]{kingma2013vae}
D.P. Kingma and M.~Welling.
\newblock Auto-encoding variational \uppercase{b}ayes.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[Kotz and Nadarajah(2004)]{kotz2004multivariate}
S.~Kotz and S.~Nadarajah.
\newblock \emph{Multivariate t-distributions and their applications}.
\newblock Cambridge University Press, 2004.

\bibitem[Krause and Golovin(2014)]{krause14}
A.~Krause and D.~Golovin.
\newblock Submodular function maximization, 2014.

\bibitem[Kushner(1964)]{kushner1964new}
H.J. Kushner.
\newblock A new method of locating the maximum point of an arbitrary multipeak
  curve in the presence of noise.
\newblock \emph{Journal of Basic Engineering}, 86\penalty0 (1), 1964.

\bibitem[Maddison et~al.(2016)Maddison, Mnih, and Teh]{maddison2016concrete}
C.J. Maddison, A.~Mnih, and Y.W. Teh.
\newblock The concrete distribution: A continuous relaxation of discrete random
  variables.
\newblock \emph{arXiv preprint arXiv:1611.00712}, 2016.

\bibitem[Martinez-Cantin(2014)]{martinez2014bayesopt}
R.~Martinez-Cantin.
\newblock Bayesopt: A {B}ayesian optimization library for nonlinear
  optimization, experimental design and bandits.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0 (1), 2014.

\bibitem[Minoux(1978)]{minoux1978accelerated}
M.~Minoux.
\newblock Accelerated greedy algorithms for maximizing submodular set
  functions.
\newblock In \emph{Optimization Techniques}. 1978.

\bibitem[Mo{\v{c}}kus(1975)]{movckus1975bayesian}
J.~Mo{\v{c}}kus.
\newblock On {B}ayesian methods for seeking the extremum.
\newblock In \emph{Optimization Techniques IFIP Technical Conference}.
  Springer, 1975.

\bibitem[Mo{\v{c}}kus(1994)]{movckus1994application}
J.~Mo{\v{c}}kus.
\newblock Application of {B}ayesian approach to numerical methods of global and
  stochastic optimization.
\newblock \emph{Journal of Global Optimization}, 4\penalty0 (4), 1994.

\bibitem[Nemhauser et~al.(1978)Nemhauser, Wolsey, and Fisher]{nemhauser78}
G.L. Nemhauser, L.A. Wolsey, and M.L. Fisher.
\newblock An analysis of approximations for maximizing submodular set
  functions—{I}.
\newblock \emph{Mathematical Programming}, 14\penalty0 (1), 1978.

\bibitem[Osborne et~al.(2009)Osborne, Garnett, and
  Roberts]{osborne2009gaussian}
M.A. Osborne, R.~Garnett, and S.J. Roberts.
\newblock \uppercase{G}aussian processes for global optimization.
\newblock In \emph{International Conference on Learning and Intelligent
  Optimization}, 2009.

\bibitem[Rahimi and Recht(2008)]{rahimi2008random}
A.~Rahimi and B.~Recht.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2008.

\bibitem[Rasmussen and Williams(2006)]{rasmussen-book06a}
C.E. Rasmussen and C.K.I. Williams.
\newblock \emph{Gaussian Processes for Machine Learning}.
\newblock The MIT Press, 2006.

\bibitem[Rezende et~al.(2014)Rezende, Shakir, and
  Wierstra]{JimenezRezende2014a}
D.J. Rezende, M.~Shakir, and D.~Wierstra.
\newblock Stochastic backpropagation and variational inference in deep latent
  {G}aussian models.
\newblock In \emph{International Conference on Machine Learning}, 2014.

\bibitem[Shah and Ghahramani(2015)]{shah2015parallel}
A.~Shah and Z.~Ghahramani.
\newblock Parallel predictive entropy search for batch global optimization of
  expensive objective functions.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2015.

\bibitem[Shahriari et~al.(2016)Shahriari, Swersky, Wang, Adams, and
  de~Freitas]{shahriari-ieee16}
B.~Shahriari, K.~Swersky, Z.~Wang, R.P. Adams, and N.~de~Freitas.
\newblock Taking the human out of the loop: A {R}eview of {B}ayesian
  {O}ptimization.
\newblock \emph{Proceedings of the IEEE}, \penalty0 (1), 2016.

\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and Adams]{snoek-nips12a}
J.~Snoek, H.~Larochelle, and R.P. Adams.
\newblock Practical {B}ayesian optimization of machine learning algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems 25}, 2012.

\bibitem[Springenberg et~al.(2016)Springenberg, Klein, Falkner, and
  Hutter]{springenberg2016bayesian}
J.T. Springenberg, A.~Klein, S.~Falkner, and F.~Hutter.
\newblock Bayesian optimization with robust \uppercase{B}ayesian neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Srinivas et~al.(2010)Srinivas, Krause, Kakade, and Seeger]{srinivas10}
N.~Srinivas, A.~Krause, S.~Kakade, and M.~Seeger.
\newblock {G}aussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock In \emph{International Conference on Machine Learning}, 2010.

\bibitem[Swersky et~al.(2013)Swersky, Snoek, and Adams]{swersky-nips13}
K.~Swersky, J.~Snoek, and R.P. Adams.
\newblock Multi-task {B}ayesian optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2013.

\bibitem[Ueno et~al.(2016)Ueno, Rhone, Hou, Mizoguchi, and
  Tsuda]{ueno2016combo}
T.~Ueno, T.D. Rhone, Z.~Hou, T.~Mizoguchi, and K.~Tsuda.
\newblock Combo: {A}n efficient {B}ayesian optimization library for materials
  science.
\newblock \emph{Materials discovery}, 4, 2016.

\bibitem[Viana and Haftka(2010)]{viana2010surrogate}
F.~Viana and R.~Haftka.
\newblock Surrogate-based optimization with parallel simulations using the
  probability of improvement.
\newblock In \emph{AIAA/ISSMO Multidisciplinary Analysis Optimization
  Conference}, 2010.

\bibitem[Wang et~al.(2016)Wang, Clark, Liu, and Frazier]{wang2016parallel}
J.~Wang, S.C. Clark, E.~Liu, and P.I. Frazier.
\newblock Parallel \uppercase{B}ayesian global optimization of expensive
  functions.
\newblock \emph{arXiv preprint arXiv:1602.05149}, 2016.

\bibitem[Wang and Jegelka(2017)]{wang2017max}
Z.~Wang and S.~Jegelka.
\newblock Max-value entropy search for efficient {B}ayesian optimization.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Wu and Frazier(2016)]{wu2016parallel}
J.~Wu and P.I. Frazier.
\newblock The parallel {K}nowledge {G}radient method for batch {B}ayesian
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Wu et~al.(2017)Wu, Poloczek, Wilson, and Frazier]{wu2017bayesian}
J.~Wu, M.~Poloczek, A.G. Wilson, and P.I. Frazier.
\newblock Bayesian optimization with gradients.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5267--5278, 2017.

\end{thebibliography}
