\begin{thebibliography}{10}

\bibitem{Alcorn19strike}
Michael~A. Alcorn, Qi~Li, Zhitao Gong, Chengfei Wang, Long Mai, Wei{-}Shinn Ku,
  and Anh Nguyen.
\newblock Strike (with) a pose: Neural networks are easily fooled by strange
  poses of familiar objects.
\newblock In {\em The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2019.

\bibitem{Athalye2018obfuscated}
Anish Athalye, Nicholas Carlini, and David Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock {\em In Proceedings of the International Conference on Machine
  Learning (ICML)}, 2018.

\bibitem{balovic2019geo}
Mislav Balunovic, Maximilian Baader, Gagandeep Singh, Timon Gehr, and Martin
  Vechev.
\newblock Certifying geometric robustness of neural networks.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem{bengio2009curriculum}
Yoshua Bengio, J\'{e}r\^{o}me Louradour, Ronan Collobert, and Jason Weston.
\newblock Curriculum learning.
\newblock In {\em In Proceedings of the International Conference on Machine
  Learning (ICML)}, 2009.

\bibitem{Biggio_2013}
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndić,
  Pavel Laskov, Giorgio Giacinto, and Fabio Roli.
\newblock Evasion attacks against machine learning at test time.
\newblock {\em Lecture Notes in Computer Science}, 2013.

\bibitem{cai2018curadv}
Qi-Zhi Cai, Chang Liu, and Dawn Song.
\newblock Curriculum adversarial training.
\newblock In {\em Proceedings of the Twenty-Seventh International Joint
  Conference on Artificial Intelligence, {IJCAI-18}}, pages 3740--3747.
  International Joint Conferences on Artificial Intelligence Organization, 7
  2018.

\bibitem{Cohen18}
Mario Cohen, Taco S.~Geiger and Maurice Weiler.
\newblock A general theory of equivariant cnns on homogeneous spaces.
\newblock {\em arXiv preprint arXiv:1811.02017}, 2018.

\bibitem{Cohen16}
Taco~S. Cohen and Max Welling.
\newblock Group equivariant convolutional networks.
\newblock {\em In Proceedings of the International Conference on Machine
  Learning (ICML)}, 2016.

\bibitem{Cohen17}
Taco~S. Cohen and Max Welling.
\newblock Steerable {CNN}s.
\newblock {\em In International Conference on Learning Representations (ICLR)},
  2017.

\bibitem{Dieleman16}
Sander Dieleman, Jeffrey De~Fauw, and Koray Kavukcuoglu.
\newblock Exploiting cyclic symmetry in convolutional neural networks.
\newblock {\em In Proceedings of the International Conference on Machine
  Learning (ICML)}, 2016.

\bibitem{ding2019advertorch}
Gavin~Weiguang Ding, Luyu Wang, and Xiaomeng Jin.
\newblock {AdverTorch} v0.1: An adversarial robustness toolbox based on
  pytorch.
\newblock {\em arXiv preprint arXiv:1902.07623}, 2019.

\bibitem{Engstrom2019Exploring}
Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander
  Madry.
\newblock Exploring the landscape of spatial robustness.
\newblock {\em In Proceedings of the International Conference on Machine
  Learning (ICML)}, 2019.

\bibitem{Esteves18}
Carlos Esteves, Christine Allen-Blanchette, Xiaowei Zhou, and Kostas
  Daniilidis.
\newblock Polar transformer networks.
\newblock {\em In International Conference on Learning Representations (ICLR)},
  2018.

\bibitem{Eykholt18robust}
Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo~Li, Amir Rahmati, Chaowei
  Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song.
\newblock Robust physical-world attacks on deep learning visual classification.
\newblock In {\em The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2018.

\bibitem{fischer2020cert}
Marc Fischer, Maximilian Baader, and Martin Vechev.
\newblock Certified defense to image transformations via randomized smoothing.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem{Gens2014deep}
Robert Gens and Pedro~M Domingos.
\newblock Deep symmetry networks.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2014.

\bibitem{Goodfellow15}
Ian~J. Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock {\em In International Conference on Learning Representations (ICLR)},
  2015.

\bibitem{hacohen2019power}
Guy Hacohen and Daphna Weinshall.
\newblock On the power of curriculum learning in training deep networks.
\newblock In {\em In Proceedings of the International Conference on Machine
  Learning (ICML)}, 2019.

\bibitem{He16}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2016.

\bibitem{hendrycks2020many}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan
  Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob
  Steinhardt, and Justin Gilmer.
\newblock The many faces of robustness: A critical analysis of
  out-of-distribution generalization.
\newblock {\em arXiv preprint arXiv:2006.16241}, 2020.

\bibitem{hendrycks2018benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{hendrycks2019nae}
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.
\newblock Natural adversarial examples.
\newblock {\em arXiv preprint arXiv:1907.07174}, 2019.

\bibitem{jacobsen2018excessive}
Joern-Henrik Jacobsen, Jens Behrmann, Richard Zemel, and Matthias Bethge.
\newblock Excessive invariance causes adversarial vulnerability.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{KatzBDJK2017}
Guy Katz, Clark~W. Barrett, David~L. Dill, Kyle Julian, and Mykel~J.
  Kochenderfer.
\newblock Reluplex: An efficient smt solver for verifying deep neural networks.
\newblock In {\em CAV}, 2017.

\bibitem{Kondor18}
Risi Kondor and Shubhendu Trivedi.
\newblock On the generalization of equivariance and convolution in neural
  networks to the action of compact groups.
\newblock {\em In Proceedings of the International Conference on Machine
  Learning (ICML)}, 2018.

\bibitem{Laptev16}
Dmitry Laptev, Nikolay Savinov, Joachim~M. Buhmann, and Marc Pollefeys.
\newblock {TI}-pooling: transformation-invariant pooling for feature learning
  in convolutional neural networks.
\newblock In {\em The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2016.

\bibitem{Li17}
Junying Li, Zichen Yang, Haifeng Liu, and Deng Cai.
\newblock Deep rotation equivariant network.
\newblock {\em arXiv preprint arXiv:1705.08623}, 2017.

\bibitem{li2020provable}
Linyi Li, Maurice Weber, Xiaojun Xu, Luka Rimanic, Tao Xie, Ce~Zhang, and
  Bo~Li.
\newblock Provable robust learning based on transformation-specific smoothing,
  2020.

\bibitem{Madry18}
Aleksander Madry, Aleksandar~A Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock {\em In International Conference on Learning Representations (ICLR)},
  2018.

\bibitem{Marcos2018scale}
Diego Marcos, Benjamin Kellenberger, Sylvain Lobry, and Devis Tuia.
\newblock Scale equivariance in cnns with vector fields.
\newblock {\em arXiv preprint arXiv:1807.11783}, 2018.

\bibitem{Marcos17}
Diego Marcos, Michele Volpi, Nikos Komodakis, and Devis Tuia.
\newblock Rotation equivariant vector field networks.
\newblock {\em In International Conference on Computer Vision (ICCV)}, 2017.

\bibitem{mohapatra2020verify}
Jeet Mohapatra, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel.
\newblock Towards verifying robustness of neural networks against a family of
  semantic perturbations.
\newblock In {\em The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2020.

\bibitem{Dezfooli16}
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard.
\newblock Deepfool: a simple and accurate method to fool deep neural networks.
\newblock {\em In Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2016.

\bibitem{roth2006intro}
Ron Roth.
\newblock {\em Introduction to Coding Theory}.
\newblock Cambridge University Press, 2006.

\bibitem{ruoss2021efficient}
Anian Ruoss, Maximilian Baader, Mislav Balunović, and Martin Vechev.
\newblock Efficient certification of spatial robustness, 2021.

\bibitem{Simonyan14}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{SinhaND2018}
Aman Sinha, Hongseok Namkoong, and John~C. Duchi.
\newblock Certifying some distributional robustness with principled adversarial
  training.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2018.

\bibitem{Sosnovik2020scale}
Ivan Sosnovik, Michał Szmaja, and Arnold Smeulders.
\newblock Scale-equivariant steerable networks.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2020.

\bibitem{Szegedy13}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian~J. Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock {\em arXiv preprint arXiv:1312.6199}, 2013.

\bibitem{taori2020measuring}
Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht,
  and Ludwig Schmidt.
\newblock Measuring robustness to natural distribution shifts in image
  classification.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem{tramer2020fundamental}
Florian Tram{\`{e}}r, Jens Behrmann, Nicholas Carlini, Nicolas Papernot, and
  J{\"{o}}rn{-}Henrik Jacobsen.
\newblock Fundamental tradeoffs between invariance and sensitivity to
  adversarial perturbations.
\newblock In {\em In Proceedings of the International Conference on Machine
  Learning (ICML)}, 2020.

\bibitem{TramerBoneh2019}
Florian Tram{\`{e}}r and Dan Boneh.
\newblock Adversarial training and robustness for multiple perturbations.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem{Tsipras2019odds}
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
  Aleksander Madry.
\newblock Robustness may be at odds with accuracy.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{weinshall2018curriculum}
Daphna Weinshall, Gad Cohen, and Dan Amir.
\newblock Curriculum learning by transfer learning: Theory and experiments with
  deep networks.
\newblock In {\em In Proceedings of the International Conference on Machine
  Learning (ICML)}, 2018.

\bibitem{Worrall16}
D.~E. {Worrall}, S.~J. {Garbin}, D.~{Turmukhambetov}, and G.~J. {Brostow}.
\newblock Harmonic networks: Deep translation and rotation equivariance.
\newblock {\em arXiv preprint arXiv:1612.04642}, 2016.

\bibitem{Worrall2019deepscale}
Daniel Worrall and Max Welling.
\newblock Deep scale-spaces: Equivariance over scale.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{xiao2018spatially}
Chaowei Xiao, Jun-Yan Zhu, Bo~Li, Warren He, Mingyan Liu, and Dawn Song.
\newblock Spatially transformed adversarial examples.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2018.

\bibitem{YangRZSC20}
Yao{-}Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Russ~R. Salakhutdinov, and
  Kamalika Chaudhuri.
\newblock A closer look at accuracy vs. robustness.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem{zhang2019theoretically}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric~P. Xing, Laurent~El Ghaoui, and
  Michael~I. Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In {\em In Proceedings of the International Conference on Machine
  Learning (ICML)}, 2019.

\end{thebibliography}
