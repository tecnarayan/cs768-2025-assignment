\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amir et~al.(2020)Amir, Attias, Koren, Mansour, and
  Livni]{amir2020prediction}
Idan Amir, Idan Attias, Tomer Koren, Yishay Mansour, and Roi Livni.
\newblock Prediction with corrupted expert advice.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Atlas et~al.(1990)Atlas, Cohn, and Ladner]{atlas1990training}
Les~E Atlas, David~A Cohn, and Richard~E Ladner.
\newblock Training connectionist networks with queries and selective sampling.
\newblock In \emph{Advances in neural information processing systems}, 1990.

\bibitem[Audibert(2007)]{audibert2007progressive}
Jean-Yves Audibert.
\newblock Progressive mixture rules are deviation suboptimal.
\newblock \emph{Advances in Neural Information Processing Systems}, 2007.

\bibitem[Azoury and Warmuth(2001)]{azoury2001relative}
Katy~S Azoury and Manfred~K Warmuth.
\newblock Relative loss bounds for on-line density estimation with the
  exponential family of distributions.
\newblock \emph{Machine Learning}, 43\penalty0 (3):\penalty0 211--246, 2001.

\bibitem[Beygelzimer et~al.(2011)Beygelzimer, Langford, Li, Reyzin, and
  Schapire]{beygelzimer2011contextual}
Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire.
\newblock Contextual bandit algorithms with supervised learning guarantees.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 19--26, 2011.

\bibitem[Bubeck(2011)]{bubeck2011introduction}
S{\'e}bastien Bubeck.
\newblock Introduction to online optimization.
\newblock \emph{Lecture Notes}, 2011.

\bibitem[Cesa-Bianchi and Lugosi(2006)]{cesa2006prediction}
Nicolo Cesa-Bianchi and G{\'a}bor Lugosi.
\newblock \emph{Prediction, learning, and games}.
\newblock Cambridge university press, 2006.

\bibitem[Cesa-Bianchi et~al.(2003)Cesa-Bianchi, Conconi, and
  Gentile]{cesa2003learning}
Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile.
\newblock Learning probabilistic linear-threshold classifiers via selective
  sampling.
\newblock In \emph{Learning Theory and Kernel Machines}, pages 373--387.
  Springer, 2003.

\bibitem[Cesa-Bianchi et~al.(2004)Cesa-Bianchi, Conconi, and
  Gentile]{cesa2004generalization}
Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile.
\newblock On the generalization ability of on-line learning algorithms.
\newblock \emph{IEEE Transactions on Information Theory}, 50\penalty0
  (9):\penalty0 2050--2057, 2004.

\bibitem[Cesa-Bianchi et~al.(2006)Cesa-Bianchi, Gentile, Zaniboni, and
  Warmuth]{cesa2006worst}
Nicolo Cesa-Bianchi, Claudio Gentile, Luca Zaniboni, and Manfred Warmuth.
\newblock Worst-case analysis of selective sampling for linear classification.
\newblock \emph{Journal of Machine Learning Research}, 7\penalty0 (7), 2006.

\bibitem[Chen et~al.(2021)Chen, Luo, and Wei]{chen2021impossible}
Liyu Chen, Haipeng Luo, and Chen-Yu Wei.
\newblock Impossible tuning made possible: A new expert algorithm and its
  applications.
\newblock In \emph{Conference on Learning Theory}, 2021.

\bibitem[Cutkosky(2019)]{cutkosky2019artificial}
Ashok Cutkosky.
\newblock Artificial constraints and hints for unbounded online learning.
\newblock In \emph{Proceedings of the Thirty-Second Conference on Learning
  Theory}, volume~99, pages 874--894. PMLR, 25--28 Jun 2019.

\bibitem[{\VAN{Erven}{Van}{van}}~Erven
  et~al.(2021){\VAN{Erven}{Van}{van}}~Erven, Koolen, and {\VAN{Hoeven}{Van
  der}{van der}}~Hoeven]{VanErven2021metagrad}
Tim {\VAN{Erven}{Van}{van}}~Erven, Wouter~M. Koolen, and Dirk {\VAN{Hoeven}{Van
  der}{van der}}~Hoeven.
\newblock Metagrad: Adaptation using multiple learning rates in online
  learning.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (161):\penalty0 1--61, 2021.

\bibitem[Freund et~al.(1997)Freund, Seung, Shamir, and
  Tishby]{freund1997selective}
Yoav Freund, H~Sebastian Seung, Eli Shamir, and Naftali Tishby.
\newblock Selective sampling using the query by committee algorithm.
\newblock \emph{Machine learning}, 28\penalty0 (2):\penalty0 133--168, 1997.

\bibitem[Gaillard and Stoltz(2014)]{gaillard2014second}
Pierre Gaillard and Tim Stoltz, Gilles and{\VAN{Erven}{Van}{van}}~Erven.
\newblock A second-order bound with excess losses.
\newblock In \emph{Conference on Learning Theory}, 2014.

\bibitem[Hazan(2016)]{hazan2016introduction}
Elad Hazan.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends in Optimization}, 2\penalty0
  (3-4):\penalty0 157--325, 2016.

\bibitem[{\VAN{Hoeven}{Van der}{van
  der}}~Hoeven(2020)]{VanderHoeven2020exploiting}
Dirk {\VAN{Hoeven}{Van der}{van der}}~Hoeven.
\newblock Exploiting the surrogate gap in online multiclass classification.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[{\VAN{Hoeven}{Van der}{van der}}~Hoeven et~al.(2018){\VAN{Hoeven}{Van
  der}{van der}}~Hoeven, {\VAN{Erven}{Van}{van}}~Erven, and
  Kotlowski]{vanderhoeven2018many}
Dirk {\VAN{Hoeven}{Van der}{van der}}~Hoeven, Tim
  {\VAN{Erven}{Van}{van}}~Erven, and Wojciech Kotlowski.
\newblock The many faces of exponential weights in online learning.
\newblock In \emph{Conference On Learning Theory}, 2018.

\bibitem[{\VAN{Hoeven}{Van der}{van der}}~Hoeven et~al.(2021){\VAN{Hoeven}{Van
  der}{van der}}~Hoeven, Fusco, and Cesa-Bianchi]{VanderHoeven2021beyond}
Dirk {\VAN{Hoeven}{Van der}{van der}}~Hoeven, Federico Fusco, and Nicolo
  Cesa-Bianchi.
\newblock Beyond bandit feedback in online multiclass classification.
\newblock In \emph{Advances in neural information processing systems}, 2021.

\bibitem[Ito(2021)]{ito2021optimal}
Shinji Ito.
\newblock On optimal robustness to adversarial corruption in online decision
  problems.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Kakade and Tewari(2008)]{kakade2008generalization}
Sham~M Kakade and Ambuj Tewari.
\newblock On the generalization ability of online strongly convex programming
  algorithms.
\newblock \emph{Advances in Neural Information Processing Systems}, 2008.

\bibitem[Kanade et~al.(2022)Kanade, Rebeschini, and
  Vaskevicius]{kanade2022exponential}
Varun Kanade, Patrick Rebeschini, and Tomas Vaskevicius.
\newblock Exponential tail local {R}ademacher complexity risk bounds without
  the {B}ernstein condition.
\newblock \emph{arXiv preprint arXiv:2202.11461}, 2022.

\bibitem[Koolen and {\VAN{Erven}{Van}{van}}~Erven(2015)]{koolen2015second}
Wouter~M. Koolen and Tim {\VAN{Erven}{Van}{van}}~Erven.
\newblock Second-order quantile methods for experts and combinatorial games.
\newblock In \emph{Conference on Learning Theory}, 2015.

\bibitem[Lecu{\'e} and Mendelson(2009)]{lecue2009aggregation}
Guillaume Lecu{\'e} and Shahar Mendelson.
\newblock Aggregation via empirical risk minimization.
\newblock \emph{Probability theory and related fields}, 145\penalty0
  (3-4):\penalty0 591--613, 2009.

\bibitem[Lecu{\'e} and Rigollet(2014)]{lecue2014optimal}
Guillaume Lecu{\'e} and Philippe Rigollet.
\newblock Optimal learning with {Q}-aggregation.
\newblock \emph{The Annals of Statistics}, 42\penalty0 (1):\penalty0 211--224,
  2014.

\bibitem[Littlestone and Warmuth(1994)]{littlestone1994weighted}
Nick Littlestone and Manfred~K Warmuth.
\newblock The weighted majority algorithm.
\newblock \emph{Information and computation}, 108\penalty0 (2):\penalty0
  212--261, 1994.

\bibitem[Lykouris et~al.(2018)Lykouris, Mirrokni, and
  Paes~Leme]{lykouris2018stochastic}
Thodoris Lykouris, Vahab Mirrokni, and Renato Paes~Leme.
\newblock Stochastic bandits robust to adversarial corruptions.
\newblock In \emph{Proceedings of the 50th Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 114--122, 2018.

\bibitem[Meyer(2000)]{meyer2000matrix}
Carl~D Meyer.
\newblock \emph{Matrix {A}nalysis and Applied Linear Algebra}, volume~71.
\newblock Siam, 2000.

\bibitem[Mhammedi and Williamson(2018)]{mhammedi2018constant}
Zakaria Mhammedi and Robert~C Williamson.
\newblock Constant regret, generalized mixability, and mirror descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Mhammedi et~al.(2019)Mhammedi, Koolen, and
  {\VAN{Erven}{Van}{van}}~Erven]{mhammedi2019lipschitz}
Zakaria Mhammedi, Wouter~M Koolen, and Tim {\VAN{Erven}{Van}{van}}~Erven.
\newblock Lipschitz adaptivity with multiple learning rates in online learning.
\newblock In \emph{Conference on Learning Theory}, 2019.

\bibitem[Mourtada and Ga{\"\i}ffas(2019)]{mourtada2019optimality}
Jaouad Mourtada and St{\'e}phane Ga{\"\i}ffas.
\newblock On the optimality of the {H}edge algorithm in the stochastic regime.
\newblock \emph{Journal of Machine Learning Research}, 20:\penalty0 1--28,
  2019.

\bibitem[Mourtada et~al.(2022)Mourtada, Vaskevicius, and
  Zhivotovskiy]{mourtada2021distribution}
Jaouad Mourtada, Tomas Vaskevicius, and Nikita Zhivotovskiy.
\newblock Distribution-free robust linear regression.
\newblock \emph{Mathematical Statistics and Learning}, 2022.

\bibitem[Nemirovski(2000)]{nemirovski2000topics}
Arkadi Nemirovski.
\newblock Topics in non-parametric statistics.
\newblock \emph{Ecole dâ€™Et{\'e} de Probabilit{\'e}s de Saint-Flour},
  28:\penalty0 85, 2000.

\bibitem[Neu and Zhivotovskiy(2020)]{neu2020fast}
Gergely Neu and Nikita Zhivotovskiy.
\newblock Fast rates for online prediction with abstention.
\newblock In \emph{Conference on Learning Theory}, 2020.

\bibitem[Orabona(2019)]{orabona2019modern}
Francesco Orabona.
\newblock A modern introduction to online learning.
\newblock \emph{arXiv preprint arXiv:1912.13213}, 2019.

\bibitem[Orabona and Cesa-Bianchi(2011)]{orabona2011better}
Francesco Orabona and Nicolo Cesa-Bianchi.
\newblock Better algorithms for selective sampling.
\newblock In \emph{International conference on machine learning}, 2011.

\bibitem[Puchkin and Zhivotovskiy(2022)]{puchkin21a}
Nikita Puchkin and Nikita Zhivotovskiy.
\newblock Exponential savings in agnostic active learning through abstention.
\newblock \emph{IEEE Transactions on Information Theory}, 2022.

\bibitem[Rakhlin and Sridharan(2014)]{rakhlin2014online}
Alexander Rakhlin and Karthik Sridharan.
\newblock Online non-parametric regression.
\newblock In \emph{Proceedings of The 27th Conference on Learning Theory},
  volume~35 of \emph{Proceedings of Machine Learning Research}, pages
  1232--1264, 2014.

\bibitem[Rakhlin et~al.(2012)Rakhlin, Shamir, and Sridharan]{rakhlin2011making}
Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock In \emph{Proceedings of the 29th International Conference on Machine
  Learning}, ICML'12, page 1571â€“1578, 2012.

\bibitem[{\VAN{Rooij}{De}{de}}~Rooij et~al.(2014){\VAN{Rooij}{De}{de}}~Rooij,
  {\VAN{Erven}{Van}{van}}~Erven, Gr{\"u}nwald, and Koolen]{derooij2014follow}
Steven {\VAN{Rooij}{De}{de}}~Rooij, Tim {\VAN{Erven}{Van}{van}}~Erven, Peter~D.
  Gr{\"u}nwald, and Wouter~M. Koolen.
\newblock {F}ollow the {L}eader if you can, {H}edge if you must.
\newblock \emph{Journal of Machine Learning Research}, 15:\penalty0 1281--1316,
  2014.

\bibitem[Tsybakov(2003)]{tsybakov2003optimal}
Alexandre~B Tsybakov.
\newblock Optimal rates of aggregation.
\newblock In \emph{Learning theory and kernel machines}, pages 303--313.
  Springer, 2003.

\bibitem[Vovk(1990)]{vovk1990aggregating}
Volodimir~G Vovk.
\newblock Aggregating strategies.
\newblock In \emph{Proceedings of the 3rd Annual Workshop on Computational
  Learning Theory}, 1990.

\bibitem[Vovk(2001)]{vovk2001competitive}
Volodimir~G Vovk.
\newblock Competitive on-line statistics.
\newblock \emph{International Statistical Review}, 69\penalty0 (2):\penalty0
  213--248, 2001.

\bibitem[Wang et~al.(2020)Wang, Lu, and Zhang]{wang2020adaptivity}
Guanghui Wang, Shiyin Lu, and Lijun Zhang.
\newblock Adaptivity and optimality: A universal algorithm for online convex
  optimization.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 659--668.
  PMLR, 2020.

\bibitem[Wintenberger(2017)]{wintenberger2017optimal}
Olivier Wintenberger.
\newblock Optimal learning with {B}ernstein online aggregation.
\newblock \emph{Machine Learning}, 106\penalty0 (1):\penalty0 119--141, 2017.

\bibitem[Zhang et~al.(2018)Zhang, Yang, Zhou, et~al.]{zhang2018dynamic}
Lijun Zhang, Tianbao Yang, Zhi-Hua Zhou, et~al.
\newblock Dynamic regret of strongly adaptive methods.
\newblock In \emph{International conference on machine learning}, 2018.

\bibitem[Zimmert and Seldin(2021)]{Zimmert2021tsallis}
Julian Zimmert and Yevgeny Seldin.
\newblock Tsallis-inf: An optimal algorithm for stochastic and adversarial
  bandits.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (28):\penalty0 1--49, 2021.

\end{thebibliography}
