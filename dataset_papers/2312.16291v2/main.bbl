\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[AI@Meta(2024)]{llama3modelcard}
AI@Meta.
\newblock Llama 3.
\newblock 2024.
\newblock URL \url{https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}.

\bibitem[Ainslie et~al.(2023)Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy, Lebrón, and Sanghai]{ainslie2023gqa}
Ainslie, J., Lee-Thorp, J., de~Jong, M., Zemlyanskiy, Y., Lebrón, F., and Sanghai, S.
\newblock Gqa: Training generalized multi-query transformer models from multi-head checkpoints, 2023.

\bibitem[Black et~al.(2021)Black, Gao, Wang, Leahy, and Biderman]{gpt-neo}
Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S.
\newblock {GPT-Neo}: Large scale autoregressive language modeling with mesh-tensorflow, 2021.
\newblock URL \url{http://github.com/eleutherai/gpt-neo}.

\bibitem[Bolukbasi et~al.(2016)Bolukbasi, Chang, Zou, Saligrama, and Kalai]{bolukbasi2016man}
Bolukbasi, T., Chang, K.-W., Zou, J.~Y., Saligrama, V., and Kalai, A.~T.
\newblock Man is to computer programmer as woman is to homemaker? debiasing word embeddings.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Bricken et~al.(2023)Bricken, Templeton, Batson, Chen, Jermyn, Conerly, Turner, Anil, Denison, Askell, Lasenby, Wu, Kravec, Schiefer, Maxwell, Joseph, Hatfield-Dodds, Tamkin, Nguyen, McLean, Burke, Hume, Carter, Henighan, and Olah]{bricken2023monosemanticity}
Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., Turner, N., Anil, C., Denison, C., Askell, A., Lasenby, R., Wu, Y., Kravec, S., Schiefer, N., Maxwell, T., Joseph, N., Hatfield-Dodds, Z., Tamkin, A., Nguyen, K., McLean, B., Burke, J.~E., Hume, T., Carter, S., Henighan, T., and Olah, C.
\newblock Towards monosemanticity: Decomposing language models with dictionary learning.
\newblock \emph{Transformer Circuits Thread}, 2023.
\newblock https://transformer-circuits.pub/2023/monosemantic-features/index.html.

\bibitem[Brody et~al.(2023)Brody, Alon, and Yahav]{brody2023expressivity}
Brody, S., Alon, U., and Yahav, E.
\newblock On the expressivity role of layernorm in transformers' attention.
\newblock \emph{arXiv preprint arXiv:2305.02582}, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt3}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  1877--1901. Curran Associates, Inc., 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Conmy et~al.(2023)Conmy, Mavor-Parker, Lynch, Heimersheim, and Garriga-Alonso]{conmy2023towards}
Conmy, A., Mavor-Parker, A.~N., Lynch, A., Heimersheim, S., and Garriga-Alonso, A.
\newblock Towards automated circuit discovery for mechanistic interpretability.
\newblock \emph{arXiv preprint arXiv:2304.14997}, 2023.

\bibitem[Crowson(2021)]{mdmm}
Crowson, K.
\newblock mdmm, 2021.
\newblock URL \url{http://github.com/crowsonkb/mdmm}.

\bibitem[Cunningham et~al.(2023)Cunningham, Ewart, Riggs, Huben, and Sharkey]{cunningham2023sparse}
Cunningham, H., Ewart, A., Riggs, L., Huben, R., and Sharkey, L.
\newblock Sparse autoencoders find highly interpretable features in language models, 2023.

\bibitem[Dar et~al.(2023)Dar, Geva, Gupta, and Berant]{dar-etal-2023-analyzing}
Dar, G., Geva, M., Gupta, A., and Berant, J.
\newblock Analyzing transformers in embedding space.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  16124--16170, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.893}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.893}.

\bibitem[Elazar et~al.(2021)Elazar, Ravfogel, Jacovi, and Goldberg]{amnesicProbing}
Elazar, Y., Ravfogel, S., Jacovi, A., and Goldberg, Y.
\newblock {Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals}.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 9:\penalty0 160--175, 03 2021.
\newblock ISSN 2307-387X.
\newblock \doi{10.1162/tacl_a_00359}.
\newblock URL \url{https://doi.org/10.1162/tacl\_a\_00359}.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann, Askell, Bai, Chen, Conerly, DasSarma, Drain, Ganguli, Hatfield-Dodds, Hernandez, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan, McCandlish, and Olah]{elhage2021mathematical}
Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma, N., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 2021.
\newblock https://transformer-circuits.pub/2021/framework/index.html.

\bibitem[Elhage et~al.(2022)Elhage, Hume, Olsson, Nanda, Henighan, Johnston, ElShowk, Joseph, DasSarma, Mann, Hernandez, Askell, Ndousse, Jones, Drain, Chen, Bai, Ganguli, Lovitt, Hatfield-Dodds, Kernion, Conerly, Kravec, Fort, Kadavath, Jacobson, Tran-Johnson, Kaplan, Clark, Brown, McCandlish, Amodei, and Olah]{elhage2022solu}
Elhage, N., Hume, T., Olsson, C., Nanda, N., Henighan, T., Johnston, S., ElShowk, S., Joseph, N., DasSarma, N., Mann, B., Hernandez, D., Askell, A., Ndousse, K., Jones, A., Drain, D., Chen, A., Bai, Y., Ganguli, D., Lovitt, L., Hatfield-Dodds, Z., Kernion, J., Conerly, T., Kravec, S., Fort, S., Kadavath, S., Jacobson, J., Tran-Johnson, E., Kaplan, J., Clark, J., Brown, T., McCandlish, S., Amodei, D., and Olah, C.
\newblock Softmax linear units.
\newblock \emph{Transformer Circuits Thread}, 2022.
\newblock https://transformer-circuits.pub/2022/solu/index.html.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, Presser, and Leahy]{gao2020pile}
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C.
\newblock The pile: An 800gb dataset of diverse text for language modeling, 2020.

\bibitem[Goldowsky-Dill et~al.(2023)Goldowsky-Dill, MacLeod, Sato, and Arora]{goldowsky2023localizing}
Goldowsky-Dill, N., MacLeod, C., Sato, L., and Arora, A.
\newblock Localizing model behavior with path patching.
\newblock \emph{arXiv preprint arXiv:2304.05969}, 2023.

\bibitem[Gurnee et~al.(2023)Gurnee, Nanda, Pauly, Harvey, Troitskii, and Bertsimas]{gurnee2023finding}
Gurnee, W., Nanda, N., Pauly, M., Harvey, K., Troitskii, D., and Bertsimas, D.
\newblock Finding neurons in a haystack: Case studies with sparse probing, 2023.

\bibitem[Hernandez et~al.(2023)Hernandez, Sharma, Haklay, Meng, Wattenberg, Andreas, Belinkov, and Bau]{hernandez2023linearity}
Hernandez, E., Sharma, A.~S., Haklay, T., Meng, K., Wattenberg, M., Andreas, J., Belinkov, Y., and Bau, D.
\newblock Linearity of relation decoding in transformer language models, 2023.

\bibitem[Jacovi et~al.(2021)Jacovi, Swayamdipta, Ravfogel, Elazar, Choi, and Goldberg]{jacovi-etal-2021-contrastive}
Jacovi, A., Swayamdipta, S., Ravfogel, S., Elazar, Y., Choi, Y., and Goldberg, Y.
\newblock Contrastive explanations for model interpretability.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pp.\  1597--1611, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.120}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.120}.

\bibitem[Kim et~al.(2018)Kim, Wattenberg, Gilmer, Cai, Wexler, Viegas, and sayres]{pmlr-v80-kim18d}
Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., and sayres, R.
\newblock Interpretability beyond feature attribution: Quantitative testing with concept activation vectors ({TCAV}).
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th International Conference on Machine Learning}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\  2668--2677. PMLR, 10--15 Jul 2018.
\newblock URL \url{https://proceedings.mlr.press/v80/kim18d.html}.

\bibitem[Lepori et~al.(2023)Lepori, Serre, and Pavlick]{lepori2023uncovering}
Lepori, M.~A., Serre, T., and Pavlick, E.
\newblock Uncovering intermediate variables in transformers using circuit probing, 2023.

\bibitem[Li et~al.(2023)Li, Patel, Viégas, Pfister, and Wattenberg]{li2023inferencetime}
Li, K., Patel, O., Viégas, F., Pfister, H., and Wattenberg, M.
\newblock Inference-time intervention: Eliciting truthful answers from a language model, 2023.

\bibitem[Mathwin et~al.(2023)Mathwin, Corlouer, Kran, Barez, and Nanda]{genderCircuits}
Mathwin, C., Corlouer, G., Kran, E., Barez, F., and Nanda, N.
\newblock Identifying a preliminary circuit for predicting gendered pronouns in gpt-2 small.
\newblock \emph{Apart Research Alignment Jam \#4 (Mechanistic Interpretability)}, 2023.
\newblock URL \url{https://cmathw.itch.io/identifying-a-preliminary-circuit-for-predicting-gendered-pronouns-in-gpt-2-smal}.

\bibitem[Millidge \& Black(2023)Millidge and Black]{SVD}
Millidge, B. and Black, S.
\newblock The singular value decompositions of transformer weight matrices are highly interpretable.
\newblock 2023.
\newblock URL \url{www.lesswrong.com/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight}.

\bibitem[Nanda(2022)]{mechInterpGlossary}
Nanda, N.
\newblock A comprehensive mechanistic interpretability explainer \& glossary, 2022.
\newblock URL \url{https://www.neelnanda.io/mechanistic-interpretability/glossary}.

\bibitem[Nanda et~al.(2023)Nanda, Olah, Olsson, Elhage, and Tristan]{attributionPatching}
Nanda, N., Olah, C., Olsson, C., Elhage, N., and Tristan, H.
\newblock Attribution patching: Activation patching at industrial scale.
\newblock 2023.
\newblock URL \url{https://www.neelnanda.io/mechanistic-interpretability/attribution-patching}.

\bibitem[Olah(2022)]{mechInterpNote}
Olah, C.
\newblock Mechanistic interpretability, variables, and the importance of interpretable bases, 2022.
\newblock URL \url{https://transformer-circuits.pub/2022/mech-interp-essay/index.html}.

\bibitem[Olah et~al.(2018)Olah, Satyanarayan, Johnson, Carter, Schubert, Ye, and Mordvintsev]{olah2018the}
Olah, C., Satyanarayan, A., Johnson, I., Carter, S., Schubert, L., Ye, K., and Mordvintsev, A.
\newblock The building blocks of interpretability.
\newblock \emph{Distill}, 2018.
\newblock \doi{10.23915/distill.00010}.
\newblock https://distill.pub/2018/building-blocks.

\bibitem[Olah et~al.(2020)Olah, Cammarata, Schubert, Goh, Petrov, and Carter]{olah2020zoom}
Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., and Carter, S.
\newblock Zoom in: An introduction to circuits.
\newblock \emph{Distill}, 2020.
\newblock \doi{10.23915/distill.00024.001}.
\newblock https://distill.pub/2020/circuits/zoom-in.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Park et~al.(2023)Park, Choe, and Veitch]{park2023linear}
Park, K., Choe, Y.~J., and Veitch, V.
\newblock The linear representation hypothesis and the geometry of large language models, 2023.

\bibitem[Petersen \& Pedersen(2012)Petersen and Pedersen]{petersen2012matrix}
Petersen, K. and Pedersen, M.
\newblock The matrix cookbook, version 2012/11/15.
\newblock \emph{Technical Univ. Denmark, Kongens Lyngby, Denmark, Tech. Rep}, 3274, 2012.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Simonyan et~al.(2013)Simonyan, Vedaldi, and Zisserman]{simonyan2013deep}
Simonyan, K., Vedaldi, A., and Zisserman, A.
\newblock Deep inside convolutional networks: Visualising image classification models and saliency maps.
\newblock \emph{arXiv preprint arXiv:1312.6034}, 2013.

\bibitem[Syed et~al.(2023)Syed, Rager, and Conmy]{syed2023attribution}
Syed, A., Rager, C., and Conmy, A.
\newblock Attribution patching outperforms automated circuit discovery, 2023.

\bibitem[Tigges et~al.(2023)Tigges, Hollinsworth, Geiger, and Nanda]{tigges2023linear}
Tigges, C., Hollinsworth, O.~J., Geiger, A., and Nanda, N.
\newblock Linear representations of sentiment in large language models.
\newblock \emph{arXiv preprint arXiv:2310.15154}, 2023.

\bibitem[{UCI Machine Learning Repository}(2020)]{genderByName}
{UCI Machine Learning Repository}.
\newblock {Gender by Name}.
\newblock UCI Machine Learning Repository, 2020.
\newblock {DOI}: https://doi.org/10.24432/C55G7X.

\bibitem[Vig et~al.(2020)Vig, Gehrmann, Belinkov, Qian, Nevo, Singer, and Shieber]{vig2020investigating}
Vig, J., Gehrmann, S., Belinkov, Y., Qian, S., Nevo, D., Singer, Y., and Shieber, S.
\newblock Investigating gender bias in language models using causal mediation analysis.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 12388--12401, 2020.

\bibitem[Wallace et~al.(2019)Wallace, Tuyls, Wang, Subramanian, Gardner, and Singh]{wallace-etal-2019-allennlp}
Wallace, E., Tuyls, J., Wang, J., Subramanian, S., Gardner, M., and Singh, S.
\newblock {A}llen{NLP} interpret: A framework for explaining predictions of {NLP} models.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations}, pp.\  7--12, Hong Kong, China, November 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-3002}.
\newblock URL \url{https://aclanthology.org/D19-3002}.

\bibitem[Wang et~al.(2022)Wang, Variengien, Conmy, Shlegeris, and Steinhardt]{wang2022interpretability}
Wang, K., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J.
\newblock Interpretability in the wild: a circuit for indirect object identification in gpt-2 small, 2022.

\bibitem[Winsor(2022)]{layernorm}
Winsor, E.
\newblock Re-examining layernorm.
\newblock 2022.
\newblock URL \url{https://www.lesswrong.com/posts/jfG6vdJZCwTQmG7kb/re-examining-layernorm}.

\bibitem[Wu et~al.(2024)Wu, Geiger, Potts, and Goodman]{wu2024interpretability}
Wu, Z., Geiger, A., Potts, C., and Goodman, N.~D.
\newblock Interpretability at scale: Identifying causal mechanisms in alpaca, 2024.

\bibitem[Yu et~al.(2023)Yu, Merullo, and Pavlick]{yu2023characterizing}
Yu, Q., Merullo, J., and Pavlick, E.
\newblock Characterizing mechanisms for factual recall in language models, 2023.

\bibitem[Zhang \& Sennrich(2019)Zhang and Sennrich]{zhang2019root}
Zhang, B. and Sennrich, R.
\newblock Root mean square layer normalization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\end{thebibliography}
