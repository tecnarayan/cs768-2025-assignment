\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amari(1998)]{amari1998natural}
Shun-ichi Amari.
\newblock Natural gradient works efficiently in learning.
\newblock \emph{Neural computation}, 10\penalty0 (2):\penalty0 251--276, 1998.

\bibitem[Heskes(2000)]{heskes2000natural}
Tom Heskes.
\newblock On “natural” learning and pruning in multilayered perceptrons.
\newblock \emph{Neural Computation}, 12\penalty0 (4):\penalty0 881--901, 2000.

\bibitem[Martens and Grosse(2015)]{martens2015optimizing}
James Martens and Roger Grosse.
\newblock Optimizing neural networks with {K}ronecker-factored approximate
  curvature.
\newblock In \emph{Proceedings of International Conference on Machine Learning
  (ICML)}, pages 2408--2417, 2015.

\bibitem[Grosse and Martens(2016)]{grosse2016kronecker}
Roger Grosse and James Martens.
\newblock A {K}ronecker-factored approximate {F}isher matrix for convolution
  layers.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  573--582, 2016.

\bibitem[Roux et~al.(2008)Roux, Manzagol, and Bengio]{roux2008topmoumoute}
Nicolas~L Roux, Pierre-Antoine Manzagol, and Yoshua Bengio.
\newblock Topmoumoute online natural gradient algorithm.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, pages 849--856, 2008.

\bibitem[Ollivier(2015)]{ollivier2015riemannian}
Yann Ollivier.
\newblock Riemannian metrics for neural networks {I}: feedforward networks.
\newblock \emph{Information and Inference: A Journal of the IMA}, 4\penalty0
  (2):\penalty0 108--153, 2015.

\bibitem[Amari et~al.(2019)Amari, Karakida, and Oizumi]{amari2018fisher}
Shun-ichi Amari, Ryo Karakida, and Masafumi Oizumi.
\newblock Fisher information and natural gradient learning of random deep
  networks.
\newblock In \emph{Proceedings of International Conference on Artificial
  Intelligence and Statistics (AISTATS)}, pages 694--702, 2019.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, pages 8571--8580, 2018.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Sohl-Dickstein, and
  Pennington]{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel~S Schoenholz, Yasaman Bahri, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2019.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019exact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Park et~al.(2000)Park, Amari, and Fukumizu]{park2000}
Hyeyoung Park, Shun-ichi Amari, and Kenji Fukumizu.
\newblock Adaptive natural gradient learning algorithms for various stochastic
  models.
\newblock \emph{Neural Networks}, 13\penalty0 (7):\penalty0 755--764, 2000.

\bibitem[Pascanu and Bengio(2014)]{pascanu2013}
Razvan Pascanu and Yoshua Bengio.
\newblock Revisiting natural gradient for deep networks.
\newblock \emph{ICLR 2014, arXiv:1301.3584}, 2014.

\bibitem[Rattray et~al.(1998)Rattray, Saad, and Amari]{saad1998}
Magnus Rattray, David Saad, and Shun-ichi Amari.
\newblock Natural gradient descent for on-line learning.
\newblock \emph{Physical review letters}, 81\penalty0 (24):\penalty0 5461,
  1998.

\bibitem[Cousseau et~al.(2008)Cousseau, Ozeki, and Amari]{cousseau2008dynamics}
Florent Cousseau, Tomoko Ozeki, and Shun-ichi Amari.
\newblock Dynamics of learning in multilayer perceptrons near singularities.
\newblock \emph{IEEE Transactions on Neural Networks}, 19\penalty0
  (8):\penalty0 1313--1328, 2008.

\bibitem[Bernacchia et~al.(2018)Bernacchia, Lengyel, and
  Hennequin]{bernacchia2018exact}
Alberto Bernacchia, M{\'a}t{\'e} Lengyel, and Guillaume Hennequin.
\newblock Exact natural gradient in deep linear networks and its application to
  the nonlinear case.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 5941--5950, 2018.

\bibitem[Zhang et~al.(2019)Zhang, Martens, and Grosse]{zhang2019fast}
Guodong Zhang, James Martens, and Roger~B Grosse.
\newblock Fast convergence of natural gradient descent for over-parameterized
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 8080--8091, 2019.

\bibitem[Cai et~al.(2019)Cai, Gao, Hou, Chen, Wang, He, Zhang, and
  Wang]{cai2019gram}
Tianle Cai, Ruiqi Gao, Jikai Hou, Siyu Chen, Dong Wang, Di~He, Zhihua Zhang,
  and Liwei Wang.
\newblock A {G}ram-{G}auss-{N}ewton method learning overparameterized deep
  neural networks for regression problems.
\newblock \emph{arXiv preprint arXiv:1905.11675}, 2019.

\bibitem[Kunstner et~al.(2019)Kunstner, Balles, and
  Hennig]{kunstner2019limitations}
Frederik Kunstner, Lukas Balles, and Philipp Hennig.
\newblock Limitations of the empirical {F}isher approximation.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2019.

\bibitem[Martens(2020)]{JMLR:v21:17-678}
James Martens.
\newblock New insights and perspectives on the natural gradient method.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (146):\penalty0 1--76, 2020.

\bibitem[Karakida et~al.(2019{\natexlab{a}})Karakida, Akaho, and
  Amari]{karakida2018universal}
Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari.
\newblock Universal statistics of {F}isher information in deep neural networks:
  Mean field approach.
\newblock In \emph{Proceedings of International Conference on Artificial
  Intelligence and Statistics (AISTATS)}, pages 1032--1041, 2019{\natexlab{a}}.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Orr, and
  M{\"u}ller]{lecun1998efficient}
Yann LeCun, L{\'e}on Bottou, Genevieve~B Orr, and Klaus-Robert M{\"u}ller.
\newblock Efficient backprop.
\newblock In \emph{Neural networks: Tricks of the trade}, pages 9--50.
  Springer, 1998.

\bibitem[Petersen and Pedersen(2012)]{cookbook}
K.~B. Petersen and M.~S. Pedersen.
\newblock The {M}atrix {C}ookbook.
\newblock 2012.

\bibitem[Thomas(2014)]{thomas2014genga}
Philip Thomas.
\newblock {G}e{NGA}: A generalization of natural gradient ascent with positive
  and negative convergence results.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  1575--1583, 2014.

\bibitem[Rudner et~al.(2019)Rudner, Wenzel, Teh, and Gal]{rudner2019}
Tim G.~J. Rudner, Florian Wenzel, Yee~Whye Teh, and Yarin Gal.
\newblock The natural neural tangent kernel: Neural network training dynamics
  under natural gradient descent.
\newblock In \emph{4th workshop on Bayesian Deep Learning (NeurIPS 2019)},
  2019.

\bibitem[Schoenholz et~al.(2017)Schoenholz, Gilmer, Ganguli, and
  Sohl-Dickstein]{schoenholz2016}
Samuel~S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein.
\newblock Deep information propagation.
\newblock \emph{ICLR2017 arXiv:1611.01232}, 2017.

\bibitem[Yang and Schoenholz(2017)]{yang2017}
Greg Yang and Samuel Schoenholz.
\newblock Mean field residual networks: On the edge of chaos.
\newblock In \emph{Proceedings of Advances in Neural Information Processing
  Systems (NIPS)}, pages 2865--2873. 2017.

\bibitem[Xiao et~al.(2018)Xiao, Bahri, Sohl-Dickstein, Schoenholz, and
  Pennington]{xiao2018dynamical}
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel~S Schoenholz, and
  Jeffrey Pennington.
\newblock Dynamical isometry and a mean field theory of {CNN}s: How to train
  10,000-layer vanilla convolutional neural networks.
\newblock In \emph{Proceedings of International Conference on Machine Learning
  (ICML)}, pages 5393--5402, 2018.

\bibitem[Karakida et~al.(2019{\natexlab{b}})Karakida, Akaho, and
  Amari]{karakida2019normalization}
Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari.
\newblock The normalization method for alleviating pathological sharpness in
  wide neural networks.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2019{\natexlab{b}}.

\bibitem[Yang(2020)]{yang2020tensor}
Greg Yang.
\newblock Tensor programs {II}: Neural tangent kernel for any architecture.
\newblock \emph{arXiv preprint arXiv:2006.14548}, 2020.

\bibitem[Willett and Wong(1965)]{willett1965discrete}
D~Willett and JSW Wong.
\newblock On the discrete analogues of some generalizations of {G}ronwall's
  inequality.
\newblock \emph{Monatshefte f{\"u}r Mathematik}, 69\penalty0 (4):\penalty0
  362--367, 1965.

\bibitem[Noschese et~al.(2013)Noschese, Pasquini, and
  Reichel]{noschese2013tridiagonal}
Silvia Noschese, Lionello Pasquini, and Lothar Reichel.
\newblock Tridiagonal {T}oeplitz matrices: properties and novel applications.
\newblock \emph{Numerical linear algebra with applications}, 20\penalty0
  (2):\penalty0 302--326, 2013.

\bibitem[da~Fonseca and Petronilho(2001)]{da2001explicit}
CM~da~Fonseca and J~Petronilho.
\newblock Explicit inverses of some tridiagonal matrices.
\newblock \emph{Linear Algebra and Its Applications}, 1\penalty0
  (325):\penalty0 7--21, 2001.

\end{thebibliography}
