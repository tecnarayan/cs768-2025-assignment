\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aitchison et~al.(2021)Aitchison, Yang, and Ober]{AitchisonDKP}
L.~Aitchison, A.~Yang, and S.~W. Ober.
\newblock Deep kernel processes.
\newblock In M.~Meila and T.~Zhang, editors, \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 130--140. PMLR, 18--24
  Jul 2021.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{Blundell2015a}
C.~Blundell, J.~Cornebise, K.~Kavukcuoglu, and D.~Wierstra.
\newblock Weight uncertainty in neural networks.
\newblock In \emph{Proceedings of the 32nd International Conference on
  International Conference on Machine Learning - Volume 37}, ICML'15, page
  1613–1622. JMLR.org, 2015.

\bibitem[Brea et~al.(2019)Brea, Simsek, Illing, and
  Gerstner]{Brea2019WeightspaceSI}
J.~Brea, B.~Simsek, B.~Illing, and W.~Gerstner.
\newblock Weight-space symmetry in deep networks gives rise to permutation
  saddles, connected by equal-loss valleys across the loss landscape.
\newblock \emph{ArXiv}, abs/1907.02911, 2019.

\bibitem[Burt et~al.(2021)Burt, Ober, Garriga-Alonso, and van~der
  Wilk]{Burt2021Functional}
D.~R. Burt, S.~W. Ober, A.~Garriga-Alonso, and M.~van~der Wilk.
\newblock Understanding variational inference in function-space.
\newblock In \emph{Third Symposium on Advances in Approximate {B}ayesian
  Inference}, 2021.

\bibitem[Coker et~al.(2022)Coker, Bruinsma, Burt, Pan, and
  Doshi-Velez]{Coker2022WideMeanField}
B.~Coker, W.~P. Bruinsma, D.~R. Burt, W.~Pan, and F.~Doshi-Velez.
\newblock Wide mean-field variational {B}ayesian neural networks ignore the
  data.
\newblock In \emph{Proceedings of the 25th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, 2022.

\bibitem[Dawid(1979)]{Dawid1979Conditional}
A.~P. Dawid.
\newblock Conditional independence in statistical theory.
\newblock \emph{Journal of the Royal Statistical Society. Series B
  (Methodological)}, 41\penalty0 (1):\penalty0 1--31, 1979.
\newblock ISSN 00359246.

\bibitem[Dusenberry et~al.(2020)Dusenberry, Jerfel, Wen, Ma, Snoek, Heller,
  Lakshminarayanan, and Tran]{Dusenberry2020Rank1Factors}
M.~W. Dusenberry, G.~Jerfel, Y.~Wen, Y.-A. Ma, J.~Snoek, K.~Heller,
  B.~Lakshminarayanan, and D.~Tran.
\newblock Efficient and scalable {B}ayesian neural nets with rank-1 factors.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, ICML'20. JMLR.org, 2020.

\bibitem[Farquhar et~al.(2020)Farquhar, Smith, and Gal]{Farquhar2020Depth}
S.~Farquhar, L.~Smith, and Y.~Gal.
\newblock Liberty or depth: Deep {B}ayesian neural nets do not need complex
  weight posterior approximations.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 4346--4357. Curran Associates, Inc., 2020.

\bibitem[Flam-Shepherd(2017)]{FlamShepherd2017MappingGP}
D.~Flam-Shepherd.
\newblock Mapping gaussian process priors to {B}ayesian neural networks.
\newblock In \emph{{B}ayesian Deep Learning NeurIPS workshop}, 2017.

\bibitem[Foong et~al.(2020)Foong, Burt, Li, and Turner]{Foong2020}
A.~Foong, D.~Burt, Y.~Li, and R.~Turner.
\newblock On the expressiveness of approximate inference in {B}ayesian neural
  networks.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 15897--15908. Curran Associates, Inc., 2020.

\bibitem[Gelfand and Sahu(1999)]{Gelfand1999identifiability}
A.~E. Gelfand and S.~K. Sahu.
\newblock Identifiability, improper priors, and gibbs sampling for generalized
  linear models.
\newblock \emph{Journal of the American Statistical Association}, 94\penalty0
  (445):\penalty0 247--253, 1999.

\bibitem[Gelman et~al.(2017)Gelman, Simpson, and
  Betancourt]{Gelman2017PriorLikelihood}
A.~Gelman, D.~Simpson, and M.~Betancourt.
\newblock The prior can often only be understood in the context of the
  likelihood.
\newblock \emph{Entropy}, 19\penalty0 (10), 2017.
\newblock ISSN 1099-4300.
\newblock \doi{10.3390/e19100555}.

\bibitem[Ghosh et~al.(2018)Ghosh, Yao, and Doshi-Velez]{Ghosh2018Horseshoe}
S.~Ghosh, J.~Yao, and F.~Doshi-Velez.
\newblock Structured variational learning of {B}ayesian neural networks with
  horseshoe priors.
\newblock In J.~Dy and A.~Krause, editors, \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pages 1744--1753. PMLR, 10--15 Jul 2018.

\bibitem[Herbrich(2002)]{herbrich2002learningkernelclassifiers}
R.~Herbrich.
\newblock \emph{Learning Kernel Classifiers: Theory and Algorithms}.
\newblock The MIT Press, 2nd edition edition, 2002.

\bibitem[Hinton and van Camp(1993)]{Hinton1993a}
G.~E. Hinton and D.~van Camp.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In \emph{Proceedings of the Sixth Annual Conference on Computational
  Learning Theory}, COLT '93, page 5–13. Association for Computing Machinery,
  1993.
\newblock ISBN 0897916115.
\newblock \doi{10.1145/168304.168306}.

\bibitem[Izmailov et~al.(2021)Izmailov, Vikram, Hoffman, and
  Wilson]{Izmailov2021What}
P.~Izmailov, S.~Vikram, M.~D. Hoffman, and A.~G.~G. Wilson.
\newblock What are {B}ayesian neural network posteriors really like?
\newblock In M.~Meila and T.~Zhang, editors, \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 4629--4640. PMLR,
  18--24 Jul 2021.

\bibitem[Kendall and Gal(2017)]{Kendall2017}
A.~Kendall and Y.~Gal.
\newblock What uncertainties do we need in {B}ayesian deep learning for
  computer vision?
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Kurle et~al.(2020)Kurle, Cseke, Klushyn, van~der Smagt, and
  G{\"{u}}nnemann]{Kurle2020LLL}
R.~Kurle, B.~Cseke, A.~Klushyn, P.~van~der Smagt, and S.~G{\"{u}}nnemann.
\newblock Continual learning with {B}ayesian neural networks for non-stationary
  data.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.

\bibitem[Kurle et~al.(2021)Kurle, Januschowski, Gasthaus, and
  Wang]{Kurle2021BDL}
R.~Kurle, T.~Januschowski, J.~Gasthaus, and B.~Wang.
\newblock On symmetries in variational {B}ayesian neural nets.
\newblock In \emph{{B}ayesian Deep Learning NeurIPS workshop}, 2021.

\bibitem[Ma et~al.(2019)Ma, Li, and Hernandez-Lobato]{Ma2019Implicit}
C.~Ma, Y.~Li, and J.~M. Hernandez-Lobato.
\newblock Variational implicit processes.
\newblock In K.~Chaudhuri and R.~Salakhutdinov, editors, \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pages 4222--4233. PMLR,
  09--15 Jun 2019.

\bibitem[Mishkin et~al.(2018)Mishkin, Kunstner, Nielsen, Schmidt, and
  Khan]{Mishkin2018LowRankCov}
A.~Mishkin, F.~Kunstner, D.~Nielsen, M.~W. Schmidt, and M.~E. Khan.
\newblock Slang: Fast structured covariance approximations for {B}ayesian deep
  learning with natural gradient.
\newblock In \emph{NeurIPS}, pages 6248--6258, 2018.

\bibitem[Moore(2016)]{moore2016symvi}
D.~A. Moore.
\newblock Symmetrized variational inference.
\newblock \emph{NIPS Workshop on Advances in Approximate {B}ayesian Inference},
  December 2016.

\bibitem[Nguyen et~al.(2018)Nguyen, Li, Bui, and Turner]{Nguyen2018VCL}
C.~V. Nguyen, Y.~Li, T.~D. Bui, and R.~E. Turner.
\newblock Variational continual learning.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Nishihara et~al.(2013)Nishihara, Minka, and
  Tarlow]{Nishihara2013DetectingPS}
R.~Nishihara, T.~P. Minka, and D.~Tarlow.
\newblock Detecting parameter symmetries in probabilistic models.
\newblock \emph{arXiv: Machine Learning}, 2013.

\bibitem[Poirier(1998)]{Poirier1998revising}
D.~J. Poirier.
\newblock Revising beliefs in nonidentified models.
\newblock \emph{Econometric Theory}, 14\penalty0 (4):\penalty0 483--509, 1998.

\bibitem[Pourzanjani et~al.(2017)Pourzanjani, Jiang, and
  Petzold]{pourzanjani2017identifiability}
A.~Pourzanjani, R.~M. Jiang, and L.~Petzold.
\newblock Improving the identifiability of neural networks for {B}ayesian
  inference.
\newblock In \emph{{B}ayesian Deep Learning NeurIPS workshop}, 2017.

\bibitem[Prakasa~Rao(1992)]{Rao1992identifiability}
B.~L.~S. Prakasa~Rao.
\newblock \emph{Identifiability in Stochastic Models}.
\newblock Academic Press, Oxford, 1992.

\bibitem[Sun et~al.(2019)Sun, Zhang, Shi, and Grosse]{Sun2019}
S.~Sun, G.~Zhang, J.~Shi, and R.~B. Grosse.
\newblock Functional variational {B}ayesian neural networks.
\newblock \emph{ArXiv}, abs/1903.05779, 2019.

\bibitem[Sussmann(1992)]{Sussmann1992Permutation}
H.~J. Sussmann.
\newblock Uniqueness of the weights for minimal feedforward nets with a given
  input-output map.
\newblock \emph{Neural Networks}, 5\penalty0 (4):\penalty0 589--593, 1992.
\newblock ISSN 0893-6080.
\newblock \doi{https://doi.org/10.1016/S0893-6080(05)80037-1}.

\bibitem[Swiatkowski et~al.(2020)Swiatkowski, Roth, Veeling, Tran, Dillon,
  Snoek, Mandt, Salimans, Jenatton, and Nowozin]{Swiatkowski2020}
J.~Swiatkowski, K.~Roth, B.~S. Veeling, L.~Tran, J.~V. Dillon, J.~Snoek,
  S.~Mandt, T.~Salimans, R.~Jenatton, and S.~Nowozin.
\newblock The k-tied normal distribution: A compact parameterization of
  gaussian mean field posteriors in {B}ayesian neural networks.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, ICML'20. JMLR.org, 2020.

\bibitem[Tomczak et~al.(2020)Tomczak, Swaroop, and Turner]{Tomczak2020LowRank}
M.~Tomczak, S.~Swaroop, and R.~Turner.
\newblock Efficient low rank gaussian variational inference for neural
  networks.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 4610--4622. Curran Associates, Inc., 2020.

\bibitem[Tran et~al.(2022)Tran, Rossi, Milios, and Filippone]{Tran2022GPPrior}
B.-H. Tran, S.~Rossi, D.~Milios, and M.~Filippone.
\newblock All you need is a good functional prior for {B}ayesian deep learning.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (74):\penalty0 1--56, 2022.

\bibitem[Trippe and Turner(2018)]{Trippe2017}
B.~Trippe and R.~Turner.
\newblock Overpruning in variational {B}ayesian neural networks.
\newblock \emph{arXiv preprint arXiv:1801.06230}, 2018.

\bibitem[Wainwright et~al.(2008)Wainwright, Jordan,
  et~al.]{wainwright2008graphical}
M.~J. Wainwright, M.~I. Jordan, et~al.
\newblock Graphical models, exponential families, and variational inference.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  1\penalty0 (1--2):\penalty0 1--305, 2008.

\bibitem[Wang et~al.(2021)Wang, Blei, and Cunningham]{Wang2021NonIdent}
Y.~Wang, D.~Blei, and J.~P. Cunningham.
\newblock Posterior collapse and latent variable non-identifiability.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W.
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 5443--5455. Curran Associates, Inc., 2021.

\bibitem[Wang et~al.(2019)Wang, Ren, Zhu, and Zhang]{Wang2018function}
Z.~Wang, T.~Ren, J.~Zhu, and B.~Zhang.
\newblock Function space particle optimization for {B}ayesian neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Wenzel et~al.(2020)Wenzel, Roth, Veeling, Swiatkowski, Tran, Mandt,
  Snoek, Salimans, Jenatton, and Nowozin]{Wenzel2020How}
F.~Wenzel, K.~Roth, B.~Veeling, J.~Swiatkowski, L.~Tran, S.~Mandt, J.~Snoek,
  T.~Salimans, R.~Jenatton, and S.~Nowozin.
\newblock How good is the {B}ayes posterior in deep neural networks really?
\newblock In H.~D. III and A.~Singh, editors, \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 10248--10259. PMLR,
  13--18 Jul 2020.

\bibitem[Wilson and Izmailov(2020)]{Wilson2022Bayesian}
A.~G. Wilson and P.~Izmailov.
\newblock Bayesian deep learning and a probabilistic perspective of
  generalization.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 4697--4708. Curran Associates, Inc., 2020.

\end{thebibliography}
