\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Behnke and Heafield(2020)]{lottery-pruning-nmt}
Maximiliana Behnke and Kenneth Heafield.
\newblock Losing heads in the lottery: Pruning transformer attention in neural
  machine translation.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 2664--2674, Online, November
  2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.211}.
\newblock URL \url{https://www.aclweb.org/anthology/2020.emnlp-main.211}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{gpt-3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
  Maria{-}Florina Balcan, and Hsuan{-}Tien Lin, editors, \emph{Advances in
  Neural Information Processing Systems 33: Annual Conference on Neural
  Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
  virtual}, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}.

\bibitem[Chang et~al.(2018)Chang, Meng, Haber, Tung, and
  Begert]{resnet-dynamical-systems}
Bo~Chang, Lili Meng, Eldad Haber, Frederick Tung, and David Begert.
\newblock Multi-level residual networks from dynamical systems view.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}. OpenReview.net, 2018.
\newblock URL \url{https://openreview.net/forum?id=SyJS-OgR-}.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and
  Duvenaud]{neural-ode}
Tian~Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud.
\newblock Neural ordinary differential equations.
\newblock In Samy Bengio, Hanna~M. Wallach, Hugo Larochelle, Kristen Grauman,
  Nicol{\`{o}} Cesa{-}Bianchi, and Roman Garnett, editors, \emph{Advances in
  Neural Information Processing Systems 31: Annual Conference on Neural
  Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
  Montr{\'{e}}al, Canada}, pages 6572--6583, 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/hash/69386f6bb1dfed68692a24c8686939b9-Abstract.html}.

\bibitem[Choromanski et~al.(2020)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarl{\'{o}}s, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and
  Weller]{performer}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tam{\'{a}}s Sarl{\'{o}}s, Peter Hawkins, Jared Davis, Afroz
  Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller.
\newblock Rethinking attention with performers.
\newblock \emph{CoRR}, abs/2009.14794, 2020.
\newblock URL \url{https://arxiv.org/abs/2009.14794}.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{bert}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio, editors,
  \emph{Proceedings of the 2019 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies,
  {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and
  Short Papers)}, pages 4171--4186. Association for Computational Linguistics,
  2019.
\newblock \doi{10.18653/v1/n19-1423}.
\newblock URL \url{https://doi.org/10.18653/v1/n19-1423}.

\bibitem[Haber and Ruthotto(2017)]{ode-neural-development-1}
Eldad Haber and Lars Ruthotto.
\newblock Stable architectures for deep neural networks.
\newblock \emph{CoRR}, abs/1705.03341, 2017.
\newblock URL \url{http://arxiv.org/abs/1705.03341}.

\bibitem[Ho et~al.(2019)Ho, Kalchbrenner, Weissenborn, and
  Salimans]{sparse-apriori-2}
Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans.
\newblock Axial attention in multidimensional transformers.
\newblock \emph{CoRR}, abs/1912.12180, 2019.
\newblock URL \url{http://arxiv.org/abs/1912.12180}.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{linear-attention}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois
  Fleuret.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 5156--5165. {PMLR},
  2020.
\newblock URL \url{http://proceedings.mlr.press/v119/katharopoulos20a.html}.

\bibitem[Khrulkov et~al.(2019)Khrulkov, Hrinchuk, Mirvakhabova, and
  Oseledets]{tensorized-2}
Valentin Khrulkov, Oleksii Hrinchuk, Leyla Mirvakhabova, and Ivan~V. Oseledets.
\newblock Tensorized embedding layers for efficient model compression.
\newblock \emph{CoRR}, abs/1901.10787, 2019.
\newblock URL \url{http://arxiv.org/abs/1901.10787}.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{reformer}
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkgNKkHtvB}.

\bibitem[Lee{-}Thorp et~al.(2021)Lee{-}Thorp, Ainslie, Eckstein, and
  Onta{\~{n}}{\'{o}}n]{fnet}
James Lee{-}Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago
  Onta{\~{n}}{\'{o}}n.
\newblock Fnet: Mixing tokens with fourier transforms.
\newblock \emph{CoRR}, abs/2105.03824, 2021.
\newblock URL \url{https://arxiv.org/abs/2105.03824}.

\bibitem[Liu et~al.(2018)Liu, Saleh, Pot, Goodrich, Sepassi, Kaiser, and
  Shazeer]{sparse-apriori-1}
Peter~J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz
  Kaiser, and Noam Shazeer.
\newblock Generating wikipedia by summarizing long sequences.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}. OpenReview.net, 2018.
\newblock URL \url{https://openreview.net/forum?id=Hyg0vbWC-}.

\bibitem[Lu et~al.(2018)Lu, Zhong, Li, and Dong]{Lu-beyond-finite-layer}
Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong.
\newblock Beyond finite layer neural networks: Bridging deep architectures and
  numerical differential equations.
\newblock In Jennifer Dy and Andreas Krause, editors, \emph{Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pages 3276--3285. PMLR,
  10--15 Jul 2018.
\newblock URL \url{http://proceedings.mlr.press/v80/lu18d.html}.

\bibitem[Lu et~al.(2019)Lu, Li, He, Sun, Dong, Qin, Wang, and
  Liu]{transformer-multiP-ode}
Yiping Lu, Zhuohan Li, Di~He, Zhiqing Sun, Bin Dong, Tao Qin, Liwei Wang, and
  Tie{-}Yan Liu.
\newblock Understanding and improving transformer from a multi-particle dynamic
  system point of view.
\newblock \emph{CoRR}, abs/1906.02762, 2019.
\newblock URL \url{http://arxiv.org/abs/1906.02762}.

\bibitem[Ma et~al.(2019)Ma, Zhang, Zhang, Duan, Hou, Zhou, and
  Song]{tensorized-1}
Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, and
  Dawei Song.
\newblock A tensorized transformer for language modeling.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
  d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett, editors, \emph{Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pages 2229--2239, 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/hash/dc960c46c38bd16e953d97cdeefdbc68-Abstract.html}.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and Potts]{imdb}
Andrew~L. Maas, Raymond~E. Daly, Peter~T. Pham, Dan Huang, Andrew~Y. Ng, and
  Christopher Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Proceedings of the 49th Annual Meeting of the Association
  for Computational Linguistics: Human Language Technologies}, pages 142--150,
  Portland, Oregon, USA, June 2011. Association for Computational Linguistics.
\newblock URL \url{http://www.aclweb.org/anthology/P11-1015}.

\bibitem[Michel et~al.(2019)Michel, Levy, and Neubig]{sixteen-heads}
Paul Michel, Omer Levy, and Graham Neubig.
\newblock Are sixteen heads really better than one?
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
  d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett, editors, \emph{Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pages 14014--14024, 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/hash/2c601ad9d2ff9bc8b282670cdd54f69f-Abstract.html}.

\bibitem[Nangia and Bowman(2018)]{listops}
Nikita Nangia and Samuel~R. Bowman.
\newblock Listops: {A} diagnostic dataset for latent tree learning.
\newblock In Silvio~Ricardo Cordeiro, Shereen Oraby, Umashanthi Pavalanathan,
  and Kyeongmin Rim, editors, \emph{Proceedings of the 2018 Conference of the
  North American Chapter of the Association for Computational Linguistics,
  {NAACL-HLT} 2018, New Orleans, Louisiana, USA, June 2-4, 2018, Student
  Research Workshop}, pages 92--99. Association for Computational Linguistics,
  2018.
\newblock \doi{10.18653/v1/n18-4013}.
\newblock URL \url{https://doi.org/10.18653/v1/n18-4013}.

\bibitem[Peng et~al.(2021)Peng, Pappas, Yogatama, Schwartz, Smith, and
  Kong]{rfa}
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah~A. Smith, and
  Lingpeng Kong.
\newblock Random feature attention.
\newblock \emph{CoRR}, abs/2103.02143, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.02143}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{J. Mach. Learn. Res.}, 21:\penalty0 140:1--140:67, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Roy et~al.(2021)Roy, Saffar, Vaswani, and Grangier]{sparse-data-1}
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier.
\newblock Efficient content-based sparse attention with routing transformers.
\newblock \emph{Trans. Assoc. Comput. Linguistics}, 9:\penalty0 53--68, 2021.
\newblock URL \url{https://transacl.org/ojs/index.php/tacl/article/view/2405}.

\bibitem[Ruthotto and Haber(2019)]{deep-pde}
Lars Ruthotto and Eldad Haber.
\newblock Deep neural networks motivated by partial differential equations.
\newblock \emph{Journal of Mathematical Imaging and Vision}, pages 1--13, 2019.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{distil-bert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of {BERT:} smaller, faster, cheaper
  and lighter.
\newblock \emph{CoRR}, abs/1910.01108, 2019.
\newblock URL \url{http://arxiv.org/abs/1910.01108}.

\bibitem[Strubell et~al.(2020)Strubell, Ganesh, and
  McCallum]{strubell2020energy}
Emma Strubell, Ananya Ganesh, and Andrew McCallum.
\newblock Energy and policy considerations for modern deep learning research.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume 34\,(09), pages 13693--13696, 2020.

\bibitem[Sukhbaatar et~al.(2019)Sukhbaatar, Grave, Bojanowski, and
  Joulin]{sparse-data-2}
Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin.
\newblock Adaptive attention span in transformers.
\newblock In Anna Korhonen, David~R. Traum, and Llu{\'{\i}}s M{\`{a}}rquez,
  editors, \emph{Proceedings of the 57th Conference of the Association for
  Computational Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2,
  2019, Volume 1: Long Papers}, pages 331--335. Association for Computational
  Linguistics, 2019.
\newblock \doi{10.18653/v1/p19-1032}.
\newblock URL \url{https://doi.org/10.18653/v1/p19-1032}.

\bibitem[Tay et~al.(2020{\natexlab{a}})Tay, Bahri, Metzler, Juan, Zhao, and
  Zheng]{synthesizer}
Yi~Tay, Dara Bahri, Donald Metzler, Da{-}Cheng Juan, Zhe Zhao, and Che Zheng.
\newblock Synthesizer: Rethinking self-attention in transformer models.
\newblock \emph{CoRR}, abs/2005.00743, 2020{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2005.00743}.

\bibitem[Tay et~al.(2020{\natexlab{b}})Tay, Bahri, Yang, Metzler, and
  Juan]{sinkhorn}
Yi~Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan.
\newblock Sparse {S}inkhorn attention.
\newblock In Hal~Daum√© III and Aarti Singh, editors, \emph{Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 9438--9447. PMLR,
  13--18 Jul 2020{\natexlab{b}}.
\newblock URL \url{http://proceedings.mlr.press/v119/tay20a.html}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{transformer-vashwani}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.

\bibitem[Vuckovic et~al.(2020)Vuckovic, Baratin, and des Combes]{jotil-paper}
James Vuckovic, Aristide Baratin, and Remi~Tachet des Combes.
\newblock A mathematical theory of attention, 2020.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Li, Khabsa, Fang, and
  Ma]{linformer}
Sinong Wang, Belinda~Z. Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{CoRR}, abs/2006.04768, 2020{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2006.04768}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Wei, Dong, Bao, Yang, and
  Zhou]{minilm}
Wenhui Wang, Furu Wei, Li~Dong, Hangbo Bao, Nan Yang, and Ming Zhou.
\newblock Minilm: Deep self-attention distillation for task-agnostic
  compression of pre-trained transformers.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
  Maria{-}Florina Balcan, and Hsuan{-}Tien Lin, editors, \emph{Advances in
  Neural Information Processing Systems 33: Annual Conference on Neural
  Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
  virtual}, 2020{\natexlab{b}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}.

\bibitem[Weinan(2017)]{weinan2017proposal}
E~Weinan.
\newblock A proposal on machine learning via dynamical systems.
\newblock \emph{Communications in Mathematics and Statistics}, 5\penalty0
  (1):\penalty0 1--11, 2017.

\bibitem[Xiong et~al.(2021)Xiong, Zeng, Chakraborty, Tan, Fung, Li, and
  Singh]{nystromformer}
Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung,
  Yin Li, and Vikas Singh.
\newblock Nystr{\"{o}}mformer: {A} nystr{\"{o}}m-based algorithm for
  approximating self-attention.
\newblock \emph{CoRR}, abs/2102.03902, 2021.
\newblock URL \url{https://arxiv.org/abs/2102.03902}.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Onta{\~{n}}{\'{o}}n, Pham, Ravula, Wang, Yang, and Ahmed]{big-bird}
Manzil Zaheer, Guru Guruganesh, Kumar~Avinava Dubey, Joshua Ainslie, Chris
  Alberti, Santiago Onta{\~{n}}{\'{o}}n, Philip Pham, Anirudh Ravula, Qifan
  Wang, Li~Yang, and Amr Ahmed.
\newblock Big bird: Transformers for longer sequences.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
  Maria{-}Florina Balcan, and Hsuan{-}Tien Lin, editors, \emph{Advances in
  Neural Information Processing Systems 33: Annual Conference on Neural
  Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
  virtual}, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html}.

\bibitem[Zhang et~al.(2015)Zhang, Zhao, and LeCun]{agnews}
Xiang Zhang, Junbo~Jake Zhao, and Yann LeCun.
\newblock Character-level convolutional networks for text classification.
\newblock In Corinna Cortes, Neil~D. Lawrence, Daniel~D. Lee, Masashi Sugiyama,
  and Roman Garnett, editors, \emph{Advances in Neural Information Processing
  Systems 28: Annual Conference on Neural Information Processing Systems 2015,
  December 7-12, 2015, Montreal, Quebec, Canada}, pages 649--657, 2015.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html}.

\bibitem[Zhu and Fu(2018)]{ode-neural-development-2}
Mai Zhu and Chong Fu.
\newblock Convolutional neural networks combined with runge-kutta methods.
\newblock \emph{CoRR}, abs/1802.08831, 2018.
\newblock URL \url{http://arxiv.org/abs/1802.08831}.

\end{thebibliography}
