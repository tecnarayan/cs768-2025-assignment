\begin{thebibliography}{10}

\bibitem{Bialek1992}
W.~Bialek and F.~Rieke.
\newblock Reliability and information transmission in spiking neurons.
\newblock {\em Trends in Neurosciences}, 15(11):428--434, 1992.

\bibitem{bolukbasi2017adaptive}
T.~Bolukbasi, J.~Wang, O.~Dekel, and V.~Saligrama.
\newblock Adaptive neural networks for efficient inference.
\newblock In {\em International Conference on Machine Learning}, pages
  527--536. PMLR, 2017.

\bibitem{bulat2020toward}
A.~Bulat, J.~Kossaifi, G.~Tzimiropoulos, and M.~Pantic.
\newblock Toward fast and accurate human pose estimation via soft-gated skip
  connections.
\newblock {\em arXiv preprint arXiv:2002.11098}, 2020.

\bibitem{bulat2017far}
A.~Bulat and G.~Tzimiropoulos.
\newblock How far are we from solving the {2D \& 3D} face alignment problem?
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1021--1030, 2017.

\bibitem{carreira2018massively}
J.~Carreira, V.~Patraucean, L.~Mazare, A.~Zisserman, and S.~Osindero.
\newblock Massively parallel video networks.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 649--666, 2018.

\bibitem{Chen2020}
X.~Chen, H.~Dai, Y.~Li, X.~Gao, and L.~Song.
\newblock Learning to stop while learning to predict.
\newblock In H.~D. III and A.~Singh, editors, {\em Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of {\em Proceedings
  of Machine Learning Research}, pages 1520--1530. PMLR, 13--18 Jul 2020.

\bibitem{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{dicarlo2012does}
J.~J. DiCarlo, D.~Zoccolan, and N.~C. Rust.
\newblock How does the brain solve visual object recognition?
\newblock {\em Neuron}, 73(3):415--434, 2012.

\bibitem{Elbayad2020}
M.~Elbayad, J.~Gu, E.~Grave, and M.~Auli.
\newblock Depth-adaptive transformer.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{felleman1991distributed}
D.~J. Felleman and D.~C. Van~Essen.
\newblock Distributed hierarchical processing in the primate cerebral cortex.
\newblock In {\em Cereb cortex}. Citeseer, 1991.

\bibitem{fischer2018streaming}
V.~Fischer, J.~K{\"o}hler, and T.~Pfeil.
\newblock The streaming rollout of deep networks-towards fully model-parallel
  execution.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4039--4050, 2018.

\bibitem{Fukushima1980}
K.~Fukushima.
\newblock {N}eocognitron: {A} self-organizing neural network model for a
  mechanism of pattern recognition unaffected by shift in position.
\newblock {\em Biological Cybernetics}, 36:193--202, 1980.

\bibitem{Graves2016}
A.~Graves.
\newblock Adaptive computation time for recurrent neural networks.
\newblock {\em CoRR}, abs/1603.08983, 2016.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hu2018anytime}
H.~Hu, D.~Dey, M.~Hebert, and J.~A. Bagnell.
\newblock Anytime neural network: a versatile trade-off between computation and
  accuracy, 2018.

\bibitem{hu2019learning}
H.~Hu, D.~Dey, M.~Hebert, and J.~A. Bagnell.
\newblock Learning anytime predictions in neural networks via adaptive loss
  balancing.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 3812--3821, 2019.

\bibitem{huang2018}
G.~Huang, D.~Chen, T.~Li, F.~Wu, L.~van~der Maaten, and K.~Weinberger.
\newblock Multi-scale dense networks for resource efficient image
  classification.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{huang2017multi}
G.~Huang, D.~Chen, T.~Li, F.~Wu, L.~van~der Maaten, and K.~Q. Weinberger.
\newblock Multi-scale dense networks for resource efficient image
  classification.
\newblock {\em arXiv preprint arXiv:1703.09844}, 2017.

\bibitem{huang2017densely}
G.~Huang, Z.~Liu, L.~Van Der~Maaten, and K.~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4700--4708, 2017.

\bibitem{iuzzolino2019convolutional}
M.~Iuzzolino, Y.~Singer, and M.~C. Mozer.
\newblock Convolutional bipartite attractor networks.
\newblock {\em arXiv preprint arXiv:1906.03504}, 2019.

\bibitem{jiang2020characterizing}
Z.~Jiang, C.~Zhang, K.~Talwar, and M.~C. Mozer.
\newblock Characterizing structural regularities of labeled data in
  overparameterized models.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, 2021.

\bibitem{JonesKinoshitaMozer2009}
M.~Jones, S.~Kinoshita, and M.~C. Mozer.
\newblock Optimal response initiation: Why recent experience matters.
\newblock In D.~Koller, D.~Schuurmans, Y.~Bengio, and L.~Bottou, editors, {\em
  Advances in Neural Information Processing Systems}, volume~21. Curran
  Associates, Inc., 2009.

\bibitem{Kar2019}
K.~Kar, J.~Kubilius, K.~Schmidt, E.~B. Issa, and J.~J. DiCarlo.
\newblock Evidence that recurrent circuits are critical to the ventral
  stream{\textquoteright}s execution of core object recognition behavior.
\newblock {\em Nature Neuroscience}, 04/2019 2019.

\bibitem{kaya2019shallow}
Y.~Kaya, S.~Hong, and T.~Dumitras.
\newblock Shallow-deep networks: Understanding and mitigating network
  overthinking.
\newblock In {\em International Conference on Machine Learning}, pages
  3301--3310. PMLR, 2019.

\bibitem{Kriegeskorte2015}
N.~Kriegeskorte.
\newblock Deep neural networks: A new framework for modeling biological vision
  and brain information processing.
\newblock {\em Annual Review of Vision Science}, 1(1):417--446, 2015.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Computer Science, University of Toronto, 2009.

\bibitem{Kubilius2018}
J.~Kubilius, M.~Schrimpf, A.~Nayebi, D.~Bear, D.~L.~K. Yamins, and J.~J.
  DiCarlo.
\newblock Cornet: Modeling the neural mechanisms of core object recognition.
\newblock {\em bioRxiv}, 2018.

\bibitem{kugele2020efficient}
A.~Kugele, T.~Pfeil, M.~Pfeiffer, and E.~Chicca.
\newblock Efficient processing of spatio-temporal data streams with spiking
  neural networks.
\newblock {\em Frontiers in Neuroscience}, 14:439, 2020.

\bibitem{Kumbhar2020}
O.~Kumbhar, E.~Sizikova, N.~J. Majaj, and D.~G. Pelli.
\newblock Anytime prediction as a model of human reaction time.
\newblock {\em CoRR}, abs/2011.12859, 2020.

\bibitem{Larsson2017}
G.~Larsson, M.~Maire, and G.~Shakhnarovich.
\newblock Fractalnet: Ultra-deep neural networks without residuals.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{lee2018anytime}
H.~Lee and J.~Shin.
\newblock Anytime neural prediction via slicing networks vertically.
\newblock {\em arXiv preprint arXiv:1807.02609}, 2018.

\bibitem{leroux2018}
S.~Leroux, P.~Molchanov, P.~Simoens, B.~Dhoedt, T.~Breuel, and J.~Kautz.
\newblock Iamnn: Iterative and adaptive mobile neural network for efficient
  image classification, 2018.

\bibitem{liang2017enhancing}
S.~Liang, Y.~Li, and R.~Srikant.
\newblock Enhancing the reliability of out-of-distribution image detection in
  neural networks.
\newblock {\em arXiv preprint arXiv:1706.02690}, 2017.

\bibitem{marquez2018deep}
E.~S. Marquez, J.~S. Hare, and M.~Niranjan.
\newblock Deep cascade learning.
\newblock {\em IEEE transactions on neural networks and learning systems},
  29(11):5475--5485, 2018.

\bibitem{masson2003using}
M.~E. Masson and G.~R. Loftus.
\newblock Using confidence intervals for graphically based data interpretation.
\newblock {\em Canadian Journal of Experimental Psychology/Revue canadienne de
  psychologie exp{\'e}rimentale}, 57(3):203, 2003.

\bibitem{mcclelland1979time}
J.~L. McClelland.
\newblock On the time relations of mental processes: an examination of systems
  of processes in cascade.
\newblock {\em Psychological review}, 86(4):287, 1979.

\bibitem{McCullochPitts}
W.~S. McCulloch and W.~Pitts.
\newblock A logical calculus of the ideas immanent in nervous activity.
\newblock {\em {Bulletin of Mathematical Biophysics}}, 5(4):115--133, 1943.

\bibitem{mcintosh2018recurrent}
L.~McIntosh, N.~Maheswaranathan, D.~Sussillo, and J.~Shlens.
\newblock Recurrent segmentation for variable computational budgets.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 1648--1657, 2018.

\bibitem{netzer2011reading}
Y.~Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, and A.~Y. Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In {\em NIPS Workshop on Deep Learning and Unsupervised Feature
  Learning 2011}, 2011.

\bibitem{newell2016stacked}
A.~Newell, K.~Yang, and J.~Deng.
\newblock Stacked hourglass networks for human pose estimation.
\newblock In {\em European conference on computer vision}, pages 483--499.
  Springer, 2016.

\bibitem{peterson2019human}
J.~C. Peterson, R.~M. Battleday, T.~L. Griffiths, and O.~Russakovsky.
\newblock Human uncertainty makes classification more robust.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 9617--9626, 2019.

\bibitem{RatcliffMcCoon2008}
R.~Ratcliff and G.~McKoon.
\newblock The diffusion decision model: theory and data for two-choice decision
  tasks.
\newblock {\em Neural Computation}, 20(4):873--â€“922, 2008.

\bibitem{ronneberger2015u}
O.~Ronneberger, P.~Fischer, and T.~Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In {\em International Conference on Medical image computing and
  computer-assisted intervention}, pages 234--241. Springer, 2015.

\bibitem{ILSVRC15}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein, A.~C. Berg, and L.~Fei-Fei.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock {\em International Journal of Computer Vision (IJCV)},
  115(3):211--252, 2015.

\bibitem{scardapane2020should}
S.~Scardapane, M.~Scarpiniti, E.~Baccarelli, and A.~Uncini.
\newblock Why should we add early exits to neural networks?
\newblock {\em Cognitive Computation}, 12(5):954--966, 2020.

\bibitem{Spoerer2020}
C.~J. Spoerer, T.~C. Kietzmann, J.~Mehrer, I.~Charest, and N.~Kriegeskorte.
\newblock Recurrent neural networks can explain flexible trading of speed and
  accuracy in biological vision.
\newblock {\em bioRxiv}, 2020.

\bibitem{Spoerer2017}
C.~J. Spoerer, P.~McClure, and N.~Kriegeskorte.
\newblock Recurrent convolutional neural networks: A better model of biological
  object recognition.
\newblock {\em Frontiers in Psychology}, 8:1551, 2017.

\bibitem{srivastava2015highway}
R.~K. Srivastava, K.~Greff, and J.~Schmidhuber.
\newblock Highway networks.
\newblock {\em arXiv preprint arXiv:1505.00387}, 2015.

\bibitem{sutton1988learning}
R.~S. Sutton.
\newblock Learning to predict by the methods of temporal differences.
\newblock {\em Machine learning}, 3(1):9--44, 1988.

\bibitem{sutton2018reinforcement}
R.~S. Sutton and A.~G. Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem{teerapittayanon2016branchynet}
S.~Teerapittayanon, B.~McDanel, and H.-T. Kung.
\newblock Branchynet: Fast inference via early exiting from deep neural
  networks.
\newblock In {\em 2016 23rd International Conference on Pattern Recognition
  (ICPR)}, pages 2464--2469. IEEE, 2016.

\bibitem{Vaswanietal2017}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, L.~u.
  Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem{wang2017idk}
X.~Wang, Y.~Luo, D.~Crankshaw, A.~Tumanov, F.~Yu, and J.~E. Gonzalez.
\newblock Idk cascades: Fast deep learning by learning not to overthink.
\newblock {\em arXiv preprint arXiv:1706.00885}, 2017.

\bibitem{yang2020resolution}
L.~Yang, Y.~Han, X.~Chen, S.~Song, J.~Dai, and G.~Huang.
\newblock Resolution adaptive networks for efficient inference.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 2369--2378, 2020.

\bibitem{yu2015lsun}
F.~Yu, A.~Seff, Y.~Zhang, S.~Song, T.~Funkhouser, and J.~Xiao.
\newblock Lsun: Construction of a large-scale image dataset using deep learning
  with humans in the loop.
\newblock {\em arXiv preprint arXiv:1506.03365}, 2015.

\bibitem{zagoruyko2016wide}
S.~Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock {\em arXiv preprint arXiv:1605.07146}, 2016.

\bibitem{zamir2017feedback}
A.~R. Zamir, T.-L. Wu, L.~Sun, W.~B. Shen, B.~E. Shi, J.~Malik, and
  S.~Savarese.
\newblock Feedback networks.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1308--1317, 2017.

\bibitem{zilberstein1996using}
S.~Zilberstein.
\newblock Using anytime algorithms in intelligent systems.
\newblock {\em AI magazine}, 17(3):73--73, 1996.

\end{thebibliography}
