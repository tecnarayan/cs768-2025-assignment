\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abnar et~al.(2021)Abnar, Dehghani, Neyshabur, and
  Sedghi]{abnar2021exploring}
Abnar, S., Dehghani, M., Neyshabur, B., and Sedghi, H.
\newblock Exploring the limits of large scale pre-training.
\newblock \emph{arXiv preprint arXiv:2110.02095}, 2021.

\bibitem[AI@Meta(2024)]{llama3}
AI@Meta.
\newblock Llama 3 model card, 2024.
\newblock URL
  \url{https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}.

\bibitem[Ainslie et~al.(2023)Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy, Lebrón,
  and Sanghai]{gqa}
Ainslie, J., Lee-Thorp, J., de~Jong, M., Zemlyanskiy, Y., Lebrón, F., and
  Sanghai, S.
\newblock Gqa: Training generalized multi-query transformer models from
  multi-head checkpoints, 2023.

\bibitem[Amini et~al.(2019)Amini, Gabriel, Lin, Koncel{-}Kedziorski, Choi, and
  Hajishirzi]{math_qa}
Amini, A., Gabriel, S., Lin, S., Koncel{-}Kedziorski, R., Choi, Y., and
  Hajishirzi, H.
\newblock Mathqa: Towards interpretable math word problem solving with
  operation-based formalisms.
\newblock \emph{CoRR}, abs/1905.13319, 2019.
\newblock URL \url{http://arxiv.org/abs/1905.13319}.

\bibitem[Barton(2024)]{Barton2024}
Barton, T.
\newblock Calibrating the mosaic evaluation gauntlet.
\newblock
  \url{https://www.databricks.com/blog/calibrating-mosaic-evaluation-gauntlet},
  April 2024.
\newblock Blog post.

\bibitem[Besiroglu et~al.(2024)Besiroglu, Erdil, Barnett, and
  You]{besiroglu2024chinchilla}
Besiroglu, T., Erdil, E., Barnett, M., and You, J.
\newblock Chinchilla scaling: A replication attempt.
\newblock \emph{arXiv preprint arXiv:2404.10102}, 2024.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Gao, Choi, et~al.]{piqa}
Bisk, Y., Zellers, R., Gao, J., Choi, Y., et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~34, pp.\  7432--7439, 2020.

\bibitem[Bordelon et~al.(2024)Bordelon, Atanasov, and
  Pehlevan]{bordelon2024dynamical}
Bordelon, B., Atanasov, A., and Pehlevan, C.
\newblock A dynamical model of neural scaling laws.
\newblock \emph{arXiv preprint arXiv:2402.01092}, 2024.

\bibitem[Caballero et~al.(2022)Caballero, Gupta, Rish, and
  Krueger]{caballero2022broken}
Caballero, E., Gupta, K., Rish, I., and Krueger, D.
\newblock Broken neural scaling laws.
\newblock \emph{arXiv preprint arXiv:2210.14891}, 2022.

\bibitem[Chen et~al.(2023)Chen, Liang, Huang, Real, Wang, Liu, Pham, Dong,
  Luong, Hsieh, Lu, and Le]{lion}
Chen, X., Liang, C., Huang, D., Real, E., Wang, K., Liu, Y., Pham, H., Dong,
  X., Luong, T., Hsieh, C.-J., Lu, Y., and Le, Q.~V.
\newblock Symbolic discovery of optimization algorithms, 2023.

\bibitem[Clark et~al.(2019)Clark, Lee, Ming-Wei~Chang, Collins, and
  Toutanova]{clark2019boolq}
Clark, C., Lee, K., Ming-Wei~Chang, T.~K., Collins, M., and Toutanova, K.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no
  questions.
\newblock In \emph{NAACL}, 2019.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,
  and Tafjord]{arc}
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and
  Tafjord, O.
\newblock Think you have solved question answering? try arc, the ai2 reasoning
  challenge.
\newblock \emph{arXiv:1803.05457v1}, 2018.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021gsm8k}
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert,
  M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[De~Vries(2023)]{blogpost}
De~Vries, H.
\newblock Go smol or go home, 2023.
\newblock URL
  \url{https://www.harmdevries.com/post/model-size-vs-compute-overhead/}.

\bibitem[Dey et~al.(2023)Dey, Gosal, Zhiming, Chen, Khachane, Marshall,
  Pathria, Tom, and Hestness]{cerebrasgpt}
Dey, N., Gosal, G., Zhiming, Chen, Khachane, H., Marshall, W., Pathria, R.,
  Tom, M., and Hestness, J.
\newblock Cerebras-gpt: Open compute-optimal language models trained on the
  cerebras wafer-scale cluster, 2023.

\bibitem[Frantar et~al.(2023)Frantar, Ashkboos, Hoefler, and Alistarh]{gptq}
Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D.
\newblock Gptq: Accurate post-training quantization for generative pre-trained
  transformers, 2023.

\bibitem[Gadre et~al.(2024)Gadre, Smyrnis, Shankar, Gururangan, Wortsman, Shao,
  Mercat, Fang, Li, Keh, et~al.]{gadre2024language}
Gadre, S.~Y., Smyrnis, G., Shankar, V., Gururangan, S., Wortsman, M., Shao, R.,
  Mercat, J., Fang, A., Li, J., Keh, S., et~al.
\newblock Language models scale reliably with over-training and on downstream
  tasks.
\newblock \emph{arXiv preprint arXiv:2403.08540}, 2024.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt]{hendrycks2020measuring}
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and
  Steinhardt, J.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Henighan et~al.(2020)Henighan, Kaplan, Katz, Chen, Hesse, Jackson,
  Jun, Brown, Dhariwal, Gray, et~al.]{henighan2020scaling}
Henighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, J., Jun, H.,
  Brown, T.~B., Dhariwal, P., Gray, S., et~al.
\newblock Scaling laws for autoregressive generative modeling.
\newblock \emph{arXiv preprint arXiv:2010.14701}, 2020.

\bibitem[Hernandez et~al.(2021)Hernandez, Kaplan, Henighan, and
  McCandlish]{hernandez2021scaling}
Hernandez, D., Kaplan, J., Henighan, T., and McCandlish, S.
\newblock Scaling laws for transfer.
\newblock \emph{arXiv preprint arXiv:2102.01293}, 2021.

\bibitem[Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun,
  Kianinejad, Patwary, Yang, and Zhou]{hestness2017deep}
Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H.,
  Patwary, M. M.~A., Yang, Y., and Zhou, Y.
\newblock Deep learning scaling is predictable, empirically.
\newblock \emph{arXiv preprint arXiv:1712.00409}, 2017.

\bibitem[Hestness et~al.(2019)Hestness, Ardalani, and
  Diamos]{hestness2019beyond}
Hestness, J., Ardalani, N., and Diamos, G.
\newblock Beyond human-level accuracy: Computational challenges in deep
  learning.
\newblock In \emph{Proceedings of the 24th symposium on principles and practice
  of parallel programming}, pp.\  1--14, 2019.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, de~Las~Casas, Hendricks, Welbl, Clark, Hennigan, Noland,
  Millican, van~den Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Rae,
  Vinyals, and Sifre]{chinchilla}
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford,
  E., de~Las~Casas, D., Hendricks, L.~A., Welbl, J., Clark, A., Hennigan, T.,
  Noland, E., Millican, K., van~den Driessche, G., Damoc, B., Guy, A.,
  Osindero, S., Simonyan, K., Elsen, E., Rae, J.~W., Vinyals, O., and Sifre, L.
\newblock Training compute-optimal large language models, 2022.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{scalinglaws}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R.,
  Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models, 2020.

\bibitem[Knight(2023)]{openaiinterview}
Knight, W.
\newblock Openai’s ceo says the age of giant ai models is already over.
\newblock \emph{Wired}, 2023.
\newblock ISSN 1059-1028.
\newblock URL
  \url{https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/}.

\bibitem[Korthikanti et~al.(2022)Korthikanti, Casper, Lym, McAfee, Andersch,
  Shoeybi, and Catanzaro]{megatronlm}
Korthikanti, V., Casper, J., Lym, S., McAfee, L., Andersch, M., Shoeybi, M.,
  and Catanzaro, B.
\newblock Reducing activation recomputation in large transformer models, 2022.

\bibitem[Krajewski et~al.(2024)Krajewski, Ludziejewski, Adamczewski, Pi{\'o}ro,
  Krutul, Antoniak, Ciebiera, Kr{\'o}l, Odrzyg{\'o}{\'z}d{\'z}, Sankowski,
  et~al.]{krajewski2024scaling}
Krajewski, J., Ludziejewski, J., Adamczewski, K., Pi{\'o}ro, M., Krutul, M.,
  Antoniak, S., Ciebiera, K., Kr{\'o}l, K., Odrzyg{\'o}{\'z}d{\'z}, T.,
  Sankowski, P., et~al.
\newblock Scaling laws for fine-grained mixture of experts.
\newblock \emph{arXiv preprint arXiv:2402.07871}, 2024.

\bibitem[Labs(2023)]{lambdalabs}
Labs, L.
\newblock Gpu cloud - vms for deep learning.
\newblock \url{https://lambdalabs.com/service/gpu-cloud}, 2023.
\newblock Accessed 2023-10-02.

\bibitem[Levesque et~al.(2012)Levesque, Davis, and Morgenstern]{winograd}
Levesque, H., Davis, E., and Morgenstern, L.
\newblock The winograd schema challenge.
\newblock In \emph{Thirteenth International Conference on the Principles of
  Knowledge Representation and Reasoning}. Citeseer, 2012.

\bibitem[Michaud et~al.(2024)Michaud, Liu, Girit, and
  Tegmark]{michaud2024quantization}
Michaud, E., Liu, Z., Girit, U., and Tegmark, M.
\newblock The quantization model of neural scaling.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and
  Sabharwal]{openbook_qa}
Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A.
\newblock Can a suit of armor conduct electricity? a new dataset for open book
  question answering.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, 2018.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:52183757}.

\bibitem[Mikami et~al.(2021)Mikami, Fukumizu, Murai, Suzuki, Kikuchi, Suzuki,
  Maeda, and Hayashi]{mikami2021scaling}
Mikami, H., Fukumizu, K., Murai, S., Suzuki, S., Kikuchi, Y., Suzuki, T.,
  Maeda, S.-i., and Hayashi, K.
\newblock A scaling law for synthetic-to-real transfer: How much is your
  pre-training effective?
\newblock \emph{arXiv preprint arXiv:2108.11018}, 2021.

\bibitem[MosaicML(2023)]{mpt}
MosaicML.
\newblock Introducing mpt-7b: A new standard for open-source, commercially
  usable llms, 2023.
\newblock URL \url{www.mosaicml.com/blog/mpt-7b}.
\newblock Accessed: 2024-01-28.

\bibitem[{MosaicML NLP Team}(2023)]{MosaicML2023LLMEvaluation}
{MosaicML NLP Team}.
\newblock Llm evaluation scores.
\newblock \url{https://www.mosaicml.com/llm-evaluation}, 2023.
\newblock Accessed: 2023-09-28.

\bibitem[Muennighoff et~al.(2023)Muennighoff, Rush, Barak, Scao, Piktus, Tazi,
  Pyysalo, Wolf, and Raffel]{dataconstrained}
Muennighoff, N., Rush, A.~M., Barak, B., Scao, T.~L., Piktus, A., Tazi, N.,
  Pyysalo, S., Wolf, T., and Raffel, C.
\newblock Scaling data-constrained language models, 2023.

\bibitem[NVIDIA(2021)]{a100datasheet}
NVIDIA.
\newblock Nvidia a100 datasheet, 2021.
\newblock URL
  \url{https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf}.

\bibitem[OpenAI \& Pilipiszyn(2021)OpenAI and Pilipiszyn]{openaidemand}
OpenAI and Pilipiszyn, A.
\newblock Gpt-3 powers the next generation of apps, Mar 2021.
\newblock URL \url{https://openai.com/blog/gpt-3-apps}.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi,
  Pezzelle, Baroni, Boleda, and Fern{\'a}ndez]{paperno2016lambada}
Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q.~N., Bernardi, R.,
  Pezzelle, S., Baroni, M., Boleda, G., and Fern{\'a}ndez, R.
\newblock The lambada dataset: Word prediction requiring a broad discourse
  context.
\newblock \emph{arXiv preprint arXiv:1606.06031}, 2016.

\bibitem[Paquette et~al.(2024)Paquette, Paquette, Xiao, and
  Pennington]{paquette20244+}
Paquette, E., Paquette, C., Xiao, L., and Pennington, J.
\newblock 4+ 3 phases of compute-optimal neural scaling laws.
\newblock \emph{arXiv preprint arXiv:2405.15074}, 2024.

\bibitem[Patel et~al.(2021)Patel, Bhattamishra, and Goyal]{patel-etal-2021-nlp}
Patel, A., Bhattamishra, S., and Goyal, N.
\newblock Are {NLP} models really able to solve simple math word problems?
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pp.\  2080--2094, Online, June 2021. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.168}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.168}.

\bibitem[Pope et~al.(2022)Pope, Douglas, Chowdhery, Devlin, Bradbury, Levskaya,
  Heek, Xiao, Agrawal, and Dean]{palminference}
Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Levskaya, A.,
  Heek, J., Xiao, K., Agrawal, S., and Dean, J.
\newblock Efficiently scaling transformer inference, 2022.

\bibitem[Porian et~al.(2024)Porian, Wortsman, Jitsev, Schmidt, and
  Carmon]{porian2024resolving}
Porian, T., Wortsman, M., Jitsev, J., Schmidt, L., and Carmon, Y.
\newblock Resolving discrepancies in compute-optimal scaling of language
  models.
\newblock \emph{arXiv preprint arXiv:2406.19146}, 2024.

\bibitem[Press et~al.(2022)Press, Smith, and Lewis]{alibi}
Press, O., Smith, N.~A., and Lewis, M.
\newblock Train short, test long: Attention with linear biases enables input
  length extrapolation, 2022.

\bibitem[Rae et~al.(2022)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young, Rutherford, Hennigan, Menick, Cassirer,
  Powell, van~den Driessche, Hendricks, Rauh, Huang, Glaese, Welbl, Dathathri,
  Huang, Uesato, Mellor, Higgins, Creswell, McAleese, Wu, Elsen, Jayakumar,
  Buchatskaya, Budden, Sutherland, Simonyan, Paganini, Sifre, Martens, Li,
  Kuncoro, Nematzadeh, Gribovskaya, Donato, Lazaridou, Mensch, Lespiau,
  Tsimpoukelli, Grigorev, Fritz, Sottiaux, Pajarskas, Pohlen, Gong, Toyama,
  de~Masson~d'Autume, Li, Terzi, Mikulik, Babuschkin, Clark, de~Las~Casas, Guy,
  Jones, Bradbury, Johnson, Hechtman, Weidinger, Gabriel, Isaac, Lockhart,
  Osindero, Rimell, Dyer, Vinyals, Ayoub, Stanway, Bennett, Hassabis,
  Kavukcuoglu, and Irving]{gopher}
Rae, J.~W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F.,
  Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan,
  T., Menick, J., Cassirer, A., Powell, R., van~den Driessche, G., Hendricks,
  L.~A., Rauh, M., Huang, P.-S., Glaese, A., Welbl, J., Dathathri, S., Huang,
  S., Uesato, J., Mellor, J., Higgins, I., Creswell, A., McAleese, N., Wu, A.,
  Elsen, E., Jayakumar, S., Buchatskaya, E., Budden, D., Sutherland, E.,
  Simonyan, K., Paganini, M., Sifre, L., Martens, L., Li, X.~L., Kuncoro, A.,
  Nematzadeh, A., Gribovskaya, E., Donato, D., Lazaridou, A., Mensch, A.,
  Lespiau, J.-B., Tsimpoukelli, M., Grigorev, N., Fritz, D., Sottiaux, T.,
  Pajarskas, M., Pohlen, T., Gong, Z., Toyama, D., de~Masson~d'Autume, C., Li,
  Y., Terzi, T., Mikulik, V., Babuschkin, I., Clark, A., de~Las~Casas, D., Guy,
  A., Jones, C., Bradbury, J., Johnson, M., Hechtman, B., Weidinger, L.,
  Gabriel, I., Isaac, W., Lockhart, E., Osindero, S., Rimell, L., Dyer, C.,
  Vinyals, O., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D., Kavukcuoglu,
  K., and Irving, G.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher, 2022.

\bibitem[Raffel et~al.(2023)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{c4}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer, 2023.

\bibitem[{Rajpurkar} et~al.(2016){Rajpurkar}, {Zhang}, {Lopyrev}, and
  {Liang}]{squad}
{Rajpurkar}, P., {Zhang}, J., {Lopyrev}, K., and {Liang}, P.
\newblock {SQuAD: 100,000+ Questions for Machine Comprehension of Text}.
\newblock \emph{arXiv e-prints}, art. arXiv:1606.05250, 2016.

\bibitem[Reddy et~al.(2019)Reddy, Chen, and Manning]{reddy-etal-2019-coqa}
Reddy, S., Chen, D., and Manning, C.~D.
\newblock {C}o{QA}: A conversational question answering challenge.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  7:\penalty0 249--266, 2019.
\newblock \doi{10.1162/tacl_a_00266}.
\newblock URL \url{https://aclanthology.org/Q19-1016}.

\bibitem[Roemmele et~al.(2011)Roemmele, Beja, and Gordon]{copa}
Roemmele, M., Beja, C.~A., and Gordon, A.~S.
\newblock Choice of plausible alternatives: An evaluation of commonsense causal
  reasoning.
\newblock \emph{Papers from the 2011 AAAI Spring Symposium}, 2011.

\bibitem[Rosenfeld et~al.(2019)Rosenfeld, Rosenfeld, Belinkov, and
  Shavit]{rosenfeld2019constructive}
Rosenfeld, J.~S., Rosenfeld, A., Belinkov, Y., and Shavit, N.
\newblock A constructive prediction of the generalization error across scales.
\newblock \emph{arXiv preprint arXiv:1909.12673}, 2019.

\bibitem[Ruan et~al.(2024)Ruan, Maddison, and Hashimoto]{ruan2024observational}
Ruan, Y., Maddison, C.~J., and Hashimoto, T.
\newblock Observational scaling laws and the predictability of language model
  performance.
\newblock \emph{arXiv preprint arXiv:2405.10938}, 2024.

\bibitem[Sakaguchi et~al.(2019)Sakaguchi, Bras, Bhagavatula, and
  Choi]{winogrande}
Sakaguchi, K., Bras, R.~L., Bhagavatula, C., and Choi, Y.
\newblock {WINOGRANDE:} an adversarial winograd schema challenge at scale.
\newblock \emph{CoRR}, abs/1907.10641, 2019.
\newblock URL \url{http://arxiv.org/abs/1907.10641}.

\bibitem[Sap et~al.(2019)Sap, Rashkin, Chen, LeBras, and
  Choi]{sap2019socialiqa}
Sap, M., Rashkin, H., Chen, D., LeBras, R., and Choi, Y.
\newblock Socialiqa: Commonsense reasoning about social interactions.
\newblock \emph{arXiv preprint arXiv:1904.09728}, 2019.

\bibitem[Sardana \& Frankle(2023)Sardana and Frankle]{sardana2023beyond}
Sardana, N. and Frankle, J.
\newblock Beyond chinchilla-optimal: Accounting for inference in language model
  scaling laws.
\newblock \emph{arXiv preprint arXiv:2401.00448}, 2023.

\bibitem[Shazeer \& Freitas(2022)Shazeer and Freitas]{characterai}
Shazeer, N. and Freitas, D.~d.
\newblock Introducing character, Dec 2022.
\newblock URL \url{https://blog.character.ai/introducing-character/}.

\bibitem[Sorscher et~al.(2022)Sorscher, Geirhos, Shekhar, Ganguli, and
  Morcos]{sorscher2022beyond}
Sorscher, B., Geirhos, R., Shekhar, S., Ganguli, S., and Morcos, A.
\newblock Beyond neural scaling laws: beating power law scaling via data
  pruning.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 19523--19536, 2022.

\bibitem[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch,
  Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2022beyond}
Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A.~M., Abid, A., Fisch, A.,
  Brown, A.~R., Santoro, A., Gupta, A., Garriga-Alonso, A., et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the
  capabilities of language models.
\newblock \emph{arXiv preprint arXiv:2206.04615}, 2022.

\bibitem[Talmor et~al.(2018)Talmor, Herzig, Lourie, and
  Berant]{talmor2018commonsenseqa}
Talmor, A., Herzig, J., Lourie, N., and Berant, J.
\newblock Commonsenseqa: A question answering challenge targeting commonsense
  knowledge.
\newblock \emph{arXiv preprint arXiv:1811.00937}, 2018.

\bibitem[Tay et~al.(2022)Tay, Dehghani, Abnar, Chung, Fedus, Rao, Narang, Tran,
  Yogatama, and Metzler]{tay2022scaling}
Tay, Y., Dehghani, M., Abnar, S., Chung, H.~W., Fedus, W., Rao, J., Narang, S.,
  Tran, V.~Q., Yogatama, D., and Metzler, D.
\newblock Scaling laws vs model architectures: How does inductive bias
  influence scaling?
\newblock \emph{arXiv preprint arXiv:2207.10551}, 2022.

\bibitem[{The Mosaic ML Team}(2021)]{composer}
{The Mosaic ML Team}.
\newblock composer.
\newblock \url{https://github.com/mosaicml/composer/}, 2021.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,
  Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave,
  and Lample]{llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
  T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin,
  A., Grave, E., and Lample, G.
\newblock Llama: Open and efficient foundation language models,
  2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert,
  Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher,
  Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami,
  Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann,
  Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov,
  Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva,
  Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang,
  Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{llama2}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,
  Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L.,
  Ferrer, C.~C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu,
  W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S.,
  Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev,
  A., Koura, P.~S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,
  Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y.,
  Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva,
  R., Smith, E.~M., Subramanian, R., Tan, X.~E., Tang, B., Taylor, R.,
  Williams, A., Kuan, J.~X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A.,
  Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and
  Scialom, T.
\newblock Llama 2: Open foundation and fine-tuned chat models,
  2023{\natexlab{b}}.

\bibitem[Tow et~al.(2023)Tow, Bellagente, Mahan, and Riquelme]{stablelm}
Tow, J., Bellagente, M., Mahan, D., and Riquelme, C.
\newblock Stablelm 3b 4e1t, 2023.
\newblock URL \url{https://huggingface.co/stabilityai/stablelm-3b-4e1t}.

\bibitem[Villalobos(2023)]{villalobos2023scaling}
Villalobos, P.
\newblock Scaling laws literature review, 2023.
\newblock URL \url{https://epochai.org/blog/scaling-laws-literature-review}.

\bibitem[Villalobos \& Atkinson(2023)Villalobos and Atkinson]{tradeoff}
Villalobos, P. and Atkinson, D.
\newblock Trading off compute in training and inference, 2023.
\newblock URL
  \url{https://epochai.org/blog/trading-off-compute-in-training-and-inference}.
\newblock Accessed: 2023-9-26.

\bibitem[Wolfe et~al.(2022)Wolfe, Tunstall, and von Platen]{jeopardy}
Wolfe, T., Tunstall, L., and von Platen, P.
\newblock Jeopardy dataset on hugging face hub.
\newblock \url{https://huggingface.co/datasets/jeopardy}, 2022.

\bibitem[Xiao et~al.(2023)Xiao, Lin, Seznec, Wu, Demouth, and Han]{smoothquant}
Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S.
\newblock Smoothquant: Accurate and efficient post-training quantization for
  large language models, 2023.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and
  Choi]{zellers2019hellaswag}
Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock \emph{arXiv preprint arXiv:1905.07830}, 2019.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Li, Zhuang, Wu, Zhuang, Li,
  Lin, Xing, Gonzalez, Stoica, and Zhang]{lmsys}
Zheng, L., Chiang, W.-L., Sheng, Y., Li, T., Zhuang, S., Wu, Z., Zhuang, Y.,
  Li, Z., Lin, Z., Xing, E.~P., Gonzalez, J.~E., Stoica, I., and Zhang, H.
\newblock Lmsys-chat-1m: A large-scale real-world llm conversation dataset,
  2023.

\bibitem[Zhong et~al.(2023)Zhong, Cui, Guo, Liang, Lu, Wang, Saied, Chen, and
  Duan]{zhong2023agieval}
Zhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W.,
  and Duan, N.
\newblock Agieval: A human-centric benchmark for evaluating foundation models,
  2023.

\end{thebibliography}
