\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,
  Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
  Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
  Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Arjovsky et~al.(2019)Arjovsky, Bottou, Gulrajani, and
  Lopez-Paz]{arjovsky2019invariant}
Martin Arjovsky, L{\'e}on Bottou, Ishaan Gulrajani, and David Lopez-Paz.
\newblock Invariant risk minimization.
\newblock \emph{arXiv preprint arXiv:1907.02893}, 2019.

\bibitem[Blanc et~al.(2020)Blanc, Gupta, Valiant, and
  Valiant]{blanc2020implicit}
Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant.
\newblock Implicit regularization for deep neural networks driven by an
  ornstein-uhlenbeck like process.
\newblock In \emph{Conference on Learning Theory}, pages 483--513. PMLR, 2020.

\bibitem[Candes(2008)]{candes2008restricted}
Emmanuel~J Candes.
\newblock The restricted isometry property and its implications for compressed
  sensing.
\newblock \emph{Comptes rendus. Mathematique}, 346\penalty0 (9-10):\penalty0
  589--592, 2008.

\bibitem[Candes and Tao(2005)]{candes2005decoding}
Emmanuel~J Candes and Terence Tao.
\newblock Decoding by linear programming.
\newblock \emph{IEEE transactions on information theory}, 51\penalty0
  (12):\penalty0 4203--4215, 2005.

\bibitem[Candès and Plan(2011)]{5730578}
Emmanuel~J. Candès and Yaniv Plan.
\newblock Tight oracle inequalities for low-rank matrix recovery from a minimal
  number of noisy random measurements.
\newblock \emph{IEEE Transactions on Information Theory}, 57\penalty0
  (4):\penalty0 2342--2359, 2011.
\newblock \doi{10.1109/TIT.2011.2111771}.

\bibitem[Chang and Tandon(2019)]{chang2019upload}
Wei-Ting Chang and Ravi Tandon.
\newblock On the upload versus download cost for secure and private matrix
  multiplication.
\newblock In \emph{2019 IEEE Information Theory Workshop (ITW)}, pages 1--5.
  IEEE, 2019.

\bibitem[Cohen et~al.(2020)Cohen, Kaur, Li, Kolter, and
  Talwalkar]{cohen2020gradient}
Jeremy Cohen, Simran Kaur, Yuanzhi Li, J~Zico Kolter, and Ameet Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of
  stability.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Damian et~al.(2021)Damian, Ma, and Lee]{damian2021label}
Alex Damian, Tengyu Ma, and Jason~D Lee.
\newblock Label noise {SGD} provably prefers flat global minimizers.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Dwork et~al.(2006)Dwork, Kenthapadi, McSherry, Mironov, and
  Naor]{dwork2006our}
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni
  Naor.
\newblock Our data, ourselves: Privacy via distributed noise generation.
\newblock In \emph{Advances in Cryptology-EUROCRYPT 2006: 24th Annual
  International Conference on the Theory and Applications of Cryptographic
  Techniques, St. Petersburg, Russia, May 28-June 1, 2006. Proceedings 25},
  pages 486--503. Springer, 2006.

\bibitem[Even et~al.(2024)Even, Pesme, Gunasekar, and Flammarion]{even2024s}
Mathieu Even, Scott Pesme, Suriya Gunasekar, and Nicolas Flammarion.
\newblock (s) gd over diagonal linear networks: Implicit bias, large stepsizes
  and edge of stability.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Fan and Liao(2014)]{fan2014endogeneity}
Jianqing Fan and Yuan Liao.
\newblock Endogeneity in high dimensions.
\newblock \emph{Annals of Statistics}, 42\penalty0 (3):\penalty0 872, 2014.

\bibitem[Fan et~al.(2023{\natexlab{a}})Fan, Fang, Gu, and
  Zhang]{fan2023environment}
Jianqing Fan, Cong Fang, Yihong Gu, and Tong Zhang.
\newblock Environment invariant linear least squares.
\newblock \emph{arXiv preprint arXiv:2303.03092}, 2023{\natexlab{a}}.

\bibitem[Fan et~al.(2023{\natexlab{b}})Fan, Yang, and Yu]{fan2023understanding}
Jianqing Fan, Zhuoran Yang, and Mengxin Yu.
\newblock Understanding implicit regularization in over-parameterized single
  index model.
\newblock \emph{Journal of the American Statistical Association}, 118\penalty0
  (544):\penalty0 2315--2328, 2023{\natexlab{b}}.

\bibitem[Ghassami et~al.(2017)Ghassami, Salehkaleybar, Kiyavash, and
  Zhang]{ghassami2017learning}
AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash, and Kun Zhang.
\newblock Learning causal structures using regression invariance.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Gissin et~al.(2019)Gissin, Shalev-Shwartz, and
  Daniely]{gissin2019implicit}
Daniel Gissin, Shai Shalev-Shwartz, and Amit Daniely.
\newblock The implicit bias of depth: How incremental learning drives
  generalization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Gu et~al.(2024)Gu, Fang, B{\"u}hlmann, and Fan]{gu2024causality}
Yihong Gu, Cong Fang, Peter B{\"u}hlmann, and Jianqing Fan.
\newblock Causality pursuit from heterogeneous environments via neural
  adversarial invariance learning.
\newblock \emph{arXiv preprint arXiv:2405.04715}, 2024.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar2017implicit}
Suriya Gunasekar, Blake~E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur,
  and Nati Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018implicit}
Suriya Gunasekar, Jason~D Lee, Daniel Soudry, and Nati Srebro.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[HaoChen et~al.(2021)HaoChen, Wei, Lee, and Ma]{haochen2021shape}
Jeff~Z HaoChen, Colin Wei, Jason Lee, and Tengyu Ma.
\newblock Shape matters: Understanding the implicit bias of the noise
  covariance.
\newblock In \emph{Conference on Learning Theory}, pages 2315--2357. PMLR,
  2021.

\bibitem[Idrissi et~al.(2022)Idrissi, Arjovsky, Pezeshki, and
  Lopez-Paz]{idrissi2022simple}
Badr~Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz.
\newblock Simple data balancing achieves competitive worst-group-accuracy.
\newblock In \emph{Conference on Causal Learning and Reasoning}, pages
  336--351. PMLR, 2022.

\bibitem[Ji and Telgarsky(2019)]{ji2019gradient}
Ziwei Ji and Matus Telgarsky.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Ji and Telgarsky(2020)]{ji2020directional}
Ziwei Ji and Matus Telgarsky.
\newblock Directional convergence and alignment in deep learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Jiang et~al.(2023)Jiang, Chen, and Ding]{jiang2023algorithmic}
Liwei Jiang, Yudong Chen, and Lijun Ding.
\newblock Algorithmic regularization in model-free overparametrized asymmetric
  matrix factorization.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 5\penalty0
  (3):\penalty0 723--744, 2023.

\bibitem[Jin et~al.(2023)Jin, Li, Lyu, Du, and Lee]{jin2023understanding}
Jikai Jin, Zhiyuan Li, Kaifeng Lyu, Simon~Shaolei Du, and Jason~D Lee.
\newblock Understanding incremental learning of gradient descent: A
  fine-grained analysis of matrix sensing.
\newblock In \emph{International Conference on Machine Learning}, pages
  15200--15238. PMLR, 2023.

\bibitem[Kairouz et~al.(2021)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji,
  Bonawitz, Charles, Cormode, Cummings, et~al.]{kairouz2021advances}
Peter Kairouz, H~Brendan McMahan, Brendan Avent, Aur{\'e}lien Bellet, Mehdi
  Bennis, Arjun~Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham
  Cormode, Rachel Cummings, et~al.
\newblock Advances and open problems in federated learning.
\newblock \emph{Foundations and trends{\textregistered} in machine learning},
  14\penalty0 (1--2):\penalty0 1--210, 2021.

\bibitem[Kalimeris et~al.(2019)Kalimeris, Kaplun, Nakkiran, Edelman, Yang,
  Barak, and Zhang]{kalimeris2019sgd}
Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan
  Yang, Boaz Barak, and Haofeng Zhang.
\newblock Sgd on neural networks learns functions of increasing complexity.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Kamath et~al.(2021)Kamath, Tangella, Sutherland, and
  Srebro]{kamath2021does}
Pritish Kamath, Akilesh Tangella, Danica Sutherland, and Nathan Srebro.
\newblock Does invariant risk minimization capture invariance?
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 4069--4077. PMLR, 2021.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2020scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
  Stich, and Ananda~Theertha Suresh.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  5132--5143. PMLR, 2020.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Hu, Beirami, and Smith]{li2021ditto}
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith.
\newblock Ditto: Fair and robust federated learning through personalization.
\newblock In \emph{International Conference on Machine Learning}, pages
  6357--6368. PMLR, 2021{\natexlab{a}}.

\bibitem[Li et~al.(2018)Li, Ma, and Zhang]{li2018algorithmic}
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock In \emph{Conference on Learning Theory}, pages 2--47. PMLR, 2018.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Wang, and Arora]{li2021happens}
Zhiyuan Li, Tianhao Wang, and Sanjeev Arora.
\newblock What happens after sgd reaches zero loss?--a mathematical framework.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{b}}.

\bibitem[Lin et~al.(2022{\natexlab{a}})Lin, Han, Li, and
  Zhang]{lin2022personalized}
Shiyun Lin, Yuze Han, Xiang Li, and Zhihua Zhang.
\newblock Personalized federated learning towards communication efficiency,
  robustness and fairness.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 30471--30485, 2022{\natexlab{a}}.

\bibitem[Lin et~al.(2022{\natexlab{b}})Lin, Dong, Wang, and
  Zhang]{lin2022bayesian}
Yong Lin, Hanze Dong, Hao Wang, and Tong Zhang.
\newblock Bayesian invariant risk minimization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 16021--16030, 2022{\natexlab{b}}.

\bibitem[Lin et~al.(2022{\natexlab{c}})Lin, Zhu, Tan, and Cui]{lin2022zin}
Yong Lin, Shengyu Zhu, Lu~Tan, and Peng Cui.
\newblock Zin: When and how to learn invariance without environment partition?
\newblock \emph{Advances in Neural Information Processing Systems}, 35,
  2022{\natexlab{c}}.

\bibitem[Lu et~al.(2021)Lu, Wu, Hern{\'a}ndez-Lobato, and
  Sch{\"o}lkopf]{lu2021nonlinear}
Chaochao Lu, Yuhuai Wu, Jo{\'s}e~Miguel Hern{\'a}ndez-Lobato, and Bernhard
  Sch{\"o}lkopf.
\newblock Nonlinear invariant risk minimization: A causal approach.
\newblock \emph{arXiv preprint arXiv:2102.12353}, 2021.

\bibitem[Lu et~al.(2023)Lu, Wu, Yang, and Zou]{lu2023benign}
Miao Lu, Beining Wu, Xiaodong Yang, and Difan Zou.
\newblock Benign oscillation of stochastic gradient descent with large learning
  rate.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Lyu et~al.(2022)Lyu, Li, and Arora]{lyu2022understanding}
Kaifeng Lyu, Zhiyuan Li, and Sanjeev Arora.
\newblock Understanding the generalization benefit of normalization layers:
  Sharpness reduction.
\newblock \emph{Advances in Neural Information Processing Systems}, 35, 2022.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial intelligence and statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem[Meinshausen et~al.(2016)Meinshausen, Hauser, Mooij, Peters, Versteeg,
  and B{\"u}hlmann]{meinshausen2016methods}
Nicolai Meinshausen, Alain Hauser, Joris~M Mooij, Jonas Peters, Philip
  Versteeg, and Peter B{\"u}hlmann.
\newblock Methods for causal inference from gene perturbation experiments and
  validation.
\newblock \emph{Proceedings of the National Academy of Sciences}, 113\penalty0
  (27):\penalty0 7361--7368, 2016.

\bibitem[Nacson et~al.(2019)Nacson, Lee, Gunasekar, Savarese, Srebro, and
  Soudry]{nacson2019convergence}
Mor~Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique~Pamplona
  Savarese, Nathan Srebro, and Daniel Soudry.
\newblock Convergence of gradient descent on separable data.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 3420--3428. PMLR, 2019.

\bibitem[Nastl and Hardt(2024)]{nastl2024predictors}
Vivian~Y Nastl and Moritz Hardt.
\newblock Predictors from causal features do not generalize better to new
  domains.
\newblock \emph{arXiv preprint arXiv:2402.09891}, 2024.

\bibitem[Peters et~al.(2016)Peters, B{\"u}hlmann, and
  Meinshausen]{peters2016causal}
Jonas Peters, Peter B{\"u}hlmann, and Nicolai Meinshausen.
\newblock Causal inference by using invariant prediction: identification and
  confidence intervals.
\newblock \emph{Journal of the Royal Statistical Society Series B: Statistical
  Methodology}, 78\penalty0 (5):\penalty0 947--1012, 2016.

\bibitem[Rosenfeld et~al.(2021)Rosenfeld, Ravikumar, and
  Risteski]{rosenfeld2021risks}
Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski.
\newblock The risks of invariant risk minimization.
\newblock In \emph{International Conference on Learning Representations},
  volume~9, 2021.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[St{\"o}ger and Soltanolkotabi(2021)]{stoger2021small}
Dominik St{\"o}ger and Mahdi Soltanolkotabi.
\newblock Small random initialization is akin to spectral learning:
  Optimization and generalization guarantees for overparameterized low-rank
  matrix reconstruction.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Vershynin(2018)]{vershynin2018high}
Roman Vershynin.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[Vivien et~al.(2022)Vivien, Reygner, and Flammarion]{vivien2022label}
Loucas~Pillaud Vivien, Julien Reygner, and Nicolas Flammarion.
\newblock Label noise (stochastic) gradient descent implicitly solves the lasso
  for quadratic parametrisation.
\newblock In \emph{Conference on Learning Theory}, pages 2127--2159. PMLR,
  2022.

\bibitem[Wald et~al.(2023)Wald, Yona, Shalit, and Carmon]{wald2023malign}
Yoav Wald, Gal Yona, Uri Shalit, and Yair Carmon.
\newblock Malign overfitting: Interpolation and invariance are fundamentally at
  odds.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Zhang et~al.(2020)Zhang, Lyle, Sodhani, Filos, Kwiatkowska, Pineau,
  Gal, and Precup]{zhang2020invariant}
Amy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle
  Pineau, Yarin Gal, and Doina Precup.
\newblock Invariant causal prediction for block mdps.
\newblock In \emph{International Conference on Machine Learning}, pages
  11214--11224. PMLR, 2020.

\bibitem[Zhang et~al.(2023)Zhang, Bauer, Bennett, Gao, Gong, Hilmkil, Jennings,
  Ma, Minka, Pawlowski, et~al.]{zhang2023understanding}
Cheng Zhang, Stefan Bauer, Paul Bennett, Jiangfeng Gao, Wenbo Gong, Agrin
  Hilmkil, Joel Jennings, Chao Ma, Tom Minka, Nick Pawlowski, et~al.
\newblock Understanding causality with large language models: Feasibility and
  opportunities.
\newblock \emph{arXiv preprint arXiv:2304.05524}, 2023.

\bibitem[Zhuo et~al.(2021)Zhuo, Kwon, Ho, and Caramanis]{zhuo2021computational}
Jiacheng Zhuo, Jeongyeol Kwon, Nhat Ho, and Constantine Caramanis.
\newblock On the computational and statistical complexity of over-parameterized
  matrix sensing.
\newblock \emph{arXiv preprint arXiv:2102.02756}, 2021.

\end{thebibliography}
