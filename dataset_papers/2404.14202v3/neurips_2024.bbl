\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and
  Szepesv{\'a}ri]{abbasi2011improved}
Yasin Abbasi-Yadkori, D{\'a}vid P{\'a}l, and Csaba Szepesv{\'a}ri.
\newblock Improved algorithms for linear stochastic bandits.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, Freund, and Schapire]{auer}
Peter Auer, Nicol{\' o} Cesa-Bianchi, Yoav Freund, and Robert~E. Schapire.
\newblock The nonstochastic multiarmed bandit problem.
\newblock \emph{SIAM Journal on Computing}, 32\penalty0 (1):\penalty0 48--77,
  2002.

\bibitem[Auer et~al.(2007)Auer, Ortner, and Szepesv{\'a}ri]{auer2007improved}
Peter Auer, Ronald Ortner, and Csaba Szepesv{\'a}ri.
\newblock Improved rates for the stochastic continuum-armed bandit problem.
\newblock In \emph{International Conference on Computational Learning Theory},
  pages 454--468. Springer, 2007.

\bibitem[Auer et~al.(2019)Auer, Gajane, and Ortner]{auer2019adaptively}
Peter Auer, Pratik Gajane, and Ronald Ortner.
\newblock Adaptively tracking the best bandit arm with an unknown number of
  distribution changes.
\newblock In \emph{Conference on Learning Theory}, pages 138--158. PMLR, 2019.

\bibitem[Bayati et~al.(2020)Bayati, Hamidi, Johari, and Khosravi]{Bayati}
Mohsen Bayati, Nima Hamidi, Ramesh Johari, and Khashayar Khosravi.
\newblock The unreasonable effectiveness of greedy algorithms in multi-armed
  bandit with many arms.
\newblock \emph{arXiv preprint arXiv:2002.10121}, 2020.

\bibitem[Berry et~al.(1997)Berry, Chen, Zame, Heath, and Shepp]{Berry}
Donald~A. Berry, Robert~W. Chen, Alan Zame, David~C. Heath, and Larry~A. Shepp.
\newblock {Bandit problems with infinitely many arms}.
\newblock \emph{The Annals of Statistics}, 25\penalty0 (5):\penalty0 2103 --
  2116, 1997.

\bibitem[Besbes et~al.(2014)Besbes, Gur, and Zeevi]{besbes2014stochastic}
Omar Besbes, Yonatan Gur, and Assaf Zeevi.
\newblock Stochastic multi-armed-bandit problem with non-stationary rewards.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Bonald and Prouti\`{e}re(2013)]{Bonald}
Thomas Bonald and Alexandre Prouti\`{e}re.
\newblock Two-target algorithms for infinite-armed bandits with bernoulli
  rewards.
\newblock In \emph{Proceedings of the 26th International Conference on Neural
  Information Processing Systems - Volume 2}, NIPS'13, page 2184–2192, Red
  Hook, NY, USA, 2013. Curran Associates Inc.

\bibitem[Brown(2011)]{brown2011wasted}
Daniel~G Brown.
\newblock How {I} wasted too long finding a concentration inequality for sums
  of geometric variables.
\newblock \emph{Found at https://cs. uwaterloo. ca/\~{} browndg/negbin. pdf},
  8\penalty0 (4), 2011.

\bibitem[Carpentier and Valko(2015)]{Carpentier}
Alexandra Carpentier and Michal Valko.
\newblock Simple regret for infinitely many armed bandits.
\newblock In \emph{Proceedings of the 32nd International Conference on
  International Conference on Machine Learning - Volume 37}, ICML'15, page
  1133–1141. JMLR.org, 2015.

\bibitem[Cheung et~al.(2019)Cheung, Simchi-Levi, and Zhu]{cheung2019learning}
Wang~Chi Cheung, David Simchi-Levi, and Ruihao Zhu.
\newblock Learning to optimize under non-stationarity.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 1079--1087. PMLR, 2019.

\bibitem[Dylan J.~Foster(2022)]{foster}
Alexander~Rakhlin Dylan J.~Foster.
\newblock Statistical reinforcement learning and decision making: Course notes.
\newblock \emph{MIT Lecture notes for course 9.S915 Fall 2022}, 2022.

\bibitem[Kim et~al.(2022)Kim, Vojnovic, and Yun]{kim2022rotting}
Jung-hun Kim, Milan Vojnovic, and Se-Young Yun.
\newblock Rotting infinitely many-armed bandits.
\newblock In \emph{International Conference on Machine Learning}, pages
  11229--11254. PMLR, 2022.

\bibitem[Kleinberg(2004)]{kleinberg2004nearly}
Robert Kleinberg.
\newblock Nearly tight bounds for the continuum-armed bandit problem.
\newblock \emph{Advances in Neural Information Processing Systems}, 17, 2004.

\bibitem[Lattimore and Szepesv{\' a}ri(2020)]{lattimore}
Tor Lattimore and Csaba Szepesv{\' a}ri.
\newblock \emph{Bandit Algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Levine et~al.(2017)Levine, Crammer, and Mannor]{levine2017rotting}
Nir Levine, Koby Crammer, and Shie Mannor.
\newblock Rotting bandits.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Li et~al.(2010)Li, Chu, Langford, and Schapire]{li2010contextual}
Lihong Li, Wei Chu, John Langford, and Robert~E Schapire.
\newblock A contextual-bandit approach to personalized news article
  recommendation.
\newblock In \emph{Proceedings of the 19th international conference on World
  wide web}, pages 661--670, 2010.

\bibitem[Rigollet and H{\"u}tter(2015)]{rigollet2015high}
Phillippe Rigollet and Jan-Christian H{\"u}tter.
\newblock High dimensional statistics.
\newblock \emph{Lecture notes for course 18S997}, 813:\penalty0 814, 2015.

\bibitem[Russac et~al.(2019)Russac, Vernade, and Capp{\'e}]{russac2019weighted}
Yoan Russac, Claire Vernade, and Olivier Capp{\'e}.
\newblock Weighted linear bandits for non-stationary environments.
\newblock \emph{arXiv preprint arXiv:1909.09146}, 2019.

\bibitem[Seznec et~al.(2019)Seznec, Locatelli, Carpentier, Lazaric, and
  Valko]{seznec2019rotting}
Julien Seznec, Andrea Locatelli, Alexandra Carpentier, Alessandro Lazaric, and
  Michal Valko.
\newblock Rotting bandits are no harder than stochastic ones.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 2564--2572. PMLR, 2019.

\bibitem[Seznec et~al.(2020)Seznec, Menard, Lazaric, and Valko]{Seznec2}
Julien Seznec, Pierre Menard, Alessandro Lazaric, and Michal Valko.
\newblock A single algorithm for both restless and rested rotting bandits.
\newblock In Silvia Chiappa and Roberto Calandra, editors, \emph{Proceedings of
  the Twenty Third International Conference on Artificial Intelligence and
  Statistics}, volume 108 of \emph{Proceedings of Machine Learning Research},
  pages 3784--3794. PMLR, 26--28 Aug 2020.

\bibitem[Tsun(2020)]{Alex}
Alex Tsun.
\newblock Probability \& statistics with applications to computing, 2020.
\newblock URL \url{https://www.alextsun.com/files/Prob_Stat_for_CS_Book.pdf}.

\bibitem[Villar et~al.(2015)Villar, Bowden, and Wason]{villar2015multi}
Sof{\'\i}a~S Villar, Jack Bowden, and James Wason.
\newblock Multi-armed bandit models for the optimal design of clinical trials:
  benefits and challenges.
\newblock \emph{Statistical science: a review journal of the Institute of
  Mathematical Statistics}, 30\penalty0 (2):\penalty0 199, 2015.

\bibitem[Wang et~al.(2008)Wang, Audibert, and Munos]{wang}
Yizao Wang, Jean-Yves Audibert, and R{\'e}mi Munos.
\newblock Algorithms for infinitely many-armed bandits.
\newblock \emph{Advances in Neural Information Processing Systems}, 21, 2008.

\bibitem[Wei and Luo(2021)]{wei2021non}
Chen-Yu Wei and Haipeng Luo.
\newblock Non-stationary reinforcement learning without prior knowledge: An
  optimal black-box approach.
\newblock In \emph{Conference on learning theory}, pages 4300--4354. PMLR,
  2021.

\end{thebibliography}
