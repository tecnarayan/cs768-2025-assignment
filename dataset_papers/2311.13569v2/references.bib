@inproceedings{agarwal2021rliable,
title	= {Deep Reinforcement Learning at the Edge of the Statistical Precipice},
author	= {Rishabh Agarwal and Max Schwarzer and Pablo Samuel Castro and Aaron Courville and Marc G. Bellemare},
year	= {2021},
booktitle	= {Advances in Neural Information Processing Systems},
url = {https://arxiv.org/abs/2108.13264},
}

@inproceedings{Adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
}

@inproceedings{Attention_all,
title={Attention Is All You Need},
author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan and Kaiser, Lukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems},
year={2017}}

@article{reinforce,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J.},
  journal={Machine Learning},
  year={1992},
  volume={8},
  pages={229-256},
}

@article{Mnih2015,
  title={Human-level control through deep reinforcement learning},
  author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Andrei A. Rusu and Joel Veness and Marc G. Bellemare and Alex Graves and Martin A. Riedmiller and Andreas Fidjeland and Georg Ostrovski and Stig Petersen and Charlie Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and Dharshan Kumaran and Daan Wierstra and Shane Legg and Demis Hassabis},
  journal={Nature},
  year={2015},
  volume={518},
  pages={529-533},
}

@article{wavenet,
title	= {WaveNet: A Generative Model for Raw Audio},
author	= {Aäron van den Oord and Sander Dieleman and Heiga Zen and Karen Simonyan and Oriol Vinyals and Alexander Graves and Nal Kalchbrenner and Andrew Senior and Koray Kavukcuoglu},
year	= {2016},
journal={ArXiv},
volume={abs/1609.03499},
}

@inproceedings{gpt3,
title={Language Models are Few-Shot Learners},
booktitle={Advances in Neural Information Processing Systems},
author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris},
year={2020}}


@article{logistics,
  title={Combinatorial optimization and Green Logistics},
  author={Sbihi, Abdelkade and Eglese, Richard W.},
  journal={4OR},
  year={2007},
  volume={5},
  pages={99-116},
}

@article{fund_science, title={Refuting conjectures in extremal combinatorics via linear programming}, volume={169}, url={https://www.sciencedirect.com/science/article/abs/pii/S0097316519301116}, DOI={10.1016/j.jcta.2019.105130}, journal={Journal of Combinatorial Theory, Series A}, author={Wagner, Adam Zsolt}, year={2020}, month={Jan}, pages={105130}}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@misc{concorde,
title={Concorde {TSP} solver},
author={Applegate, David and Bixby, Ribert and Chvatal, Vasek and Cook, William},
year={2006}
}

@article{lkh3,
  author = {K. Helsgaun},
  title = {An extension of the lin-kernighan-helsgaun tsp
solver for constrained traveling salesman and vehicle routing
problems},
  journal  = {Roskilde University, Tech. Rep.},
  year = {2017},
}

@misc{jax2018,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@book{Sutton1998,
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  publisher = {The MIT Press},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}

@article{MAZYAVKINA2021,
title = {Reinforcement learning for combinatorial optimization: A survey},
journal = {Computers \& Operations Research},
volume = {134},
pages = {105400},
year = {2021},
author = {Nina Mazyavkina and Sergey Sviridov and Sergei Ivanov and Evgeny Burnaev},
}

@article{BENGIO2021,
title = {Machine learning for combinatorial optimization: A methodological tour d’horizon},
journal = {European Journal of Operational Research},
volume = {290},
number = {2},
pages = {405-421},
year = {2021},
author = {Yoshua Bengio and Andrea Lodi and Antoine Prouvost},
}

@article{Silver2017,
  title={Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
  author={David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and L. Sifre and Dharshan Kumaran and Thore Graepel and Timothy P. Lillicrap and Karen Simonyan and Demis Hassabis},
  journal={ArXiv},
  year={2017},
  volume={abs/1712.01815},
}

% Improvement methods
@inproceedings{Chen2019,
  author    = {Xinyun Chen and
               Yuandong Tian},
  title     = {Learning to Perform Local Rewriting for Combinatorial Optimization},
  booktitle={Advances in Neural Information Processing Systems},
  year = {2019},
}

@inproceedings{
    hottung2020neural,
    title={Neural Large Neighborhood Search for the Capacitated Vehicle Routing Problem},
    author={Andr{\'e} Hottung and Kevin Tierney},
    booktitle={24th European Conference on Artificial Intelligence (ECAI 2020)},
    year={2020}
}


@inproceedings{Lu2020,
author = {Hao Lu and Xingwen Zhang and Shuang Yang},
title = {A Learning-based Iterative Method for Solving Vehicle Routing Problems},
booktitle={International Conference on Learning Representations},
year      = {2020},
}

@inproceedings{Costa2020,
author = {Paulo R. de O. da Costa and
           Jason Rhuggenaath and
           Yingqian Zhang and
           Alp Akcay},
title = {Learning 2-opt Heuristics for the Traveling Salesman Problem via Deep Reinforcement Learning},
booktitle={Asian Conference
on Machine Learning},
year      = {2020},
}

@inproceedings{Hottung2021,
author = {André Hottung and Bhanu Bhandari and Kevin Tierney},
title = {Learning a latent search space for routing
problems using variational autoencoders},
booktitle={International Conference on Learning Representations},
year      = {2021},
}

% Construction methods

@article{Hopfield85,
author = {Hopfield, J J and Tank, W David},
title = {“Neural” computation of decisions in optimization problems},
journal = {Biological cybernetics},
volume = {52},
number = {3},
pages = {141–152},
year = {1985},
}

@inproceedings{pointernetworks,
  author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  title = {Pointer Networks},
  booktitle={Advances in Neural Information Processing Systems},
  year = {2015},
}

@article{Bello16,
  author = {Bello, Irwan and Pham, Hieu and Le, Quoc V. and Norouzi, Mohammad and Bengio, Samy},
  title = {Neural Combinatorial Optimization with Reinforcement Learning},
  journal  = {arXiv preprint arXiv:1611.09940},
  year = {2016},
}

@inproceedings{Dai17,
  author = {Dai, Hanjun and Khalil, Elias B. and Zhang, Yuyu and Dilkina, Bistra and Song, Le},
  title = {Learning Combinatorial Optimization Algorithms over Graphs},
  booktitle={Advances in Neural Information Processing Systems},
  year = {2017},
}

@inproceedings{Khalil2017,
 author = {Khalil, Elias and Dai, Hanjun and Zhang, Yuyu and Dilkina, Bistra and Song, Le},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Learning Combinatorial Optimization Algorithms over Graphs},
 year = {2017},
}

@InProceedings{Deudon2018,
author="Deudon, Michel
and Cournut, Pierre
and Lacoste, Alexandre
and Adulyasak, Yossiri
and Rousseau, Louis-Martin",
title="Learning Heuristics for the TSP by Policy Gradient",
booktitle="Integration of Constraint Programming, Artificial Intelligence, and Operations Research",
year="2018",
publisher="Springer International Publishing",
pages="170--181",
}

@inproceedings{Nazari2018,
  author = {Nazari, Mohammadreza and Oroojlooy, Afshin and Snyder, Lawrence V. and Takáč, Martin},
booktitle={Advances in Neural Information Processing Systems},
  title = {Reinforcement Learning for Solving the Vehicle Routing Problem},
  year = {2018},
}

@inproceedings{Zhuwen2018,
  author = {Li, Zhuwen and Chen, Qifeng and Koltun, Vladlen},
  title = {Combinatorial Optimization with Graph Convolutional Networks and Guided Tree Search},
  booktitle={Advances in Neural Information Processing Systems},
  year = {2018},
}

@inproceedings{Tom2019,
  author = {Barrett, Thomas D. and Clements, William R. and Foerster, Jakob N. and Lvovsky, A. I.},
  title = {Exploratory Combinatorial Optimization with Reinforcement Learning},
  booktitle = {	In Proceedings of the 34th National Conference on Artificial Intelligence, AAAI},
  year = {2020},
}

@inproceedings{XinSCZ21,
  author    = {Liang Xin and
               Wen Song and
               Zhiguang Cao and
               Jie Zhang},
  title     = {Multi-Decoder Attention Model with Embedding Glimpse for Solving Vehicle
               Routing Problems},
  booktitle = {In Proceedings of the 35th National Conference on Artificial Intelligence, AAAI},
  year      = {2021},
}

@inproceedings{POMO,
author = {Kwon, Yeong-Dae and  Jinho Choo, Byoungjip Kim and Iljoo Yoon, Youngjune Gwon and Seungjai Min},
booktitle={Advances in Neural Information Processing Systems},
title = {POMO: Policy Optimization with Multiple Optima for Reinforcement Learning},
year = {2020},
}

@inproceedings{kim2022symnco,
  title={Sym-NCO: Leveraging Symmetricity for Neural Combinatorial Optimization},
  author={Kim, Minsu and Park, Junyoung and Park, Jinkyoo},
  booktitle={Advances in Neural Information Processing Systems},
  pages={},
  year={2022},
  doi={10.48550/arXiv.2205.13209}
}

@article{poppy,
  title={Population-Based Reinforcement Learning for Combinatorial Optimization},
  author={Grinsztajn, Nathan and Furelos-Blanco, Daniel and Barrett, Thomas D.},
  journal={arXiv preprint arXiv:2210.03475},
  year={2022},
}


@inproceedings{Kool2019,
  author = {Kool, Wouter and van Hoof, Herke and Welling, Max},
  title = {Attention, Learn to Solve Routing Problems!},
  booktitle = {International Conference on Learning Representations},
  year = {2019},
}

@inproceedings{Zhang2020,
  author    = {Cong Zhang and
               Wen Song and
               Zhiguang Cao and
               Jie Zhang and
               Puay Siew Tan and
               Chi Xu},
 booktitle={Advances in Neural Information Processing Systems},
  title     = {Learning to Dispatch for Job Shop Scheduling via Deep Reinforcement
               Learning},
 year = {2020},
}

@article{Joshi2019,
  author    = {Chaitanya K. Joshi and
               Thomas Laurent and
               Xavier Bresson},
  title     = {An Efficient Graph Convolutional Network Technique for the Travelling
               Salesman Problem},
  journal      = {arXiv preprint arXiv:1906.01227},
  year      = {2019},
}

@inproceedings{choo2022simulationguided,
  title={Simulation-guided Beam Search for Neural Combinatorial Optimization},
  author={Choo, Jinho and Kwon, Yeong-Dae and Kim, Jihoon and Jae, Jeongwoo and Hottung, André and Tierney, Kevin and Gwon, Youngjune},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022},
  url={https://arxiv.org/abs/2207.06190},
  eprint={2207.06190},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}


@inproceedings{Fu2021,
  author    = {Zhang{-}Hua Fu and
               Kai{-}Bin Qiu and
               Hongyuan Zha},
  title     = {Generalize a Small Pre-trained Model to Arbitrarily Large {TSP} Instances},
  booktitle = {Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI}
               2021, Thirty-Third Conference on Innovative Applications of Artificial
               Intelligence, {IAAI} 2021, The Eleventh Symposium on Educational Advances
               in Artificial Intelligence, {EAAI} 2021, Virtual Event, February 2-9,
               2021},
  pages     = {7474--7482},
  publisher = {{AAAI} Press},
  year      = {2021},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/16916},
  timestamp = {Wed, 02 Jun 2021 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/conf/aaai/FuQZ21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{grinsztajn2021,
 author = {Grinsztajn, Nathan and Beaumont, Olivier and Jeannot, Emmanuel and Preux, Philippe},
 booktitle = {IEEE International Conference on Cluster Computing (CLUSTER)},
 title = {READYS: A Reinforcement Learning Based Strategy for Heterogeneous Dynamic Scheduling},
 year = {2021},
}

@article{Kool2021,
  author    = {Kool, Wouter and van Hoof, Herke and Gromicho, Joaquim and Welling, Max},
  title     = {Deep Policy Dynamic Programming for Vehicle Routing Problems},
  year      = {2021},
  journal      = {arXiv preprint arXiv:2102.11756},
}

@article{ecord,
  author    = {Barrett, Thomas and Parsonson, Christopher and Laterre, Alexandre},
  title     = {Learning to Solve Combinatorial Graph Partitioning Problems via Efficient Exploration},
  year      = {2022},
  journal      = {arXiv preprint arXiv:2205.14105},
}

@inproceedings{hottung2022efficient,
      title={Efficient Active Search for Combinatorial Optimization Problems}, 
      author={André Hottung and Yeong-Dae Kwon and Kevin Tierney},
      booktitle = {International Conference on Learning Representations},
      year={2022},

}

% Population-based RL

 @article{Doan_Mazoure2020, title={Attraction-Repulsion Actor-Critic for Continuous Control Reinforcement Learning}, url={http://arxiv.org/abs/1909.07543},
 journal={arXiv preprint arXiv:1909.07543},
 author={Doan, Thang and Mazoure, Bogdan and Abdar, Moloud and Durand, Audrey and Pineau, Joelle and Hjelm, R. Devon},
 year={2020}}
 
@inproceedings{Eysenbach2018, title={Diversity is All You Need: Learning Skills without a Reward Function},
      booktitle = {International Conference on Learning Representations}, 
      author={Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey}, year={2019}}
 
@inproceedings{Flajolet2022, title={Fast Population-Based Reinforcement Learning on a Single Machine},
author={Flajolet, Arthur and Monroc, Claire Bizon and Beguir, Karim and Pierrot, Thomas},
year={2022},
booktitle = {International Conference on Machine Learning}}
 
@inproceedings{Gupta2018, title={Meta-Reinforcement Learning of Structured Exploration Strategies},
booktitle = {International Conference on Neural Information Processing Systems},
author={Gupta, Abhishek and Mendonca, Russell and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey}, year={2018}}
 
@inproceedings{Hartikainen2020, title={Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery},
 author={Hartikainen, Kristian and Geng, Xinyang and Haarnoja, Tuomas and Levine, Sergey}, year={2020},
 booktitle = {International Conference on Learning Representations}, 
 }
 
@inproceedings{Hong2018, title={Diversity-Driven Exploration Strategy for Deep Reinforcement Learning}, booktitle = {International Conference on Neural Information Processing Systems}, author={Hong, Zhang-Wei and Shann, Tzu-Yun and Su, Shih-Yang and Chang, Yi-Hsiang and Lee, Chun-Yi}, year={2018}}
 
@inproceedings{Jackson2019, title={Novelty Search for Deep Reinforcement Learning Policy Network Weights by Action Sequence Edit Metric Distance}, booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion}, author={Jackson, Ethan C. and Daley, Mark}, year={2019}, month={Feb} }
 
 @article{Jaderberg2018, title={Human-level performance in first-person multiplayer games with population-based deep reinforcement learning}, volume={364}, ISSN={0036-8075, 1095-9203}, DOI={10.1126/science.aau6249},note={arXiv:1807.01281 [cs, stat]}, number={6443}, journal={Science}, author={Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Castaneda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore}, year={2018}, month={Jul}, pages={859–865}}
 
 @article{Jaderberg2017, title={Population Based Training of Neural Networks}, journal={arXiv preprint arXiv:1711.09846}, author={Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray}, year={2017}}
 
 @inproceedings{Jung2020, title={Population-Guided Parallel Policy Search for Reinforcement Learning}, booktitle = {International Conference on Learning Representations}, author={Jung, Whiyoung and Park, Giseung and Sung, Youngchul}, year={2020}}
 
 @inproceedings{Khadka2020, title={Evolutionary Reinforcement Learning for Sample-Efficient Multiagent Coordination}, booktitle = {International Conference on Machine Learning}, author={Khadka, Shauharda and Majumdar, Somdeb and Miret, Santiago and McAleer, Stephen and Tumer, Kagan}, year={2020}}
 
 @inproceedings{Khadka2019, title={Collaborative Evolutionary Reinforcement Learning},  booktitle = {International Conference on Machine Learning}, author={Khadka, Shauharda and Majumdar, Somdeb and Nassar, Tarek and Dwiel, Zach and Tumer, Evren and Miret, Santiago and Liu, Yinyin and Tumer, Kagan}, year={2019}}
 
 @inproceedings{Khadka2018, title={Evolution-Guided Policy Gradient in Reinforcement Learning}, booktitle={Conference on Neural Information Processing Systems}, author={Khadka, Shauharda and Tumer, Kagan}, year={2018}}
 
 @inproceedings{Kim2021, title={Learning Collaborative Policies to Solve NP-hard Routing Problems}, volume={34},
 url={https://proceedings.neurips.cc/paper/2021/hash/564127c03caab942e503ee6f810f54fd-Abstract.html}, booktitle={Advances in Neural Information Processing Systems}, publisher={Curran Associates, Inc.}, author={Kim, Minsu and Park, Jinkyoo and kim, joungho}, year={2021}, pages={10418–10430} }
 
 @inproceedings{Liu2019, title={Emergent Coordination Through Competition}, booktitle = {International Conference on Learning Representations}, author={Liu, Siqi and Lever, Guy and Merel, Josh and Tunyasuvunakool, Saran and Heess, Nicolas and Graepel, Thore}, year={2019}}
 
 @inproceedings{Ma_Li2021, title={Learning to Iteratively Solve Routing Problems with Dual-Aspect Collaborative Transformer}, volume={34}, url={https://proceedings.neurips.cc/paper/2021/hash/5c53292c032b6cb8510041c54274e65f-Abstract.html}, booktitle={Advances in Neural Information Processing Systems}, publisher={Curran Associates, Inc.}, author={Ma, Yining and Li, Jingwen and Cao, Zhiguang and Song, Wen and Zhang, Le and Chen, Zhenghua and Tang, Jing}, year={2021}, pages={11096–11107} }
 
 @article{Mouret2015, title={Illuminating search spaces by mapping elites}, journal={arXiv preprint arXiv:1504.04909}, author={Mouret, Jean-Baptiste and Clune, Jeff}, year={2015}, month={Apr} }
 
 @inproceedings{Parker-Holder2020, title={Effective Diversity in Population Based Reinforcement Learning}, booktitle={Advances in Neural Information Processing Systems}, author={Parker-Holder, Jack and Pacchiano, Aldo and Choromanski, Krzysztof and Roberts, Stephen}, year={2020}}
 
 @inproceedings{Pierrot2022, title={Diversity Policy Gradient for Sample Efficient Quality-Diversity Optimization},
 booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
 author={Pierrot, Thomas and Macé, Valentin and Chalumeau, Félix and Flajolet, Arthur and Cideron, Geoffrey and Beguir, Karim and Cully, Antoine and Sigaud, Olivier and Perrin-Gilbert, Nicolas}, year={2022}}
 
 @inproceedings{Pong_Dalal2020, title={Skew-Fit: State-Covering Self-Supervised Reinforcement Learning},
 booktitle={International Conference on Machine Learning},
 author={Pong, Vitchyr H. and Dalal, Murtaza and Lin, Steven and Nair, Ashvin and Bahl, Shikhar and Levine, Sergey}, year={2020}}
 
 @inproceedings{Pourchot_Sigaud2019, title={CEM-RL: Combining evolutionary and gradient-based methods for policy search}, 
 booktitle = {International Conference on Learning Representations},
 author={Pourchot, Aloïs and Sigaud, Olivier}, year={2019}}

@ARTICLE{QD1,
author={Pugh, Justin K. and Soros, Lisa B. and Stanley, Kenneth O.},   
title={Quality Diversity: A New Frontier for Evolutionary Computation},      
journal={Frontiers in Robotics and AI},      
volume={3},           
year={2016},      
doi={10.3389/frobt.2016.00040},      
issn={2296-9144},   
}

@ARTICLE{QD2,
  author={Cully, Antoine and Demiris, Yiannis},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Quality and Diversity Optimization: A Unifying Modular Framework}, 
  year={2018},
  volume={22},
  number={2},
  pages={245-259},
  doi={10.1109/TEVC.2017.2704781}}

@inproceedings{SuttonMSM99,
  author    = {Richard S. Sutton and
               David A. McAllester and
               Satinder Singh and
               Yishay Mansour},
  title     = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {1057--1063},
  year      = {1999}
}

@misc{jumanji2023github,
  author = {Clément Bonnet and Daniel Luo and Donal Byrne and Sasha Abramowitz
        and Vincent Coyette and Paul Duckworth and Daniel Furelos-Blanco and
        Nathan Grinsztajn and Tristan Kalloniatis and Victor Le and Omayma Mahjoub
        and Laurence Midgley and Shikha Surana and Cemlyn Waters and Alexandre Laterre},
  title = {Jumanji: a Suite of Diverse and Challenging Reinforcement Learning Environments in JAX},
  url = {https://github.com/instadeepai/jumanji},
  version = {0.2.2},
  year = {2023},
}

@inproceedings{diverse_tsp_instances,
author = {Bossek, Jakob and Kerschke, Pascal and Neumann, Aneta and Wagner, Markus and Neumann, Frank and Trautmann, Heike},
title = {Evolving Diverse TSP Instances by Means of Novel and Creative Mutation Operators},
year = {2019},
isbn = {9781450362542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299904.3340307},
doi = {10.1145/3299904.3340307},
abstract = {Evolutionary algorithms have successfully been applied to evolve problem instances that exhibit a significant difference in performance for a given algorithm or a pair of algorithms inter alia for the Traveling Salesperson Problem (TSP). Creating a large variety of instances is crucial for successful applications in the blooming field of algorithm selection. In this paper, we introduce new and creative mutation operators for evolving instances of the TSP. We show that adopting those operators in an evolutionary algorithm allows for the generation of benchmark sets with highly desirable properties: (1) novelty by clear visual distinction to established benchmark sets in the field, (2) visual and quantitative diversity in the space of TSP problem characteristics, and (3) significant performance differences with respect to the restart versions of heuristic state-of-the-art TSP solvers EAX and LKH. The important aspect of diversity is addressed and achieved solely by the proposed mutation operators and not enforced by explicit diversity preservation.},
booktitle = {Proceedings of the 15th ACM/SIGEVO Conference on Foundations of Genetic Algorithms},
pages = {58–71},
numpages = {14},
keywords = {traveling salesperson problem, problem generation, optimization, instance features, benchmarking},
location = {Potsdam, Germany},
series = {FOGA '19}
}

@InProceedings{cp_rl_solver,
author="Chalumeau, F{\'e}lix
and Coulon, Ilan
and Cappart, Quentin
and Rousseau, Louis-Martin",
editor="Stuckey, Peter J.",
title="SeaPearl: A Constraint Programming Solver Guided by Reinforcement Learning",
booktitle="Integration of Constraint Programming, Artificial Intelligence, and Operations Research",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="392--409",
abstract="The design of efficient and generic algorithms for solving combinatorial optimization problems has been an active field of research for many years. Standard exact solving approaches are based on a clever and complete enumeration of the solution set. A critical and non-trivial design choice with such methods is the branching strategy, directing how the search is performed. The last decade has shown an increasing interest in the design of machine learning-based heuristics to solve combinatorial optimization problems. The goal is to leverage knowledge from historical data to solve similar new instances of a problem. Used alone, such heuristics are only able to provide approximate solutions efficiently, but cannot prove optimality nor bounds on their solution. Recent works have shown that reinforcement learning can be successfully used for driving the search phase of constraint programming (CP) solvers. However, it has also been shown that this hybridization is challenging to build, as standard CP frameworks do not natively include machine learning mechanisms, leading to some sources of inefficiencies. This paper presents the proof of concept for SeaPearl, a new CP solver implemented in Julia, that supports machine learning routines in order to learn branching decisions using reinforcement learning. Support for modeling the learning component is also provided. We illustrate the modeling and solution performance of this new solver on two problems. Although not yet competitive with industrial solvers, SeaPearl aims to provide a flexible and open-source framework in order to facilitate future research in the hybridization of constraint programming and machine learning.",
isbn="978-3-030-78230-6"
}


@article{LKH,
title = {An effective implementation of the Lin–Kernighan traveling salesman heuristic},
journal = {European Journal of Operational Research},
volume = {126},
number = {1},
pages = {106-130},
year = {2000},
issn = {0377-2217},
doi = {https://doi.org/10.1016/S0377-2217(99)00284-2},
url = {https://www.sciencedirect.com/science/article/pii/S0377221799002842},
author = {Keld Helsgaun},
keywords = {Traveling salesman problem, Heuristics, Lin–Kernighan},
abstract = {This paper describes an implementation of the Lin–Kernighan heuristic, one of the most successful methods for generating optimal or near-optimal solutions for the symmetric traveling salesman problem (TSP). Computational tests show that the implementation is highly effective. It has found optimal solutions for all solved problem instances we have been able to obtain, including a 13,509-city problem (the largest non-trivial problem instance solved to optimality today).}
}

@misc{ortools,
  title = {OR-Tools},
  version = { v9.6 },
  author = {Laurent Perron and Vincent Furnon},
  organization = {Google},
  url = {https://developers.google.com/optimization/},
  date = { 2023-03-13 }
}

@article{MAZYAVKINA2021105400,
title = {Reinforcement learning for combinatorial optimization: A survey},
journal = {Computers \& Operations Research},
volume = {134},
pages = {105400},
year = {2021},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2021.105400},
url = {https://www.sciencedirect.com/science/article/pii/S0305054821001660},
author = {Nina Mazyavkina and Sergey Sviridov and Sergei Ivanov and Evgeny Burnaev},
keywords = {Reinforcement learning, Operations research, Combinatorial optimization, Value-based methods, Policy-based methods},
abstract = {Many traditional algorithms for solving combinatorial optimization problems involve using hand-crafted heuristics that sequentially construct a solution. Such heuristics are designed by domain experts and may often be suboptimal due to the hard nature of the problems. Reinforcement learning (RL) proposes a good alternative to automate the search of these heuristics by training an agent in a supervised or self-supervised manner. In this survey, we explore the recent advancements of applying RL frameworks to hard combinatorial problems. Our survey provides the necessary background for operations research and machine learning communities and showcases the works that are moving the field forward. We juxtapose recently proposed RL methods, laying out the timeline of the improvements for each problem, as well as we make a comparison with traditional algorithms, indicating that RL models can become a promising direction for solving combinatorial problems.}
}

@book{co_book,
author = {Papadimitriou, Christos and Steiglitz, Kenneth},
year = {1982},
month = {01},
pages = {},
title = {Combinatorial Optimization: Algorithms and Complexity},
volume = {32},
isbn = {0-13-152462-3},
journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
doi = {10.1109/TASSP.1984.1164450}
}

@inproceedings{steinbiss1994improvements,
  title={Improvements in beam search},
  author={Steinbiss, Volker and Tran, Bach-Hiep and Ney, Hermann},
  booktitle={Third international conference on spoken language processing},
  year={1994}
}

@article{browne2012survey,
  title={A survey of monte carlo tree search methods},
  author={Browne, Cameron B and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M and Cowling, Peter I and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
  journal={IEEE Transactions on Computational Intelligence and AI in games},
  volume={4},
  number={1},
  pages={1--43},
  year={2012},
  publisher={IEEE}
}

@book{contardo2012balancing,
  title={Balancing a dynamic public bike-sharing system},
  author={Contardo, Claudio and Morency, Catherine and Rousseau, Louis-Martin},
  volume={4},
  year={2012}
}

@article{laterre2018ranked,
  title={Ranked reward: Enabling self-play reinforcement learning for combinatorial optimization},
  author={Laterre, Alexandre and Fu, Yunguan and Jabri, Mohamed Khalil and Cohen, Alain-Sam and Kas, David and Hajjar, Karl and Dahl, Torbjorn S and Kerkeni, Amine and Beguir, Karim},
  journal={arXiv preprint arXiv:1807.01672},
  year={2018}
}

@article{FROGER2016695,
title = {Maintenance scheduling in the electricity industry: A literature review},
journal = {European Journal of Operational Research},
volume = {251},
number = {3},
pages = {695-706},
year = {2016},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2015.08.045},
url = {https://www.sciencedirect.com/science/article/pii/S0377221715008012},
author = {Aurélien Froger and Michel Gendreau and Jorge E. Mendoza and Éric Pinson and Louis-Martin Rousseau},
keywords = {Maintenance, OR in energy, Scheduling, Regulated and deregulated power systems},
abstract = {The reliability of the power plants and transmission lines in the electricity industry is crucial for meeting demand. Consequently, timely maintenance plays a major role reducing breakdowns and avoiding expensive production shutdowns. By now, the literature contains a sound body of work focused on improving decision making in generating units and transmission lines maintenance scheduling. The purpose of this paper is to review that literature. We update previous surveys and provide a more global view of the problem: we study both regulated and deregulated power systems and explore some important features such as network considerations, fuel management, and data uncertainty.}
}


@ARTICLE{hansen2001cmaes,
  author={Hansen, Nikolaus and Ostermeier, Andreas},
  journal={Evolutionary Computation}, 
  title={Completely Derandomized Self-Adaptation in Evolution Strategies}, 
  year={2001},
  volume={9},
  number={2},
  pages={159-195},
  doi={10.1162/106365601750190398}
}

@inproceedings{fontaine2020cmame,
author = {Fontaine, Matthew C. and Togelius, Julian and Nikolaidis, Stefanos and Hoover, Amy K.},
title = {Covariance Matrix Adaptation for the Rapid Illumination of Behavior Space},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3390232},
doi = {10.1145/3377930.3390232},
abstract = {We focus on the challenge of finding a diverse collection of quality solutions on complex continuous domains. While quality diversity (QD) algorithms like Novelty Search with Local Competition (NSLC) and MAP-Elites are designed to generate a diverse range of solutions, these algorithms require a large number of evaluations for exploration of continuous spaces. Meanwhile, variants of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) are among the best-performing derivative-free optimizers in single-objective continuous domains. This paper proposes a new QD algorithm called Covariance Matrix Adaptation MAP-Elites (CMA-ME). Our new algorithm combines the self-adaptation techniques of CMA-ES with archiving and mapping techniques for maintaining diversity in QD. Results from experiments based on standard continuous optimization benchmarks show that CMA-ME finds better-quality solutions than MAP-Elites; similarly, results on the strategic game Hearthstone show that CMA-ME finds both a higher overall quality and broader diversity of strategies than both CMA-ES and MAP-Elites. Overall, CMA-ME more than doubles the performance of MAP-Elites using standard QD performance metrics. These results suggest that QD algorithms augmented by operators from state-of-the-art optimization algorithms can yield high-performing methods for simultaneously exploring and optimizing continuous search spaces, with significant applications to design, testing, and reinforcement learning among other domains.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {94–102},
numpages = {9},
keywords = {evolutionary algorithms, illumination algorithms, hearthstone, optimization, MAP-Elites, quality diversity},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@article{du1999cvt,
author = {Du, Qiang and Faber, Vance and Gunzburger, Max},
title = {Centroidal Voronoi Tessellations: Applications and Algorithms},
journal = {SIAM Review},
volume = {41},
number = {4},
pages = {637-676},
year = {1999},
doi = {10.1137/S0036144599352836},
URL = { 
    
        https://doi.org/10.1137/S0036144599352836
    
    

},
eprint = { 
        https://doi.org/10.1137/S0036144599352836
}
,
    abstract = { A centroidal Voronoi tessellation is a Voronoi tessellation whose generating points are the centroids (centers of mass) of the corresponding Voronoi regions. We give some applications of such tessellations to problems in image compression, quadrature, finite difference methods, distribution of resources, cellular biology, statistics, and the territorial behavior of animals. We discuss methodsfor computing these tessellations, provide some analyses concerning both the tessellations and the methods for their determination, and, finally, present the results of some numerical experiments. }
}

@misc{perronor,
  title = {{OR-Tools}},
  version = { v9.6 },
  author = {Laurent Perron and Vincent Furnon},
  organization = {Google},
  url = {https://developers.google.com/optimization/},
  year = {2019},
}

@inproceedings{eysenbach2018diversity,
  title={Diversity is All You Need: Learning Skills without a Reward Function},
  author={Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
  booktitle={International Conference on Learning Representations},
  year={2019},
}

@article{sharma2019dynamics,
  title={Dynamics-aware unsupervised discovery of skills},
  author={Sharma, Archit and Gu, Shixiang and Levine, Sergey and Kumar, Vikash and Hausman, Karol},
  journal={arXiv preprint arXiv:1907.01657},
  year={2019}
}

@article{cully2017quality,
  title={Quality and diversity optimization: A unifying modular framework},
  author={Cully, Antoine and Demiris, Yiannis},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={22},
  number={2},
  pages={245--259},
  year={2017},
  publisher={IEEE}
}

@inproceedings{
    chalumeau2023neuroevolution,
    title={Neuroevolution is a Competitive Alternative to Reinforcement Learning for Skill Discovery},
    author={Felix Chalumeau and Raphael Boige and Bryan Lim and Valentin Mac{\'e} and Maxime Allard and Arthur Flajolet and Antoine Cully and Thomas Pierrot},
    booktitle={International Conference on Learning Representations},
    year={2023},
    url={https://openreview.net/forum?id=6BHlZgyPOZY}
}

@article{lim2022accelerated,
  title={Accelerated Quality-Diversity for Robotics through Massive Parallelism},
  author={Lim, Bryan and Allard, Maxime and Grillotti, Luca and Cully, Antoine},
  journal={arXiv preprint arXiv:2202.01258},
  year={2022}
}

@inproceedings{cully2021multiemitter,
author = {Cully, Antoine},
title = {Multi-Emitter MAP-Elites: Improving Quality, Diversity and Data Efficiency with Heterogeneous Sets of Emitters},
year = {2021},
isbn = {9781450383509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449639.3459326},
doi = {10.1145/3449639.3459326},
abstract = {Quality-Diversity (QD) optimisation is a new family of learning algorithms that aims at generating collections of diverse and high-performing solutions. Among those algorithms, the recently introduced Covariance Matrix Adaptation MAP-Elites (CMA-ME) algorithm proposes the concept of emitters, which uses a predefined heuristic to drive the algorithm's exploration. This algorithm was shown to outperform MAP-Elites, a popular QD algorithm that has demonstrated promising results in numerous applications. In this paper, we introduce Multi-Emitter MAP-Elites (ME-MAP-Elites), an algorithm that directly extends CMA-ME and improves its quality, diversity and data efficiency. It leverages the diversity of a heterogeneous set of emitters, in which each emitter type improves the optimisation process in different ways. A bandit algorithm dynamically finds the best selection of emitters depending on the current situation. We evaluate the performance of ME-MAP-Elites on six tasks, ranging from standard optimisation problems (in 100 dimensions) to complex locomotion tasks in robotics. Our comparisons against CMA-ME and MAP-Elites show that ME-MAP-Elites is faster at providing collections of solutions that are significantly more diverse and higher performing. Moreover, in cases where no fruitful synergy can be found between the different emitters, ME-MAP-Elites is equivalent to the best of the compared algorithms.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {84–92},
numpages = {9},
keywords = {quality-diversity optimization, MAP-Elites, evolutionary robotics},
location = {Lille, France},
series = {GECCO '21}
}

@misc{chalumeau2023qdax,
      title={QDax: A Library for Quality-Diversity and Population-based Algorithms with Hardware Acceleration}, 
      author={Felix Chalumeau and Bryan Lim and Raphael Boige and Maxime Allard and Luca Grillotti and Manon Flageat and Valentin Macé and Arthur Flajolet and Thomas Pierrot and Antoine Cully},
      year={2023},
      eprint={2308.03665},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{chen2019learning,
  title={Learning to Perform Local Rewriting for Combinatorial Optimization},
  author={Chen, Xinyun and Tian, Yuandong},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@inproceedings{kim2021learning,
  title={Learning Collaborative Policies to Solve NP-hard Routing Problems},
  author={Kim, Minsu and Park, Jinkyoo and Kim, Joungho},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

@inproceedings{ma2021learning,
 author = {Ma, Yining and Li, Jingwen and Cao, Zhiguang and Song, Wen and Zhang, Le and Chen, Zhenghua and Tang, Jing},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {11096--11107},
 title = {Learning to Iteratively Solve Routing Problems with Dual-Aspect Collaborative Transformer},
 volume = {34},
 year = {2021}
}

@article{wu2021learning,
  title={Learning Improvement Heuristics for Solving Routing Problems},
  author={Wu, Yaoxin and Song, Wen and Cao, Zhiguang and Zhang, Jie and Lim, Andrew},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  volume={33},
  number={9},
  pages={5057-5069}
}


@InProceedings{son2023metasage,
  title =      {Meta-{SAGE}: Scale Meta-Learning Scheduled Adaptation with Guided Exploration for Mitigating Scale Shift on Combinatorial Optimization},
  author =       {Son, Jiwoo and Kim, Minsu and Kim, Hyeonah and Park, Jinkyoo},
  booktitle =      {Proceedings of the 40th International Conference on Machine Learning},
  pages =      {32194--32210},
  year =      {2023},
  editor =      {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume =      {202},
  series =      {Proceedings of Machine Learning Research},
  month =      {23--29 Jul},
  publisher =    {PMLR},
  pdf =      {https://proceedings.mlr.press/v202/son23a/son23a.pdf},
  url =      {https://proceedings.mlr.press/v202/son23a.html},
  abstract =      {This paper proposes Meta-SAGE, a novel approach for improving the scalability of deep reinforcement learning models for combinatorial optimization (CO) tasks. Our method adapts pre-trained models to larger-scale problems in test time by suggesting two components: a scale meta-learner (SML) and scheduled adaptation with guided exploration (SAGE). First, SML transforms the context embedding for subsequent adaptation of SAGE based on scale information. Then, SAGE adjusts the model parameters dedicated to the context embedding for a specific instance. SAGE introduces locality bias, which encourages selecting nearby locations to determine the next location. The locality bias gradually decays as the model is adapted to the target instance. Results show that Meta-SAGE outperforms previous adaptation methods and significantly improves scalability in representative CO tasks. Our source code is available at https://github.com/kaist-silab/meta-sage.}
}