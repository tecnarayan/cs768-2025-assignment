\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu(2017{\natexlab{a}})]{allen2017katyusha}
Zeyuan Allen-Zhu.
\newblock Katyusha: the first direct acceleration of stochastic gradient
  methods.
\newblock In \emph{Proceedings of the 49th Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 1200--1205. ACM, 2017{\natexlab{a}}.

\bibitem[Allen-Zhu(2017{\natexlab{b}})]{allen2017natasha}
Zeyuan Allen-Zhu.
\newblock Natasha 2: Faster non-convex optimization than sgd.
\newblock \emph{arXiv preprint arXiv:1708.08694}, 2017{\natexlab{b}}.

\bibitem[Allen-Zhu and Hazan(2016)]{allen2016variance}
Zeyuan Allen-Zhu and Elad Hazan.
\newblock Variance reduction for faster non-convex optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  699--707, 2016.

\bibitem[Anitescu(2000)]{anitescu2000degenerate}
Mihai Anitescu.
\newblock Degenerate nonlinear programming with a quadratic growth condition.
\newblock \emph{SIAM Journal on Optimization}, 10\penalty0 (4):\penalty0
  1116--1135, 2000.

\bibitem[Aravkin and Davis(2016)]{aravkin2016smart}
Aleksandr Aravkin and Damek Davis.
\newblock A smart stochastic algorithm for nonconvex optimization with
  applications to robust machine learning.
\newblock \emph{arXiv preprint arXiv:1610.01101}, 2016.

\bibitem[Csiba and Richt{\'a}rik(2017)]{csiba2017global}
Dominik Csiba and Peter Richt{\'a}rik.
\newblock Global convergence of arbitrary-block gradient methods for
  generalized polyak-{\l}ojasiewicz functions.
\newblock \emph{arXiv preprint arXiv:1709.03014}, 2017.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1646--1654, 2014.

\bibitem[Fu et~al.(2018)Fu, Chi, and Liang]{fu2018local}
Haoyu Fu, Yuejie Chi, and Yingbin Liang.
\newblock Local geometry of one-hidden-layer neural networks for logistic
  regression.
\newblock \emph{arXiv preprint arXiv:1802.06463}, 2018.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Ghadimi et~al.(2016)Ghadimi, Lan, and Zhang]{ghadimi2016mini}
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang.
\newblock Mini-batch stochastic approximation methods for nonconvex stochastic
  composite optimization.
\newblock \emph{Mathematical Programming}, 155\penalty0 (1-2):\penalty0
  267--305, 2016.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch sgd: training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Johnson and Zhang(2013)]{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in neural information processing systems}, pages
  315--323, 2013.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{karimi2016linear}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 795--811. Springer, 2016.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Lan and Zhou(2015)]{lan2015optimal}
Guanghui Lan and Yi~Zhou.
\newblock An optimal randomized incremental gradient method.
\newblock \emph{arXiv preprint arXiv:1507.02000}, 2015.

\bibitem[Lan and Zhou(2018)]{lan2018random}
Guanghui Lan and Yi~Zhou.
\newblock Random gradient extrapolation for distributed and stochastic
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (4):\penalty0
  2753--2782, 2018.

\bibitem[Lei et~al.(2017)Lei, Ju, Chen, and Jordan]{lei2017non}
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael~I Jordan.
\newblock Non-convex finite-sum optimization via scsg methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2345--2355, 2017.

\bibitem[Li et~al.(2017)Li, Zhou, Liang, and Varshney]{Li2017convergence}
Qunwei Li, Yi~Zhou, Yingbin Liang, and Pramod~K Varshney.
\newblock Convergence analysis of proximal gradient with momentum for nonconvex
  optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  2111--2119, 2017.

\bibitem[Luo and Tseng(1993)]{luo1993error}
Zhi-Quan Luo and Paul Tseng.
\newblock Error bounds and convergence analysis of feasible descent methods: a
  general approach.
\newblock \emph{Annals of Operations Research}, 46\penalty0 (1):\penalty0
  157--178, 1993.

\bibitem[Necoara et~al.(2015)Necoara, Nesterov, and Glineur]{necoara2015linear}
Ion Necoara, Yurii Nesterov, and Francois Glineur.
\newblock Linear convergence of first order methods for non-strongly convex
  optimization.
\newblock \emph{arXiv preprint arXiv:1504.06298}, 2015.

\bibitem[Nesterov(2004)]{nesterov2014introductory}
Yurii Nesterov.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Kluwer, 2004.

\bibitem[Polyak(1963)]{polyak1963gradient}
Boris~Teodorovich Polyak.
\newblock Gradient methods for minimizing functionals.
\newblock \emph{Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
  3\penalty0 (4):\penalty0 643--653, 1963.

\bibitem[Reddi et~al.(2016{\natexlab{a}})Reddi, Hefny, Sra, P{\'o}czos, and
  Smola]{reddi2016stochastic}
Sashank~J Reddi, Ahmed Hefny, Suvrit Sra, Barnab{\'a}s P{\'o}czos, and Alex
  Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In \emph{International conference on machine learning}, pages
  314--323, 2016{\natexlab{a}}.

\bibitem[Reddi et~al.(2016{\natexlab{b}})Reddi, Sra, P{\'o}czos, and
  Smola]{reddi2016proximal}
Sashank~J Reddi, Suvrit Sra, Barnab{\'a}s P{\'o}czos, and Alexander~J Smola.
\newblock Proximal stochastic methods for nonsmooth nonconvex finite-sum
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1145--1153, 2016{\natexlab{b}}.

\bibitem[Xiao and Zhang(2014)]{xiao2014proximal}
Lin Xiao and Tong Zhang.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock \emph{SIAM Journal on Optimization}, 24\penalty0 (4):\penalty0
  2057--2075, 2014.

\bibitem[Zhong et~al.(2017)Zhong, Song, Jain, Bartlett, and
  Dhillon]{zhong2017recovery}
Kai Zhong, Zhao Song, Prateek Jain, Peter~L Bartlett, and Inderjit~S Dhillon.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock \emph{arXiv preprint arXiv:1706.03175}, 2017.

\bibitem[Zhou et~al.(2018)Zhou, Xu, and Gu]{zhou2018stochastic}
Dongruo Zhou, Pan Xu, and Quanquan Gu.
\newblock Stochastic nested variance reduction for nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1806.07811}, 2018.

\end{thebibliography}
