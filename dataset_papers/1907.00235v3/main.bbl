\begin{thebibliography}{10}

\bibitem{vaswani2017attentionp17}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem{durbin2012timep1}
James Durbin and Siem~Jan Koopman.
\newblock {\em Time series analysis by state space methods}.
\newblock Oxford university press, 2012.

\bibitem{flunkert2017deeparp2}
Valentin Flunkert, David Salinas, and Jan Gasthaus.
\newblock Deepar: Probabilistic forecasting with autoregressive recurrent
  networks.
\newblock {\em arXiv preprint arXiv:1704.04110}, 2017.

\bibitem{graves2013generatingp3}
Alex Graves.
\newblock Generating sequences with recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1308.0850}, 2013.

\bibitem{sutskever2014sequencep4}
Ilya Sutskever, Oriol Vinyals, and Quoc~V Le.
\newblock Sequence to sequence learning with neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  3104--3112, 2014.

\bibitem{NIPS2018_8004p10}
Syama~Sundar Rangapuram, Matthias~W Seeger, Jan Gasthaus, Lorenzo Stella,
  Yuyang Wang, and Tim Januschowski.
\newblock Deep state space models for time series forecasting.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7785--7794, 2018.

\bibitem{lai2018modelingp6}
Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu.
\newblock Modeling long-and short-term temporal patterns with deep neural
  networks.
\newblock In {\em The 41st International ACM SIGIR Conference on Research \&
  Development in Information Retrieval}, pages 95--104. ACM, 2018.

\bibitem{yu2017longp7}
Rose Yu, Stephan Zheng, Anima Anandkumar, and Yisong Yue.
\newblock Long-term forecasting using tensor-train rnns.
\newblock {\em arXiv preprint arXiv:1711.00073}, 2017.

\bibitem{DBLP:journals/corr/abs-1812-00098p8}
Danielle~C Maddix, Yuyang Wang, and Alex Smola.
\newblock Deep factors with gaussian processes for forecasting.
\newblock {\em arXiv preprint arXiv:1812.00098}, 2018.

\bibitem{pascanu2012difficultyp14}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In {\em International conference on machine learning}, pages
  1310--1318, 2013.

\bibitem{DBLP:journals/neco/HochreiterS97p11}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem{cho-etal-2014-propertiesp12}
Kyunghyun Cho, Bart Van~Merri{\"e}nboer, Dzmitry Bahdanau, and Yoshua Bengio.
\newblock On the properties of neural machine translation: Encoder-decoder
  approaches.
\newblock {\em arXiv preprint arXiv:1409.1259}, 2014.

\bibitem{kh2018sharpp15}
Urvashi Khandelwal, He~He, Peng Qi, and Dan Jurafsky.
\newblock Sharp nearby, fuzzy far away: How neural language models use context.
\newblock {\em arXiv preprint arXiv:1805.04623}, 2018.

\bibitem{Parikh_2016p16}
Ankur~P Parikh, Oscar T{\"a}ckstr{\"o}m, Dipanjan Das, and Jakob Uszkoreit.
\newblock A decomposable attention model for natural language inference.
\newblock {\em arXiv preprint arXiv:1606.01933}, 2016.

\bibitem{10.2307/2985674p21}
George~EP Box and Gwilym~M Jenkins.
\newblock Some recent advances in forecasting and control.
\newblock {\em Journal of the Royal Statistical Society. Series C (Applied
  Statistics)}, 17(2):91--109, 1968.

\bibitem{box_jenkins_reinsel_ljung_2016p22}
George~EP Box, Gwilym~M Jenkins, Gregory~C Reinsel, and Greta~M Ljung.
\newblock {\em Time series analysis: forecasting and control}.
\newblock John Wiley \& Sons, 2015.

\bibitem{NIPS2016_6160p23}
Hsiang-Fu Yu, Nikhil Rao, and Inderjit~S Dhillon.
\newblock Temporal regularized matrix factorization for high-dimensional time
  series prediction.
\newblock In {\em Advances in neural information processing systems}, pages
  847--855, 2016.

\bibitem{pmlr-v32-chapados14p33}
Nicolas Chapados.
\newblock Effective bayesian modeling of groups of related count time series.
\newblock {\em arXiv preprint arXiv:1405.3738}, 2014.

\bibitem{2017arXiv171111053Wp27}
Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy, and Dhruv Madeka.
\newblock A multi-horizon quantile recurrent forecaster.
\newblock {\em arXiv preprint arXiv:1711.11053}, 2017.

\bibitem{Jin2019ICML}
Xiaoyong Jin, Shiyang Li, Yunkai Zhang, and Xifeng Yan.
\newblock Multi-step deep autoregressive forecasting with latent states.
\newblock {\em URL
  http://roseyu.com/time-series-workshop/submissions/2019/timeseries-ICML19\_paper\_19.pdf},
  2019.

\bibitem{huang2018musicp18}
Cheng-Zhi~Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Curtis
  Hawthorne, Andrew~M Dai, Matthew~D Hoffman, and Douglas Eck.
\newblock An improved relative self-attention mechanism for transformer with
  application to music generation.
\newblock {\em arXiv preprint arXiv:1809.04281}, 2018.

\bibitem{Povey2018ATSp19}
Daniel Povey, Hossein Hadian, Pegah Ghahremani, Ke~Li, and Sanjeev Khudanpur.
\newblock A time-restricted self-attention layer for asr.
\newblock In {\em 2018 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 5874--5878. IEEE, 2018.

\bibitem{parmar2018imagep20}
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, {\L}ukasz Kaiser, Noam Shazeer,
  Alexander Ku, and Dustin Tran.
\newblock Image transformer.
\newblock {\em arXiv preprint arXiv:1802.05751}, 2018.

\bibitem{RePEc:eee:intfor:v:34:y:2018:i:4:p:802-808p34}
Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos.
\newblock The m4 competition: Results, findings, conclusion and way forward.
\newblock {\em International Journal of Forecasting}, 34(4):802--808, 2018.

\bibitem{poggio2017and}
Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli
  Liao.
\newblock Why and when can deep-but not shallow-networks avoid the curse of
  dimensionality: a review.
\newblock {\em International Journal of Automation and Computing},
  14(5):503--519, 2017.

\bibitem{fukushima1980neocognitron}
Kunihiko Fukushima.
\newblock Neocognitron: A self-organizing neural network model for a mechanism
  of pattern recognition unaffected by shift in position.
\newblock {\em Biological cybernetics}, 36(4):193--202, 1980.

\bibitem{mhaskar2016deep}
Hrushikesh~N Mhaskar and Tomaso Poggio.
\newblock Deep vs. shallow networks: An approximation theory perspective.
\newblock {\em Analysis and Applications}, 14(06):829--848, 2016.

\bibitem{adam2014}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{van2016wavenetp5}
A{\"a}ron Van Den~Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol
  Vinyals, Alex Graves, Nal Kalchbrenner, Andrew~W Senior, and Koray
  Kavukcuoglu.
\newblock Wavenet: A generative model for raw audio.
\newblock {\em SSW}, 125, 2016.

\bibitem{Borovykh2017ConditionalTSp9}
Anastasia Borovykh, Sander Bohte, and Cornelis~W Oosterlee.
\newblock Conditional time series forecasting with convolutional neural
  networks.
\newblock {\em arXiv preprint arXiv:1703.04691}, 2017.

\bibitem{Sparse2017}
Scott Gray, Alec Radford, and Diederik~P. Kingma.
\newblock Gpu kernels for block-sparse weights.
\newblock {\em arXiv preprint arXiv:1711.09224}, 2017.

\bibitem{cooijmans2016recurrentp13}
Tim Cooijmans, Nicolas Ballas, C{\'e}sar Laurent, {\c{C}}a{\u{g}}lar
  G{\"u}l{\c{c}}ehre, and Aaron Courville.
\newblock Recurrent batch normalization.
\newblock {\em arXiv preprint arXiv:1603.09025}, 2016.

\bibitem{doi:10.1137/1.9781611974973.87p24}
Rose Yu, Yaguang Li, Cyrus Shahabi, Ugur Demiryurek, and Yan Liu.
\newblock Deep learning: A generic approach for extreme condition traffic
  forecasting.
\newblock In {\em Proceedings of the 2017 SIAM International Conference on Data
  Mining}, pages 777--785. SIAM, 2017.

\bibitem{ZHANG199835p25}
Guoqiang Zhang, B~Eddy Patuwo, and Michael~Y Hu.
\newblock Forecasting with artificial neural networks:: The state of the art.
\newblock {\em International journal of forecasting}, 14(1):35--62, 1998.

\bibitem{child2019generatingp28}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock {\em arXiv preprint arXiv:1904.10509}, 2019.

\bibitem{liu2018generatingp29}
Peter~J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz
  Kaiser, and Noam Shazeer.
\newblock Generating wikipedia by summarizing long sequences.
\newblock {\em arXiv preprint arXiv:1801.10198}, 2018.

\bibitem{Radford2018ImprovingLUp30}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock {\em URL https://s3-us-west-2. amazonaws.
  com/openai-assets/research-covers/languageunsupervised/language understanding
  paper. pdf}, 2018.

\bibitem{devlin2018bertp31}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{child2019generatingp32}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock {\em arXiv preprint arXiv:1904.10509}, 2019.

\bibitem{Laptev2017TimeseriesEEp35}
Nikolay Laptev, Jason Yosinski, Li~Erran Li, and Slawek Smyl.
\newblock Time-series extreme event forecasting with neural networks at uber.
\newblock In {\em International Conference on Machine Learning}, number~34,
  pages 1--5, 2017.

\end{thebibliography}
