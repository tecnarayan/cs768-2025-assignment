\begin{thebibliography}{10}

\bibitem{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock {\em The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem{nesterov1983method}
Yurii~Evgen'evich Nesterov.
\newblock A method of solving a convex programming problem with convergence rate o$\backslash$bigl(k\^{}2$\backslash$bigr).
\newblock In {\em Doklady Akademii Nauk}, volume 269, pages 543--547. Russian Academy of Sciences, 1983.

\bibitem{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic optimization.
\newblock {\em JMLR}, 12(7), 2011.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In {\em Proc. {ICML}}, 2014.

\bibitem{ppo}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {arXiv:1707.06347}, 2017.

\bibitem{lan2020first}
Guanghui Lan.
\newblock {\em First-order and stochastic optimization methods for machine learning}, volume~1.
\newblock Springer, 2020.

\bibitem{drl}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin~A. Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock {arXiv:1312.5602}, 2013.

\bibitem{KrizhevskySH12}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E. Hinton.
\newblock Image{N}et classification with deep convolutional neural networks.
\newblock In {\em Proc. {NIPS}}, 2012.

\bibitem{AssranAFJR20}
Mahmoud Assran, Arda Aytekin, Hamid~Reza Feyzmahdavian, Mikael Johansson, and Michael~G. Rabbat.
\newblock Advances in asynchronous parallel and distributed optimization.
\newblock {\em Proc. {IEEE}}, 2020.

\bibitem{recht2011hogwild}
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.
\newblock Hogwild!: A lock-free approach to parallelizing stochastic gradient descent.
\newblock In {\em Proc. {NeurIPS}}, 2011.

\bibitem{MnihBMGLHSK16}
Volodymyr Mnih, Adri{\`{a}}~Puigdom{\`{e}}nech Badia, Mehdi Mirza, Alex Graves, Timothy~P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em Proc. {ICML}}, 2016.

\bibitem{YuYZ19}
Hao Yu, Sen Yang, and Shenghuo Zhu.
\newblock Parallel restarted {SGD} with faster convergence and less communication: Demystifying why model averaging works for deep learning.
\newblock In {\em Proc. {AAAI}}, 2019.

\bibitem{DeanCMCDLMRSTYN12}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc~V. Le, Mark~Z. Mao, Marc'Aurelio Ranzato, Andrew~W. Senior, Paul~A. Tucker, Ke~Yang, and Andrew~Y. Ng.
\newblock Large scale distributed deep networks.
\newblock In {\em Proc. {NIPS}}, 2012.

\bibitem{harlap2018pipedream}
Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg Ganger, and Phil Gibbons.
\newblock Pipedream: {F}ast and efficient pipeline parallel dnn training.
\newblock {\em arXiv preprint arXiv:1806.03377}, 2018.

\bibitem{HuangCBFCCLNLWC19}
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia~Xu Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc~V. Le, Yonghui Wu, and Zhifeng Chen.
\newblock {GP}ipe: {E}fficient training of giant neural networks using pipeline parallelism.
\newblock In {\em Proc. {NeurIPS}}, 2019.

\bibitem{Johnson013}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance reduction.
\newblock In {\em Proc. {NIPS}}, 2013.

\bibitem{ZhouSC18}
Kaiwen Zhou, Fanhua Shang, and James Cheng.
\newblock A simple stochastic variance reduced algorithm with fast convergence rates.
\newblock In {\em Proc. {ICML}}, 2018.

\bibitem{SebbouhGJBG19}
Othmane Sebbouh, Nidham Gazagnadou, Samy Jelassi, Francis~R. Bach, and Robert~M. Gower.
\newblock Towards closing the gap between the theory and practice of {SVRG}.
\newblock In {\em Proc. {NeurIPS}}, 2019.

\bibitem{ZhuangTDTDPD20}
Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar Tatikonda, Nicha~C. Dvornek, Xenophon Papademetris, and James~S. Duncan.
\newblock {AdaBelief} optimizer: {Adapting} stepsizes by the belief in observed gradients.
\newblock In {\em Proc. {NeurIPS}}, 2020.

\bibitem{0003GY20}
Yanli Liu, Yuan Gao, and Wotao Yin.
\newblock An improved analysis of stochastic gradient descent with momentum.
\newblock In {\em Proc. {NeurIPS}}, 2020.

\bibitem{LuoWY0Z18}
Rui Luo, Jianhong Wang, Yaodong Yang, Jun Wang, and Zhanxing Zhu.
\newblock Thermostat-assisted continuously-tempered {H}amiltonian {M}onte {C}arlo for bayesian learning.
\newblock In {\em Proc. {NeurIPS}}, 2018.

\bibitem{HeLT19}
Fengxiang He, Tongliang Liu, and Dacheng Tao.
\newblock Control batch size and learning rate to generalize well: {T}heoretical and empirical evidence.
\newblock In {\em Proc. {NeurIPS}}, 2019.

\bibitem{abs-2109-09833}
Yixin Wu, Rui Luo, Chen Zhang, Jun Wang, and Yaodong Yang.
\newblock Revisiting the characteristics of stochastic gradient noise and dynamics.
\newblock {arXiv:2109.09833}, 2021.

\bibitem{RasmussenW06}
Carl~Edward Rasmussen and Christopher K.~I. Williams.
\newblock {\em Gaussian processes for machine learning}.
\newblock Adaptive computation and machine learning. {MIT} Press, 2006.

\bibitem{zord}
Yao Shu, Zhongxiang Dai, Weicong Sng, Arun Verma, Patrick Jaillet, and Bryan Kian~Hsiang Low.
\newblock Zeroth-order optimization with trajectory-informed derivative estimation.
\newblock In {\em Proc. {ICLR}}, 2023.

\bibitem{shu2023federated}
Yao Shu, Xiaoqiang Lin, Zhongxiang Dai, and Bryan Kian~Hsiang Low.
\newblock Federated zeroth-order optimization using trajectory-informed surrogate gradients.
\newblock {arXiv:2308.04077}, 2023.

\bibitem{GoodBengCour16}
Ian~J. Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock {\em Deep {L}earning}.
\newblock MIT Press, Cambridge, MA, USA, 2016.

\bibitem{Sutton2018}
Richard~S. Sutton and Andrew~G. Barto.
\newblock {\em Reinforcement Learning: {A}n Introduction}.
\newblock The MIT Press, second edition, 2018.

\bibitem{opt4ml}
L{\'{e}}on Bottou, Frank~E. Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock {\em {SIAM} Rev.}, 60(2):223--311, 2018.

\bibitem{lederer2019posterior}
Armin Lederer, Jonas Umlauft, and Sandra Hirche.
\newblock Posterior variance analysis of {Gaussian} processes with application to average learning curves.
\newblock {arXiv:1906.01404}, 2019.

\bibitem{DaiSLJ22}
Zhongxiang Dai, Yao Shu, Bryan Kian~Hsiang Low, and Patrick Jaillet.
\newblock Sample-then-optimize batch neural {T}hompson sampling.
\newblock In {\em Proc. {NeurIPS}}, 2022.

\bibitem{ChowdhuryG17}
Sayak~Ray Chowdhury and Aditya Gopalan.
\newblock On kernelized multi-armed bandits.
\newblock In {\em Proc. {ICML}}, 2017.

\bibitem{dai2022federated}
Zhongxiang Dai, Yao Shu, Arun Verma, Flint~Xiaofeng Fan, Bryan Kian~Hsiang Low, and Patrick Jaillet.
\newblock Federated neural bandit.
\newblock In {\em Proc. {ICLR}}, 2023.

\bibitem{ChowdhuryG21}
Sayak~Ray Chowdhury and Aditya Gopalan.
\newblock No-regret algorithms for multi-task {B}ayesian optimization.
\newblock In {\em Proc. {AISTATS}}, 2021.

\bibitem{SrinivasKKS10}
Niranjan Srinivas, Andreas Krause, Sham~M. Kakade, and Matthias~W. Seeger.
\newblock Gaussian process optimization in the bandit setting: {N}o regret and experimental design.
\newblock In {\em Proc. {ICML}}, 2010.

\bibitem{LiuNNEN23}
Zijian Liu, Ta~Duy Nguyen, Thien~Hang Nguyen, Alina Ene, and Huy Nguyen.
\newblock High probability convergence of stochastic gradient methods.
\newblock In {\em Proc. {ICML}}, 2023.

\bibitem{DroriS20}
Yoel Drori and Ohad Shamir.
\newblock The complexity of finding stationary points with stochastic gradient descent.
\newblock In {\em Proc. {ICML}}, 2020.

\bibitem{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba.
\newblock {OpenAI Gym}.
\newblock {arXiv:1606.01540}, 2016.

\bibitem{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness, Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em nature}, 518(7540):529--533, 2015.

\bibitem{HeZRS16}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proc. {CVPR}}, 2016.

\bibitem{cifar}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem{haiku2020github}
Tom Hennigan, Trevor Cai, Tamara Norman, Lena Martens, and Igor Babuschkin.
\newblock {H}aiku: {S}onnet for {JAX}, 2020.

\bibitem{KeskarMNST17}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: {G}eneralization gap and sharp minima.
\newblock In {\em Proc. {ICLR}}, 2017.

\bibitem{laurent2000adaptive}
Beatrice Laurent and Pascal Massart.
\newblock Adaptive estimation of a quadratic functional by model selection.
\newblock {\em Annals of Statistics}, pages 1302--1338, 2000.

\bibitem{Kassraie022}
Parnian Kassraie and Andreas Krause.
\newblock Neural contextual bandits without regret.
\newblock In {\em Proc. {AISTATS}}, 2022.

\bibitem{SSBD}
Shai Shalev{-}Shwartz and Shai Ben{-}David.
\newblock {\em Understanding Machine Learning - From Theory to Algorithms}.
\newblock Cambridge University Press, 2014.

\bibitem{lecun2010mnist}
Yann LeCun, Corinna Cortes, and CJ~Burges.
\newblock {MNIST} handwritten digit database.
\newblock {\em ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist}, 2, 2010.

\bibitem{xiao2017fashion}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-{M}nist: {A} novel image dataset for benchmarking machine learning algorithms.
\newblock {arXiv:1708.07747}, 2017.

\bibitem{ostrovskii2018efficient}
Dmitrii Ostrovskii and Zaid Harchaoui.
\newblock Efficient first-order algorithms for adaptive signal denoising.
\newblock In {\em Proc. {ICML}}, 2018.

\bibitem{ShallueLASFD19}
Christopher~J. Shallue, Jaehoon Lee, Joseph~M. Antognini, Jascha Sohl{-}Dickstein, Roy Frostig, and George~E. Dahl.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock {\em JMLR}, 20:112:1--112:49, 2019.

\bibitem{yu2020hyper}
Tong Yu and Hong Zhu.
\newblock Hyper-parameter optimization: {A} review of algorithms and applications.
\newblock {arXiv:2003.05689}, 2020.

\end{thebibliography}
