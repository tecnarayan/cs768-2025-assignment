\begin{thebibliography}{10}

\bibitem{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on open problems.
\newblock {\em arXiv preprint arXiv:2005.01643}, 2020.

\bibitem{lange2012batch}
Sascha Lange, Thomas Gabel, and Martin Riedmiller.
\newblock Batch reinforcement learning.
\newblock In {\em Reinforcement learning: State-of-the-art}, pages 45--73. Springer, 2012.

\bibitem{kumar2019stabilizing}
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{lyu2022mildly}
Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu.
\newblock Mildly conservative q-learning for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 35:1711--1724, 2022.

\bibitem{wu2019behavior}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:1911.11361}, 2019.

\bibitem{fujimoto2019off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In {\em International conference on machine learning}, pages 2052--2062. PMLR, 2019.

\bibitem{fujimoto2021minimalist}
Scott Fujimoto and Shixiang~Shane Gu.
\newblock A minimalist approach to offline reinforcement learning.
\newblock {\em Advances in neural information processing systems}, 34:20132--20145, 2021.

\bibitem{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 33:1179--1191, 2020.

\bibitem{wu2021uncertainty}
Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov, and Hanlin Goh.
\newblock Uncertainty weighted actor-critic for offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:2105.08140}, 2021.

\bibitem{nair2020awac}
Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine.
\newblock Awac: Accelerating online reinforcement learning with offline datasets.
\newblock {\em arXiv preprint arXiv:2006.09359}, 2020.

\bibitem{laskin2020reinforcement}
Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas.
\newblock Reinforcement learning with augmented data.
\newblock {\em Advances in neural information processing systems}, 33:19884--19895, 2020.

\bibitem{sinha2022s4rl}
Samarth Sinha, Ajay Mandlekar, and Animesh Garg.
\newblock {S4RL}: Surprisingly simple self-supervision for offline reinforcement learning in robotics.
\newblock In {\em Conference on Robot Learning}, pages 907--917. PMLR, 2022.

\bibitem{lu2023synthetic}
Cong Lu, Philip~J Ball, and Jack Parker-Holder.
\newblock Synthetic experience replay.
\newblock {\em arXiv preprint arXiv:2303.06614}, 2023.

\bibitem{he2023diffusion}
Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan Zhang, Dong Wang, Bin Zhao, and Xuelong Li.
\newblock Diffusion model is an effective planner and data synthesizer for multi-task reinforcement learning.
\newblock {\em arXiv preprint arXiv:2305.18459}, 2023.

\bibitem{fu2020d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock {\em arXiv preprint arXiv:2004.07219}, 2020.

\bibitem{lu2022challenges}
Cong Lu, Philip~J Ball, Tim~GJ Rudner, Jack Parker-Holder, Michael~A Osborne, and Yee~Whye Teh.
\newblock Challenges and opportunities in offline reinforcement learning from visual observations.
\newblock {\em arXiv preprint arXiv:2206.04779}, 2022.

\bibitem{laskin2020curl}
Michael Laskin, Aravind Srinivas, and Pieter Abbeel.
\newblock Curl: Contrastive unsupervised representations for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages 5639--5650. PMLR, 2020.

\bibitem{yarats2021image}
Denis Yarats, Rob Fergus, and Ilya Kostrikov.
\newblock Image augmentation is all you need: Regularizing deep reinforcement learning from pixels.
\newblock In {\em 9th International Conference on Learning Representations, ICLR 2021}, 2021.

\bibitem{ball2021augmented}
Philip~J Ball, Cong Lu, Jack Parker-Holder, and Stephen Roberts.
\newblock Augmented world models facilitate zero-shot dynamics generalization from a single offline environment.
\newblock In {\em International Conference on Machine Learning}, pages 619--629. PMLR, 2021.

\bibitem{jackson2024policy}
Matthew~Thomas Jackson, Michael~Tryfan Matthews, Cong Lu, Benjamin Ellis, Shimon Whiteson, and Jakob Foerster.
\newblock Policy-guided diffusion.
\newblock {\em arXiv preprint arXiv:2404.06356}, 2024.

\bibitem{janner2022planning}
Michael Janner, Yilun Du, Joshua~B Tenenbaum, and Sergey Levine.
\newblock Planning with diffusion for flexible behavior synthesis.
\newblock {\em arXiv preprint arXiv:2205.09991}, 2022.

\bibitem{ajay2022conditional}
Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal.
\newblock Is conditional generative modeling all you need for decision-making?
\newblock {\em arXiv preprint arXiv:2211.15657}, 2022.

\bibitem{ho2022classifier}
Jonathan Ho and Tim Salimans.
\newblock Classifier-free diffusion guidance.
\newblock {\em arXiv preprint arXiv:2207.12598}, 2022.

\bibitem{liang2023adaptdiffuser}
Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi Tomizuka, and Ping Luo.
\newblock Adaptdiffuser: Diffusion models as adaptive self-evolving planners.
\newblock {\em arXiv preprint arXiv:2302.01877}, 2023.

\bibitem{yu2020mopo}
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James~Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma.
\newblock Mopo: Model-based offline policy optimization.
\newblock {\em Advances in Neural Information Processing Systems}, 33:14129--14142, 2020.

\bibitem{kidambi2020morel}
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.
\newblock Morel: Model-based offline reinforcement learning.
\newblock {\em Advances in neural information processing systems}, 33:21810--21823, 2020.

\bibitem{yu2021combo}
Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.
\newblock Combo: Conservative offline model-based policy optimization.
\newblock {\em Advances in neural information processing systems}, 34:28954--28967, 2021.

\bibitem{rigter2022rambo}
Marc Rigter, Bruno Lacerda, and Nick Hawes.
\newblock Rambo-rl: Robust adversarial model-based offline reinforcement learning.
\newblock {\em Advances in neural information processing systems}, 35:16082--16097, 2022.

\bibitem{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem{karras2022elucidating}
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
\newblock Elucidating the design space of diffusion-based generative models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:26565--26577, 2022.

\bibitem{dhariwal2021diffusion}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock {\em Advances in neural information processing systems}, 34:8780--8794, 2021.

\bibitem{song2020score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock {\em arXiv preprint arXiv:2011.13456}, 2020.

\bibitem{tolstikhin2021mlp}
Ilya~O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et~al.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock {\em Advances in neural information processing systems}, 34:24261--24272, 2021.

\bibitem{kumar2020model}
Aviral Kumar and Sergey Levine.
\newblock Model inversion networks for model-based optimization.
\newblock {\em Advances in Neural Information Processing Systems}, 33:5126--5137, 2020.

\bibitem{krishnamoorthy2023diffusion}
Siddarth Krishnamoorthy, Satvik~Mehul Mashkaria, and Aditya Grover.
\newblock Diffusion models for black-box optimization.
\newblock {\em arXiv preprint arXiv:2306.07180}, 2023.

\bibitem{kostrikov2021offline}
Ilya Kostrikov, Ashvin Nair, and Sergey Levine.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock {\em arXiv preprint arXiv:2110.06169}, 2021.

\bibitem{kim2023bootstrapped}
Minsu Kim, Federico Berto, Sungsoo Ahn, and Jinkyoo Park.
\newblock Bootstrapped training of score-conditioned generator for offline design of biological sequences.
\newblock {\em arXiv preprint arXiv:2306.03111}, 2023.

\bibitem{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock {\em Advances in neural information processing systems}, 34:15084--15097, 2021.

\bibitem{tarasov2022corl}
Denis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav Kurenkov, and Sergey Kolesnikov.
\newblock Corl: Research-oriented deep offline reinforcement learning library.
\newblock {\em arXiv preprint arXiv:2210.07105}, 2022.

\bibitem{offinerlkit}
Yihao Sun.
\newblock Offlinerl-kit: An elegant pytorch offline reinforcement learning library.
\newblock \url{https://github.com/yihaosun1124/OfflineRL-Kit}, 2023.

\bibitem{lu2021revisiting}
Cong Lu, Philip~J Ball, Jack Parker-Holder, Michael~A Osborne, and Stephen~J Roberts.
\newblock Revisiting design choices in offline model-based reinforcement learning.
\newblock {\em arXiv preprint arXiv:2110.04135}, 2021.

\bibitem{jain2022biological}
Moksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure~FP Dossou, Chanakya~Ajit Ekbote, Jie Fu, Tianyu Zhang, Michael Kilgour, Dinghuai Zhang, et~al.
\newblock Biological sequence design with gflownets.
\newblock In {\em International Conference on Machine Learning}, pages 9786--9801. PMLR, 2022.

\bibitem{hong2023harnessing}
Zhang-Wei Hong, Pulkit Agrawal, R{\'e}mi Tachet~des Combes, and Romain Laroche.
\newblock Harnessing mixed offline reinforcement learning datasets via trajectory weighting.
\newblock {\em arXiv preprint arXiv:2306.13085}, 2023.

\bibitem{welch1947generalization}
Bernard~L Welch.
\newblock The generalization of ‘student's’problem when several different population varlances are involved.
\newblock {\em Biometrika}, 34(1-2):28--35, 1947.

\bibitem{wang2022bootstrapped}
Kerong Wang, Hanye Zhao, Xufang Luo, Kan Ren, Weinan Zhang, and Dongsheng Li.
\newblock Bootstrapped transformer for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 35:34748--34761, 2022.

\bibitem{luo2022antigen}
Shitong Luo, Yufeng Su, Xingang Peng, Sheng Wang, Jian Peng, and Jianzhu Ma.
\newblock Antigen-specific antibody design and optimization with diffusion-based generative models for protein structures.
\newblock {\em Advances in Neural Information Processing Systems}, 35:9754--9767, 2022.

\bibitem{meng2021sdedit}
Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.
\newblock Sdedit: Guided image synthesis and editing with stochastic differential equations.
\newblock {\em arXiv preprint arXiv:2108.01073}, 2021.

\bibitem{trabucco2023effective}
Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov.
\newblock Effective data augmentation with diffusion models.
\newblock {\em arXiv preprint arXiv:2302.07944}, 2023.

\end{thebibliography}
