\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Acerbi \& Tasche(2002)Acerbi and Tasche]{acerbi2002coherence}
Acerbi, C. and Tasche, D.
\newblock On the coherence of expected shortfall.
\newblock \emph{Journal of Banking \& Finance}, 26\penalty0 (7):\penalty0
  1487--1503, 2002.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and
  Munos]{bellemare2017distributional}
Bellemare, M.~G., Dabney, W., and Munos, R.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  449--458, 2017.

\bibitem[Chen et~al.(2021)Chen, Wang, Zhou, and Ross]{chen2021randomized}
Chen, X., Wang, C., Zhou, Z., and Ross, K.~W.
\newblock Randomized ensembled double q-learning: Learning fast without a
  model.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Chow \& Ghavamzadeh(2014)Chow and Ghavamzadeh]{chow2014algorithms}
Chow, Y. and Ghavamzadeh, M.
\newblock Algorithms for cvar optimization in {MDP}s.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3509--3517, 2014.

\bibitem[Chow et~al.(2015)Chow, Tamar, Mannor, and Pavone]{chow2015risk}
Chow, Y., Tamar, A., Mannor, S., and Pavone, M.
\newblock Risk-sensitive and robust decision-making: a cvar optimization
  approach.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1522--1530, 2015.

\bibitem[Chung et~al.(2014)Chung, Gulcehre, Cho, and
  Bengio]{chung2014empirical}
Chung, J., Gulcehre, C., Cho, K., and Bengio, Y.
\newblock Empirical evaluation of gated recurrent neural networks on sequence
  modeling.
\newblock In \emph{Advances in Neural Information Processing Systems 2014
  Workshop on Deep Learning}, 2014.

\bibitem[Dabney et~al.(2018{\natexlab{a}})Dabney, Ostrovski, Silver, and
  Munos]{dabney2018IQN}
Dabney, W., Ostrovski, G., Silver, D., and Munos, R.
\newblock Implicit quantile networks for distributional reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1096--1105, 2018{\natexlab{a}}.

\bibitem[Dabney et~al.(2018{\natexlab{b}})Dabney, Rowland, Bellemare, and
  Munos]{dabney2018distributional}
Dabney, W., Rowland, M., Bellemare, M.~G., and Munos, R.
\newblock Distributional reinforcement learning with quantile regression.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pp.\  2892--2901, 2018{\natexlab{b}}.

\bibitem[Foerster et~al.(2017)Foerster, Farquhar, Afouras, Nardelli, and
  Whiteson]{foerster2017counterfactual}
Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.
\newblock Counterfactual multi-agent policy gradients.
\newblock \emph{arXiv preprint arXiv:1705.08926}, 2017.

\bibitem[Garc{\'\i}a et~al.(2015)]{garcia2015comprehensive}
Garc{\'\i}a, J. et~al.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 16\penalty0
  (42):\penalty0 1437--1480, 2015.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I.~J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
  Ozair, S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2672--2680, 2014.

\bibitem[Hasselt(2010)]{hasselt2010double}
Hasselt, H.~V.
\newblock Double q-learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2613--2621, 2010.

\bibitem[Hiraoka et~al.(2019)Hiraoka, Imagawa, Mori, Onishi, and
  Tsuruoka]{hiraoka2019learning}
Hiraoka, T., Imagawa, T., Mori, T., Onishi, T., and Tsuruoka, Y.
\newblock Learning robust options by conditional value at risk optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2619--2629, 2019.

\bibitem[Hu et~al.(2020)Hu, Harding, Wu, and Liao]{hu2020qr}
Hu, J., Harding, S.~A., Wu, H., and Liao, S.-w.
\newblock Qr-mix: Distributional value function factorisation for cooperative
  multi-agent reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2009.04197}, 2020.

\bibitem[H{\"u}ttenrauch et~al.(2017)H{\"u}ttenrauch, {\v{S}}o{\v{s}}i{\'c},
  and Neumann]{huttenrauch2017guided}
H{\"u}ttenrauch, M., {\v{S}}o{\v{s}}i{\'c}, A., and Neumann, G.
\newblock Guided deep reinforcement learning for swarm systems.
\newblock \emph{In AAMAS 2017 Autonomous Robots and Multirobot Systems (ARMS)
  Workshop}, 2017.

\bibitem[Iancu et~al.(2015)Iancu, Petrik, and Subramanian]{iancu2015tight}
Iancu, D.~A., Petrik, M., and Subramanian, D.
\newblock Tight approximations of dynamic risk measures.
\newblock \emph{Mathematics of Operations Research}, 40\penalty0 (3):\penalty0
  655--682, 2015.

\bibitem[Iqbal \& Sha(2019)Iqbal and Sha]{iqbal2019actor}
Iqbal, S. and Sha, F.
\newblock Actor-attention-critic for multi-agent reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2961--2970, 2019.

\bibitem[Keramati et~al.(2020)Keramati, Dann, Tamkin, and
  Brunskill]{keramati2019being}
Keramati, R., Dann, C., Tamkin, A., and Brunskill, E.
\newblock Being optimistic to be conservative: Quickly learning a cvar policy.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pp.\  4436--4443, 2020.

\bibitem[Kolla et~al.(2019)Kolla, Prashanth, Bhat, and
  Jagannathan]{kolla2019concentration}
Kolla, R.~K., Prashanth, L., Bhat, S.~P., and Jagannathan, K.
\newblock Concentration bounds for empirical conditional value-at-risk: The
  unbounded case.
\newblock \emph{Operations Research Letters}, 47\penalty0 (1):\penalty0 16--20,
  2019.

\bibitem[Lan et~al.(2019)Lan, Pan, Fyshe, and White]{lan2019maxmin}
Lan, Q., Pan, Y., Fyshe, A., and White, M.
\newblock Maxmin q-learning: Controlling the estimation bias of q-learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Lowe et~al.(2017)Lowe, Wu, Tamar, Harb, Abbeel, and
  Mordatch]{lowe2017multi}
Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6379--6390, 2017.

\bibitem[Lyu \& Amato(2020)Lyu and Amato]{lyu2020likelihood}
Lyu, X. and Amato, C.
\newblock Likelihood quantile networks for coordinating multi-agent
  reinforcement learning.
\newblock In \emph{Proceedings of the 19th International Conference on
  Autonomous Agents and MultiAgent Systems}, pp.\  798--806, 2020.

\bibitem[Ma et~al.(2020)Ma, Zhang, Xia, Zhou, Yang, and
  Zhao]{ma2020distributional}
Ma, X., Zhang, Q., Xia, L., Zhou, Z., Yang, J., and Zhao, Q.
\newblock Distributional soft actor critic for risk sensitive learning.
\newblock \emph{arXiv preprint arXiv:2004.14547}, 2020.

\bibitem[Mahajan et~al.(2019)Mahajan, Rashid, Samvelyan, and
  Whiteson]{mahajan2019maven}
Mahajan, A., Rashid, T., Samvelyan, M., and Whiteson, S.
\newblock {MAVEN}: Multi-agent variational exploration.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  7613--7624, 2019.

\bibitem[Majumdar \& Pavone(2020)Majumdar and Pavone]{majumdar2020should}
Majumdar, A. and Pavone, M.
\newblock How should a robot assess risk? {T}owards an axiomatic theory of risk
  in robotics.
\newblock In \emph{Robotics Research}, pp.\  75--84. Springer, 2020.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Oliehoek et~al.(2008)Oliehoek, Spaan, and
  Vlassis]{oliehoek2008optimal}
Oliehoek, F.~A., Spaan, M.~T., and Vlassis, N.
\newblock Optimal and approximate q-value functions for decentralized {POMDP}s.
\newblock \emph{Journal of Artificial Intelligence Research}, 32:\penalty0
  289--353, 2008.

\bibitem[Oliehoek et~al.(2016)Oliehoek, Amato, et~al.]{oliehoek2016concise}
Oliehoek, F.~A., Amato, C., et~al.
\newblock \emph{A Concise Introduction to Decentralized {POMDPs}}, volume~1.
\newblock Springer, 2016.

\bibitem[Privault(2020)]{fra}
Privault, N.
\newblock Notes on {F}inancial {R}isk and {A}nalytics.
\newblock Course notes, 268 pages, 2020.
\newblock Accessed: 2020-09-27.

\bibitem[Rashid et~al.(2018)Rashid, Samvelyan, Schroeder, Farquhar, Foerster,
  and Whiteson]{rashid2018qmix}
Rashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and
  Whiteson, S.
\newblock {QMIX}: Monotonic value function factorisation for deep multi-agent
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4295--4304, 2018.

\bibitem[Rashid et~al.(2020)Rashid, Samvelyan, De~Witt, Farquhar, Foerster, and
  Whiteson]{rashid2020monotonic}
Rashid, T., Samvelyan, M., De~Witt, C.~S., Farquhar, G., Foerster, J., and
  Whiteson, S.
\newblock Monotonic value function factorisation for deep multi-agent
  reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (178):\penalty0 1--51, 2020.

\bibitem[Rockafellar \& Uryasev(2002)Rockafellar and
  Uryasev]{rockafellar2002conditional}
Rockafellar, R.~T. and Uryasev, S.
\newblock Conditional value-at-risk for general loss distributions.
\newblock \emph{Journal of Banking \& Finance}, 26\penalty0 (7):\penalty0
  1443--1471, 2002.

\bibitem[Rockafellar et~al.(2000)Rockafellar, Uryasev,
  et~al.]{rockafellar2000optimization}
Rockafellar, R.~T., Uryasev, S., et~al.
\newblock Optimization of conditional value-at-risk.
\newblock \emph{Journal of Risk}, 2:\penalty0 21--42, 2000.

\bibitem[Ruszczy{\'n}ski(2010)]{ruszczynski2010risk}
Ruszczy{\'n}ski, A.
\newblock Risk-averse dynamic programming for markov decision processes.
\newblock \emph{Mathematical Programming}, 125\penalty0 (2):\penalty0 235--261,
  2010.

\bibitem[Samvelyan et~al.(2019)Samvelyan, Rashid, de~Witt, Farquhar, Nardelli,
  Rudner, Hung, Torr, Foerster, and Whiteson]{samvelyan19smac}
Samvelyan, M., Rashid, T., de~Witt, C.~S., Farquhar, G., Nardelli, N., Rudner,
  T. G.~J., Hung, C.-M., Torr, P. H.~S., Foerster, J., and Whiteson, S.
\newblock {The} {StarCraft} {Multi}-{Agent} {Challenge}.
\newblock \emph{CoRR}, abs/1902.04043, 2019.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering1}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
  A., Hubert, T., Baker, L., Lai, M., Bolton, A., et~al.
\newblock Mastering the game of {Go} without human knowledge.
\newblock \emph{Nature}, 550\penalty0 (7676):\penalty0 354â€“359, 2017.

\bibitem[Singh et~al.(2020)Singh, Kumar, and Lau]{singh2020hierarchical}
Singh, A.~J., Kumar, A., and Lau, H.~C.
\newblock Hierarchical multiagent reinforcement learning for maritime traffic
  management.
\newblock In \emph{Proceedings of the 19th International Conference on
  Autonomous Agents and MultiAgent Systems}, pp.\  1278--1286, 2020.

\bibitem[Son et~al.(2019)Son, Kim, Kang, Hostallero, and Yi]{son2019qtran}
Son, K., Kim, D., Kang, W.~J., Hostallero, D.~E., and Yi, Y.
\newblock {QTRAN}: Learning to factorize with transformation for cooperative
  multi-agent reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5887--5896, 2019.

\bibitem[Sunehag et~al.(2017)Sunehag, Lever, Gruslys, Czarnecki, Zambaldi,
  Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, et~al.]{sunehag2017value}
Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg,
  M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., et~al.
\newblock Value-decomposition networks for cooperative multi-agent learning.
\newblock \emph{arXiv preprint arXiv:1706.05296}, 2017.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock MIT press, 2018.

\bibitem[Tamar et~al.(2015)Tamar, Chow, Ghavamzadeh, and
  Mannor]{tamar2015policy}
Tamar, A., Chow, Y., Ghavamzadeh, M., and Mannor, S.
\newblock Policy gradient for coherent risk measures.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1468--1476, 2015.

\bibitem[Tampuu et~al.(2017)Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus, Aru,
  Aru, and Vicente]{tampuu2017multiagent}
Tampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru,
  J., and Vicente, R.
\newblock Multiagent cooperation and competition with deep reinforcement
  learning.
\newblock \emph{PLoS ONE}, 12\penalty0 (4), 2017.

\bibitem[Tang et~al.(2020)Tang, Zhang, and Salakhutdinov]{tang2019worst}
Tang, Y.~C., Zhang, J., and Salakhutdinov, R.
\newblock Worst cases policy gradients.
\newblock In \emph{Conference on Robot Learning}, pp.\  1078--1093, 2020.

\bibitem[Thrun \& Schwartz(1993)Thrun and Schwartz]{thrun1993issues}
Thrun, S. and Schwartz, A.
\newblock Issues in using function approximation for reinforcement learning.
\newblock In \emph{Proceedings of the Fourth Connectionist Models Summer
  School}, pp.\  255--263, 1993.

\bibitem[Vinyals et~al.(2017)Vinyals, Ewalds, Bartunov, Georgiev, Vezhnevets,
  Yeo, Makhzani, K{\"u}ttler, Agapiou, Schrittwieser,
  et~al.]{vinyals2017starcraft}
Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A.~S., Yeo,
  M., Makhzani, A., K{\"u}ttler, H., Agapiou, J., Schrittwieser, J., et~al.
\newblock {StarCraft II}: A new challenge for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1708.04782}, 2017.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al.
\newblock Grandmaster level in {StarCraft II} using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Von~Neumann \& Morgenstern(1947)Von~Neumann and
  Morgenstern]{von1947theory}
Von~Neumann, J. and Morgenstern, O.
\newblock \emph{Theory of {G}ames and {E}conomic {B}ehavior, 2nd rev}.
\newblock Princeton university press, 1947.

\bibitem[Yang et~al.(2020)Yang, Hao, Liao, Shao, Chen, Liu, and
  Tang]{yang2020qatten}
Yang, Y., Hao, J., Liao, B., Shao, K., Chen, G., Liu, W., and Tang, H.
\newblock Qatten: A general framework for cooperative multiagent reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2002.03939}, 2020.

\bibitem[Zhang et~al.(2020)Zhang, Bedi, Wang, and Koppel]{zhang2020cautious}
Zhang, J., Bedi, A.~S., Wang, M., and Koppel, A.
\newblock Cautious reinforcement learning via distributional risk in the dual
  domain.
\newblock \emph{arXiv preprint arXiv:2002.12475}, 2020.

\end{thebibliography}
