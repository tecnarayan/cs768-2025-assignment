\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abel(2019)]{abel2019theory}
David Abel.
\newblock A theory of state abstraction for reinforcement learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~33, pages 9876--9877, 2019.

\bibitem[Allen et~al.(2021)Allen, Parikh, Gottesman, and Konidaris]{allen2021learning}
Cameron Allen, Neev Parikh, Omer Gottesman, and George Konidaris.
\newblock Learning {M}arkov state abstractions for deep reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 8229--8241, 2021.

\bibitem[Bembom and van~der Laan(2008)]{bembom2008data}
Oliver Bembom and Mark~J van~der Laan.
\newblock Data-adaptive selection of the truncation level for inverse-probability-of-treatment-weighted estimators.
\newblock 2008.

\bibitem[Billingsley(2013)]{billingsley2013convergence}
Patrick Billingsley.
\newblock \emph{Convergence of Probability Measures}.
\newblock John Wiley \& Sons, 2013.

\bibitem[Bottou et~al.(2013)Bottou, Peters, Quiñonero-Candela, Charles, Chickering, Portugaly, Ray, Simard, and Snelson]{bottou2013counterfactual}
Léon Bottou, Jonas Peters, Joaquin Quiñonero-Candela, Denis~X Charles, D~Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed~Snelson.
\newblock Counterfactual reasoning and learning systems: The example of computational advertising.
\newblock \emph{Journal of Machine Learning Research}, 14\penalty0 (11), 2013.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider, Schulman, Tang, and Zaremba]{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba.
\newblock {OpenAI} gym.
\newblock \emph{arXiv Preprint arXiv:1606.01540}, 2016.

\bibitem[Chapelle and Li(2011)]{chapelle2011empirical}
Olivier Chapelle and Lihong Li.
\newblock An empirical evaluation of {T}hompson sampling.
\newblock \emph{Advances in Neural Information Processing Systems}, 24, 2011.

\bibitem[Choudhary et~al.(2024)Choudhary, Gupta, and Thomas]{anonymous2024}
Kartik Choudhary, Dhawal Gupta, and Philip~S. Thomas.
\newblock {ICU-Sepsis}: A benchmark {MDP} built from real medical data, 2024.

\bibitem[Efroni et~al.(2022)Efroni, Jin, Krishnamurthy, and Miryoosefi]{efroni2022provable}
Yonathan Efroni, Chi Jin, Akshay Krishnamurthy, and Sobhan Miryoosefi.
\newblock Provable reinforcement learning with a short-term memory.
\newblock In \emph{Proceedings of the International Conference on Machine Learning}, pages 5832--5850. PMLR, 2022.

\bibitem[Farahmand and Szepesv{\'a}ri(2011)]{farahmand2011model}
Amir-Massoud Farahmand and Csaba Szepesv{\'a}ri.
\newblock Model selection in reinforcement learning.
\newblock \emph{Machine Learning}, 85\penalty0 (3):\penalty0 299--332, 2011.

\bibitem[Farajtabar et~al.(2018)Farajtabar, Chow, and Ghavamzadeh]{farajtabar2018more}
Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh.
\newblock More robust doubly robust off-policy evaluation.
\newblock In \emph{Proceedings of the International Conference on Machine Learning}, pages 1447--1456. PMLR, 2018.

\bibitem[Ferns et~al.(2012)Ferns, Panangaden, and Precup]{ferns2012metrics}
Norman Ferns, Prakash Panangaden, and Doina Precup.
\newblock Metrics for finite {M}arkov decision processes.
\newblock \emph{arXiv Preprint arXiv:1207.4114}, 2012.

\bibitem[Field and Smith(1994)]{field1994robust}
Chris Field and B~Smith.
\newblock Robust estimation: A weighted maximum likelihood approach.
\newblock \emph{International Statistical Review/Revue Internationale de Statistique}, pages 405--424, 1994.

\bibitem[Fu et~al.(2021)Fu, Norouzi, Nachum, Tucker, Wang, Novikov, Yang, Zhang, Chen, Kumar, et~al.]{fu2021benchmarks}
Justin Fu, Mohammad Norouzi, Ofir Nachum, George Tucker, Ziyu Wang, Alexander Novikov, Mengjiao Yang, Michael~R Zhang, Yutian Chen, Aviral Kumar, et~al.
\newblock Benchmarks for deep off-policy evaluation.
\newblock \emph{arXiv Preprint arXiv:2103.16596}, 2021.

\bibitem[Gao et~al.(2023)Gao, Ju, Ausin, and Chi]{gao2023hope}
Ge~Gao, Song Ju, Markel~Sanz Ausin, and Min Chi.
\newblock Hope: Human-centric off-policy evaluation for e-learning and healthcare.
\newblock \emph{arXiv Preprint arXiv:2302.09212}, 2023.

\bibitem[Gariepy and Ronald(2015)]{gariepy2015measure}
LC~Evans-RF Gariepy and F~Ronald.
\newblock \emph{Measure theory and fine properties of functions, Revised edition}.
\newblock {CRC} Press, 2015.

\bibitem[Guo et~al.(2017{\natexlab{a}})Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q Weinberger.
\newblock On calibration of modern neural networks.
\newblock In \emph{International conference on machine learning}, pages 1321--1330. PMLR, 2017{\natexlab{a}}.

\bibitem[Guo et~al.(2017{\natexlab{b}})Guo, Thomas, and Brunskill]{guo2017using}
Zhaohan Guo, Philip~S Thomas, and Emma Brunskill.
\newblock Using options and covariance testing for long horizon off-policy policy evaluation.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017{\natexlab{b}}.

\bibitem[Hanna et~al.(2019)Hanna, Niekum, and Stone]{hanna2019importance}
Josiah Hanna, Scott Niekum, and Peter Stone.
\newblock Importance sampling policy evaluation with an estimated behavior policy.
\newblock In \emph{Proceedings of the International Conference on Machine Learning}, pages 2605--2613. PMLR, 2019.

\bibitem[Ionides(2008)]{ionides2008truncated}
Edward~L Ionides.
\newblock Truncated importance sampling.
\newblock \emph{Journal of Computational and Graphical Statistics}, 17\penalty0 (2):\penalty0 295--311, 2008.

\bibitem[Jiang(2018)]{jiang2018notes}
Nan Jiang.
\newblock Notes on state abstractions, 2018.
\newblock URL \url{https://nanjiang.cs.illinois.edu/files/cs598/note4.pdf}.

\bibitem[Jiang and Li(2016)]{jiang2016doubly}
Nan Jiang and Lihong Li.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine Learning}, pages 652--661. PMLR, 2016.

\bibitem[Jiang et~al.(2015)Jiang, Kulesza, and Singh]{jiang2015abstraction}
Nan Jiang, Alex Kulesza, and Satinder Singh.
\newblock Abstraction selection in model-based reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine Learning}, pages 179--188. PMLR, 2015.

\bibitem[Johnson et~al.(2016)Johnson, Pollard, Shen, Lehman, Feng, Ghassemi, Moody, Szolovits, Anthony~Celi, and Mark]{johnson2016mimic}
Alistair~EW Johnson, Tom~J Pollard, Lu~Shen, Li-wei~H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony~Celi, and Roger~G Mark.
\newblock {MIMIC-III}, a freely accessible critical care database.
\newblock \emph{Scientific Data}, 3\penalty0 (1):\penalty0 1--9, 2016.

\bibitem[Kahn and Harris(1951)]{kahn1951estimation}
Herman Kahn and Theodore~E Harris.
\newblock Estimation of particle transmission by random sampling.
\newblock \emph{National Bureau of Standards Applied Mathematics Series}, 12:\penalty0 27--30, 1951.

\bibitem[Komorowski et~al.(2018)Komorowski, Celi, Badawi, Gordon, and Faisal]{komorowski2018artificial}
Matthieu Komorowski, Leo~A Celi, Omar Badawi, Anthony~C Gordon, and A~Aldo Faisal.
\newblock The artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care.
\newblock \emph{Nature Medicine}, 24\penalty0 (11):\penalty0 1716--1720, 2018.

\bibitem[Le et~al.(2019)Le, Voloshin, and Yue]{le2019batch}
Hoang Le, Cameron Voloshin, and Yisong Yue.
\newblock Batch policy learning under constraints.
\newblock In \emph{Proceedings of the International Conference on Machine Learning}, pages 3703--3712. PMLR, 2019.

\bibitem[Li et~al.(2006)Li, Walsh, and Littman]{li2006towards}
Lihong Li, Thomas~J Walsh, and Michael~L Littman.
\newblock Towards a unified theory of state abstraction for {MDP}s.
\newblock \emph{Artificial Intelligence and Machine Learning (AI\&M)}, 1\penalty0 (2):\penalty0 3, 2006.

\bibitem[Li et~al.(2015)Li, Munos, and Szepesv{\'a}ri]{li2015toward}
Lihong Li, R{\'e}mi Munos, and Csaba Szepesv{\'a}ri.
\newblock Toward minimax off-policy value estimation.
\newblock In \emph{Proceedings of the Artificial Intelligence and Statistics Conference}, pages 608--616. PMLR, 2015.

\bibitem[Littman and Sutton(2001)]{littman2001predictive}
Michael Littman and Richard~S. Sutton.
\newblock Predictive representations of state.
\newblock \emph{Advances in Neural Information Processing Systems}, 14, 2001.

\bibitem[Liu et~al.(2018{\natexlab{a}})Liu, Li, Tang, and Zhou]{liu2018breaking}
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou.
\newblock Breaking the curse of horizon: Infinite-horizon off-policy estimation.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018{\natexlab{a}}.

\bibitem[Liu et~al.(2018{\natexlab{b}})Liu, Gottesman, Raghu, Komorowski, Faisal, Doshi-Velez, and Brunskill]{liu2018representation}
Yao Liu, Omer Gottesman, Aniruddh Raghu, Matthieu Komorowski, Aldo~A Faisal, Finale Doshi-Velez, and Emma Brunskill.
\newblock Representation balancing {MDP}s for off-policy policy evaluation.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018{\natexlab{b}}.

\bibitem[Lloyd(1982)]{lloyd1982least}
Stuart Lloyd.
\newblock {Least Squares Quantization in PCM}.
\newblock \emph{IEEE Transactions on Information Theory}, 28\penalty0 (2):\penalty0 129--137, 1982.

\bibitem[MacQueen(1967)]{macqueen1967some}
James MacQueen.
\newblock Some methods for classification and analysis of multivariate observations.
\newblock In \emph{Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability}, volume~1, pages 281--297, Oakland, CA, USA, 1967. University of California Press.

\bibitem[Mandel et~al.(2014)Mandel, Liu, Levine, Brunskill, and Popovic]{mandel2014offline}
Travis Mandel, Yun-En Liu, Sergey Levine, Emma Brunskill, and Zoran Popovic.
\newblock Offline policy evaluation across representations with applications to educational games.
\newblock In \emph{Proceedings of the International Conference on Autonomous Agents and Multiagent Systems}, volume 1077, 2014.

\bibitem[Marivate(2015)]{marivate2015improved}
Vukosi~N Marivate.
\newblock \emph{Improved Empirical Methods in Reinforcement-Learning Evaluation}.
\newblock Rutgers The State University of New Jersey, School of Graduate Studies, 2015.

\bibitem[Markov(1954)]{markov1954theory}
Andrei~Andreevich Markov.
\newblock The theory of algorithms.
\newblock \emph{Trudy Matematicheskogo Instituta Imeni VA Steklova}, 42:\penalty0 3--375, 1954.

\bibitem[Murphy et~al.(2001)Murphy, van~der Laan, Robins, and Group]{murphy2001marginal}
Susan~A Murphy, Mark~J van~der Laan, James~M Robins, and Conduct Problems Prevention~Research Group.
\newblock Marginal mean models for dynamic regimes.
\newblock \emph{Journal of the American Statistical Association}, 96\penalty0 (456):\penalty0 1410--1423, 2001.

\bibitem[Nguyen(2021)]{nguyen2021converting}
Khanh~Xuan Nguyen.
\newblock Converting {POMDP}s into {MDP}s using history representation.
\newblock \emph{Engineering Archive}, 2021.

\bibitem[Paduraru(2013)]{paduraru2013off}
Cosmin Paduraru.
\newblock \emph{Off-Policy Evaluation in {M}arkov Decision Processes}.
\newblock PhD thesis, 2013.

\bibitem[Pavse and Hanna(2023)]{pavse2023scaling}
Brahma~S Pavse and Josiah~P Hanna.
\newblock Scaling marginalized importance sampling to high-dimensional state-spaces via state abstraction.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pages 9417--9425, 2023.

\bibitem[Precup(2000)]{precup2000eligibility}
Doina Precup.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock \emph{Computer Science Department Faculty Publication Series}, page~80, 2000.

\bibitem[Ravindran(2004)]{ravindran2004algebraic}
Balaraman Ravindran.
\newblock \emph{An Algebraic Approach to Abstraction in Reinforcement Learning}.
\newblock University of Massachusetts Amherst, 2004.

\bibitem[Rubinstein and Kroese(2016)]{rubinstein2016simulation}
Reuven~Y Rubinstein and Dirk~P Kroese.
\newblock \emph{Simulation and the Monte Carlo Method}.
\newblock John Wiley \& Sons, 2016.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and Moritz]{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{Proceedings of the International Conference on Machine Learning}, pages 1889--1897. PMLR, 2015.

\bibitem[Sen and Singer(1994)]{sen1994large}
Pranab~K Sen and Julio~M Singer.
\newblock \emph{Large Sample Methods in Statistics: An Introduction with Applications}, volume~25.
\newblock CRC Press, 1994.

\bibitem[Sinclair et~al.(2019)Sinclair, Banerjee, and Yu]{sinclair2019adaptive}
Sean~R Sinclair, Siddhartha Banerjee, and Christina~Lee Yu.
\newblock Adaptive discretization for episodic reinforcement learning in metric spaces.
\newblock \emph{Proceedings of the ACM on Measurement and Analysis of Computing Systems}, 3\penalty0 (3):\penalty0 1--44, 2019.

\bibitem[Su et~al.(2020)Su, Srinath, and Krishnamurthy]{su2020adaptive}
Yi~Su, Pavithra Srinath, and Akshay Krishnamurthy.
\newblock Adaptive estimator selection for off-policy evaluation.
\newblock In \emph{Proceedings of the International Conference on Machine Learning}, pages 9196--9205. PMLR, 2020.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock MIT Press, 2018.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{sutton1999between}
Richard~S Sutton, Doina Precup, and Satinder Singh.
\newblock Between {MDP}s and semi-{MDP}s: A framework for temporal abstraction in reinforcement learning.
\newblock \emph{Artificial Intelligence}, 112\penalty0 (1-2):\penalty0 181--211, 1999.

\bibitem[Talvitie(2010)]{talvitie2010simple}
Erik~N Talvitie.
\newblock \emph{Simple Partial Models for Complex Dynamical Systems}.
\newblock PhD thesis, University of Michigan, 2010.

\bibitem[Tang and Wiens(2021)]{tang2021model}
Shengpu Tang and Jenna Wiens.
\newblock Model selection for offline reinforcement learning: Practical considerations for healthcare settings.
\newblock In \emph{Proceedings of the Machine Learning for Healthcare Conference}, pages 2--35. PMLR, 2021.

\bibitem[Thomas and Brunskill(2016)]{thomas2016data}
Philip Thomas and Emma Brunskill.
\newblock Data-efficient off-policy policy evaluation for reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine Learning}, pages 2139--2148. PMLR, 2016.

\bibitem[Tolman(1948)]{tolman1948cognitive}
Edward~C Tolman.
\newblock Cognitive maps in rats and men.
\newblock \emph{Psychological Review}, 55\penalty0 (4):\penalty0 189, 1948.

\bibitem[Udagawa et~al.(2023)Udagawa, Kiyohara, Narita, Saito, and Tateno]{udagawa2023policy}
Takuma Udagawa, Haruka Kiyohara, Yusuke Narita, Yuta Saito, and Kei Tateno.
\newblock Policy-adaptive estimator selection for off-policy evaluation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pages 10025--10033, 2023.

\bibitem[Uehara et~al.(2022)Uehara, Shi, and Kallus]{uehara2022review}
Masatoshi Uehara, Chengchun Shi, and Nathan Kallus.
\newblock A review of off-policy evaluation in reinforcement learning.
\newblock \emph{arXiv Preprint arXiv:2212.06355}, 2022.

\bibitem[Van~der Vaart(2000)]{van2000asymptotic}
Aad~W Van~der Vaart.
\newblock \emph{Asymptotic Statistics}, volume~3.
\newblock Cambridge University Press, 2000.

\bibitem[Voloshin et~al.(2019)Voloshin, Le, Jiang, and Yue]{voloshin2019empirical}
Cameron Voloshin, Hoang~M Le, Nan Jiang, and Yisong Yue.
\newblock Empirical study of off-policy policy evaluation for reinforcement learning.
\newblock \emph{arXiv Preprint arXiv:1911.06854}, 2019.

\bibitem[Wang et~al.(2022)Wang, Du, Torralba, Isola, Zhang, and Tian]{wang2022denoised}
Tongzhou Wang, Simon~S Du, Antonio Torralba, Phillip Isola, Amy Zhang, and Yuandong Tian.
\newblock Denoised {MDP}s: Learning world models better than the world itself.
\newblock \emph{arXiv Preprint arXiv:2206.15477}, 2022.

\bibitem[Weiss(2005)]{weiss2005course}
Neil~A. Weiss.
\newblock \emph{A Course in Probability}.
\newblock Addison-Wesley, Boston, 2005.
\newblock ISBN 0-321-18954-X.

\bibitem[Young and Tian(2019)]{young2019minatar}
Kenny Young and Tian Tian.
\newblock {MinAtar}: An {Atari}-inspired testbed for thorough and reproducible reinforcement learning experiments.
\newblock \emph{arXiv Preprint arXiv:1903.03176}, 2019.

\bibitem[Zhang et~al.(2021)Zhang, Paine, Nachum, Paduraru, Tucker, Wang, and Norouzi]{zhang2021autoregressive}
Michael~R Zhang, Tom~Le Paine, Ofir Nachum, Cosmin Paduraru, George Tucker, Ziyu Wang, and Mohammad Norouzi.
\newblock Autoregressive dynamics models for offline policy evaluation and optimization.
\newblock \emph{arXiv Preprint arXiv:2104.13877}, 2021.

\bibitem[Zhang et~al.(2020)Zhang, Dai, Li, and Schuurmans]{zhang2020gendice}
Ruiyi Zhang, Bo~Dai, Lihong Li, and Dale Schuurmans.
\newblock Gendice: Generalized offline estimation of stationary values.
\newblock \emph{arXiv Preprint arXiv:2002.09072}, 2020.

\bibitem[Zwillinger and Kokoska(1999)]{zwillinger1999crc}
Daniel Zwillinger and Stephen Kokoska.
\newblock \emph{{CRC} Standard Probability and Statistics Tables and Formulae}.
\newblock CRC Press, 1999.

\end{thebibliography}
