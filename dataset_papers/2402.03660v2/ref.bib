@techreport{rumelhart1985learning,
  title={Learning internal representations by error propagation},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  year={1985},
  institution={California Univ San Diego La Jolla Inst for Cognitive Science}
}

@INPROCEEDINGS{dilip2011sparsity,
  author={Krishnan, Dilip and Tay, Terence and Fergus, Rob},
  booktitle={CVPR 2011}, 
  title={Blind deconvolution using a normalized sparsity measure}, 
  year={2011},
  volume={},
  number={},
  pages={233-240},
  doi={10.1109/CVPR.2011.5995521}}

@InProceedings{li2015convergent,
  title = 	 {Convergent Learning: Do different neural networks learn the same representations?},
  author = 	 {Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John},
  booktitle = 	 {Proceedings of the 1st International Workshop on Feature Extraction: Modern Questions and Challenges at NIPS 2015},
  pages = 	 {196--212},
  year = 	 {2015},
  editor = 	 {Storcheus, Dmitry and Rostamizadeh, Afshin and Kumar, Sanjiv},
  volume = 	 {44},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Montreal, Canada},
  month = 	 {11 Dec},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v44/li15convergent.pdf},
  url = 	 {https://proceedings.mlr.press/v44/li15convergent.html}
}


@inproceedings{
    frankle2018lottery,
    title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
    author={Jonathan Frankle and Michael Carbin},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=rJl-b3RcF7},
}

@inproceedings{HechtNielsen1990ONTA,
  title={ON THE ALGEBRAIC STRUCTURE OF FEEDFORWARD NETWORK WEIGHT SPACES},
  author={Robert Hecht-Nielsen},
  year={1990}
}

@article{nagarajan2019uniform,
  title={Uniform convergence may be unable to explain generalization in deep learning},
  author={Nagarajan, Vaishnavh and Kolter, J Zico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{wortsman2022model,
  title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  booktitle={International Conference on Machine Learning},
  pages={23965--23998},
  year={2022},
  organization={PMLR}
}

@inproceedings{freeman2017topology,
    title={Topology and Geometry of Half-Rectified Network Optimization},
    author={C. Daniel Freeman and Joan Bruna},
    booktitle={International Conference on Learning Representations},
    year={2017},
    url={https://openreview.net/forum?id=Bk0FWVcgx}
}

@inproceedings{
    nguyen2018loss,
    title={On the loss landscape of a class of deep neural networks with no bad local valleys},
    author={Quynh Nguyen and Mahesh Chandra Mukkamala and Matthias Hein},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=HJgXsjA5tQ},
}

@inproceedings{nguyen2019connected,
  title={On connected sublevel sets in deep learning},
  author={Nguyen, Quynh},
  booktitle={International conference on machine learning},
  pages={4790--4799},
  year={2019},
  organization={PMLR}
}

@article{kuditipudi2019explaining,
  title={Explaining landscape connectivity of low-cost solutions for multilayer nets},
  author={Kuditipudi, Rohith and Wang, Xiang and Lee, Holden and Zhang, Yi and Li, Zhiyuan and Hu, Wei and Ge, Rong and Arora, Sanjeev},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{liang2018understanding,
  title={Understanding the loss surface of neural networks for binary classification},
  author={Liang, Shiyu and Sun, Ruoyu and Li, Yixuan and Srikant, Rayadurgam},
  booktitle={International Conference on Machine Learning},
  pages={2835--2843},
  year={2018},
  organization={PMLR}
}

@inproceedings{draxler2018essentially,
  title={Essentially no barriers in neural network energy landscape},
  author={Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred},
  booktitle={International conference on machine learning},
  pages={1309--1318},
  year={2018},
  organization={PMLR}
}

@article{garipov2018loss,
  title={Loss surfaces, mode connectivity, and fast ensembling of dnns},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew G},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle={International Conference on Machine Learning},
  pages={3259--3269},
  year={2020},
  organization={PMLR}
}

@misc{lubana2023mechanistic,
    title={Mechanistic Mode Connectivity},
    author={Ekdeep Singh Lubana and Eric J Bigelow and Robert P. Dick and David Krueger and Hidenori Tanaka},
    year={2023},
    url={https://openreview.net/forum?id=NZZoABNZECq}
}

@inproceedings{
    yunis2022on,
    title={On Convexity and Linear Mode Connectivity in Neural Networks},
    author={David Yunis and Kumar Kshitij Patel and Pedro Henrique Pamplona Savarese and Gal Vardi and Jonathan Frankle and Matthew Walter and Karen Livescu and Michael Maire},
    booktitle={OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)},
    year={2022},
    url={https://openreview.net/forum?id=TZQ3PKL3fPr}
}

@article{fort2020deep,
  title={Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel},
  author={Fort, Stanislav and Dziugaite, Gintare Karolina and Paul, Mansheej and Kharaghani, Sepideh and Roy, Daniel M and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5850--5861},
  year={2020}
}

@inproceedings{ashmore2015method,
  title={A method for finding similarity between multi-layer perceptrons by Forward Bipartite Alignment},
  author={Ashmore, Stephen and Gashler, Michael},
  booktitle={2015 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--7},
  year={2015},
  organization={IEEE}
}


@article{tatro2020optimizing,
  title={Optimizing mode connectivity via neuron alignment},
  author={Tatro, Norman and Chen, Pin-Yu and Das, Payel and Melnyk, Igor and Sattigeri, Prasanna and Lai, Rongjie},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15300--15311},
  year={2020}
}


@inproceedings{entezari2022the,
    title={The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks},
    author={Rahim Entezari and Hanie Sedghi and Olga Saukh and Behnam Neyshabur},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=dNigytemkL}
}

@inproceedings{ainsworth2023git,
    title={Git Re-Basin: Merging Models modulo Permutation Symmetries},
    author={Samuel Ainsworth and Jonathan Hayase and Siddhartha Srinivasa},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=CQsmMYmlP5T}
}

@inproceedings{Wang2020Federated,
    title={Federated Learning with Matched Averaging},
    author={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=BkluqlSFDS}
}

@article{singh2020model,
  title={Model fusion via optimal transport},
  author={Singh, Sidak Pal and Jaggi, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={22045--22055},
  year={2020}
}

@inproceedings{DBLP:conf/icml/LiuLWXSY22,
  author={Chang Liu and Chenfei Lou and Runzhong Wang and Alan Yuhan Xi and Li Shen and Junchi Yan},
  title={Deep Neural Network Fusion via Graph Matching with Applications to Model Ensemble and Federated Learning},
  year={2022},
  cdate={1640995200000},
  pages={13857-13869},
  url={https://proceedings.mlr.press/v162/liu22k.html},
  booktitle={ICML},
  crossref={conf/icml/2022}
}

@inproceedings{
    Li2020An,
    title={An Exponential Learning Rate Schedule for Deep Learning},
    author={Zhiyuan Li and Sanjeev Arora},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=rJg8TeSFDH}
}

@article{krizhevsky2009learning,
	title={Learning multiple layers of features from tiny images},
	author={Krizhevsky, Alex and Hinton, Geoffrey and others},
	year={2009},
	publisher={Citeseer}
}

@article{lecun1998gradient,
	title={Gradient-based learning applied to document recognition},
	author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
	journal={Proceedings of the IEEE},
	volume={86},
	number={11},
	pages={2278--2324},
	year={1998},
	publisher={Ieee}
}

@inproceedings{simonyan2015vgg,
  author       = {Karen Simonyan and
                  Andrew Zisserman},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1409.1556},
  timestamp    = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SimonyanZ14a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{kaiming2016residual,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}
}

@article{venturi2018spurious,
  author       = {Luca Venturi and
                  Afonso S. Bandeira and
                  Joan Bruna},
  title        = {Spurious Valleys in One-hidden-layer Neural Network Optimization Landscapes},
  journal      = {J. Mach. Learn. Res.},
  volume       = {20},
  pages        = {133:1--133:34},
  year         = {2019},
  url          = {http://jmlr.org/papers/v20/18-674.html},
  timestamp    = {Thu, 18 Jun 2020 22:13:25 +0200},
  biburl       = {https://dblp.org/rec/journals/jmlr/VenturiBB19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lenc2015understanding,
  title={Understanding image representations by measuring their equivariance and equivalence},
  author={Lenc, Karel and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={991--999},
  year={2015}
}
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{bansal2021revisiting,
    title={Revisiting Model Stitching to Compare Neural Representations},
    author={Yamini Bansal and Preetum Nakkiran and Boaz Barak},
    booktitle={Advances in Neural Information Processing Systems},
    editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
    year={2021},
    url={https://openreview.net/forum?id=ak06J5jNR4}
}

%new

@inproceedings{
zhou2023going,
title={Going Beyond Linear Mode Connectivity: The Layerwise Linear Feature Connectivity},
author={Zhanpeng Zhou and Yongyi Yang and Xiaojiang Yang and Junchi Yan and Wei Hu},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=vORUHrVEnH}
}

@inproceedings{
ilharco2023editing,
title={Editing models with task arithmetic},
author={Gabriel Ilharco and Marco Tulio Ribeiro and Mitchell Wortsman and Ludwig Schmidt and Hannaneh Hajishirzi and Ali Farhadi},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=6t0Kwf8-jrj}
}

@inproceedings{
ortiz-jimenez2023task,
title={Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models},
author={Guillermo Ortiz-Jimenez and Alessandro Favero and Pascal Frossard},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=0A9f2jZDGW}
}

@inproceedings{
Frankle2020The,
title={The Early Phase of Neural Network Training},
author={Jonathan Frankle and David J. Schwab and Ari S. Morcos},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Hkl1iRNFwS}
}

@inproceedings{
zhao2023understanding,
title={Understanding Mode Connectivity via Parameter Space Symmetry},
author={Bo Zhao and Nima Dehmamy and Robin Walters and Rose Yu},
booktitle={UniReps:  the First Workshop on Unifying Representations in Neural Models},
year={2023},
url={https://openreview.net/forum?id=aP2a5i1iUf}
}

@article{ferbach2023proving,
  title={Proving Linear Mode Connectivity of Neural Networks via Optimal Transport},
  author={Ferbach, Damien and Goujaud, Baptiste and Gidel, Gauthier and Dieuleveut, Aymeric},
  journal={arXiv preprint arXiv:2310.19103},
  year={2023}
}

@inproceedings{qin-etal-2022-exploring,
    title = "Exploring Mode Connectivity for Pre-trained Language Models",
    author = "Qin, Yujia  and
      Qian, Cheng  and
      Yi, Jing  and
      Chen, Weize  and
      Lin, Yankai  and
      Han, Xu  and
      Liu, Zhiyuan  and
      Sun, Maosong  and
      Zhou, Jie",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.451",
    doi = "10.18653/v1/2022.emnlp-main.451",
    pages = "6726--6746",
    abstract = "Recent years have witnessed the prevalent application of pre-trained language models (PLMs) in NLP. From the perspective of parameter space, PLMs provide generic initialization, starting from which high-performance minima could be found. Although plenty of works have studied how to effectively and efficiently adapt PLMs to high-performance minima, little is known about the connection of various minima reached under different adaptation configurations. In this paper, we investigate the geometric connections of different minima through the lens of mode connectivity, which measures whether two minima can be connected with a low-loss path. We conduct empirical analyses to investigate three questions: (1) how could hyperparameters, specific tuning methods, and training data affect PLM{'}s mode connectivity? (2) How does mode connectivity change during pre-training? (3) How does the PLM{'}s task knowledge change along the path connecting two minima? In general, exploring the mode connectivity of PLMs conduces to understanding the geometric connection of different minima, which may help us fathom the inner workings of PLM downstream adaptation. The codes are publicly available at \url{https://github.com/thunlp/Mode-Connectivity-PLM}.",
}

@inproceedings{
mirzadeh2021linear,
title={Linear Mode Connectivity in Multitask and Continual Learning},
author={Seyed Iman Mirzadeh and Mehrdad Farajtabar and Dilan Gorur and Razvan Pascanu and Hassan Ghasemzadeh},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=Fmg_fQYUejf}
}

@inproceedings{DBLP:conf/uai/IzmailovPGVW18,
  author       = {Pavel Izmailov and
                  Dmitrii Podoprikhin and
                  Timur Garipov and
                  Dmitry P. Vetrov and
                  Andrew Gordon Wilson},
  editor       = {Amir Globerson and
                  Ricardo Silva},
  title        = {Averaging Weights Leads to Wider Optima and Better Generalization},
  booktitle    = {Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial
                  Intelligence, {UAI} 2018, Monterey, California, USA, August 6-10,
                  2018},
  pages        = {876--885},
  publisher    = {{AUAI} Press},
  year         = {2018},
  url          = {http://auai.org/uai2018/proceedings/papers/313.pdf},
  timestamp    = {Wed, 09 Mar 2022 16:53:19 +0100},
  biburl       = {https://dblp.org/rec/conf/uai/IzmailovPGVW18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
matena2022merging,
title={Merging Models with Fisher-Weighted Averaging},
author={Michael S Matena and Colin Raffel},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=LSKlp_aceOC}
}

@InProceedings{pmlr-v202-rame23a,
  title = 	 {Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization},
  author =       {Rame, Alexandre and Ahuja, Kartik and Zhang, Jianyu and Cord, Matthieu and Bottou, Leon and Lopez-Paz, David},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {28656--28679},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/rame23a/rame23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/rame23a.html},
  abstract = 	 {Foundation models are redefining how AI systems are built. Practitioners now follow a standard procedure to build their machine learning solutions: from a pre-trained foundation model, they fine-tune the weights on the target task of interest. So, the Internet is swarmed by a handful of foundation models fine-tuned on many diverse tasks: these individual fine-tunings exist in isolation without benefiting from each other. In our opinion, this is a missed opportunity, as these specialized models contain rich and diverse features. In this paper, we thus propose model ratatouille, a new strategy to recycle the multiple fine-tunings of the same foundation model on diverse auxiliary tasks. Specifically, we repurpose these auxiliary weights as initializations for multiple parallel fine-tunings on the target task; then, we average all fine-tuned weights to obtain the final model. This recycling strategy aims at maximizing the diversity in weights by leveraging the diversity in auxiliary tasks. Empirically, it improves the state of the art on the reference DomainBed benchmark for out-of-distribution generalization. Looking forward, this work contributes to the emerging paradigm of updatable machine learning where, akin to open-source software development, the community collaborates to reliably update machine learning models.}
}

@inproceedings{
rame2022diverse,
title={Diverse Weight Averaging for Out-of-Distribution Generalization},
author={Alexandre Rame and Matthieu Kirchmeyer and Thibaud Rahier and Alain Rakotomamonjy and patrick gallinari and Matthieu Cord},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=tq_J_MqB3UB}
}

@InProceedings{Wortsman_2022_CVPR,
    author    = {Wortsman, Mitchell and Ilharco, Gabriel and Kim, Jong Wook and Li, Mike and Kornblith, Simon and Roelofs, Rebecca and Lopes, Raphael Gontijo and Hajishirzi, Hannaneh and Farhadi, Ali and Namkoong, Hongseok and Schmidt, Ludwig},
    title     = {Robust Fine-Tuning of Zero-Shot Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {7959-7971}
}

@inproceedings{
ilharco2022patching,
title={Patching open-vocabulary models by interpolating weights},
author={Gabriel Ilharco and Mitchell Wortsman and Samir Yitzhak Gadre and Shuran Song and Hannaneh Hajishirzi and Simon Kornblith and Ali Farhadi and Ludwig Schmidt},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=CZZFRxbOLC}
}

@inproceedings{
li2022branchtrainmerge,
title={Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models},
author={Margaret Li and Suchin Gururangan and Tim Dettmers and Mike Lewis and Tim Althoff and Noah A. Smith and Luke Zettlemoyer},
booktitle={ First Workshop on Interpolation Regularizers and Beyond at NeurIPS 2022},
year={2022},
url={https://openreview.net/forum?id=SQgVgE2Sq4}
}

@inproceedings{
yadav2023tiesmerging,
title={{TIES}-Merging: Resolving Interference When Merging Models},
author={Prateek Yadav and Derek Tam and Leshem Choshen and Colin Raffel and Mohit Bansal},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=xtaX3WyCj1}
}

@misc{stoica2023zipit,
      title={ZipIt! Merging Models from Different Tasks without Training},
      author={George Stoica and Daniel Bolya and Jakob Bjorner and Taylor Hearn and Judy Hoffman},
      year={2023},
      eprint={2305.03053},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{
jin2023dataless,
title={Dataless Knowledge Fusion by Merging Weights of Language Models},
author={Xisen Jin and Xiang Ren and Daniel Preotiuc-Pietro and Pengxiang Cheng},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=FCnohuR6AnM}
}

@misc{yu2023language,
      title={Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch}, 
      author={Le Yu and Bowen Yu and Haiyang Yu and Fei Huang and Yongbin Li},
      year={2023},
      eprint={2311.03099},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@InProceedings{pmlr-v162-liu22k,
  title = 	 {Deep Neural Network Fusion via Graph Matching with Applications to Model Ensemble and Federated Learning},
  author =       {Liu, Chang and Lou, Chenfei and Wang, Runzhong and Xi, Alan Yuhan and Shen, Li and Yan, Junchi},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {13857--13869},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/liu22k/liu22k.pdf},
  url = 	 {https://proceedings.mlr.press/v162/liu22k.html},
  abstract = 	 {Model fusion without accessing training data in machine learning has attracted increasing interest due to the practical resource-saving and data privacy issues. During the training process, the neural weights of each model can be randomly permuted, and we have to align the channels of each layer before fusing them. Regrading the channels as nodes and weights as edges, aligning the channels to maximize weight similarity is a challenging NP-hard assignment problem. Due to its quadratic assignment nature, we formulate the model fusion problem as a graph matching task, considering the second-order similarity of model weights instead of previous work merely formulating model fusion as a linear assignment problem. For the rising problem scale and multi-model consistency issues, we propose an efficient graduated assignment-based model fusion method, dubbed GAMF, which iteratively updates the matchings in a consistency-maintaining manner. We apply GAMF to tackle the compact model ensemble task and federated learning task on MNIST, CIFAR-10, CIFAR-100, and Tiny-Imagenet. The performance shows the efficacy of our GAMF compared to state-of-the-art baselines.}
}

@article{adilova2023layerwise,
  title={Layerwise Linear Mode Connectivity},
  author={Adilova, Linara and Fischer, Asja and Jaggi, Martin},
  journal={arXiv preprint arXiv:2307.06966},
  year={2023}
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{olsson2022context,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova
and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and
Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and
Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom
and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}

@inproceedings{
ouyang2022training,
title={Training language models to follow instructions with human feedback},
author={Long Ouyang and Jeffrey Wu and Xu Jiang and Diogo Almeida and Carroll Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Gray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=TG8KACxEON}
}

@misc{weidinger2021ethical,
      title={Ethical and social risks of harm from Language Models}, 
      author={Laura Weidinger and John Mellor and Maribeth Rauh and Conor Griffin and Jonathan Uesato and Po-Sen Huang and Myra Cheng and Mia Glaese and Borja Balle and Atoosa Kasirzadeh and Zac Kenton and Sasha Brown and Will Hawkins and Tom Stepleton and Courtney Biles and Abeba Birhane and Julia Haas and Laura Rimell and Lisa Anne Hendricks and William Isaac and Sean Legassick and Geoffrey Irving and Iason Gabriel},
      year={2021},
      eprint={2112.04359},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
juneja2023linear,
title={Linear Connectivity Reveals Generalization Strategies},
author={Jeevesh Juneja and Rachit Bansal and Kyunghyun Cho and Jo{\~a}o Sedoc and Naomi Saphra},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=hY6M0JHl3uL}
}

@incollection{mccloskey1989catastrophic,
  title={Catastrophic interference in connectionist networks: The sequential learning problem},
  author={McCloskey, Michael and Cohen, Neal J},
  booktitle={Psychology of learning and motivation},
  volume={24},
  pages={109--165},
  year={1989},
  publisher={Elsevier}
}
% ================ appendix =======================
@article{Krause20133DOR,
  title={3D Object Representations for Fine-Grained Categorization},
  author={Jonathan Krause and Michael Stark and Jia Deng and Li Fei-Fei},
  journal={2013 IEEE International Conference on Computer Vision Workshops},
  year={2013},
  pages={554-561},
  url={https://api.semanticscholar.org/CorpusID:14342571}
}

@inproceedings{cimpoi2014describing,
  title={Describing textures in the wild},
  author={Cimpoi, Mircea and Maji, Subhransu and Kokkinos, Iasonas and Mohamed, Sammy and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3606--3613},
  year={2014}
}

@article{helber2019eurosat,
  title={Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification},
  author={Helber, Patrick and Bischke, Benjamin and Dengel, Andreas and Borth, Damian},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  volume={12},
  number={7},
  pages={2217--2226},
  year={2019},
  publisher={IEEE}
}
% gtsrb
@inproceedings{stallkamp2011german,
  title={The German traffic sign recognition benchmark: a multi-class classification competition},
  author={Stallkamp, Johannes and Schlipsing, Marc and Salmen, Jan and Igel, Christian},
  booktitle={The 2011 international joint conference on neural networks},
  pages={1453--1460},
  year={2011},
  organization={IEEE}
}

@inproceedings{LeCun2005TheMD,
  title={The mnist database of handwritten digits},
  author={Yann LeCun and Corinna Cortes},
  year={2005},
  url={https://api.semanticscholar.org/CorpusID:60282629}
}

@article{cheng2017remote,
  title={Remote sensing image scene classification: Benchmark and state of the art},
  author={Cheng, Gong and Han, Junwei and Lu, Xiaoqiang},
  journal={Proceedings of the IEEE},
  volume={105},
  number={10},
  pages={1865--1883},
  year={2017},
  publisher={IEEE}
}

@article{xiao2016sun,
  title={Sun database: Exploring a large collection of scene categories},
  author={Xiao, Jianxiong and Ehinger, Krista A and Hays, James and Torralba, Antonio and Oliva, Aude},
  journal={International Journal of Computer Vision},
  volume={119},
  pages={3--22},
  year={2016},
  publisher={Springer}
}

@article{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  year={2011}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@inproceedings{maas2011learning,
  title={Learning word vectors for sentiment analysis},
  author={Maas, Andrew and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies},
  pages={142--150},
  year={2011}
}

@article{lai2017race,
  title={Race: Large-scale reading comprehension dataset from examinations},
  author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  journal={arXiv preprint arXiv:1704.04683},
  year={2017}
}

@inproceedings{khot2020qasc,
  title={Qasc: A dataset for question answering via sentence composition},
  author={Khot, Tushar and Clark, Peter and Guerquin, Michal and Jansen, Peter and Sabharwal, Ashish},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={8082--8090},
  year={2020}
}

@article{fabbri2019multi,
  title={Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model},
  author={Fabbri, Alexander R and Li, Irene and She, Tianwei and Li, Suyi and Radev, Dragomir R},
  journal={arXiv preprint arXiv:1906.01749},
  year={2019}
}
@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@article{lin2019commongen,
  title={CommonGen: A constrained text generation challenge for generative commonsense reasoning},
  author={Lin, Bill Yuchen and Zhou, Wangchunshu and Shen, Ming and Zhou, Pei and Bhagavatula, Chandra and Choi, Yejin and Ren, Xiang},
  journal={arXiv preprint arXiv:1911.03705},
  year={2019}
}

@article{ni2021sentence,
  title={Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models},
  author={Ni, Jianmo and {\'A}brego, Gustavo Hern{\'a}ndez and Constant, Noah and Ma, Ji and Hall, Keith B and Cer, Daniel and Yang, Yinfei},
  journal={arXiv preprint arXiv:2108.08877},
  year={2021}
}


@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}


@inproceedings{
chen2024going,
title={Going Beyond Neural Network Feature Similarity: The Network Feature Complexity and Its Interpretation Using Category Theory},
author={Yiting Chen and Zhanpeng Zhou and Junchi Yan},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=4bSQ3lsfEV}
}


@misc{xu2023wizardlm,
      title={WizardLM: Empowering Large Language Models to Follow Complex Instructions}, 
      author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
      year={2023},
      eprint={2304.12244},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{luo2023wizardmath,
      title={WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct}, 
      author={Haipeng Luo and Qingfeng Sun and Can Xu and Pu Zhao and Jianguang Lou and Chongyang Tao and Xiubo Geng and Qingwei Lin and Shifeng Chen and Dongmei Zhang},
      year={2023},
      eprint={2308.09583},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{codealpaca,
  author = {Sahil Chaudhary},
  title = {Code Alpaca: An Instruction-following LLaMA model for code generation},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/sahil280114/codealpaca}},
}

@misc{alpaca_eval,
  author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}}
}

@misc{cobbe2021training,
      title={Training Verifiers to Solve Math Word Problems}, 
      author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Austin2021ProgramSW,
  title={Program Synthesis with Large Language Models},
  author={Jacob Austin and Augustus Odena and Maxwell Nye and Maarten Bosma and Henryk Michalewski and David Dohan and Ellen Jiang and Carrie J. Cai and Michael Terry and Quoc V. Le and Charles Sutton},
  journal={ArXiv},
  year={2021},
  volume={abs/2108.07732},
  url={https://api.semanticscholar.org/CorpusID:237142385}
}

@INPROCEEDINGS{imagenet,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  keywords={Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  doi={10.1109/CVPR.2009.5206848}
}

@software{torchvision2016,
    title        = {TorchVision: PyTorch's Computer Vision library},
    author       = {TorchVision contributors},
    year         = 2016,
    journal      = {GitHub repository},
    publisher    = {GitHub},
    howpublished = {\url{https://github.com/pytorch/vision}}
}