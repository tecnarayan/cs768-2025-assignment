\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal et~al.(2020)Agrawal, Papamarkou, and Hinkle]{agrawal2020wide}
Agrawal, D., Papamarkou, T., and Hinkle, J.
\newblock Wide neural networks with bottlenecks are deep {G}aussian processes.
\newblock \emph{JMLR}, 2020.

\bibitem[Aitchison(2020)]{aitchison2020bigger}
Aitchison, L.
\newblock Why bigger is not always better: on finite and infinite neural
  networks.
\newblock In \emph{ICML}, 2020.

\bibitem[Aitchison et~al.(2021)Aitchison, Yang, and Ober]{aitchison2021deep}
Aitchison, L., Yang, A., and Ober, S.~W.
\newblock Deep kernel processes.
\newblock In \emph{ICML}, 2021.

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{ICML}, 2019.

\bibitem[Bernstein \& Yue(2021)Bernstein and Yue]{bernstein2021computing}
Bernstein, J. and Yue, Y.
\newblock Computing the information content of trained neural networks.
\newblock \emph{arXiv}, 2021.

\bibitem[Billingsley(1999)]{billingsley1999convergence}
Billingsley, P.
\newblock \emph{Convergence of probability measures}.
\newblock Wiley, second edition, 1999.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., Necula, G., Paszke, A., Vander{P}las, J., Wanderman-{M}ilne, S., and
  Zhang, Q.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Brooks et~al.(2011)Brooks, Gelman, Jones, and
  Meng]{brooks2011handbook}
Brooks, S., Gelman, A., Jones, G., and Meng, X.-L.
\newblock \emph{Handbook of {M}arkov chain {M}onte {C}arlo}.
\newblock CRC press, 2011.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2019lazy}
Chizat, L., Oyallon, E., and Bach, F.
\newblock On lazy training in differentiable programming.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Cholesky(1924)]{cholesky}
Cholesky, A.-L.
\newblock Note sur une m{\'e}thode de r{\'e}solution des {\'e}quations normales
  provenant de l'application de la m{\'e}thode des moindres carr{\'e}s a un
  syst{\`e}me d'{\'e}quations lin{\'e}aires en nombre inf{\'e}rieur a celui des
  inconnues. ---application de la m{\'e}thode a la r{\'e}solution d'un
  syst{\`e}me defini d'{\'e}quations lin{\'e}aires.
\newblock \emph{Bulletin g{\'e}od{\'e}sique}, 1924.

\bibitem[Damianou \& Lawrence(2013)Damianou and Lawrence]{damianou2013deep}
Damianou, A. and Lawrence, N.~D.
\newblock Deep {G}aussian processes.
\newblock In \emph{AISTATS}, 2013.

\bibitem[Daxberger et~al.(2021)Daxberger, Nalisnick, Allingham, Antoran, and
  Hernandez-Lobato]{daxberger21a}
Daxberger, E., Nalisnick, E., Allingham, J.~U., Antoran, J., and
  Hernandez-Lobato, J.~M.
\newblock Bayesian deep learning via subnetwork inference.
\newblock In \emph{ICML}, 2021.

\bibitem[Dillon et~al.(2017)Dillon, Langmore, Tran, Brevdo, Vasudevan, Moore,
  Patton, Alemi, Hoffman, and Saurous]{dillon2017tensorflow}
Dillon, J.~V., Langmore, I., Tran, D., Brevdo, E., Vasudevan, S., Moore, D.,
  Patton, B., Alemi, A., Hoffman, M., and Saurous, R.~A.
\newblock Tensorflow distributions.
\newblock \emph{arXiv}, 2017.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{ICML}, 2019.

\bibitem[Dusenberry et~al.(2020)Dusenberry, Jerfel, Wen, Ma, Snoek, Heller,
  Lakshminarayanan, and Tran]{dusenberry20a}
Dusenberry, M., Jerfel, G., Wen, Y., Ma, Y., Snoek, J., Heller, K.,
  Lakshminarayanan, B., and Tran, D.
\newblock Efficient and scalable {B}ayesian neural nets with rank-1 factors.
\newblock In \emph{ICML}, 2020.

\bibitem[Dziugaite \& Roy(2017)Dziugaite and Roy]{dziugaite2017computing}
Dziugaite, G.~K. and Roy, D.~M.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{UAI}, 2017.

\bibitem[El~Moselhy \& Marzouk(2012)El~Moselhy and Marzouk]{el2012bayesian}
El~Moselhy, T.~A. and Marzouk, Y.~M.
\newblock Bayesian inference with optimal maps.
\newblock \emph{Journal of Computational Physics}, 2012.

\bibitem[Garriga-Alonso et~al.(2019)Garriga-Alonso, Rasmussen, and
  Aitchison]{alonso2018deep}
Garriga-Alonso, A., Rasmussen, C.~E., and Aitchison, L.
\newblock Deep convolutional networks as shallow {G}aussian processes.
\newblock In \emph{ICLR}, 2019.

\bibitem[Gelman \& Rubin(1992)Gelman and Rubin]{gelman1992inference}
Gelman, A. and Rubin, D.~B.
\newblock Inference from iterative simulation using multiple sequences.
\newblock \emph{Statistical Science}, 1992.

\bibitem[Gray(2011)]{gray2011entropy}
Gray, R.~M.
\newblock \emph{Entropy and Information Theory}.
\newblock Springer, second edition, 2011.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016gaussian}
Hendrycks, D. and Gimpel, K.
\newblock Gaussian error linear units ({GELU}s).
\newblock \emph{arXiv}, 2016.

\bibitem[Hoffman et~al.(2018)Hoffman, Sountsov, Dillon, Langmore, Tran, and
  Vasudevan]{hoffman2019neutra}
Hoffman, M., Sountsov, P., Dillon, J.~V., Langmore, I., Tran, D., and
  Vasudevan, S.
\newblock Neu{T}ra-lizing bad geometry in {H}amiltonian {M}onte {C}arlo using
  neural transport.
\newblock \emph{AABI}, 2018.

\bibitem[Hron et~al.(2020{\natexlab{a}})Hron, Bahri, Novak, Pennington, and
  Sohl-Dickstein]{hron2020exact}
Hron, J., Bahri, Y., Novak, R., Pennington, J., and Sohl-Dickstein, J.
\newblock Exact posterior distributions of wide {B}ayesian neural networks.
\newblock \emph{UDL}, 2020{\natexlab{a}}.

\bibitem[Hron et~al.(2020{\natexlab{b}})Hron, Bahri, Sohl-Dickstein, and
  Novak]{hron2020infinite}
Hron, J., Bahri, Y., Sohl-Dickstein, J., and Novak, R.
\newblock Infinite attention: {NNGP} and {NTK} for deep attention networks.
\newblock In \emph{ICML}, 2020{\natexlab{b}}.

\bibitem[Izmailov et~al.(2021)Izmailov, Vikram, Hoffman, and
  Wilson]{izmailov2021bayesian}
Izmailov, P., Vikram, S., Hoffman, M.~D., and Wilson, A. G.~G.
\newblock What are {B}ayesian neural network posteriors really like?
\newblock In \emph{ICML}, 2021.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Khan et~al.(2018)Khan, Nielsen, Tangkaratt, Lin, Gal, and
  Srivastava]{khan18a}
Khan, M., Nielsen, D., Tangkaratt, V., Lin, W., Gal, Y., and Srivastava, A.
\newblock Fast and scalable {B}ayesian deep learning by weight-perturbation in
  {A}dam.
\newblock In \emph{ICML}, 2018.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images, 2009.

\bibitem[Lee et~al.(2018)Lee, Sohl-dickstein, Pennington, Novak, Schoenholz,
  and Bahri]{lee2018deep}
Lee, J., Sohl-dickstein, J., Pennington, J., Novak, R., Schoenholz, S., and
  Bahri, Y.
\newblock Deep neural networks as {G}aussian processes.
\newblock In \emph{ICLR}, 2018.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019wide}
Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J.,
  and Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Levenberg(1944)]{levenberg1944method}
Levenberg, K.
\newblock A method for the solution of certain non-linear problems in least
  squares.
\newblock \emph{Quarterly of applied mathematics}, 1944.

\bibitem[Maddox et~al.(2019)Maddox, Izmailov, Garipov, Vetrov, and
  Wilson]{maddox2019}
Maddox, W.~J., Izmailov, P., Garipov, T., Vetrov, D.~P., and Wilson, A.~G.
\newblock A simple baseline for bayesian uncertainty in deep learning.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Marquardt(1963)]{marquardt1963algorithm}
Marquardt, D.~W.
\newblock An algorithm for least-squares estimation of nonlinear parameters.
\newblock \emph{Journal of the society for Industrial and Applied Mathematics},
  1963.

\bibitem[Marzouk \& Parno(2014)Marzouk and Parno]{marzouk2014transport}
Marzouk, Y. and Parno, M.
\newblock Transport map-accelerated {M}arkov chain {M}onte {C}arlo for
  {B}ayesian parameter inference.
\newblock In \emph{AGU Fall Meeting Abstracts}, 2014.

\bibitem[Marzouk et~al.(2016)Marzouk, Moselhy, Parno, and
  Spantini]{marzouk2016introduction}
Marzouk, Y., Moselhy, T., Parno, M., and Spantini, A.
\newblock An introduction to sampling via measure transport.
\newblock \emph{arXiv}, 2016.

\bibitem[Matthews et~al.(2016)Matthews, Hensman, Turner, and
  Ghahramani]{matthews2016kl}
Matthews, A., Hensman, J., Turner, R., and Ghahramani, Z.
\newblock On sparse variational methods and the {K}ullback-{L}eibler divergence
  between stochastic processes.
\newblock In \emph{AISTATS}, 2016.

\bibitem[Matthews et~al.(2017)Matthews, Hron, Turner, and
  Ghahramani]{matthews2017sample}
Matthews, A., Hron, J., Turner, R., and Ghahramani, Z.
\newblock Sample-then-optimize posterior sampling for {B}ayesian linear models.
\newblock In \emph{AABI}, 2017.

\bibitem[Matthews et~al.(2018)Matthews, Hron, Rowland, Turner, and
  Ghahramani]{matthews2018gaussian}
Matthews, A., Hron, J., Rowland, M., Turner, R.~E., and Ghahramani, Z.
\newblock {G}aussian process behaviour in wide deep neural networks.
\newblock In \emph{ICLR}, 2018.

\bibitem[Neal(1993)]{Neal93probabilisticinference}
Neal, R.~M.
\newblock Probabilistic inference using {M}arkov chain {M}onte {C}arlo methods.
\newblock Technical report, University of Toronto, 1993.

\bibitem[Novak et~al.(2019)Novak, Xiao, Bahri, Lee, Yang, Hron, Abolafia,
  Pennington, and Sohl-dickstein]{novak2019bayesian}
Novak, R., Xiao, L., Bahri, Y., Lee, J., Yang, G., Hron, J., Abolafia, D.~A.,
  Pennington, J., and Sohl-dickstein, J.
\newblock Bayesian deep convolutional networks with many channels are
  {G}aussian processes.
\newblock In \emph{ICLR}, 2019.

\bibitem[Novak et~al.(2020)Novak, Xiao, Hron, Lee, Alemi, Sohl-Dickstein, and
  Schoenholz]{neuraltangents2020}
Novak, R., Xiao, L., Hron, J., Lee, J., Alemi, A.~A., Sohl-Dickstein, J., and
  Schoenholz, S.~S.
\newblock Neural {T}angents: Fast and easy infinite neural networks in
  {P}ython.
\newblock In \emph{ICLR}, 2020.
\newblock URL \url{https://github.com/google/neural-tangents}.

\bibitem[Novak et~al.(2022)Novak, Sohl-Dickstein, and
  Schoenholz]{novak2021fast}
Novak, R., Sohl-Dickstein, J., and Schoenholz, S.~S.
\newblock Fast finite width neural tangent kernel.
\newblock In \emph{ICML}, 2022.

\bibitem[Ober \& Aitchison(2021)Ober and Aitchison]{ober2021global}
Ober, S.~W. and Aitchison, L.
\newblock Global inducing point variational posteriors for {B}ayesian neural
  networks and deep {G}aussian processes.
\newblock In \emph{ICML}, 2021.

\bibitem[Osawa et~al.(2019)Osawa, Swaroop, Khan, Jain, Eschenhagen, Turner, and
  Yokota]{osawa2019}
Osawa, K., Swaroop, S., Khan, M. E.~E., Jain, A., Eschenhagen, R., Turner,
  R.~E., and Yokota, R.
\newblock Practical deep learning with {B}ayesian principles.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Parno(2015)]{parno2015transport}
Parno, M.~D.
\newblock \emph{Transport maps for accelerated Bayesian computation}.
\newblock PhD thesis, MIT, 2015.

\bibitem[Rasmussen \& Williams(2005)Rasmussen and Williams]{rasmussen2005gps}
Rasmussen, C.~E. and Williams, C. K.~I.
\newblock \emph{Gaussian Processes for Machine Learning}.
\newblock MIT Press, 2005.

\bibitem[Ripley(1989)]{ripley1989}
Ripley, B.
\newblock Stochastic simulation.
\newblock \emph{Statistical Papers}, 1989.

\bibitem[Roberts et~al.(2021)Roberts, Yaida, and Hanin]{roberts2021principles}
Roberts, D.~A., Yaida, S., and Hanin, B.
\newblock The principles of deep learning theory.
\newblock \emph{arXiv}, 2021.

\bibitem[Rossky et~al.(1978)Rossky, Doll, and Friedman]{rossky1978}
Rossky, P.~J., Doll, J.~D., and Friedman, H.~L.
\newblock Brownian dynamics as smart monte carlo simulation.
\newblock \emph{The Journal of Chemical Physics}, 1978.

\bibitem[Shao et~al.(2020)Shao, Hu, Wang, Xue, and Raj]{shao2020normalisation}
Shao, J., Hu, K., Wang, C., Xue, X., and Raj, B.
\newblock Is normalization indispensable for training deep neural network?
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Shwartz-Ziv \& Alemi(2020)Shwartz-Ziv and
  Alemi]{shwartz2020information}
Shwartz-Ziv, R. and Alemi, A.~A.
\newblock Information in infinite ensembles of infinitely-wide neural networks.
\newblock In \emph{AABI}, 2020.

\bibitem[Tierney \& Kadane(1986)Tierney and Kadane]{tierney1986accurate}
Tierney, L. and Kadane, J.~B.
\newblock Accurate approximations for posterior moments and marginal densities.
\newblock \emph{Journal of the {A}merican statistical association}, 1986.

\bibitem[Titsias(2017)]{titsias2017learning}
Titsias, M.~K.
\newblock Learning model reparametrizations: Implicit variational inference by
  fitting {MCMC} distributions.
\newblock \emph{arXiv}, 2017.

\bibitem[Vakili et~al.(2021)Vakili, Bromberg, Garcia, Shiu, and
  Bernacchia]{vakili2021uniform}
Vakili, S., Bromberg, M., Garcia, J., Shiu, D.-s., and Bernacchia, A.
\newblock Uniform generalization bounds for overparameterized neural networks.
\newblock \emph{arXiv}, 2021.

\bibitem[Valle-Perez et~al.(2018)Valle-Perez, Camargo, and
  Louis]{valle2018deep}
Valle-Perez, G., Camargo, C.~Q., and Louis, A.~A.
\newblock Deep learning generalizes because the parameter-function map is
  biased towards simple functions.
\newblock In \emph{ICLR}, 2018.

\bibitem[Voevodin \& Kuznetsov(1984)Voevodin and Kuznetsov]{voevodov}
Voevodin, V. and Kuznetsov, Y.
\newblock Matrices and computations.
\newblock \emph{Nauka}, 1984.

\bibitem[Wainwright(2019)]{wainwright2019high}
Wainwright, M.~J.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint}.
\newblock Cambridge University Press, 2019.

\bibitem[Yaida(2020)]{yaida2020non}
Yaida, S.
\newblock Non-{G}aussian processes and neural networks at finite widths.
\newblock In \emph{Mathematical and Scientific Machine Learning}, 2020.

\bibitem[Yang(2019)]{yang2019scaling}
Yang, G.
\newblock Scaling limits of wide neural networks with weight sharing:
  {G}aussian process behavior, gradient independence, and neural tangent kernel
  derivation.
\newblock \emph{arXiv}, 2019.

\bibitem[Yang(2020)]{yang2020tensor}
Yang, G.
\newblock Tensor programs {III}: Neural matrix laws.
\newblock \emph{arXiv}, 2020.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wideresnet}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock In \emph{BMVC}, 2016.

\bibitem[Zavatone-Veth \& Pehlevan(2021)Zavatone-Veth and
  Pehlevan]{zavatone2021exact}
Zavatone-Veth, J. and Pehlevan, C.
\newblock Exact marginal prior distributions of finite {B}ayesian neural
  networks.
\newblock \emph{NeurIPS}, 2021.

\end{thebibliography}
