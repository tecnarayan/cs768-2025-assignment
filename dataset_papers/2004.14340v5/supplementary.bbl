\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock \emph{nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Schmidhuber(2015)]{schmidhuber2015deep}
J{\"u}rgen Schmidhuber.
\newblock Deep learning in neural networks: An overview.
\newblock \emph{Neural networks}, 61:\penalty0 85--117, 2015.

\bibitem[Ren et~al.(2015)Ren, He, Girshick, and Sun]{NIPS2015_5638}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock In C.~Cortes, N.~D. Lawrence, D.~D. Lee, M.~Sugiyama, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems 28}, pages
  91--99. Curran Associates, Inc., 2015.
\newblock URL
  \url{http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{He_2016}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, Jun 2016.
\newblock \doi{10.1109/cvpr.2016.90}.
\newblock URL \url{http://dx.doi.org/10.1109/CVPR.2016.90}.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding, 2018.

\bibitem[Radford(2018)]{Radford2018ImprovingLU}
Alec Radford.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Rajbhandari et~al.(2019)Rajbhandari, Rasley, Ruwase, and
  He]{rajbhandari2019zero}
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.
\newblock Zero: Memory optimization towards training a trillion parameter
  models.
\newblock ArXiv, October 2019.
\newblock URL
  \url{https://www.microsoft.com/en-us/research/publication/zero-memory-optimization-towards-training-a-trillion-parameter-models/}.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{NIPS1989_250}
Yann LeCun, John~S. Denker, and Sara~A. Solla.
\newblock Optimal brain damage.
\newblock In D.~S. Touretzky, editor, \emph{Advances in Neural Information
  Processing Systems 2}, pages 598--605. Morgan-Kaufmann, 1990.
\newblock URL \url{http://papers.nips.cc/paper/250-optimal-brain-damage.pdf}.

\bibitem[Mozer and Smolensky(1989)]{mozer1989skeletonization}
Michael~C Mozer and Paul Smolensky.
\newblock Skeletonization: A technique for trimming the fat from a network via
  relevance assessment.
\newblock In \emph{Advances in neural information processing systems}, pages
  107--115, 1989.

\bibitem[Hassibi and Stork(1993)]{NIPS1992_647}
Babak Hassibi and David~G. Stork.
\newblock Second order derivatives for network pruning: Optimal brain surgeon.
\newblock In S.~J. Hanson, J.~D. Cowan, and C.~L. Giles, editors,
  \emph{Advances in Neural Information Processing Systems 5}, pages 164--171.
  Morgan-Kaufmann, 1993.
\newblock URL
  \url{http://papers.nips.cc/paper/647-second-order-derivatives-for-network-pruning-optimal-brain-surgeon.pdf}.

\bibitem[Blalock et~al.(2020)Blalock, Ortiz, Frankle, and
  Guttag]{blalock2020state}
Davis Blalock, Jose Javier~Gonzalez Ortiz, Jonathan Frankle, and John Guttag.
\newblock What is the state of neural network pruning?
\newblock \emph{arXiv preprint arXiv:2003.03033}, 2020.

\bibitem[Nesterov and Polyak(2006)]{nesterov2006cubic}
Yurii Nesterov and Boris~T Polyak.
\newblock Cubic regularization of newton method and its global performance.
\newblock \emph{Mathematical Programming}, 108\penalty0 (1):\penalty0 177--205,
  2006.

\bibitem[Duchi et~al.(2010)Duchi, Hazan, and Singer]{Duchi2010AdaptiveSM}
John~C. Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{J. Mach. Learn. Res.}, 12:\penalty0 2121--2159, 2010.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization, 2014.

\bibitem[Martens and Grosse(2015)]{martens2015optimizing}
James Martens and Roger Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature, 2015.

\bibitem[Koh and Liang(2017)]{koh2017understanding}
Pang~Wei Koh and Percy Liang.
\newblock Understanding black-box predictions via influence functions, 2017.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska, Hassabis, Clopath,
  Kumaran, and Hadsell]{Kirkpatrick3521}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and
  Raia Hadsell.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.
\newblock ISSN 0027-8424.
\newblock \doi{10.1073/pnas.1611835114}.
\newblock URL \url{https://www.pnas.org/content/114/13/3521}.

\bibitem[Gale et~al.(2019)Gale, Elsen, and Hooker]{gale2019state}
Trevor Gale, Erich Elsen, and Sara Hooker.
\newblock The state of sparsity in deep neural networks, 2019.

\bibitem[Zhu and Gupta(2017)]{zhu2017prune}
Michael Zhu and Suyog Gupta.
\newblock To prune, or not to prune: exploring the efficacy of pruning for
  model compression, 2017.

\bibitem[Theis et~al.(2018)Theis, Korshunova, Tejani, and
  Huszár]{theis2018faster}
Lucas Theis, Iryna Korshunova, Alykhan Tejani, and Ferenc Huszár.
\newblock Faster gaze prediction with dense networks and fisher pruning, 2018.

\bibitem[Wang et~al.(2019)Wang, Grosse, Fidler, and Zhang]{wang2019eigendamage}
Chaoqi Wang, Roger Grosse, Sanja Fidler, and Guodong Zhang.
\newblock Eigendamage: Structured pruning in the kronecker-factored eigenbasis,
  2019.

\bibitem[Zeng and Urtasun(2019)]{zeng2019mlprune}
Wenyuan Zeng and Raquel Urtasun.
\newblock {MLP}rune: Multi-layer pruning for automated neural network
  compression, 2019.
\newblock URL \url{https://openreview.net/forum?id=r1g5b2RcKm}.

\bibitem[Dettmers and Zettlemoyer(2019)]{dettmers2019sparse}
Tim Dettmers and Luke Zettlemoyer.
\newblock Sparse networks from scratch: Faster training without losing
  performance, 2019.

\bibitem[Lin et~al.(2020)Lin, Stich, Barba, Dmitriev, and
  Jaggi]{Lin2020Dynamic}
Tao Lin, Sebastian~U. Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi.
\newblock Dynamic model pruning with feedback.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=SJem8lSFwB}.

\bibitem[Woodbury(1950)]{woodbury}
Max~A. Woodbury.
\newblock \emph{Inverting modified matrices}.
\newblock {SRG} {Memorandum} report ; 42. Princeton, NJ: Department of
  Statistics, Princeton University, 1950.

\bibitem[Amari et~al.(2000)Amari, Park, and
  Fukumizu]{doi:10.1162/089976600300015420}
Shun-ichi Amari, Hyeyoung Park, and Kenji Fukumizu.
\newblock Adaptive method of realizing natural gradient learning for multilayer
  perceptrons.
\newblock \emph{Neural Computation}, 12\penalty0 (6):\penalty0 1399--1409,
  2000.
\newblock \doi{10.1162/089976600300015420}.
\newblock URL \url{https://doi.org/10.1162/089976600300015420}.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{Howard2017MobileNetsEC}
Andrew~G. Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{ArXiv}, abs/1704.04861, 2017.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International Journal of Computer Vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[Kusupati et~al.(2020)Kusupati, Ramanujan, Somani, Wortsman, Jain,
  Kakade, and Farhadi]{Kusupati2020SoftTW}
Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek
  Jain, Sham~M. Kakade, and Ali Farhadi.
\newblock Soft threshold weight reparameterization for learnable sparsity.
\newblock \emph{ArXiv}, abs/2002.03231, 2020.

\bibitem[Inc.(2020)]{NM}
Neural~Magic Inc.
\newblock Early access signup for the sparse inference engine, 2020.
\newblock URL \url{https://neuralmagic.com/earlyaccess/}.

\bibitem[Martens(2014)]{martens2014new}
James Martens.
\newblock New insights and perspectives on the natural gradient method, 2014.

\bibitem[Kunstner et~al.(2019)Kunstner, Balles, and
  Hennig]{kunstner2019limitations}
Frederik Kunstner, Lukas Balles, and Philipp Hennig.
\newblock Limitations of the empirical fisher approximation for natural
  gradient descent, 2019.

\bibitem[Singh(2020)]{Singh:277227}
Sidak~Pal Singh.
\newblock Efficient second-order methods for model compression.
\newblock \emph{EPFL Master Thesis}, 2020.
\newblock URL \url{http://infoscience.epfl.ch/record/277227}.

\bibitem[Amari(1998)]{doi:10.1162/089976698300017746}
Shun-ichi Amari.
\newblock Natural gradient works efficiently in learning.
\newblock \emph{Neural Computation}, 10\penalty0 (2):\penalty0 251--276, 1998.
\newblock \doi{10.1162/089976698300017746}.
\newblock URL \url{https://doi.org/10.1162/089976698300017746}.

\bibitem[Schraudolph(2002)]{Schraudolph02}
Nicol~N. Schraudolph.
\newblock Fast curvature matrix-vector products for second-order gradient
  descent.
\newblock \emph{Neural Computation}, 14\penalty0 (7):\penalty0 1723--1738,
  2002.
\newblock \doi{10.1162/08997660260028683}.
\newblock URL \url{https://doi.org/10.1162/08997660260028683}.

\bibitem[Thomas et~al.(2019)Thomas, Pedregosa, van Merriënboer, Mangazol,
  Bengio, and Roux]{thomas2019interplay}
Valentin Thomas, Fabian Pedregosa, Bart van Merriënboer, Pierre-Antoine
  Mangazol, Yoshua Bengio, and Nicolas~Le Roux.
\newblock On the interplay between noise and curvature and its effect on
  optimization and generalization, 2019.

\bibitem[Le~Roux et~al.(2008)Le~Roux, Manzagol, and
  Bengio]{leroux2008topmoumoute}
Nicolas Le~Roux, Pierre-Antoine Manzagol, and Yoshua Bengio.
\newblock Topmoumoute online natural gradient algorithm.
\newblock In \emph{NIPS2007}, January 2008.
\newblock URL
  \url{https://www.microsoft.com/en-us/research/publication/topmoumoute-online-natural-gradient-algorithm/}.

\bibitem[Dong et~al.(2017)Dong, Chen, and Pan]{dong2017learning}
Xin Dong, Shangyu Chen, and Sinno~Jialin Pan.
\newblock Learning to prune deep neural networks via layer-wise optimal brain
  surgeon, 2017.

\bibitem[Martens(2010)]{martens_free}
James Martens.
\newblock Deep learning via hessian-free optimization.
\newblock In \emph{Proceedings of the 27th International Conference on
  International Conference on Machine Learning}, ICML’10, page 735–742,
  Madison, WI, USA, 2010. Omnipress.
\newblock ISBN 9781605589077.

\bibitem[Krishnan et~al.(2018)Krishnan, Xiao, and Saurous]{krishnan2018neumann}
Shankar Krishnan, Ying Xiao, and Rif.~A. Saurous.
\newblock Neumann optimizer: A practical optimization algorithm for deep neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=rkLyJl-0-}.

\bibitem[Agarwal et~al.(2016)Agarwal, Bullins, and
  Hazan]{agarwal2016secondorder}
Naman Agarwal, Brian Bullins, and Elad Hazan.
\newblock Second-order stochastic optimization for machine learning in linear
  time, 2016.

\bibitem[Heskes(2000)]{Heskes2000OnNL}
Tom Heskes.
\newblock On natural learning and pruning in multilayered perceptrons.
\newblock \emph{Neural Computation}, 12:\penalty0 881--901, 2000.

\bibitem[Ba et~al.(2016)Ba, Grosse, and Martens]{ba2016distributed}
Jimmy Ba, Roger Grosse, and James Martens.
\newblock Distributed second-order optimization using kronecker-factored
  approximations.
\newblock 2016.

\bibitem[Osawa et~al.(2019)Osawa, Tsuji, Ueno, Naruse, Yokota, and
  Matsuoka]{Osawa_2019}
Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi
  Matsuoka.
\newblock Large-scale distributed second-order optimization using
  kronecker-factored approximate curvature for deep convolutional neural
  networks.
\newblock \emph{2019 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, Jun 2019.
\newblock \doi{10.1109/cvpr.2019.01264}.
\newblock URL \url{http://dx.doi.org/10.1109/CVPR.2019.01264}.

\bibitem[Grosse and Martens(2016)]{grosse2016kroneckerfactored}
Roger Grosse and James Martens.
\newblock A kronecker-factored approximate fisher matrix for convolution
  layers, 2016.

\bibitem[Martens et~al.(2018)Martens, Ba, and
  Johnson]{martens2018kroneckerfactored}
James Martens, Jimmy Ba, and Matt Johnson.
\newblock Kronecker-factored curvature approximations for recurrent neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=HyMTkQZAb}.

\bibitem[Laurent et~al.(2018)Laurent, George, Bouthillier, Ballas, and
  Vincent]{laurent2018an}
César Laurent, Thomas George, Xavier Bouthillier, Nicolas Ballas, and Pascal
  Vincent.
\newblock An evaluation of fisher approximations beyond kronecker
  factorization, 2018.
\newblock URL \url{https://openreview.net/forum?id=ryVC6tkwG}.

\bibitem[Han et~al.(2015)Han, Mao, and Dally]{han2015deep}
Song Han, Huizi Mao, and William~J. Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding, 2015.

\bibitem[Evci et~al.(2019)Evci, Gale, Menick, Castro, and
  Elsen]{Evci2019RiggingTL}
Utku Evci, Trevor Gale, Jacob Menick, Pablo~Samuel Castro, and Erich Elsen.
\newblock Rigging the lottery: Making all tickets winners.
\newblock \emph{ArXiv}, abs/1911.11134, 2019.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and
  Vetrov]{molchanov2017variational}
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov.
\newblock Variational dropout sparsifies deep neural networks, 2017.

\bibitem[Louizos et~al.(2017)Louizos, Welling, and Kingma]{louizos2017learning}
Christos Louizos, Max Welling, and Diederik~P. Kingma.
\newblock Learning sparse neural networks through $l_0$ regularization, 2017.

\bibitem[Frankle and Carbin(2018)]{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks, 2018.

\bibitem[Mostafa and Wang(2019)]{mostafa2019parameter}
Hesham Mostafa and Xin Wang.
\newblock Parameter efficient training of deep convolutional neural networks by
  dynamic sparse reparameterization, 2019.

\bibitem[Wortsman et~al.(2019)Wortsman, Farhadi, and
  Rastegari]{wortsman2019discovering}
Mitchell Wortsman, Ali Farhadi, and Mohammad Rastegari.
\newblock Discovering neural wirings, 2019.

\bibitem[Corporation(2002)]{ONNX}
Microsoft Corporation.
\newblock The onnx runtime, 2002.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need, 2017.

\bibitem[Wu et~al.(2017)Wu, Mansimov, Grosse, Liao, and Ba]{WuMGLB17}
Yuhuai Wu, Elman Mansimov, Roger~B. Grosse, Shun Liao, and Jimmy Ba.
\newblock Second-order optimization for deep reinforcement learning using
  kronecker-factored approximation.
\newblock In \emph{NIPS}, pages 5285--5294, 2017.
\newblock URL
  \url{http://papers.nips.cc/paper/7112-second-order-optimization-for-deep-reinforcement-learning-using-kronecker-factored-approximation}.

\bibitem[Pearlmutter(1994)]{10.1162/neco.1994.6.1.147}
Barak~A. Pearlmutter.
\newblock Fast exact multiplication by the hessian.
\newblock \emph{Neural Comput.}, 6\penalty0 (1):\penalty0 147–160, January
  1994.
\newblock ISSN 0899-7667.
\newblock \doi{10.1162/neco.1994.6.1.147}.
\newblock URL \url{https://doi.org/10.1162/neco.1994.6.1.147}.

\end{thebibliography}
