@book{lattimore2018bandits,
  author={Lattimore, Tor and Szep\'{e}svari, Csaba},
  title={Bandit Algorithms},
  publisher={Cambridge University Press.},
  year={2018}
}

@book{slivkins2019bandits,
  author={Slivkins, Aleksandrs},
  title={Introduction to Multi-Armed Bandits},
  publisher={Foundations and Trends in Machine Learning},
  year={2019}
}

@article{mansour2015bic, 
  author = {Mansour, Yishay and Slivkins, Aleksandrs and Syrgkanis, Vasilis},
  title = {Bayesian incentive-compatible bandit exploration}, 
  journal ={In 15th ACM Conf. on Economics and Computation (ACM EC)},
  year = 2015
} 

@inproceedings{zinkevich2003online,
   author={Zinkevich, Martin},
   title={Online Convex Programming and Generalized Infinitesimal Gradient Ascent},
   booktitle={Proceedings of the Twentieth International Conference on Machine Learning (ICML-2003)},
   year = {2003}
}

@article{active-arms-elimination-2006,
    author  =   {Even-Dar, Eyal and Mannor, Shie and Mansour, Yishay},
    title   =   {Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems},
    journal =    {Journal of Machine Learning Research (JMLR)}, 
    volume  =   {7},
    pages   =   {1079-1105},
    year    =   {2006}
}



@inproceedings{KremerMP13,
  author    = {Ilan Kremer and
               Yishay Mansour and
               Motty Perry},
  editor    = {Michael J. Kearns and
               R. Preston McAfee and
               {\'{E}}va Tardos},
  title     = {Implementing the "Wisdom of the Crowd"},
  booktitle = {Proceedings of the fourteenth {ACM} Conference on Electronic Commerce,
               {EC} 2013, Philadelphia, PA, USA, June 16-20, 2013},
  pages     = {605--606},
  publisher = {{ACM}},
  year      = {2013},
  url       = {https://doi.org/10.1145/2492002.2482542},
  doi       = {10.1145/2492002.2482542},
  timestamp = {Tue, 27 Nov 2018 11:56:48 +0100},
  biburl    = {https://dblp.org/rec/conf/sigecom/KremerMP13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{MansourSSW16,
  author    = {Yishay Mansour and
               Aleksandrs Slivkins and
               Vasilis Syrgkanis and
               Zhiwei Steven Wu},
  editor    = {Vincent Conitzer and
               Dirk Bergemann and
               Yiling Chen},
  title     = {Bayesian Exploration: Incentivizing Exploration in Bayesian Games},
  booktitle = {Proceedings of the 2016 {ACM} Conference on Economics and Computation,
               {EC} '16, Maastricht, The Netherlands, July 24-28, 2016},
  pages     = {661},
  publisher = {{ACM}},
  year      = {2016},
  url       = {https://doi.org/10.1145/2940716.2940755},
  doi       = {10.1145/2940716.2940755},
  timestamp = {Tue, 06 Nov 2018 16:57:13 +0100},
  biburl    = {https://dblp.org/rec/conf/sigecom/MansourSSW16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{ImmorlicaMSW19,
author = {Immorlica, Nicole and Mao, Jieming and Slivkins, Aleksandrs and Wu, Zhiwei Steven},
title = {Bayesian Exploration with Heterogeneous Agents},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313649},
doi = {10.1145/3308558.3313649},
booktitle = {The World Wide Web Conference},
pages = {751–761},
numpages = {11},
keywords = {bayesian exploration, incentivizing exploration, heterogeneous agents},
location = {San Francisco, CA, USA},
series = {WWW ’19}
}
  



@article{SS20,
  author    = {Mark Sellke and
               Aleksandrs Slivkins},
  title     = {Sample Complexity of Incentivized Exploration},
  journal   = {CoRR},
  volume    = {abs/2002.00558},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.00558},
  archivePrefix = {arXiv},
  eprint    = {2002.00558},
  timestamp = {Mon, 10 Feb 2020 15:12:57 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-00558.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@article{bloom,
 ISSN = {0022166X},
 URL = {http://www.jstor.org/stable/146183},
 abstract = {This paper examines the benefits and costs of Job Training Partnership Act (JTPA) Title II-A programs for economically disadvantaged adults and out-of-school youths. It is based on a 21,000-person randomized experiment conducted within ongoing Title II-A programs at 16 local JTPA Service Delivery Areas (SDAs) from around the country. In the paper, we present the background and design of our study, describe the methodology used to estimate program impacts, present estimates of program impacts on earnings and educational attainment, and assess the overall success of the programs studied through a benefit-cost analysis.},
 author = {Howard S. Bloom and Larry L. Orr and Stephen H. Bell and George Cave and Fred Doolittle and Winston Lin and Johannes M. Bos},
 journal = {The Journal of Human Resources},
 number = {3},
 pages = {549--576},
 publisher = {[University of Wisconsin Press, Board of Regents of the University of Wisconsin System]},
 title = {The Benefits and Costs of JTPA Title II-A Programs: Key Findings from the National Job Training Partnership Act Study},
 volume = {32},
 year = {1997}
}




@article {matilda,
	Title = {Non-compliance--or how many aunts has Matilda?},
	Author = {Wright, EC},
	DOI = {10.1016/0140-6736(93)91951-h},
	Number = {8876},
	Volume = {342},
	Month = {October},
	Year = {1993},
	Journal = {Lancet (London, England)},
	ISSN = {0140-6736},
	Pages = {909—913},
	URL = {https://doi.org/10.1016/0140-6736(93)91951-h},
}


@InProceedings{schmit18a,
  title = 	 {Human Interaction with Recommendation Systems},
  author = 	 {Sven Schmit and Carlos Riquelme},
  booktitle = 	 {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = 	 {862--870},
  year = 	 {2018},
  editor = 	 {Amos Storkey and Fernando Perez-Cruz},
  volume = 	 {84},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Playa Blanca, Lanzarote, Canary Islands},
  month = 	 {09--11 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v84/schmit18a/schmit18a.pdf},
  url = 	 {http://proceedings.mlr.press/v84/schmit18a.html},
  abstract = 	 {Many recommendation algorithms rely on user data to generate recommendations. However, these recommendations also affect the data obtained from future users. This work aims to understand the effects of this dynamic interaction. We propose a simple model where users with heterogeneous preferences arrive over time. Based on this model, we prove that naive estimators, i.e. those which ignore this feedback loop, are not consistent. We show that consistent estimators are efficient in the presence of myopic agents. Our results are validated using extensive simulations. }
}

@misc{besson2018doubling,
    title={What Doubling Tricks Can and Can't Do for Multi-Armed Bandits},
    author={Lilian Besson and Emilie Kaufmann},
    year={2018},
    eprint={1803.06971},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{angrist1995tsls,
author = { Joshua D.   Angrist  and  Guido W.   Imbens },
title = {Two-Stage Least Squares Estimation of Average Causal Effects in Models with Variable Treatment Intensity},
journal = {Journal of the American Statistical Association},
volume = {90},
number = {430},
pages = {431-442},
year  = {1995},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.1995.10476535},

URL = { 
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476535
    
},
eprint = { 
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1995.10476535
    
}}

@article {angrist1996iv,
    author = {Imbens, Guido and Angrist, Joshua and Rubin, Donald},
	title = {Identification of Causal effects Using Instrumental Variables},
	journal = {Journal of Econometrics},
	volume = {71},
	number = {1-2},
	year = {1996},
	pages = {145-160}
}
	
@article {angrist2001iv,
    author = {Angrist, Joshua and Krueger, Alan},
	title = {Instrumental Variables and the Search for Identification: From Supply and Demand to Natural Experiments},
	journal = {Journal of Economic Perspectives},
	volume = {15},
	number = {4},
	year = {2001},
	pages = {69-85},
	author = {Guido Imbens and Joshua Angrist and Donald Rubin}
}
	
@article{Kallus2018InstrumentArmedB,
  title={Instrument-Armed Bandits},
  author={Nathan Kallus},
  journal={ArXiv},
  year={2018},
  volume={abs/1705.07377}
}

@article{auer2002mab,
  author={Auer, P. and Cesa-Bianchi, N. and Fischer, P.},
  title={Finite-time Analysis of the Multiarmed Bandit Problem},
  journal={Machine Learning},
  year={2002},
  volume={47},
  pages = {235–256},
  doi = {10.1023/A:1013689704352}
}

@article{katehakis1987mab,
  title={The Multi-Armed Bandit Problem: Decomposition and Computation},
  author={M. Katehakis and A. F. Veinott},
  journal={Math. Oper. Res.},
  year={1987},
  volume={12},
  pages={262-268}
}

@inproceedings{bareinboim2015causal,
  title={Bandits with unobserved confounders: A causal
approach},
  author={E. Bareinboim and A. Forney and J. Pearl},
  booktitle = {Advances in Neural Information Processing Systems,},
  year={2015},
  pages={1342–1350}
}

@book{angrist_mostly_2008,
  added-at = {2014-11-20T12:13:56.000+0100},
  author = {Angrist, Joshua D. and Pischke, Jörn-Steffen},
  biburl = {https://www.bibsonomy.org/bibtex/2bad3c0e83619dbdc442142ce714e5529/rlipp},
  interhash = {6d41194af2b0685c2adb94316d607d56},
  intrahash = {bad3c0e83619dbdc442142ce714e5529},
  isbn = {0691120358},
  keywords = {imported},
  month = dec,
  publisher = {Princeton University Press},
  shorttitle = {Mostly Harmless Econometrics},
  timestamp = {2014-11-20T12:13:56.000+0100},
  title = {Mostly Harmless Econometrics: An Empiricist's Companion},
  year = 2008
}


@online{covid,
    author = {Jackson, Chris and Mallory Newall and Jinhee Yi},
  title = {Americans prioritize frontline workers for coronavirus vaccine},
  year = 2020,
  url = {https://www.ipsos.com/en-us/news-polls/abc-news-coronavirus-poll}
}



@online{covid2,
author = {Liz Hamel and Ashley Kirzinger and Lunna Lopes and Grace Sparks and Audrey Kearney and Mellisha Stokes and  Mollyann Brodie},
  title = {KFF COVID-19 Vaccine Monitor: May 2021},
  year = 2021,
  url = {https://www.kff.org/coronavirus-covid-19/poll-finding/kff-covid-19-vaccine-monitor-may-2021/}
}


@article{Slivkins19,
  author    = {Aleksandrs Slivkins},
  title     = {Introduction to Multi-Armed Bandits},
  journal   = {Found. Trends Mach. Learn.},
  volume    = {12},
  number    = {1-2},
  pages     = {1--286},
  year      = {2019},
  url       = {https://doi.org/10.1561/2200000068},
  doi       = {10.1561/2200000068},
  timestamp = {Thu, 18 Jun 2020 22:08:17 +0200},
  biburl    = {https://dblp.org/rec/journals/ftml/Slivkins19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@techreport{angrist06,
 title = "Instrumental Variables Methods in Experimental Criminological Research: What, Why, and How?",
 author = "Angrist, Joshua",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Technical Working Paper Series",
 number = "314",
 year = "2005",
 month = "September",
 doi = {10.3386/t0314},
 URL = "http://www.nber.org/papers/t0314",
 abstract = {Quantitative criminology focuses on straightforward causal questions that are ideally addressed with randomized experiments. In practice, however, traditional randomized trials are difficult to implement in the untidy world of criminal justice. Even when randomized trials are implemented, not everyone is treated as intended and some control subjects may obtain experimental services. Treatments may also be more complicated than a simple yes/no coding can capture. This paper argues that the instrumental variables methods (IV) used by economists to solve omitted variables bias problems in observational studies also solve the major statistical problems that arise in imperfect criminological experiments. In general, IV methods estimate the causal effect of treatment on subjects that are induced to comply with a treatment by virtue of the random assignment of intended treatment. The use of IV in criminology is illustrated through a re-analysis of the Minneapolis Domestic Violence Experiment.},
}

@article{bp,
Author = {Kamenica, Emir and Gentzkow, Matthew},
Title = {Bayesian Persuasion},
Journal = {American Economic Review},
Volume = {101},
Number = {6},
Year = {2011},
Month = {October},
Pages = {2590-2615},
DOI = {10.1257/aer.101.6.2590},
URL = {https://www.aeaweb.org/articles?id=10.1257/aer.101.6.2590}}

@article{slivkins17,
author = {Slivkins, Aleksandrs},
title = {Incentivizing Exploration via Information Asymmetry},
year = {2017},
issue_date = {Fall 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/3123744},
doi = {10.1145/3123744},
abstract = {As self-interested individuals make decisions over time, they utilize information revealed by others in the past and produce information that may help others in the future. So how can we incentivize exploration for the sake of the common good?},
journal = {XRDS},
month = sep,
pages = {38–41},
numpages = {4}
}

@article{groth2010honorarium,
  title={Honorarium or coercion: use of incentives for participants in clinical research},
  author={Groth, Susan W},
  journal={The Journal of the New York State Nurses' Association},
  volume={41},
  number={1},
  pages={11},
  year={2010},
  publisher={NIH Public Access}
}

@inproceedings{iemoney,
author = {Frazier, Peter and Kempe, David and Kleinberg, Jon and Kleinberg, Robert},
title = {Incentivizing Exploration},
year = {2014},
isbn = {9781450325653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600057.2602897},
doi = {10.1145/2600057.2602897},
abstract = {We study a Bayesian multi-armed bandit (MAB) setting in which a principal seeks to maximize the sum of expected time-discounted rewards obtained by pulling arms, when the arms are actually pulled by selfish and myopic individuals. Since such individuals pull the arm with highest expected posterior reward (i.e., they always exploit and never explore), the principal must incentivize them to explore by offering suitable payments. Among others, this setting models crowdsourced information discovery and funding agencies incentivizing scientists to perform high-risk, high-reward research.We explore the tradeoff between the principal's total expected time-discounted incentive payments, and the total time-discounted rewards realized. Specifically, with a time-discount factor γ ∈ (0,1), let OPT denote the total expected time-discounted reward achievable by a principal who pulls arms directly in a MAB problem, without having to incentivize selfish agents. We call a pair (ρ,b) ∈ [0,1]2 consisting of a reward ρ and payment b achievable if for every MAB instance, using expected time-discounted payments of at most b•OPT, the principal can guarantee an expected time-discounted reward of at least ρ•OPT. Our main result is an essentially complete characterization of achievable (payment, reward) pairs: if √b+√1-ρ&gt;√γ, then (ρ,b) is achievable, and if √b+√1-ρ&lt;√γ, then (ρ,b) is not achievable.In proving this characterization, we analyze so-called time-expanded policies, which in each step let the agents choose myopically with some probability p, and incentivize them to choose "optimally" with probability 1-p. The analysis of time-expanded policies leads to a question that may be of independent interest: If the same MAB instance (without selfish agents) is considered under two different time-discount rates γ &gt; η, how small can the ratio of OPTη to OPTγ be? We give a complete answer to this question, showing that OPTη ≥ (1-γ)2/(1-η)2 • OPTγ, and that this bound is tight.},
booktitle = {Proceedings of the Fifteenth ACM Conference on Economics and Computation},
pages = {5–22},
numpages = {18},
keywords = {incentives, exploration, multi-armed bandit problems, crowdsourcing},
location = {Palo Alto, California, USA},
series = {EC '14}
}


@InProceedings{chen18a, title = {Incentivizing Exploration by Heterogeneous Users}, author = {Chen, Bangrui and Frazier, Peter and Kempe, David}, booktitle = {Proceedings of the 31st Conference On Learning Theory}, pages = {798--818}, year = {2018}, editor = {Sébastien Bubeck and Vianney Perchet and Philippe Rigollet}, volume = {75}, series = {Proceedings of Machine Learning Research}, address = {}, month = {06--09 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v75/chen18a/chen18a.pdf}, url = {http://proceedings.mlr.press/v75/chen18a.html}, abstract = {We consider the problem of incentivizing exploration with heterogeneous agents. In this problem, $N$ bandit arms provide vector-valued outcomes equal to an unknown arm-specific attribute vector, perturbed by independent noise.Agents arrive sequentially and choose arms to pull based on their own private and heterogeneous linear utility functions over attributes and the estimates of the arms’ attribute vectors derived from observations of other agents’ past pulls. Agents are myopic and selfish and thus would choose the arm with maximum estimated utility. A principal, knowing only the distribution from which agents’ preferences are drawn, but not the specific draws, can offer arm-specific incentive payments to encourage agents to explore underplayed arms. The principal seeks to minimize the total expected cumulative regret incurred by agents relative to their best arms, while also making a small expected cumulative payment. We propose an algorithm that incentivizes arms played infrequently in the past whose probability of being played in the next round would be small without incentives. Under the assumption that each arm is preferred by at least a fraction $p > 0$ of agents, we show that this algorithm achieves expected cumulative regret of $O (N \e^{2/p} + N \log^3(T))$, using expected cumulative payments of $O(N^2 \e^{2/p})$. If $p$ is known or the distribution over agent preferences is discrete, the exponential term $\e^{2/p}$ can be replaced with suitable polynomials in $N$ and $1/p$. For discrete preferences, the regret’s dependence on $T$ can be eliminated entirely, giving constant (depending only polynomially on $N$ and $1/p$) expected regret and payments. This constant regret stands in contrast to the $\Theta(\log(T))$ dependence of regret in standard multi-armed bandit problems. It arises because even unobserved heterogeneity in agent preferences causes exploitation of arms to also explore arms fully; succinctly, heterogeneity provides free exploration. } }


@inproceedings{fairie,
author = {Kannan, Sampath and Kearns, Michael and Morgenstern, Jamie and Pai, Mallesh and Roth, Aaron and Vohra, Rakesh and Wu, Zhiwei Steven},
title = {Fairness Incentives for Myopic Agents},
year = {2017},
isbn = {9781450345279},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3033274.3085154},
doi = {10.1145/3033274.3085154},
abstract = {We consider settings in which we wish to incentivize myopic agents (such as Airbnb landlords, who may emphasize short-term profits and property safety) to treat arriving clients fairly, in order to prevent overall discrimination against individuals or groups. We model such settings in both classical and contextual bandit models in which the myopic agents maximize rewards according to current empirical averages, but are also amenable to exogenous payments that may cause them to alter their choices. Our notion of fairness asks that more qualified individuals are never (probabilistically) preferred over less qualifie ones [8].We investigate whether it is possible to design inexpensive subsidy or payment schemes for a principal to motivate myopic agents to play fairly in all or almost all rounds. When the principal has full information about the state of the myopic agents, we show it is possible to induce fair play on every round with a subsidy scheme of total cost o(T) (for the classic setting with k arms, ~{O}(sqrtk3T), and for the d-dimensional linear contextual setting ~{O}(dsqrtk3T)). If the principal has much more limited information (as might often be the case for an external regulator or watchdog), and only observes the number of rounds in which members from each of the k groups were selected, but not the empirical estimates maintained by the myopic agent, the design of such a scheme becomes more complex. We show both positive and negative results in the classic and linear bandit settings by upper and lower bounding the cost of fair subsidy schemes.},
booktitle = {Proceedings of the 2017 ACM Conference on Economics and Computation},
pages = {369–386},
numpages = {18},
location = {Cambridge, Massachusetts, USA},
series = {EC '17}
}