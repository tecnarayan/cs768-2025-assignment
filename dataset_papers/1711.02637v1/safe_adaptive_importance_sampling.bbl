\begin{thebibliography}{10}

\bibitem{Alain:2015arxiv}
Guillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron Courville, and Yoshua
  Bengio.
\newblock {Variance Reduction in SGD by Distributed Importance Sampling}.
\newblock {\em arXiv.org}, February 2015.

\bibitem{AllenZhu:2016wja}
Zeyuan Allen-Zhu, Zheng Qu, Peter Richt{\'a}rik, and Yang Yuan.
\newblock {Even Faster Accelerated Coordinate Descent Using Non-Uniform
  Sampling}.
\newblock In {\em ICML 2017 - Proceedings of the 34th International Conference
  on Machine Learning}, pages 1110--1119. June 2016.

\bibitem{Shibagaki:2017}
Ichiro~Takeuchi Atsushi~Shibagaki.
\newblock {Stochastic Primal Dual Coordinate Method with Non-Uniform Sampling
  Based on Optimality Violations}.
\newblock {\em arXiv.org}, October 2017.

\bibitem{Boyd:2004uz}
Stephen~P Boyd and Lieven Vandenberghe.
\newblock {\em {Convex optimization}}.
\newblock Cambridge University Press, 2004.

\bibitem{Csiba:2015ue}
Dominik Csiba, Zheng Qu, and Peter Richt{\'a}rik.
\newblock {Stochastic Dual Coordinate Ascent with Adaptive Probabilities}.
\newblock In {\em ICML 2015 - Proceedings of the 32th International Conference
  on Machine Learning}, February 2015.

\bibitem{Csiba:2016ub}
Dominik Csiba and Peter Richt{\'a}rik.
\newblock {Importance Sampling for Minibatches}.
\newblock {\em arXiv.org}, February 2016.

\bibitem{Friedman:2007ut}
Jerome Friedman, Trevor Hastie, Holger H{\"o}fling, and Robert Tibshirani.
\newblock {Pathwise coordinate optimization}.
\newblock {\em The Annals of Applied Statistics}, 1(2):302--332, December 2007.

\bibitem{Friedman:2010wm}
Jerome Friedman, Trevor Hastie, and Robert Tibshirani.
\newblock {Regularization Paths for Generalized Linear Models via Coordinate
  Descent}.
\newblock {\em Journal of Statistical Software}, 33(1):1--22, 2010.

\bibitem{Fu:1998cd}
Wenjiang~J. Fu.
\newblock Penalized regressions: The bridge versus the lasso.
\newblock {\em Journal of Computational and Graphical Statistics},
  7(3):397--416, 1998.

\bibitem{He:2015arxiv}
Xi~He and Martin Tak\'{a}\v{c}.
\newblock {Dual Free Adaptive Mini-batch SDCA for Empirical Risk Minimization}.
\newblock {\em arXiv.org}, October 2015.

\bibitem{Hsieh:2008bd}
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S~Sathiya Keerthi, and
  S~Sundararajan.
\newblock {A Dual Coordinate Descent Method for Large-scale Linear SVM}.
\newblock In {\em ICML 2008 - the 25th International Conference on Machine
  Learning}, pages 408--415, New York, USA, 2008. ACM Press.

\bibitem{komiya1988}
Hidetoshi Komiya.
\newblock Elementary proof for sion's minimax theorem.
\newblock {\em Kodai Math. J.}, 11(1):5--7, 1988.

\bibitem{LacosteJulien:2012uo}
Simon Lacoste-Julien, Mark Schmidt, and Francis Bach.
\newblock {A simpler approach to obtaining an O(1/t) convergence rate for
  projected stochastic subgradient descent}.
\newblock {\em arXiv.org}, December 2012.

\bibitem{Liu:2014uz}
Jun Liu, Zheng Zhao, Jie Wang, and Jieping Ye.
\newblock {Safe Screening with Variational Inequalities and Its Application to
  Lasso}.
\newblock In {\em ICML 2014 - Proceedings of the 31st International Conference
  on Machine Learning}, pages 289--297, 2014.

\bibitem{Ndiaye:2017tt}
Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, and Joseph Salmon.
\newblock {Gap Safe screening rules for sparsity enforcing penalties}.
\newblock {\em JMLR}, 2017.

\bibitem{Needell:2014wa}
Deanna Needell, Rachel Ward, and Nathan Srebro.
\newblock {Stochastic Gradient Descent, Weighted Sampling, and the Randomized
  Kaczmarz algorithm}.
\newblock In {\em NIPS 2014 - Advances in Neural Information Processing Systems
  27}, pages 1017--1025, 2014.

\bibitem{Nemirovski:2009}
A.~Nemirovski, A.~Juditsky, G.~Lan, and A.~Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock {\em SIAM Journal on Optimization}, 19(4):1574--1609, 2009.

\bibitem{Nesterov:2012fa}
Yurii Nesterov.
\newblock {Efficiency of Coordinate Descent Methods on Huge-Scale Optimization
  Problems}.
\newblock {\em SIAM Journal on Optimization}, 22(2):341--362, 2012.

\bibitem{Nesterov:2017}
Yurii Nesterov and Sebastian~U. Stich.
\newblock Efficiency of the accelerated coordinate descent method on structured
  optimization problems.
\newblock {\em SIAM Journal on Optimization}, 27(1):110--123, 2017.

\bibitem{nutini2015coordinate}
Julie Nutini, Mark~W Schmidt, Issam~H Laradji, Michael~P Friedlander, and
  Hoyt~A Koepke.
\newblock {Coordinate Descent Converges Faster with the Gauss-Southwell Rule
  Than Random Selection}.
\newblock In {\em ICML}, pages 1632--1641, 2015.

\bibitem{Osokin:2016:MGB}
Anton Osokin, Jean-Baptiste Alayrac, Isabella Lukasewitz, Puneet~K. Dokania,
  and Simon Lacoste-Julien.
\newblock Minding the gaps for block frank-wolfe optimization of structured
  svms.
\newblock In {\em Proceedings of the 33rd International Conference on
  International Conference on Machine Learning - Volume 48}, ICML'16, pages
  593--602. JMLR.org, 2016.

\bibitem{Papa2015}
Guillaume Papa, Pascal Bianchi, and St{\'e}phan Cl{\'e}men{\c c}on.
\newblock {Adaptive Sampling for Incremental Optimization Using Stochastic
  Gradient Descent}.
\newblock {\em ALT 2015 - 26th International Conference on Algorithmic Learning
  Theory}, pages 317--331, 2015.

\bibitem{Perekrestenko17a}
Dmytro Perekrestenko, Volkan Cevher, and Martin Jaggi.
\newblock {Faster Coordinate Descent via Adaptive Importance Sampling}.
\newblock In {\em AISTATS 2017 - Proceedings of the 20th International
  Conference on Artificial Intelligence and Statistics}, volume~54, pages
  869--877. PMLR, 20--22 Apr 2017.

\bibitem{Qu:2014uw}
Zheng Qu, Peter Richt{\'a}rik, and Tong Zhang.
\newblock {Randomized Dual Coordinate Ascent with Arbitrary Sampling}.
\newblock {\em arXiv.org}, November 2014.

\bibitem{Richtarik:2016nsync}
Peter Richt{\'a}rik and Martin Tak{\'a}{\v{c}}.
\newblock On optimal probabilities in stochastic coordinate descent methods.
\newblock {\em Optimization Letters}, 10(6):1233--1243, 2016.

\bibitem{Schmidt:15}
Mark Schmidt, Reza Babanezhad, Mohamed Ahmed, Aaron Defazio, Ann Clifton, and
  Anoop Sarkar.
\newblock {Non-Uniform Stochastic Average Gradient Method for Training
  Conditional Random Fields}.
\newblock In {\em AISTATS 2015 - Proceedings of the Eighteenth International
  Conference on Artificial Intelligence and Statistics}, volume~38, pages
  819--828. PMLR, 09--12 May 2015.

\bibitem{ShalevShwartz:2010cg}
Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter.
\newblock {Pegasos: Primal Estimated Sub-Gradient Solver for SVM}.
\newblock {\em Mathematical Programming}, 127(1):3--30, October 2010.

\bibitem{ShalevShwartz:2011vo}
Shai Shalev-Shwartz and Ambuj Tewari.
\newblock {Stochastic Methods for l$_{1}$-regularized Loss Minimization}.
\newblock {\em JMLR}, 12:1865--1892, June 2011.

\bibitem{ShalevShwartz:2013wl}
Shai Shalev-Shwartz and Tong Zhang.
\newblock {Stochastic Dual Coordinate Ascent Methods for Regularized Loss
  Minimization}.
\newblock {\em JMLR}, 14:567--599, February 2013.

\bibitem{Sion1958}
Maurice Sion.
\newblock On general minimax theorems.
\newblock {\em Pacific Journal of Mathematics}, 8(1):171--176, 1958.

\bibitem{Stich2016variable}
S.~U. Stich, C.~L. M{\"u}ller, and B.~G{\"a}rtner.
\newblock Variable metric random pursuit.
\newblock {\em Mathematical Programming}, 156(1):549--579, Mar 2016.

\bibitem{stich2017steep}
Sebastian~U. Stich, Anant Raj, and Martin Jaggi.
\newblock Approximate steepest coordinate descent.
\newblock In Doina Precup and Yee~Whye Teh, editors, {\em ICML 2017 -
  Proceedings of the 34th International Conference on Machine Learning},
  volume~70, pages 3251--3259. PMLR, 06--11 Aug 2017.

\bibitem{Strohmer2008}
Thomas Strohmer and Roman Vershynin.
\newblock A randomized kaczmarz algorithm with exponential convergence.
\newblock {\em Journal of Fourier Analysis and Applications}, 15(2):262, 2008.

\bibitem{Tseng2009}
Paul Tseng and Sangwoon Yun.
\newblock A coordinate gradient descent method for nonsmooth separable
  minimization.
\newblock {\em Mathematical Programming}, 117(1):387--423, 2009.

\bibitem{wright2015}
Stephen~J Wright.
\newblock Coordinate descent algorithms.
\newblock {\em Mathematical Programming}, 151(1):3--34, 2015.

\bibitem{Zhaoa15}
Peilin Zhao and Tong Zhang.
\newblock Stochastic optimization with importance sampling for regularized loss
  minimization.
\newblock In {\em ICML 2015 - Proceedings of the 32nd International Conference
  on Machine Learning}, volume~37, pages 1--9. PMLR, 07--09 Jul 2015.

\bibitem{Zhu:2016}
Rong Zhu.
\newblock Gradient-based sampling: An adaptive importance sampling for
  least-squares.
\newblock In {\em NIPS - Advances in Neural Information Processing Systems 29},
  pages 406--414. 2016.

\end{thebibliography}
