\begin{thebibliography}{85}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Antonakopoulos(2021)]{antonakopoulos2021adaptive2}
K~Antonakopoulos.
\newblock Adaptive extra-gradient methods for min-max optimization and games.
\newblock In \emph{ICLR}, volume~3, page~7, 2021.

\bibitem[Antonakopoulos and Mertikopoulos(2021)]{antonakopoulos2021adaptive}
Kimon Antonakopoulos and Panayotis Mertikopoulos.
\newblock Adaptive first-order methods revisited: Convex minimization without
  lipschitz requirements.
\newblock \emph{NeurIPS}, 34, 2021.

\bibitem[Antonakopoulos et~al.(2019)Antonakopoulos, Belmega, and
  Mertikopoulos]{antonakopoulos2019adaptive}
Kimon Antonakopoulos, Veronica Belmega, and Panayotis Mertikopoulos.
\newblock An adaptive mirror-prox method for variational inequalities with
  singular operators.
\newblock \emph{NeurIPS}, 32, 2019.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{arjovsky2017wasserstein}
Martin Arjovsky, Soumith Chintala, and L{\'e}on Bottou.
\newblock Wasserstein generative adversarial networks.
\newblock In \emph{ICML}, pages 214--223. PMLR, 2017.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Gentile]{auer2002adaptive}
Peter Auer, Nicolo Cesa-Bianchi, and Claudio Gentile.
\newblock Adaptive and self-confident on-line learning algorithms.
\newblock \emph{Journal of Computer and System Sciences}, 64\penalty0
  (1):\penalty0 48--75, 2002.

\bibitem[Bach and Levy(2019)]{bach2019universal}
Francis Bach and Kfir~Y Levy.
\newblock A universal algorithm for variational inequalities adaptive to
  smoothness and noise.
\newblock In \emph{COLT}, pages 164--194. PMLR, 2019.

\bibitem[Beck(2017)]{beck2017first}
Amir Beck.
\newblock \emph{First-order methods in optimization}.
\newblock SIAM, 2017.

\bibitem[Bo{\c{t}} and B{\"o}hm(2020)]{boct2020alternating}
Radu~Ioan Bo{\c{t}} and Axel B{\"o}hm.
\newblock Alternating proximal-gradient steps for (stochastic)
  nonconvex-concave minimax problems.
\newblock \emph{arXiv preprint arXiv:2007.13605}, 2020.

\bibitem[Carmon and Hinder(2022)]{carmon2022making}
Yair Carmon and Oliver Hinder.
\newblock Making sgd parameter-free.
\newblock \emph{arXiv preprint arXiv:2205.02160}, 2022.

\bibitem[Chen et~al.(2021)Chen, Sun, and Yin]{chen2021closing}
Tianyi Chen, Yuejiao Sun, and Wotao Yin.
\newblock Closing the gap: Tighter analysis of alternating stochastic gradient
  methods for bilevel problems.
\newblock \emph{NeurIPS}, 34, 2021.

\bibitem[Chen et~al.(2019)Chen, Liu, Sun, and Hong]{chen2018convergence}
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
\newblock On the convergence of a class of adam-type algorithms for non-convex
  optimization.
\newblock In \emph{ICLR}, 2019.

\bibitem[Cutkosky and Boahen(2017)]{NIPS2017_6aed000a}
Ashok Cutkosky and Kwabena~A Boahen.
\newblock Stochastic and adversarial online learning without hyperparameters.
\newblock In \emph{NeurIPS}, volume~30, 2017.

\bibitem[Cutkosky and Orabona(2018)]{cutkosky2018black}
Ashok Cutkosky and Francesco Orabona.
\newblock Black-box reductions for parameter-free online learning in banach
  spaces.
\newblock In \emph{COLT}, pages 1493--1529. PMLR, 2018.

\bibitem[Dai et~al.(2017)Dai, He, Pan, Boots, and Song]{dai2017learning}
Bo~Dai, Niao He, Yunpeng Pan, Byron Boots, and Le~Song.
\newblock Learning from conditional distributions via dual embeddings.
\newblock In \emph{AISTATS}, pages 1458--1467. PMLR, 2017.

\bibitem[Diakonikolas(2020)]{diakonikolas2020halpern}
Jelena Diakonikolas.
\newblock Halpern iteration for near-optimal and parameter-free monotone
  inclusion and strong solutions to variational inequalities.
\newblock In \emph{COLT}, pages 1428--1451. PMLR, 2020.

\bibitem[Dou and Li(2021)]{dou2021one}
Zehao Dou and Yuanzhi Li.
\newblock On the one-sided convergence of adam-type algorithms in non-convex
  non-concave min-max optimization.
\newblock \emph{arXiv preprint arXiv:2109.14213}, 2021.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of machine learning research}, 12\penalty0 (7), 2011.

\bibitem[Ene and Nguyen(2020)]{ene2020adaptive}
Alina Ene and Huy~L Nguyen.
\newblock Adaptive and universal algorithms for variational inequalities with
  optimal convergence.
\newblock \emph{arXiv preprint arXiv:2010.07799}, 2020.

\bibitem[Fiez et~al.(2021)Fiez, Ratliff, Mazumdar, Faulkner, and
  Narang]{fiez2021global}
Tanner Fiez, Lillian Ratliff, Eric Mazumdar, Evan Faulkner, and Adhyyan Narang.
\newblock Global convergence to local minmax equilibrium in classes of
  nonconvex zero-sum games.
\newblock \emph{NeurIPS}, 34, 2021.

\bibitem[Fontaine et~al.(2021)Fontaine, De~Bortoli, and
  Durmus]{fontaine2021convergence}
Xavier Fontaine, Valentin De~Bortoli, and Alain Durmus.
\newblock Convergence rates and approximation results for sgd and its
  continuous-time counterpart.
\newblock In \emph{COLT}, pages 1965--2058. PMLR, 2021.

\bibitem[Ganin et~al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle,
  Laviolette, Marchand, and Lempitsky]{ganin2016domain}
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
  Larochelle, Fran{\c{c}}ois Laviolette, Mario Marchand, and Victor Lempitsky.
\newblock Domain-adversarial training of neural networks.
\newblock \emph{The journal of machine learning research}, 17\penalty0
  (1):\penalty0 2096--2030, 2016.

\bibitem[Gasnikov et~al.(2019)Gasnikov, Dvurechensky, Stonyakin, and
  Titov]{gasnikov2019adaptive}
Alexander~Vladimirovich Gasnikov, PE~Dvurechensky, Fedor~Sergeevich Stonyakin,
  and Aleksandr~Aleksandrovich Titov.
\newblock An adaptive proximal method for variational inequalities.
\newblock \emph{Computational Mathematics and Mathematical Physics},
  59\penalty0 (5):\penalty0 836--841, 2019.

\bibitem[Gidel et~al.(2019)Gidel, Berard, Vignoud, Vincent, and
  Lacoste-Julien]{gidel2018variational}
Gauthier Gidel, Hugo Berard, Ga{\"e}tan Vignoud, Pascal Vincent, and Simon
  Lacoste-Julien.
\newblock A variational inequality perspective on generative adversarial
  networks.
\newblock In \emph{ICLR}, 2019.

\bibitem[Goodfellow(2016)]{goodfellow2016nips}
Ian Goodfellow.
\newblock Nips 2016 tutorial: Generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1701.00160}, 2016.

\bibitem[Goodfellow et~al.(2014{\natexlab{a}})Goodfellow, Pouget-Abadie, Mirza,
  Xu, Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock \emph{NeurIPS}, 27, 2014{\natexlab{a}}.

\bibitem[Goodfellow et~al.(2014{\natexlab{b}})Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014{\natexlab{b}}.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{gulrajani2017improved}
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron~C
  Courville.
\newblock Improved training of wasserstein gans.
\newblock \emph{NeurIPS}, 30, 2017.

\bibitem[Guo et~al.(2021)Guo, Xu, Yin, Jin, and Yang]{guo2021novel}
Zhishuai Guo, Yi~Xu, Wotao Yin, Rong Jin, and Tianbao Yang.
\newblock A novel convergence analysis for algorithms of the adam family and
  beyond.
\newblock \emph{arXiv preprint arXiv:2104.14840}, 2021.

\bibitem[Harvey et~al.(2019)Harvey, Liaw, and Randhawa]{harvey2019simple}
Nicholas~JA Harvey, Christopher Liaw, and Sikander Randhawa.
\newblock Simple and optimal high-probability bounds for strongly-convex
  stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:1909.00843}, 2019.

\bibitem[Hazan and Kakade(2019)]{hazan2019revisiting}
Elad Hazan and Sham Kakade.
\newblock Revisiting the polyak step size.
\newblock \emph{arXiv preprint arXiv:1905.00313}, 2019.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, and Swersky]{hinton2012neural}
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky.
\newblock Neural networks for machine learning lecture 6a overview of
  mini-batch gradient descent.
\newblock 2012.

\bibitem[Ho and Ermon(2016)]{ho2016generative}
Jonathan Ho and Stefano Ermon.
\newblock Generative adversarial imitation learning.
\newblock \emph{NeurIPS}, 29, 2016.

\bibitem[Huang and Huang(2021)]{huang2021adagda}
Feihu Huang and Heng Huang.
\newblock Adagda: Faster adaptive gradient descent ascent methods for minimax
  optimization.
\newblock \emph{arXiv preprint arXiv:2106.16101}, 2021.

\bibitem[Huang et~al.(2021)Huang, Wu, and Huang]{huang2021efficient}
Feihu Huang, Xidong Wu, and Heng Huang.
\newblock Efficient mirror descent ascent methods for nonsmooth minimax
  problems.
\newblock \emph{NeurIPS}, 34, 2021.

\bibitem[Jain et~al.(2019)Jain, Nagaraj, and Netrapalli]{jain2019making}
Prateek Jain, Dheeraj Nagaraj, and Praneeth Netrapalli.
\newblock Making the last iterate of sgd information theoretically optimal.
\newblock In \emph{COLT}, pages 1752--1755. PMLR, 2019.

\bibitem[Jin et~al.(2019)Jin, Netrapalli, Ge, Kakade, and Jordan]{jin2019short}
Chi Jin, Praneeth Netrapalli, Rong Ge, Sham~M Kakade, and Michael~I Jordan.
\newblock A short note on concentration inequalities for random vectors with
  subgaussian norm.
\newblock \emph{arXiv preprint arXiv:1902.03736}, 2019.

\bibitem[Jin et~al.(2021)Jin, Netrapalli, Ge, Kakade, and
  Jordan]{jin2021nonconvex}
Chi Jin, Praneeth Netrapalli, Rong Ge, Sham~M Kakade, and Michael~I Jordan.
\newblock On nonconvex optimization for machine learning: Gradients,
  stochasticity, and saddle points.
\newblock \emph{Journal of the ACM (JACM)}, 68\penalty0 (2):\penalty0 1--29,
  2021.

\bibitem[Kakade and Tewari(2008)]{kakade2008generalization}
Sham~M Kakade and Ambuj Tewari.
\newblock On the generalization ability of online strongly convex programming
  algorithms.
\newblock \emph{NeurIPS}, 21, 2008.

\bibitem[Kavis et~al.(2019)Kavis, Levy, Bach, and Cevher]{kavis2019unixgrad}
Ali Kavis, Kfir~Y Levy, Francis Bach, and Volkan Cevher.
\newblock Unixgrad: A universal, adaptive algorithm with optimal guarantees for
  constrained optimization.
\newblock \emph{NeurIPS}, 32, 2019.

\bibitem[Kavis et~al.(2022)Kavis, Levy, and Cevher]{kavis2022high}
Ali Kavis, Kfir~Yehuda Levy, and Volkan Cevher.
\newblock High probability bounds for a class of nonconvex algorithms with
  adagrad stepsize.
\newblock In \emph{ICLR}, 2022.

\bibitem[Kingma and Ba(2015)]{kingma2015adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}, 2015.

\bibitem[Lacoste-Julien et~al.(2012)Lacoste-Julien, Schmidt, and
  Bach]{lacoste2012simpler}
Simon Lacoste-Julien, Mark Schmidt, and Francis Bach.
\newblock A simpler approach to obtaining an o (1/t) convergence rate for the
  projected stochastic subgradient method.
\newblock \emph{arXiv preprint arXiv:1212.2002}, 2012.

\bibitem[LeCun(1998)]{lecun1998mnist}
Yann LeCun.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[Levy(2017)]{levy2017online}
Kfir Levy.
\newblock Online to offline conversions, universality and adaptive minibatch
  sizes.
\newblock \emph{NeurIPS}, 30, 2017.

\bibitem[Levy et~al.(2018)Levy, Yurtsever, and Cevher]{levy2018online}
Kfir~Y Levy, Alp Yurtsever, and Volkan Cevher.
\newblock Online adaptive methods, universality and acceleration.
\newblock \emph{NeurIPS}, 31, 2018.

\bibitem[Li et~al.(2021)Li, Tian, Zhang, and Jadbabaie]{li2021complexity}
Haochuan Li, Yi~Tian, Jingzhao Zhang, and Ali Jadbabaie.
\newblock Complexity lower bounds for nonconvex-strongly-concave min-max
  optimization.
\newblock \emph{NeurIPS}, 34, 2021.

\bibitem[Li et~al.(2022)Li, Farnia, Das, and Jadbabaie]{li2022convergence}
Haochuan Li, Farzan Farnia, Subhro Das, and Ali Jadbabaie.
\newblock On convergence of gradient descent ascent: A tight local analysis.
\newblock In \emph{ICML}, pages 12717--12740. PMLR, 2022.

\bibitem[Li and Orabona(2019)]{li2019convergence}
Xiaoyu Li and Francesco Orabona.
\newblock On the convergence of stochastic gradient descent with adaptive
  stepsizes.
\newblock In \emph{AISTATS}, pages 983--992. PMLR, 2019.

\bibitem[Li and Orabona(2020)]{DBLP:journals/corr/abs-2007-14294}
Xiaoyu Li and Francesco Orabona.
\newblock A high probability analysis of adaptive {SGD} with momentum.
\newblock \emph{CoRR}, abs/2007.14294, 2020.

\bibitem[Lin et~al.(2020)Lin, Jin, and Jordan]{lin2020gradient}
Tianyi Lin, Chi Jin, and Michael Jordan.
\newblock On gradient descent ascent for nonconvex-concave minimax problems.
\newblock In \emph{ICML}, pages 6083--6093. PMLR, 2020.

\bibitem[Liu et~al.(2020{\natexlab{a}})Liu, Jiang, He, Chen, Liu, Gao, and
  Han]{liu2019variance}
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng
  Gao, and Jiawei Han.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock In \emph{ICLR}, 2020{\natexlab{a}}.

\bibitem[Liu et~al.(2020{\natexlab{b}})Liu, Mroueh, Ross, Zhang, Cui, Das, and
  Yang]{liu2019towards}
Mingrui Liu, Youssef Mroueh, Jerret Ross, Wei Zhang, Xiaodong Cui, Payel Das,
  and Tianbao Yang.
\newblock Towards better understanding of adaptive gradient algorithms in
  generative adversarial nets.
\newblock In \emph{ICLR}, 2020{\natexlab{b}}.

\bibitem[Liu et~al.(2021)Liu, Rafique, Lin, and Yang]{liu2021first}
Mingrui Liu, Hassan Rafique, Qihang Lin, and Tianbao Yang.
\newblock First-order convergence theory for weakly-convex-weakly-concave
  min-max problems.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (169):\penalty0 1--34, 2021.

\bibitem[Loizou et~al.(2021)Loizou, Vaswani, Laradji, and
  Lacoste-Julien]{loizou2021stochastic}
Nicolas Loizou, Sharan Vaswani, Issam~Hadj Laradji, and Simon Lacoste-Julien.
\newblock Stochastic polyak step-size for sgd: An adaptive learning rate for
  fast convergence.
\newblock In \emph{AISTATS}, pages 1306--1314. PMLR, 2021.

\bibitem[Lu et~al.(2020)Lu, Tsaknakis, Hong, and Chen]{lu2020hybrid}
Songtao Lu, Ioannis Tsaknakis, Mingyi Hong, and Yongxin Chen.
\newblock Hybrid block successive approximation for one-sided non-convex
  min-max problems: algorithms and applications.
\newblock \emph{IEEE Transactions on Signal Processing}, 68:\penalty0
  3676--3691, 2020.

\bibitem[Lv(2019)]{githubcode}
Louis Lv.
\newblock Reproducing "certifying some distributional robustness with
  principled adversarial training".
\newblock
  \url{https://github.com/Louis-udm/Reproducing-certifiable-distributional-robustness},
  2019.

\bibitem[Madden et~al.(2020)Madden, Dall’Anese, and Becker]{madden2020high}
Liam Madden, Emiliano Dall’Anese, and Stephen Becker.
\newblock High-probability convergence bounds for non-convex stochastic
  gradient descent.
\newblock \emph{arXiv preprint arXiv:2006.05610}, 2020.

\bibitem[Malitsky(2020)]{malitsky2020golden}
Yura Malitsky.
\newblock Golden ratio algorithms for variational inequalities.
\newblock \emph{Mathematical Programming}, 184\penalty0 (1):\penalty0 383--410,
  2020.

\bibitem[Modi et~al.(2021)Modi, Chen, Krishnamurthy, Jiang, and
  Agarwal]{modi2021model}
Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal.
\newblock Model-free representation learning and exploration in low-rank mdps.
\newblock \emph{arXiv preprint arXiv:2102.07035}, 2021.

\bibitem[Moulines and Bach(2011)]{moulines2011non}
Eric Moulines and Francis Bach.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock \emph{NeurIPS}, 24, 2011.

\bibitem[Mukkamala and Hein(2017)]{mukkamala2017variants}
Mahesh~Chandra Mukkamala and Matthias Hein.
\newblock Variants of rmsprop and adagrad with logarithmic regret bounds.
\newblock In \emph{ICML}, pages 2545--2553. PMLR, 2017.

\bibitem[Nemirovski(2004)]{nemirovski2004prox}
Arkadi Nemirovski.
\newblock Prox-method with rate of convergence o (1/t) for variational
  inequalities with lipschitz continuous monotone operators and smooth
  convex-concave saddle point problems.
\newblock \emph{SIAM Journal on Optimization}, 15\penalty0 (1):\penalty0
  229--251, 2004.

\bibitem[Nouiehed et~al.(2019)Nouiehed, Sanjabi, Huang, Lee, and
  Razaviyayn]{nouiehed2019solving}
Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason~D Lee, and Meisam
  Razaviyayn.
\newblock Solving a class of non-convex min-max games using iterative first
  order methods.
\newblock \emph{NeurIPS}, 32, 2019.

\bibitem[Orabona and P{\'a}l(2018)]{orabona2018scale}
Francesco Orabona and D{\'a}vid P{\'a}l.
\newblock Scale-free online learning.
\newblock \emph{Theoretical Computer Science}, 716:\penalty0 50--69, 2018.

\bibitem[Orvieto et~al.(2022)Orvieto, Lacoste-Julien, and
  Loizou]{orvieto2022dynamics}
Antonio Orvieto, Simon Lacoste-Julien, and Nicolas Loizou.
\newblock Dynamics of sgd with stochastic polyak stepsizes: Truly adaptive
  variants and convergence to exact solution.
\newblock \emph{arXiv preprint arXiv:2205.04583}, 2022.

\bibitem[Pang(1987)]{pang1987posteriori}
Jong-Shi Pang.
\newblock A posteriori error bounds for the linearly-constrained variational
  inequality problem.
\newblock \emph{Mathematics of Operations Research}, 12\penalty0 (3):\penalty0
  474--484, 1987.

\bibitem[Rakhlin et~al.(2012)Rakhlin, Shamir, and Sridharan]{rakhlin2012making}
Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock In \emph{ICML}, pages 1571--1578, 2012.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{reddi2018convergence}
Sashank~J Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock In \emph{ICLR}, 2018.

\bibitem[Sebbouh et~al.(2022)Sebbouh, Cuturi, and
  Peyr{\'e}]{sebbouh2022randomized}
Othmane Sebbouh, Marco Cuturi, and Gabriel Peyr{\'e}.
\newblock Randomized stochastic gradient descent ascent.
\newblock In \emph{AISTATS}, pages 2941--2969. PMLR, 2022.

\bibitem[Sinha et~al.(2018)Sinha, Namkoong, and
  Duchi]{DBLP:conf/iclr/SinhaND18}
Aman Sinha, Hongseok Namkoong, and John~C. Duchi.
\newblock Certifying some distributional robustness with principled adversarial
  training.
\newblock In \emph{{ICLR}}, 2018.

\bibitem[Stonyakin et~al.(2018)Stonyakin, Gasnikov, Dvurechensky, Alkousa, and
  Titov]{stonyakin2018generalized}
Fedor Stonyakin, Alexander Gasnikov, Pavel Dvurechensky, Mohammad Alkousa, and
  Alexander Titov.
\newblock Generalized mirror prox for monotone variational inequalities:
  Universality and inexact oracle.
\newblock \emph{arXiv preprint arXiv:1806.05140}, 2018.

\bibitem[Tram{\`e}r et~al.(2018)Tram{\`e}r, Kurakin, Papernot, Goodfellow,
  Boneh, and McDaniel]{tramer2018ensemble}
Florian Tram{\`e}r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan
  Boneh, and Patrick McDaniel.
\newblock Ensemble adversarial training: Attacks and defenses.
\newblock In \emph{ICLR}, 2018.

\bibitem[Vaswani et~al.(2019)Vaswani, Mishkin, Laradji, Schmidt, Gidel, and
  Lacoste-Julien]{vaswani2019painless}
Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and
  Simon Lacoste-Julien.
\newblock Painless stochastic gradient: Interpolation, line-search, and
  convergence rates.
\newblock \emph{NeurIPS}, 32, 2019.

\bibitem[Vaswani et~al.(2020)Vaswani, Kunstner, Laradji, Meng, Schmidt, and
  Lacoste-Julien]{vaswani2020adaptive}
Sharan Vaswani, Frederik Kunstner, Issam~H. Laradji, Si~Yi Meng, Mark~W.
  Schmidt, and Simon Lacoste-Julien.
\newblock Adaptive gradient methods converge faster with over-parameterization
  (and you can do a line-search).
\newblock \emph{ArXiv}, abs/2006.06835, 2020.

\bibitem[Vaswani et~al.(2021)Vaswani, Dubois-Taine, and
  Babanezhad]{vaswani2021towards}
Sharan Vaswani, Benjamin Dubois-Taine, and Reza Babanezhad.
\newblock Towards noise-adaptive, problem-adaptive stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:2110.11442}, 2021.

\bibitem[Wang et~al.(2020)Wang, Lu, Cheng, Tu, and Zhang]{wang2019sadam}
Guanghui Wang, Shiyin Lu, Quan Cheng, Wei-wei Tu, and Lijun Zhang.
\newblock Sadam: A variant of adam for strongly convex functions.
\newblock In \emph{ICLR}, 2020.

\bibitem[Ward et~al.(2019)Ward, Wu, and Bottou]{ward2019adagrad}
Rachel Ward, Xiaoxia Wu, and Leon Bottou.
\newblock Adagrad stepsizes: Sharp convergence over nonconvex landscapes.
\newblock In \emph{ICML}, pages 6677--6686. PMLR, 2019.

\bibitem[Xie et~al.(2020)Xie, Wu, and Ward]{xie2020linear}
Yuege Xie, Xiaoxia Wu, and Rachel Ward.
\newblock Linear convergence of adaptive stochastic gradient descent.
\newblock In \emph{AISTATS}, pages 1475--1485. PMLR, 2020.

\bibitem[Yang et~al.(2020{\natexlab{a}})Yang, Kiyavash, and He]{yang2020global}
Junchi Yang, Negar Kiyavash, and Niao He.
\newblock Global convergence and variance reduction for a class of
  nonconvex-nonconcave minimax problems.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1153--1165, 2020{\natexlab{a}}.

\bibitem[Yang et~al.(2020{\natexlab{b}})Yang, Zhang, Kiyavash, and
  He]{yang2020catalyst}
Junchi Yang, Siqi Zhang, Negar Kiyavash, and Niao He.
\newblock A catalyst framework for minimax optimization.
\newblock \emph{NeurIPS}, 33:\penalty0 5667--5678, 2020{\natexlab{b}}.

\bibitem[Yang et~al.(2022)Yang, Orvieto, Lucchi, and He]{yang2021faster}
Junchi Yang, Antonio Orvieto, Aurelien Lucchi, and Niao He.
\newblock Faster single-loop algorithms for minimax optimization without strong
  concavity.
\newblock In \emph{AISTATS}, pages 5485--5517. PMLR, 2022.

\bibitem[Zaheer et~al.(2018)Zaheer, Reddi, Sachan, Kale, and
  Kumar]{zaheer2018adaptive}
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock \emph{NeurIPS}, 31, 2018.

\bibitem[Zeiler(2012)]{zeiler2012adadelta}
Matthew~D Zeiler.
\newblock Adadelta: an adaptive learning rate method.
\newblock \emph{arXiv preprint arXiv:1212.5701}, 2012.

\bibitem[Zhang et~al.(2020)Zhang, Xiao, Sun, and Luo]{zhang2020single}
Jiawei Zhang, Peijun Xiao, Ruoyu Sun, and Zhiquan Luo.
\newblock A single-loop smoothed gradient descent-ascent algorithm for
  nonconvex-concave min-max problems.
\newblock \emph{NeurIPS}, 33:\penalty0 7377--7389, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Yang, Guzm{\'a}n, Kiyavash, and
  He]{zhang2021complexity}
Siqi Zhang, Junchi Yang, Crist{\'o}bal Guzm{\'a}n, Negar Kiyavash, and Niao He.
\newblock The complexity of nonconvex-strongly-concave minimax optimization.
\newblock In \emph{UAI}, pages 482--492. PMLR, 2021.

\end{thebibliography}
