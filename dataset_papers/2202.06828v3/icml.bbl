\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Antos et~al.(2008)Antos, Szepesv{\'{a}}ri, and
  Munos]{antos2008learning}
Antos, A., Szepesv{\'{a}}ri, C., and Munos, R.
\newblock Learning near-optimal policies with bellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock \emph{Mach. Learn.}, 2008.

\bibitem[Baird(1995)]{baird1995residual}
Baird, L.~C.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In \emph{Machine Learning, Proceedings of the Twelfth International
  Conference on Machine Learning, Tahoe City, California, USA, July 9-12,
  1995}, 1995.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare13arcade}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{J. Artif. Intell. Res.}, 2013.

\bibitem[Benveniste et~al.(1990)Benveniste, M{\'{e}}tivier, and
  Priouret]{DBLP:books/sp/BenvenisteMP90}
Benveniste, A., M{\'{e}}tivier, M., and Priouret, P.
\newblock \emph{Adaptive Algorithms and Stochastic Approximations}, volume~22
  of \emph{Applications of Mathematics}.
\newblock Springer, 1990.
\newblock ISBN 978-3-642-75896-6.
\newblock \doi{10.1007/978-3-642-75894-2}.
\newblock URL \url{https://doi.org/10.1007/978-3-642-75894-2}.

\bibitem[Bertsekas \& Tsitsiklis(1996)Bertsekas and
  Tsitsiklis]{bertsekas1996neuro}
Bertsekas, D.~P. and Tsitsiklis, J.~N.
\newblock \emph{Neuro-Dynamic Programming}.
\newblock Athena Scientific Belmont, MA, 1996.

\bibitem[Bhandari et~al.(2018)Bhandari, Russo, and Singal]{bhandari2018finite}
Bhandari, J., Russo, D., and Singal, R.
\newblock A finite time analysis of temporal difference learning with linear
  function approximation.
\newblock In \emph{Conference On Learning Theory, {COLT} 2018, Stockholm,
  Sweden, 6-9 July 2018}, 2018.

\bibitem[Borkar(2009)]{borkar2009stochastic}
Borkar, V.~S.
\newblock \emph{Stochastic approximation: a dynamical systems viewpoint},
  volume~48.
\newblock Springer, 2009.

\bibitem[Chen et~al.(2021)Chen, Maguluri, Shakkottai, and
  Shanmugam]{chen2021lyapunov}
Chen, Z., Maguluri, S.~T., Shakkottai, S., and Shanmugam, K.
\newblock A lyapunov theory for finite-sample guarantees of asynchronous
  q-learning and td-learning variants.
\newblock \emph{arXiv preprint arXiv:2102.01567}, 2021.

\bibitem[Dalal et~al.(2018)Dalal, Sz{\"o}r{\'e}nyi, Thoppe, and
  Mannor]{dalal2018finiteaaai}
Dalal, G., Sz{\"o}r{\'e}nyi, B., Thoppe, G., and Mannor, S.
\newblock Finite sample analyses for td (0) with function approximation.
\newblock In \emph{Thirty-second AAAI conference on artificial intelligence},
  2018.

\bibitem[De~Farias \& Van~Roy(2000)De~Farias and Van~Roy]{de2000existence}
De~Farias, D.~P. and Van~Roy, B.
\newblock On the existence of fixed points for approximate value iteration and
  temporal-difference learning.
\newblock \emph{Journal of Optimization theory and Applications}, 105\penalty0
  (3):\penalty0 589--608, 2000.

\bibitem[Farahmand et~al.(2010)Farahmand, Munos, and
  Szepesv{\'a}ri]{farahmand2010error}
Farahmand, A.~M., Munos, R., and Szepesv{\'a}ri, C.
\newblock Error propagation for approximate policy and value iteration.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2010.

\bibitem[Gopalan \& Thoppe(2022)Gopalan and Thoppe]{gopalan2022approximate}
Gopalan, A. and Thoppe, G.
\newblock Approximate q-learning and sarsa (0) under the $epsilon $-greedy
  policy: a differential inclusion analysis.
\newblock \emph{arXiv preprint arXiv:2205.13617}, 2022.

\bibitem[Gordon(1996)]{gordon1996chattering}
Gordon, G.~J.
\newblock Chattering in sarsa (lambda)-a cmu learning lab internal report.
\newblock 1996.

\bibitem[Gordon(2001)]{gordon2001reinforcement}
Gordon, G.~J.
\newblock Reinforcement learning with function approximation converges to a
  region.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1040--1046, 2001.

\bibitem[Konda \& Tsitsiklis(1999)Konda and Tsitsiklis]{konda2000actor}
Konda, V.~R. and Tsitsiklis, J.~N.
\newblock Actor-critic algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems 12,
  {[NIPS} Conference, Denver, Colorado, USA, November 29 - December 4, 1999]},
  1999.

\bibitem[Kushner \& Yin(2003)Kushner and Yin]{kushner2003stochastic}
Kushner, H. and Yin, G.~G.
\newblock \emph{Stochastic approximation and recursive algorithms and
  applications}, volume~35.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Lagoudakis \& Parr(2003)Lagoudakis and Parr]{lagoudakis2003least}
Lagoudakis, M.~G. and Parr, R.
\newblock Least-squares policy iteration.
\newblock \emph{J. Mach. Learn. Res.}, 2003.

\bibitem[Lakshminarayanan \& Szepesv{\'{a}}ri(2018)Lakshminarayanan and
  Szepesv{\'{a}}ri]{lakshminarayanan2018linear}
Lakshminarayanan, C. and Szepesv{\'{a}}ri, C.
\newblock Linear stochastic approximation: How far does constant step-size and
  iterate averaging go?
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics, {AISTATS} 2018, 9-11 April 2018, Playa Blanca, Lanzarote, Canary
  Islands, Spain}, 2018.

\bibitem[Lazaric et~al.(2012)Lazaric, Ghavamzadeh, and
  Munos]{lazaric2012finite}
Lazaric, A., Ghavamzadeh, M., and Munos, R.
\newblock Finite-sample analysis of least-squares policy iteration.
\newblock \emph{Journal of Machine Learning Research}, 13:\penalty0 3041--3074,
  2012.

\bibitem[Lazaric et~al.(2016)Lazaric, Ghavamzadeh, and
  Munos]{lazaric2016analysis}
Lazaric, A., Ghavamzadeh, M., and Munos, R.
\newblock Analysis of classification-based policy iteration algorithms.
\newblock 2016.

\bibitem[Liang et~al.(2015)Liang, Machado, Talvitie, and
  Bowling]{liang2015state}
Liang, Y., Machado, M.~C., Talvitie, E., and Bowling, M.
\newblock State of the art control of atari games using shallow reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1512.01563}, 2015.

\bibitem[Marbach \& Tsitsiklis(2001)Marbach and
  Tsitsiklis]{marbach2001simulation}
Marbach, P. and Tsitsiklis, J.~N.
\newblock Simulation-based optimization of markov reward processes.
\newblock \emph{{IEEE} Trans. Autom. Control.}, 2001.

\bibitem[Melo et~al.(2008)Melo, Meyn, and Ribeiro]{melo2008analysis}
Melo, F.~S., Meyn, S.~P., and Ribeiro, M.~I.
\newblock An analysis of reinforcement learning with function approximation.
\newblock In \emph{Machine Learning, Proceedings of the Twenty-Fifth
  International Conference {(ICML} 2008), Helsinki, Finland, June 5-9, 2008},
  2008.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T.~P., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{Proceedings of the 33nd International Conference on Machine
  Learning, {ICML} 2016, New York City, NY, USA, June 19-24, 2016}, 2016.

\bibitem[Perkins \& Precup(2002)Perkins and Precup]{perkins2003convergent}
Perkins, T.~J. and Precup, D.
\newblock A convergent form of approximate policy iteration.
\newblock In \emph{Advances in Neural Information Processing Systems 15 [Neural
  Information Processing Systems, {NIPS} 2002, December 9-14, 2002, Vancouver,
  British Columbia, Canada]}, 2002.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{robbins1951stochastic}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, 1951.

\bibitem[Rummery \& Niranjan(1994)Rummery and Niranjan]{rummery1994line}
Rummery, G.~A. and Niranjan, M.
\newblock \emph{On-line Q-learning using connectionist systems}.
\newblock University of Cambridge, Department of Engineering Cambridge, UK,
  1994.

\bibitem[Singh et~al.(2000)Singh, Jaakkola, Littman, and
  Szepesv{\'{a}}ri]{singh2000convergence}
Singh, S.~P., Jaakkola, T.~S., Littman, M.~L., and Szepesv{\'{a}}ri, C.
\newblock Convergence results for single-step on-policy reinforcement-learning
  algorithms.
\newblock \emph{Mach. Learn.}, 2000.

\bibitem[Srikant \& Ying(2019)Srikant and Ying]{srikant2019finite}
Srikant, R. and Ying, L.
\newblock Finite-time error bounds for linear stochastic approximation andtd
  learning.
\newblock In \emph{Conference on Learning Theory}, pp.\  2803--2830. PMLR,
  2019.

\bibitem[Sutton(1988)]{sutton1988learning}
Sutton, R.~S.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Mach. Learn.}, 1988.

\bibitem[Sutton(1999)]{sutton1999open}
Sutton, R.~S.
\newblock Open theoretical questions in reinforcement learning.
\newblock In \emph{European Conference on Computational Learning Theory}, pp.\
  11--17. Springer, 1999.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement Learning: An Introduction (2nd Edition)}.
\newblock MIT press, 2018.

\bibitem[Tsitsiklis \& Roy(1996)Tsitsiklis and Roy]{tsitsiklis1997analysis}
Tsitsiklis, J.~N. and Roy, B.~V.
\newblock Analysis of temporal-diffference learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems 9, NIPS,
  Denver, CO, USA, December 2-5, 1996}, 1996.

\bibitem[Watkins(1989)]{watkins1989learning}
Watkins, C. J. C.~H.
\newblock \emph{Learning from delayed rewards}.
\newblock PhD thesis, King's College, Cambridge, 1989.

\bibitem[Wu et~al.(2020)Wu, Zhang, Xu, and Gu]{wu2020finite}
Wu, Y., Zhang, W., Xu, P., and Gu, Q.
\newblock A finite-time analysis of two time-scale actor-critic methods.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Yao, and Whiteson]{zhang2021breaking}
Zhang, S., Yao, H., and Whiteson, S.
\newblock Breaking the deadly triad with a target network.
\newblock \emph{Proceedings of the 38th International Conference on Machine
  Learning, {ICML} 2021, 18-24 July 2021, Virtual Event}, 2021.

\bibitem[Zhang et~al.(2022)Zhang, des Combes, and Laroche]{zhang2021global}
Zhang, S., des Combes, R.~T., and Laroche, R.
\newblock Global optimality and finite sample analysis of softmax off-policy
  actor critic under state distribution mismatch.
\newblock \emph{Journal of Machine Learning Research}, 2022.

\bibitem[Zou et~al.(2019)Zou, Xu, and Liang]{zou2019finite}
Zou, S., Xu, T., and Liang, Y.
\newblock Finite-sample analysis for {SARSA} with linear function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, 2019.

\end{thebibliography}
