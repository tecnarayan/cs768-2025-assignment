\begin{thebibliography}{10}

\bibitem{akopyan2015truenorth}
Filipp Akopyan, Jun Sawada, Andrew Cassidy, Rodrigo Alvarez-Icaza, John Arthur, Paul Merolla, Nabil Imam, Yutaka Nakamura, Pallab Datta, Gi-Joon Nam, et~al.
\newblock Truenorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip.
\newblock {\em IEEE transactions on computer-aided design of integrated circuits and systems}, 34(10):1537--1557, 2015.

\bibitem{bellec_solution_2020}
Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass.
\newblock A solution to the learning dilemma for recurrent networks of spiking neurons.
\newblock {\em Nature communications}, 11(1):3625, 2020.

\bibitem{bengio2013estimating}
Yoshua Bengio, Nicholas L{\'e}onard, and Aaron Courville.
\newblock Estimating or propagating gradients through stochastic neurons for conditional computation.
\newblock {\em arXiv preprint arXiv:1308.3432}, 2013.

\bibitem{bohnstingl_online_2023}
Thomas Bohnstingl, Stanis{\l}aw Wo{\'z}niak, Angeliki Pantazi, and Evangelos Eleftheriou.
\newblock Online spatio-temporal learning in deep neural networks.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems}, 2022.

\bibitem{brock2021high}
Andy Brock, Soham De, Samuel~L Smith, and Karen Simonyan.
\newblock High-performance large-scale image recognition without normalization.
\newblock In {\em International Conference on Machine Learning}, pages 1059--1071. PMLR, 2021.

\bibitem{bu2023rate}
Tong Bu, Jianhao Ding, Zecheng Hao, and Zhaofei Yu.
\newblock Rate gradient approximation attack threats deep spiking neural networks.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 7896--7906, 2023.

\bibitem{bu_optimal_2023}
Tong Bu, Wei Fang, Jianhao Ding, PengLin Dai, Zhaofei Yu, and Tiejun Huang.
\newblock Optimal ann-snn conversion for high-accuracy and ultra-low-latency spiking neural networks.
\newblock {\em arXiv preprint arXiv:2303.04347}, 2023.

\bibitem{cao_spiking_2015}
Yongqiang Cao, Yang Chen, and Deepak Khosla.
\newblock Spiking deep convolutional neural networks for energy-efficient object recognition.
\newblock {\em International Journal of Computer Vision}, 113:54--66, 2015.

\bibitem{cubuk2019autoaugment}
Ekin~D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V Le.
\newblock Autoaugment: Learning augmentation strategies from data.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 113--123, 2019.

\bibitem{davies2018loihi}
Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri~Harsha Choday, Georgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain, et~al.
\newblock Loihi: A neuromorphic manycore processor with on-chip learning.
\newblock {\em Ieee Micro}, 38(1):82--99, 2018.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern recognition}, pages 248--255. Ieee, 2009.

\bibitem{deng_optimal_2021}
Shikuang Deng and Shi Gu.
\newblock Optimal conversion of conventional artificial neural networks to spiking neural networks.
\newblock {\em arXiv preprint arXiv:2103.00476}, 2021.

\bibitem{deng_surrogate_2023}
Shikuang Deng, Hao Lin, Yuhang Li, and Shi Gu.
\newblock Surrogate module learning: Reduce the gradient error accumulation in training spiking neural networks.
\newblock In {\em International Conference on Machine Learning}, pages 7645--7657. PMLR, 2023.

\bibitem{devries2017improved}
Terrance DeVries and Graham~W Taylor.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock {\em arXiv preprint arXiv:1708.04552}, 2017.

\bibitem{diehl_fast-classifying_2015}
Peter~U Diehl, Daniel Neil, Jonathan Binas, Matthew Cook, Shih-Chii Liu, and Michael Pfeiffer.
\newblock Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing.
\newblock In {\em 2015 International joint conference on neural networks (IJCNN)}, pages 1--8. ieee, 2015.

\bibitem{ding2021optimal}
Jianhao Ding, Zhaofei Yu, Yonghong Tian, and Tiejun Huang.
\newblock Optimal ann-snn conversion for fast and accurate inference in deep spiking neural networks.
\newblock {\em arXiv preprint arXiv:2105.11654}, 2021.

\bibitem{duan_temporal_2022}
Chaoteng Duan, Jianhao Ding, Shiyan Chen, Zhaofei Yu, and Tiejun Huang.
\newblock Temporal effective batch normalization in spiking neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 35:34377--34390, 2022.

\bibitem{el-allami_securing_2021}
Rida El-Allami, Alberto Marchisio, Muhammad Shafique, and Ihsen Alouani.
\newblock Securing deep spiking neural networks against adversarial attacks through inherent structural parameters.
\newblock In {\em 2021 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)}, pages 774--779. IEEE, 2021.

\bibitem{fang_spikingjelly_2023}
Wei Fang, Yanqi Chen, Jianhao Ding, Zhaofei Yu, Timoth{\'e}e Masquelier, Ding Chen, Liwei Huang, Huihui Zhou, Guoqi Li, and Yonghong Tian.
\newblock Spikingjelly: An open-source machine learning infrastructure platform for spike-based intelligence.
\newblock {\em Science Advances}, 9(40):eadi1480, 2023.

\bibitem{fang_deep_2021}
Wei Fang, Zhaofei Yu, Yanqi Chen, Tiejun Huang, Timoth{\'e}e Masquelier, and Yonghong Tian.
\newblock Deep residual learning in spiking neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 34:21056--21069, 2021.

\bibitem{fang_incorporating_2021}
Wei Fang, Zhaofei Yu, Yanqi Chen, Timoth{\'e}e Masquelier, Tiejun Huang, and Yonghong Tian.
\newblock Incorporating learnable membrane time constant to enhance learning of spiking neural networks.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 2661--2671, 2021.

\bibitem{gu_stca_2019}
Pengjie Gu, Rong Xiao, Gang Pan, and Huajin Tang.
\newblock Stca: Spatio-temporal credit assignment with delayed feedback in deep spiking neural networks.
\newblock In {\em IJCAI}, volume~15, pages 1366--1372, 2019.

\bibitem{guo2021neural}
Wenzhe Guo, Mohammed~E Fouda, Ahmed~M Eltawil, and Khaled~Nabil Salama.
\newblock Neural coding in spiking neural networks: A comparative study for robust neuromorphic systems.
\newblock {\em Frontiers in Neuroscience}, 15:638474, 2021.

\bibitem{guo_im-loss_2022}
Yufei Guo, Yuanpei Chen, Liwen Zhang, Xiaode Liu, Yinglei Wang, Xuhui Huang, and Zhe Ma.
\newblock Im-loss: information maximization loss for spiking neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 35:156--166, 2022.

\bibitem{guo_direct_2023}
Yufei Guo, Xuhui Huang, and Zhe Ma.
\newblock Direct learning-based deep spiking neural networks: a review.
\newblock {\em Frontiers in Neuroscience}, 17:1209795, 2023.

\bibitem{han_deep_2020}
Bing Han and Kaushik Roy.
\newblock Deep spiking neural network: Energy efficiency through time based coding.
\newblock In {\em European Conference on Computer Vision}, pages 388--404. Springer, 2020.

\bibitem{han_rmp-snn_2020}
Bing Han, Gopalakrishnan Srinivasan, and Kaushik Roy.
\newblock Rmp-snn: Residual membrane potential neuron for enabling deeper high-accuracy and low-latency spiking neural network.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 13558--13567, 2020.

\bibitem{hao_reducing_2023}
Zecheng Hao, Tong Bu, Jianhao Ding, Tiejun Huang, and Zhaofei Yu.
\newblock Reducing ann-snn conversion error through residual membrane potential.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pages 11--21, 2023.

\bibitem{hao_threaten_2023}
Zecheng Hao, Tong Bu, Xinyu Shi, Zihan Huang, Zhaofei Yu, and Tiejun Huang.
\newblock Threaten spiking neural networks through combining rate and temporal information.
\newblock In {\em The Twelfth International Conference on Learning Representations}.

\bibitem{haothreaten}
Zecheng Hao, Tong Bu, Xinyu Shi, Zihan Huang, Zhaofei Yu, and Tiejun Huang.
\newblock Threaten spiking neural networks through combining rate and temporal information.
\newblock In {\em The Twelfth International Conference on Learning Representations}.

\bibitem{hao_bridging_2023}
Zecheng Hao, Jianhao Ding, Tong Bu, Tiejun Huang, and Zhaofei Yu.
\newblock Bridging the gap between anns and snns by calibrating offset spikes.
\newblock {\em arXiv preprint arXiv:2302.10685}, 2023.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778, 2016.

\bibitem{he2016identity}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Identity mappings in deep residual networks.
\newblock In {\em Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV 14}, pages 630--645. Springer, 2016.

\bibitem{jiang_unified_2023}
Haiyan Jiang, Srinivas Anumasa, Giulia De~Masi, Huan Xiong, and Bin Gu.
\newblock A unified optimization framework of ann-snn conversion: Towards optimal mapping from activation values to firing rates.
\newblock In {\em International Conference on Machine Learning}, pages 14945--14974. PMLR, 2023.

\bibitem{kim_unifying_2020}
Jinseok Kim, Kyungsu Kim, and Jae-Joon Kim.
\newblock Unifying activation-and timing-based learning rules for spiking neural networks.
\newblock {\em Advances in neural information processing systems}, 33:19534--19544, 2020.

\bibitem{kim_spiking-yolo_2020}
Seijoon Kim, Seongsik Park, Byunggook Na, and Sungroh Yoon.
\newblock Spiking-yolo: spiking neural network for energy-efficient object detection.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~34, pages 11270--11277, 2020.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{kundu_hire-snn_2021}
Souvik Kundu, Massoud Pedram, and Peter~A Beerel.
\newblock Hire-snn: Harnessing the inherent robustness of energy-efficient deep spiking neural networks by training with crafted input noise.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 5209--5218, 2021.

\bibitem{lee_training_2016}
Jun~Haeng Lee, Tobi Delbruck, and Michael Pfeiffer.
\newblock Training deep spiking neural networks using backpropagation.
\newblock {\em Frontiers in neuroscience}, 10:228000, 2016.

\bibitem{li_quantization_2022}
Chen Li, Lei Ma, and Steve Furber.
\newblock Quantization framework for fast spiking neural networks.
\newblock {\em Frontiers in Neuroscience}, 16:918793, 2022.

\bibitem{li2017cifar10}
Hongmin Li, Hanchao Liu, Xiangyang Ji, Guoqi Li, and Luping Shi.
\newblock Cifar10-dvs: an event-stream dataset for object classification.
\newblock {\em Frontiers in neuroscience}, 11:244131, 2017.

\bibitem{li_free_2021}
Yuhang Li, Shikuang Deng, Xin Dong, Ruihao Gong, and Shi Gu.
\newblock A free lunch from ann: Towards efficient, accurate spiking neural networks calibration.
\newblock In {\em International conference on machine learning}, pages 6316--6325. PMLR, 2021.

\bibitem{li_differentiable_2021}
Yuhang Li, Yufei Guo, Shanghang Zhang, Shikuang Deng, Yongqing Hai, and Shi Gu.
\newblock Differentiable spike: Rethinking gradient-descent for training spiking neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 34:23426--23439, 2021.

\bibitem{li_uncovering_2023}
Yuhang Li, Youngeun Kim, Hyoungseob Park, and Priyadarshini Panda.
\newblock Uncovering the representation of spiking neural networks trained with surrogate gradient.
\newblock {\em arXiv preprint arXiv:2304.13098}, 2023.

\bibitem{loshchilov2016sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock {\em arXiv preprint arXiv:1608.03983}, 2016.

\bibitem{maass_networks_1997}
Wolfgang Maass.
\newblock Networks of spiking neurons: the third generation of neural network models.
\newblock {\em Neural networks}, 10(9):1659--1671, 1997.

\bibitem{meng2022training}
Qingyan Meng, Mingqing Xiao, Shen Yan, Yisen Wang, Zhouchen Lin, and Zhi-Quan Luo.
\newblock Training high-performance low-latency spiking neural networks by differentiation on spike representation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 12444--12453, 2022.

\bibitem{meng2023towards}
Qingyan Meng, Mingqing Xiao, Shen Yan, Yisen Wang, Zhouchen Lin, and Zhi-Quan Luo.
\newblock Towards memory-and time-efficient backpropagation for training spiking neural networks.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 6166--6176, 2023.

\bibitem{mostafa_supervised_2018}
Hesham Mostafa.
\newblock Supervised learning based on temporal coding in spiking neural networks.
\newblock {\em IEEE transactions on neural networks and learning systems}, 29(7):3227--3235, 2017.

\bibitem{mukhoty2023certified}
Bhaskar Mukhoty, Hilal AlQuabeh, Giulia De~Masi, Huan Xiong, and Bin Gu.
\newblock Certified adversarial robustness for rate encoded spiking neural networks.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{neftci_surrogate_2019}
Emre~O Neftci, Hesham Mostafa, and Friedemann Zenke.
\newblock Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks.
\newblock {\em IEEE Signal Processing Magazine}, 36(6):51--63, 2019.

\bibitem{panzeri_unified_2001}
Stefano Panzeri and Simon~R Schultz.
\newblock A unified approach to the study of temporal, correlational, and rate coding.
\newblock {\em Neural Computation}, 13(6):1311--1349, 2001.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{pei2019towards}
Jing Pei, Lei Deng, Sen Song, Mingguo Zhao, Youhui Zhang, Shuang Wu, Guanrui Wang, Zhe Zou, Zhenzhi Wu, Wei He, et~al.
\newblock Towards artificial general intelligence with hybrid tianjic chip architecture.
\newblock {\em Nature}, 572(7767):106--111, 2019.

\bibitem{rathi_diet-snn_2023}
Nitin Rathi and Kaushik Roy.
\newblock Diet-snn: A low-latency spiking neural network with direct input encoding and leakage and threshold optimization.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems}, 34(6):3174--3182, 2021.

\bibitem{roy2019towards}
Kaushik Roy, Akhilesh Jaiswal, and Priyadarshini Panda.
\newblock Towards spike-based machine intelligence with neuromorphic computing.
\newblock {\em Nature}, 575(7784):607--617, 2019.

\bibitem{rueckauer_conversion_2017}
Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer, and Shih-Chii Liu.
\newblock Conversion of continuous-valued deep networks to efficient event-driven networks for image classification.
\newblock {\em Frontiers in neuroscience}, 11:294078, 2017.

\bibitem{rumelhart1986learning}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning representations by back-propagating errors.
\newblock {\em nature}, 323(6088):533--536, 1986.

\bibitem{sengupta_going_2019}
Abhronil Sengupta, Yuting Ye, Robert Wang, Chiao Liu, and Kaushik Roy.
\newblock Going deeper in spiking neural networks: Vgg and residual architectures.
\newblock {\em Frontiers in neuroscience}, 13:95, 2019.

\bibitem{sharmin_inherent_2020}
Saima Sharmin, Nitin Rathi, Priyadarshini Panda, and Kaushik Roy.
\newblock Inherent adversarial robustness of deep spiking neural networks: Effects of discrete input encoding and non-linear activations.
\newblock In {\em Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXIX 16}, pages 399--414. Springer, 2020.

\bibitem{shen_exploiting_2023}
Guobin Shen, Dongcheng Zhao, and Yi~Zeng.
\newblock Exploiting high performance spiking neural networks with efficient spiking patterns.
\newblock {\em arXiv preprint arXiv:2301.12356}, 2023.

\bibitem{shrestha_slayer_2018}
Sumit~B Shrestha and Garrick Orchard.
\newblock Slayer: Spike layer error reassignment in time.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{srivastava2017motor}
Kyle~H Srivastava, Caroline~M Holmes, Michiel Vellema, Andrea~R Pack, Coen~PH Elemans, Ilya Nemenman, and Samuel~J Sober.
\newblock Motor control by precisely timed spike patterns.
\newblock {\em Proceedings of the National Academy of Sciences}, 114(5):1171--1176, 2017.

\bibitem{stockl_optimized_2021}
Christoph St{\"o}ckl and Wolfgang Maass.
\newblock Optimized spiking neurons can classify images with high accuracy through temporal coding with two spikes.
\newblock {\em Nature Machine Intelligence}, 3(3):230--238, 2021.

\bibitem{suetake_smathmsup_2023}
Kazuma Suetake, Shin-ichi Ikegawa, Ryuji Saiin, and Yoshihide Sawada.
\newblock S3nn: Time step reduction of spiking surrogate gradients for training energy efficient single-step spiking neural networks.
\newblock {\em Neural Networks}, 159:208--219, 2023.

\bibitem{thiele_spikegrad_2019}
Johannes~Christian Thiele, Olivier Bichler, and Antoine Dupret.
\newblock Spikegrad: An ann-equivalent computation model for implementing backpropagation with spikes.
\newblock {\em arXiv preprint arXiv:1906.00851}, 2019.

\bibitem{wang2023ssf}
Jingtao Wang, Zengjie Song, Yuxi Wang, Jun Xiao, Yuran Yang, Shuqi Mei, and Zhaoxiang Zhang.
\newblock Ssf: Accelerating training of spiking neural networks with stabilized spiking flow.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 5982--5991, 2023.

\bibitem{wang_adaptive_2023}
Ziming Wang, Runhao Jiang, Shuang Lian, Rui Yan, and Huajin Tang.
\newblock Adaptive smoothing gradient learning for spiking neural networks.
\newblock In {\em International Conference on Machine Learning}, pages 35798--35816. PMLR, 2023.

\bibitem{wang_towards_2024}
Ziming Wang, Shuang Lian, Yuhao Zhang, Xiaoxin Cui, Rui Yan, and Huajin Tang.
\newblock Towards lossless ann-snn conversion under ultra-low latency with dual-phase optimization.
\newblock {\em arXiv preprint arXiv:2205.07473}, 2022.

\bibitem{williams_learning_1989}
Ronald~J Williams and David Zipser.
\newblock A learning algorithm for continually running fully recurrent neural networks.
\newblock {\em Neural computation}, 1(2):270--280, 1989.

\bibitem{wu_training_2021}
Hao Wu, Yueyi Zhang, Wenming Weng, Yongting Zhang, Zhiwei Xiong, Zheng-Jun Zha, Xiaoyan Sun, and Feng Wu.
\newblock Training spiking neural networks with accumulated spiking flow.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~35, pages 10320--10328, 2021.

\bibitem{wu_tandem_2023}
Jibin Wu, Yansong Chua, Malu Zhang, Guoqi Li, Haizhou Li, and Kay~Chen Tan.
\newblock A tandem learning rule for effective training and rapid inference of deep spiking neural networks.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems}, 34(1):446--460, 2021.

\bibitem{wu_spatio-temporal_2018}
Yujie Wu, Lei Deng, Guoqi Li, and Luping Shi.
\newblock Spatio-temporal backpropagation for training high-performance spiking neural networks.
\newblock {\em Frontiers in neuroscience}, 12:323875, 2018.

\bibitem{wunderlich_event-based_2021}
Timo~C Wunderlich and Christian Pehle.
\newblock Event-based backpropagation can compute exact gradients for spiking neural networks.
\newblock {\em Scientific Reports}, 11(1):12829, 2021.

\bibitem{xiao2022online}
Mingqing Xiao, Qingyan Meng, Zongpeng Zhang, Di~He, and Zhouchen Lin.
\newblock Online training through time for spiking neural networks.
\newblock {\em Advances in neural information processing systems}, 35:20717--20730, 2022.

\bibitem{xiao2021training}
Mingqing Xiao, Qingyan Meng, Zongpeng Zhang, Yisen Wang, and Zhouchen Lin.
\newblock Training feedback spiking neural networks by implicit differentiation on the equilibrium state.
\newblock {\em Advances in neural information processing systems}, 34:14516--14528, 2021.

\bibitem{yan_near_2021}
Zhanglu Yan, Jun Zhou, and Weng-Fai Wong.
\newblock Near lossless transfer learning for spiking neural networks.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~35, pages 10577--10584, 2021.

\bibitem{yao_temporal-wise_2021}
Man Yao, Huanhuan Gao, Guangshe Zhao, Dingheng Wang, Yihan Lin, Zhaoxu Yang, and Guoqi Li.
\newblock Temporal-wise attention spiking neural networks for event streams classification.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 10221--10230, 2021.

\bibitem{yao_glif_2022}
Xingting Yao, Fanrong Li, Zitao Mo, and Jian Cheng.
\newblock Glif: A unified gated leaky integrate-and-fire neuron for spiking neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 35:32160--32171, 2022.

\bibitem{yin_effective_2020}
Bojian Yin, Federico Corradi, and Sander~M Boht{\'e}.
\newblock Effective and efficient computation with multiple-timescale spiking recurrent neural networks.
\newblock In {\em International Conference on Neuromorphic Systems 2020}, pages 1--8, 2020.

\bibitem{yin_accurate_2023}
Bojian Yin, Federico Corradi, and Sander~M Boht{\'e}.
\newblock Accurate online training of dynamical spiking neural networks through forward propagation through time.
\newblock {\em Nature Machine Intelligence}, 5(5):518--527, 2023.

\bibitem{yu_stsc-snn_2022}
Chengting Yu, Zheming Gu, Da~Li, Gaoang Wang, Aili Wang, and Erping Li.
\newblock Stsc-snn: Spatio-temporal synaptic connection with temporal convolution and attention for spiking neural networks.
\newblock {\em Frontiers in Neuroscience}, 16:1079357, 2022.

\bibitem{zenke_superspike_2018}
Friedemann Zenke and Surya Ganguli.
\newblock Superspike: Supervised learning in multilayer spiking neural networks.
\newblock {\em Neural computation}, 30(6):1514--1541, 2018.

\bibitem{zenke_remarkable_2021}
Friedemann Zenke and Tim~P Vogels.
\newblock The remarkable robustness of surrogate gradient learning for instilling complex function in spiking neural networks.
\newblock {\em Neural computation}, 33(4):899--925, 2021.

\bibitem{zhang_temporal_2020}
Wenrui Zhang and Peng Li.
\newblock Temporal spike sequence learning via backpropagation for deep spiking neural networks.
\newblock {\em Advances in neural information processing systems}, 33:12022--12033, 2020.

\bibitem{zheng_going_2021}
Hanle Zheng, Yujie Wu, Lei Deng, Yifan Hu, and Guoqi Li.
\newblock Going deeper with directly-trained larger spiking neural networks.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~35, pages 11062--11070, 2021.

\bibitem{zhou_temporal-coded_2021}
Shibo Zhou, Xiaohua Li, Ying Chen, Sanjeev~T Chandrasekaran, and Arindam Sanyal.
\newblock Temporal-coded deep spiking neural network with easy training and robust performance.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~35, pages 11143--11151, 2021.

\bibitem{zhu_online_2023}
Yaoyu Zhu, Jianhao Ding, Tiejun Huang, Xiaodong Xie, and Zhaofei Yu.
\newblock Online stabilization of spiking neural networks.
\newblock In {\em The Twelfth International Conference on Learning Representations}.

\end{thebibliography}
