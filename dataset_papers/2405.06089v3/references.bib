@inproceedings{tian_toward_2023,
  title={Toward Understanding Latent Model Learning in MuZero: A Case Study in Linear Quadratic Gaussian Control},
  author={Tian, Yi and Zhang, Kaiqing and Tedrake, Russ and Sra, Suvrit},
  booktitle={ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems},
  year={2023}
}
@inproceedings{lee_improved_2022,
	title = {Improved rates for prediction and identification of partially observed linear dynamical systems},
	url = {https://proceedings.mlr.press/v167/lee22a.html},
	abstract = {Identification of a linear time-invariant dynamical system from partial observations is a fundamental problem in control theory. Particularly challenging are systems exhibiting long-term memory. A natural question is how learn such systems with non-asymptotic statistical rates depending on the inherent dimensionality (order) ùëëdd of the system, rather than on the possibly much larger memory length. We propose an algorithm that given a single trajectory of length ùëáTT with gaussian observation noise, learns the system with a near-optimal rate of ùëÇÀú(ùëëùëá‚Äæ‚Äæ‚àö)O{\textasciitilde}(dT){\textbackslash}widetilde O{\textbackslash}left({\textbackslash}sqrt{\textbackslash}frac\{d\}\{T\}{\textbackslash}right) in Óà¥2H2{\textbackslash}mathcal\{H\}\_2 error, with only logarithmic, rather than polynomial dependence on memory length. We also give bounds under process noise and improved bounds for learning a realization of the system. Our algorithm is based on multi-scale low-rank approximation: SVD applied to Hankel matrices of geometrically increasing sizes. Our analysis relies on careful application of concentration bounds on the Fourier domain‚Äîwe give sharper concentration bounds for sample covariance of correlated inputs and for Óà¥‚àûH‚àû{\textbackslash}mathcal H\_{\textbackslash}infty norm estimation, which may be of independent interest.},
	language = {en},
	urldate = {2024-06-05},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Algorithmic} {Learning} {Theory}},
	publisher = {PMLR},
	author = {Lee, Holden},
	month = mar,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {668--698},
}

@article{saul_introduction_2001,
  title={An introduction to locally linear embedding},
  author={Saul, Lawrence K and Roweis, Sam T},
  journal={unpublished. Available at: http://www. cs. toronto. edu/\~{} roweis/lle/publications. html},
  year={2000}
}

@article{talebi_data-driven_2023,
	title = {Data-driven {Optimal} {Filtering} for {Linear} {Systems} with {Unknown} {Noise} {Covariances}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/dbe8185809cb7032ec7ec6e365e3ed3b-Abstract-Conference.html},
	language = {en},
	urldate = {2024-06-05},
	journal = {Advances in Neural Information Processing Systems},
	author = {Talebi, Shahriar and Taghvaei, Amirhossein and Mesbahi, Mehran},
	month = dec,
	year = {2023},
	pages = {69546--69585},
}

@misc{lale_adaptive_2020,
	title = {Adaptive {Control} and {Regret} {Minimization} in {Linear} {Quadratic} {Gaussian} ({LQG}) {Setting}},
	url = {http://arxiv.org/abs/2003.05999},
	doi = {10.48550/arXiv.2003.05999},
	abstract = {We study the problem of adaptive control in partially observable linear quadratic Gaussian control systems, where the model dynamics are unknown a priori. We propose LqgOpt, a novel reinforcement learning algorithm based on the principle of optimism in the face of uncertainty, to effectively minimize the overall control cost. We employ the predictor state evolution representation of the system dynamics and deploy a recently proposed closed-loop system identification method, estimation, and confidence bound construction. LqgOpt efficiently explores the system dynamics, estimates the model parameters up to their confidence interval, and deploys the controller of the most optimistic model for further exploration and exploitation. We provide stability guarantees for LqgOpt and prove the regret upper bound of \${\textbackslash}tilde\{{\textbackslash}mathcal\{O\}\}({\textbackslash}sqrt\{T\})\$ for adaptive control of linear quadratic Gaussian (LQG) systems, where \$T\$ is the time horizon of the problem.},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Lale, Sahin and Azizzadenesheli, Kamyar and Hassibi, Babak and Anandkumar, Anima},
	month = jun,
	year = {2020},
	note = {arXiv:2003.05999 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@misc{lale_logarithmic_2020,
	title = {Logarithmic {Regret} {Bound} in {Partially} {Observable} {Linear} {Dynamical} {Systems}},
	url = {http://arxiv.org/abs/2003.11227},
	doi = {10.48550/arXiv.2003.11227},
	abstract = {We study the problem of system identification and adaptive control in partially observable linear dynamical systems. Adaptive and closed-loop system identification is a challenging problem due to correlations introduced in data collection. In this paper, we present the first model estimation method with finite-time guarantees in both open and closed-loop system identification. Deploying this estimation method, we propose adaptive control online learning (AdaptOn), an efficient reinforcement learning algorithm that adaptively learns the system dynamics and continuously updates its controller through online learning steps. AdaptOn estimates the model dynamics by occasionally solving a linear regression problem through interactions with the environment. Using policy re-parameterization and the estimated model, AdaptOn constructs counterfactual loss functions to be used for updating the controller through online gradient descent. Over time, AdaptOn improves its model estimates and obtains more accurate gradient updates to improve the controller. We show that AdaptOn achieves a regret upper bound of \${\textbackslash}text\{polylog\}{\textbackslash}left(T{\textbackslash}right)\$, after \$T\$ time steps of agent-environment interaction. To the best of our knowledge, AdaptOn is the first algorithm that achieves \${\textbackslash}text\{polylog\}{\textbackslash}left(T{\textbackslash}right)\$ regret in adaptive control of unknown partially observable linear dynamical systems which includes linear quadratic Gaussian (LQG) control.},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Lale, Sahin and Azizzadenesheli, Kamyar and Hassibi, Babak and Anandkumar, Anima},
	month = jun,
	year = {2020},
	note = {arXiv:2003.11227 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@misc{mania_certainty_2019,
	title = {Certainty {Equivalence} is {Efficient} for {Linear} {Quadratic} {Control}},
	url = {http://arxiv.org/abs/1902.07826},
	abstract = {We study the performance of the certainty equivalent controller on Linear Quadratic (LQ) control problems with unknown transition dynamics. We show that for both the fully and partially observed settings, the sub-optimality gap between the cost incurred by playing the certainty equivalent controller on the true system and the cost incurred by using the optimal LQ controller enjoys a fast statistical rate, scaling as the square of the parameter error. To the best of our knowledge, our result is the Ô¨Årst sub-optimality guarantee in the partially observed Linear Quadratic Gaussian (LQG) setting. Furthermore, in the fully observed Linear Quadratic Regulator (LQR), our result improves upon recent work by Dean et al. [10], who present an algorithm achieving a sub-optimality gap linear in the parameter error. A key part of our analysis relies on perturbation bounds for discrete Riccati equations. We provide two new perturbation bounds, one that expands on an existing result from Konstantinov et al. [24], and another based on a new elementary proof strategy.},
	language = {en},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Mania, Horia and Tu, Stephen and Recht, Benjamin},
	month = jun,
	year = {2019},
	note = {arXiv:1902.07826 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@misc{djehiche_efficient_2022,
	title = {Efficient learning of hidden state {LTI} state space models of unknown order},
	url = {http://arxiv.org/abs/2202.01625},
	doi = {10.48550/arXiv.2202.01625},
	abstract = {The aim of this paper is to address two related estimation problems arising in the setup of hidden state linear time invariant (LTI) state space systems when the dimension of the hidden state is unknown. Namely, the estimation of any finite number of the system's Markov parameters and the estimation of a minimal realization for the system, both from the partial observation of a single trajectory. For both problems, we provide statistical guarantees in the form of various estimation error upper bounds, \${\textbackslash}rank\$ recovery conditions, and sample complexity estimates. Specifically, we first show that the low \${\textbackslash}rank\$ solution of the Hankel penalized least square estimator satisfies an estimation error in \$S\_p\$-norms for \$p {\textbackslash}in [1,2]\$ that captures the effect of the system order better than the existing operator norm upper bound for the simple least square. We then provide a stability analysis for an estimation procedure based on a variant of the Ho-Kalman algorithm that improves both the dependence on the dimension and the least singular value of the Hankel matrix of the Markov parameters. Finally, we propose an estimation algorithm for the minimal realization that uses both the Hankel penalized least square estimator and the Ho-Kalman based estimation procedure and guarantees with high probability that we recover the correct order of the system and satisfies a new fast rate in the \$S\_2\$-norm with a polynomial reduction in the dependence on the dimension and other parameters of the problem.},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Djehiche, Boualem and Mazhar, Othmane},
	month = feb,
	year = {2022},
	note = {arXiv:2202.01625 [math, stat]},
	keywords = {62J07, 62M05, 62M10, 62M15, 60E15, 62C20, 62F10, Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{simchowitz_learning_2019,
	title = {Learning {Linear} {Dynamical} {Systems} with {Semi}-{Parametric} {Least} {Squares}},
	url = {http://arxiv.org/abs/1902.00768},
	doi = {10.48550/arXiv.1902.00768},
	abstract = {We analyze a simple prefiltered variation of the least squares estimator for the problem of estimation with biased, semi-parametric noise, an error model studied more broadly in causal statistics and active learning. We prove an oracle inequality which demonstrates that this procedure provably mitigates the variance introduced by long-term dependencies. We then demonstrate that prefiltered least squares yields, to our knowledge, the first algorithm that provably estimates the parameters of partially-observed linear systems that attains rates which do not not incur a worst-case dependence on the rate at which these dependencies decay. The algorithm is provably consistent even for systems which satisfy the weaker marginal stability condition obeyed by many classical models based on Newtonian mechanics. In this context, our semi-parametric framework yields guarantees for both stochastic and worst-case noise.},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Simchowitz, Max and Boczar, Ross and Recht, Benjamin},
	month = feb,
	year = {2019},
	note = {arXiv:1902.00768 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{tian_can_2024,
	title = {Can {Direct} {Latent} {Model} {Learning} {Solve} {Linear} {Quadratic} {Gaussian} {Control}?},
	url = {http://arxiv.org/abs/2212.14511},
	doi = {10.48550/arXiv.2212.14511},
	abstract = {We study the task of learning state representations from potentially high-dimensional observations, with the goal of controlling an unknown partially observable system. We pursue a direct latent model learning approach, where a dynamic model in some latent state space is learned by predicting quantities directly related to planning (e.g., costs) without reconstructing the observations. In particular, we focus on an intuitive cost-driven state representation learning method for solving Linear Quadratic Gaussian (LQG) control, one of the most fundamental partially observable control problems. As our main results, we establish finite-sample guarantees of finding a near-optimal state representation function and a near-optimal controller using the directly learned latent model. To the best of our knowledge, despite various empirical successes, prior to this work it was unclear if such a cost-driven latent model learner enjoys finite-sample guarantees. Our work underscores the value of predicting multi-step costs, an idea that is key to our theory, and notably also an idea that is known to be empirically valuable for learning state representations.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Tian, Yi and Zhang, Kaiqing and Tedrake, Russ and Sra, Suvrit},
	month = mar,
	year = {2024},
	note = {arXiv:2212.14511 [cs, eess, math, stat]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{balzano_streaming_2018,
	title = {Streaming {PCA} and {Subspace} {Tracking}: {The} {Missing} {Data} {Case}},
	volume = {106},
	issn = {1558-2256},
	shorttitle = {Streaming {PCA} and {Subspace} {Tracking}},
	url = {https://ieeexplore.ieee.org/document/8417980},
	doi = {10.1109/JPROC.2018.2847041},
	abstract = {For many modern applications in science and engineering, data are collected in a streaming fashion carrying time-varying information, and practitioners need to process them with a limited amount of memory and computational resources in a timely manner for decision making. This often is coupled with the missing data problem, such that only a small fraction of data attributes are observed. These complications impose significant, and unconventional, constraints on the problem of streaming principal component analysis (PCA) and subspace tracking, which is an essential building block for many inference tasks in signal processing and machine learning. This survey article reviews a variety of classical and recent algorithms for solving this problem with low computational and memory complexities, particularly those applicable in the big data regime with missing data. We illustrate that streaming PCA and subspace tracking algorithms can be understood through algebraic and geometric perspectives, and they need to be adjusted carefully to handle missing data. Both asymptotic and nonasymptotic convergence guarantees are reviewed. Finally, we benchmark the performance of several competitive algorithms in the presence of missing data for both well-conditioned and ill-conditioned systems.},
	number = {8},
	urldate = {2024-04-29},
	journal = {Proceedings of the IEEE},
	author = {Balzano, Laura and Chi, Yuejie and Lu, Yue M.},
	month = aug,
	year = {2018},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Complexity theory, Machine learning algorithms, Missing data, Principal component analysis, Radar tracking, Signal processing, Signal processing algorithms, Statistical analysis, Time-varying systems, ordinary differential equation (ODE) analysis, streaming principal component analysis (PCA), subspace and low-rank models, subspace tracking},
	pages = {1293--1310},
}

@misc{ziemann_tutorial_2023,
	title = {A {Tutorial} on the {Non}-{Asymptotic} {Theory} of {System} {Identification}},
	url = {http://arxiv.org/abs/2309.03873},
	abstract = {This tutorial serves as an introduction to recently developed non-asymptotic methods in the theory of -- mainly linear -- system identification. We emphasize tools we deem particularly useful for a range of problems in this domain, such as the covering technique, the Hanson-Wright Inequality and the method of self-normalized martingales. We then employ these tools to give streamlined proofs of the performance of various least-squares based estimators for identifying the parameters in autoregressive models. We conclude by sketching out how the ideas presented herein can be extended to certain nonlinear identification problems.},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Ziemann, Ingvar and Tsiamis, Anastasios and Lee, Bruce and Jedra, Yassir and Matni, Nikolai and Pappas, George J.},
	month = sep,
	year = {2023},
	note = {arXiv:2309.03873 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Statistics - Machine Learning},
}

@inproceedings{tian_toward_2023-1,
	title = {Toward {Understanding} {State} {Representation} {Learning} in {MuZero}: {A} {Case} {Study} in {Linear} {Quadratic} {Gaussian} {Control}},
	shorttitle = {Toward {Understanding} {State} {Representation} {Learning} in {MuZero}},
	url = {https://ieeexplore.ieee.org/abstract/document/10383754},
	doi = {10.1109/CDC49753.2023.10383754},
	abstract = {We study the problem of representation learning for control from partial and potentially high-dimensional observations. We approach this problem via direct latent model learning, where one directly learns a dynamical model in some latent state space by predicting costs. In particular, we establish finite-sample guarantees of finding a near-optimal representation function and a near-optimal controller using the directly learned latent model for infinite-horizon time-invariant Linear Quadratic Gaussian (LQG) control. A part of our approach to latent model learning closely resembles MuZero, a recent breakthrough in empirical reinforcement learning, in that it learns latent dynamics implicitly by predicting cumulative costs. A key technical contribution of this work is to prove persistency of excitation for a new stochastic process that arises from the analysis of quadratic regression in our approach.},
	urldate = {2024-03-21},
	booktitle = {2023 62nd {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Tian, Yi and Zhang, Kaiqing and Tedrake, Russ and Sra, Suvrit},
	month = dec,
	year = {2023},
	note = {ISSN: 2576-2370},
	keywords = {Aerospace electronics, Costs, Predictive models, Reinforcement learning, Representation learning, Stochastic processes},
	pages = {6166--6171},
}

@article{anderson_linear_2022,
	title = {Linear {System} {Challenges} of {Dynamic} {Factor} {Models}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2225-1146},
	url = {https://www.mdpi.com/2225-1146/10/4/35},
	doi = {10.3390/econometrics10040035},
	abstract = {A survey is provided dealing with the formulation of modelling problems for dynamic factor models, and the various algorithm possibilities for solving these modelling problems. Emphasis is placed on understanding requirements for the handling of errors, noting the relevance of the proposed application of the model, be it for example prediction or business cycle determination. Mixed frequency problems are also considered, in which certain entries of an underlying vector process are only available for measurement at a submultiple frequency of the original process. Certain classes of processes are shown to be generically identifiable, and others not to have this property.},
	language = {en},
	number = {4},
	urldate = {2024-02-03},
	journal = {Econometrics},
	author = {Anderson, Brian D. O. and Deistler, Manfred and Lippi, Marco},
	month = dec,
	year = {2022},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {dynamic factor model, high-dimensional time series, linear systems theory},
	pages = {35},
}

@book{stewart_matrix_1990,
	address = {Boston},
	series = {Computer science and scientific computing},
	title = {Matrix perturbation theory},
	isbn = {0-12-670230-6},
	language = {eng},
	publisher = {Academic Press},
	author = {Stewart, G. W. (Gilbert W.)},
	year = {1990},
	keywords = {Perturbation (Mathematics)},
}

@article{pandarinath_inferring_2018,
	title = {Inferring single-trial neural population dynamics using sequential auto-encoders},
	volume = {15},
	copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-018-0109-9},
	doi = {10.1038/s41592-018-0109-9},
	abstract = {Neuroscience is experiencing a revolution in which simultaneous recording of thousands of neurons is revealing population dynamics that are not apparent from single-neuron responses. This structure is typically extracted from data averaged across many trials, but deeper understanding requires studying phenomena detected in single trials, which is challenging due to incomplete sampling of the neural population, trial-to-trial variability, and fluctuations in action potential timing. We introduce latent factor analysis via dynamical systems, a deep learning method to infer latent dynamics from single-trial neural spiking data. When applied to a variety of macaque and human motor cortical datasets, latent factor analysis via dynamical systems accurately predicts observed behavioral variables, extracts precise firing rate estimates of neural dynamics on single trials, infers perturbations to those dynamics that correlate with behavioral choices, and combines data from non-overlapping recording sessions spanning months to improve inference of underlying dynamics.},
	language = {en},
	number = {10},
	urldate = {2024-02-01},
	journal = {Nature Methods},
	author = {Pandarinath, Chethan and O‚ÄôShea, Daniel J. and Collins, Jasmine and Jozefowicz, Rafal and Stavisky, Sergey D. and Kao, Jonathan C. and Trautmann, Eric M. and Kaufman, Matthew T. and Ryu, Stephen I. and Hochberg, Leigh R. and Henderson, Jaimie M. and Shenoy, Krishna V. and Abbott, L. F. and Sussillo, David},
	month = oct,
	year = {2018},
	note = {Number: 10
Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Machine learning, Motor control},
	pages = {805--815},
}

@article{pandarinath_inferring_2018-1,
	title = {Inferring single-trial neural population dynamics using sequential auto-encoders},
	volume = {15},
	copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-018-0109-9},
	doi = {10.1038/s41592-018-0109-9},
	abstract = {Neuroscience is experiencing a revolution in which simultaneous recording of thousands of neurons is revealing population dynamics that are not apparent from single-neuron responses. This structure is typically extracted from data averaged across many trials, but deeper understanding requires studying phenomena detected in single trials, which is challenging due to incomplete sampling of the neural population, trial-to-trial variability, and fluctuations in action potential timing. We introduce latent factor analysis via dynamical systems, a deep learning method to infer latent dynamics from single-trial neural spiking data. When applied to a variety of macaque and human motor cortical datasets, latent factor analysis via dynamical systems accurately predicts observed behavioral variables, extracts precise firing rate estimates of neural dynamics on single trials, infers perturbations to those dynamics that correlate with behavioral choices, and combines data from non-overlapping recording sessions spanning months to improve inference of underlying dynamics.},
	language = {en},
	number = {10},
	urldate = {2024-02-01},
	journal = {Nature Methods},
	author = {Pandarinath, Chethan and O‚ÄôShea, Daniel J. and Collins, Jasmine and Jozefowicz, Rafal and Stavisky, Sergey D. and Kao, Jonathan C. and Trautmann, Eric M. and Kaufman, Matthew T. and Ryu, Stephen I. and Hochberg, Leigh R. and Henderson, Jaimie M. and Shenoy, Krishna V. and Abbott, L. F. and Sussillo, David},
	month = oct,
	year = {2018},
	note = {Number: 10
Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Machine learning, Motor control},
	pages = {805--815},
}

@book{hespanha_linear_2018,
	title = {Linear {Systems} {Theory}: {Second} {Edition}},
	isbn = {978-1-4008-9008-8},
	shorttitle = {Linear {Systems} {Theory}},
	abstract = {A fully updated textbook on linear systems theoryLinear systems theory is the cornerstone of control theory and a well-established discipline that focuses on linear differential equations from the perspective of control and estimation. This updated second edition of Linear Systems Theory covers the subject's key topics in a unique lecture-style format, making the book easy to use for instructors and students. Jo√£o Hespanha looks at system representation, stability, controllability and state feedback, observability and state estimation, and realization theory. He provides the background for advanced modern control design techniques and feedback linearization and examines advanced foundational topics, such as multivariable poles and zeros and LQG/LQR. The textbook presents only the most essential mathematical derivations and places comments, discussion, and terminology in sidebars so that readers can follow the core material easily and without distraction. Annotated proofs with sidebars explain the techniques of proof construction, including contradiction, contraposition, cycles of implications to prove equivalence, and the difference between necessity and sufficiency. Annotated theoretical developments also use sidebars to discuss relevant commands available in MATLAB, allowing students to understand these tools. This second edition contains a large number of new practice exercises with solutions. Based on typical problems, these exercises guide students to succinct and precise answers, helping to clarify issues and consolidate knowledge. The book's balanced chapters can each be covered in approximately two hours of lecture time, simplifying course planning and student review. Easy-to-use textbook in unique lecture-style formatSidebars explain topics in further detailAnnotated proofs and discussions of MATLAB commandsBalanced chapters can each be taught in two hours of course lectureNew practice exercises with solutions included},
	language = {en},
	publisher = {Princeton University Press},
	author = {Hespanha, Jo√£o P.},
	month = feb,
	year = {2018},
	keywords = {Mathematics / Applied, Mathematics / Differential Equations / General, Technology \& Engineering / Electrical},
}

@article{rudelson_random_1999,
	title = {Random {Vectors} in the {Isotropic} {Position}},
	volume = {164},
	issn = {0022-1236},
	url = {https://www.sciencedirect.com/science/article/pii/S0022123698933845},
	doi = {10.1006/jfan.1998.3384},
	abstract = {Letybe a random vector in Rn, satisfyingEy‚äóy=id.LetMbe a natural number and lety1,¬†‚Ä¶,¬†yMbe independent copies ofy. We study the question of approximation of the identity operator by finite sums of the tensorsyi‚äóyi. We prove that for some absolute constantCE1M‚àëi=1Myi‚äóyi‚àíid‚©ΩC¬∑lognM¬∑(E‚Äñy‚ÄñlogM)1/logM,provided that the last expression is smaller than 1. We apply this estimate to improve a result of Bourgain concerning the number of random points needed to bring a convex body into a nearly isotropic position.},
	number = {1},
	urldate = {2024-01-31},
	journal = {Journal of Functional Analysis},
	author = {Rudelson, M.},
	month = may,
	year = {1999},
	pages = {60--72},
}

@article{rudelson_random_1999-1,
	title = {Random {Vectors} in the {Isotropic} {Position}},
	volume = {164},
	issn = {0022-1236},
	url = {https://www.sciencedirect.com/science/article/pii/S0022123698933845},
	doi = {10.1006/jfan.1998.3384},
	abstract = {Letybe a random vector in Rn, satisfyingEy‚äóy=id.LetMbe a natural number and lety1,¬†‚Ä¶,¬†yMbe independent copies ofy. We study the question of approximation of the identity operator by finite sums of the tensorsyi‚äóyi. We prove that for some absolute constantCE1M‚àëi=1Myi‚äóyi‚àíid‚©ΩC¬∑lognM¬∑(E‚Äñy‚ÄñlogM)1/logM,provided that the last expression is smaller than 1. We apply this estimate to improve a result of Bourgain concerning the number of random points needed to bring a convex body into a nearly isotropic position.},
	number = {1},
	urldate = {2024-01-31},
	journal = {Journal of Functional Analysis},
	author = {Rudelson, M.},
	month = may,
	year = {1999},
	pages = {60--72},
}

@article{stewart_perturbation_nodate,
	title = {Perturbation {Theory} for the {Singular} {Value} {Decomposition}},
	abstract = {The singular value decomposition has a number of applications in digital signal processing. However, the the decomposition must be computed from a matrix consisting of both signal and noise. It is therefore important to be able to assess the e ects of the noise on the singular values and singular vectors {\textbar} a problem in classical perturbation theory. In this paper we survey the perturbation theory of the singular value decomposition.},
	language = {en},
	author = {Stewart, G W},
}

@book{vershynin_high-dimensional_2018,
	address = {Cambridge},
	series = {Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}},
	title = {High-{Dimensional} {Probability}: {An} {Introduction} with {Applications} in {Data} {Science}},
	isbn = {978-1-108-41519-4},
	shorttitle = {High-{Dimensional} {Probability}},
	url = {https://www.cambridge.org/core/books/highdimensional-probability/797C466DA29743D2C8213493BD2D2102},
	abstract = {High-dimensional probability offers insight into the behavior of random vectors, random matrices, random subspaces, and objects used to quantify uncertainty in high dimensions. Drawing on ideas from probability, analysis, and geometry, it lends itself to applications in mathematics, statistics, theoretical computer science, signal processing, optimization, and more. It is the first to integrate theory, key tools, and modern applications of high-dimensional probability. Concentration inequalities form the core, and it covers both classical results such as Hoeffding's and Chernoff's inequalities and modern developments such as the matrix Bernstein's inequality. It then introduces the powerful methods based on stochastic processes, including such tools as Slepian's, Sudakov's, and Dudley's inequalities, as well as generic chaining and bounds based on VC dimension. A broad range of illustrations is embedded throughout, including classical and modern results for covariance estimation, clustering, networks, semidefinite programming, coding, dimension reduction, matrix completion, machine learning, compressed sensing, and sparse regression.},
	urldate = {2024-01-30},
	publisher = {Cambridge University Press},
	author = {Vershynin, Roman},
	year = {2018},
	doi = {10.1017/9781108231596},
}

@inproceedings{abbasi-yadkori_improved_2011,
	title = {Improved {Algorithms} for {Linear} {Stochastic} {Bandits}},
	volume = {24},
	url = {https://papers.nips.cc/paper_files/paper/2011/hash/e1d5be1c7f2f456670de3d53c7b54f4a-Abstract.html},
	abstract = {We improve the theoretical analysis and empirical performance of algorithms for the stochastic multi-armed bandit problem and the linear stochastic multi-armed bandit problem. In particular, we show that a simple modification of Auer‚Äôs UCB algorithm (Auer, 2002) achieves with high probability constant regret. More importantly, we modify and, consequently, improve the analysis of the algorithm for the for linear stochastic bandit problem studied by Auer (2002), Dani et al. (2008), Rusmevichientong and Tsitsiklis (2010), Li et al. (2010). Our modification improves the regret bound by a logarithmic factor, though experiments show a vast improvement. In both cases, the improvement stems from the construction of smaller confidence sets. For their construction we use a novel tail inequality for vector-valued martingales.},
	urldate = {2024-01-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Abbasi-yadkori, Yasin and P√°l, D√°vid and Szepesv√°ri, Csaba},
	year = {2011},
}

@book{boucheron_concentration_2013,
	address = {Oxford, New York},
	title = {Concentration {Inequalities}: {A} {Nonasymptotic} {Theory} of {Independence}},
	isbn = {978-0-19-953525-5},
	shorttitle = {Concentration {Inequalities}},
	abstract = {Concentration inequalities for functions of independent random variables is an area of probability theory that has witnessed a great revolution in the last few decades, and has applications in a wide variety of areas such as machine learning, statistics, discrete mathematics, and high-dimensional geometry. Roughly speaking, if a function of many independent random variables does not depend too much on any of the variables then it is concentrated in the sense that with high probability, it is close to its expected value. This book offers a host of inequalities to illustrate this rich theory in an accessible way by covering the key developments and applications in the field. The authors describe the interplay between the probabilistic structure (independence) and a variety of tools ranging from functional inequalities to transportation arguments to information theory. Applications to the study of empirical processes, random projections, random matrix theory, and threshold phenomena are also presented. A self-contained introduction to concentration inequalities, it includes a survey of concentration of sums of independent random variables, variance bounds, the entropy method, and the transportation method. Deep connections with isoperimetric problems are revealed whilst special attention is paid to applications to the supremum of empirical processes. Written by leading experts in the field and containing extensive exercise sections this book will be an invaluable resource for researchers and graduate students in mathematics, theoretical computer science, and engineering. 
            ,  
             Concentration inequalities for functions of independent random variables is an area of probability theory that has witnessed a great revolution in the last few decades, and has applications in a wide variety of areas such as machine learning, statistics, discrete mathematics, and high-dimensional geometry. Roughly speaking, if a function of many independent random variables does not depend too much on any of the variables then it is concentrated in the sense that with high probability, it is close to its expected value. This book offers a host of inequalities to illustrate this rich theory in an accessible way by covering the key developments and applications in the field. The authors describe the interplay between the probabilistic structure (independence) and a variety of tools ranging from functional inequalities to transportation arguments to information theory. Applications to the study of empirical processes, random projections, random matrix theory, and threshold phenomena are also presented. A self-contained introduction to concentration inequalities, it includes a survey of concentration of sums of independent random variables, variance bounds, the entropy method, and the transportation method. Deep connections with isoperimetric problems are revealed whilst special attention is paid to applications to the supremum of empirical processes. Written by leading experts in the field and containing extensive exercise sections this book will be an invaluable resource for researchers and graduate students in mathematics, theoretical computer science, and engineering.},
	publisher = {Oxford University Press},
	author = {Boucheron, Stephane and Lugosi, Gabor and Massart, {and} Pascal},
	month = mar,
	year = {2013},
}

@article{blanchard_statistical_2007,
	title = {Statistical properties of kernel principal component analysis},
	volume = {66},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-006-6895-9},
	doi = {10.1007/s10994-006-6895-9},
	abstract = {The main goal of this paper is to prove inequalities on the reconstruction error for kernel principal component analysis. With respect to previous work on this topic, our contribution is twofold: (1) we give bounds that explicitly take into account the empirical centering step in this algorithm, and (2) we show that a ‚Äúlocalized‚Äù approach allows to obtain more accurate bounds. In particular, we show faster rates of convergence towards the minimum reconstruction error; more precisely, we prove that the convergence rate can typically be faster than n‚àí1/2. We also obtain a new relative bound on the error.},
	language = {en},
	number = {2},
	urldate = {2024-01-29},
	journal = {Machine Learning},
	author = {Blanchard, Gilles and Bousquet, Olivier and Zwald, Laurent},
	month = mar,
	year = {2007},
	keywords = {Covariance operator, Fast convergence rates, Kernel integral operator, Kernel principal components analysis, Kernel spectrum estimation},
	pages = {259--294},
}

@inproceedings{dasgupta_learning_1999,
	title = {Learning mixtures of {Gaussians}},
	url = {https://ieeexplore.ieee.org/abstract/document/814639?casa_token=ndxOyuIcIpUAAAAA:JQBo67XjMspdAR_PrMnq8t9rkLvBwwMlDgqG_S9xnAz0_Vmpc9BDHjlZMvku0NiBvk82Trk},
	doi = {10.1109/SFFCS.1999.814639},
	abstract = {Mixtures of Gaussians are among the most fundamental and widely used statistical models. Current techniques for learning such mixtures from data are local search heuristics with weak performance guarantees. We present the first provably correct algorithm for learning a mixture of Gaussians. This algorithm is very simple and returns the true centers of the Gaussians to within the precision specified by the user with high probability. It runs in time only linear in the dimension of the data and polynomial in the number of Gaussians.},
	urldate = {2024-01-29},
	booktitle = {40th {Annual} {Symposium} on {Foundations} of {Computer} {Science} ({Cat}. {No}.{99CB37039})},
	author = {Dasgupta, S.},
	month = oct,
	year = {1999},
	note = {ISSN: 0272-5428},
	keywords = {Astrophysics, Clustering algorithms, Electrical capacitance tomography, Gaussian processes, Geology, History, Probability, Psychology, Read only memory, Statistics},
	pages = {634--644},
}

@misc{noauthor_subspace_nodate,
	title = {Subspace {Learning} with {Partial} {Information} - {The} {Weizmann} {Institute} of {Science}},
	url = {https://weizmann.esploro.exlibrisgroup.com/esploro/outputs/journalArticle/Subspace-Learning-with-Partial-Information/993264097503596},
	urldate = {2024-01-29},
}

@article{galbraith_estimation_2002,
	title = {Estimation of the {Vector} {Moving} {Average} {Model} by {Vector} {Autoregression}},
	volume = {21},
	issn = {0747-4938},
	url = {https://doi.org/10.1081/ETC-120014349},
	doi = {10.1081/ETC-120014349},
	abstract = {We examine a simple estimator for the multivariate moving average model based on vector autoregressive approximation. In finite samples the estimator has a bias which is low where roots of the characteristic equation are well away from the unit circle, and more substantial where one or more roots have modulus near unity. We show that the representation estimated by this multivariate technique is consistent and asymptotically invertible. This estimator has significant computational advantages over Maximum Likelihood, and more importantly may be more robust than ML to mis-specification of the vector moving average model. The estimation method is applied to a VMA model of wholesale and retail inventories, using Canadian data on inventory investment, and allows us to examine the propagation of shocks between the two classes of inventory.},
	number = {2},
	urldate = {2024-01-29},
	journal = {Econometric Reviews},
	author = {Galbraith, John W. and Ullah, Aman and Zinde-Walsh, Victoria},
	month = jan,
	year = {2002},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1081/ETC-120014349},
	keywords = {C12, C22, JEL Classification:, Vector autoregression, Vector moving average},
	pages = {205--219},
}

@article{breitung_dynamic_2006,
	title = {Dynamic factor models},
	volume = {90},
	issn = {1614-0176},
	url = {https://doi.org/10.1007/s10182-006-0219-z},
	doi = {10.1007/s10182-006-0219-z},
	abstract = {Factor models can cope with many variables without running into scarce degrees of freedom problems often faced in a regression-based analysis. In this article we review recent work on dynamic factor models that have become popular in macroeconomic policy analysis and forecasting. By means of an empirical application we demonstrate that these models turn out to be usefu in investigating macroeconomic problems.},
	language = {en},
	number = {1},
	urldate = {2024-01-29},
	journal = {Allgemeines Statistisches Archiv},
	author = {Breitung, J√∂rg and Eickmeier, Sandra},
	month = mar,
	year = {2006},
	keywords = {C33, C51, Principal components, dynamic factors, forecasting. JEL C13},
	pages = {27--42},
}

@article{breitung_dynamic_2006-1,
	title = {Dynamic factor models},
	volume = {90},
	issn = {1614-0176},
	url = {https://doi.org/10.1007/s10182-006-0219-z},
	doi = {10.1007/s10182-006-0219-z},
	abstract = {Factor models can cope with many variables without running into scarce degrees of freedom problems often faced in a regression-based analysis. In this article we review recent work on dynamic factor models that have become popular in macroeconomic policy analysis and forecasting. By means of an empirical application we demonstrate that these models turn out to be usefu in investigating macroeconomic problems.},
	language = {en},
	number = {1},
	urldate = {2024-01-29},
	journal = {Allgemeines Statistisches Archiv},
	author = {Breitung, J√∂rg and Eickmeier, Sandra},
	month = mar,
	year = {2006},
	keywords = {C33, C51, Principal components, dynamic factors, forecasting. JEL C13},
	pages = {27--42},
}

@article{anderson_linear_2022-1,
	title = {Linear {System} {Challenges} of {Dynamic} {Factor} {Models}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2225-1146},
	url = {https://www.mdpi.com/2225-1146/10/4/35},
	doi = {10.3390/econometrics10040035},
	abstract = {A survey is provided dealing with the formulation of modelling problems for dynamic factor models, and the various algorithm possibilities for solving these modelling problems. Emphasis is placed on understanding requirements for the handling of errors, noting the relevance of the proposed application of the model, be it for example prediction or business cycle determination. Mixed frequency problems are also considered, in which certain entries of an underlying vector process are only available for measurement at a submultiple frequency of the original process. Certain classes of processes are shown to be generically identifiable, and others not to have this property.},
	language = {en},
	number = {4},
	urldate = {2024-01-29},
	journal = {Econometrics},
	author = {Anderson, Brian D. O. and Deistler, Manfred and Lippi, Marco},
	month = dec,
	year = {2022},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {dynamic factor model, high-dimensional time series, linear systems theory},
	pages = {35},
}

@article{hallin_factor_2023,
	title = {Factor models for high-dimensional functional time series {I}: {Representation} results},
	volume = {44},
	copyright = {¬© 2023 John Wiley \& Sons Ltd.},
	issn = {1467-9892},
	shorttitle = {Factor models for high-dimensional functional time series {I}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jtsa.12676},
	doi = {10.1111/jtsa.12676},
	abstract = {In this article, which consists of two parts (Part I: representation results; Part II: estimation and forecasting methods), we set up the theoretical foundations for a high-dimensional functional factor model approach in the analysis of large cross-sections (panels) of functional time series (FTS). In Part I, we establish a representation result stating that, under mild assumptions on the covariance operator of the cross-section, we can represent each FTS as the sum of a common component driven by scalar factors loaded via functional loadings, and a mildly cross-correlated idiosyncratic component. Our model and theory are developed in a general Hilbert space setting that allows for mixed panels of functional and scalar time series. We then turn, in Part II, to the identification of the number of factors, and the estimation of the factors, their loadings, and the common components. We provide a family of information criteria for identifying the number of factors, and prove their consistency. We provide average error bounds for the estimators of the factors, loadings, and common components; our results encompass the scalar case, for which they reproduce and extend, under weaker conditions, well-established similar results. Under slightly stronger assumptions, we also provide uniform bounds for the estimators of factors, loadings, and common components, thus extending existing scalar results. Our consistency results in the asymptotic regime where the number N of series and the number T of time observations diverge thus extend to the functional context the ‚Äòblessing of dimensionality‚Äô that explains the success of factor models in the analysis of high-dimensional (scalar) time series. We provide numerical illustrations that corroborate the convergence rates predicted by the theory, and provide a finer understanding of the interplay between N and T for estimation purposes. We conclude with an application to forecasting mortality curves, where we demonstrate that our approach outperforms existing methods.},
	language = {en},
	number = {5-6},
	urldate = {2024-01-29},
	journal = {Journal of Time Series Analysis},
	author = {Hallin, Marc and Nisol, Gilles and Tavakoli, Shahin},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jtsa.12676},
	keywords = {Functional time series, factor model, functional data analysis, high-dimensional time series, panel data},
	pages = {578--600},
}

@article{qin_latent_2022,
	title = {Latent vector autoregressive modeling and feature analysis of high dimensional and noisy data from dynamic systems},
	volume = {68},
	issn = {0001-1541},
	url = {https://ui.adsabs.harvard.edu/abs/2022AIChE..68E7703Q},
	doi = {10.1002/aic.17703},
	abstract = {In this article, a novel latent vector autoregressive (LaVAR) modeling algorithm with a canonical correlation analysis (CCA) objective is proposed to estimate a fully‚Äëinteracting reduced‚Äëdimensional dynamic model. This algorithm is an advancement of the dynamic inner canonical correlation analysis (DiCCA) algorithm, which builds univariate latent autoregressive models that are noninteracting. The dynamic latent variable scores of the proposed algorithm are guaranteed to be orthogonal with a descending order of predictability, retaining the properties of DiCCA. Further, the LaVAR‚ÄëCCA algorithm solves multiple latent variables simultaneously with a statistical interpretation of the profile likelihood. The Lorenz oscillator with noisy measurements and an application case study on an industrial dataset are used to illustrate the superiority of the proposed algorithm. The reduced‚Äëdimensional latent dynamic model has numerous potential applications for prediction, feature analysis, and diagnosis of systems with rich measurements.},
	urldate = {2024-01-29},
	journal = {AIChE Journal},
	author = {Qin, S. Joe},
	month = jun,
	year = {2022},
	note = {ADS Bibcode: 2022AIChE..68E7703Q},
	pages = {e17703},
}

@article{fan_estimating_2022,
	title = {Estimating {Number} of {Factors} by {Adjusted} {Eigenvalues} {Thresholding}},
	volume = {117},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2020.1825448},
	doi = {10.1080/01621459.2020.1825448},
	abstract = {Determining the number of common factors is an important and practical topic in high-dimensional factor models. The existing literature is mainly based on the eigenvalues of the covariance matrix. Owing to the incomparability of the eigenvalues of the covariance matrix caused by the heterogeneous scales of the observed variables, it is not easy to find an accurate relationship between these eigenvalues and the number of common factors. To overcome this limitation, we appeal to the correlation matrix and demonstrate, surprisingly, that the number of eigenvalues greater than 1 of the population correlation matrix is the same as the number of common factors under certain mild conditions. To use such a relationship, we study random matrix theory based on the sample correlation matrix to correct biases in estimating the top eigenvalues and to take into account of estimation errors in eigenvalue estimation. Thus, we propose a tuning-free scale-invariant adjusted correlation thresholding (ACT) method for determining the number of common factors in high-dimensional factor models, taking into account the sampling variabilities and biases of top sample eigenvalues. We also establish the optimality of the proposed ACT method in terms of minimal signal strength and the optimal threshold. Simulation studies lend further support to our proposed method and show that our estimator outperforms competing methods in most test cases. Supplementary materials for this article are available online.},
	number = {538},
	urldate = {2024-01-29},
	journal = {Journal of the American Statistical Association},
	author = {Fan, Jianqing and Guo, Jianhua and Zheng, Shurong},
	month = apr,
	year = {2022},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2020.1825448},
	keywords = {Adjusted eigenvalues, Bias corrections, Factor models, Number of factors, Random matrices},
	pages = {852--861},
}

@article{fan_estimating_2022-1,
	title = {Estimating {Number} of {Factors} by {Adjusted} {Eigenvalues} {Thresholding}},
	volume = {117},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2020.1825448},
	doi = {10.1080/01621459.2020.1825448},
	abstract = {Determining the number of common factors is an important and practical topic in high-dimensional factor models. The existing literature is mainly based on the eigenvalues of the covariance matrix. Owing to the incomparability of the eigenvalues of the covariance matrix caused by the heterogeneous scales of the observed variables, it is not easy to find an accurate relationship between these eigenvalues and the number of common factors. To overcome this limitation, we appeal to the correlation matrix and demonstrate, surprisingly, that the number of eigenvalues greater than 1 of the population correlation matrix is the same as the number of common factors under certain mild conditions. To use such a relationship, we study random matrix theory based on the sample correlation matrix to correct biases in estimating the top eigenvalues and to take into account of estimation errors in eigenvalue estimation. Thus, we propose a tuning-free scale-invariant adjusted correlation thresholding (ACT) method for determining the number of common factors in high-dimensional factor models, taking into account the sampling variabilities and biases of top sample eigenvalues. We also establish the optimality of the proposed ACT method in terms of minimal signal strength and the optimal threshold. Simulation studies lend further support to our proposed method and show that our estimator outperforms competing methods in most test cases. Supplementary materials for this article are available online.},
	number = {538},
	urldate = {2024-01-29},
	journal = {Journal of the American Statistical Association},
	author = {Fan, Jianqing and Guo, Jianhua and Zheng, Shurong},
	month = apr,
	year = {2022},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2020.1825448},
	keywords = {Adjusted eigenvalues, Bias corrections, Factor models, Number of factors, Random matrices},
	pages = {852--861},
}

@article{pan_modelling_2008,
	title = {Modelling multiple time series via common factors},
	volume = {95},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/asn009},
	doi = {10.1093/biomet/asn009},
	abstract = {We propose a new method for estimating common factors of multiple time series. One distinctive feature of the new approach is that it is applicable to some nonstationary time series. The unobservable, nonstationary factors are identified by expanding the white noise space step by step, thereby solving a high-dimensional optimization problem by several low-dimensional sub-problems. Asymptotic properties of the estimation are investigated. The proposed methodology is illustrated with both simulated and real datasets.},
	number = {2},
	urldate = {2024-01-29},
	journal = {Biometrika},
	author = {Pan, Jiazhu and Yao, Qiwei},
	month = jun,
	year = {2008},
	pages = {365--379},
}

@article{pan_modelling_2008-1,
	title = {Modelling multiple time series via common factors},
	volume = {95},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/asn009},
	doi = {10.1093/biomet/asn009},
	abstract = {We propose a new method for estimating common factors of multiple time series. One distinctive feature of the new approach is that it is applicable to some nonstationary time series. The unobservable, nonstationary factors are identified by expanding the white noise space step by step, thereby solving a high-dimensional optimization problem by several low-dimensional sub-problems. Asymptotic properties of the estimation are investigated. The proposed methodology is illustrated with both simulated and real datasets.},
	number = {2},
	urldate = {2024-01-29},
	journal = {Biometrika},
	author = {Pan, Jiazhu and Yao, Qiwei},
	month = jun,
	year = {2008},
	pages = {365--379},
}

@inproceedings{stewart_perturbation_1990,
	title = {Perturbation {Theory} for the {Singular} {Value} {Decomposition} {Perturbation} {Theory} for the {Singular} {Value} {Decomposition}},
	abstract = {abstract The singular value decomposition has a number of applications in digital signal processing. However, the the decomposition must be computed from a matrix consisting of both signal and noise. It is therefore important to be able to assess the eeects of the noise on the singular values and singular vectors {\textbar} a problem in classical perturbation theory. In this paper we survey the perturbation theory of the singular value decomposition. Abstract The singular value decomposition has a number of applications in digital signal processing. However, the the decomposition must be computed from a matrix consisting of both signal and noise. It is therefore important to be able to assess the eeects of the noise on the singular values and singular vectors {\textbar} a problem in classical perturbation theory. In this paper we survey the perturbation theory of the singular value decomposition.},
	urldate = {2024-01-17},
	author = {Stewart, G.},
	year = {1990},
}

@book{medsker_recurrent_1999,
	title = {Recurrent {Neural} {Networks}: {Design} and {Applications}},
	isbn = {978-1-4200-4917-6},
	shorttitle = {Recurrent {Neural} {Networks}},
	abstract = {With existent uses ranging from motion detection to music synthesis to financial forecasting, recurrent neural networks have generated widespread attention. The tremendous interest in these networks drives Recurrent Neural Networks: Design and Applications, a summary of the design, applications, current research, and challenges of this subfield of artificial neural networks.This overview incorporates every aspect of recurrent neural networks. It outlines the wide variety of complex learning techniques and associated research projects. Each chapter addresses architectures, from fully connected to partially connected, including recurrent multilayer feedforward. It presents problems involving trajectories, control systems, and robotics, as well as RNN use in chaotic systems. The authors also share their expert knowledge of ideas for alternate designs and advances in theoretical aspects.The dynamical behavior of recurrent neural networks is useful for solving problems in science, engineering, and business. This approach will yield huge advances in the coming years. Recurrent Neural Networks illuminates the opportunities and provides you with a broad view of the current events in this rich field.},
	language = {en},
	publisher = {CRC Press},
	author = {Medsker, Larry and Jain, Lakhmi C.},
	month = dec,
	year = {1999},
	note = {Google-Books-ID: ME1SAkN0PyMC},
	keywords = {Computers / Computer Engineering, Computers / General, Computers / Software Development \& Engineering / Systems Analysis \& Design, Technology \& Engineering / Electronics / General},
}

@article{sikander_linear_2015,
	title = {Linear time-invariant system reduction using a mixed methods approach},
	volume = {39},
	issn = {0307-904X},
	url = {https://www.sciencedirect.com/science/article/pii/S0307904X15002504},
	doi = {10.1016/j.apm.2015.04.014},
	abstract = {In this study, we propose a new method for obtaining a reduced order model of high order linear time-invariant single input‚Äìsingle output (SISO) and multiple input‚Äìmultiple output (MIMO) systems, which is based on the stability equation method and factor division algorithm. For a given higher order linear time-invariant system, the coefficients of the numerator polynomial are estimated by the factor division algorithm while the coefficients of the denominator polynomial are calculated by the stability equation method. The distinctive feature of the proposed method is that the reduced model will always be stable if the original system is stable. Numerical examples are provided that demonstrate the superior performance of the proposed method compared with other well-known published methods.},
	number = {16},
	urldate = {2024-01-28},
	journal = {Applied Mathematical Modelling},
	author = {Sikander, Afzal and Prasad, Rajendra},
	month = aug,
	year = {2015},
	keywords = {Factor division algorithm, Integral squared error, Model reduction, Stability, Stability equation method},
	pages = {4848--4858},
}

@article{sikander_linear_2015-1,
	title = {Linear time-invariant system reduction using a mixed methods approach},
	volume = {39},
	issn = {0307-904X},
	url = {https://www.sciencedirect.com/science/article/pii/S0307904X15002504},
	doi = {10.1016/j.apm.2015.04.014},
	abstract = {In this study, we propose a new method for obtaining a reduced order model of high order linear time-invariant single input‚Äìsingle output (SISO) and multiple input‚Äìmultiple output (MIMO) systems, which is based on the stability equation method and factor division algorithm. For a given higher order linear time-invariant system, the coefficients of the numerator polynomial are estimated by the factor division algorithm while the coefficients of the denominator polynomial are calculated by the stability equation method. The distinctive feature of the proposed method is that the reduced model will always be stable if the original system is stable. Numerical examples are provided that demonstrate the superior performance of the proposed method compared with other well-known published methods.},
	number = {16},
	urldate = {2024-01-28},
	journal = {Applied Mathematical Modelling},
	author = {Sikander, Afzal and Prasad, Rajendra},
	month = aug,
	year = {2015},
	keywords = {Factor division algorithm, Integral squared error, Model reduction, Stability, Stability equation method},
	pages = {4848--4858},
}

@article{bui-thanh_model_2008,
	title = {Model {Reduction} for {Large}-{Scale} {Systems} with {High}-{Dimensional} {Parametric} {Input} {Space}},
	volume = {30},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/070694855},
	doi = {10.1137/070694855},
	abstract = {A greedy algorithm for the construction of a reduced model with reduction in both parameter and state is developed for an efficient solution of statistical inverse problems governed by partial differential equations with distributed parameters. Large-scale models are too costly to evaluate repeatedly, as is required in the statistical setting. Furthermore, these models often have high-dimensional parametric input spaces, which compounds the difficulty of effectively exploring the uncertainty space. We simultaneously address both challenges by constructing a projection-based reduced model that accepts low-dimensional parameter inputs and whose model evaluations are inexpensive. The associated parameter and state bases are obtained through a greedy procedure that targets the governing equations, model outputs, and prior information. The methodology and results are presented for groundwater inverse problems in one and two dimensions.},
	number = {6},
	urldate = {2024-01-28},
	journal = {SIAM Journal on Scientific Computing},
	author = {Bui-Thanh, T. and Willcox, K. and Ghattas, O.},
	month = jan,
	year = {2008},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {3270--3288},
}

@article{gallego_long-term_2020,
	title = {Long-term stability of cortical population dynamics underlying consistent behavior},
	volume = {23},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-019-0555-4},
	doi = {10.1038/s41593-019-0555-4},
	abstract = {Animals readily execute learned behaviors in a consistent manner over long periods of time, and yet no equally stable neural correlate has been demonstrated. How does the cortex achieve this stable control? Using the sensorimotor system as a model of cortical processing, we investigated the hypothesis that the dynamics of neural latent activity, which captures the dominant co-variation patterns within the neural population, must be preserved across time. We recorded from populations of neurons in premotor, primary motor and somatosensory cortices as monkeys performed a reaching task, for up to 2‚Äâyears. Intriguingly, despite a steady turnover in the recorded neurons, the low-dimensional latent dynamics remained stable. The stability allowed reliable decoding of behavioral features for the entire timespan, while fixed decoders based directly on the recorded neural activity degraded substantially. We posit that stable latent cortical dynamics within the manifold are the fundamental building blocks underlying consistent behavioral execution.},
	language = {en},
	number = {2},
	urldate = {2024-01-28},
	journal = {Nature Neuroscience},
	author = {Gallego, Juan A. and Perich, Matthew G. and Chowdhury, Raeed H. and Solla, Sara A. and Miller, Lee E.},
	month = feb,
	year = {2020},
	note = {Number: 2
Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Neuroscience},
	pages = {260--270},
}

@article{gallego_long-term_2020-1,
	title = {Long-term stability of cortical population dynamics underlying consistent behavior},
	volume = {23},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-019-0555-4},
	doi = {10.1038/s41593-019-0555-4},
	abstract = {Animals readily execute learned behaviors in a consistent manner over long periods of time, and yet no equally stable neural correlate has been demonstrated. How does the cortex achieve this stable control? Using the sensorimotor system as a model of cortical processing, we investigated the hypothesis that the dynamics of neural latent activity, which captures the dominant co-variation patterns within the neural population, must be preserved across time. We recorded from populations of neurons in premotor, primary motor and somatosensory cortices as monkeys performed a reaching task, for up to 2‚Äâyears. Intriguingly, despite a steady turnover in the recorded neurons, the low-dimensional latent dynamics remained stable. The stability allowed reliable decoding of behavioral features for the entire timespan, while fixed decoders based directly on the recorded neural activity degraded substantially. We posit that stable latent cortical dynamics within the manifold are the fundamental building blocks underlying consistent behavioral execution.},
	language = {en},
	number = {2},
	urldate = {2024-01-28},
	journal = {Nature Neuroscience},
	author = {Gallego, Juan A. and Perich, Matthew G. and Chowdhury, Raeed H. and Solla, Sara A. and Miller, Lee E.},
	month = feb,
	year = {2020},
	note = {Number: 2
Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Neuroscience},
	pages = {260--270},
}

@article{maliar_merging_2015,
	title = {Merging simulation and projection approaches to solve high-dimensional problems with an application to a new {Keynesian} model},
	volume = {6},
	copyright = {Copyright ¬© 2015 Lilia Maliar and Serguei Maliar},
	issn = {1759-7331},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/QE364},
	doi = {10.3982/QE364},
	abstract = {We introduce a numerical algorithm for solving dynamic economic models that merges stochastic simulation and projection approaches: we use simulation to approximate the ergodic measure of the solution, we cover the support of the constructed ergodic measure with a fixed grid, and we use projection techniques to accurately solve the model on that grid. The construction of the grid is the key novel piece of our analysis: we replace a large cloud of simulated points with a small set of ‚Äúrepresentative‚Äù points. We present three alternative techniques for constructing representative points: a clustering method, an Œµ-distinguishable set method, and a locally-adaptive variant of the Œµ-distinguishable set method. As an illustration, we solve one- and multi-agent neoclassical growth models and a large-scale new Keynesian model with a zero lower bound on nominal interest rates. The proposed solution algorithm is tractable in problems with high dimensionality (hundreds of state variables) on a desktop computer.},
	language = {en},
	number = {1},
	urldate = {2024-01-28},
	journal = {Quantitative Economics},
	author = {Maliar, Lilia and Maliar, Serguei},
	year = {2015},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.3982/QE364},
	keywords = {C61, C63, C68, E31, E52, Ergodic set, ZLB, adaptive grid, clusters, discrepancy, large-scale model, new Keynesian model, stochastic simulation, Œµ-distinguishable set},
	pages = {1--47},
}

@article{sussillo_opening_2013,
	title = {Opening the {Black} {Box}: {Low}-{Dimensional} {Dynamics} in {High}-{Dimensional} {Recurrent} {Neural} {Networks}},
	volume = {25},
	issn = {0899-7667},
	shorttitle = {Opening the {Black} {Box}},
	url = {https://doi.org/10.1162/NECO_a_00409},
	doi = {10.1162/NECO_a_00409},
	abstract = {Recurrent neural networks (RNNs) are useful tools for learning nonlinear relationships between time-varying inputs and outputs with complex temporal dependencies. Recently developed algorithms have been successful at training RNNs to perform a wide variety of tasks, but the resulting networks have been treated as black boxes: their mechanism of operation remains unknown. Here we explore the hypothesis that fixed points, both stable and unstable, and the linearized dynamics around them, can reveal crucial aspects of how RNNs implement their computations. Further, we explore the utility of linearization in areas of phase space that are not true fixed points but merely points of very slow movement. We present a simple optimization technique that is applied to trained RNNs to find the fixed and slow points of their dynamics. Linearization around these slow regions can be used to explore, or reverse-engineer, the behavior of the RNN. We describe the technique, illustrate it using simple examples, and finally showcase it on three high-dimensional RNN examples: a 3-bit flip-flop device, an input-dependent sine wave generator, and a two-point moving average. In all cases, the mechanisms of trained networks could be inferred from the sets of fixed and slow points and the linearized dynamics around them.},
	number = {3},
	urldate = {2024-01-28},
	journal = {Neural Computation},
	author = {Sussillo, David and Barak, Omri},
	month = mar,
	year = {2013},
	pages = {626--649},
}

@article{yu_analysis_2021,
	title = {Analysis of different {RNN} autoencoder variants for time series classification and machine prognostics},
	volume = {149},
	issn = {0888-3270},
	url = {https://www.sciencedirect.com/science/article/pii/S0888327020307081},
	doi = {10.1016/j.ymssp.2020.107322},
	abstract = {Recurrent neural network (RNN) based autoencoders, trained in an unsupervised manner, have been widely used to generate fixed-dimensional vector representations or embeddings for varying length multivariate time series. These embeddings have been demonstrated to be useful for time series reconstruction, classification, and creation of health index (HI) curves of machines being used in industrial applications, based on which the remaining useful life (RUL) of machines can be estimated. In this study, we extend the traditional form of RNN autoencoders as a feature extractor for multivariate time series to a more general form in terms of arranging the order of input or output sequences and the hidden unit architecture. We apply the embeddings obtained by different variants of RNN autoencoders for a time series classification task and a machine RUL estimation problem using two publicly available datasets. A random research strategy is used to find the optimal hyperparameters of all variants for each task in order to give a fair comparison of the general performance among different variants over a large hyperparameter space, as well as the best performance that each variant can achieve compared with the best reported values in the literature. Our results show that traditional reversing the order of output time series while maintaining the order of input time series when training an RNN autoencoder does not show improved performance for the two studied cases. Thus, intentionally arranging the input or output order seems unnecessary for training the RNN autoencoder as a feature extractor of time series. We further observe that only the RNN architectures with gating mechanism can achieve the functionality of encoding for the time series, and none of the three common gated architectures we studied shows significantly and consistently improved performance compared to the others on the two studied cases. However, the bidirectional RNN autoencoders yield slightly better performance than their unidirectional counterparts.},
	urldate = {2024-01-28},
	journal = {Mechanical Systems and Signal Processing},
	author = {Yu, Wennian and Kim, Il Yong and Mechefske, Chris},
	month = feb,
	year = {2021},
	keywords = {Autoencoder, Random search, Recurrent neural network, Remaining useful life, Time series},
	pages = {107322},
}

@article{marks_stimulus-dependent_2021,
	title = {Stimulus-dependent representational drift in primary visual cortex},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-25436-3},
	doi = {10.1038/s41467-021-25436-3},
	abstract = {To produce consistent sensory perception, neurons must maintain stable representations of sensory input. However, neurons in many regions exhibit progressive drift across days. Longitudinal studies have found stable responses to artificial stimuli across sessions in visual areas, but it is unclear whether this stability extends to naturalistic stimuli. We performed chronic 2-photon imaging of mouse V1 populations to directly compare the representational stability of artificial versus naturalistic visual stimuli over weeks. Responses to gratings were highly stable across sessions. However, neural responses to naturalistic movies exhibited progressive representational drift across sessions. Differential drift was present across cortical layers, in inhibitory interneurons, and could not be explained by differential response strength or higher order stimulus statistics. However, representational drift was accompanied by similar differential changes in local population correlation structure. These results suggest representational stability in V1 is stimulus-dependent and may relate to differences in preexisting circuit architecture of co-tuned neurons.},
	language = {en},
	number = {1},
	urldate = {2024-01-28},
	journal = {Nature Communications},
	author = {Marks, Tyler D. and Goard, Michael J.},
	month = aug,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Neural circuits, Sensory processing},
	pages = {5169},
}

@article{hajnal_continuous_2023,
	title = {Continuous multiplexed population representations of task context in the mouse primary visual cortex},
	volume = {14},
	copyright = {2023 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-023-42441-w},
	doi = {10.1038/s41467-023-42441-w},
	abstract = {Effective task execution requires the representation of multiple task-related variables that determine how stimuli lead to correct responses. Even the primary visual cortex (V1) represents other task-related variables such as expectations, choice, and context. However, it is unclear how V1 can flexibly accommodate these variables without interfering with visual representations. We trained mice on a context-switching cross-modal decision task, where performance depends on inferring task context. We found that the context signal that emerged in V1 was behaviorally relevant as it strongly covaried with performance, independent from movement. Importantly, this signal was integrated into V1 representation by multiplexing visual and context signals into orthogonal subspaces. In addition, auditory and choice signals were also multiplexed as these signals were orthogonal to the context representation. Thus, multiplexing allows V1 to integrate visual inputs with other sensory modalities and cognitive variables to avoid interference with the visual representation while ensuring the maintenance of task-relevant variables.},
	language = {en},
	number = {1},
	urldate = {2024-01-28},
	journal = {Nature Communications},
	author = {Hajnal, M√°rton Albert and Tran, Duy and Einstein, Michael and Martelo, Mauricio Vallejo and Safaryan, Karen and Polack, Pierre-Olivier and Golshani, Peyman and Orb√°n, Gerg≈ë},
	month = oct,
	year = {2023},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Neural encoding, Sensory processing, Striate cortex},
	pages = {6687},
}

@article{xia_stable_2021,
	title = {Stable representation of a naturalistic movie emerges from episodic activity with gain variability},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-25437-2},
	doi = {10.1038/s41467-021-25437-2},
	abstract = {Visual cortical responses are known to be highly variable across trials within an experimental session. However, the long-term stability of visual cortical responses is poorly understood. Here using chronic imaging of V1 in mice we show that neural responses to repeated natural movie clips are unstable across weeks. Individual neuronal responses consist of sparse episodic activity which are stable in time but unstable in gain across weeks. Further, we find that the individual episode, instead of neuron, serves as the basic unit of the week-to-week fluctuation. To investigate how population activity encodes the stimulus, we extract a stable one-dimensional representation of the time in the natural movie, using an unsupervised method. Most week-to-week fluctuation is perpendicular to the stimulus encoding direction, thus leaving the stimulus representation largely unaffected. We propose that precise episodic activity with coordinated gain changes are keys to maintain a stable stimulus representation in V1.},
	language = {en},
	number = {1},
	urldate = {2024-01-28},
	journal = {Nature Communications},
	author = {Xia, Ji and Marks, Tyler D. and Goard, Michael J. and Wessel, Ralf},
	month = aug,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Neural decoding, Neural encoding, Striate cortex},
	pages = {5170},
}

@article{xia_stable_2021-1,
	title = {Stable representation of a naturalistic movie emerges from episodic activity with gain variability},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-25437-2},
	doi = {10.1038/s41467-021-25437-2},
	abstract = {Visual cortical responses are known to be highly variable across trials within an experimental session. However, the long-term stability of visual cortical responses is poorly understood. Here using chronic imaging of V1 in mice we show that neural responses to repeated natural movie clips are unstable across weeks. Individual neuronal responses consist of sparse episodic activity which are stable in time but unstable in gain across weeks. Further, we find that the individual episode, instead of neuron, serves as the basic unit of the week-to-week fluctuation. To investigate how population activity encodes the stimulus, we extract a stable one-dimensional representation of the time in the natural movie, using an unsupervised method. Most week-to-week fluctuation is perpendicular to the stimulus encoding direction, thus leaving the stimulus representation largely unaffected. We propose that precise episodic activity with coordinated gain changes are keys to maintain a stable stimulus representation in V1.},
	language = {en},
	number = {1},
	urldate = {2024-01-28},
	journal = {Nature Communications},
	author = {Xia, Ji and Marks, Tyler D. and Goard, Michael J. and Wessel, Ralf},
	month = aug,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Neural decoding, Neural encoding, Striate cortex},
	pages = {5170},
}

@article{baig_autoencoder_2023,
	title = {Autoencoder neural networks enable low dimensional structure analyses of microbial growth dynamics},
	volume = {14},
	copyright = {2023 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-023-43455-0},
	doi = {10.1038/s41467-023-43455-0},
	abstract = {The ability to effectively represent microbiome dynamics is a crucial challenge in their quantitative analysis and engineering. By using autoencoder neural networks, we show that microbial growth dynamics can be compressed into low-dimensional representations and reconstructed with high fidelity. These low-dimensional embeddings are just as effective, if not better, than raw data for tasks such as identifying bacterial strains, predicting traits like antibiotic resistance, and predicting community dynamics. Additionally, we demonstrate that essential dynamical information of these systems can be captured using far fewer variables than traditional mechanistic models. Our work suggests that machine learning can enable the creation of concise representations of high-dimensional microbiome dynamics to facilitate data analysis and gain new biological insights.},
	language = {en},
	number = {1},
	urldate = {2024-01-28},
	journal = {Nature Communications},
	author = {Baig, Yasa and Ma, Helena R. and Xu, Helen and You, Lingchong},
	month = dec,
	year = {2023},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Dynamical systems, Machine learning},
	pages = {7937},
}

@article{masini_machine_2023,
	title = {Machine learning advances for time series forecasting},
	volume = {37},
	copyright = {¬© 2021 John Wiley \& Sons Ltd.},
	issn = {1467-6419},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/joes.12429},
	doi = {10.1111/joes.12429},
	abstract = {In this paper, we survey the most recent advances in supervised machine learning (ML) and high-dimensional models for time-series forecasting. We consider both linear and nonlinear alternatives. Among the linear methods, we pay special attention to penalized regressions and ensemble of models. The nonlinear methods considered in the paper include shallow and deep neural networks, in their feedforward and recurrent versions, and tree-based methods, such as random forests and boosted trees. We also consider ensemble and hybrid models by combining ingredients from different alternatives. Tests for superior predictive ability are briefly reviewed. Finally, we discuss application of ML in economics and finance and provide an illustration with high-frequency financial data.},
	language = {en},
	number = {1},
	urldate = {2024-01-28},
	journal = {Journal of Economic Surveys},
	author = {Masini, Ricardo P. and Medeiros, Marcelo C. and Mendes, Eduardo F.},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/joes.12429},
	keywords = {bagging, boosting, deep learning, forecasting, machine learning, neural networks, nonlinear models, penalized regressions, random forests, regression trees, regularization, sieve approximation, statistical learning theory},
	pages = {76--111},
}

@article{stringer_high-dimensional_2019,
	title = {High-dimensional geometry of population responses in visual cortex},
	volume = {571},
	issn = {1476-4687},
	doi = {10.1038/s41586-019-1346-5},
	abstract = {A neuronal population encodes information most efficiently when its stimulus responses are high-dimensional and uncorrelated, and most robustly when they are lower-dimensional and correlated. Here we analysed the dimensionality of the encoding of natural images by large populations of neurons in the visual cortex of awake mice. The evoked population activity was high-dimensional, and correlations obeyed an unexpected power law: the nth principal component variance scaled as 1/n. This scaling was not inherited from the power law spectrum of natural images, because it persisted after stimulus whitening. We proved mathematically that if the variance spectrum was to decay more slowly then the population code could not be smooth, allowing small changes in input to dominate population activity. The theory also predicts larger power-law exponents for lower-dimensional stimulus ensembles, which we validated experimentally. These results suggest that coding smoothness may represent a fundamental constraint that determines correlations in neural population codes.},
	language = {eng},
	number = {7765},
	journal = {Nature},
	author = {Stringer, Carsen and Pachitariu, Marius and Steinmetz, Nicholas and Carandini, Matteo and Harris, Kenneth D.},
	month = jul,
	year = {2019},
	pmid = {31243367},
	pmcid = {PMC6642054},
	keywords = {Animals, Female, Male, Mice, Models, Neurological, Photic Stimulation, Reproducibility of Results, Visual Cortex},
	pages = {361--365},
}

@article{stringer_high-dimensional_2019-1,
	title = {High-dimensional geometry of population responses in visual cortex},
	volume = {571},
	issn = {1476-4687},
	doi = {10.1038/s41586-019-1346-5},
	abstract = {A neuronal population encodes information most efficiently when its stimulus responses are high-dimensional and uncorrelated, and most robustly when they are lower-dimensional and correlated. Here we analysed the dimensionality of the encoding of natural images by large populations of neurons in the visual cortex of awake mice. The evoked population activity was high-dimensional, and correlations obeyed an unexpected power law: the nth principal component variance scaled as 1/n. This scaling was not inherited from the power law spectrum of natural images, because it persisted after stimulus whitening. We proved mathematically that if the variance spectrum was to decay more slowly then the population code could not be smooth, allowing small changes in input to dominate population activity. The theory also predicts larger power-law exponents for lower-dimensional stimulus ensembles, which we validated experimentally. These results suggest that coding smoothness may represent a fundamental constraint that determines correlations in neural population codes.},
	language = {eng},
	number = {7765},
	journal = {Nature},
	author = {Stringer, Carsen and Pachitariu, Marius and Steinmetz, Nicholas and Carandini, Matteo and Harris, Kenneth D.},
	month = jul,
	year = {2019},
	pmid = {31243367},
	pmcid = {PMC6642054},
	keywords = {Animals, Female, Male, Mice, Models, Neurological, Photic Stimulation, Reproducibility of Results, Visual Cortex},
	pages = {361--365},
}

@article{mathis_deeplabcut_2018,
	title = {{DeepLabCut}: markerless pose estimation of user-defined body parts with deep learning},
	volume = {21},
	issn = {1546-1726},
	shorttitle = {{DeepLabCut}},
	doi = {10.1038/s41593-018-0209-y},
	abstract = {Quantifying behavior is crucial for many applications in neuroscience. Videography provides easy methods for the observation and recording of animal behavior in diverse settings, yet extracting particular aspects of a behavior for further analysis can be highly time consuming. In motor control studies, humans or other animals are often marked with reflective markers to assist with computer-based tracking, but markers are intrusive, and the number and location of the markers must be determined a priori. Here we present an efficient method for markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results with minimal training data. We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors. Remarkably, even when only a small number of frames are labeled ({\textasciitilde}200), the algorithm achieves excellent tracking performance on test frames that is comparable to human accuracy.},
	language = {eng},
	number = {9},
	journal = {Nature Neuroscience},
	author = {Mathis, Alexander and Mamidanna, Pranav and Cury, Kevin M. and Abe, Taiga and Murthy, Venkatesh N. and Mathis, Mackenzie Weygandt and Bethge, Matthias},
	month = sep,
	year = {2018},
	pmid = {30127430},
	keywords = {Algorithms, Animals, Behavior, Behavior, Animal, Deep Learning, Drosophila melanogaster, Humans, Male, Mice, Mice, Inbred C57BL, Nerve Net, Neural Networks, Computer, Odorants, Posture, Psychomotor Performance, Transfer, Psychology, Video Recording},
	pages = {1281--1289},
}

@article{churchland_neural_2012,
	title = {Neural population dynamics during reaching},
	volume = {487},
	copyright = {2012 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature11129},
	doi = {10.1038/nature11129},
	abstract = {Most theories of motor cortex have assumed that neural activity represents movement parameters. This view derives from what is known about primary visual cortex, where neural activity represents patterns of light. Yet it is unclear how well the analogy between motor and visual cortex holds. Single-neuron responses in motor cortex are complex, and there is marked disagreement regarding which movement parameters are represented. A better analogy might be with other motor systems, where a common principle is rhythmic neural activity. Here we find that motor cortex responses during reaching contain a brief but strong oscillatory component, something quite unexpected for a non-periodic behaviour. Oscillation amplitude and phase followed naturally from the preparatory state, suggesting a mechanistic role for preparatory neural activity. These results demonstrate an unexpected yet surprisingly simple structure in the population response. This underlying structure explains many of the confusing features of individual neural responses.},
	language = {en},
	number = {7405},
	urldate = {2024-01-28},
	journal = {Nature},
	author = {Churchland, Mark M. and Cunningham, John P. and Kaufman, Matthew T. and Foster, Justin D. and Nuyujukian, Paul and Ryu, Stephen I. and Shenoy, Krishna V.},
	month = jul,
	year = {2012},
	note = {Number: 7405
Publisher: Nature Publishing Group},
	keywords = {Motor cortex, Neuronal physiology, Population dynamics},
	pages = {51--56},
}

@article{churchland_neural_2012-1,
	title = {Neural population dynamics during reaching},
	volume = {487},
	copyright = {2012 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature11129},
	doi = {10.1038/nature11129},
	abstract = {Most theories of motor cortex have assumed that neural activity represents movement parameters. This view derives from what is known about primary visual cortex, where neural activity represents patterns of light. Yet it is unclear how well the analogy between motor and visual cortex holds. Single-neuron responses in motor cortex are complex, and there is marked disagreement regarding which movement parameters are represented. A better analogy might be with other motor systems, where a common principle is rhythmic neural activity. Here we find that motor cortex responses during reaching contain a brief but strong oscillatory component, something quite unexpected for a non-periodic behaviour. Oscillation amplitude and phase followed naturally from the preparatory state, suggesting a mechanistic role for preparatory neural activity. These results demonstrate an unexpected yet surprisingly simple structure in the population response. This underlying structure explains many of the confusing features of individual neural responses.},
	language = {en},
	number = {7405},
	urldate = {2024-01-28},
	journal = {Nature},
	author = {Churchland, Mark M. and Cunningham, John P. and Kaufman, Matthew T. and Foster, Justin D. and Nuyujukian, Paul and Ryu, Stephen I. and Shenoy, Krishna V.},
	month = jul,
	year = {2012},
	note = {Number: 7405
Publisher: Nature Publishing Group},
	keywords = {Motor cortex, Neuronal physiology, Population dynamics},
	pages = {51--56},
}

@article{cunningham_dimensionality_2014,
	title = {Dimensionality reduction for large-scale neural recordings},
	volume = {17},
	copyright = {2014 Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn.3776},
	doi = {10.1038/nn.3776},
	abstract = {Many recent studies have adopted dimensionality reduction to analyze neural population activity and to find features that are not apparent at the level of individual neurons. The authors describe the scientific motivation for population analyses and the dimensionality reduction methods commonly applied to population activity. They also offer practical advice about selecting methods and interpreting their outputs.},
	language = {en},
	number = {11},
	urldate = {2024-01-28},
	journal = {Nature Neuroscience},
	author = {Cunningham, John P. and Yu, Byron M.},
	month = nov,
	year = {2014},
	note = {Number: 11
Publisher: Nature Publishing Group},
	keywords = {Learning algorithms},
	pages = {1500--1509},
}

@article{cunningham_dimensionality_2014-1,
	title = {Dimensionality reduction for large-scale neural recordings},
	volume = {17},
	copyright = {2014 Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn.3776},
	doi = {10.1038/nn.3776},
	abstract = {Many recent studies have adopted dimensionality reduction to analyze neural population activity and to find features that are not apparent at the level of individual neurons. The authors describe the scientific motivation for population analyses and the dimensionality reduction methods commonly applied to population activity. They also offer practical advice about selecting methods and interpreting their outputs.},
	language = {en},
	number = {11},
	urldate = {2024-01-28},
	journal = {Nature Neuroscience},
	author = {Cunningham, John P. and Yu, Byron M.},
	month = nov,
	year = {2014},
	note = {Number: 11
Publisher: Nature Publishing Group},
	keywords = {Learning algorithms},
	pages = {1500--1509},
}

@article{mudassir_time-series_2020,
	title = {Time-series forecasting of {Bitcoin} prices using high-dimensional features: a machine learning approach},
	issn = {1433-3058},
	shorttitle = {Time-series forecasting of {Bitcoin} prices using high-dimensional features},
	url = {https://doi.org/10.1007/s00521-020-05129-6},
	doi = {10.1007/s00521-020-05129-6},
	abstract = {Bitcoin is a decentralized cryptocurrency, which is a type of digital asset that provides the basis for peer-to-peer financial transactions based on blockchain technology. One of the main problems with decentralized cryptocurrencies is price volatility, which indicates the need for studying the underlying price model. Moreover, Bitcoin prices exhibit non-stationary behavior, where the statistical distribution of data changes over time. This paper demonstrates high-performance machine learning-based classification and regression models for predicting Bitcoin price movements and prices in short and medium terms. In previous works, machine learning-based classification has been studied for an only one-day time frame, while this work goes beyond that by using machine learning-based models for one, seven, thirty and ninety days. The developed models are feasible and have high performance, with the classification models scoring up to 65\% accuracy for next-day forecast and scoring from 62 to 64\% accuracy for seventh‚Äìninetieth-day forecast. For daily price forecast, the error percentage is as low as 1.44\%, while it varies from 2.88 to 4.10\% for horizons of seven to ninety days. These results indicate that the presented models outperform the existing models in the literature.},
	language = {en},
	urldate = {2024-01-28},
	journal = {Neural Computing and Applications},
	author = {Mudassir, Mohammed and Bennbaia, Shada and Unal, Devrim and Hammoudeh, Mohammad},
	month = jul,
	year = {2020},
	keywords = {Blockchain, Deep learning, Machine learning, Time-series forecasting},
}

@article{mudassir_time-series_2020-1,
	title = {Time-series forecasting of {Bitcoin} prices using high-dimensional features: a machine learning approach},
	issn = {1433-3058},
	shorttitle = {Time-series forecasting of {Bitcoin} prices using high-dimensional features},
	url = {https://doi.org/10.1007/s00521-020-05129-6},
	doi = {10.1007/s00521-020-05129-6},
	abstract = {Bitcoin is a decentralized cryptocurrency, which is a type of digital asset that provides the basis for peer-to-peer financial transactions based on blockchain technology. One of the main problems with decentralized cryptocurrencies is price volatility, which indicates the need for studying the underlying price model. Moreover, Bitcoin prices exhibit non-stationary behavior, where the statistical distribution of data changes over time. This paper demonstrates high-performance machine learning-based classification and regression models for predicting Bitcoin price movements and prices in short and medium terms. In previous works, machine learning-based classification has been studied for an only one-day time frame, while this work goes beyond that by using machine learning-based models for one, seven, thirty and ninety days. The developed models are feasible and have high performance, with the classification models scoring up to 65\% accuracy for next-day forecast and scoring from 62 to 64\% accuracy for seventh‚Äìninetieth-day forecast. For daily price forecast, the error percentage is as low as 1.44\%, while it varies from 2.88 to 4.10\% for horizons of seven to ninety days. These results indicate that the presented models outperform the existing models in the literature.},
	language = {en},
	urldate = {2024-01-28},
	journal = {Neural Computing and Applications},
	author = {Mudassir, Mohammed and Bennbaia, Shada and Unal, Devrim and Hammoudeh, Mohammad},
	month = jul,
	year = {2020},
	keywords = {Blockchain, Deep learning, Machine learning, Time-series forecasting},
}

@book{castle_methodology_2009,
	title = {The {Methodology} and {Practice} of {Econometrics}: {A} {Festschrift} in {Honour} of {David} {F}. {Hendry}},
	isbn = {978-0-19-155325-7},
	shorttitle = {The {Methodology} and {Practice} of {Econometrics}},
	abstract = {David F. Hendry is a seminal figure in modern econometrics. He has pioneered the LSE approach to econometrics, and his influence is wide ranging. This book is a collection of papers dedicated to him and his work. Many internationally renowned econometricians who have collaborated with Hendry or have been influenced by his research have contributed to this volume, which provides a reflection on the recent advances in econometrics and considers the future progress for the methodology of econometrics. Central themes of the book include dynamic modelling and the properties of time series data, model selection and model evaluation, forecasting, policy analysis, exogeneity and causality, and encompassing. The book strikes a balance between econometric theory and empirical work, and demonstrates the influence that Hendry's research has had on the direction of modern econometrics. Contributors include: Karim Abadir, Anindya Banerjee, Gunnar B√•rdsen, Andreas Beyer, Mike Clements, James Davidson, Juan Dolado, Jurgen Doornik, Robert Engle, Neil Ericsson, Jesus Gonzalo, Clive Granger, David Hendry, Kevin Hoover, S√∏ren Johansen, Katarina Juselius, Steven Kamin, Pauline Kennedy, Maozu Lu, Massimiliano Marcellino, Laura Mayoral, Grayham Mizon, Bent Nielsen, Ragnor Nymoen, Jim Stock, Pravin Trivedi, Paolo Paruolo, Mark Watson, Hal White, and David Zimmer.},
	language = {en},
	publisher = {OUP Oxford},
	author = {Castle, Jennifer and Shephard, Neil},
	month = apr,
	year = {2009},
	keywords = {Business \& Economics / Econometrics, Mathematics / Probability \& Statistics / General},
}

@article{belloni_high-dimensional_2014,
	title = {High-{Dimensional} {Methods} and {Inference} on {Structural} and {Treatment} {Effects}},
	volume = {28},
	issn = {0895-3309},
	url = {https://www.aeaweb.org/articles?id=10.1257/jep.28.2.29},
	doi = {10.1257/jep.28.2.29},
	abstract = {Data with a large number of variables relative to the sample size‚Äî"high-dimensional data"‚Äîare readily available and increasingly common in empirical economics. High-dimensional data arise through a combination of two phenomena. First, the data may be inherently high dimensional in that many different characteristics per observation are available. For example, the US Census collects information on hundreds of individual characteristics and scanner datasets record transaction-level data for households across a wide range of products. Second, even when the number of available variables is relatively small, researchers rarely know the exact functional form with which the small number of variables enter the model of interest. Researchers are thus faced with a large set of potential variables formed by different ways of interacting and transforming the underlying variables. This paper provides an overview of how innovations in "data mining"¬ù can be adapted and modified to provide high-quality inference about model parameters. Note that we use the term "data mining" in a modern sense which denotes a principled search for "true" predictive power that guards against false discovery and overfitting, does not erroneously equate in-sample fit to out-of-sample predictive ability, and accurately accounts for using the same data to examine many different hypotheses or models.},
	language = {en},
	number = {2},
	urldate = {2024-01-28},
	journal = {Journal of Economic Perspectives},
	author = {Belloni, Alexandre and Chernozhukov, Victor and Hansen, Christian},
	month = may,
	year = {2014},
	keywords = {Modeling with Large Data Sets},
	pages = {29--50},
}

@misc{zhang_meta-learning_2023,
	title = {Meta-{Learning} {Operators} to {Optimality} from {Multi}-{Task} {Non}-{IID} {Data}},
	url = {http://arxiv.org/abs/2308.04428},
	doi = {10.48550/arXiv.2308.04428},
	abstract = {A powerful concept behind much of the recent progress in machine learning is the extraction of common features across data from heterogeneous sources or tasks. Intuitively, using all of one's data to learn a common representation function benefits both computational effort and statistical generalization by leaving a smaller number of parameters to fine-tune on a given task. Toward theoretically grounding these merits, we propose a general setting of recovering linear operators \$M\$ from noisy vector measurements \$y = Mx + w\$, where the covariates \$x\$ may be both non-i.i.d. and non-isotropic. We demonstrate that existing isotropy-agnostic meta-learning approaches incur biases on the representation update, which causes the scaling of the noise terms to lose favorable dependence on the number of source tasks. This in turn can cause the sample complexity of representation learning to be bottlenecked by the single-task data size. We introduce an adaptation, \${\textbackslash}texttt\{De-bias \& Feature-Whiten\}\$ (\${\textbackslash}texttt\{DFW\}\$), of the popular alternating minimization-descent (AMD) scheme proposed in Collins et al., (2021), and establish linear convergence to the optimal representation with noise level scaling down with the \${\textbackslash}textit\{total\}\$ source data size. This leads to generalization bounds on the same order as an oracle empirical risk minimizer. We verify the vital importance of \${\textbackslash}texttt\{DFW\}\$ on various numerical simulations. In particular, we show that vanilla alternating-minimization descent fails catastrophically even for iid, but mildly non-isotropic data. Our analysis unifies and generalizes prior work, and provides a flexible framework for a wider range of applications, such as in controls and dynamical systems.},
	urldate = {2024-01-27},
	publisher = {arXiv},
	author = {Zhang, Thomas T. C. K. and Toso, Leonardo F. and Anderson, James and Matni, Nikolai},
	month = aug,
	year = {2023},
	note = {arXiv:2308.04428 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Statistics - Machine Learning},
}

@misc{zhang_meta-learning_2023-1,
	title = {Meta-{Learning} {Operators} to {Optimality} from {Multi}-{Task} {Non}-{IID} {Data}},
	url = {http://arxiv.org/abs/2308.04428},
	doi = {10.48550/arXiv.2308.04428},
	abstract = {A powerful concept behind much of the recent progress in machine learning is the extraction of common features across data from heterogeneous sources or tasks. Intuitively, using all of one's data to learn a common representation function benefits both computational effort and statistical generalization by leaving a smaller number of parameters to fine-tune on a given task. Toward theoretically grounding these merits, we propose a general setting of recovering linear operators \$M\$ from noisy vector measurements \$y = Mx + w\$, where the covariates \$x\$ may be both non-i.i.d. and non-isotropic. We demonstrate that existing isotropy-agnostic meta-learning approaches incur biases on the representation update, which causes the scaling of the noise terms to lose favorable dependence on the number of source tasks. This in turn can cause the sample complexity of representation learning to be bottlenecked by the single-task data size. We introduce an adaptation, \${\textbackslash}texttt\{De-bias \& Feature-Whiten\}\$ (\${\textbackslash}texttt\{DFW\}\$), of the popular alternating minimization-descent (AMD) scheme proposed in Collins et al., (2021), and establish linear convergence to the optimal representation with noise level scaling down with the \${\textbackslash}textit\{total\}\$ source data size. This leads to generalization bounds on the same order as an oracle empirical risk minimizer. We verify the vital importance of \${\textbackslash}texttt\{DFW\}\$ on various numerical simulations. In particular, we show that vanilla alternating-minimization descent fails catastrophically even for iid, but mildly non-isotropic data. Our analysis unifies and generalizes prior work, and provides a flexible framework for a wider range of applications, such as in controls and dynamical systems.},
	urldate = {2024-01-27},
	publisher = {arXiv},
	author = {Zhang, Thomas T. C. K. and Toso, Leonardo F. and Anderson, James and Matni, Nikolai},
	month = aug,
	year = {2023},
	note = {arXiv:2308.04428 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Statistics - Machine Learning},
}

@inproceedings{tripuraneni_provable_2021,
	title = {Provable {Meta}-{Learning} of {Linear} {Representations}},
	url = {https://proceedings.mlr.press/v139/tripuraneni21a.html},
	abstract = {Meta-learning, or learning-to-learn, seeks to design algorithms that can utilize previous experience to rapidly learn new skills or adapt to new environments. Representation learning‚Äîa key tool for performing meta-learning‚Äîlearns a data representation that can transfer knowledge across multiple tasks, which is essential in regimes where data is scarce. Despite a recent surge of interest in the practice of meta-learning, the theoretical underpinnings of meta-learning algorithms are lacking, especially in the context of learning transferable representations. In this paper, we focus on the problem of multi-task linear regression‚Äîin which multiple linear regression models share a common, low-dimensional linear representation. Here, we provide provably fast, sample-efficient algorithms to address the dual challenges of (1) learning a common set of features from multiple, related tasks, and (2) transferring this knowledge to new, unseen tasks. Both are central to the general problem of meta-learning. Finally, we complement these results by providing information-theoretic lower bounds on the sample complexity of learning these linear features.},
	language = {en},
	urldate = {2024-01-27},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Tripuraneni, Nilesh and Jin, Chi and Jordan, Michael},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {10434--10443},
}

@article{vaswani_robust_2018,
	title = {Robust {Subspace} {Learning}: {Robust} {PCA}, {Robust} {Subspace} {Tracking}, and {Robust} {Subspace} {Recovery}},
	volume = {35},
	issn = {1053-5888},
	shorttitle = {Robust {Subspace} {Learning}},
	url = {https://ui.adsabs.harvard.edu/abs/2018ISPM...35d..32V},
	doi = {10.1109/MSP.2018.2826566},
	abstract = {PCA is one of the most widely used dimension reduction techniques. A related easier problem is "subspace learning" or "subspace estimation". Given relatively clean data, both are easily solved via singular value decomposition (SVD). The problem of subspace learning or PCA in the presence of outliers is called robust subspace learning or robust PCA (RPCA). For long data sequences, if one tries to use a single lower dimensional subspace to represent the data, the required subspace dimension may end up being quite large. For such data, a better model is to assume that it lies in a low-dimensional subspace that can change over time, albeit gradually. The problem of tracking such data (and the subspaces) while being robust to outliers is called robust subspace tracking (RST). This article provides a magazine-style overview of the entire field of robust subspace learning and tracking. In particular solutions for three problems are discussed in detail: RPCA via sparse+low-rank matrix decomposition (S+LR), RST via S+LR, and "robust subspace recovery (RSR)". RSR assumes that an entire data vector is either an outlier or an inlier. The S+LR formulation instead assumes that outliers occur on only a few data vector indices and hence are well modeled as sparse corruptions.},
	urldate = {2024-01-27},
	journal = {IEEE Signal Processing Magazine},
	author = {Vaswani, Namrata and Bouwmans, Thierry and Javed, Sajid and Narayanamurthy, Praneeth},
	month = jul,
	year = {2018},
	note = {ADS Bibcode: 2018ISPM...35d..32V},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Theory, Statistics - Machine Learning, Statistics - Methodology},
	pages = {32--55},
}

@article{gribonval_sample_2015,
	title = {Sample {Complexity} of {Dictionary} {Learning} and {Other} {Matrix} {Factorizations}},
	volume = {6},
	issn = {0018-9448, 1557-9654},
	url = {https://www.infona.pl//resource/bwmeta1.element.ieee-art-000007088631},
	doi = {10.1109/TIT.2015.2424238},
	abstract = {Many modern tools in machine learning and signal processing, such as sparse dictionary learning, principal component analysis, non-negative matrix factorization, \&lt;inline-formula\&gt; \&lt;tex-math notation="LaTeX"\&gt;\$K\$ \&lt;/tex-math\&gt;\&lt;/inline-formula\&gt;-means clustering, and so on, rely on the factorization of a matrix obtained by concatenating high-dimensional vectors from a training collection. While the idealized task would be to optimize the expected quality of the factors over the underlying distribution of training vectors, it is achieved in practice by minimizing an empirical average over the considered collection. The focus of this paper is to provide sample complexity estimates to uniformly control how much the empirical average deviates from the expected cost function. Standard arguments imply that the performance of the empirical predictor also exhibit such guarantees. The level of genericity of the approach encompasses several possible constraints on the factors (tensor product structure, shift-invariance, sparsity\&amp;\#x2026;), thus providing a unified perspective on the sample complexity of several widely used matrix factorization schemes. The derived generalization bounds behave proportional to \&lt;inline-formula\&gt; \&lt;tex-math notation="LaTeX"\&gt;\$({\textbackslash}log (n)/n){\textasciicircum}\{1/2\}\$ \&lt;/tex-math\&gt;\&lt;/inline-formula\&gt; with respect to the number of samples \&lt;inline-formula\&gt; \&lt;tex-math notation="LaTeX"\&gt;\$n\$ \&lt;/tex-math\&gt;\&lt;/inline-formula\&gt; for the considered matrix factorization techniques.},
	language = {English},
	number = {61},
	urldate = {2024-01-27},
	journal = {IEEE Transactions on Information Theory},
	author = {Gribonval, Remi and Jenatton, Rodolphe and Bach, Francis and Kleinsteuber, Martin and Seibert, Matthias},
	year = {2015},
	pages = {3469--3486},
}

@article{gribonval_sample_2015-1,
	title = {Sample {Complexity} of {Dictionary} {Learning} and {Other} {Matrix} {Factorizations}},
	volume = {61},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/abstract/document/7088631?casa_token=8NgExaCmZFgAAAAA:ZaMZJgqeUzl7dJkmtfhmFRftF1iY8ytJf38xlb8Z6Na3cVGM2UuhcqZsRUg6U59G9T_tuuA},
	doi = {10.1109/TIT.2015.2424238},
	abstract = {Many modern tools in machine learning and signal processing, such as sparse dictionary learning, principal component analysis, non-negative matrix factorization, K-means clustering, and so on, rely on the factorization of a matrix obtained by concatenating high-dimensional vectors from a training collection. While the idealized task would be to optimize the expected quality of the factors over the underlying distribution of training vectors, it is achieved in practice by minimizing an empirical average over the considered collection. The focus of this paper is to provide sample complexity estimates to uniformly control how much the empirical average deviates from the expected cost function. Standard arguments imply that the performance of the empirical predictor also exhibit such guarantees. The level of genericity of the approach encompasses several possible constraints on the factors (tensor product structure, shift-invariance, sparsity...), thus providing a unified perspective on the sample complexity of several widely used matrix factorization schemes. The derived generalization bounds behave proportional to (log (n)/n)1/2 with respect to the number of samples n for the considered matrix factorization techniques.},
	number = {6},
	urldate = {2024-01-27},
	journal = {IEEE Transactions on Information Theory},
	author = {Gribonval, R√©mi and Jenatton, Rodolphe and Bach, Francis and Kleinsteuber, Martin and Seibert, Matthias},
	month = jun,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Information Theory},
	keywords = {Complexity theory, Dictionary learning, K-means clustering, Principal component analysis, Probability distribution, Sparse matrices, Training, non-negative matrix factorization, principal component analysis, sample complexity, sparse coding, structured learning},
	pages = {3469--3486},
}

@article{candes_robust_2011,
	title = {Robust principal component analysis?},
	volume = {58},
	issn = {0004-5411},
	url = {https://dl.acm.org/doi/10.1145/1970392.1970395},
	doi = {10.1145/1970392.1970395},
	abstract = {This article is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the ‚Ñì1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.},
	number = {3},
	urldate = {2024-01-27},
	journal = {Journal of the ACM},
	author = {Cand√®s, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
	month = jun,
	year = {2011},
	keywords = {Principal components, duality, low-rank matrices, nuclear-norm minimization, robustness vis-a-vis outliers, sparsity, video surveillance, ‚Ñì1-norm minimization},
	pages = {11:1--11:37},
}

@inproceedings{y_yu_controllability_2021,
	title = {On {Controllability} and {Persistency} of {Excitation} in {Data}-{Driven} {Control}: {Extensions} of {Willems}‚Äô {Fundamental} {Lemma}},
	isbn = {2576-2370},
	doi = {10.1109/CDC45484.2021.9682952},
	booktitle = {2021 60th {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {{Y. Yu} and {S. Talebi} and {H. J. van Waarde} and {U. Topcu} and {M. Mesbahi} and {B. A√ßƒ±kme»ôe}},
	month = dec,
	year = {2021},
	note = {Journal Abbreviation: 2021 60th IEEE Conference on Decision and Control (CDC)},
	pages = {6485--6490},
}

@article{willems_note_2005,
	title = {A note on persistency of excitation},
	volume = {54},
	issn = {0167-6911},
	url = {https://www.sciencedirect.com/science/article/pii/S0167691104001434},
	doi = {10.1016/j.sysconle.2004.09.003},
	abstract = {We prove that if a component of the response signal of a controllable linear time-invariant system is persistently exciting of sufficiently high order, then the windows of the signal span the full system behavior. This is then applied to obtain conditions under which the state trajectory of a state representation spans the whole state space. The related question of when the matrix formed from a state sequence has linearly independent rows from the matrix formed from an input sequence and a finite number of its shifts is of central importance in subspace system identification.},
	number = {4},
	urldate = {2024-01-26},
	journal = {Systems \& Control Letters},
	author = {Willems, Jan C. and Rapisarda, Paolo and Markovsky, Ivan and De Moor, Bart L. M.},
	month = apr,
	year = {2005},
	keywords = {Annihilators, Behavioral systems, Lags, Persistency of excitation, System identification},
	pages = {325--329},
}

@article{willems_note_2005-1,
	title = {A note on persistency of excitation},
	volume = {54},
	issn = {0167-6911},
	url = {https://www.sciencedirect.com/science/article/pii/S0167691104001434},
	doi = {10.1016/j.sysconle.2004.09.003},
	abstract = {We prove that if a component of the response signal of a controllable linear time-invariant system is persistently exciting of sufficiently high order, then the windows of the signal span the full system behavior. This is then applied to obtain conditions under which the state trajectory of a state representation spans the whole state space. The related question of when the matrix formed from a state sequence has linearly independent rows from the matrix formed from an input sequence and a finite number of its shifts is of central importance in subspace system identification.},
	number = {4},
	urldate = {2024-01-26},
	journal = {Systems \& Control Letters},
	author = {Willems, Jan C. and Rapisarda, Paolo and Markovsky, Ivan and De Moor, Bart L. M.},
	month = apr,
	year = {2005},
	keywords = {Annihilators, Behavioral systems, Lags, Persistency of excitation, System identification},
	pages = {325--329},
}

@article{willems_note_2005-2,
	title = {A note on persistency of excitation},
	volume = {54},
	issn = {0167-6911},
	url = {https://www.sciencedirect.com/science/article/pii/S0167691104001434},
	doi = {10.1016/j.sysconle.2004.09.003},
	abstract = {We prove that if a component of the response signal of a controllable linear time-invariant system is persistently exciting of sufficiently high order, then the windows of the signal span the full system behavior. This is then applied to obtain conditions under which the state trajectory of a state representation spans the whole state space. The related question of when the matrix formed from a state sequence has linearly independent rows from the matrix formed from an input sequence and a finite number of its shifts is of central importance in subspace system identification.},
	number = {4},
	urldate = {2024-01-26},
	journal = {Systems \& Control Letters},
	author = {Willems, Jan C. and Rapisarda, Paolo and Markovsky, Ivan and De Moor, Bart L. M.},
	month = apr,
	year = {2005},
	keywords = {Annihilators, Behavioral systems, Lags, Persistency of excitation, System identification},
	pages = {325--329},
}

@inproceedings{yu_online_2023,
	title = {Online {Adversarial} {Stabilization} of {Unknown} {Linear} {Time}-{Varying} {Systems}},
	url = {https://ieeexplore.ieee.org/abstract/document/10383849},
	doi = {10.1109/CDC49753.2023.10383849},
	abstract = {This paper studies the problem of online stabilization of an unknown discrete-time linear time-varying (LTV) system under bounded non-stochastic (potentially adversarial) disturbances. We propose a novel algorithm based on convex body chasing (CBC). Under the assumption of infrequently changing or slowly drifting dynamics, the algorithm guarantees bounded-input-bounded-output stability in the closed loop. Our approach avoids system identification and applies, with minimal disturbance assumptions, to a variety of LTV systems of practical importance. We demonstrate the algorithm numerically on examples of LTV systems including Markov linear jump systems with finitely many jumps.},
	urldate = {2024-01-26},
	booktitle = {2023 62nd {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Yu, Jing and Gupta, Varun and Wierman, Adam},
	month = dec,
	year = {2023},
	note = {ISSN: 2576-2370},
	keywords = {Heuristic algorithms, Markov processes, Numerical stability, System identification, Time-varying systems},
	pages = {8320--8327},
}

@article{talebi_regularizability_2022,
	title = {On {Regularizability} and {Its} {Application} to {Online} {Control} of {Unstable} {LTI} {Systems}},
	volume = {67},
	issn = {1558-2523},
	url = {https://ieeexplore.ieee.org/abstract/document/9627635},
	doi = {10.1109/TAC.2021.3131148},
	abstract = {Learning, say through direct policy updates, often requires assumptions such as knowing a priori that the initial policy (gain) is stabilizing, or persistently exciting (PE) input‚Äìoutput data, is available. In this article, we examine online regulation of (possibly unstable) partially unknown linear systems with no prior access to an initial stabilizing controller nor PE input‚Äìoutput data; we instead leverage the knowledge of the input matrix for online regulation. First, we introduce and characterize the notion of ‚Äúregularizability‚Äù for linear systems that gauges the extent by which a system can be regulated in finite-time in contrast to its asymptotic behavior (commonly characterized by stabilizability/controllability). Next, having access only to the input matrix, we propose the data-guided regulation (DGR) synthesis procedure that‚Äîas its name suggests‚Äîregulates the underlying state while also generating informative data that can subsequently be used for data-driven stabilization or system identification. We further improve the computational performance of DGR via a rank-one update and demonstrate its utility in online regulation of the X-29 aircraft.},
	number = {12},
	urldate = {2024-01-26},
	journal = {IEEE Transactions on Automatic Control},
	author = {Talebi, Shahriar and Alemzadeh, Siavash and Rahimi, Niyousha and Mesbahi, Mehran},
	month = dec,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Automatic Control},
	keywords = {Eigenvalues and eigenfunctions, Heuristic algorithms, Iterative control, Linear systems, Regulation, Safety, Symmetric matrices, Trajectory, online regulation, single trajectory learning, unstable linear systems},
	pages = {6413--6428},
}

@inproceedings{tsiamis_linear_2021,
	title = {Linear {Systems} can be {Hard} to {Learn}},
	url = {https://ieeexplore.ieee.org/document/9682778},
	doi = {10.1109/CDC45484.2021.9682778},
	abstract = {In this paper, we investigate when system identification is statistically easy or hard, in the finite sample regime. Statistically easy to learn linear system classes have sample complexity that is polynomial with the system dimension. Most prior research in the finite sample regime falls in this category, focusing on systems that are directly excited by process noise. Statistically hard to learn linear system classes have worst-case sample complexity that is at least exponential with the system dimension, regardless of the identification algorithm. Using tools from minimax theory, we show that classes of linear systems can be hard to learn. Such classes include, for example, under-actuated or under-excited systems with weak coupling among the states. Having classified some systems as easy or hard to learn, a natural question arises as to what system properties fundamentally affect the hardness of system identifiability. Towards this direction, we characterize how the controllability index of linear systems affects the sample complexity of identification. More specifically, we show that the sample complexity of robustly controllable linear systems is upper bounded by an exponential function of the controllability index. This implies that identification is easy for classes of linear systems with small controllability index and potentially hard if the controllability index is large. Our analysis is based on recent statistical tools for finite sample analysis of system identification as well as a novel lower bound that relates controllability index with the least singular value of the controllability Gramian.},
	urldate = {2024-01-25},
	booktitle = {2021 60th {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Tsiamis, Anastasios and Pappas, George J.},
	month = dec,
	year = {2021},
	note = {ISSN: 2576-2370},
	keywords = {Complexity theory, Controllability, Eigenvalues and eigenfunctions, Indexes, Linear systems, System identification, Task analysis},
	pages = {2903--2910},
}

@article{schutter_minimal_2000,
	title = {Minimal state-space realization in linear system theory: an overview},
	volume = {121},
	issn = {0377-0427},
	shorttitle = {Minimal state-space realization in linear system theory},
	url = {https://www.sciencedirect.com/science/article/pii/S0377042700003411},
	doi = {10.1016/S0377-0427(00)00341-1},
	abstract = {We give a survey of the results in connection with the minimal state-space realization problem for linear time-invariant systems. We start with a brief historical overview and a short introduction to linear system theory. Next we present some of the basic algorithms for the reduction of nonminimal state-space realizations and for the minimal state-space realization of infinite or finite sequences of Markov parameters of linear time-invariant systems. Finally, we discuss some extensions of this problem to other classes of systems and point out some related problems.},
	number = {1},
	urldate = {2024-01-24},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Schutter, B. De},
	month = sep,
	year = {2000},
	keywords = {Linear system theory, Minimal realization, State-space models},
	pages = {331--354},
}

@misc{chen_consistent_2015,
	title = {Consistent {Estimation} of {Low}-{Dimensional} {Latent} {Structure} in {High}-{Dimensional} {Data}},
	url = {http://arxiv.org/abs/1510.03497},
	abstract = {We consider the problem of extracting a low-dimensional, linear latent variable structure from high-dimensional random variables. Specifically, we show that under mild conditions and when this structure manifests itself as a linear space that spans the conditional means, it is possible to consistently recover the structure using only information up to the second moments of these random variables. This finding, specialized to one-parameter exponential families whose variance function is quadratic in their means, allows for the derivation of an explicit estimator of such latent structure. This approach serves as a latent variable model estimator and as a tool for dimension reduction for a high-dimensional matrix of data composed of many related variables. Our theoretical results are verified by simulation studies and an application to genomic data.},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Chen, Xiongzhi and Storey, John D.},
	month = oct,
	year = {2015},
	note = {arXiv:1510.03497 [stat]},
	keywords = {Statistics - Machine Learning},
}

@misc{chen_consistent_2015-1,
	title = {Consistent {Estimation} of {Low}-{Dimensional} {Latent} {Structure} in {High}-{Dimensional} {Data}},
	url = {http://arxiv.org/abs/1510.03497},
	abstract = {We consider the problem of extracting a low-dimensional, linear latent variable structure from high-dimensional random variables. Specifically, we show that under mild conditions and when this structure manifests itself as a linear space that spans the conditional means, it is possible to consistently recover the structure using only information up to the second moments of these random variables. This finding, specialized to one-parameter exponential families whose variance function is quadratic in their means, allows for the derivation of an explicit estimator of such latent structure. This approach serves as a latent variable model estimator and as a tool for dimension reduction for a high-dimensional matrix of data composed of many related variables. Our theoretical results are verified by simulation studies and an application to genomic data.},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Chen, Xiongzhi and Storey, John D.},
	month = oct,
	year = {2015},
	note = {arXiv:1510.03497 [stat]},
	keywords = {Statistics - Machine Learning},
}

@article{lecun_path_nodate,
	title = {A {Path} {Towards} {Autonomous} {Machine} {Intelligence} {Version} 0.9.2, 2022-06-27},
	abstract = {How could machines learn as eÔ¨Éciently as humans and animals? How could machines learn to reason and plan? How could machines learn representations of percepts and action plans at multiple levels of abstraction, enabling them to reason, predict, and plan at multiple time horizons? This position paper proposes an architecture and training paradigms with which to construct autonomous intelligent agents. It combines concepts such as conÔ¨Ågurable predictive world model, behavior driven through intrinsic motivation, and hierarchical joint embedding architectures trained with self-supervised learning.},
	language = {en},
	author = {LeCun, Yann},
}

@article{tsiamis_statistical_2023,
	title = {Statistical {Learning} {Theory} for {Control}: {A} {Finite}-{Sample} {Perspective}},
	volume = {43},
	issn = {1941-000X},
	shorttitle = {Statistical {Learning} {Theory} for {Control}},
	url = {https://ieeexplore.ieee.org/abstract/document/10317607?casa_token=EPB1dgOaF1QAAAAA:pT4-2rKBp00IwkqdPs9KfSSkfse2KXSUFV-DKpCXQ_djy4O9joCUcXo0CMGO_KGNiJko_iU},
	doi = {10.1109/MCS.2023.3310345},
	abstract = {Learning algorithms have become an integral component to modern engineering solutions. Examples range from self-driving cars and recommender systems to finance and even critical infrastructure, many of which are typically under the purview of control theory. While these algorithms have already shown tremendous promise in certain applications [1], there are considerable challenges, in particular, with respect to guaranteeing safety and gauging fundamental limits of operation. Thus, as we integrate tools from machine learning into our systems, we also require an integrated theoretical understanding of how they operate in the presence of dynamic and system-theoretic phenomena. Over the past few years, intense efforts toward this goal‚Äîan integrated theoretical understanding of learning, dynamics, and control‚Äîhave been made. While much work remains to be done, a relatively clear and complete picture has begun to emerge for (fully observed) linear dynamical systems. These systems already allow for reasoning about concrete failure modes, thus helping to indicate a path forward. Moreover, while simple at a glance, these systems can be challenging to analyze. Recently, a host of methods from learning theory and high-dimensional statistics, not typically in the control-theoretic toolbox, have been introduced to our community. This tutorial survey serves as an introduction to these results for learning in the context of unknown linear dynamical systems (see ‚ÄúSummary‚Äù). We review the current state of the art and emphasize which tools are needed to arrive at these results. Our focus is on characterizing the sample efficiency and fundamental limits of learning algorithms. Along the way, we also delineate a number of open problems. More concretely, this article is structured as follows. We begin by revisiting recent advances in the finite-sample analysis of system identification. Next, we discuss how these finite-sample bounds can be used downstream to give guaranteed performance for learning-based offline control. The final technical section discusses the more challenging online control setting. Finally, in light of the material discussed, we outline a number of future directions.},
	number = {6},
	urldate = {2024-01-24},
	journal = {IEEE Control Systems Magazine},
	author = {Tsiamis, Anastasios and Ziemann, Ingvar and Matni, Nikolai and Pappas, George J.},
	month = dec,
	year = {2023},
	note = {Conference Name: IEEE Control Systems Magazine},
	keywords = {Algorithm design and theory, Control theory, Dynamical systems, Heuristic algorithms, Learning systems, Machine learning, Machine learning algorithms, Recommender systems, Safety, Statistical analysis, Surveys, System identification, Tutorials},
	pages = {67--97},
}

@inproceedings{tsiamis_learning_2022,
	title = {Learning to {Control} {Linear} {Systems} can be {Hard}},
	url = {https://proceedings.mlr.press/v178/tsiamis22a.html},
	abstract = {In this paper, we study the statistical difficulty of learning to control linear systems. We focus on two standard benchmarks, the sample complexity of stabilization, and the regret of the online learning of the Linear Quadratic Regulator (LQR). Prior results state that the statistical difficulty for both benchmarks scales polynomially with the system state dimension up to system-theoretic quantities. However, this does not reveal the whole picture. By utilizing minimax lower bounds for both benchmarks, we prove that there exist non-trivial classes of systems for which learning complexity scales dramatically, i.e. exponentially, with the system dimension. This situation arises in the case of underactuated systems, i.e. systems with fewer inputs than states. Such systems are structurally difficult to control and their system theoretic quantities can scale exponentially with the system dimension dominating learning complexity. Under some additional structural assumptions (bounding systems away from uncontrollability), we provide qualitatively matching upper bounds. We prove that learning complexity can be at most exponential with the controllability index of the system, that is the degree of underactuation.},
	language = {en},
	urldate = {2024-01-24},
	booktitle = {Proceedings of {Thirty} {Fifth} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Tsiamis, Anastasios and Ziemann, Ingvar M. and Morari, Manfred and Matni, Nikolai and Pappas, George J.},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {3820--3857},
}

@misc{djehiche_non_2021,
	title = {Non asymptotic estimation lower bounds for {LTI} state space models with {Cram}{\textbackslash}'er-{Rao} and van {Trees}},
	url = {http://arxiv.org/abs/2109.08582},
	doi = {10.48550/arXiv.2109.08582},
	abstract = {We study the estimation problem for linear time-invariant (LTI) state-space models with Gaussian excitation of an unknown covariance. We provide non asymptotic lower bounds for the expected estimation error and the mean square estimation risk of the least square estimator, and the minimax mean square estimation risk. These bounds are sharp with explicit constants when the matrix of the dynamics has no eigenvalues on the unit circle and are rate-optimal when they do. Our results extend and improve existing lower bounds to lower bounds in expectation of the mean square estimation risk and to systems with a general noise covariance. Instrumental to our derivation are new concentration results for rescaled sample covariances and deviation results for the corresponding multiplication processes of the covariates, a differential geometric construction of a prior on the unit operator ball of small Fisher information, and an extension of the Cram{\textbackslash}'er-Rao and van Treesinequalities to matrix-valued estimators.},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Djehiche, Boualem and Mazhar, Othmane},
	month = sep,
	year = {2021},
	note = {arXiv:2109.08582 [math, stat]},
	keywords = {62J05, 62M10, 60B20, 60E15, 62C20, 62B11, Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{djehiche_non_2021-1,
	title = {Non asymptotic estimation lower bounds for {LTI} state space models with {Cram}{\textbackslash}'er-{Rao} and van {Trees}},
	url = {http://arxiv.org/abs/2109.08582},
	doi = {10.48550/arXiv.2109.08582},
	abstract = {We study the estimation problem for linear time-invariant (LTI) state-space models with Gaussian excitation of an unknown covariance. We provide non asymptotic lower bounds for the expected estimation error and the mean square estimation risk of the least square estimator, and the minimax mean square estimation risk. These bounds are sharp with explicit constants when the matrix of the dynamics has no eigenvalues on the unit circle and are rate-optimal when they do. Our results extend and improve existing lower bounds to lower bounds in expectation of the mean square estimation risk and to systems with a general noise covariance. Instrumental to our derivation are new concentration results for rescaled sample covariances and deviation results for the corresponding multiplication processes of the covariates, a differential geometric construction of a prior on the unit operator ball of small Fisher information, and an extension of the Cram{\textbackslash}'er-Rao and van Treesinequalities to matrix-valued estimators.},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Djehiche, Boualem and Mazhar, Othmane},
	month = sep,
	year = {2021},
	note = {arXiv:2109.08582 [math, stat]},
	keywords = {62J05, 62M10, 60B20, 60E15, 62C20, 62B11, Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@inproceedings{fattahi_learning_2021,
	title = {Learning {Partially} {Observed} {Linear} {Dynamical} {Systems} from {Logarithmic} {Number} of {Samples}},
	url = {https://proceedings.mlr.press/v144/fattahi21a.html},
	abstract = {In this work, we study the problem of learning partially observed linear dynamical systems from a single sample trajectory. A major practical challenge in the existing system identification methods is the undesirable dependency of their required sample size on the system dimension: roughly speaking, they presume and rely on sample sizes that scale linearly with the system dimension. Evidently, in high-dimensional regime where the system dimension is large, it may be costly, if not impossible, to collect as many samples from the unknown system. In this paper, we introduce an regularized estimator that can accurately estimate the Markov parameters of the system, provided that the number of samples scale poly-logarithmically with the system dimension. Our result significantly improves the sample complexity of learning partially observed linear dynamical systems: it shows that the Markov parameters of the system can be learned in the high-dimensional setting, where the number of samples is significantly smaller than the system dimension.},
	language = {en},
	urldate = {2024-01-24},
	booktitle = {Proceedings of the 3rd {Conference} on {Learning} for {Dynamics} and {Control}},
	publisher = {PMLR},
	author = {Fattahi, Salar},
	month = may,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {60--72},
}

@article{jedra_finite-time_2023,
	title = {Finite-{Time} {Identification} of {Linear} {Systems}: {Fundamental} {Limits} and {Optimal} {Algorithms}},
	volume = {68},
	issn = {1558-2523},
	shorttitle = {Finite-{Time} {Identification} of {Linear} {Systems}},
	url = {https://ieeexplore.ieee.org/document/9946382},
	doi = {10.1109/TAC.2022.3221705},
	abstract = {We investigate the linear system identification problem in the so-called fixed budget and fixed confidence settings. In the fixed budget setting, the learner aims at estimating the state transition matrix A from a random system trajectory of fixed length, whereas in the fixed confidence setting, the learner also controls the length of the observed trajectory ‚Äì she can stop when she believes that enough information has been gathered. For both settings, we analyze the sample complexity in the probably approximately correct (PAC) framework defined as the length of the observed trajectory required to identify the system parameters with prescribed accuracy and confidence levels ({\textbackslash}varepsilon, {\textbackslash}delta). In the fixed budget setting, we first establish problem-specific sample complexity lower bounds. We then present a finite-time analysis of the estimation error of the least-squares estimator (LSE) for stable systems, and show that in the high-accuracy regime, the sample complexity of the LSE matches our lower bounds. Our analysis of the LSE is sharper and easier to interpret than existing analyzes, and relies on novel concentration results for the covariates matrix. In the fixed confidence setting, in addition to the estimation objective, the learner also has to decide when to stop the collection of observations. The sample complexity then corresponds to the expected stopping time. For this setting, we also provide problem specific sample complexity lower bounds. We also propose a stopping rule which combined to the LSE enjoys a sample complexity that matches our lower bounds in the high-accuracy and high-confidence regime.},
	number = {5},
	urldate = {2024-01-24},
	journal = {IEEE Transactions on Automatic Control},
	author = {Jedra, Yassir and Proutiere, Alexandre},
	month = may,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Automatic Control},
	keywords = {Complexity theory, Finite-time identification, Least mean squares methods, Linear systems, Optimization, System dynamics, Trajectory, Upper bound, fixed budget identification, fixed confidence identification, learning theory, least-squares estimation, linear systems, sample complexity lower bounds, system identification},
	pages = {2805--2820},
}

@article{bauer_order_2001,
	title = {Order estimation for subspace methods},
	volume = {37},
	issn = {0005-1098},
	url = {https://www.sciencedirect.com/science/article/pii/S0005109801001182},
	doi = {10.1016/S0005-1098(01)00118-2},
	abstract = {In this paper the question of estimating the order in the context of subspace methods is addressed. Three different approaches are presented and the asymptotic properties thereof derived. Two of these methods are based on the information contained in the estimated singular values, while the third method is based on the estimated innovation variance. The case with observed inputs is treated as well as the case without exogenous inputs. The two methods based on the singular values are shown to be consistent under fairly mild assumptions, while the same result for the third approach is only obtained on a generic set. The former can be applied to Larimore type of procedures as well as to MOESP type of procedures, whereas the third is only applied to Larimore type of algorithms. This has implications for the estimation of the order of systems, which are close to the exceptional set, as is shown in a numerical example. All the estimation methods involve the choice of a penalty term. Sufficient conditions on the penalty term to guarantee consistency are derived. The effects of different choices of the penalty term are investigated in a simulation study.},
	number = {10},
	urldate = {2024-01-24},
	journal = {Automatica},
	author = {Bauer, Dietmar},
	month = oct,
	year = {2001},
	keywords = {Asymptotic properties, Estimation, Subspace methods, System order},
	pages = {1561--1573},
}

@misc{sun_finite_2023,
	title = {Finite {Time} {Performance} {Analysis} of {MIMO} {Systems} {Identification}},
	url = {http://arxiv.org/abs/2310.11790},
	doi = {10.48550/arXiv.2310.11790},
	abstract = {This paper is concerned with the finite time identification performance of an n dimensional discrete-time Multiple-Input Multiple-Output (MIMO) Linear Time-Invariant system, with p inputs and m outputs. We prove that the widely-used Ho-Kalman algorithm and Multivariable Output Error State Space (MOESP) algorithm are ill-conditioned for MIMO system when n/m or n/p is large. Moreover, by analyzing the Cramer-Rao bound, we derive a fundamental limit for identifying the real and stable (or marginally stable) poles of MIMO system and prove that the sample complexity for any unbiased pole estimation algorithm to reach a certain level of accuracy explodes superpolynomially with respect to n/(pm). Numerical results are provided to illustrate the ill-conditionedness of Ho-Kalman algorithm and MOESP algorithm as well as the fundamental limit on identification.},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Sun, Shuai and Li, Jiayun and Mo, Yilin},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11790 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control},
}

@misc{sun_finite_2023-1,
	title = {Finite {Time} {Performance} {Analysis} of {MIMO} {Systems} {Identification}},
	url = {http://arxiv.org/abs/2310.11790},
	doi = {10.48550/arXiv.2310.11790},
	abstract = {This paper is concerned with the finite time identification performance of an n dimensional discrete-time Multiple-Input Multiple-Output (MIMO) Linear Time-Invariant system, with p inputs and m outputs. We prove that the widely-used Ho-Kalman algorithm and Multivariable Output Error State Space (MOESP) algorithm are ill-conditioned for MIMO system when n/m or n/p is large. Moreover, by analyzing the Cramer-Rao bound, we derive a fundamental limit for identifying the real and stable (or marginally stable) poles of MIMO system and prove that the sample complexity for any unbiased pole estimation algorithm to reach a certain level of accuracy explodes superpolynomially with respect to n/(pm). Numerical results are provided to illustrate the ill-conditionedness of Ho-Kalman algorithm and MOESP algorithm as well as the fundamental limit on identification.},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Sun, Shuai and Li, Jiayun and Mo, Yilin},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11790 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control},
}

@inproceedings{bakshi_new_2023,
	address = {New York, NY, USA},
	series = {{STOC} 2023},
	title = {A {New} {Approach} to {Learning} {Linear} {Dynamical} {Systems}},
	isbn = {978-1-4503-9913-5},
	url = {https://dl.acm.org/doi/10.1145/3564246.3585247},
	doi = {10.1145/3564246.3585247},
	abstract = {Linear dynamical systems are the foundational statistical model upon which control theory is built. Both the celebrated Kalman filter and the linear quadratic regulator require knowledge of the system dynamics to provide analytic guarantees. Naturally, learning the dynamics of a linear dynamical system from linear measurements has been intensively studied since Rudolph Kalman's pioneering work in the 1960's. Towards these ends, we provide the first polynomial time algorithm for learning a linear dynamical system from a polynomial length trajectory up to polynomial error in the system parameters under essentially minimal assumptions; observability, controllability, and marginal stability. Our algorithm is built on a method of moments estimator to directly estimate Markov parameters from which the dynamics can be extracted. Furthermore we provide statistical lower bounds when our observability and controllability assumptions are violated.},
	urldate = {2024-01-24},
	booktitle = {Proceedings of the 55th {Annual} {ACM} {Symposium} on {Theory} of {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Bakshi, Ainesh and Liu, Allen and Moitra, Ankur and Yau, Morris},
	month = jun,
	year = {2023},
	keywords = {controllability, information-theoretically minimal, linear dynamical systems, marginal stability, method of moments, minimal assumptions, observability},
	pages = {335--348},
}

@article{sun_finite_2022,
	title = {Finite {Sample} {Identification} of {Low}-{Order} {LTI} {Systems} via {Nuclear} {Norm} {Regularization}},
	volume = {1},
	issn = {2694-085X},
	url = {https://ieeexplore.ieee.org/document/9870857},
	doi = {10.1109/OJCSYS.2022.3200015},
	abstract = {This paper studies the problem of identifying low-order linear time-invariant systems via Hankel nuclear norm (HNN) regularization. This regularization encourages the Hankel matrix to be low-rank, which corresponds to the dynamical system being of low order. We provide novel statistical analysis for this regularization, and contrast it with the unregularized ordinary least-squares (OLS) estimator. Our analysis leads to new finite-sample error bounds on estimating the impulse response and the Hankel matrix associated with the linear system using HNN regularization. We design a suitable input excitation, and show that we can recover the system using a number of observations that scales optimally with the true system order and achieves strong statistical estimation rates. Complementing these, we also demonstrate that the input design indeed matters by proving that intuitive choices, such as i.i.d. Gaussian input, lead to sub-optimal sample complexity. To better understand the benefits of regularization, we also revisit the OLS estimator. Besides refining existing bounds, we experimentally identify when HNN regularization improves over OLS: (1) For low-order systems with slow impulse-response decay, OLS method performs poorly in terms of sample complexity, (2) the Hankel matrix returned by regularization has a more clear singular value gap that makes determining the system order easier, (3) HNN regularization is less sensitive to hyperparameter choice. To choose the regularization parameter, we also outline a simple joint train-validation procedure.},
	urldate = {2024-01-24},
	journal = {IEEE Open Journal of Control Systems},
	author = {Sun, Yue and Oymak, Samet and Fazel, Maryam},
	year = {2022},
	note = {Conference Name: IEEE Open Journal of Control Systems},
	keywords = {Complexity theory, Control systems, Estimation, Frequency-domain analysis, Identification for control, Kernel, Linear systems, Splines (mathematics), information theory for control, linear systems},
	pages = {237--254},
}

@misc{mao_finite-time_2021,
	title = {Finite-{Time} {Model} {Inference} {From} {A} {Single} {Noisy} {Trajectory}},
	url = {http://arxiv.org/abs/2010.06616},
	doi = {10.48550/arXiv.2010.06616},
	abstract = {This paper proposes a novel model inference procedure to identify system matrix from a single noisy trajectory over a finite-time interval. The proposed inference procedure comprises an observation data processor, a redundant data processor and an ordinary least-square estimator, wherein the data processors mitigate the influence of observation noise on inference error. We first systematically investigate the comparisons with naive least-square-regression based model inference and uncover that 1) the same observation data has identical influence on the feasibility of the proposed and the naive model inferences, 2) the naive model inference uses all of the redundant data, while the proposed model inference optimally uses the basis and the redundant data. We then study the sample complexity of the proposed model inference in the presence of observation noise, which leads to the dependence of the processed bias in the observed system trajectory on time and coordinates. Particularly, we derive the sample-complexity upper bound (on the number of observations sufficient to infer a model with prescribed levels of accuracy and confidence) and the sample-complexity lower bound (high-probability lower bound on model error). Finally, the proposed model inference is numerically validated and analyzed.},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Mao, Yanbing and Hovakimyan, Naira and Voulgaris, Petros and Sha, Lui},
	month = jan,
	year = {2021},
	note = {arXiv:2010.06616 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control},
}

@misc{mao_finite-time_2021-1,
	title = {Finite-{Time} {Model} {Inference} {From} {A} {Single} {Noisy} {Trajectory}},
	url = {http://arxiv.org/abs/2010.06616},
	doi = {10.48550/arXiv.2010.06616},
	abstract = {This paper proposes a novel model inference procedure to identify system matrix from a single noisy trajectory over a finite-time interval. The proposed inference procedure comprises an observation data processor, a redundant data processor and an ordinary least-square estimator, wherein the data processors mitigate the influence of observation noise on inference error. We first systematically investigate the comparisons with naive least-square-regression based model inference and uncover that 1) the same observation data has identical influence on the feasibility of the proposed and the naive model inferences, 2) the naive model inference uses all of the redundant data, while the proposed model inference optimally uses the basis and the redundant data. We then study the sample complexity of the proposed model inference in the presence of observation noise, which leads to the dependence of the processed bias in the observed system trajectory on time and coordinates. Particularly, we derive the sample-complexity upper bound (on the number of observations sufficient to infer a model with prescribed levels of accuracy and confidence) and the sample-complexity lower bound (high-probability lower bound on model error). Finally, the proposed model inference is numerically validated and analyzed.},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Mao, Yanbing and Hovakimyan, Naira and Voulgaris, Petros and Sha, Lui},
	month = jan,
	year = {2021},
	note = {arXiv:2010.06616 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control},
}

@article{deng_invariant_2020,
	title = {Invariant subspace learning for time series data based on dynamic time warping distance},
	volume = {102},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320300169},
	doi = {10.1016/j.patcog.2020.107210},
	abstract = {Low-dimensional and compact representation of time series data is of importance for mining and storage. In practice, time series data are vulnerable to various temporal transformations, such as shift and temporal scaling, however, which are unavoidable in the process of data collection. If a learning algorithm directly calculates the difference between such transformed data based on Euclidean distance, the measurement cannot faithfully reflect the similarity and hence could not learn the underlying discriminative features. In order to solve this problem, we develop a novel subspace learning algorithm based on dynamic time warping (DTW) distance which is an elastic distance defined in a DTW space. The algorithm aims to minimize the reconstruction error in the DTW space. However, since DTW space is a semi-pseudo metric space, it is difficult to generalize common subspace learning algorithms for such semi-pseudo metric spaces. In this work, we introduce warp operators with which DTW reconstruction error can be approximated by reconstruction error between transformed series and their reconstructions in a subspace. The warp operators align time series data with their linear representations in the DTW space, which is in particular important for misaligned time series, so that the subspace can be learned to obtain an intrinsic basis (dictionary) for the representation of the data. The warp operators and the subspace are optimized alternatively until reaching equilibrium. Experiments results show that the proposed algorithm outperforms traditional subspace learning algorithms and temporal transform-invariance based methods (including SIDL, Kernel PCA, and SPMC et. al), and obtains competitive results with the state-of-the-art algorithms, such as BOSS algorithm.},
	urldate = {2024-01-24},
	journal = {Pattern Recognition},
	author = {Deng, Huiqi and Chen, Weifu and Shen, Qi and Ma, Andy J. and Yuen, Pong C. and Feng, Guocan},
	month = jun,
	year = {2020},
	keywords = {Dictionary learning, Dynamic time warping (DTW), Invariant subspace learning, Time series},
	pages = {107210},
}

@article{davis_rotation_1970,
	title = {The {Rotation} of {Eigenvectors} by a {Perturbation}. {III}},
	volume = {7},
	issn = {0036-1429},
	url = {https://www.jstor.org/stable/2949580},
	abstract = {When a Hermitian linear operator is slightly perturbed, by how much can its invariant subspaces change? Given some approximations to a cluster of neighboring eigenvalues and to the corresponding eigenvectors of a real symmetric matrix, and given an estimate for the gap that separates the cluster from all other eigenvalues, how much can the subspace spanned by the eigenvectors differ from the subspace spanned by our approximations? These questions are closely related; both are investigated here. The difference between the two subspaces is characterized in terms of certain angles through which one subspace must be rotated in order most directly to reach the other. These angles unify the treatment of natural geometric, operator-theoretic and error-analytic questions concerning those subspaces. Sharp bounds upon trigonometric functions of these angles are obtained from the gap and from bounds upon either the perturbation (1st question) or a computable residual (2nd question). An example is included.},
	number = {1},
	urldate = {2024-01-23},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Davis, Chandler and Kahan, W. M.},
	year = {1970},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {1--46},
}

@article{li_relative_1998,
	title = {Relative {Perturbation} {Theory}: {II}. {Eigenspace} and {Singular} {Subspace} {Variations}},
	volume = {20},
	issn = {0895-4798},
	shorttitle = {Relative {Perturbation} {Theory}},
	url = {https://epubs.siam.org/doi/abs/10.1137/S0895479896298506},
	doi = {10.1137/S0895479896298506},
	abstract = {The classical perturbation theory for Hermitian matrix eigenvalue and singular value problems provides bounds on the absolute differences between approximate eigenvalues (singular values) and the true eigenvalues (singular values) of a matrix. These bounds may be bad news for small eigenvalues (singular values), which thereby suffer worse relative uncertainty than large ones. However, there are situations where even small eigenvalues are determined to high relative accuracy by the data much more accurately than the classical perturbation theory would indicate. In this paper, we study how eigenvalues of a Hermitian matrix A change when it is perturbed to \${\textbackslash}wtd A=D{\textasciicircum}*AD\$, where D is close to a unitary matrix, and how singular values of a (nonsquare) matrix B change when it is perturbed to \${\textbackslash}wtd B=D\_1{\textasciicircum}*BD\_2\$, where D1 and D2 are nearly unitary. It is proved that under these kinds of perturbations small eigenvalues (singular values) suffer relative changes no worse than large eigenvalues (singular values). Many well-known perturbation theorems, including the Hoffman--Wielandt and Weyl--Lidskii theorems, are extended.},
	number = {2},
	urldate = {2024-01-23},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Li, Ren-Cang},
	month = jun,
	year = {1998},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {471--492},
}

@article{stewart_perturbation_1998,
	title = {Perturbation {Theory} for the {Singular} {Value} {Decomposition}},
	url = {http://hdl.handle.net/1903/552},
	abstract = {The singular value decomposition has a number of applications in digital signal processing. However, the the decomposition must be computed from a matrix consisting of both signal and noise. It is therefore important to be able to assess the effects of the noise on the singular values and singular vectors{\textbackslash},---{\textbackslash},a problem in classical perturbation theory. In this paper we survey the perturbation theory of the singular value decomposition. (Also cross-referenced as UMIACS-TR-90-124) Appeared in SVD and Signal Processing, II, R. J. Vacarro ed., Elsevier, Amsterdam, 1991.},
	language = {en\_US},
	urldate = {2024-01-23},
	author = {Stewart, G. W.},
	month = oct,
	year = {1998},
}

@article{lakshmanan_extracting_2015,
	title = {Extracting {Low}-{Dimensional} {Latent} {Structure} from {Time} {Series} in the {Presence} of {Delays}},
	volume = {27},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/27/9/1825-1856/8103},
	doi = {10.1162/NECO_a_00759},
	abstract = {Noisy, high-dimensional time series observations can often be described by a set of low-dimensional latent variables. Commonly used methods to extract these latent variables typically assume instantaneous relationships between the latent and observed variables. In many physical systems, changes in the latent variables manifest as changes in the observed variables after time delays. Techniques that do not account for these delays can recover a larger number of latent variables than are present in the system, thereby making the latent representation more difficult to interpret. In this work, we introduce a novel probabilistic technique, time-delay gaussian-process factor analysis (TD-GPFA), that performs dimensionality reduction in the presence of a different time delay between each pair of latent and observed variables. We demonstrate how using a gaussian process to model the evolution of each latent variable allows us to tractably learn these delays over a continuous domain. Additionally, we show how TD-GPFA combines temporal smoothing and dimensionality reduction into a common probabilistic framework. We present an expectation/conditional maximization either (ECME) algorithm to learn the model parameters. Our simulations demonstrate that when time delays are present, TD-GPFA is able to correctly identify these delays and recover the latent space. We then applied TD-GPFA to the activity of tens of neurons recorded simultaneously in the macaque motor cortex during a reaching task. TD-GPFA is able to better describe the neural activity using a more parsimonious latent space than GPFA, a method that has been used to interpret motor cortex data but does not account for time delays. More broadly, TD-GPFA can help to unravel the mechanisms underlying high-dimensional time series data by taking into account physical delays in the system.},
	language = {en},
	number = {9},
	urldate = {2024-01-23},
	journal = {Neural Computation},
	author = {Lakshmanan, Karthik C. and Sadtler, Patrick T. and Tyler-Kabara, Elizabeth C. and Batista, Aaron P. and Yu, Byron M.},
	month = sep,
	year = {2015},
	pages = {1825--1856},
}

@article{dong_extracting_2022,
	title = {Extracting a low-dimensional predictable time series},
	volume = {23},
	issn = {1389-4420, 1573-2924},
	url = {https://link.springer.com/10.1007/s11081-021-09643-x},
	doi = {10.1007/s11081-021-09643-x},
	abstract = {Large scale multi-dimensional time series can be found in many disciplines, including finance, econometrics, biomedical engineering, and industrial engineering systems. It has long been recognized that the time dependent components of the vector time series often reside in a subspace, leaving its complement independent over time. In this paper we develop a method for projecting the time series onto a low-dimensional time-series that is predictable , in the sense that an auto-regressive model achieves low prediction error. Our formulation and method follow ideas from principal component analysis, so we refer to the extracted low-dimensional time series as principal time series . In one special case we can compute the optimal projection exactly; in others, we give a heuristic method that seems to work well in practice. The effectiveness of the method is demonstrated on synthesized and real time series.},
	language = {en},
	number = {2},
	urldate = {2024-01-23},
	journal = {Optimization and Engineering},
	author = {Dong, Yining and Qin, S. Joe and Boyd, Stephen P.},
	month = jun,
	year = {2022},
	pages = {1189--1214},
}

@misc{noauthor_estimation_nodate,
	title = {{ESTIMATION} {OF} {THE} {VECTOR} {MOVING} {AVERAGE} {MODEL} {BY} {VECTOR} {AUTOREGRESSION}},
	url = {https://www-tandfonline-com.ezp-prod1.hul.harvard.edu/doi/epdf/10.1081/ETC-120014349?needAccess=true},
	language = {en},
	urldate = {2024-01-22},
	doi = {10.1081/ETC-120014349},
}

@article{kascha_comparison_2012,
	title = {A {Comparison} of {Estimation} {Methods} for {Vector} {Autoregressive} {Moving}-{Average} {Models}},
	volume = {31},
	issn = {0747-4938},
	url = {https://doi.org/10.1080/07474938.2011.607343},
	doi = {10.1080/07474938.2011.607343},
	abstract = {Recently, there has been a renewed interest in modeling economic time series by vector autoregressive moving-average models. However, this class of models has been unpopular in practice because of estimation problems and the complexity of the identification stage. These disadvantages could have led to the dominant use of vector autoregressive models in macroeconomic research. In this article, several simple estimation methods for vector autoregressive moving-average models are compared among each other and with pure vector autoregressive modeling using ordinary least squares by means of a Monte Carlo study. Different evaluation criteria are used to judge the relative performances of the algorithms.},
	number = {3},
	urldate = {2024-01-22},
	journal = {Econometric Reviews},
	author = {Kascha, Christian},
	month = may,
	year = {2012},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/07474938.2011.607343},
	keywords = {C15, C32, C63, Estimation algorithms, Forecasting, VARMA models},
	pages = {297--324},
}

@misc{noauthor_full_nodate,
	title = {Full article: {ESTIMATION} {OF} {THE} {VECTOR} {MOVING} {AVERAGE} {MODEL} {BY} {VECTOR} {AUTOREGRESSION}},
	url = {https://www-tandfonline-com.ezp-prod1.hul.harvard.edu/doi/full/10.1081/ETC-120014349},
	urldate = {2024-01-22},
}

@article{lutkepohl_estimating_1991,
	title = {Estimating {Orthogonal} {Impulse} {Responses} via {Vector} {Autoregressive} {Models}},
	volume = {7},
	issn = {0266-4666},
	url = {https://www.jstor.org/stable/3532394},
	abstract = {Impulse response functions from time series models are standard tools for analyzing the relationship between economic variables. The asymptotic distribution of orthogonalized impulse responses is derived under the assumption that finite order vector autoregressive (VAR) models are fitted to time series generated by possibly infinite order processes. The resulting asymptotic distributions of forecast error variance decompositions are also given.},
	number = {4},
	urldate = {2024-01-22},
	journal = {Econometric Theory},
	author = {L√ºtkepohl, Helmut and Poskitt, D. S.},
	year = {1991},
	note = {Publisher: Cambridge University Press},
	pages = {487--496},
}

@article{poloni_closed-form_2019,
	title = {Closed-form results for vector moving average models with a univariate estimation approach},
	volume = {10},
	issn = {2452-3062},
	url = {https://www.sciencedirect.com/science/article/pii/S2452306218300327},
	doi = {10.1016/j.ecosta.2018.06.003},
	abstract = {The estimation of a vector moving average (VMA) process represents a challenging task since the likelihood estimator is extremely slow to converge, even for small-dimensional systems. An alternative estimation method is provided, based on computing several aggregations of the variables of the system and applying likelihood estimators to the resulting univariate processes; the VMA parameters are then recovered using linear algebra tools. This avoids the complexity of maximizing the multivariate likelihood directly. Closed-form results are presented and used to compute the parameters of the process as a function of its autocovariances, using linear algebra tools. Then, an autocovariance estimation method based on the estimation of univariate models only is introduced. It is proved that the resulting estimator is consistent and asymptotically normal. A Monte Carlo simulation shows the good performance of this estimator in small samples.},
	urldate = {2024-01-22},
	journal = {Econometrics and Statistics},
	author = {Poloni, Federico and Sbrana, Giacomo},
	month = apr,
	year = {2019},
	keywords = {Canonical factorization, Maximum likelihood, VARMA estimation},
	pages = {27--52},
}

@article{zhang_identification_2020,
	title = {On the {Identification} of {Noise} {Covariances} and {Adaptive} {Kalman} {Filtering}: {A} {New} {Look} at a 50 {Year}-{Old} {Problem}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {On the {Identification} of {Noise} {Covariances} and {Adaptive} {Kalman} {Filtering}},
	url = {https://ieeexplore.ieee.org/document/9044358/},
	doi = {10.1109/ACCESS.2020.2982407},
	abstract = {The Kalman Ô¨Ålter requires knowledge of the noise statistics; however, the noise covariances are generally unknown. Although this problem has a long history, reliable algorithms for their estimation are scant, and necessary and sufÔ¨Åcient conditions for identiÔ¨Åability of the covariances are in dispute. We address both of these issues in this paper. We Ô¨Årst present the necessary and sufÔ¨Åcient condition for unknown noise covariance estimation; these conditions are related to the rank of a matrix involving the auto and cross-covariances of a weighted sum of innovations, where the weights are the coefÔ¨Åcients of the minimal polynomial of the closed-loop system transition matrix of a stable, but not necessarily optimal, Kalman Ô¨Ålter. We present an optimization criterion and a novel six-step approach based on a successive approximation, coupled with a gradient algorithm with adaptive step sizes, to estimate the steady-state Kalman Ô¨Ålter gain, the unknown noise covariance matrices, as well as the state prediction (and updated) error covariance matrix. Our approach enforces the structural assumptions on unknown noise covariances and ensures symmetry and positive deÔ¨Åniteness of the estimated covariance matrices. We provide several approaches to estimate the unknown measurement noise covariance R via post-Ô¨Åt residuals, an approach not yet exploited in the literature. The validation of the proposed method on Ô¨Åve different test cases from the literature demonstrates that the proposed method signiÔ¨Åcantly outperforms previous state-of-the-art methods. It also offers a number of novel machine learning motivated approaches, such as sequential (one sample at a time) and mini-batchbased methods, to speed up the computations.},
	language = {en},
	urldate = {2024-01-22},
	journal = {IEEE Access},
	author = {Zhang, Lingyi and Sidoti, David and Bienkowski, Adam and Pattipati, Krishna R. and Bar-Shalom, Yaakov and Kleinman, David L.},
	year = {2020},
	pages = {59362--59388},
}

@inproceedings{sarkar_near_2019,
	title = {Near optimal finite time identification of arbitrary linear dynamical systems},
	url = {https://proceedings.mlr.press/v97/sarkar19a.html},
	abstract = {We derive finite time error bounds for estimating general linear time-invariant (LTI) systems from a single observed trajectory using the method of least squares. We provide the first analysis of the general case when eigenvalues of the LTI system are arbitrarily distributed in three regimes: stable, marginally stable, and explosive. Our analysis yields sharp upper bounds for each of these cases separately. We observe that although the underlying process behaves quite differently in each of these three regimes, the systematic analysis of a self‚Äìnormalized martingale difference term helps bound identification error up to logarithmic factors of the lower bound. On the other hand, we demonstrate that the least squares solution may be statistically inconsistent under certain conditions even when the signal-to-noise ratio is high.},
	language = {en},
	urldate = {2024-01-22},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sarkar, Tuhin and Rakhlin, Alexander},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {5610--5618},
}

@article{zheng_non-asymptotic_2021,
	title = {Non-{Asymptotic} {Identification} of {Linear} {Dynamical} {Systems} {Using} {Multiple} {Trajectories}},
	volume = {5},
	issn = {2475-1456},
	url = {https://ieeexplore.ieee.org/document/9284539},
	doi = {10.1109/LCSYS.2020.3042924},
	abstract = {This letter considers the problem of linear time-invariant (LTI) system identification using input/output data. Recent work has provided non-asymptotic results on partially observed LTI system identification using a single trajectory but is only suitable for stable systems. We provide finite-time analysis for learning Markov parameters based on the ordinary least-squares (OLS) estimator using multiple trajectories, which covers both stable and unstable systems. For unstable systems, our results suggest that the Markov parameters are harder to estimate in the presence of process noise. Without process noise, our upper bound on the estimation error is independent of the spectral radius of system dynamics with high probability. These two features are different from fully observed LTI systems for which recent work has shown that unstable systems with a bigger spectral radius are easier to estimate. Extensive numerical experiments demonstrate the performance of our OLS estimator.},
	number = {5},
	urldate = {2024-01-22},
	journal = {IEEE Control Systems Letters},
	author = {Zheng, Yang and Li, Na},
	month = nov,
	year = {2021},
	note = {Conference Name: IEEE Control Systems Letters},
	pages = {1693--1698},
}

@misc{oymak_non-asymptotic_2019,
	title = {Non-asymptotic {Identification} of {LTI} {Systems} from a {Single} {Trajectory}},
	url = {http://arxiv.org/abs/1806.05722},
	abstract = {We consider the problem of learning a realization for a linear time-invariant (LTI) dynamical system from input/output data. Given a single input/output trajectory, we provide finite time analysis for learning the system's Markov parameters, from which a balanced realization is obtained using the classical Ho-Kalman algorithm. By proving a stability result for the Ho-Kalman algorithm and combining it with the sample complexity results for Markov parameters, we show how much data is needed to learn a balanced realization of the system up to a desired accuracy with high probability.},
	urldate = {2024-01-08},
	publisher = {arXiv},
	author = {Oymak, Samet and Ozay, Necmiye},
	month = feb,
	year = {2019},
	note = {arXiv:1806.05722 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@inproceedings{simchowitz_learning_2018,
	title = {Learning {Without} {Mixing}: {Towards} {A} {Sharp} {Analysis} of {Linear} {System} {Identification}},
	shorttitle = {Learning {Without} {Mixing}},
	url = {https://proceedings.mlr.press/v75/simchowitz18a.html},
	abstract = {We prove that the ordinary least-squares (OLS) estimator attains nearly minimax optimal performance for the identification of linear dynamical systems from a single observed trajectory. Our upper bound relies on a generalization of Mendelson‚Äôs small-ball method to dependent data, eschewing the use of standard mixing-time arguments. Our lower bounds reveal that these upper bounds match up to logarithmic factors. In particular, we capture the correct signal-to-noise behavior of the problem, showing that {\textbackslash}emph\{more unstable\} linear systems are {\textbackslash}emph\{easier\} to estimate. This behavior is qualitatively different from arguments which rely on mixing-time calculations that suggest that unstable systems are more difficult to estimate. We generalize our technique to provide bounds for a more general class of linear response time-series.},
	language = {en},
	urldate = {2024-01-22},
	booktitle = {Proceedings of the 31st  {Conference} {On} {Learning} {Theory}},
	publisher = {PMLR},
	author = {Simchowitz, Max and Mania, Horia and Tu, Stephen and Jordan, Michael I. and Recht, Benjamin},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {439--473},
}

@article{shirani_faradonbeh_finite_2018,
	title = {Finite time identification in unstable linear systems},
	volume = {96},
	issn = {0005-1098},
	url = {https://www.sciencedirect.com/science/article/pii/S0005109818303546},
	doi = {10.1016/j.automatica.2018.07.008},
	abstract = {Identification of the parameters of stable linear dynamical systems is a well-studied problem in the literature, both in the low and high-dimensionalsettings. However, there are hardly any results for the unstable case, especially regarding finite time bounds. For this setting, classical results on least-squares estimation of the dynamics parameters are not applicable and therefore new concepts and technical approaches need to be developed to address the issue. Unstable linear systems arise in key real applications in control theory, econometrics, and finance. This study establishes finite time bounds for the identification error of the least-squares estimates for a fairly large class of heavy-tailed noise distributions, and transition matrices of such systems. The results relate the time length (samples) required for estimation to a function of the problem dimension and key characteristics of the true underlying transition matrix and the noise distribution. To establish them, appropriate concentration inequalities for random matrices and for sequences of martingale differences are leveraged.},
	urldate = {2024-01-21},
	journal = {Automatica},
	author = {Shirani Faradonbeh, Mohamad Kazem and Tewari, Ambuj and Michailidis, George},
	month = oct,
	year = {2018},
	keywords = {Adaptive control, Autoregressive process, Finite time stabilization, Linear dynamics, Non-asymptotic estimation, Stochastic control},
	pages = {342--353},
}

@inproceedings{chen_black-box_2021,
	title = {Black-{Box} {Control} for {Linear} {Dynamical} {Systems}},
	url = {https://proceedings.mlr.press/v134/chen21c.html},
	abstract = {We consider the problem of black-box control: the task of controlling an unknown linear time-invariant dynamical system from a single trajectory without a stabilizing controller. Under the assumption that the system is controllable, we give the first \{{\textbackslash}it efficient\} algorithm that is capable of attaining sublinear regret under the setting of online nonstochastic control. This resolves an open problem since the work of Abbasi-Yadkori and Szepesvari(2011) on the stochastic LQR problem, and in a more challenging setting that allows for adversarial perturbations and adversarially chosen changing convex loss functions. We give finite-time regret bounds for our algorithm on the order of 2ùëùùëúùëôùë¶(ùëë)+ùëÇÃÉ(ùëùùëúùëôùë¶(ùëë)ùëá2/3)2poly(d)+O{\textasciitilde}(poly(d)T2/3)2{\textasciicircum}\{poly(d)\} + {\textbackslash}tilde\{O\}(poly(d) T{\textasciicircum}\{2/3\}) for general nonstochastic control, and 2ùëùùëúùëôùë¶(ùëë)+ùëÇÃÉ(ùëùùëúùëôùë¶(ùëë)ùëá‚Äæ‚Äæ‚àö)2poly(d)+O{\textasciitilde}(poly(d)T)2{\textasciicircum}\{poly(d)\} + {\textbackslash}tilde\{O\}(poly(d) {\textbackslash}sqrt\{T\}) for black-box LQR. To complete the picture, we investigate the complexity of the online black-box control problem and give a matching regret lower bound of 2Œ©(ùëë)2Œ©(d)2{\textasciicircum}\{{\textbackslash}Omega(d)\}, showing that the exponential cost is inevitable. This lower bound holds even in the noiseless setting, and applies to any, randomized or deterministic, black-box control method.},
	language = {en},
	urldate = {2024-01-21},
	booktitle = {Proceedings of {Thirty} {Fourth} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Chen, Xinyi and Hazan, Elad},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {1114--1143},
}

@article{stewart_perturbation_nodate,
	title = {Perturbation {Theory} for the {Singular} {Value} {Decomposition}},
	abstract = {The singular value decomposition has a number of applications in digital signal processing. However, the the decomposition must be computed from a matrix consisting of both signal and noise. It is therefore important to be able to assess the e ects of the noise on the singular values and singular vectors {\textbar} a problem in classical perturbation theory. In this paper we survey the perturbation theory of the singular value decomposition.},
	language = {en},
	author = {Stewart, G W},
}

@article{sarkar_finite_2021,
	title = {Finite time {LTI} system identification},
	volume = {22},
	issn = {1532-4435},
	abstract = {We address the problem of learning the parameters of a stable linear time invariant (LTI) system with unknown latent space dimension, or order, from a single time‚Äîseries of noisy input-output data. We focus on learning the best lower order approximation allowed by finite data. Motivated by subspace algorithms in systems theory, where the doubly infinite system Hankel matrix captures both order and good lower order approximations, we construct a Hankel-like matrix from noisy finite data using ordinary least squares. This circumvents the non-convexities that arise in system identification, and allows accurate estimation of the underlying LTI system. Our results rely on careful analysis of self-normalized martingale difference terms that helps bound identification error up to logarithmic factors of the lower bound. We provide a data-dependent scheme for order selection and find an accurate realization of system parameters, corresponding to that order, by an approach that is closely related to the Ho-Kalman subspace algorithm. We demonstrate that the proposed model order selection procedure is not overly conservative, i.e., for the given data length it is not possible to estimate higher order models or find higher order approximations with reasonable accuracy.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Sarkar, Tuhin and Rakhlin, Alexander and Dahleh, Munther A.},
	month = jan,
	year = {2021},
	keywords = {control theory, linear dynamical systems, non-parametric statistics, statistical learning theory, system identification},
	pages = {26:1186--26:1246},
}

@inproceedings{jedra_sample_2019,
	title = {Sample {Complexity} {Lower} {Bounds} for {Linear} {System} {Identification}},
	url = {https://ieeexplore.ieee.org/document/9029303},
	doi = {10.1109/CDC40024.2019.9029303},
	abstract = {This paper establishes problem-specific sample complexity lower bounds for linear system identification problems. The sample complexity is defined in the PAC framework: it corresponds to the time it takes to identify the system parameters with prescribed accuracy and confidence levels. By problem-specific, we mean that the lower bound explicitly depends on the system to be identified (which contrasts with minimax lower bounds), and hence really captures the identification hardness specific to the system. We consider both uncontrolled and controlled systems. For uncontrolled systems, the lower bounds are valid for any linear system, stable or not, and only depend on the system finite-time controllability gramian. A simplified lower bound depending on the spectrum of the system only is also derived. In view of recent finite-time analysis of classical estimation methods (e.g. ordinary least squares), our sample complexity lower bounds are tight for many systems. For controlled systems, our lower bounds are not as explicit as in the case of uncontrolled systems, but could well provide interesting insights into the design of control policy with minimal sample complexity.},
	urldate = {2024-01-17},
	booktitle = {2019 {IEEE} 58th {Conference} on {Decision} and {Control} ({CDC})},
	author = {Jedra, Yassir and Proutiere, Alexandre},
	month = dec,
	year = {2019},
	note = {ISSN: 2576-2370},
	pages = {2676--2681},
}

@book{van_overschee_subspace_1996,
	address = {Boston, MA},
	title = {Subspace {Identification} for {Linear} {Systems}},
	isbn = {978-1-4613-8061-0 978-1-4613-0465-4},
	url = {http://link.springer.com/10.1007/978-1-4613-0465-4},
	language = {en},
	urldate = {2024-01-17},
	publisher = {Springer US},
	author = {Van Overschee, Peter and De Moor, Bart},
	year = {1996},
	doi = {10.1007/978-1-4613-0465-4},
	keywords = {Matlab, Signal, algebra, algorithm, dynamical systems, manufacturing, mechatronics, model, optimal control, production, system, systems theory},
}

@article{recht_guaranteed_2010,
	title = {Guaranteed {Minimum}-{Rank} {Solutions} of {Linear} {Matrix} {Equations} via {Nuclear} {Norm} {Minimization}},
	copyright = {Copyright Society for Industrial and Applied Mathematics Sep 2010},
	issn = {00361445},
	url = {https://www.proquest.com/docview/749711670/abstract/14A3109501834448PQ/1},
	abstract = {The affine rank minimization problem consists of finding a matrix of minimum rank that satisfies a given system of linear equality constraints. Such problems have appeared in the literature of a diverse set of fields including system identification and control, Euclidean embedding, and collaborative filtering. Although specific instances can often be solved with specialized algorithms, the general affine rank minimization problem is NP-hard because it contains vector cardinality minimization as a special case. In this paper, we show that if a certain restricted isometry property holds for the linear transformation defining the constraints, the minimum-rank solution can be recovered by solving a convex optimization problem, namely, the minimization of the nuclear norm over the given affine space. We present several random ensembles of equations where the restricted isometry property holds with overwhelming probability, provided the codimension of the subspace is sufficiently large. The techniques used in our analysis have strong parallels in the compressed sensing framework. We discuss how affine rank minimization generalizes this preexisting concept and outline a dictionary relating concepts from cardinality minimization to those of rank minimization. We also discuss several algorithmic approaches to minimizing the nuclear norm and illustrate our results with numerical examples. [PUBLICATION ABSTRACT]},
	language = {English},
	urldate = {2023-11-02},
	author = {Recht, Benjamin and Fazel, Maryam and Parrilo, Pablo A.},
	month = sep,
	year = {2010},
	note = {Num Pages: 471-501
Publisher: Society for Industrial and Applied Mathematics},
	pages = {471--501},
}

@inproceedings{fazel_rank_2004,
	address = {Boston, MA, USA},
	title = {Rank minimization and applications in system theory},
	isbn = {978-0-7803-8335-7},
	url = {https://ieeexplore.ieee.org/document/1384521/},
	doi = {10.23919/ACC.2004.1384521},
	abstract = {In this tutorial paper, we consider the problem Of minimizing the rank of a matrix over a convex set. The Rank Minimization Problem (RMP) arises in diverse areas such as control, system identification, statistics and signal processing, and is known to be computationally NP-hard. We give an overview of the problem, its interpretations, applications, and solution methods. In particular, we focus on how convex optimization can he used to develop heuristic methods for this problem.},
	language = {en},
	urldate = {2023-11-02},
	booktitle = {Proceedings of the 2004 {American} {Control} {Conference}},
	publisher = {IEEE},
	author = {Fazel, M. and Hindi, H. and Boyd, S.},
	year = {2004},
	pages = {3273--3278 vol.4},
}

@inproceedings{fazel_rank_2001,
	address = {Arlington, VA, USA},
	title = {A rank minimization heuristic with application to minimum order system approximation},
	isbn = {978-0-7803-6495-0},
	url = {http://ieeexplore.ieee.org/document/945730/},
	doi = {10.1109/ACC.2001.945730},
	abstract = {Several problems arising in control system analysis and design,such as reduced order controller synthesis, involve minimizing the rank of a matrix variable subject to linear matrix inequality (LMI) constraints. Except in some special cases, solving this rank minimization probiem (globally) is very difficult. One simple and surprisingly effective heuristic, applicable when the matrix variable is symmetric and positive semidefinite, is to minimize its trace in place of its rank. This results in a semidefinite program (SDP)which can be efficiently solved.},
	language = {en},
	urldate = {2023-11-02},
	booktitle = {Proceedings of the 2001 {American} {Control} {Conference}. ({Cat}. {No}.{01CH37148})},
	publisher = {IEEE},
	author = {Fazel, M. and Hindi, H. and Boyd, S.P.},
	year = {2001},
	pages = {4734--4739 vol.6},
}

@article{fazel_hankel_2013,
	title = {Hankel {Matrix} {Rank} {Minimization} with {Applications} to {System} {Identification} and {Realization}},
	volume = {34},
	issn = {0895-4798, 1095-7162},
	url = {http://epubs.siam.org/doi/10.1137/110853996},
	doi = {10.1137/110853996},
	abstract = {We introduce a Ô¨Çexible optimization framework for nuclear norm minimization of matrices with linear structure, including Hankel, Toeplitz and moment structures, and catalog applications from diverse Ô¨Åelds under this framework. We discuss various Ô¨Årst-order methods for solving the resulting optimization problem, including alternating direction methods of multipliers, proximal point algorithms and gradient projection methods. We perform computational experiments to compare these methods on system identiÔ¨Åcation problem and system realization problem. For the system identiÔ¨Åcation problem, the gradient projection method (accelerated by Nesterov‚Äôs extrapolation techniques) and the proximal point algorithm usually outperform other Ô¨Årst-order methods in terms of CPU time on both real and simulated data, for small and large regularization parameters respectively; while for the system realization problem, the alternating direction method of multipliers, as applied to a certain primal reformulation, usually outperforms other Ô¨Årst-order methods in terms of CPU time. We also study the convergence of the proximal alternating directions methods of multipliers used in this paper.},
	language = {en},
	number = {3},
	urldate = {2023-11-02},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Fazel, Maryam and Pong, Ting Kei and Sun, Defeng and Tseng, Paul},
	month = jan,
	year = {2013},
	pages = {946--977},
}
