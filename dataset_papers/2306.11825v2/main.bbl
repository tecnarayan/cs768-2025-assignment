\begin{thebibliography}{25}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,
  Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt4}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
  Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
  Shyamal Anadkat, et~al. 2023.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}.

\bibitem[{Anderson et~al.(2017)Anderson, Fernando, Johnson, and
  Gould}]{anderson2017guided}
Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2017.
\newblock Guided open vocabulary image captioning with constrained beam search.
\newblock In \emph{{EMNLP}}, pages 936--945. Association for Computational
  Linguistics.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al. 2020.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:1877--1901.

\bibitem[{Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma et~al.}]{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al. 2022.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv:2210.11416}.

\bibitem[{Dathathri et~al.(2020)Dathathri, Madotto, Lan, Hung, Frank, Molino,
  Yosinski, and Liu}]{dathathri2020pplm}
Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero
  Molino, Jason Yosinski, and Rosanne Liu. 2020.
\newblock Plug and play language models: {A} simple approach to controlled text
  generation.
\newblock In \emph{{ICLR}}. OpenReview.net.

\bibitem[{Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and
  Sui}]{dong2022survey}
Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun,
  Jingjing Xu, and Zhifang Sui. 2022.
\newblock A survey for in-context learning.
\newblock \emph{arXiv preprint arXiv:2301.00234}.

\bibitem[{Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and
  Chen}]{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen. 2021.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}.

\bibitem[{Keskar et~al.(2019)Keskar, McCann, Varshney, Xiong, and
  Socher}]{keskar2019ctrl}
Nitish~Shirish Keskar, Bryan McCann, Lav~R. Varshney, Caiming Xiong, and
  Richard Socher. 2019.
\newblock {CTRL:} {A} conditional transformer language model for controllable
  generation.
\newblock \emph{CoRR}, abs/1909.05858.

\bibitem[{Krause et~al.(2021)Krause, Gotmare, McCann, Keskar, Joty, Socher, and
  Rajani}]{krause2021gedi}
Ben Krause, Akhilesh~Deepak Gotmare, Bryan McCann, Nitish~Shirish Keskar,
  Shafiq~R. Joty, Richard Socher, and Nazneen~Fatema Rajani. 2021.
\newblock Gedi: Generative discriminator guided sequence generation.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  {EMNLP} 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November,
  2021}, pages 4929--4952. Association for Computational Linguistics.

\bibitem[{Lester et~al.(2021)Lester, Al{-}Rfou, and
  Constant}]{letster2021power}
Brian Lester, Rami Al{-}Rfou, and Noah Constant. 2021.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{{EMNLP} {(1)}}, pages 3045--3059. Association for
  Computational Linguistics.

\bibitem[{Li and Liang(2021)}]{li2021prefix}
Xiang~Lisa Li and Percy Liang. 2021.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In \emph{{ACL/IJCNLP} {(1)}}, pages 4582--4597. Association for
  Computational Linguistics.

\bibitem[{Lin et~al.(2020)Lin, Zhou, Shen, Zhou, Bhagavatula, Choi, and
  Ren}]{lin2019comgen}
Bill~Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula,
  Yejin Choi, and Xiang Ren. 2020.
\newblock Commongen: A constrained text generation challenge for generative
  commonsense reasoning.
\newblock \emph{Findings of EMNLP}.

\bibitem[{Lin and Riedl(2021)}]{lin2021plug}
Zhiyu Lin and Mark~O. Riedl. 2021.
\newblock Plug-and-blend: {A} framework for plug-and-play controllable story
  generation with sketches.
\newblock In \emph{{AIIDE}}, pages 58--65. {AAAI} Press.

\bibitem[{Liu et~al.(2021)Liu, Sap, Lu, Swayamdipta, Bhagavatula, Smith, and
  Choi}]{liu2021dexpert}
Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula,
  Noah~A. Smith, and Yejin Choi. 2021.
\newblock Dexperts: Decoding-time controlled text generation with experts and
  anti-experts.
\newblock In \emph{{ACL/IJCNLP} {(1)}}, pages 6691--6706. Association for
  Computational Linguistics.

\bibitem[{Lu et~al.(2021{\natexlab{a}})Lu, Welleck, West, Jiang, Kasai,
  Khashabi, Bras, Qin, Yu, Zellers et~al.}]{lu2021astarneurologic}
Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi,
  Ronan~Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, et~al.
  2021{\natexlab{a}}.
\newblock Neurologic a* esque decoding: Constrained text generation with
  lookahead heuristics.
\newblock \emph{arXiv preprint arXiv:2112.08726}.

\bibitem[{Lu et~al.(2021{\natexlab{b}})Lu, West, Zellers, Bras, Bhagavatula,
  and Choi}]{lu2021neurologic}
Ximing Lu, Peter West, Rowan Zellers, Ronan~Le Bras, Chandra Bhagavatula, and
  Yejin Choi. 2021{\natexlab{b}}.
\newblock Neurologic decoding: (un)supervised neural text generation with
  predicate logic constraints.
\newblock In \emph{{NAACL-HLT}}, pages 4288--4299. Association for
  Computational Linguistics.

\bibitem[{Meng et~al.(2022)Meng, Lu, Peng, and Chang}]{tao2022controllable}
Tao Meng, Sidi Lu, Nanyun Peng, and Kai-Wei Chang. 2022.
\newblock \href
  {https://proceedings.neurips.cc/paper_files/paper/2022/file/b40d5797756800c97f3d525c2e4c8357-Paper-Conference.pdf}
  {Controllable text generation with neurally-decomposed oracle}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~35, pages 28125--28139. Curran Associates, Inc.

\bibitem[{Post et~al.(2013)Post, Kumar, Lopez, Karakos, Callison-Burch, and
  Khudanpur}]{post2013improved}
Matt Post, Gaurav Kumar, Adam Lopez, Damianos Karakos, Chris Callison-Burch,
  and Sanjeev Khudanpur. 2013.
\newblock Improved speech-to-text translation with the fisher and callhome
  spanish-english speech translation corpus.
\newblock In \emph{Proceedings of the 10th International Workshop on Spoken
  Language Translation: Papers}.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever
  et~al.}]{radford2019gpt2}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al. 2019.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1(8):9.

\bibitem[{Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu}]{raffel2020t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu. 2020.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{J. Mach. Learn. Res.}, 21:140:1--140:67.

\bibitem[{Salesky et~al.(2019)Salesky, Sperber, and Waibel}]{salesky2019fluent}
Elizabeth Salesky, Matthias Sperber, and Alexander Waibel. 2019.
\newblock Fluent translations from disfluent speech in end-to-end speech
  translation.
\newblock In \emph{{NAACL-HLT} {(1)}}, pages 2786--2792. Association for
  Computational Linguistics.

\bibitem[{Shin et~al.(2020)Shin, Razeghi, IV, Wallace, and
  Singh}]{shin2020autoprompt}
Taylor Shin, Yasaman Razeghi, Robert L.~Logan IV, Eric Wallace, and Sameer
  Singh. 2020.
\newblock Autoprompt: Eliciting knowledge from language models with
  automatically generated prompts.
\newblock In \emph{{EMNLP} {(1)}}, pages 4222--4235. Association for
  Computational Linguistics.

\bibitem[{Yang and Klein(2021)}]{yang2021fudge}
Kevin Yang and Dan Klein. 2021.
\newblock {FUDGE:} controlled text generation with future discriminators.
\newblock In \emph{{NAACL-HLT}}, pages 3511--3535. Association for
  Computational Linguistics.

\bibitem[{Zhang et~al.(2022)Zhang, Song, Li, Zhou, and Song}]{zhang2022survey}
Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, and Dawei Song. 2022.
\newblock A survey of controllable text generation using transformer-based
  pre-trained language models.
\newblock \emph{CoRR}, abs/2201.05337.

\bibitem[{Zhang et~al.(2023)Zhang, Dang, Peng, and Van~den
  Broeck}]{zhang2023gelato}
Honghua Zhang, Meihua Dang, Nanyun Peng, and Guy Van~den Broeck. 2023.
\newblock Tractable control for autoregressive language generation.
\newblock In \emph{International Conference on Machine Learning}, pages
  40932--40945. PMLR.

\end{thebibliography}
