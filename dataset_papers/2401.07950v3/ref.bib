@article{ai4science2023impact,
  title={The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4},
  author={AI4Science, Microsoft Research and Quantum, Microsoft Azure},
  journal={arXiv preprint arXiv:2311.07361},
  year={2023}
}

@article{taylor2022galactica,
  title={Galactica: A large language model for science},
  author={Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
  journal={arXiv preprint arXiv:2211.09085},
  year={2022}
}

@article{li2023reflection,
  title={Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning},
  author={Li, Ming and Chen, Lichang and Chen, Jiuhai and He, Shwai and Huang, Heng and Gu, Jiuxiang and Zhou, Tianyi},
  journal={arXiv preprint arXiv:2310.11716},
  year={2023}
}

@article{zhang2024rest,
  title={ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search},
  author={Zhang, Dan and Zhoubian, Sining and Yue, Yisong and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2406.03816},
  year={2024}
}

@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3843--3857},
  year={2022}
}

@article{shinn2023reflexion,
  title={Reflexion: an autonomous agent with dynamic memory and self-reflection},
  author={Shinn, Noah and Labash, Beck and Gopinath, Ashwin},
  journal={arXiv preprint arXiv:2303.11366},
  year={2023}
}

@article{weather_forecasting,
author = {Bi, Kaifeng and Xie, Lingxi and Zhang, Hengheng and Chen, Xin and Gu, Xiaotao and Tian, Qi},
year = {2023},
month = {07},
pages = {1-6},
title = {Accurate medium-range global weather forecasting with 3D neural networks},
volume = {619},
journal = {Nature},
doi = {10.1038/s41586-023-06185-3}
}


@article{chen2021evaluating_codex,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{deng2023learning,
  title={Learning A Foundation Language Model for Geoscience Knowledge Understanding and Utilization},
  author={Deng, Cheng and Zhang, Tianhang and He, Zhongmou and Chen, Qiyuan and Shi, Yuanyuan and Zhou, Le and Fu, Luoyi and Zhang, Weinan and Wang, Xinbing and Zhou, Chenghu and others},
  journal={arXiv preprint arXiv:2306.05064},
  year={2023}
}

@article{yuan2023scaling,
  title={Scaling relationship on learning mathematical reasoning with large language models},
  author={Yuan, Zheng and Yuan, Hongyi and Li, Chengpeng and Dong, Guanting and Tan, Chuanqi and Zhou, Chang},
  journal={arXiv preprint arXiv:2308.01825},
  year={2023}
}


@article{luo2023wizardmath,
  title={Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct},
  author={Luo, Haipeng and Sun, Qingfeng and Xu, Can and Zhao, Pu and Lou, Jianguang and Tao, Chongyang and Geng, Xiubo and Lin, Qingwei and Chen, Shifeng and Zhang, Dongmei},
  journal={arXiv preprint arXiv:2308.09583},
  year={2023}
}

@article{yue2023MAmmoTH,
  title={MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning},
  author={Yue, Xiang and Qu, Xingwei and Zhang, Ge and Fu, Yao and Huang, Wenhao and Sun, Huan and Su, Yu and Chen, Wenhu},
  journal={arXiv preprint arXiv:2309.05653},
  year={2023}
}

@article{yue2024mammoth2,
  title={MAmmoTH2: Scaling Instructions from the Web},
  author={Yue, Xiang and Zheng, Tuney and Zhang, Ge and Chen, Wenhu},
  journal={arXiv preprint arXiv:2405.03548},
  year={2024}
}


@misc{MetaAI2024,
  author = {{Meta AI}},
  title = {Meta LLaMA 3},
  year = {2024},
  howpublished = {\url{https://ai.meta.com/blog/meta-llama-3/}},
  note = {Accessed: 2024-05-22}
}

@article{azerbayev2023llemma,
  title={Llemma: An open language model for mathematics},
  author={Azerbayev, Zhangir and Schoelkopf, Hailey and Paster, Keiran and Santos, Marco Dos and McAleer, Stephen and Jiang, Albert Q and Deng, Jia and Biderman, Stella and Welleck, Sean},
  journal={arXiv preprint arXiv:2310.10631},
  year={2023}
}

@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}


@article{yu2023metamath,
  title={Metamath: Bootstrap your own mathematical questions for large language models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  journal={arXiv preprint arXiv:2309.12284},
  year={2023}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}


@article{zhong2023agieval,
  title={Agieval: A human-centric benchmark for evaluating foundation models},
  author={Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
  journal={arXiv preprint arXiv:2304.06364},
  year={2023}
}

@article{davies2021advancing,
  title={Advancing mathematics by guiding human intuition with AI},
  author={Davies, Alex and Veli{\v{c}}kovi{\'c}, Petar and Buesing, Lars and Blackwell, Sam and Zheng, Daniel and Toma{\v{s}}ev, Nenad and Tanburn, Richard and Battaglia, Peter and Blundell, Charles and Juh{\'a}sz, Andr{\'a}s and others},
  journal={Nature},
  volume={600},
  number={7887},
  pages={70--74},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{lilong2024autore,
  title={AutoRE: Document-Level Relation Extraction with Large Language Models},
  author={Lilong, Xue and Dan, Zhang and Yuxiao, Dong and Jie, Tang},
  journal={arXiv preprint arXiv:2403.14888},
  year={2024}
}

@article{huang2023ceval,
  title={C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models},
  author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Lei, Jiayi and others},
  journal={arXiv preprint arXiv:2305.08322},
  year={2023}
}

@inproceedings{hosseini2014learning,
  title={Learning to solve arithmetic word problems with verb categorization},
  author={Hosseini, Mohammad Javad and Hajishirzi, Hannaneh and Etzioni, Oren and Kushman, Nate},
  booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={523--533},
  year={2014}
}

@article{wang2023scibench,
  title={Scibench: Evaluating college-level scientific problem-solving abilities of large language models},
  author={Wang, Xiaoxuan and Hu, Ziniu and Lu, Pan and Zhu, Yanqiao and Zhang, Jieyu and Subramaniam, Satyen and Loomba, Arjun R and Zhang, Shichang and Sun, Yizhou and Wang, Wei},
  journal={arXiv preprint arXiv:2307.10635},
  year={2023}
}
@article{sun2023scieval,
  title={SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research},
  author={Sun, Liangtai and Han, Yang and Zhao, Zihan and Ma, Da and Shen, Zhennan and Chen, Baocai and Chen, Lu and Yu, Kai},
  journal={arXiv preprint arXiv:2308.13149},
  year={2023}
}

@article{rein2023gpqa,
  title={GPQA: A Graduate-Level Google-Proof Q\&A Benchmark},
  author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2311.12022},
  year={2023}
}

@inproceedings{yang2023leandojo,
    title={{LeanDojo}: Theorem Proving with Retrieval-Augmented Language Models},
    author={Yang, Kaiyu and Swope, Aidan and Gu, Alex and Chalamala, Rahul and Song, Peiyang and Yu, Shixing and Godil, Saad and Prenger, Ryan and Anandkumar, Anima},
    booktitle={Neural Information Processing Systems (NeurIPS)},
    year={2023}
}

@article{yao2022react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022}
}

@article{zeng2023agenttuning,
  title={Agenttuning: Enabling generalized agent abilities for llms},
  author={Zeng, Aohan and Liu, Mingdao and Lu, Rui and Wang, Bowen and Liu, Xiao and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2310.12823},
  year={2023}
}

@article{chen2022program,
  title={Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks},
  author={Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W},
  journal={arXiv preprint arXiv:2211.12588},
  year={2022}
}

@article{gou2023critic,
  title={Critic: Large language models can self-correct with tool-interactive critiquing},
  author={Gou, Zhibin and Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Yang, Yujiu and Duan, Nan and Chen, Weizhu},
  journal={arXiv preprint arXiv:2305.11738},
  year={2023}
}


@article{koncel2015parsing,
  title={Parsing algebraic word problems into equations},
  author={Koncel-Kedziorski, Rik and Hajishirzi, Hannaneh and Sabharwal, Ashish and Etzioni, Oren and Ang, Siena Dumas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={3},
  pages={585--597},
  year={2015},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}


@article{roy2016solving,
  title={Solving general arithmetic word problems},
  author={Roy, Subhro and Roth, Dan},
  journal={arXiv preprint arXiv:1608.01413},
  year={2016}
}

@article{patel2021nlp,
  title={Are NLP models really able to solve simple math word problems?},
  author={Patel, Arkil and Bhattamishra, Satwik and Goyal, Navin},
  journal={arXiv preprint arXiv:2103.07191},
  year={2021}
}


@article{ling2017program,
  title={Program induction by rationale generation: Learning to solve and explain algebraic word problems},
  author={Ling, Wang and Yogatama, Dani and Dyer, Chris and Blunsom, Phil},
  journal={arXiv preprint arXiv:1705.04146},
  year={2017}
}

@article{amini2019mathqa,
  title={Mathqa: Towards interpretable math word problem solving with operation-based formalisms},
  author={Amini, Aida and Gabriel, Saadia and Lin, Peter and Koncel-Kedziorski, Rik and Choi, Yejin and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:1905.13319},
  year={2019}
}

@article{mishra2022numglue,
  title={Numglue: A suite of fundamental yet challenging mathematical reasoning tasks},
  author={Mishra, Swaroop and Mitra, Arindam and Varshney, Neeraj and Sachdeva, Bhavdeep and Clark, Peter and Baral, Chitta and Kalyan, Ashwin},
  journal={arXiv preprint arXiv:2204.05660
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2022}
}

@article{mishra2022lila,
  title={Lila: A unified benchmark for mathematical reasoning},
  author={Mishra, Swaroop and Finlayson, Matthew and Lu, Pan and Tang, Leonard and Welleck, Sean and Baral, Chitta and Rajpurohit, Tanmay and Tafjord, Oyvind and Sabharwal, Ashish and Clark, Peter and others},
  journal={arXiv preprint arXiv:2210.17517
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2022}
}

@article{chen2023theoremqa,
  title={Theoremqa: A theorem-driven question answering dataset},
  author={Chen, Wenhu and Yin, Ming and Ku, Max and Lu, Pan and Wan, Yixin and Ma, Xueguang and Xu, Jianyu and Wang, Xinyi and Xia, Tony},
  journal={arXiv preprint arXiv:2305.12524
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2022}
}

@article{suzgun2022challenging,
  title={Challenging big-bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2022}
}

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2021}
}

@article{wang2022iteratively,
  title={Iteratively prompt pre-trained language models for chain of thought},
  author={Wang, Boshi and Deng, Xiang and Sun, Huan},
  journal={arXiv preprint arXiv:2203.08383
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2022}
}

@article{wang2022towards,
  title={Towards understanding chain-of-thought prompting: An empirical study of what matters},
  author={Wang, Boshi and Min, Sewon and Deng, Xiang and Shen, Jiaming and Wu, You and Zettlemoyer, Luke and Sun, Huan},
  journal={arXiv preprint arXiv:2212.10001
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2022}
}


@article{DBLP:journals/corr/abs-2203-07814,
  author       = {Yujia Li and
                  David H. Choi and
                  Junyoung Chung and
                  Nate Kushman and
                  Julian Schrittwieser and
                  R{\'{e}}mi Leblond and
                  Tom Eccles and
                  James Keeling and
                  Felix Gimeno and
                  Agustin Dal Lago and
                  Thomas Hubert and
                  Peter Choy and
                  Cyprien de Masson d'Autume and
                  Igor Babuschkin and
                  Xinyun Chen and
                  Po{-}Sen Huang and
                  Johannes Welbl and
                  Sven Gowal and
                  Alexey Cherepanov and
                  James Molloy and
                  Daniel J. Mankowitz and
                  Esme Sutherland Robson and
                  Pushmeet Kohli and
                  Nando de Freitas and
                  Koray Kavukcuoglu and
                  Oriol Vinyals},
  title        = {Competition-Level Code Generation with AlphaCode},
  journal      = {CoRR},
  volume       = {abs/2203.07814},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2203.07814},
  doi          = {10.48550/ARXIV.2203.07814},
  eprinttype    = {arXiv},
  eprint       = {2203.07814},
  timestamp    = {Sat, 02 Dec 2023 13:23:51 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2203-07814.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2305-20050,
  author       = {Hunter Lightman and
                  Vineet Kosaraju and
                  Yura Burda and
                  Harrison Edwards and
                  Bowen Baker and
                  Teddy Lee and
                  Jan Leike and
                  John Schulman and
                  Ilya Sutskever and
                  Karl Cobbe},
  title        = {Let's Verify Step by Step},
  journal      = {CoRR},
  volume       = {abs/2305.20050},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.20050},
  doi          = {10.48550/ARXIV.2305.20050},
  eprinttype    = {arXiv},
  eprint       = {2305.20050},
  timestamp    = {Wed, 07 Jun 2023 17:14:28 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-20050.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{DBLP:journals/corr/abs-2307-13702,
  author       = {Tamera Lanham and
                  Anna Chen and
                  Ansh Radhakrishnan and
                  Benoit Steiner and
                  Carson Denison and
                  Danny Hernandez and
                  Dustin Li and
                  Esin Durmus and
                  Evan Hubinger and
                  Jackson Kernion and
                  Kamile Lukosiute and
                  Karina Nguyen and
                  Newton Cheng and
                  Nicholas Joseph and
                  Nicholas Schiefer and
                  Oliver Rausch and
                  Robin Larson and
                  Sam McCandlish and
                  Sandipan Kundu and
                  Saurav Kadavath and
                  Shannon Yang and
                  Thomas Henighan and
                  Timothy Maxwell and
                  Timothy Telleen{-}Lawton and
                  Tristan Hume and
                  Zac Hatfield{-}Dodds and
                  Jared Kaplan and
                  Jan Brauner and
                  Samuel R. Bowman and
                  Ethan Perez},
  title        = {Measuring Faithfulness in Chain-of-Thought Reasoning},
  journal      = {CoRR},
  volume       = {abs/2307.13702},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.13702},
  doi          = {10.48550/ARXIV.2307.13702},
  eprinttype    = {arXiv},
  eprint       = {2307.13702},
  timestamp    = {Wed, 02 Aug 2023 15:37:53 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-13702.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cot,
  author       = {Jason Wei and
                  Xuezhi Wang and
                  Dale Schuurmans and
                  Maarten Bosma and
                  Brian Ichter and
                  Fei Xia and
                  Ed H. Chi and
                  Quoc V. Le and
                  Denny Zhou},
  editor       = {Sanmi Koyejo and
                  S. Mohamed and
                  A. Agarwal and
                  Danielle Belgrave and
                  K. Cho and
                  A. Oh},
  title        = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  booktitle    = {Advances in Neural Information Processing Systems 35: Annual Conference
                  on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
                  LA, USA, November 28 - December 9, 2022},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
  timestamp    = {Mon, 08 Jan 2024 16:31:37 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/Wei0SBIXCLZ22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{li2023making,
  title={Making language models better reasoners with step-aware verifier},
  author={Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={5315--5333},
  year={2023}
}

@article{wang2023making,
  title={Making large language models better reasoners with alignment},
  author={Wang, Peiyi and Li, Lei and Chen, Liang and Song, Feifan and Lin, Binghuai and Cao, Yunbo and Liu, Tianyu and Sui, Zhifang},
  journal={arXiv preprint arXiv:2309.02144
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{drozdov2022compositional,
  title={Compositional semantic parsing with large language models},
  author={Drozdov, Andrew and Sch{\"a}rli, Nathanael and Aky{\"u}rek, Ekin and Scales, Nathan and Song, Xinying and Chen, Xinyun and Bousquet, Olivier and Zhou, Denny},
  journal={arXiv preprint arXiv:2209.15003
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2022}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and others},
  journal={arXiv preprint arXiv:2205.10625
        
        
        
        
        
        
        
        
        
        },
  year={2022}
}

@article{xie2021explanation,
  title={An explanation of in-context learning as implicit bayesian inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2111.02080
        
        
        
        
        
        
        
        
        
        },
  year={2021}
}

@article{wang2023plan,
  title={Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models},
  author={Wang, Lei and Xu, Wanyu and Lan, Yihuai and Hu, Zhiqiang and Lan, Yunshi and Lee, Roy Ka-Wei and Lim, Ee-Peng},
  journal={arXiv preprint arXiv:2305.04091
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2021}
}

@article{sanh2021multitask,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
  journal={arXiv preprint arXiv:2110.08207
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2021}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416
        
        
        
        
        
        
        
        
        
        },
  year={2022}
}

@article{longpre2023flan,
  title={The flan collection: Designing data and methods for effective instruction tuning},
  author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
  journal={arXiv preprint arXiv:2301.13688
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{lee2023platypus,
  title={Platypus: Quick, cheap, and powerful refinement of llms},
  author={Lee, Ariel N and Hunter, Cole J and Ruiz, Nataniel},
  journal={arXiv preprint arXiv:2308.07317
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{wang2022self-instruct,
  title={Self-instruct: Aligning language model with self generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2022}
}

@article{xu2023wizardlm,
  title={Wizardlm: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{peng2023instruction,
  title={Instruction tuning with gpt-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{zhou2023lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={arXiv preprint arXiv:2305.11206
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{wang2023far,
  title={How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources},
  author={Wang, Yizhong and Ivison, Hamish and Dasigi, Pradeep and Hessel, Jack and Khot, Tushar and Chandu, Khyathi Raghavi and Wadden, David and MacMillan, Kelsey and Smith, Noah A and Beltagy, Iz and others},
  journal={arXiv preprint arXiv:2306.04751
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{zeng2022glm,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414
        
        },
  year={2022}
}
@inproceedings{du2022glm,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971
        
        
        
        },
  year={2023}
}

@article{Chen2021EvaluatingLL,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde and Jared Kaplan and Harrison Edwards and Yura Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and David W. Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William H. Guss and Alex Nichol and Igor Babuschkin and Suchir Balaji and Shantanu Jain and Andrew Carr and Jan Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew M. Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.03374},
  url={https://api.semanticscholar.org/CorpusID:235755472}
}

@article{yao2023tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{austin2021program,
  title={Program Synthesis with Large Language Models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article {Chen2023.07.05.547496,
	author = {Bo Chen and Xingyi Cheng and Yangli-ao Geng and Shen Li and Xin Zeng and Boyan Wang and Jing Gong and Chiming Liu and Aohan Zeng and Yuxiao Dong and Jie Tang and Le Song},
	title = {xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein},
	elocation-id = {2023.07.05.547496},
	year = {2023},
	doi = {10.1101/2023.07.05.547496},
	publisher = {Cold Spring Harbor Laboratory},
	URL = {https://www.biorxiv.org/content/early/2023/07/06/2023.07.05.547496},
	eprint = {https://www.biorxiv.org/content/early/2023/07/06/2023.07.05.547496.full.pdf},
	journal = {bioRxiv}
}

@article{glm2024chatglm,
  title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools},
  author={GLM, Team and Zeng, Aohan and Xu, Bin and Wang, Bowen and Zhang, Chenhui and Yin, Da and Rojas, Diego and Feng, Guanyu and Zhao, Hanlin and Lai, Hanyu and others},
  journal={arXiv preprint arXiv:2406.12793},
  year={2024}
}

@article{openai2023gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@phdthesis{barras1997coq,
  title={The {Coq} proof assistant reference manual: Version 6.1},
  author={Barras, Bruno and Boutin, Samuel and Cornes, Cristina and Courant, Judica{\"e}l and Filliatre, Jean-Christophe and Gimenez, Eduardo and Herbelin, Hugo and Huet, Gerard and Munoz, Cesar and Murthy, Chetan and others},
  year={1997},
  school={Inria}
}

@book{nipkow2002isabelle,
  title={Isabelle/HOL: a proof assistant for higher-order logic},
  author={Nipkow, Tobias and Wenzel, Markus and Paulson, Lawrence C},
  year={2002},
  publisher={Springer}
}

@inproceedings{de2015lean,
  title={The Lean theorem prover (system description)},
  author={De Moura, Leonardo and Kong, Soonho and Avigad, Jeremy and Van Doorn, Floris and von Raumer, Jakob},
  booktitle={Automated Deduction-CADE-25: 25th International Conference on Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings 25},
  pages={378--388},
  year={2015},
  organization={Springer}
}
