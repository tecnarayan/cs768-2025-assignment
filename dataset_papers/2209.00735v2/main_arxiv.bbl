\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbe and Sandon(2020)]{abbe2020universality}
Emmanuel Abbe and Colin Sandon.
\newblock On the universality of deep learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 20061--20072, 2020.

\bibitem[Abbe et~al.(2021)Abbe, Kamath, Malach, Sandon, and
  Srebro]{abbe2021power}
Emmanuel Abbe, Pritish Kamath, Eran Malach, Colin Sandon, and Nathan Srebro.
\newblock On the power of differentiable learning versus {PAC} and {SQ}
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Alom et~al.(2021)Alom, Hasan, Yakopcic, Taha, and
  Asari]{alom2021inception}
Md~Zahangir Alom, Mahmudul Hasan, Chris Yakopcic, Tarek~M Taha, and Vijayan~K
  Asari.
\newblock Inception recurrent convolutional neural network for object
  recognition.
\newblock \emph{Machine Vision and Applications}, 32\penalty0 (1):\penalty0
  1--14, 2021.

\bibitem[Arora and Zhang(2021)]{arora2021rip}
Sanjeev Arora and Yi~Zhang.
\newblock Rip van winkle's razor: A simple estimate of overfit to test data.
\newblock \emph{arXiv preprint arXiv:2102.13189}, 2021.

\bibitem[Balcan(2020)]{balcan2020data}
Maria-Florina Balcan.
\newblock Data-driven algorithm design.
\newblock \emph{arXiv preprint arXiv:2011.07177}, 2020.

\bibitem[Banino et~al.(2021)Banino, Balaguer, and
  Blundell]{banino2021pondernet}
Andrea Banino, Jan Balaguer, and Charles Blundell.
\newblock Pondernet: Learning to ponder.
\newblock \emph{arXiv preprint arXiv:2107.05407}, 2021.

\bibitem[Barron(1993)]{barron1993universal}
Andrew~R Barron.
\newblock Universal approximation bounds for superpositions of a sigmoidal
  function.
\newblock \emph{IEEE Transactions on Information theory}, 39\penalty0
  (3):\penalty0 930--945, 1993.

\bibitem[Barron(1994)]{barron1994approximation}
Andrew~R Barron.
\newblock Approximation and estimation bounds for artificial neural networks.
\newblock \emph{Machine learning}, 14\penalty0 (1):\penalty0 115--133, 1994.

\bibitem[Bhattamishra et~al.(2020{\natexlab{a}})Bhattamishra, Ahuja, and
  Goyal]{bhattamishra2020ability}
Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal.
\newblock On the ability and limitations of transformers to recognize formal
  languages.
\newblock \emph{arXiv preprint arXiv:2009.11264}, 2020{\natexlab{a}}.

\bibitem[Bhattamishra et~al.(2020{\natexlab{b}})Bhattamishra, Patel, and
  Goyal]{bhattamishra2020computational}
Satwik Bhattamishra, Arkil Patel, and Navin Goyal.
\newblock On the computational power of transformers and its implications in
  sequence modeling.
\newblock \emph{arXiv preprint arXiv:2006.09286}, 2020{\natexlab{b}}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Cybenko(1989)]{cybenko1989approximation}
George Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \emph{Mathematics of control, signals and systems}, 2\penalty0
  (4):\penalty0 303--314, 1989.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai2019transformer}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc~V Le, and Ruslan
  Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock \emph{arXiv preprint arXiv:1901.02860}, 2019.

\bibitem[Dehghani et~al.(2018)Dehghani, Gouws, Vinyals, Uszkoreit, and
  Kaiser]{dehghani2018universal}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and {\L}ukasz
  Kaiser.
\newblock Universal transformers.
\newblock \emph{arXiv preprint arXiv:1807.03819}, 2018.

\bibitem[Devroye et~al.(1994)Devroye, Gyorfi, Krzyzak, and
  Lugosi]{devroye1994strong}
Luc Devroye, Laszlo Gyorfi, Adam Krzyzak, and G{\'a}bor Lugosi.
\newblock On the strong universal consistency of nearest neighbor regression
  function estimates.
\newblock \emph{The Annals of Statistics}, pages 1371--1385, 1994.

\bibitem[Edelman et~al.(2021)Edelman, Goel, Kakade, and
  Zhang]{edelman2021inductive}
Benjamin~L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang.
\newblock Inductive biases and variable creation in self-attention mechanisms.
\newblock \emph{arXiv preprint arXiv:2110.10090}, 2021.

\bibitem[Fedus et~al.(2021)Fedus, Zoph, and Shazeer]{fedus2021switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock \emph{arXiv preprint arXiv:2101.03961}, 2021.

\bibitem[Funahashi(1989)]{funahashi1989approximate}
Ken-Ichi Funahashi.
\newblock On the approximate realization of continuous mappings by neural
  networks.
\newblock \emph{Neural networks}, 2\penalty0 (3):\penalty0 183--192, 1989.

\bibitem[Garg et~al.(2022)Garg, Tsipras, Liang, and Valiant]{garg2022can}
Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant.
\newblock What can transformers learn in-context? a case study of simple
  function classes.
\newblock \emph{arXiv preprint arXiv:2208.01066}, 2022.

\bibitem[Girshick et~al.(2014)Girshick, Donahue, Darrell, and
  Malik]{girshick2014rich}
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.
\newblock Rich feature hierarchies for accurate object detection and semantic
  segmentation.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, June 2014.

\bibitem[Grave et~al.(2016)Grave, Joulin, and Usunier]{grave2016improving}
Edouard Grave, Armand Joulin, and Nicolas Usunier.
\newblock Improving neural language models with a continuous cache.
\newblock \emph{arXiv preprint arXiv:1612.04426}, 2016.

\bibitem[Graves et~al.(2014)Graves, Wayne, and Danihelka]{graves2014neural}
Alex Graves, Greg Wayne, and Ivo Danihelka.
\newblock Neural turing machines.
\newblock \emph{arXiv preprint arXiv:1410.5401}, 2014.

\bibitem[Hopcroft et~al.(2001)Hopcroft, Motwani, and
  Ullman]{hopcroft2001introduction}
John~E Hopcroft, Rajeev Motwani, and Jeffrey~D Ullman.
\newblock Introduction to automata theory, languages, and computation.
\newblock \emph{Acm Sigact News}, 32\penalty0 (1):\penalty0 60--65, 2001.

\bibitem[Hornik et~al.(1989)Hornik, Stinchcombe, and
  White]{hornik1989multilayer}
Kurt Hornik, Maxwell Stinchcombe, and Halbert White.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural networks}, 2\penalty0 (5):\penalty0 359--366, 1989.

\bibitem[Hospedales et~al.(2021)Hospedales, Antoniou, Micaelli, and
  Storkey]{hospedales2021meta}
Timothy~M Hospedales, Antreas Antoniou, Paul Micaelli, and Amos~J Storkey.
\newblock Meta-learning in neural networks: A survey.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 2021.

\bibitem[Kaiser and Sutskever(2015)]{kaiser2015neural}
{\L}ukasz Kaiser and Ilya Sutskever.
\newblock Neural gpus learn algorithms.
\newblock \emph{arXiv preprint arXiv:1511.08228}, 2015.

\bibitem[Kasai et~al.(2021)Kasai, Peng, Zhang, Yogatama, Ilharco, Pappas, Mao,
  Chen, and Smith]{kasai2021finetuning}
Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos
  Pappas, Yi~Mao, Weizhu Chen, and Noah~A Smith.
\newblock Finetuning pretrained transformers into rnns.
\newblock \emph{arXiv preprint arXiv:2103.13076}, 2021.

\bibitem[Kearns and Vazirani(1994)]{kearns94clt}
M.~J. Kearns and U.~V. Vazirani.
\newblock \emph{An Introduction to Computational Learning Theory}.
\newblock MIT Press, Cambridge, MA, USA, 1994.

\bibitem[Kearns and Valiant(1994)]{kv94}
Michael Kearns and Leslie Valiant.
\newblock Cryptographic limitations on learning boolean formulae and finite
  automata.
\newblock \emph{J. ACM}, 41\penalty0 (1):\penalty0 67â€“95, jan 1994.
\newblock ISSN 0004-5411.
\newblock \doi{10.1145/174644.174647}.
\newblock URL \url{https://doi.org/10.1145/174644.174647}.

\bibitem[Kearns and Valiant(1993)]{kearns1993cryptographic}
Michael~J Kearns and Leslie~G Valiant.
\newblock Cryptographic limitations on learning boolean formulae and finite
  automata.
\newblock In \emph{Machine Learning: From Theory to Applications}, pages
  29--49. Springer, 1993.

\bibitem[Leblond et~al.(2021)Leblond, Alayrac, Sifre, Pislar, Lespiau,
  Antonoglou, Simonyan, and Vinyals]{leblond2021machine}
R{\'e}mi Leblond, Jean-Baptiste Alayrac, Laurent Sifre, Miruna Pislar,
  Jean-Baptiste Lespiau, Ioannis Antonoglou, Karen Simonyan, and Oriol Vinyals.
\newblock Machine translation decoding beyond beam search.
\newblock \emph{arXiv preprint arXiv:2104.05336}, 2021.

\bibitem[Lee et~al.(2017)Lee, Ge, Ma, Risteski, and Arora]{lee2017ability}
Holden Lee, Rong Ge, Tengyu Ma, Andrej Risteski, and Sanjeev Arora.
\newblock On the ability of neural nets to express distributions.
\newblock In \emph{Conference on Learning Theory}, pages 1271--1296. PMLR,
  2017.

\bibitem[Levin(1973)]{Levin73}
Leonid~A. Levin.
\newblock Universal sequential search problems.
\newblock \emph{Problems of Information Transmission}, 9\penalty0 (3), 1973.

\bibitem[Liang and Hu(2015)]{liang2015recurrent}
Ming Liang and Xiaolin Hu.
\newblock Recurrent convolutional neural network for object recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 3367--3375, 2015.

\bibitem[Lin et~al.(2021)Lin, Yang, Bai, Zhou, Jiang, Jia, Wang, Zhang, Li,
  Lin, et~al.]{lin2021m6}
Junyang Lin, An~Yang, Jinze Bai, Chang Zhou, Le~Jiang, Xianyan Jia, Ang Wang,
  Jie Zhang, Yong Li, Wei Lin, et~al.
\newblock M6-10t: A sharing-delinking paradigm for efficient multi-trillion
  parameter pretraining.
\newblock \emph{arXiv preprint arXiv:2110.03888}, 2021.

\bibitem[Nielsen(2015)]{nielsen2015neural}
Michael~A Nielsen.
\newblock \emph{Neural networks and deep learning}, volume~25.
\newblock Determination press San Francisco, CA, 2015.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{pascanu2013difficulty}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{International conference on machine learning}, pages
  1310--1318. PMLR, 2013.

\bibitem[Pinheiro and Collobert(2014)]{pinheiro2014recurrent}
Pedro Pinheiro and Ronan Collobert.
\newblock Recurrent convolutional neural networks for scene labeling.
\newblock In \emph{International conference on machine learning}, pages 82--90.
  PMLR, 2014.

\bibitem[Reddy et~al.(1977)]{reddy1977speech}
D~Raj Reddy et~al.
\newblock Speech understanding systems: A summary of results of the five-year
  research effort.
\newblock \emph{Department of Computer Science. Camegie-Mell University,
  Pittsburgh, PA}, 17:\penalty0 138, 1977.

\bibitem[Schwarzschild et~al.(2021)Schwarzschild, Borgnia, Gupta, Huang,
  Vishkin, Goldblum, and Goldstein]{schwarzschild2021can}
Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah
  Goldblum, and Tom Goldstein.
\newblock Can you learn an algorithm? generalizing from easy to hard problems
  with recurrent networks.
\newblock \emph{arXiv preprint arXiv:2106.04537}, 2021.

\bibitem[Siegelmann and Sontag(1995)]{siegelmann1995computational}
Hava~T Siegelmann and Eduardo~D Sontag.
\newblock On the computational power of neural nets.
\newblock \emph{Journal of computer and system sciences}, 50\penalty0
  (1):\penalty0 132--150, 1995.

\bibitem[Spoerer et~al.(2017)Spoerer, McClure, and
  Kriegeskorte]{spoerer2017recurrent}
Courtney~J Spoerer, Patrick McClure, and Nikolaus Kriegeskorte.
\newblock Recurrent convolutional neural networks: a better model of biological
  object recognition.
\newblock \emph{Frontiers in psychology}, 8:\penalty0 1551, 2017.

\bibitem[Sukhbaatar et~al.(2015)Sukhbaatar, Szlam, Weston, and
  Fergus]{sukhbaatar2015end}
Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.
\newblock End-to-end memory networks.
\newblock \emph{arXiv preprint arXiv:1503.08895}, 2015.

\bibitem[Tamar et~al.(2016)Tamar, Wu, Thomas, Levine, and
  Abbeel]{tamar2016value}
Aviv Tamar, Yi~Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel.
\newblock Value iteration networks.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, pages 2154--2162, 2016.

\bibitem[Wang et~al.(2004)Wang, Chen, and Chen]{wang2004rbf}
Junping Wang, Quanshi Chen, and Yong Chen.
\newblock Rbf kernel based support vector machine with universal approximation
  and its application.
\newblock In \emph{International symposium on neural networks}, pages 512--517.
  Springer, 2004.

\bibitem[Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, and
  Zhou]{wang2022self}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language
  models.
\newblock \emph{arXiv preprint arXiv:2203.11171}, 2022.

\bibitem[Wei et~al.(2021)Wei, Chen, and Ma]{wei2021statistically}
Colin Wei, Yining Chen, and Tengyu Ma.
\newblock Statistically meaningful approximation: a case study on approximating
  turing machines with transformers.
\newblock \emph{arXiv preprint arXiv:2107.13163}, 2021.

\bibitem[Yun et~al.(2019)Yun, Bhojanapalli, Rawat, Reddi, and
  Kumar]{yun2019transformers}
Chulhee Yun, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank~J Reddi, and
  Sanjiv Kumar.
\newblock Are transformers universal approximators of sequence-to-sequence
  functions?
\newblock \emph{arXiv preprint arXiv:1912.10077}, 2019.

\end{thebibliography}
