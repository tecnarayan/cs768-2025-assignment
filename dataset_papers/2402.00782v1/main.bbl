\begin{thebibliography}{81}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbeel \& Ng(2004)Abbeel and Ng]{abbeel2004apprenticeship}
Abbeel, P. and Ng, A.~Y.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In \emph{Proceedings of the twenty-first international conference on Machine learning}, pp.\ ~1, 2004.

\bibitem[Anthropic(2023)]{anthropic2023claude}
Anthropic.
\newblock Model card and evaluations for claude models, 2023.
\newblock URL \url{https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf}.

\bibitem[Arjona-Medina et~al.(2019)Arjona-Medina, Gillhofer, Widrich, Unterthiner, Brandstetter, and Hochreiter]{arjona2019rudder}
Arjona-Medina, J.~A., Gillhofer, M., Widrich, M., Unterthiner, T., Brandstetter, J., and Hochreiter, S.
\newblock Rudder: Return decomposition for delayed rewards.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{auer2002finite}
Auer, P., Cesa-Bianchi, N., and Fischer, P.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock \emph{Machine learning}, 47\penalty0 (2):\penalty0 235--256, 2002.

\bibitem[Azar et~al.(2023)Azar, Rowland, Piot, Guo, Calandriello, Valko, and Munos]{azar2023general}
Azar, M.~G., Rowland, M., Piot, B., Guo, D., Calandriello, D., Valko, M., and Munos, R.
\newblock A general theoretical paradigm to understand learning from human preferences.
\newblock \emph{arXiv preprint arXiv:2310.12036}, 2023.

\bibitem[Bai et~al.(2022)Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional}
Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et~al.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock \emph{arXiv preprint arXiv:2212.08073}, 2022.

\bibitem[Bain \& Sammut(1995)Bain and Sammut]{bain1995framework}
Bain, M. and Sammut, C.
\newblock A framework for behavioural cloning.
\newblock In \emph{Machine Intelligence 15}, pp.\  103--129, 1995.

\bibitem[Beeching et~al.(2023)Beeching, Fourrier, Habib, Han, Lambert, Rajani, Sanseviero, Tunstall, and Wolf]{open-llm-leaderboard}
Beeching, E., Fourrier, C., Habib, N., Han, S., Lambert, N., Rajani, N., Sanseviero, O., Tunstall, L., and Wolf, T.
\newblock Open llm leaderboard.
\newblock \url{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}, 2023.

\bibitem[Bouteiller et~al.(2020)Bouteiller, Ramstedt, Beltrame, Pal, and Binas]{bouteiller2020reinforcement}
Bouteiller, Y., Ramstedt, S., Beltrame, G., Pal, C., and Binas, J.
\newblock Reinforcement learning with random delays.
\newblock In \emph{International conference on learning representations}, 2020.

\bibitem[Bradley \& Terry(1952)Bradley and Terry]{bradley1952rank}
Bradley, R.~A. and Terry, M.~E.
\newblock Rank analysis of incomplete block designs: I. the method of paired comparisons.
\newblock \emph{Biometrika}, 39\penalty0 (3/4):\penalty0 324--345, 1952.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Chan et~al.(2021)Chan, Curth, and van~der Schaar]{chan2021inverse}
Chan, A., Curth, A., and van~der Schaar, M.
\newblock Inverse online learning: Understanding non-stationary and reactionary policies.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Chan et~al.(2023)Chan, H{\"u}y{\"u}k, and van~der Schaar]{chan2023optimising}
Chan, A., H{\"u}y{\"u}k, A., and van~der Schaar, M.
\newblock Optimising human-ai collaboration by learning convincing explanations.
\newblock In \emph{XAI in Action: Past, Present, and Future Applications}, 2023.

\bibitem[Chan \& van~der Schaar(2020)Chan and van~der Schaar]{chan2020scalable}
Chan, A.~J. and van~der Schaar, M.
\newblock Scalable bayesian inverse reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang, Zhuang, Gonzalez, Stoica, and Xing]{vicuna2023}
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J.~E., Stoica, I., and Xing, E.~P.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023.
\newblock URL \url{https://lmsys.org/blog/2023-03-30-vicuna/}.

\bibitem[Choshen et~al.(2019)Choshen, Fox, Aizenbud, and Abend]{choshen2019weaknesses}
Choshen, L., Fox, L., Aizenbud, Z., and Abend, O.
\newblock On the weaknesses of reinforcement learning for neural machine translation.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and Amodei]{christiano2017deep}
Christiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training}
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Coste et~al.(2023)Coste, Anwar, Kirk, and Krueger]{coste2023reward}
Coste, T., Anwar, U., Kirk, R., and Krueger, D.
\newblock Reward model ensembles help mitigate overoptimization.
\newblock \emph{arXiv preprint arXiv:2310.02743}, 2023.

\bibitem[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{dettmers2023qlora}
Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock \emph{arXiv preprint arXiv:2305.14314}, 2023.

\bibitem[Dong et~al.(2023)Dong, Xiong, Goyal, Pan, Diao, Zhang, Shum, and Zhang]{dong2023raft}
Dong, H., Xiong, W., Goyal, D., Pan, R., Diao, S., Zhang, J., Shum, K., and Zhang, T.
\newblock Raft: Reward ranked finetuning for generative foundation model alignment.
\newblock \emph{arXiv preprint arXiv:2304.06767}, 2023.

\bibitem[Engstrom et~al.(2019)Engstrom, Ilyas, Santurkar, Tsipras, Janoos, Rudolph, and Madry]{engstrom2019implementation}
Engstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Janoos, F., Rudolph, L., and Madry, A.
\newblock Implementation matters in deep rl: A case study on ppo and trpo.
\newblock In \emph{International conference on learning representations}, 2019.

\bibitem[Ferret et~al.(2021)Ferret, Marinier, Geist, and Pietquin]{ferret2021self}
Ferret, J., Marinier, R., Geist, M., and Pietquin, O.
\newblock Self-attentional credit assignment for transfer in reinforcement learning.
\newblock In \emph{Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence}, pp.\  2655--2661, 2021.

\bibitem[Gangwani et~al.(2020)Gangwani, Zhou, and Peng]{gangwani2020learning}
Gangwani, T., Zhou, Y., and Peng, J.
\newblock Learning guidance rewards with trajectory-space smoothing.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 822--832, 2020.

\bibitem[Gao et~al.(2023)Gao, Schulman, and Hilton]{gao2023scaling}
Gao, L., Schulman, J., and Hilton, J.
\newblock Scaling laws for reward model overoptimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\  10835--10866. PMLR, 2023.

\bibitem[Gemini-Team(2023)]{geminiteam2023gemini}
Gemini-Team, G.~D.
\newblock Gemini: A family of highly capable multimodal models, 2023.

\bibitem[Geng \& Liu(2023)Geng and Liu]{openlm2023openllama}
Geng, X. and Liu, H.
\newblock Openllama: An open reproduction of llama, 2023.
\newblock URL \url{https://github.com/openlm-research/open_llama}.

\bibitem[Geng et~al.(2023)Geng, Gudibande, Liu, Wallace, Abbeel, Levine, and Song]{koala_blogpost_2023}
Geng, X., Gudibande, A., Liu, H., Wallace, E., Abbeel, P., Levine, S., and Song, D.
\newblock Koala: A dialogue model for academic research.
\newblock Blog post, April 2023.
\newblock URL \url{https://bair.berkeley.edu/blog/2023/04/03/koala/}.

\bibitem[Han et~al.(2022)Han, Ren, Wu, Zhou, and Peng]{han2022off}
Han, B., Ren, Z., Wu, Z., Zhou, Y., and Peng, J.
\newblock Off-policy reinforcement learning with delayed rewards.
\newblock In \emph{International Conference on Machine Learning}, pp.\  8280--8303. PMLR, 2022.

\bibitem[Harutyunyan et~al.(2019)Harutyunyan, Dabney, Mesnard, Gheshlaghi~Azar, Piot, Heess, van Hasselt, Wayne, Singh, Precup, et~al.]{harutyunyan2019hindsight}
Harutyunyan, A., Dabney, W., Mesnard, T., Gheshlaghi~Azar, M., Piot, B., Heess, N., van Hasselt, H.~P., Wayne, G., Singh, S., Precup, D., et~al.
\newblock Hindsight credit assignment.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[H{\'e}liou et~al.(2020)H{\'e}liou, Mertikopoulos, and Zhou]{heliou2020gradient}
H{\'e}liou, A., Mertikopoulos, P., and Zhou, Z.
\newblock Gradient-free online learning in continuous games with delayed rewards.
\newblock In \emph{International conference on machine learning}, pp.\  4172--4181. PMLR, 2020.

\bibitem[Henderson et~al.(2018)Henderson, Islam, Bachman, Pineau, Precup, and Meger]{henderson2018deep}
Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D.
\newblock Deep reinforcement learning that matters.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~32, 2018.

\bibitem[Holt et~al.(2023)Holt, H{\"u}y{\"u}k, Qian, Sun, and van~der Schaar]{holt2023neural}
Holt, S., H{\"u}y{\"u}k, A., Qian, Z., Sun, H., and van~der Schaar, M.
\newblock Neural laplace control for continuous-time delayed systems.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  1747--1778. PMLR, 2023.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Jaques et~al.(2019)Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza, Jones, Gu, and Picard]{jaques2019way}
Jaques, N., Ghandeharioun, A., Shen, J.~H., Ferguson, C., Lapedriza, A., Jones, N., Gu, S., and Picard, R.
\newblock Way off-policy batch deep reinforcement learning of implicit human preferences in dialog.
\newblock \emph{arXiv preprint arXiv:1907.00456}, 2019.

\bibitem[K{\"o}pf et~al.(2023)K{\"o}pf, Kilcher, von R{\"u}tte, Anagnostidis, Tam, Stevens, Barhoum, Duc, Stanley, Nagyfi, et~al.]{kopf2023openassistant}
K{\"o}pf, A., Kilcher, Y., von R{\"u}tte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N.~M., Stanley, O., Nagyfi, R., et~al.
\newblock Openassistant conversations--democratizing large language model alignment.
\newblock \emph{arXiv preprint arXiv:2304.07327}, 2023.

\bibitem[Lee et~al.(2023)Lee, Phatale, Mansoor, Lu, Mesnard, Bishop, Carbune, and Rastogi]{lee2023rlaif}
Lee, H., Phatale, S., Mansoor, H., Lu, K., Mesnard, T., Bishop, C., Carbune, V., and Rastogi, A.
\newblock Rlaif: Scaling reinforcement learning from human feedback with ai feedback.
\newblock \emph{arXiv preprint arXiv:2309.00267}, 2023.

\bibitem[Li et~al.(2023)Li, Zhang, Dubois, Taori, Gulrajani, Guestrin, Liang, and Hashimoto]{alpaca_eval}
Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P., and Hashimoto, T.~B.
\newblock Alpacaeval: An automatic evaluator of instruction-following models.
\newblock \url{https://github.com/tatsu-lab/alpaca_eval}, 2023.

\bibitem[Lightman et~al.(2023)Lightman, Kosaraju, Burda, Edwards, Baker, Lee, Leike, Schulman, Sutskever, and Cobbe]{lightman2023let}
Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K.
\newblock Let's verify step by step.
\newblock \emph{arXiv preprint arXiv:2305.20050}, 2023.

\bibitem[Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le, Zoph, Wei, et~al.]{longpre2023flan}
Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H.~W., Tay, Y., Zhou, D., Le, Q.~V., Zoph, B., Wei, J., et~al.
\newblock The flan collection: Designing data and methods for effective instruction tuning.
\newblock \emph{arXiv preprint arXiv:2301.13688}, 2023.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and Potts]{maas2011imdb}
Maas, A.~L., Daly, R.~E., Pham, P.~T., Huang, D., Ng, A.~Y., and Potts, C.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies}, pp.\  142--150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics.
\newblock URL \url{http://www.aclweb.org/anthology/P11-1015}.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou, Wierstra, and Riedmiller]{mnih2013playing}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{ng1999policy}
Ng, A.~Y., Harada, D., and Russell, S.
\newblock Policy invariance under reward transformations: Theory and application to reward shaping.
\newblock In \emph{International Conference on Machine Learning}, volume~99, pp.\  278--287. Citeseer, 1999.

\bibitem[Ng et~al.(2000)Ng, Russell, et~al.]{ng2000algorithms}
Ng, A.~Y., Russell, S.~J., et~al.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{Icml}, volume~1, pp.\ ~2, 2000.

\bibitem[Nguyen et~al.(2017)Nguyen, Daum{\'e}~III, and Boyd-Graber]{nguyen2017reinforcement}
Nguyen, K., Daum{\'e}~III, H., and Boyd-Graber, J.
\newblock Reinforcement learning for bandit neural machine translation with simulated human feedback.
\newblock In \emph{Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, pp.\  1464--1474, 2017.

\bibitem[Ni et~al.(2021)Ni, Eysenbach, and Salakhutdinov]{ni2021recurrent}
Ni, T., Eysenbach, B., and Salakhutdinov, R.
\newblock Recurrent model-free rl can be a strong baseline for many pomdps.
\newblock \emph{arXiv preprint arXiv:2110.05038}, 2021.

\bibitem[Nilsson et~al.(1998)Nilsson, Bernhardsson, and Wittenmark]{nilsson1998stochastic}
Nilsson, J., Bernhardsson, B., and Wittenmark, B.
\newblock Stochastic analysis and control of real-time systems with random time delays.
\newblock \emph{Automatica}, 34\penalty0 (1):\penalty0 57--64, 1998.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Pace et~al.(2021)Pace, Chan, and van~der Schaar]{pace2021poetree}
Pace, A., Chan, A., and van~der Schaar, M.
\newblock Poetree: Interpretable policy learning with adaptive decision trees.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Peng et~al.(2023)Peng, Song, Tian, Jin, Mi, and Yu]{peng2023stabilizing}
Peng, B., Song, L., Tian, Y., Jin, L., Mi, H., and Yu, D.
\newblock Stabilizing rlhf through advantage model and selective rehearsal.
\newblock \emph{arXiv preprint arXiv:2309.10202}, 2023.

\bibitem[Pignatelli et~al.(2023)Pignatelli, Ferret, Geist, Mesnard, van Hasselt, and Toni]{pignatelli2023survey}
Pignatelli, E., Ferret, J., Geist, M., Mesnard, T., van Hasselt, H., and Toni, L.
\newblock A survey of temporal credit assignment in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2312.01072}, 2023.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners, 2019.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Ermon, Manning, and Finn]{rafailov2023direct}
Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C.~D., and Finn, C.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock \emph{arXiv preprint arXiv:2305.18290}, 2023.

\bibitem[Rajaraman et~al.(2020)Rajaraman, Yang, Jiao, and Ramchandran]{rajaraman2020toward}
Rajaraman, N., Yang, L., Jiao, J., and Ramchandran, K.
\newblock Toward the fundamental limits of imitation learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 2914--2924, 2020.

\bibitem[Ramamurthy et~al.(2022)Ramamurthy, Ammanabrolu, Brantley, Hessel, Sifa, Bauckhage, Hajishirzi, and Choi]{ramamurthy2022reinforcement}
Ramamurthy, R., Ammanabrolu, P., Brantley, K., Hessel, J., Sifa, R., Bauckhage, C., Hajishirzi, H., and Choi, Y.
\newblock Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Razin et~al.(2023)Razin, Zhou, Saremi, Thilak, Bradley, Nakkiran, Susskind, and Littwin]{razin2023vanishing}
Razin, N., Zhou, H., Saremi, O., Thilak, V., Bradley, A., Nakkiran, P., Susskind, J., and Littwin, E.
\newblock Vanishing gradients in reinforcement finetuning of language models.
\newblock \emph{arXiv preprint arXiv:2310.20703}, 2023.

\bibitem[Ren et~al.(2021)Ren, Guo, Zhou, and Peng]{ren2021learning}
Ren, Z., Guo, R., Zhou, Y., and Peng, J.
\newblock Learning long-term reward redistribution via randomized return decomposition.
\newblock \emph{arXiv preprint arXiv:2111.13485}, 2021.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Sennrich et~al.(2015)Sennrich, Haddow, and Birch]{sennrich2015neural}
Sennrich, R., Haddow, B., and Birch, A.
\newblock Neural machine translation of rare words with subword units.
\newblock \emph{arXiv preprint arXiv:1508.07909}, 2015.

\bibitem[Shrikumar et~al.(2017)Shrikumar, Greenside, and Kundaje]{shrikumar2017learning}
Shrikumar, A., Greenside, P., and Kundaje, A.
\newblock Learning important features through propagating activation differences.
\newblock In \emph{International conference on machine learning}, pp.\  3145--3153. PMLR, 2017.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, and Christiano]{stiennon2020learning}
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P.~F.
\newblock Learning to summarize with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 3008--3021, 2020.

\bibitem[Sutton(1984)]{sutton1984temporal}
Sutton, R.~S.
\newblock \emph{Temporal credit assignment in reinforcement learning}.
\newblock University of Massachusetts Amherst, 1984.

\bibitem[Tang et~al.(2021)Tang, Ho, and Liu]{tang2021bandit}
Tang, W., Ho, C.-J., and Liu, Y.
\newblock Bandit learning with delayed impact of actions.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 26804--26817, 2021.

\bibitem[TogetherComputer(2023)]{together2023redpajama}
TogetherComputer.
\newblock Redpajama-data: An open source recipe to reproduce llama training dataset, 2023.
\newblock URL \url{https://github.com/togethercomputer/RedPajama-Data}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi{\`e}re, B., Goyal, N., Hambro, E., Azhar, F., et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Uesato et~al.(2022)Uesato, Kushman, Kumar, Song, Siegel, Wang, Creswell, Irving, and Higgins]{uesato2022solving}
Uesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and Higgins, I.
\newblock Solving math word problems with process-and outcome-based feedback.
\newblock \emph{arXiv preprint arXiv:2211.14275}, 2022.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[V{\"o}lske et~al.(2017)V{\"o}lske, Potthast, Syed, and Stein]{volske2017tl}
V{\"o}lske, M., Potthast, M., Syed, S., and Stein, B.
\newblock Tl; dr: Mining reddit to learn automatic summarization.
\newblock In \emph{Proceedings of the Workshop on New Frontiers in Summarization}, pp.\  59--63, 2017.

\bibitem[von Werra et~al.(2020)von Werra, Belkada, Tunstall, Beeching, Thrush, Lambert, and Huang]{vonwerra2022trl}
von Werra, L., Belkada, Y., Tunstall, L., Beeching, E., Thrush, T., Lambert, N., and Huang, S.
\newblock Trl: Transformer reinforcement learning.
\newblock \url{https://github.com/huggingface/trl}, 2020.

\bibitem[Walsh et~al.(2009)Walsh, Nouri, Li, and Littman]{walsh2009learning}
Walsh, T.~J., Nouri, A., Li, L., and Littman, M.~L.
\newblock Learning and planning in environments with delayed feedback.
\newblock \emph{Autonomous Agents and Multi-Agent Systems}, 18:\penalty0 83--105, 2009.

\bibitem[Wang \& Komatsuzaki(2021)Wang and Komatsuzaki]{gpt-j}
Wang, B. and Komatsuzaki, A.
\newblock {GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}.
\newblock \url{https://github.com/kingoflolz/mesh-transformer-jax}, May 2021.

\bibitem[Wang et~al.(2022)Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi]{selfinstruct}
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.~A., Khashabi, D., and Hajishirzi, H.
\newblock Self-instruct: Aligning language model with self generated instructions, 2022.

\bibitem[Wu et~al.(2021)Wu, Ouyang, Ziegler, Stiennon, Lowe, Leike, and Christiano]{wu2021recursively}
Wu, J., Ouyang, L., Ziegler, D.~M., Stiennon, N., Lowe, R., Leike, J., and Christiano, P.
\newblock Recursively summarizing books with human feedback.
\newblock \emph{arXiv preprint arXiv:2109.10862}, 2021.

\bibitem[Wu et~al.(2016)Wu, Schuster, Chen, Le, Norouzi, Macherey, Krikun, Cao, Gao, Macherey, et~al.]{wu2016google}
Wu, Y., Schuster, M., Chen, Z., Le, Q.~V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., et~al.
\newblock Google's neural machine translation system: Bridging the gap between human and machine translation.
\newblock \emph{arXiv preprint arXiv:1609.08144}, 2016.

\bibitem[Wu et~al.(2023)Wu, Hu, Shi, Dziri, Suhr, Ammanabrolu, Smith, Ostendorf, and Hajishirzi]{wu2023fine}
Wu, Z., Hu, Y., Shi, W., Dziri, N., Suhr, A., Ammanabrolu, P., Smith, N.~A., Ostendorf, M., and Hajishirzi, H.
\newblock Fine-grained human feedback gives better rewards for language model training.
\newblock \emph{arXiv preprint arXiv:2306.01693}, 2023.

\bibitem[Yuan et~al.(2023)Yuan, Yuan, Tan, Wang, Huang, and Huang]{yuan2023rrhf}
Yuan, Z., Yuan, H., Tan, C., Wang, W., Huang, S., and Huang, F.
\newblock Rrhf: Rank responses to align language models with human feedback without tears.
\newblock \emph{arXiv preprint arXiv:2304.05302}, 2023.

\bibitem[Zhao et~al.(2023)Zhao, Joshi, Liu, Khalman, Saleh, and Liu]{zhao2023slic}
Zhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and Liu, P.~J.
\newblock Slic-hf: Sequence likelihood calibration with human feedback.
\newblock \emph{arXiv preprint arXiv:2305.10425}, 2023.

\bibitem[Zheng et~al.(2023)Zheng, Dou, Gao, Hua, Shen, Wang, Liu, Jin, Liu, Zhou, et~al.]{zheng2023secrets}
Zheng, R., Dou, S., Gao, S., Hua, Y., Shen, W., Wang, B., Liu, Y., Jin, S., Liu, Q., Zhou, Y., et~al.
\newblock Secrets of rlhf in large language models part i: Ppo.
\newblock \emph{arXiv preprint arXiv:2307.04964}, 2023.

\bibitem[Zhou et~al.(2018)Zhou, Mertikopoulos, Bambos, Glynn, Ye, Li, and Fei-Fei]{zhou2018distributed}
Zhou, Z., Mertikopoulos, P., Bambos, N., Glynn, P., Ye, Y., Li, L.-J., and Fei-Fei, L.
\newblock Distributed asynchronous optimization with unbounded delays: How slow can you go?
\newblock In \emph{International Conference on Machine Learning}, pp.\  5970--5979. PMLR, 2018.

\bibitem[Zhou et~al.(2019)Zhou, Xu, and Blanchet]{zhou2019learning}
Zhou, Z., Xu, R., and Blanchet, J.
\newblock Learning in generalized linear contextual bandits with stochastic delays.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and Dey]{ziebart2008maximum}
Ziebart, B.~D., Maas, A.~L., Bagnell, J.~A., and Dey, A.~K.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{Aaai}, volume~8, pp.\  1433--1438. Chicago, IL, USA, 2008.

\end{thebibliography}
