\begin{thebibliography}{10}

\bibitem{achille2019dynamics}
Alessandro Achille, Glen Mbeng, and Stefano Soatto.
\newblock Dynamics and reachability of learning tasks, 2019, arXiv preprint:1810.02440.

\bibitem{achille2018information}
Alessandro Achille and Stefano Soatto.
\newblock Information dropout: Learning optimal representations through noisy computation.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence}, 40(12):2897--2905, 2018.

\bibitem{arditi2024refusal}
Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Rimsky, Wes Gurnee, and Neel Nanda.
\newblock Refusal in language models is mediated by a single direction.
\newblock {\em arXiv preprint arXiv:2406.11717}, 2024.

\bibitem{bai2023qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu.
\newblock Qwen technical report, 2023, arXiv preprint:2309.16609.

\bibitem{balakrishnan-etal-2019-constrained}
Anusha Balakrishnan, Jinfeng Rao, Kartikeya Upasani, Michael White, and Rajen Subba.
\newblock Constrained decoding for neural {NLG} from compositional representations in task-oriented dialogue.
\newblock In Anna Korhonen, David Traum, and Llu{\'\i}s M{\`a}rquez, editors, {\em Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 831--844, Florence, Italy, July 2019. Association for Computational Linguistics.

\bibitem{10.1162/coli_a_00422}
Yonatan Belinkov.
\newblock {Probing Classifiers: Promises, Shortcomings, and Advances}.
\newblock {\em Computational Linguistics}, 48(1):207--219, 04 2022, arXiv preprint:https://direct.mit.edu/coli/article-pdf/48/1/207/2006605/coli\_a\_00422.pdf.

\bibitem{belrose2023eliciting}
Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt.
\newblock Eliciting latent predictions from transformers with the tuned lens, 2023, arXiv preprint:2303.08112.

\bibitem{bhardwaj2023language}
Rishabh Bhardwaj and Soujanya Poria.
\newblock Language model unalignment: Parametric red-teaming to expose hidden harms and biases.
\newblock {\em arXiv preprint arXiv:2310.14303}, 2023.

\bibitem{bhatt2023purple}
Manish Bhatt, Sahana Chennabasappa, Cyrus Nikolaidis, Shengye Wan, Ivan Evtimov, Dominik Gabi, Daniel Song, Faizan Ahmad, Cornelius Aschermann, Lorenzo Fontana, Sasha Frolov, Ravi~Prakash Giri, Dhaval Kapil, Yiannis Kozyrakis, David LeBlanc, James Milazzo, Aleksandar Straumann, Gabriel Synnaeve, Varun Vontimitta, Spencer Whitman, and Joshua Saxe.
\newblock Purple llama cyberseceval: A secure coding benchmark for language models, 2023, arXiv preprint:2312.04724.

\bibitem{carlini2024stealing}
Nicholas Carlini, Daniel Paleka, Krishnamurthy~Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A.~Feder Cooper, Katherine Lee, Matthew Jagielski, Milad Nasr, Arthur Conmy, Eric Wallace, David Rolnick, and Florian Tramèr.
\newblock Stealing part of a production language model, 2024, arXiv preprint:2403.06634.

\bibitem{casper2024defendingunforeseenfailuremodes}
Stephen Casper, Lennart Schulze, Oam Patel, and Dylan Hadfield-Menell.
\newblock Defending against unforeseen failure modes with latent adversarial training, 2024, arXiv preprint:2403.05030.

\bibitem{chan2023hazards}
Alan Chan, Ben Bucknall, Herbie Bradley, and David Krueger.
\newblock Hazards from increasingly accessible fine-tuning of downloadable foundation models, 2023, arXiv preprint:2312.14751.

\bibitem{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock {\em arXiv preprint arXiv:1803.05457}, 2018.

\bibitem{deng2024sophon}
Jiangyi Deng, Shengyuan Pang, Yanjiao Chen, Liangming Xia, Yijie Bai, Haiqin Weng, and Wenyuan Xu.
\newblock {SOPHON}: Non-fine-tunable learning to restrain task transferability for pre-trained models, 2024, arXiv preprint:2404.12699.

\bibitem{e2e_cleaned}
Ondřej Dušek, David~M Howcroft, and Verena Rieser.
\newblock Semantic {Noise} {Matters} for {Neural} {Natural} {Language} {Generation}.
\newblock In {\em Proceedings of the 12th {International} {Conference} on {Natural} {Language} {Generation} ({INLG} 2019)}, pages 421--426, Tokyo, Japan, 2019.

\bibitem{esiobu2023robbie}
David Esiobu, Xiaoqing Tan, Saghar Hosseini, Megan Ung, Yuchen Zhang, Jude Fernandes, Jane Dwivedi-Yu, Eleonora Presani, Adina Williams, and Eric~Michael Smith.
\newblock Robbie: Robust bias evaluation of large generative language models, 2023, arXiv preprint:2311.18140.

\bibitem{ganguli2022red}
Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark.
\newblock Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned, 2022, arXiv preprint:2209.07858.

\bibitem{eval-harness}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le~Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
\newblock A framework for few-shot language model evaluation, 12 2023.

\bibitem{gehman2020realtoxicityprompts}
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah~A. Smith.
\newblock {RealToxicityPrompts}: Evaluating neural toxic degeneration in language models, 2020, arXiv preprint:2009.11462.

\bibitem{gehrmann-etal-2021-gem}
Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan~Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi~Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ond{\v{r}}ej Du{\v{s}}ek, Chris~Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa~Prasad Majumder, Pedro~Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo~Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan~Ramesh Rao, Vikas Raunak, Juan~Diego Rodriguez, Sashank Santhanam, Jo{\~a}o Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco~Antonio Sobrevilla~Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou.
\newblock The {GEM} benchmark: Natural language generation, its evaluation and metrics.
\newblock In Antoine Bosselut, Esin Durmus, Varun~Prashant Gangal, Sebastian Gehrmann, Yacine Jernite, Laura Perez-Beltrachini, Samira Shaikh, and Wei Xu, editors, {\em Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)}, pages 96--120, Online, August 2021. Association for Computational Linguistics.

\bibitem{gopal2023releasing}
Anjali Gopal, Nathan Helm-Burger, Lennart Justen, Emily~H. Soice, Tiffany Tzeng, Geetha Jeyapragasan, Simon Grimm, Benjamin Mueller, and Kevin~M. Esvelt.
\newblock Will releasing the weights of future large language models grant widespread access to pandemic agents?, 2023, arXiv preprint:2310.18233.

\bibitem{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on {ImageNet} classification, 2015, arXiv preprint:1502.01852.

\bibitem{he2023debertav3}
Pengcheng He, Jianfeng Gao, and Weizhu Chen.
\newblock {DeBERTaV3}: Improving {DeBERTa} using {ELECTRA-Style} pre-training with gradient-disentangled embedding sharing, 2023, arXiv preprint:2111.09543.

\bibitem{henderson_self-destructing_2023}
Peter Henderson, Eric Mitchell, Christopher Manning, Dan Jurafsky, and Chelsea Finn.
\newblock Self-destructing models: Increasing the costs of harmful dual uses of foundation models.
\newblock In {\em Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society}, pages 287--296, 2023.

\bibitem{hendrycks2020aligning}
Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt.
\newblock Aligning {AI} with shared human values.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock {\em arXiv preprint arXiv:2009.03300}, 2020.

\bibitem{holtzman2020curious}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration, 2020, arXiv preprint:1904.09751.

\bibitem{hsu2024safe}
Chia-Yi Hsu, Yu-Lin Tsai, Chih-Hsun Lin, Pin-Yu Chen, Chia-Mu Yu, and Chun-Ying Huang.
\newblock Safe lora: the silver lining of reducing safety risks when fine-tuning large language models.
\newblock {\em arXiv preprint arXiv:2405.16833}, 2024.

\bibitem{hu2021lora}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models, 2021, arXiv preprint:2106.09685.

\bibitem{huang2021unlearnable}
Hanxun Huang, Xingjun Ma, Sarah~Monazam Erfani, James Bailey, and Yisen Wang.
\newblock Unlearnable examples: Making personal data unexploitable, 2021, arXiv preprint:2101.04898.

\bibitem{huang2024lazysafetyalignmentlarge}
Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim~Furkan Tekin, and Ling Liu.
\newblock Lazy safety alignment for large language models against harmful fine-tuning, 2024, arXiv preprint:2405.18641.

\bibitem{Huang2024VaccinePA}
Tiansheng Huang, Sihao Hu, and Ling Liu.
\newblock Vaccine: Perturbation-aware alignment for large language model.
\newblock {\em arXiv preprint arXiv:2402.01109}, 2024.

\bibitem{hubinger2024sleeper}
Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel~M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec, Yuntao Bai, Zachary Witten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul Christiano, Samuel~R. Bowman, Logan Graham, Jared Kaplan, Sören Mindermann, Ryan Greenblatt, Buck Shlegeris, Nicholas Schiefer, and Ethan Perez.
\newblock Sleeper agents: Training deceptive llms that persist through safety training, 2024, arXiv preprint:2401.05566.

\bibitem{jain2023baselinedefensesadversarialattacks}
Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein.
\newblock Baseline defenses for adversarial attacks against aligned language models, 2023, arXiv preprint:2309.00614.

\bibitem{jain2023mechanistically}
Samyak Jain, Robert Kirk, Ekdeep~Singh Lubana, Robert~P. Dick, Hidenori Tanaka, Edward Grefenstette, Tim Rocktäschel, and David~Scott Krueger.
\newblock Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks, 2023, arXiv preprint:2311.12786.

\bibitem{ji2023beavertails}
Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce~Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang.
\newblock Beavertails: Towards improved safety alignment of llm via a human-preference dataset.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, {\em Advances in Neural Information Processing Systems}, volume~36, pages 24678--24704. Curran Associates, Inc., 2023.

\bibitem{juraska-etal-2019-viggo}
Juraj Juraska, Kevin Bowden, and Marilyn Walker.
\newblock {V}i{GGO}: A video game corpus for data-to-text generation in open-domain conversation.
\newblock In {\em Proceedings of the 12th International Conference on Natural Language Generation}, pages 164--172, Tokyo, Japan, October{--}November 2019. Association for Computational Linguistics.

\bibitem{Lee2024AMU}
Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan~K. Kummerfeld, and Rada Mihalcea.
\newblock A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity.
\newblock {\em ArXiv}, abs/2401.01967, 2024.

\bibitem{lees2022new}
Alyssa Lees, Vinh~Q. Tran, Yi~Tay, Jeffrey Sorensen, Jai Gupta, Donald Metzler, and Lucy Vasserman.
\newblock A new generation of perspective api: Efficient multilingual character-level transformers, 2022, arXiv preprint:2202.11176.

\bibitem{lermen2023lora}
Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish.
\newblock Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b.
\newblock {\em arXiv preprint arXiv:2310.20624}, 2023.

\bibitem{lin-etal-2022-truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock {T}ruthful{QA}: Measuring how models mimic human falsehoods.
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, {\em Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 3214--3252, Dublin, Ireland, May 2022. Association for Computational Linguistics.

\bibitem{liu2024tuninglanguagemodelsproxy}
Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah~A. Smith.
\newblock Tuning language models by proxy, 2024, arXiv preprint:2401.08565.

\bibitem{liu2023jailbreaking}
Yi~Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu.
\newblock Jailbreaking chatgpt via prompt engineering: An empirical study.
\newblock {\em arXiv preprint arXiv:2305.13860}, 2023.

\bibitem{lyu2024keeping}
Kaifeng Lyu, Haoyu Zhao, Xinran Gu, Dingli Yu, Anirudh Goyal, and Sanjeev Arora.
\newblock Keeping llms aligned after fine-tuning: The crucial role of prompt templates.
\newblock {\em arXiv preprint arXiv:2402.18540}, 2024.

\bibitem{mazeika2024harmbench}
Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo~Li, David Forsyth, and Dan Hendrycks.
\newblock Harmbench: A standardized evaluation framework for automated red teaming and robust refusal, 2024, arXiv preprint:2402.04249.

\bibitem{mukhoti2024finetuning}
Jishnu Mukhoti, Yarin Gal, Philip Torr, and Puneet~K. Dokania.
\newblock Fine-tuning can cripple foundation models; preserving features may be the solution, 2024.

\bibitem{nan-etal-2021-dart}
Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi~Chern Tan, Xi~Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen~Fatema Rajani.
\newblock {DART}: Open-domain structured data record to text generation.
\newblock In {\em Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 432--447, Online, June 2021. Association for Computational Linguistics.

\bibitem{nangia2020crows}
Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel Bowman.
\newblock {CrowS-Pairs}: A challenge dataset for measuring social biases in masked language models.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 1953--1967, 2020.

\bibitem{pelrine2023exploiting}
Kellin Pelrine, Mohammad Taufeeque, Michal Zajc, Euan McLean, and Adam Gleave.
\newblock Exploiting novel gpt-4 apis.
\newblock {\em arXiv preprint arXiv:2312.14302}, 2023.

\bibitem{perez2022red}
Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving.
\newblock Red teaming language models with language models.
\newblock {\em arXiv preprint arXiv:2202.03286}, 2022.

\bibitem{qi_fine-tuning_2023}
Xiangyu Qi, Yi~Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson.
\newblock Fine-tuning aligned language models compromises safety, even when users do not intend to!
\newblock {\em arXiv preprint arXiv:2310.03693}, 2023.

\bibitem{robey2024smoothllmdefendinglargelanguage}
Alexander Robey, Eric Wong, Hamed Hassani, and George~J. Pappas.
\newblock Smoothllm: Defending large language models against jailbreaking attacks, 2024, arXiv preprint:2310.03684.

\bibitem{rosati2024immunization}
Domenic Rosati, Jan Wehner, Kai Williams, Łukasz Bartoszcze, Jan Batzner, Hassan Sajjad, and Frank Rudzicz.
\newblock Immunization against harmful fine-tuning attacks, 2024, arXiv preprint:2402.16382.

\bibitem{röttger2024xstest}
Paul Röttger, Hannah~Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy.
\newblock {XSTest}: A test suite for identifying exaggerated safety behaviours in large language models, 2024, arXiv preprint:2308.01263.

\bibitem{shen2023large}
Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, and Deyi Xiong.
\newblock Large language model alignment: A survey, 2023, arXiv preprint:2309.15025.

\bibitem{shen2023anything}
Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang.
\newblock " do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models.
\newblock {\em arXiv preprint arXiv:2308.03825}, 2023.

\bibitem{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem{Tarun_2024}
Ayush~K. Tarun, Vikram~S. Chundawat, Murari Mandal, and Mohan Kankanhalli.
\newblock Fast yet effective machine unlearning.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems}, page 1–10, 2024.

\bibitem{tishby2015deep}
Naftali Tishby and Noga Zaslavsky.
\newblock Deep learning and the information bottleneck principle.
\newblock In {\em 2015 ieee information theory workshop (itw)}, pages 1--5. IEEE, 2015.

\bibitem{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{van-der-lee-etal-2020-cacapo}
Chris van~der Lee, Chris Emmery, Sander Wubben, and Emiel Krahmer.
\newblock The {CACAPO} dataset: A multilingual, multi-domain dataset for neural pipeline and end-to-end data-to-text generation.
\newblock In Brian Davis, Yvette Graham, John Kelleher, and Yaji Sripada, editors, {\em Proceedings of the 13th International Conference on Natural Language Generation}, pages 68--79, Dublin, Ireland, December 2020. Association for Computational Linguistics.

\bibitem{wang2024decodingtrust}
Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang~T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu~Cheng, Sanmi Koyejo, Dawn Song, and Bo~Li.
\newblock Decodingtrust: A comprehensive assessment of trustworthiness in gpt models, 2024, arXiv preprint:2306.11698.

\bibitem{wang_non-transferable_2021}
Lixu Wang, Shichao Xu, Ruiqi Xu, Xiao Wang, and Qi~Zhu.
\newblock Non-{Transferable} {Learning}: {A} {New} {Approach} for {Model} {Ownership} {Verification} and {Applicability} {Authorization}.
\newblock October 2021.

\bibitem{wei2024assessing}
Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, and Peter Henderson.
\newblock Assessing the brittleness of safety alignment via pruning and low-rank modifications.
\newblock {\em arXiv preprint arXiv:2402.05162}, 2024.

\bibitem{Wu_2019}
Tailin Wu, Ian Fischer, Isaac~L. Chuang, and Max Tegmark.
\newblock Learnability for the information bottleneck.
\newblock {\em Entropy}, 21(10):924, September 2019.

\bibitem{yang_shadow_2023}
Xianjun Yang, Xiao Wang, Qi~Zhang, Linda Petzold, William~Yang Wang, Xun Zhao, and Dahua Lin.
\newblock Shadow alignment: The ease of subverting safely-aligned language models.
\newblock {\em arXiv preprint arXiv:2310.02949}, 2023.

\bibitem{yi2024opensource}
Jingwei Yi, Rui Ye, Qisi Chen, Bin~Benjamin Zhu, Siheng Chen, Defu Lian, Guangzhong Sun, Xing Xie, and Fangzhao Wu.
\newblock Open-source can be dangerous: On the vulnerability of value alignment in open-source {LLM}s, 2024.

\bibitem{yi2024safety}
Xin Yi, Shunfan Zheng, Linlin Wang, Xiaoling Wang, and Liang He.
\newblock A safety realignment framework via subspace-oriented model fusion for large language models.
\newblock {\em arXiv preprint arXiv:2405.09055}, 2024.

\bibitem{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock {HellaSwag}: Can a machine really finish your sentence?
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 4791--4800, 2019.

\bibitem{zhou_making_2023}
Xin Zhou, Yi~Lu, Ruotian Ma, Tao Gui, Qi~Zhang, and Xuanjing Huang.
\newblock Making harmful behaviors unlearnable for large language models.
\newblock {\em arXiv preprint arXiv:2311.02105}, 2023.

\bibitem{zong2024safety}
Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, and Timothy Hospedales.
\newblock Safety fine-tuning at (almost) no cost: A baseline for vision large language models.
\newblock {\em arXiv preprint arXiv:2402.02207}, 2024.

\bibitem{zou2023universal}
Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J.~Zico Kolter, and Matt Fredrikson.
\newblock Universal and transferable adversarial attacks on aligned language models, 2023, arXiv preprint:2307.15043.

\end{thebibliography}
