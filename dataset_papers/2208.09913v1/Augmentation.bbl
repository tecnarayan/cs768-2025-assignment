\begin{thebibliography}{10}

\bibitem{arora2021dropout}
R.~Arora, P.~Bartlett, P.~Mianjy, and N.~Srebro.
\newblock Dropout: Explicit forms and capacity control.
\newblock {\em ICML}, 2021.

\bibitem{bartlett2002rademacher}
P.~L. Bartlett and S.~Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock {\em Journal of Machine Learning Research}, 3(Nov):463--482, 2002.

\bibitem{beckham2019adversarial}
C.~Beckham, S.~Honari, V.~Verma, A.~M. Lamb, F.~Ghadiri, R.~D. Hjelm,
  Y.~Bengio, and C.~Pal.
\newblock On adversarial mixup resynthesis.
\newblock {\em NeurIPS}, 2019.

\bibitem{berthelot2019mixmatch}
D.~Berthelot, N.~Carlini, I.~Goodfellow, N.~Papernot, A.~Oliver, and C.~A.
  Raffel.
\newblock Mixmatch: A holistic approach to semi-supervised learning.
\newblock {\em NeurIPS}, 2019.

\bibitem{bochkovskiy2020yolov4}
A.~Bochkovskiy, C.-Y. Wang, and H.-Y.~M. Liao.
\newblock Yolov4: Optimal speed and accuracy of object detection.
\newblock {\em arXiv preprint arXiv:2004.10934}, 2020.

\bibitem{gpt3}
T.~B. Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em NeurIPS}, 2020.

\bibitem{buitinck2013api}
L.~Buitinck, G.~Louppe, M.~Blondel, F.~Pedregosa, A.~Mueller, O.~Grisel,
  V.~Niculae, P.~Prettenhofer, A.~Gramfort, J.~Grobler, et~al.
\newblock Api design for machine learning software: experiences from the
  scikit-learn project.
\newblock {\em arXiv preprint arXiv:1309.0238}, 2013.

\bibitem{carratino2020mixup}
L.~Carratino, M.~Ciss{\'e}, R.~Jenatton, and J.-P. Vert.
\newblock On mixup regularization.
\newblock {\em arXiv preprint arXiv:2006.06049}, 2020.

\bibitem{cha2021swad}
J.~Cha, S.~Chun, K.~Lee, H.-C. Cho, S.~Park, Y.~Lee, and S.~Park.
\newblock Swad: Domain generalization by seeking flat minima.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem{chang2020mixup}
Y.-T. Chang, Q.~Wang, W.-C. Hung, R.~Piramuthu, Y.-H. Tsai, and M.-H. Yang.
\newblock Mixup-cam: Weakly-supervised semantic segmentation via uncertainty
  regularization.
\newblock {\em arXiv preprint arXiv:2008.01201}, 2020.

\bibitem{chen2020gridmask}
P.~Chen, S.~Liu, H.~Zhao, and J.~Jia.
\newblock Gridmask data augmentation.
\newblock {\em arXiv preprint arXiv:2001.04086}, 2020.

\bibitem{chidambaram2021towards}
M.~Chidambaram, X.~Wang, Y.~Hu, C.~Wu, and R.~Ge.
\newblock Towards understanding the data dependency of mixup-style training.
\newblock {\em ICLR}, 2022.

\bibitem{chun2019icmlw}
S.~Chun, S.~J. Oh, S.~Yun, D.~Han, J.~Choe, and Y.~Yoo.
\newblock An empirical evaluation on robustness and uncertainty of
  regularization methods.
\newblock {\em ICML Workshop on Uncertainty and Robustness in Deep Learning},
  2019.

\bibitem{chun2021styleaugment}
S.~Chun and S.~Park.
\newblock Styleaugment: Learning texture de-biased representations by style
  augmentation without pre-defined textures.
\newblock {\em arXiv preprint arXiv:2108.10549}, 2021.

\bibitem{randaug}
E.~D. Cubuk, B.~Zoph, J.~Shlens, and Q.~V. Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock {\em CVPR}, 2020.

\bibitem{dabouei2021supermix}
A.~Dabouei, S.~Soleymani, F.~Taherkhani, and N.~M. Nasrabadi.
\newblock Supermix: Supervising the mixing data augmentation.
\newblock {\em CVPR}, 2021.

\bibitem{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock {\em CVPR}, 2009.

\bibitem{faramarzi2020patchup}
M.~Faramarzi, M.~Amini, A.~Badrinaaraayanan, V.~Verma, and S.~Chandar.
\newblock Patchup: A regularization technique for convolutional neural
  networks.
\newblock {\em arXiv preprint arXiv:2006.07794}, 2020.

\bibitem{foret2020sharpness}
P.~Foret, A.~Kleiner, H.~Mobahi, and B.~Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{french2019semi}
G.~French, S.~Laine, T.~Aila, M.~Mackiewicz, and G.~Finlayson.
\newblock Semi-supervised semantic segmentation needs strong, varied
  perturbations.
\newblock {\em BMVC}, 2020.

\bibitem{garipov2018fge}
T.~Garipov, P.~Izmailov, D.~Podoprikhin, D.~P. Vetrov, and A.~G. Wilson.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock In {\em Neural Information Processing Systems}, 2018.

\bibitem{fgsm}
I.~J. Goodfellow, J.~Shlens, and C.~Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock {\em ICLR}, 2015.

\bibitem{greenewald2021k}
K.~Greenewald, A.~Gu, M.~Yurochkin, J.~Solomon, and E.~Chien.
\newblock k-mixup regularization for deep learning via optimal transport.
\newblock {\em arXiv preprint arXiv:2106.02933}, 2021.

\bibitem{guo2019augmenting}
H.~Guo, Y.~Mao, and R.~Zhang.
\newblock Augmenting data with mixup for sentence classification: An empirical
  study.
\newblock {\em arXiv preprint arXiv:1905.08941}, 2019.

\bibitem{harris2020fmix}
E.~Harris, A.~Marcu, M.~Painter, M.~Niranjan, A.~Pr{\"u}gel-Bennett, and
  J.~Hare.
\newblock Fmix: Enhancing mixed sample data augmentation.
\newblock {\em arXiv preprint arXiv:2002.12047}, 2020.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock {\em CVPR}, 2016.

\bibitem{he2016identity}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Identity mappings in deep residual networks.
\newblock {\em ECCV}, 2016.

\bibitem{hendrycks2018benchmarking}
D.~Hendrycks and T.~Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock {\em ICLR}, 2018.

\bibitem{hong2021stylemix}
M.~Hong, J.~Choi, and G.~Kim.
\newblock Stylemix: Separating content and style for enhanced data
  augmentation.
\newblock {\em CVPR}, 2021.

\bibitem{inoue2018data}
H.~Inoue.
\newblock Data augmentation by pairing samples for images classification.
\newblock {\em arXiv preprint arXiv:1801.02929}, 2018.

\bibitem{izmailov2018swa}
P.~Izmailov, D.~Podoprikhin, T.~Garipov, D.~Vetrov, and A.~G. Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock {\em Conference on Uncertainty in Artificial Intelligence}, 2018.

\bibitem{jeong2021observations}
J.~Jeong, S.~Cha, Y.~Yoo, S.~Yun, T.~Moon, and J.~Choi.
\newblock Observations on k-image expansion of image-mixing augmentation for
  classification.
\newblock {\em arXiv preprint arXiv:2110.04248}, 2021.

\bibitem{align_google}
C.~Jia, Y.~Yang, Y.~Xia, Y.-T. Chen, Z.~Parekh, H.~Pham, Q.~V. Le, Y.~Sung,
  Z.~Li, and T.~Duerig.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock {\em ICML}, 2021.

\bibitem{jindal2020leveraging}
A.~Jindal, D.~Gnaneshwar, R.~Sawhney, and R.~R. Shah.
\newblock Leveraging bert with mixup for sentence classification (student
  abstract).
\newblock {\em AAAI}, 2020.

\bibitem{kalantidis2020hard}
Y.~Kalantidis, M.~B. Sariyildiz, N.~Pion, P.~Weinzaepfel, and D.~Larlus.
\newblock Hard negative mixing for contrastive learning.
\newblock {\em NeurIPS}, 2020.

\bibitem{keskar2016largebatch}
N.~S. Keskar, D.~Mudigere, J.~Nocedal, M.~Smelyanskiy, and P.~T.~P. Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{kim2021specmix}
G.~Kim, D.~K. Han, and H.~Ko.
\newblock Specmix: A mixed sample data augmentation method for training
  withtime-frequency domain features.
\newblock {\em INTERSPEECH}, 2021.

\bibitem{kim2021co}
J.~H. Kim, W.~Choo, H.~Jeong, and H.~O. Song.
\newblock Co-mixup: Saliency guided joint mixup with supermodular diversity.
\newblock {\em ICLR}, 2021.

\bibitem{kim2020puzzle}
J.~H. Kim, W.~Choo, and H.~O. Song.
\newblock Puzzle mix: Exploiting saliency and local statistics for optimal
  mixup.
\newblock {\em ICML}, 2020.

\bibitem{kim2020mixco}
S.~Kim, G.~Lee, S.~Bae, and S.-Y. Yun.
\newblock Mixco: Mix-up contrastive learning for visual representation.
\newblock {\em arXiv preprint arXiv:2010.06300}, 2020.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images, 2009.

\bibitem{lee2021imix}
K.~Lee, Y.~Zhu, K.~Sohn, C.-L. Li, J.~Shin, and H.~Lee.
\newblock {\$}i{\$}-mix: A domain-agnostic strategy for contrastive
  representation learning.
\newblock {\em ICLR}, 2021.

\bibitem{Li2020DivideMix}
J.~Li, R.~Socher, and S.~C. Hoi.
\newblock Dividemix: Learning with noisy labels as semi-supervised learning.
\newblock {\em ICLR}, 2020.

\bibitem{liu2022decoupled}
Z.~Liu, S.~Li, G.~Wang, C.~Tan, L.~Wu, and S.~Z. Li.
\newblock Decoupled mixup for data-efficient learning.
\newblock {\em arXiv preprint arXiv:2203.10761}, 2022.

\bibitem{liu2021unveiling}
Z.~Liu, S.~Li, D.~Wu, Z.~Chen, L.~Wu, J.~Guo, and S.~Z. Li.
\newblock Automix: Unveiling the power of mixup.
\newblock {\em ECCV}, 2022.

\bibitem{ma2021linear}
C.~Ma and L.~Ying.
\newblock On linear stability of sgd and input-smoothness of neural networks.
\newblock {\em NeurIPS}, 2021.

\bibitem{instagramnet}
D.~Mahajan, R.~Girshick, V.~Ramanathan, K.~He, M.~Paluri, Y.~Li, A.~Bharambe,
  and L.~Van Der~Maaten.
\newblock Exploring the limits of weakly supervised pretraining.
\newblock {\em ECCV}, 2018.

\bibitem{medennikov2018investigation}
I.~Medennikov, Y.~Y. Khokhlov, A.~Romanenko, D.~Popov, N.~A. Tomashenko,
  I.~Sorokin, and A.~Zatvornitskiy.
\newblock An investigation of mixup training strategies for acoustic models in
  asr.
\newblock {\em Interspeech}, 2018.

\bibitem{meng2021mixspeech}
L.~Meng, J.~Xu, X.~Tan, J.~Wang, T.~Qin, and B.~Xu.
\newblock Mixspeech: Data augmentation for low-resource automatic speech
  recognition.
\newblock {\em ICASSP}, 2021.

\bibitem{misra2019mish}
D.~Misra.
\newblock Mish: A self regularized non-monotonic activation function.
\newblock {\em BMVC}, 2020.

\bibitem{moosavi2019robustness}
S.-M. Moosavi-Dezfooli, A.~Fawzi, J.~Uesato, and P.~Frossard.
\newblock Robustness via curvature regularization, and vice versa.
\newblock {\em CVPR}, 2019.

\bibitem{mustafa2020input}
W.~Mustafa, R.~A. Vandermeulen, and M.~Kloft.
\newblock Input hessian regularization of neural networks.
\newblock {\em arXiv preprint arXiv:2009.06571}, 2020.

\bibitem{specaug}
D.~S. Park, W.~Chan, Y.~Zhang, C.-C. Chiu, B.~Zoph, E.~D. Cubuk, and Q.~V. Le.
\newblock Specaugment: A simple data augmentation method for automatic speech
  recognition.
\newblock In {\em Interspeech}, 2019.

\bibitem{qin2020resizemix}
J.~Qin, J.~Fang, Q.~Zhang, W.~Liu, X.~Wang, and X.~Wang.
\newblock Resizemix: Mixing data with preserved object information and true
  labels.
\newblock {\em arXiv preprint arXiv:2012.11101}, 2020.

\bibitem{rebuffi2021data}
S.-A. Rebuffi, S.~Gowal, D.~A. Calian, F.~Stimberg, O.~Wiles, and T.~A. Mann.
\newblock Data augmentation can improve robustness.
\newblock {\em NeurIPS}, 2021.

\bibitem{ren2022sdmp}
S.~Ren, H.~Wang, Z.~Gao, S.~He, A.~Yuille, Y.~Zhou, and C.~Xie.
\newblock A simple data mixing prior for improving self-supervised learning.
\newblock {\em CVPR}, 2022.

\bibitem{sohn2022genlabel}
J.~Y. Sohn, L.~Shang, H.~Chen, J.~Moon, D.~Papailiopoulos, and K.~Lee.
\newblock Genlabel: Mixup relabeling using generative models.
\newblock {\em arXiv preprint arXiv:2201.02354}, 2022.

\bibitem{sohn2020fixmatch}
K.~Sohn, D.~Berthelot, N.~Carlini, Z.~Zhang, H.~Zhang, C.~A. Raffel, E.~D.
  Cubuk, A.~Kurakin, and C.-L. Li.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and
  confidence.
\newblock {\em NeurIPS}, 2020.

\bibitem{takahashi2019ricap}
R.~Takahashi, T.~Matsubara, and K.~Uehara.
\newblock Data augmentation using random image cropping and patching for deep
  cnns.
\newblock {\em IEEE Transactions on Circuits and Systems for Video Technology},
  30(9):2917--2931, 2019.

\bibitem{tian2019contrastive}
Y.~Tian, D.~Krishnan, and P.~Isola.
\newblock Contrastive multiview coding.
\newblock {\em arXiv preprint arXiv:1906.05849}, 2019.

\bibitem{tokozume2018between}
Y.~Tokozume, Y.~Ushiku, and T.~Harada.
\newblock Between-class learning for image classification.
\newblock {\em CVPR}, 2018.

\bibitem{tokozume2017learning}
Y.~Tokozume, Y.~Ushiku, and T.~Harada.
\newblock Learning from between-class examples for deep sound recognition.
\newblock {\em ICLR}, 2018.

\bibitem{touvron2021training}
H.~Touvron, M.~Cord, M.~Douze, F.~Massa, A.~Sablayrolles, and H.~J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock {\em ICML}, 2021.

\bibitem{uddin2020saliencymix}
A.~Uddin, M.~Monira, W.~Shin, T.~Chung, S.~H. Bae, et~al.
\newblock Saliencymix: A saliency guided data augmentation strategy for better
  regularization.
\newblock {\em ICLR}, 2021.

\bibitem{verma2019manifold}
V.~Verma, A.~Lamb, C.~Beckham, A.~Najafi, I.~Mitliagkas, D.~Lopez-Paz, and
  Y.~Bengio.
\newblock Manifold mixup: Better representations by interpolating hidden
  states.
\newblock {\em ICML}, 2019.

\bibitem{wager2013dropout}
S.~Wager, S.~Wang, and P.~S. Liang.
\newblock Dropout training as adaptive regularization.
\newblock {\em NeurIPS}, 2013.

\bibitem{walawalkar2020attentive}
D.~Walawalkar, Z.~Shen, Z.~Liu, and M.~Savvides.
\newblock Attentive cutmix: An enhanced data augmentation approach for deep
  learning based image classification.
\newblock {\em arXiv preprint arXiv:2003.13048}, 2020.

\bibitem{wightman2021resnet}
R.~Wightman, H.~Touvron, and H.~J{\'e}gou.
\newblock Resnet strikes back: An improved training procedure in timm.
\newblock {\em NeurIPS}, 2021.

\bibitem{yao2021improving}
H.~Yao, L.-K. Huang, L.~Zhang, Y.~Wei, L.~Tian, J.~Zou, J.~Huang, et~al.
\newblock Improving generalization in meta-learning via task augmentation.
\newblock {\em ICML}, 2021.

\bibitem{yun2019cutmix}
S.~Yun, D.~Han, S.~J. Oh, S.~Chun, J.~Choe, and Y.~Yoo.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock {\em ICCV}, 2019.

\bibitem{yun2021re}
S.~Yun, S.~J. Oh, B.~Heo, D.~Han, J.~Choe, and S.~Chun.
\newblock Re-labeling imagenet: from single to multi-labels, from global to
  localized labels.
\newblock {\em CVPR}, 2021.

\bibitem{yun2020videomix}
S.~Yun, S.~J. Oh, B.~Heo, D.~Han, and J.~Kim.
\newblock Videomix: Rethinking data augmentation for video classification.
\newblock {\em arXiv preprint arXiv:2012.03457}, 2020.

\bibitem{zagoruyko2016wide}
S.~Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock {\em BMVC}, 2016.

\bibitem{zhang2017mixup}
H.~Zhang, M.~Cisse, Y.~N. Dauphin, and D.~Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock {\em ICLR}, 2018.

\bibitem{zhang2020does}
L.~Zhang, Z.~Deng, K.~Kawaguchi, A.~Ghorbani, and J.~Zou.
\newblock How does mixup help with robustness and generalization?
\newblock {\em ICLR}, 2021.

\bibitem{zhang2021and}
L.~Zhang, Z.~Deng, K.~Kawaguchi, and J.~Zou.
\newblock When and how mixup improves calibration.
\newblock {\em arXiv preprint arXiv:2102.06289}, 2021.

\end{thebibliography}
