\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arnold et~al.(2019)Arnold, Mahajan, Datta, and
  Bunner]{learn2learn2019}
Arnold, S.~M., Mahajan, P., Datta, D., and Bunner, I.
\newblock \emph{learn2learn}, 2019.
\newblock \url{https://github.com/learnables/learn2learn}.

\bibitem[Bertinetto et~al.(2018)Bertinetto, Henriques, Torr, and
  Vedaldi]{bertinetto2018meta}
Bertinetto, L., Henriques, J.~F., Torr, P., and Vedaldi, A.
\newblock Meta-learning with differentiable closed-form solvers.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Bracken \& McGill(1973)Bracken and McGill]{bracken1973mathematical}
Bracken, J. and McGill, J.~T.
\newblock Mathematical programs with optimization problems in the constraints.
\newblock \emph{Operations Research}, 21\penalty0 (1):\penalty0 37--44, 1973.

\bibitem[Domke(2012)]{domke2012generic}
Domke, J.
\newblock Generic methods for optimization-based modeling.
\newblock In \emph{Artificial Intelligence and Statistics (AISTATS)}, pp.\
  318--326, 2012.

\bibitem[Feurer \& Hutter(2019)Feurer and Hutter]{feurer2019hyperparameter}
Feurer, M. and Hutter, F.
\newblock Hyperparameter optimization.
\newblock In \emph{Automated Machine Learning}, pp.\  3--33. Springer, Cham,
  2019.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proc. International Conference on Machine Learning (ICML)},
  pp.\  1126--1135, 2017.

\bibitem[Flamary et~al.(2014)Flamary, Rakotomamonjy, and
  Gasso]{flamary2014learning}
Flamary, R., Rakotomamonjy, A., and Gasso, G.
\newblock Learning constrained task similarities in graphregularized multi-task
  learning.
\newblock \emph{Regularization, Optimization, Kernels, and Support Vector
  Machines}, pp.\  103, 2014.

\bibitem[Franceschi et~al.(2017)Franceschi, Donini, Frasconi, and
  Pontil]{franceschi2017forward}
Franceschi, L., Donini, M., Frasconi, P., and Pontil, M.
\newblock Forward and reverse gradient-based hyperparameter optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  1165--1173, 2017.

\bibitem[Franceschi et~al.(2018)Franceschi, Frasconi, Salzo, Grazzi, and
  Pontil]{franceschi2018bilevel}
Franceschi, L., Frasconi, P., Salzo, S., Grazzi, R., and Pontil, M.
\newblock Bilevel programming for hyperparameter optimization and
  meta-learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  1568--1577, 2018.

\bibitem[Ghadimi \& Wang(2018)Ghadimi and Wang]{ghadimi2018approximation}
Ghadimi, S. and Wang, M.
\newblock Approximation methods for bilevel programming.
\newblock \emph{arXiv preprint arXiv:1802.02246}, 2018.

\bibitem[Gould et~al.(2016)Gould, Fernando, Cherian, Anderson, Cruz, and
  Guo]{gould2016differentiating}
Gould, S., Fernando, B., Cherian, A., Anderson, P., Cruz, R.~S., and Guo, E.
\newblock On differentiating parameterized argmin and argmax problems with
  application to bi-level optimization.
\newblock \emph{arXiv preprint arXiv:1607.05447}, 2016.

\bibitem[Grazzi et~al.(2020)Grazzi, Franceschi, Pontil, and
  Salzo]{grazzi2020iteration}
Grazzi, R., Franceschi, L., Pontil, M., and Salzo, S.
\newblock On the iteration complexity of hypergradient computation.
\newblock In \emph{Proc. International Conference on Machine Learning (ICML)},
  2020.

\bibitem[Hansen et~al.(1992)Hansen, Jaumard, and Savard]{hansen1992new}
Hansen, P., Jaumard, B., and Savard, G.
\newblock New branch-and-bound rules for linear bilevel programming.
\newblock \emph{SIAM Journal on Scientific and Statistical Computing},
  13\penalty0 (5):\penalty0 1194--1217, 1992.

\bibitem[Hong et~al.(2020)Hong, Wai, Wang, and Yang]{hong2020two}
Hong, M., Wai, H.-T., Wang, Z., and Yang, Z.
\newblock A two-timescale framework for bilevel optimization: Complexity
  analysis and application to actor-critic.
\newblock \emph{arXiv preprint arXiv:2007.05170}, 2020.

\bibitem[Ji \& Liang(2021)Ji and Liang]{ji2021lower}
Ji, K. and Liang, Y.
\newblock Lower bounds and accelerated algorithms for bilevel optimization.
\newblock \emph{arXiv preprint arXiv:2102.03926}, 2021.

\bibitem[Ji et~al.(2020{\natexlab{a}})Ji, Lee, Liang, and
  Poor]{ji2020convergence}
Ji, K., Lee, J.~D., Liang, Y., and Poor, H.~V.
\newblock Convergence of meta-learning with task-specific adaptation over
  partial parameters.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020{\natexlab{a}}.

\bibitem[Ji et~al.(2020{\natexlab{b}})Ji, Yang, and Liang]{ji2020multi}
Ji, K., Yang, J., and Liang, Y.
\newblock Multi-step model-agnostic meta-learning: Convergence and improved
  algorithms.
\newblock \emph{arXiv preprint arXiv:2002.07836}, 2020{\natexlab{b}}.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2014.

\bibitem[Konda \& Tsitsiklis(2000)Konda and Tsitsiklis]{konda2000actor}
Konda, V.~R. and Tsitsiklis, J.~N.
\newblock Actor-critic algorithms.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, pp.\  1008--1014, 2000.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kunapuli et~al.(2008)Kunapuli, Bennett, Hu, and
  Pang]{kunapuli2008classification}
Kunapuli, G., Bennett, K.~P., Hu, J., and Pang, J.-S.
\newblock Classification model selection via bilevel programming.
\newblock \emph{Optimization Methods \& Software}, 23\penalty0 (4):\penalty0
  475--489, 2008.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Li et~al.(2020)Li, Gu, and Huang]{li2020improved}
Li, J., Gu, B., and Huang, H.
\newblock Improved bilevel model: Fast and optimal algorithm with theoretical
  guarantee.
\newblock \emph{arXiv preprint arXiv:2009.00690}, 2020.

\bibitem[Liao et~al.(2018)Liao, Xiong, Fetaya, Zhang, Yoon, Pitkow, Urtasun,
  and Zemel]{liao2018reviving}
Liao, R., Xiong, Y., Fetaya, E., Zhang, L., Yoon, K., Pitkow, X., Urtasun, R.,
  and Zemel, R.
\newblock Reviving and improving recurrent back-propagation.
\newblock In \emph{Proc. International Conference on Machine Learning (ICML)},
  2018.

\bibitem[Liu et~al.(2020)Liu, Mu, Yuan, Zeng, and Zhang]{liu2020generic}
Liu, R., Mu, P., Yuan, X., Zeng, S., and Zhang, J.
\newblock A generic first-order algorithmic framework for bi-level programming
  beyond lower-level singleton.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Lorraine et~al.(2020)Lorraine, Vicol, and
  Duvenaud]{lorraine2020optimizing}
Lorraine, J., Vicol, P., and Duvenaud, D.
\newblock Optimizing millions of hyperparameters by implicit differentiation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, pp.\  1540--1552. PMLR, 2020.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and
  Adams]{maclaurin2015gradient}
Maclaurin, D., Duvenaud, D., and Adams, R.
\newblock Gradient-based hyperparameter optimization through reversible
  learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  2113--2122, 2015.

\bibitem[Moore(2010)]{moore2010bilevel}
Moore, G.~M.
\newblock \emph{Bilevel programming algorithms for machine learning model
  selection}.
\newblock Rensselaer Polytechnic Institute, 2010.

\bibitem[Okuno et~al.(2018)Okuno, Takeda, and Kawana]{okuno2018hyperparameter}
Okuno, T., Takeda, A., and Kawana, A.
\newblock Hyperparameter learning via bilevel nonsmooth optimization.
\newblock \emph{arXiv preprint arXiv:1806.01520}, 2018.

\bibitem[Oreshkin et~al.(2018)Oreshkin, L{\'o}pez, and
  Lacoste]{oreshkin2018tadam}
Oreshkin, B., L{\'o}pez, P.~R., and Lacoste, A.
\newblock Tadam: Task dependent adaptive metric for improved few-shot learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  721--731, 2018.

\bibitem[Pedregosa(2016)]{pedregosa2016hyperparameter}
Pedregosa, F.
\newblock Hyperparameter optimization with approximate gradient.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  737--746, 2016.

\bibitem[Raghu et~al.(2019)Raghu, Raghu, Bengio, and Vinyals]{raghu2019rapid}
Raghu, A., Raghu, M., Bengio, S., and Vinyals, O.
\newblock Rapid learning or feature reuse? towards understanding the
  effectiveness of {MAML}.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019.

\bibitem[Rajeswaran et~al.(2019)Rajeswaran, Finn, Kakade, and
  Levine]{rajeswaran2019meta}
Rajeswaran, A., Finn, C., Kakade, S.~M., and Levine, S.
\newblock Meta-learning with implicit gradients.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  113--124, 2019.

\bibitem[Roth et~al.(2016)Roth, Ullman, and Wu]{roth2016watch}
Roth, A., Ullman, J., and Wu, Z.~S.
\newblock Watch and learn: Optimizing from revealed preferences feedback.
\newblock In \emph{Annual ACM Symposium on Theory of Computing (STOC)}, pp.\
  949--962, 2016.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and
  Fei-Fei]{russakovsky2015imagenet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., Berg, A.~C., and Fei-Fei, L.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International Journal of Computer Vision}, 3\penalty0
  (115):\penalty0 211--252, 2015.

\bibitem[Shaban et~al.(2019)Shaban, Cheng, Hatch, and
  Boots]{shaban2019truncated}
Shaban, A., Cheng, C.-A., Hatch, N., and Boots, B.
\newblock Truncated back-propagation for bilevel optimization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, pp.\  1723--1732, 2019.

\bibitem[Shi et~al.(2005)Shi, Lu, and Zhang]{shi2005extended}
Shi, C., Lu, J., and Zhang, G.
\newblock An extended kuhn--tucker approach for linear bilevel programming.
\newblock \emph{Applied Mathematics and Computation}, 162\penalty0
  (1):\penalty0 51--63, 2005.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{snell2017prototypical}
Snell, J., Swersky, K., and Zemel, R.
\newblock Prototypical networks for few-shot learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, and
  Wierstra]{vinyals2016matching}
Vinyals, O., Blundell, C., Lillicrap, T., and Wierstra, D.
\newblock Matching networks for one shot learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2016.

\bibitem[Yu \& Zhu(2020)Yu and Zhu]{yu2020hyper}
Yu, T. and Zhu, H.
\newblock Hyper-parameter optimization: A review of algorithms and
  applications.
\newblock \emph{arXiv preprint arXiv:2003.05689}, 2020.

\bibitem[Z{\"u}gner \& G{\"u}nnemann(2019)Z{\"u}gner and
  G{\"u}nnemann]{zugner2019adversarial}
Z{\"u}gner, D. and G{\"u}nnemann, S.
\newblock Adversarial attacks on graph neural networks via meta learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\end{thebibliography}
