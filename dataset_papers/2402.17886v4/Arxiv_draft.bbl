\begin{thebibliography}{10}

\bibitem{balasubramanian2022towards}
K.~Balasubramanian, S.~Chewi, M.~A. Erdogdu, A.~Salim, and S.~Zhang.
\newblock Towards a theory of non-log-concave sampling: first-order stationarity guarantees for langevin monte carlo.
\newblock In {\em Conference on Learning Theory}, pages 2896--2923. PMLR, 2022.

\bibitem{belisle1993hit}
C.~J. B{\'e}lisle, H.~E. Romeijn, and R.~L. Smith.
\newblock Hit-and-run algorithms for generating multivariate distributions.
\newblock {\em Mathematics of Operations Research}, 18(2):255--266, 1993.

\bibitem{benton2023linear}
J.~Benton, V.~De~Bortoli, A.~Doucet, and G.~Deligiannidis.
\newblock Linear convergence bounds for diffusion models via stochastic localization.
\newblock {\em ICLR}, 2024.

\bibitem{chen2023improved}
H.~Chen, H.~Lee, and J.~Lu.
\newblock Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions.
\newblock In {\em International Conference on Machine Learning}, pages 4735--4763. PMLR, 2023.

\bibitem{chen2022sampling}
S.~Chen, S.~Chewi, J.~Li, Y.~Li, A.~Salim, and A.~Zhang.
\newblock Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions.
\newblock {\em ICLR}, 2022.

\bibitem{chen2022improved}
Y.~Chen, S.~Chewi, A.~Salim, and A.~Wibisono.
\newblock Improved analysis for a proximal algorithm for sampling.
\newblock In {\em Conference on Learning Theory}, pages 2984--3014. PMLR, 2022.

\bibitem{chen2022localization}
Y.~Chen and R.~Eldan.
\newblock Localization schemes: A framework for proving mixing bounds for markov chains.
\newblock In {\em 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)}, pages 110--122. IEEE, 2022.

\bibitem{chen2023simple}
Y.~Chen and K.~Gatmiry.
\newblock A simple proof of the mixing of metropolis-adjusted langevin algorithm under smoothness and isoperimetry.
\newblock {\em arXiv preprint arXiv:2304.04095}, 2023.

\bibitem{chewisamplingbook}
S.~Chewi.
\newblock {\em Log-concave sampling}.
\newblock 2023.
\newblock Book draft available at \url{https://chewisinho.github.io/}.

\bibitem{chewi2020svgd}
S.~Chewi, T.~Le~Gouic, C.~Lu, T.~Maunu, and P.~Rigollet.
\newblock Svgd as a kernelized wasserstein gradient flow of the chi-squared divergence.
\newblock {\em Advances in Neural Information Processing Systems}, 33:2098--2109, 2020.

\bibitem{conforti2023score}
G.~Conforti, A.~Durmus, and M.~G. Silveri.
\newblock Score diffusion models without early stopping: finite fisher information is all you need.
\newblock {\em arXiv preprint arXiv:2308.12240}, 2023.

\bibitem{dalalyan2019user}
A.~S. Dalalyan and A.~Karagulyan.
\newblock User-friendly guarantees for the langevin monte carlo with inaccurate gradient.
\newblock {\em Stochastic Processes and their Applications}, 129(12):5278--5311, 2019.

\bibitem{dalalyan2020sampling}
A.~S. Dalalyan and L.~Riou-Durand.
\newblock On sampling from a log-concave density using kinetic langevin diffusions.
\newblock {\em Bernoulli}, 26(3):1956--1988, 2020.

\bibitem{de2022convergence}
V.~De~Bortoli.
\newblock Convergence of denoising diffusion models under the manifold hypothesis.
\newblock {\em TMLR}, 2022.

\bibitem{del2006sequential}
P.~Del~Moral, A.~Doucet, and A.~Jasra.
\newblock Sequential monte carlo samplers.
\newblock {\em Journal of the Royal Statistical Society Series B: Statistical Methodology}, 68(3):411--436, 2006.

\bibitem{dwivedi2019log}
R.~Dwivedi, Y.~Chen, M.~J. Wainwright, and B.~Yu.
\newblock Log-concave sampling: Metropolis-hastings algorithms are fast.
\newblock {\em Journal of Machine Learning Research}, 20(183):1--42, 2019.

\bibitem{dyer1991random}
M.~Dyer, A.~Frieze, and R.~Kannan.
\newblock A random polynomial-time algorithm for approximating the volume of convex bodies.
\newblock {\em Journal of the ACM (JACM)}, 38(1):1--17, 1991.

\bibitem{fan2023improved}
J.~Fan, B.~Yuan, and Y.~Chen.
\newblock Improved dimension dependence of a proximal algorithm for sampling.
\newblock In {\em The Thirty Sixth Annual Conference on Learning Theory}, pages 1473--1521. PMLR, 2023.

\bibitem{garbuno2020interacting}
A.~Garbuno-Inigo, F.~Hoffmann, W.~Li, and A.~M. Stuart.
\newblock Interacting langevin diffusions: Gradient structure and ensemble kalman sampler.
\newblock {\em SIAM Journal on Applied Dynamical Systems}, 19(1):412--441, 2020.

\bibitem{pmlr-v235-grenioux24a}
L.~Grenioux, M.~Noble, M.~Gabri\'{e}, and A.~Oliviero~Durmus.
\newblock Stochastic localization via iterative posterior sampling.
\newblock In {\em Proceedings of the 41st International Conference on Machine Learning}, volume 235, pages 16337--16376. PMLR, 21--27 Jul 2024.

\bibitem{he2020ergodicity}
Y.~He, K.~Balasubramanian, and M.~A. Erdogdu.
\newblock On the ergodicity, bias and asymptotic normality of randomized midpoint sampling method.
\newblock {\em Advances in Neural Information Processing Systems}, 33:7366--7376, 2020.

\bibitem{he2022regularized}
Y.~He, K.~Balasubramanian, B.~K. Sriperumbudur, and J.~Lu.
\newblock Regularized stein variational gradient flow.
\newblock {\em arXiv preprint arXiv:2211.07861}, 2022.

\bibitem{he2024mean}
Y.~He, T.~Farghly, K.~Balasubramanian, and M.~A. Erdogdu.
\newblock Mean-square analysis of discretized it{\^o} diffusions for heavy-tailed sampling.
\newblock {\em Journal of Machine Learning Research}, 25(43):1--44, 2024.

\bibitem{Ho2020DenoisingDP}
J.~Ho, A.~Jain, and P.~Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in Neural Information Processing Systems}, 33:6840--6851, 2020.

\bibitem{holzmuller2023convergence}
D.~Holzm{\"u}ller and F.~Bach.
\newblock Convergence rates for non-log-concave sampling and log-partition estimation.
\newblock {\em arXiv preprint arXiv:2303.03237}, 2023.

\bibitem{huang2023monte}
X.~Huang, H.~Dong, Y.~Hao, Y.~Ma, and T.~Zhang.
\newblock Reverse diffusion monte carlo.
\newblock {\em ICLR}, 2024.

\bibitem{huang2024faster}
X.~Huang, D.~Zou, H.~Dong, Y.~Ma, and T.~Zhang.
\newblock Faster sampling without isoperimetry via diffusion-based monte carlo.
\newblock {\em COLT}, 2024.

\bibitem{iglesias2013ensemble}
M.~A. Iglesias, K.~J. Law, and A.~M. Stuart.
\newblock Ensemble kalman methods for inverse problems.
\newblock {\em Inverse Problems}, 29(4):045001, 2013.

\bibitem{lee2023convergence}
H.~Lee, J.~Lu, and Y.~Tan.
\newblock Convergence of score-based generative modeling for general data distributions.
\newblock In {\em International Conference on Algorithmic Learning Theory}, pages 946--985. PMLR, 2023.

\bibitem{lee2023parallel}
H.~Lee and Z.~Shen.
\newblock Improved bound for mixing time of parallel tempering.
\newblock {\em arXiv preprint arXiv:2304.01303}, 2023.

\bibitem{lee2021structured}
Y.~T. Lee, R.~Shen, and K.~Tian.
\newblock Structured logconcave sampling with a restricted gaussian oracle.
\newblock In {\em Conference on Learning Theory}, pages 2993--3050. PMLR, 2021.

\bibitem{li2023towards}
G.~Li, Y.~Wei, Y.~Chen, and Y.~Chi.
\newblock Towards faster non-asymptotic convergence for diffusion-based generative models.
\newblock {\em ICLR}, 2024.

\bibitem{li2022mirror}
R.~Li, M.~Tao, S.~S. Vempala, and A.~Wibisono.
\newblock The mirror {L}angevin algorithm converges with vanishing bias.
\newblock In {\em International Conference on Algorithmic Learning Theory}, pages 718--742. PMLR, 2022.

\bibitem{li2021sqrt}
R.~Li, H.~Zha, and M.~Tao.
\newblock {Sqrt(d) Dimension Dependence of Langevin Monte Carlo}.
\newblock In {\em ICLR}, 2021.

\bibitem{li2024automated}
X.~H. Li and M.~Tao.
\newblock Automated construction of effective potential via algorithmic implicit bias.
\newblock {\em arXiv preprint arXiv:2401.03511}, 2024.

\bibitem{liang2022proximal}
J.~Liang and Y.~Chen.
\newblock A proximal algorithm for sampling.
\newblock {\em arXiv preprint arXiv:2202.13975}, 2022.

\bibitem{liang2022proximal1}
J.~Liang and Y.~Chen.
\newblock A proximal algorithm for sampling.
\newblock {\em arXiv preprint arXiv:2202.13975}, 2022.

\bibitem{liu2017stein}
Q.~Liu.
\newblock Stein variational gradient descent as gradient flow.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{lovasz1990mixing}
L.~Lov{\'a}sz and M.~Simonovits.
\newblock The mixing rate of markov chains, an isoperimetric inequality, and computing the volume.
\newblock In {\em Proceedings [1990] 31st annual symposium on foundations of computer science}, pages 346--354. IEEE, 1990.

\bibitem{lovasz2004hit}
L.~Lov{\'a}sz and S.~Vempala.
\newblock Hit-and-run from a corner.
\newblock In {\em Proceedings of the thirty-sixth annual ACM symposium on Theory of computing}, pages 310--314, 2004.

\bibitem{mengersen1996rates}
K.~L. Mengersen and R.~L. Tweedie.
\newblock Rates of convergence of the hastings and metropolis algorithms.
\newblock {\em The annals of Statistics}, 24(1):101--121, 1996.

\bibitem{neal2001annealed}
R.~M. Neal.
\newblock Annealed importance sampling.
\newblock {\em Statistics and computing}, 11:125--139, 2001.

\bibitem{pardo2018statistical}
L.~Pardo.
\newblock {\em Statistical inference based on divergence measures}.
\newblock CRC press, 2018.

\bibitem{richter2023improved}
L.~Richter, J.~Berner, and G.-H. Liu.
\newblock Improved sampling via learned diffusions.
\newblock {\em ICLR}, 2024.

\bibitem{robbins1992empirical}
H.~E. Robbins.
\newblock An empirical bayes approach to statistics.
\newblock In {\em Breakthroughs in Statistics: Foundations and basic theory}, pages 388--394. Springer, 1992.

\bibitem{roberts1996geometric}
G.~O. Roberts and R.~L. Tweedie.
\newblock Geometric convergence and central limit theorems for multidimensional hastings and metropolis algorithms.
\newblock {\em Biometrika}, 83(1):95--110, 1996.

\bibitem{roy2022stochastic}
A.~Roy, L.~Shen, K.~Balasubramanian, and S.~Ghadimi.
\newblock Stochastic zeroth-order discretizations of langevin diffusions for bayesian inference.
\newblock {\em Bernoulli}, 28(3):1810--1834, 2022.

\bibitem{salim2022convergence}
A.~Salim, L.~Sun, and P.~Richtarik.
\newblock A convergence theory for svgd in the population limit under talagrandâ€™s inequality t1.
\newblock In {\em International Conference on Machine Learning}, pages 19139--19152. PMLR, 2022.

\bibitem{schlichting2019poincare}
A.~Schlichting.
\newblock Poincar{\'e} and log--sobolev inequalities for mixtures.
\newblock {\em Entropy}, 21(1):89, 2019.

\bibitem{shen2019randomized}
R.~Shen and Y.~T. Lee.
\newblock The randomized midpoint method for log-concave sampling.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{sohl2015deep}
J.~Sohl-Dickstein, E.~Weiss, N.~Maheswaranathan, and S.~Ganguli.
\newblock {Deep unsupervised learning using nonequilibrium thermodynamics}.
\newblock {\em ICML}, 2015.

\bibitem{song2021SGM}
Y.~Song, J.~Sohl-Dickstein, D.~P. Kingma, A.~Kumar, S.~Ermon, and B.~Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{vargas2023denoising}
F.~Vargas, W.~Grathwohl, and A.~Doucet.
\newblock Denoising diffusion samplers.
\newblock {\em arXiv preprint arXiv:2302.13834}, 2023.

\bibitem{vargas2023bayesian}
F.~Vargas, A.~Ovsianas, D.~Fernandes, M.~Girolami, N.~D. Lawrence, and N.~N{\"u}sken.
\newblock Bayesian learning via neural schr{\"o}dinger--f{\"o}llmer flows.
\newblock {\em Statistics and Computing}, 33(1):3, 2023.

\bibitem{vargas2024transport}
F.~Vargas, S.~Padhy, D.~Blessing, and N.~N{\"u}sken.
\newblock Transport meets variational inference: Controlled monte carlo diffusions.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{vempala2019rapid}
S.~Vempala and A.~Wibisono.
\newblock Rapid convergence of the unadjusted langevin algorithm: Isoperimetry suffices.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{yingxi2022convergence}
K.~Yingxi~Yang and A.~Wibisono.
\newblock Convergence of the inexact langevin algorithm and score-based generative models in kl divergence.
\newblock {\em arXiv e-prints}, pages arXiv--2211, 2022.

\bibitem{zhang2022path}
Q.~Zhang and Y.~Chen.
\newblock Path integral sampler: a stochastic control approach for sampling.
\newblock {\em ICLR}, 2022.

\end{thebibliography}
