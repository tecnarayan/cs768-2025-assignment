\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abe et~al.(2022)Abe, Buchanan, Pleiss, Zemel, and
  Cunningham]{abe2202ensemblenecessary}
T.~Abe, E.~K. Buchanan, G.~Pleiss, R.~S. Zemel, and J.~P. Cunningham.
\newblock Deep ensembles work, but are they necessary?
\newblock \emph{CoRR}, abs/2202.06985, 2022.

\bibitem[Bahdanau et~al.(2015)Bahdanau, Cho, and
  Bengio]{Bahdanau2015seq2seqattn}
D.~Bahdanau, K.~Cho, and Y.~Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock In \emph{ICLR}, 2015.

\bibitem[Blum et~al.(2015)Blum, Haghtalab, and Procaccia]{blum2015varidrop}
A.~Blum, N.~Haghtalab, and A.~D. Procaccia.
\newblock Variational dropout and the local reparameterization trick.
\newblock In \emph{NIPS}, 2015.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell15weightuncertainty}
C.~Blundell, J.~Cornebise, K.~Kavukcuoglu, and D.~Wierstra.
\newblock Weight uncertainty in neural network.
\newblock In \emph{ICML}, 2015.

\bibitem[Chen et~al.(2014)Chen, Fox, and Guestrin]{chen2014sghmc}
T.~Chen, E.~B. Fox, and C.~Guestrin.
\newblock Stochastic gradient hamiltonian monte carlo.
\newblock In \emph{ICML}, 2014.

\bibitem[Farquhar et~al.(2020)Farquhar, Osborne, and Gal]{farquhar2020radial}
S.~Farquhar, M.~A. Osborne, and Y.~Gal.
\newblock Radial bayesian neural networks: Beyond discrete support in
  large-scale bayesian deep learning.
\newblock In \emph{AISTATS}, 2020.

\bibitem[Filos et~al.(2019)Filos, Farquhar, Gomez, Rudner, Kenton, Smith,
  Alizadeh, De~Kroon, and Gal]{filos2019systematic}
A.~Filos, S.~Farquhar, A.~N. Gomez, T.~G. Rudner, Z.~Kenton, L.~Smith,
  M.~Alizadeh, A.~De~Kroon, and Y.~Gal.
\newblock A systematic comparison of bayesian deep learning robustness in
  diabetic retinopathy tasks.
\newblock \emph{arXiv preprint arXiv:1912.10481}, 2019.

\bibitem[Gal and Ghahramani(2016)]{gal2016mcdropout}
Y.~Gal and Z.~Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{ICML}, 2016.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{garipov2018fge}
T.~Garipov, P.~Izmailov, D.~Podoprikhin, D.~P. Vetrov, and A.~G. Wilson.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Graves(2011)]{graves2011practical}
A.~Graves.
\newblock Practical variational inference for neural networks.
\newblock In \emph{NIPS}, 2011.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
C.~Guo, G.~Pleiss, Y.~Sun, and K.~Q. Weinberger.
\newblock On calibration of modern neural networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Gustafsson et~al.(2020)Gustafsson, Danelljan, and
  Sch{\"{o}}n]{gustafsson2020bdlbanchmark}
F.~K. Gustafsson, M.~Danelljan, and T.~B. Sch{\"{o}}n.
\newblock Evaluating scalable bayesian deep learning methods for robust
  computer vision.
\newblock In \emph{CVPR Workshop}, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016preresnet}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{ECCV}, 2016.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2019cifarc}
D.~Hendrycks and T.~G. Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock In \emph{ICLR}, 2019.

\bibitem[Hendrycks and Gimpel(2017)]{Hendrycks2017ood}
D.~Hendrycks and K.~Gimpel.
\newblock A baseline for detecting misclassified and out-of-distribution
  examples in neural networks.
\newblock In \emph{ICLR}, 2017.

\bibitem[Hernandez-Lobato and Adams(2015)]{lobato2015pbp}
J.~M. Hernandez-Lobato and R.~Adams.
\newblock Probabilistic backpropagation for scalable learning of bayesian
  neural networks.
\newblock In \emph{ICML}, 2015.

\bibitem[Huang et~al.(2017{\natexlab{a}})Huang, Li, Pleiss, Liu, Hopcroft, and
  Weinberger]{huang2017snapshotens}
G.~Huang, Y.~Li, G.~Pleiss, Z.~Liu, J.~E. Hopcroft, and K.~Q. Weinberger.
\newblock Snapshot ensembles: Train 1, get {M} for free.
\newblock In \emph{ICLR}, 2017{\natexlab{a}}.

\bibitem[Huang et~al.(2017{\natexlab{b}})Huang, Liu, van~der Maaten, and
  Weinberger]{huang2017densely}
G.~Huang, Z.~Liu, L.~van~der Maaten, and K.~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{CVPR}, 2017{\natexlab{b}}.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2018swa}
P.~Izmailov, D.~Podoprikhin, T.~Garipov, D.~P. Vetrov, and A.~G. Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock In \emph{UAI}, 2018.

\bibitem[Kendall and Gal(2017)]{kendall2017whatuncertainty}
A.~Kendall and Y.~Gal.
\newblock What uncertainties do we need in bayesian deep learning for computer
  vision?
\newblock In \emph{NIPS}, 2017.

\bibitem[Khan et~al.(2018)Khan, Nielsen, Tangkaratt, Lin, Gal, and
  Srivastava]{khan2018vadam}
M.~E. Khan, D.~Nielsen, V.~Tangkaratt, W.~Lin, Y.~Gal, and A.~Srivastava.
\newblock Fast and scalable bayesian deep learning by weight-perturbation in
  adam.
\newblock In \emph{ICML}, 2018.

\bibitem[Kingma and Welling(2014)]{kingma2013vae}
D.~P. Kingma and M.~Welling.
\newblock Auto-encoding variational bayes.
\newblock In \emph{ICLR}, 2014.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{kirkpatrick2017overcoming}
J.~Kirkpatrick, R.~Pascanu, N.~Rabinowitz, J.~Veness, G.~Desjardins, A.~A.
  Rusu, K.~Milan, J.~Quan, T.~Ramalho, A.~Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{PNAS}, 2017.

\bibitem[Koller and Friedman(2009)]{koller2009pgmbook}
D.~Koller and N.~Friedman.
\newblock \emph{Probabilistic Graphical Models: Principles and Techniques}.
\newblock MIT Press, 2009.

\bibitem[Krizhevsky and Hinton(2009)]{krizhevsky2009cifar10}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{lakshminarayanan2017deepensemble}
B.~Lakshminarayanan, A.~Pritzel, and C.~Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In \emph{NIPS}, 2017.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deeplearning}
Y.~LeCun, Y.~Bengio, and G.~Hinton.
\newblock Deep learning.
\newblock \emph{Nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Lee et~al.(2015)Lee, Purushwalkam, Cogswell, Crandall, and
  Batra]{lee2015treenet}
S.~Lee, S.~Purushwalkam, M.~Cogswell, D.~Crandall, and D.~Batra.
\newblock Why m heads are better than one: Training a diverse ensemble of deep
  networks.
\newblock \emph{arXiv preprint arXiv:1511.06314}, 2015.

\bibitem[Leibig et~al.(2017)Leibig, Allken, Ayhan, Berens, and
  Wahl]{leibig2017leveraging}
C.~Leibig, V.~Allken, M.~S. Ayhan, P.~Berens, and S.~Wahl.
\newblock Leveraging uncertainty information from deep neural networks for
  disease detection.
\newblock \emph{Nature Scientific reports}, 7\penalty0 (1):\penalty0 1--14,
  2017.

\bibitem[Liang et~al.(2018)Liang, Li, and Srikant]{liang2018odin}
S.~Liang, Y.~Li, and R.~Srikant.
\newblock Enhancing the reliability of out-of-distribution image detection in
  neural networks.
\newblock In \emph{ICLR}, 2018.

\bibitem[Maddox et~al.(2019)Maddox, Izmailov, Garipov, Vetrov, and
  Wilson]{maddox2019swag}
W.~J. Maddox, P.~Izmailov, T.~Garipov, D.~P. Vetrov, and A.~G. Wilson.
\newblock A simple baseline for bayesian uncertainty in deep learning.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Masegosa(2020)]{Masegosa2020pac2}
A.~R. Masegosa.
\newblock Learning under model misspecification: Applications to variational
  and ensemble methods.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and
  Vetrov]{molchanov2017varidrop}
D.~Molchanov, A.~Ashukha, and D.~P. Vetrov.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011svhn}
Y.~Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, and A.~Y. Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In \emph{NIPS Workshop}, 2011.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~Kopf, E.~Yang, Z.~DeVito,
  M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang, J.~Bai, and
  S.~Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Ritter et~al.(2018)Ritter, Botev, and Barber]{ritter2018a}
H.~Ritter, A.~Botev, and D.~Barber.
\newblock A scalable laplace approximation for neural networks.
\newblock In \emph{ICLR}, 2018.

\bibitem[Sensoy et~al.(2018)Sensoy, Kaplan, and
  Kandemir]{sensoy2018evidentialdl}
M.~Sensoy, L.~M. Kaplan, and M.~Kandemir.
\newblock Evidential deep learning to quantify classification uncertainty.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Settles(2009)]{settles2009activelearning}
B.~Settles.
\newblock Active learning literature survey.
\newblock Technical report, University of Wisconsin--Madison, 2009.

\bibitem[Shen and Cremers(2020)]{shen2020chain}
Y.~Shen and D.~Cremers.
\newblock A chain graph interpretation of real-world neural networks.
\newblock \emph{arXiv preprint arXiv:2006.16856}, 2020.

\bibitem[Simonyan and Zisserman(2015)]{simonyan2014vgg}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{ICLR}, 2015.

\bibitem[Snoek et~al.(2019)Snoek, Ovadia, Fertig, Lakshminarayanan, Nowozin,
  Sculley, Dillon, Ren, and Nado]{snoek2019canyoutrust}
J.~Snoek, Y.~Ovadia, E.~Fertig, B.~Lakshminarayanan, S.~Nowozin, D.~Sculley,
  J.~V. Dillon, J.~Ren, and Z.~Nado.
\newblock {Can you trust your model's uncertainty? Evaluating predictive
  uncertainty under dataset shift}.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Song and Chai(2018)]{song2018collaborative}
G.~Song and W.~Chai.
\newblock Collaborative learning for deep neural networks.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Tomani et~al.(2021)Tomani, Gruber, Erdem, Cremers, and
  Buettner]{tomani2021calibration}
C.~Tomani, S.~Gruber, M.~E. Erdem, D.~Cremers, and F.~Buettner.
\newblock Post-hoc uncertainty calibration for domain drift scenarios.
\newblock In \emph{CVPR}, 2021.

\bibitem[Welling and Teh(2011)]{welling2011sgld}
M.~Welling and Y.~W. Teh.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In \emph{ICML}, 2011.

\bibitem[Wen et~al.(2018)Wen, Vicol, Ba, Tran, and Grosse]{wen2018flipout}
Y.~Wen, P.~Vicol, J.~Ba, D.~Tran, and R.~B. Grosse.
\newblock Flipout: Efficient pseudo-independent weight perturbations on
  mini-batches.
\newblock In \emph{ICLR}, 2018.

\bibitem[Wen et~al.(2020)Wen, Tran, and Ba]{wen2020batchensemble}
Y.~Wen, D.~Tran, and J.~Ba.
\newblock Batchensemble: an alternative approach to efficient ensemble and
  lifelong learning.
\newblock In \emph{ICLR}, 2020.

\bibitem[Wenzel et~al.(2020)Wenzel, Snoek, Tran, and
  Jenatton]{Wenzel2020hyperens}
F.~Wenzel, J.~Snoek, D.~Tran, and R.~Jenatton.
\newblock Hyperparameter ensembles for robustness and uncertainty
  quantification.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Wilson and Izmailov(2020)]{wilson2020multiswag}
A.~G. Wilson and P.~Izmailov.
\newblock Bayesian deep learning and a probabilistic perspective of
  generalization.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Zhang et~al.(2020)Zhang, Jiang, Shao, and Cui]{Zhang2020edde}
W.~Zhang, J.~Jiang, Y.~Shao, and B.~Cui.
\newblock Efficient diversity-driven ensemble for deep neural networks.
\newblock In \emph{ICDE}, 2020.

\bibitem[Zhang et~al.(2015)Zhang, Zhao, and LeCun]{Zhang2015agnews}
X.~Zhang, J.~J. Zhao, and Y.~LeCun.
\newblock Character-level convolutional networks for text classification.
\newblock In \emph{NIPS}, 2015.

\bibitem[Zhang et~al.(2018)Zhang, Xiang, Hospedales, and Lu]{zhang2018dml}
Y.~Zhang, T.~Xiang, T.~M. Hospedales, and H.~Lu.
\newblock Deep mutual learning.
\newblock In \emph{CVPR}, 2018.

\end{thebibliography}
