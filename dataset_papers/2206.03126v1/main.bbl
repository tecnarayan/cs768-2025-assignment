\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pages
  242--252. PMLR, 2019.

\bibitem[Arpit et~al.(2019)Arpit, Campos, and Bengio]{arpit2019initialize}
Devansh Arpit, V{\'\i}ctor Campos, and Yoshua Bengio.
\newblock How to initialize your network? robust initialization for weightnorm
  \& resnets.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Bachlechner et~al.(2021)Bachlechner, Majumder, Mao, Cottrell, and
  McAuley]{bachlechner2021rezero}
Thomas Bachlechner, Bodhisattwa~Prasad Majumder, Henry Mao, Gary Cottrell, and
  Julian McAuley.
\newblock Rezero is all you need: Fast convergence at large depth.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 1352--1361.
  PMLR, 2021.

\bibitem[Baevski et~al.(2020)Baevski, Zhou, Mohamed, and
  Auli]{baevski2020wav2vec}
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.
\newblock wav2vec 2.0: A framework for self-supervised learning of speech
  representations.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 12449--12460, 2020.

\bibitem[Bengio et~al.(1994)Bengio, Simard, and Frasconi]{bengio1994learning}
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock \emph{IEEE transactions on neural networks}, 5\penalty0 (2):\penalty0
  157--166, 1994.

\bibitem[Biggio et~al.(2021)Biggio, Bendinelli, Neitz, Lucchi, and
  Parascandolo]{biggio2021neural}
Luca Biggio, Tommaso Bendinelli, Alexander Neitz, Aurelien Lucchi, and
  Giambattista Parascandolo.
\newblock Neural symbolic regression that scales.
\newblock In \emph{International Conference on Machine Learning}, pages
  936--945. PMLR, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Chen et~al.(2018)Chen, Firat, Bapna, Johnson, Macherey, Foster, Jones,
  Parmar, Schuster, Chen, et~al.]{chen2018best}
Mia~Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey,
  George Foster, Llion Jones, Niki Parmar, Mike Schuster, Zhifeng Chen, et~al.
\newblock The best of both worlds: Combining recent advances in neural machine
  translation.
\newblock \emph{arXiv preprint arXiv:1804.09849}, 2018.

\bibitem[Cho and Saul(2009)]{cho2009kernel}
Youngmin Cho and Lawrence Saul.
\newblock Kernel methods for deep learning.
\newblock \emph{Advances in neural information processing systems}, 22, 2009.

\bibitem[Conneau and Lample(2019)]{mtlample}
Alexis Conneau and Guillaume Lample.
\newblock \emph{Cross-Lingual Language Model Pretraining}.
\newblock Curran Associates Inc., Red Hook, NY, USA, 2019.

\bibitem[Daneshmand et~al.(2020{\natexlab{a}})Daneshmand, Kohler, Bach,
  Hofmann, and Lucchi]{daneshmand2020batch}
Hadi Daneshmand, Jonas Kohler, Francis Bach, Thomas Hofmann, and Aurelien
  Lucchi.
\newblock Batch normalization provably avoids rank collapse for randomly
  initialised deep networks.
\newblock \emph{arXiv preprint arXiv:2003.01652}, 2020{\natexlab{a}}.

\bibitem[Daneshmand et~al.(2020{\natexlab{b}})Daneshmand, Kohler, Bach,
  Hofmann, and Lucchi]{jonas}
Hadi Daneshmand, Jonas Kohler, Francis Bach, Thomas Hofmann, and Aurelien
  Lucchi.
\newblock Batch normalization provably avoids ranks collapse for randomly
  initialised deep networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 18387--18398, 2020{\natexlab{b}}.

\bibitem[Daunizeau(2017)]{daunizeau2017semi}
Jean Daunizeau.
\newblock Semi-analytical approximations to statistical moments of sigmoid and
  softmax mappings of normal variables.
\newblock \emph{arXiv preprint arXiv:1703.00091}, 2017.

\bibitem[Dong et~al.(2021)Dong, Cordonnier, and Loukas]{dong2021attention}
Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas.
\newblock Attention is not all you need: Pure attention loses rank doubly
  exponentially with depth.
\newblock In \emph{International Conference on Machine Learning}, pages
  2793--2803. PMLR, 2021.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Glorot and Bengio(2010{\natexlab{a}})]{glorot2010init}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256, 2010{\natexlab{a}}.

\bibitem[Glorot and Bengio(2010{\natexlab{b}})]{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256. JMLR Workshop and
  Conference Proceedings, 2010{\natexlab{b}}.

\bibitem[Hanin(2018)]{hanin2018neural}
Boris Hanin.
\newblock Which neural net architectures give rise to exploding and vanishing
  gradients?
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Hanin and Rolnick(2018)]{hanin2018start}
Boris Hanin and David Rolnick.
\newblock How to start training: The effect of initialization and architecture.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Hayou et~al.(2021)Hayou, Clerico, He, Deligiannidis, Doucet, and
  Rousseau]{hayou2021stable}
Soufiane Hayou, Eugenio Clerico, Bobby He, George Deligiannidis, Arnaud Doucet,
  and Judith Rousseau.
\newblock Stable resnet.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1324--1332. PMLR, 2021.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet2016}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{In IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 770--778, 2016.

\bibitem[Hochreiter(1991)]{hochreiter1991untersuchungen}
Sepp Hochreiter.
\newblock Untersuchungen zu dynamischen neuronalen netzen.
\newblock \emph{Diploma, Technische Universit{\"a}t M{\"u}nchen}, 91\penalty0
  (1), 1991.

\bibitem[Huang et~al.(2018)Huang, Vaswani, Uszkoreit, Simon, Hawthorne,
  Shazeer, Dai, Hoffman, Dinculescu, and Eck]{huang2018music}
Cheng-Zhi~Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis
  Hawthorne, Noam Shazeer, Andrew~M Dai, Matthew~D Hoffman, Monica Dinculescu,
  and Douglas Eck.
\newblock Music transformer: Generating music with long-term structure.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Huang et~al.(2020)Huang, Perez, Ba, and Volkovs]{huang2020improving}
Xiao~Shi Huang, Felipe Perez, Jimmy Ba, and Maksims Volkovs.
\newblock Improving transformer optimization through better initialization.
\newblock In \emph{International Conference on Machine Learning}, pages
  4475--4483. PMLR, 2020.

\bibitem[Isserlis(1918)]{isserlis1918formula}
Leon Isserlis.
\newblock On a formula for the product-moment coefficient of any order of a
  normal frequency distribution in any number of variables.
\newblock \emph{Biometrika}, 12\penalty0 (1/2):\penalty0 134--139, 1918.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Liu et~al.(2019)Liu, Jiang, He, Chen, Liu, Gao, and Han]{liuadam}
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng
  Gao, and Jiawei Han.
\newblock On the variance of the adaptive learning rate and beyond, 2019.
\newblock URL \url{https://arxiv.org/abs/1908.03265}.

\bibitem[Liu et~al.(2020)Liu, Liu, Gao, Chen, and Han]{liu2020understanding}
Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han.
\newblock Understanding the difficulty of training transformers.
\newblock \emph{arXiv preprint arXiv:2004.08249}, 2020.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Magnus and Neudecker(2019)]{magnus2019matrix}
Jan~R Magnus and Heinz Neudecker.
\newblock \emph{Matrix differential calculus with applications in statistics
  and econometrics}.
\newblock John Wiley \& Sons, 2019.

\bibitem[Nachum et~al.(2021)Nachum, H{\k{a}}z{\l}a, Gastpar, and
  Khina]{nachum2021johnson}
Ido Nachum, Jan H{\k{a}}z{\l}a, Michael Gastpar, and Anatoly Khina.
\newblock A johnson--lindenstrauss framework for randomly initialized cnns.
\newblock \emph{arXiv preprint arXiv:2111.02155}, 2021.

\bibitem[Nguyen et~al.(2010)Nguyen, Wainwright, and
  Jordan]{nguyen2010estimating}
XuanLong Nguyen, Martin~J Wainwright, and Michael~I Jordan.
\newblock Estimating divergence functionals and the likelihood ratio by convex
  risk minimization.
\newblock \emph{IEEE Transactions on Information Theory}, 56\penalty0
  (11):\penalty0 5847--5861, 2010.

\bibitem[Noci et~al.(2021)Noci, Bachmann, Roth, Nowozin, and
  Hofmann]{noci2021precise}
Lorenzo Noci, Gregor Bachmann, Kevin Roth, Sebastian Nowozin, and Thomas
  Hofmann.
\newblock Precise characterization of the prior predictive distribution of deep
  relu networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Orvieto et~al.(2021)Orvieto, Kohler, Pavllo, Hofmann, and
  Lucchi]{orvieto2021vanishing}
Antonio Orvieto, Jonas Kohler, Dario Pavllo, Thomas Hofmann, and Aurelien
  Lucchi.
\newblock Vanishing curvature and the power of adaptive methods in randomly
  initialized deep networks.
\newblock \emph{AISTATS 2022 (to appear)}, 2021.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli]{ott2019fairseq}
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
  David Grangier, and Michael Auli.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock In \emph{Proceedings of NAACL-HLT 2019: Demonstrations}, 2019.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu]{papineni2002bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In \emph{Proceedings of the 40th annual meeting of the Association
  for Computational Linguistics}, pages 311--318, 2002.

\bibitem[Pennington et~al.(2017)Pennington, Schoenholz, and
  Ganguli]{NIPS2017_d9fc0cdb}
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli.
\newblock Resurrecting the sigmoid in deep learning through dynamical isometry:
  theory and practice.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/file/d9fc0cdb67638d50f411432d0d41d0ba-Paper.pdf}.

\bibitem[Polu et~al.(2022)Polu, Han, Zheng, Baksys, Babuschkin, and
  Sutskever]{polu2022formal}
Stanislas Polu, Jesse~Michael Han, Kunhao Zheng, Mantas Baksys, Igor
  Babuschkin, and Ilya Sutskever.
\newblock Formal mathematics statement curriculum learning.
\newblock \emph{arXiv preprint arXiv:2202.01344}, 2022.

\bibitem[Poole et~al.(2016)Poole, Lahiri, Raghu, Sohl-Dickstein, and
  Ganguli]{poole2016exponential}
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya
  Ganguli.
\newblock Exponential expressivity in deep neural networks through transient
  chaos.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}, 2019.

\bibitem[Saxe et~al.(2013)Saxe, McClelland, and Ganguli]{saxe2013exact}
Andrew~M Saxe, James~L McClelland, and Surya Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6120}, 2013.

\bibitem[Schoenholz et~al.(2016)Schoenholz, Gilmer, Ganguli, and
  Sohl-Dickstein]{schoenholz2016deep}
Samuel~S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein.
\newblock Deep information propagation.
\newblock \emph{arXiv preprint arXiv:1611.01232}, 2016.

\bibitem[Schoenholz et~al.(2017)Schoenholz, Gilmer, Ganguli, and
  Sohl-Dickstein]{schoenholz2017deepinformationpropagation}
Samuel~S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein.
\newblock Deep information propagation.
\newblock In \emph{ICLR}, 2017.

\bibitem[Shekhovtsov and Flach(2018)]{shekhovtsov2018feed}
Alexander Shekhovtsov and Boris Flach.
\newblock Feed-forward propagation in probabilistic neural networks with
  categorical and max layers.
\newblock In \emph{International conference on learning representations}, 2018.

\bibitem[Singh et~al.(2021)Singh, Bachmann, and Hofmann]{singh2021analytic}
Sidak~Pal Singh, Gregor Bachmann, and Thomas Hofmann.
\newblock Analytic insights into structure and rank of neural network hessian
  maps.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan,
  editors, \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=otDgw7LM7Nn}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{arXiv preprint arXiv:1706.03762}, 2017.

\bibitem[Wang et~al.(2022)Wang, Ma, Dong, Huang, Zhang, and
  Wei]{wang2022deepnet}
Hongyu Wang, Shuming Ma, Li~Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.
\newblock Deepnet: Scaling transformers to 1,000 layers.
\newblock \emph{arXiv preprint arXiv:2203.00555}, 2022.

\bibitem[Wang et~al.(2019)Wang, Li, Xiao, Zhu, Li, Wong, and
  Chao]{wang2019learning}
Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek~F Wong, and
  Lidia~S Chao.
\newblock Learning deep transformer models for machine translation.
\newblock \emph{arXiv preprint arXiv:1906.01787}, 2019.

\bibitem[Xiao et~al.(2018)Xiao, Bahri, Sohl-Dickstein, Schoenholz, and
  Pennington]{pmlr-v80-xiao18a}
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and
  Jeffrey Pennington.
\newblock Dynamical isometry and a mean field theory of {CNN}s: How to train
  10,000-layer vanilla convolutional neural networks.
\newblock In Jennifer Dy and Andreas Krause, editors, \emph{Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pages 5393--5402. PMLR,
  10--15 Jul 2018.
\newblock URL \url{https://proceedings.mlr.press/v80/xiao18a.html}.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan,
  Wang, and Liu]{xiong2020layer}
Ruibin Xiong, Yunchang Yang, Di~He, Kai Zheng, Shuxin Zheng, Chen Xing,
  Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu.
\newblock On layer normalization in the transformer architecture.
\newblock In \emph{International Conference on Machine Learning}, pages
  10524--10533. PMLR, 2020.

\bibitem[Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le]{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ~R Salakhutdinov,
  and Quoc~V Le.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Zavatone-Veth and Pehlevan(2021)]{zavatone2021exact}
Jacob Zavatone-Veth and Cengiz Pehlevan.
\newblock Exact marginal prior distributions of finite bayesian neural
  networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Zhang et~al.(2019)Zhang, Dauphin, and Ma]{zhang2019fixup}
Hongyi Zhang, Yann~N Dauphin, and Tengyu Ma.
\newblock Fixup initialization: Residual learning without normalization.
\newblock \emph{arXiv preprint arXiv:1901.09321}, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Karimireddy, Veit, Kim, Reddi, Kumar, and
  Sra]{zhang2020adaptive}
Jingzhao Zhang, Sai~Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank
  Reddi, Sanjiv Kumar, and Suvrit Sra.
\newblock Why are adaptive methods good for attention models?
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15383--15393, 2020.

\end{thebibliography}
