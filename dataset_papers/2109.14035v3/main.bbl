\begin{thebibliography}{10}

\bibitem{abolfathi2021coachnet}
Elmira~Amirloo Abolfathi, Jun Luo, Peyman Yadmellat, and Kasra Rezaee.
\newblock {CoachNet}: An adversarial sampling approach for reinforcement
  learning, 2021.

\bibitem{aljundi2018memory}
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and
  Tinne Tuytelaars.
\newblock Memory aware synapses: Learning what (not) to forget.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 139--154, 2018.

\bibitem{beaulieu2020learning}
Shawn Beaulieu, Lapo Frati, Thomas Miconi, Joel Lehman, Kenneth~O Stanley, Jeff
  Clune, and Nick Cheney.
\newblock Learning to continually learn.
\newblock {\em arXiv preprint arXiv:2002.09571}, 2020.

\bibitem{bellman2015adaptive}
Richard~E Bellman.
\newblock {\em Adaptive control processes: a guided tour}.
\newblock Princeton university press, 2015.

\bibitem{caccia2021online}
Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin,
  Lucas Caccia, Issam Laradji, Irina Rish, Alexandre Lacoste, David Vazquez,
  and Laurent Charlin.
\newblock Online fast adaptation and knebowledge accumulation: a new approach
  to continual learning, 2021.

\bibitem{Caccia2020OnlineFA}
Massimo Caccia, Pau Rodr{\'i}guez, Oleksiy Ostapenko, Fabrice Normandin, Min
  Lin, Lucas Caccia, Issam~H. Laradji, Irina Rish, Alexande Lacoste, David
  V{\'a}zquez, and Laurent Charlin.
\newblock Online fast adaptation and knowledge accumulation: {A} new approach
  to continual learning.
\newblock {\em ArXiv}, abs/2003.05856, 2020.

\bibitem{carpenter1987massively}
Gail~A Carpenter and Stephen Grossberg.
\newblock A massively parallel architecture for a self-organizing neural
  pattern recognition machine.
\newblock {\em Computer vision, graphics, and image processing}, 37(1):54--115,
  1987.

\bibitem{chaudhry2020continual}
Arslan Chaudhry, Naeemullah Khan, Puneet~K Dokania, and Philip~HS Torr.
\newblock Continual learning in low-rank orthogonal subspaces.
\newblock {\em arXiv preprint arXiv:2010.11635}, 2020.

\bibitem{chaudhry2019continual}
Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan,
  Puneet~K Dokania, Philip~HS Torr, and Marc'Aurelio Ranzato.
\newblock Continual learning with tiny episodic memories.
\newblock {\em arXiv preprint arXiv:1902.10486}, 2019.

\bibitem{chaudhry2019tiny}
Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan,
  Puneet~K Dokania, Philip~HS Torr, and Marc'Aurelio Ranzato.
\newblock On tiny episodic memories in continual learning.
\newblock {\em arXiv preprint arXiv:1902.10486}, 2019.

\bibitem{doan2021theoretical}
Thang Doan, Mehdi~Abbana Bennani, Bogdan Mazoure, Guillaume Rabusseau, and
  Pierre Alquier.
\newblock A theoretical analysis of catastrophic forgetting through the {NTK}
  overlap matrix.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1072--1080. PMLR, 2021.

\bibitem{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em Journal of Machine Learning Research}, 12(7), 2011.

\bibitem{ebrahimi2020adversarial}
Sayna Ebrahimi, Franziska Meier, Roberto Calandra, Trevor Darrell, and Marcus
  Rohrbach.
\newblock Adversarial continual learning, 2020.

\bibitem{farajtabar2019orthogonal}
Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li.
\newblock Orthogonal gradient descent for continual learning, 2019.

\bibitem{fini2020online}
Enrico Fini, St{\'e}phane Lathuili{\`e}re, Enver Sangineto, Moin Nabi, and
  Elisa Ricci.
\newblock Online continual learning under extreme memory constraints.
\newblock In {\em European Conference on Computer Vision}, pages 720--735.
  Springer, 2020.

\bibitem{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1126--1135. JMLR. org, 2017.

\bibitem{finn2019online}
Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine.
\newblock Online meta-learning.
\newblock {\em arXiv preprint arXiv:1902.08438}, 2019.

\bibitem{gupta2020lamaml}
Gunshi Gupta, Karmesh Yadav, and Liam Paull.
\newblock {La-MAML}: Look-ahead meta learning for continual learning, 2020.

\bibitem{Hsu18_EvalCL}
Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira.
\newblock Re-evaluating continual learning scenarios: A categorization and case
  for strong baselines.
\newblock In {\em NeurIPS Continual learning Workshop}, 2018.

\bibitem{javed2019meta}
Khurram Javed and Martha White.
\newblock Meta-learning representations for continual learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1818--1828, 2019.

\bibitem{joseph2020metaconsolidation}
K~J Joseph and Vineeth~N Balasubramanian.
\newblock Meta-consolidation for continual learning, 2020.

\bibitem{jung2020continual}
Sangwon Jung, Hongjoon Ahn, Sungmin Cha, and Taesup Moon.
\newblock Continual learning with node-importance based adaptive group sparse
  regularization.
\newblock {\em arXiv e-prints}, pages arXiv--2003, 2020.

\bibitem{ke2020continual}
Zixuan Ke, Bing Liu, and Xingchang Huang.
\newblock Continual learning of a mixed sequence of similar and dissimilar
  tasks.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock {\em Proceedings of the National Academy of Sciences},
  114(13):3521--3526, 2017.

\bibitem{knoblauch2020optimal}
Jeremias Knoblauch, Hisham Husain, and Tom Diethe.
\newblock Optimal continual learning has perfect memory and is {NP}-hard.
\newblock In {\em International Conference on Machine Learning}, pages
  5327--5337. PMLR, 2020.

\bibitem{lewis2012optimal}
Frank~L Lewis, Draguna Vrabie, and Vassilis~L Syrmos.
\newblock {\em Optimal control}.
\newblock John Wiley \& Sons, 2012.

\bibitem{LwF}
Zhizhong Li and Derek Hoiem.
\newblock Learning without forgetting.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  40(12):2935--2947, 2017.

\bibitem{lin1992self}
Long-Ji Lin.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock {\em Machine Learning}, 8(3-4):293--321, 1992.

\bibitem{lopez2017gradient}
David Lopez-Paz and Marc'Aurelio Ranzato.
\newblock Gradient episodic memory for continual learning.
\newblock In {\em Advances in neural information processing systems}, pages
  6467--6476, 2017.

\bibitem{GEM}
David Lopez-Paz and Marc'Aurelio Ranzato.
\newblock Gradient episodic memory for continual learning.
\newblock {\em arXiv preprint arXiv:1706.08840}, 2017.

\bibitem{mirzadeh2020understanding}
Seyed~Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pascanu, and Hassan
  Ghasemzadeh.
\newblock Understanding the role of training regimes in continual learning.
\newblock {\em arXiv preprint arXiv:2006.06958}, 2020.

\bibitem{nagabandi2019deep}
Anusha Nagabandi, Chelsea Finn, and Sergey Levine.
\newblock Deep online learning via meta-learning: Continual adaptation for
  model-based {RL}, 2019.

\bibitem{nichol2018firstorder}
Alex Nichol, Joshua Achiam, and John Schulman.
\newblock On first-order meta-learning algorithms, 2018.

\bibitem{pan2020continual}
Pingbo Pan, Siddharth Swaroop, Alexander Immer, Runa Eschenhagen, Richard~E
  Turner, and Mohammad~Emtiyaz Khan.
\newblock Continual deep learning by functional regularisation of memorable
  past.
\newblock {\em arXiv preprint arXiv:2004.14070}, 2020.

\bibitem{riemer2018learning}
Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu,
  and Gerald Tesauro.
\newblock Learning to learn without forgetting by maximizing transfer and
  minimizing interference.
\newblock {\em arXiv preprint arXiv:1810.11910}, 2018.

\bibitem{rusu2016progressive}
Andrei~A Rusu, Neil~C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
  Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
\newblock Progressive neural networks.
\newblock {\em arXiv preprint arXiv:1606.04671}, 2016.

\bibitem{schwarz2018progress}
Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka
  Grabska-Barwinska, Yee~Whye Teh, Razvan Pascanu, and Raia Hadsell.
\newblock Progress \& compress: A scalable framework for continual learning.
\newblock In {\em International Conference on Machine Learning}, pages
  4528--4537. PMLR, 2018.

\bibitem{DGR}
Hanul Shin, Jung~Kwon Lee, Jaehong Kim, and Jiwon Kim.
\newblock Continual learning with deep generative replay.
\newblock {\em arXiv preprint arXiv:1705.08690}, 2017.

\bibitem{snell2017prototypical}
Jake Snell, Kevin Swersky, and Richard Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock In {\em Advances in neural information processing systems}, pages
  4077--4087, 2017.

\bibitem{titsias2019functional}
Michalis~K Titsias, Jonathan Schwarz, Alexander G de~G Matthews, Razvan
  Pascanu, and Yee~Whye Teh.
\newblock Functional regularisation for continual learning with {G}aussian
  processes.
\newblock {\em arXiv preprint arXiv:1901.11356}, 2019.

\bibitem{vandeven2019generative}
Gido~M. van~de Ven and Andreas~S. Tolias.
\newblock Generative replay with feedback connections as a general strategy for
  continual learning, 2019.

\bibitem{vinyals2016matching}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et~al.
\newblock Matching networks for one shot learning.
\newblock {\em Advances in Neural Information Processing Systems},
  29:3630--3638, 2016.

\bibitem{DBLP:journals/corr/abs-1806-06928}
Risto Vuorio, Dong{-}Yeon Cho, Daejoong Kim, and Jiwon Kim.
\newblock Meta continual learning.
\newblock {\em CoRR}, abs/1806.06928, 2018.

\bibitem{yao2020don}
Huaxiu Yao, Longkai Huang, Ying Wei, Li~Tian, Junzhou Huang, and Zhenhui Li.
\newblock Don't overlook the support set: Towards improving generalization in
  meta-learning.
\newblock {\em arXiv preprint arXiv:2007.13040}, 2020.

\bibitem{yin2020optimization}
Dong Yin, Mehrdad Farajtabar, Ang Li, Nir Levine, and Alex Mott.
\newblock Optimization and generalization of regularization-based continual
  learning: a loss approximation viewpoint, 2020.

\bibitem{yoon2017lifelong}
Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung~Ju Hwang.
\newblock Lifelong learning with dynamically expandable networks.
\newblock {\em arXiv preprint arXiv:1708.01547}, 2017.

\bibitem{zenke2017continual}
Friedemann Zenke, Ben Poole, and Surya Ganguli.
\newblock Continual learning through synaptic intelligence.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 3987--3995. JMLR. org, 2017.

\end{thebibliography}
