\begin{thebibliography}{75}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[apt(2019)]{aptos2019}
Aptos 2019 blindness detection.
\newblock \url{https://www.kaggle.com/c/aptos2019-blindness-detection/}, 2019.
\newblock Accessed: 10 June 2020.

\bibitem[Akiba et~al.(2019)Akiba, Sano, Yanase, Ohta, and Koyama]{akiba2019}
Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama.
\newblock Optuna: A next-generation hyperparameter optimization framework.
\newblock \emph{arXiv preprint arXiv:1907.10902}, 2019.

\bibitem[Ando and Huang(2017)]{ando2017deep}
Shin Ando and Chun~Yuan Huang.
\newblock Deep over-sampling framework for classifying imbalanced data.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 770--785. Springer, 2017.

\bibitem[Bansal et~al.(2021)Bansal, Goldblum, Cherepanova, Schwarzschild,
  Bruss, and Goldstein]{bansal2021metabalance}
Arpit Bansal, Micah Goldblum, Valeriia Cherepanova, Avi Schwarzschild, C~Bayan
  Bruss, and Tom Goldstein.
\newblock Metabalance: High-performance neural networks for class-imbalanced
  data.
\newblock \emph{arXiv preprint arXiv:2106.09643}, 2021.

\bibitem[Bardes et~al.(2021)Bardes, Ponce, and LeCun]{bardes2021vicreg}
Adrien Bardes, Jean Ponce, and Yann LeCun.
\newblock Vicreg: Variance-invariance-covariance regularization for
  self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2105.04906}, 2021.

\bibitem[Bertsimas et~al.(2018)Bertsimas, Gupta, and Kallus]{bertsimas2018data}
Dimitris Bertsimas, Vishal Gupta, and Nathan Kallus.
\newblock Data-driven robust optimization.
\newblock \emph{Mathematical Programming}, 167\penalty0 (2):\penalty0 235--292,
  2018.

\bibitem[Blackard and Dean(1999)]{BLACKARD1999131}
Jock~A. Blackard and Denis~J. Dean.
\newblock Comparative accuracies of artificial neural networks and discriminant
  analysis in predicting forest cover types from cartographic variables.
\newblock \emph{Computers and Electronics in Agriculture}, 24\penalty0
  (3):\penalty0 131--151, 1999.
\newblock ISSN 0168-1699.
\newblock \doi{https://doi.org/10.1016/S0168-1699(99)00046-0}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0168169999000460}.

\bibitem[Buda et~al.(2018)Buda, Maki, and Mazurowski]{buda2018systematic}
Mateusz Buda, Atsuto Maki, and Maciej~A Mazurowski.
\newblock A systematic study of the class imbalance problem in convolutional
  neural networks.
\newblock \emph{Neural networks}, 106:\penalty0 249--259, 2018.

\bibitem[Cao et~al.(2019)Cao, Wei, Gaidon, Arechiga, and Ma]{cao2019learning}
Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma.
\newblock Learning imbalanced datasets with label-distribution-aware margin
  loss.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Chawla et~al.(2002)Chawla, Bowyer, Hall, and
  Kegelmeyer]{chawla2002smote}
Nitesh~V Chawla, Kevin~W Bowyer, Lawrence~O Hall, and W~Philip Kegelmeyer.
\newblock Smote: synthetic minority over-sampling technique.
\newblock \emph{Journal of artificial intelligence research}, 16:\penalty0
  321--357, 2002.

\bibitem[Chen and Guestrin(2016)]{chen2016xgboost}
Tianqi Chen and Carlos Guestrin.
\newblock Xgboost: A scalable tree boosting system.
\newblock In \emph{Proceedings of the 22nd acm sigkdd international conference
  on knowledge discovery and data mining}, pages 785--794, 2016.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pages
  1597--1607. PMLR, 2020.

\bibitem[Cubuk et~al.(2018)Cubuk, Zoph, Mane, Vasudevan, and
  Le]{cubuk2018autoaugment}
Ekin~D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V Le.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock \emph{arXiv preprint arXiv:1805.09501}, 2018.

\bibitem[Cui et~al.(2019)Cui, Jia, Lin, Song, and Belongie]{cui2019class}
Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie.
\newblock Class-balanced loss based on effective number of samples.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 9268--9277, 2019.

\bibitem[Darlow et~al.(2018)Darlow, Crowley, Antoniou, and
  Storkey]{darlow2018cinic}
Luke~N Darlow, Elliot~J Crowley, Antreas Antoniou, and Amos~J Storkey.
\newblock Cinic-10 is not imagenet or cifar-10.
\newblock \emph{arXiv preprint arXiv:1810.03505}, 2018.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem[Drummond et~al.(2003)Drummond, Holte, et~al.]{drummond2003c4}
Chris Drummond, Robert~C Holte, et~al.
\newblock C4. 5, class imbalance, and cost sensitivity: why under-sampling
  beats over-sampling.
\newblock In \emph{Workshop on learning from imbalanced datasets II},
  volume~11, pages 1--8. Citeseer, 2003.

\bibitem[Fang et~al.(2023)Fang, Kornblith, and Schmidt]{fang2023does}
Alex Fang, Simon Kornblith, and Ludwig Schmidt.
\newblock Does progress on imagenet transfer to real-world datasets?
\newblock \emph{arXiv preprint arXiv:2301.04644}, 2023.

\bibitem[Foret et~al.(2020)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2020sharpness}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Geiping et~al.(2023)Geiping, Goldblum, Somepalli, Shwartz-Ziv,
  Goldstein, and Wilson]{geiping2022much}
Jonas Geiping, Micah Goldblum, Gowthami Somepalli, Ravid Shwartz-Ziv, Tom
  Goldstein, and Andrew~Gordon Wilson.
\newblock How much data are augmentations worth? an investigation into scaling
  laws, invariance, and implicit regularization.
\newblock \emph{International Conference on Learning Representations}, 2023.

\bibitem[Goh and Sim(2010)]{goh2010distributionally}
Joel Goh and Melvyn Sim.
\newblock Distributionally robust optimization and its tractable
  approximations.
\newblock \emph{Operations research}, 58\penalty0 (4-part-1):\penalty0
  902--917, 2010.

\bibitem[Gorishniy et~al.(2021)Gorishniy, Rubachev, Khrulkov, and
  Babenko]{gorishniy2021revisiting}
Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.
\newblock Revisiting deep learning models for tabular data.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 18932--18943, 2021.

\bibitem[Gorishniy et~al.(2022)Gorishniy, Rubachev, and
  Babenko]{gorishniy2022embeddingsa}
Yury Gorishniy, Ivan Rubachev, and Artem Babenko.
\newblock On embeddings for numerical features in tabular deep learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 24991--25004, 2022.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond,
  Buchatskaya, Doersch, Avila~Pires, Guo, Gheshlaghi~Azar,
  et~al.]{grill2020bootstrap}
Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, Corentin Tallec, Pierre
  Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila~Pires, Zhaohan
  Guo, Mohammad Gheshlaghi~Azar, et~al.
\newblock Bootstrap your own latent-a new approach to self-supervised learning.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 21271--21284, 2020.

\bibitem[Guo and Viktor(2004)]{imbalncedboosting2004}
Hongyu Guo and Herna~L. Viktor.
\newblock Learning from imbalanced data sets with boosting and data generation:
  The databoost-im approach.
\newblock \emph{SIGKDD Explor. Newsl.}, 6\penalty0 (1):\penalty0 30–39, jun
  2004.
\newblock ISSN 1931-0145.
\newblock \doi{10.1145/1007730.1007736}.
\newblock URL \url{https://doi.org/10.1145/1007730.1007736}.

\bibitem[Ha et~al.(2020)Ha, Liu, and Liu]{ha2020identifying}
Qishen Ha, Bo~Liu, and Fuxu Liu.
\newblock Identifying melanoma images using efficientnet ensemble: Winning
  solution to the siim-isic melanoma classification challenge.
\newblock \emph{arXiv preprint arXiv:2010.05351}, 2020.

\bibitem[Han et~al.(2005)Han, Wang, and Mao]{han2005borderline}
Hui Han, Wen-Yuan Wang, and Bing-Huan Mao.
\newblock Borderline-smote: a new over-sampling method in imbalanced data sets
  learning.
\newblock In \emph{International conference on intelligent computing}, pages
  878--887. Springer, 2005.

\bibitem[He and Garcia(2009)]{he2009learning}
Haibo He and Edwardo~A Garcia.
\newblock Learning from imbalanced data.
\newblock \emph{IEEE Transactions on knowledge and data engineering},
  21\penalty0 (9):\penalty0 1263--1284, 2009.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Helber et~al.(2019)Helber, Bischke, Dengel, and
  Borth]{helber2019eurosat}
Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth.
\newblock Eurosat: A novel dataset and deep learning benchmark for land use and
  land cover classification.
\newblock \emph{IEEE Journal of Selected Topics in Applied Earth Observations
  and Remote Sensing}, 2019.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Mu, Cubuk, Zoph, Gilmer, and
  Lakshminarayanan]{hendrycks2019augmix}
Dan Hendrycks, Norman Mu, Ekin~D Cubuk, Barret Zoph, Justin Gilmer, and Balaji
  Lakshminarayanan.
\newblock Augmix: A simple data processing method to improve robustness and
  uncertainty.
\newblock \emph{arXiv preprint arXiv:1912.02781}, 2019.

\bibitem[Hu et~al.(2020)Hu, Jiang, Tang, Chen, Miao, and Zhang]{hu2020learning}
Xinting Hu, Yi~Jiang, Kaihua Tang, Jingyuan Chen, Chunyan Miao, and Hanwang
  Zhang.
\newblock Learning to segment the tail.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 14045--14054, 2020.

\bibitem[Huang et~al.(2016)Huang, Li, Loy, and Tang]{Huang_2016_CVPR}
Chen Huang, Yining Li, Chen~Change Loy, and Xiaoou Tang.
\newblock Learning deep representation for imbalanced classification.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, June 2016.

\bibitem[Huang et~al.(2019{\natexlab{a}})Huang, Li, Loy, and
  Tang]{huang2019deep}
Chen Huang, Yining Li, Chen~Change Loy, and Xiaoou Tang.
\newblock Deep imbalanced learning for face recognition and attribute
  prediction.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 42\penalty0 (11):\penalty0 2781--2794, 2019{\natexlab{a}}.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4700--4708, 2017.

\bibitem[Huang et~al.(2019{\natexlab{b}})Huang, Emam, Goldblum, Fowl, Terry,
  Huang, and Goldstein]{huang2019understanding}
W~Ronny Huang, Zeyad Emam, Micah Goldblum, Liam Fowl, Justin~K Terry, Furong
  Huang, and Tom Goldstein.
\newblock Understanding generalization through visualizations.
\newblock \emph{arXiv preprint arXiv:1906.03291}, 2019{\natexlab{b}}.

\bibitem[{Kaggle}({2015})]{otto_data}
{Kaggle}.
\newblock {Otto Group Product Classification Challenge}, {2015}.
\newblock URL
  \url{{https://www.kaggle.com/c/otto-group-product-classification-challenge/data}}.

\bibitem[Kang et~al.(2019)Kang, Xie, Rohrbach, Yan, Gordo, Feng, and
  Kalantidis]{kang2019decoupling}
Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi
  Feng, and Yannis Kalantidis.
\newblock Decoupling representation and classifier for long-tailed recognition.
\newblock \emph{arXiv preprint arXiv:1910.09217}, 2019.

\bibitem[Kang et~al.(2020)Kang, Xie, Rohrbach, Yan, Gordo, Feng, and
  Kalantidis]{Kang2020Decoupling}
Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi
  Feng, and Yannis Kalantidis.
\newblock Decoupling representation and classifier for long-tailed recognition.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=r1gRTCVFvB}.

\bibitem[Kenton and Toutanova(2019)]{kenton2019bert}
Jacob Devlin Ming-Wei~Chang Kenton and Lee~Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of NAACL-HLT}, pages 4171--4186, 2019.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kim et~al.(2020)Kim, Jeong, and Shin]{kim2020m2m}
Jaehyung Kim, Jongheon Jeong, and Jinwoo Shin.
\newblock M2m: Imbalanced classification via major-to-minor translation.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 13896--13905, 2020.

\bibitem[Kohavi and Sahami(1996)]{kohavi96}
Ron Kohavi and Mehran Sahami.
\newblock Error-based and entropy-based discretization of continuous features.
\newblock In \emph{Proceedings of the Second International Conference on
  Knowledge Discovery and Data Mining}, KDD'96, page 114–119. AAAI Press,
  1996.

\bibitem[Kotar et~al.(2021)Kotar, Ilharco, Schmidt, Ehsani, and
  Mottaghi]{kotar2021contrasting}
Klemen Kotar, Gabriel Ilharco, Ludwig Schmidt, Kiana Ehsani, and Roozbeh
  Mottaghi.
\newblock Contrasting contrastive self-supervised representation learning
  pipelines.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9949--9959, 2021.

\bibitem[Kothapalli et~al.(2022)Kothapalli, Rasromani, and
  Awatramani]{kothapalli2022neural}
Vignesh Kothapalli, Ebrahim Rasromani, and Vasudev Awatramani.
\newblock Neural collapse: A review on modelling principles and generalization.
\newblock \emph{arXiv preprint arXiv:2206.04041}, 2022.

\bibitem[Krizhevsky(2009)]{krizhevsky_learning_2009}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Le and Yang(2015)]{Le2015TinyIV}
Ya~Le and Xuan~S. Yang.
\newblock Tiny imagenet visual recognition challenge.
\newblock 2015.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:16664790}.

\bibitem[LeCun(1998)]{lecun1998mnist}
Yann LeCun.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[Levin et~al.(2022)Levin, Cherepanova, Schwarzschild, Bansal, Bruss,
  Goldstein, Wilson, and Goldblum]{levin2022transfer}
Roman Levin, Valeriia Cherepanova, Avi Schwarzschild, Arpit Bansal, C~Bayan
  Bruss, Tom Goldstein, Andrew~Gordon Wilson, and Micah Goldblum.
\newblock Transfer learning with deep tabular models.
\newblock \emph{arXiv preprint arXiv:2206.15306}, 2022.

\bibitem[Lin et~al.(2017)Lin, Goyal, Girshick, He, and
  Doll{\'a}r]{lin2017focal}
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll{\'a}r.
\newblock Focal loss for dense object detection.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 2980--2988, 2017.

\bibitem[Liu et~al.(2021)Liu, HaoChen, Gaidon, and Ma]{liu2021self}
Hong Liu, Jeff~Z HaoChen, Adrien Gaidon, and Tengyu Ma.
\newblock Self-supervised learning is more robust to dataset imbalance.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Hu, Lin, Yao, Xie, Wei, Ning, Cao,
  Zhang, Dong, et~al.]{liu2022swin}
Ze~Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue
  Cao, Zheng Zhang, Li~Dong, et~al.
\newblock Swin transformer v2: Scaling up capacity and resolution.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 12009--12019, 2022{\natexlab{a}}.

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Mao, Wu, Feichtenhofer, Darrell,
  and Xie]{liu2022convnet}
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,
  and Saining Xie.
\newblock A convnet for the 2020s.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 11976--11986, 2022{\natexlab{b}}.

\bibitem[Liu et~al.(2019)Liu, Miao, Zhan, Wang, Gong, and Yu]{liu2019large}
Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella~X
  Yu.
\newblock Large-scale long-tailed recognition in an open world.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 2537--2546, 2019.

\bibitem[Lotfi et~al.(2022)Lotfi, Finzi, Kapoor, Potapczynski, Goldblum, and
  Wilson]{lotfi2022pac}
Sanae Lotfi, Marc Finzi, Sanyam Kapoor, Andres Potapczynski, Micah Goldblum,
  and Andrew~G Wilson.
\newblock Pac-bayes compression bounds so tight that they can explain
  generalization.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 31459--31473, 2022.

\bibitem[McElfresh et~al.(2023)McElfresh, Khandagale, Valverde, Ramakrishnan,
  Goldblum, White, et~al.]{mcelfresh2023neural}
Duncan McElfresh, Sujay Khandagale, Jonathan Valverde, Ganesh Ramakrishnan,
  Micah Goldblum, Colin White, et~al.
\newblock When do neural nets outperform boosted trees on tabular data?
\newblock \emph{arXiv preprint arXiv:2305.02997}, 2023.

\bibitem[M{\"u}ller et~al.(2019)M{\"u}ller, Kornblith, and
  Hinton]{muller2019does}
Rafael M{\"u}ller, Simon Kornblith, and Geoffrey~E Hinton.
\newblock When does label smoothing help?
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[M{\"u}ller and Hutter(2021)]{muller2021trivialaugment}
Samuel~G M{\"u}ller and Frank Hutter.
\newblock Trivialaugment: Tuning-free yet state-of-the-art data augmentation.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 774--782, 2021.

\bibitem[Ouyang et~al.(2016)Ouyang, Wang, Zhang, and Yang]{ouyang2016factors}
Wanli Ouyang, Xiaogang Wang, Cong Zhang, and Xiaokang Yang.
\newblock Factors in finetuning deep model for object detection with long-tail
  distribution.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 864--873, 2016.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{papyan2020prevalence}
Vardan Papyan, XY~Han, and David~L Donoho.
\newblock Prevalence of neural collapse during the terminal phase of deep
  learning training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (40):\penalty0 24652--24663, 2020.

\bibitem[Ryou et~al.(2019)Ryou, Jeong, and Perona]{ryou2019anchor}
Serim Ryou, Seong-Gyun Jeong, and Pietro Perona.
\newblock Anchor loss: Modulating loss scale based on prediction difficulty.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 5992--6001, 2019.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{sandler2018mobilenetv2}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4510--4520, 2018.

\bibitem[Shwartz-Ziv and Armon(2022)]{shwartz2022tabular}
Ravid Shwartz-Ziv and Amitai Armon.
\newblock Tabular data: Deep learning is not all you need.
\newblock \emph{Information Fusion}, 81:\penalty0 84--90, 2022.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Somepalli et~al.(2021)Somepalli, Goldblum, Schwarzschild, Bruss, and
  Goldstein]{somepalli2021saint}
Gowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C~Bayan Bruss, and Tom
  Goldstein.
\newblock Saint: Improved neural networks for tabular data via row attention
  and contrastive pre-training.
\newblock \emph{arXiv preprint arXiv:2106.01342}, 2021.

\bibitem[Somepalli et~al.(2022)Somepalli, Fowl, Bansal, Yeh-Chiang, Dar,
  Baraniuk, Goldblum, and Goldstein]{somepalli2022can}
Gowthami Somepalli, Liam Fowl, Arpit Bansal, Ping Yeh-Chiang, Yehuda Dar,
  Richard Baraniuk, Micah Goldblum, and Tom Goldstein.
\newblock Can neural nets learn the same model twice? investigating
  reproducibility and double descent from the decision boundary perspective.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 13699--13708, 2022.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{szegedy2015going}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
  Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
\newblock Going deeper with convolutions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1--9, 2015.

\bibitem[Tan and Le(2019)]{tan2019efficientnet}
Mingxing Tan and Quoc Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In \emph{International conference on machine learning}, pages
  6105--6114. PMLR, 2019.

\bibitem[Wightman et~al.(2021)Wightman, Touvron, and Jegou]{wightman2021resnet}
Ross Wightman, Hugo Touvron, and Herve Jegou.
\newblock Resnet strikes back: An improved training procedure in timm.
\newblock In \emph{NeurIPS 2021 Workshop on ImageNet: Past, Present, and
  Future}, 2021.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'a}r, Tu, and
  He]{xie2017aggregated}
Saining Xie, Ross Girshick, Piotr Doll{\'a}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1492--1500, 2017.

\bibitem[Yang and Xu(2020)]{yang2020rethinking}
Yuzhe Yang and Zhi Xu.
\newblock Rethinking the value of labels for improving class-imbalanced
  learning.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 19290--19301, 2020.

\bibitem[Yun et~al.(2019)Yun, Han, Oh, Chun, Choe, and Yoo]{yun2019cutmix}
Sangdoo Yun, Dongyoon Han, Seong~Joon Oh, Sanghyuk Chun, Junsuk Choe, and
  Youngjoon Yoo.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 6023--6032, 2019.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017.

\bibitem[Zhong et~al.(2021)Zhong, Cui, Liu, and Jia]{zhong2021improving}
Zhisheng Zhong, Jiequan Cui, Shu Liu, and Jiaya Jia.
\newblock Improving calibration for long-tailed recognition.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 16489--16498, 2021.

\end{thebibliography}
