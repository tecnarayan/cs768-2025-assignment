\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrei(2008)]{Andrei08anunconstrained}
Andrei, N.
\newblock An unconstrained optimization test functions collection.
\newblock \emph{Advanced Modeling and Optimization}, 10\penalty0 (1):\penalty0
  147--161, 2008.

\bibitem[Aydore et~al.(2019)Aydore, Zhu, and Foster]{NEURIPS2019_50a074e6}
Aydore, S., Zhu, T., and Foster, D.~P.
\newblock Dynamic local regret for non-convex online forecasting.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  7982--7991, 2019.

\bibitem[Blake \& Zisserman(1987)Blake and Zisserman]{blake1987visual}
Blake, A. and Zisserman, A.
\newblock \emph{Visual reconstruction}.
\newblock MIT press, 1987.

\bibitem[Brox \& Malik(2010)Brox and Malik]{brox2010large}
Brox, T. and Malik, J.
\newblock Large displacement optical flow: descriptor matching in variational
  motion estimation.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 33\penalty0 (3):\penalty0 500--513, 2010.

\bibitem[C.~Zach(2018)]{Zach2018}
C.~Zach, G.~B.
\newblock Descending, lifting or smoothing: Secrets of robust cost
  optimization.
\newblock In \emph{Proc. ECCV}, volume~12, pp.\  558--574, 2018.

\bibitem[Carlini \& Wagner(2017)Carlini and Wagner]{carlini2017towards}
Carlini, N. and Wagner, D.
\newblock Towards evaluating the robustness of neural networks.
\newblock In \emph{2017 IEEE Symposium on Security and Privacy}, pp.\  39--57,
  2017.

\bibitem[Chen \& Harker(1993)Chen and Harker]{chen1993non}
Chen, B. and Harker, P.~T.
\newblock A non-interior-point continuation method for linear complementarity
  problems.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 14\penalty0
  (4):\penalty0 1168--1190, 1993.

\bibitem[Chen(2012)]{chen2012smoothing}
Chen, X.
\newblock Smoothing methods for nonsmooth, nonconvex minimization.
\newblock \emph{Mathematical programming}, 134\penalty0 (1):\penalty0 71--99,
  2012.

\bibitem[Chen et~al.(2019)Chen, Liu, Xu, Li, Lin, Hong, and Cox]{chen2019zo}
Chen, X., Liu, S., Xu, K., Li, X., Lin, X., Hong, M., and Cox, D.
\newblock Zo-adamm: Zeroth-order adaptive momentum method for black-box
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  7204--7215, 2019.

\bibitem[Cutkosky \& Orabona(2019)Cutkosky and Orabona]{NEURIPS2019_b8002139}
Cutkosky, A. and Orabona, F.
\newblock Momentum-based variance reduction in non-convex sgd.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  15236--15245. Curran Associates, Inc., 2019.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{Dinh2017sharp}
Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, pp.\  1019--1028. PMLR, 2017.

\bibitem[Evans(2010)]{evans2010partial}
Evans, L.
\newblock \emph{Partial Differential Equations}.
\newblock American Mathematical Society, 2010.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013stochastic}
Ghadimi, S. and Lan, G.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Hazan et~al.(2016)Hazan, Levy, and Shalev-Shwartz]{hazan2016graduated}
Hazan, E., Levy, K.~Y., and Shalev-Shwartz, S.
\newblock On graduated optimization for stochastic non-convex problems.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning}, pp.\  1833--1841, 2016.

\bibitem[Hemeda(2012)]{hemeda2012homotopy}
Hemeda, A.~A.
\newblock Homotopy perturbation method for solving systems of nonlinear coupled
  equations.
\newblock \emph{Applied Mathematical Sciences}, 6\penalty0 (93-96):\penalty0
  4787--4800, 2012.

\bibitem[Jain \& Kar(2017)Jain and Kar]{jain2017nonconv}
Jain, P. and Kar, P.
\newblock Non-convex optimization for machine learning.
\newblock \emph{Foundations and Trends in Machine Learning}, 10\penalty0
  (3–4):\penalty0 142–336, 2017.

\bibitem[Jin et~al.(2018)Jin, Liu, Ge, and Jordan]{jin2018local}
Jin, C., Liu, L.~T., Ge, R., and Jordan, M.~I.
\newblock On the local minima of the empirical risk.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pp.\  4901--4910, 2018.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{3rd International Conference on Learning Representations},
  2015.

\bibitem[Li et~al.(2017)Li, Xu, Taylor, Studer, and
  Goldstein]{li2017visualizing}
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6389--6399, 2017.

\bibitem[Liu et~al.(2018)Liu, Kailkhura, Chen, Ting, Chang, and
  Amini]{liu2018zeroth}
Liu, S., Kailkhura, B., Chen, P.~Y., Ting, P.~S., Chang, S.~Y., and Amini, L.
\newblock Zeroth-order stochastic variance reduction for nonconvex
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3727–3737, 2018.

\bibitem[Mertikopoulos et~al.(2020)Mertikopoulos, Hallak, Kavis, and
  Cevher]{NEURIPS2020_0cb5ebb1}
Mertikopoulos, P., Hallak, N., Kavis, A., and Cevher, V.
\newblock On the almost sure convergence of stochastic gradient descent in
  non-convex problems.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1117--1128. Curran Associates, Inc., 2020.

\bibitem[Mobahi(2016)]{mobahi2016closed}
Mobahi, H.
\newblock Closed form for some gaussian convolutions.
\newblock \emph{arXiv preprint arXiv:1602.05610}, 2016.

\bibitem[Mobahi \& Fisher~III(2015{\natexlab{a}})Mobahi and
  Fisher~III]{mobahi2015link}
Mobahi, H. and Fisher~III, J.~W.
\newblock On the link between gaussian homotopy continuation and convex
  envelopes.
\newblock In \emph{Energy Minimization Methods in Computer Vision and Pattern
  Recognition}, pp.\  43--56, 2015{\natexlab{a}}.

\bibitem[Mobahi \& Fisher~III(2015{\natexlab{b}})Mobahi and
  Fisher~III]{mobahi2015theoretical}
Mobahi, H. and Fisher~III, J.~W.
\newblock A theoretical analysis of optimization by gaussian continuation.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  29\penalty0 (1), 2015{\natexlab{b}}.

\bibitem[Mobahi \& Ma(2012)Mobahi and Ma]{mobahi2012gaussian}
Mobahi, H. and Ma, Y.
\newblock Gaussian smoothing and asymptotic convexity.
\newblock Technical report, Coordinated Science Laboratory, University of
  Illinois at Urbana-Champaign, 2012.

\bibitem[Molga \& Smutnicki(2005)Molga and Smutnicki]{molga2005test}
Molga, M. and Smutnicki, C.
\newblock Test functions for optimization needs.
\newblock 2005.
\newblock URL
  \url{https://robertmarks.org/Classes/ENGR5358/Papers/functions.pdf}.

\bibitem[Nesterov(2004)]{nesterov2004intro}
Nesterov, Y.
\newblock \emph{Introductory Lectures on Convex Optimization: a basic course}.
\newblock Kluwer Academic Publishers, 2004.

\bibitem[Nesterov \& Spokoiny(2017)Nesterov and Spokoiny]{nesterov2017random}
Nesterov, Y. and Spokoiny, V.
\newblock Random gradient-free minimization of convex functions.
\newblock \emph{Foundations of Computational Mathematics}, 17\penalty0
  (2):\penalty0 527--566, 2017.

\bibitem[Nielsen(1993)]{Nielsen1993}
Nielsen, M.
\newblock Graduated non-convexity by smoothness focusing.
\newblock In \emph{Proceedings of the British Machine Vision Conference}, pp.\
  60.1–--60.10. BMVA Press, 1993.

\bibitem[Shao et~al.(2019)Shao, Gei{\ss}ler, and Sivrikaya]{shao2019graduated}
Shao, W.~J., Gei{\ss}ler, C., and Sivrikaya, F.
\newblock Graduated optimization of black-box functions.
\newblock \emph{arXiv preprint arXiv:1906.01279}, 2019.

\bibitem[Sokolov et~al.(2016)Sokolov, Kreutzer, Riezler, and
  Lo]{sokolov2016stochastic}
Sokolov, A., Kreutzer, J., Riezler, S., and Lo, C.
\newblock Stochastic structured prediction under bandit feedback.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1489--1497, 2016.

\bibitem[Stein(1972)]{stein1972bound}
Stein, C.
\newblock A bound for the error in the normal approximation to the distribution
  of a sum of dependent random variables.
\newblock In \emph{Proceedings of the Sixth Berkeley Symposium on Mathematical
  Statistics and Probability, Volume 2: Probability Theory}. The Regents of the
  University of California, 1972.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{sutskever2013importance}
Sutskever, I., Martens, J., Dahl, G., and Hinton, G.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning}, pp.\  1139--1147. PMLR, 2013.

\bibitem[Widder(1976)]{widder1976heat}
Widder, D.~V.
\newblock \emph{The heat equation}.
\newblock Academic Press, 1976.

\bibitem[Wu(1996)]{wu1996effective}
Wu, Z.
\newblock The effective energy transformation scheme as a special continuation
  approach to global optimization with application to molecular conformation.
\newblock \emph{SIAM Journal on Optimization}, 6\penalty0 (3):\penalty0
  748--768, 1996.

\bibitem[Xu et~al.(2020)Xu, Joshi, Singh, and Dubrawski]{xu2020zeroth}
Xu, Y.~C., Joshi, A., Singh, A., and Dubrawski, A.
\newblock Zeroth order non-convex optimization with dueling-choice bandits.
\newblock In \emph{Proceedings of the 36th Conference on Uncertainty in
  Artificial Intelligence}, pp.\  899--908. PMLR, 2020.

\end{thebibliography}
