\begin{thebibliography}{10}

\bibitem{baxes1994digital}
Gregory~A Baxes.
\newblock {\em Digital image processing: principles and applications}.
\newblock John Wiley \& Sons, Inc., 1994.

\bibitem{beyer2020imagenet_real}
Lucas Beyer, Olivier~J H{\'e}naff, Alexander Kolesnikov, Xiaohua Zhai, and
  A{\"a}ron van~den Oord.
\newblock Are we done with imagenet?
\newblock {\em arXiv preprint arXiv:2006.07159}, 2020.

\bibitem{carion2020detr}
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
  Kirillov, and Sergey Zagoruyko.
\newblock End-to-end object detection with transformers.
\newblock In {\em ECCV}, pages 213--229. Springer, 2020.

\bibitem{TransTracking}
Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, and Huchuan Lu.
\newblock Transformer tracking.
\newblock In {\em CVPR}, 2021.

\bibitem{cheng2021maskformer}
Bowen Cheng, Alexander~G. Schwing, and Alexander Kirillov.
\newblock Per-pixel classification is not all you need for semantic
  segmentation.
\newblock {\em NeurIPS}, 2021.

\bibitem{chi2020fast}
Lu~Chi, Borui Jiang, and Yadong Mu.
\newblock Fast fourier convolution.
\newblock {\em NeurIPS}, 33, 2020.

\bibitem{cooley1965algorithmfft}
James~W Cooley and John~W Tukey.
\newblock An algorithm for the machine calculation of complex fourier series.
\newblock {\em Mathematics of computation}, 19(90):297--301, 1965.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em CVPR}, pages 248--255, 2009.

\bibitem{ding2017circnn}
Caiwen Ding, Siyu Liao, Yanzhi Wang, Zhe Li, Ning Liu, Youwei Zhuo, Chao Wang,
  Xuehai Qian, Yu~Bai, Geng Yuan, et~al.
\newblock Circnn: accelerating and compressing deep neural networks using
  block-circulant weight matrices.
\newblock In {\em MICRO}, pages 395--408, 2017.

\bibitem{dosovitskiy2020vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{gong2021improve}
Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, and Qiang Liu.
\newblock Improve vision transformers training by suppressing over-smoothing.
\newblock {\em arXiv preprint arXiv:2104.12753}, 2021.

\bibitem{goodfellow2014explaining_fgsm}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock {\em arXiv preprint arXiv:1412.6572}, 2014.

\bibitem{he2017mask}
Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick.
\newblock Mask r-cnn.
\newblock In {\em ICCV}, 2017.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, pages 770--778, 2016.

\bibitem{hendrycks2019benchmarking_imagenetc}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock {\em arXiv preprint arXiv:1903.12261}, 2019.

\bibitem{hendrycks2021natural_imageneta}
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.
\newblock Natural adversarial examples.
\newblock In {\em CVPR}, pages 15262--15271, 2021.

\bibitem{repeataug}
Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel
  Soudry.
\newblock Augment your batch: Improving generalization through instance
  repetition.
\newblock In {\em CVPR}, pages 8129--8138, 2020.

\bibitem{stochasticdepth}
Gao Huang, Yu~Sun, Zhuang Liu, Daniel Sedra, and Kilian~Q Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In {\em ECCV}, pages 646--661, 2016.

\bibitem{jiang2021token}
Zihang Jiang, Qibin Hou, Li~Yuan, Daquan Zhou, Xiaojie Jin, Anran Wang, and
  Jiashi Feng.
\newblock Token labeling: Training a 85.5\% top-1 accuracy vision transformer
  with 56m parameters on imagenet.
\newblock {\em arXiv preprint arXiv:2104.10858}, 2021.

\bibitem{kirillov2019panoptic}
Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Doll{\'a}r.
\newblock Panoptic feature pyramid networks.
\newblock In {\em CVPR}, pages 6399--6408, 2019.

\bibitem{cars}
Jonathan Krause, Michael Stark, Jia Deng, and Li~Fei-Fei.
\newblock 3d object representations for fine-grained categorization.
\newblock In {\em ICCVW}, pages 554--561, 2013.

\bibitem{cifar}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{krizhevsky2012alex}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock {\em NeurIPS}, 25:1097--1105, 2012.

\bibitem{lee2018single}
Jae-Han Lee, Minhyeok Heo, Kyung-Rae Kim, and Chang-Su Kim.
\newblock Single-image depth estimation based on fourier domain analysis.
\newblock In {\em CVPR}, pages 330--339, 2018.

\bibitem{lee2021fnet}
James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon.
\newblock Fnet: Mixing tokens with fourier transforms.
\newblock {\em arXiv preprint arXiv:2105.03824}, 2021.

\bibitem{li2020falcon}
Shaohua Li, Kaiping Xue, Bin Zhu, Chenkai Ding, Xindi Gao, David Wei, and Tao
  Wan.
\newblock Falcon: A fourier transform based approach for fast and secure
  convolutional neural network predictions.
\newblock In {\em CVPR}, pages 8705--8714, 2020.

\bibitem{li2020fourier}
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik
  Bhattacharya, Andrew Stuart, and Anima Anandkumar.
\newblock Fourier neural operator for parametric partial differential
  equations.
\newblock {\em ICLR}, 2021.

\bibitem{liu2021pay}
Hanxiao Liu, Zihang Dai, David~R So, and Quoc~V Le.
\newblock Pay attention to mlps.
\newblock {\em arXiv preprint arXiv:2105.08050}, 2021.

\bibitem{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock {\em arXiv preprint arXiv:2103.14030}, 2021.

\bibitem{adamw}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{madry2017towards_pgd}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock {\em arXiv preprint arXiv:1706.06083}, 2017.

\bibitem{mao2021rethinking}
Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Shaokai Ye, Yuan He, and Hui
  Xue.
\newblock Rethinking the design principles of robust vision transformer.
\newblock {\em arXiv preprint arXiv:2105.07926}, 2021.

\bibitem{flower}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In {\em ICVGIP}, pages 722--729, 2008.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em NeurIPS}, 2019.

\bibitem{pitas2000digital}
Ioannis Pitas.
\newblock {\em Digital image processing algorithms and applications}.
\newblock John Wiley \& Sons, 2000.

\bibitem{ema}
Boris~T Polyak and Anatoli~B Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock {\em SIAM journal on control and optimization}, 30(4):838--855, 1992.

\bibitem{regnet}
Ilija Radosavovic, Raj~Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr
  Doll{\'a}r.
\newblock Designing network design spaces.
\newblock In {\em CVPR}, 2020.

\bibitem{rao2021dynamicvit}
Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh.
\newblock Dynamicvit: Efficient vision transformers with dynamic token
  sparsification.
\newblock In {\em NeurIPS}, 2021.

\bibitem{recht2019imagenet_v2}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In {\em ICML}, pages 5389--5400. PMLR, 2019.

\bibitem{tan2019efficientnet}
Mingxing Tan and Quoc Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In {\em ICML}, pages 6105--6114. PMLR, 2019.

\bibitem{tolstikhin2021mlp}
Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai,
  Thomas Unterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario
  Lucic, et~al.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock {\em arXiv preprint arXiv:2105.01601}, 2021.

\bibitem{touvron2021resmlp}
Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin
  El-Nouby, Edouard Grave, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and
  Herv{\'e} J{\'e}gou.
\newblock Resmlp: Feedforward networks for image classification with
  data-efficient training.
\newblock {\em arXiv preprint arXiv:2105.03404}, 2021.

\bibitem{touvron2020deit}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock {\em arXiv preprint arXiv:2012.12877}, 2020.

\bibitem{touvron2021cait}
Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and
  Herv{\'e} J{\'e}gou.
\newblock Going deeper with image transformers.
\newblock {\em arXiv preprint arXiv:2103.17239}, 2021.

\bibitem{touvron2021going}
Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and
  Herv{\'e} J{\'e}gou.
\newblock Going deeper with image transformers.
\newblock {\em arXiv preprint arXiv:2103.17239}, 2021.

\bibitem{transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em NeurIPS}, pages 5998--6008, 2017.

\bibitem{wang2021pyramid}
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong
  Lu, Ping Luo, and Ling Shao.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions, 2021.

\bibitem{wu2021cvt}
Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu~Yuan, and Lei
  Zhang.
\newblock Cvt: Introducing convolutions to vision transformers.
\newblock {\em arXiv preprint arXiv:2103.15808}, 2021.

\bibitem{xie2017aggregated}
Saining Xie, Ross Girshick, Piotr Doll{\'a}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In {\em CVPR}, pages 1492--1500, 2017.

\bibitem{yang2020fda}
Yanchao Yang and Stefano Soatto.
\newblock Fda: Fourier domain adaptation for semantic segmentation.
\newblock In {\em CVPR}, pages 4085--4095, 2020.

\bibitem{yu2021pointr}
Xumin Yu, Yongming Rao, Ziyi Wang, Zuyan Liu, Jiwen Lu, and Jie Zhou.
\newblock Pointr: Diverse point cloud completion with geometry-aware
  transformers.
\newblock In {\em ICCV}, 2021.

\bibitem{yuan2021t2t}
Li~Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis~EH
  Tay, Jiashi Feng, and Shuicheng Yan.
\newblock Tokens-to-token vit: Training vision transformers from scratch on
  imagenet.
\newblock {\em arXiv preprint arXiv:2101.11986}, 2021.

\bibitem{setr}
Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang,
  Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip~HS Torr, et~al.
\newblock Rethinking semantic segmentation from a sequence-to-sequence
  perspective with transformers.
\newblock {\em arXiv preprint arXiv:2012.15840}, 2020.

\bibitem{zhong2020randomerasing}
Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi~Yang.
\newblock Random erasing data augmentation.
\newblock In {\em AAAI}, volume~34, pages 13001--13008, 2020.

\bibitem{zhou2017scene}
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio
  Torralba.
\newblock Scene parsing through ade20k dataset.
\newblock In {\em CVPR}, pages 633--641, 2017.

\end{thebibliography}
