\begin{thebibliography}{19}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bauschke \& Combettes(2017)Bauschke and Combettes]{bau17}
Bauschke, H.~H. and Combettes, P.~L.
\newblock \emph{Convex Analysis and Monotone Operator Theory in Hilbert
  Spaces}.
\newblock Springer, New York, 2nd edition, 2017.

\bibitem[Bauschke et~al.(2011)Bauschke, Burachik, Combettes, Elser, Luke, and
  Wolkowicz]{bau11a}
Bauschke, H.~H., Burachik, R.~S., Combettes, P.~L., Elser, V., Luke, D.~R., and
  Wolkowicz, H. (eds.).
\newblock \emph{Fixed-Point Algorithms for Inverse Problems in Science and
  Engineering}.
\newblock Springer, 2011.

\bibitem[Chraibi et~al.(2019)Chraibi, Khaled, Kovalev, Richt{\'a}rik, Salim,
  and Tak\'{a}\v{c}]{Chraibi2019DistributedFP}
Chraibi, S., Khaled, A., Kovalev, D., Richt{\'a}rik, P., Salim, A., and
  Tak\'{a}\v{c}, M.
\newblock Distributed fixed point methods with compressed iterates.
\newblock \emph{preprint ArXiv:1912.09925}, 2019.

\bibitem[Combettes \& Woodstock(2020)Combettes and Woodstock]{com20}
Combettes, P.~L. and Woodstock, Z.~C.
\newblock A fixed point framework for recovering signals from nonlinear
  transformations.
\newblock preprint arXiv:2003.01260, 2020.

\bibitem[Combettes \& Yamada(2015)Combettes and Yamada]{comy15}
Combettes, P.~L. and Yamada, I.
\newblock Compositions and convex combinations of averaged nonexpansive
  operators.
\newblock \emph{Journal of Mathematical Analysis and Applications},
  425\penalty0 (1):\penalty0 55--70, 2015.

\bibitem[Davis \& Yin(2016)Davis and Yin]{dav16}
Davis, D. and Yin, W.
\newblock Convergence rate analysis of several splitting schemes.
\newblock In Glowinski, R., Osher, S.~J., and Yin, W. (eds.), \emph{Splitting
  Methods in Communication, Imaging, Science, and Engineering}, pp.\  115--163,
  Cham, 2016. Springer International Publishing.

\bibitem[Haddadpour \& Mahdavi(2019)Haddadpour and
  Mahdavi]{haddadpour2019convergence}
Haddadpour, F. and Mahdavi, M.
\newblock On the convergence of local descent methods in federated learning.
\newblock \emph{preprint arXiv:1910.14425}, 2019.

\bibitem[Khaled \& Richt\'{a}rik(2019)Khaled and Richt\'{a}rik]{GDCI}
Khaled, A. and Richt\'{a}rik, P.
\newblock Gradient descent with compressed iterates.
\newblock In \emph{NeurIPS Workshop on Federated Learning for Data Privacy and
  Confidentiality}, 2019.

\bibitem[Khaled et~al.(2019)Khaled, Mishchenko, and Richt\'{a}rik]{localGD}
Khaled, A., Mishchenko, K., and Richt\'{a}rik, P.
\newblock First analysis of local {GD} on heterogeneous data.
\newblock In \emph{NeurIPS Workshop on Federated Learning for Data Privacy and
  Confidentiality}, 2019.

\bibitem[Khaled et~al.(2020)Khaled, Mishchenko, and
  Richt\'{a}rik]{localSGD-AISTATS2020}
Khaled, A., Mishchenko, K., and Richt\'{a}rik, P.
\newblock Tighter theory for local {SGD} on identical and heterogeneous data.
\newblock In \emph{The 23rd International Conference on Artificial Intelligence
  and Statistics (AISTATS 2020)}, 2020.

\bibitem[Kone\v{c}n\'{y} et~al.(2016)Kone\v{c}n\'{y}, McMahan, Yu,
  Richt\'{a}rik, Suresh, and Bacon]{ja2016}
Kone\v{c}n\'{y}, J., McMahan, H.~B., Yu, F.~X., Richt\'{a}rik, P., Suresh,
  A.~T., and Bacon, D.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock In \emph{NIPS Workshop on Private Multi-Party Machine Learning},
  2016.

\bibitem[Lessard et~al.(2016)Lessard, Recht, and Packards]{les16}
Lessard, L., Recht, B., and Packards, A.
\newblock Analysis and design of optimization algorithms via integral quadratic
  constraints.
\newblock \emph{SIAM J. Optim.}, 26\penalty0 (1):\penalty0 57--95, 2016.

\bibitem[Ma et~al.(2017)Ma, Kone{\v{c}}n{\'y}, Jaggi, Smith, Jordan,
  Richt{\'a}rik, and Tak{\'a}{\v{c}}]{ma2017distributed}
Ma, C., Kone{\v{c}}n{\'y}, J., Jaggi, M., Smith, V., Jordan, M.~I.,
  Richt{\'a}rik, P., and Tak{\'a}{\v{c}}, M.
\newblock Distributed optimization with arbitrary local solvers.
\newblock \emph{Optimization Methods and Software}, 32\penalty0 (4):\penalty0
  813--848, 2017.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and {Ag\"{u}era y
  Arcas}]{FL2017-AISTATS}
McMahan, H.~B., Moore, E., Ramage, D., Hampson, S., and {Ag\"{u}era y Arcas},
  B.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, 2017.

\bibitem[Nesterov(2004)]{NesterovBook}
Nesterov, Y.
\newblock \emph{Introductory lectures on convex optimization: a basic course}.
\newblock Kluwer Academic Publishers, 2004.

\bibitem[Pesquet \& Repetti(2015)Pesquet and Repetti]{pes15}
Pesquet, J.-C. and Repetti, A.
\newblock A class of randomized primal-dual algorithms for distributed
  optimization.
\newblock \emph{J. Nonlinear Convex Anal.}, 12\penalty0 (16), December 2015.

\bibitem[Richt{\'a}rik \& Tak{\'a}{\v{c}}(2014)Richt{\'a}rik and
  Tak{\'a}{\v{c}}]{ric14}
Richt{\'a}rik, P. and Tak{\'a}{\v{c}}, M.
\newblock Iteration complexity of randomized block-coordinate descent methods
  for minimizing a composite function.
\newblock \emph{Math. Program.}, 144\penalty0 (1--2):\penalty0 1--38, April
  2014.

\bibitem[Stich(2019)]{Stich2018}
Stich, S.~U.
\newblock {Local SGD Converges Fast and Communicates Little}.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Yu(2013)]{yu13}
Yu, Y.-L.
\newblock On decomposing the proximal map.
\newblock In \emph{Proc. of 26th Int. Conf. Neural Information Processing
  Systems (NIPS)}, pp.\  91--99, 2013.

\end{thebibliography}
