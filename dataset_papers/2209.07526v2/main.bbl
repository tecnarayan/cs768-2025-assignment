\begin{thebibliography}{10}

\bibitem{agrawal2019nocaps}
H.~Agrawal, K.~Desai, Y.~Wang, X.~Chen, R.~Jain, M.~Johnson, D.~Batra,
  D.~Parikh, S.~Lee, and P.~Anderson.
\newblock Nocaps: Novel object captioning at scale.
\newblock In {\em ICCV}, 2019.

\bibitem{akbari2021vatt}
H.~Akbari, L.~Yuan, R.~Qian, W.-H. Chuang, S.-F. Chang, Y.~Cui, and B.~Gong.
\newblock Vatt: Transformers for multimodal self-supervised learning from raw
  video, audio and text.
\newblock In {\em NeurIPS}, 2021.

\bibitem{alayrac2022flamingo}
J.-B. Alayrac, J.~Donahue, P.~Luc, A.~Miech, I.~Barr, Y.~Hasson, K.~Lenc,
  A.~Mensch, K.~Millican, M.~Reynolds, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock In {\em NeurIPS}, 2022.

\bibitem{alayrac2020self}
J.-B. Alayrac, A.~Recasens, R.~Schneider, R.~Arandjelovi{\'c}, J.~Ramapuram,
  J.~De~Fauw, L.~Smaira, S.~Dieleman, and A.~Zisserman.
\newblock Self-supervised multimodal versatile networks.
\newblock In {\em NeurIPS}, 2020.

\bibitem{anne2017localizing}
L.~Anne~Hendricks, O.~Wang, E.~Shechtman, J.~Sivic, T.~Darrell, and B.~Russell.
\newblock Localizing moments in video with natural language.
\newblock In {\em ICCV}, 2017.

\bibitem{bain2021frozen}
M.~Bain, A.~Nagrani, G.~Varol, and A.~Zisserman.
\newblock Frozen in time: A joint video and image encoder for end-to-end
  retrieval.
\newblock In {\em ICCV}, 2021.

\bibitem{bao2021beit}
H.~Bao, L.~Dong, and F.~Wei.
\newblock Beit: Bert pre-training of image transformers.
\newblock In {\em ICLR}, 2022.

\bibitem{gberta_2021_ICML}
G.~Bertasius, H.~Wang, and L.~Torresani.
\newblock Is space-time attention all you need for video understanding?
\newblock In {\em ICML}, 2021.

\bibitem{bogolin2022cross}
S.-V. Bogolin, I.~Croitoru, H.~Jin, Y.~Liu, and S.~Albanie.
\newblock Cross modal retrieval with querybank normalisation.
\newblock In {\em CVPR}, 2022.

\bibitem{bommasani2021opportunities}
R.~Bommasani, D.~A. Hudson, E.~Adeli, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock {\em ArXiv}, 2021.

\bibitem{changpinyo2021conceptual}
S.~Changpinyo, P.~Sharma, N.~Ding, and R.~Soricut.
\newblock Conceptual 12m: Pushing web-scale image-text pre-training to
  recognize long-tail visual concepts.
\newblock In {\em CVPR}, 2021.

\bibitem{chen2020simple}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In {\em ICML}, 2020.

\bibitem{chen2020uniter}
Y.-C. Chen, L.~Li, L.~Yu, A.~E. Kholy, F.~Ahmed, Z.~Gan, Y.~Cheng, and J.~Liu.
\newblock Uniter: Universal image-text representation learning.
\newblock In {\em ECCV}, 2020.

\bibitem{croitoru2021teachtext}
I.~Croitoru, S.-V. Bogolin, M.~Leordeanu, H.~Jin, A.~Zisserman, S.~Albanie, and
  Y.~Liu.
\newblock Teachtext: Crossmodal generalized distillation for text-video
  retrieval.
\newblock In {\em ICCV}, 2021.

\bibitem{cubuk2020randaugment}
E.~D. Cubuk, B.~Zoph, J.~Shlens, and Q.~V. Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In {\em CVPRW}, 2020.

\bibitem{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em CVPR}, 2009.

\bibitem{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em NAACL}, 2019.

\bibitem{dong2022cswin}
X.~Dong, J.~Bao, D.~Chen, W.~Zhang, N.~Yu, L.~Yuan, D.~Chen, and B.~Guo.
\newblock Cswin transformer: A general vision transformer backbone with
  cross-shaped windows.
\newblock In {\em CVPR}, 2022.

\bibitem{dong2021peco}
X.~Dong, J.~Bao, T.~Zhang, D.~Chen, W.~Zhang, L.~Yuan, D.~Chen, F.~Wen, and
  N.~Yu.
\newblock Peco: Perceptual codebook for bert pre-training of vision
  transformers.
\newblock {\em arXiv preprint arXiv:2111.12710}, 2021.

\bibitem{dong2022bootstrapped}
X.~Dong, J.~Bao, T.~Zhang, D.~Chen, W.~Zhang, L.~Yuan, D.~Chen, F.~Wen, and
  N.~Yu.
\newblock Bootstrapped masked autoencoders for vision bert pretraining.
\newblock In {\em ECCV}, 2022.

\bibitem{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit,
  and N.~Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em ICLR}, 2021.

\bibitem{dou2021empirical}
Z.-Y. Dou, Y.~Xu, Z.~Gan, J.~Wang, S.~Wang, L.~Wang, C.~Zhu, Z.~Liu, M.~Zeng,
  et~al.
\newblock An empirical study of training end-to-end vision-and-language
  transformers.
\newblock {\em arXiv preprint arXiv:2111.02387}, 2021.

\bibitem{fu2021violet}
T.-J. Fu, L.~Li, Z.~Gan, K.~Lin, W.~Y. Wang, L.~Wang, and Z.~Liu.
\newblock Violet: End-to-end video-language transformers with masked
  visual-token modeling.
\newblock {\em arXiv preprint arXiv:2111.12681}, 2021.

\bibitem{goyal2017something}
R.~Goyal, S.~Ebrahimi~Kahou, V.~Michalski, J.~Materzynska, S.~Westphal, H.~Kim,
  V.~Haenel, I.~Fruend, P.~Yianilos, M.~Mueller-Freitag, et~al.
\newblock The" something something" video database for learning and evaluating
  visual common sense.
\newblock In {\em ICCV}, 2017.

\bibitem{he2020momentum}
K.~He, H.~Fan, Y.~Wu, S.~Xie, and R.~Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In {\em CVPR}, 2020.

\bibitem{he2019rethinking}
K.~He, R.~Girshick, and P.~Doll{\'a}r.
\newblock Rethinking imagenet pre-training.
\newblock In {\em ICCV}, 2019.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, 2016.

\bibitem{hessel2019case}
J.~Hessel, B.~Pang, Z.~Zhu, and R.~Soricut.
\newblock A case study on combining asr and visual features for generating
  instructional video captions.
\newblock {\em arXiv preprint arXiv:1910.02930}, 2019.

\bibitem{hu2021scaling}
X.~Hu, Z.~Gan, J.~Wang, Z.~Yang, Z.~Liu, Y.~Lu, and L.~Wang.
\newblock Scaling up vision-language pre-training for image captioning.
\newblock {\em arXiv preprint arXiv:2111.12233}, 2021.

\bibitem{jia2021scaling}
C.~Jia, Y.~Yang, Y.~Xia, Y.-T. Chen, Z.~Parekh, H.~Pham, Q.~Le, Y.-H. Sung,
  Z.~Li, and T.~Duerig.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In {\em ICML}, 2021.

\bibitem{jia2021exploring}
M.~Jia, Z.~Wu, A.~Reiter, C.~Cardie, S.~Belongie, and S.-N. Lim.
\newblock Exploring visual engagement signals for representation learning.
\newblock In {\em ICCV}, 2021.

\bibitem{kay2017kinetics}
W.~Kay, J.~Carreira, K.~Simonyan, B.~Zhang, C.~Hillier, S.~Vijayanarasimhan,
  F.~Viola, T.~Green, T.~Back, P.~Natsev, et~al.
\newblock The kinetics human action video dataset.
\newblock {\em arXiv preprint arXiv:1705.06950}, 2017.

\bibitem{kolesnikov2020big}
A.~Kolesnikov, L.~Beyer, X.~Zhai, J.~Puigcerver, J.~Yung, S.~Gelly, and
  N.~Houlsby.
\newblock Big transfer (bit): General visual representation learning.
\newblock In {\em ECCV}, 2020.

\bibitem{krishna2017visual}
R.~Krishna, Y.~Zhu, O.~Groth, J.~Johnson, K.~Hata, J.~Kravitz, S.~Chen,
  Y.~Kalantidis, L.-J. Li, D.~A. Shamma, et~al.
\newblock Visual genome: Connecting language and vision using crowdsourced
  dense image annotations.
\newblock {\em IJCV}, 2017.

\bibitem{lei2021less}
J.~Lei, L.~Li, L.~Zhou, Z.~Gan, T.~L. Berg, M.~Bansal, and J.~Liu.
\newblock Less is more: Clipbert for video-and-language learning via sparse
  sampling.
\newblock In {\em CVPR}, 2021.

\bibitem{li2021prompt}
D.~Li, J.~Li, H.~Li, J.~C. Niebles, and S.~C. Hoi.
\newblock Align and prompt: Video-and-language pre-training with entity
  prompts.
\newblock In {\em CVPR}, 2022.

\bibitem{li2022blip}
J.~Li, D.~Li, C.~Xiong, and S.~Hoi.
\newblock Blip: Bootstrapping language-image pre-training for unified
  vision-language understanding and generation.
\newblock In {\em ICML}, 2022.

\bibitem{li2021align}
J.~Li, R.~Selvaraju, A.~Gotmare, S.~Joty, C.~Xiong, and S.~C.~H. Hoi.
\newblock Align before fuse: Vision and language representation learning with
  momentum distillation.
\newblock In {\em NeurIPS}, 2021.

\bibitem{li2020hero}
L.~Li, Y.-C. Chen, Y.~Cheng, Z.~Gan, L.~Yu, and J.~Liu.
\newblock Hero: Hierarchical encoder for video+ language omni-representation
  pre-training.
\newblock In {\em EMNLP}, 2020.

\bibitem{li2021improve}
S.~Li, D.~Chen, Y.~Chen, L.~Yuan, L.~Zhang, Q.~Chu, B.~Liu, and N.~Yu.
\newblock Improve unsupervised pretraining for few-label transfer.
\newblock In {\em ICCV}, 2021.

\bibitem{li2020unimo}
W.~Li, C.~Gao, G.~Niu, X.~Xiao, H.~Liu, J.~Liu, H.~Wu, and H.~Wang.
\newblock Unimo: Towards unified-modal understanding and generation via
  cross-modal contrastive learning.
\newblock {\em arXiv preprint arXiv:2012.15409}, 2020.

\bibitem{li2020oscar}
X.~Li, X.~Yin, C.~Li, P.~Zhang, X.~Hu, L.~Zhang, L.~Wang, H.~Hu, L.~Dong,
  F.~Wei, et~al.
\newblock Oscar: Object-semantics aligned pre-training for vision-language
  tasks.
\newblock In {\em ECCV}, 2020.

\bibitem{lin2014microsoft}
T.-Y. Lin, M.~Maire, S.~Belongie, J.~Hays, P.~Perona, D.~Ramanan,
  P.~Doll{\'a}r, and C.~L. Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em ECCV}, 2014.

\bibitem{loshchilov2017decoupled}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{lu2019vilbert}
J.~Lu, D.~Batra, D.~Parikh, and S.~Lee.
\newblock Vilbert: Pretraining task-agnostic visiolinguistic representations
  for vision-and-language tasks.
\newblock In {\em NeurIPS}, 2019.

\bibitem{lu2021pretrained}
K.~Lu, A.~Grover, P.~Abbeel, and I.~Mordatch.
\newblock Pretrained transformers as universal computation engines.
\newblock {\em arXiv preprint arXiv:2103.05247}, 2021.

\bibitem{luo2020univl}
H.~Luo, L.~Ji, B.~Shi, H.~Huang, N.~Duan, T.~Li, J.~Li, T.~Bharti, and M.~Zhou.
\newblock Univl: A unified video and language pre-training model for multimodal
  understanding and generation.
\newblock {\em arXiv preprint arXiv:2002.06353}, 2020.

\bibitem{miech2021thinking}
A.~Miech, J.-B. Alayrac, I.~Laptev, J.~Sivic, and A.~Zisserman.
\newblock Thinking fast and slow: Efficient text-to-visual retrieval with
  transformers.
\newblock In {\em CVPR}, 2021.

\bibitem{miech2020end}
A.~Miech, J.-B. Alayrac, L.~Smaira, I.~Laptev, J.~Sivic, and A.~Zisserman.
\newblock End-to-end learning of visual representations from uncurated
  instructional videos.
\newblock In {\em CVPR}, 2020.

\bibitem{ordonez2011im2text}
V.~Ordonez, G.~Kulkarni, and T.~Berg.
\newblock Im2text: Describing images using 1 million captioned photographs.
\newblock In {\em NeurIPS}, 2011.

\bibitem{patrick2020support}
M.~Patrick, P.-Y. Huang, Y.~Asano, F.~Metze, A.~Hauptmann, J.~Henriques, and
  A.~Vedaldi.
\newblock Support-set bottlenecks for video-text representation learning.
\newblock {\em arXiv preprint arXiv:2010.02824}, 2020.

\bibitem{plummer2015flickr30k}
B.~A. Plummer, L.~Wang, C.~M. Cervantes, J.~C. Caicedo, J.~Hockenmaier, and
  S.~Lazebnik.
\newblock Flickr30k entities: Collecting region-to-phrase correspondences for
  richer image-to-sentence models.
\newblock In {\em ICCV}, 2015.

\bibitem{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark, et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em ICML}, 2021.

\bibitem{sharma2018conceptual}
P.~Sharma, N.~Ding, S.~Goodman, and R.~Soricut.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset
  for automatic image captioning.
\newblock In {\em ACL}, 2018.

\bibitem{singh2021flava}
A.~Singh, R.~Hu, V.~Goswami, G.~Couairon, W.~Galuba, M.~Rohrbach, and D.~Kiela.
\newblock Flava: A foundational language and vision alignment model.
\newblock In {\em CVPR}, 2022.

\bibitem{Su2020VL-BERT}
W.~Su, X.~Zhu, Y.~Cao, B.~Li, L.~Lu, F.~Wei, and J.~Dai.
\newblock Vl-bert: Pre-training of generic visual-linguistic representations.
\newblock In {\em ICLR}, 2020.

\bibitem{sun2019videobert}
C.~Sun, A.~Myers, C.~Vondrick, K.~Murphy, and C.~Schmid.
\newblock Videobert: A joint model for video and language representation
  learning.
\newblock In {\em ICCV}, 2019.

\bibitem{tan2019lxmert}
H.~Tan and M.~Bansal.
\newblock Lxmert: Learning cross-modality encoder representations from
  transformers.
\newblock In {\em EMNLP}, 2019.

\bibitem{wang2021actionclip}
M.~Wang, J.~Xing, and Y.~Liu.
\newblock Actionclip: A new paradigm for video action recognition.
\newblock {\em arXiv preprint arXiv:2109.08472}, 2021.

\bibitem{wang2022OFA}
P.~Wang, A.~Yang, R.~Men, J.~Lin, S.~Bai, Z.~Li, J.~Ma, C.~Zhou, J.~Zhou, and
  H.~Yang.
\newblock Unifying architectures, tasks, and modalities through a simple
  sequence-to-sequence learning framework.
\newblock In {\em ICML}, 2022.

\bibitem{wang2022bevt}
R.~Wang, D.~Chen, Z.~Wu, Y.~Chen, X.~Dai, M.~Liu, Y.-G. Jiang, L.~Zhou, and
  L.~Yuan.
\newblock Bevt: Bert pretraining of video transformers.
\newblock In {\em CVPR}, 2022.

\bibitem{wang2021vlmo}
W.~Wang, H.~Bao, L.~Dong, and F.~Wei.
\newblock Vlmo: Unified vision-language pre-training with
  mixture-of-modality-experts.
\newblock {\em arXiv preprint arXiv:2111.02358}, 2021.

\bibitem{wang2022simvlm}
Z.~Wang, J.~Yu, A.~W. Yu, Z.~Dai, Y.~Tsvetkov, and Y.~Cao.
\newblock Sim{VLM}: Simple visual language model pretraining with weak
  supervision.
\newblock In {\em ICLR}, 2022.

\bibitem{xie2018rethinking}
S.~Xie, C.~Sun, J.~Huang, Z.~Tu, and K.~Murphy.
\newblock Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs
  in video classification.
\newblock In {\em ECCV}, 2018.

\bibitem{xu2017video}
D.~Xu, Z.~Zhao, J.~Xiao, F.~Wu, H.~Zhang, X.~He, and Y.~Zhuang.
\newblock Video question answering via gradually refined attention over
  appearance and motion.
\newblock In {\em ACM MM}, 2017.

\bibitem{xu2021videoclip}
H.~Xu, G.~Ghosh, P.-Y. Huang, D.~Okhonko, A.~Aghajanyan, F.~Metze,
  L.~Zettlemoyer, and C.~Feichtenhofer.
\newblock Videoclip: Contrastive pre-training for zero-shot video-text
  understanding.
\newblock {\em arXiv preprint arXiv:2109.14084}, 2021.

\bibitem{xu2016msr}
J.~Xu, T.~Mei, T.~Yao, and Y.~Rui.
\newblock Msr-vtt: A large video description dataset for bridging video and
  language.
\newblock In {\em CVPR}, 2016.

\bibitem{yang2021just}
A.~Yang, A.~Miech, J.~Sivic, I.~Laptev, and C.~Schmid.
\newblock Just ask: Learning to answer questions from millions of narrated
  videos.
\newblock In {\em ICCV}, 2021.

\bibitem{yang2022unified}
J.~Yang, C.~Li, P.~Zhang, B.~Xiao, C.~Liu, L.~Yuan, and J.~Gao.
\newblock Unified contrastive learning in image-text-label space.
\newblock In {\em CVPR}, 2022.

\bibitem{yu2022coca}
J.~Yu, Z.~Wang, V.~Vasudevan, L.~Yeung, M.~Seyedhosseini, and Y.~Wu.
\newblock Coca: Contrastive captioners are image-text foundation models.
\newblock {\em TMLR}, 2022.

\bibitem{yuan2021florence}
L.~Yuan, D.~Chen, Y.-L. Chen, N.~Codella, X.~Dai, J.~Gao, H.~Hu, X.~Huang,
  B.~Li, C.~Li, et~al.
\newblock Florence: A new foundation model for computer vision.
\newblock {\em arXiv preprint arXiv:2111.11432}, 2021.

\bibitem{zellers2021merlot}
R.~Zellers, X.~Lu, J.~Hessel, Y.~Yu, J.~S. Park, J.~Cao, A.~Farhadi, and
  Y.~Choi.
\newblock Merlot: Multimodal neural script knowledge models.
\newblock In {\em NeurIPS}, 2021.

\bibitem{zhang2021cotraining}
B.~Zhang, J.~Yu, C.~Fifty, W.~Han, A.~M. Dai, R.~Pang, and F.~Sha.
\newblock Co-training transformer with videos and images improves action
  recognition.
\newblock {\em arXiv preprint arXiv:2112.07175}, 2021.

\bibitem{vinvl}
P.~Zhang, X.~Li, X.~Hu, J.~Yang, L.~Zhang, L.~Wang, Y.~Choi, and J.~Gao.
\newblock Vinvl: Revisiting visual representations in vision-language models.
\newblock In {\em CVPR}, 2021.

\bibitem{zhou2020unified}
L.~Zhou, H.~Palangi, L.~Zhang, H.~Hu, J.~Corso, and J.~Gao.
\newblock Unified vision-language pre-training for image captioning and vqa.
\newblock In {\em AAAI}, 2020.

\bibitem{zhou2018towards}
L.~Zhou, C.~Xu, and J.~J. Corso.
\newblock Towards automatic learning of procedures from web instructional
  videos.
\newblock In {\em AAAI}, 2018.

\bibitem{zhou2018end}
L.~Zhou, Y.~Zhou, J.~J. Corso, R.~Socher, and C.~Xiong.
\newblock End-to-end dense video captioning with masked transformer.
\newblock In {\em CVPR}, 2018.

\bibitem{zhu2020actbert}
L.~Zhu and Y.~Yang.
\newblock Actbert: Learning global-local video-text representations.
\newblock In {\em CVPR}, 2020.

\bibitem{zhu2021uni}
X.~Zhu, J.~Zhu, H.~Li, X.~Wu, X.~Wang, H.~Li, X.~Wang, and J.~Dai.
\newblock Uni-perceiver: Pre-training unified architecture for generic
  perception for zero-shot and few-shot tasks.
\newblock {\em arXiv preprint arXiv:2112.01522}, 2021.

\bibitem{zhu2020comprehensive}
Y.~Zhu, X.~Li, C.~Liu, M.~Zolfaghari, Y.~Xiong, C.~Wu, Z.~Zhang, J.~Tighe,
  R.~Manmatha, and M.~Li.
\newblock A comprehensive study of deep video action recognition.
\newblock {\em arXiv preprint arXiv:2012.06567}, 2020.

\end{thebibliography}
