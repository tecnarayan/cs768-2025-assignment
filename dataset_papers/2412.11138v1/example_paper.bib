@article{wang2024development,
  title={Development of a humanoid robot control system based on AR-BCI and SLAM navigation},
  author={Wang, Yao and Zhang, Mingxing and Li, Meng and Cui, Hongyan and Chen, Xiaogang},
  journal={Cognitive Neurodynamics},
  pages={1--14},
  year={2024},
  publisher={Springer}
}
@article{he2024personalized,
  title={Personalized robotic control via constrained multi-objective reinforcement learning},
  author={He, Xiangkun and Hu, Zhongxu and Yang, Haohan and Lv, Chen},
  journal={Neurocomputing},
  volume={565},
  pages={126986},
  year={2024},
  publisher={Elsevier}
}
@article{yang2018unified,
  title={A unified approach for multi-step temporal-difference learning with eligibility traces in reinforcement learning},
  author={Yang, Long and Shi, Minhao and Zheng, Qian and Meng, Wenjia and Pan, Gang},
  journal={arXiv preprint arXiv:1802.03171},
  year={2018}
}
@article{shi2019tbq,
  title={TBQ ($\sigma$): Improving Efficiency of Trace Utilization for Off-Policy Reinforcement Learning},
  author={Shi, Longxiang and Li, Shijian and Cao, Longbing and Yang, Long and Pan, Gang},
  journal={arXiv preprint arXiv:1905.07237},
  year={2019}
}
@article{gao_review_2023,
	title = {A {Review} and {Outlook} on {Predictive} {Cruise} {Control} of {Vehicles} and {Typical} {Applications} {Under} {Cloud} {Control} {System}},
	volume = {20},
	issn = {2731-5398},
	url = {https://doi.org/10.1007/s11633-022-1395-3},
	doi = {10.1007/s11633-022-1395-3},
	abstract = {With the application of mobile communication technology in the automotive industry, intelligent connected vehicles equipped with communication and sensing devices have been rapidly promoted. The road and traffic information perceived by intelligent vehicles has important potential application value, especially for improving the energy-saving and safe-driving of vehicles as well as the efficient operation of traffic. Therefore, a type of vehicle control technology called predictive cruise control (PCC) has become a hot research topic. It fully taps the perceived or predicted environmental information to carry out predictive cruise control of vehicles and improves the comprehensive performance of the vehicle-road system. Most existing reviews focus on the economical driving of vehicles, but few scholars have conducted a comprehensive survey of PCC from theory to the status quo. In this paper, the methods and advances of PCC technologies are reviewed comprehensively by investigating the global literature, and typical applications under a cloud control system (CCS) are proposed. Firstly, the methodology of PCC is generally introduced. Then according to typical scenarios, the PCC-related research is deeply surveyed, including freeway and urban traffic scenarios involving traditional vehicles, new energy vehicles, intelligent vehicles, and multi-vehicle platoons. Finally, the general architecture and three typical applications of the cloud control system (CCS) on PCC are briefly introduced, and the prospect and future trends of PCC are proposed.},
	number = {5},
	journal = {Machine Intelligence Research},
	author = {Gao, Bolin and Wan, Keke and Chen, Qien and Wang, Zhou and Li, Rui and Jiang, Yu and Mei, Run and Luo, Yinghui and Li, Keqiang},
	month = oct,
	year = {2023},
	pages = {614--639},
}

@article{ji2024beavertails,
  title={Beavertails: Towards improved safety alignment of llm via a human-preference dataset},
  author={Ji, Jiaming and Liu, Mickel and Dai, Josef and Pan, Xuehai and Zhang, Chi and Bian, Ce and Chen, Boyuan and Sun, Ruiyang and Wang, Yizhou and Yang, Yaodong},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{ji2024aligner,
  title={Aligner: Achieving efficient alignment through weak-to-strong correction},
  author={Ji, Jiaming and Chen, Boyuan and Lou, Hantao and Hong, Donghai and Zhang, Borong and Pan, Xuehai and Dai, Juntao and Yang, Yaodong},
  journal={arXiv preprint arXiv:2402.02416},
  year={2024}
}
@article{ji2023ai,
  title={Ai alignment: A comprehensive survey},
  author={Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and others},
  journal={arXiv preprint arXiv:2310.19852},
  year={2023}
}
@inproceedings{
    dai2024safe,
    title={Safe {RLHF}: Safe Reinforcement Learning from Human Feedback},
    author={Josef Dai and Xuehai Pan and Ruiyang Sun and Jiaming Ji and Xinbo Xu and Mickel Liu and Yizhou Wang and Yaodong Yang},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=TyFrPOKYXw}
}
@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}
@ARTICLE{9334437,
  author={Meng, Wenjia and Zheng, Qian and Shi, Yue and Pan, Gang},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={An Off-Policy Trust Region Policy Optimization Method With Monotonic Improvement Guarantee for Deep Reinforcement Learning}, 
  year={2022},
  volume={33},
  number={5},
  pages={2223-2235},
  keywords={Linear programming;TV;Reinforcement learning;Task analysis;Standards;Space stations;Optimization methods;Deep reinforcement learning;off-policy data;policy-based method;trust region},
  doi={10.1109/TNNLS.2020.3044196}}

@inproceedings{parmas2023model,
  title={Model-based reinforcement learning with scalable composite policy gradient estimators},
  author={Parmas, Paavo and Seno, Takuma and Aoki, Yuma},
  booktitle={International Conference on Machine Learning},
  pages={27346--27377},
  year={2023},
  organization={PMLR}
}

@inproceedings{mora2021pods, 
title = {PODS: Policy Optimization via Differentiable Simulation}, 
author = {Mora, Miguel Angel Zamora and Peychev, Momchil and Ha, Sehoon and Vechev, Martin and Coros, Stelian}, 
booktitle = {Proceedings of the 38th International Conference on Machine Learning}, 
pages = {7805--7817}, 
year = {2021}, 
editor = {Meila, Marina and Zhang, Tong}, 
volume = {139}, series = {Proceedings of Machine Learning Research}, 
month = {18--24 Jul}, 
publisher = {PMLR}, 
pdf = {http://proceedings.mlr.press/v139/mora21a/mora21a.pdf} }

@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@inproceedings{eldahshan2022deep,
  title={Deep Reinforcement Learning based Video Games: A Review},
  author={ElDahshan, Kamal A and Farouk, Hesham and Mofreh, Eslam},
  booktitle={2022 2nd International Mobile, Intelligent, and Ubiquitous Computing Conference (MIUCC)},
  pages={302--309},
  year={2022},
  organization={IEEE}
}

@article{singh2022reinforcement,
  title={Reinforcement learning in robotic applications: a comprehensive survey},
  author={Singh, Bharat and Kumar, Rajesh and Singh, Vinay Pratap},
  journal={Artificial Intelligence Review},
  pages={1--46},
  year={2022},
  publisher={Springer}
}

@inproceedings{okamura2000overview,
  title={An overview of dexterous manipulation},
  author={Okamura, Allison M and Smaby, Niels and Cutkosky, Mark R},
  booktitle={Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No. 00CH37065)},
  volume={1},
  pages={255--262},
  year={2000},
  organization={IEEE}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@article{griewank_ad,
author = {Griewank, Andreas and Walther, Andrea},
title = {Introduction to Automatic Differentiation},
journal = {PAMM},
volume = {2},
number = {1},
pages = {45-49},
year = {2003}
}

@inproceedings{BouAmmar2015,
  title={Autonomous cross-domain knowledge transfer in lifelong policy gradient reinforcement learning},
  author={Ammar, Haitham Bou and Eaton, Eric and Luna, Jos{\'e} Marcio and Ruvolo, Paul},
  booktitle={Twenty-fourth international joint conference on artificial intelligence},
  year={2015}
}

@misc{tessler2018reward,
      title={Reward Constrained Policy Optimization}, 
      author={Chen Tessler and Daniel J. Mankowitz and Shie Mannor},
      year={2018},
      eprint={1805.11074},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{pmlr-v37-schulman15,
  title = 	 {Trust Region Policy Optimization},
  author = 	 {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1889--1897},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/schulman15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/schulman15.html},
  abstract = 	 {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.}
}


@misc{achiam2017constrained,
      title={Constrained Policy Optimization}, 
      author={Joshua Achiam and David Held and Aviv Tamar and Pieter Abbeel},
      year={2017},
      eprint={1705.10528},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{huang2021plasticinelab,
      title={PlasticineLab: A Soft-Body Manipulation Benchmark with Differentiable Physics}, 
      author={Zhiao Huang and Yuanming Hu and Tao Du and Siyuan Zhou and Hao Su and Joshua B. Tenenbaum and Chuang Gan},
      year={2021},
      eprint={2104.03311},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{coumans2016pybullet,
  title={Pybullet, a python module for physics simulation for games, robotics and machine learning},
  author={Coumans, Erwin and Bai, Yunfei},
  year={2016}
}

@inproceedings{heiden2021neuralsim,
  author =	  {Heiden, Eric and Millard, David and Coumans, Erwin and Sheng, Yizhou and Sukhatme, Gaurav S},
  year =		  {2021},
  title =		  {Neural{S}im: Augmenting Differentiable Simulators with Neural Networks},
  booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)}
}

@inproceedings{
freeman2021brax,
title={Brax - A Differentiable Physics Engine for Large Scale Rigid Body Simulation},
author={C. Daniel Freeman and Erik Frey and Anton Raichuk and Sertan Girgin and Igor Mordatch and Olivier Bachem},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
year={2021}
}

@article{bptt,
author = {Mozer, Michael},
year = {1995},
month = {01},
pages = {},
title = {A Focused Backpropagation Algorithm for Temporal Pattern Recognition},
volume = {3},
journal = {Complex Systems}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}


@article{zhang2020first,
  title={First order constrained optimization in policy space},
  author={Zhang, Yiming and Vuong, Quan and Ross, Keith},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15338--15349},
  year={2020}
}

% human-ai safe1
@inproceedings{human_ai_safe1,
  title={Increasing robotic wheelchair safety with collaborative control: Evidence from secondary task experiments},
  author={Carlson, Tom and Demiris, Yiannis},
  booktitle={2010 IEEE International Conference on Robotics and Automation},
  pages={5582--5587},
  year={2010},
  organization={IEEE}
}

% human-ai safe2
@article{human_ai_safe2,
  title={Safety assurance mechanisms of collaborative robotic systems in manufacturing},
  author={Bi, Zhu Ming and Luo, Chaomin and Miao, Zhonghua and Zhang, Bing and Zhang, WJ and Wang, Lihui},
  journal={Robotics and Computer-Integrated Manufacturing},
  volume={67},
  pages={102022},
  year={2021},
  publisher={Elsevier}
}

@article{xu2022trustworthy,
  title={Trustworthy reinforcement learning against intrinsic vulnerabilities: Robustness, safety, and generalizability},
  author={Xu, Mengdi and Liu, Zuxin and Huang, Peide and Ding, Wenhao and Cen, Zhepeng and Li, Bo and Zhao, Ding},
  journal={arXiv preprint arXiv:2209.08025},
  year={2022}
}
@article{garcia2015comprehensive,
  title={A comprehensive survey on safe reinforcement learning},
  author={Garc{\i}a, Javier and Fern{\'a}ndez, Fernando},
  journal={Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={1437--1480},
  year={2015}
}

@article{yang2020projection,
  title={Projection-based constrained policy optimization},
  author={Yang, Tsung-Yen and Rosca, Justinian and Narasimhan, Karthik and Ramadge, Peter J},
  journal={arXiv preprint arXiv:2010.03152},
  year={2020}
}

@article{du2021diffpd,
    author = {Du, Tao and Wu, Kui and Ma, Pingchuan and Wah, Sebastien and Spielberg, Andrew and Rus, Daniela and Matusik, Wojciech},
    title = {DiffPD: Differentiable Projective Dynamics},
    year = {2021},
    issue_date = {April 2022},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {41},
    number = {2},
    journal = {ACM Trans. Graph.},
    month = {nov},
    articleno = {13},
    numpages = {21},
    keywords = {differentiable simulation, Projective dynamics}
}

@inproceedings{Qiao2021Efficient,
author={Qiao, Yi-Ling and Liang, Junbang and Koltun, Vladlen and Lin, Ming C.},
title={Efficient Differentiable Simulation of Articulated Bodies},
booktitle = {ICML},
year={2021},
}

@inproceedings{Jie2022shac,
author  = {Jie Xu, Viktor and Makoviychuk, Yashraj 
 and Narang, Fabio Ramos},
title   = {Accelerated Policy Learning with Parallel Differentiable Simulation},
booktitle = {ICLR},
year    = {2022},
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{achiam2017icml,
    title={Constrained Policy Optimization},
    author={Joshua Achiam,David Held and Aviv Tamar, Pieter Abbee},
    booktitle = {ICML},
    year    = {2017}
}

@book{altman1999constrained,
  title={Constrained Markov decision processes},
  author={Altman, Eitan},
  volume={7},
  year={1999},
  publisher={CRC press}
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@incollection{mozer2013focused,
  title={A focused backpropagation algorithm for temporal pattern recognition},
  author={Mozer, Michael C},
  booktitle={Backpropagation},
  pages={137--169},
  year={2013},
  publisher={Psychology Press}
}
@article{ji2023safety,
  title={Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark},
  author={Ji, Jiaming and Zhang, Borong and Zhou, Jiayi and Pan, Xuehai and Huang, Weidong and Sun, Ruiyang and Geng, Yiran and Zhong, Yifan and Dai, Juntao and Yang, Yaodong},
  journal={arXiv preprint arXiv:2310.12567},
  year={2023}
}

@article{chow2018risk,
  title={Risk-constrained reinforcement learning with percentile risk criteria},
  author={Chow, Yinlam and Ghavamzadeh, Mohammad and Janson, Lucas and Pavone, Marco},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={167},
  pages={1--51},
  year={2018}
}

@article{ray2019benchmarking,
  title={Benchmarking safe exploration in deep reinforcement learning},
  author={Ray, Alex and Achiam, Joshua and Amodei, Dario},
  journal={arXiv preprint arXiv:1910.01708},
  volume={7},
  number={1},
  pages={2},
  year={2019}
}

@article{as2022constrained,
  title={Constrained policy optimization via bayesian world models},
  author={As, Yarden and Usmanova, Ilnura and Curi, Sebastian and Krause, Andreas},
  journal={arXiv preprint arXiv:2201.09802},
  year={2022}
}

@article{wah2000improving,
  title={Improving the performance of weighted Lagrange-multiplier methods for nonlinear constrained optimization},
  author={Wah, Benjamin W and Wang, Tao and Shang, Yi and Wu, Zhe},
  journal={Information Sciences},
  volume={124},
  number={1-4},
  pages={241--272},
  year={2000},
  publisher={Elsevier}
}

@inproceedings{platt1987constrained,
  title={Constrained differential optimization},
  author={Platt, John and Barr, Alan},
  booktitle={Neural Information Processing Systems},
  year={1987}
}

@article{degrave2019differentiable,
  title={A differentiable physics engine for deep learning in robotics},
  author={Degrave, Jonas and Hermans, Michiel and Dambre, Joni and others},
  journal={Frontiers in neurorobotics},
  pages={6},
  year={2019},
  publisher={Frontiers}
}

@inproceedings{werling2021fast,
  title={Fast and feature-complete differentiable physics engine for articulated rigid bodies with contact constraints},
  author={Werling, Keenon and Omens, Dalton and Lee, Jeongseok and Exarchos, Ioannis and Liu, C Karen},
  booktitle={Robotics: Science and Systems},
  year={2021}
}

@article{xian2023fluidlab,
  title={Fluidlab: A differentiable environment for benchmarking complex fluid manipulation},
  author={Xian, Zhou and Zhu, Bo and Xu, Zhenjia and Tung, Hsiao-Yu and Torralba, Antonio and Fragkiadaki, Katerina and Gan, Chuang},
  journal={arXiv preprint arXiv:2303.02346},
  year={2023}
}


@InProceedings{pmlr-v202-parmas23a,
  title = 	 {Model-based Reinforcement Learning with Scalable Composite Policy Gradient Estimators},
  author =       {Parmas, Paavo and Seno, Takuma and Aoki, Yuma},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {27346--27377},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/parmas23a/parmas23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/parmas23a.html},
  abstract = 	 {In model-based reinforcement learning (MBRL), policy gradients can be estimated either by derivative-free RL methods, such as likelihood ratio gradients (LR), or by backpropagating through a differentiable model via reparameterization gradients (RP). Instead of using one or the other, the Total Propagation (TP) algorithm in prior work showed that a combination of LR and RP estimators averaged using inverse variance weighting (IVW) can achieve orders of magnitude improvement over either method. However, IVW-based composite estimators have not yet been applied in modern RL tasks, as it is unclear if they can be implemented scalably. We propose a scalable method, Total Propagation X (TPX) that improves over TP by changing the node used for IVW, and employing coordinate wise weighting. We demonstrate the scalability of TPX by applying it to the state of the art visual MBRL algorithm Dreamer. The experiments showed that Dreamer fails with long simulation horizons, while our TPX works reliably for only a fraction of additional computation. One key advantage of TPX is its ease of implementation, which will enable experimenting with IVW on many tasks beyond MBRL.}
}

@article{clavera2020model,
  title={Model-augmented actor-critic: Backpropagating through paths},
  author={Clavera, Ignasi and Fu, Violet and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2005.08068},
  year={2020}
}
@inproceedings{kakade2002approximately,
author = {Kakade, Sham and Langford, John},
title = {Approximately Optimal Approximate Reinforcement Learning},
year = {2002},
isbn = {1558608737},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Nineteenth International Conference on Machine Learning},
pages = {267–274},
numpages = {8},
series = {ICML '02}
}
@article{puterman1990markov,
  title={Markov decision processes},
  author={Puterman, Martin L},
  journal={Handbooks in operations research and management science},
  volume={2},
  pages={331--434},
  year={1990},
  publisher={Elsevier}
}
@article{bogue2017robots,
  title={Robots that interact with humans: a review of safety technologies and standards},
  author={Bogue, Robert},
  journal={Industrial Robot: An International Journal},
  volume={44},
  number={4},
  pages={395--400},
  year={2017},
  publisher={Emerald Publishing Limited}
}
@article{muhammad2020deep,
  title={Deep learning for safe autonomous driving: Current challenges and future directions},
  author={Muhammad, Khan and Ullah, Amin and Lloret, Jaime and Del Ser, Javier and de Albuquerque, Victor Hugo C},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  volume={22},
  number={7},
  pages={4316--4336},
  year={2020},
  publisher={IEEE}
}
@article{ji2023omnisafe,
  title={OmniSafe: An Infrastructure for Accelerating Safe Reinforcement Learning Research},
  author={Ji, Jiaming and Zhou, Jiayi and Zhang, Borong and Dai, Juntao and Pan, Xuehai and Sun, Ruiyang and Huang, Weidong and Geng, Yiran and Liu, Mickel and Yang, Yaodong},
  journal={arXiv preprint arXiv:2305.09304},
  year={2023}
}
@article{gronauer2022bullet,
  title={Bullet-safety-gym: A framework for constrained reinforcement learning},
  author={Gronauer, Sven},
  year={2022}
}
@article{mohamed2020monte,
  title={Monte carlo gradient estimation in machine learning},
  author={Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5183--5244},
  year={2020},
  publisher={JMLRORG}
}

@inproceedings{dai2023augmented,
  title={Augmented proximal policy optimization for safe reinforcement learning},
  author={Dai, Juntao and Ji, Jiaming and Yang, Long and Zheng, Qian and Pan, Gang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={6},
  pages={7288--7295},
  year={2023}
}

@article{yang2022constrained,
  title={Constrained update projection approach to safe policy optimization},
  author={Yang, Long and Ji, Jiaming and Dai, Juntao and Zhang, Linrui and Zhou, Binbin and Li, Pengfei and Yang, Yaodong and Pan, Gang},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={9111--9124},
  year={2022}
}

@article{metz2021gradients,
  title={Gradients are not all you need},
  author={Metz, Luke and Freeman, C Daniel and Schoenholz, Samuel S and Kachman, Tal},
  journal={arXiv preprint arXiv:2111.05803},
  year={2021}
}

@article{jaisson2022deep,
  title={Deep differentiable reinforcement learning and optimal trading},
  author={Jaisson, Thibault},
  journal={Quantitative Finance},
  volume={22},
  number={8},
  pages={1429--1443},
  year={2022},
  publisher={Taylor \& Francis}
}
@inproceedings{stooke2020responsive,
  title={Responsive safety in reinforcement learning by pid lagrangian methods},
  author={Stooke, Adam and Achiam, Joshua and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={9133--9143},
  year={2020},
  organization={PMLR}
}

@inproceedings{kalagarla2021sample,
  title={A sample-efficient algorithm for episodic finite-horizon MDP with constraints},
  author={Kalagarla, Krishna C and Jain, Rahul and Nuzzo, Pierluigi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={9},
  pages={8030--8037},
  year={2021}
}

@inproceedings{guin2023policy,
  title={A policy gradient approach for finite horizon constrained Markov decision processes},
  author={Guin, Soumyajit and Bhatnagar, Shalabh},
  booktitle={2023 62nd IEEE Conference on Decision and Control (CDC)},
  pages={3353--3359},
  year={2023},
  organization={IEEE}
}

@inproceedings{liu2022constrained,
  title={Constrained variational policy optimization for safe reinforcement learning},
  author={Liu, Zuxin and Cen, Zhepeng and Isenbaev, Vladislav and Liu, Wei and Wu, Steven and Li, Bo and Zhao, Ding},
  booktitle={International Conference on Machine Learning},
  pages={13644--13668},
  year={2022},
  organization={PMLR}
}

@article{popescu2009multilayer,
  title={Multilayer perceptron and neural networks},
  author={Popescu, Marius-Constantin and Balas, Valentina E and Perescu-Popescu, Liliana and Mastorakis, Nikos},
  journal={WSEAS Transactions on Circuits and Systems},
  volume={8},
  number={7},
  pages={579--588},
  year={2009},
  publisher={World Scientific and Engineering Academy and Society (WSEAS) Stevens Point~…}
}

@article{howell2022dojo,
  title={Dojo: A differentiable physics engine for robotics},
  author={Howell, Taylor A and Cleac'h, Simon Le and Br{\"u}digam, Jan and Kolter, J Zico and Schwager, Mac and Manchester, Zachary},
  journal={arXiv preprint arXiv:2203.00806},
  year={2022}
}

@article{lee2018reparameterization,
  title={Reparameterization gradient for non-differentiable models},
  author={Lee, Wonyeol and Yu, Hangyeol and Yang, Hongseok},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{NILSSON20179083,
title = {Trajectory planning with miscellaneous safety critical zones**This work was supported by FFI - Strategic Vehicle Research and Innovation.},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {9083-9088},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.1649},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317322541},
author = {J. Nilsson and J. Fredriksson and E. Coelingh},
abstract = {Highly automated vehicles have the potential to provide a variety of benefits e.g., decreasing traffic injuries and fatalities by offering people the freedom to choose how to spend their time in their vehicle without jeopardizing the safety of themselves or other traffic participants. Since smooth and safe trajectory planning is essential for successfully commercialization of automated vehicles, this paper presents a low-complexity trajectory planning algorithm in the Model Predictive Control (MPC) framework. In particular, the proposed algorithm accounts for safety critical zones of miscellaneous shape defined by both the planned longitudinal and lateral motion of the automated vehicle. The automated vehicle is thereby able to efficiently utilize the free road space and traverse dense traffic situation in a self-assertive manner rather than exhibit an excessively conservative behavior. The proposed algorithm is thereby considered to be a building block for Advanced Driver Assistance Systems (ADAS) and eventually highly automated vehicles which are safe, smooth, and self-assertive.}
}
@inproceedings{suh2022differentiable,
  title={Do differentiable simulators give better policy gradients?},
  author={Suh, Hyung Ju and Simchowitz, Max and Zhang, Kaiqing and Tedrake, Russ},
  booktitle={International Conference on Machine Learning},
  pages={20668--20696},
  year={2022},
  organization={PMLR}
}