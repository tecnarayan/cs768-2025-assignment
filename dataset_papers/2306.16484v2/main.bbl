\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ajalloeian and Stich(2020)]{ajalloeian2020convergence}
Ahmad Ajalloeian and Sebastian~U Stich.
\newblock On the convergence of {SGD} with biased gradients.
\newblock \emph{arXiv preprint arXiv:2008.00051}, 2020.

\bibitem[Alam et~al.(2022)Alam, Liu, Yan, and Zhang]{alam2022fedrolex}
Samiul Alam, Luyang Liu, Ming Yan, and Mi~Zhang.
\newblock {FedRolex}: Model-heterogeneous federated learning with rolling
  sub-model extraction.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=OtxyysUdBE}.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock {QSGD}: Communication-efficient {SGD} via gradient quantization and
  encoding.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Arjevani et~al.(2020)Arjevani, Shamir, and Srebro]{arjevani2020tight}
Yossi Arjevani, Ohad Shamir, and Nathan Srebro.
\newblock A tight convergence analysis for stochastic gradient descent with
  delayed updates.
\newblock In \emph{Algorithmic Learning Theory}, pages 111--132. PMLR, 2020.

\bibitem[Bian et~al.(2023)Bian, Li, Wang, Xing, and Venkataraman]{bian2023does}
Song Bian, Dacheng Li, Hongyi Wang, Eric~P Xing, and Shivaram Venkataraman.
\newblock Does compressing activations help model parallel training?
\newblock \emph{arXiv preprint arXiv:2301.02654}, 2023.

\bibitem[Caldas et~al.(2018)Caldas, Kone{\v{c}}ny, McMahan, and
  Talwalkar]{caldas2018expanding}
Sebastian Caldas, Jakub Kone{\v{c}}ny, H~Brendan McMahan, and Ameet Talwalkar.
\newblock Expanding the reach of federated learning by reducing client resource
  requirements.
\newblock \emph{arXiv preprint arXiv:1812.07210}, 2018.

\bibitem[Charles et~al.(2022)Charles, Bonawitz, Chiknavaryan, McMahan,
  et~al.]{charles2022federated}
Zachary Charles, Kallista Bonawitz, Stanislav Chiknavaryan, Brendan McMahan,
  et~al.
\newblock Federated select: A primitive for communication-and memory-efficient
  federated learning.
\newblock \emph{arXiv preprint arXiv:2208.09432}, 2022.

\bibitem[Chayti and Karimireddy(2024)]{chayti2022optimization}
El~Mahdi Chayti and Sai~Praneeth Karimireddy.
\newblock Optimization with access to auxiliary information.
\newblock \emph{Transactions on Machine Learning Research}, 2024.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=kxYqgSkH8I}.

\bibitem[Chen et~al.(2023)Chen, Chen, Wu, and Yu]{chen2022fedobd}
Yuanyuan Chen, Zichen Chen, Pengcheng Wu, and Han Yu.
\newblock {FEDOBD}: Opportunistic block dropout for efficiently training
  large-scale neural networks through federated learning.
\newblock In \emph{Proceedings of the Thirty-Second International Joint
  Conference on Artificial Intelligence}, pages 3541--3549, 2023.

\bibitem[Chraibi et~al.(2019)Chraibi, Khaled, Kovalev, Richt{\'a}rik, Salim,
  and Takáč]{chraibi2019distributed}
Sélim Chraibi, Ahmed Khaled, Dmitry Kovalev, Peter Richt{\'a}rik, Adil Salim,
  and Martin Takáč.
\newblock Distributed fixed point methods with compressed iterates.
\newblock \emph{arXiv preprint arXiv:2102.07245}, 2019.

\bibitem[Cunha et~al.(2022)Cunha, Gidel, Pedregosa, Scieur, and
  Paquette]{cunha2022only}
Leonardo Cunha, Gauthier Gidel, Fabian Pedregosa, Damien Scieur, and Courtney
  Paquette.
\newblock Only tails matter: Average-case universality and robustness in the
  convex regime.
\newblock In \emph{International Conference on Machine Learning}, pages
  4474--4491. PMLR, 2022.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, Ranzato,
  Senior, Tucker, Yang, et~al.]{dean2012large}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
  Marc'aurelio Ranzato, Andrew Senior, Paul Tucker, Ke~Yang, et~al.
\newblock Large scale distributed deep networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 25, 2012.

\bibitem[Demidovich et~al.(2023)Demidovich, Malinovsky, Shulgin, and
  Richt{\'a}rik]{demidovich2023mast}
Yury Demidovich, Grigory Malinovsky, Egor Shulgin, and Peter Richt{\'a}rik.
\newblock {MAST}: Model-agnostic sparsified training.
\newblock \emph{arXiv preprint arXiv:2311.16086}, 2023.

\bibitem[Diao et~al.(2021)Diao, Ding, and Tarokh]{diao2020heterofl}
Enmao Diao, Jie Ding, and Vahid Tarokh.
\newblock Hetero{FL}: Computation and communication efficient federated
  learning for heterogeneous clients.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=TNkPBBYFkXg}.

\bibitem[Dun et~al.(2022)Dun, Wolfe, Jermaine, and Kyrillidis]{dun2022resist}
Chen Dun, Cameron~R Wolfe, Christopher~M Jermaine, and Anastasios Kyrillidis.
\newblock {ResIST}: Layer-wise decomposition of resnets for distributed
  training.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 610--620.
  PMLR, 2022.

\bibitem[Dun et~al.(2023)Dun, Hipolito, Jermaine, Dimitriadis, and
  Kyrillidis]{dun2023efficient}
Chen Dun, Mirian Hipolito, Chris Jermaine, Dimitrios Dimitriadis, and
  Anastasios Kyrillidis.
\newblock Efficient and light-weight federated learning via asynchronous
  distributed dropout.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 6630--6660. PMLR, 2023.

\bibitem[Farber and Asanovic(1997)]{farber1997parallel}
Philipp Farber and Krste Asanovic.
\newblock Parallel neural network training on multi-spert.
\newblock In \emph{Proceedings of 3rd International Conference on Algorithms
  and Architectures for Parallel Processing}, pages 659--666. IEEE, 1997.

\bibitem[Gorbunov et~al.(2020)Gorbunov, Hanzely, and
  Richt{\'a}rik]{gorbunov2020unified}
Eduard Gorbunov, Filip Hanzely, and Peter Richt{\'a}rik.
\newblock A unified theory of {SGD}: Variance reduction, sampling, quantization
  and coordinate descent.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 680--690. PMLR, 2020.

\bibitem[Goujaud et~al.(2022)Goujaud, Scieur, Dieuleveut, Taylor, and
  Pedregosa]{goujaud2022super}
Baptiste Goujaud, Damien Scieur, Aymeric Dieuleveut, Adrien~B Taylor, and
  Fabian Pedregosa.
\newblock Super-acceleration with cyclical step-sizes.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3028--3065. PMLR, 2022.

\bibitem[Gower et~al.(2019)Gower, Loizou, Qian, Sailanbayev, Shulgin, and
  Richt{\'a}rik]{gower2019sgd}
Robert~Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor
  Shulgin, and Peter Richt{\'a}rik.
\newblock {SGD}: General analysis and improved rates.
\newblock \emph{Proceedings of the 36th International Conference on Machine
  Learning, Long Beach, California}, 2019.

\bibitem[Goyal et~al.(2018)Goyal, Dollár, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2018accurate}
Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski,
  Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch {SGD}: Training {ImageNet} in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Horvath et~al.(2021)Horvath, Laskaridis, Almeida, Leontiadis,
  Venieris, and Lane]{horvath2021fjord}
Samuel Horvath, Stefanos Laskaridis, Mario Almeida, Ilias Leontiadis, Stylianos
  Venieris, and Nicholas Lane.
\newblock {FjORD}: Fair and accurate federated learning under heterogeneous
  targets with ordered dropout.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 12876--12889, 2021.

\bibitem[Jiang et~al.(2022)Jiang, Wang, Valls, Ko, Lee, Leung, and
  Tassiulas]{jiang2022model}
Yuang Jiang, Shiqiang Wang, Victor Valls, Bong~Jun Ko, Wei-Han Lee, Kin~K
  Leung, and Leandros Tassiulas.
\newblock Model pruning enables efficient federated learning on edge devices.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  2022.

\bibitem[Kairouz et~al.(2021)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji,
  Bonawitz, Charles, Cormode, Cummings, D'Oliveira, Eichner, Rouayheb, Evans,
  Gardner, Garrett, Gasc{\'{o}}n, Ghazi, Gibbons, Gruteser, Harchaoui, He, He,
  Huo, Hutchinson, Hsu, Jaggi, Javidi, Joshi, Khodak, Kone{\v{c}}n{\'y},
  Korolova, Koushanfar, Koyejo, Lepoint, Liu, Mittal, Mohri, Nock,
  {\"{O}}zg{\"{u}}r, Pagh, Qi, Ramage, Raskar, Raykova, Song, Song, Stich, Sun,
  Suresh, Tram{\`{e}}r, Vepakomma, Wang, Xiong, Xu, Yang, Yu, Yu, and
  Zhao]{kairouz2021advances}
Peter Kairouz, H.~Brendan McMahan, Brendan Avent, Aur{\'{e}}lien Bellet, Mehdi
  Bennis, Arjun~Nitin Bhagoji, Kallista~A. Bonawitz, Zachary Charles, Graham
  Cormode, Rachel Cummings, Rafael G.~L. D'Oliveira, Hubert Eichner, Salim~El
  Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adri{\`{a}}
  Gasc{\'{o}}n, Badih Ghazi, Phillip~B. Gibbons, Marco Gruteser, Za{\"{\i}}d
  Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu,
  Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub
  Kone{\v{c}}n{\'y}, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo,
  Tancr{\`{e}}de Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard
  Nock, Ayfer {\"{O}}zg{\"{u}}r, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh
  Raskar, Mariana Raykova, Dawn Song, Weikang Song, Sebastian~U. Stich, Ziteng
  Sun, Ananda~Theertha Suresh, Florian Tram{\`{e}}r, Praneeth Vepakomma, Jianyu
  Wang, Li~Xiong, Zheng Xu, Qiang Yang, Felix~X. Yu, Han Yu, and Sen Zhao.
\newblock Advances and open problems in federated learning.
\newblock \emph{Found. Trends Mach. Learn.}, 14\penalty0 (1-2):\penalty0
  1--210, 2021.
\newblock \doi{10.1561/2200000083}.
\newblock URL \url{https://doi.org/10.1561/2200000083}.

\bibitem[Khaled and Richt{\'a}rik(2019)]{khaled2019gradient}
Ahmed Khaled and Peter Richt{\'a}rik.
\newblock Gradient descent with compressed iterates.
\newblock \emph{arXiv preprint arXiv:1909.04716}, 2019.

\bibitem[Khaled and Richt{\'a}rik(2023)]{khaled2022better}
Ahmed Khaled and Peter Richt{\'a}rik.
\newblock Better theory for {SGD} in the nonconvex world.
\newblock \emph{Transactions on Machine Learning Research}, 2023.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=AU4qHN2VkS}.
\newblock Survey Certification.

\bibitem[Khaled et~al.(2020)Khaled, Mishchenko, and
  Richt{\'a}rik]{khaled2020tighter}
Ahmed Khaled, Konstantin Mishchenko, and Peter Richt{\'a}rik.
\newblock Tighter theory for local {SGD} on identical and heterogeneous data.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 4519--4529. PMLR, 2020.

\bibitem[Khirirat et~al.(2018)Khirirat, Feyzmahdavian, and
  Johansson]{khirirat2018distributed}
Sarit Khirirat, Hamid~Reza Feyzmahdavian, and Mikael Johansson.
\newblock Distributed learning with compressed gradients.
\newblock \emph{arXiv preprint arXiv:1806.06573}, 2018.

\bibitem[Konečný et~al.(2016)Konečný, McMahan, Yu, Richt{\'a}rik, Suresh,
  and Bacon]{konecny2017federated}
Jakub Konečný, H.~Brendan McMahan, Felix~X. Yu, Peter Richt{\'a}rik,
  Ananda~Theertha Suresh, and Dave Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock \emph{NIPS Private Multi-Party Machine Learning Workshop}, 2016.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Nair, and
  Hinton]{krizhevsky2009cifar}
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
\newblock Cifar-10 and cifar-100 datasets.
\newblock \emph{URl: https://www. cs. toronto. edu/kriz/cifar. html},
  6\penalty0 (1):\penalty0 1, 2009.

\bibitem[Li et~al.(2021)Li, Bao, Zhang, and Richt{\'a}rik]{li2021page}
Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richt{\'a}rik.
\newblock {PAGE}: A simple and optimal probabilistic gradient estimator for
  nonconvex optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  6286--6295. PMLR, 2021.

\bibitem[Liao and Kyrillidis(2022)]{liao2022on}
Fangshuo Liao and Anastasios Kyrillidis.
\newblock On the convergence of shallow neural network training with randomly
  masked neurons.
\newblock \emph{Transactions on Machine Learning Research}, 2022.
\newblock URL \url{https://openreview.net/forum?id=e7mYYMSyZH}.

\bibitem[Lin et~al.(2022)Lin, Xiao, Yang, Zhao, Xiong, Motta, and
  Beaufays]{lin2022federated}
Rongmei Lin, Yonghui Xiao, Tien-Ju Yang, Ding Zhao, Li~Xiong, Giovanni Motta,
  and Fran{\c{c}}oise Beaufays.
\newblock Federated pruning: Improving neural network efficiency with federated
  learning.
\newblock \emph{arXiv preprint arXiv:2209.06359}, 2022.

\bibitem[Lin et~al.(2019)Lin, Stich, Barba, Dmitriev, and
  Jaggi]{lin2019dynamic}
Tao Lin, Sebastian~U Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi.
\newblock Dynamic model pruning with feedback.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial intelligence and statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem[Mishchenko et~al.(2022)Mishchenko, Malinovsky, Stich, and
  Richt{\'a}rik]{mishchenko2022proxskip}
Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter
  Richt{\'a}rik.
\newblock {ProxSkip}: Yes! {L}ocal gradient steps provably lead to
  communication acceleration! {F}inally!
\newblock In \emph{International Conference on Machine Learning}, pages
  15750--15769. PMLR, 2022.

\bibitem[Mohtashami et~al.(2022)Mohtashami, Jaggi, and
  Stich]{mohtashami2022masked}
Amirkeivan Mohtashami, Martin Jaggi, and Sebastian Stich.
\newblock Masked training of neural networks with partial gradients.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 5876--5890. PMLR, 2022.

\bibitem[Nesterov(2012)]{nesterov2012efficiency}
Yu~Nesterov.
\newblock Efficiency of coordinate descent methods on huge-scale optimization
  problems.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0
  341--362, 2012.

\bibitem[Qiu et~al.(2022)Qiu, Fernandez-Marques, Gusmao, Gao, Parcollet, and
  Lane]{qiu2022zerofl}
Xinchi Qiu, Javier Fernandez-Marques, Pedro~PB Gusmao, Yan Gao, Titouan
  Parcollet, and Nicholas~Donald Lane.
\newblock Zero{FL}: Efficient on-device training for federated learning with
  local sparsity.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=2sDQwC_hmnM}.

\bibitem[Richt{\'a}rik and Tak{\'a}{\v{c}}(2014)]{richtarik2014iteration}
Peter Richt{\'a}rik and Martin Tak{\'a}{\v{c}}.
\newblock Iteration complexity of randomized block-coordinate descent methods
  for minimizing a composite function.
\newblock \emph{Mathematical Programming}, 144\penalty0 (1-2):\penalty0 1--38,
  2014.

\bibitem[Richt{\'a}rik and Tak{\'a}{\v{c}}(2016)]{Hydra}
Peter Richt{\'a}rik and Martin Tak{\'a}{\v{c}}.
\newblock Distributed coordinate descent method for learning with big data.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (75):\penalty0 1--25, 2016.

\bibitem[Safaryan et~al.(2021)Safaryan, Hanzely, and
  Richt{\'a}rik]{safaryan2021smoothness}
Mher Safaryan, Filip Hanzely, and Peter Richt{\'a}rik.
\newblock Smoothness matrices beat smoothness constants: Better communication
  compression techniques for distributed optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 25688--25702, 2021.

\bibitem[Seide et~al.(2014)Seide, Fu, Droppo, Li, and Yu]{seide20141}
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech {DNNs}.
\newblock In \emph{Fifteenth Annual Conference of the International Speech
  Communication Association}, 2014.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
  and Bryan Catanzaro.
\newblock Megatron-{LM}: Training multi-billion parameter language models using
  model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Shulgin and Richt{\'a}rik(2022)]{shulgin2022shifted}
Egor Shulgin and Peter Richt{\'a}rik.
\newblock Shifted compression framework: Generalizations and improvements.
\newblock In \emph{The 38th Conference on Uncertainty in Artificial
  Intelligence}, 2022.

\bibitem[Stich(2019)]{stich2019unified}
Sebastian~U Stich.
\newblock Unified optimal analysis of the (stochastic) gradient method.
\newblock \emph{arXiv preprint arXiv:1907.04232}, 2019.

\bibitem[Szlendak et~al.(2022)Szlendak, Tyurin, and
  Richt{\'a}rik]{szlendak2022permutation}
Rafa{\l} Szlendak, Alexander Tyurin, and Peter Richt{\'a}rik.
\newblock Permutation compressors for provably faster distributed nonconvex
  optimization.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=GugZ5DzzAu}.

\bibitem[Wang et~al.(2022)Wang, Safaryan, and
  Richt{\'a}rik]{wang2022theoretically}
Bokun Wang, Mher Safaryan, and Peter Richt{\'a}rik.
\newblock Theoretically better and numerically faster distributed optimization
  with smoothness-aware quantization techniques.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 9841--9852, 2022.

\bibitem[Wang et~al.(2021)Wang, Charles, Xu, Joshi, McMahan, Al-Shedivat,
  Andrew, Avestimehr, Daly, Data, et~al.]{wang2021field}
Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H~Brendan McMahan, Maruan
  Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data,
  et~al.
\newblock A field guide to federated optimization.
\newblock \emph{arXiv preprint arXiv:2107.06917}, 2021.

\bibitem[Wen et~al.(2022)Wen, Jeon, and Huang]{wen2022federated}
Dingzhu Wen, Ki-Jun Jeon, and Kaibin Huang.
\newblock Federated dropout—a simple approach for enabling federated learning
  on resource constrained devices.
\newblock \emph{IEEE Wireless Communications Letters}, 11\penalty0
  (5):\penalty0 923--927, 2022.

\bibitem[Wolfe et~al.(2023)Wolfe, Yang, Liao, Chowdhury, Dun, Bayer, Segarra,
  and Kyrillidis]{wolfe2021gist}
Cameron~R Wolfe, Jingkang Yang, Fangshuo Liao, Arindam Chowdhury, Chen Dun,
  Artun Bayer, Santiago Segarra, and Anastasios Kyrillidis.
\newblock {GIST}: Distributed training for large-scale graph convolutional
  networks.
\newblock \emph{Journal of Applied and Computational Topology}, pages 1--53,
  2023.

\bibitem[Yang et~al.(2022)Yang, Guliani, Beaufays, and Motta]{yang2022partial}
Tien-Ju Yang, Dhruv Guliani, Fran{\c{c}}oise Beaufays, and Giovanni Motta.
\newblock Partial variable training for efficient on-device federated learning.
\newblock In \emph{ICASSP 2022-2022 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 4348--4352. IEEE, 2022.

\bibitem[Yuan et~al.(2022)Yuan, Wolfe, Dun, Tang, Kyrillidis, and
  Jermaine]{yuan2022distributed}
Binhang Yuan, Cameron~R Wolfe, Chen Dun, Yuxin Tang, Anastasios Kyrillidis, and
  Chris Jermaine.
\newblock Distributed learning of fully connected neural networks using
  independent subnet training.
\newblock \emph{Proceedings of the VLDB Endowment}, 15\penalty0 (8):\penalty0
  1581--1590, 2022.

\bibitem[Zhang et~al.(2019)Zhang, Li, Nado, Martens, Sachdeva, Dahl, Shallue,
  and Grosse]{zhang2019algorithmic}
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George
  Dahl, Chris Shallue, and Roger~B Grosse.
\newblock Which algorithmic choices matter at which batch sizes? insights from
  a noisy quadratic model.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Zhang et~al.(1989)Zhang, Mckenna, Mesirov, and
  Waltz]{zhang1989efficient}
Xiru Zhang, Michael Mckenna, Jill Mesirov, and David Waltz.
\newblock An efficient implementation of the back-propagation algorithm on the
  connection machine {CM-2}.
\newblock \emph{Advances in neural information processing systems}, 2, 1989.

\bibitem[Zhou et~al.(2022)Zhou, Lan, Venkataramani, and
  Ding]{zhou2022convergence}
Hanhan Zhou, Tian Lan, Guru Venkataramani, and Wenbo Ding.
\newblock On the convergence of heterogeneous federated learning with arbitrary
  adaptive online model pruning.
\newblock \emph{arXiv preprint arXiv:2201.11803}, 2022.

\bibitem[Zhu et~al.(2023)Zhu, Liu, Radhakrishnan, and Belkin]{zhu2022quadratic}
Libin Zhu, Chaoyue Liu, Adityanarayanan Radhakrishnan, and Mikhail Belkin.
\newblock Quadratic models for understanding neural network dynamics.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2023.

\bibitem[Zinkevich et~al.(2010)Zinkevich, Weimer, Li, and
  Smola]{zinkevich2010parallelized}
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex Smola.
\newblock Parallelized stochastic gradient descent.
\newblock \emph{Advances in neural information processing systems}, 23, 2010.

\end{thebibliography}
