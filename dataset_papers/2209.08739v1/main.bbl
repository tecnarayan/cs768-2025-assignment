\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aneja et~al.(2021)Aneja, Schwing, Kautz, and Vahdat]{aneja2021ncpvae}
Jyoti Aneja, Alexander Schwing, Jan Kautz, and Arash Vahdat.
\newblock A contrastive learning approach for training variational autoencoder
  priors.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Bauer and Mnih(2019)]{bauer2019resampled}
Matthias Bauer and Andriy Mnih.
\newblock Resampled priors for variational autoencoders.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 66--75. PMLR, 2019.

\bibitem[Bengio et~al.(2013)Bengio, Mesnil, Dauphin, and
  Rifai]{bengio2013better}
Yoshua Bengio, Gr{\'e}goire Mesnil, Yann Dauphin, and Salah Rifai.
\newblock Better mixing via deep representations.
\newblock In \emph{International conference on machine learning}, pages
  552--560. PMLR, 2013.

\bibitem[Dai and Wipf(2019)]{dai2019diagnosing}
Bin Dai and David Wipf.
\newblock Diagnosing and enhancing vae models.
\newblock \emph{arXiv preprint arXiv:1903.05789}, 2019.

\bibitem[Esser et~al.(2021)Esser, Rombach, and Ommer]{esser2021taming}
Patrick Esser, Robin Rombach, and Bjorn Ommer.
\newblock Taming transformers for high-resolution image synthesis.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 12873--12883, 2021.

\bibitem[Fawcett(2006)]{fawcett2006introduction}
Tom Fawcett.
\newblock An introduction to roc analysis.
\newblock \emph{Pattern recognition letters}, 27\penalty0 (8):\penalty0
  861--874, 2006.

\bibitem[Gao et~al.(2020)Gao, Nijkamp, Kingma, Xu, Dai, and Wu]{gao2020flow}
Ruiqi Gao, Erik Nijkamp, Diederik~P Kingma, Zhen Xu, Andrew~M Dai, and
  Ying~Nian Wu.
\newblock Flow contrastive estimation of energy-based models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 7518--7528, 2020.

\bibitem[Gelman and Meng(1998)]{gelman1998simulating}
Andrew Gelman and Xiao-Li Meng.
\newblock Simulating normalizing constants: From importance sampling to bridge
  sampling to path sampling.
\newblock \emph{Statistical science}, pages 163--185, 1998.

\bibitem[Ghosh et~al.(2019)Ghosh, Sajjadi, Vergari, Black, and
  Sch{\"o}lkopf]{ghosh2019variational}
Partha Ghosh, Mehdi~SM Sajjadi, Antonio Vergari, Michael Black, and Bernhard
  Sch{\"o}lkopf.
\newblock From variational to deterministic autoencoders.
\newblock \emph{arXiv preprint arXiv:1903.12436}, 2019.

\bibitem[Glorot and Bengio(2010)]{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Gutmann and Hyv{{\"a}}rinen(2012)]{JMLR:v13:gutmann12a}
Michael~U. Gutmann and Aapo Hyv{{\"a}}rinen.
\newblock Noise-contrastive estimation of unnormalized statistical models, with
  applications to natural image statistics.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (11):\penalty0 307--361, 2012.

\bibitem[Han et~al.(2017)Han, Lu, Zhu, and Wu]{han2017alternating}
Tian Han, Yang Lu, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Alternating back-propagation for generator network.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~31, 2017.

\bibitem[Han et~al.(2019)Han, Lu, Wu, Xing, and Wu]{han2019learning}
Tian Han, Yang Lu, Jiawen Wu, Xianglei Xing, and Ying~Nian Wu.
\newblock Learning generator networks for dynamic patterns.
\newblock In \emph{2019 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pages 809--818. IEEE, 2019.

\bibitem[Havtorn et~al.(2021)Havtorn, Frellsen, Hauberg, and
  Maal{\o}e]{havtorn2021hierarchical}
Jakob~D Havtorn, Jes Frellsen, S{\o}ren Hauberg, and Lars Maal{\o}e.
\newblock Hierarchical vaes know what they donâ€™t know.
\newblock In \emph{International Conference on Machine Learning}, pages
  4117--4128. PMLR, 2021.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Hinton(2002)]{hinton2002training}
Geoffrey~E Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock \emph{Neural computation}, 14\penalty0 (8):\penalty0 1771--1800,
  2002.

\bibitem[Hoffman and Johnson(2016)]{hoffman2016elbo}
Matthew~D Hoffman and Matthew~J Johnson.
\newblock Elbo surgery: yet another way to carve up the variational evidence
  lower bound.
\newblock In \emph{Workshop in Advances in Approximate Bayesian Inference,
  NIPS}, volume~1, 2016.

\bibitem[Izmailov et~al.(2020)Izmailov, Kirichenko, Finzi, and
  Wilson]{izmailov2020semi}
Pavel Izmailov, Polina Kirichenko, Marc Finzi, and Andrew~Gordon Wilson.
\newblock Semi-supervised learning with normalizing flows.
\newblock In \emph{International Conference on Machine Learning}, pages
  4615--4630. PMLR, 2020.

\bibitem[Jin et~al.(2017)Jin, Lazarow, and Tu]{jin2017introspective}
Long Jin, Justin Lazarow, and Zhuowen Tu.
\newblock Introspective classification with convolutional nets.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Karras et~al.(2019)Karras, Laine, and Aila]{karras2019style}
Tero Karras, Samuli Laine, and Timo Aila.
\newblock A style-based generator architecture for generative adversarial
  networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 4401--4410, 2019.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma and Welling(2014)]{kingma2014vae}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock In \emph{The International Conference on Learning Representations
  (ICLR)}, 2014.

\bibitem[Kirkpatrick et~al.(1983)Kirkpatrick, Gelatt, and
  Vecchi]{kirkpatrick1983optimization}
Scott Kirkpatrick, C~Daniel Gelatt, and Mario~P Vecchi.
\newblock Optimization by simulated annealing.
\newblock \emph{science}, 220\penalty0 (4598):\penalty0 671--680, 1983.

\bibitem[Krizhevsky et~al.(2010)Krizhevsky, Nair, and
  Hinton]{krizhevsky2010cifar}
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
\newblock Cifar-10 (canadian institute for advanced research).
\newblock \emph{URL http://www. cs. toronto. edu/kriz/cifar. html}, 5:\penalty0
  4, 2010.

\bibitem[Kumar et~al.(2019)Kumar, Ozair, Goyal, Courville, and
  Bengio]{kumar2019maximum}
Rithesh Kumar, Sherjil Ozair, Anirudh Goyal, Aaron Courville, and Yoshua
  Bengio.
\newblock Maximum entropy generators for energy-based models.
\newblock \emph{arXiv preprint arXiv:1901.08508}, 2019.

\bibitem[Lazarow et~al.(2017)Lazarow, Jin, and Tu]{lazarow2017introspective}
Justin Lazarow, Long Jin, and Zhuowen Tu.
\newblock Introspective neural networks for generative modeling.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pages 2774--2783, 2017.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015deep}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 3730--3738, 2015.

\bibitem[Marinari and Parisi(1992)]{marinari1992simulated}
Enzo Marinari and Giorgio Parisi.
\newblock Simulated tempering: a new monte carlo scheme.
\newblock \emph{EPL (Europhysics Letters)}, 19\penalty0 (6):\penalty0 451,
  1992.

\bibitem[Neal et~al.(2011)]{neal2011mcmc}
Radford~M Neal et~al.
\newblock Mcmc using hamiltonian dynamics.
\newblock \emph{Handbook of markov chain monte carlo}, 2\penalty0
  (11):\penalty0 2, 2011.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and Ng]{37648}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y.
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In \emph{NIPS Workshop on Deep Learning and Unsupervised Feature
  Learning 2011}, 2011.

\bibitem[Nijkamp et~al.(2019)Nijkamp, Hill, Zhu, and Wu]{nijkamp2019learning}
Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Learning non-convergent non-persistent short-run mcmc toward
  energy-based model.
\newblock In \emph{Proceedings of the 33rd International Conference on Neural
  Information Processing Systems}, pages 5232--5242, 2019.

\bibitem[Nijkamp et~al.(2020)Nijkamp, Pang, Han, Zhou, Zhu, and
  Wu]{nijkamp2020learning}
Erik Nijkamp, Bo~Pang, Tian Han, Linqi Zhou, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Learning multi-layer latent variable model via variational
  optimization of short run mcmc for approximate inference.
\newblock In \emph{European Conference on Computer Vision}, pages 361--378,
  2020.

\bibitem[Oord et~al.(2017)Oord, Vinyals, and Kavukcuoglu]{oord2017neural}
Aaron van~den Oord, Oriol Vinyals, and Koray Kavukcuoglu.
\newblock Neural discrete representation learning.
\newblock \emph{arXiv preprint arXiv:1711.00937}, 2017.

\bibitem[Pang and Wu(2021)]{pang2021latent}
Bo~Pang and Ying~Nian Wu.
\newblock Latent space energy-based model of symbol-vector coupling for text
  generation and classification.
\newblock In \emph{International Conference on Machine Learning}, pages
  8359--8370. PMLR, 2021.

\bibitem[Pang et~al.(2020{\natexlab{a}})Pang, Han, Nijkamp, Zhu, and
  Wu]{pang2020ebmprior}
Bo~Pang, Tian Han, Erik Nijkamp, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Learning latent space energy-based prior model.
\newblock \emph{NeurIPS}, 2020{\natexlab{a}}.

\bibitem[Pang et~al.(2020{\natexlab{b}})Pang, Nijkamp, Cui, Han, and
  Wu]{pang2020semi}
Bo~Pang, Erik Nijkamp, Jiali Cui, Tian Han, and Ying~Nian Wu.
\newblock Semi-supervised learning by latent space energy-based model of
  symbol-vector coupling.
\newblock \emph{arXiv preprint arXiv:2010.09359}, 2020{\natexlab{b}}.

\bibitem[Papamakarios et~al.(2021)Papamakarios, Nalisnick, Rezende, Mohamed,
  and Lakshminarayanan]{papamakarios2021normalizing}
George Papamakarios, Eric Nalisnick, Danilo~Jimenez Rezende, Shakir Mohamed,
  and Balaji Lakshminarayanan.
\newblock Normalizing flows for probabilistic modeling and inference.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (57):\penalty0 1--64, 2021.

\bibitem[Patrini et~al.(2020)Patrini, van~den Berg, Forre, Carioni, Bhargav,
  Welling, Genewein, and Nielsen]{patrini2020sinkhorn}
Giorgio Patrini, Rianne van~den Berg, Patrick Forre, Marcello Carioni, Samarth
  Bhargav, Max Welling, Tim Genewein, and Frank Nielsen.
\newblock Sinkhorn autoencoders.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 733--743.
  PMLR, 2020.

\bibitem[Radford et~al.(2015)Radford, Metz, and
  Chintala]{radford2015unsupervised}
Alec Radford, Luke Metz, and Soumith Chintala.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1511.06434}, 2015.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Danilo~Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{International conference on machine learning}, pages
  1278--1286. PMLR, 2014.

\bibitem[Rhodes et~al.(2020)Rhodes, Xu, and Gutmann]{rhodes2020telescoping}
Benjamin Rhodes, Kai Xu, and Michael~U Gutmann.
\newblock Telescoping density-ratio estimation.
\newblock \emph{arXiv preprint arXiv:2006.12204}, 2020.

\bibitem[Saito et~al.(2020)Saito, Saito, Koyama, and Kobayashi]{saito2020train}
Masaki Saito, Shunta Saito, Masanori Koyama, and Sosuke Kobayashi.
\newblock Train sparsely, generate densely: Memory-efficient unsupervised
  training of high-resolution temporal gan.
\newblock \emph{International Journal of Computer Vision}, 128:\penalty0
  2586--2606, 2020.

\bibitem[Sugiyama et~al.(2012)Sugiyama, Suzuki, and
  Kanamori]{sugiyama2012density}
Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori.
\newblock \emph{Density ratio estimation in machine learning}.
\newblock Cambridge University Press, 2012.

\bibitem[Tomczak and Welling(2018)]{tomczak2018vae}
Jakub Tomczak and Max Welling.
\newblock Vae with a vampprior.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1214--1223. PMLR, 2018.

\bibitem[Vahdat and Kautz(2020)]{vahdat2020nvae}
Arash Vahdat and Jan Kautz.
\newblock {NVAE}: A deep hierarchical variational autoencoder.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Wu et~al.(2018)Wu, Gao, Han, and Zhu]{wu2018tale}
Ying~Nian Wu, Ruiqi Gao, Tian Han, and Song-Chun Zhu.
\newblock A tale of three probabilistic families: discriminative, descriptive
  and generative models.
\newblock \emph{arXiv preprint arXiv:1810.04261}, 2018.

\bibitem[Xiao et~al.(2019)Xiao, Yan, and Amit]{xiao2019generative}
Zhisheng Xiao, Qing Yan, and Yali Amit.
\newblock Generative latent flow.
\newblock \emph{arXiv preprint arXiv:1905.10485}, 2019.

\bibitem[Xiao et~al.(2020)Xiao, Yan, and Amit]{xiao2020likelihood}
Zhisheng Xiao, Qing Yan, and Yali Amit.
\newblock Likelihood regret: An out-of-distribution detection score for
  variational auto-encoder.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 20685--20696, 2020.

\bibitem[Xiao et~al.(2021)Xiao, Kreis, Kautz, and Vahdat]{xiao2020vaebm}
Zhisheng Xiao, Karsten Kreis, Jan Kautz, and Arash Vahdat.
\newblock Vaebm: A symbiosis between variational autoencoders and energy-based
  models.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Xiao et~al.(2022)Xiao, Kreis, and Vahdat]{xiao2021tackling}
Zhisheng Xiao, Karsten Kreis, and Arash Vahdat.
\newblock Tackling the generative learning trilemma with denoising diffusion
  gans.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Xie et~al.(2019)Xie, Gao, Zheng, Zhu, and Wu]{xie2019learning}
Jianwen Xie, Ruiqi Gao, Zilong Zheng, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Learning dynamic generator model by alternating back-propagation
  through time.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 5498--5507, 2019.

\bibitem[Yu et~al.(2021)Yu, Xie, Ma, Zhu, Wu, and Zhu]{yu2021unsupervised}
Peiyu Yu, Sirui Xie, Xiaojian Ma, Yixin Zhu, Ying~Nian Wu, and Song-Chun Zhu.
\newblock Unsupervised foreground extraction via deep region competition.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 14264--14279, 2021.

\bibitem[Yu et~al.(2022)Yu, Xie, Ma, Jia, Pang, Gao, Zhu, Zhu, and
  Wu]{yu2022latent}
Peiyu Yu, Sirui Xie, Xiaojian Ma, Baoxiong Jia, Bo~Pang, Ruigi Gao, Yixin Zhu,
  Song-Chun Zhu, and Ying~Nian Wu.
\newblock Latent diffusion energy-based model for interpretable text modeling.
\newblock \emph{arXiv preprint arXiv:2206.05895}, 2022.

\bibitem[Zenati et~al.(2018)Zenati, Foo, Lecouat, Manek, and
  Chandrasekhar]{zenati2018efficient}
Houssam Zenati, Chuan~Sheng Foo, Bruno Lecouat, Gaurav Manek, and
  Vijay~Ramaseshan Chandrasekhar.
\newblock Efficient gan-based anomaly detection.
\newblock \emph{arXiv preprint arXiv:1802.06222}, 2018.

\end{thebibliography}
