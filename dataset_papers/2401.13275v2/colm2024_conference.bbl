\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anonymous(2023)]{INSIDE}
Anonymous.
\newblock {INSIDE}: {LLM}s' internal states retain the power of hallucination detection.
\newblock In \emph{Submitted to The Twelfth International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=Zj12nzlQbz}.
\newblock under review.

\bibitem[Anthropic(2023)]{Claude}
Anthropic.
\newblock Introducing claude, 2023.
\newblock URL \url{https://www.anthropic.com/index/introducing-claude}.

\bibitem[Asai et~al.(2023)Asai, Wu, Wang, Sil, and Hajishirzi]{Self-RAG}
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi.
\newblock Self-rag: Learning to retrieve, generate, and critique through self-reflection.
\newblock \emph{CoRR}, abs/2310.11511, 2023.
\newblock \doi{10.48550/ARXIV.2310.11511}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2310.11511}.

\bibitem[Askell et~al.(2021)Askell, Bai, Chen, Drain, Ganguli, Henighan, Jones, Joseph, Mann, DasSarma, Elhage, Hatfield{-}Dodds, Hernandez, Kernion, Ndousse, Olsson, Amodei, Brown, Clark, McCandlish, Olah, and Kaplan]{HHH}
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Benjamin Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield{-}Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom~B. Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan.
\newblock A general language assistant as a laboratory for alignment.
\newblock \emph{CoRR}, abs/2112.00861, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.00861}.

\bibitem[Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, Joseph, Kadavath, Kernion, Conerly, Showk, Elhage, Hatfield{-}Dodds, Hernandez, Hume, Johnston, Kravec, Lovitt, Nanda, Olsson, Amodei, Brown, Clark, McCandlish, Olah, Mann, and Kaplan]{Anthropic_HH}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer~El Showk, Nelson Elhage, Zac Hatfield{-}Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom~B. Brown, Jack Clark, Sam McCandlish, Chris Olah, Benjamin Mann, and Jared Kaplan.
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback.
\newblock \emph{CoRR}, abs/2204.05862, 2022.
\newblock \doi{10.48550/ARXIV.2204.05862}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2204.05862}.

\bibitem[Baichuan(2023)]{baichuan2}
Baichuan.
\newblock Baichuan 2: Open large-scale language models.
\newblock \emph{arXiv preprint arXiv:2309.10305}, 2023.
\newblock URL \url{https://arxiv.org/abs/2309.10305}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{GPT-3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria{-}Florina Balcan, and Hsuan{-}Tien Lin (eds.), \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}.

\bibitem[Burns et~al.(2023)Burns, Ye, Klein, and Steinhardt]{discovering_latent_knowledge}
Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt.
\newblock Discovering latent knowledge in language models without supervision.
\newblock In \emph{The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}. OpenReview.net, 2023.
\newblock URL \url{https://openreview.net/pdf?id=ETKGuby0hcs}.

\bibitem[Cheng et~al.(2023)Cheng, Sun, Zhang, Wang, Liu, Zhang, He, Huang, Yin, Chen, and Qiu]{HalluQA}
Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang, Siyin Wang, Xiangyang Liu, Mozhi Zhang, Junliang He, Mianqiu Huang, Zhangyue Yin, Kai Chen, and Xipeng Qiu.
\newblock Evaluating hallucinations in chinese large language models.
\newblock \emph{CoRR}, abs/2310.03368, 2023.
\newblock \doi{10.48550/ARXIV.2310.03368}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2310.03368}.

\bibitem[Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur{-}Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei, Meier{-}Hellstern, Eck, Dean, Petrov, and Fiedel]{PaLM}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur{-}Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai, Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier{-}Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{J. Mach. Learn. Res.}, 24:\penalty0 240:1--240:113, 2023.
\newblock URL \url{http://jmlr.org/papers/v24/22-1144.html}.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and Amodei]{RLHP}
Paul~F. Christiano, Jan Leike, Tom~B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna~M. Wallach, Rob Fergus, S.~V.~N. Vishwanathan, and Roman Garnett (eds.), \emph{Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, {USA}}, pp.\  4299--4307, 2017.
\newblock URL \url{https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html}.

\bibitem[Chuang et~al.(2023)Chuang, Xie, Luo, Kim, Glass, and He]{Dola}
Yung{-}Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James~R. Glass, and Pengcheng He.
\newblock Dola: Decoding by contrasting layers improves factuality in large language models.
\newblock \emph{CoRR}, abs/2309.03883, 2023.
\newblock \doi{10.48550/ARXIV.2309.03883}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2309.03883}.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, Webson, Gu, Dai, Suzgun, Chen, Chowdhery, Narang, Mishra, Yu, Zhao, Huang, Dai, Yu, Petrov, Chi, Dean, Devlin, Roberts, Zhou, Le, and Wei]{flan-t5}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang~Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent~Y. Zhao, Yanping Huang, Andrew~M. Dai, Hongkun Yu, Slav Petrov, Ed~H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc~V. Le, and Jason Wei.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{CoRR}, abs/2210.11416, 2022.
\newblock \doi{10.48550/ARXIV.2210.11416}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2210.11416}.

\bibitem[Evans et~al.(2021)Evans, Cotton{-}Barratt, Finnveden, Bales, Balwit, Wills, Righetti, and Saunders]{TruthfulAI}
Owain Evans, Owen Cotton{-}Barratt, Lukas Finnveden, Adam Bales, Avital Balwit, Peter Wills, Luca Righetti, and William Saunders.
\newblock Truthful {AI:} developing and governing {AI} that does not lie.
\newblock \emph{CoRR}, abs/2110.06674, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.06674}.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, de~Las~Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock, Scao, Lavril, Wang, Lacroix, and Sayed]{Mistral-7b}
Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~Las~Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L{\'{e}}lio~Renard Lavaud, Marie{-}Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timoth{\'{e}}e Lacroix, and William~El Sayed.
\newblock Mistral 7b.
\newblock \emph{CoRR}, abs/2310.06825, 2023.
\newblock \doi{10.48550/ARXIV.2310.06825}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2310.06825}.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and Zettlemoyer]{TriviaQA}
Mandar Joshi, Eunsol Choi, Daniel~S. Weld, and Luke Zettlemoyer.
\newblock Triviaqa: {A} large scale distantly supervised challenge dataset for reading comprehension.
\newblock In Regina Barzilay and Min{-}Yen Kan (eds.), \emph{Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, {ACL} 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers}, pp.\  1601--1611. Association for Computational Linguistics, 2017.
\newblock \doi{10.18653/V1/P17-1147}.
\newblock URL \url{https://doi.org/10.18653/v1/P17-1147}.

\bibitem[Kadavath et~al.(2022)Kadavath, Conerly, Askell, Henighan, Drain, Perez, Schiefer, Hatfield{-}Dodds, DasSarma, Tran{-}Johnson, Johnston, Showk, Jones, Elhage, Hume, Chen, Bai, Bowman, Fort, Ganguli, Hernandez, Jacobson, Kernion, Kravec, Lovitt, Ndousse, Olsson, Ringer, Amodei, Brown, Clark, Joseph, Mann, McCandlish, Olah, and Kaplan]{Know_what_they_know}
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield{-}Dodds, Nova DasSarma, Eli Tran{-}Johnson, Scott Johnston, Sheer~El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan.
\newblock Language models (mostly) know what they know.
\newblock \emph{CoRR}, abs/2207.05221, 2022.
\newblock \doi{10.48550/ARXIV.2207.05221}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2207.05221}.

\bibitem[Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins, Parikh, Alberti, Epstein, Polosukhin, Devlin, Lee, Toutanova, Jones, Kelcey, Chang, Dai, Uszkoreit, Le, and Petrov]{NQ}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur~P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming{-}Wei Chang, Andrew~M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.
\newblock Natural questions: a benchmark for question answering research.
\newblock \emph{Trans. Assoc. Comput. Linguistics}, 7:\penalty0 452--466, 2019.
\newblock \doi{10.1162/TACL\_A\_00276}.
\newblock URL \url{https://doi.org/10.1162/tacl\_a\_00276}.

\bibitem[Li et~al.(2023)Li, Patel, Vi{\'{e}}gas, Pfister, and Wattenberg]{ITI}
Kenneth Li, Oam Patel, Fernanda~B. Vi{\'{e}}gas, Hanspeter Pfister, and Martin Wattenberg.
\newblock Inference-time intervention: Eliciting truthful answers from a language model.
\newblock \emph{CoRR}, abs/2306.03341, 2023.
\newblock \doi{10.48550/ARXIV.2306.03341}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2306.03341}.

\bibitem[Lin et~al.(2022{\natexlab{a}})Lin, Hilton, and Evans]{TruthfulQA}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland, May 22-27, 2022}, pp.\  3214--3252. Association for Computational Linguistics, 2022{\natexlab{a}}.
\newblock \doi{10.18653/v1/2022.acl-long.229}.
\newblock URL \url{https://doi.org/10.18653/v1/2022.acl-long.229}.

\bibitem[Lin et~al.(2022{\natexlab{b}})Lin, Hilton, and Evans]{express_uncertainty}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Teaching models to express their uncertainty in words.
\newblock \emph{Trans. Mach. Learn. Res.}, 2022, 2022{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=8s8K2UZGTZ}.

\bibitem[Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le, Zoph, Wei, and Roberts]{flanv2}
Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny Zhou, Quoc~V. Le, Barret Zoph, Jason Wei, and Adam Roberts.
\newblock The flan collection: Designing data and methods for effective instruction tuning, 2023.

\bibitem[Manakul et~al.(2023)Manakul, Liusie, and Gales]{SelfCheckGPT}
Potsawee Manakul, Adian Liusie, and Mark J.~F. Gales.
\newblock Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023}, pp.\  9004--9017. Association for Computational Linguistics, 2023.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.557}.

\bibitem[OpenAI(2022)]{ChatGPT}
OpenAI.
\newblock Introducing chatgpt, 2022.
\newblock URL \url{https://openai.com/blog/chatgpt}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe]{InstructGPT}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul~F. Christiano, Jan Leike, and Ryan Lowe.
\newblock Training language models to follow instructions with human feedback.
\newblock In \emph{NeurIPS}, 2022.
\newblock URL \url{http://papers.nips.cc/paper\_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html}.

\bibitem[Qwen-Team(2023)]{Qwen}
Qwen-Team.
\newblock Qwen technical report.
\newblock 2023.
\newblock URL \url{https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf}.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Ermon, Manning, and Finn]{DPO}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher~D. Manning, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock \emph{CoRR}, abs/2305.18290, 2023.
\newblock \doi{10.48550/ARXIV.2305.18290}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2305.18290}.

\bibitem[Ren et~al.(2023)Ren, Wang, Qu, Zhao, Liu, Tian, Wu, Wen, and Wang]{LLM-KB}
Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne~Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji{-}Rong Wen, and Haifeng Wang.
\newblock Investigating the factual knowledge boundary of large language models with retrieval augmentation.
\newblock \emph{CoRR}, abs/2307.11019, 2023.
\newblock \doi{10.48550/ARXIV.2307.11019}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2307.11019}.

\bibitem[Sanh et~al.(2022)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler, Raja, Dey, Bari, Xu, Thakker, Sharma, Szczechla, Kim, Chhablani, Nayak, Datta, Chang, Jiang, Wang, Manica, Shen, Yong, Pandey, Bawden, Wang, Neeraj, Rozen, Sharma, Santilli, F{\'{e}}vry, Fries, Teehan, Scao, Biderman, Gao, Wolf, and Rush]{T0}
Victor Sanh, Albert Webson, Colin Raffel, Stephen~H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M~Saiful Bari, Canwen Xu, Urmish Thakker, Shanya~Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal~V. Nayak, Debajyoti Datta, Jonathan Chang, Mike~Tian{-}Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng~Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F{\'{e}}vry, Jason~Alan Fries, Ryan Teehan, Teven~Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander~M. Rush.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In \emph{The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}. OpenReview.net, 2022.
\newblock URL \url{https://openreview.net/forum?id=9Vrb9D0WI4}.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{PPO}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{CoRR}, abs/1707.06347, 2017.
\newblock URL \url{http://arxiv.org/abs/1707.06347}.

\bibitem[Shuster et~al.(2021)Shuster, Poff, Chen, Kiela, and Weston]{Hallucination_in_Conversation}
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston.
\newblock Retrieval augmentation reduces hallucination in conversation.
\newblock In Marie{-}Francine Moens, Xuanjing Huang, Lucia Specia, and Scott~Wen{-}tau Yih (eds.), \emph{Findings of the Association for Computational Linguistics: {EMNLP} 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021}, pp.\  3784--3803. Association for Computational Linguistics, 2021.
\newblock \doi{10.18653/v1/2021.findings-emnlp.320}.
\newblock URL \url{https://doi.org/10.18653/v1/2021.findings-emnlp.320}.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, and Christiano]{Summary_RLHF}
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel~M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul~F. Christiano.
\newblock Learning to summarize from human feedback.
\newblock \emph{CoRR}, abs/2009.01325, 2020.
\newblock URL \url{https://arxiv.org/abs/2009.01325}.

\bibitem[Sun et~al.(2023)Sun, Zhang, He, Li, Cheng, Yan, Liu, Shao, Tang, Zhao, Chen, Zheng, Zhou, Li, Zhan, Zhou, Li, Yang, Wu, Yin, Huang, and Qiu]{sun2023moss}
Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Hang Yan, Xiangyang Liu, Yunfan Shao, Qiong Tang, Xingjian Zhao, Ke~Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu, Zhangyue Yin, Xuanjing Huang, and Xipeng Qiu.
\newblock Moss: Training conversational language models from synthetic data.
\newblock 2023.

\bibitem[Tian et~al.(2023)Tian, Mitchell, Yao, Manning, and Finn]{Fine-tune_for_factuality}
Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher~D. Manning, and Chelsea Finn.
\newblock Fine-tuning language models for factuality.
\newblock \emph{CoRR}, abs/2311.08401, 2023.
\newblock \doi{10.48550/ARXIV.2311.08401}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2311.08401}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`{e}}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{llama1}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie{-}Anne Lachaux, Timoth{\'{e}}e Lacroix, Baptiste Rozi{\`{e}}re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur{\'{e}}lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{CoRR}, abs/2302.13971, 2023.
\newblock \doi{10.48550/arXiv.2302.13971}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2302.13971}.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Cheng, Guo, Xu, Ding, Wang, Hu, Zhang, and Zhang]{QA-eval}
Cunxiang Wang, Sirui Cheng, Qipeng Guo, Zhikun Xu, Bowen Ding, Yidong Wang, Xiangkun Hu, Zheng Zhang, and Yue Zhang.
\newblock Evaluating open-qa evaluation, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2305.12421}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Liu, Yue, Tang, Zhang, Cheng, Yao, Gao, Hu, Qi, Wang, Yang, Wang, Xie, Zhang, and Zhang]{Factuality_Survey}
Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Jiayang Cheng, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang, Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang, and Yue Zhang.
\newblock Survey on factuality in large language models: Knowledge, retrieval and domain-specificity.
\newblock \emph{CoRR}, abs/2310.07521, 2023{\natexlab{b}}.
\newblock \doi{10.48550/ARXIV.2310.07521}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2310.07521}.

\bibitem[Wang et~al.(2023{\natexlab{c}})Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi]{Self-instruct}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language models with self-generated instructions.
\newblock In Anna Rogers, Jordan~L. Boyd{-}Graber, and Naoaki Okazaki (eds.), \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2023, Toronto, Canada, July 9-14, 2023}, pp.\  13484--13508. Association for Computational Linguistics, 2023{\natexlab{c}}.
\newblock \doi{10.18653/V1/2023.ACL-LONG.754}.
\newblock URL \url{https://doi.org/10.18653/v1/2023.acl-long.754}.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{flan}
Jason Wei, Maarten Bosma, Vincent~Y. Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M. Dai, and Quoc~V. Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}. OpenReview.net, 2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=gEZrGCozdqR}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler, Chi, Hashimoto, Vinyals, Liang, Dean, and Fedus]{Emergent_Abilities}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed~H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus.
\newblock Emergent abilities of large language models.
\newblock \emph{Trans. Mach. Learn. Res.}, 2022, 2022{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=yzkSU5zdwD}.

\bibitem[Yin et~al.(2023{\natexlab{a}})Yin, Huang, and Wan]{ALCUNA}
Xunjian Yin, Baizhou Huang, and Xiaojun Wan.
\newblock {ALCUNA:} large language models meet new knowledge.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023}, pp.\  1397--1414. Association for Computational Linguistics, 2023{\natexlab{a}}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.87}.

\bibitem[Yin et~al.(2023{\natexlab{b}})Yin, Sun, Guo, Wu, Qiu, and Huang]{Self-aware}
Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang.
\newblock Do large language models know what they don't know?
\newblock In Anna Rogers, Jordan~L. Boyd{-}Graber, and Naoaki Okazaki (eds.), \emph{Findings of the Association for Computational Linguistics: {ACL} 2023, Toronto, Canada, July 9-14, 2023}, pp.\  8653--8665. Association for Computational Linguistics, 2023{\natexlab{b}}.
\newblock \doi{10.18653/v1/2023.findings-acl.551}.
\newblock URL \url{https://doi.org/10.18653/v1/2023.findings-acl.551}.

\bibitem[Zeng et~al.(2023)Zeng, Liu, Du, Wang, Lai, Ding, Yang, Xu, Zheng, Xia, Tam, Ma, Xue, Zhai, Chen, Liu, Zhang, Dong, and Tang]{GLM130B}
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng~Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang.
\newblock {GLM-130B:} an open bilingual pre-trained model.
\newblock In \emph{The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}. OpenReview.net, 2023.
\newblock URL \url{https://openreview.net/pdf?id=-Aw0rrrPUF}.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Liu, Wong, Abbeel, and Gonzalez]{HIR}
Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter Abbeel, and Joseph~E. Gonzalez.
\newblock The wisdom of hindsight makes language models better instruction followers.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), \emph{International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  41414--41428. {PMLR}, 2023{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v202/zhang23ab.html}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Li, Cui, Cai, Liu, Fu, Huang, Zhao, Zhang, Chen, Wang, Luu, Bi, Shi, and Shi]{Hallucination_survey}
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu~Zhang, Yulong Chen, Longyue Wang, Anh~Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi.
\newblock Siren's song in the {AI} ocean: {A} survey on hallucination in large language models.
\newblock \emph{CoRR}, abs/2309.01219, 2023{\natexlab{b}}.
\newblock \doi{10.48550/ARXIV.2309.01219}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2309.01219}.

\bibitem[Zhao et~al.(2023)Zhao, Yan, Sun, Xing, Meng, Wang, Cheng, Ren, and Yin]{Knowing-What-LLMs-Do-Not-Know}
Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Chong Meng, Shuaiqiang Wang, Zhicong Cheng, Zhaochun Ren, and Dawei Yin.
\newblock Knowing what llms {DO} {NOT} know: {A} simple yet effective self-detection method.
\newblock \emph{CoRR}, abs/2310.17918, 2023.
\newblock \doi{10.48550/ARXIV.2310.17918}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2310.17918}.

\bibitem[Zou et~al.(2023)Zou, Phan, Chen, Campbell, Guo, Ren, Pan, Yin, Mazeika, Dombrowski, Goel, Li, Byun, Wang, Mallen, Basart, Koyejo, Song, Fredrikson, Kolter, and Hendrycks]{Representation_Engineering}
Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann{-}Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael~J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J.~Zico Kolter, and Dan Hendrycks.
\newblock Representation engineering: {A} top-down approach to {AI} transparency.
\newblock \emph{CoRR}, abs/2310.01405, 2023.
\newblock \doi{10.48550/ARXIV.2310.01405}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2310.01405}.

\end{thebibliography}
