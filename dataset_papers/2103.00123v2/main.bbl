\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ash et~al.(2020)Ash, Zhang, Krishnamurthy, Langford, and
  Agarwal]{ash2020deep}
Ash, J.~T., Zhang, C., Krishnamurthy, A., Langford, J., and Agarwal, A.
\newblock Deep batch active learning by diverse, uncertain gradient lower
  bounds.
\newblock In \emph{ICLR}, 2020.

\bibitem[Campbell \& Broderick(2018)Campbell and
  Broderick]{campbell2018bayesian}
Campbell, T. and Broderick, T.
\newblock Bayesian coreset construction via greedy iterative geodesic ascent.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  698--706, 2018.

\bibitem[Clarkson(2010)]{clarkson2010coresets}
Clarkson, K.~L.
\newblock Coresets, sparse greedy approximation, and the frank-wolfe algorithm.
\newblock \emph{ACM Transactions on Algorithms (TALG)}, 6\penalty0
  (4):\penalty0 1--30, 2010.

\bibitem[Coleman et~al.(2020)Coleman, Yeh, Mussmann, Mirzasoleiman, Bailis,
  Liang, Leskovec, and Zaharia]{coleman2020selection}
Coleman, C., Yeh, C., Mussmann, S., Mirzasoleiman, B., Bailis, P., Liang, P.,
  Leskovec, J., and Zaharia, M.
\newblock Selection via proxy: Efficient data selection for deep learning,
  2020.

\bibitem[Das \& Kempe(2011)Das and Kempe]{das2011submodular}
Das, A. and Kempe, D.
\newblock Submodular meets spectral: Greedy algorithms for subset selection,
  sparse approximation and dictionary selection.
\newblock \emph{arXiv preprint arXiv:1102.3975}, 2011.

\bibitem[Elenberg et~al.(2018)Elenberg, Khanna, Dimakis, Negahban,
  et~al.]{elenberg2018restricted}
Elenberg, E.~R., Khanna, R., Dimakis, A.~G., Negahban, S., et~al.
\newblock Restricted strong convexity implies weak submodularity.
\newblock \emph{The Annals of Statistics}, 46\penalty0 (6B):\penalty0
  3539--3568, 2018.

\bibitem[Feldman(2020)]{feldman2020core}
Feldman, D.
\newblock Core-sets: Updated survey.
\newblock In \emph{Sampling Techniques for Supervised or Unsupervised Tasks},
  pp.\  23--44. Springer, 2020.

\bibitem[Fujishige(2005)]{fujishige2005submodular}
Fujishige, S.
\newblock \emph{Submodular functions and optimization}.
\newblock Elsevier, 2005.

\bibitem[Gatmiry \& Gomez-Rodriguez(2018)Gatmiry and
  Gomez-Rodriguez]{gatmiry2018non}
Gatmiry, K. and Gomez-Rodriguez, M.
\newblock Non-submodular function maximization subject to a matroid constraint,
  with applications.
\newblock \emph{arXiv preprint arXiv:1811.07863}, 2018.

\bibitem[Har-Peled \& Mazumdar(2004)Har-Peled and Mazumdar]{har2004coresets}
Har-Peled, S. and Mazumdar, S.
\newblock On coresets for k-means and k-median clustering.
\newblock In \emph{Proceedings of the thirty-sixth annual ACM symposium on
  Theory of computing}, pp.\  291--300, 2004.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Jia et~al.(2018)Jia, Song, He, Wang, Rong, Zhou, Xie, Guo, Yang, Yu,
  et~al.]{jia2018highly}
Jia, X., Song, S., He, W., Wang, Y., Rong, H., Zhou, F., Xie, L., Guo, Z.,
  Yang, Y., Yu, L., et~al.
\newblock Highly scalable deep learning training system with mixed-precision:
  Training imagenet in four minutes.
\newblock \emph{arXiv preprint arXiv:1807.11205}, 2018.

\bibitem[Kaufman et~al.(1987)Kaufman, Rousseeuw, and
  Dodge]{kaufman1987clustering}
Kaufman, L., Rousseeuw, P., and Dodge, Y.
\newblock Clustering by means of medoids in statistical data analysis based on
  the, 1987.

\bibitem[Kaushal et~al.(2019)Kaushal, Iyer, Kothawade, Mahadev, Doctor, and
  Ramakrishnan]{kaushal2019learning}
Kaushal, V., Iyer, R., Kothawade, S., Mahadev, R., Doctor, K., and
  Ramakrishnan, G.
\newblock Learning from less data: A unified data subset selection and active
  learning framework for computer vision.
\newblock In \emph{2019 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pp.\  1289--1299. IEEE, 2019.

\bibitem[Killamsetty et~al.(2021)Killamsetty, Sivasubramanian, Ramakrishnan,
  and Iyer]{killamsetty2021glister}
Killamsetty, K., Sivasubramanian, D., Ramakrishnan, G., and Iyer, R.
\newblock Glister: Generalization based data subset selection for efficient and
  robust learning.
\newblock \emph{In AAAI}, 2021.

\bibitem[Kirchhoff \& Bilmes(2014)Kirchhoff and
  Bilmes]{kirchhoff2014submodularity}
Kirchhoff, K. and Bilmes, J.
\newblock Submodularity for data selection in machine translation.
\newblock In \emph{Proceedings of the 2014 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  131--141, 2014.

\bibitem[Krizhevsky(2009)]{Krizhevsky09learningmultiple}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[LeCun et~al.(1989)LeCun, Boser, Denker, Henderson, Howard, Hubbard,
  and Jackel]{lecun1989backpropagation}
LeCun, Y., Boser, B., Denker, J.~S., Henderson, D., Howard, R.~E., Hubbard, W.,
  and Jackel, L.~D.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock \emph{Neural computation}, 1\penalty0 (4):\penalty0 541--551, 1989.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{lecun2010mnist}
LeCun, Y., Cortes, C., and Burges, C.
\newblock Mnist handwritten digit database.
\newblock \emph{ATT Labs [Online]. Available:
  http://yann.lecun.com/exdb/mnist}, 2, 2010.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017sgdr}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts, 2017.

\bibitem[Mirzasoleiman et~al.(2015)Mirzasoleiman, Karbasi, Badanidiyuru, and
  Krause]{mirzasoleiman2015distributed}
Mirzasoleiman, B., Karbasi, A., Badanidiyuru, A., and Krause, A.
\newblock Distributed submodular cover: Succinctly summarizing massive data.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2881--2889, 2015.

\bibitem[Mirzasoleiman et~al.(2020{\natexlab{a}})Mirzasoleiman, Bilmes, and
  Leskovec]{mirzasoleiman2020coresets}
Mirzasoleiman, B., Bilmes, J., and Leskovec, J.
\newblock Coresets for data-efficient training of machine learning models,
  2020{\natexlab{a}}.

\bibitem[Mirzasoleiman et~al.(2020{\natexlab{b}})Mirzasoleiman, Cao, and
  Leskovec]{mirzasoleiman2020coresetsnoise}
Mirzasoleiman, B., Cao, K., and Leskovec, J.
\newblock Coresets for robust training of neural networks against noisy labels.
\newblock \emph{arXiv preprint arXiv:2011.07451}, 2020{\natexlab{b}}.

\bibitem[Nemhauser et~al.(1978)Nemhauser, Wolsey, and
  Fisher]{nemhauser1978analysis}
Nemhauser, G.~L., Wolsey, L.~A., and Fisher, M.~L.
\newblock An analysis of approximations for maximizing submodular set
  functionsâ€”i.
\newblock \emph{Mathematical programming}, 14\penalty0 (1):\penalty0 265--294,
  1978.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{Netzer2011}
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A.~Y.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{ILSVRC15}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., Berg, A.~C., and Fei-Fei, L.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 115\penalty0
  (3):\penalty0 211--252, 2015.
\newblock \doi{10.1007/s11263-015-0816-y}.

\bibitem[Schwartz et~al.(2019)Schwartz, Dodge, Smith, and
  Etzioni]{schwartz2019green}
Schwartz, R., Dodge, J., Smith, N.~A., and Etzioni, O.
\newblock Green ai.
\newblock \emph{arXiv preprint arXiv:1907.10597}, 2019.

\bibitem[{Settles}(2012)]{6813092}
{Settles}, B.
\newblock 2012.
\newblock \doi{10.2200/S00429ED1V01Y201207AIM018}.

\bibitem[Sharir et~al.(2020)Sharir, Peleg, and Shoham]{sharir2020cost}
Sharir, O., Peleg, B., and Shoham, Y.
\newblock The cost of training nlp models: A concise overview.
\newblock \emph{arXiv preprint arXiv:2004.08900}, 2020.

\bibitem[Strubell et~al.(2019)Strubell, Ganesh, and
  McCallum]{strubell2019energy}
Strubell, E., Ganesh, A., and McCallum, A.
\newblock Energy and policy considerations for deep learning in nlp.
\newblock \emph{arXiv preprint arXiv:1906.02243}, 2019.

\bibitem[Toneva et~al.(2019)Toneva, Sordoni, des Combes, Trischler, Bengio, and
  Gordon]{toneva2018an}
Toneva, M., Sordoni, A., des Combes, R.~T., Trischler, A., Bengio, Y., and
  Gordon, G.~J.
\newblock An empirical study of example forgetting during deep neural network
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=BJlxm30cKm}.

\bibitem[Wang et~al.(2019)Wang, Jiang, Chen, Xu, Zhao, Lin, and
  Wang]{wang2019e2}
Wang, Y., Jiang, Z., Chen, X., Xu, P., Zhao, Y., Lin, Y., and Wang, Z.
\newblock E2-train: Training state-of-the-art cnns with over 80\% energy
  savings.
\newblock \emph{arXiv preprint arXiv:1910.13349}, 2019.

\bibitem[Wei et~al.(2014{\natexlab{a}})Wei, Iyer, and Bilmes]{wei2014fast}
Wei, K., Iyer, R., and Bilmes, J.
\newblock Fast multi-stage submodular maximization.
\newblock In \emph{International conference on machine learning}, pp.\
  1494--1502. PMLR, 2014{\natexlab{a}}.

\bibitem[Wei et~al.(2014{\natexlab{b}})Wei, Liu, Kirchhoff, Bartels, and
  Bilmes]{wei2014submodular}
Wei, K., Liu, Y., Kirchhoff, K., Bartels, C., and Bilmes, J.
\newblock Submodular subset selection for large-scale speech training data.
\newblock In \emph{2014 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pp.\  3311--3315. IEEE, 2014{\natexlab{b}}.

\bibitem[Wei et~al.(2014{\natexlab{c}})Wei, Liu, Kirchhoff, and
  Bilmes]{wei2014unsupervised}
Wei, K., Liu, Y., Kirchhoff, K., and Bilmes, J.
\newblock Unsupervised submodular subset selection for speech data.
\newblock In \emph{2014 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pp.\  4107--4111. IEEE, 2014{\natexlab{c}}.

\bibitem[Wei et~al.(2015)Wei, Iyer, and Bilmes]{wei2015submodularity}
Wei, K., Iyer, R., and Bilmes, J.
\newblock Submodularity in data subset selection and active learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1954--1963, 2015.

\bibitem[Wilcoxon(1992)]{wilcoxon1992individual}
Wilcoxon, F.
\newblock Individual comparisons by ranking methods.
\newblock In \emph{Breakthroughs in statistics}, pp.\  196--202. Springer,
  1992.

\bibitem[Wolf(2011)]{wolf2011facility}
Wolf, G.~W.
\newblock Facility location: concepts, models, algorithms and case studies.
  series: Contributions to management science: edited by zanjirani farahani,
  reza and hekmatfar, masoud, heidelberg, germany, physica-verlag, 2009, 2011.

\bibitem[Wolsey(1982)]{wolsey1982analysis}
Wolsey, L.~A.
\newblock An analysis of the greedy algorithm for the submodular set covering
  problem.
\newblock \emph{Combinatorica}, 2\penalty0 (4):\penalty0 385--393, 1982.

\end{thebibliography}
