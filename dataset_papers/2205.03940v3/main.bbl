\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Balakrishnan et~al.(2020)Balakrishnan, Xiong, Xia, and
  Perona]{balakrishnanFace}
Balakrishnan, G., Xiong, Y., Xia, W., and Perona, P.
\newblock Towards causal benchmarking of bias in face analysis algorithms.
\newblock In \emph{European Conference on Computer Vision}, 2020.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and Telgarsky]{bartlett}
Bartlett, P.~L., Foster, D.~J., and Telgarsky, M.~J.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In \emph{Neural Information Processing Systems}, 2017.

\bibitem[Boser et~al.(1992)Boser, Guyon, and Vapnik]{boser_guyon_vapnik}
Boser, B.~E., Guyon, I.~M., and Vapnik, V.~N.
\newblock A training algorithm for optimal margin classifiers.
\newblock In \emph{Workshop on Computational Learning Theory}, 1992.

\bibitem[Cho \& Saul(2009)Cho and Saul]{choandsaul}
Cho, Y. and Saul, L.
\newblock Kernel methods for deep learning.
\newblock In \emph{Neural Information Processing Systems}, 2009.

\bibitem[Cortes \& Vapnik(1995)Cortes and Vapnik]{cortes1995support}
Cortes, C. and Vapnik, V.
\newblock Support-vector networks.
\newblock \emph{Machine Learning}, 1995.

\bibitem[de~G.~Matthews et~al.(2018)de~G.~Matthews, Hron, Rowland, Turner, and
  Ghahramani]{g.2018gaussian}
de~G.~Matthews, A.~G., Hron, J., Rowland, M., Turner, R.~E., and Ghahramani, Z.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[De~Palma et~al.(2019)De~Palma, Kiani, and Lloyd]{de2019random}
De~Palma, G., Kiani, B., and Lloyd, S.
\newblock Random deep neural networks are biased towards simple functions.
\newblock \emph{Neural Information Processing Systems}, 2019.

\bibitem[Dziugaite \& Roy(2017)Dziugaite and Roy]{danroy_nonvacuous}
Dziugaite, G.~K. and Roy, D.~M.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock In \emph{Uncertainty in Artificial Intelligence}, 2017.

\bibitem[Dziugaite et~al.(2020)Dziugaite, Drouin, Neal, Rajkumar, Caballero,
  Wang, Mitliagkas, and Roy]{dziugaite2020search}
Dziugaite, G.~K., Drouin, A., Neal, B., Rajkumar, N., Caballero, E., Wang, L.,
  Mitliagkas, I., and Roy, D.~M.
\newblock In search of robust measures of generalization.
\newblock In \emph{Neural Information Processing Systems}, 2020.

\bibitem[Elsayed et~al.(2018)Elsayed, Krishnan, Mobahi, Regan, and
  Bengio]{elsayed2018large}
Elsayed, G.~F., Krishnan, D., Mobahi, H., Regan, K., and Bengio, S.
\newblock Large margin deep networks for classification.
\newblock In \emph{Neural Information Processing Systems}, 2018.

\bibitem[Geiger et~al.(2020)Geiger, Spigler, Jacot, and Wyart]{Geiger_2020}
Geiger, M., Spigler, S., Jacot, A., and Wyart, M.
\newblock Disentangling feature and lazy training in deep neural networks.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment}, 2020.

\bibitem[Herbrich \& Graepel(2001)Herbrich and Graepel]{why-svms-work}
Herbrich, R. and Graepel, T.
\newblock A {PAC-B}ayesian margin bound for linear classifiers: Why {SVM}s
  work.
\newblock In \emph{Neural Information Processing Systems}, 2001.

\bibitem[Hui \& Belkin(2021)Hui and Belkin]{HuiSquareCrossEntropy}
Hui, L. and Belkin, M.
\newblock Evaluation of neural architectures trained with square loss vs.\
  cross-entropy in classification tasks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{NTKjacot}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Neural Information Processing Systems}, 2018.

\bibitem[Jiang et~al.(2019)Jiang, Krishnan, Mobahi, and
  Bengio]{jiang2018predicting}
Jiang, Y., Krishnan, D., Mobahi, H., and Bengio, S.
\newblock Predicting the generalization gap in deep networks with margin
  distributions.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Jiang et~al.(2020)Jiang, Neyshabur, Mobahi, Krishnan, and
  Bengio]{jiang2019fantastic}
Jiang, Y., Neyshabur, B., Mobahi, H., Krishnan, D., and Bengio, S.
\newblock Fantastic generalization measures and where to find them.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{Keskar2017OnLT}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{NIPS2017_9ef2ed4b}
Lakshminarayanan, B., Pritzel, A., and Blundell, C.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In \emph{Neural Information Processing Systems}, 2017.

\bibitem[Langford \& Shawe-Taylor(2003)Langford and
  Shawe-Taylor]{langford2003pac}
Langford, J. and Shawe-Taylor, J.
\newblock {PAC-B}ayes \& margins.
\newblock \emph{Neural Information Processing Systems}, 2003.

\bibitem[Lee et~al.(2018)Lee, Sohl-Dickstein, Pennington, Novak, Schoenholz,
  and Bahri]{lee2018deep}
Lee, J., Sohl-Dickstein, J., Pennington, J., Novak, R., Schoenholz, S., and
  Bahri, Y.
\newblock Deep neural networks as {G}aussian processes.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{NEURIPS2018_a41b3bb3}
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{Neural Information Processing Systems}, 2018.

\bibitem[Liu et~al.(2021)Liu, Bernstein, Meister, and Yue]{pmlr-v139-liu21c}
Liu, Y., Bernstein, J., Meister, M., and Yue, Y.
\newblock Learning by turning: Neural architecture aware optimisation.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[McAllester(1999)]{mcallester1999some}
McAllester, D.~A.
\newblock Some {PAC-B}ayesian theorems.
\newblock \emph{Machine Learning}, 1999.

\bibitem[Mehta et~al.(2021)Mehta, Cutkosky, and Neyshabur]{extremeMemorization}
Mehta, H., Cutkosky, A., and Neyshabur, B.
\newblock Extreme memorization via scale of initialization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Mingard et~al.(2021)Mingard, Valle-P{\'e}rez, Skalse, and
  Louis]{mingard2021sgd}
Mingard, C., Valle-P{\'e}rez, G., Skalse, J., and Louis, A.~A.
\newblock Is {SGD} a {B}ayesian sampler? {W}ell, almost.
\newblock \emph{Journal of Machine Learning Research}, 2021.

\bibitem[Nagarajan \& Kolter(2017)Nagarajan and
  Kolter]{nagarajan2019generalization}
Nagarajan, V. and Kolter, J.~Z.
\newblock Generalization in deep networks: The role of distance from
  initialization.
\newblock In \emph{NeurIPS Workshop on Deep Learning: Bridging Theory and
  Practice}, 2017.

\bibitem[Nagarajan \& Kolter(2019)Nagarajan and Kolter]{NEURIPS2019_05e97c20}
Nagarajan, V. and Kolter, J.~Z.
\newblock Uniform convergence may be unable to explain generalization in deep
  learning.
\newblock In \emph{Neural Information Processing Systems}, 2019.

\bibitem[Nakkiran et~al.(2020)Nakkiran, Kaplun, Bansal, Yang, Barak, and
  Sutskever]{Nakkiran2020Deep}
Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., and Sutskever, I.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Neal(1994)]{radford}
Neal, R.~M.
\newblock \emph{Bayesian Learning for Neural Networks}.
\newblock {Ph.D.} thesis, Department of Computer Science, University of
  Toronto, 1994.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and
  Srebro]{neyshabur2015norm}
Neyshabur, B., Tomioka, R., and Srebro, N.
\newblock Norm-based capacity control in neural networks.
\newblock In \emph{Conference on Learning Theory}, 2015.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, and
  Srebro]{neyshabur2017pac}
Neyshabur, B., Bhojanapalli, S., and Srebro, N.
\newblock A {PAC-B}ayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[P{\'e}rez \& Louis(2020)P{\'e}rez and Louis]{Prez2020GeneralizationBF}
P{\'e}rez, G.~V. and Louis, A.~A.
\newblock Generalization bounds for deep learning.
\newblock \emph{arXiv:2012.04115}, 2020.

\bibitem[Rasmussen \& Williams(2005)Rasmussen and Williams]{gpml}
Rasmussen, C.~E. and Williams, C. K.~I.
\newblock \emph{Gaussian Processes for Machine Learning}.
\newblock MIT Press, 2005.

\bibitem[Rivasplata et~al.(2020)Rivasplata, Kuzborskij, Szepesvári, and
  Shawe-Taylor]{rivasplataKSS20}
Rivasplata, O., Kuzborskij, I., Szepesvári, C., and Shawe-Taylor, J.
\newblock {PAC-B}ayes analysis beyond the usual bounds.
\newblock In \emph{Neural Information Processing Systems}, 2020.

\bibitem[Rosset et~al.(2003)Rosset, Zhu, and Hastie]{rosset2003margin}
Rosset, S., Zhu, J., and Hastie, T.
\newblock Margin maximizing loss functions.
\newblock In \emph{Neural Information Processing Systems}, 2003.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Soudry, D., Hoffer, E., Nacson, M.~S., Gunasekar, S., and Srebro, N.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{Journal of Machine Learning Research}, 2018.

\bibitem[Valle-Perez et~al.(2019)Valle-Perez, Camargo, and
  Louis]{valle_perez2018deep}
Valle-Perez, G., Camargo, C.~Q., and Louis, A.~A.
\newblock Deep learning generalizes because the parameter--function map is
  biased towards simple functions.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Vapnik(1999)]{vapnik1999nature}
Vapnik, V.
\newblock \emph{The Nature of Statistical Learning Theory}.
\newblock Springer, 1999.

\bibitem[Woodworth et~al.(2020)Woodworth, Gunasekar, Lee, Moroshko, Savarese,
  Golan, Soudry, and Srebro]{woodworth20a}
Woodworth, B., Gunasekar, S., Lee, J.~D., Moroshko, E., Savarese, P., Golan,
  I., Soudry, D., and Srebro, N.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In \emph{Conference on Learning Theory}, 2020.

\bibitem[Wu et~al.(2017)Wu, Zhu, and Weinan]{Wu2017TowardsUG}
Wu, L., Zhu, Z., and Weinan, E.
\newblock Towards understanding generalization of deep learning: Perspective of
  loss landscapes.
\newblock In \emph{ICML Workshop on Principled Approaches to Deep Learning},
  2017.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2021understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock \emph{Communications of the ACM}, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Reid, Pérez, and
  Louis]{zhang2021flatness}
Zhang, S., Reid, I., Pérez, G.~V., and Louis, A.
\newblock Why flatness does and does not correlate with generalization for deep
  neural networks.
\newblock \emph{arXiv:2103.06219}, 2021{\natexlab{b}}.

\end{thebibliography}
