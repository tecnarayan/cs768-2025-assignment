@inproceedings{zintgraf2020varibad,
  title={VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning},
  author={Zintgraf, Luisa and Shiarlis, Kyriacos and Igl, Maximilian and Schulze, Sebastian and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon},
  booktitle={International Conference on Learning Representation (ICLR)},
  year={2020}}
  
  @inproceedings{deisenroth2011pilco,
  title={PILCO: A model-based and data-efficient approach to policy search},
  author={Deisenroth, Marc and Rasmussen, Carl E},
  booktitle={Proceedings of the 28th International Conference on machine learning (ICML-11)},
  pages={465--472},
  year={2011},
  organization={Citeseer}
}

@article{diaconis1986consistency,
  title={On the consistency of Bayes estimates},
  author={Diaconis, Persi and Freedman, David},
  journal={The Annals of Statistics},
  pages={1--26},
  year={1986},
  publisher={JSTOR}
}

@article{diaconis1986inconsistent,
  title={On inconsistent Bayes estimates of location},
  author={Diaconis, Persi and Freedman, David},
  journal={The Annals of Statistics},
  pages={68--87},
  year={1986},
  publisher={JSTOR}
}

@book{martin1967bayesian,
  title={Bayesian decision problems and Markov chains},
  author={Martin, James John},
  year={1967},
  publisher={Wiley}
}

@book{duff2002optimal,
  title={Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes},
  author={Duff, Michael O'Gordon},
  year={2002},
  publisher={University of Massachusetts Amherst}
}

@article{adams2007bayesian,
  title={Bayesian online changepoint detection},
  author={Adams, Ryan Prescott and MacKay, David JC},
  journal={arXiv preprint arXiv:0710.3742},
  year={2007}
}

@article{kaelbling1998planning,
  title={Planning and acting in partially observable stochastic domains},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Cassandra, Anthony R},
  journal={Artificial intelligence},
  volume={101},
  number={1-2},
  pages={99--134},
  year={1998},
  publisher={Elsevier}
}

@article{fellows2021bayesian,
  title={Bayesian Bellman Operators},
  author={Fellows, Mattie and Hartikainen, Kristian and Whiteson, Shimon},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{xie2021deep,
  title={Deep Reinforcement Learning amidst Continual Structured Non-Stationarity},
  author={Xie, Annie and Harrison, James and Finn, Chelsea},
  booktitle={International Conference on Machine Learning},
  pages={11393--11403},
  year={2021},
  organization={PMLR}
}

@inproceedings{chandak2020optimizing,
  title={Optimizing for the future in non-stationary mdps},
  author={Chandak, Yash and Theocharous, Georgios and Shankar, Shiv and White, Martha and Mahadevan, Sridhar and Thomas, Philip},
  booktitle={International Conference on Machine Learning},
  pages={1414--1425},
  year={2020},
  organization={PMLR}
}

@article{chandak2020towards,
  title={Towards safe policy improvement for non-stationary MDPs},
  author={Chandak, Yash and Jordan, Scott and Theocharous, Georgios and White, Martha and Thomas, Philip S},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9156--9168},
  year={2020}
}

@article{xie2022robust,
  title={Robust Policy Learning over Multiple Uncertainty Sets},
  author={Xie, Annie and Sodhani, Shagun and Finn, Chelsea and Pineau, Joelle and Zhang, Amy},
  journal={arXiv preprint arXiv:2202.07013},
  year={2022}
}

@article{sodhani2021block,
  title={Block Contextual MDPs for Continual Learning},
  author={Sodhani, Shagun and Meier, Franziska and Pineau, Joelle and Zhang, Amy},
  journal={arXiv preprint arXiv:2110.06972},
  year={2021}
}

@article{hallak2015contextual,
  title={Contextual markov decision processes},
  author={Hallak, Assaf and Di Castro, Dotan and Mannor, Shie},
  journal={arXiv preprint arXiv:1502.02259},
  year={2015}
}

@article{ren2022reinforcement,
  title={Reinforcement Learning in Presence of Discrete Markovian Context Evolution},
  author={Ren, Hang and Sootla, Aivar and Jafferjee, Taher and Shen, Junxiao and Wang, Jun and Bou-Ammar, Haitham},
  journal={arXiv preprint arXiv:2202.06557},
  year={2022}
}

@article{poiani2021meta,
  title={Meta-Reinforcement Learning by Tracking Task Non-stationarity},
  author={Poiani, Riccardo and Tirinzoni, Andrea and Restelli, Marcello},
  journal={arXiv preprint arXiv:2105.08834},
  year={2021}
}

@article{al2017continuous,
  title={Continuous adaptation via meta-learning in nonstationary and competitive environments},
  author={Al-Shedivat, Maruan and Bansal, Trapit and Burda, Yuri and Sutskever, Ilya and Mordatch, Igor and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1710.03641},
  year={2017}
}

@article{kamienny2020learning,
  title={Learning adaptive exploration strategies in dynamic environments through informed policy regularization},
  author={Kamienny, Pierre-Alexandre and Pirotta, Matteo and Lazaric, Alessandro and Lavril, Thibault and Usunier, Nicolas and Denoyer, Ludovic},
  journal={arXiv preprint arXiv:2005.02934},
  year={2020}
}

@inproceedings{kumar2021rma,
title={Rma: Rapid motor adaptation for legged robots},
author={Kumar, Ashish and Fu, Zipeng and Pathak, Deepak and Malik, Jitendra},
journal={Robotics: Science and Systems},
year={2021}
}

@article{nagabandi2018deep,
  title={Deep online learning via meta-learning: Continual adaptation for model-based RL},
  author={Nagabandi, Anusha and Finn, Chelsea and Levine, Sergey},
  journal={arXiv preprint arXiv:1812.07671},
  year={2018}
}

@article{feng2022factored,
  title={Factored Adaptation for Non-Stationary Reinforcement Learning},
  author={Feng, Fan and Huang, Biwei and Zhang, Kun and Magliacane, Sara},
  journal={arXiv preprint arXiv:2203.16582},
  year={2022}
}

@article{alegre2021minimum,
  title={Minimum-delay adaptation in non-stationary reinforcement learning via online high-confidence change-point detection},
  author={Alegre, Lucas N and Bazzan, Ana LC and da Silva, Bruno C},
  journal={arXiv preprint arXiv:2105.09452},
  year={2021}
}

@article{zhao2020meld,
  title={Meld: Meta-reinforcement learning from images via latent state models},
  author={Zhao, Tony Z and Nagabandi, Anusha and Rakelly, Kate and Finn, Chelsea and Levine, Sergey},
  journal={arXiv preprint arXiv:2010.13957},
  year={2020}
}

@inproceedings{rakelly2019efficient,
  title={Efficient off-policy meta-reinforcement learning via probabilistic context variables},
  author={Rakelly, Kate and Zhou, Aurick and Finn, Chelsea and Levine, Sergey and Quillen, Deirdre},
  booktitle={International conference on machine learning},
  pages={5331--5340},
  year={2019},
  organization={PMLR}
}

@inproceedings{cassandra1994acting,
  title={Acting optimally in partially observable stochastic domains},
  author={Cassandra, Anthony R and Kaelbling, Leslie Pack and Littman, Michael L},
  booktitle={Aaai},
  volume={94},
  pages={1023--1028},
  year={1994}
}

@article{mnih2013atari,
  abstract = {We present the first deep learning model to successfully learn control
policies directly from high-dimensional sensory input using reinforcement
learning. The model is a convolutional neural network, trained with a variant
of Q-learning, whose input is raw pixels and whose output is a value function
estimating future rewards. We apply our method to seven Atari 2600 games from
the Arcade Learning Environment, with no adjustment of the architecture or
learning algorithm. We find that it outperforms all previous approaches on six
of the games and surpasses a human expert on three of them.},
  added-at = {2019-07-12T20:11:01.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/2a00ec4c09f5dc9b3f8a1836f4e02bb5d/lanteunis},
  interhash = {78966703f649bae69a08a6a23a4e8879},
  intrahash = {a00ec4c09f5dc9b3f8a1836f4e02bb5d},
  keywords = {},
  note = {cite arxiv:1312.5602Comment: NIPS Deep Learning Workshop 2013},
  timestamp = {2019-07-12T20:11:01.000+0200},
  title = {Playing Atari with Deep Reinforcement Learning},
  url = {http://arxiv.org/abs/1312.5602},
  year = 2013
}


@article{bellemare2012ale,
  abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a
challenge problem and a platform and methodology for evaluating the development
of general, domain-independent AI technology. ALE provides an interface to
hundreds of Atari 2600 game environments, each one different, interesting, and
designed to be a challenge for human players. ALE presents significant research
challenges for reinforcement learning, model learning, model-based planning,
imitation learning, transfer learning, and intrinsic motivation. Most
importantly, it provides a rigorous testbed for evaluating and comparing
approaches to these problems. We illustrate the promise of ALE by developing
and benchmarking domain-independent agents designed using well-established AI
techniques for both reinforcement learning and planning. In doing so, we also
propose an evaluation methodology made possible by ALE, reporting empirical
results on over 55 different games. All of the software, including the
benchmark agents, is publicly available.},
  added-at = {2019-12-30T13:47:50.000+0100},
  author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/20ab3dc05ba807da8085ca30679390cde/lanteunis},
  doi = {10.1613/jair.3912},
  interhash = {ad133244b4e33eda52c994d9b6174f27},
  intrahash = {0ab3dc05ba807da8085ca30679390cde},
  journal = {Journal of Artificial Intelligence Research},
  keywords = {ALE Atari DRLAlgoComparison ReinforcementLearning reinforcement},
  note = {cite arxiv:1207.4708},
  pages = {253-279},
  timestamp = {2019-12-30T13:55:38.000+0100},
  title = {The Arcade Learning Environment: An Evaluation Platform for General
  Agents},
  url = {http://arxiv.org/abs/1207.4708},
  volume = {Vol. 47},
  year = 2012
}


@article{hessel2017rainbow,
  abstract = {The deep reinforcement learning community has made several independent
improvements to the DQN algorithm. However, it is unclear which of these
extensions are complementary and can be fruitfully combined. This paper
examines six extensions to the DQN algorithm and empirically studies their
combination. Our experiments show that the combination provides
state-of-the-art performance on the Atari 2600 benchmark, both in terms of data
efficiency and final performance. We also provide results from a detailed
ablation study that shows the contribution of each component to overall
performance.},
  added-at = {2020-03-03T23:40:35.000+0100},
  author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  biburl = {https://www.bibsonomy.org/bibtex/223e587e36693531fa1a5bd1f8d2a1bdd/kirk86},
  description = {[1710.02298] Rainbow: Combining Improvements in Deep Reinforcement Learning},
  interhash = {f4fb4d30fac6e6290d70a94e7420777a},
  intrahash = {23e587e36693531fa1a5bd1f8d2a1bdd},
  keywords = {reinforcement-learning},
  note = {cite arxiv:1710.02298Comment: Under review as a conference paper at AAAI 2018},
  timestamp = {2020-03-03T23:40:35.000+0100},
  title = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
  url = {http://arxiv.org/abs/1710.02298},
  year = 2017
}

@article{schulman2017ppo,
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time. },
  added-at = {2019-12-16T18:31:56.000+0100},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  biburl = {https://www.bibsonomy.org/bibtex/24bbcce6aa1c42ae7f61ef8cf5475aa85/lanteunis},
  ee = {http://arxiv.org/abs/1707.06347},
  interhash = {f57ff463a90dbafb77d55a25aea8355c},
  intrahash = {4bbcce6aa1c42ae7f61ef8cf5475aa85},
  journal = {CoRR},
  keywords = {DRLAlgoComparison ppo reinforcement_learning},
  timestamp = {2019-12-18T21:15:59.000+0100},
  title = {Proximal Policy Optimization Algorithms.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1707.html#SchulmanWDRK17},
  volume = {abs/1707.06347},
  year = 2017
}

@inproceedings{schulman2015trpo,
  abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters. },
  added-at = {2019-12-16T18:19:24.000+0100},
  author = {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael I. and Moritz, Philipp},
  biburl = {https://www.bibsonomy.org/bibtex/27462ef83a282ebeba5963e16176f9e29/lanteunis},
  booktitle = {ICML},
  editor = {Bach, Francis R. and Blei, David M.},
  ee = {http://proceedings.mlr.press/v37/schulman15.html},
  interhash = {a8e265b2ea2ac6ead6b6e17a5a4bdc09},
  intrahash = {7462ef83a282ebeba5963e16176f9e29},
  keywords = {DRLAlgoComparison reinforcement_learning},
  pages = {1889-1897},
  publisher = {JMLR.org},
  series = {JMLR Workshop and Conference Proceedings},
  timestamp = {2019-12-16T21:10:27.000+0100},
  title = {Trust Region Policy Optimization.},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2015.html#SchulmanLAJM15},
  volume = 37,
  year = 2015
}

@article{asmuth2012learning,
  title={Learning is planning: near Bayes-optimal reinforcement learning via Monte-Carlo tree search},
  author={Asmuth, John and Littman, Michael L},
  journal={arXiv preprint arXiv:1202.3699},
  year={2012}
}

@article{guez2012efficient,
  title={Efficient Bayes-adaptive reinforcement learning using sample-based search},
  author={Guez, Arthur and Silver, David and Dayan, Peter},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@article{humplik2019meta,
  title={Meta reinforcement learning as task inference},
  author={Humplik, Jan and Galashov, Alexandre and Hasenclever, Leonard and Ortega, Pedro A and Teh, Yee Whye and Heess, Nicolas},
  journal={arXiv preprint arXiv:1905.06424},
  year={2019}
}

@inproceedings{choi1999hidden,
  title={Hidden-mode Markov decision processes},
  author={Choi, Samuel PM and Yeung, Dit-Yan and Zhang, Nevin Lianwen},
  booktitle={Proceedings of the 16th international joint conference on artificial intelligence (IJCAI-99), workshop on neural symbolic, and reinforcement methods for sequence learning, Stockholm, Sweden},
  year={1999}
}

@article{krishnan2015deep,
  title={Deep kalman filters},
  author={Krishnan, Rahul G and Shalit, Uri and Sontag, David},
  journal={arXiv preprint arXiv:1511.05121},
  year={2015}
}

@article{karl2016deep,
  title={Deep variational bayes filters: Unsupervised learning of state space models from raw data},
  author={Karl, Maximilian and Soelch, Maximilian and Bayer, Justin and Van der Smagt, Patrick},
  journal={arXiv preprint arXiv:1605.06432},
  year={2016}
}

@inproceedings{doerr2018probabilistic,
  title={Probabilistic recurrent state-space models},
  author={Doerr, Andreas and Daniel, Christian and Schiegg, Martin and Duy, Nguyen-Tuong and Schaal, Stefan and Toussaint, Marc and Sebastian, Trimpe},
  booktitle={International Conference on Machine Learning},
  pages={1280--1289},
  year={2018},
  organization={PMLR}
}

@article{buesing2018learning,
  title={Learning and querying fast generative models for reinforcement learning},
  author={Buesing, Lars and Weber, Theophane and Racaniere, S{\'e}bastien and Eslami, SM and Rezende, Danilo and Reichert, David P and Viola, Fabio and Besse, Frederic and Gregor, Karol and Hassabis, Demis and others},
  journal={arXiv preprint arXiv:1802.03006},
  year={2018}
}

@article{ha2018world,
  title={World models},
  author={Ha, David and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:1803.10122},
  year={2018}
}

@misc{han2019variational,
      title={Variational Recurrent Models for Solving Partially Observable Control Tasks}, 
      author={Dongqi Han and Kenji Doya and Jun Tani},
      year={2019},
      eprint={1912.10703},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hafner2019learning,
      title={Learning Latent Dynamics for Planning from Pixels}, 
      author={Danijar Hafner and Timothy Lillicrap and Ian Fischer and Ruben Villegas and David Ha and Honglak Lee and James Davidson},
      year={2019},
      eprint={1811.04551},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{hafner2019dream,
  title={Dream to control: Learning behaviors by latent imagination},
  author={Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  journal={arXiv preprint arXiv:1912.01603},
  year={2019}
}

@misc{hafner2021mastering,
      title={Mastering Atari with Discrete World Models}, 
      author={Danijar Hafner and Timothy Lillicrap and Mohammad Norouzi and Jimmy Ba},
      year={2021},
      eprint={2010.02193},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{hausknecht2015deep,
  title={Deep recurrent q-learning for partially observable mdps},
  author={Hausknecht, Matthew and Stone, Peter},
  booktitle={2015 aaai fall symposium series},
  year={2015}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ international conference on intelligent robots and systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@inproceedings{akhtar2018oboe,
  title={Oboe: Auto-tuning video ABR algorithms to network conditions},
  author={Akhtar, Zahaib and Nam, Yun Seong and Govindan, Ramesh and Rao, Sanjay and Chen, Jessica and Katz-Bassett, Ethan and Ribeiro, Bruno and Zhan, Jibin and Zhang, Hui},
  booktitle={Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
  pages={44--58},
  year={2018}
}

@inproceedings{zhang2001constancy,
  title={On the constancy of Internet path properties},
  author={Zhang, Yin and Duffield, Nick},
  booktitle={Proceedings of the 1st ACM SIGCOMM Workshop on Internet Measurement},
  pages={197--211},
  year={2001}
}

@inproceedings{eo2022opennetlab,
author = {Eo, Jeongyoon and Niu, Zhixiong and Cheng, Wenxue and Yan, Francis Y. and Gao, Rui and Kardhashi, Jorina and Inglis, Scott and Revow, Michael and Chun, Byung-Gon and Cheng, Peng and Xiong, Yongqiang},
title = {OpenNetLab: Open Platform for RL-based Congestion Control for Real-Time Communications},
booktitle = {APNet 2022},
year = {2022},
month = {July},
abstract = {With the growing importance of real-time communications (RTC), designing congestion control (CC) algorithms for RTC that achieve high network performance and QoE is gaining attention. Recently, data-driven, reinforcement learning (RL)-based CC algorithms for RTC have shown great potential, outperforming traditional rule-based counterparts. However, there are no open platforms tailored for training, evaluation, and validation of the algorithms that can facilitate this emerging research area. We present OpenNetLab, an open platform for fast training, reproducible end-to-end evaluation, and performance validation of RL-based CC algorithms for RTC. Preliminary use cases confirm that OpenNetLab concretely aided the training of novel RL-based CC algorithms for RTC that outperform a well-established rule-based baseline in both network performance and QoE metrics.},
url = {https://www.microsoft.com/en-us/research/publication/opennetlab-open-platform-for-rl-based-congestion-control-for-real-time-communications/},
}

@inproceedings{alphartc,
  title={AlphaRTC},
  author={OpenNetLab},
  booktitle={https://github.com/OpenNetLab/AlphaRTC/},
  year={2021}
}

@article{fang2019reinforcement,
  title={Reinforcement learning for bandwidth estimation and congestion control in real-time communications},
  author={Fang, Joyce and Ellis, Martin and Li, Bin and Liu, Siyao and Hosseinkashi, Yasaman and Revow, Michael and Sadovnikov, Albert and Liu, Ziyuan and Cheng, Peng and Ashok, Sachin and others},
  journal={arXiv preprint arXiv:1912.02222},
  year={2019}
}