\begin{thebibliography}{10}

\bibitem{abbe_2022_staircase}
Emmanuel Abbe, Enric~Boix Adsera, and Theodor Misiakiewicz.
\newblock The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks.
\newblock In {\em Conference on Learning Theory}, pages 4782--4887. PMLR, 2022.

\bibitem{abbe_2021_staircase}
Emmanuel Abbe, Enric Boix-Adser{\`a}, Matthew~Stewart Brennan, Guy Bresler, and Dheeraj~Mysore Nagaraj.
\newblock The staircase property: How hierarchical structure can guide deep learning.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan, editors, {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{advani_2017_independent_diag}
Madhu~S. Advani and Andrew~M. Saxe.
\newblock High-dimensional dynamics of generalization error in neural networks, 2017.

\bibitem{alex2014isotropic}
Bloemendal Alex, L{\'a}szl{\'o} Erdos, Antti Knowles, Horng-Tzer Yau, and Jun Yin.
\newblock Isotropic local laws for sample covariance and generalized wigner matrices.
\newblock 2014.

\bibitem{Allen-Zhu2018}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock pages 242--252, 2019.

\bibitem{arora_2018_DLN_convergence}
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu.
\newblock A convergence analysis of gradient descent for deep linear neural networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{arora_2018_depth_speed_Lp}
Sanjeev Arora, Nadav Cohen, and Elad Hazan.
\newblock On the optimization of deep networks: Implicit acceleration by overparameterization.
\newblock In Jennifer Dy and Andreas Krause, editors, {\em Proceedings of the 35th International Conference on Machine Learning}, volume~80 of {\em Proceedings of Machine Learning Research}, pages 244--253. PMLR, 10--15 Jul 2018.

\bibitem{arora_2019_matrix_factorization}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{exact_arora2019}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{arous2022summary_stats}
Gerard~Ben Arous, Reza Gheissari, and Aukosh Jagannath.
\newblock High-dimensional limit theorems for {SGD}: Effective dynamics and critical scaling.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{bach2017_F1_norm}
Francis Bach.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock {\em The Journal of Machine Learning Research}, 18(1):629--681, 2017.

\bibitem{bhatia2013matrix}
Rajendra Bhatia.
\newblock {\em Matrix analysis}, volume 169.
\newblock Springer Science \& Business Media, 2013.

\bibitem{bordelon2022deep_MF}
Blake Bordelon and Cengiz Pehlevan.
\newblock Self-consistent dynamical field theory of kernel evolution in wide neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 35:32240--32256, 2022.

\bibitem{Braun_2022_exact_DLN}
Lukas Braun, Cl\'{e}mentine Domin\'{e}, James Fitzgerald, and Andrew Saxe.
\newblock Exact learning dynamics of deep linear networks with prior knowledge.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh, editors, {\em Advances in Neural Information Processing Systems}, volume~35, pages 6615--6629. Curran Associates, Inc., 2022.

\bibitem{chizat2018note}
Lenaic Chizat and Francis Bach.
\newblock A note on lazy training in supervised differentiable programming.
\newblock {\em arXiv preprint arXiv:1812.07956}, 2018.

\bibitem{Chizat2018}
L\'{e}na\"{\i}c Chizat and Francis Bach.
\newblock {On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport}.
\newblock In {\em Advances in Neural Information Processing Systems 31}, pages 3040--3050. Curran Associates, Inc., 2018.

\bibitem{chizat_2020_implicit_bias}
L\'ena\"ic Chizat and Francis Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss.
\newblock In Jacob Abernethy and Shivani Agarwal, editors, {\em Proceedings of Thirty Third Conference on Learning Theory}, volume 125 of {\em Proceedings of Machine Learning Research}, pages 1305--1338. PMLR, 09--12 Jul 2020.

\bibitem{chizat_2022_mean_field_DLN}
L{\'e}na{\"\i}c Chizat, Maria Colombo, Xavier Fern{\'a}ndez-Real, and Alessio Figalli.
\newblock Infinite-width limit of deep linear neural networks.
\newblock {\em Communications on Pure and Applied Mathematics}, 2022.

\bibitem{dai_2021_repres_cost_DLN}
Zhen Dai, Mina Karzand, and Nathan Srebro.
\newblock Representation costs of linear neural networks: Analysis and design.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan, editors, {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{Du2019}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{Fukumizu1998_solution_shallow_DLN}
Kenji Fukumizu.
\newblock Effect of batch learning in multilayer neural networks.
\newblock In {\em International Conference on Neural Information Processing}, 1998.

\bibitem{geiger2019disentangling}
Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart.
\newblock Disentangling feature and lazy training in deep neural networks.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment}, 2020(11):113301, 2020.

\bibitem{gidel_2019_independent_diag}
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien.
\newblock Implicit regularization of discrete gradient dynamics in linear neural networks.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{jacot_2023_bottleneck2}
Arthur Jacot.
\newblock Bottleneck structure in learned features: Low-dimension vs regularity tradeoff.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, {\em Advances in Neural Information Processing Systems}, volume~36, pages 23607--23629. Curran Associates, Inc., 2023.

\bibitem{jacot_2022_BN_rank_short}
Arthur Jacot.
\newblock Implicit bias of large depth networks: a notion of rank for nonlinear functions.
\newblock {\em ICLR}, 2023.

\bibitem{jacot_2022_BN_rank}
Arthur Jacot.
\newblock Implicit bias of large depth networks: a notion of rank for nonlinear functions.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl\'ement Hongler.
\newblock {Neural Tangent Kernel: Convergence and Generalization in Neural Networks}.
\newblock In {\em Advances in Neural Information Processing Systems 31}, pages 8580--8589. Curran Associates, Inc., 2018.

\bibitem{jacot-2021-DLN-Saddle}
Arthur Jacot, Fran\c{c}ois Ged, Berfin \c{S}im\c{s}ek, Cl\'ement Hongler, and Franck Gabriel.
\newblock Saddle-to-saddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity, 2022.

\bibitem{jacot_2022_L2_reformulation}
Arthur Jacot, Eugene Golikov, Cl{\'e}ment Hongler, and Franck Gabriel.
\newblock Feature learning in $l_{2}$-regularized dnns: Attraction/repulsion and sparsity.
\newblock In {\em Advances in Neural Information Processing Systems}, volume~36, 2022.

\bibitem{Ji_2018_directional}
Ziwei Ji and Matus Telgarsky.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock {\em CoRR}, abs/1810.02032, 2018.

\bibitem{jin_2023_incremental_learning}
Jikai Jin, Zhiyuan Li, Kaifeng Lyu, Simon~Shaolei Du, and Jason~D. Lee.
\newblock Understanding incremental learning of gradient descent: A fine-grained analysis of matrix sensing.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, {\em Proceedings of the 40th International Conference on Machine Learning}, volume 202 of {\em Proceedings of Machine Learning Research}, pages 15200--15238. PMLR, 23--29 Jul 2023.

\bibitem{kunin_2024_richquickexactsolutions}
Daniel Kunin, Allan Raventós, Clémentine Dominé, Feng Chen, David Klindt, Andrew Saxe, and Surya Ganguli.
\newblock Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning, 2024.

\bibitem{lewkowycz_2020_large_lr}
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari.
\newblock The large learning rate phase of deep learning: the catapult mechanism.
\newblock {\em arXiv preprint arXiv:2003.02218}, 2020.

\bibitem{lewkowycz2020_L2_training}
Aitor Lewkowycz and Guy Gur-Ari.
\newblock On the training dynamics of deep networks with $ l\_2 $ regularization.
\newblock {\em Advances in Neural Information Processing Systems}, 33:4790--4799, 2020.

\bibitem{li2020towards}
Zhiyuan Li, Yuping Luo, and Kaifeng Lyu.
\newblock Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{liu2020_NTK-PL-inequ}
Chaoyue Liu, Libin Zhu, and Mikhail Belkin.
\newblock Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning.
\newblock {\em arXiv preprint arXiv:2003.00307}, 2020.

\bibitem{min_2021_imbalance_shallow_DLN}
Hancheng Min, Salma Tarmoun, Ren{\'e} Vidal, and Enrique Mallada.
\newblock On the explicit role of initialization on the convergence and implicit bias of overparametrized linear networks.
\newblock In {\em International Conference on Machine Learning}, pages 7760--7768. PMLR, 2021.

\bibitem{min_2023_imbalance_deep}
Hancheng Min, Rene Vidal, and Enrique Mallada.
\newblock On the convergence of gradient flow on multi-layer linear models.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, {\em Proceedings of the 40th International Conference on Machine Learning}, volume 202 of {\em Proceedings of Machine Learning Research}, pages 24850--24887. PMLR, 23--29 Jul 2023.

\bibitem{Ongie_2020_repres_bounded_norm_shallow_ReLU_net}
Greg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro.
\newblock A function space view of bounded norm infinite width relu nets: The multivariate case.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock 2019.

\bibitem{Pesme_2023_S-to-S_diag_DLN}
Scott Pesme and Nicolas Flammarion.
\newblock Saddle-to-saddle dynamics in diagonal linear networks.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, {\em Advances in Neural Information Processing Systems}, volume~36, pages 7475--7505. Curran Associates, Inc., 2023.

\bibitem{Pesme_2021_SGD_bias_diag_nets}
Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion.
\newblock Implicit bias of sgd for diagonal linear networks: a provable benefit of stochasticity.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman Vaughan, editors, {\em Advances in Neural Information Processing Systems}, volume~34, pages 29218--29230. Curran Associates, Inc., 2021.

\bibitem{Rotskoff2018}
Grant Rotskoff and Eric Vanden-Eijnden.
\newblock {Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks}.
\newblock In {\em Advances in Neural Information Processing Systems 31}, pages 7146--7155. Curran Associates, Inc., 2018.

\bibitem{saxe_2014_exact}
Andrew~M Saxe, James~L McClelland, and Surya Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear neural networks, 2014.

\bibitem{Saxe_2019_independent_diag}
Andrew~M. Saxe, James~L. McClelland, and Surya Ganguli.
\newblock A mathematical theory of semantic development in deep neural networks.
\newblock {\em Proceedings of the National Academy of Sciences}, 116(23):11537--11546, 2019.

\bibitem{tarmoun_2021_GF_matrix_fact}
Salma Tarmoun, Guilherme Franca, Benjamin~D Haeffele, and Rene Vidal.
\newblock Understanding the dynamics of gradient flow in overparameterized linear models.
\newblock In Marina Meila and Tong Zhang, editors, {\em Proceedings of the 38th International Conference on Machine Learning}, volume 139 of {\em Proceedings of Machine Learning Research}, pages 10153--10161. PMLR, 18--24 Jul 2021.

\bibitem{wang_2023_bias_SGD_L2}
Zihan Wang and Arthur Jacot.
\newblock Implicit bias of {SGD} in $l_2$-regularized linear {DNN}s: One-way jumps from high to low rank.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{woodworth_2020_diagnet_bias}
Blake Woodworth, Suriya Gunasekar, Pedro Savarese, Edward Moroshko, Itay Golan, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and rich regimes in overparametrized models, 2020.

\bibitem{xiong_2023_overparam_balanced_hurt_conv}
Nuoya Xiong, Lijun Ding, and Simon~Shaolei Du.
\newblock How over-parameterization slows down gradient descent in matrix sensing: The curses of symmetry and initialization.
\newblock In {\em OPT 2023: Optimization for Machine Learning}, 2023.

\bibitem{xu_2023_imbalance_GD}
Ziqing Xu, Hancheng Min, Salma Tarmoun, Enrique Mallada, and Rene Vidal.
\newblock Linear convergence of gradient descent for finite width over-parametrized linear networks with general initialization.
\newblock In Francisco Ruiz, Jennifer Dy, and Jan-Willem van~de Meent, editors, {\em Proceedings of The 26th International Conference on Artificial Intelligence and Statistics}, volume 206 of {\em Proceedings of Machine Learning Research}, pages 2262--2284. PMLR, 25--27 Apr 2023.

\bibitem{yang2020NTK}
Greg Yang.
\newblock Tensor programs ii: Neural tangent kernel for any architecture.
\newblock {\em arXiv preprint arXiv:2006.14548}, 2020.

\bibitem{yang2020feature}
Greg Yang and Edward~J. Hu.
\newblock Feature learning in infinite-width neural networks, 2020.

\bibitem{yu2015useful}
Yi~Yu, Tengyao Wang, and Richard~J Samworth.
\newblock A useful variant of the davis--kahan theorem for statisticians.
\newblock {\em Biometrika}, 102(2):315--323, 2015.

\bibitem{zou2020global}
Difan Zou, Philip~M Long, and Quanquan Gu.
\newblock On the global convergence of training deep linear resnets.
\newblock {\em arXiv preprint arXiv:2003.01094}, 2020.

\end{thebibliography}
