\begin{thebibliography}{}

\bibitem[Abbasi-Yadkori et~al., 2011]{abbasi2011improved}
Abbasi-Yadkori, Y., P{\'a}l, D., and Szepesv{\'a}ri, C. (2011).
\newblock Improved algorithms for linear stochastic bandits.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2312--2320.

\bibitem[Abe et~al., 2010]{abe2010optimizing}
Abe, N., Melville, P., Pendus, C., Reddy, C.~K., Jensen, D.~L., Thomas, V.~P.,
  Bennett, J.~J., Anderson, G.~F., Cooley, B.~R., Kowalczyk, M., et~al. (2010).
\newblock Optimizing debt collections using constrained reinforcement learning.
\newblock In {\em Proceedings of the 16th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 75--84.

\bibitem[Achiam et~al., 2017]{achiam2017constrained}
Achiam, J., Held, D., Tamar, A., and Abbeel, P. (2017).
\newblock Constrained policy optimization.
\newblock In {\em International Conference on Machine Learning}, pages 22--31.
  PMLR.

\bibitem[Amani et~al., 2019]{amani2019linear}
Amani, S., Alizadeh, M., and Thrampoulidis, C. (2019).
\newblock Linear stochastic bandits under safety constraints.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9252--9262.

\bibitem[Amani and Thrampoulidis, 2021]{Amani_Thrampoulidis_2021}
Amani, S. and Thrampoulidis, C. (2021).
\newblock Decentralized multi-agent linear bandits with safety constraints.
\newblock {\em Proceedings of the AAAI Conference on Artificial Intelligence},
  35(8):6627--6635.

\bibitem[Bai et~al., 2019]{bai2019provably}
Bai, Y., Xie, T., Jiang, N., and Wang, Y.-X. (2019).
\newblock Provably efficient q-learning with low switching cost.
\newblock {\em arXiv preprint arXiv:1905.12849}.

\bibitem[Berkenkamp et~al., 2017]{berkenkamp2017safe}
Berkenkamp, F., Turchetta, M., Schoellig, A., and Krause, A. (2017).
\newblock Safe model-based reinforcement learning with stability guarantees.
\newblock In {\em Advances in neural information processing systems}, pages
  908--918.

\bibitem[Bertsekas et~al., 2000]{bertsekas2000dynamic}
Bertsekas, D.~P. et~al. (2000).
\newblock {\em Dynamic programming and optimal control: Vol. 1}.
\newblock Athena scientific Belmont.

\bibitem[Bradtke and Barto, 1996]{bradtke1996linear}
Bradtke, S.~J. and Barto, A.~G. (1996).
\newblock Linear least-squares algorithms for temporal difference learning.
\newblock {\em Machine learning}, 22(1-3):33--57.

\bibitem[Brockman et~al., 2016]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W. (2016).
\newblock Openai gym.
\newblock {\em arXiv preprint arXiv:1606.01540}.

\bibitem[Chow et~al., 2018]{chow2018lyapunov}
Chow, Y., Nachum, O., Duenez-Guzman, E., and Ghavamzadeh, M. (2018).
\newblock A lyapunov-based approach to safe reinforcement learning.
\newblock In {\em Advances in neural information processing systems}, pages
  8092--8101.

\bibitem[Ding et~al., 2020a]{ding2020provably}
Ding, D., Wei, X., Yang, Z., Wang, Z., and Jovanovi{\'c}, M.~R. (2020a).
\newblock Provably efficient safe exploration via primal-dual policy
  optimization.
\newblock {\em arXiv preprint arXiv:2003.00534}.

\bibitem[Ding et~al., 2020b]{ding2020natural}
Ding, D., Zhang, K., Basar, T., and Jovanovic, M. (2020b).
\newblock Natural policy gradient primal-dual method for constrained markov
  decision processes.
\newblock {\em Advances in Neural Information Processing Systems}, 33.

\bibitem[Efroni et~al., 2020]{efroni2020exploration}
Efroni, Y., Mannor, S., and Pirotta, M. (2020).
\newblock Exploration-exploitation in constrained mdps.
\newblock {\em arXiv preprint arXiv:2003.02189}.

\bibitem[Garcelon et~al., 2020]{garcelon2020conservative}
Garcelon, E., Ghavamzadeh, M., Lazaric, A., and Pirotta, M. (2020).
\newblock Conservative exploration in reinforcement learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1431--1441. PMLR.

\bibitem[Jin et~al., 2020]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I. (2020).
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In {\em Conference on Learning Theory}, pages 2137--2143.

\bibitem[Kalagarla et~al., 2020]{kalagarla2020sample}
Kalagarla, K.~C., Jain, R., and Nuzzo, P. (2020).
\newblock A sample-efficient algorithm for episodic finite-horizon mdp with
  constraints.
\newblock {\em arXiv preprint arXiv:2009.11348}.

\bibitem[Moradipari et~al., 2019]{moradipari2019safe}
Moradipari, A., Amani, S., Alizadeh, M., and Thrampoulidis, C. (2019).
\newblock Safe linear thompson sampling.
\newblock {\em arXiv preprint arXiv:1911.02156}.

\bibitem[Pacchiano et~al., 2020]{pacchiano2020stochastic}
Pacchiano, A., Ghavamzadeh, M., Bartlett, P., and Jiang, H. (2020).
\newblock Stochastic bandits with linear constraints.
\newblock {\em arXiv preprint arXiv:2006.10185}.

\bibitem[Paternain et~al., 2019a]{paternain2019safe}
Paternain, S., Calvo-Fullana, M., Chamon, L.~F., and Ribeiro, A. (2019a).
\newblock Safe policies for reinforcement learning via primal-dual methods.
\newblock {\em arXiv preprint arXiv:1911.09101}.

\bibitem[Paternain et~al., 2019b]{paternain2019constrained}
Paternain, S., Chamon, L.~F., Calvo-Fullana, M., and Ribeiro, A. (2019b).
\newblock Constrained reinforcement learning has zero duality gap.
\newblock {\em arXiv preprint arXiv:1910.13393}.

\bibitem[Qiu et~al., 2020]{qiu2020upper}
Qiu, S., Wei, X., Yang, Z., Ye, J., and Wang, Z. (2020).
\newblock Upper confidence primal-dual optimization: Stochastically constrained
  markov decision processes with adversarial losses and unknown transitions.
\newblock {\em arXiv preprint arXiv:2003.00660}.

\bibitem[Stooke et~al., 2020]{stooke2020responsive}
Stooke, A., Achiam, J., and Abbeel, P. (2020).
\newblock Responsive safety in reinforcement learning by pid lagrangian
  methods.
\newblock In {\em International Conference on Machine Learning}, pages
  9133--9143. PMLR.

\bibitem[Sutton and Barto, 2018]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G. (2018).
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press.

\bibitem[Szepesv{\'a}ri, 2010]{szepesvari2010algorithms}
Szepesv{\'a}ri, C. (2010).
\newblock Algorithms for reinforcement learning.
\newblock {\em Synthesis lectures on artificial intelligence and machine
  learning}, 4(1):1--103.

\bibitem[Tessler et~al., 2018]{tessler2018reward}
Tessler, C., Mankowitz, D.~J., and Mannor, S. (2018).
\newblock Reward constrained policy optimization.
\newblock {\em arXiv preprint arXiv:1805.11074}.

\bibitem[Turchetta et~al., 2016]{turchetta2016safe}
Turchetta, M., Berkenkamp, F., and Krause, A. (2016).
\newblock Safe exploration in finite markov decision processes with gaussian
  processes.
\newblock {\em arXiv preprint arXiv:1606.04753}.

\bibitem[Turchetta et~al., 2020]{turchetta2020safe}
Turchetta, M., Kolobov, A., Shah, S., Krause, A., and Agarwal, A. (2020).
\newblock Safe reinforcement learning via curriculum induction.
\newblock {\em arXiv preprint arXiv:2006.12136}.

\bibitem[Wachi and Sui, 2020]{wachi2020safe}
Wachi, A. and Sui, Y. (2020).
\newblock Safe reinforcement learning in constrained markov decision processes.
\newblock In {\em International Conference on Machine Learning}, pages
  9797--9806. PMLR.

\bibitem[Wachi et~al., 2018]{wachi2018safe}
Wachi, A., Sui, Y., Yue, Y., and Ono, M. (2018).
\newblock Safe exploration and optimization of constrained mdps using gaussian
  processes.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32.

\bibitem[Xu et~al., 2020]{xu2020primal}
Xu, T., Liang, Y., and Lan, G. (2020).
\newblock A primal approach to constrained policy optimization: Global
  optimality and finite-time analysis.
\newblock {\em arXiv preprint arXiv:2011.05869}.

\bibitem[Yang and Wang, 2019]{yang2019sample}
Yang, L. and Wang, M. (2019).
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock In {\em International Conference on Machine Learning}, pages
  6995--7004. PMLR.

\bibitem[Yang et~al., 2020]{yang2020projection}
Yang, T.-Y., Rosca, J., Narasimhan, K., and Ramadge, P.~J. (2020).
\newblock Projection-based constrained policy optimization.
\newblock {\em arXiv preprint arXiv:2010.03152}.

\bibitem[Yu et~al., 2019]{yu2019convergent}
Yu, M., Yang, Z., Kolar, M., and Wang, Z. (2019).
\newblock Convergent policy optimization for safe reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3127--3139.

\bibitem[Zheng and Ratliff, 2020]{zheng2020constrained}
Zheng, L. and Ratliff, L.~J. (2020).
\newblock Constrained upper confidence reinforcement learning.
\newblock {\em arXiv preprint arXiv:2001.09377}.

\end{thebibliography}
