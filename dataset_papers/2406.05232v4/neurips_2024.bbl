\begin{thebibliography}{10}

\bibitem{GPT35}
OpenAI.
\newblock {O}pen{AI} {M}odels - {GPT}3.5, 2022.

\bibitem{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
  Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
  Lample.
\newblock Llama: Open and efficient foundation language models, 2023.

\bibitem{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{Llama3}
Meta.
\newblock {M}eta {M}odels - {LlaMa}3, 2024.

\bibitem{claude}
Anthropic.
\newblock {A}nthropic {M}odels - {C}laude3, 2024.

\bibitem{m2022exploring}
Muneer M~Alshater.
\newblock Exploring the role of artificial intelligence in enhancing academic
  performance: A case study of chatgpt.
\newblock {\em Available at SSRN 4312358}, 2022.

\bibitem{ahmed2021detectingfakenews}
Alim Al~Ayub Ahmed, Ayman Aljabouh, Praveen~Kumar Donepudi, and Myung~Suh Choi.
\newblock Detecting fake news using machine learning: A systematic literature
  review.
\newblock {\em arXiv preprint arXiv:2102.04458}, 2021.

\bibitem{adelani2020generatingreviews}
David~Ifeoluwa Adelani, Haotian Mai, Fuming Fang, Huy~H Nguyen, Junichi
  Yamagishi, and Isao Echizen.
\newblock Generating sentiment-preserving fake online reviews using neural
  language models and their human-and machine-based detection.
\newblock In {\em Advanced information networking and applications: Proceedings
  of the 34th international conference on advanced information networking and
  applications (AINA-2020)}, pages 1341--1354. Springer, 2020.

\bibitem{lee2023languageplagiarize}
Jooyoung Lee, Thai Le, Jinghui Chen, and Dongwon Lee.
\newblock Do language models plagiarize?
\newblock In {\em Proceedings of the ACM Web Conference 2023}, pages
  3637--3647, 2023.

\bibitem{gao2022comparing}
Catherine~A Gao, Frederick~M Howard, Nikolay~S Markov, Emma~C Dyer, Siddhi
  Ramesh, Yuan Luo, and Alexander~T Pearson.
\newblock Comparing scientific abstracts generated by chatgpt to original
  abstracts using an artificial intelligence output detector, plagiarism
  detector, and blinded human reviewers.
\newblock {\em bioRxiv}, pages 2022--12, 2022.

\bibitem{Else2023AbstractsWB}
Holly Else.
\newblock Abstracts written by chatgpt fool scientists.
\newblock {\em Nature}, 613:423 -- 423, 2023.

\bibitem{chakraborty2023possibilities}
Souradip Chakraborty, Amrit~Singh Bedi, Sicheng Zhu, Bang An, Dinesh Manocha,
  and Furong Huang.
\newblock On the possibilities of ai-generated text detection.
\newblock {\em arXiv preprint arXiv:2304.04736}, 2023.

\bibitem{krishna2023paraphrasing}
Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer.
\newblock Paraphrasing evades detectors of ai-generated text, but retrieval is
  an effective defense, 2023.

\bibitem{sadasivan2023can}
Vinu~Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and
  Soheil Feizi.
\newblock Can ai-generated text be reliably detected?
\newblock {\em arXiv preprint arXiv:2303.11156}, 2023.

\bibitem{dugan2024raid}
Liam Dugan, Alyssa Hwang, Filip Trhlik, Josh~Magnus Ludan, Andrew Zhu, Hainiu
  Xu, Daphne Ippolito, and Chris Callison-Burch.
\newblock Raid: A shared benchmark for robust evaluation of machine-generated
  text detectors, 2024.

\bibitem{lin2024detecting}
Li~Lin, Neeraj Gupta, Yue Zhang, Hainan Ren, Chun-Hao Liu, Feng Ding, Xin Wang,
  Xin Li, Luisa Verdoliva, and Shu Hu.
\newblock Detecting multimedia generated by large ai models: A survey.
\newblock {\em arXiv preprint arXiv:2402.00045}, 2024.

\bibitem{abdelnabi21oakland}
Sahar Abdelnabi and Mario Fritz.
\newblock Adversarial watermarking transformer: Towards tracing text provenance
  with data hiding.
\newblock In {\em 42nd IEEE Symposium on Security and Privacy}, 2021.

\bibitem{kirchenbauer2023watermark}
John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom
  Goldstein.
\newblock A watermark for large language models.
\newblock {\em arXiv preprint arXiv:2301.10226}, 2023.

\bibitem{yoo2023robust}
KiYoon Yoo, Wonhyuk Ahn, Jiho Jang, and Nojun Kwak.
\newblock Robust natural language watermarking through invariant features.
\newblock {\em arXiv preprint arXiv:2305.01904}, 2023.

\bibitem{christ2023undetectable}
Miranda Christ, Sam Gunn, and Or~Zamir.
\newblock Undetectable watermarks for language models.
\newblock {\em Cryptology ePrint Archive}, 2023.

\bibitem{GPTZero}
Edward Tian.
\newblock Gptzero: An ai text detector, 2023.

\bibitem{verma2023ghostbuster}
Vivek Verma, Eve Fleisig, Nicholas Tomlin, and Dan Klein.
\newblock Ghostbuster: Detecting text ghostwritten by large language models,
  2023.

\bibitem{solaiman2019release}
Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss,
  Jeff Wu, Alec Radford, Gretchen Krueger, Jong~Wook Kim, Sarah Kreps, et~al.
\newblock Release strategies and the social impacts of language models.
\newblock {\em arXiv preprint arXiv:1908.09203}, 2019.

\bibitem{wu2023llmdet}
Kangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua.
\newblock Llmdet: A large language models detection tool, 2023.

\bibitem{yu2023gpt}
Xiao Yu, Yuang Qi, Kejiang Chen, Guoqiang Chen, Xi~Yang, Pengyuan Zhu, Weiming
  Zhang, and Nenghai Yu.
\newblock Gpt paternity test: Gpt generated text detection with gpt genetic
  inheritance.
\newblock {\em arXiv preprint arXiv:2305.12519}, 2023.

\bibitem{mitchell2023detectgpt}
Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher~D Manning, and
  Chelsea Finn.
\newblock Detectgpt: Zero-shot machine-generated text detection using
  probability curvature.
\newblock {\em arXiv preprint arXiv:2301.11305}, 2023.

\bibitem{yang2023dna}
Xianjun Yang, Wei Cheng, Linda Petzold, William~Yang Wang, and Haifeng Chen.
\newblock Dna-gpt: Divergent n-gram analysis for training-free detection of
  gpt-generated text.
\newblock {\em arXiv preprint arXiv:2305.17359}, 2023.

\bibitem{bao2023fast}
Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, and Yue Zhang.
\newblock Fast-detectgpt: Efficient zero-shot detection of machine-generated
  text via conditional probability curvature.
\newblock {\em arXiv preprint arXiv:2310.05130}, 2023.

\bibitem{yang2023survey}
Xianjun Yang, Liangming Pan, Xuandong Zhao, Haifeng Chen, Linda Petzold,
  William~Yang Wang, and Wei Cheng.
\newblock A survey on detection of llms-generated content.
\newblock {\em arXiv preprint arXiv:2310.15654}, 2023.

\bibitem{gehrmann2019gltr}
Sebastian Gehrmann, SEAS Harvard, Hendrik Strobelt, and Alexander~M Rush.
\newblock Gltr: Statistical detection and visualization of generated text.
\newblock {\em ACL 2019}, page 111, 2019.

\bibitem{grechnikov2009detection}
EA~Grechnikov, GG~Gusev, AA~Kustarev, and AM~Raigorodsky.
\newblock Detection of artificial texts.
\newblock {\em RCDL2009 Proceedings. Petrozavodsk}, pages 306--308, 2009.

\bibitem{badaskar2008identifying}
Sameer Badaskar, Sachin Agarwal, and Shilpa Arora.
\newblock Identifying real or fake articles: Towards better language modeling.
\newblock In {\em Proceedings of the Third International Joint Conference on
  Natural Language Processing: Volume-II}, 2008.

\bibitem{deng2023efficient}
Zhijie Deng, Hongcheng Gao, Yibo Miao, and Hao Zhang.
\newblock Efficient detection of llm-generated texts with a bayesian surrogate
  model, 2023.

\bibitem{wang2023seqxgpt}
Pengyu Wang, Linyang Li, Ke~Ren, Botian Jiang, Dong Zhang, and Xipeng Qiu.
\newblock Seqxgpt: Sentence-level ai-generated text detection.
\newblock {\em arXiv preprint arXiv:2310.08903}, 2023.

\bibitem{li2023origin}
Linyang Li, Pengyu Wang, Ke~Ren, Tianxiang Sun, and Xipeng Qiu.
\newblock Origin tracing and detecting of llms.
\newblock {\em arXiv preprint arXiv:2304.14072}, 2023.

\bibitem{guo2023authentigpt}
Zhen Guo and Shangdi Yu.
\newblock Authentigpt: Detecting machine-generated text via black-box language
  models denoising, 2023.

\bibitem{mao2024raidar}
Chengzhi Mao, Carl Vondrick, Hao Wang, and Junfeng Yang.
\newblock Raidar: generative ai detection via rewriting, 2024.

\bibitem{soto2024fewshot}
Rafael~Rivera Soto, Kailin Koch, Aleem Khan, Barry Chen, Marcus Bishop, and
  Nicholas Andrews.
\newblock Few-shot detection of machine-generated text using style
  representations, 2024.

\bibitem{AITextClassifier}
OpenAI.
\newblock {AI} text classifier, Jan 2023.

\bibitem{zhan2023g3detector}
Haolan Zhan, Xuanli He, Qiongkai Xu, Yuxiang Wu, and Pontus Stenetorp.
\newblock G3detector: General gpt-generated text detector.
\newblock {\em arXiv preprint arXiv:2305.12680}, 2023.

\bibitem{chen2023gptsentinel}
Yutian Chen, Hao Kang, Vivian Zhai, Liangze Li, Rita Singh, and Bhiksha Raj.
\newblock Gpt-sentinel: Distinguishing human and chatgpt generated content.
\newblock {\em arXiv preprint arXiv:2305.07969}, 2023.

\bibitem{hans2024spotting}
Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi,
  Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein.
\newblock Spotting llms with binoculars: Zero-shot detection of
  machine-generated text, 2024.

\bibitem{su2023detectllm}
Jinyan Su, Terry~Yue Zhuo, Di~Wang, and Preslav Nakov.
\newblock Detectllm: Leveraging log rank information for zero-shot detection of
  machine-generated text, 2023.

\bibitem{venkatraman2024gptwho}
Saranya Venkatraman, Adaku Uchendu, and Dongwon Lee.
\newblock Gpt-who: An information density-based machine-generated text
  detector, 2024.

\bibitem{shi2024words}
Yuhui Shi, Qiang Sheng, Juan Cao, Hao Mi, Beizhe Hu, and Danding Wang.
\newblock Ten words only still help: Improving black-box ai-generated text
  detection via proxy-guided efficient re-sampling, 2024.

\bibitem{mireshghallah2024smaller}
Niloofar Mireshghallah, Justus Mattern, Sicun Gao, Reza Shokri, and Taylor
  Berg-Kirkpatrick.
\newblock Smaller language models are better black-box machine-generated text
  detectors, 2024.

\bibitem{tulchinskii2023intrinsic}
Eduard Tulchinskii, Kristian Kuznetsov, Laida Kushnareva, Daniil Cherniavskii,
  Serguei Barannikov, Irina Piontkovskaya, Sergey Nikolenko, and Evgeny
  Burnaev.
\newblock Intrinsic dimension estimation for robust detection of ai-generated
  texts, 2023.

\bibitem{yang2023zero}
Xianjun Yang, Kexun Zhang, Haifeng Chen, Linda Petzold, William~Yang Wang, and
  Wei Cheng.
\newblock Zero-shot detection of machine-generated codes.
\newblock {\em arXiv preprint arXiv:2310.05103}, 2023.

\bibitem{nicks2024language}
Charlotte Nicks, Eric Mitchell, Rafael Rafailov, Archit Sharma, Christopher~D
  Manning, Chelsea Finn, and Stefano Ermon.
\newblock Language model detectors are easily optimized against.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock {\em arXiv preprint arXiv:2106.09685}, 2021.

\bibitem{narayan-etal-2018-dont}
Shashi Narayan, Shay~B. Cohen, and Mirella Lapata.
\newblock Don{'}t give me the details, just the summary! topic-aware
  convolutional neural networks for extreme summarization.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 1797--1807, 2018.

\bibitem{fan-etal-2018-hierarchical}
Angela Fan, Mike Lewis, and Yann Dauphin.
\newblock Hierarchical neural story generation.
\newblock In {\em Proceedings of the 56th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 889--898,
  Melbourne, Australia, July 2018. Association for Computational Linguistics.

\bibitem{bojar-EtAl:2016:WMT1}
Ond~{r}ej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry
  Haddow, Matthias Huck, Antonio Jimeno~Yepes, Philipp Koehn, Varvara
  Logacheva, Christof Monz, Matteo Negri, Aurelie Neveol, Mariana Neves, Martin
  Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco
  Turchi, Karin Verspoor, and Marcos Zampieri.
\newblock Findings of the 2016 conference on machine translation.
\newblock In {\em Proceedings of the First Conference on Machine Translation},
  pages 131--198, Berlin, Germany, August 2016. Association for Computational
  Linguistics.

\bibitem{jin-etal-2019-pubmedqa}
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu.
\newblock {P}ub{M}ed{QA}: A dataset for biomedical research question answering.
\newblock In {\em Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 2567--2577, 2019.

\bibitem{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em The Journal of Machine Learning Research}, 21(1):5485--5551,
  2020.

\bibitem{gpt-neo}
Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman.
\newblock {GPT-Neo: Large Scale Autoregressive Language Modeling with
  Mesh-Tensorflow}, March 2021.
\newblock {If you use this software, please cite it using these metadata.}

\bibitem{gpt-j}
Ben Wang and Aran Komatsuzaki.
\newblock {GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}.
\newblock \url{https://github.com/kingoflolz/mesh-transformer-jax}, May 2021.

\bibitem{zhao2024wildchat}
Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian
  Deng.
\newblock Wildchat: 1m chatgpt interaction logs in the wild.
\newblock {\em arXiv preprint arXiv:2405.01470}, 2024.

\bibitem{liang2023gpt}
Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, and James Zou.
\newblock Gpt detectors are biased against non-native english writers.
\newblock {\em arXiv preprint arXiv:2304.02819}, 2023.

\bibitem{llama3.1}
Meta.
\newblock Llama 3.1: Open foundation and fine-tuned chat models, 2024.
\newblock Accessed: 2024-10-27.

\bibitem{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
  Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel,
  Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock {\em arXiv preprint arXiv:2310.06825}, 2023.

\bibitem{ju22robust}
Haotian Ju, Dongyue Li, and Hongyang~R Zhang.
\newblock Robust fine-tuning of deep neural networks with hessian-based
  generalization guarantees.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, {\em Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of {\em Proceedings
  of Machine Learning Research}, pages 10431--10461. PMLR, 17--23 Jul 2022.

\bibitem{sason2013entropy}
Igal Sason.
\newblock Entropy bounds for discrete random variables via coupling.
\newblock In {\em 2013 IEEE International Symposium on Information Theory},
  pages 414--418, 2013.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{ippolito2019automatic}
Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck.
\newblock Automatic detection of generated text is easiest when humans are
  fooled.
\newblock {\em arXiv preprint arXiv:1911.00650}, 2019.

\bibitem{yang2023code}
Xianjun Yang, Kexun Zhang, Haifeng Chen, Linda Petzold, William~Yang Wang, and
  Wei Cheng.
\newblock Zero-shot detection of machine-generated codes.
\newblock {\em arXiv preprint arXiv:2310.05103}, 2023.

\bibitem{hendrycks2021measuring}
Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora,
  Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et~al.
\newblock Measuring coding challenge competence with apps.
\newblock {\em arXiv preprint arXiv:2105.09938}, 2021.

\end{thebibliography}
