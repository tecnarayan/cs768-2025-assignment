\begin{thebibliography}{82}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and Szepesv{\'a}ri]{abbasi2011improved}
Yasin Abbasi-Yadkori, D{\'a}vid P{\'a}l, and Csaba Szepesv{\'a}ri.
\newblock Improved algorithms for linear stochastic bandits.
\newblock \emph{Advances in neural information processing systems}, 24:\penalty0 2312--2320, 2011.

\bibitem[Abeille and Lazaric(2017)]{abeille2017Linear}
Marc Abeille and Alessandro Lazaric.
\newblock {Linear Thompson Sampling Revisited}.
\newblock In Aarti Singh and Jerry Zhu, editors, \emph{Proceedings of the 20th International Conference on Artificial Intelligence and Statistics}, volume~54 of \emph{Proceedings of Machine Learning Research}, pages 176--184. PMLR, PMLR, 20--22 Apr 2017.

\bibitem[Abeille et~al.(2021)Abeille, Faury, and Calauz{\`e}nes]{abeille2021instance}
Marc Abeille, Louis Faury, and Cl{\'e}ment Calauz{\`e}nes.
\newblock Instance-wise minimax-optimal algorithms for logistic bandits.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 3691--3699. PMLR, 2021.

\bibitem[Agarwal and Zhang(2022{\natexlab{a}})]{agarwal2022model}
Alekh Agarwal and Tong Zhang.
\newblock Model-based rl with optimistic posterior sampling: Structural conditions and sample complexity.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 35284--35297, 2022{\natexlab{a}}.

\bibitem[Agarwal and Zhang(2022{\natexlab{b}})]{agarwal2022non}
Alekh Agarwal and Tong Zhang.
\newblock Non-linear reinforcement learning in large action spaces: Structural conditions and sample-efficiency of posterior sampling.
\newblock In \emph{Conference on Learning Theory}, pages 2776--2814. PMLR, 2022{\natexlab{b}}.

\bibitem[Agrawal et~al.(2023)Agrawal, Tulabandhula, and Avadhanula]{agrawal2023tractable}
Priyank Agrawal, Theja Tulabandhula, and Vashist Avadhanula.
\newblock A tractable online learning algorithm for the multinomial logit contextual bandit.
\newblock \emph{European Journal of Operational Research}, 2023.

\bibitem[Agrawal and Jia(2017)]{agrawal2017posterior}
Shipra Agrawal and Randy Jia.
\newblock Posterior sampling for reinforcement learning: worst-case regret bounds.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages 1184--1194, 2017.

\bibitem[Amani and Thrampoulidis(2021)]{amani2021ucb}
Sanae Amani and Christos Thrampoulidis.
\newblock Ucb-based algorithms for multinomial logistic regression bandits.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 2913--2924, 2021.

\bibitem[Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang, and Yang]{ayoub2020model}
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \emph{International Conference on Machine Learning}, pages 463--474. PMLR, 2020.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages 263--272. PMLR, 2017.

\bibitem[Bach(2010)]{bach2010self}
Francis Bach.
\newblock Self-concordant analysis for logistic regression.
\newblock \emph{Electronic Journal of Statistics}, 4\penalty0 (2):\penalty0 384 -- 414, 2010.

\bibitem[Bartlett et~al.(2005)Bartlett, Bousquet, and Mendelson]{bartlett2005local}
Peter~L Bartlett, Olivier Bousquet, and Shahar Mendelson.
\newblock Local rademacher complexities.
\newblock \emph{The Annals of Statistics}, 2005.

\bibitem[Bradtke and Barto(1996)]{bradtke1996linear}
Steven~J Bradtke and Andrew~G Barto.
\newblock Linear least-squares algorithms for temporal difference learning.
\newblock \emph{Machine learning}, 22\penalty0 (1):\penalty0 33--57, 1996.

\bibitem[Cai et~al.(2020)Cai, Yang, Jin, and Wang]{cai2020provably}
Qi~Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang.
\newblock Provably efficient exploration in policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pages 1283--1294. PMLR, 2020.

\bibitem[Campolongo and Orabona(2020)]{campolongo2020temporal}
Nicolo Campolongo and Francesco Orabona.
\newblock Temporal variability in implicit online learning.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 12377--12387, 2020.

\bibitem[Chapelle and Li(2011)]{chapelle2011empirical}
Olivier Chapelle and Lihong Li.
\newblock An empirical evaluation of thompson sampling.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Chen et~al.(2020)Chen, Wang, and Zhou]{chen2020dynamic}
Xi~Chen, Yining Wang, and Yuan Zhou.
\newblock Dynamic assortment optimization with changing contextual information.
\newblock \emph{Journal of machine learning research}, 2020.

\bibitem[Chen et~al.(2023)Chen, Li, Yuan, Gu, and Jordan]{chen2023a}
Zixiang Chen, Chris~Junchi Li, Huizhuo Yuan, Quanquan Gu, and Michael Jordan.
\newblock A general framework for sample-efficient function approximation in reinforcement learning.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Dann et~al.(2018)Dann, Jiang, Krishnamurthy, Agarwal, Langford, and Schapire]{dann2018oracle}
Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E Schapire.
\newblock On oracle-efficient pac rl with rich observations.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~31, 2018.

\bibitem[Du et~al.(2019)Du, Krishnamurthy, Jiang, Agarwal, Dudik, and Langford]{du2019provably}
Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford.
\newblock Provably efficient rl with rich observations via latent state decoding.
\newblock In \emph{International Conference on Machine Learning}, pages 1665--1674. PMLR, 2019.

\bibitem[Du et~al.(2021)Du, Kakade, Lee, Lovett, Mahajan, Sun, and Wang]{du2021bilinear}
Simon Du, Sham Kakade, Jason Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang.
\newblock Bilinear classes: A structural framework for provable generalization in rl.
\newblock In \emph{International Conference on Machine Learning}, pages 2826--2836. PMLR, 2021.

\bibitem[Du et~al.(2020)Du, Kakade, Wang, and Yang]{du2020is}
Simon~S. Du, Sham~M. Kakade, Ruosong Wang, and Lin~F. Yang.
\newblock Is a good representation sufficient for sample efficient reinforcement learning?
\newblock In \emph{8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020.

\bibitem[Faury et~al.(2020)Faury, Abeille, Calauz{\`e}nes, and Fercoq]{faury2020improved}
Louis Faury, Marc Abeille, Cl{\'e}ment Calauz{\`e}nes, and Olivier Fercoq.
\newblock Improved optimistic algorithms for logistic bandits.
\newblock In \emph{International Conference on Machine Learning}, pages 3052--3060. PMLR, 2020.

\bibitem[Faury et~al.(2022)Faury, Abeille, Jun, and Calauz{\`e}nes]{faury2022jointly}
Louis Faury, Marc Abeille, Kwang-Sung Jun, and Cl{\'e}ment Calauz{\`e}nes.
\newblock Jointly efficient and optimal algorithms for logistic bandits.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 546--580. PMLR, 2022.

\bibitem[Fawzi et~al.(2022)Fawzi, Balog, Huang, Hubert, Romera-Paredes, Barekatain, Novikov, R~Ruiz, Schrittwieser, Swirszcz, et~al.]{fawzi2022discovering}
Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco~J R~Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, et~al.
\newblock Discovering faster matrix multiplication algorithms with reinforcement learning.
\newblock \emph{Nature}, 610\penalty0 (7930):\penalty0 47--53, 2022.

\bibitem[Filippi et~al.(2010)Filippi, Capp\'{e}, Garivier, and Szepesv\'{a}ri]{filippi2010parametric}
Sarah Filippi, Olivier Capp\'{e}, Aur\'{e}lien Garivier, and Csaba Szepesv\'{a}ri.
\newblock Parametric bandits: The generalized linear case.
\newblock In \emph{Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1}, NIPS'10, page 586â€“594, Red Hook, NY, USA, 2010. Curran Associates Inc.

\bibitem[Foster et~al.(2018)Foster, Kale, Luo, Mohri, and Sridharan]{foster2018logistic}
Dylan~J Foster, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik Sridharan.
\newblock Logistic regression: The importance of being improper.
\newblock In \emph{Conference On Learning Theory}, pages 167--208. PMLR, 2018.

\bibitem[Foster et~al.(2021)Foster, Kakade, Qian, and Rakhlin]{foster2021statistical}
Dylan~J Foster, Sham~M Kakade, Jian Qian, and Alexander Rakhlin.
\newblock The statistical complexity of interactive decision making.
\newblock \emph{arXiv preprint arXiv:2112.13487}, 2021.

\bibitem[Freedman(1975)]{freedman1975tail}
David~A Freedman.
\newblock On tail probabilities for martingales.
\newblock \emph{the Annals of Probability}, pages 100--118, 1975.

\bibitem[Hazan et~al.(2007)Hazan, Agarwal, and Kale]{hazan2007logarithmic}
Elad Hazan, Amit Agarwal, and Satyen Kale.
\newblock Logarithmic regret algorithms for online convex optimization.
\newblock \emph{Machine Learning}, 69\penalty0 (2):\penalty0 169--192, 2007.

\bibitem[Hazan et~al.(2014)Hazan, Koren, and Levy]{hazan2014logistic}
Elad Hazan, Tomer Koren, and Kfir~Y Levy.
\newblock Logistic regression: Tight bounds for stochastic and online optimization.
\newblock In \emph{Conference on Learning Theory}, pages 197--209. PMLR, 2014.

\bibitem[Hazan et~al.(2016)]{hazan2016introduction}
Elad Hazan et~al.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends{\textregistered} in Optimization}, 2\penalty0 (3-4):\penalty0 157--325, 2016.

\bibitem[He et~al.(2021)He, Zhou, and Gu]{he2021logarithmic}
Jiafan He, Dongruo Zhou, and Quanquan Gu.
\newblock Logarithmic regret for reinforcement learning with linear function approximation.
\newblock In \emph{International Conference on Machine Learning}, pages 4171--4180. PMLR, 2021.

\bibitem[He et~al.(2023)He, Zhao, Zhou, and Gu]{he2023nearly}
Jiafan He, Heyang Zhao, Dongruo Zhou, and Quanquan Gu.
\newblock Nearly minimax optimal reinforcement learning for linear markov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pages 12790--12822. PMLR, 2023.

\bibitem[Hwang and Oh(2023)]{hwang2023model}
Taehyun Hwang and Min-hwan Oh.
\newblock Model-based reinforcement learning with multinomial logistic function approximation.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, pages 7971--7979, 2023.

\bibitem[Hwang et~al.(2023)Hwang, Chai, and Oh]{hwang2023combinatorial}
Taehyun Hwang, Kyuwook Chai, and Min-Hwan Oh.
\newblock Combinatorial neural bandits.
\newblock In \emph{Proceedings of the 40th International Conference on Machine Learning}. PMLR, 2023.

\bibitem[Ishfaq et~al.(2021)Ishfaq, Cui, Nguyen, Ayoub, Yang, Wang, Precup, and Yang]{ishfaq2021randomized}
Haque Ishfaq, Qiwen Cui, Viet Nguyen, Alex Ayoub, Zhuoran Yang, Zhaoran Wang, Doina Precup, and Lin Yang.
\newblock Randomized exploration in reinforcement learning with general value function approximation.
\newblock In \emph{International Conference on Machine Learning}, volume 139, pages 4607--4616. PMLR, PMLR, 2021.

\bibitem[Ishfaq et~al.(2024{\natexlab{a}})Ishfaq, Lan, Xu, Mahmood, Precup, Anandkumar, and Azizzadenesheli]{ishfaq2024provable}
Haque Ishfaq, Qingfeng Lan, Pan Xu, A.~Rupam Mahmood, Doina Precup, Anima Anandkumar, and Kamyar Azizzadenesheli.
\newblock Provable and practical: Efficient exploration in reinforcement learning via langevin monte carlo.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=nfIAEJFiBZ}.

\bibitem[Ishfaq et~al.(2024{\natexlab{b}})Ishfaq, Tan, Yang, Lan, Lu, Mahmood, Precup, and Xu]{ishfaq2024more}
Haque Ishfaq, Yixin Tan, Yu~Yang, Qingfeng Lan, Jianfeng Lu, A~Rupam Mahmood, Doina Precup, and Pan Xu.
\newblock More efficient randomized exploration for reinforcement learning via approximate sampling.
\newblock \emph{Reinforcement Learning Journal}, 3\penalty0 (1), 2024{\natexlab{b}}.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (4), 2010.

\bibitem[Jia et~al.(2020)Jia, Yang, Szepesvari, and Wang]{jia2020model}
Zeyu Jia, Lin Yang, Csaba Szepesvari, and Mengdi Wang.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \emph{Learning for Dynamics and Control}, pages 666--686. PMLR, 2020.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and Schapire]{jiang2017contextual}
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E Schapire.
\newblock Contextual decision processes with low bellman rank are pac-learnable.
\newblock In \emph{International Conference on Machine Learning}, pages 1704--1713. PMLR, 2017.

\bibitem[Jin et~al.(2020)Jin, Yang, Wang, and Jordan]{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function approximation.
\newblock In \emph{Conference on Learning Theory}, pages 2137--2143. PMLR, 2020.

\bibitem[Jin et~al.(2021)Jin, Liu, and Miryoosefi]{jin2021bellman}
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi.
\newblock Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 13406--13418, 2021.

\bibitem[Jun et~al.(2017)Jun, Bhargava, Nowak, and Willett]{jun2017scalable}
Kwang-Sung Jun, Aniruddha Bhargava, Robert Nowak, and Rebecca Willett.
\newblock Scalable generalized linear bandits: Online computation and hashing.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Kim et~al.(2022)Kim, Yang, and Jun]{kim2022improved}
Yeoneung Kim, Insoon Yang, and Kwang-Sung Jun.
\newblock Improved regret analysis for variance-adaptive linear bandits and horizon-free linear mixture mdps.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 1060--1072, 2022.

\bibitem[Kober et~al.(2013)Kober, Bagnell, and Peters]{kober2013reinforcement}
Jens Kober, J~Andrew Bagnell, and Jan Peters.
\newblock Reinforcement learning in robotics: A survey.
\newblock \emph{The International Journal of Robotics Research}, 32\penalty0 (11):\penalty0 1238--1274, 2013.

\bibitem[Krishnamurthy et~al.(2016)Krishnamurthy, Agarwal, and Langford]{krishnamurthy2016pac}
Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock Pac reinforcement learning with rich observations.
\newblock \emph{Advances in Neural Information Processing Systems}, 29:\penalty0 1840--1848, 2016.

\bibitem[Kveton et~al.(2020)Kveton, Szepesv{\'a}ri, Ghavamzadeh, and Boutilier]{kveton2020perturbed}
Branislav Kveton, Csaba Szepesv{\'a}ri, Mohammad Ghavamzadeh, and Craig Boutilier.
\newblock Perturbed-history exploration in stochastic linear bandits.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 530--540. PMLR, 2020.

\bibitem[Lee and Oh(2024)]{lee2024nearly}
Joongkyu Lee and Min-hwan Oh.
\newblock Nearly minimax optimal regret for multinomial logistic bandit.
\newblock \emph{arXiv preprint arXiv:2405.09831}, 2024.

\bibitem[Li et~al.(2017)Li, Lu, and Zhou]{li2017provably}
Lihong Li, Yu~Lu, and Dengyong Zhou.
\newblock Provably optimal algorithms for generalized linear contextual bandits.
\newblock In \emph{International Conference on Machine Learning}, pages 2071--2080. PMLR, 2017.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness, Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Modi et~al.(2020)Modi, Jiang, Tewari, and Singh]{modi2020sample}
Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh.
\newblock Sample complexity of reinforcement learning using linearly combined model ensembles.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 2010--2020. PMLR, 2020.

\bibitem[Oh and Iyengar(2019)]{oh2019thompson}
Min-hwan Oh and Garud Iyengar.
\newblock Thompson sampling for multinomial logit contextual bandits.
\newblock \emph{Advances in Neural Information Processing Systems}, 32:\penalty0 3151--3161, 2019.

\bibitem[Oh and Iyengar(2021)]{oh2021multinomial}
Min-hwan Oh and Garud Iyengar.
\newblock Multinomial logit contextual bandits: Provable optimality and practicality.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, pages 9205--9213, 2021.

\bibitem[Osband and Roy(2014)]{osband2014modelbased}
Ian Osband and Benjamin~Van Roy.
\newblock Model-based reinforcement learning and the eluder dimension.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages 1466--1474, 2014.

\bibitem[Osband and Van~Roy(2017)]{osband2017posterior}
Ian Osband and Benjamin Van~Roy.
\newblock Why is posterior sampling better than optimism for reinforcement learning?
\newblock In \emph{International conference on machine learning}, pages 2701--2710. PMLR, 2017.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{osband2013more}
Ian Osband, Daniel Russo, and Benjamin Van~Roy.
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Osband et~al.(2016)Osband, Van~Roy, and Wen]{osband2016generalization}
Ian Osband, Benjamin Van~Roy, and Zheng Wen.
\newblock Generalization and exploration via randomized value functions.
\newblock In \emph{International Conference on Machine Learning}, pages 2377--2386. PMLR, 2016.

\bibitem[Pacchiano et~al.(2021)Pacchiano, Ball, Parker-Holder, Choromanski, and Roberts]{pacchiano2021towards}
Aldo Pacchiano, Philip Ball, Jack Parker-Holder, Krzysztof Choromanski, and Stephen Roberts.
\newblock Towards tractable optimism in model-based reinforcement learning.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 1413--1423. PMLR, 2021.

\bibitem[Perivier and Goyal(2022)]{perivier2022dynamic}
Noemie Perivier and Vineet Goyal.
\newblock Dynamic pricing and assortment under a contextual mnl demand.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 3461--3474, 2022.

\bibitem[Russo(2019)]{russo2019worst}
Daniel Russo.
\newblock Worst-case regret bounds for exploration via randomized value functions.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Russo and Van~Roy(2013)]{russo2013eluder}
Daniel Russo and Benjamin Van~Roy.
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages 2256--2264, 2013.

\bibitem[Russo et~al.(2018)Russo, Van~Roy, Kazerouni, Osband, Wen, et~al.]{russo2018tutorial}
Daniel~J Russo, Benjamin Van~Roy, Abbas Kazerouni, Ian Osband, Zheng Wen, et~al.
\newblock A tutorial on thompson sampling.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 11\penalty0 (1):\penalty0 1--96, 2018.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang, Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Silver et~al.(2018)Silver, Hubert, Schrittwieser, Antonoglou, Lai, Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2018general}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et~al.
\newblock A general reinforcement learning algorithm that masters chess, shogi, and go through self-play.
\newblock \emph{Science}, 362\penalty0 (6419):\penalty0 1140--1144, 2018.

\bibitem[Tiapkin et~al.(2022)Tiapkin, Belomestny, Calandriello, Moulines, Munos, Naumov, Rowland, Valko, and M{\'e}nard]{tiapkin2022optimistic}
Daniil Tiapkin, Denis Belomestny, Daniele Calandriello, Eric Moulines, Remi Munos, Alexey Naumov, Mark Rowland, Michal Valko, and Pierre M{\'e}nard.
\newblock Optimistic posterior sampling for reinforcement learning with few samples and tight guarantees.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 10737--10751, 2022.

\bibitem[Wang et~al.(2020)Wang, Salakhutdinov, and Yang]{wang2020reinforcement_eluder}
Ruosong Wang, Russ~R Salakhutdinov, and Lin Yang.
\newblock Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Wang et~al.(2021)Wang, Wang, Du, and Krishnamurthy]{wang2021glmRL}
Yining Wang, Ruosong Wang, Simon~Shaolei Du, and Akshay Krishnamurthy.
\newblock Optimism in reinforcement learning with generalized linear function approximation.
\newblock In \emph{9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}, 2021.

\bibitem[Weisz et~al.(2021)Weisz, Amortila, and Szepesv{\'a}ri]{weisz2021exponential}
Gell{\'e}rt Weisz, Philip Amortila, and Csaba Szepesv{\'a}ri.
\newblock Exponential lower bounds for planning in mdps with linearly-realizable optimal action-value functions.
\newblock In \emph{Algorithmic Learning Theory}, pages 1237--1264. PMLR, 2021.

\bibitem[Yang and Wang(2019)]{yang2019sample}
Lin Yang and Mengdi Wang.
\newblock Sample-optimal parametric q-learning using linearly additive features.
\newblock In \emph{International Conference on Machine Learning}, pages 6995--7004. PMLR, 2019.

\bibitem[Yang and Wang(2020)]{yang2020reinforcement}
Lin Yang and Mengdi Wang.
\newblock Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound.
\newblock In \emph{International Conference on Machine Learning}, pages 10746--10756. PMLR, 2020.

\bibitem[Zanette et~al.(2020)Zanette, Brandfonbrener, Brunskill, Pirotta, and Lazaric]{zanette2020frequentist}
Andrea Zanette, David Brandfonbrener, Emma Brunskill, Matteo Pirotta, and Alessandro Lazaric.
\newblock Frequentist regret bounds for randomized least-squares value iteration.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 1954--1964. PMLR, 2020.

\bibitem[Zhang et~al.(2016)Zhang, Yang, Jin, Xiao, and Zhou]{zhang2016online}
Lijun Zhang, Tianbao Yang, Rong Jin, Yichi Xiao, and Zhi-Hua Zhou.
\newblock Online stochastic linear optimization under one-bit feedback.
\newblock In \emph{International Conference on Machine Learning}, pages 392--401. PMLR, 2016.

\bibitem[Zhang(2022)]{zhang2022feel}
Tong Zhang.
\newblock Feel-good thompson sampling for contextual bandits and reinforcement learning.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 4\penalty0 (2):\penalty0 834--857, 2022.

\bibitem[Zhang and Sugiyama(2023)]{zhang2023online}
Yu-Jie Zhang and Masashi Sugiyama.
\newblock Online (multinomial) logistic bandit: Improved regret and constant computation cost.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Zhang et~al.(2020)Zhang, Zhou, and Ji]{zhang2020almostoptimal}
Zihan Zhang, Yuan Zhou, and Xiangyang Ji.
\newblock Almost optimal model-free reinforcement learningvia reference-advantage decomposition.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~33, pages 15198--15207, 2020.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Yang, Ji, and Du]{zhang2021improved}
Zihan Zhang, Jiaqi Yang, Xiangyang Ji, and Simon~S Du.
\newblock Improved variance-aware confidence sets for linear bandits and linear mixture mdp.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 4342--4355, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Zhou, and Ji]{zhang2021model}
Zihan Zhang, Yuan Zhou, and Xiangyang Ji.
\newblock Model-free reinforcement learning: from clipped pseudo-regret to sample complexity.
\newblock In \emph{International Conference on Machine Learning}, pages 12653--12662. PMLR, 2021{\natexlab{b}}.

\bibitem[Zhou and Gu(2022)]{zhou2022computationally}
Dongruo Zhou and Quanquan Gu.
\newblock Computationally efficient horizon-free reinforcement learning for linear mixture mdps.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 36337--36349, 2022.

\bibitem[Zhou et~al.(2021{\natexlab{a}})Zhou, Gu, and Szepesvari]{zhou2021nearly}
Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari.
\newblock Nearly minimax optimal reinforcement learning for linear mixture markov decision processes.
\newblock In \emph{Conference on Learning Theory}, pages 4532--4576. PMLR, 2021{\natexlab{a}}.

\bibitem[Zhou et~al.(2021{\natexlab{b}})Zhou, He, and Gu]{zhou2021provably}
Dongruo Zhou, Jiafan He, and Quanquan Gu.
\newblock Provably efficient reinforcement learning for discounted mdps with feature mapping.
\newblock In \emph{International Conference on Machine Learning}, pages 12793--12802. PMLR, 2021{\natexlab{b}}.

\end{thebibliography}
