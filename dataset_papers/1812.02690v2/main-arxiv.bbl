\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{OBvdOM17}

\bibitem[AN04]{Abbeel2004ApprenticeshipLV}
Pieter Abbeel and Andrew~Y. Ng.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In {\em ICML}, 2004.

\bibitem[AOM17]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock {\em arXiv preprint arXiv:1703.05449}, 2017.

\bibitem[B{\etalchar{+}}15]{bubeck2015convex}
S{\'e}bastien Bubeck et~al.
\newblock Convex optimization: Algorithms and complexity.
\newblock {\em Foundations and Trends in Machine Learning}, 8(3-4):231--357,
  2015.

\bibitem[BCP{\etalchar{+}}16]{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym.
\newblock {\em arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[BEP{\etalchar{+}}18]{pathak18largescale}
Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and
  Alexei~A. Efros.
\newblock Large-scale study of curiosity-driven learning.
\newblock In {\em arXiv:1808.04355}, 2018.

\bibitem[Ber05]{bertsekas2005dynamic}
Dimitri~P Bertsekas.
\newblock {\em Dynamic programming and optimal control}, volume~1.
\newblock Athena Scientific, 2005.

\bibitem[BESK18]{RandNetDist}
Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov.
\newblock Exploration by random network distillation.
\newblock {\em arXiv preprint arXiv:1810.12894}, 2018.

\bibitem[BSO{\etalchar{+}}16]{DBLP:conf/nips/BellemareSOSSM16}
Marc~G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David
  Saxton, and R{\'{e}}mi Munos.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In {\em Advances in Neural Information Processing Systems 29: Annual
  Conference on Neural Information Processing Systems 2016, December 5-10,
  2016, Barcelona, Spain}, pages 1471--1479, 2016.

\bibitem[CBS05]{NIPS2004_2552}
Nuttapong Chentanez, Andrew~G. Barto, and Satinder~P. Singh.
\newblock Intrinsically motivated reinforcement learning.
\newblock In L.~K. Saul, Y.~Weiss, and L.~Bottou, editors, {\em Advances in
  Neural Information Processing Systems 17}, pages 1281--1288. MIT Press, 2005.

\bibitem[DB15]{dann2015sample}
Christoph Dann and Emma Brunskill.
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2818--2826, 2015.

\bibitem[DFVR03]{de2003linear}
Daniela~Pucci De~Farias and Benjamin Van~Roy.
\newblock The linear programming approach to approximate dynamic programming.
\newblock {\em Operations research}, 51(6):850--865, 2003.

\bibitem[FCRL17]{NIPS2017_6851}
Justin Fu, John Co-Reyes, and Sergey Levine.
\newblock Ex2: Exploration with exemplar models for deep reinforcement
  learning.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems 30}, pages 2577--2587. Curran Associates, Inc., 2017.

\bibitem[FGKM18]{fazel2018global}
Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi.
\newblock Global convergence of policy gradient methods for the linear
  quadratic regulator.
\newblock In {\em International Conference on Machine Learning}, pages
  1466--1475, 2018.

\bibitem[FW56]{frank1956algorithm}
Marguerite Frank and Philip Wolfe.
\newblock An algorithm for quadratic programming.
\newblock {\em Naval Research Logistics (NRL)}, 3(1-2):95--110, 1956.

\bibitem[HCC{\etalchar{+}}16]{NIPS2016_6591}
Rein Houthooft, Xi~Chen, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and
  Pieter Abbeel.
\newblock Vime: Variational information maximizing exploration.
\newblock In D.~D. Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett,
  editors, {\em Advances in Neural Information Processing Systems 29}, pages
  1109--1117. Curran Associates, Inc., 2016.

\bibitem[HZAL18]{DBLP:journals/corr/abs-1801-01290}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock {\em CoRR}, abs/1801.01290, 2018.

\bibitem[Kae93]{Kaelbling93b}
Leslie~Pack Kaelbling.
\newblock Learning to achieve goals.
\newblock In {\em Proceedings of the Thirteenth International Joint Conference
  on Artificial Intelligence}, Chambery, France, 1993. Morgan Kaufmann.

\bibitem[Kak03]{kakade2003sample}
Sham~Machandranath Kakade.
\newblock {\em On the sample complexity of reinforcement learning}.
\newblock PhD thesis, University of London London, England, 2003.

\bibitem[KS02]{kearns2002near}
Michael Kearns and Satinder Singh.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock {\em Machine learning}, 49(2-3):209--232, 2002.

\bibitem[LH14]{lattimore2014near}
Tor Lattimore and Marcus Hutter.
\newblock Near-optimal pac bounds for discounted mdps.
\newblock {\em Theoretical Computer Science}, 558:125--143, 2014.

\bibitem[LLTyO12]{NIPS2012_4642}
Manuel Lopes, Tobias Lang, Marc Toussaint, and Pierre yves Oudeyer.
\newblock Exploration in model-based reinforcement learning by empirically
  estimating learning progress.
\newblock In F.~Pereira, C.~J.~C. Burges, L.~Bottou, and K.~Q. Weinberger,
  editors, {\em Advances in Neural Information Processing Systems 25}, pages
  206--214. Curran Associates, Inc., 2012.

\bibitem[MJR15]{NIPS2015_5668}
Shakir Mohamed and Danilo Jimenez~Rezende.
\newblock Variational information maximisation for intrinsically motivated
  reinforcement learning.
\newblock In C.~Cortes, N.~D. Lawrence, D.~D. Lee, M.~Sugiyama, and R.~Garnett,
  editors, {\em Advances in Neural Information Processing Systems 28}, pages
  2125--2133. Curran Associates, Inc., 2015.

\bibitem[MKS{\etalchar{+}}15]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529, 2015.

\bibitem[NHR99]{Ng1999PolicyIU}
Andrew~Y. Ng, Daishi Harada, and Stuart~J. Russell.
\newblock Policy invariance under reward transformations: Theory and
  application to reward shaping.
\newblock In {\em ICML}, 1999.

\bibitem[NPD{\etalchar{+}}18]{DBLP:journals/corr/abs-1807-04742}
Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey
  Levine.
\newblock Visual reinforcement learning with imagined goals.
\newblock {\em CoRR}, abs/1807.04742, 2018.

\bibitem[OAC18]{NIPS2018_8080}
Ian Osband, John Aslanides, and Albin Cassirer.
\newblock Randomized prior functions for deep reinforcement learning.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, {\em Advances in Neural Information Processing
  Systems 31}, pages 8625--8637. Curran Associates, Inc., 2018.

\bibitem[OBPVR16]{NIPS2016_6501}
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van~Roy.
\newblock Deep exploration via bootstrapped dqn.
\newblock In D.~D. Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett,
  editors, {\em Advances in Neural Information Processing Systems 29}, pages
  4026--4034. Curran Associates, Inc., 2016.

\bibitem[OBvdOM17]{DBLP:conf/icml/OstrovskiBOM17}
Georg Ostrovski, Marc~G. Bellemare, A{\"{a}}ron van~den Oord, and R{\'{e}}mi
  Munos.
\newblock Count-based exploration with neural density models.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017}, pages
  2721--2730, 2017.

\bibitem[PAED17]{pathakICMl17curiosity}
Deepak Pathak, Pulkit Agrawal, Alexei~A. Efros, and Trevor Darrell.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In {\em ICML}, 2017.

\bibitem[Put14]{puterman2014markov}
Martin~L Puterman.
\newblock {\em Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[RGB11]{Ross2011ARO}
St{\'e}phane Ross, Geoffrey~J. Gordon, and J.~Andrew Bagnell.
\newblock A reduction of imitation learning and structured prediction to
  no-regret online learning.
\newblock In {\em AISTATS}, 2011.

\bibitem[SHM{\etalchar{+}}16]{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em nature}, 529(7587):484, 2016.

\bibitem[SLB09]{Singh_wheredo}
Satinder Singh, Richard~L. Lewis, and Andrew~G. Barto.
\newblock Where do rewards come from?
\newblock {\em Proceedings of the Annual Conference of the Cognitive Science
  Society (CogSci)}, 2009.

\bibitem[SLBS10]{Singh2010IntrinsicallyMR}
Satinder~P. Singh, Richard~L. Lewis, Andrew~G. Barto, and Jonathan Sorg.
\newblock Intrinsically motivated reinforcement learning: An evolutionary
  perspective.
\newblock {\em IEEE Transactions on Autonomous Mental Development}, 2:70--82,
  2010.

\bibitem[SLW{\etalchar{+}}06]{strehl2006pac}
Alexander~L Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael~L
  Littman.
\newblock Pac model-free reinforcement learning.
\newblock In {\em Proceedings of the 23rd international conference on Machine
  learning}, pages 881--888. ACM, 2006.

\bibitem[SMSM00]{sutton2000policy}
Richard~S Sutton, David~A McAllester, Satinder~P Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In {\em Advances in neural information processing systems}, pages
  1057--1063, 2000.

\bibitem[SRM{\etalchar{+}}18]{DBLP:journals/corr/abs-1810-02274}
Nikolay Savinov, Anton Raichuk, Rapha{\"{e}}l Marinier, Damien Vincent, Marc
  Pollefeys, Timothy~P. Lillicrap, and Sylvain Gelly.
\newblock Episodic curiosity through reachability.
\newblock {\em CoRR}, abs/1810.02274, 2018.

\bibitem[SS10]{szita2010model}
Istv{\'a}n Szita and Csaba Szepesv{\'a}ri.
\newblock Model-based reinforcement learning with nearly tight exploration
  complexity bounds.
\newblock In {\em Proceedings of the 27th International Conference on Machine
  Learning (ICML-10)}, pages 1031--1038, 2010.

\bibitem[THF{\etalchar{+}}17]{NIPS2017_6868}
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi~Chen, Yan
  Duan, John Schulman, Filip DeTurck, and Pieter Abbeel.
\newblock \#exploration: A study of count-based exploration for deep
  reinforcement learning.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems 30}, pages 2753--2762. Curran Associates, Inc., 2017.

\bibitem[WdWK{\etalchar{+}}18]{DBLP:journals/corr/abs-1811-11359}
David Warde{-}Farley, Tom~Van de~Wiele, Tejas Kulkarni, Catalin Ionescu, Steven
  Hansen, and Volodymyr Mnih.
\newblock Unsupervised control through non-parametric discriminative rewards.
\newblock {\em CoRR}, abs/1811.11359, 2018.

\bibitem[WRR{\etalchar{+}}17]{weber2017imagination}
Th{\'e}ophane Weber, S{\'e}bastien Racani{\`e}re, David~P Reichert, Lars
  Buesing, Arthur Guez, Danilo~Jimenez Rezende, Adria~Puigdomenech Badia, Oriol
  Vinyals, Nicolas Heess, Yujia Li, et~al.
\newblock Imagination-augmented agents for deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1707.06203}, 2017.

\bibitem[ZOS18]{Zheng2018OnLI}
Zeyu Zheng, Junhyuk Oh, and Satinder Singh.
\newblock On learning intrinsic rewards for policy gradient methods.
\newblock {\em CoRR}, abs/1804.06459, 2018.

\end{thebibliography}
