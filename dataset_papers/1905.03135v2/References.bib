%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Patrick Rebeschini at 2017-12-16 15:46:39 +0000 

@article{tsianos2016efficient,
  title={Efficient distributed online prediction and stochastic optimization with approximate distributed averaging},
  author={Tsianos, Konstantinos I and Rabbat, Michael G},
  journal={IEEE Transactions on Signal and Information Processing over Networks},
  volume={2},
  number={4},
  pages={489--506},
  year={2016},
  publisher={IEEE}
}


%% Saved with string encoding Unicode (UTF-8) 
@INPROCEEDINGS{Rabbat15, 
author={M. {Rabbat}}, 
booktitle={2015 IEEE 6th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)}, 
title={Multi-agent mirror descent for decentralized stochastic optimization}, 
year={2015}, 
volume={}, 
number={}, 
pages={517-520}, 
keywords={convex programming;iterative methods;multi-agent systems;stochastic processes;decentralized algorithm;stochastic composite optimization problem;consensus-based multiagent optimization;mirror descent algorithm;iterative method;convex composite regularization term;Optimization;Mirrors;Stochastic processes;Signal processing algorithms;Conferences;Convergence;Convex functions}, 
doi={10.1109/CAMSAP.2015.7383850}, 
ISSN={}, 
month={Dec},}
@article{lian2017asynchronous,
  title={Asynchronous decentralized parallel stochastic gradient descent},
  author={Lian, Xiangru and Zhang, Wei and Zhang, Ce and Liu, Ji},
  journal={arXiv preprint arXiv:1710.06952},
  year={2017}
}
@article{assran2018stochastic,
  title={Stochastic gradient push for distributed deep learning},
  author={Assran, Mahmoud and Loizou, Nicolas and Ballas, Nicolas and Rabbat, Michael},
  journal={arXiv preprint arXiv:1811.10792},
  year={2018}
}
@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}
@inproceedings{lian2017can,
  title={Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent},
  author={Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5330--5340},
  year={2017}
}
@article{braca2008enforcing,
  title={Enforcing consensus while monitoring the environment in wireless sensor networks},
  author={Braca, Paolo and Marano, Stefano and Matta, Vincenzo},
  journal={IEEE Transactions on Signal Processing},
  volume={56},
  number={7},
  pages={3375--3380},
  year={2008},
  publisher={IEEE}
}
@article{polyak1992acceleration,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM Journal on Control and Optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992},
  publisher={SIAM}
}
@article{zhang2005learning,
  title={Learning bounds for kernel regression using effective data dimensionality},
  author={Zhang, Tong},
  journal={Neural Computation},
  volume={17},
  number={9},
  pages={2077--2098},
  year={2005},
  publisher={MIT Press}
}
@article{bijral2017data,
  title={Data-Dependent Convergence for Consensus Stochastic Optimization},
  author={Bijral, Avleen S and Sarwate, Anand D and Srebro, Nathan},
  journal={IEEE Transactions on Automatic Control},
  volume={62},
  number={9},
  pages={4483--4498},
  year={2017},
  publisher={IEEE}
}
@incollection{robbins1985stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  booktitle={Herbert Robbins Selected Papers},
  pages={102--109},
  year={1985},
  publisher={Springer}
}
@article{pinelis1986remarks,
  title={Remarks on inequalities for large deviation probabilities},
  author={Pinelis, IF and Sakhanenko, AI},
  journal={Theory of Probability \& Its Applications},
  volume={30},
  number={1},
  pages={143--148},
  year={1986},
  publisher={SIAM}
}
@article{mucke2018parallelizing,
  title={Parallelizing spectrally regularized kernel algorithms},
  author={M{\"u}cke, Nicole and Blanchard, Gilles},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={1069--1097},
  year={2018},
  publisher={JMLR. org}
}

@article{smale2007learning,
  title={Learning theory estimates via integral operators and their approximations},
  author={Smale, Steve and Zhou, Ding-Xuan},
  journal={Constructive approximation},
  volume={26},
  number={2},
  pages={153--172},
  year={2007},
  publisher={Springer}
}
@article{guo2017learning,
  title={Learning theory of distributed spectral algorithms},
  author={Guo, Zheng-Chu and Lin, Shao-Bo and Zhou, Ding-Xuan},
  journal={Inverse Problems},
  volume={33},
  number={7},
  pages={074009},
  year={2017},
  publisher={IOP Publishing}
}
@article{zhang2015divide,
  title={Divide and conquer kernel ridge regression: A distributed algorithm with minimax optimal rates},
  author={Zhang, Yuchen and Duchi, John and Wainwright, Martin},
  journal={The Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={3299--3340},
  year={2015},
  publisher={JMLR. org}
}
@article{lin2017distributed,
  title={Distributed learning with regularized least squares},
  author={Lin, Shao-Bo and Guo, Xin and Zhou, Ding-Xuan},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={3202--3232},
  year={2017},
  publisher={JMLR. org}
}
@article{richards2018graph,
  title={Graph-Dependent Implicit Regularisation for Distributed Stochastic Subgradient Descent},
  author={Richards, Dominic and Rebeschini, Patrick},
  journal={arXiv preprint arXiv:1809.06958},
  year={2018}
}
@article{dimakis2010gossip,
author = {Dimakis, Alexandros G and Kar, Soummya and Moura, Jos{\'{e}} M F and Rabbat, Michael G and Scaglione, Anna},
file = {:data/firetail/richards/DistributedOptimization/Papers/BibTexStuff/GossipAlgosForSignalProcessing.pdf:pdf},
journal = {Proceedings of the IEEE},
number = {11},
pages = {1847--1864},
publisher = {IEEE},
title = {{Gossip algorithms for distributed signal processing}},
volume = {98},
year = {2010}
}
@article{PolyakAverage1990,
author = {Polyak, Boris},
year = {1990},
month = {01},
pages = {98-107},
title = {New stochastic approximation type procedures},
volume = {7},
booktitle = {Avtomatica i Telemekhanika}
}

@techreport{ruppert1988efficient,
  title={Efficient estimations from a slowly convergent Robbins-Monro process},
  author={Ruppert, David},
  year={1988},
  institution={Cornell University Operations Research and Industrial Engineering}
}
@book{bertsekas1989parallel,
author = {Bertsekas, Dimitri P and Tsitsiklis, John N},
mendeley-groups = {LocalOrderOptimal},
publisher = {Prentice hall Englewood Cliffs, NJ},
title = {{Parallel and distributed computation: numerical methods}},
volume = {23},
year = {1989}
}
@book{bhatia2009positive,
  title={Positive definite matrices},
  author={Bhatia, Rajendra},
  volume={24},
  year={2009},
  publisher={Princeton university press}
}
@book{madras2013self,
  title={The self-avoiding walk},
  author={Madras, Neal and Slade, Gordon},
  year={2013},
  publisher={Springer Science \& Business Media}
}
@article{flory1949configuration,
  title={The configuration of real polymer chains},
  author={Flory, Paul J},
  journal={The Journal of Chemical Physics},
  volume={17},
  number={3},
  pages={303--310},
  year={1949},
  publisher={AIP}
}

@article{Jung2006,
author = {Jung, Kyomin},
doi = {10.1109/ITW.2006.1633783},
file = {:data/firetail/richards/DistributedOptimization/Papers/GossipAlgorithms/FastGossipNonRevJung.pdf:pdf},
isbn = {1424400368},
number = {c},
pages = {67--71},
title = {{Fast Gossip via Non-reversible Random Walk With {\~{}}{\~{}}{\~{}}{\~{}} th ' eeomn}},
year = {2006}
}

@article{Li2010,
abstract = {Existing works on distributed consensus explore linear iterations based on reversible Markov chains, which contribute to the slow convergence of the algorithms. It has been observed that by overcoming the diffusive behavior of reversible chains, certain nonreversible chains lifted from reversible ones mix substantially faster than the original chains. In this paper, we investigate the idea of accelerating distributed consensus via lifting Markov chains, and propose a class of Location-Aided Distributed Averaging (LADA) algorithms for wireless networks, where nodes' coarse location information is used to construct nonreversible chains that facilitate distributed computing and cooperative processing. First, two general pseudo-algorithms are presented to illustrate the notion of distributed averaging through chain-lifting. These pseudo-algorithms are then respectively instantiated through one LADA algorithm on grid networks, and one on general wireless networks. For a {\$}k\backslashtimes k{\$} grid network, the proposed LADA algorithm achieves an {\$}\backslashepsilon{\$}-averaging time of {\$}O(k\backslashlog(\backslashepsilon{\^{}}{\{}-1{\}})){\$}. Based on this algorithm, in a wireless network with transmission range {\$}r{\$}, an {\$}\backslashepsilon{\$}-averaging time of {\$}O(r{\^{}}{\{}-1{\}}\backslashlog(\backslashepsilon{\^{}}{\{}-1{\}})){\$} can be attained through a centralized algorithm. Subsequently, we present a fully-distributed LADA algorithm for wireless networks, which utilizes only the direction information of neighbors to construct nonreversible chains. It is shown that this distributed LADA algorithm achieves the same scaling law in averaging time as the centralized scheme. Finally, we propose a cluster-based LADA (C-LADA) algorithm, which, requiring no central coordination, provides the additional benefit of reduced message complexity compared with the distributed LADA algorithm.},
archivePrefix = {arXiv},
arxivId = {0707.0500},
author = {Li, Wenjun and Dai, Huaiyu and Zhang, Yanbing},
doi = {10.1109/TIT.2010.2081030},
eprint = {0707.0500},
file = {:data/firetail/richards/DistributedOptimization/Papers/GossipAlgorithms/LocationAidedDistributedConsensus.pdf:pdf},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Clustering,distributed computation,distributed consensus,message complexity,mixing time,nonreversible Markov chains,time complexity},
number = {12},
pages = {6208--6227},
title = {{Location-aided fast distributed consensus in wireless networks}},
volume = {56},
year = {2010}
}
@inproceedings{chen1999lifting,
  title={Lifting Markov chains to speed up mixing},
  author={Chen, Fang and Lov{\'a}sz, L{\'a}szl{\'o} and Pak, Igor},
  booktitle={Proceedings of the thirty-first annual ACM symposium on Theory of computing},
  pages={275--281},
  year={1999},
  organization={ACM}
}
@article{diaconis2000analysis,
author = {Diaconis, Persi and Holmes, Susan and Neal, Radford M},
file = {:data/firetail/richards/DistributedOptimization/Papers/BibTexStuff/Persi{\_}Lifted{\_}MarkovChains.ps:ps},
journal = {Annals of Applied Probability},
pages = {726--752},
publisher = {JSTOR},
title = {{Analysis of a nonreversible Markov chain sampler}},
year = {2000}
}



@article{Scaglione2010,
author = {Scaglione, Anna and Ieee, Senior Member},
file = {:data/firetail/richards/DistributedOptimization/Papers/BibTexStuff/GossipAlgosForSignalProcessing.pdf:pdf},
isbn = {9550101029},
number = {11},
title = {{Gossip Algorithms for Distributed Signal Processing}},
volume = {98},
year = {2010}
}

@article{degroot1974reaching,
author = {DeGroot, Morris H},
file = {:data/firetail/richards/DistributedOptimization/Papers/GossipAlgorithms/degroot consensus.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {345},
pages = {118--121},
publisher = {Taylor $\backslash${\&} Francis},
title = {{Reaching a consensus}},
volume = {69},
year = {1974}
}
@article{Xiao2004,
abstract = {We consider the problem of finding a linear iteration that yields distributed averaging consensus over a network, i.e., that asymptotically computes the average of some initial values given at the nodes. When the iteration is assumed symmetric, the problem of finding the fastest converging linear iteration can be cast as a semidefinite program, and therefore efficiently and globally solved. These optimal linear iterations are often substantially faster than several common heuristics that are based on the Laplacian of the associated graph. We show how problem structure can be exploited to speed up interior-point methods for solving the fastest distributed linear iteration problem, for networks with up to a thousand or so edges. We also describe a simple subgradient method that handles far larger problems, with up to 100000 edges. We give several extensions and variations on the basic problem. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Xiao, Lin and Boyd, Stephen},
doi = {10.1016/j.sysconle.2004.02.022},
file = {:data/firetail/richards/DistributedOptimization/Papers/GossipAlgorithms/FastLinearIterationsXiaoBoyd.pdf:pdf},
isbn = {0-7803-7924-1},
issn = {01676911},
journal = {Systems and Control Letters},
keywords = {Distributed consensus,Graph Laplacian,Linear system,Semidefinite programming,Spectral radius},
number = {1},
pages = {65--78},
title = {{Fast linear iterations for distributed averaging}},
volume = {53},
year = {2004}
}

@article{Kempe2003,
abstract = {Over the last decade, we have seen a revolution in connectivity between computers, and a resulting paradigm shift from centralized to highly distributed systems. With massive scale also comes massive instability, as node and link failures become the norm rather than the exception. For such highly volatile systems, decentralized gossip-based protocols are emerging as an approach to maintaining simplicity and scalability while achieving fault-tolerant information dissemination.In this paper, we study the problem of computing aggregates with gossip-style protocols. Our first contribution is an analysis of simple gossip-based protocols for the computations of sums, averages, random samples, quantiles, and other aggregate functions, and we show that our protocols converge exponentially fast to the true answer when using uniform gossip.Our second contribution is the definition of a precise notion of the speed with which a node's data diffuses through the network. We show that this diffusion speed is at the heart of the approximation guarantees for all of the above problems. We analyze the diffusion speed of uniform gossip in the presence of node and link failures, as well as for flooding-based mechanisms. The latter expose interesting connections to random walks on graphs.},
author = {Kempe, David and Dobra, Alin and Gehrke, Johannes},
file = {:data/firetail/richards/DistributedOptimization/Papers/GossipAlgorithms/focs2003-gossip.pdf:pdf},
isbn = {978-0-7695-2040-7},
pages = {482--},
title = {{Gossip-Based Computation of Aggregate Information}},
url = {http://dl.acm.org/citation.cfm?id=946243.946317},
year = {2003}
}
@article{Boyd2006,
abstract = {Motivated by applications to sensor, peer-to-peer, and ad hoc networks, we study distributed algorithms, also known as gossip algorithms, for exchanging information and for computing in an arbitrarily connected network of nodes. The topology of such networks changes continuously as new nodes join and old nodes leave the network. Algorithms for such networks need to be robust against changes in topology. Additionally, nodes in sensor networks operate under limited computational, communication, and energy resources. These constraints have motivated the design of "gossip" algorithms: schemes which distribute the computational burden and in which a node communicates with a randomly chosen neighbor. We analyze the averaging problem under the gossip constraint for an arbitrary network graph, and find that the averaging time of a gossip algorithm depends on the second largest eigenvalue of a doubly stochastic matrix characterizing the algorithm. Designing the fastest gossip algorithm corresponds to minimizing this eigenvalue, which is a semidefinite program (SDP). In general, SDPs cannot be solved in a distributed fashion; however, exploiting problem structure, we propose a distributed subgradient method that solves the optimization problem over the network. The relation of averaging time to the second largest eigenvalue naturally relates it to the mixing time of a random walk with transition probabilities derived from the gossip algorithm. We use this connection to study the performance and scaling of gossip algorithms on two popular networks: Wireless Sensor Networks, which are modeled as Geometric Random Graphs, and the Internet graph under the so-called Preferential Connectivity (PC) model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1203.6028v1},
author = {Boyd, Stephen and Ghosh, Arpita and Prabhakar, Balaji and Shah, Devavrat},
doi = {10.1109/TIT.2006.874516},
eprint = {arXiv:1203.6028v1},
file = {:data/firetail/richards/DistributedOptimization/Papers/GossipAlgorithms/gossip.pdf:pdf},
isbn = {0780386825},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Distributed averaging,Gossip,Random walk,Scaling laws,Semidefinite programming,Sensor networks},
number = {6},
pages = {2508--2530},
pmid = {1638541},
title = {{Randomized gossip algorithms}},
volume = {52},
year = {2006}
}
@article{Freschi2017,
author = {Freschi, Valerio and Lattanzi, Emanuele and Bogliolo, Alessandro},
doi = {10.1007/s11277-017-4451-5},
file = {:data/firetail/richards/DistributedOptimization/Papers/GossipAlgorithms/Fast Distributed Consensus Through Path Averaging on Random Walks.pdf:pdf},
issn = {1572834X},
journal = {Wireless Personal Communications},
keywords = {Ad hoc networks,Distributed consensus,Wireless sensor networks},
pages = {1--15},
publisher = {Springer US},
title = {{Fast Distributed Consensus Through Path Averaging on Random Walks}},
year = {2017}
}
@article{Liu2013,
abstract = {Gossiping is a distributed process whose purpose is to enable the members of a group of n{\textgreater}1 autonomous agents to asymptotically determine in a decentralized manner, the average of the initial values of their scalar gossip variables. This paper analyzes the accelerated gossip algorithms, first proposed in Cao, Spielman, and Yeh (2006), in which local memory is exploited by installing shift-registers at each agent. For the two-register case, the existence of the desired convergence is established under a symmetry assumption by separately studying the convergence in expectation and in mean square. In particular, the optimal rate of convergence in expectation is derived which is faster than that of the standard gossip algorithm, and a sufficient condition on the adjustable parameter for the convergence in mean square is provided. These theoretical results are validated for some classes of networks by comparison with existing empirical data. More general multi-register cases are also discussed. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
author = {Liu, Ji and Anderson, Brian D.O. and Cao, Ming and Morse, A. Stephen},
doi = {10.1016/j.automatica.2013.01.001},
file = {:data/firetail/richards/DistributedOptimization/Papers/GossipAlgorithms/2013{\_}automatica{\_}gossip.pdf:pdf},
isbn = {9781424438716},
issn = {00051098},
journal = {Automatica},
keywords = {Convergence rate,Cooperative control,Distributed averaging},
number = {4},
pages = {873--883},
title = {{Analysis of accelerated gossip algorithms}},
volume = {49},
year = {2013}
}
@article{Nedic2017,
abstract = {In this paper, we develop a class of decentralized algorithms for solving a convex resource allocation problem in a network of {\$}n{\$} agents, where the agent objectives are decoupled while the resource constraints are coupled. The agents communicate over a connected undirected graph, and they want to collaboratively determine a solution to the overall network problem, while each agent only communicates with its neighbors. We first study the connection between the decentralized resource allocation problem and the decentralized consensus optimization problem. Then, using a class of algorithms for solving consensus optimization problems, we propose a novel class of decentralized schemes for solving resource allocation problems in a distributed manner. Specifically, we first propose an algorithm for solving the resource allocation problem with an {\$}o(1/k){\$} convergence rate guarantee when the agents' objective functions are generally convex (could be nondifferentiable) and per agent local convex constraints are allowed; We then propose a gradient-based algorithm for solving the resource allocation problem when per agent local constraints are absent and show that such scheme can achieve geometric rate when the objective functions are strongly convex and have Lipschitz continuous gradients. We have also provided scalability/network dependency analysis. Based on these two algorithms, we have further proposed a gradient projection-based algorithm which can handle smooth objective and simple constraints more efficiently. Numerical experiments demonstrates the viability and performance of all the proposed algorithms.},
archivePrefix = {arXiv},
arxivId = {1706.05441},
author = {Nedi{\{}$\backslash$'c{\}}, Angelia and Olshevsky, Alex and Shi, Wei},
eprint = {1706.05441},
file = {:data/firetail/richards/DistributedOptimization/Papers/DistributedOptim/DistributedResourceAllocation.pdf:pdf},
journal = {arXiv},
pages = {1--36},
title = {{Improved Convergence Rates for Distributed Resource Allocation}},
url = {http://arxiv.org/abs/1706.05441},
year = {2017}
}
@article{sinclair1992improved,
  title={Improved bounds for mixing rates of Markov chains and multicommodity flow},
  author={Sinclair, Alistair},
  journal={Combinatorics, probability and Computing},
  volume={1},
  number={4},
  pages={351--370},
  year={1992},
  publisher={Cambridge University Press}
}

@article{diaconis1991geometric,
  title={Geometric bounds for eigenvalues of Markov chains},
  author={Diaconis, Persi and Stroock, Daniel},
  journal={The Annals of Applied Probability},
  pages={36--61},
  year={1991},
  publisher={JSTOR}
}
@article{Boyd2004,
abstract = {We study the distributed averaging problem on an arbitrary network with a gossip constraint, which means that no node communicates with more than one neighbour in every time slot. We consider algorithms which are linear iterations, where each iteration is described by a random matrix picked i.i.d. from some distribution. We derive conditions that this distribution must satisfy so that the sequence of iterations converges to the vector of averages in different senses. We then analyze a simple asynchronous randomized gossip algorithm for averaging, and show that the problem of optimizing the parameters of this algorithm for fastest convergence is a semi-definite program. Finally we study the relation between Markov chains and the averaging problem, and relate the averaging time of the algorithm to the mixing time of a related Markov chain on the graph.},
author = {Boyd, S. and Ghosh, A. and Prabhakar, B. and Shah, D.},
doi = {10.1109/CDC.2004.1429652},
file = {:data/firetail/richards/DistributedOptimization/Papers/GossipAlgorithms/AnalysisAndOptimGossipAlgo.pdf:pdf},
isbn = {0-7803-8682-5},
issn = {0191-2216},
journal = {2004 43rd IEEE Conference on Decision and Control (CDC) (IEEE Cat. No.04CH37601)},
mendeley-groups = {LocalOrderOptimal},
pages = {5310--5315},
title = {{Analysis and optimization of randomized gossip algorithms}},
volume = {5},
year = {2004}
}

@misc{Li2007,
author = {Li, Wenjun and Dai, Huaiyu},
file = {:data/firetail/richards/DistributedOptimization/Papers/GossipAlgorithms/LiftedMarkovChainGeometricGraph.pdf:pdf},
title = {{Accelerating distributed consensus via lifting markov chains}},
year = {2007}
}
@book{penrose2003random,
author = {Penrose, Mathew},
number = {5},
publisher = {Oxford University Press},
title = {{Random geometric graphs}},
year = {2003}
}

@article{Benezit2010,
abstract = {Gossip algorithms have recently received significant attention, mainly because they constitute simple and robust message-passing schemes for distributed information processing over networks. However, for many topologies that are realistic for wireless ad-hoc and sensor networks (like grids and random geometric graphs), the standard nearest-neighbor gossip converges as slowly as flooding (O(n2) messages). A recently proposed algorithm called geographic gossip improves gossip efficiency by a {\&}{\#}x221A;n factor, by exploiting geographic information to enable multihop long-distance communications. This paper proves that a variation of geographic gossip that averages along routed paths, improves efficiency by an additional {\&}{\#}x221A;n factor, and is order optimal (O(n) messages) for grids and random geometric graphs with high probability. We develop a general technique (travel agency method) based on Markov chain mixing time inequalities which can give bounds on the performance of randomized message-passing algorithms operating over various graph topologies.},
archivePrefix = {arXiv},
arxivId = {0802.2587},
author = {B{\'{e}}n{\'{e}}zit, Florence and Dimakis, Alexandros G. and Thiran, Patrick and Vetterli, Martin},
doi = {10.1109/TIT.2010.2060050},
eprint = {0802.2587},
file = {:data/firetail/richards/DistributedOptimization/Papers/GossipAlgorithms/Order-Optimal Consensus Through Randomized Path Averaging.pdf:pdf},
isbn = {9781605600864},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Average consensus,distributed algorithms,gossip algorithms,sensor networks},
number = {10},
pages = {5150--5167},
title = {{Order-optimal consensus through randomized path averaging}},
volume = {56},
year = {2010}
}

@article{Wai2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1612.01216v1},
author = {Wai, Hoi-To and Lafond, Jean and Scaglione, Anna and Moulines, Eric},
doi = {10.1109/TAC.2017.2685559},
eprint = {arXiv:1612.01216v1},
file = {:data/firetail/richards/DistributedOptimization/Papers/DistributedOptim/Decentralized Frank-Wolfe Algorithm.pdf:pdf},
issn = {0018-9286},
journal = {IEEE Transactions on Automatic Control},
number = {c},
pages = {1--1},
title = {{Decentralized Frank-Wolfe Algorithm for Convex and Non-convex Problems}},
url = {http://ieeexplore.ieee.org/document/7883821/},
volume = {9286},
year = {2017}
}
@article{Dimakis2006,
abstract = {Gossip algorithms for aggregation have recently received significant attention for sensor network applications because of their simplicity and robustness in noisy and uncertain environments. However, gossip algorithms can waste significant energy by essentially passing around redundant information multiple times. For realistic sensor network model topologies like grids and random geometric graphs, the inefficiency of gossip schemes is caused by slow mixing times of random walks on those graphs. We propose and analyze an alternative gossiping scheme that exploits geographic information. By utilizing a simple resampling method, we can demonstrate substantial gains over previously proposed gossip protocols. In particular, for random geometric graphs, our algorithm computes the true average to accuracy 1/na using O(n1.5radic(logn)) radio transmissions, which reduces the energy consumption by a radic(n/logn) factor over standard gossip algorithms},
archivePrefix = {arXiv},
arxivId = {cs/0602071},
author = {Dimakis, A. G and Sarwate, A. D and Wainwright, M. J},
doi = {10.1109/IPSN.2006.244081},
eprint = {0602071},
file = {:data/firetail/richards/DistributedOptimization/Papers/GossipAlgorithms/GeographicGossip.pdf:pdf},
isbn = {1-59593-334-4},
journal = {2006 5th International Conference on Information Processing in Sensor Networks},
keywords = {distributed aggre-,distributed consensus,gossip algorithms,random geometric graphs,sensor networks},
pages = {69--76},
primaryClass = {cs},
title = {{Geographic gossip: efficient aggregation for sensor networks}},
year = {2006}
}

@article{Benezit2010a,
abstract = {This paper presents a general class of gossip-based averaging algorithms, which are inspired from Uniform Gossip. While Uniform Gossip works synchronously on complete graphs, weighted gossip algorithms allow asynchronous rounds and converge on any connected, directed or undirected graph. Unlike most previous gossip algorithms, Weighted Gossip admits stochastic update matrices which need not be doubly stochastic. Double-stochasticity being very restrictive in a distributed setting, this novel degree of freedom is essential and it opens the perspective of designing a large number of new gossip-based algorithms. To give an example, we present one of these algorithms, which we call One-Way Averaging. It is based on random geographic routing, just like Path Averaging, except that routes are one way instead of round trip. Hence in this example, getting rid of double stochasticity allows us to add robustness to Path Averaging.},
author = {B{\'{e}}n{\'{e}}zit, Florence and Blondel, Vincent and Thiran, Patrick and Tsitsiklis, John and Vetterli, Martin},
doi = {10.1109/ISIT.2010.5513273},
file = {:data/firetail/richards/DistributedOptimization/Papers/GossipAlgorithms/NonDoublyStochasticGossip.pdf:pdf},
isbn = {9781424469604},
issn = {21578103},
journal = {IEEE International Symposium on Information Theory - Proceedings},
pages = {1753--1757},
title = {{Weighted gossip: Distributed averaging using non-doubly stochastic matrices}},
year = {2010}
}

@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}
@book{cucker2007learning,
  title={Learning theory: an approximation theory viewpoint},
  author={Cucker, Felipe and Zhou, Ding Xuan},
  volume={24},
  year={2007},
  publisher={Cambridge University Press}
}

@inproceedings{shamir2014distributed,
  title={Distributed stochastic optimization and learning},
  author={Shamir, Ohad and Srebro, Nathan},
  booktitle={Communication, Control, and Computing (Allerton), 2014 52nd Annual Allerton Conference on},
  pages={850--857},
  year={2014},
  organization={IEEE}
}
@inproceedings{rosasco2015learning,
  title={Learning with incremental iterative regularization},
  author={Rosasco, Lorenzo and Villa, Silvia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1630--1638},
  year={2015}
}
@article{hoerl1970ridge,
  title={Ridge regression: Biased estimation for nonorthogonal problems},
  author={Hoerl, Arthur E and Kennard, Robert W},
  journal={Technometrics},
  volume={12},
  number={1},
  pages={55--67},
  year={1970},
  publisher={Taylor \& Francis Group}
}
@InProceedings{ShamirDANE,
  title =    {Communication-Efficient Distributed Optimization using an Approximate Newton-type Method},
  author =   {Ohad Shamir and Nati Srebro and Tong Zhang},
  booktitle =    {Proceedings of the 31st International Conference on Machine Learning},
  pages =    {1000--1008},
  year =   {2014},
  editor =   {Eric P. Xing and Tony Jebara},
  volume =   {32},
  number =       {2},
  series =   {Proceedings of Machine Learning Research},
  address =    {Bejing, China},
  month =    {22--24 Jun},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v32/shamir14.pdf},
  url =    {http://proceedings.mlr.press/v32/shamir14.html},
  abstract =   {We present a novel Newton-type method for distributed optimization,  which is particularly well suited for stochastic optimization and  learning problems.  For quadratic objectives, the method enjoys a  linear rate of convergence which provably \emphimproves with the  data size, requiring an essentially constant number of iterations  under reasonable assumptions.  We provide theoretical and empirical  evidence of the advantages of our method compared to other  approaches, such as one-shot parameter averaging and ADMM.}
}

@article{aronszajn1950theory,
  title={Theory of reproducing kernels},
  author={Aronszajn, Nachman},
  journal={Transactions of the American mathematical society},
  volume={68},
  number={3},
  pages={337--404},
  year={1950},
  publisher={JSTOR}
}
@incollection{pillaud2018statistical,
title = {Statistical Optimality of Stochastic Gradient Descent on Hard Learning Problems through Multiple Passes},
author = {Pillaud-Vivien, Loucas and Rudi, Alessandro and Bach, Francis},
booktitle = {Advances in Neural Information Processing Systems 31},
pages = {8125--8135},
year = {2018},
}
@article{sodin2007random,
  title={Random matrices, nonbacktracking walks, and orthogonal polynomials},
  author={Sodin, Sasha},
  journal={Journal of Mathematical Physics},
  volume={48},
  number={12},
  pages={123503},
  year={2007},
  publisher={AIP}
}
@inproceedings{bach2013non,
  title={Non-strongly-convex smooth stochastic approximation with convergence rate O (1/n)},
  author={Bach, Francis and Moulines, Eric},
  booktitle={Advances in neural information processing systems},
  pages={773--781},
  year={2013}
}
@ARTICLE{berthier2018gossip,
       author = {{Berthier}, Rapha{\"e}l and {Bach}, Francis and {Gaillard}, Pierre},
        title = "{Accelerated Gossip in Networks of Given Dimension using Jacobi Polynomial Iterations}",
        journal={arXiv preprint arXiv:1805.08531},
         year = "2018",
        month = "May"
}
@article{mohar1989survey,
  title={A survey on spectra of infinite graphs},
  author={Mohar, Bojan and Woess, Wolfgang},
  journal={Bulletin of the London Mathematical Society},
  volume={21},
  number={3},
  pages={209--234},
  year={1989},
  publisher={Wiley Online Library}
}
@book{gu2013smoothing,
  title={Smoothing spline ANOVA models},
  author={Gu, Chong},
  volume={297},
  year={2013},
  publisher={Springer Science \& Business Media}
}
@inproceedings{hsu2012random,
  title={Random design analysis of ridge regression},
  author={Hsu, Daniel and Kakade, Sham M and Zhang, Tong},
  booktitle={Conference on Learning Theory},
  pages={9--1},
  year={2012}
}
@incollection{tsybakov2003optimal,
  title={Optimal rates of aggregation},
  author={Tsybakov, Alexandre B},
  booktitle={Learning Theory and Kernel Machines},
  pages={303--313},
  year={2003},
  publisher={Springer}
}
@book{gyorfi2006distribution,
  title={A distribution-free theory of nonparametric regression},
  author={Gy{\"o}rfi, L{\'a}szl{\'o} and Kohler, Michael and Krzyzak, Adam and Walk, Harro},
  year={2006},
  publisher={Springer Science \& Business Media}
}
@article{dimakis2010gossipSignal,
  title={Gossip algorithms for distributed signal processing},
  author={Dimakis, Alexandros G and Kar, Soummya and Moura, Jos{\'e} MF and Rabbat, Michael G and Scaglione, Anna},
  journal={Proceedings of the IEEE},
  volume={98},
  number={11},
  pages={1847--1864},
  year={2010},
  publisher={IEEE}
}
@article{stankovic2011decentralized,
  title={Decentralized parameter estimation by consensus based stochastic approximation},
  author={Stankovic, Srdjan S and Stankovic, Milo{\v{s}} S and Stipanovic, Du{\v{s}}an M},
  journal={IEEE Transactions on Automatic Control},
  volume={56},
  number={3},
  pages={531--543},
  year={2011},
  publisher={IEEE}
}
@article{kar2011convergence,
  title={Convergence rate analysis of distributed gossip (linear parameter) estimation: Fundamental limits and tradeoffs},
  author={Kar, Soummya and Moura, Jos{\'e} MF},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={5},
  number={4},
  pages={674--690},
  year={2011},
  publisher={IEEE}
}
@article{kar2013distributed,
  title={Distributed linear parameter estimation: Asymptotically efficient adaptive strategies},
  author={Kar, Soummya and Moura, Jos{\'e} MF and Poor, H Vincent},
  journal={SIAM Journal on Control and Optimization},
  volume={51},
  number={3},
  pages={2200--2229},
  year={2013},
  publisher={SIAM}
}
@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}

@article{kar2012distributed,
  title={Distributed parameter estimation in sensor networks: Nonlinear observation models and imperfect communication},
  author={Kar, Soummya and Moura, Jos{\'e} MF and Ramanan, Kavita},
  journal={IEEE Transactions on Information Theory},
  volume={58},
  number={6},
  pages={3575--3605},
  year={2012},
  publisher={Institute of Electrical and Electronics Engineers, Inc., 345 E. 47 th St. NY NY 10017-2394 United States}
}
@article{jain2018parallelizing,
  title={Parallelizing Stochastic Gradient Descent for Least Squares Regression: Mini-batching, Averaging, and Model Misspecification},
  author={Jain, Prateek and Kakade, Sham M and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={223},
  pages={1--42},
  year={2018}
}
@article{de2005model,
  title={Model selection for regularized least-squares algorithm in learning theory},
  author={De Vito, Ernesto and Caponnetto, Andrea and Rosasco, Lorenzo},
  journal={Foundations of Computational Mathematics},
  volume={5},
  number={1},
  pages={59--85},
  year={2005},
  publisher={Springer}
}
@article{caponnetto2007optimal,
  title={Optimal rates for the regularized least-squares algorithm},
  author={Caponnetto, Andrea and De Vito, Ernesto},
  journal={Foundations of Computational Mathematics},
  volume={7},
  number={3},
  pages={331--368},
  year={2007},
  publisher={Springer}
}

@InProceedings{RosascoIterateAveraging,
  title =    {Iterate Averaging as Regularization for Stochastic Gradient Descent},
  author =   {Neu, Gergely and Rosasco, Lorenzo},
  booktitle =    {Proceedings of the 31st  Conference On Learning Theory},
  pages =    {3222--3242},
  year =   {2018},
  editor =   {Bubeck, S\'ebastien and Perchet, Vianney and Rigollet, Philippe},
  volume =   {75},
  series =   {Proceedings of Machine Learning Research},
  address =    {},
  month =    {06--09 Jul},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v75/neu18a/neu18a.pdf},
  url =    {http://proceedings.mlr.press/v75/neu18a.html},
  abstract =   {We propose and analyze a variant of the classic  Polyak–Ruppert averaging scheme, broadly used in stochastic gradient methods.  Rather than a uniform average of the iterates, we consider a weighted average, with weights decaying in a geometric fashion. In the context of linear least-squares regression, we show that this averaging scheme has the same regularizing effect, and indeed is asymptotically equivalent, to ridge regression. In particular, we derive finite-sample bounds for the proposed approach that match the best known results for regularized stochastic gradient methods.}
}
@book{scholkopf2002learning,
  title={Learning with kernels: support vector machines, regularization, optimization, and beyond},
  author={Sch{\"o}lkopf, Bernhard and Smola, Alexander J and others},
  year={2002},
  publisher={MIT press}
}
@article{gusrialdi2017distributed,
  title={Distributed estimation of all the eigenvalues and eigenvectors of matrices associated with strongly connected digraphs},
  author={Gusrialdi, Azwirman and Qu, Zhihua},
  journal={IEEE control systems letters},
  volume={1},
  number={2},
  pages={328--333},
  year={2017},
  publisher={IEEE}
}
@book{steinwart2008support,
  title={Support vector machines},
  author={Steinwart, Ingo and Christmann, Andreas},
  year={2008},
  publisher={Springer Science \& Business Media}
}
@book{levin2017markov,
  title={Markov chains and mixing times},
  author={Levin, David A and Peres, Yuval},
  volume={107},
  year={2017},
  publisher={American Mathematical Soc.}
}
@book{chung1997spectral,
  title={Spectral graph theory},
  author={Chung, Fan R.K. and Graham, Fan Chung},
  number={92},
  year={1997},
  publisher={American Mathematical Soc.}
}
@inproceedings{zhang2015disco,
  title={DiSCO: Distributed optimization for self-concordant empirical loss},
  author={Zhang, Yuchen and Lin, Xiao},
  booktitle={International conference on machine learning},
  pages={362--370},
  year={2015}
}
@book{nesterov2013introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2013},
  publisher={Springer Science \& Business Media}
}
@article{yao2007early,
  title={On early stopping in gradient descent learning},
  author={Yao, Yuan and Rosasco, Lorenzo and Caponnetto, Andrea},
  journal={Constructive Approximation},
  volume={26},
  number={2},
  pages={289--315},
  year={2007},
  publisher={Springer}
}
@article{sayed2014adaptive,
  title={Adaptive networks},
  author={Sayed, Ali H.},
  journal={Proceedings of the IEEE},
  volume={102},
  number={4},
  pages={460--497},
  year={2014},
  publisher={IEEE}
}
@inproceedings{liu2017distributed,
  title={Distributed multi-task relationship learning},
  author={Liu, Sulin and Pan, Sinno Jialin and Ho, Qirong},
  booktitle={Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={937--946},
  year={2017},
  organization={ACM}
}

@article{LRX16,
 author = {Lin, Junhong and Rosasco, Lorenzo and Zhou, Ding-Xuan},
 title = {Iterative Regularization for Learning with Convex Loss Functions},
 journal = {J. Mach. Learn. Res.},
 issue_date = {January 2016},
 volume = {17},
 number = {1},
 year = {2016},
 pages = {2718--2755}
} 
@article{jung2010distributed,
  title={Distributed averaging via lifted Markov chains},
  author={Jung, Kyomin and Shah, Devavrat and Shin, Jinwoo},
  journal={IEEE Transactions on Information Theory},
  volume={56},
  number={1},
  pages={634--647},
  year={2010},
  publisher={IEEE}
}
@ARTICLE{lowerbounds12, 
author={A. Agarwal and P. L. Bartlett and P. Ravikumar and M. J. Wainwright}, 
journal={IEEE Transactions on Information Theory}, 
title={Information-Theoretic Lower Bounds on the Oracle Complexity of Stochastic Convex Optimization}, 
year={2012}, 
volume={58}, 
number={5}, 
pages={3235-3249}, 
keywords={computational complexity;convex programming;learning (artificial intelligence);parameter estimation;statistical analysis;stochastic programming;function classes;information theory lower bound;machine learning;minimax complexity estimation;oracle complexity;statistical parameter estimation;stochastic convex optimization;upper bounds;Complexity theory;Convex functions;Optimization methods;Power capacitors;Stochastic processes;Upper bound;Computational learning theory;Fano's inequality;convex optimization;information-based complexity;minimax analysis;oracle complexity}, 
doi={10.1109/TIT.2011.2182178}, 
ISSN={0018-9448}, 
month={May},}

@Article{Lan2012,
author="Lan, Guanghui",
title="An optimal method for stochastic composite optimization",
journal="Mathematical Programming",
year="2012",
volume="133",
number="1",
pages="365--397",
abstract="This paper considers an important class of convex programming (CP) problems, namely, the stochastic composite optimization (SCO), whose objective function is given by the summation of general nonsmooth and smooth stochastic components. Since SCO covers non-smooth, smooth and stochastic CP as certain special cases, a valid lower bound on the rate of convergence for solving these problems is known from the classic complexity theory of convex programming. Note however that the optimization algorithms that can achieve this lower bound had never been developed. In this paper, we show that the simple mirror-descent stochastic approximation method exhibits the best-known rate of convergence for solving these problems. Our major contribution is to introduce the accelerated stochastic approximation (AC-SA) algorithm based on Nesterov's optimal method for smooth CP (Nesterov in Doklady AN SSSR 269:543--547, 1983; Nesterov in Math Program 103:127--152, 2005), and show that the AC-SA algorithm can achieve the aforementioned lower bound on the rate of convergence for SCO. To the best of our knowledge, it is also the first universally optimal algorithm in the literature for solving non-smooth, smooth and stochastic CP problems. We illustrate the significant advantages of the AC-SA algorithm over existing methods in the context of solving a special but broad class of stochastic programming problems."
}


@ARTICLE{BSS17, 
author={Avleen S. Bijral and Anand D. Sarwate and Nathan Srebro}, 
journal={IEEE Transactions on Automatic Control}, 
title={Data-Dependent Convergence for Consensus Stochastic Optimization}, 
year={2017}, 
volume={62}, 
number={9}, 
pages={4483-4498}, 
keywords={convergence;covariance matrices;gradient methods;optimisation;stochastic processes;consensus stochastic optimization;data-dependent convergence rate;distributed SGD algorithms;distributed consensus-based stochastic gradient descent algorithm;sample covariance matrix;Manganese;Silicon;Convergence;distributed computing;machine learning;minimization;optimization}
}

@article{Lin:255609,
      title = {Optimal Distributed Learning with Multi-pass Stochastic  Gradient Methods},
      author = {Lin, Junhong and Cevher, Volkan},
      journal = {Proceedings of the 35th International Conference on  Machine Learning},
      pages = {27},
      year = {2018},
      abstract = {We study generalization properties of distributed  algorithms in the setting of nonparametric regression over  a reproducing kernel Hilbert space (RKHS). We investigate  distributed stochastic gradient methods (SGM), with  mini-batches and multi-passes over the data. We show that  optimal generalization error bounds can be retained for  distributed SGM  provided that the partition level is not  too large.  Our results are superior to the  state-of-the-art theory, covering the cases that the  regression function may not be in the hypothesis spaces.  Particularly, our results show that distributed SGM has a  smaller theoretical computational complexity, compared with  distributed kernel ridge regression (KRR) and classic SGM.  },
}

@inproceedings{vanhaesebrouck2017decentralized,
  title={Decentralized collaborative learning of personalized models over networks},
  author={Vanhaesebrouck, Paul and Bellet, Aur{\'e}lien and Tommasi, Marc},
  booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2017}
}

@article{Wang2018distributed,
author = {Wang, Weiran and Wang, Jialei and Kolar, Mladen and Srebro, Nathan},
year = {2018},
month = {02},
journal={arXiv preprint arXiv:1802.03830},
title = {Distributed Stochastic Multi-Task Learning with Graph Regularization}
}

@book{bousquet2011advanced,
  title={Advanced Lectures on Machine Learning: ML Summer Schools 2003, Canberra, Australia, February 2-14, 2003, T{\"u}bingen, Germany, August 4-16, 2003, Revised Lectures},
  author={Bousquet, Olivier and von Luxburg, Ulrike and R{\"a}tsch, Gunnar},
  volume={3176},
  year={2011},
  publisher={Springer}
}
@inproceedings{sridharan2009fast,
  title={Fast rates for regularized objectives},
  author={Sridharan, Karthik and Shalev-Shwartz, Shai and Srebro, Nathan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1545--1552},
  year={2009}
}
@article{bartlett2005local,
  title={Local {R}ademacher complexities},
  author={Bartlett, Peter L and Bousquet, Olivier and Mendelson, Shahar and others},
  journal={The Annals of Statistics},
  volume={33},
  number={4},
  pages={1497--1537},
  year={2005},
  publisher={Institute of Mathematical Statistics}
}
@article{bartlett2002rademacher,
  title={Rademacher and Gaussian complexities: Risk bounds and structural results},
  author={Bartlett, Peter L and Mendelson, Shahar},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Nov},
  pages={463--482},
  year={2002}
}

@inproceedings{jiang2017collaborative,
  title={Collaborative Deep Learning in Fixed Topology Networks},
  author={Jiang, Zhanhong and Balu, Aditya and Hegde, Chinmay and Sarkar, Soumik},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5906--5916},
  year={2017}
}
@article{polyak1963gradient,
  title={Gradient methods for minimizing functionals},
  author={Polyak, Boris Teodorovich},
  journal={Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
  volume={3},
  number={4},
  pages={643--653},
  year={1963},
  publisher={Russian Academy of Sciences, Branch of Mathematical Sciences}
}
@inproceedings{evgeniou2004regularized,
  title={Regularized multi--task learning},
  author={Evgeniou, Theodoros and Pontil, Massimiliano},
  booktitle={Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={109--117},
  year={2004},
  organization={ACM}
}
@inproceedings{maurer2006rademacher,
  title={The {R}ademacher complexity of linear transformation classes},
  author={Maurer, Andreas},
  booktitle={International Conference on Computational Learning Theory},
  pages={65--78},
  year={2006},
  organization={Springer}
}
@article{bauer2007regularization,
  title={On regularization algorithms in learning theory},
  author={Bauer, Frank and Pereverzev, Sergei and Rosasco, Lorenzo},
  journal={Journal of complexity},
  volume={23},
  number={1},
  pages={52--72},
  year={2007},
  publisher={Elsevier}
}
@article{caponnetto2010cross,
  title={Cross-validation based adaptation for regularization operators in learning theory},
  author={Caponnetto, Andrea and Yao, Yuan},
  journal={Analysis and Applications},
  volume={8},
  number={02},
  pages={161--183},
  year={2010},
  publisher={World Scientific}
}
@article{konevcny2015federated,
  title={Federated optimization: Distributed optimization beyond the datacenter},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, Brendan and Ramage, Daniel},
  journal={arXiv preprint arXiv:1511.03575},
  year={2015}
}\\

@inproceedings{shalev2009stochastic,
  title={Stochastic Convex Optimization.},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
  booktitle={COLT},
  year={2009}
}
@article{xiao2010dual,
  title={Dual averaging methods for regularized stochastic learning and online optimization},
  author={Xiao, Lin},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Oct},
  pages={2543--2596},
  year={2010}
}


@inproceedings{shamir2014communication,
  title={Communication-efficient distributed optimization using an approximate newton-type method},
  author={Shamir, Ohad and Srebro, Nathan and Zhang, Tong},
  booktitle={International conference on machine learning},
  pages={1000--1008},
  year={2014}
}

@inproceedings{karimi2016linear,
  title={Linear convergence of gradient and proximal-gradient methods under the polyak-{\l}ojasiewicz condition},
  author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={795--811},
  year={2016},
  organization={Springer}
}

@article{kuzborskij2017data,
  title={Data-dependent stability of stochastic gradient descent},
  author={Kuzborskij, Ilja and Lampert, Christoph},
  journal={arXiv preprint arXiv:1703.01678},
  year={2017}
}
@inproceedings{lin2016generalization,
  title={Generalization properties and implicit regularization for multiple passes SGM},
  author={Lin, Junhong and Camoriano, Raffaello and Rosasco, Lorenzo},
  booktitle={International Conference on Machine Learning},
  pages={2340--2348},
  year={2016}
}
@book{Vapnik:1995:NSL:211359,
 author = {Vapnik, Vladimir N.},
 title = {The Nature of Statistical Learning Theory},
 year = {1995},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
} 
@book{vapnik2013nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={2013},
  publisher={Springer science \& business media}
}
@article{boucheron2005theory,
  title={Theory of classification: A survey of some recent advances},
  author={Boucheron, St{\'e}phane and Bousquet, Olivier and Lugosi, G{\'a}bor},
  journal={ESAIM: probability and statistics},
  volume={9},
  pages={323--375},
  year={2005},
  publisher={EDP Sciences}
}
@inproceedings{srebro2010smoothness,
  title={Smoothness, low noise and fast rates},
  author={Srebro, Nathan and Sridharan, Karthik and Tewari, Ambuj},
  booktitle={Advances in neural information processing systems},
  pages={2199--2207},
  year={2010}
}

@article{Jakovetic2014,
abstract = {We study distributed optimization problems when {\$}N{\$} nodes minimize the sum of their individual costs subject to a common vector variable. The costs are convex, have Lipschitz continuous gradient (with constant {\$}L{\$}), and bounded gradient. We propose two fast distributed gradient algorithms based on the centralized Nesterov gradient algorithm and establish their convergence rates in terms of the per-node communications {\$}\backslashmathcal{\{}K{\}}{\$} and the per-node gradient evaluations {\$}k{\$}. Our first method, Distributed Nesterov Gradient, achieves rates {\$}O\backslashleft({\{}\backslashlog \backslashmathcal{\{}K{\}}{\}}/{\{}\backslashmathcal{\{}K{\}}{\}}\backslashright){\$} and {\$}O\backslashleft({\{}\backslashlog k{\}}/{\{}k{\}}\backslashright){\$}. Our second method, Distributed Nesterov gradient with Consensus iterations, assumes at all nodes knowledge of {\$}L{\$} and {\$}\backslashmu(W){\$} -- the second largest singular value of the {\$}N \backslashtimes N{\$} doubly stochastic weight matrix {\$}W{\$}. It achieves rates {\$}O\backslashleft({\{}1{\}}/{\{}\backslashmathcal{\{}K{\}}{\^{}}{\{}2-\backslashxi{\}}{\}}\backslashright){\$} and {\$}O\backslashleft({\{}1{\}}/{\{}k{\^{}}2{\}}\backslashright){\$} ({\$}\backslashxi{\textgreater}0{\$} arbitrarily small). Further, we give with both methods explicit dependence of the convergence constants on {\$}N{\$} and {\$}W{\$}. Simulation examples illustrate our findings.},
archivePrefix = {arXiv},
arxivId = {1112.2972},
author = {Jakovetic, Dusan and Xavier, Joao and Moura, Jose M.F.},
doi = {10.1109/TAC.2014.2298712},
eprint = {1112.2972},
isbn = {9550101029},
issn = {00189286},
journal = {IEEE Transactions on Automatic Control},
keywords = {Consensus,Nesterov gradient,convergence rate,distributed optimization},
mendeley-groups = {Distributed Optimisation},
number = {5},
pages = {1131--1146},
title = {{Fast distributed gradient methods}},
volume = {59},
year = {2014}
}
@article{barnsley1989recurrent,
  title={Recurrent iterated function systems},
  author={Barnsley, Michael F and Elton, John H and Hardin, Douglas P},
  journal={Constructive approximation},
  volume={5},
  number={1},
  pages={3--31},
  year={1989},
  publisher={Springer}
}
@inproceedings{Chen2012,
abstract = {We present a distributed proximal-gradient method for optimizing the average of convex functions, each of which is the private local objective of an agent in a network with time-varying topology. The local objectives have distinct differentiable components, but they share a common nondifferentiable component, which has a favorable structure suitable for effective computation of the proximal operator. In our method, each agent iteratively updates its estimate of the global minimum by optimizing its local objective function, and exchanging estimates with others via communication in the network. Using Nesterov-type acceleration techniques and multiple communication steps per iteration, we show that this method converges at the rate 1/k (where k is the number of communication rounds between the agents), which is faster than the convergence rate of the existing distributed methods for solving this problem. The superior convergence rate of our method is also verified by numerical experiments.},
archivePrefix = {arXiv},
arxivId = {1210.2289},
author = {Chen, Annie I. and Ozdaglar, Asuman},
booktitle = {2012 50th Annual Allerton Conference on Communication, Control, and Computing, Allerton 2012},
doi = {10.1109/Allerton.2012.6483273},
eprint = {1210.2289},
isbn = {9781467345385},
mendeley-groups = {Distributed Optimisation},
pages = {601--608},
title = {{A fast distributed proximal-gradient method}},
year = {2012}
}


@article{Nedic2009,
abstract = {We consider distributed iterative algorithms for the averaging problem over time-varying topologies. Our focus is on the convergence time of such algorithms when complete (unquantized) information is available, and on the degradation of performance when only quantized information is available. We study a large and natural class of averaging algorithms, which includes the vast majority of algorithms proposed to date, and provide tight polynomial bounds on their convergence time. We also describe an algorithm within this class whose convergence time is the best among currently available averaging algorithms for time-varying topologies. We then propose and analyze distributed averaging algorithms under the additional constraint that agents can only store and communicate quantized information, so that they can only converge to the average of the initial values of the agents within some error. We establish bounds on the error and tight bounds on the convergence time, as a function of the number of quantization levels. {\textcopyright} 2009 IEEE.},
annote = {With Distributed Stochastic Sub gradient projection (2010), provides bounds that grow polynomially in the network size, but still no explicit dependance on the network topology.},
archivePrefix = {arXiv},
arxivId = {0711.4179},
author = {Nedi{\'{c}}, Angelia and Olshevsky, Alex and Ozdaglar, Asuman and Tsitsiklis, John N.},
journal = {IEEE Transactions on Automatic Control},
keywords = {Decentralized and distributed control,Multiagent systems},
mendeley-groups = {Distributed Optimisation},
number = {11},
pages = {2506--2517},
title = {{On distributed averaging algorithms and quantization effects}},
volume = {54},
year = {2009}
}
\\

@article{nedic2015distributed,
  title={Distributed optimization over time-varying directed graphs},
  author={Nedi{\'c}, Angelia and Olshevsky, Alex},
  journal={IEEE Transactions on Automatic Control},
  volume={60},
  number={3},
  pages={601--615},
  year={2015},
  publisher={IEEE}
}
@article{zhu2012distributed,
  title={On distributed convex optimization under inequality and equality constraints},
  author={Zhu, Minghui and Mart{\'\i}nez, Sonia},
  journal={IEEE Transactions on Automatic Control},
  volume={57},
  number={1},
  pages={151--164},
  year={2012},
  publisher={IEEE}
}
@article{bubeck2015convex,
  title={Convex optimization: Algorithms and complexity},
  author={Bubeck, S{\'e}bastien and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers, Inc.}
}
@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent
         and Thirion, Bertrand and Grisel, Oliver and Blondel, Mathieu and Prettenhofer, Peter
         and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Aalexandre and
         Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}\\
@article{srivastava2011distributed,
  title={Distributed asynchronous constrained stochastic optimization},
  author={Srivastava, Kunal and Nedic, Angelia},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={5},
  number={4},
  pages={772--790},
  year={2011},
  publisher={IEEE}
}
@article{duda1995pattern,
  title={Pattern classification and scene analysis 2nd ed},
  author={Duda, Richard O and Hart, Peter E and Stork, David G},
  journal={ed: Wiley Interscience},
  year={1995}
}
@inproceedings{nedic2017geometrically,
  title={Geometrically convergent distributed optimization with uncoordinated step-sizes},
  author={Nedi{\'c}, Angelia and Olshevsky, Alex and Shi, Wei and Uribe, C{\'e}sar A},
  booktitle={American Control Conference (ACC), 2017},
  pages={3950--3955},
  year={2017},
  organization={IEEE}
}
@article{matei2011performance,
  title={Performance evaluation of the consensus-based distributed subgradient method under random communication topologies},
  author={Matei, Ion and Baras, John S},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={5},
  number={4},
  pages={754--771},
  year={2011},
  publisher={IEEE}
}
@article{nedic2016stochastic,
  title={Stochastic gradient-push for strongly convex functions on time-varying directed graphs},
  author={Nedi{\'c}, Angelia and Olshevsky, Alex},
  journal={IEEE Transactions on Automatic Control},
  volume={61},
  number={12},
  pages={3936--3947},
  year={2016},
  publisher={IEEE}
}

@article{bartlett2006empirical,
  title={Empirical minimization},
  author={Bartlett, Peter L and Mendelson, Shahar},
  journal={Probability Theory and Related Fields},
  volume={135},
  number={3},
  pages={311--334},
  year={2006},
  publisher={Springer}
}
@article{mukherjee2006learning,
  title={Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization},
  author={Mukherjee, Sayan and Niyogi, Partha and Poggio, Tomaso and Rifkin, Ryan},
  journal={Advances in Computational Mathematics},
  volume={25},
  number={1-3},
  pages={161--193},
  year={2006},
  publisher={Springer}
}
@inproceedings{kempe2003gossip,
  title={Gossip-based computation of aggregate information},
  author={Kempe, David and Dobra, Alin and Gehrke, Johannes},
  booktitle={Foundations of Computer Science, 2003. Proceedings. 44th Annual IEEE Symposium on},
  pages={482--491},
  year={2003},
  organization={IEEE}
}
@ARTICLE{5545370, 
author={Alexandros G. Dimakis and Soummya Kar and Jos{\'e} M.F. Moura and Michael G. Rabbat and Anna Scaglione}, 
journal={Proceedings of the IEEE}, 
title={Gossip Algorithms for Distributed Signal Processing}, 
year={2010}, 
volume={98}, 
number={11}, 
pages={1847-1864}
}
@inproceedings{tsianos2012communication,
  title={Communication/computation tradeoffs in consensus-based distributed optimization},
  author={Tsianos, Konstantinos and Lawlor, Sean and Rabbat, Michael G},
  booktitle={Advances in neural information processing systems},
  pages={1943--1951},
  year={2012}
}

@inproceedings{tsianos2012push,
  title={Push-sum distributed dual averaging for convex optimization},
  author={Tsianos, Konstantinos I and Lawlor, Sean and Rabbat, Michael G},
  booktitle={Decision and Control (CDC), 2012 IEEE 51st Annual Conference on},
  pages={5453--5458},
  year={2012},
  organization={IEEE}
}
@article{langford2009slow,
  title={Slow learners are fast},
  author={Langford, John and Smola, Alexander J and Zinkevich, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={22},
  pages={2331--2339},
  year={2009}
}

@article{nedic2001incremental,
  title={Incremental subgradient methods for nondifferentiable optimization},
  author={Nedic, Angelia and Bertsekas, Dimitri P},
  journal={SIAM Journal on Optimization},
  volume={12},
  number={1},
  pages={109--138},
  year={2001},
  publisher={SIAM}
}
@inproceedings{johansson2007simple,
  title={A simple peer-to-peer algorithm for distributed optimization in sensor networks},
  author={Johansson, Bjorn and Rabi, Maben and Johansson, Mikael},
  booktitle={Decision and Control, 2007 46th IEEE Conference on},
  pages={4705--4710},
  year={2007},
  organization={IEEE}
}
@inproceedings{zhang2012communication,
  title={Communication-efficient algorithms for statistical optimization},
  author={Zhang, Yuchen and Wainwright, Martin J. and Duchi, John C.},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1502--1510},
  year={2012}
}

@inproceedings{zinkevich2010parallelized,
  title={Parallelized stochastic gradient descent},
  author={Zinkevich, Martin and Weimer, Markus and Li, Lihong and Smola, Alex J},
  booktitle={Advances in neural information processing systems},
  pages={2595--2603},
  year={2010}
}
@article{dekel2012optimal,
  title={Optimal distributed online prediction using mini-batches},
  author={Dekel, Ofer and Gilad-Bachrach, Ran and Shamir, Ohad and Xiao, Lin},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={Jan},
  pages={165--202},
  year={2012}
}
@article{nedic2010constrained,
  title={Constrained consensus and optimization in multi-agent networks},
  author={Nedic, Angelia and Ozdaglar, Asuman and Parrilo, Pablo A},
  journal={IEEE Transactions on Automatic Control},
  volume={55},
  number={4},
  pages={922--938},
  year={2010},
  publisher={IEEE}
}

@article{shalev2010learnability,
  title={Learnability, stability and uniform convergence},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Oct},
  pages={2635--2670},
  year={2010}
}
@inproceedings{agarwal2011distributed,
  title={Distributed delayed stochastic optimization},
  author={Agarwal, Alekh and Duchi, John C.},
  booktitle={Advances in Neural Information Processing Systems},
  pages={873--881},
  year={2011}
}
@article{ram2009incremental,
  title={Incremental stochastic subgradient algorithms for convex optimization},
  author={Ram, S Sundhar and Nedi{\'c}, A and Veeravalli, Venugopal V},
  journal={SIAM Journal on Optimization},
  volume={20},
  number={2},
  pages={691--717},
  year={2009},
  publisher={SIAM}
}
@article{jakovetic2014fast,
  title={Fast distributed gradient methods},
  author={Jakoveti{\'c}, Du{\v{s}}an and Xavier, Joao and Moura, Jos{\'e} MF},
  journal={IEEE Transactions on Automatic Control},
  volume={59},
  number={5},
  pages={1131--1146},
  year={2014},
  publisher={IEEE}
}

@article{hong2017stochastic,
  title={Stochastic proximal gradient consensus over random networks},
  author={Hong, Mingyi and Chang, Tsung-Hui},
  journal={IEEE Transactions on Signal Processing},
  volume={65},
  number={11},
  pages={2933--2948},
  year={2017},
  publisher={IEEE}
}


@article{lobel2011distributed,
  title={Distributed subgradient methods for convex optimization over random networks},
  author={Lobel, Ilan and Ozdaglar, Asuman},
  journal={IEEE Transactions on Automatic Control},
  volume={56},
  number={6},
  pages={1291--1306},
  year={2011},
  publisher={IEEE}
}
@inproceedings{wei2012distributed,
  title={Distributed alternating direction method of multipliers},
  author={Wei, Ermin and Ozdaglar, Asuman},
  booktitle={Decision and Control (CDC), 2012 IEEE 51st Annual Conference on},
  pages={5445--5450},
  year={2012},
  organization={IEEE}
}

@incollection{NIPS2012_4633,
title = {A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets},
author = {Nicolas L. Roux and Schmidt, Mark and Francis R. Bach},
booktitle = {Advances in Neural Information Processing Systems},
pages = {2663--2671},
year = {2012}
}

@article{NET-014,
	Author = {Devavrat Shah},
	Date-Added = {2017-04-24 17:05:27 +0000},
	Date-Modified = {2017-04-24 17:05:45 +0000},
	Journal = {Foundations and Trends{\textregistered} in Networking},
	Number = {1},
	Pages = {1-125},
	Title = {Gossip Algorithms},
	Volume = {3},
	Year = {2009}
	}


@article{shi2015extra,
  title={Extra: An exact first-order algorithm for decentralized consensus optimization},
  author={Shi, Wei and Ling, Qing and Wu, Gang and Yin, Wotao},
  journal={SIAM Journal on Optimization},
  volume={25},
  number={2},
  pages={944--966},
  year={2015},
  publisher={SIAM}
}
@inproceedings{cesa2002generalization,
  title={On the generalization ability of on-line learning algorithms},
  author={Cesa-Bianchi, Nicol{\'o} and Conconi, Alex and Gentile, Claudio},
  booktitle={Advances in neural information processing systems},
  pages={359--366},
  year={2002}
}
@article{mokhtari2016dsa,
  title={DSA: Decentralized double stochastic averaging gradient algorithm},
  author={Mokhtari, Aryan and Ribeiro, Alejandro},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={61},
  pages={1--35},
  year={2016}
}

@article{Boyd:2011:DOS:2185815.2185816,
 author = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
 title = {Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers},
 journal = {Found. Trends Mach. Learn.},
 issue_date = {2011},
 volume = {3},
 number = {1},
 month = jan,
 year = {2011},
 pages = {1--122},
 numpages = {122},
 publisher = {Now Publishers Inc.},
 address = {Hanover, MA, USA},
} 

@article{shi2014linear,
  title={On the Linear Convergence of the ADMM in Decentralized Consensus Optimization.},
  author={Shi, Wei and Ling, Qing and Yuan, Kun and Wu, Gang and Yin, Wotao},
  journal={IEEE Trans. Signal Processing},
  volume={62},
  number={7},
  pages={1750--1761},
  year={2014}
}\\
@article{schmidt2017minimizing,
  title={Minimizing finite sums with the stochastic average gradient},
  author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  journal={Mathematical Programming},
  volume={162},
  number={1-2},
  pages={83--112},
  year={2017},
  publisher={Springer}
}
@inproceedings{bottou2004large,
  title={Large scale online learning},
  author={Bottou, L{\'e}on and Cun, Yann L},
  booktitle={Advances in Neural Information Processing Systems},
  pages={217--224},
  year={2004}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@inproceedings{defazio2014saga,
  title={SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1646--1654},
  year={2014}
}

@article{nedic2017achieving,
  title={Achieving geometric convergence for distributed optimization over time-varying graphs},
  author={Nedic, Angelia and Olshevsky, Alex and Shi, Wei},
  journal={SIAM Journal on Optimization},
  volume={27},
  number={4},
  pages={2597--2633},
  year={2017},
  publisher={SIAM}
}\\

@article{yuan2016convergence,
  title={On the convergence of decentralized gradient descent},
  author={Yuan, Kun and Ling, Qing and Yin, Wotao},
  journal={SIAM Journal on Optimization},
  volume={26},
  number={3},
  pages={1835--1854},
  year={2016},
  publisher={SIAM}
}\\
@article{ram2010distributed,
  title={Distributed stochastic subgradient projection algorithms for convex optimization},
  author={Ram, S. Sundhar and Nedi{\'c}, Angelia and Veeravalli, Venugopal V.},
  journal={Journal of optimization theory and applications},
  volume={147},
  number={3},
  pages={516--545},
  year={2010},
  publisher={Springer}
}
@inproceedings{bellet2015distributed,
  title={A distributed frank-wolfe algorithm for communication-efficient sparse learning},
  author={Bellet, Aur{\'e}lien and Liang, Yingyu and Garakani, Alireza Bagheri and Balcan, Maria-Florina and Sha, Fei},
  booktitle={Proceedings of the 2015 SIAM International Conference on Data Mining},
  pages={478--486},
  year={2015},
  organization={SIAM}
}
@inproceedings{xiao2005scheme,
  title={A scheme for robust distributed sensor fusion based on average consensus},
  author={Xiao, Lin and Boyd, Stephen and Lall, Sanjay},
  booktitle={Information Processing in Sensor Networks, 2005. IPSN 2005. Fourth International Symposium on},
  pages={63--70},
  year={2005},
  organization={IEEE}
}
@article{johansson2009randomized,
  title={A randomized incremental subgradient method for distributed optimization in networked systems},
  author={Johansson, Bj{\"o}rn and Rabi, Maben and Johansson, Mikael},
  journal={SIAM Journal on Optimization},
  volume={20},
  number={3},
  pages={1157--1170},
  year={2009},
  publisher={SIAM}
}\\
@ARTICLE{2018arXiv180906958R,
   author = {{Richards}, D. and {Rebeschini}, P.},
    title = "{Graph-Dependent Implicit Regularisation for Distributed Stochastic Subgradient Descent}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1809.06958},
 keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
     year = {2018},
    month = {sep},
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180906958R},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{tsitsiklis1986distributed,
  title={Distributed asynchronous deterministic and stochastic gradient optimization algorithms},
  author={Tsitsiklis, John and Bertsekas, Dimitri and Athans, Michael},
  journal={IEEE transactions on automatic control},
  volume={31},
  number={9},
  pages={803--812},
  year={1986},
  publisher={IEEE}
}
@article{dimakis2008geographic,
  title={Geographic gossip: Efficient averaging for sensor networks},
  author={Dimakis, Alexandros DG and Sarwate, Anand D and Wainwright, Martin J},
  journal={IEEE Transactions on Signal Processing},
  volume={56},
  number={3},
  pages={1205--1216},
  year={2008},
  publisher={IEEE}
}\\

@article{boyd2006randomized,
  title={Randomized gossip algorithms},
  author={Boyd, Stephen and Ghosh, Arpita and Prabhakar, Balaji and Shah, Devavrat},
  journal={IEEE transactions on information theory},
  volume={52},
  number={6},
  pages={2508--2530},
  year={2006},
  publisher={IEEE}
}
@article{benezit2010order,
  title={Order-optimal consensus through randomized path averaging},
  author={B{\'e}n{\'e}zit, Florence and Dimakis, Alexandros G and Thiran, Patrick and Vetterli, Martin},
  journal={IEEE Transactions on Information Theory},
  volume={56},
  number={10},
  pages={5150--5167},
  year={2010},
  publisher={IEEE}
}


@techreport{tsitsiklis1984problems,
  title={Problems in decentralized decision making and computation.},
  author={Tsitsiklis, John Nikolas},
  year={1984},
  institution={Massachusetts Inst Of Tech Cambridge Lab For Information And Decision Systems}
}

@article{boyd2011distributed,
  title={Distributed optimization and statistical learning via the alternating direction method of multipliers},
  author={Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={3},
  number={1},
  pages={1--122},
  year={2011},
  publisher={Now Publishers, Inc.}
}
@inproceedings{ram2009distributed,
  title={Distributed subgradient projection algorithm for convex optimization},
  author={Ram, S. Sundhar and Nedic, Angelia and Veeravalli, Venugopal V.},
  booktitle={Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on},
  pages={3653--3656},
  year={2009},
  organization={IEEE}
}
@article{borkar1982asymptotic,
  title={Asymptotic agreement in distributed estimation},
  author={Borkar, Vivek and Varaiya, Pravin},
  journal={IEEE Transactions on Automatic Control},
  volume={27},
  number={3},
  pages={650--655},
  year={1982},
  publisher={IEEE}
}
@article{nedic2009distributed,
  title={Distributed subgradient methods for multi-agent optimization},
  author={Nedic, Angelia and Ozdaglar, Asuman},
  journal={IEEE Transactions on Automatic Control},
  volume={54},
  number={1},
  pages={48--61},
  year={2009},
  publisher={IEEE}
}
@inproceedings{tsianos2012distributed,
  title={Distributed strongly convex optimization},
  author={Tsianos, Konstantinos I and Rabbat, Michael G},
  booktitle={Communication, Control, and Computing (Allerton), 2012 50th Annual Allerton Conference on},
  pages={593--600},
  year={2012},
  organization={IEEE}
}
@article{kearns1999algorithmic,
  title={Algorithmic stability and sanity-check bounds for leave-one-out cross-validation},
  author={Kearns, Michael and Ron, Dana},
  journal={Neural computation},
  volume={11},
  number={6},
  pages={1427--1453},
  year={1999},
  publisher={MIT Press}
}
@article{mania2015perturbed,
  title={Perturbed iterate analysis for asynchronous stochastic optimization},
  author={Mania, Horia and Pan, Xinghao and Papailiopoulos, Dimitris and Recht, Benjamin and Ramchandran, Kannan and Jordan, Michael I},
  journal={arXiv preprint arXiv:1507.06970},
  year={2015}
}
@inproceedings{arjevani2015communication,
  title={Communication complexity of distributed convex learning and optimization},
  author={Arjevani, Yossi and Shamir, Ohad},
  booktitle={Advances in neural information processing systems},
  pages={1756--1764},
  year={2015}
}
@inproceedings{shamir2014fundamental,
  title={Fundamental limits of online and distributed algorithms for statistical learning and estimation},
  author={Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={163--171},
  year={2014}
}
@inproceedings{scaman2018optimal,
  title={Optimal algorithms for non-smooth distributed optimization in networks},
  author={Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Massouli{\'e}, Laurent and Lee, Yin Tat},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2745--2754},
  year={2018}
}

@article{defossez2014constant,
  title={Constant step size least-mean-square: Bias-variance trade-offs and optimal sampling distributions},
  author={D{\'e}fossez, Alexandre and Bach, Francis},
  journal={arXiv preprint arXiv:1412.0156},
  year={2014}
}
@article{SDCA2012,
  title={Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={arXiv preprint arXiv:arXiv:1209.1873},
  year={2012}
}

@article{dieuleveut2017harder,
  title={Harder, better, faster, stronger convergence rates for least-squares regression},
  author={Dieuleveut, Aymeric and Flammarion, Nicolas and Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={3520--3570},
  year={2017},
  publisher={JMLR. org}
}
@article{smale2005shannon,
  title={Shannon sampling II: Connections to learning theory},
  author={Smale, Steve and Zhou, Ding-Xuan},
  journal={Applied and Computational Harmonic Analysis},
  volume={19},
  number={3},
  pages={285--302},
  year={2005},
  publisher={Elsevier}
}

@article{lin2017optimal,
  title={Optimal rates for multi-pass stochastic gradient methods},
  author={Lin, Junhong and Rosasco, Lorenzo},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={97},
  pages={1--47},
  year={2017}
}

@inproceedings{recht2011hogwild,
  title={Hogwild: A lock-free approach to parallelizing stochastic gradient descent},
  author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
  booktitle={Advances in Neural Information Processing Systems},
  pages={693--701},
  year={2011}
}
@article{rogers1978finite,
  title={A finite sample distribution-free performance bound for local discrimination rules},
  author={Rogers, William H. and Wagner, Terry J.},
  journal={The Annals of Statistics},
  pages={506--514},
  year={1978},
  publisher={JSTOR}
}
@misc{aldous2002reversible,
  title={Reversible Markov chains and random walks on graphs},
  author={Aldous, David and Fill, Jim},
  year={2002},
  publisher={Berkeley}
}
@article{DAW12,
	Author = {John C. Duchi and Alekh Agarwal and Martin J. Wainwright},
	Date-Added = {2017-12-16 15:45:52 +0000},
	Date-Modified = {2017-12-16 15:46:38 +0000},
	Journal = {IEEE Transactions on Automatic Control},
	Number = {3},
	Pages = {592-606},
	Title = {Dual Averaging for Distributed Optimization: Convergence Analysis and Network Scaling},
	Volume = {57},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TAC.2011.2161027}
	}
  @inproceedings{lakshminarayanan2018linear,
  title={Linear Stochastic Approximation: How Far Does Constant Step-Size and Iterate Averaging Go?},
  author={Lakshminarayanan, Chandrashekar and Szepesvari, Csaba},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1347--1355},
  year={2018}
}
@article{tarres2014online,
  title={Online Learning as Stochastic Approximation of Regularization Paths: Optimality and Almost-Sure Convergence.},
  author={Tarres, Pierre and Yao, Yuan},
  journal={IEEE Trans. Information Theory},
  volume={60},
  number={9},
  pages={5716--5735},
  year={2014}
}
@article{ying2008online,
  title={Online gradient descent learning algorithms},
  author={Ying, Yiming and Pontil, Massimiliano},
  journal={Foundations of Computational Mathematics},
  volume={8},
  number={5},
  pages={561--596},
  year={2008},
  publisher={Springer}
}
@inproceedings{carratino2018learning,
  title={Learning with sgd and random features},
  author={Carratino, Luigi and Rudi, Alessandro and Rosasco, Lorenzo},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10192--10203},
  year={2018}
}


@article{lin2018optimal,
  title={Optimal convergence for distributed learning with stochastic gradient methods and spectral-regularization algorithms},
  author={Lin, Junhong and Cevher, Volkan},
  journal={arXiv preprint arXiv:1801.07226},
  year={2018}
}
@article{dieuleveut2016nonparametric,
  title={Nonparametric stochastic approximation with large step-sizes},
  author={Dieuleveut, Aymeric and Bach, Francis and others},
  journal={The Annals of Statistics},
  volume={44},
  number={4},
  pages={1363--1399},
  year={2016},
  publisher={Institute of Mathematical Statistics}
}

@article{blanchard2018optimal,
  title={Optimal rates for regularization of statistical inverse learning problems},
  author={Blanchard, Gilles and M{\"u}cke, Nicole},
  journal={Foundations of Computational Mathematics},
  volume={18},
  number={4},
  pages={971--1013},
  year={2018},
  publisher={Springer}
}
@inproceedings{cao2006accelerated,
  title={Accelerated gossip algorithms for distributed computation},
  author={Cao, Ming and Spielman, Daniel A and Yeh, Edmund M},
  booktitle={Proc. of the 44th Annual Allerton Conference on Communication, Control, and Computation},
  pages={952--959},
  year={2006},
  organization={Citeseer}
}
@book{young2014iterative,
  title={Iterative solution of large linear systems},
  author={Young, David M},
  year={2014},
  publisher={Elsevier}
}

@article{Bousquet:2002,
	Author = {Bousquet, Olivier and Elisseeff, Andr{\'e}},
	Date-Added = {2017-12-04 19:19:03 +0000},
	Date-Modified = {2017-12-04 19:19:23 +0000},
	Journal = {J. Mach. Learn. Res.},
	Pages = {499--526},
	Title = {Stability and Generalization},
	Volume = {2},
	Year = {2002},
	Bdsk-Url-1 = {http://dx.doi.org/10.1162/153244302760200704}}
	
@inproceedings{scaman2017optimal,
  title={Optimal algorithms for smooth and strongly convex distributed optimization in networks},
  author={Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Lee, Yin Tat and Massouli{\'e}, Laurent},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={3027--3036},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{bousquet2008tradeoffs,
  title={The tradeoffs of large scale learning},
  author={Bousquet, Olivier and Bottou, L{\'e}on},
  booktitle={Advances in Neural Information Processing Systems},
  pages={161--168},
  year={2008}
}
@inproceedings{Hardt:2016:TFG:3045390.3045520,
	Author = {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
	Booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
	Date-Added = {2017-12-04 19:02:16 +0000},
	Date-Modified = {2017-12-04 19:02:33 +0000},
	Pages = {1225--1234},
	Series = {ICML'16},
	Title = {Train Faster, Generalize Better: Stability of Stochastic Gradient Descent},
	Year = {2016},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3045390.3045520}}

@misc{Kutin01theinteraction,
	Author = {Samuel Kutin and Partha Niyogi},
	Date-Added = {2017-12-04 19:00:23 +0000},
	Date-Modified = {2017-12-04 19:00:45 +0000},
	Howpublished = {Technical Report TR-2001-30, Computer Science Department, University of Chicago},
	Title = {The interaction of stability and weakness in AdaBoost},
	Year = {2001}}

@article{Freund:2003:EBA:945365.964285,
	Author = {Freund, Yoav and Iyer, Raj and Schapire, Robert E. and Singer, Yoram},
	Date-Added = {2017-12-04 18:58:07 +0000},
	Date-Modified = {2017-12-04 18:58:22 +0000},
	Journal = {J. Mach. Learn. Res.},
	Pages = {933--969},
	Title = {An Efficient Boosting Algorithm for Combining Preferences},
	Volume = {4},
	Year = {2003},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=945365.964285}}

@inbook{Agarwal2005,
	Abstract = {The problem of ranking, in which the goal is to learn a real-valued ranking function that induces a ranking or ordering over an instance space, has recently gained attention in machine learning. We study generalization properties of ranking algorithms, in a particular setting of the ranking problem known as the bipartite ranking problem, using the notion of algorithmic stability.In particular, we derive generalization bounds for bipartite ranking algorithms that have good stability properties. We show that kernel-based ranking algorithms that perform regularization in a reproducing kernel Hilbert space have such stability properties, and therefore our bounds can be applied to these algorithms; this is in contrast with previous generalization bounds for ranking, which are based on uniform convergence and in many cases cannot be applied to these algorithms. A comparison of the bounds we obtain with corresponding bounds for classification algorithms yields some interesting insights into the difference in generalization behaviour between ranking and classification.},
	Author = {Agarwal, Shivani and Niyogi, Partha},
	Editor = {Auer, Peter and Meir, Ron},
	Pages = {32--47},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Stability and Generalization of Bipartite Ranking Algorithms},
	Year = {2005}
}

@article{Agarwal:2009,
	Author = {Agarwal, Shivani and Niyogi, Partha},
	Date-Added = {2017-12-04 18:48:18 +0000},
	Date-Modified = {2017-12-04 18:48:36 +0000},
	Journal = {J. Mach. Learn. Res.},
	Month = jun,
	Pages = {441--474},
	Title = {Generalization Bounds for Ranking Algorithms via Algorithmic Stability},
	Volume = {10},
	Year = {2009},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1577069.1577085}}

@article{Boyd:2006,
	Author = {Boyd, Stephen and Ghosh, Arpita and Prabhakar, Balaji and Shah, Devavrat},
	Date-Added = {2017-11-28 23:23:26 +0000},
	Date-Modified = {2017-11-28 23:23:50 +0000},
	Journal = {IEEE/ACM Trans. Netw.},
	Pages = {2508--2530},
	Title = {Randomized Gossip Algorithms},
	Volume = {14},
	Year = {2006}}

@book{BT97,
	Author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
	Date-Added = {2017-11-28 23:20:26 +0000},
	Date-Modified = {2017-11-28 23:20:26 +0000},
	Publisher = {Athena Scientific},
	Title = {Parallel and Distributed Computation: Numerical Methods},
	Year = {1997}}

@article{Schapire1999,
	Abstract = {We describe several improvements to Freund and Schapire's AdaBoost boosting algorithm, particularly in a setting in which hypotheses may assign confidences to each of their predictions. We give a simplified analysis of AdaBoost in this setting, and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses. We give a specific method for assigning confidences to the predictions of decision trees, a method closely related to one used by Quinlan. This method also suggests a technique for growing decision trees which turns out to be identical to one proposed by Kearns and Mansour. We focus next on how to apply the new boosting algorithms to multiclass classification problems, particularly to the multi-label case in which each example may belong to more than one class. We give two boosting methods for this problem, plus a third method based on output coding. One of these leads to a new method for handling the single-label case which is simpler but as effective as techniques suggested by Freund and Schapire. Finally, we give some experimental results comparing a few of the algorithms discussed in this paper.},
	Author = {Schapire, Robert E. and Singer, Yoram},
	Date-Added = {2017-11-28 23:12:46 +0000},
	Date-Modified = {2017-11-28 23:12:56 +0000},
	Journal = {Machine Learning},
	Month = {Dec},
	Number = {3},
	Pages = {297--336},
	Title = {Improved Boosting Algorithms Using Confidence-rated Predictions},
	Volume = {37},
	Year = {1999},
	Bdsk-Url-1 = {https://doi.org/10.1023/A:1007614523901},
	Bdsk-Url-2 = {http://dx.doi.org/10.1023/A:1007614523901}}

@inproceedings{DistributedFW,
	Author = {Bellet, Aur{\'e}lien and Liang, Yingyu and Garakani, Alireza Bagheri and Balcan, Maria-Florina and Sha, Fei},
	Booktitle = {SDM},
	Date-Added = {2017-11-28 23:06:22 +0000},
	Date-Modified = {2017-11-28 23:06:52 +0000},
	Pages = {478-486},
	Publisher = {SIAM},
	Title = {A Distributed Frank-Wolfe Algorithm for Communication-Efficient Sparse Learning.},
	Year = 2015,
	Bdsk-Url-1 = {http://dblp.uni-trier.de/db/conf/sdm/sdm2015.html#BelletLGBS15}}

@article{Gambs2007,
	Abstract = {We describe two algorithms, BiBoost (Bipartite Boosting) and MultBoost (Multiparty Boosting), that allow two or more participants to construct a boosting classifier without explicitly sharing their data sets. We analyze both the computational and the security aspects of the algorithms. The algorithms inherit the excellent generalization performance of AdaBoost. Experiments indicate that the algorithms are better than AdaBoost executed separately by the participants, and that, independently of the number of participants, they perform close to AdaBoost executed using the entire data set.},
	Author = {Gambs, S{\'e}bastien and K{\'e}gl, Bal{\'a}zs and A{\"\i}meur, Esma},
	Date-Added = {2017-11-28 23:03:49 +0000},
	Date-Modified = {2017-11-28 23:04:04 +0000},
	Journal = {Data Mining and Knowledge Discovery},
	Number = {1},
	Pages = {131--170},
	Title = {Privacy-preserving boosting},
	Volume = {14},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s10618-006-0051-9}}

@book{Schapire:2012,
	Author = {Schapire, Robert E. and Freund, Yoav},
	Date-Added = {2017-05-26 18:26:32 +0000},
	Date-Modified = {2017-05-26 18:26:39 +0000},
	Publisher = {The MIT Press},
	Title = {Boosting: Foundations and Algorithms},
	Year = {2012}}
