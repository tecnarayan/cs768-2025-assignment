\begin{thebibliography}{10}

\bibitem{abbeel2007application}
Pieter Abbeel, Adam Coates, Morgan Quigley, and Andrew~Y Ng.
\newblock An application of reinforcement learning to aerobatic helicopter
  flight.
\newblock In {\em Advances in neural information processing systems}, pages
  1--8, 2007.

\bibitem{abbeel2004apprenticeship}
Pieter Abbeel and Andrew~Y Ng.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In {\em Proceedings of the twenty-first international conference on
  Machine learning}, page~1, 2004.

\bibitem{argall2009survey}
Brenna~D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning.
\newblock A survey of robot learning from demonstration.
\newblock {\em Robotics and autonomous systems}, 57(5):469--483, 2009.

\bibitem{balakrishna2019policy}
Ashwin Balakrishna, Brijen Thananjeyan, Jonathan Lee, Felix Li, Arsh Zahed,
  Joseph~E. Gonzalez, and Ken Goldberg.
\newblock On-policy robot imitation learning from a converging supervisor.
\newblock In Leslie~Pack Kaelbling, Danica Kragic, and Komei Sugiura, editors,
  {\em 3rd Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan,
  October 30 - November 1, 2019, Proceedings}, volume 100 of {\em Proceedings
  of Machine Learning Research}, pages 24--41. {PMLR}, 2019.

\bibitem{baxter2001infinite}
Jonathan Baxter and Peter~L Bartlett.
\newblock Infinite-horizon policy-gradient estimation.
\newblock {\em Journal of Artificial Intelligence Research}, 15:319--350, 2001.

\bibitem{DBLP:journals/corr/BrockmanCPSSTZ16}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym.
\newblock {\em CoRR}, abs/1606.01540, 2016.

\bibitem{brown2019extrapolating}
Daniel Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum.
\newblock Extrapolating beyond suboptimal demonstrations via inverse
  reinforcement learning from observations.
\newblock In {\em International Conference on Machine Learning}, pages
  783--792, 2019.

\bibitem{casella2002statistical}
George Casella and Roger~L Berger.
\newblock {\em Statistical inference}, volume~2.
\newblock Duxbury Pacific Grove, CA, 2002.

\bibitem{castro2019inverse}
Pablo~Samuel Castro, Shijian Li, and Daqing Zhang.
\newblock Inverse reinforcement learning with multiple ranked experts.
\newblock {\em arXiv preprint arXiv:1907.13411}, 2019.

\bibitem{chen2013noisy}
Yudong Chen and Constantine Caramanis.
\newblock Noisy and missing data regression: Distribution-oblivious support
  recovery.
\newblock In {\em International Conference on Machine Learning}, pages
  383--391, 2013.

\bibitem{christiano2017deep}
Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
  Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4299--4307, 2017.

\bibitem{chung2010mobile}
Shu-Yun Chung and Han-Pang Huang.
\newblock A mobile robot that understands pedestrian spatial behaviors.
\newblock In {\em 2010 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pages 5861--5866. IEEE, 2010.

\bibitem{goyal2019first}
Vineet Goyal and Julien Grand-Clement.
\newblock A first-order approach to accelerated value iteration.
\newblock {\em arXiv preprint arXiv:1905.09963}, 2019.

\bibitem{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock {\em arXiv preprint arXiv:1801.01290}, 2018.

\bibitem{hussein2017imitation}
Ahmed Hussein, Mohamed~Medhat Gaber, Eyad Elyan, and Chrisina Jayne.
\newblock Imitation learning: A survey of learning methods.
\newblock {\em ACM Computing Surveys (CSUR)}, 50(2):21, 2017.

\bibitem{ibarz2018reward}
Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario
  Amodei.
\newblock Reward learning from human preferences and demonstrations in atari.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8011--8023, 2018.

\bibitem{lflpaper}
Alexis Jacq, Matthieu Geist, Ana Paiva, and Olivier Pietquin.
\newblock Learning from a learner.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, {\em
  Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of {\em Proceedings of Machine Learning Research}, pages
  2990--2999, Long Beach, California, USA, 09--15 Jun 2019. PMLR.

\bibitem{mcwilliams2014fast}
Brian McWilliams, Gabriel Krummenacher, Mario Lucic, and Joachim~M Buhmann.
\newblock Fast and robust least squares estimation in corrupted linear models.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  415--423, 2014.

\bibitem{metelli2017compatible}
Alberto~Maria Metelli, Matteo Pirotta, and Marcello Restelli.
\newblock Compatible reward inverse reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2050--2059, 2017.

\bibitem{ng2000algorithms}
Andrew~Y. Ng and Stuart~J. Russell.
\newblock Algorithms for inverse reinforcement learning.
\newblock In {\em Proceedings of the Seventeenth International Conference on
  Machine Learning {(ICML} 2000), Stanford University, Stanford, CA, USA, June
  29 - July 2, 2000}, pages 663--670. Morgan Kaufmann, 2000.

\bibitem{DBLP:journals/ftrob/OsaPNBA018}
Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J.~Andrew Bagnell, Pieter
  Abbeel, and Jan Peters.
\newblock An algorithmic perspective on imitation learning.
\newblock {\em Foundations and Trends in Robotics}, 7(1-2):1--179, 2018.

\bibitem{peters2006policy}
Jan Peters and Stefan Schaal.
\newblock Policy gradient methods for robotics.
\newblock In {\em 2006 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pages 2219--2225. IEEE, 2006.

\bibitem{peters2008reinforcement}
Jan Peters and Stefan Schaal.
\newblock Reinforcement learning of motor skills with policy gradients.
\newblock {\em Neural networks}, 21(4):682--697, 2008.

\bibitem{pirotta2016inverse}
Matteo Pirotta and Marcello Restelli.
\newblock Inverse reinforcement learning through policy gradient minimization.
\newblock In Dale Schuurmans and Michael~P. Wellman, editors, {\em Proceedings
  of the Thirtieth {AAAI} Conference on Artificial Intelligence, February
  12-17, 2016, Phoenix, Arizona, {USA}}, pages 1993--1999. {AAAI} Press, 2016.

\bibitem{pirotta2013adaptive}
Matteo Pirotta, Marcello Restelli, and Luca Bascetta.
\newblock Adaptive step-size for policy gradient methods.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1394--1402, 2013.

\bibitem{puterman1994markov}
Martin~L. Puterman.
\newblock {\em Markov Decision Processes: Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons, Inc., New York, NY, USA, 1994.

\bibitem{rabinowitz2018machine}
Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, SM~Ali Eslami, and
  Matthew Botvinick.
\newblock Machine theory of mind.
\newblock In {\em International Conference on Machine Learning}, pages
  4218--4227, 2018.

\bibitem{fixeddesign}
Philippe Rigollet.
\newblock High-dimensional statistics. spring 2015.
\newblock {\em Massachusetts Institute of Technology: MIT OpenCourseWare},
  2015.

\bibitem{schulman2017equivalence}
John Schulman, Xi~Chen, and Pieter Abbeel.
\newblock Equivalence between policy gradients and soft q-learning.
\newblock {\em arXiv preprint arXiv:1704.06440}, 2017.

\bibitem{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}, 2017.

\bibitem{shteingart2014reinforcement}
Hanan Shteingart and Yonatan Loewenstein.
\newblock Reinforcement learning and human behavior.
\newblock {\em Current Opinion in Neurobiology}, 25:93--98, 2014.

\bibitem{shum2019theory}
Michael Shum, Max Kleiman-Weiner, Michael~L Littman, and Joshua~B Tenenbaum.
\newblock Theory of minds: Understanding behavior in groups through inverse
  planning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 6163--6170, 2019.

\bibitem{spokoiny2012parametric}
Vladimir Spokoiny et~al.
\newblock Parametric estimation. finite sample theory.
\newblock {\em The Annals of Statistics}, 40(6):2877--2909, 2012.

\bibitem{sutton1998reinforcement}
Richard~S. Sutton and Andrew~G. Barto.
\newblock {\em Reinforcement Learning: An Introduction}.
\newblock A Bradford book. Bradford Book, 1998.

\bibitem{sutton2000policy}
Richard~S Sutton, David~A McAllester, Satinder~P Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1057--1063, 2000.

\bibitem{tseng2001convergence}
Paul Tseng.
\newblock Convergence of a block coordinate descent method for
  nondifferentiable minimization.
\newblock {\em Journal of optimization theory and applications},
  109(3):475--494, 2001.

\bibitem{vogel2012improving}
Adam Vogel, Deepak Ramachandran, Rakesh Gupta, and Antoine Raux.
\newblock Improving hybrid vehicle fuel efficiency using inverse reinforcement
  learning.
\newblock In {\em Twenty-Sixth AAAI Conference on Artificial Intelligence},
  2012.

\bibitem{wedin1973perturbation}
Perke Wedin.
\newblock Perturbation theory for pseudo-inverses.
\newblock {\em BIT Numerical Mathematics}, 13(2):217--232, 1973.

\bibitem{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine learning}, 8(3-4):229--256, 1992.

\bibitem{ziebart2008maximum}
Brian~D. Ziebart, Andrew~L. Maas, J.~Andrew Bagnell, and Anind~K. Dey.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In Dieter Fox and Carla~P. Gomes, editors, {\em Proceedings of the
  Twenty-Third {AAAI} Conference on Artificial Intelligence, {AAAI} 2008,
  Chicago, Illinois, USA, July 13-17, 2008}, pages 1433--1438. {AAAI} Press,
  2008.

\bibitem{ziebart2008navigate}
Brian~D Ziebart, Andrew~L Maas, Anind~K Dey, and J~Andrew Bagnell.
\newblock Navigate like a cabbie: Probabilistic reasoning from observed
  context-aware behavior.
\newblock In {\em Proceedings of the 10th international conference on
  Ubiquitous computing}, pages 322--331, 2008.

\end{thebibliography}
