\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba \& Frey(2013)Ba and Frey]{ba2013adaptive}
Ba, J. and Frey, B.
\newblock Adaptive dropout for training deep neural networks.
\newblock \emph{Neural Information Processing Systems (NIPS)}, 2013.

\bibitem[Bengio et~al.(2021{\natexlab{a}})Bengio, Jain, Korablyov, Precup, and
  Bengio]{bengio2021flow}
Bengio, E., Jain, M., Korablyov, M., Precup, D., and Bengio, Y.
\newblock Flow network based generative models for non-iterative diverse
  candidate generation.
\newblock \emph{Neural Information Processing Systems (NeurIPS)},
  2021{\natexlab{a}}.

\bibitem[Bengio et~al.(2021{\natexlab{b}})Bengio, Deleu, Hu, Lahlou, Tiwari,
  and Bengio]{bengio2021gflownet}
Bengio, Y., Deleu, T., Hu, E.~J., Lahlou, S., Tiwari, M., and Bengio, E.
\newblock Gflownet foundations.
\newblock \emph{arXiv preprint arXiv:2111.09266}, 2021{\natexlab{b}}.

\bibitem[Bhatt et~al.(2021)Bhatt, Antor{\'a}n, Zhang, Liao, Sattigeri,
  Fogliato, Melan{\c{c}}on, Krishnan, Stanley, Tickoo,
  et~al.]{bhatt2021uncertainty}
Bhatt, U., Antor{\'a}n, J., Zhang, Y., Liao, Q.~V., Sattigeri, P., Fogliato,
  R., Melan{\c{c}}on, G., Krishnan, R., Stanley, J., Tickoo, O., et~al.
\newblock Uncertainty as a form of transparency: Measuring, communicating, and
  using uncertainty.
\newblock In \emph{Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics,
  and Society}, pp.\  401--413, 2021.

\bibitem[Boluki et~al.(2020)Boluki, Ardywibowo, Dadaneh, Zhou, and
  Qian]{boluki2020learnable}
Boluki, S., Ardywibowo, R., Dadaneh, S.~Z., Zhou, M., and Qian, X.
\newblock Learnable {Bernoulli} dropout for {Bayesian} deep learning.
\newblock \emph{Artificial Intelligence and Statistics (AISTATS)}, 2020.

\bibitem[Damianou \& Lawrence(2013)Damianou and Lawrence]{damianou2013deep}
Damianou, A. and Lawrence, N.~D.
\newblock Deep {Gaussian} processes.
\newblock \emph{Artificial Intelligence and Statistics (AISTATS)}, 2013.

\bibitem[Daxberger et~al.(2021)Daxberger, Nalisnick, Allingham, Antor{\'a}n,
  and Hern{\'a}ndez-Lobato]{daxberger2021bayesian}
Daxberger, E., Nalisnick, E., Allingham, J.~U., Antor{\'a}n, J., and
  Hern{\'a}ndez-Lobato, J.~M.
\newblock Bayesian deep learning via subnetwork inference.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Deleu et~al.(2022)Deleu, G{\'o}is, Emezue, Rankawat, Lacoste-Julien,
  Bauer, and Bengio]{deleu2022bayesian}
Deleu, T., G{\'o}is, A., Emezue, C., Rankawat, M., Lacoste-Julien, S., Bauer,
  S., and Bengio, Y.
\newblock Bayesian structure learning with generative flow networks.
\newblock \emph{Uncertainty in Artificial Intelligence (UAI)}, 2022.

\bibitem[Fan et~al.(2021)Fan, Zhang, Tanwisuth, Qian, and
  Zhou]{fan2021contextual}
Fan, X., Zhang, S., Tanwisuth, K., Qian, X., and Zhou, M.
\newblock Contextual dropout: An efficient sample-dependent dropout module.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2021.

\bibitem[Foong et~al.(2020)Foong, Burt, Li, and
  Turner]{foong2020expressiveness}
Foong, A., Burt, D., Li, Y., and Turner, R.
\newblock On the expressiveness of approximate inference in {Bayesian} neural
  networks.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Fort et~al.(2019)Fort, Hu, and Lakshminarayanan]{fort2019deep}
Fort, S., Hu, H., and Lakshminarayanan, B.
\newblock Deep ensembles: A loss landscape perspective.
\newblock \emph{arXiv preprint arXiv:1912.02757}, 2019.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{gal2016dropout}
Gal, Y. and Ghahramani, Z.
\newblock Dropout as a {Bayesian} approximation: Representing model uncertainty
  in deep learning.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2016.

\bibitem[Gal et~al.(2017)Gal, Hron, and Kendall]{gal2017concrete}
Gal, Y., Hron, J., and Kendall, A.
\newblock Concrete dropout.
\newblock \emph{Neural Information Processing Systems (NIPS)}, 30, 2017.

\bibitem[Ghiasi et~al.(2018)Ghiasi, Lin, and Le]{ghiasi2018dropblock}
Ghiasi, G., Lin, T.-Y., and Le, Q.~V.
\newblock Dropblock: A regularization method for convolutional networks.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
\newblock On calibration of modern neural networks.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock \emph{Computer Vision and Pattern Recognition (CVPR)}, 2016.

\bibitem[Hendrycks \& Dietterich(2019)Hendrycks and
  Dietterich]{hendrycks2019benchmarking}
Hendrycks, D. and Dietterich, T.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, Krizhevsky, Sutskever, and
  Salakhutdinov]{hinton2012improving}
Hinton, G.~E., Srivastava, N., Krizhevsky, A., Sutskever, I., and
  Salakhutdinov, R.~R.
\newblock Improving neural networks by preventing co-adaptation of feature
  detectors.
\newblock \emph{arXiv preprint arXiv:1207.0580}, 2012.

\bibitem[Jain et~al.(2022)Jain, Bengio, Hernandez-Garcia, Rector-Brooks,
  Dossou, Ekbote, Fu, Zhang, Kilgour, Zhang, Simine, Das, and
  Bengio]{jain2022biological}
Jain, M., Bengio, E., Hernandez-Garcia, A., Rector-Brooks, J., Dossou, B.~F.,
  Ekbote, C.~A., Fu, J., Zhang, T., Kilgour, M., Zhang, D., Simine, L., Das,
  P., and Bengio, Y.
\newblock Biological sequence design with gflownets.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Jain et~al.(2023)Jain, Lahlou, Nekoei, Butoi, Bertin, Rector-Brooks,
  Korablyov, and Bengio]{jain2021deup}
Jain, M., Lahlou, S., Nekoei, H., Butoi, V., Bertin, P., Rector-Brooks, J.,
  Korablyov, M., and Bengio, Y.
\newblock {DEUP}: Direct epistemic uncertainty prediction.
\newblock \emph{Transactions on Machine Learning Research (TMLR)}, 2023.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and
  Welling]{kingma2015variational}
Kingma, D.~P., Salimans, T., and Welling, M.
\newblock Variational dropout and the local reparameterization trick.
\newblock \emph{Neural Information Processing Systems (NIPS)}, 2015.

\bibitem[Kuleshov et~al.(2018)Kuleshov, Fenner, and
  Ermon]{kuleshov2018accurate}
Kuleshov, V., Fenner, N., and Ermon, S.
\newblock Accurate uncertainties for deep learning using calibrated regression.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Le~Folgoc et~al.(2021)Le~Folgoc, Baltatzis, Desai, Devaraj, Ellis,
  Manzanera, Nair, Qiu, Schnabel, and Glocker]{le2021mc}
Le~Folgoc, L., Baltatzis, V., Desai, S., Devaraj, A., Ellis, S., Manzanera, O.
  E.~M., Nair, A., Qiu, H., Schnabel, J., and Glocker, B.
\newblock Is {MC} dropout bayesian?
\newblock \emph{arXiv preprint arXiv:2110.04286}, 2021.

\bibitem[Lee et~al.(2020)Lee, Nam, Yang, and Hwang]{lee2020meta}
Lee, H.~B., Nam, T., Yang, E., and Hwang, S.~J.
\newblock Meta dropout: Learning to perturb latent features for generalization.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2020.

\bibitem[Lotfi et~al.(2022)Lotfi, Izmailov, Benton, Goldblum, and
  Wilson]{lotfi2022bayesian}
Lotfi, S., Izmailov, P., Benton, G., Goldblum, M., and Wilson, A.~G.
\newblock Bayesian model selection, the marginal likelihood, and
  generalization.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[MacKay(1992)]{mackay1992practical}
MacKay, D.~J.
\newblock A practical {Bayesian} framework for backpropagation networks.
\newblock \emph{Neural Computation}, 4\penalty0 (3):\penalty0 448--472, 1992.

\bibitem[Madan et~al.(2023)Madan, Rector-Brooks, Korablyov, Bengio, Jain, Nica,
  Bosc, Bengio, and Malkin]{madan2022learning}
Madan, K., Rector-Brooks, J., Korablyov, M., Bengio, E., Jain, M., Nica, A.,
  Bosc, T., Bengio, Y., and Malkin, N.
\newblock Learning {GFlowNets} from partial episodes for improved convergence
  and stability.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2023.

\bibitem[Malkin et~al.(2022)Malkin, Jain, Bengio, Sun, and
  Bengio]{malkin2022trajectory}
Malkin, N., Jain, M., Bengio, E., Sun, C., and Bengio, Y.
\newblock Trajectory balance: Improved credit assignment in {GFlowNets}.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem[Malkin et~al.(2023)Malkin, Lahlou, Deleu, Ji, Hu, Everett, Zhang, and
  Bengio]{malkin2022gflownets}
Malkin, N., Lahlou, S., Deleu, T., Ji, X., Hu, E., Everett, K., Zhang, D., and
  Bengio, Y.
\newblock {GFlowNets} and variational inference.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2023.

\bibitem[Nado et~al.(2021)Nado, Band, Collier, Djolonga, Dusenberry, Farquhar,
  Filos, Havasi, Jenatton, Jerfel, Liu, Mariet, Nixon, Padhy, Ren, Rudner, Wen,
  Wenzel, Murphy, Sculley, Lakshminarayanan, Snoek, Gal, and
  Tran]{nado2021uncertainty}
Nado, Z., Band, N., Collier, M., Djolonga, J., Dusenberry, M., Farquhar, S.,
  Filos, A., Havasi, M., Jenatton, R., Jerfel, G., Liu, J., Mariet, Z., Nixon,
  J., Padhy, S., Ren, J., Rudner, T., Wen, Y., Wenzel, F., Murphy, K., Sculley,
  D., Lakshminarayanan, B., Snoek, J., Gal, Y., and Tran, D.
\newblock {Uncertainty Baselines}: Benchmarks for uncertainty \& robustness in
  deep learning.
\newblock \emph{arXiv preprint arXiv:2106.04015}, 2021.

\bibitem[Neal(2012)]{neal2012bayesian}
Neal, R.~M.
\newblock \emph{Bayesian learning for neural networks}, volume 118.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Nguyen et~al.(2015)Nguyen, Yosinski, and Clune]{nguyen2015deep}
Nguyen, A., Yosinski, J., and Clune, J.
\newblock Deep neural networks are easily fooled: High confidence predictions
  for unrecognizable images.
\newblock \emph{Computer Vision and Pattern Recognition (CVPR)}, 2015.

\bibitem[Nguyen et~al.(2021)Nguyen, Nguyen, Nguyen, Than, Bui, and
  Ho]{nguyen2021structured}
Nguyen, S., Nguyen, D., Nguyen, K., Than, K., Bui, H., and Ho, N.
\newblock Structured dropout variational inference for {Bayesian} neural
  networks.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Nica et~al.(2022)Nica, Jain, Bengio, Liu, Korablyov, Bronstein, and
  Bengio]{nica2022evaluating}
Nica, A.~C., Jain, M., Bengio, E., Liu, C.-H., Korablyov, M., Bronstein, M.~M.,
  and Bengio, Y.
\newblock Evaluating generalization in gflownets for molecule design.
\newblock \emph{ICLR 2022 Machine Learning for Drug Discovery workshop}, 2022.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{osband2013more}
Osband, I., Russo, D., and Van~Roy, B.
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock \emph{Neural Information Processing Systems (NIPS)}, 2013.

\bibitem[Ovadia et~al.(2019)Ovadia, Fertig, Ren, Nado, Sculley, Nowozin,
  Dillon, Lakshminarayanan, and Snoek]{ovadia2019can}
Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Nowozin, S., Dillon,
  J., Lakshminarayanan, B., and Snoek, J.
\newblock Can you trust your model's uncertainty? evaluating predictive
  uncertainty under dataset shift.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem[Pan et~al.(2023)Pan, Zhang, Courville, Huang, and
  Bengio]{Pan2022GenerativeAF}
Pan, L., Zhang, D., Courville, A.~C., Huang, L., and Bengio, Y.
\newblock Generative augmented flow networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2023.

\bibitem[Park \& Kwak(2016)Park and Kwak]{park2016analysis}
Park, S. and Kwak, N.
\newblock Analysis on the dropout effect in convolutional neural networks.
\newblock \emph{Asian Conference on Computer Vision}, 2016.

\bibitem[Pham \& Le(2021)Pham and Le]{pham2021autodropout}
Pham, H. and Le, Q.
\newblock Autodropout: Learning dropout patterns to regularize deep networks.
\newblock \emph{Association for the Advancement of Artificial Intelligence
  (AAAI)}, 2021.

\bibitem[Pollard et~al.(2018)Pollard, Johnson, Raffa, Celi, Mark, and
  Badawi]{pollard2018eicu}
Pollard, T.~J., Johnson, A.~E., Raffa, J.~D., Celi, L.~A., Mark, R.~G., and
  Badawi, O.
\newblock The {eICU Collaborative Research Database}, a freely available
  multi-center database for critical care research.
\newblock \emph{Scientific data}, 5\penalty0 (1):\penalty0 1--13, 2018.

\bibitem[Sensoy et~al.(2018)Sensoy, Kaplan, and Kandemir]{sensoy2018evidential}
Sensoy, M., Kaplan, L., and Kandemir, M.
\newblock Evidential deep learning to quantify classification uncertainty.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Wilson \& Izmailov(2020)Wilson and Izmailov]{wilson2020bayesian}
Wilson, A.~G. and Izmailov, P.
\newblock Bayesian deep learning and a probabilistic perspective of
  generalization.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Xie et~al.(2019)Xie, Ma, Zhang, Xue, Tan, and Guo]{xie2019soft}
Xie, J., Ma, Z., Zhang, G., Xue, J.-H., Tan, Z.-H., and Guo, J.
\newblock Soft dropout and its variational bayes approximation.
\newblock \emph{Machine Learning for Signal Processing (MLSP)}, 2019.

\bibitem[Yang et~al.(2020)Yang, Tang, Torun, Becker, Hejase, and
  Swaminathan]{yang2020rx}
Yang, X., Tang, J., Torun, H.~M., Becker, W.~D., Hejase, J.~A., and
  Swaminathan, M.
\newblock Rx equalization for a high-speed channel based on bayesian active
  learning using dropout.
\newblock \emph{Electrical Performance of Electronic Packaging and Systems
  (EPEPS)}, 2020.

\bibitem[Yu et~al.(2019)Yu, Yu, Cui, Tao, and Tian]{yu2019deep}
Yu, Z., Yu, J., Cui, Y., Tao, D., and Tian, Q.
\newblock Deep modular co-attention networks for visual question answering.
\newblock \emph{Computer Vision and Pattern Recognition (CVPR)}, 2019.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Chen, Malkin, and
  Bengio]{zhang2022unifying}
Zhang, D., Chen, R.~T., Malkin, N., and Bengio, Y.
\newblock Unifying generative models with gflownets.
\newblock \emph{arXiv preprint arXiv:2209.02606}, 2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Malkin, Liu, Volokhova,
  Courville, and Bengio]{zhang2022generative}
Zhang, D., Malkin, N., Liu, Z., Volokhova, A., Courville, A., and Bengio, Y.
\newblock Generative flow networks for discrete probabilistic modeling.
\newblock \emph{International Conference on Machine Learning (ICML)},
  2022{\natexlab{b}}.

\end{thebibliography}
