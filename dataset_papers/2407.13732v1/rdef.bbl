\begin{thebibliography}{100}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Acar et~al.(2020)Acar, Gangrade, and Saligrama]{acar2020budget}
Durmus Alp~Emre Acar, Aditya Gangrade, and Venkatesh Saligrama.
\newblock Budget learning via bracketing.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 4109--4119, 2020.

\bibitem[Awasthi et~al.(2022{\natexlab{a}})Awasthi, Mao, Mohri, and
  Zhong]{awasthi2022Hconsistency}
Pranjal Awasthi, Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock {${\mathscr H}$}-consistency bounds for surrogate loss minimizers.
\newblock In \emph{International Conference on Machine Learning},
  2022{\natexlab{a}}.

\bibitem[Awasthi et~al.(2022{\natexlab{b}})Awasthi, Mao, Mohri, and
  Zhong]{awasthi2022multi}
Pranjal Awasthi, Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Multi-class {${\mathscr H}$}-consistency bounds.
\newblock In \emph{Advances in neural information processing systems},
  2022{\natexlab{b}}.

\bibitem[Awasthi et~al.(2023{\natexlab{a}})Awasthi, Mao, Mohri, and
  Zhong]{AwasthiMaoMohriZhong2023theoretically}
Pranjal Awasthi, Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Theoretically grounded loss functions and algorithms for adversarial
  robustness.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 10077--10094, 2023{\natexlab{a}}.

\bibitem[Awasthi et~al.(2023{\natexlab{b}})Awasthi, Mao, Mohri, and
  Zhong]{awasthi2023dc}
Pranjal Awasthi, Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock {DC}-programming for neural network optimizations.
\newblock \emph{Journal of Global Optimization}, 2023{\natexlab{b}}.

\bibitem[Bartlett and Wegkamp(2008)]{bartlett2008classification}
Peter~L Bartlett and Marten~H Wegkamp.
\newblock Classification with a reject option using a hinge loss.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0 (8), 2008.

\bibitem[Bartlett et~al.(2006)Bartlett, Jordan, and
  McAuliffe]{bartlett2006convexity}
Peter~L. Bartlett, Michael~I. Jordan, and Jon~D. McAuliffe.
\newblock Convexity, classification, and risk bounds.
\newblock \emph{Journal of the American Statistical Association}, 101\penalty0
  (473):\penalty0 138--156, 2006.

\bibitem[Battleday et~al.(2020)Battleday, Peterson, and
  Griffiths]{battleday2020capturing}
Ruairidh~M Battleday, Joshua~C Peterson, and Thomas~L Griffiths.
\newblock Capturing human categorization of natural images by combining deep
  networks and cognitive models.
\newblock \emph{Nature communications}, 11\penalty0 (1):\penalty0 5418, 2020.

\bibitem[Benz and Rodriguez(2022)]{benz2022counterfactual}
Nina L~Corvelo Benz and Manuel~Gomez Rodriguez.
\newblock Counterfactual inference of second opinions.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 453--463,
  2022.

\bibitem[Berkson(1944)]{Berkson1944}
Joseph Berkson.
\newblock Application of the logistic function to bio-assay.
\newblock \emph{Journal of the American Statistical Association}, 39:\penalty0
  357–--365, 1944.

\bibitem[Berkson(1951)]{Berkson1951}
Joseph Berkson.
\newblock Why {I} prefer logits to probits.
\newblock \emph{Biometrics}, 7\penalty0 (4):\penalty0 327–--339, 1951.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz,
  Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
  Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg,
  et~al.
\newblock Sparks of artificial general intelligence: Early experiments with
  gpt-4.
\newblock \emph{arXiv preprint arXiv:2303.12712}, 2023.

\bibitem[Cao et~al.(2022)Cao, Cai, Feng, Gu, Gu, An, Niu, and
  Sugiyama]{caogeneralizing}
Yuzhou Cao, Tianchi Cai, Lei Feng, Lihong Gu, Jinjie Gu, Bo~An, Gang Niu, and
  Masashi Sugiyama.
\newblock Generalizing consistent multi-class classification with rejection to
  be compatible with arbitrary losses.
\newblock In \emph{Advances in neural information processing systems}, 2022.

\bibitem[Cao et~al.(2023)Cao, Mozannar, Feng, Wei, and An]{cao2023defense}
Yuzhou Cao, Hussein Mozannar, Lei Feng, Hongxin Wei, and Bo~An.
\newblock In defense of softmax parametrization for calibrated and consistent
  learning to defer.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Charoenphakdee et~al.(2021)Charoenphakdee, Cui, Zhang, and
  Sugiyama]{charoenphakdee2021classification}
Nontawat Charoenphakdee, Zhenghang Cui, Yivan Zhang, and Masashi Sugiyama.
\newblock Classification with rejection based on cost-sensitive classification.
\newblock In \emph{International Conference on Machine Learning}, pages
  1507--1517, 2021.

\bibitem[Charusaie and Samadi(2024)]{Mohammad2024}
Mohammad-Amin Charusaie and Samira Samadi.
\newblock A unifying post-processing framework for multi-objective
  learn-to-defer problems.
\newblock \emph{arXiv preprint arXiv:2407.12710}, 2024.

\bibitem[Charusaie et~al.(2022)Charusaie, Mozannar, Sontag, and
  Samadi]{charusaie2022sample}
Mohammad-Amin Charusaie, Hussein Mozannar, David Sontag, and Samira Samadi.
\newblock Sample efficient learning of predictors that complement humans.
\newblock In \emph{International Conference on Machine Learning}, pages
  2972--3005, 2022.

\bibitem[Chen et~al.(2024)Chen, Li, Sun, and Wang]{chen2024learning}
Guanting Chen, Xiaocheng Li, Chunlin Sun, and Hanzhao Wang.
\newblock Learning to make adherence-aware advice.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Cheng et~al.(2023)Cheng, Cao, Wang, Wei, An, and
  Feng]{cheng2023regression}
Xin Cheng, Yuzhou Cao, Haobo Wang, Hongxin Wei, Bo~An, and Lei Feng.
\newblock Regression with cost-based rejection.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Chow(1970)]{chow1970optimum}
C~Chow.
\newblock On optimum recognition error and reject tradeoff.
\newblock \emph{IEEE Transactions on information theory}, 16\penalty0
  (1):\penalty0 41--46, 1970.

\bibitem[Chow(1957)]{Chow1957}
C.K. Chow.
\newblock An optimum character recognition system using decision function.
\newblock \emph{IEEE T. C.}, 1957.

\bibitem[Cortes et~al.(2016{\natexlab{a}})Cortes, DeSalvo, and
  Mohri]{CortesDeSalvoMohri2016}
Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri.
\newblock Learning with rejection.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pages 67--82, 2016{\natexlab{a}}.

\bibitem[Cortes et~al.(2016{\natexlab{b}})Cortes, DeSalvo, and
  Mohri]{CortesDeSalvoMohri2016bis}
Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri.
\newblock Boosting with abstention.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1660--1668, 2016{\natexlab{b}}.

\bibitem[Cortes et~al.(2023)Cortes, DeSalvo, and Mohri]{CortesDeSalvoMohri2023}
Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri.
\newblock Theory and algorithms for learning with rejection in binary
  classification.
\newblock \emph{Annals of Mathematics and Artificial Intelligence}, pages
  1--39, 2023.

\bibitem[Cortes et~al.(2024)Cortes, Mao, Mohri, Mohri, and
  Zhong]{cortes2024cardinality}
Corinna Cortes, Anqi Mao, Christopher Mohri, Mehryar Mohri, and Yutao Zhong.
\newblock Cardinality-aware set prediction and top-$ k $ classification.
\newblock \emph{arXiv preprint arXiv:2407.07140}, 2024.

\bibitem[Davidson et~al.(2017)Davidson, Warmsley, Macy, and
  Weber]{davidson2017automated}
Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber.
\newblock Automated hate speech detection and the problem of offensive
  language.
\newblock In \emph{Proceedings of the international AAAI conference on web and
  social media}, volume~11, pages 512--515, 2017.

\bibitem[De et~al.(2020)De, Koley, Ganguly, and
  Gomez-Rodriguez]{de2020regression}
Abir De, Paramita Koley, Niloy Ganguly, and Manuel Gomez-Rodriguez.
\newblock Regression under human assistance.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pages 2611--2620, 2020.

\bibitem[De et~al.(2021)De, Okati, Zarezade, and
  Rodriguez]{de2021classification}
Abir De, Nastaran Okati, Ali Zarezade, and Manuel~Gomez Rodriguez.
\newblock Classification under human assistance.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pages 5905--5913, 2021.

\bibitem[Dressel and Farid(2018)]{dressel2018accuracy}
Julia Dressel and Hany Farid.
\newblock The accuracy, fairness, and limits of predicting recidivism.
\newblock \emph{Science advances}, 4\penalty0 (1):\penalty0 eaao5580, 2018.

\bibitem[El-Yaniv and Wiener(2012)]{el2012active}
Ran El-Yaniv and Yair Wiener.
\newblock Active learning via perfect selective classification.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0 (2), 2012.

\bibitem[El-Yaniv et~al.(2010)]{el2010foundations}
Ran El-Yaniv et~al.
\newblock On the foundations of noise-free selective classification.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (5), 2010.

\bibitem[Gangrade et~al.(2021)Gangrade, Kag, and
  Saligrama]{gangrade2021selective}
Aditya Gangrade, Anil Kag, and Venkatesh Saligrama.
\newblock Selective classification via one-sided prediction.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2179--2187, 2021.

\bibitem[Gao et~al.(2021)Gao, Saar-Tsechansky, De-Arteaga, Han, Lee, and
  Lease]{gao2021human}
Ruijiang Gao, Maytal Saar-Tsechansky, Maria De-Arteaga, Ligong Han, Min~Kyung
  Lee, and Matthew Lease.
\newblock Human-ai collaboration with bandit feedback.
\newblock \emph{arXiv preprint arXiv:2105.10614}, 2021.

\bibitem[Geifman and El-Yaniv(2017)]{geifman2017selective}
Yonatan Geifman and Ran El-Yaniv.
\newblock Selective classification for deep neural networks.
\newblock In \emph{Advances in neural information processing systems}, 2017.

\bibitem[Geifman and El-Yaniv(2019)]{geifman2019selectivenet}
Yonatan Geifman and Ran El-Yaniv.
\newblock Selectivenet: A deep neural network with an integrated reject option.
\newblock In \emph{International conference on machine learning}, pages
  2151--2159, 2019.

\bibitem[Ghosh et~al.(2017)Ghosh, Kumar, and Sastry]{ghosh2017robust}
Aritra Ghosh, Himanshu Kumar, and P~Shanti Sastry.
\newblock Robust loss functions under label noise for deep neural networks.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, 2017.

\bibitem[Hemmer et~al.(2022)Hemmer, Schellhammer, V{\"o}ssing, Jakubik, and
  Satzger]{hemmer2022forming}
Patrick Hemmer, Sebastian Schellhammer, Michael V{\"o}ssing, Johannes Jakubik,
  and Gerhard Satzger.
\newblock Forming effective human-ai teams: Building machine learning models
  that complement the capabilities of multiple experts.
\newblock \emph{arXiv preprint arXiv:2206.07948}, 2022.

\bibitem[Hemmer et~al.(2023)Hemmer, Thede, V{\"o}ssing, Jakubik, and
  K{\"u}hl]{hemmer2023learning}
Patrick Hemmer, Lukas Thede, Michael V{\"o}ssing, Johannes Jakubik, and Niklas
  K{\"u}hl.
\newblock Learning to defer with limited expert predictions.
\newblock \emph{arXiv preprint arXiv:2304.07306}, 2023.

\bibitem[Jiang et~al.(2020)Jiang, Zhao, and Wang]{jiang2020risk}
Wenming Jiang, Ying Zhao, and Zehan Wang.
\newblock Risk-controlled selective prediction for regression deep neural
  network models.
\newblock In \emph{2020 International Joint Conference on Neural Networks
  (IJCNN)}, pages 1--8, 2020.

\bibitem[Jitkrittum et~al.(2023)Jitkrittum, Gupta, Menon, Narasimhan, Rawat,
  and Kumar]{jitkrittum2023does}
Wittawat Jitkrittum, Neha Gupta, Aditya~K Menon, Harikrishna Narasimhan, Ankit
  Rawat, and Sanjiv Kumar.
\newblock When does confidence-based cascade deferral suffice?
\newblock In \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Joshi et~al.(2021)Joshi, Parbhoo, and Doshi-Velez]{joshi2021pre}
Shalmali Joshi, Sonali Parbhoo, and Finale Doshi-Velez.
\newblock Pre-emptive learning-to-defer for sequential medical decision-making
  under uncertainty.
\newblock \emph{arXiv preprint arXiv:2109.06312}, 2021.

\bibitem[Kerrigan et~al.(2021)Kerrigan, Smyth, and
  Steyvers]{kerrigan2021combining}
Gavin Kerrigan, Padhraic Smyth, and Mark Steyvers.
\newblock Combining human predictions with model probabilities via confusion
  matrices and calibration.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 4421--4434, 2021.

\bibitem[Keswani et~al.(2021)Keswani, Lease, and
  Kenthapadi]{keswani2021towards}
Vijay Keswani, Matthew Lease, and Krishnaram Kenthapadi.
\newblock Towards unbiased and accurate deferral to multiple experts.
\newblock In \emph{Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics,
  and Society}, pages 154--165, 2021.

\bibitem[Li et~al.(2023)Li, Liu, Sun, and Wang]{li2023no}
Xiaocheng Li, Shang Liu, Chunlin Sun, and Hanzhao Wang.
\newblock When no-rejection learning is optimal for regression with rejection.
\newblock \emph{arXiv preprint arXiv:2307.02932}, 2023.

\bibitem[Liu et~al.(2022)Liu, Gallego, and Barbieri]{liu2022incorporating}
Jessie Liu, Blanca Gallego, and Sebastiano Barbieri.
\newblock Incorporating uncertainty in learning to defer algorithms for safe
  computer-aided diagnosis.
\newblock \emph{Scientific reports}, 12\penalty0 (1):\penalty0 1762, 2022.

\bibitem[Liu et~al.(2024)Liu, Cao, Zhang, Feng, and An]{liu2024mitigating}
Shuqi Liu, Yuzhou Cao, Qiaozhen Zhang, Lei Feng, and Bo~An.
\newblock Mitigating underfitting in learning to defer with consistent losses.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 4816--4824, 2024.

\bibitem[Long and Servedio(2013)]{long2013consistency}
Phil Long and Rocco Servedio.
\newblock Consistency versus realizable {H}-consistency for multiclass
  classification.
\newblock In \emph{International Conference on Machine Learning}, pages
  801--809, 2013.

\bibitem[Madras et~al.(2018)Madras, Creager, Pitassi, and
  Zemel]{madras2018learning}
David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel.
\newblock Learning adversarially fair and transferable representations.
\newblock \emph{arXiv preprint arXiv:1802.06309}, 2018.

\bibitem[Mao et~al.(2023{\natexlab{a}})Mao, Mohri, Mohri, and
  Zhong]{MaoMohriMohriZhong2023two}
Anqi Mao, Christopher Mohri, Mehryar Mohri, and Yutao Zhong.
\newblock Two-stage learning to defer with multiple experts.
\newblock In \emph{Advances in neural information processing systems},
  2023{\natexlab{a}}.

\bibitem[Mao et~al.(2023{\natexlab{b}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2023characterization}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock {H}-consistency bounds: Characterization and extensions.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2023{\natexlab{b}}.

\bibitem[Mao et~al.(2023{\natexlab{c}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2023ranking}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock {H}-consistency bounds for pairwise misranking loss surrogates.
\newblock In \emph{International conference on Machine learning},
  2023{\natexlab{c}}.

\bibitem[Mao et~al.(2023{\natexlab{d}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2023rankingabs}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Ranking with abstention.
\newblock In \emph{ICML 2023 Workshop The Many Facets of Preference-Based
  Learning}, 2023{\natexlab{d}}.

\bibitem[Mao et~al.(2023{\natexlab{e}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2023structured}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Structured prediction with stronger consistency guarantees.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2023{\natexlab{e}}.

\bibitem[Mao et~al.(2023{\natexlab{f}})Mao, Mohri, and Zhong]{mao2023cross}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Cross-entropy loss functions: Theoretical analysis and applications.
\newblock In \emph{International Conference on Machine Learning},
  2023{\natexlab{f}}.

\bibitem[Mao et~al.(2024{\natexlab{a}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2024deferral}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Principled approaches for learning to defer with multiple experts.
\newblock In \emph{International Symposium on Artificial Intelligence and
  Mathematics}, 2024{\natexlab{a}}.

\bibitem[Mao et~al.(2024{\natexlab{b}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2024predictor}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Predictor-rejector multi-class abstention: Theoretical analysis and
  algorithms.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pages 822--867, 2024{\natexlab{b}}.

\bibitem[Mao et~al.(2024{\natexlab{c}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2024score}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Theoretically grounded loss functions and algorithms for score-based
  multi-class abstention.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 4753--4761, 2024{\natexlab{c}}.

\bibitem[Mao et~al.(2024{\natexlab{d}})Mao, Mohri, and Zhong]{mao2024h}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock {$ H $}-consistency guarantees for regression.
\newblock \emph{arXiv preprint arXiv:2403.19480}, 2024{\natexlab{d}}.

\bibitem[Mao et~al.(2024{\natexlab{e}})Mao, Mohri, and
  Zhong]{mao2024regression}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Regression with multi-expert deferral.
\newblock \emph{arXiv preprint arXiv:2403.19494}, 2024{\natexlab{e}}.

\bibitem[Mao et~al.(2024{\natexlab{f}})Mao, Mohri, and Zhong]{mao2024top}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Top-$ k $ classification and cardinality-aware prediction.
\newblock \emph{arXiv preprint arXiv:2403.19625}, 2024{\natexlab{f}}.

\bibitem[Mao et~al.(2024{\natexlab{g}})Mao, Mohri, and Zhong]{mao2024universal}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock A universal growth rate for learning with smooth surrogate losses.
\newblock \emph{arXiv preprint arXiv:2405.05968}, 2024{\natexlab{g}}.

\bibitem[Mohri et~al.(2024)Mohri, Andor, Choi, Collins, Mao, and
  Zhong]{MohriAndorChoiCollinsMaoZhong2024learning}
Christopher Mohri, Daniel Andor, Eunsol Choi, Michael Collins, Anqi Mao, and
  Yutao Zhong.
\newblock Learning to reject with a fixed predictor: Application to
  decontextualization.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Mohri et~al.(2018)Mohri, Rostamizadeh, and
  Talwalkar]{MohriRostamizadehTalwalkar2018}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock \emph{Foundations of Machine Learning}.
\newblock {MIT} Press, second edition, 2018.

\bibitem[Mozannar and Sontag(2020)]{mozannar2020consistent}
Hussein Mozannar and David Sontag.
\newblock Consistent estimators for learning to defer to an expert.
\newblock In \emph{International Conference on Machine Learning}, pages
  7076--7087, 2020.

\bibitem[Mozannar et~al.(2022)Mozannar, Satyanarayan, and
  Sontag]{mozannar2022teaching}
Hussein Mozannar, Arvind Satyanarayan, and David Sontag.
\newblock Teaching humans when to defer to a classifier via exemplars.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pages 5323--5331, 2022.

\bibitem[Mozannar et~al.(2023)Mozannar, Lang, Wei, Sattigeri, Das, and
  Sontag]{pmlr-v206-mozannar23a}
Hussein Mozannar, Hunter Lang, Dennis Wei, Prasanna Sattigeri, Subhro Das, and
  David Sontag.
\newblock Who should predict? exact algorithms for learning to defer to humans.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 10520--10545, 2023.

\bibitem[Narasimhan et~al.(2022)Narasimhan, Jitkrittum, Menon, Rawat, and
  Kumar]{narasimhanpost}
Harikrishna Narasimhan, Wittawat Jitkrittum, Aditya~Krishna Menon, Ankit~Singh
  Rawat, and Sanjiv Kumar.
\newblock Post-hoc estimators for learning to defer to an expert.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  29292--29304, 2022.

\bibitem[Neyman and Pearson(1933)]{neyman1933ix}
Jerzy Neyman and Egon~Sharpe Pearson.
\newblock Ix. on the problem of the most efficient tests of statistical
  hypotheses.
\newblock \emph{Philosophical Transactions of the Royal Society of London.
  Series A, Containing Papers of a Mathematical or Physical Character},
  231\penalty0 (694-706):\penalty0 289--337, 1933.

\bibitem[Ni et~al.(2019)Ni, Charoenphakdee, Honda, and Sugiyama]{NiCHS19}
Chenri Ni, Nontawat Charoenphakdee, Junya Honda, and Masashi Sugiyama.
\newblock On the calibration of multiclass classification with rejection.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2582--2592, 2019.

\bibitem[Okati et~al.(2021)Okati, De, and Rodriguez]{okati2021differentiable}
Nastaran Okati, Abir De, and Manuel Rodriguez.
\newblock Differentiable learning under triage.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 9140--9151, 2021.

\bibitem[Palomba et~al.(2024)Palomba, Pugnana, Alvarez, and
  Ruggieri]{palomba2024causal}
Filippo Palomba, Andrea Pugnana, Jos{\'e}~Manuel Alvarez, and Salvatore
  Ruggieri.
\newblock A causal framework for evaluating deferring systems.
\newblock \emph{arXiv preprint arXiv:2405.18902}, 2024.

\bibitem[Pradier et~al.(2021)Pradier, Zazo, Parbhoo, Perlis, Zazzi, and
  Doshi-Velez]{pradier2021preferential}
Melanie~F Pradier, Javier Zazo, Sonali Parbhoo, Roy~H Perlis, Maurizio Zazzi,
  and Finale Doshi-Velez.
\newblock Preferential mixture-of-experts: Interpretable models that rely on
  human expertise as much as possible.
\newblock \emph{AMIA Summits on Translational Science Proceedings},
  2021:\penalty0 525, 2021.

\bibitem[Raghu et~al.(2019)Raghu, Blumer, Corrado, Kleinberg, Obermeyer, and
  Mullainathan]{raghu2019algorithmic}
Maithra Raghu, Katy Blumer, Greg Corrado, Jon Kleinberg, Ziad Obermeyer, and
  Sendhil Mullainathan.
\newblock The algorithmic automation problem: Prediction, triage, and human
  effort.
\newblock \emph{arXiv preprint arXiv:1903.12220}, 2019.

\bibitem[Raman and Yee(2021)]{raman2021improving}
Naveen Raman and Michael Yee.
\newblock Improving learning-to-defer algorithms through fine-tuning.
\newblock \emph{arXiv preprint arXiv:2112.10768}, 2021.

\bibitem[Ramaswamy et~al.(2018)Ramaswamy, Tewari, and
  Agarwal]{ramaswamy2018consistent}
Harish~G Ramaswamy, Ambuj Tewari, and Shivani Agarwal.
\newblock Consistent algorithms for multiclass classification with an abstain
  option.
\newblock \emph{Electronic Journal of Statistics}, 12\penalty0 (1):\penalty0
  530--554, 2018.

\bibitem[Reid and Williamson(2010)]{reid2010composite}
Mark~D Reid and Robert~C Williamson.
\newblock Composite binary losses.
\newblock \emph{The Journal of Machine Learning Research}, 11:\penalty0
  2387--2422, 2010.

\bibitem[Shah et~al.(2022)Shah, Bu, Lee, Das, Panda, Sattigeri, and
  Wornell]{shah2022selective}
Abhin Shah, Yuheng Bu, Joshua~K Lee, Subhro Das, Rameswar Panda, Prasanna
  Sattigeri, and Gregory~W Wornell.
\newblock Selective regression under fairness criteria.
\newblock In \emph{International Conference on Machine Learning}, pages
  19598--19615, 2022.

\bibitem[Steinwart(2007)]{steinwart2007compare}
Ingo Steinwart.
\newblock How to compare different loss functions and their risks.
\newblock \emph{Constructive Approximation}, 26\penalty0 (2):\penalty0
  225--287, 2007.

\bibitem[Straitouri et~al.(2021)Straitouri, Singla, Meresht, and
  Gomez-Rodriguez]{straitouri2021reinforcement}
Eleni Straitouri, Adish Singla, Vahid~Balazadeh Meresht, and Manuel
  Gomez-Rodriguez.
\newblock Reinforcement learning under algorithmic triage.
\newblock \emph{arXiv preprint arXiv:2109.11328}, 2021.

\bibitem[Straitouri et~al.(2022)Straitouri, Wang, Okati, and
  Rodriguez]{straitouri2022provably}
Eleni Straitouri, Lequn Wang, Nastaran Okati, and Manuel~Gomez Rodriguez.
\newblock Provably improving expert predictions with conformal prediction.
\newblock \emph{arXiv preprint arXiv:2201.12006}, 2022.

\bibitem[Tailor et~al.(2024)Tailor, Patra, Verma, Manggala, and
  Nalisnick]{tailor2024learning}
Dharmesh Tailor, Aditya Patra, Rajeev Verma, Putra Manggala, and Eric
  Nalisnick.
\newblock Learning to defer to a population: A meta-learning approach.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3475--3483, 2024.

\bibitem[Tewari and Bartlett(2007)]{tewari2007consistency}
Ambuj Tewari and Peter~L. Bartlett.
\newblock On the consistency of multiclass classification methods.
\newblock \emph{Journal of Machine Learning Research}, 8\penalty0
  (36):\penalty0 1007--1025, 2007.

\bibitem[Verhulst(1838)]{Verhulst1838}
Pierre~François Verhulst.
\newblock Notice sur la loi que la population suit dans son accroissement.
\newblock \emph{Correspondance math\'ematique et physique}, 10:\penalty0
  113–--121, 1838.

\bibitem[Verhulst(1845)]{Verhulst1845}
Pierre~François Verhulst.
\newblock Recherches math\'ematiques sur la loi d'accroissement de la
  population.
\newblock \emph{Nouveaux M\'emoires de l'Acad\'emie Royale des Sciences et
  Belles-Lettres de Bruxelles}, 18:\penalty0 1–--42, 1845.

\bibitem[Verma and Nalisnick(2022)]{verma2022calibrated}
Rajeev Verma and Eric Nalisnick.
\newblock Calibrated learning to defer with one-vs-all classifiers.
\newblock In \emph{International Conference on Machine Learning}, pages
  22184--22202, 2022.

\bibitem[Verma et~al.(2023)Verma, Barrej{\'o}n, and
  Nalisnick]{verma2023learning}
Rajeev Verma, Daniel Barrej{\'o}n, and Eric Nalisnick.
\newblock Learning to defer to multiple experts: Consistent surrogate losses,
  confidence calibration, and conformal ensembles.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 11415--11434, 2023.

\bibitem[Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama,
  Bosma, Zhou, Metzler, Chi, Hashimoto, Vinyals, Liang, Dean, and
  Fedus]{WeiEtAl2022}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
  Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed~H.
  Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
  Fedus.
\newblock Emergent abilities of large language models.
\newblock \emph{CoRR}, abs/2206.07682, 2022.

\bibitem[Wiener and El-Yaniv(2011)]{wiener2011agnostic}
Yair Wiener and Ran El-Yaniv.
\newblock Agnostic selective classification.
\newblock In \emph{Advances in neural information processing systems}, 2011.

\bibitem[Wiener and El-Yaniv(2012)]{wiener2012pointwise}
Yair Wiener and Ran El-Yaniv.
\newblock Pointwise tracking the optimal regression function.
\newblock \emph{Advances in Neural Information Processing Systems}, 25, 2012.

\bibitem[Wiener and El-Yaniv(2015)]{wiener2015agnostic}
Yair Wiener and Ran El-Yaniv.
\newblock Agnostic pointwise-competitive selective classification.
\newblock \emph{Journal of Artificial Intelligence Research}, 52:\penalty0
  171--201, 2015.

\bibitem[Wilder et~al.(2021)Wilder, Horvitz, and Kamar]{wilder2021learning}
Bryan Wilder, Eric Horvitz, and Ece Kamar.
\newblock Learning to complement humans.
\newblock In \emph{International Joint Conferences on Artificial Intelligence},
  pages 1526--1533, 2021.

\bibitem[Yuan and Wegkamp(2010)]{yuan2010classification}
Ming Yuan and Marten Wegkamp.
\newblock Classification methods with reject option based on convex risk
  minimization.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (1), 2010.

\bibitem[Yuan and Wegkamp(2011)]{WegkampYuan2011}
Ming Yuan and Marten Wegkamp.
\newblock {SVM}s with a reject option.
\newblock In \emph{Bernoulli}, 2011.

\bibitem[Zaoui et~al.(2020)Zaoui, Denis, and Hebiri]{zaoui2020regression}
Ahmed Zaoui, Christophe Denis, and Mohamed Hebiri.
\newblock Regression with reject option and application to knn.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  20073--20082, 2020.

\bibitem[Zhang and Agarwal(2020)]{zhang2020bayes}
Mingyuan Zhang and Shivani Agarwal.
\newblock Bayes consistency vs. {H}-consistency: The interplay between
  surrogate loss functions and the scoring function class.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Zhang(2004{\natexlab{a}})]{Zhang2003}
Tong Zhang.
\newblock Statistical behavior and consistency of classification methods based
  on convex risk minimization.
\newblock \emph{The Annals of Statistics}, 32\penalty0 (1):\penalty0 56--85,
  2004{\natexlab{a}}.

\bibitem[Zhang(2004{\natexlab{b}})]{zhang2004statistical}
Tong Zhang.
\newblock Statistical analysis of some multi-category large margin
  classification methods.
\newblock \emph{Journal of Machine Learning Research}, 5\penalty0
  (Oct):\penalty0 1225--1251, 2004{\natexlab{b}}.

\bibitem[Zhang and Sabuncu(2018)]{zhang2018generalized}
Zhilu Zhang and Mert Sabuncu.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock In \emph{Advances in neural information processing systems}, 2018.

\bibitem[Zhao et~al.(2021)Zhao, Agrawal, Razavi, and Sontag]{zhao2021directing}
Jason Zhao, Monica Agrawal, Pedram Razavi, and David Sontag.
\newblock Directing human attention in event localization for clinical timeline
  creation.
\newblock In \emph{Machine Learning for Healthcare Conference}, pages 80--102,
  2021.

\bibitem[Zheng et~al.(2023)Zheng, Wu, Bao, Cao, Li, and
  Zhu]{zheng2023revisiting}
Chenyu Zheng, Guoqiang Wu, Fan Bao, Yue Cao, Chongxuan Li, and Jun Zhu.
\newblock Revisiting discriminative vs. generative classifiers: Theory and
  implications.
\newblock In \emph{International Conference on Machine Learning}, pages
  42420--42477, 2023.

\end{thebibliography}
