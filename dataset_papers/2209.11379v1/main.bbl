\begin{thebibliography}{10}

\bibitem{arivazhagan2019massively}
Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson,
  Maxim Krikun, Mia~Xu Chen, Yuan Cao, George Foster, Colin Cherry, et~al.
\newblock Massively multilingual neural machine translation in the wild:
  Findings and challenges.
\newblock {\em arXiv preprint arXiv:1907.05019}, 2019.

\bibitem{bapna2022mslam}
Ankur Bapna, Colin Cherry, Yu~Zhang, Ye~Jia, Melvin Johnson, Yong Cheng, Simran
  Khanuja, Jason Riesa, and Alexis Conneau.
\newblock mslam: Massively multilingual joint pre-training for speech and text.
\newblock {\em arXiv preprint arXiv:2202.01374}, 2022.

\bibitem{boyd2004convex}
Stephen Boyd and Lieven Vandenberghe.
\newblock {\em Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem{chen2018gradnorm}
Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich.
\newblock Gradnorm: Gradient normalization for adaptive loss balancing in deep
  multitask networks.
\newblock In {\em International Conference on Machine Learning}, pages
  794--803. PMLR, 2018.

\bibitem{chen2020just}
Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning
  Chai, and Dragomir Anguelov.
\newblock Just pick a sign: Optimizing deep multitask models with gradient sign
  dropout.
\newblock {\em Advances in Neural Information Processing Systems},
  33:2039--2050, 2020.

\bibitem{cordts2016cityscapes}
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,
  Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.
\newblock The cityscapes dataset for semantic urban scene understanding.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 3213--3223, 2016.

\bibitem{desideri2012multiple}
Jean-Antoine D{\'e}sid{\'e}ri.
\newblock Multiple-gradient descent algorithm (mgda) for multiobjective
  optimization.
\newblock {\em Comptes Rendus Mathematique}, 350(5-6):313--318, 2012.

\bibitem{donoho201750}
David Donoho.
\newblock 50 years of data science.
\newblock {\em Journal of Computational and Graphical Statistics},
  26(4):745--766, 2017.

\bibitem{fifty2021efficiently}
Chris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn.
\newblock Efficiently identifying task groupings for multi-task learning.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{graves2017automated}
Alex Graves, Marc~G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu.
\newblock Automated curriculum learning for neural networks.
\newblock In {\em international conference on machine learning}, pages
  1311--1320. PMLR, 2017.

\bibitem{greff2016lstm}
Klaus Greff, Rupesh~K Srivastava, Jan Koutn{\'\i}k, Bas~R Steunebrink, and
  J{\"u}rgen Schmidhuber.
\newblock Lstm: A search space odyssey.
\newblock {\em IEEE transactions on neural networks and learning systems},
  28(10):2222--2232, 2016.

\bibitem{gulrajani2020search}
Ishaan Gulrajani and David Lopez-Paz.
\newblock In search of lost domain generalization.
\newblock {\em arXiv preprint arXiv:2007.01434}, 2020.

\bibitem{kendall2018multi}
Alex Kendall, Yarin Gal, and Roberto Cipolla.
\newblock Multi-task learning using uncertainty to weigh losses for scene
  geometry and semantics.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7482--7491, 2018.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kreutzer2021bandits}
Julia Kreutzer, David Vilar, and Artem Sokolov.
\newblock Bandits don't follow rules: Balancing multi-facet machine translation
  with multi-armed bandits.
\newblock {\em arXiv preprint arXiv:2110.06997}, 2021.

\bibitem{kurin2022defense}
Vitaly Kurin, Alessandro De~Palma, Ilya Kostrikov, Shimon Whiteson, and M~Pawan
  Kumar.
\newblock In defense of the unitary scalarization for deep multi-task learning.
\newblock {\em arXiv preprint arXiv:2201.04122}, 2022.

\bibitem{lepikhin2020gshard}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping
  Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
\newblock Gshard: Scaling giant models with conditional computation and
  automatic sharding.
\newblock {\em arXiv preprint arXiv:2006.16668}, 2020.

\bibitem{li2021robust}
Xian Li and Hongyu Gong.
\newblock Robust optimization for multilingual translation with imbalanced
  data.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{liberman2010obituary}
Mark Liberman.
\newblock Obituary: Fred jelinek.
\newblock {\em Computational Linguistics}, 36(4):595--599, 2010.

\bibitem{lin2021closer}
Baijiong Lin, Feiyang Ye, and Yu~Zhang.
\newblock A closer look at loss weighting in multi-task learning.
\newblock {\em arXiv preprint arXiv:2111.10603}, 2021.

\bibitem{liu2021towards}
Liyang Liu, Yi~Li, Zhanghui Kuang, J~Xue, Yimin Chen, Wenming Yang, Qingmin
  Liao, and Wayne Zhang.
\newblock Towards impartial multi-task learning.
\newblock ICLR, 2021.

\bibitem{liu2015deep}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 3730--3738, 2015.

\bibitem{musgrave2021unsupervised}
Kevin Musgrave, Serge Belongie, and Ser-Nam Lim.
\newblock Unsupervised domain adaptation: A reality check.
\newblock {\em arXiv preprint arXiv:2111.15672}, 2021.

\bibitem{papineni2002bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In {\em Proceedings of the 40th annual meeting of the Association for
  Computational Linguistics}, pages 311--318, 2002.

\bibitem{post2018call}
Matt Post.
\newblock A call for clarity in reporting bleu scores.
\newblock {\em arXiv preprint arXiv:1804.08771}, 2018.

\bibitem{sener2018multi}
Ozan Sener and Vladlen Koltun.
\newblock Multi-task learning as multi-objective optimization.
\newblock In {\em Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 525--536, 2018.

\bibitem{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em The journal of machine learning research}, 15(1):1929--1958,
  2014.

\bibitem{su2022comparison}
Xin Su, Yiyun Zhao, and Steven Bethard.
\newblock A comparison of strategies for source-free domain adaptation.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 8352--8367, 2022.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2020gradient}
Zirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao.
\newblock Gradient vaccine: Investigating and improving multi-task optimization
  in massively multilingual models.
\newblock {\em arXiv preprint arXiv:2010.05874}, 2020.

\bibitem{yu2020gradient}
Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and
  Chelsea Finn.
\newblock Gradient surgery for multi-task learning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:5824--5836, 2020.

\end{thebibliography}
