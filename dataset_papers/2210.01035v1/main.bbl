\begin{thebibliography}{10}

\bibitem{bao2021beit}
Hangbo Bao, Li~Dong, and Furu Wei.
\newblock Beit: Bert pre-training of image transformers.
\newblock {\em arXiv preprint arXiv:2106.08254}, 2021.

\bibitem{blalock2020state}
Davis Blalock, Jose~Javier Gonzalez~Ortiz, Jonathan Frankle, and John Guttag.
\newblock What is the state of neural network pruning?
\newblock {\em Proceedings of machine learning and systems}, 2:129--146, 2020.

\bibitem{bousselham2021efficient}
Walid Bousselham, Guillaume Thibault, Lucas Pagano, Archana Machireddy, Joe
  Gray, Young~Hwan Chang, and Xubo Song.
\newblock Efficient self-ensemble framework for semantic segmentation.
\newblock {\em arXiv preprint arXiv:2111.13280}, 2021.

\bibitem{carion20}
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
  Kirillov, and Sergey Zagoruyko.
\newblock End-to-end object detection with {T}ransformers.
\newblock In {\em ECCV}, 2020.

\bibitem{chavan2022vision}
Arnav Chavan, Zhiqiang Shen, Zhuang Liu, Zechun Liu, Kwang-Ting Cheng, and Eric
  Xing.
\newblock Vision transformer slimming: Multi-dimension searching in continuous
  optimization space.
\newblock {\em arXiv preprint arXiv:2201.00814}, 2022.

\bibitem{chen2021autoformer}
Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin Ling.
\newblock Autoformer: Searching transformers for visual recognition.
\newblock In {\em ICCV}, pages 12270--12280, 2021.

\bibitem{chen2021chasing}
Tianlong Chen, Yu~Cheng, Zhe Gan, Lu~Yuan, Lei Zhang, and Zhangyang Wang.
\newblock Chasing sparsity in vision transformers: An end-to-end exploration.
\newblock {\em NeurIPS}, 34, 2021.

\bibitem{chen2021simple}
Wuyang Chen, Xianzhi Du, Fan Yang, Lucas Beyer, Xiaohua Zhai, Tsung-Yi Lin,
  Huizhong Chen, Jing Li, Xiaodan Song, Zhangyang Wang, et~al.
\newblock A simple single-scale vision transformer for object localization and
  instance segmentation.
\newblock {\em arXiv preprint arXiv:2112.09747}, 2021.

\bibitem{chen2021auto}
Wuyang Chen, Wei Huang, Xianzhi Du, Xiaodan Song, Zhangyang Wang, and Denny
  Zhou.
\newblock Auto-scaling vision transformers without training.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{chen2021mobile}
Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu~Yuan,
  and Zicheng Liu.
\newblock Mobile-former: Bridging mobilenet and transformer.
\newblock {\em arXiv preprint arXiv:2108.05895}, 2021.

\bibitem{cheng2021mask2former}
Bowen Cheng, Ishan Misra, Alexander~G. Schwing, Alexander Kirillov, and Rohit
  Girdhar.
\newblock Masked-attention mask transformer for universal image segmentation.
\newblock 2022.

\bibitem{choromanski2020rethinking}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
  Lukasz Kaiser, et~al.
\newblock Rethinking attention with performers.
\newblock {\em arXiv preprint arXiv:2009.14794}, 2020.

\bibitem{cityscapes}
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,
  Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.
\newblock The {C}ityscapes dataset for semantic urban scene understanding.
\newblock In {\em CVPR}, 2016.

\bibitem{dai2021coatnet}
Zihang Dai, Hanxiao Liu, Quoc~V Le, and Mingxing Tan.
\newblock Coatnet: Marrying convolution and attention for all data sizes.
\newblock {\em NeurIPS}, 34:3965--3977, 2021.

\bibitem{daras2020smyrf}
Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros~G Dimakis.
\newblock Smyrf-efficient attention using asymmetric clustering.
\newblock {\em NeurIPS}, 33:6476--6489, 2020.

\bibitem{dingVLT}
Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang.
\newblock Vision-language transformer and query generation for referring
  segmentation.
\newblock In {\em ICCV}, pages 16321--16330, 2021.

\bibitem{ding2022davit}
Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong Wang, and Lu~Yuan.
\newblock Davit: Dual attention vision transformer.
\newblock {\em arXiv preprint arXiv:2204.03645}, 2022.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{d2021convit}
St{\'e}phane d’Ascoli, Hugo Touvron, Matthew~L Leavitt, Ari~S Morcos, Giulio
  Biroli, and Levent Sagun.
\newblock Convit: Improving vision transformers with soft convolutional
  inductive biases.
\newblock In {\em ICML}, pages 2286--2296. PMLR, 2021.

\bibitem{el2021xcit}
Alaaeldin El-Nouby, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs
  Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob
  Verbeek, et~al.
\newblock Xcit: Cross-covariance image transformers.
\newblock {\em arXiv preprint arXiv:2106.09681}, 2021.

\bibitem{fan2021multiscale}
Haoqi Fan, Bo~Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra
  Malik, and Christoph Feichtenhofer.
\newblock Multiscale vision transformers.
\newblock In {\em ICCV}, pages 6824--6835, 2021.

\bibitem{geiger2012we}
Andreas Geiger, Philip Lenz, and Raquel Urtasun.
\newblock Are we ready for autonomous driving? the kitti vision benchmark
  suite.
\newblock In {\em 2012 IEEE conference on computer vision and pattern
  recognition}, pages 3354--3361. IEEE, 2012.

\bibitem{gu2021hrvit}
Jiaqi Gu, Hyoukjun Kwon, Dilin Wang, Wei Ye, Meng Li, Yu-Hsin Chen, Liangzhen
  Lai, Vikas Chandra, and David~Z Pan.
\newblock Hrvit: Multi-scale high-resolution vision transformer.
\newblock {\em arXiv preprint arXiv:2111.01236}, 2021.

\bibitem{guibas2021efficient}
John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, and
  Bryan Catanzaro.
\newblock Efficient token mixing for transformers via adaptive fourier neural
  operators.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{han2020model}
Kai Han, Yunhe Wang, Qiulin Zhang, Wei Zhang, Chunjing Xu, and Tong Zhang.
\newblock Model rubik’s cube: Twisting resolution, depth and width for
  tinynets.
\newblock {\em NeurIPS}, 33:19353--19364, 2020.

\bibitem{he2022mlseg}
Haodi He, Yuhui Yuan, Xiangyu Yue, and Han Hu.
\newblock Mlseg: Image and video segmentation as multi-label classification and
  selected-label pixel classification.
\newblock {\em arXiv preprint arXiv:2203.04187}, 2022.

\bibitem{he2021masked}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross
  Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock {\em arXiv preprint arXiv:2111.06377}, 2021.

\bibitem{He2016ResNet}
Kaiming {He}, Xiangyu {Zhang}, Shaoqing {Ren}, and Jian {Sun}.
\newblock {Deep Residual Learning for Image Recognition}.
\newblock In {\em CVPR}, 2016.

\bibitem{heo2021rethinking}
Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and
  Seong~Joon Oh.
\newblock Rethinking spatial dimensions of vision transformers.
\newblock In {\em ICCV}, pages 11936--11945, 2021.

\bibitem{hoefler2021sparsity}
Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste.
\newblock Sparsity in deep learning: Pruning and growth for efficient inference
  and training in neural networks.
\newblock {\em JMLR}, 22(241):1--124, 2021.

\bibitem{hou2021multi}
Zejiang Hou and Sun-Yuan Kung.
\newblock Multi-dimensional model compression of vision transformer.
\newblock {\em arXiv preprint arXiv:2201.00043}, 2021.

\bibitem{jain2021semask}
Jitesh Jain, Anukriti Singh, Nikita Orlov, Zilong Huang, Jiachen Li, Steven
  Walton, and Humphrey Shi.
\newblock Semask: Semantically masked transformers for semantic segmentation.
\newblock {\em arXiv preprint arXiv:2112.12782}, 2021.

\bibitem{jampani2018superpixel}
Varun Jampani, Deqing Sun, Ming-Yu Liu, Ming-Hsuan Yang, and Jan Kautz.
\newblock Superpixel sampling networks.
\newblock In {\em ECCV}, pages 352--368, 2018.

\bibitem{jia2021scaling}
Chao Jia, Yinfei Yang, Ye~Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
  Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In {\em ICML}, pages 4904--4916. PMLR, 2021.

\bibitem{kahatapitiya2021swat}
Kumara Kahatapitiya and Michael~S Ryoo.
\newblock Swat: Spatial structure within and among tokens.
\newblock {\em arXiv preprint arXiv:2111.13677}, 2021.

\bibitem{kitaev2020reformer}
Nikita Kitaev, {\L}ukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock {\em arXiv preprint arXiv:2001.04451}, 2020.

\bibitem{kong2021spvit}
Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei Niu, Mengshu Sun, Bin
  Ren, Minghai Qin, Hao Tang, and Yanzhi Wang.
\newblock Spvit: Enabling faster vision transformers via soft token pruning.
\newblock {\em arXiv preprint arXiv:2112.13890}, 2021.

\bibitem{li2021grounded}
Liunian~Harold Li*, Pengchuan Zhang*, Haotian Zhang*, Jianwei Yang, Chunyuan
  Li, Yiwu Zhong, Lijuan Wang, Lu~Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei
  Chang, and Jianfeng Gao.
\newblock Grounded language-image pre-training.
\newblock In {\em CVPR}, 2022.

\bibitem{li2022exploring}
Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.
\newblock Exploring plain vision transformer backbones for object detection.
\newblock {\em arXiv preprint arXiv:2203.16527}, 2022.

\bibitem{li2021improved}
Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo~Xiong, Jitendra
  Malik, and Christoph Feichtenhofer.
\newblock Improved multiscale vision transformers for classification and
  detection.
\newblock {\em arXiv preprint arXiv:2112.01526}, 2021.

\bibitem{li2021benchmarking}
Yanghao Li, Saining Xie, Xinlei Chen, Piotr Dollar, Kaiming He, and Ross
  Girshick.
\newblock Benchmarking detection transfer learning with vision transformers.
\newblock {\em arXiv preprint arXiv:2111.11429}, 2021.

\bibitem{li2022depthformer}
Zhenyu Li, Zehui Chen, Xianming Liu, and Junjun Jiang.
\newblock Depthformer: Depthformer: Exploiting long-range correlation and local
  information for accurate monocular depth estimation.
\newblock {\em arXiv preprint arXiv:2203.14211}, 2022.

\bibitem{liang2021swinir}
Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van~Gool, and Radu
  Timofte.
\newblock Swinir: Image restoration using swin transformer.
\newblock {\em arXiv preprint arXiv:2108.10257}, 2021.

\bibitem{liang2021evit}
Youwei Liang, GE~Chongjian, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie.
\newblock Evit: Expediting vision transformers via token reorganizations.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em ECCV}, pages 740--755. Springer, 2014.

\bibitem{liu2021swinv2}
Ze~Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue
  Cao, Zheng Zhang, Li~Dong, et~al.
\newblock Swin transformer v2: Scaling up capacity and resolution.
\newblock {\em arXiv preprint arXiv:2111.09883}, 2021.

\bibitem{liu2021swinv1}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In {\em ICCV}, pages 10012--10022, 2021.

\bibitem{liu2017learning}
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui
  Zhang.
\newblock Learning efficient convolutional networks through network slimming.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 2736--2744, 2017.

\bibitem{liu2018rethinking}
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell.
\newblock Rethinking the value of network pruning.
\newblock {\em arXiv preprint arXiv:1810.05270}, 2018.

\bibitem{marin2021token}
Dmitrii Marin, Jen-Hao~Rick Chang, Anurag Ranjan, Anish Prabhu, Mohammad
  Rastegari, and Oncel Tuzel.
\newblock Token pooling in vision transformers.
\newblock {\em arXiv preprint arXiv:2110.03860}, 2021.

\bibitem{mehta2021mobilevit}
Sachin Mehta and Mohammad Rastegari.
\newblock Mobilevit: light-weight, general-purpose, and mobile-friendly vision
  transformer.
\newblock {\em arXiv preprint arXiv:2110.02178}, 2021.

\bibitem{pascal_context}
Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam{-}Gyu Cho, Seong{-}Whan Lee,
  Sanja Fidler, Raquel Urtasun, and Alan~L. Yuille.
\newblock The role of context for object detection and semantic segmentation in
  the wild.
\newblock In {\em CVPR}, 2014.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em ICML}, pages 8748--8763. PMLR, 2021.

\bibitem{ranftl2021vision}
Ren{\'e} Ranftl, Alexey Bochkovskiy, and Vladlen Koltun.
\newblock Vision transformers for dense prediction.
\newblock In {\em ICCV}, pages 12179--12188, 2021.

\bibitem{rao2021dynamicvit}
Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh.
\newblock Dynamicvit: Efficient vision transformers with dynamic token
  sparsification.
\newblock {\em arXiv preprint arXiv:2106.02034}, 2021.

\bibitem{renggli2022learning}
Cedric Renggli, Andr{\'e}~Susano Pinto, Neil Houlsby, Basil Mustafa, Joan
  Puigcerver, and Carlos Riquelme.
\newblock Learning to merge tokens in vision transformers.
\newblock {\em arXiv preprint arXiv:2202.12015}, 2022.

\bibitem{roy2021efficient}
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier.
\newblock Efficient content-based sparse attention with routing transformers.
\newblock {\em ACL}, 9:53--68, 2021.

\bibitem{ryoo2021tokenlearner}
Michael Ryoo, AJ~Piergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia
  Angelova.
\newblock Tokenlearner: Adaptive space-time tokenization for videos.
\newblock {\em NeurIPS}, 34, 2021.

\bibitem{silberman2012indoor}
Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus.
\newblock Indoor segmentation and support inference from rgbd images.
\newblock In {\em ECCV}, pages 746--760. Springer, 2012.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{singh2022revisiting}
Mannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de~Freitas Reis, Bugra
  Gedik, Raj~Prateek Kosaraju, Dhruv Mahajan, Ross Girshick, Piotr Doll{\'a}r,
  and Laurens van~der Maaten.
\newblock Revisiting weakly supervised pre-training of visual perception
  models, 2022.

\bibitem{song2021dynamic}
Lin Song, Songyang Zhang, Songtao Liu, Zeming Li, Xuming He, Hongbin Sun, Jian
  Sun, and Nanning Zheng.
\newblock Dynamic grained encoder for vision transformers.
\newblock {\em NeurIPS}, 34, 2021.

\bibitem{Segmenter}
Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid.
\newblock Segmenter: Transformer for semantic segmentation.
\newblock {\em arXiv preprint arXiv:2105.05633}, 2021.

\bibitem{szegedy2015going}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
  Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
\newblock Going deeper with convolutions.
\newblock In {\em CVPR}, pages 1--9, 2015.

\bibitem{tan2019efficientnet}
Mingxing Tan and Quoc Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In {\em ICML}, pages 6105--6114. PMLR, 2019.

\bibitem{tay2020efficient}
Yi~Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
\newblock Efficient transformers: A survey.
\newblock {\em arXiv preprint arXiv:2009.06732}, 2020.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em NeurIPS}, 30, 2017.

\bibitem{vyas2020fast}
Apoorv Vyas, Angelos Katharopoulos, and Fran{\c{c}}ois Fleuret.
\newblock Fast transformers with clustered attention.
\newblock {\em NeurIPS}, 33, 2020.

\bibitem{wang2021pnp}
Tao Wang, Li~Yuan, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan.
\newblock Pnp-detr: towards efficient visual analysis with transformers.
\newblock In {\em ICCV}, pages 4661--4670, 2021.

\bibitem{wang2021pyramid}
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong
  Lu, Ping Luo, and Ling Shao.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock In {\em ICCV}, pages 568--578, 2021.

\bibitem{wang2021accelerate}
Wenxiao Wang, Minghao Chen, Shuai Zhao, Long Chen, Jinming Hu, Haifeng Liu,
  Deng Cai, Xiaofei He, and Wei Liu.
\newblock Accelerate cnns from three dimensions: A comprehensive pruning
  framework.
\newblock In {\em ICML}, pages 10717--10726. PMLR, 2021.

\bibitem{wang2019dbp}
Wenxiao Wang, Shuai Zhao, Minghao Chen, Jinming Hu, Deng Cai, and Haifeng Liu.
\newblock Dbp: discrimination based block-level pruning for deep model
  acceleration.
\newblock {\em arXiv preprint arXiv:1912.10178}, 2019.

\bibitem{wang2021not}
Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, and Gao Huang.
\newblock Not all images are worth 16x16 words: Dynamic transformers for
  efficient image recognition.
\newblock {\em NeurIPS}, 34, 2021.

\bibitem{wortsman2022model}
Mitchell Wortsman, Gabriel Ilharco, Samir~Yitzhak Gadre, Rebecca Roelofs,
  Raphael Gontijo-Lopes, Ari~S Morcos, Hongseok Namkoong, Ali Farhadi, Yair
  Carmon, Simon Kornblith, et~al.
\newblock Model soups: averaging weights of multiple fine-tuned models improves
  accuracy without increasing inference time.
\newblock {\em arXiv preprint arXiv:2203.05482}, 2022.

\bibitem{xie2021segformer}
Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose~M Alvarez, and Ping
  Luo.
\newblock Segformer: Simple and efficient design for semantic segmentation with
  transformers.
\newblock {\em NeurIPS}, 34, 2021.

\bibitem{xie2021simmim}
Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao,
  Qi~Dai, and Han Hu.
\newblock Simmim: A simple framework for masked image modeling.
\newblock {\em arXiv preprint arXiv:2111.09886}, 2021.

\bibitem{xu2022vitpose}
Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao.
\newblock Vitpose: Simple vision transformer baselines for human pose
  estimation.
\newblock {\em arXiv preprint arXiv:2204.12484}, 2022.

\bibitem{yang2021focal}
Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu~Yuan, and
  Jianfeng Gao.
\newblock Focal self-attention for local-global interactions in vision
  transformers.
\newblock {\em arXiv preprint arXiv:2107.00641}, 2021.

\bibitem{yu2018slimmable}
Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and Thomas Huang.
\newblock Slimmable neural networks.
\newblock {\em arXiv preprint arXiv:1812.08928}, 2018.

\bibitem{yuan2020object}
Yuhui Yuan, Xilin Chen, and Jingdong Wang.
\newblock Object-contextual representations for semantic segmentation.
\newblock In {\em ECCV}, pages 173--190. Springer, 2020.

\bibitem{yuan2021hrformer}
Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang, Xilin Chen, and
  Jingdong Wang.
\newblock Hrformer: High-resolution transformer for dense prediction.
\newblock {\em arXiv preprint arXiv:2110.09408}, 2021.

\bibitem{yuan2018ocnet}
Yuhui Yuan, Lang Huang, Jianyuan Guo, Chao Zhang, Xilin Chen, and Jingdong
  Wang.
\newblock Ocnet: Object context network for scene parsing.
\newblock {\em arXiv preprint arXiv:1809.00916}, 2018.

\bibitem{yuan2020segfix}
Yuhui Yuan, Jingyi Xie, Xilin Chen, and Jingdong Wang.
\newblock Segfix: Model-agnostic boundary refinement for segmentation.
\newblock In {\em ECCV}. Springer, 2020.

\bibitem{zhai2022lscalevit}
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
\newblock Scaling vision transformers.
\newblock {\em CVPR}, 2022.

\bibitem{zhang2021styleswin}
Bowen Zhang, Shuyang Gu, Bo~Zhang, Jianmin Bao, Dong Chen, Fang Wen, Yong Wang,
  and Baining Guo.
\newblock Styleswin: Transformer-based gan for high-resolution image
  generation.
\newblock {\em arXiv preprint arXiv:2112.10762}, 2021.

\bibitem{zhang2022dino}
Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel~M Ni, and
  Heung-Yeung Shum.
\newblock Dino: Detr with improved denoising anchor boxes for end-to-end object
  detection.
\newblock {\em arXiv preprint arXiv:2203.03605}, 2022.

\bibitem{zhang2022edgeformer}
Haokui Zhang, Wenze Hu, and Xiaoyu Wang.
\newblock Edgeformer: Improving light-weight convnets by learning from vision
  transformers.
\newblock {\em arXiv preprint arXiv:2203.03952}, 2022.

\bibitem{zhang2021multi}
Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu~Yuan, Lei Zhang, and
  Jianfeng Gao.
\newblock Multi-scale vision longformer: A new vision transformer for
  high-resolution image encoding.
\newblock In {\em ICCV}, pages 2998--3008, 2021.

\bibitem{zheng2020end}
Minghang Zheng, Peng Gao, Renrui Zhang, Kunchang Li, Xiaogang Wang, Hongsheng
  Li, and Hao Dong.
\newblock End-to-end object detection with adaptive clustering transformer.
\newblock {\em arXiv preprint arXiv:2011.09315}, 2020.

\bibitem{ade20k}
Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso,
  and Antonio Torralba.
\newblock Semantic understanding of scenes through the {ADE20K} dataset.
\newblock {\em IJCV}, 2019.

\end{thebibliography}
