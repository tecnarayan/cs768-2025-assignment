\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ailon \& Chazelle(2009)Ailon and Chazelle]{2009_Ailon_Fast}
Ailon, N. and Chazelle, B.
\newblock The fast johnson–lindenstrauss transform and approximate nearest
  neighbors.
\newblock \emph{SIAM Journal on Computing}, 39\penalty0 (1):\penalty0 302--322,
  2009.

\bibitem[Ailon \& Liberty(2008)Ailon and Liberty]{2008_Ailon_Fast}
Ailon, N. and Liberty, E.
\newblock Fast dimension reduction using rademacher series on dual bch codes.
\newblock \emph{Discrete \& Computational Geometry}, 42\penalty0 (4):\penalty0
  615--630, 2008.

\bibitem[Allen-Zhu(2016)]{allen2016katyusha}
Allen-Zhu, Z.
\newblock Katyusha: The first direct acceleration of stochastic gradient
  methods.
\newblock \emph{arXiv preprint arXiv:1603.05953}, 2016.

\bibitem[Baraniuk et~al.(2008)Baraniuk, Davenport, DeVore, and
  Wakin]{baraniuk2008simple}
Baraniuk, R., Davenport, M., DeVore, R., and Wakin, M.
\newblock A simple proof of the restricted isometry property for random
  matrices.
\newblock \emph{Constructive Approximation}, 28\penalty0 (3):\penalty0
  253--263, 2008.

\bibitem[Beck \& Teboulle(2009)Beck and Teboulle]{2009_Beck_Fast}
Beck, A. and Teboulle, M.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock \emph{SIAM Journal on Imaging Sciences}, 2\penalty0 (1):\penalty0
  183--202, 2009.

\bibitem[Candes et~al.(2006)Candes, Romberg, and Tao]{candes2006stable}
Candes, E., Romberg, J., and Tao, T.
\newblock Stable signal recovery from incomplete and inaccurate measurements.
\newblock \emph{Communications on pure and applied mathematics}, 59\penalty0
  (8):\penalty0 1207--1223, 2006.

\bibitem[Chandrasekaran \& Jordan(2013)Chandrasekaran and
  Jordan]{chandrasekaran2013computational}
Chandrasekaran, V. and Jordan, M.~I.
\newblock Computational and statistical tradeoffs via convex relaxation.
\newblock \emph{Proceedings of the National Academy of Sciences}, 110\penalty0
  (13):\penalty0 E1181--E1190, 2013.

\bibitem[Chandrasekaran et~al.(2012)Chandrasekaran, Recht, Parrilo, and
  Willsky]{2012_Chandrasekaran_Convex}
Chandrasekaran, V., Recht, B., Parrilo, P.~A., and Willsky, A.~S.
\newblock The convex geometry of linear inverse problems.
\newblock \emph{Foundations of Computational mathematics}, 12\penalty0
  (6):\penalty0 805--849, 2012.

\bibitem[Clarkson \& Woodruff(2013)Clarkson and Woodruff]{clarkson2013low}
Clarkson, K.~L. and Woodruff, D.~P.
\newblock Low rank approximation and regression in input sparsity time.
\newblock In \emph{Proceedings of the forty-fifth annual ACM symposium on
  Theory of computing}, pp.\  81--90. ACM, 2013.

\bibitem[Dasgupta \& Gupta(2003)Dasgupta and Gupta]{dasgupta2003elementary}
Dasgupta, S. and Gupta, A.
\newblock An elementary proof of a theorem of johnson and lindenstrauss.
\newblock \emph{Random Structures \& Algorithms}, 22\penalty0 (1):\penalty0
  60--65, 2003.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Defazio, A., Bach, F., and Lacoste-Julien, S.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1646--1654, 2014.

\bibitem[Donoho(2006)]{2006_Donoho_Compressed}
Donoho, D.~L.
\newblock Compressed sensing.
\newblock \emph{Information Theory, IEEE Transactions on}, 52\penalty0
  (4):\penalty0 1289--1306, 2006.

\bibitem[Drineas et~al.(2011)Drineas, Mahoney, Muthukrishnan, and
  Sarl{\'o}s]{drineas2011faster}
Drineas, P., Mahoney, M.~W., Muthukrishnan, S., and Sarl{\'o}s, T.
\newblock Faster least squares approximation.
\newblock \emph{Numerische Mathematik}, 117\penalty0 (2):\penalty0 219--249,
  2011.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{2013_Johnson_Accelerating}
Johnson, R. and Zhang, T.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems 26}, pp.\
  315--323. Curran Associates, Inc., 2013.

\bibitem[Johnson \& Lindenstrauss(1984)Johnson and
  Lindenstrauss]{johnson1984extensions}
Johnson, W.~B. and Lindenstrauss, J.
\newblock Extensions of lipschitz mappings into a hilbert space.
\newblock \emph{Contemporary mathematics}, 26\penalty0 (189-206):\penalty0 1,
  1984.

\bibitem[Kapralov et~al.(2016)Kapralov, Potluru, and
  Woodruff]{kapralov2016fake}
Kapralov, M., Potluru, V.~K., and Woodruff, D.~P.
\newblock How to fake multiply by a gaussian matrix.
\newblock \emph{arXiv preprint arXiv:1606.05732}, 2016.

\bibitem[Kone{\v{c}}n{\`y} \& Richt{\'a}rik(2013)Kone{\v{c}}n{\`y} and
  Richt{\'a}rik]{2013_Konecny_Semi-stochastic}
Kone{\v{c}}n{\`y}, J. and Richt{\'a}rik, P.
\newblock Semi-stochastic gradient descent methods.
\newblock \emph{arXiv preprint arXiv:1312.1666}, 2013.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2016)Kone{\v{c}}n{\`y}, Liu, Richt{\'a}rik,
  and Tak{\'a}{\v{c}}]{konevcny2016mini}
Kone{\v{c}}n{\`y}, J., Liu, J., Richt{\'a}rik, P., and Tak{\'a}{\v{c}}, M.
\newblock Mini-batch semi-stochastic gradient descent in the proximal setting.
\newblock \emph{IEEE Journal of Selected Topics in Signal Processing},
  10\penalty0 (2):\penalty0 242--255, 2016.

\bibitem[Langford et~al.(2009)Langford, Li, and Zhang]{langford2009sparse}
Langford, J., Li, L., and Zhang, T.
\newblock Sparse online learning via truncated gradient.
\newblock \emph{Journal of Machine Learning Research}, 10\penalty0
  (Mar):\penalty0 777--801, 2009.

\bibitem[Lichman(2013)]{Lichman:2013}
Lichman, M.
\newblock {UCI} machine learning repository, 2013.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Mahoney(2011)]{mahoney2011randomized}
Mahoney, M.~W.
\newblock Randomized algorithms for matrices and data.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  3\penalty0 (2):\penalty0 123--224, 2011.

\bibitem[Nesterov(2007)]{nesterov2007gradient}
Nesterov, Y.
\newblock Gradient methods for minimizing composite objective function.
\newblock Technical report, UCL, 2007.

\bibitem[Nesterov(2013{\natexlab{a}})]{nesterov2013gradient}
Nesterov, Y.
\newblock Gradient methods for minimizing composite functions.
\newblock \emph{Mathematical Programming}, 140\penalty0 (1):\penalty0 125--161,
  2013{\natexlab{a}}.

\bibitem[Nesterov(2013{\natexlab{b}})]{nesterov2013introductory}
Nesterov, Y.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2013{\natexlab{b}}.

\bibitem[O'Donoghue \& Candes(2015)O'Donoghue and Candes]{o2015adaptive}
O'Donoghue, B. and Candes, E.
\newblock Adaptive restart for accelerated gradient schemes.
\newblock \emph{Foundations of computational mathematics}, 15\penalty0
  (3):\penalty0 715--732, 2015.

\bibitem[Oymak et~al.(2015)Oymak, Recht, and Soltanolkotabi]{2015_Oymak_Sharp}
Oymak, S., Recht, B., and Soltanolkotabi, M.
\newblock Sharp time--data tradeoffs for linear inverse problems.
\newblock \emph{arXiv preprint arXiv:1507.04793}, 2015.

\bibitem[Pilanci \& Wainwright(2015)Pilanci and
  Wainwright]{2015_Pilanci_Randomized}
Pilanci, M. and Wainwright, M.~J.
\newblock Randomized sketches of convex programs with sharp guarantees.
\newblock \emph{Information Theory, IEEE Transactions on}, 61\penalty0
  (9):\penalty0 5096--5115, 2015.

\bibitem[Pilanci \& Wainwright(2016)Pilanci and
  Wainwright]{2016_Pilanci_Iterative}
Pilanci, M. and Wainwright, M.~J.
\newblock Iterative hessian sketch: Fast and accurate solution approximation
  for constrained least-squares.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (53):\penalty0 1--38, 2016.

\bibitem[Schmidt et~al.(2013)Schmidt, Le~Roux, and Bach]{schmidt2013minimizing}
Schmidt, M., Le~Roux, N., and Bach, F.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, pp.\  1--30, 2013.

\bibitem[Shalev-Shwartz \& Tewari(2011)Shalev-Shwartz and
  Tewari]{shalev2011stochastic}
Shalev-Shwartz, S. and Tewari, A.
\newblock Stochastic methods for l1-regularized loss minimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jun):\penalty0 1865--1892, 2011.

\bibitem[Van Den~Berg \& Friedlander(2007)Van Den~Berg and
  Friedlander]{van2007spgl1}
Van Den~Berg, E. and Friedlander, M.~P.
\newblock Spgl1: A solver for large-scale sparse reconstruction, 2007.

\bibitem[Wang(2015)]{wang2015practical}
Wang, S.
\newblock A practical guide to randomized matrix computations with matlab
  implementations.
\newblock \emph{arXiv preprint arXiv:1505.07570}, 2015.

\bibitem[Xiao \& Zhang(2014)Xiao and Zhang]{xiao2014proximal}
Xiao, L. and Zhang, T.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock \emph{SIAM Journal on Optimization}, 24\penalty0 (4):\penalty0
  2057--2075, 2014.

\end{thebibliography}
