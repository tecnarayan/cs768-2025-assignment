\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ali and Silvey(1966)]{ali1966general}
Syed~Mumtaz Ali and Samuel~D Silvey.
\newblock A general class of coefficients of divergence of one distribution
  from another.
\newblock \emph{Journal of the Royal Statistical Society. Series B
  (Methodological)}, pages 131--142, 1966.

\bibitem[Amari and Nagaoka(2000)]{anMO}
S.-I. Amari and H.~Nagaoka.
\newblock \emph{Methods of Information Geometry}.
\newblock Oxford University Press, 2000.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{arjovsky2017wasserstein}
Martin Arjovsky, Soumith Chintala, and L{\'e}on Bottou.
\newblock Wasserstein gan.
\newblock \emph{arXiv preprint arXiv:1701.07875}, 2017.

\bibitem[Banerjee et~al.(2005)Banerjee, Merugu, Dhillon, and Ghosh]{bmdgCW}
A.~Banerjee, S.~Merugu, I.~Dhillon, and J.~Ghosh.
\newblock Clustering with {B}regman divergences.
\newblock \emph{JMLR}, 6:\penalty0 1705--1749, 2005.

\bibitem[Barndorff-Nielsen(1978)]{bIA}
O.~Barndorff-Nielsen.
\newblock \emph{Information and Exponential Families in Statistical Theory}.
\newblock Wiley Publishers, 1978.

\bibitem[Bartlett and Traskin(2006)]{btAI}
P.~Bartlett and M.~Traskin.
\newblock Adaboost is consistent.
\newblock In \emph{NIPS*19}, 2006.

\bibitem[Csisz{\'a}r(1967)]{csiszar1967information}
Imre Csisz{\'a}r.
\newblock Information--type measures of difference of probability
  distributions.
\newblock \emph{Studia Scientiarum Mathematicarum Hungarica}, 17:\penalty0
  123--149, 1967.

\bibitem[Dud{\'{\i}}k et~al.(2004)Dud{\'{\i}}k, Phillips, and Schapire]{dpsPG}
M.~Dud{\'{\i}}k, S.-J. Phillips, and R.-E. Schapire.
\newblock Performance guarantees for regularized maximum entropy density
  estimation.
\newblock In \emph{COLT}, 2004.

\bibitem[Freund and Schapire(1997)]{fsAD}
Y.~Freund and R.~E. Schapire.
\newblock A {D}ecision-{T}heoretic generalization of on-line learning and an
  application to {B}oosting.
\newblock \emph{JCSS}, 55:\penalty0 119--139, 1997.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in neural information processing systems}, pages
  2672--2680, 2014.

\bibitem[Grover and Ermon(2018)]{grover2017boosted}
Aditya Grover and Stefano Ermon.
\newblock Boosted generative models.
\newblock In \emph{AAAI}, 2018.

\bibitem[Guo et~al.(2016)Guo, Wang, Fan, Broderick, and Dunson]{gwfbdBV}
F.~Guo, X.~Wang, K.~Fan, T.~Broderick, and D.-B. Dunson.
\newblock Boosting variational inference.
\newblock \emph{CoRR}, abs/1611.05559, 2016.

\bibitem[Innes(2018)]{innes:2018}
Mike Innes.
\newblock Flux: Elegant machine learning with julia.
\newblock \emph{Journal of Open Source Software}, 2018.
\newblock \doi{10.21105/joss.00602}.

\bibitem[Kearns(1988)]{kTO}
Michael Kearns.
\newblock Thoughts on hypothesis boosting.
\newblock \emph{Unpublished manuscript}, 45:\penalty0 105, 1988.

\bibitem[Khan et~al.(2016)Khan, Babanezhad, Lin, Schmidt, and
  Sugiyama]{kblssFS}
M.-E. Khan, R.~Babanezhad, W.~Lin, M.-W. Schmidt, and M.~Sugiyama.
\newblock Faster stochastic variational inference using proximal-gradient
  methods with general divergence functions.
\newblock In \emph{UAI}, 2016.

\bibitem[Li and Barron(2000)]{lbMD}
Jonathan~Q Li and Andrew~R Barron.
\newblock Mixture density estimation.
\newblock In \emph{Advances in neural information processing systems}, pages
  279--285, 2000.

\bibitem[Locatello et~al.(2017)Locatello, Khanna, Ghosh, and
  R{\"{a}}tsch]{lkgrBV}
F.~Locatello, R.~Khanna, J.~Ghosh, and G.~R{\"{a}}tsch.
\newblock Boosting variational inference: an optimization perspective.
\newblock \emph{CoRR}, abs/1708.01733, 2017.

\bibitem[McDiarmid(1998)]{mdC}
C.~McDiarmid.
\newblock Concentration.
\newblock In M.~Habib, C.~McDiarmid, J.~Ramirez-Alfonsin, and B.~Reed, editors,
  \emph{Probabilistic Methods for Algorithmic Discrete Mathematics}, pages
  1--54. Springer Verlag, 1998.

\bibitem[Miller et~al.(2017)Miller, Foti, and Adams]{mfaVB}
A.-C. Miller, N.-J. Foti, and R.-P. Adams.
\newblock Variational boosting: Iteratively refining posterior approximations.
\newblock In \emph{ICML}, pages 2420--2429, 2017.

\bibitem[Naito and Eguchi(2013)]{neDE}
K.~Naito and S.~Eguchi.
\newblock Density estimation with minimization of {U}-divergence.
\newblock \emph{Machine Learning}, 90\penalty0 (1):\penalty0 29--57, 2013.

\bibitem[Nguyen et~al.(2010)Nguyen, Wainwright, and
  Jordan]{nguyen2010estimating}
XuanLong Nguyen, Martin~J Wainwright, and Michael~I Jordan.
\newblock Estimating divergence functionals and the likelihood ratio by convex
  risk minimization.
\newblock \emph{IEEE Transactions on Information Theory}, 56\penalty0
  (11):\penalty0 5847--5861, 2010.

\bibitem[Nock and Nielsen(2007)]{nnAR}
Richard Nock and Frank Nielsen.
\newblock {A $\mathbb{R}$eal Generalization of discrete AdaBoost}.
\newblock \emph{Artificial Intelligence}, 171:\penalty0 25--41, 2007.

\bibitem[Nock and Nielsen(2008)]{nnOT}
Richard Nock and Frank Nielsen.
\newblock On the efficient minimization of classification-calibrated
  surrogates.
\newblock In \emph{Advances in neural information processing systems}, pages
  1201--1208, 2008.

\bibitem[Nock et~al.(2017)Nock, Cranko, Menon, Qu, and Williamson]{nock2017f}
Richard Nock, Zac Cranko, Aditya~K Menon, Lizhen Qu, and Robert~C Williamson.
\newblock {$f$}-gans in an information geometric nutshell.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  456--464, 2017.

\bibitem[Nowozin et~al.(2016)Nowozin, Cseke, and Tomioka]{nowozin2016f}
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka.
\newblock {$f$}-gan: Training generative neural samplers using variational
  divergence minimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  271--279, 2016.

\bibitem[Penot(2012)]{penot2012calculus}
Jean-Paul Penot.
\newblock \emph{Calculus without derivatives}, volume 266.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Reid and Williamson(2010)]{rwCB}
M.-D. Reid and R.-C. Williamson.
\newblock Composite binary losses.
\newblock \emph{JMLR}, 11:\penalty0 2387--2422, 2010.

\bibitem[Reid and Williamson(2011)]{reid2011information}
Mark~D Reid and Robert~C Williamson.
\newblock Information, divergence and risk for binary experiments.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Mar):\penalty0 731--817, 2011.

\bibitem[Rosset and Segal(2002)]{rsBD}
S.~Rosset and E.~Segal.
\newblock Boosting density estimation.
\newblock In \emph{NIPS}, pages 641--648, 2002.

\bibitem[Schapire(1990)]{sTS}
R.~E. Schapire.
\newblock The strength of weak learnability.
\newblock \emph{Machine Learning}, pages 197--227, 1990.

\bibitem[Schapire and Singer(1999)]{ssIBj}
R.~E. Schapire and Y.~Singer.
\newblock Improved boosting algorithms using confidence-rated predictions.
\newblock \emph{Machine Learning}, 37:\penalty0 297--336, 1999.

\bibitem[Simi{\'c}(2009{\natexlab{a}})]{sOAN}
S.~Simi{\'c}.
\newblock On a new converse of {J}ensen's inequality.
\newblock \emph{Publications de l'Institut Math{\'e}matique}, 85:\penalty0
  107--110, 2009{\natexlab{a}}.

\bibitem[Simi{\'c}(2009{\natexlab{b}})]{sOAU}
S.~Simi{\'c}.
\newblock On an upperbound for {J}ensen's inequality.
\newblock \emph{Journal of Inequalities in Pure and Applied Mathematics}, 10,
  2009{\natexlab{b}}.

\bibitem[Tolstikhin et~al.(2017)Tolstikhin, Gelly, Bousquet, Simon{-}Gabriel,
  and Sch{\"{o}}lkopf]{tgbssAB}
I.-O. Tolstikhin, S.~Gelly, O.~Bousquet, C.{-}J. Simon{-}Gabriel, and
  B.~Sch{\"{o}}lkopf.
\newblock Adagan: Boosting generative models.
\newblock In \emph{NIPS}, pages 5430--5439, 2017.

\bibitem[Zhang(2003)]{zSG}
T.~Zhang.
\newblock Sequential greedy approximation for certain convex optimization
  problems.
\newblock \emph{IEEE Trans. IT}, 49:\penalty0 682--691, 2003.

\end{thebibliography}
