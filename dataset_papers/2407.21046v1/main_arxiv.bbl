\begin{thebibliography}{91}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anari et~al.(2023)Anari, Huang, Liu, Vuong, Xu, and Yu]{anari2023parallel}
Nima Anari, Yizhi Huang, Tianyu Liu, Thuy-Duong Vuong, Brian Xu, and Katherine Yu.
\newblock Parallel discrete sampling via continuous walks.
\newblock In \emph{Proceedings of the 55th Annual ACM Symposium on Theory of Computing}, STOC 2023, pp.\  103–116, New York, NY, USA, 2023. Association for Computing Machinery.
\newblock ISBN 9781450399135.
\newblock \doi{10.1145/3564246.3585207}.
\newblock URL \url{https://doi.org/10.1145/3564246.3585207}.

\bibitem[Austin et~al.(2021)Austin, Johnson, Ho, Tarlow, and van~den Berg]{austin2021structured}
Jacob Austin, Daniel~D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van~den Berg.
\newblock Structured denoising diffusion models in discrete state-spaces.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=h7-XixPCAL}.

\bibitem[Bachmann \& Nagarajan(2024)Bachmann and Nagarajan]{bachmann2024pitfalls}
Gregor Bachmann and Vaishnavh Nagarajan.
\newblock The pitfalls of next-token prediction, 2024.

\bibitem[Besag(1975)]{besag1975statistical}
Julian Besag.
\newblock Statistical analysis of non-lattice data.
\newblock \emph{Journal of the Royal Statistical Society Series D: The Statistician}, 24\penalty0 (3):\penalty0 179--195, 1975.

\bibitem[Bobkov \& Tetali(2006)Bobkov and Tetali]{bobkov2006modified}
Sergey~G Bobkov and Prasad Tetali.
\newblock Modified logarithmic sobolev inequalities in discrete settings.
\newblock \emph{Journal of Theoretical Probability}, 19\penalty0 (2):\penalty0 289--336, 2006.

\bibitem[Bojar et~al.(2016)Bojar, Chatterjee, Federmann, Graham, Haddow, Huck, Jimeno~Yepes, Koehn, Logacheva, Monz, Negri, Neveol, Neves, Popel, Post, Rubino, Scarton, Specia, Turchi, Verspoor, and Zampieri]{wmt16}
Ond~{r}ej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno~Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aurelie Neveol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Marcos Zampieri.
\newblock Findings of the 2016 conference on machine translation.
\newblock In \emph{Proceedings of the First Conference on Machine Translation}, pp.\  131--198, Berlin, Germany, August 2016. Association for Computational Linguistics.
\newblock URL \url{http://www.aclweb.org/anthology/W/W16/W16-2301}.

\bibitem[Bojar et~al.(2014)Bojar, Buck, Federmann, Haddow, Koehn, Leveling, Monz, Pecina, Post, Saint-Amand, Soricut, Specia, and Tamchyna]{wmt14}
Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale~{s} Tamchyna.
\newblock Findings of the 2014 workshop on statistical machine translation.
\newblock In \emph{Proceedings of the Ninth Workshop on Statistical Machine Translation}, pp.\  12--58, Baltimore, Maryland, USA, June 2014. Association for Computational Linguistics.
\newblock URL \url{http://www.aclweb.org/anthology/W/W14/W14-3302}.

\bibitem[Bojar et~al.(2017)Bojar, Chatterjee, Federmann, Graham, Haddow, Huang, Huck, Koehn, Liu, Logacheva, Monz, Negri, Post, Rubino, Specia, and Turchi]{wmt17}
Ond{\v{r}}ej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi.
\newblock Findings of the 2017 conference on machine translation ({WMT}17).
\newblock In Ond{\v{r}}ej Bojar, Christian Buck, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio~Jimeno Yepes, Philipp Koehn, and Julia Kreutzer (eds.), \emph{Proceedings of the Second Conference on Machine Translation}, pp.\  169--214, Copenhagen, Denmark, September 2017. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/W17-4717}.
\newblock URL \url{https://aclanthology.org/W17-4717}.

\bibitem[Bosc \& Vincent(2020)Bosc and Vincent]{bosc2020sequence}
Tom Bosc and Pascal Vincent.
\newblock Do sequence-to-sequence {VAE}s learn global features of sentences?
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pp.\  4296--4318, Online, November 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.350}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.350}.

\bibitem[Bowman et~al.(2016)Bowman, Vilnis, Vinyals, Dai, Jozefowicz, and Bengio]{bowman2016generating}
Samuel~R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, and Samy Bengio.
\newblock Generating sentences from a continuous space.
\newblock In \emph{Proceedings of the 20th {SIGNLL} Conference on Computational Natural Language Learning}, pp.\  10--21, Berlin, Germany, August 2016. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/K16-1002}.
\newblock URL \url{https://aclanthology.org/K16-1002}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  1877--1901. Curran Associates, Inc., 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Cai et~al.(2024)Cai, Li, Geng, Peng, Lee, Chen, and Dao]{cai2024medusa}
Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason~D. Lee, Deming Chen, and Tri Dao.
\newblock Medusa: Simple llm inference acceleration framework with multiple decoding heads, 2024.

\bibitem[Caputo \& Parisi(2021)Caputo and Parisi]{caputo2021block}
Pietro Caputo and Daniel Parisi.
\newblock Block factorization of the relative entropy via spatial mixing.
\newblock \emph{Communications in Mathematical Physics}, 388\penalty0 (2):\penalty0 793--818, 2021.

\bibitem[Caputo et~al.(2015)Caputo, Menz, and Tetali]{caputo2015approximate}
Pietro Caputo, Georg Menz, and Prasad Tetali.
\newblock Approximate tensorization of entropy at high temperature.
\newblock In \emph{Annales de la Facult{\'e} des sciences de Toulouse: Math{\'e}matiques}, volume~24, pp.\  691--716, 2015.

\bibitem[Chan et~al.(2020)Chan, Saharia, Hinton, Norouzi, and Jaitly]{chan2020imputer}
William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, and Navdeep Jaitly.
\newblock Imputer: Sequence modelling via imputation and dynamic programming.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1403--1413. PMLR, 2020.

\bibitem[Che et~al.(2017)Che, Li, Zhang, Hjelm, Li, Song, and Bengio]{che2017maximumlikelihood}
Tong Che, Yanran Li, Ruixiang Zhang, R~Devon Hjelm, Wenjie Li, Yangqiu Song, and Yoshua Bengio.
\newblock Maximum-likelihood augmented discrete generative adversarial networks, 2017.

\bibitem[Deng et~al.(2020)Deng, Bakhtin, Ott, Szlam, and Ranzato]{deng2020residual}
Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc'Aurelio Ranzato.
\newblock Residual energy-based models for text generation.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=B1l4SgHKDH}.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pp.\  4171--4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://aclanthology.org/N19-1423}.

\bibitem[Diaconis \& Saloff-Coste(1996)Diaconis and Saloff-Coste]{diaconis1996logarithmic}
Persi Diaconis and Laurent Saloff-Coste.
\newblock Logarithmic sobolev inequalities for finite markov chains.
\newblock \emph{The Annals of Applied Probability}, 6\penalty0 (3):\penalty0 695--750, 1996.

\bibitem[Edelman et~al.(2022)Edelman, Goel, Kakade, and Zhang]{edelman2022inductive}
Benjamin~L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang.
\newblock Inductive biases and variable creation in self-attention mechanisms.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), \emph{Proceedings of the 39th International Conference on Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  5793--5831. PMLR, 17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/edelman22a.html}.

\bibitem[Ghazvininejad et~al.(2019)Ghazvininejad, Levy, Liu, and Zettlemoyer]{ghazvininejad2019mask}
Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer.
\newblock Mask-predict: Parallel decoding of conditional masked language models.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pp.\  6112--6121, Hong Kong, China, November 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-1633}.
\newblock URL \url{https://aclanthology.org/D19-1633}.

\bibitem[Ghazvininejad et~al.(2020)Ghazvininejad, Levy, and Zettlemoyer]{ghazvininejad2020semiautoregressive}
Marjan Ghazvininejad, Omer Levy, and Luke Zettlemoyer.
\newblock Semi-autoregressive training improves mask-predict decoding, 2020.

\bibitem[Gong et~al.(2023)Gong, Li, Feng, Wu, and Kong]{gong2023diffuseq}
Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and Lingpeng Kong.
\newblock Diffuseq: Sequence to sequence text generation with diffusion models.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=jQj-_rLVXsj}.

\bibitem[Goyal et~al.(2022)Goyal, Dyer, and Berg-Kirkpatrick]{goyal2022exposing}
Kartik Goyal, Chris Dyer, and Taylor Berg-Kirkpatrick.
\newblock Exposing the implicit energy networks behind masked language models via metropolis--hastings.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=6PvWo1kEvlT}.

\bibitem[Gu \& Kong(2021)Gu and Kong]{gu2021fully}
Jiatao Gu and Xiang Kong.
\newblock Fully non-autoregressive neural machine translation: Tricks of the trade.
\newblock In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), \emph{Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}, pp.\  120--133, Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.findings-acl.11}.
\newblock URL \url{https://aclanthology.org/2021.findings-acl.11}.

\bibitem[Gu et~al.(2018)Gu, Bradbury, Xiong, Li, and Socher]{gu2018nonautoregressive}
Jiatao Gu, James Bradbury, Caiming Xiong, Victor~O.K. Li, and Richard Socher.
\newblock Non-autoregressive neural machine translation.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=B1l8BtlCb}.

\bibitem[Guo et~al.(2018)Guo, Lu, Cai, Zhang, Yu, and Wang]{guo2018long}
Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang.
\newblock Long text generation via adversarial training with leaked information.
\newblock In \emph{Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence}, AAAI'18/IAAI'18/EAAI'18. AAAI Press, 2018.
\newblock ISBN 978-1-57735-800-8.

\bibitem[Guo et~al.(2020)Guo, Xu, and Chen]{guo2020jointly}
Junliang Guo, Linli Xu, and Enhong Chen.
\newblock Jointly masked sequence-to-sequence model for non-autoregressive neural machine translation.
\newblock In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pp.\  376--385, Online, July 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.36}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.36}.

\bibitem[H{\'a}jek(1972)]{hajek1972local}
Jaroslav H{\'a}jek.
\newblock Local asymptotic minimax and admissibility in estimation.
\newblock In \emph{Proceedings of the sixth Berkeley symposium on mathematical statistics and probability}, volume~1, pp.\  175--194, 1972.

\bibitem[Holtzman et~al.(2020)Holtzman, Buys, Du, Forbes, and Choi]{holtzman2020the}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rygGQyrFvH}.

\bibitem[Hoogeboom et~al.(2021)Hoogeboom, Nielsen, Jaini, Forr{\'e}, and Welling]{hoogeboom2021argmax}
Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr{\'e}, and Max Welling.
\newblock Argmax flows and multinomial diffusion: Learning categorical distributions.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=6nbpPqUCIi7}.

\bibitem[Huang \& Ogata(2002)Huang and Ogata]{huang2002generalized}
Fuchun Huang and Yosihiko Ogata.
\newblock Generalized pseudo-likelihood estimates for markov random fields on lattice.
\newblock \emph{Annals of the Institute of Statistical Mathematics}, 54:\penalty0 1--18, 2002.

\bibitem[Jelassi et~al.(2022)Jelassi, Sander, and Li]{jelassi2022vision}
Samy Jelassi, Michael~Eli Sander, and Yuanzhi Li.
\newblock Vision transformers provably learn spatial structure.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=eMW9AkXaREI}.

\bibitem[Kasai et~al.(2020)Kasai, Cross, Ghazvininejad, and Gu]{kasai2020nonautoregressive}
Jungo Kasai, James Cross, Marjan Ghazvininejad, and Jiatao Gu.
\newblock Non-autoregressive machine translation with disentangled context transformer.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning}, ICML'20. JMLR.org, 2020.

\bibitem[Kasai et~al.(2021)Kasai, Pappas, Peng, Cross, and Smith]{kasai2021deep}
Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, and Noah Smith.
\newblock Deep encoder, shallow decoder: Reevaluating non-autoregressive machine translation.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=KpfasTaLUpq}.

\bibitem[Kim \& Rush(2016)Kim and Rush]{kim2016sequence}
Yoon Kim and Alexander~M. Rush.
\newblock Sequence-level knowledge distillation.
\newblock In Jian Su, Kevin Duh, and Xavier Carreras (eds.), \emph{Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing}, pp.\  1317--1327, Austin, Texas, November 2016. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D16-1139}.
\newblock URL \url{https://aclanthology.org/D16-1139}.

\bibitem[Koehler et~al.(2023)Koehler, Heckett, and Risteski]{koehler2023statistical}
Frederic Koehler, Alexander Heckett, and Andrej Risteski.
\newblock Statistical efficiency of score matching: The view from isoperimetry.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=TD7AnQjNzR6}.

\bibitem[Kong et~al.(2020)Kong, Zhang, and Hovy]{kong2020incorporating}
Xiang Kong, Zhisong Zhang, and Eduard Hovy.
\newblock Incorporating a local translation mechanism into non-autoregressive translation.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pp.\  1067--1073, Online, November 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.79}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.79}.

\bibitem[Kreutzer et~al.(2020)Kreutzer, Foster, and Cherry]{kreutzer2020inference}
Julia Kreutzer, George Foster, and Colin Cherry.
\newblock Inference strategies for machine translation with conditional masking.
\newblock In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pp.\  5774--5782, Online, November 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.465}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.465}.

\bibitem[Kudo \& Richardson(2018)Kudo and Richardson]{kudo2018sp}
Taku Kudo and John Richardson.
\newblock {S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for neural text processing.
\newblock In Eduardo Blanco and Wei Lu (eds.), \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pp.\  66--71, Brussels, Belgium, November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-2012}.
\newblock URL \url{https://aclanthology.org/D18-2012}.

\bibitem[Lee(2023)]{lee2023parallelising}
Holden Lee.
\newblock Parallelising glauber dynamics, 2023.

\bibitem[Lee et~al.(2018)Lee, Mansimov, and Cho]{lee2018deterministic}
Jason Lee, Elman Mansimov, and Kyunghyun Cho.
\newblock Deterministic non-autoregressive neural sequence modeling by iterative refinement.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pp.\  1173--1182, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-1149}.
\newblock URL \url{https://aclanthology.org/D18-1149}.

\bibitem[Lee et~al.(2020)Lee, Shu, and Cho]{lee2020iterative}
Jason Lee, Raphael Shu, and Kyunghyun Cho.
\newblock Iterative refinement in the continuous space for non-autoregressive neural machine translation.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pp.\  1006--1015, Online, November 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.73}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.73}.

\bibitem[Li et~al.(2022)Li, Thickstun, Gulrajani, Liang, and Hashimoto]{li2022diffusionlm}
Xiang~Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto.
\newblock Diffusion-{LM} improves controllable text generation.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=3s9IrEsjLyk}.

\bibitem[Li \& Risteski(2021)Li and Risteski]{li2021limitations}
Yuchen Li and Andrej Risteski.
\newblock The limitations of limited context for constituency parsing.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pp.\  2675--2687, Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.208}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.208}.

\bibitem[Li et~al.(2023)Li, Li, and Risteski]{li2023Transformers}
Yuchen Li, Yuanzhi Li, and Andrej Risteski.
\newblock How do transformers learn topic structure: Towards a mechanistic understanding.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), \emph{Proceedings of the 40th International Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  19689--19729. PMLR, 23--29 Jul 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/li23p.html}.

\bibitem[Lin et~al.(2021)Lin, Jaech, Li, Gormley, and Eisner]{lin2021limitations}
Chu-Cheng Lin, Aaron Jaech, Xin Li, Matthew~R. Gormley, and Jason Eisner.
\newblock Limitations of autoregressive models and their alternatives.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  5147--5173, Online, June 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.405}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.405}.

\bibitem[Lin et~al.(2017)Lin, Li, He, Zhang, and Sun]{lin2017adversarial}
Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang, and Ming-Ting Sun.
\newblock Adversarial ranking for language generation.
\newblock In \emph{Proceedings of the 31st International Conference on Neural Information Processing Systems}, NIPS'17, pp.\  3158–3168, Red Hook, NY, USA, 2017. Curran Associates Inc.
\newblock ISBN 9781510860964.

\bibitem[Liu et~al.(2022)Liu, Hsu, Ravikumar, and Risteski]{liu2022masked}
Bingbin Liu, Daniel Hsu, Pradeep~Kumar Ravikumar, and Andrej Risteski.
\newblock Masked prediction: A parameter identifiability view.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=Hbvlb4D1aFC}.

\bibitem[Liu et~al.(2023)Liu, Ash, Goel, Krishnamurthy, and Zhang]{liu2023Transformers}
Bingbin Liu, Jordan~T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang.
\newblock Transformers learn shortcuts to automata.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=De4FYqjFueZ}.

\bibitem[Lou et~al.(2023)Lou, Meng, and Ermon]{lou2023discrete}
Aaron Lou, Chenlin Meng, and Stefano Ermon.
\newblock Discrete diffusion language modeling by estimating the ratios of the data distribution, 2023.

\bibitem[Lu et~al.(2021)Lu, Mao, and Nayak]{lu2021on}
Haoye Lu, Yongyi Mao, and Amiya Nayak.
\newblock On the dynamics of training attention models.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=1OCTOShAmqB}.

\bibitem[Ma et~al.(2019)Ma, Zhou, Li, Neubig, and Hovy]{ma2019flowseq}
Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neubig, and Eduard Hovy.
\newblock {F}low{S}eq: Non-autoregressive conditional sequence generation with generative flow.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pp.\  4282--4292, Hong Kong, China, November 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-1437}.
\newblock URL \url{https://aclanthology.org/D19-1437}.

\bibitem[Marton(2013)]{marton2013inequality}
Katalin Marton.
\newblock An inequality for relative entropy and logarithmic sobolev inequalities in euclidean spaces.
\newblock \emph{Journal of Functional Analysis}, 264\penalty0 (1):\penalty0 34--61, 2013.

\bibitem[Marton(2015)]{marton2015logarithmic}
Katalin Marton.
\newblock Logarithmic sobolev inequalities in discrete product spaces: a proof by a transportation cost distance.
\newblock \emph{arXiv preprint arXiv:1507.02803}, 2015.

\bibitem[Meng et~al.(2023)Meng, Krishnan, Wang, Wang, Mao, Fang, Ghazvininejad, Han, and Zettlemoyer]{meng2023representation}
Yu~Meng, Jitin Krishnan, Sinong Wang, Qifan Wang, Yuning Mao, Han Fang, Marjan Ghazvininejad, Jiawei Han, and Luke Zettlemoyer.
\newblock Representation deficiency in masked language modeling.
\newblock \emph{arXiv preprint arXiv:2302.02060}, 2023.

\bibitem[Pabbaraju et~al.(2023)Pabbaraju, Rohatgi, Sevekari, Lee, Moitra, and Risteski]{pabbaraju2023provable}
Chirag Pabbaraju, Dhruv Rohatgi, Anish Sevekari, Holden Lee, Ankur Moitra, and Andrej Risteski.
\newblock Provable benefits of score matching.
\newblock \emph{arXiv preprint arXiv:2306.01993}, 2023.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and Zhu]{papineni2002bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock {B}leu: a method for automatic evaluation of machine translation.
\newblock In Pierre Isabelle, Eugene Charniak, and Dekang Lin (eds.), \emph{Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics}, pp.\  311--318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics.
\newblock \doi{10.3115/1073083.1073135}.
\newblock URL \url{https://aclanthology.org/P02-1040}.

\bibitem[Post(2018)]{post2018call}
Matt Post.
\newblock A call for clarity in reporting {BLEU} scores.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation: Research Papers}, pp.\  186--191, Belgium, Brussels, October 2018. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/W18-6319}.

\bibitem[Pu et~al.(2021)Pu, Chung, Parikh, Gehrmann, and Sellam]{pu2021metrics}
Amy Pu, Hyung~Won Chung, Ankur~P. Parikh, Sebastian Gehrmann, and Thibault Sellam.
\newblock Learning compact metrics for {MT}.
\newblock \emph{CoRR}, abs/2110.06341, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.06341}.

\bibitem[Qian et~al.(2021)Qian, Zhou, Bao, Wang, Qiu, Zhang, Yu, and Li]{qian2021glancing}
Lihua Qian, Hao Zhou, Yu~Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, and Lei Li.
\newblock Glancing transformer for non-autoregressive neural machine translation.
\newblock In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pp.\  1993--2003, Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.155}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.155}.

\bibitem[Qin et~al.(2022)Qin, Welleck, Khashabi, and Choi]{qin2022cold}
Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi.
\newblock {COLD} decoding: Energy-based constrained text generation with langevin dynamics.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=TiZYrQ-mPup}.

\bibitem[Qin \& Risteski(2023)Qin and Risteski]{qin2023fit}
Yilong Qin and Andrej Risteski.
\newblock Fit like you sample: Sample-efficient generalized score matching from fast mixing diffusions, 2023.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0 (1):\penalty0 5485--5551, 2020.

\bibitem[Ravikumar et~al.(2010)Ravikumar, Wainwright, and Lafferty]{ravikumar2010high}
Pradeep Ravikumar, Martin~J Wainwright, and John~D Lafferty.
\newblock High-dimensional ising model selection using l1-regularized logistic regression.
\newblock \emph{Ann. Statist. 38(3): 1287-1319 (June 2010). DOI: 10.1214/09-AOS691}, 2010.

\bibitem[Reid et~al.(2022)Reid, Hellendoorn, and Neubig]{reid2022diffuser}
Machel Reid, Vincent~Josua Hellendoorn, and Graham Neubig.
\newblock Diffuser: Diffusion via edit-based reconstruction.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Ren et~al.(2020)Ren, Liu, Tan, Zhao, Zhao, and Liu]{ren2020study}
Yi~Ren, Jinglin Liu, Xu~Tan, Zhou Zhao, Sheng Zhao, and Tie-Yan Liu.
\newblock A study of non-autoregressive model for sequence generation.
\newblock In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pp.\  149--159, Online, July 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.15}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.15}.

\bibitem[Roberts et~al.(2022)Roberts, Chung, Levskaya, Mishra, Bradbury, Andor, Narang, Lester, Gaffney, Mohiuddin, Hawthorne, Lewkowycz, Salcianu, van Zee, Austin, Goodman, Soares, Hu, Tsvyashchenko, Chowdhery, Bastings, Bulian, Garcia, Ni, Chen, Kenealy, Clark, Lee, Garrette, Lee-Thorp, Raffel, Shazeer, Ritter, Bosma, Passos, Maitin-Shepard, Fiedel, Omernick, Saeta, Sepassi, Spiridonov, Newlan, and Gesmundo]{roberts2022t5x}
Adam Roberts, Hyung~Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio~Baldini Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan~H. Clark, Stephan Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo.
\newblock Scaling up models and data with $\texttt{t5x}$ and $\texttt{seqio}$.
\newblock \emph{arXiv preprint arXiv:2203.17189}, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.17189}.

\bibitem[Saharia et~al.(2020)Saharia, Chan, Saxena, and Norouzi]{saharia2020non}
Chitwan Saharia, William Chan, Saurabh Saxena, and Mohammad Norouzi.
\newblock Non-autoregressive machine translation with latent alignments.
\newblock \emph{arXiv preprint arXiv:2004.07437}, 2020.

\bibitem[Savinov et~al.(2022)Savinov, Chung, Binkowski, Elsen, and van~den Oord]{savinov2022stepunrolled}
Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, and Aaron van~den Oord.
\newblock Step-unrolled denoising autoencoders for text generation.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=T0GpzBQ1Fg6}.

\bibitem[Schmidt et~al.(2022)Schmidt, Pires, Peitz, and Lööf]{schmidt2022clarity}
Robin~M. Schmidt, Telmo Pires, Stephan Peitz, and Jonas Lööf.
\newblock Non-autoregressive neural machine translation: A call for clarity, 2022.

\bibitem[Sellam et~al.(2020)Sellam, Das, and Parikh]{sellam2020bleurt}
Thibault Sellam, Dipanjan Das, and Ankur~P. Parikh.
\newblock {BLEURT:} learning robust metrics for text generation.
\newblock \emph{CoRR}, abs/2004.04696, 2020.
\newblock URL \url{https://arxiv.org/abs/2004.04696}.

\bibitem[Shazeer \& Stern(2018)Shazeer and Stern]{shazeer2018adafactor}
Noam Shazeer and Mitchell Stern.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock \emph{CoRR}, abs/1804.04235, 2018.
\newblock URL \url{http://arxiv.org/abs/1804.04235}.

\bibitem[Stern et~al.(2019)Stern, Chan, Kiros, and Uszkoreit]{stern2019insertion}
Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit.
\newblock Insertion transformer: Flexible sequence generation via insertion operations.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5976--5985. PMLR, 2019.

\bibitem[Toda(2011)]{toda2011operator}
Alexis~Akira Toda.
\newblock Operator reverse monotonicity of the inverse.
\newblock \emph{The American Mathematical Monthly}, 118\penalty0 (1):\penalty0 82--83, 2011.

\bibitem[Torroba~Hennigen \& Kim(2023)Torroba~Hennigen and Kim]{hennigen2023deriving}
Lucas Torroba~Hennigen and Yoon Kim.
\newblock Deriving language models from masked language models.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pp.\  1149--1159, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-short.99}.
\newblock URL \url{https://aclanthology.org/2023.acl-short.99}.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Van~der Vaart(2000)]{van2000asymptotic}
Aad~W Van~der Vaart.
\newblock \emph{Asymptotic statistics}, volume~3.
\newblock Cambridge university press, 2000.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus, S.~Vishwanathan, and R.~Garnett (eds.), \emph{Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL \url{https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.

\bibitem[Vuffray et~al.(2016)Vuffray, Misra, Lokhov, and Chertkov]{vuffray2016interaction}
Marc Vuffray, Sidhant Misra, Andrey Lokhov, and Michael Chertkov.
\newblock Interaction screening: Efficient and sample-optimal learning of ising models.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Wang \& Cho(2019)Wang and Cho]{wang2019bert}
Alex Wang and Kyunghyun Cho.
\newblock {BERT} has a mouth, and it must speak: {BERT} as a {M}arkov random field language model.
\newblock In Antoine Bosselut, Asli Celikyilmaz, Marjan Ghazvininejad, Srinivasan Iyer, Urvashi Khandelwal, Hannah Rashkin, and Thomas Wolf (eds.), \emph{Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation}, pp.\  30--36, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/W19-2304}.
\newblock URL \url{https://aclanthology.org/W19-2304}.

\bibitem[Wei et~al.(2021)Wei, Chen, and Ma]{wei2021statistically}
Colin Wei, Yining Chen, and Tengyu Ma.
\newblock Statistically meaningful approximation: a case study on approximating turing machines with transformers, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.13163}.

\bibitem[Wen et~al.(2023)Wen, Li, Liu, and Risteski]{wen2023uninterpretability}
Kaiyue Wen, Yuchen Li, Bingbin Liu, and Andrej Risteski.
\newblock Transformers are uninterpretable with myopic methods: a case study with bounded dyck grammars.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=OitmaxSAUu}.

\bibitem[Yao et~al.(2021)Yao, Peng, Papadimitriou, and Narasimhan]{yao2021self}
Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan.
\newblock Self-attention networks can process bounded hierarchical languages.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pp.\  3770--3785, Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.292}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.292}.

\bibitem[Young \& You(2022)Young and You]{young2022inconsistencies}
Tom Young and Yang You.
\newblock On the inconsistencies of conditionals learned by masked language models.
\newblock \emph{arXiv preprint arXiv:2301.00068}, 2022.

\bibitem[Yu et~al.(2017)Yu, Zhang, Wang, and Yu]{yu2017seqgan}
Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
\newblock Seqgan: Sequence generative adversarial nets with policy gradient.
\newblock In \emph{Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence}, AAAI'17, pp.\  2852–2858. AAAI Press, 2017.

\bibitem[Yun et~al.(2020)Yun, Bhojanapalli, Rawat, Reddi, and Kumar]{yun2020are}
Chulhee Yun, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank Reddi, and Sanjiv Kumar.
\newblock Are transformers universal approximators of sequence-to-sequence functions?
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=ByxRM0Ntvr}.

\bibitem[Zhao et~al.(2023)Zhao, Panigrahi, Ge, and Arora]{zhao2023Transformers}
Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora.
\newblock Do transformers parse while predicting the masked word?
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pp.\  16513--16542, Singapore, December 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.1029}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.1029}.

\bibitem[Zheng et~al.(2023)Zheng, Yuan, Yu, and Kong]{zheng2023reparameterized}
Lin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong.
\newblock A reparameterized discrete diffusion model for text generation, 2023.

\bibitem[Zhou et~al.(2020)Zhou, Gu, and Neubig]{zhou2020understanding}
Chunting Zhou, Jiatao Gu, and Graham Neubig.
\newblock Understanding knowledge distillation in non-autoregressive machine translation.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=BygFVAEKDH}.

\bibitem[Ziegler \& Rush(2019)Ziegler and Rush]{ziegler2019latent}
Zachary Ziegler and Alexander Rush.
\newblock Latent normalizing flows for discrete sequences.
\newblock In \emph{International Conference on Machine Learning}, pp.\  7673--7682. PMLR, 2019.

\end{thebibliography}
