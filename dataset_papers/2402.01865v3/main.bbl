\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aljundi et~al.(2019{\natexlab{a}})Aljundi, Belilovsky, Tuytelaars, Charlin, Caccia, Lin, and Page{-}Caccia]{DBLP:conf/nips/AljundiBTCCLP19}
Aljundi, R., Belilovsky, E., Tuytelaars, T., Charlin, L., Caccia, M., Lin, M., and Page{-}Caccia, L.
\newblock Online continual learning with maximal interfered retrieval.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019{\natexlab{a}}.

\bibitem[Aljundi et~al.(2019{\natexlab{b}})Aljundi, Lin, Goujaud, and Bengio]{Aljundi2019GradientBS}
Aljundi, R., Lin, M., Goujaud, B., and Bengio, Y.
\newblock Gradient based sample selection for online continual learning.
\newblock In \emph{Neural Information Processing Systems}, 2019{\natexlab{b}}.

\bibitem[Bach et~al.(2022)Bach, Sanh, Yong, Webson, Raffel, Nayak, Sharma, Kim, Bari, F{\'e}vry, et~al.]{bach2022promptsource}
Bach, S., Sanh, V., Yong, Z.~X., Webson, A., Raffel, C., Nayak, N.~V., Sharma, A., Kim, T., Bari, M.~S., F{\'e}vry, T., et~al.
\newblock Promptsource: An integrated development environment and repository for natural language prompts.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics: System Demonstrations}, 2022.

\bibitem[Buzzega et~al.(2020{\natexlab{a}})Buzzega, Boschini, Porrello, Abati, and Calderara]{buzzega2020dark}
Buzzega, P., Boschini, M., Porrello, A., Abati, D., and Calderara, S.
\newblock Dark experience for general continual learning: a strong, simple baseline.
\newblock \emph{Advances in neural information processing systems}, 2020{\natexlab{a}}.

\bibitem[Buzzega et~al.(2020{\natexlab{b}})Buzzega, Boschini, Porrello, and Calderara]{Buzzega2020RethinkingER}
Buzzega, P., Boschini, M., Porrello, A., and Calderara, S.
\newblock Rethinking experience replay: a bag of tricks for continual learning.
\newblock \emph{2020 25th International Conference on Pattern Recognition (ICPR)}, 2020{\natexlab{b}}.

\bibitem[Chaudhry et~al.(2019)Chaudhry, Ranzato, Rohrbach, and Elhoseiny]{chaudhry2018efficient}
Chaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny, M.
\newblock Efficient lifelong learning with a-{GEM}.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et~al.]{chung2022scaling}
Chung, H.~W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv:2210.11416}, 2022.

\bibitem[De~Cao et~al.(2021)De~Cao, Aziz, and Titov]{de2021editing}
De~Cao, N., Aziz, W., and Titov, I.
\newblock Editing factual knowledge in language models.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing}, 2021.

\bibitem[de~Masson~D'Autume et~al.(2019)de~Masson~D'Autume, Ruder, Kong, and Yogatama]{de2019episodic}
de~Masson~D'Autume, C., Ruder, S., Kong, L., and Yogatama, D.
\newblock Episodic memory in lifelong language learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Doan et~al.(2020)Doan, Bennani, Mazoure, Rabusseau, and Alquier]{Doan2020ATA}
Doan, T.~V., Bennani, M.~A., Mazoure, B., Rabusseau, G., and Alquier, P.
\newblock A theoretical analysis of catastrophic forgetting through the ntk overlap matrix.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, 2020.

\bibitem[Evron et~al.(2022)Evron, Moroshko, Ward, Srebro, and Soudry]{evron2022catastrophic}
Evron, I., Moroshko, E., Ward, R., Srebro, N., and Soudry, D.
\newblock How catastrophic can catastrophic forgetting be in linear regression?
\newblock In \emph{Conference on Learning Theory}, pp.\  4028--4079. PMLR, 2022.

\bibitem[Farajtabar et~al.(2020)Farajtabar, Azizan, Mott, and Li]{farajtabar2020orthogonal}
Farajtabar, M., Azizan, N., Mott, A., and Li, A.
\newblock Orthogonal gradient descent for continual learning.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  3762--3773. PMLR, 2020.

\bibitem[Hartvigsen et~al.(2022)Hartvigsen, Sankaranarayanan, Palangi, Kim, and Ghassemi]{Hartvigsen2022AgingWG}
Hartvigsen, T., Sankaranarayanan, S., Palangi, H., Kim, Y., and Ghassemi, M.
\newblock Aging with grace: Lifelong model editing with discrete key-value adaptors.
\newblock \emph{ArXiv}, abs/2211.11031, 2022.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring}
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Hu et~al.(2021)Hu, Wallis, Allen-Zhu, Li, Wang, Wang, Chen, et~al.]{hu2021lora}
Hu, E.~J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et~al.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Huang et~al.(2023)Huang, Shen, Zhang, Zhou, Rong, and Xiong]{huang2022transformer}
Huang, Z., Shen, Y., Zhang, X., Zhou, J., Rong, W., and Xiong, Z.
\newblock Transformer-patcher: One mistake worth one neuron.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Ilyas et~al.(2022)Ilyas, Park, Engstrom, Leclerc, and Madry]{Ilyas2022DatamodelsPP}
Ilyas, A., Park, S.~M., Engstrom, L., Leclerc, G., and Madry, A.
\newblock Datamodels: Predicting predictions from training data.
\newblock \emph{ArXiv}, abs/2202.00622, 2022.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural networks.
\newblock \emph{Advances in neural information processing systems}, 2018.

\bibitem[Jagielski et~al.(2023)Jagielski, Thakkar, Tramer, Ippolito, Lee, Carlini, Wallace, Song, Thakurta, Papernot, and Zhang]{jagielski2023measuring}
Jagielski, M., Thakkar, O., Tramer, F., Ippolito, D., Lee, K., Carlini, N., Wallace, E., Song, S., Thakurta, A.~G., Papernot, N., and Zhang, C.
\newblock Measuring forgetting of memorized training examples.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Jang et~al.(2021)Jang, Ye, Yang, Shin, Han, Kim, Choi, and Seo]{Jang2021TowardsCK}
Jang, J., Ye, S., Yang, S., Shin, J., Han, J., Kim, G., Choi, S.~J., and Seo, M.
\newblock Towards continual knowledge learning of language models.
\newblock \emph{ArXiv}, abs/2110.03215, 2021.

\bibitem[Jin et~al.(2022)Jin, Zhang, Zhu, Xiao, Li, Wei, Arnold, and Ren]{jin-etal-2022-lifelong-pretraining}
Jin, X., Zhang, D., Zhu, H., Xiao, W., Li, S.-W., Wei, X., Arnold, A., and Ren, X.
\newblock Lifelong pretraining: Continually adapting language models to emerging corpora.
\newblock In \emph{North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, 2022.

\bibitem[Karakida \& Akaho(2021)Karakida and Akaho]{Karakida2021LearningCF}
Karakida, R. and Akaho, S.
\newblock Learning curves for continual learning in neural networks: Self-knowledge transfer and forgetting.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Killamsetty et~al.(2021)Killamsetty, Sivasubramanian, Ramakrishnan, De, and Iyer]{Killamsetty2021GRADMATCHGM}
Killamsetty, K., Sivasubramanian, D., Ramakrishnan, G., De, A., and Iyer, R.~K.
\newblock Grad-match: Gradient matching based data subset selection for efficient deep model training.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Kleiman et~al.(2023)Kleiman, Frankle, Kakade, and Paul]{KleimanPredictingTF}
Kleiman, A., Frankle, J., Kakade, S.~M., and Paul, M.
\newblock Predicting task forgetting in large language models.
\newblock In \emph{ICML Workshop DeployableGenerativeAI homepage}, 2023.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein, and Pennington]{lee2019wide}
Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J., and Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under gradient descent.
\newblock \emph{Advances in neural information processing systems}, 2019.

\bibitem[Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, rahman Mohamed, Levy, Stoyanov, and Zettlemoyer]{Lewis2019BARTDS}
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., rahman Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics}, 2019.

\bibitem[Lin et~al.(2022{\natexlab{a}})Lin, Tan, Miller, Tian, and Ren]{lin2022unsupervised}
Lin, B.~Y., Tan, K., Miller, C., Tian, B., and Ren, X.
\newblock Unsupervised cross-task generalization via retrieval augmentation.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022{\natexlab{a}}.

\bibitem[Lin et~al.(2022{\natexlab{b}})Lin, Wang, Lin, Jia, Xiao, Ren, and tau Yih]{Lin2022OnCM}
Lin, B.~Y., Wang, S.~I., Lin, X.~V., Jia, R., Xiao, L., Ren, X., and tau Yih, W.
\newblock On continual model refinement in out-of-distribution data streams.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics}, 2022{\natexlab{b}}.

\bibitem[Lopez-Paz \& Ranzato(2017)Lopez-Paz and Ranzato]{LopezPaz2017GradientEM}
Lopez-Paz, D. and Ranzato, M.
\newblock Gradient episodic memory for continual learning.
\newblock In \emph{Neural Information Processing Systems}, 2017.

\bibitem[Maini et~al.(2022)Maini, Garg, Lipton, and Kolter]{maini2022characterizing}
Maini, P., Garg, S., Lipton, Z., and Kolter, J.~Z.
\newblock Characterizing datapoints via second-split forgetting.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov]{Meng2022LocatingAE}
Meng, K., Bau, D., Andonian, A., and Belinkov, Y.
\newblock Locating and editing factual associations in gpt.
\newblock In \emph{Neural Information Processing Systems}, 2022.

\bibitem[Miller(2017)]{Miller2017ExplanationIA}
Miller, T.
\newblock Explanation in artificial intelligence: Insights from the social sciences.
\newblock \emph{Artif. Intell.}, 267:\penalty0 1--38, 2017.

\bibitem[Mirzasoleiman et~al.(2019)Mirzasoleiman, Bilmes, and Leskovec]{Mirzasoleiman2019CoresetsFD}
Mirzasoleiman, B., Bilmes, J.~A., and Leskovec, J.
\newblock Coresets for data-efficient training of machine learning models.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Mitchell et~al.(2021)Mitchell, Lin, Bosselut, Finn, and Manning]{mitchell2021fast}
Mitchell, E., Lin, C., Bosselut, A., Finn, C., and Manning, C.~D.
\newblock Fast model editing at scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Novak et~al.(2022)Novak, Sohl-Dickstein, and Schoenholz]{Novak2022FastFW}
Novak, R., Sohl-Dickstein, J.~N., and Schoenholz, S.~S.
\newblock Fast finite width neural tangent kernel.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Onoe et~al.(2023)Onoe, Zhang, Padmanabhan, Durrett, and Choi]{onoe-etal-2023-lms}
Onoe, Y., Zhang, M., Padmanabhan, S., Durrett, G., and Choi, E.
\newblock Can {LM}s learn new entities from descriptions? challenges in propagating injected knowledge.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, 2023.

\bibitem[OpenAI(2023)]{OpenAI2023GPT4TR}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{ArXiv}, abs/2303.08774, 2023.

\bibitem[Raffel(2023)]{Raffel2023BuildingML}
Raffel, C.
\newblock Building machine learning models like open source software.
\newblock \emph{Communications of the ACM}, 2023.

\bibitem[Ramasesh et~al.(2020)Ramasesh, Dyer, and Raghu]{ramasesh2020anatomy}
Ramasesh, V.~V., Dyer, E., and Raghu, M.
\newblock Anatomy of catastrophic forgetting: Hidden representations and task semantics.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Robins(1995)]{robins1995catastrophic}
Robins, A.
\newblock Catastrophic forgetting, rehearsal and pseudorehearsal.
\newblock \emph{Connection Science}, 7\penalty0 (2):\penalty0 123--146, 1995.

\bibitem[Saha et~al.(2021)Saha, Garg, and Roy]{saha2021gradient}
Saha, G., Garg, I., and Roy, K.
\newblock Gradient projection memory for continual learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Tao et~al.(2023)Tao, Feng, and Zhao]{tao2023can}
Tao, M., Feng, Y., and Zhao, D.
\newblock Can {BERT} refrain from forgetting on sequential tasks? a probing study.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Tirumala et~al.(2022)Tirumala, Markosyan, Zettlemoyer, and Aghajanyan]{tirumala2022memorization}
Tirumala, K., Markosyan, A., Zettlemoyer, L., and Aghajanyan, A.
\newblock Memorization without overfitting: Analyzing the training dynamics of large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, pp.\  38274--38290, 2022.

\bibitem[Toneva et~al.(2018)Toneva, Sordoni, des Combes, Trischler, Bengio, and Gordon]{toneva2018empirical}
Toneva, M., Sordoni, A., des Combes, R.~T., Trischler, A., Bengio, Y., and Gordon, G.~J.
\newblock An empirical study of example forgetting during deep neural network learning.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Wu et~al.(2021)Wu, Caccia, Li, Li, Qi, and Haffari]{wu2021pretrained}
Wu, T., Caccia, M., Li, Z., Li, Y.-F., Qi, G., and Haffari, G.
\newblock Pretrained language model in continual learning: A comparative study.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Xia et~al.(2020)Xia, Anastasopoulos, Xu, Yang, and Neubig]{Xia2020PredictingPF}
Xia, M., Anastasopoulos, A., Xu, R., Yang, Y., and Neubig, G.
\newblock Predicting performance for natural language processing tasks.
\newblock \emph{ArXiv}, abs/2005.00870, 2020.

\bibitem[Yao et~al.(2021)Yao, Chen, Ye, Jin, and Ren]{Yao2021RefiningLM}
Yao, H., Chen, Y., Ye, Q., Jin, X., and Ren, X.
\newblock Refining language models with compositional explanations.
\newblock In \emph{Neural Information Processing Systems}, 2021.

\bibitem[Yao et~al.(2023)Yao, Wang, Tian, Cheng, Li, Deng, Chen, and Zhang]{yao2023editing}
Yao, Y., Wang, P., Tian, B., Cheng, S., Li, Z., Deng, S., Chen, H., and Zhang, N.
\newblock Editing large language models: Problems, methods, and opportunities.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing}, 2023.

\bibitem[Ye et~al.(2023)Ye, Fu, Ren, and Jia]{Ye2023HowPA}
Ye, Q., Fu, H.~Y., Ren, X., and Jia, R.
\newblock How predictable are large language model capabilities? a case study on big-bench.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing}, 2023.

\bibitem[Yoon et~al.(2022)Yoon, Madaan, Yang, and Hwang]{yoon2022online}
Yoon, J., Madaan, D., Yang, E., and Hwang, S.~J.
\newblock Online coreset selection for rehearsal-based continual learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Zeng, Lin, Wang, Ye, Xiao, Han, Liu, Li, Sun, and Zhou]{Zhang2023PlugandPlayKI}
Zhang, Z., Zeng, Z., Lin, Y., Wang, H., Ye, D., Xiao, C., Han, X., Liu, Z., Li, P., Sun, M., and Zhou, J.
\newblock Plug-and-play knowledge injection for pre-trained language models.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics}, 2023.

\end{thebibliography}
