\begin{thebibliography}{10}

\bibitem{Vapnik1998}
Vladimir~N. Vapnik.
\newblock {\em Statistical Learning Theory}.
\newblock Wiley-Interscience, 1998.

\bibitem{mohri2018foundations}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock {\em Foundations of machine learning}.
\newblock MIT Press, 2018.

\bibitem{amodei2016concrete}
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and
  Dan Man{\'e}.
\newblock Concrete problems in ai safety.
\newblock {\em arXiv preprint arXiv:1606.06565}, 2016.

\bibitem{gonzalez2015mismatched}
Carlos~R Gonz{\'a}lez and Yaser~S Abu-Mostafa.
\newblock Mismatched training and test distributions can outperform matched
  ones.
\newblock {\em Neural computation}, 27(2):365--387, 2015.

\bibitem{su2019one}
Jiawei Su, Danilo~Vasconcellos Vargas, and Kouichi Sakurai.
\newblock One pixel attack for fooling deep neural networks.
\newblock {\em IEEE Transactions on Evolutionary Computation}, 23(5):828--841,
  2019.

\bibitem{engstrom2019exploring}
Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander
  Madry.
\newblock Exploring the landscape of spatial robustness.
\newblock In {\em International Conference on Machine Learning}, pages
  1802--1811. PMLR, 2019.

\bibitem{recht2019imagenet}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In {\em International Conference on Machine Learning}, pages
  5389--5400. PMLR, 2019.

\bibitem{hendrycks2019benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock {\em arXiv preprint arXiv:1903.12261}, 2019.

\bibitem{wahba1990spline}
Grace Wahba.
\newblock {\em Spline models for observational data}.
\newblock SIAM, 1990.

\bibitem{evgeniou2000regularization}
Theodoros Evgeniou, Massimiliano Pontil, and Tomaso Poggio.
\newblock Regularization networks and support vector machines.
\newblock {\em Advances in computational mathematics}, 13(1):1, 2000.

\bibitem{shawe2004kernel}
John Shawe-Taylor, Nello Cristianini, et~al.
\newblock {\em Kernel methods for pattern analysis}.
\newblock Cambridge university press, 2004.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem{belkin2018understand}
Mikhail Belkin, Siyuan Ma, and Soumik Mandal.
\newblock To understand deep learning we need to understand kernel learning.
\newblock In {\em International Conference on Machine Learning}, pages
  541--549, 2018.

\bibitem{mezard1987spin}
Marc M{\'e}zard, Giorgio Parisi, and Miguel Virasoro.
\newblock {\em Spin glass theory and beyond: An Introduction to the Replica
  Method and Its Applications}, volume~9.
\newblock World Scientific Publishing Company, 1987.

\bibitem{bordelon2020spectrum}
Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan.
\newblock Spectrum dependent learning curves in kernel regression and wide
  neural networks.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}, 2020.

\bibitem{canatar2020spectral}
Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan.
\newblock Spectral bias and task-model alignment explain generalization in
  kernel regression and infinitely wide neural networks.
\newblock {\em Nature Communications}, 12(1):1--12, 2021.

\bibitem{quinonero2009dataset}
Joaquin Qui{\~n}onero-Candela, Masashi Sugiyama, Neil~D Lawrence, and Anton
  Schwaighofer.
\newblock {\em Dataset shift in machine learning}.
\newblock Mit Press, 2009.

\bibitem{ben2010theory}
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and
  Jennifer~Wortman Vaughan.
\newblock A theory of learning from different domains.
\newblock {\em Machine learning}, 79(1):151--175, 2010.

\bibitem{tsipras2018robustness}
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
  Aleksander Madry.
\newblock Robustness may be at odds with accuracy.
\newblock {\em arXiv preprint arXiv:1805.12152}, 2018.

\bibitem{krueger2020outofdistribution}
David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan
  Binas, Dinghuai Zhang, Remi~Le Priol, and Aaron Courville.
\newblock Out-of-distribution generalization via risk extrapolation (rex),
  2020.

\bibitem{arjovsky2019invariant}
Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz.
\newblock Invariant risk minimization, 2019.

\bibitem{arjovsky2020out}
Martin Arjovsky.
\newblock {\em Out of Distribution Generalization in Machine Learning}.
\newblock PhD thesis, New York University, 2020.

\bibitem{redko2019advances}
Ievgen Redko, Emilie Morvant, Amaury Habrard, Marc Sebban, and Younes Bennani.
\newblock {\em Advances in domain adaptation theory}.
\newblock Elsevier, 2019.

\bibitem{belkin2019reconciling}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock {\em Proceedings of the National Academy of Sciences},
  116(32):15849--15854, 2019.

\bibitem{engel2001statistical}
Andreas Engel and Christian Van~den Broeck.
\newblock {\em Statistical mechanics of learning}.
\newblock Cambridge University Press, 2001.

\bibitem{RasmussenWilliams}
Carl~Edward Rasmussen and Christopher K.~I. Williams.
\newblock {\em Gaussian Processes for Machine Learning (Adaptive Computation
  and Machine Learning)}.
\newblock The MIT Press, 2005.

\bibitem{advani2013statistical}
Madhu Advani, Subhaneil Lahiri, and Surya Ganguli.
\newblock Statistical mechanics of complex neural systems and high dimensional
  data.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment},
  2013(03):P03014, 2013.

\bibitem{sompolinsky1999statistical}
Rainer Dietrich, Manfred Opper, and Haim Sompolinsky.
\newblock Statistical mechanics of support vector networks.
\newblock {\em Physical review letters}, 82(14):2975, 1999.

\bibitem{neuraltangents2020}
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander~A. Alemi, Jascha
  Sohl-Dickstein, and Samuel~S. Schoenholz.
\newblock Neural tangents: Fast and easy infinite neural networks in python.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, George Necula, Adam Paszke, Jake Vander{P}las, Skye
  Wanderman-{M}ilne, and Qiao Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.

\bibitem{nakkiran2019deep}
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya
  Sutskever.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{krogh}
A~Krogh and J.~Hertz.
\newblock Generalization in a linear perceptron in the presence of noise.
\newblock {\em Journal of Physics A: Mathematical and General}, 25:1135, 01
  1999.

\bibitem{Hertz_1989}
J~A Hertz, A~Krogh, and G~I Thorbergsson.
\newblock Phase transitions in simple learning.
\newblock {\em Journal of Physics A: Mathematical and General},
  22(12):2133--2150, Jun 1989.

\bibitem{montanari2019surprises}
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan~J. Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock {\em arXiv preprint arXiv:1903.08560}, 2019.

\bibitem{nakkiran2019moredata}
Preetum Nakkiran.
\newblock More data can hurt for linear regression: Sample-wise double descent.
\newblock {\em arXiv preprint arXiv:1912.07242}, 2019.

\bibitem{nakkiran2020optimal}
Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma.
\newblock Optimal regularization can mitigate double descent.
\newblock {\em arXiv preprint arXiv:2003.01897}, 2020.

\bibitem{dascoli2020triple}
Stéphane d'Ascoli, Levent Sagun, and Giulio Biroli.
\newblock Triple descent and the two kinds of overfitting: Where and why do
  they appear?
\newblock {\em arXiv preprint arXiv:2006.03509}, 2020.

\bibitem{liang_isometry}
Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai.
\newblock On the multiple descent of minimum-norm interpolants and restricted
  lower isometry of kernels.
\newblock volume 125 of {\em Proceedings of Machine Learning Research}, pages
  2683--2711. PMLR, 09--12 Jul 2020.

\bibitem{chen2020multipledesign}
Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi.
\newblock Multiple descent: Design your own generalization curve, 2020.

\bibitem{lee2017deep}
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel~S Schoenholz, Jeffrey
  Pennington, and Jascha Sohl-Dickstein.
\newblock Deep neural networks as gaussian processes.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In {\em Advances in neural information processing systems}, pages
  8570--8581, 2019.

\bibitem{loureiro2021capturing}
Bruno Loureiro, C{\'e}dric Gerbelot, Hugo Cui, Sebastian Goldt, Florent
  Krzakala, Marc M{\'e}zard, and Lenka Zdeborov{\'a}.
\newblock Capturing the learning curves of generic features maps for realistic
  data sets with a teacher-student model.
\newblock {\em arXiv preprint arXiv:2102.08127}, 2021.

\bibitem{sompolinsky1992examples}
H.~S. Seung, H.~Sompolinsky, and N.~Tishby.
\newblock Statistical mechanics of learning from examples.
\newblock {\em Phys. Rev. A}, 45:6056--6091, Apr 1992.

\bibitem{optimalperceptron}
M~Opper, W~Kinzel, J~Kleinz, and R~Nehl.
\newblock On the ability of the optimal perceptron to generalise.
\newblock {\em Journal of Physics A: Mathematical and General},
  23(11):L581--L586, jun 1990.

\bibitem{dascoli2020double}
Stéphane d'Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala.
\newblock Double trouble in double descent : Bias and variance(s) in the lazy
  regime.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}, 2020.

\bibitem{kouw2019review}
Wouter~Marco Kouw and Marco Loog.
\newblock A review of domain adaptation without target labels.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  2019.

\bibitem{wald1945statistical}
Abraham Wald.
\newblock Statistical decision functions which minimize the maximum risk.
\newblock {\em Annals of Mathematics}, pages 265--280, 1945.

\bibitem{sagawa2019distributionally}
Shiori Sagawa, Pang~Wei Koh, Tatsunori~B Hashimoto, and Percy Liang.
\newblock Distributionally robust neural networks for group shifts: On the
  importance of regularization for worst-case generalization.
\newblock {\em arXiv preprint arXiv:1911.08731}, 2019.

\bibitem{ilyas2019adversarial}
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon
  Tran, and Aleksander Madry.
\newblock Adversarial examples are not bugs, they are features.
\newblock {\em arXiv preprint arXiv:1905.02175}, 2019.

\bibitem{sagawa2020investigation}
Shiori Sagawa, Aditi Raghunathan, Pang~Wei Koh, and Percy Liang.
\newblock An investigation of why overparameterization exacerbates spurious
  correlations.
\newblock In {\em International Conference on Machine Learning}, pages
  8346--8356. PMLR, 2020.

\bibitem{ben2009robust}
Aharon Ben-Tal, Laurent El~Ghaoui, and Arkadi Nemirovski.
\newblock {\em Robust optimization}.
\newblock Princeton university press, 2009.

\bibitem{dai2014scalable}
Bo~Dai, Bo~Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina Balcan, and
  Le~Song.
\newblock Scalable kernel methods via doubly stochastic gradients, 2014.

\bibitem{scholkopf2002learning}
Bernhard Sch{\"o}lkopf, Alexander~J Smola, Francis Bach, et~al.
\newblock {\em Learning with kernels: support vector machines, regularization,
  optimization, and beyond}.
\newblock MIT press, 2002.

\bibitem{spinGlassReview}
Tommaso Castellani and Andrea Cavagna.
\newblock Spin-glass theory for pedestrians.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment},
  2005(05):P05012, May 2005.

\bibitem{dai2013spherical}
Feng Dai and Yuan Xu.
\newblock {\em Approximation Theory and Harmonic Analysis on Spheres and
  Balls}.
\newblock Springer New York, 2013.

\bibitem{bietti2019inductive}
Alberto Bietti and Julien Mairal.
\newblock On the inductive bias of neural tangent kernels.
\newblock {\em arXiv preprint arXiv:1905.12173}, 2019.

\bibitem{xu2021neural}
Keyulu Xu, Mozhi Zhang, Jingling Li, Simon~S. Du, Ken ichi Kawarabayashi, and
  Stefanie Jegelka.
\newblock How neural networks extrapolate: From feedforward to graph neural
  networks, 2021.

\bibitem{bisong2019google}
Ekaba Bisong.
\newblock Google colaboratory.
\newblock In {\em Building Machine Learning and Deep Learning Models on Google
  Cloud Platform}, pages 59--64. Springer, 2019.

\bibitem{lecun2010mnist}
Yann LeCun, Corinna Cortes, and CJ~Burges.
\newblock {MNIST} handwritten digit database.
\newblock {\em ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
  2, 2010.

\end{thebibliography}
