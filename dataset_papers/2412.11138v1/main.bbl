\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2017)Achiam, Held, Tamar, and Abbeel]{achiam2017constrained}
Achiam, J., Held, D., Tamar, A., and Abbeel, P.
\newblock Constrained policy optimization, 2017.

\bibitem[Altman(1999)]{altman1999constrained}
Altman, E.
\newblock \emph{Constrained Markov decision processes}, volume~7.
\newblock CRC press, 1999.

\bibitem[As et~al.(2022)As, Usmanova, Curi, and Krause]{as2022constrained}
As, Y., Usmanova, I., Curi, S., and Krause, A.
\newblock Constrained policy optimization via bayesian world models.
\newblock \emph{arXiv preprint arXiv:2201.09802}, 2022.

\bibitem[Bogue(2017)]{bogue2017robots}
Bogue, R.
\newblock Robots that interact with humans: a review of safety technologies and standards.
\newblock \emph{Industrial Robot: An International Journal}, 44\penalty0 (4):\penalty0 395--400, 2017.

\bibitem[Chow et~al.(2018)Chow, Ghavamzadeh, Janson, and Pavone]{chow2018risk}
Chow, Y., Ghavamzadeh, M., Janson, L., and Pavone, M.
\newblock Risk-constrained reinforcement learning with percentile risk criteria.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0 (167):\penalty0 1--51, 2018.

\bibitem[Clavera et~al.(2020)Clavera, Fu, and Abbeel]{clavera2020model}
Clavera, I., Fu, V., and Abbeel, P.
\newblock Model-augmented actor-critic: Backpropagating through paths.
\newblock \emph{arXiv preprint arXiv:2005.08068}, 2020.

\bibitem[Dai et~al.(2023)Dai, Ji, Yang, Zheng, and Pan]{dai2023augmented}
Dai, J., Ji, J., Yang, L., Zheng, Q., and Pan, G.
\newblock Augmented proximal policy optimization for safe reinforcement learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pp.\  7288--7295, 2023.

\bibitem[Dai et~al.(2024)Dai, Pan, Sun, Ji, Xu, Liu, Wang, and Yang]{dai2024safe}
Dai, J., Pan, X., Sun, R., Ji, J., Xu, X., Liu, M., Wang, Y., and Yang, Y.
\newblock Safe {RLHF}: Safe reinforcement learning from human feedback.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=TyFrPOKYXw}.

\bibitem[Degrave et~al.(2019)Degrave, Hermans, Dambre, et~al.]{degrave2019differentiable}
Degrave, J., Hermans, M., Dambre, J., et~al.
\newblock A differentiable physics engine for deep learning in robotics.
\newblock \emph{Frontiers in neurorobotics}, pp.\ ~6, 2019.

\bibitem[ElDahshan et~al.(2022)ElDahshan, Farouk, and Mofreh]{eldahshan2022deep}
ElDahshan, K.~A., Farouk, H., and Mofreh, E.
\newblock Deep reinforcement learning based video games: A review.
\newblock In \emph{2022 2nd International Mobile, Intelligent, and Ubiquitous Computing Conference (MIUCC)}, pp.\  302--309. IEEE, 2022.

\bibitem[Freeman et~al.(2021)Freeman, Frey, Raichuk, Girgin, Mordatch, and Bachem]{freeman2021brax}
Freeman, C.~D., Frey, E., Raichuk, A., Girgin, S., Mordatch, I., and Bachem, O.
\newblock Brax - a differentiable physics engine for large scale rigid body simulation.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)}, 2021.

\bibitem[Gao et~al.(2023)Gao, Wan, Chen, Wang, Li, Jiang, Mei, Luo, and Li]{gao_review_2023}
Gao, B., Wan, K., Chen, Q., Wang, Z., Li, R., Jiang, Y., Mei, R., Luo, Y., and Li, K.
\newblock A {Review} and {Outlook} on {Predictive} {Cruise} {Control} of {Vehicles} and {Typical} {Applications} {Under} {Cloud} {Control} {System}.
\newblock \emph{Machine Intelligence Research}, 20\penalty0 (5):\penalty0 614--639, October 2023.
\newblock ISSN 2731-5398.
\newblock \doi{10.1007/s11633-022-1395-3}.
\newblock URL \url{https://doi.org/10.1007/s11633-022-1395-3}.

\bibitem[Garc{\i}a \& Fern{\'a}ndez(2015)Garc{\i}a and Fern{\'a}ndez]{garcia2015comprehensive}
Garc{\i}a, J. and Fern{\'a}ndez, F.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 16\penalty0 (1):\penalty0 1437--1480, 2015.

\bibitem[Gronauer(2022)]{gronauer2022bullet}
Gronauer, S.
\newblock Bullet-safety-gym: A framework for constrained reinforcement learning.
\newblock 2022.

\bibitem[Guin \& Bhatnagar(2023)Guin and Bhatnagar]{guin2023policy}
Guin, S. and Bhatnagar, S.
\newblock A policy gradient approach for finite horizon constrained markov decision processes.
\newblock In \emph{2023 62nd IEEE Conference on Decision and Control (CDC)}, pp.\  3353--3359. IEEE, 2023.

\bibitem[He et~al.(2024)He, Hu, Yang, and Lv]{he2024personalized}
He, X., Hu, Z., Yang, H., and Lv, C.
\newblock Personalized robotic control via constrained multi-objective reinforcement learning.
\newblock \emph{Neurocomputing}, 565:\penalty0 126986, 2024.

\bibitem[Howell et~al.(2022)Howell, Cleac'h, Br{\"u}digam, Kolter, Schwager, and Manchester]{howell2022dojo}
Howell, T.~A., Cleac'h, S.~L., Br{\"u}digam, J., Kolter, J.~Z., Schwager, M., and Manchester, Z.
\newblock Dojo: A differentiable physics engine for robotics.
\newblock \emph{arXiv preprint arXiv:2203.00806}, 2022.

\bibitem[Jaisson(2022)]{jaisson2022deep}
Jaisson, T.
\newblock Deep differentiable reinforcement learning and optimal trading.
\newblock \emph{Quantitative Finance}, 22\penalty0 (8):\penalty0 1429--1443, 2022.

\bibitem[Ji et~al.(2023{\natexlab{a}})Ji, Qiu, Chen, Zhang, Lou, Wang, Duan, He, Zhou, Zhang, et~al.]{ji2023ai}
Ji, J., Qiu, T., Chen, B., Zhang, B., Lou, H., Wang, K., Duan, Y., He, Z., Zhou, J., Zhang, Z., et~al.
\newblock Ai alignment: A comprehensive survey.
\newblock \emph{arXiv preprint arXiv:2310.19852}, 2023{\natexlab{a}}.

\bibitem[Ji et~al.(2023{\natexlab{b}})Ji, Zhang, Zhou, Pan, Huang, Sun, Geng, Zhong, Dai, and Yang]{ji2023safety}
Ji, J., Zhang, B., Zhou, J., Pan, X., Huang, W., Sun, R., Geng, Y., Zhong, Y., Dai, J., and Yang, Y.
\newblock Safety-gymnasium: A unified safe reinforcement learning benchmark.
\newblock \emph{arXiv preprint arXiv:2310.12567}, 2023{\natexlab{b}}.

\bibitem[Ji et~al.(2023{\natexlab{c}})Ji, Zhou, Zhang, Dai, Pan, Sun, Huang, Geng, Liu, and Yang]{ji2023omnisafe}
Ji, J., Zhou, J., Zhang, B., Dai, J., Pan, X., Sun, R., Huang, W., Geng, Y., Liu, M., and Yang, Y.
\newblock Omnisafe: An infrastructure for accelerating safe reinforcement learning research.
\newblock \emph{arXiv preprint arXiv:2305.09304}, 2023{\natexlab{c}}.

\bibitem[Ji et~al.(2024{\natexlab{a}})Ji, Chen, Lou, Hong, Zhang, Pan, Dai, and Yang]{ji2024aligner}
Ji, J., Chen, B., Lou, H., Hong, D., Zhang, B., Pan, X., Dai, J., and Yang, Y.
\newblock Aligner: Achieving efficient alignment through weak-to-strong correction.
\newblock \emph{arXiv preprint arXiv:2402.02416}, 2024{\natexlab{a}}.

\bibitem[Ji et~al.(2024{\natexlab{b}})Ji, Liu, Dai, Pan, Zhang, Bian, Chen, Sun, Wang, and Yang]{ji2024beavertails}
Ji, J., Liu, M., Dai, J., Pan, X., Zhang, C., Bian, C., Chen, B., Sun, R., Wang, Y., and Yang, Y.
\newblock Beavertails: Towards improved safety alignment of llm via a human-preference dataset.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024{\natexlab{b}}.

\bibitem[Jie~Xu et~al.(2022)Jie~Xu, Makoviychuk, and Narang]{Jie2022shac}
Jie~Xu, V., Makoviychuk, Y., and Narang, F.~R.
\newblock Accelerated policy learning with parallel differentiable simulation.
\newblock In \emph{ICLR}, 2022.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{kakade2002approximately}
Kakade, S. and Langford, J.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{Proceedings of the Nineteenth International Conference on Machine Learning}, ICML '02, pp.\  267â€“274, San Francisco, CA, USA, 2002. Morgan Kaufmann Publishers Inc.
\newblock ISBN 1558608737.

\bibitem[Kalagarla et~al.(2021)Kalagarla, Jain, and Nuzzo]{kalagarla2021sample}
Kalagarla, K.~C., Jain, R., and Nuzzo, P.
\newblock A sample-efficient algorithm for episodic finite-horizon mdp with constraints.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, pp.\  8030--8037, 2021.

\bibitem[Lee et~al.(2018)Lee, Yu, and Yang]{lee2018reparameterization}
Lee, W., Yu, H., and Yang, H.
\newblock Reparameterization gradient for non-differentiable models.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Liu et~al.(2022)Liu, Cen, Isenbaev, Liu, Wu, Li, and Zhao]{liu2022constrained}
Liu, Z., Cen, Z., Isenbaev, V., Liu, W., Wu, S., Li, B., and Zhao, D.
\newblock Constrained variational policy optimization for safe reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  13644--13668. PMLR, 2022.

\bibitem[Meng et~al.(2022)Meng, Zheng, Shi, and Pan]{9334437}
Meng, W., Zheng, Q., Shi, Y., and Pan, G.
\newblock An off-policy trust region policy optimization method with monotonic improvement guarantee for deep reinforcement learning.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems}, 33\penalty0 (5):\penalty0 2223--2235, 2022.
\newblock \doi{10.1109/TNNLS.2020.3044196}.

\bibitem[Metz et~al.(2021)Metz, Freeman, Schoenholz, and Kachman]{metz2021gradients}
Metz, L., Freeman, C.~D., Schoenholz, S.~S., and Kachman, T.
\newblock Gradients are not all you need.
\newblock \emph{arXiv preprint arXiv:2111.05803}, 2021.

\bibitem[Mohamed et~al.(2020)Mohamed, Rosca, Figurnov, and Mnih]{mohamed2020monte}
Mohamed, S., Rosca, M., Figurnov, M., and Mnih, A.
\newblock Monte carlo gradient estimation in machine learning.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0 (1):\penalty0 5183--5244, 2020.

\bibitem[Mora et~al.(2021)Mora, Peychev, Ha, Vechev, and Coros]{mora2021pods}
Mora, M. A.~Z., Peychev, M., Ha, S., Vechev, M., and Coros, S.
\newblock Pods: Policy optimization via differentiable simulation.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th International Conference on Machine Learning}, volume 139 of \emph{Proceedings of Machine Learning Research}, pp.\  7805--7817. PMLR, 18--24 Jul 2021.

\bibitem[Mozer(1995)]{bptt}
Mozer, M.
\newblock A focused backpropagation algorithm for temporal pattern recognition.
\newblock \emph{Complex Systems}, 3, 01 1995.

\bibitem[Mozer(2013)]{mozer2013focused}
Mozer, M.~C.
\newblock A focused backpropagation algorithm for temporal pattern recognition.
\newblock In \emph{Backpropagation}, pp.\  137--169. Psychology Press, 2013.

\bibitem[Muhammad et~al.(2020)Muhammad, Ullah, Lloret, Del~Ser, and de~Albuquerque]{muhammad2020deep}
Muhammad, K., Ullah, A., Lloret, J., Del~Ser, J., and de~Albuquerque, V. H.~C.
\newblock Deep learning for safe autonomous driving: Current challenges and future directions.
\newblock \emph{IEEE Transactions on Intelligent Transportation Systems}, 22\penalty0 (7):\penalty0 4316--4336, 2020.

\bibitem[Nilsson et~al.(2017)Nilsson, Fredriksson, and Coelingh]{NILSSON20179083}
Nilsson, J., Fredriksson, J., and Coelingh, E.
\newblock Trajectory planning with miscellaneous safety critical zones**this work was supported by ffi - strategic vehicle research and innovation.
\newblock \emph{IFAC-PapersOnLine}, 50\penalty0 (1):\penalty0 9083--9088, 2017.
\newblock ISSN 2405-8963.
\newblock \doi{https://doi.org/10.1016/j.ifacol.2017.08.1649}.
\newblock URL \url{https://www.sciencedirect.com/science/article/pii/S2405896317322541}.
\newblock 20th IFAC World Congress.

\bibitem[Okamura et~al.(2000)Okamura, Smaby, and Cutkosky]{okamura2000overview}
Okamura, A.~M., Smaby, N., and Cutkosky, M.~R.
\newblock An overview of dexterous manipulation.
\newblock In \emph{Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No. 00CH37065)}, volume~1, pp.\  255--262. IEEE, 2000.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Parmas et~al.(2023{\natexlab{a}})Parmas, Seno, and Aoki]{parmas2023model}
Parmas, P., Seno, T., and Aoki, Y.
\newblock Model-based reinforcement learning with scalable composite policy gradient estimators.
\newblock In \emph{International Conference on Machine Learning}, pp.\  27346--27377. PMLR, 2023{\natexlab{a}}.

\bibitem[Parmas et~al.(2023{\natexlab{b}})Parmas, Seno, and Aoki]{pmlr-v202-parmas23a}
Parmas, P., Seno, T., and Aoki, Y.
\newblock Model-based reinforcement learning with scalable composite policy gradient estimators.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), \emph{Proceedings of the 40th International Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  27346--27377. PMLR, 23--29 Jul 2023{\natexlab{b}}.
\newblock URL \url{https://proceedings.mlr.press/v202/parmas23a.html}.

\bibitem[Platt \& Barr(1987)Platt and Barr]{platt1987constrained}
Platt, J. and Barr, A.
\newblock Constrained differential optimization.
\newblock In \emph{Neural Information Processing Systems}, 1987.

\bibitem[Popescu et~al.(2009)Popescu, Balas, Perescu-Popescu, and Mastorakis]{popescu2009multilayer}
Popescu, M.-C., Balas, V.~E., Perescu-Popescu, L., and Mastorakis, N.
\newblock Multilayer perceptron and neural networks.
\newblock \emph{WSEAS Transactions on Circuits and Systems}, 8\penalty0 (7):\penalty0 579--588, 2009.

\bibitem[Puterman(1990)]{puterman1990markov}
Puterman, M.~L.
\newblock Markov decision processes.
\newblock \emph{Handbooks in operations research and management science}, 2:\penalty0 331--434, 1990.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Ermon, Manning, and Finn]{rafailov2023direct}
Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C.~D., and Finn, C.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock \emph{arXiv preprint arXiv:2305.18290}, 2023.

\bibitem[Ray et~al.(2019)Ray, Achiam, and Amodei]{ray2019benchmarking}
Ray, A., Achiam, J., and Amodei, D.
\newblock Benchmarking safe exploration in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.01708}, 7\penalty0 (1):\penalty0 2, 2019.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\  1889--1897. PMLR, 2015.

\bibitem[Shi et~al.(2019)Shi, Li, Cao, Yang, and Pan]{shi2019tbq}
Shi, L., Li, S., Cao, L., Yang, L., and Pan, G.
\newblock Tbq ($\sigma$): Improving efficiency of trace utilization for off-policy reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1905.07237}, 2019.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang, Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Singh et~al.(2022)Singh, Kumar, and Singh]{singh2022reinforcement}
Singh, B., Kumar, R., and Singh, V.~P.
\newblock Reinforcement learning in robotic applications: a comprehensive survey.
\newblock \emph{Artificial Intelligence Review}, pp.\  1--46, 2022.

\bibitem[Stooke et~al.(2020)Stooke, Achiam, and Abbeel]{stooke2020responsive}
Stooke, A., Achiam, J., and Abbeel, P.
\newblock Responsive safety in reinforcement learning by pid lagrangian methods.
\newblock In \emph{International Conference on Machine Learning}, pp.\  9133--9143. PMLR, 2020.

\bibitem[Suh et~al.(2022)Suh, Simchowitz, Zhang, and Tedrake]{suh2022differentiable}
Suh, H.~J., Simchowitz, M., Zhang, K., and Tedrake, R.
\newblock Do differentiable simulators give better policy gradients?
\newblock In \emph{International Conference on Machine Learning}, pp.\  20668--20696. PMLR, 2022.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Wah et~al.(2000)Wah, Wang, Shang, and Wu]{wah2000improving}
Wah, B.~W., Wang, T., Shang, Y., and Wu, Z.
\newblock Improving the performance of weighted lagrange-multiplier methods for nonlinear constrained optimization.
\newblock \emph{Information Sciences}, 124\penalty0 (1-4):\penalty0 241--272, 2000.

\bibitem[Wang et~al.(2024)Wang, Zhang, Li, Cui, and Chen]{wang2024development}
Wang, Y., Zhang, M., Li, M., Cui, H., and Chen, X.
\newblock Development of a humanoid robot control system based on ar-bci and slam navigation.
\newblock \emph{Cognitive Neurodynamics}, pp.\  1--14, 2024.

\bibitem[Werling et~al.(2021)Werling, Omens, Lee, Exarchos, and Liu]{werling2021fast}
Werling, K., Omens, D., Lee, J., Exarchos, I., and Liu, C.~K.
\newblock Fast and feature-complete differentiable physics engine for articulated rigid bodies with contact constraints.
\newblock In \emph{Robotics: Science and Systems}, 2021.

\bibitem[Xian et~al.(2023)Xian, Zhu, Xu, Tung, Torralba, Fragkiadaki, and Gan]{xian2023fluidlab}
Xian, Z., Zhu, B., Xu, Z., Tung, H.-Y., Torralba, A., Fragkiadaki, K., and Gan, C.
\newblock Fluidlab: A differentiable environment for benchmarking complex fluid manipulation.
\newblock \emph{arXiv preprint arXiv:2303.02346}, 2023.

\bibitem[Xu et~al.(2022)Xu, Liu, Huang, Ding, Cen, Li, and Zhao]{xu2022trustworthy}
Xu, M., Liu, Z., Huang, P., Ding, W., Cen, Z., Li, B., and Zhao, D.
\newblock Trustworthy reinforcement learning against intrinsic vulnerabilities: Robustness, safety, and generalizability.
\newblock \emph{arXiv preprint arXiv:2209.08025}, 2022.

\bibitem[Yang et~al.(2018)Yang, Shi, Zheng, Meng, and Pan]{yang2018unified}
Yang, L., Shi, M., Zheng, Q., Meng, W., and Pan, G.
\newblock A unified approach for multi-step temporal-difference learning with eligibility traces in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1802.03171}, 2018.

\bibitem[Yang et~al.(2022)Yang, Ji, Dai, Zhang, Zhou, Li, Yang, and Pan]{yang2022constrained}
Yang, L., Ji, J., Dai, J., Zhang, L., Zhou, B., Li, P., Yang, Y., and Pan, G.
\newblock Constrained update projection approach to safe policy optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 9111--9124, 2022.

\bibitem[Yang et~al.(2020)Yang, Rosca, Narasimhan, and Ramadge]{yang2020projection}
Yang, T.-Y., Rosca, J., Narasimhan, K., and Ramadge, P.~J.
\newblock Projection-based constrained policy optimization.
\newblock \emph{arXiv preprint arXiv:2010.03152}, 2020.

\bibitem[Zhang et~al.(2020)Zhang, Vuong, and Ross]{zhang2020first}
Zhang, Y., Vuong, Q., and Ross, K.
\newblock First order constrained optimization in policy space.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 15338--15349, 2020.

\end{thebibliography}
