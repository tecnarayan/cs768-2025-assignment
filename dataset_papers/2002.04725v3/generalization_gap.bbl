\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Athalye et~al.(2018)Athalye, Carlini, and
  Wagner]{athalye2018obfuscated}
Anish Athalye, Nicholas Carlini, and David Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock In \emph{International Conference on Machine Learning}, pages
  274--283, 2018.

\bibitem[Berry(1941)]{berry1941accuracy}
Andrew~C Berry.
\newblock The accuracy of the gaussian approximation to the sum of independent
  variates.
\newblock \emph{Transactions of the american mathematical society}, 49\penalty0
  (1):\penalty0 122--136, 1941.

\bibitem[Bhagoji et~al.(2019)Bhagoji, Cullina, and Mittal]{bhagoji2019lower}
Arjun~Nitin Bhagoji, Daniel Cullina, and Prateek Mittal.
\newblock Lower bounds on adversarial robustness from optimal transport.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  7496--7508, 2019.

\bibitem[Bubeck et~al.(2019)Bubeck, Lee, Price, and
  Razenshteyn]{bubeck2019adversarial}
Sebastien Bubeck, Yin~Tat Lee, Eric Price, and Ilya Razenshteyn.
\newblock Adversarial examples from computational constraints.
\newblock In \emph{International Conference on Machine Learning}, pages
  831--840, 2019.

\bibitem[Carlini and Wagner(2017)]{carlini2017adversarial}
Nicholas Carlini and David Wagner.
\newblock Adversarial examples are not easily detected: Bypassing ten detection
  methods.
\newblock In \emph{Proceedings of the 10th ACM Workshop on Artificial
  Intelligence and Security}, pages 3--14. ACM, 2017.

\bibitem[Carlini and Wagner(2018)]{carlini2018audio}
Nicholas Carlini and David Wagner.
\newblock Audio adversarial examples: Targeted attacks on speech-to-text.
\newblock In \emph{2018 IEEE Security and Privacy Workshops (SPW)}, pages 1--7.
  IEEE, 2018.

\bibitem[Carmon et~al.(2019)Carmon, Raghunathan, Schmidt, Duchi, and
  Liang]{carmon2019unlabeled}
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John~C Duchi, and Percy~S
  Liang.
\newblock Unlabeled data improves adversarial robustness.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  11192--11203, 2019.

\bibitem[Cohen et~al.(2019)Cohen, Rosenfeld, and Kolter]{cohen2019certified}
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter.
\newblock Certified adversarial robustness via randomized smoothing.
\newblock In \emph{International Conference on Machine Learning}, pages
  1310--1320, 2019.

\bibitem[Cullina et~al.(2018)Cullina, Bhagoji, and Mittal]{cullina2018pac}
Daniel Cullina, Arjun~Nitin Bhagoji, and Prateek Mittal.
\newblock Pac-learning in the presence of adversaries.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  230--241, 2018.

\bibitem[Diochnos et~al.(2018)Diochnos, Mahloujifar, and
  Mahmoody]{diochnos2018adversarial}
Dimitrios Diochnos, Saeed Mahloujifar, and Mohammad Mahmoody.
\newblock Adversarial risk and robustness: General definitions and implications
  for the uniform distribution.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10359--10368, 2018.

\bibitem[Diochnos et~al.(2020)Diochnos, Mahloujifar, and
  Mahmoody]{diochnos2019lower}
Dimitrios~I Diochnos, Saeed Mahloujifar, and Mohammad Mahmoody.
\newblock Lower bounds for adversarially robust pac learning.
\newblock In \emph{ISAIM}, 2020.

\bibitem[Dohmatob(2019)]{dohmatob2019generalized}
Elvis Dohmatob.
\newblock Generalized no free lunch theorem for adversarial robustness.
\newblock In \emph{International Conference on Machine Learning}, pages
  1646--1654, 2019.

\bibitem[Fawzi et~al.(2018)Fawzi, Fawzi, and Fawzi]{fawzi2018adversarial}
Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi.
\newblock Adversarial vulnerability for any classifier.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1178--1187, 2018.

\bibitem[Garg et~al.(2020)Garg, Jha, Mahloujifar, and
  Mohammad]{garg2020adversarially}
Sanjam Garg, Somesh Jha, Saeed Mahloujifar, and Mahmoody Mohammad.
\newblock Adversarially robust learning could leverage computational hardness.
\newblock In \emph{Algorithmic Learning Theory}, pages 364--385, 2020.

\bibitem[Gilmer et~al.(2018)Gilmer, Metz, Faghri, Schoenholz, Raghu,
  Wattenberg, and Goodfellow]{gilmer2018adversarial}
Justin Gilmer, Luke Metz, Fartash Faghri, Samuel~S Schoenholz, Maithra Raghu,
  Martin Wattenberg, and Ian Goodfellow.
\newblock Adversarial spheres.
\newblock In \emph{ICLR workshop}, 2018.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock In \emph{ICLR}, 2015.

\bibitem[Graybill(1961)]{graybill1961introduction}
Franklin~A Graybill.
\newblock An introduction to linear statistical models.
\newblock Technical report, 1961.

\bibitem[Gu and Rigazio(2015)]{gu2014towards}
Shixiang Gu and Luca Rigazio.
\newblock Towards deep neural network architectures robust to adversarial
  examples.
\newblock In \emph{ICLR workshop}, 2015.

\bibitem[He et~al.(2017)He, Wei, Chen, Carlini, and Song]{he2017adversarial}
Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song.
\newblock Adversarial example defense: Ensembles of weak defenses are not
  strong.
\newblock In \emph{11th USENIX Workshop on Offensive Technologies (WOOT 17)},
  2017.

\bibitem[Ilyas et~al.(2019)Ilyas, Santurkar, Tsipras, Engstrom, Tran, and
  Madry]{ilyas2019adversarial}
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon
  Tran, and Aleksander Madry.
\newblock Adversarial examples are not bugs, they are features.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  125--136, 2019.

\bibitem[Khim and Loh(2018)]{khim2018adversarial}
Justin Khim and Po-Ling Loh.
\newblock Adversarial risk bounds for binary classification via function
  transformation.
\newblock \emph{arXiv preprint arXiv:1810.09519}, 2018.

\bibitem[Kos et~al.(2018)Kos, Fischer, and Song]{kos2018adversarial}
Jernej Kos, Ian Fischer, and Dawn Song.
\newblock Adversarial examples for generative models.
\newblock In \emph{2018 IEEE Security and Privacy Workshops (SPW)}, pages
  36--42. IEEE, 2018.

\bibitem[Lecuyer et~al.(2019)Lecuyer, Atlidakis, Geambasu, Hsu, and
  Jana]{lecuyer2019certified}
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman
  Jana.
\newblock Certified robustness to adversarial examples with differential
  privacy.
\newblock In \emph{2019 IEEE Symposium on Security and Privacy (SP)}, pages
  656--672. IEEE, 2019.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{ICLR}, 2018.

\bibitem[Mahloujifar et~al.(2019)Mahloujifar, Diochnos, and
  Mahmoody]{mahloujifar2019curse}
Saeed Mahloujifar, Dimitrios~I Diochnos, and Mohammad Mahmoody.
\newblock The curse of concentration in robust learning: Evasion and poisoning
  attacks from concentration of measure.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 4536--4543, 2019.

\bibitem[Montasser et~al.(2019)Montasser, Hanneke, and Srebro]{montasser2019vc}
Omar Montasser, Steve Hanneke, and Nathan Srebro.
\newblock Vc classes are adversarially robustly learnable, but only improperly.
\newblock In \emph{Conference on Learning Theory}, pages 2512--2530, 2019.

\bibitem[Moosavi-Dezfooli et~al.(2016)Moosavi-Dezfooli, Fawzi, and
  Frossard]{moosavi2016deepfool}
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard.
\newblock Deepfool: a simple and accurate method to fool deep neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2574--2582, 2016.

\bibitem[Najafi et~al.(2019)Najafi, Maeda, Koyama, and
  Miyato]{najafi2019robustness}
Amir Najafi, Shin-ichi Maeda, Masanori Koyama, and Takeru Miyato.
\newblock Robustness to adversarial perturbations in learning from incomplete
  data.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5541--5551, 2019.

\bibitem[Nakkiran(2019)]{nakkiran2019adversarial}
Preetum Nakkiran.
\newblock Adversarial robustness may be at odds with simplicity.
\newblock \emph{arXiv preprint arXiv:1901.00532}, 2019.

\bibitem[Owhadi and Scovel(2016)]{owhadi2013brittleness}
Houman Owhadi and Clint Scovel.
\newblock Brittleness of bayesian inference and new selberg formulas.
\newblock \emph{Communications in Mathematical Sciences}, 14\penalty0
  (1):\penalty0 83--145, 2016.

\bibitem[Owhadi and Scovel(2017)]{owhadi2017qualitative}
Houman Owhadi and Clint Scovel.
\newblock Qualitative robustness in bayesian inference.
\newblock \emph{ESAIM: Probability and Statistics}, 21:\penalty0 251--274,
  2017.

\bibitem[Owhadi et~al.(2015{\natexlab{a}})Owhadi, Scovel, and
  Sullivan]{owhadi2015brittleness1}
Houman Owhadi, Clint Scovel, and Tim Sullivan.
\newblock On the brittleness of bayesian inference.
\newblock \emph{SIAM Review}, 57\penalty0 (4):\penalty0 566--582,
  2015{\natexlab{a}}.

\bibitem[Owhadi et~al.(2015{\natexlab{b}})Owhadi, Scovel, Sullivan,
  et~al.]{owhadi2015brittleness2}
Houman Owhadi, Clint Scovel, Tim Sullivan, et~al.
\newblock Brittleness of bayesian inference under finite information in a
  continuous world.
\newblock \emph{Electronic Journal of Statistics}, 9\penalty0 (1):\penalty0
  1--79, 2015{\natexlab{b}}.

\bibitem[Papernot et~al.(2016)Papernot, McDaniel, Jha, Fredrikson, Celik, and
  Swami]{papernot2016limitations}
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z~Berkay
  Celik, and Ananthram Swami.
\newblock The limitations of deep learning in adversarial settings.
\newblock In \emph{2016 IEEE European Symposium on Security and Privacy
  (EuroS\&P)}, pages 372--387. IEEE, 2016.

\bibitem[Papernot et~al.(2018)Papernot, McDaniel, Sinha, and
  Wellman]{papernot2016towards}
Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael~P Wellman.
\newblock Sok: Security and privacy in machine learning.
\newblock In \emph{2018 IEEE European Symposium on Security and Privacy
  (EuroS\&P)}, pages 399--414. IEEE, 2018.

\bibitem[Raghunathan et~al.(2018{\natexlab{a}})Raghunathan, Steinhardt, and
  Liang]{raghunathan2018certified}
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang.
\newblock Certified defenses against adversarial examples.
\newblock In \emph{ICLR}, 2018{\natexlab{a}}.

\bibitem[Raghunathan et~al.(2018{\natexlab{b}})Raghunathan, Steinhardt, and
  Liang]{raghunathan2018semidefinite}
Aditi Raghunathan, Jacob Steinhardt, and Percy~S Liang.
\newblock Semidefinite relaxations for certifying robustness to adversarial
  examples.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10877--10887, 2018{\natexlab{b}}.

\bibitem[Raghunathan et~al.(2019)Raghunathan, Xie, Yang, Duchi, and
  Liang]{raghunathan2019adversarial}
Aditi Raghunathan, Sang~Michael Xie, Fanny Yang, John~C Duchi, and Percy Liang.
\newblock Adversarial training can hurt generalization.
\newblock In \emph{ICML 2019 Workshop on Identifying and Understanding Deep
  Learning Phenomena}, 2019.

\bibitem[Raghunathan et~al.(2020)Raghunathan, Xie, Yang, Duchi, and
  Liang]{raghunathan2020understanding}
Aditi Raghunathan, Sang~Michael Xie, Fanny Yang, John Duchi, and Percy Liang.
\newblock Understanding and mitigating the tradeoff between robustness and
  accuracy.
\newblock \emph{ICML}, 2020.

\bibitem[Rudin(1964)]{rudin1964principles}
Walter Rudin.
\newblock \emph{Principles of mathematical analysis}, volume~3.
\newblock McGraw-hill New York, 1964.

\bibitem[Schmidt et~al.(2018)Schmidt, Santurkar, Tsipras, Talwar, and
  Madry]{schmidt2018adversarially}
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and
  Aleksander Madry.
\newblock Adversarially robust generalization requires more data.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5014--5026, 2018.

\bibitem[Schulz(2016)]{schulz2016}
Jona Schulz.
\newblock \emph{The optimal Berry-Esseen constant in the binomial case}.
\newblock Dissertation, Universit{\"a}t Trier, 2016.

\bibitem[Shafahi et~al.(2018)Shafahi, Huang, Studer, Feizi, and
  Goldstein]{shafahi2018adversarial}
Ali Shafahi, W~Ronny Huang, Christoph Studer, Soheil Feizi, and Tom Goldstein.
\newblock Are adversarial examples inevitable?
\newblock In \emph{ICLR}, 2018.

\bibitem[Shaham et~al.(2018)Shaham, Yamada, and
  Negahban]{shaham2018understanding}
Uri Shaham, Yutaro Yamada, and Sahand Negahban.
\newblock Understanding adversarial training: Increasing local stability of
  supervised models through robust optimization.
\newblock \emph{Neurocomputing}, 307:\penalty0 195--204, 2018.

\bibitem[Shevtsova(2014)]{shevtsova2014absolute}
Irina Shevtsova.
\newblock On the absolute constants in the berry-esseen-type inequalities.
\newblock \emph{Doklady Mathematics}, 89\penalty0 (3):\penalty0 378--381, 2014.

\bibitem[Stutz et~al.(2019)Stutz, Hein, and Schiele]{stutz2019disentangling}
David Stutz, Matthias Hein, and Bernt Schiele.
\newblock Disentangling adversarial robustness and generalization.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 6976--6987, 2019.

\bibitem[Stutz et~al.(2020)Stutz, Hein, and Schiele]{stutz2020confidence}
David Stutz, Matthias Hein, and Bernt Schiele.
\newblock Confidence-calibrated adversarial training: Generalizing to unseen
  attacks.
\newblock In \emph{ICML}, 2020.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock In \emph{ICLR}, 2014.

\bibitem[Tsipras et~al.(2019)Tsipras, Santurkar, Engstrom, Turner, and
  Madry]{tsipras2018robustness}
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
  Aleksander Madry.
\newblock Robustness may be at odds with accuracy.
\newblock In \emph{ICLR}, 2019.

\bibitem[Weng et~al.(2018)Weng, Zhang, Chen, Song, Hsieh, Daniel, Boning, and
  Dhillon]{weng2018towards}
Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel,
  Duane Boning, and Inderjit Dhillon.
\newblock Towards fast computation of certified robustness for relu networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  5276--5285, 2018.

\bibitem[Wong and Kolter(2018)]{wong2018provable}
Eric Wong and Zico Kolter.
\newblock Provable defenses against adversarial examples via the convex outer
  adversarial polytope.
\newblock In \emph{International Conference on Machine Learning}, pages
  5283--5292, 2018.

\bibitem[Xu and Mannor(2012)]{xu2012robustness}
Huan Xu and Shie Mannor.
\newblock Robustness and generalization.
\newblock \emph{Machine learning}, 86\penalty0 (3):\penalty0 391--423, 2012.

\bibitem[Yin et~al.(2019)Yin, Kannan, and Bartlett]{yin2019rademacher}
Dong Yin, Ramchandran Kannan, and Peter Bartlett.
\newblock Rademacher complexity for adversarially robust generalization.
\newblock In \emph{ICML}, pages 7085--7094, 2019.

\bibitem[Zhang et~al.(2019)Zhang, Yu, Jiao, Xing, El~Ghaoui, and
  Jordan]{zhang2019theoretically}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El~Ghaoui, and
  Michael Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In \emph{International Conference on Machine Learning}, pages
  7472--7482, 2019.

\bibitem[Zhang et~al.(2018)Zhang, Weng, Chen, Hsieh, and
  Daniel]{zhang2018efficient}
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel.
\newblock Efficient neural network robustness certification with general
  activation functions.
\newblock In \emph{Advances in neural information processing systems}, pages
  4939--4948, 2018.

\bibitem[Zhang et~al.(2020)Zhang, Xu, Han, Niu, Cui, Sugiyama, and
  Kankanhalli]{zhang2020attacks}
Jingfeng Zhang, Xilie Xu, Bo~Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and
  Mohan Kankanhalli.
\newblock Attacks which do not kill training make adversarial learning
  stronger.
\newblock In \emph{ICML}, 2020.

\end{thebibliography}
