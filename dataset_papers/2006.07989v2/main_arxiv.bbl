% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{alexnet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``Imagenet classification with
  deep convolutional neural networks,'' in \emph{Advances in neural information
  processing systems}, 2012, pp. 1097--1105.

\bibitem{resnet}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{Proceedings of the IEEE conference on computer vision
  and pattern recognition}, 2016, pp. 770--778.

\bibitem{zhuo2020cogradient}
L.~Zhuo, B.~Zhang, L.~Yang, H.~Chen, Q.~Ye, D.~Doermann, R.~Ji, and G.~Guo,
  ``Cogradient descent for bilinear optimization,'' in \emph{Proceedings of the
  IEEE Conference on Computer Vision and Pattern Recognition}, 2020, pp.
  7959--7967.

\bibitem{jiang2020multi}
K.~Jiang, Z.~Wang, P.~Yi, C.~Chen, B.~Huang, Y.~Luo, J.~Ma, and J.~Jiang,
  ``Multi-scale progressive fusion network for single image deraining,'' in
  \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition}, 2020, pp. 8346--8355.

\bibitem{ren2015faster}
S.~Ren, K.~He, R.~Girshick, and J.~Sun, ``Faster r-cnn: Towards real-time
  object detection with region proposal networks,'' in \emph{Advances in neural
  information processing systems}, 2015, pp. 91--99.

\bibitem{he2017mask}
K.~He, G.~Gkioxari, P.~Doll{\'a}r, and R.~Girshick, ``Mask r-cnn,'' in
  \emph{Proceedings of the IEEE international conference on computer vision},
  2017, pp. 2961--2969.

\bibitem{chen2017deeplab}
L.-C. Chen, G.~Papandreou, I.~Kokkinos, K.~Murphy, and A.~L. Yuille, ``Deeplab:
  Semantic image segmentation with deep convolutional nets, atrous convolution,
  and fully connected crfs,'' \emph{IEEE transactions on pattern analysis and
  machine intelligence}, vol.~40, no.~4, pp. 834--848, 2017.

\bibitem{fcn}
J.~Long, E.~Shelhamer, and T.~Darrell, ``Fully convolutional networks for
  semantic segmentation,'' in \emph{Proceedings of the IEEE conference on
  computer vision and pattern recognition}, 2015, pp. 3431--3440.

\bibitem{dropout}
\BIBentryALTinterwordspacing
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov,
  ``Dropout: A simple way to prevent neural networks from overfitting,''
  \emph{Journal of Machine Learning Research}, vol.~15, no.~56, pp. 1929--1958,
  2014. [Online]. Available:
  \url{http://jmlr.org/papers/v15/srivastava14a.html}
\BIBentrySTDinterwordspacing

\bibitem{cutout}
T.~DeVries and G.~W. Taylor, ``Improved regularization of convolutional neural
  networks with cutout,'' \emph{arXiv preprint arXiv:1708.04552}, 2017.

\bibitem{bestpractices}
P.~Y. Simard, D.~Steinkraus, J.~C. Platt \emph{et~al.}, ``Best practices for
  convolutional neural networks applied to visual document analysis.'' in
  \emph{Icdar}, vol.~3, no. 2003, 2003.

\bibitem{zhang2018mixup}
\BIBentryALTinterwordspacing
H.~Zhang, M.~Cisse, Y.~N. Dauphin, and D.~Lopez-Paz, ``mixup: Beyond empirical
  risk minimization,'' in \emph{International Conference on Learning
  Representations}, 2018. [Online]. Available:
  \url{https://openreview.net/forum?id=r1Ddp1-Rb}
\BIBentrySTDinterwordspacing

\bibitem{yun2019cutmix}
S.~Yun, D.~Han, S.~J. Oh, S.~Chun, J.~Choe, and Y.~Yoo, ``Cutmix:
  Regularization strategy to train strong classifiers with localizable
  features,'' in \emph{Proceedings of the IEEE International Conference on
  Computer Vision}, 2019, pp. 6023--6032.

\bibitem{gradnoise}
A.~Neelakantan, L.~Vilnis, Q.~V. Le, I.~Sutskever, L.~Kaiser, K.~Kurach, and
  J.~Martens, ``Adding gradient noise improves learning for very deep
  networks,'' \emph{arXiv preprint arXiv:1511.06807}, 2015.

\bibitem{stochdepth}
G.~Huang, Y.~Sun, Z.~Liu, D.~Sedra, and K.~Q. Weinberger, ``Deep networks with
  stochastic depth,'' in \emph{European conference on computer vision}.\hskip
  1em plus 0.5em minus 0.4em\relax Springer, 2016, pp. 646--661.

\bibitem{larsson2016fractalnet}
G.~Larsson, M.~Maire, and G.~Shakhnarovich, ``Fractalnet: Ultra-deep neural
  networks without residuals,'' \emph{arXiv preprint arXiv:1605.07648}, 2016.

\bibitem{cubuk2019autoaugment}
E.~D. Cubuk, B.~Zoph, D.~Mane, V.~Vasudevan, and Q.~V. Le, ``Autoaugment:
  Learning augmentation strategies from data,'' in \emph{Proceedings of the
  IEEE conference on computer vision and pattern recognition}, 2019, pp.
  113--123.

\bibitem{ghiasi2018dropblock}
G.~Ghiasi, T.-Y. Lin, and Q.~V. Le, ``Dropblock: A regularization method for
  convolutional networks,'' in \emph{Advances in Neural Information Processing
  Systems}, 2018, pp. 10\,727--10\,737.

\bibitem{mixed1}
C.~Summers and M.~J. Dinneen, ``Improved mixed-example data augmentation,'' in
  \emph{2019 IEEE Winter Conference on Applications of Computer Vision
  (WACV)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2019, pp. 1262--1270.

\bibitem{mixed2}
R.~Takahashi, T.~Matsubara, and K.~Uehara, ``Ricap: Random image cropping and
  patching data augmentation for deep cnns,'' in \emph{Asian Conference on
  Machine Learning}, 2018, pp. 786--798.

\bibitem{shakeshake}
X.~Gastaldi, ``Shake-shake regularization,'' \emph{arXiv preprint
  arXiv:1705.07485}, 2017.

\bibitem{yamada2019shakedrop}
Y.~Yamada, M.~Iwamura, T.~Akiba, and K.~Kise, ``Shakedrop regularization for
  deep residual learning,'' \emph{IEEE Access}, vol.~7, pp. 186\,126--186\,136,
  2019.

\bibitem{pyramidnet}
D.~Han, J.~Kim, and J.~Kim, ``Deep pyramidal residual networks,'' in
  \emph{Proceedings of the IEEE conference on computer vision and pattern
  recognition}, 2017, pp. 5927--5935.

\bibitem{cam}
B.~Zhou, A.~Khosla, A.~Lapedriza, A.~Oliva, and A.~Torralba, ``Learning deep
  features for discriminative localization,'' in \emph{Proceedings of the IEEE
  conference on computer vision and pattern recognition}, 2016, pp. 2921--2929.

\bibitem{bn}
S.~Ioffe and C.~Szegedy, ``Batch normalization: Accelerating deep network
  training by reducing internal covariate shift,'' \emph{arXiv preprint
  arXiv:1502.03167}, 2015.

\bibitem{usnet}
J.~Yu and T.~S. Huang, ``Universally slimmable networks and improved training
  techniques,'' in \emph{Proceedings of the IEEE International Conference on
  Computer Vision}, 2019, pp. 1803--1811.

\bibitem{imagenet}
J.~{Deng}, W.~{Dong}, R.~{Socher}, L.~{Li}, {Kai Li}, and {Li Fei-Fei},
  ``Imagenet: A large-scale hierarchical image database,'' in \emph{2009 IEEE
  Conference on Computer Vision and Pattern Recognition}, 2009, pp. 248--255.

\bibitem{bagoftricks}
T.~He, Z.~Zhang, H.~Zhang, Z.~Zhang, J.~Xie, and M.~Li, ``Bag of tricks for
  image classification with convolutional neural networks,'' in
  \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition}, 2019, pp. 558--567.

\bibitem{cifar}
A.~Krizhevsky \emph{et~al.}, ``Learning multiple layers of features from tiny
  images,'' 2009.

\bibitem{zagoruyko2016wide}
S.~Zagoruyko and N.~Komodakis, ``Wide residual networks,'' \emph{arXiv preprint
  arXiv:1605.07146}, 2016.

\bibitem{coco}
T.-Y. Lin, M.~Maire, S.~Belongie, J.~Hays, P.~Perona, D.~Ramanan,
  P.~Doll{\'a}r, and C.~L. Zitnick, ``Microsoft coco: Common objects in
  context,'' in \emph{European conference on computer vision}.\hskip 1em plus
  0.5em minus 0.4em\relax Springer, 2014, pp. 740--755.

\bibitem{fpn}
T.-Y. Lin, P.~Doll{\'a}r, R.~Girshick, K.~He, B.~Hariharan, and S.~Belongie,
  ``Feature pyramid networks for object detection,'' in \emph{Proceedings of
  the IEEE conference on computer vision and pattern recognition}, 2017, pp.
  2117--2125.

\bibitem{chen2019mmdetection}
K.~Chen, J.~Wang, J.~Pang, Y.~Cao, Y.~Xiong, X.~Li, S.~Sun, W.~Feng, Z.~Liu,
  J.~Xu \emph{et~al.}, ``Mmdetection: Open mmlab detection toolbox and
  benchmark,'' \emph{arXiv preprint arXiv:1906.07155}, 2019.

\bibitem{imagecorruption}
D.~Hendrycks and T.~Dietterich, ``Benchmarking neural network robustness to
  common corruptions and perturbations,'' \emph{arXiv preprint
  arXiv:1903.12261}, 2019.

\bibitem{fgsm}
I.~J. Goodfellow, J.~Shlens, and C.~Szegedy, ``Explaining and harnessing
  adversarial examples,'' \emph{arXiv preprint arXiv:1412.6572}, 2014.

\bibitem{meanteacher}
A.~Tarvainen and H.~Valpola, ``Mean teachers are better role models:
  Weight-averaged consistency targets improve semi-supervised deep learning
  results,'' in \emph{Advances in neural information processing systems}, 2017,
  pp. 1195--1204.

\bibitem{ssleval}
A.~Oliver, A.~Odena, C.~A. Raffel, E.~D. Cubuk, and I.~Goodfellow, ``Realistic
  evaluation of deep semi-supervised learning algorithms,'' in \emph{Advances
  in Neural Information Processing Systems}, 2018, pp. 3235--3246.

\bibitem{stl10}
\BIBentryALTinterwordspacing
A.~Coates, A.~Ng, and H.~Lee, ``An analysis of single-layer networks in
  unsupervised feature learning,'' in \emph{Proceedings of the Fourteenth
  International Conference on Artificial Intelligence and Statistics}, ser.
  Proceedings of Machine Learning Research, G.~Gordon, D.~Dunson, and
  M.~Dud√≠k, Eds., vol.~15.\hskip 1em plus 0.5em minus 0.4em\relax Fort
  Lauderdale, FL, USA: PMLR, 11--13 Apr 2011, pp. 215--223. [Online].
  Available: \url{http://proceedings.mlr.press/v15/coates11a.html}
\BIBentrySTDinterwordspacing

\bibitem{teacherfreekd}
L.~Yuan, F.~E. Tay, G.~Li, T.~Wang, and J.~Feng, ``Revisiting knowledge
  distillation via label smoothing regularization,'' in \emph{Proceedings of
  the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2020,
  pp. 3903--3911.

\bibitem{kingma2014adam}
D.~P. Kingma and J.~Ba, ``Adam: A method for stochastic optimization,''
  \emph{arXiv preprint arXiv:1412.6980}, 2014.

\end{thebibliography}
