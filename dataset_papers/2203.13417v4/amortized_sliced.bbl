\begin{thebibliography}{10}

\bibitem{altschuler2017near}
J.~Altschuler, J.~Niles-Weed, and P.~Rigollet.
\newblock Near-linear time approximation algorithms for optimal transport via
  {S}inkhorn iteration.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1964--1974, 2017.

\bibitem{amos2022tutorial}
B.~Amos.
\newblock Tutorial on amortized optimization for learning to optimize over
  continuous domains.
\newblock {\em arXiv preprint arXiv:2202.00665}, 2022.

\bibitem{arjovsky2017wasserstein}
M.~Arjovsky, S.~Chintala, and L.~Bottou.
\newblock Wasserstein generative adversarial networks.
\newblock In {\em International Conference on Machine Learning}, pages
  214--223, 2017.

\bibitem{bonneel2015sliced}
N.~Bonneel, J.~Rabin, G.~Peyr{\'e}, and H.~Pfister.
\newblock Sliced and {R}adon {W}asserstein barycenters of measures.
\newblock {\em Journal of Mathematical Imaging and Vision}, 1(51):22--45, 2015.

\bibitem{chang2015structural}
K.-W. Chang, S.~Upadhyay, G.~Kundu, and D.~Roth.
\newblock Structural learning with amortized inference.
\newblock In {\em Twenty-Ninth AAAI Conference on Artificial Intelligence},
  2015.

\bibitem{chen2022augmented}
X.~Chen, Y.~Yang, and Y.~Li.
\newblock Augmented sliced {W}asserstein distances.
\newblock {\em International Conference on Learning Representations}, 2022.

\bibitem{coates2011analysis}
A.~Coates, A.~Ng, and H.~Lee.
\newblock An analysis of single-layer networks in unsupervised feature
  learning.
\newblock In {\em Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pages 215--223. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem{cuturi2013sinkhorn}
M.~Cuturi.
\newblock Sinkhorn distances: Lightspeed computation of optimal transport.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2292--2300, 2013.

\bibitem{dai2021sliced}
B.~Dai and U.~Seljak.
\newblock Sliced iterative normalizing flows.
\newblock In {\em International Conference on Machine Learning}, pages
  2352--2364. PMLR, 2021.

\bibitem{deshpande2019max}
I.~Deshpande, Y.-T. Hu, R.~Sun, A.~Pyrros, N.~Siddiqui, S.~Koyejo, Z.~Zhao,
  D.~Forsyth, and A.~G. Schwing.
\newblock Max-sliced {W}asserstein distance and its use for {GAN}s.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 10648--10656, 2019.

\bibitem{deshpande2018generative}
I.~Deshpande, Z.~Zhang, and A.~G. Schwing.
\newblock Generative modeling using the sliced {W}asserstein distance.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3483--3491, 2018.

\bibitem{fatras2020learning}
K.~Fatras, Y.~Zine, R.~Flamary, R.~Gribonval, and N.~Courty.
\newblock Learning with minibatch {W}asserstein: asymptotic and gradient
  properties.
\newblock In {\em AISTATS 2020-23nd International Conference on Artificial
  Intelligence and Statistics}, volume 108, pages 1--20, 2020.

\bibitem{fatras2021minibatch}
K.~Fatras, Y.~Zine, S.~Majewski, R.~Flamary, R.~Gribonval, and N.~Courty.
\newblock Minibatch optimal transport distances; analysis and applications.
\newblock {\em arXiv preprint arXiv:2101.01792}, 2021.

\bibitem{genevay2018learning}
A.~Genevay, G.~Peyr{\'e}, and M.~Cuturi.
\newblock Learning generative models with {S}inkhorn divergences.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1608--1617. PMLR, 2018.

\bibitem{gershman2014amortized}
S.~Gershman and N.~Goodman.
\newblock Amortized inference in probabilistic reasoning.
\newblock In {\em Proceedings of the Annual Meeting of the Cognitive Science
  Society}, volume~36, 2014.

\bibitem{goldfeld2021sliced}
Z.~Goldfeld and K.~Greenewald.
\newblock Sliced mutual information: A scalable measure of statistical
  dependence.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{goodfellow2014generative}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2672--2680, 2014.

\bibitem{heusel2017gans}
M.~Heusel, H.~Ramsauer, T.~Unterthiner, B.~Nessler, and S.~Hochreiter.
\newblock {GAN}s trained by a two time-scale update rule converge to a local
  {N}ash equilibrium.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6626--6637, 2017.

\bibitem{ho2020denoising}
J.~Ho, A.~Jain, and P.~Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6840--6851, 2020.

\bibitem{huang2021riemannian}
M.~Huang, S.~Ma, and L.~Lai.
\newblock A {R}iemannian block coordinate descent method for computing the
  projection robust {W}asserstein distance.
\newblock In {\em International Conference on Machine Learning}, pages
  4446--4455. PMLR, 2021.

\bibitem{pmlr-v139-huang21e}
M.~Huang, S.~Ma, and L.~Lai.
\newblock A riemannian block coordinate descent method for computing the
  projection robust wasserstein distance.
\newblock In M.~Meila and T.~Zhang, editors, {\em Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of {\em Proceedings
  of Machine Learning Research}, pages 4446--4455. PMLR, 18--24 Jul 2021.

\bibitem{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kingma2013auto}
D.~P. Kingma and M.~Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{kolouri2019generalized}
S.~Kolouri, K.~Nadjahi, U.~Simsekli, R.~Badeau, and G.~Rohde.
\newblock Generalized sliced {W}asserstein distances.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  261--272, 2019.

\bibitem{kolouri2018sliced}
S.~Kolouri, P.~E. Pope, C.~E. Martin, and G.~K. Rohde.
\newblock Sliced {W}asserstein auto-encoders.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock {\em Master's thesis, Department of Computer Science, University of
  Toronto}, 2009.

\bibitem{lezama2021run}
J.~Lezama, W.~Chen, and Q.~Qiu.
\newblock Run-sort-rerun: Escaping batch size limitations in sliced
  {W}asserstein generative models.
\newblock In {\em International Conference on Machine Learning}, pages
  6275--6285. PMLR, 2021.

\bibitem{lin2020projection}
T.~Lin, C.~Fan, N.~Ho, M.~Cuturi, and M.~Jordan.
\newblock Projection robust {W}asserstein distance and {R}iemannian
  optimization.
\newblock {\em Advances in Neural Information Processing Systems},
  33:9383--9397, 2020.

\bibitem{Lin-2020-Revisiting}
T.~Lin, N.~Ho, X.~Chen, M.~Cuturi, and M.~I. Jordan.
\newblock Fixed-support {W}asserstein barycenters: Computational hardness and
  fast algorithm.
\newblock In {\em NeurIPS}, pages 5368--5380, 2020.

\bibitem{lin2019efficient}
T.~Lin, N.~Ho, and M.~Jordan.
\newblock On efficient optimal transport: An analysis of greedy and accelerated
  mirror descent algorithms.
\newblock In {\em International Conference on Machine Learning}, pages
  3982--3991, 2019.

\bibitem{Lin-2019-Efficiency}
T.~Lin, N.~Ho, and M.~I. Jordan.
\newblock On the efficiency of the {S}inkhorn and {G}reenkhorn algorithms and
  their acceleration for optimal transport.
\newblock {\em ArXiv Preprint: 1906.01437}, 2019.

\bibitem{liu2015faceattributes}
Z.~Liu, P.~Luo, X.~Wang, and X.~Tang.
\newblock Deep learning face attributes in the wild.
\newblock In {\em Proceedings of International Conference on Computer Vision
  (ICCV)}, December 2015.

\bibitem{marino2018iterative}
J.~Marino, Y.~Yue, and S.~Mandt.
\newblock Iterative amortized inference.
\newblock In {\em International Conference on Machine Learning}, pages
  3403--3412. PMLR, 2018.

\bibitem{Mena_2019}
G.~Mena and J.~Weed.
\newblock Statistical bounds for entropic optimal transport: sample complexity
  and the central limit theorem.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{miyato2018spectral}
T.~Miyato, T.~Kataoka, M.~Koyama, and Y.~Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock {\em arXiv preprint arXiv:1802.05957}, 2018.

\bibitem{nadjahi2020statistical}
K.~Nadjahi, A.~Durmus, L.~Chizat, S.~Kolouri, S.~Shahrampour, and U.~Simsekli.
\newblock Statistical and topological properties of sliced probability
  divergences.
\newblock {\em Advances in Neural Information Processing Systems},
  33:20802--20812, 2020.

\bibitem{nadjahi2021fast}
K.~Nadjahi, A.~Durmus, P.~E. Jacob, R.~Badeau, and U.~Simsekli.
\newblock Fast approximation of the sliced-{W}asserstein distance using
  concentration of random projections.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{nadjahi2019asymptotic}
K.~Nadjahi, A.~Durmus, U.~Simsekli, and R.~Badeau.
\newblock Asymptotic guarantees for learning generative models with the
  sliced-{W}asserstein distance.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{nguyen2021distributional}
K.~Nguyen, N.~Ho, T.~Pham, and H.~Bui.
\newblock Distributional sliced-{W}asserstein and applications to generative
  modeling.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{nguyen2022transportation}
K.~Nguyen, D.~Nguyen, Q.~Nguyen, T.~Pham, H.~Bui, D.~Phung, T.~Le, and N.~Ho.
\newblock On transportation of mini-batches: A hierarchical approach.
\newblock In {\em Proceedings of the 39th International Conference on Machine
  Learning}, 2022.

\bibitem{nguyen2022improving}
K.~Nguyen, D.~Nguyen, T.~Pham, and N.~Ho.
\newblock Improving mini-batch optimal transport via partial transportation.
\newblock In {\em Proceedings of the 39th International Conference on Machine
  Learning}, 2022.

\bibitem{nguyen2021improving}
K.~Nguyen, S.~Nguyen, N.~Ho, T.~Pham, and H.~Bui.
\newblock Improving relational regularized autoencoders with spherical sliced
  fused {G}romov-{W}asserstein.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{nowozin2016f}
S.~Nowozin, B.~Cseke, and R.~Tomioka.
\newblock f-gan: Training generative neural samplers using variational
  divergence minimization.
\newblock {\em Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem{paty2019subspace}
F.-P. Paty and M.~Cuturi.
\newblock Subspace robust {W}asserstein distances.
\newblock In {\em International Conference on Machine Learning}, pages
  5072--5081, 2019.

\bibitem{peyre2019computational}
G.~Peyr{\'e} and M.~Cuturi.
\newblock Computational optimal transport: With applications to data science.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  11(5-6):355--607, 2019.

\bibitem{peyre2020computational}
G.~Peyré and M.~Cuturi.
\newblock Computational optimal transport, 2020.

\bibitem{rezende2014stochastic}
D.~J. Rezende, S.~Mohamed, and D.~Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In {\em International Conference on Machine Learning}, pages
  1278--1286. PMLR, 2014.

\bibitem{ritchie2016deep}
D.~Ritchie, P.~Horsfall, and N.~D. Goodman.
\newblock Deep amortized inference for probabilistic programs.
\newblock {\em arXiv preprint arXiv:1610.05735}, 2016.

\bibitem{rowland2019orthogonal}
M.~Rowland, J.~Hron, Y.~Tang, K.~Choromanski, T.~Sarlos, and A.~Weller.
\newblock Orthogonal estimation of {W}asserstein distances.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 186--195. PMLR, 2019.

\bibitem{salimans2016improved}
T.~Salimans, I.~Goodfellow, W.~Zaremba, V.~Cheung, A.~Radford, and X.~Chen.
\newblock Improved techniques for training {GAN}s.
\newblock {\em Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem{salimans2018improving}
T.~Salimans, H.~Zhang, A.~Radford, and D.~Metaxas.
\newblock Improving {GAN}s using optimal transport.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{ruishu2017}
R.~Shu.
\newblock Amortized optimization
  \url{http://ruishu.io/2017/11/07/amortized-optimization/}.
\newblock {\em Personal Blog}, 2017.

\bibitem{shu2018amortized}
R.~Shu, H.~H. Bui, S.~Zhao, M.~J. Kochenderfer, and S.~Ermon.
\newblock Amortized inference regularization.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{song2019generative}
Y.~Song and S.~Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{stanczuk2021wasserstein}
J.~Stanczuk, C.~Etmann, L.~M. Kreusser, and C.-B. Sch{\"o}nlieb.
\newblock Wasserstein {GAN}s work because they fail (to approximate the
  {W}asserstein distance).
\newblock {\em arXiv preprint arXiv:2103.01678}, 2021.

\bibitem{Villani-09}
C.~Villani.
\newblock {\em Optimal transport: Old and New}.
\newblock Springer, 2008.

\bibitem{wu2019sliced}
J.~Wu, Z.~Huang, D.~Acharya, W.~Li, J.~Thoma, D.~P. Paudel, and L.~V. Gool.
\newblock Sliced {W}asserstein generative models.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3713--3722, 2019.

\bibitem{wu2020meta}
M.~Wu, K.~Choi, N.~Goodman, and S.~Ermon.
\newblock Meta-amortized variational inference and learning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 6404--6412, 2020.

\end{thebibliography}
