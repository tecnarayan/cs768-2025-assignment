
@article{dettmers2023spqr,
  title={SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression},
  author={Dettmers, Tim and Svirschevski, Ruslan and Egiazarian, Vage and Kuznedelev, Denis and Frantar, Elias and Ashkboos, Saleh and Borzunov, Alexander and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2306.03078},
  year={2023}
}

@article{dettmers2023qlora,
  title={{QLoRA}: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023}
}

@article{Qian1999OnTM,
  title={On the momentum term in gradient descent learning algorithms},
  author={N. Qian},
  journal={Neural networks : the official journal of the International Neural Network Society},
  year={1999},
  volume={12 1},
  pages={
          145-151
        }
}
% https://huggingface.co/tiiuae/falcon-40b
@misc{yao2023comprehensive,
      title={A Comprehensive Study on Post-Training Quantization for Large Language Models}, 
      author={Zhewei Yao and Cheng Li and Xiaoxia Wu and Stephen Youn and Yuxiong He},
      year={2023},
      eprint={2303.08302},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{zeng2022glm,
  title={GLM-130B: An Open Bilingual Pre-trained Model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}



@article{hinton2012neural,
  title={Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
  author={Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  journal={Cited on},
  volume={14},
  number={8},
  year={2012}
}

@article{dettmers20168bit,
  title={8-bit approximations for parallelism in deep learning},
  author={Dettmers, Tim},
  journal={International Conference on Learning Representations (ICLR)},
  year={2016}
}

@inproceedings{seetharaman2020autoclip,
  title={AutoClip: Adaptive gradient clipping for source separation networks},
  author={Seetharaman, Prem and Wichern, Gordon and Pardo, Bryan and Le Roux, Jonathan},
  booktitle={2020 IEEE 30th International Workshop on Machine Learning for Signal Processing (MLSP)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}

@article{brock2021high,
  title={High-Performance Large-Scale Image Recognition Without Normalization},
  author={Brock, Andrew and De, Soham and Smith, Samuel L and Simonyan, Karen},
  journal={arXiv preprint arXiv:2102.06171},
  year={2021}
}

@article{rmsnorm,
  author       = {Biao Zhang and
                  Rico Sennrich},
  title        = {Root Mean Square Layer Normalization},
  journal      = {CoRR},
  volume       = {abs/1910.07467},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.07467},
  eprinttype    = {arXiv},
  eprint       = {1910.07467},
  timestamp    = {Fri, 21 Oct 2022 14:36:42 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-07467.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@inproceedings{zhu2015aligning,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}

@misc{nagel2016cc,
  title={Cc-news},
  author={Nagel, Sebastian},
  year={2016}
}

@article{trinh2018simple,
  title={A simple method for commonsense reasoning},
  author={Trinh, Trieu H and Le, Quoc V},
  journal={arXiv preprint arXiv:1806.02847},
  year={2018}
}

@article{gokaslan2019openwebtext,
  title={Openwebtext corpus},
  author={Gokaslan, Aaron and Cohen, Vanya},
  journal={urlhttp://Skylion007. github. io/OpenWebTextCorpus},
  year={2019}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}

@article{dettmers2019sparse,
  title={Sparse networks from scratch: Faster training without losing performance},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1907.04840},
  year={2019}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  pages={1097--1105},
  year={2012}
}

@article{baevski2018adaptive,
  title={Adaptive input representations for neural language modeling},
  author={Baevski, Alexei and Auli, Michael},
  journal={arXiv preprint arXiv:1809.10853},
  year={2018}
}

@article{loshchilov2018fixing,
  title={Fixing weight decay regularization in adam},
  author={Loshchilov, Ilya and Hutter, Frank},
  year={2018}
}

@article{ott2019fairseq,
  title={fairseq: A fast, extensible toolkit for sequence modeling},
  author={Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
  journal={arXiv preprint arXiv:1904.01038},
  year={2019}
}

@article{wolf2019huggingface,
  title={HuggingFace's Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@article{ott2018scaling,
  title={Scaling neural machine translation},
  author={Ott, Myle and Edunov, Sergey and Grangier, David and Auli, Michael},
  journal={arXiv preprint arXiv:1806.00187},
  year={2018}
}

@article{lewis2021base,
  title={BASE Layers: Simplifying Training of Large, Sparse Models},
  author={Lewis, Mike and Bhosale, Shruti and Dettmers, Tim and Goyal, Naman and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2103.16716},
  year={2021}
}

@article{zhang2022opt,
  title={OPT: Open Pre-trained Transformer Language Models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{chen2020statistical,
  title={A statistical framework for low-bitwidth training of deep neural networks},
  author={Chen, Jianfei and Gai, Yu and Yao, Zhewei and Mahoney, Michael W and Gonzalez, Joseph E},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={883--894},
  year={2020}
}

@article{lin2020towards,
  title={Towards fully 8-bit integer inference for the transformer model},
  author={Lin, Ye and Li, Yanyang and Liu, Tengbo and Xiao, Tong and Liu, Tongran and Zhu, Jingbo},
  journal={arXiv preprint arXiv:2009.08034},
  year={2020}
}

@inproceedings{voita-etal-2019-analyzing,
    title = "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
    author = "Voita, Elena  and
      Talbot, David  and
      Moiseev, Fedor  and
      Sennrich, Rico  and
      Titov, Ivan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1580",
    doi = "10.18653/v1/P19-1580",
    pages = "5797--5808",
    abstract = "Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.",
}

@inproceedings{zafrir2019q8bert,
  title={Q8bert: Quantized 8bit bert},
  author={Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe},
  booktitle={2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)},
  pages={36--39},
  year={2019},
  organization={IEEE}
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{dettmers2022optimizers,
  title={8-bit Optimizers via Block-wise Quantization},
  author={Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Zettlemoyer, Luke},
  journal={9th International Conference on Learning Representations, ICLR},
  year={2022}
}

@article{artetxe2021efficient,
  title={Efficient Large Scale Language Modeling with Mixtures of Experts},
  author={Artetxe, Mikel and Bhosale, Shruti and Goyal, Naman and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Lin, Xi Victoria and Du, Jingfei and Iyer, Srinivasan and Pasunuru, Ramakanth and others},
  journal={arXiv preprint arXiv:2112.10684},
  year={2021}
}



@article{fan2020training,
  title={Training with quantization noise for extreme model compression},
  author={Fan, Angela and Stock, Pierre and Graham, Benjamin and Grave, Edouard and Gribonval, R{\'e}mi and Jegou, Herve and Joulin, Armand},
  journal={arXiv preprint arXiv:2004.07320},
  year={2020}
}

@article{PQ,
  title={Product quantization for nearest neighbor search},
  author={Jegou, Herve and Douze, Matthijs and Schmid, Cordelia},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={33},
  number={1},
  pages={117--128},
  year={2010},
  publisher={IEEE}
}

@article{jin2022f8net,
  title={F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization},
  author={Jin, Qing and Ren, Jian and Zhuang, Richard and Hanumante, Sumant and Li, Zhengang and Chen, Zhiyu and Wang, Yanzhi and Yang, Kaiyuan and Tulyakov, Sergey},
  journal={arXiv preprint arXiv:2202.05239},
  year={2022}
}

@inproceedings{Zhang2020TernaryBERTDU,
  title={TernaryBERT: Distillation-aware Ultra-low Bit BERT},
  author={Wei Zhang and Lu Hou and Yichun Yin and Lifeng Shang and Xiao Chen and Xin Jiang and Qun Liu},
  booktitle={EMNLP},
  year={2020}
}


@article{Bai2021BinaryBERTPT,
  title={BinaryBERT: Pushing the Limit of BERT Quantization},
  author={Haoli Bai and Wei Zhang and Lu Hou and Lifeng Shang and Jing Jin and Xin Jiang and Qun Liu and Michael R. Lyu and Irwin King},
  journal={ArXiv},
  year={2021},
  volume={abs/2012.15701}
}


@inproceedings{Rastegari2016xnor,
  author    = {Mohammad Rastegari and
               Vicente Ordonez and
               Joseph Redmon and
               Ali Farhadi},
  editor    = {Bastian Leibe and
               Jiri Matas and
               Nicu Sebe and
               Max Welling},
  title     = {XNOR-Net: ImageNet Classification Using Binary Convolutional Neural
               Networks},
  booktitle = {Computer Vision - {ECCV} 2016 - 14th European Conference, Amsterdam,
               The Netherlands, October 11-14, 2016, Proceedings, Part {IV}},
  series    = {Lecture Notes in Computer Science},
  volume    = {9908},
  pages     = {525--542},
  publisher = {Springer},
  year      = {2016},
  url       = {https://doi.org/10.1007/978-3-319-46493-0\_32},
  doi       = {10.1007/978-3-319-46493-0\_32},
  timestamp = {Wed, 25 Sep 2019 18:11:12 +0200},
  biburl    = {https://dblp.org/rec/conf/eccv/RastegariORF16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{courbariaux2015binaryconnect,
  author    = {Matthieu Courbariaux and
               Yoshua Bengio and
               Jean{-}Pierre David},
  editor    = {Corinna Cortes and
               Neil D. Lawrence and
               Daniel D. Lee and
               Masashi Sugiyama and
               Roman Garnett},
  title     = {BinaryConnect: Training Deep Neural Networks with binary weights during
               propagations},
  booktitle = {Advances in Neural Information Processing Systems 28: Annual Conference
               on Neural Information Processing Systems 2015, December 7-12, 2015,
               Montreal, Quebec, Canada},
  pages     = {3123--3131},
  year      = {2015},
  url       = {https://proceedings.neurips.cc/paper/2015/hash/3e15cc11f979ed25912dff5b0669f2cd-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:22 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/CourbariauxBD15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{courbariaux2016bit1,
  author    = {Matthieu Courbariaux and
               Yoshua Bengio},
  title     = {BinaryNet: Training Deep Neural Networks with Weights and Activations
               Constrained to +1 or -1},
  journal   = {CoRR},
  volume    = {abs/1602.02830},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.02830},
  eprinttype = {arXiv},
  eprint    = {1602.02830},
  timestamp = {Mon, 13 Aug 2018 16:46:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/CourbariauxB16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{wang2018training8bit,
  author    = {Naigang Wang and
               Jungwook Choi and
               Daniel Brand and
               Chia{-}Yu Chen and
               Kailash Gopalakrishnan},
  editor    = {Samy Bengio and
               Hanna M. Wallach and
               Hugo Larochelle and
               Kristen Grauman and
               Nicol{\`{o}} Cesa{-}Bianchi and
               Roman Garnett},
  title     = {Training Deep Neural Networks with 8-bit Floating Point Numbers},
  booktitle = {Advances in Neural Information Processing Systems 31: Annual Conference
               on Neural Information Processing Systems 2018, NeurIPS 2018, December
               3-8, 2018, Montr{\'{e}}al, Canada},
  pages     = {7686--7695},
  year      = {2018},
  url       = {https://proceedings.neurips.cc/paper/2018/hash/335d3d1cd7ef05ec77714a215134914c-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/WangCBCG18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sun2019hybrid8bit,
  author    = {Xiao Sun and
               Jungwook Choi and
               Chia{-}Yu Chen and
               Naigang Wang and
               Swagath Venkataramani and
               Vijayalakshmi Srinivasan and
               Xiaodong Cui and
               Wei Zhang and
               Kailash Gopalakrishnan},
  editor    = {Hanna M. Wallach and
               Hugo Larochelle and
               Alina Beygelzimer and
               Florence d'Alch{\'{e}}{-}Buc and
               Emily B. Fox and
               Roman Garnett},
  title     = {Hybrid 8-bit Floating Point {(HFP8)} Training and Inference for Deep
               Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, December
               8-14, 2019, Vancouver, BC, Canada},
  pages     = {4901--4910},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/65fc9fb4897a89789352e211ca2d398f-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:19 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/SunCCWVSCZG19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cambier2020shiftsqueeze,
  author    = {L{\'{e}}opold Cambier and
               Anahita Bhiwandiwalla and
               Ting Gong and
               Oguz H. Elibol and
               Mehran Nekuii and
               Hanlin Tang},
  title     = {Shifted and Squeezed 8-bit Floating Point format for Low-Precision
               Training of Deep Neural Networks},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=Bkxe2AVtPS},
  timestamp = {Thu, 07 May 2020 17:11:47 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/CambierBGENT20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{rvq,
  title={Approximate nearest neighbor search by residual vector quantization},
  author={Chen, Yongjian and Guan, Tao and Wang, Cheng},
  journal={Sensors},
  volume={10},
  number={12},
  pages={11259--11273},
  year={2010},
  publisher={Molecular Diversity Preservation International (MDPI)}
}

@inproceedings{aq,
  title={Additive quantization for extreme vector compression},
  author={Babenko, Artem and Lempitsky, Victor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={931--938},
  year={2014}
}


@INPROCEEDINGS{vq1,

  author={Burton, D. and Shore, J. and Buck, J.},

  booktitle={ICASSP '83. IEEE International Conference on Acoustics, Speech, and Signal Processing}, 

  title={A generalization of isolated word recognition using vector quantization}, 

  year={1983},

  volume={8},

  number={},

  pages={1021-1024},

  doi={10.1109/ICASSP.1983.1171915}}


@ARTICLE{vq2,

  author={Gray, R.},

  journal={IEEE ASSP Magazine}, 

  title={Vector quantization}, 

  year={1984},

  volume={1},

  number={2},

  pages={4-29},

  doi={10.1109/MASSP.1984.1162229}}


@inproceedings{lsq,
  title={Revisiting additive quantization},
  author={Martinez, Julieta and Clement, Joris and Hoos, Holger H and Little, James J},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14},
  pages={137--153},
  year={2016},
  organization={Springer}
}
@inproceedings{lsq++,
  title={LSQ++: Lower running time and higher recall in multi-codebook quantization},
  author={Martinez, Julieta and Zakhmi, Shobhit and Hoos, Holger H and Little, James J},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={491--506},
  year={2018}
}

@article{besag1986statistical,
  title={On the statistical analysis of dirty pictures},
  author={Besag, Julian},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={48},
  number={3},
  pages={259--279},
  year={1986},
  publisher={Oxford University Press}
}

@software{together2023redpajama,
  author = {Together Computer},
  title = {RedPajama: an Open Dataset for Training Large Language Models},
  month = October,
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}
@inproceedings{maddness2021,
  title={Multiplying matrices without multiplying},
  author={Blalock, Davis and Guttag, John},
  booktitle={International Conference on Machine Learning},
  pages={992--1004},
  year={2021},
  organization={PMLR}
}

@article{McCarter2022LookupsAN,
  title={Look-ups are not (yet) all you need for deep learning inference},
  author={Calvin McCarter and Nicholas Dronen},
  journal={ArXiv},
  year={2022},
  volume={abs/2207.05808},
  url={https://api.semanticscholar.org/CorpusID:250491319}
}

@article{FernndezMarqus2023AreWT,
  title={Are We There Yet? Product Quantization and its Hardware Acceleration},
  author={Javier Fern{\'a}ndez-Marqu{\'e}s and Ahmed F. AbouElhamayed and Nicholas Donald Lane and Mohamed Saleh Abdelfattah},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.18334},
  url={https://api.semanticscholar.org/CorpusID:258967539}
}

@inproceedings{norouzi13,
  title={Cartesian k-means},
  author={Norouzi, Mohammad and Fleet, David J},
  booktitle={Proceedings of the IEEE Conference on computer Vision and Pattern Recognition},
  pages={3017--3024},
  year={2013}
}

@ARTICLE{competitveq,

  author={Ozan, Ezgi Can and Kiranyaz, Serkan and Gabbouj, Moncef},

  journal={IEEE Transactions on Knowledge and Data Engineering}, 

  title={Competitive Quantization for Approximate Nearest Neighbor Search}, 

  year={2016},

  volume={28},

  number={11},

  pages={2884-2894},

  doi={10.1109/TKDE.2016.2597834}}



@inproceedings{compositeq,
  title={Composite quantization for approximate nearest neighbor search},
  author={Zhang, Ting and Du, Chao and Wang, Jingdong},
  booktitle={International Conference on Machine Learning},
  pages={838--846},
  year={2014},
  organization={PMLR}
}




@article{opq,
  title={Optimized product quantization},
  author={Ge, Tiezheng and He, Kaiming and Ke, Qifa and Sun, Jian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={36},
  number={4},
  pages={744--755},
  year={2013},
  publisher={IEEE}
}




@inproceedings{drumond2018hybridblock,
  author    = {Mario Drumond and
               Tao Lin and
               Martin Jaggi and
               Babak Falsafi},
  editor    = {Samy Bengio and
               Hanna M. Wallach and
               Hugo Larochelle and
               Kristen Grauman and
               Nicol{\`{o}} Cesa{-}Bianchi and
               Roman Garnett},
  title     = {Training DNNs with Hybrid Block Floating Point},
  booktitle = {Advances in Neural Information Processing Systems 31: Annual Conference
               on Neural Information Processing Systems 2018, NeurIPS 2018, December
               3-8, 2018, Montr{\'{e}}al, Canada},
  pages     = {451--461},
  year      = {2018},
  url       = {https://proceedings.neurips.cc/paper/2018/hash/6a9aeddfc689c1d0e3b9ccc3ab651bc5-Abstract.html},
  timestamp = {Tue, 01 Jun 2021 10:12:08 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/DrumondLJF18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{arc_allenai,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{vig2019multiscale,
  title={A multiscale visualization of attention in the transformer model},
  author={Vig, Jesse},
  journal={arXiv preprint arXiv:1906.05714},
  year={2019}
}

@inproceedings{zhu2017ternary,
  author    = {Chenzhuo Zhu and
               Song Han and
               Huizi Mao and
               William J. Dally},
  title     = {Trained Ternary Quantization},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=S1\_pAu9xl},
  timestamp = {Fri, 20 Nov 2020 16:16:07 +0100},
  biburl    = {https://dblp.org/rec/conf/iclr/ZhuHMD17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{su2021roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}

@inproceedings{li2019bit4,
  author    = {Rundong Li and
               Yan Wang and
               Feng Liang and
               Hongwei Qin and
               Junjie Yan and
               Rui Fan},
  title     = {Fully Quantized Network for Object Detection},
  booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
               2019, Long Beach, CA, USA, June 16-20, 2019},
  pages     = {2810--2819},
  publisher = {Computer Vision Foundation / {IEEE}},
  year      = {2019},
  url       = {http://openaccess.thecvf.com/content\_CVPR\_2019/html/Li\_Fully\_Quantized\_Network\_for\_Object\_Detection\_CVPR\_2019\_paper.html},
  doi       = {10.1109/CVPR.2019.00292},
  timestamp = {Mon, 30 Aug 2021 17:01:14 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/LiWLQYF19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{kim2021bert,
  title={I-bert: Integer-only bert quantization},
  author={Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={International conference on machine learning},
  pages={5506--5518},
  year={2021},
  organization={PMLR}
}


@article{fbgemm,
  title={FBGEMM: Enabling High-Performance Low-Precision Deep Learning Inference},
  author={Khudia, Daya and Huang, Jianyu and Basu, Protonu and Deng, Summer and Liu, Haixin and Park, Jongsoo and Smelyanskiy, Mikhail},
  journal={arXiv preprint arXiv:2101.05615},
  year={2021}
}

@article{shazeer2018mesh,
  title={Mesh-tensorflow: Deep learning for supercomputers},
  author={Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and others},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{gong2019softquant,
  author    = {Ruihao Gong and
               Xianglong Liu and
               Shenghu Jiang and
               Tianxiang Li and
               Peng Hu and
               Jiazhen Lin and
               Fengwei Yu and
               Junjie Yan},
  title     = {Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit
               Neural Networks},
  booktitle = {2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
               2019, Seoul, Korea (South), October 27 - November 2, 2019},
  pages     = {4851--4860},
  publisher = {{IEEE}},
  year      = {2019},
  url       = {https://doi.org/10.1109/ICCV.2019.00495},
  doi       = {10.1109/ICCV.2019.00495},
  timestamp = {Thu, 05 Mar 2020 13:43:22 +0100},
  biburl    = {https://dblp.org/rec/conf/iccv/GongLJLHLYY19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{choi2019bit2,
  author    = {Jungwook Choi and
               Swagath Venkataramani and
               Vijayalakshmi Srinivasan and
               Kailash Gopalakrishnan and
               Zhuo Wang and
               Pierce Chuang},
  editor    = {Ameet Talwalkar and
               Virginia Smith and
               Matei Zaharia},
  title     = {Accurate and Efficient 2-bit Quantized Neural Networks},
  booktitle = {Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford,
               CA, USA, March 31 - April 2, 2019},
  publisher = {mlsys.org},
  year      = {2019},
  url       = {https://proceedings.mlsys.org/book/268.pdf},
  timestamp = {Thu, 18 Jun 2020 15:48:01 +0200},
  biburl    = {https://dblp.org/rec/conf/mlsys/ChoiVSGWC19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{mellempudi2019bit8,
  author    = {Naveen Mellempudi and
               Sudarshan Srinivasan and
               Dipankar Das and
               Bharat Kaul},
  title     = {Mixed Precision Training With 8-bit Floating Point},
  journal   = {CoRR},
  volume    = {abs/1905.12334},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.12334},
  eprinttype = {arXiv},
  eprint    = {1905.12334},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-12334.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}}

@article{qin2020survey1bit,
  author    = {Haotong Qin and
               Ruihao Gong and
               Xianglong Liu and
               Xiao Bai and
               Jingkuan Song and
               Nicu Sebe},
  title     = {Binary Neural Networks: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/2004.03333},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.03333},
  eprinttype = {arXiv},
  eprint    = {2004.03333},
  timestamp = {Wed, 08 Apr 2020 17:08:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-03333.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{courbariaux2014training,
  title={Training deep neural networks with low precision multiplications},
  author={Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
  journal={arXiv preprint arXiv:1412.7024},
  year={2014}
}


@inproceedings{ilharco-etal-2020-high,
    title = "High Performance Natural Language Processing",
    author = "Ilharco, Gabriel  and
      Ilharco, Cesar  and
      Turc, Iulia  and
      Dettmers, Tim  and
      Ferreira, Felipe  and
      Lee, Kenton",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-tutorials.4",
    doi = "10.18653/v1/2020.emnlp-tutorials.4",
    pages = "24--27",
    abstract = "Scale has played a central role in the rapid progress natural language processing has enjoyed in recent years. While benchmarks are dominated by ever larger models, efficient hardware use is critical for their widespread adoption and further progress in the field. In this cutting-edge tutorial, we will recapitulate the state-of-the-art in natural language processing with scale in perspective. After establishing these foundations, we will cover a wide range of techniques for improving efficiency, including knowledge distillation, quantization, pruning, more efficient architectures, along with case studies and practical implementation tricks.",
}

@inproceedings{shen2020q,
  title={Q-bert: Hessian based ultra low precision quantization of bert},
  author={Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={8815--8821},
  year={2020}
}

@article{jacob2017quantization,
  title={Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference. arXiv e-prints, art},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  journal={arXiv preprint arXiv:1712.05877},
  year={2017}
}

@inproceedings{zhao2021distribution,
  title={Distribution adaptive int8 quantization for training cnns},
  author={Zhao, Kang and Huang, Sida and Pan, Pan and Li, Yinghan and Zhang, Yingya and Gu, Zhenyu and Xu, Yinghui},
  booktitle={Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence},
  year={2021}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{chen2020improved,
  title={Improved baselines with momentum contrastive learning},
  author={Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming},
  journal={arXiv preprint arXiv:2003.04297},
  year={2020}
}


@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{micikevicius2017mixed,
  title={Mixed precision training},
  author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
  journal={arXiv preprint arXiv:1710.03740},
  year={2017}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{henighan2020scaling,
  title={Scaling laws for autoregressive generative modeling},
  author={Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B and Dhariwal, Prafulla and Gray, Scott and others},
  journal={arXiv preprint arXiv:2010.14701},
  year={2020}
}

@article{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2102.12092},
  year={2021}
}

@article{krizhevsky2014one,
  title={One weird trick for parallelizing convolutional neural networks},
  author={Krizhevsky, Alex},
  journal={arXiv preprint arXiv:1404.5997},
  year={2014}
}

@article{li20211,
  title={1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed},
  author={Li, Conglong and Awan, Ammar Ahmad and Tang, Hanlin and Rajbhandari, Samyam and He, Yuxiong},
  journal={arXiv preprint arXiv:2104.06069},
  year={2021}
}

@article{tang20211,
  title={1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed},
  author={Tang, Hanlin and Gan, Shaoduo and Awan, Ammar Ahmad and Rajbhandari, Samyam and Li, Conglong and Lian, Xiangru and Liu, Ji and Zhang, Ce and He, Yuxiong},
  journal={arXiv preprint arXiv:2102.02888},
  year={2021}
}

@inproceedings{seide20141,
  title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Fifteenth Annual Conference of the International Speech Communication Association},
  year={2014}
}

@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@article{huang2018gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  journal={arXiv preprint arXiv:1811.06965},
  year={2018}
}

@article{harlap2018pipedream,
  title={Pipedream: Fast and efficient pipeline parallel dnn training},
  author={Harlap, Aaron and Narayanan, Deepak and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil and Ganger, Greg and Gibbons, Phil},
  journal={arXiv preprint arXiv:1806.03377},
  year={2018}
}

@article{gomez2017reversible,
  title={The reversible residual network: Backpropagation without storing activations},
  author={Gomez, Aidan N and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B},
  journal={arXiv preprint arXiv:1707.04585},
  year={2017}
}

@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@article{pudipeddi2020training,
  title={Training large neural networks with constant memory using a new execution algorithm},
  author={Pudipeddi, Bharadwaj and Mesmakhosroshahi, Maral and Xi, Jinwen and Bharadwaj, Sujeeth},
  journal={arXiv preprint arXiv:2002.05645},
  year={2020}
}

@article{rajbhandari2021zero,
  title={ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning},
  author={Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
  journal={arXiv preprint arXiv:2104.07857},
  year={2021}
}
  
  @article{lepikhin2020gshard,
  title={Gshard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}


@article{greenwald2001space,
  title={Space-efficient online computation of quantile summaries},
  author={Greenwald, Michael and Khanna, Sanjeev},
  journal={ACM SIGMOD Record},
  volume={30},
  number={2},
  pages={58--66},
  year={2001},
  publisher={ACM New York, NY, USA}
}

@inproceedings{govindaraju2005fast,
  title={Fast and approximate stream mining of quantiles and frequencies using graphics processors},
  author={Govindaraju, Naga K and Raghuvanshi, Nikunj and Manocha, Dinesh},
  booktitle={Proceedings of the 2005 ACM SIGMOD international conference on Management of data},
  pages={611--622},
  year={2005}
}

@article{dunning2019computing,
  title={Computing extremely accurate quantiles using t-digests},
  author={Dunning, Ted and Ertl, Otmar},
  journal={arXiv preprint arXiv:1902.04023},
  year={2019}
}

@inproceedings{chen2001quantile,
  title={Quantile and histogram estimation},
  author={Chen, E Jack and Kelton, W David},
  booktitle={Proceeding of the 2001 Winter Simulation Conference (Cat. No. 01CH37304)},
  volume={1},
  pages={451--459},
  year={2001},
  organization={IEEE}
} 

@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@article{fedus2021switch,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021}
}


@inproceedings{wenzek-etal-2020-ccnet,
    title = "{CCN}et: Extracting High Quality Monolingual Datasets from Web Crawl Data",
    author = "Wenzek, Guillaume  and
      Lachaux, Marie-Anne  and
      Conneau, Alexis  and
      Chaudhary, Vishrav  and
      Guzm{\'a}n, Francisco  and
      Joulin, Armand  and
      Grave, Edouard",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://www.aclweb.org/anthology/2020.lrec-1.494",
    pages = "4003--4012",
    abstract = "Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@inproceedings{machavcek2014results,
  title={Results of the WMT14 metrics shared task},
  author={Mach{\'a}{\v{c}}ek, Matou{\v{s}} and Bojar, Ond{\v{r}}ej},
  booktitle={Proceedings of the Ninth Workshop on Statistical Machine Translation},
  pages={293--301},
  year={2014}
}

@article{sennrich2016edinburgh,
  title={Edinburgh neural machine translation systems for wmt 16},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1606.02891},
  year={2016}
}

@misc{gonzalez2002digital,
  title={Digital image processing},
  author={Gonzalez, Rafael C and Woods, Richard E and others},
  year={2002},
  publisher={Prentice hall Upper Saddle River, NJ}
}



@article{hyndman1996sample,
  title={Sample quantiles in statistical packages},
  author={Hyndman, Rob J and Fan, Yanan},
  journal={The American Statistician},
  volume={50},
  number={4},
  pages={361--365},
  year={1996},
  publisher={Taylor \& Francis}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1310--1318},
  year={2013},
  organization={PMLR}
}

@article{sennrich2015neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1508.07909},
  year={2015}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}



@article{wu2020integer,
  title={Integer quantization for deep learning inference: Principles and empirical evaluation},
  author={Wu, Hao and Judd, Patrick and Zhang, Xiaojie and Isaev, Mikhail and Micikevicius, Paulius},
  journal={arXiv preprint arXiv:2004.09602},
  year={2020}
}


@inproceedings{dong2019hawq,
  title={Hawq: Hessian aware quantization of neural networks with mixed-precision},
  author={Dong, Zhen and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={293--302},
  year={2019}
}

@inproceedings{cai2020zeroq,
  title={Zeroq: A novel zero shot quantization framework},
  author={Cai, Yaohui and Yao, Zhewei and Dong, Zhen and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13169--13178},
  year={2020}
}

@inproceedings{gong2019differentiable,
  title={Differentiable soft quantization: Bridging full-precision and low-bit neural networks},
  author={Gong, Ruihao and Liu, Xianglong and Jiang, Shenghu and Li, Tianxiang and Hu, Peng and Lin, Jiazhen and Yu, Fengwei and Yan, Junjie},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4852--4861},
  year={2019}
}

@inproceedings{zhang2018lq,
  title={Lq-nets: Learned quantization for highly accurate and compact deep neural networks},
  author={Zhang, Dongqing and Yang, Jiaolong and Ye, Dongqiangzi and Hua, Gang},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={365--382},
  year={2018}
}

@article{esser2019learned,
  title={Learned step size quantization},
  author={Esser, Steven K and McKinstry, Jeffrey L and Bablani, Deepika and Appuswamy, Rathinakumar and Modha, Dharmendra S},
  journal={arXiv preprint arXiv:1902.08153},
  year={2019}
}

@article{gholami2021survey,
  title={A survey of quantization methods for efficient neural network inference},
  author={Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2103.13630},
  year={2021}
}

@software{eval-harness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}

@article{mmlu,
  author       = {Dan Hendrycks and
                  Collin Burns and
                  Steven Basart and
                  Andy Zou and
                  Mantas Mazeika and
                  Dawn Song and
                  Jacob Steinhardt},
  title        = {Measuring Massive Multitask Language Understanding},
  journal      = {CoRR},
  volume       = {abs/2009.03300},
  year         = {2020},
  url          = {https://arxiv.org/abs/2009.03300},
  eprinttype    = {arXiv},
  eprint       = {2009.03300},
  timestamp    = {Thu, 17 Sep 2020 12:49:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2009-03300.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{gsm8k,
  author       = {Karl Cobbe and
                  Vineet Kosaraju and
                  Mohammad Bavarian and
                  Mark Chen and
                  Heewoo Jun and
                  Lukasz Kaiser and
                  Matthias Plappert and
                  Jerry Tworek and
                  Jacob Hilton and
                  Reiichiro Nakano and
                  Christopher Hesse and
                  John Schulman},
  title        = {Training Verifiers to Solve Math Word Problems},
  journal      = {CoRR},
  volume       = {abs/2110.14168},
  year         = {2021},
  url          = {https://arxiv.org/abs/2110.14168},
  eprinttype    = {arXiv},
  eprint       = {2110.14168},
  timestamp    = {Mon, 12 Jun 2023 08:23:44 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2110-14168.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{hoffmann2022training,
  title={Training Compute-Optimal Large Language Models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}



@article{yao2022zeroquant,
  title={ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers},
  author={Yao, Zhewei and Aminabadi, Reza Yazdani and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  journal={arXiv preprint arXiv:2206.01861},
  year={2022}
}

@article{bondarenko2021understanding,
  title={Understanding and overcoming the challenges of efficient transformer quantization},
  author={Bondarenko, Yelysei and Nagel, Markus and Blankevoort, Tijmen},
  journal={arXiv preprint arXiv:2109.12948},
  year={2021}
}

@article{zhao2021automatic,
  title={Automatic Mixed-Precision Quantization Search of BERT},
  author={Zhao, Changsheng and Hua, Ting and Shen, Yilin and Lou, Qian and Jin, Hongxia},
  journal={arXiv preprint arXiv:2112.14938},
  year={2021}
}

@inproceedings{yao2021hawq,
  title={Hawq-v3: Dyadic neural network quantization},
  author={Yao, Zhewei and Dong, Zhen and Zheng, Zhangcheng and Gholami, Amir and Yu, Jiali and Tan, Eric and Wang, Leyuan and Huang, Qijing and Wang, Yida and Mahoney, Michael and others},
  booktitle={International Conference on Machine Learning},
  pages={11875--11886},
  year={2021},
  organization={PMLR}
}


@inproceedings{luo-etal-2021-positional,
    title = "Positional Artefacts Propagate Through Masked Language Model Embeddings",
    author = "Luo, Ziyang  and
      Kulmizev, Artur  and
      Mao, Xiaoxi",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.413",
    doi = "10.18653/v1/2021.acl-long.413",
    pages = "5312--5327",
    abstract = "In this work, we demonstrate that the contextualized word vectors derived from pretrained masked language model-based encoders share a common, perhaps undesirable pattern across layers. Namely, we find cases of persistent outlier neurons within BERT and RoBERTa{'}s hidden state vectors that consistently bear the smallest or largest values in said vectors. In an attempt to investigate the source of this information, we introduce a neuron-level analysis method, which reveals that the outliers are closely related to information captured by positional embeddings. We also pre-train the RoBERTa-base models from scratch and find that the outliers disappear without using positional embeddings. These outliers, we find, are the major cause of anisotropy of encoders{'} raw vector spaces, and clipping them leads to increased similarity across vectors. We demonstrate this in practice by showing that clipped vectors can more accurately distinguish word senses, as well as lead to better sentence embeddings when mean pooling. In three supervised tasks, we find that clipping does not affect the performance.",
}

@article{puccetti2022outliers,
  title={Outliers Dimensions that Disrupt Transformers Are Driven by Frequency},
  author={Puccetti, Giovanni and Rogers, Anna and Drozd, Aleksandr and Dell'Orletta, Felice},
  journal={arXiv preprint arXiv:2205.11380},
  year={2022}
}

@article{gao2019representation,
  title={Representation degeneration problem in training natural language generation models},
  author={Gao, Jun and He, Di and Tan, Xu and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:1907.12009},
  year={2019}
}

@article{kovaleva2021bert,
  title={BERT busters: Outlier dimensions that disrupt transformers},
  author={Kovaleva, Olga and Kulshreshtha, Saurabh and Rogers, Anna and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2105.06990},
  year={2021}
}

@article{luo2020positional,
  title={Positional artefacts propagate through masked language model embeddings},
  author={Luo, Ziyang and Kulmizev, Artur and Mao, Xiaoxi},
  journal={arXiv preprint arXiv:2011.04393},
  year={2020}
}

@article{timkey2021all,
  title={All bark and no bite: Rogue dimensions in transformer language models obscure representational quality},
  author={Timkey, William and van Schijndel, Marten},
  journal={arXiv preprint arXiv:2109.04404},
  year={2021}
}

@article{wei2022outlier,
  title={Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models},
  author={Wei, Xiuying and Zhang, Yunchen and Zhang, Xiangguo and Gong, Ruihao and Zhang, Shanghang and Zhang, Qi and Yu, Fengwei and Liu, Xianglong},
  journal={arXiv preprint arXiv:2209.13325},
  year={2022}
}

@article{black2022gpt,
  title={Gpt-neox-20b: An open-source autoregressive language model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={arXiv preprint arXiv:2204.06745},
  year={2022}
}

@article{frantar2022gptq,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{scao2022bloom,
  title={BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@article{dettmers2022llm,
  title        = {{LLM}.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
  author       = {Tim Dettmers and Mike Lewis and Younes Belkada and Luke Zettlemoyer},
  year         = 2022,
  journal      = {Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022},
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{micikevicius2022fp8,
  title={FP8 Formats for Deep Learning},
  author={Micikevicius, Paulius and Stosic, Dusan and Burgess, Neil and Cornea, Marius and Dubey, Pradeep and Grisenthwaite, Richard and Ha, Sangwon and Heinecke, Alexander and Judd, Patrick and Kamalu, John and others},
  journal={arXiv preprint arXiv:2209.05433},
  year={2022}
}

@inproceedings{DBLP:conf/aaai/piqa2020,
  author    = {Yonatan Bisk and
               Rowan Zellers and
               Ronan LeBras and
               Jianfeng Gao and
               Yejin Choi},
  title     = {{PIQA:} Reasoning about Physical Commonsense in Natural Language},
  booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
               2020, The Thirty-Second Innovative Applications of Artificial Intelligence
               Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
               Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
               February 7-12, 2020},
  pages     = {7432--7439},
  publisher = {{AAAI} Press},
  year      = {2020},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/6239},
  timestamp = {Mon, 07 Mar 2022 16:58:16 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/BiskZLGC20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@inproceedings{DBLP:conf/acl/hellaswag2019,
  author    = {Rowan Zellers and
               Ari Holtzman and
               Yonatan Bisk and
               Ali Farhadi and
               Yejin Choi},
  editor    = {Anna Korhonen and
               David R. Traum and
               Llu{\'{\i}}s M{\`{a}}rquez},
  title     = {HellaSwag: Can a Machine Really Finish Your Sentence?},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {4791--4800},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/p19-1472},
  doi       = {10.18653/v1/p19-1472},
  timestamp = {Fri, 06 Aug 2021 00:41:01 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/ZellersHBFC19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/cacm/winogrande2021,
  author    = {Keisuke Sakaguchi and
               Ronan Le Bras and
               Chandra Bhagavatula and
               Yejin Choi},
  title     = {WinoGrande: an adversarial winograd schema challenge at scale},
  journal   = {Commun. {ACM}},
  volume    = {64},
  number    = {9},
  pages     = {99--106},
  year      = {2021},
  url       = {https://doi.org/10.1145/3474381},
  doi       = {10.1145/3474381},
  timestamp = {Mon, 20 Sep 2021 17:52:06 +0200},
  biburl    = {https://dblp.org/rec/journals/cacm/SakaguchiBBC21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{paperno-etal-2016-lambada,
    title = "The {LAMBADA} dataset: Word prediction requiring a broad discourse context",
    author = "Paperno, Denis  and
      Kruszewski, Germ{\'a}n  and
      Lazaridou, Angeliki  and
      Pham, Ngoc Quan  and
      Bernardi, Raffaella  and
      Pezzelle, Sandro  and
      Baroni, Marco  and
      Boleda, Gemma  and
      Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1144",
    doi = "10.18653/v1/P16-1144",
    pages = "1525--1534",
}

@article{xiao2022smoothquant,
  title={SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Demouth, Julien and Han, Song},
  journal={arXiv preprint arXiv:2211.10438},
  year={2022}
}



@article{jain2020trained,
  title={Trained quantization thresholds for accurate and efficient fixed-point inference of deep neural networks},
  author={Jain, Sambhav and Gural, Albert and Wu, Michael and Dick, Chris},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={112--128},
  year={2020}
}

@inproceedings{nagel2019data,
  title={Data-free quantization through weight equalization and bias correction},
  author={Nagel, Markus and Baalen, Mart van and Blankevoort, Tijmen and Welling, Max},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1325--1334},
  year={2019}
}

@article{krishnamoorthi2018quantizing,
  title={Quantizing deep convolutional networks for efficient inference: A whitepaper},
  author={Krishnamoorthi, Raghuraman},
  journal={arXiv preprint arXiv:1806.08342},
  year={2018}
}

@article{rusci2020memory,
  title={Memory-driven mixed low precision quantization for enabling deep network inference on microcontrollers},
  author={Rusci, Manuele and Capotondi, Alessandro and Benini, Luca},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={326--335},
  year={2020}
}


@article{gong2014compressing,
  title={Compressing deep convolutional networks using vector quantization},
  author={Gong, Yunchao and Liu, Liu and Yang, Ming and Bourdev, Lubomir},
  journal={arXiv preprint arXiv:1412.6115},
  year={2014}
}

@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@article{choi2016towards,
  title={Towards the limit of network quantization},
  author={Choi, Yoojin and El-Khamy, Mostafa and Lee, Jungwon},
  journal={arXiv preprint arXiv:1612.01543},
  year={2016}
}

@inproceedings{wu2016quantized,
  title={Quantized convolutional neural networks for mobile devices},
  author={Wu, Jiaxiang and Leng, Cong and Wang, Yuhang and Hu, Qinghao and Cheng, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4820--4828},
  year={2016}
}

@inproceedings{park2017weighted,
  title={Weighted-entropy-based quantization for deep neural networks},
  author={Park, Eunhyeok and Ahn, Junwhan and Yoo, Sungjoo},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5456--5464},
  year={2017}
}

@article{hou2016loss,
  title={Loss-aware binarization of deep networks},
  author={Hou, Lu and Yao, Quanming and Kwok, James T},
  journal={arXiv preprint arXiv:1611.01600},
  year={2016}
}

@inproceedings{leng2018extremely,
  title={Extremely low bit neural network: Squeeze the last bit out with admm},
  author={Leng, Cong and Dou, Zesheng and Li, Hao and Zhu, Shenghuo and Jin, Rong},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{jaszczur2021sparse,
  title={Sparse is enough in scaling transformers},
  author={Jaszczur, Sebastian and Chowdhery, Aakanksha and Mohiuddin, Afroz and Kaiser, Lukasz and Gajewski, Wojciech and Michalewski, Henryk and Kanerva, Jonni},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={9895--9907},
  year={2021}
}


@Misc{dongarra2022,
  author =   {Jack Dongarra},
  title =    {A Not So Simple Matter of Software},
  howpublished = {\url{https://www.youtube.com/watch?v=cSO0Tc2w5Dg}},
  month =    {November},
  day =          16,
  year =     2022
}

@Misc{falcon2023,
  author =   {{TII UAE}},
  title =    {The {Falcon} Family of Large Language Models},
  howpublished = {\url{https://huggingface.co/tiiuae/falcon-40b}},
  month =    {May},
  year =     2023
}

@Misc{refinedweb2023,
  author =   {{TII UAE}},
  title =    {The {Refined} {Web} Dataset},
  howpublished = {\url{https://huggingface.co/datasets/tiiuae/falcon-refinedweb}},
  month =    {May},
  year =     2023
}

%https://huggingface.co/datasets/tiiuae/falcon-refinedweb


@article{jia2019dissecting,
  title={Dissecting the NVidia Turing T4 GPU via microbenchmarking},
  author={Jia, Zhe and Maggioni, Marco and Smith, Jeffrey and Scarpazza, Daniele Paolo},
  journal={arXiv preprint arXiv:1903.07486},
  year={2019}
}

@article{rosenfeld2019constructive,
  title={A constructive prediction of the generalization error across scales},
  author={Rosenfeld, Jonathan S and Rosenfeld, Amir and Belinkov, Yonatan and Shavit, Nir},
  journal={arXiv preprint arXiv:1909.12673},
  year={2019}
}

@article{hestness2017deep,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md and Ali, Mostofa and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}

@article{pope2022efficiently,
  title={Efficiently Scaling Transformer Inference},
  author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Levskaya, Anselm and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  journal={arXiv preprint arXiv:2211.05102},
  year={2022}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@article{openai2023gpt,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={arXiv},
  year={2023}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  journal={arXiv preprint arXiv:2304.01373},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{dettmers2022case,
  title={The case for 4-bit precision: k-bit Inference Scaling Laws},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2212.09720},
  year={2022}
}


%%%%%%%%%%%%%%%%%%%%
%%% Dan's sparsity references
%%%%%%%%%%%%%%%%%%%%


@string{TPAMI = "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)"}
@string{IJCV = "International Journal of Computer Vision (IJCV)"}
@string{JMLR = "Journal of Machine Learning Research (JMLR)"}
@string{ML = "Machine Learning"}
@string{PR = "Pattern Recognition"}
@string{PRL = "Pattern Recognition Letters"}
@string{TNN = "IEEE Transactions on Neural Networks (TNN)"}

@string{CVPR = "Conference on Computer Vision and Pattern Recognition (CVPR)"}
@string{ECCV = "European Conference on Computer Vision (ECCV)"}
@string{ICCV = "International Conference on Computer Vision (ICCV)"}
@string{BMVC = "British Machine Vision Conference (BMVC)"}

@string{NeurIPS = "Conference on Neural Information Processing Systems (NeurIPS)"}
@string{NIPS = "Conference on Neural Information Processing Systems (NeurIPS)"}
@string{ICML = "International Conference on Machine Learning (ICML)"}
@string{ICLR = "International Conference on Learning Representations (ICLR)"}
@string{COLT = "Workshop on Computational Learning Theory (COLT)"}
@string{UAI = "Uncertainty in Artificial Intelligence (UAI)"}
@string{ECML = "European Conference on Marchine Learning (ECML)"}
@string{AISTATS = "International Conference on Artificial Intelligence and Statistics (AISTATS)"}
@string{KDD = "International conference on Knowledge Discovery and Data Mining (KDD)"}
@string{EUROCRYPT = "International Conference on the Theory and Applications of Cryptographic Techniques (Eurocrypt)"}

@string{WILEY = "Wiley"},
@string{MIT = "The MIT Press"},
@string{CAMBRIDGE = "Cambridge University Press"},
@string{SPRINGER = "Springer"},
@string{KLUWER = "Kluwer Academic Press"}
@string{ELSEVIER = "Elsevier"}
@string{OXFORD = "Oxford University Press"},




@incollection{parallel_scan,
  added-at = {2009-09-10T14:36:22.000+0200},
  author = {Harris, Mark and Sengupta, Shubhabrata and Owens, John D.},
  biburl = {https://www.bibsonomy.org/bibtex/2af00089a47e6be14a796740e12ffc6fe/gregoryy},
  booktitle = {GPU Gems 3},
  chapter = 39,
  editor = {Nguyen, Hubert},
  interhash = {4f92b11742376af7621df70d49f37772},
  intrahash = {af00089a47e6be14a796740e12ffc6fe},
  keywords = {imported},
  month = {August},
  owner = {gregor},
  pages = {851--876},
  publisher = {Addison Wesley},
  timestamp = {2009-09-10T14:36:48.000+0200},
  title = {Parallel Prefix Sum (Scan) with {CUDA}},
  year = 2007
}
% Sparsity survey
@article{hoefler2021sparsity,
  title={Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks},
  author={Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
  journal={arXiv preprint arXiv:2102.00554},
  year={2021}
}

@article{david2020tensorflow,
  title={{TensorFlow Lite Micro: Embedded machine learning on TinyML systems}},
  author={David, Robert and Duke, Jared and Jain, Advait and Reddi, Vijay Janapa and Jeffries, Nat and Li, Jian and Kreeger, Nick and Nappier, Ian and Natraj, Meghna and Regev, Shlomi and others},
  journal={arXiv preprint arXiv:2010.08678},
  year={2020}
}

@misc{weidinger2021ethical,
      title={Ethical and social risks of harm from Language Models}, 
      author={Laura Weidinger and John Mellor and Maribeth Rauh and Conor Griffin and Jonathan Uesato and Po-Sen Huang and Myra Cheng and Mia Glaese and Borja Balle and Atoosa Kasirzadeh and Zac Kenton and Sasha Brown and Will Hawkins and Tom Stepleton and Courtney Biles and Abeba Birhane and Julia Haas and Laura Rimell and Lisa Anne Hendricks and William Isaac and Sean Legassick and Geoffrey Irving and Iason Gabriel},
      year={2021},
      eprint={2112.04359},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% Sparse training methods

@article{bellec2017deep,
  title={Deep rewiring: Training very sparse deep networks},
  author={Bellec, Guillaume and Kappel, David and Maass, Wolfgang and Legenstein, Robert},
  journal=ICLR,
  year={2018}
}



@article{courbariaux2016binarized,
  title={Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1},
  author={Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1602.02830},
  year={2016}
}

@inproceedings{chen2018tvm,
  title={{TVM}: An automated end-to-end optimizing compiler for deep learning},
  author={Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and others},
  booktitle={13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)},
  pages={578--594},
  year={2018}
}

@article{NVIDIASparse,
  title={Accelerating Sparse Deep Neural Networks},
  author={Mishra, Asit and Latorre, Jorge Albericio and Pool, Jeff and Stosic, Darko and Stosic, Dusan and Venkatesh, Ganesh and Yu, Chong and Micikevicius, Paulius},
  journal={arXiv preprint arXiv:2104.08378},
  year={2021}
}

@unpublished{vanholder2016efficient,
  title={Efficient inference with {TensorRT}},
  author={Vanholder, Han},
  year={2017},
  Note={{NVIDIA GTC} On-Demand. Slides available at  https://on-demand-gtc.gputechconf.com/gtcnew/sessionview.php?sessionName=23425-efficient+inference+with+tensorrt}
}


@article{mocanu2018scalable,
  title={Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
  author={Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
  journal={Nature communications},
  volume={9},
  number={1},
  pages={1--12},
  year={2018},
  publisher={Nature Publishing Group}
}


@inproceedings{kusupati2020soft,
  title={Soft threshold weight reparameterization for learnable sparsity},
  author={Kusupati, Aditya and Ramanujan, Vivek and Somani, Raghav and Wortsman, Mitchell and Jain, Prateek and Kakade, Sham and Farhadi, Ali},
  booktitle=ICML,
  year={2020}
}

@article{evci2018mean,
  title={Mean Replacement Pruning},
  author={Evci, Utku and Le Roux, Nicolas and Castro, Pablo and Bottou, Leon},
  year={2018}
}


@inproceedings{evci2020rigging,
  title={Rigging the lottery: Making all tickets winners},
  author={Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  booktitle=ICML,
  year={2020}
}


@inproceedings{jayakumar2020top,
  title={{Top-KAST}: {Top-K} always sparse training},
  author={Jayakumar, Siddhant and Pascanu, Razvan and Rae, Jack and Osindero, Simon and Elsen, Erich},
  booktitle=NeurIPS,
  year={2020}
}


@inproceedings{lin2019dynamic,
  title={Dynamic Model Pruning with Feedback},
  author={Lin, Tao and Stich, Sebastian U and Barba, Luis and Dmitriev, Daniil and Jaggi, Martin},
  booktitle=ICLR,
  year={2019}
}

@misc{NM,
title={{NeuralMagic DeepSparse Inference Engine}},
author={DeepSparse},
year={2021},
url={https://github.com/neuralmagic/deepsparse}
}

@misc{graphcore,
title={{Graphcore Poplar SDK 2.0}},
author={Graphcore},
year={2021},
url={https://github.com/graphcore/poplibs}
}



@inproceedings{elsen2020fast,
  title={Fast sparse convnets},
  author={Elsen, Erich and Dukhan, Marat and Gale, Trevor and Simonyan, Karen},
  booktitle=CVPR,
  year={2020}
}

@inproceedings{frankle2018lottery,
  title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  author={Frankle, Jonathan and Carbin, Michael},
  booktitle=ICLR,
  year={2019}
}

@article{frankle2019stabilizing,
  title={Stabilizing the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M and Carbin, Michael},
  journal={arXiv preprint arXiv:1903.01611},
  year={2019}
}

@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle=ICML,
  pages={3259--3269},
  year={2020},
  organization={PMLR}
}

@article{wortsman2019discovering,
  title={Discovering Neural Wirings},
  author={Wortsman, Mitchell and Farhadi, Ali and Rastegari, Mohammad},
  journal=NeurIPS,
  volume={32},
  pages={2684--2694},
  year={2019}
}

@article{laurencconbigscience,
  title={The {BigScience} Corpus: A 1.6 {TB} Composite Multilingual Dataset},
  author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and del Moral, Albert Villanova and Le Scao, Teven and Von Werra, Leandro and Mou, Chenghao and Ponferrada, Eduardo Gonz{\'a}lez and Nguyen, Huu and others}, 
  year={2022}
}

@inproceedings{2017-dong,
  title={Learning to prune deep neural networks via layer-wise optimal brain surgeon},
  author={Dong, Xin and Chen, Shangyu and Pan, Sinno Jialin},
  booktitle=NeurIPS,
  year={2017}
}

@inproceedings{strubell2020energy,
  title={Energy and policy considerations for modern deep learning research},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={13693--13696},
  year={2020}
}

@inproceedings{alistarh2018convergence,
  title={The convergence of sparsified gradient methods},
  author={Alistarh, Dan and Hoefler, Torsten and Johansson, Mikael and Khirirat, Sarit and Konstantinov, Nikola and Renggli, C{\'e}dric},
  booktitle=NeurIPS,
  year={2018}
}

@article{shi2019understanding,
  title={Understanding top-k sparsification in distributed deep learning},
  author={Shi, Shaohuai and Chu, Xiaowen and Cheung, Ka Chun and See, Simon},
  journal={arXiv preprint arXiv:1911.08772},
  year={2019}
}

@article{dettmers2019sparse,
  title={Sparse networks from scratch: Faster training without losing performance},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1907.04840},
  year={2019}
}

@article{mohtashami2021simultaneous,
  title={Simultaneous Training of Partially Masked Neural Networks},
  author={Mohtashami, Amirkeivan and Jaggi, Martin and Stich, Sebastian U},
  journal={arXiv preprint arXiv:2106.08895},
  year={2021}
}

@article{kurtic2022optimal,
  title={The {Optimal BERT Surgeon}: Scalable and Accurate Second-Order Pruning for Large Language Models},
  author={Kurtic, Eldar and Campos, Daniel and Nguyen, Tuan and Frantar, Elias and Kurtz, Mark and Fineran, Benjamin and Goin, Michael and Alistarh, Dan},
  journal={arXiv preprint arXiv:2203.07259},
  year={2022}
}

@inproceedings{iofinova2022well,
  title={How Well Do Sparse {ImageNet} Models Transfer?},
  author={Iofinova, Eugenia and Peste, Alexandra and Kurtz, Mark and Alistarh, Dan},
  booktitle=CVPR,
  year={2022}
}


@inproceedings{alistarh2016qsgd,
	title={{QSGD}: Randomized Quantization for Communication-Efficient Stochastic Gradient Descent},
	author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
	booktitle=NeurIPS,
	year={2017}
}

@article{liang2021pruning,
  title={Pruning and quantization for deep neural network acceleration: A survey},
  author={Liang, Tailin and Glossner, John and Wang, Lei and Shi, Shaobo and Zhang, Xiaotong},
  journal={Neurocomputing},
  volume={461},
  pages={370--403},
  year={2021},
  publisher={Elsevier}
}


% Post training pruning

@inproceedings{lecun1990optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle=NeurIPS,
  year={1990}
}


@inproceedings{hassibi1993optimal,
  title={Optimal brain surgeon and general network pruning},
  author={Hassibi, Babak and Stork, David G and Wolff, Gregory J},
  booktitle={IEEE International Conference on Neural Networks},
  year={1993}
}



@inproceedings{molchanov2017variational,
  title={Variational dropout sparsifies deep neural networks},
  author={Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
  booktitle=ICML,
  pages={2498--2507},
  year={2017},
  organization={PMLR}
}

@inproceedings{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  booktitle=ICML,
  year={2019}
}

@inproceedings{wang2019eigendamage,
  title={Eigendamage: Structured pruning in the {K}ronecker-factored eigenbasis},
  author={Wang, Chaoqi and Grosse, Roger and Fidler, Sanja and Zhang, Guodong},
  booktitle=ICML,
  year={2019}
}

@article{zhu2017prune,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={Zhu, Michael and Gupta, Suyog},
  journal={arXiv preprint arXiv:1710.01878},
  year={2017}
}

@inproceedings{han2015learning,
  title={Learning both weights and connections for efficient neural networks},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William J},
  booktitle=NeurIPS,
  year={2015}
}

@inproceedings{zhou2021learning,
  title={Learning {N:M} Fine-grained Structured Sparse Neural Networks From Scratch},
  author={Zhou, Aojun and Ma, Yukun and Zhu, Junnan and Liu, Jianbo and Zhang, Zhijie and Yuan, Kun and Sun, Wenxiu and Li, Hongsheng},
  booktitle=ICLR,
  year={2021}
}


% General IHT References

@article{blumensath2008iterative,
  title={Iterative thresholding for sparse approximations},
  author={Blumensath, Thomas and Davies, Mike E},
  journal={Journal of Fourier Analysis and Applications},
  volume={14},
  number={5-6},
  pages={629--654},
  year={2008},
  publisher={Springer}
}

@article{foucart2011hard,
  title={Hard thresholding pursuit: an algorithm for compressive sensing},
  author={Foucart, Simon},
  journal={SIAM Journal on Numerical Analysis},
  volume={49},
  number={6},
  pages={2543--2563},
  year={2011},
  publisher={SIAM}
}

@incollection{foucart2012sparse,
  title={Sparse recovery algorithms: sufficient conditions in terms of restricted isometry constants},
  author={Foucart, Simon},
  booktitle={Approximation Theory XIII: San Antonio 2010},
  pages={65--77},
  year={2012},
  publisher={Springer}
}

@inproceedings{yuan2014gradient,
  title={Gradient hard thresholding pursuit for sparsity-constrained optimization},
  author={Yuan, Xiaotong and Li, Ping and Zhang, Tong},
  booktitle=ICML,
  pages={127--135},
  year={2014},
  organization={PMLR}
}

@inproceedings{jain2014iterative,
  title={On iterative hard thresholding methods for high-dimensional {M}-estimation},
  author={Jain, Prateek and Tewari, Ambuj and Kar, Purushottam},
  booktitle=NeurIPS,
  pages={685--693},
  year={2014}
}

@inproceedings{axiotis2020sparse,
  title={Sparse Convex Optimization via Adaptively Regularized Hard Thresholding},
  author={Axiotis, Kyriakos and Sviridenko, Maxim},
  booktitle=ICML,
  pages={452--462},
  year={2020},
  organization={PMLR}
}

% IHT Methods

@article{jin2016training,
  title={Training skinny deep neural networks with iterative hard thresholding methods},
  author={Jin, Xiaojie and Yuan, Xiaotong and Feng, Jiashi and Yan, Shuicheng},
  journal={arXiv preprint arXiv:1607.05423},
  year={2016}
}


@article{han2016dsd,
  title={{DSD}: Dense-sparse-dense training for deep neural networks},
  author={Han, Song and Pool, Jeff and Narang, Sharan and Mao, Huizi and Gong, Enhao and Tang, Shijian and Elsen, Erich and Vajda, Peter and Paluri, Manohar and Tran, John and others},
  journal=ICLR,
  year={2017}
}


% Datasets

@inproceedings{PTB,
  title={The Penn treebank: Annotating predicate argument structure},
  author={Marcus, Mitch and Kim, Grace and Marcinkiewicz, Mary Ann and MacIntyre, Robert and Bies, Ann and Ferguson, Mark and Katz, Karen and Schasberger, Britta},
  booktitle={Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994},
  year={1994}
}


@article{cifar100,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@article{imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International Journal of Computer Vision},
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@inproceedings{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle=NeurIPS,
  year={2019}
}

@article{wikitext103,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}




@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal=ICLR,
  year={2017}
}

@article{hagiwara1994,
	title = {A simple and effective method for removal of hidden units and weights},
	journal = {Neurocomputing},
	volume = {6},
	number = {2},
	pages = {207 - 218},
	year = {1994},
	note = {Backpropagation, Part IV},
	issn = {0925-2312},
	author = {Masafumi Hagiwara},
}

@article{lee2018snip,
  title={{SNIP}: Single-shot network pruning based on connection sensitivity},
  author={Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS},
  journal=ICLR,
  year={2019}
}

@article{tanaka2020pruning,
  title={Pruning neural networks without any data by iteratively conserving synaptic flow},
  author={Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel L and Ganguli, Surya},
  journal=NeurIPS,
  volume={33},
  year={2020}
}

% Models, architectures

@inproceedings{zagoruyko2016wide,
  title={Wide Residual Networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  booktitle=BMVC,
  year={2016}
}


@article{hooker2020characterising,
  title={Characterising bias in compressed models},
  author={Hooker, Sara and Moorosi, Nyalleng and Clark, Gregory and Bengio, Samy and Denton, Emily},
  journal={arXiv preprint arXiv:2010.03058},
  year={2020}
}

@article{howard2017mobilenets,
  title={{MobileNets}: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}


@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle=CVPR,
  year={2016}
}

@inproceedings{dai2019transformer,
  title={{Transformer-XL: Attentive language models beyond a fixed-length context}},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  year={2019}
}

@inproceedings{you2019large,
  title={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  booktitle=ICLR,
  year={2020}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal=ICLR,
  year={2015}
}


% MISC

@misc{wandb,
title = {{Experiment Tracking with Weights and Biases}},
year = {2020},
note = {Software available from wandb.com},
url={https://www.wandb.com/},
author = {Biewald, Lukas},
}

@incollection{pytorch,
title = {{PyTorch}: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = NeurIPS,
year = {2019},
}

@article{qian1999momentum,
  title={On the momentum term in gradient descent learning algorithms},
  author={Qian, Ning},
  journal={Neural networks},
  volume={12},
  number={1},
  pages={145--151},
  year={1999},
  publisher={Elsevier}
}

@inproceedings{karimi2016linear,
  title={Linear convergence of gradient and proximal-gradient methods under the {P}olyak-{{\L}}ojasiewicz condition},
  author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={795--811},
  year={2016},
  organization={Springer}
}

@inproceedings{haroush2020knowledge,
  title={The knowledge within: Methods for data-free model compression},
  author={Haroush, Matan and Hubara, Itay and Hoffer, Elad and Soudry, Daniel},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8494--8502},
  year={2020}
}


@article{liu2020toward,
  title={Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2003.00307},
  year={2020}
}

@inproceedings{allen2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle=ICML,
  pages={242--252},
  year={2019},
  organization={PMLR}
}

@article{candes2006near,
  title={Near-optimal signal recovery from random projections: Universal encoding strategies?},
  author={Candes, Emmanuel J and Tao, Terence},
  journal={IEEE transactions on information theory},
  volume={52},
  number={12},
  pages={5406--5425},
  year={2006},
  publisher={IEEE}
}

@article{goodfellow2014qualitatively,
  title={Qualitatively characterizing neural network optimization problems},
  author={Goodfellow, Ian J and Vinyals, Oriol and Saxe, Andrew M},
  journal={arXiv preprint arXiv:1412.6544},
  year={2014}
}

@inproceedings{shevchenko2020landscape,
  title={Landscape connectivity and dropout stability of {SGD} solutions for over-parameterized neural networks},
  author={Shevchenko, Alexander and Mondelli, Marco},
  booktitle=ICML,
  pages={8773--8784},
  year={2020},
  organization={PMLR}
}

%%%%%%%%%%%%%%%%%%%%
%%% End of Dan's reference list
%%%%%%%%%%%%%%%%%%%%



@article{aflalo2020knapsack,
  title={Knapsack Pruning with Inner Distillation},
  author={Aflalo, Yonathan and Noy, Asaf and Lin, Ming and Friedman, Itamar and Zelnik, Lihi},
  journal={arXiv preprint arXiv:2002.08258},
  year={2020}
}

@inproceedings{wu2020constraint,
  title={Constraint-Aware Importance Estimation for Global Filter Pruning under Multiple Resource Constraints},
  author={Wu, Yu-Cheng and Liu, Chih-Ting and Chen, Bo-Ying and Chien, Shao-Yi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={686--687},
  year={2020}
}

@inproceedings{yao2021hawq,
  title={{HAWQ-v3}: Dyadic Neural Network Quantization},
  author={Yao, Zhewei and Dong, Zhen and Zheng, Zhangcheng and Gholami, Amir and Yu, Jiali and Tan, Eric and Wang, Leyuan and Huang, Qijing and Wang, Yida and Mahoney, Michael and others},
  booktitle=ICML,
  year={2021}
}

@inproceedings{hubara2021accurate,
  title={Accurate Post Training Quantization With Small Calibration Sets},
  author={Hubara, Itay and Nahshan, Yury and Hanani, Yair and Banner, Ron and Soudry, Daniel},
  booktitle=ICML,
  year={2021}
}

@article{hubara2020improving
,
  title={Improving post training neural quantization: Layer-wise calibration and integer programming},
  author={Hubara, Itay and Nahshan, Yury and Hanani, Yair and Banner, Ron and Soudry, Daniel},
  journal={arXiv preprint arXiv:2006.10518},
  year={2020}
}


@inproceedings{he2018amc,
  title={{AMC}: {AutoML} for Model Compression and Acceleration on Mobile Devices},
  author={He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
  booktitle=ECCV,
  year={2018}
}

@inproceedings{liebenwein2021compressing,
  title={Compressing Neural Networks: Towards Determining the Optimal Layer-wise Decomposition},
  author={Liebenwein, Lucas and Maalouf, Alaa and Feldman, Dan and Rus, Daniela},
  booktitle=NeurIPS,
  year={2021}
}

@article{markov2021project,
  title={Project {CGX}: Scalable Deep Learning on Commodity {GPUs}},
  author={Markov, Ilia and Ramezani, Hamidreza and Alistarh, Dan},
  journal={arXiv preprint arXiv:2111.08617},
  year={2021}
}

@inproceedings{cai2019once,
  title={{Once-for-All}: Train One Network and Specialize it for Efficient Deployment},
  author={Cai, Han and Gan, Chuang and Wang, Tianzhe and Zhang, Zhekai and Han, Song},
  booktitle=ICLR,
  year={2019}
}

@inproceedings{hubara2021accelerated,
  title={Accelerated Sparse Neural Training: A Provable and Efficient Method to find {N:M} Transposable Masks},
  author={Hubara, Itay and Chmiel, Brian and Island, Moshe and Banner, Ron and Naor, Seffi and Soudry, Daniel},
  booktitle=NeurIPS,
  year={2021}
}

@article{li2016pruning,
  title={Pruning Filters for Efficient Convnets},
  author={Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  journal={arXiv preprint arXiv:1608.08710},
  year={2016}
}

@inproceedings{singh2020woodfisher,
  title={{WoodFisher}: Efficient Second-Order Approximation for Neural Network Compression},
  author={Singh, Sidak Pal and Alistarh, Dan},
  booktitle=NeurIPS,
  year={2020}
}

@inproceedings{frantar2021m,
  title={{M-FAC}: Efficient Matrix-Free Approximations of Second-Order Information},
  author={Frantar, Elias and Kurtic, Eldar and Alistarh, Dan},
  booktitle=NeurIPS,
  year={2021}
}

@inproceedings{nagel2020up,
  title={Up or Down? {A}daptive Rounding for Post-Training Quantization},
  author={Nagel, Markus and Amjad, Rana Ali and Van Baalen, Mart and Louizos, Christos and Blankevoort, Tijmen},
  booktitle=ICML,
  year={2020}
}

@inproceedings{liu2021group,
  title={Group Fisher Pruning for Practical Network Compression},
  author={Liu, Liyang and Zhang, Shilong and Kuang, Zhanghui and Zhou, Aojun and Xue, Jing-Hao and Wang, Xinjiang and Chen, Yimin and Yang, Wenming and Liao, Qingmin and Zhang, Wayne},
  booktitle=ICML,
  year={2021}
}

@InProceedings{
    pmlr-v119-kurtz20a, 
    title = {Inducing and Exploiting Activation Sparsity for Fast Inference on Deep Neural Networks}, 
    author = {Kurtz, Mark and Kopinsky, Justin and Gelashvili, Rati and Matveev, Alexander and Carr, John and Goin, Michael and Leiserson, William and Moore, Sage and Nell, Bill and Shavit, Nir and Alistarh, Dan}, 
    booktitle = ICML,
    year = {2020}
}

@inproceedings{sgk_sc2020,
  author    = {Trevor Gale and Matei Zaharia and Cliff Young and Erich Elsen},
  title     = {Sparse {GPU} Kernels for Deep Learning},
  booktitle = {International Conference for High Performance Computing, Networking, Storage and Analysis (SC)},
  year      = {2020},
}

@article{dave2021hardware,
  title={Hardware Acceleration of Sparse and Irregular Tensor Computations of {ML} Models: A Survey and Insights},
  author={Dave, Shail and Baghdadi, Riyadh and Nowatzki, Tony and Avancha, Sasikanth and Shrivastava, Aviral and Li, Baoxin},
  journal={Proceedings of the IEEE},
  volume={109},
  number={10},
  pages={1706--1752},
  year={2021},
  publisher={IEEE}
}

@article{2020-sanh,
	title={Movement Pruning: Adaptive Sparsity by Fine-Tuning}, 
	author={Victor Sanh and Thomas Wolf and Alexander M. Rush},
	year={2020},
	journal={arXiv preprint arXiv:2005.07683}
}

@inproceedings{peste2021ac,
  title={{AC/DC}: Alternating Compressed/DeCompressed Training of Deep Neural Networks},
  author={Peste, Alexandra and Iofinova, Eugenia and Vladu, Adrian and Alistarh, Dan},
  booktitle=NeurIPS,
  year={2021}
}

@inproceedings{he2019filter,
  title={Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration},
  author={He, Yang and Liu, Ping and Wang, Ziwei and Hu, Zhilan and Yang, Yi},
  booktitle=CVPR,
  year={2019}
}

@article{jayakumar2021top,
  title={{Top-KAST}: {Top-K} Always Sparse Training},
  author={Jayakumar, Siddhant M and Pascanu, Razvan and Rae, Jack W and Osindero, Simon and Elsen, Erich},
  journal={arXiv preprint arXiv:2106.03517},
  year={2021}
}

@inproceedings{he2017channel,
  title={Channel Pruning for Accelerating Very Deep Neural Networks},
  author={He, Yihui and Zhang, Xiangyu and Sun, Jian},
  booktitle=ICCV,
  year={2017}
}

@inproceedings{ashok2018n2n,
  title={{N2N} Learning: Network to Network Compression via Policy Gradient Reinforcement Learning},
  author={Ashok, Anubhav and Rhinehart, Nicholas and Beainy, Fares and Kitani, Kris M},
  booktitle=ICLR,
  year={2018}
}

@inproceedings{yang2021netadaptv2,
  title={{NetAdaptV2}: Efficient Neural Architecture Search with Fast Super-Network Training and Architecture Optimization},
  author={Yang, Tien-Ju and Liao, Yi-Lun and Sze, Vivienne},
  booktitle=CVPR,
  year={2021}
}

@inproceedings{real2019regularized,
  title={Regularized Evolution for Image Classifier Architecture Search},
  author={Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2019}
}

@inproceedings{devlin2018bert,
  title={{BERT}: Pre-Training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={North American Chapter of the Association for Computational Linguistics (NAACL)},
  year={2019}
}



@inproceedings{deng2009imagenet,
  title={{ImageNet}: A Large-Scale Hierarchical Image Database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle=CVPR,
  pages={248--255},
  year={2009},
  organization={IEEE}
}


@inproceedings{marcel2010torchvision,
  title={Torchvision the Machine-Vision Package of Torch},
  author={Marcel, S{\'e}bastien and Rodriguez, Yann},
  booktitle={ACM International Conference on Multimedia},
  year={2010}
}



@inproceedings{rajpurkar2016squad,
  title={{SQuAD}: 100,000+ Questions for Machine Comprehension of Text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  booktitle={Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2016}
}

@article{pool2021channel,
  title={Channel Permutations for {N:M} Sparsity},
  author={Pool, Jeff and Yu, Chong},
  journal=NeurIPS,
  volume={34},
  year={2021}
}

@inproceedings{schwarz2021powerpropagation,
  title={Powerpropagation: A sparsity inducing weight reparameterisation},
  author={Schwarz, Jonathan and Jayakumar, Siddhant and Pascanu, Razvan and Latham, Peter and Teh, Yee},
  booktitle=NeurIPS,
  year={2021}
}

@inproceedings{lagunas21block,
    title = "Block Pruning For Faster Transformers",
    author = "Lagunas, Fran{\c{c}}ois  and
      Charlaix, Ella  and
      Sanh, Victor  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    year = "2021",
    publisher = "Association for Computational Linguistics",
    pages = "10619--10629",
}

@inproceedings{li2021brecq,
  title={{BRECQ}: Pushing the Limit of Post-Training Quantization by Block Reconstruction},
  author={Li, Yuhang and Gong, Ruihao and Tan, Xu and Yang, Yang and Hu, Peng and Zhang, Qi and Yu, Fengwei and Wang, Wei and Gu, Shi},
  booktitle=ICLR,
  year={2021}
}

@inproceedings{cai2018proxylessnas,
  title={{ProxylessNAS}: Direct Neural Architecture Search on Target Task and Hardware},
  author={Cai, Han and Zhu, Ligeng and Han, Song},
  booktitle=ICLR,
  year={2018}
}

@article{nagel2021white,
  title={A White Paper on Neural Network Quantization},
  author={Nagel, Markus and Fournarakis, Marios and Amjad, Rana Ali and Bondarenko, Yelysei and van Baalen, Mart and Blankevoort, Tijmen},
  journal={arXiv preprint arXiv:2106.08295},
  year={2021}
}

@misc{deepsparse,
  author = "NeuralMagic",
  address = "NeuralMagic, Inc.",
  title = "{DeepSparse}",
  year = "2022",
  url = "https://github.com/neuralmagic/deepsparse"
}

@misc{yolov5,
  author = "Glenn Jocher",
  address = "Ultralytics",
  title = "{YOLOv5}",
  year = "2022.",
  howpublished = "https://github.com/ultralytics/yolov5"
}

@inproceedings{carreira2018learning,
  title={{Learning-Compression} algorithms for neural net pruning},
  author={Carreira-Perpin{\'a}n, Miguel A and Idelbayev, Yerlan},
  booktitle=CVPR,
  year={2018}
}

@inproceedings{idelbayev2021beyond,
  title={Beyond {FLOPs} in low-rank compression of neural networks: Optimizing device-specific inference runtime},
  author={Idelbayev, Yerlan and Carreira-Perpi{\~n}{\'a}n, Miguel {\'A}},
  booktitle={IEEE International Conference on Image Processing (ICIP)},
  year={2021}
}

@inproceedings{liebenwein2019provable,
  title={Provable Filter Pruning for Efficient Neural Networks},
  author={Liebenwein, Lucas and Baykal, Cenk and Lang, Harry and Feldman, Dan and Rus, Daniela},
  booktitle=ICLR,
  year={2019}
}

@article{baykal2019sipping,
  title={{SiPPing} neural networks: Sensitivity-informed provable pruning of neural networks},
  author={Baykal, Cenk and Liebenwein, Lucas and Gilitschenski, Igor and Feldman, Dan and Rus, Daniela},
  journal={arXiv preprint arXiv:1910.05422},
  year={2019}
}

@article{parnami2021pruning,
  title={Pruning Attention Heads of Transformer Models Using {A*} Search: A Novel Approach to Compress Big {NLP} Architectures},
  author={Parnami, Archit and Singh, Rahul and Joshi, Tarun},
  journal={arXiv preprint arXiv:2110.15225},
  year={2021}
}

@inproceedings{huang2020rethinking,
  title={Rethinking the Pruning Criteria for Convolutional Neural Network},
  author={Huang, Zhongzhan and Wang, Xinjiang and Luo, Ping},
  booktitle=NeurIPS,
  year={2021}
}

@article{jiao2019survey,
  title={A survey of deep learning-based object detection},
  author={Jiao, Licheng and Zhang, Fan and Liu, Fang and Yang, Shuyuan and Li, Lingling and Feng, Zhixi and Qu, Rong},
  journal={IEEE Access},
  volume={7},
  pages={128837--128868},
  year={2019}
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with {AlphaFold}},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@inproceedings{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and {Huffman} coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  booktitle=ICLR,
  year={2016}
}

@article{zhang2015accelerating,
  title={Accelerating very deep convolutional networks for classification and detection},
  author={Zhang, Xiangyu and Zou, Jianhua and He, Kaiming and Sun, Jian},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={38},
  number={10},
  pages={1943--1955},
  year={2015}
}

@inproceedings{dubey2018coreset,
  title={Coreset-based neural network compression},
  author={Dubey, Abhimanyu and Chatterjee, Moitreya and Ahuja, Narendra},
  booktitle={European Conference on Computer Vision (ECCV)},
  year={2018}
}

@inproceedings{tan2019efficientnet,
  title={{EfficientNet}: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle=ICML,
  year={2019}
}

@inproceedings{sui2021chip,
  title={{CHIP}: CHannel Independence-based Pruning for Compact Neural Networks},
  author={Sui, Yang and Yin, Miao and Xie, Yi and Phan, Huy and Aliari Zonouz, Saman and Yuan, Bo},
  booktitle=NeurIPS,
  year={2021}
}

@article{huang2021rethinking,
  title={Rethinking the Pruning Criteria for Convolutional Neural Network},
  author={Huang, Zhongzhan and Shao, Wenqi and Wang, Xinjiang and Lin, Liang and Luo, Ping},
  journal=NeurIPS,
  volume={34},
  year={2021}
}

@inproceedings{liu2018rethinking,
  title={Rethinking the Value of Network Pruning},
  author={Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  booktitle=ICLR,
  year={2018}
}

@inproceedings{lagunas2021block,
  title={Block Pruning For Faster Transformers},
  author={Lagunas, Fran{\c{c}}ois and Charlaix, Ella and Sanh, Victor and Rush, Alexander M},
  booktitle={Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2021}
}

@inproceedings{wanghaq,
  title={{HAQ}: Hardware-aware Automated Quantization with Mixed Precision},
  author={Wang, Kuan and Liu, Zhijian and Lin, Yujun and Lin, Ji and Han, Song},
  booktitle=CVPR,
  year=2019
}

@inproceedings{2017-wu,
	author={Yuhuai Wu and Elman Mansimov and Roger B. Grosse and Shun Liao and Jimmy Ba},
	title={Second-order Optimization for Deep Reinforcement Learning using {K}ronecker-factored Approximation},
	year={2017},
	booktitle=NeurIPS,
}

@article{doi:10.1162/089976698300017746,
	author = {Amari, Shun-ichi},
	title = {Natural Gradient Works Efficiently in Learning},
	journal = {Neural Computation},
	volume = {10},
	number = {2},
	pages = {251-276},
	year = {1998},
	doi = {10.1162/089976698300017746},
	
	URL = { 
	https://doi.org/10.1162/089976698300017746
	
	},
	eprint = { 
	https://doi.org/10.1162/089976698300017746
	
	}
}

@article{frantar2022spdy,
  title={{SPDY:} {A}ccurate Pruning with Speedup Guarantees},
  author={Frantar, Elias and Alistarh, Dan},
  journal={arXiv preprint arXiv:2201.13096},
  year={2022}
}

@inproceedings{wang2020towards,
  title={Towards accurate post-training network quantization via bit-split and stitching},
  author={Wang, Peisong and Chen, Qiang and He, Xiangyu and Cheng, Jian},
  booktitle=ICML,
  year={2020},
}


@article{nahshan2021loss,
  title={Loss aware post-training quantization},
  author={Nahshan, Yury and Chmiel, Brian and Baskin, Chaim and Zheltonozhskii, Evgenii and Banner, Ron and Bronstein, Alex M and Mendelson, Avi},
  journal={Machine Learning},
  volume={110},
  number={11},
  pages={3245--3262},
  year={2021},
  publisher={Springer}
}

@inproceedings{choukroun2019low,
  title={Low-bit quantization of neural networks for efficient inference},
  author={Choukroun, Yoni and Kravchik, Eli and Yang, Fan and Kisilev, Pavel},
  booktitle={International Conference on Computer Vision Workshop (ICCVW)},
  year={2019}
}

@inproceedings{2015-martens,
	title={Optimizing Neural Networks with Kronecker-factored Approximate Curvature}, 
	author={James Martens and Roger Grosse},
	year={2015},
	booktitle=ICML
}

@inproceedings{grosse2016kroneckerfactored,
	title={A {K}ronecker-factored approximate {F}isher matrix for convolution layers},
	author={Roger Grosse and James Martens},
	year={2016},
	booktitle=ICML
}

@inproceedings{yang2020automatic,
  title={Automatic neural network compression by sparsity-quantization joint learning: A constrained optimization-based approach},
  author={Yang, Haichuan and Gui, Shupeng and Zhu, Yuhao and Liu, Ji},
  booktitle=CVPR,
  year={2020}
}

@inproceedings{lin2014microsoft,
  title={Microsoft {COCO}: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle=ECCV,
  year={2014}
}

@article{chmiel2022optimal,
  title={Optimal Fine-Grained {N:M} sparsity for Activations and Neural Gradients},
  author={Chmiel, Brian and Hubara, Itay and Banner, Ron and Soudry, Daniel},
  journal={arXiv preprint arXiv:2203.10991},
  year={2022}
}

@inproceedings{nagel2019data,
  title={Data-free quantization through weight equalization and bias correction},
  author={Nagel, Markus and Baalen, Mart van and Blankevoort, Tijmen and Welling, Max},
  booktitle=ICCV,
  year={2019}
}

@inproceedings{banner2019post,
  title={Post training 4-bit quantization of convolutional networks for rapid-deployment},
  author={Banner, Ron and Nahshan, Yury and Soudry, Daniel},
  booktitle=NeurIPS,
  year={2019}
}

@article{frantar2022obc,
  title={{Optimal Brain Compression}: A Framework for Accurate Post-Training Quantization and Pruning},
  author={Frantar, Elias and Sidak Pal Singh and Alistarh, Dan},
  journal={arXiv preprint arXiv:2208.11580},
  note={Accepted to NeurIPS 2022, to appear.},
  year={2022}
}


@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle=NeurIPS,
  year={2020}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{C4,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1--67}
}

@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={2021 ACM Conference on Fairness, Accountability, and Transparency},
  year={2021}
}


@article{li2020gpt,
  title={Demistifying GPT-3},
  author={Li, Chuan},
  journal={Lambda Cloud Blog},
  year={2021}, 
  note={\url{https://lambdalabs.com/blog/demystifying-gpt-3/}}
}

@article{wu2022extreme,
  title={Extreme Compression for Pre-trained Transformers Made Simple and Efficient},
  author={Wu, Xiaoxia and Yao, Zhewei and Zhang, Minjia and Li, Conglong and He, Yuxiong},
  journal={arXiv preprint arXiv:2206.01859},
  year={2022}
}




@article{park2022nuqmm,
  title={{nuQmm}: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models},
  author={Park, Gunho and Park, Baeseong and Kwon, Se Jung and Kim, Byeongwook and Lee, Youngjoo and Lee, Dongsoo},
  journal={arXiv preprint arXiv:2206.09557},
  year={2022}
}

@inproceedings{strom22_interspeech,
  author={Nikko Strom and Haidar Khan and Wael Hamza},
  title={{Squashed Weight Distribution for Low Bit Quantization of Deep Models}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={3953--3957},
  doi={10.21437/Interspeech.2022-50}
}

@article{ahmadian2023intriguing,
  title={Intriguing Properties of Quantization at Scale},
  author={Ahmadian, Arash and Dash, Saurabh and Chen, Hongyu and Venkitesh, Bharat and Gou, Stephen and Blunsom, Phil and {\"U}st{\"u}n, Ahmet and Hooker, Sara},
  journal={arXiv preprint arXiv:2305.19268},
  year={2023}
}

@article{wortsman2023stable,
  title={Stable and low-precision training for large-scale vision-language models},
  author={Wortsman, Mitchell and Dettmers, Tim and Zettlemoyer, Luke and Morcos, Ari and Farhadi, Ali and Schmidt, Ludwig},
  journal={arXiv preprint arXiv:2304.13013},
  year={2023}
}


%%%%%%
%zeroShot Datasets
%%%%%%

% LAMBADA:
@article{paperno2016lambada,
  title={The {LAMBADA} dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  journal={arXiv preprint arXiv:1606.06031},
  year={2016}
}

% PIQA:
@inproceedings{tata2003piqa,
  title={{PiQA}: An algebra for querying protein data sets},
  author={Tata, Sandeep and Patel, Jignesh M},
  booktitle={International Conference on Scientific and Statistical Database Management},
  year={2003}
}

% ARC:
@article{boratko2018systematic,
  title={A systematic classification of knowledge, reasoning, and context within the {ARC} dataset},
  author={Boratko, Michael and Padigela, Harshit and Mikkilineni, Divyendra and Yuvraj, Pritish and Das, Rajarshi and McCallum, Andrew and Chang, Maria and Fokoue-Nkoutche, Achille and Kapanipathi, Pavan and Mattei, Nicholas and others},
  journal={arXiv preprint arXiv:1806.00358},
  year={2018}
}

% StoryCloze
@inproceedings{mostafazadeh2017lsdsem,
  title={Lsdsem 2017 shared task: The story cloze test},
  author={Mostafazadeh, Nasrin and Roth, Michael and Louis, Annie and Chambers, Nathanael and Allen, James},
  booktitle={Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics},
  pages={46--51},
  year={2017}
}

@article{tao2022compression,
  title={Compression of Generative Pre-trained Language Models via Quantization},
  author={Tao, Chaofan and Hou, Lu and Zhang, Wei and Shang, Lifeng and Jiang, Xin and Liu, Qun and Luo, Ping and Wong, Ngai},
  journal={arXiv preprint arXiv:2203.10705},
  year={2022}
}

@article{frantar2023massive,
  title={Massive Language Models Can Be Accurately Pruned in One-Shot},
  author={Frantar, Elias and Alistarh, Dan},
  journal={arXiv preprint arXiv:2301.00774},
  year={2023}
}

@inproceedings{gorbachev2019openvino,
  title={Openvino deep learning workbench: Comprehensive analysis and tuning of neural networks inference},
  author={Gorbachev, Yury and Fedorov, Mikhail and Slavutin, Iliya and Tugarev, Artyom and Fatekhov, Marat and Tarkan, Yaroslav},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops},
  pages={0--0},
  year={2019}
}

@article{zheng2022alpa,
  title={Alpa: Automating Inter-and Intra-Operator Parallelism for Distributed Deep Learning},
  author={Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and Zhuang, Yonghao and Chen, Zhifeng and Huang, Yanping and Wang, Yida and Xu, Yuanzhong and Zhuo, Danyang and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2201.12023},
  year={2022}
}

@article{dao2022flashattention,
  title={{FlashAttention}: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author={Dao, Tri and Fu, Daniel Y and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2205.14135},
  year={2022}
}
@inproceedings{reddi2020mlperf,
  title={Mlperf inference benchmark},
  author={Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and others},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={446--459},
  year={2020},
  organization={IEEE}
}

@inproceedings{gale2020sparse,
  title={Sparse GPU kernels for deep learning},
  author={Gale, Trevor and Zaharia, Matei and Young, Cliff and Elsen, Erich},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--14},
  year={2020},
  organization={IEEE}
}

@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}
@misc{shazeer2020glu,
      title={GLU Variants Improve Transformer}, 
      author={Noam Shazeer},
      year={2020},
      eprint={2002.05202},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{chee2023quip,
      title={QuIP: 2-Bit Quantization of Large Language Models With Guarantees}, 
      author={Jerry Chee and Yaohui Cai and Volodymyr Kuleshov and Christopher De Sa},
      year={2023},
      eprint={2307.13304},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{malinovskii2024pv,
  title={PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression},
  author={Malinovskii, Vladimir and Mazur, Denis and Ilin, Ivan and Kuznedelev, Denis and Burlachenko, Konstantin and Yi, Kai and Alistarh, Dan and Richtarik, Peter},
  journal={arXiv preprint arXiv:2405.14852},
  year={2024}
}

@misc{quip-sharp,
      title={QuIP\#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks}, 
      author={Albert Tseng and Jerry Chee and Qingyao Sun and Volodymyr Kuleshov and Christopher De Sa},
      year={2024},
      eprint={2402.04396},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{lin2023awq,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}

@article{kim2023squeezellm,
  title={SqueezeLLM: Dense-and-Sparse Quantization},
  author={Kim, Sehoon and Hooper, Coleman and Gholami, Amir and Dong, Zhen and Li, Xiuyu and Shen, Sheng and Mahoney, Michael W and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2306.07629},
  year={2023}
}
@inproceedings{guo2016quantization,
  title={Quantization based fast inner product search},
  author={Guo, Ruiqi and Kumar, Sanjiv and Choromanski, Krzysztof and Simcha, David},
  booktitle={Artificial intelligence and statistics},
  pages={482--490},
  year={2016},
  organization={PMLR}
}


@Article{Zhou2017,
author={Zhou, Shu-Chang
and Wang, Yu-Zhi
and Wen, He
and He, Qin-Yao
and Zou, Yu-Heng},
title={Balanced Quantization: An Effective and Efficient Approach to Quantized Neural Networks},
journal={Journal of Computer Science and Technology},
year={2017},
month={Jul},
day={01},
volume={32},
number={4},
pages={667-682},
issn={1860-4749},
doi={10.1007/s11390-017-1750-y},
url={https://doi.org/10.1007/s11390-017-1750-y}
}

@misc{li2017performance,
      title={Performance Guaranteed Network Acceleration via High-Order Residual Quantization}, 
      author={Zefan Li and Bingbing Ni and Wenjun Zhang and Xiaokang Yang and Wen Gao},
      year={2017},
      eprint={1708.08687},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@article{Chen_Wang_Pan_2019, title={Deep Neural Network Quantization via Layer-Wise Optimization Using Limited Training Data}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4206}, DOI={10.1609/aaai.v33i01.33013329}, abstractNote={&lt;p&gt;The advancement of deep models poses great challenges to real-world deployment because of the limited computational ability and storage space on edge devices. To solve this problem, existing works have made progress to prune or quantize deep models. However, most existing methods rely heavily on a supervised training process to achieve satisfactory performance, acquiring large amount of labeled training data, which may not be practical for real deployment. In this paper, we propose a novel layer-wise quantization method for deep neural networks, which only requires limited training data (1% of original dataset). Specifically, we formulate parameters quantization for each layer as a discrete optimization problem, and solve it using Alternative Direction Method of Multipliers (ADMM), which gives an efficient closed-form solution. We prove that the final performance drop after quantization is bounded by a linear combination of the reconstructed errors caused at each layer. Based on the proved theorem, we propose an algorithm to quantize a deep neural network layer by layer with an additional weights update step to minimize the final error. Extensive experiments on benchmark deep models are conducted to demonstrate the effectiveness of our proposed method using 1% of CIFAR10 and ImageNet datasets. Codes are available in: https://github.com/csyhhu/L-DNQ&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Chen, Shangyu and Wang, Wenya and Pan, Sinno Jialin}, year={2019}, month={Jul.}, pages={3329-3336} 
}

@misc{hinton2015distilling,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{sun2019patient,
  title={Patient knowledge distillation for bert model compression},
  author={Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
  journal={arXiv preprint arXiv:1908.09355},
  year={2019}
}

@misc{kurtic2023sparse,
      title={Sparse Fine-tuning for Inference Acceleration of Large Language Models}, 
      author={Eldar Kurtic and Denis Kuznedelev and Elias Frantar and Michael Goin and Dan Alistarh},
      year={2023},
      eprint={2310.06927},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{frantar2023qmoe,
  title={QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models},
  author={Frantar, Elias and Alistarh, Dan},
  journal={arXiv preprint arXiv:2310.16795},
  year={2023}
}
