\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Baldassi et~al.(2015)Baldassi, Ingrosso, Lucibello, Saglietti, and
  Zecchina]{baldassi2015subdominant}
Baldassi, C., Ingrosso, A., Lucibello, C., Saglietti, L., and Zecchina, R.
\newblock Subdominant dense clusters allow for simple learning and high
  computational performance in neural networks with discrete synapses.
\newblock \emph{Physical review letters}, 115\penalty0 (12):\penalty0 128101,
  2015.
\newblock \doi{10.1103/PhysRevLett.115.128101}.
\newblock URL \url{https://doi.org/10.1103/PhysRevLett.115.128101}.

\bibitem[Baldassi et~al.(2016)Baldassi, Borgs, Chayes, Ingrosso, Lucibello,
  Saglietti, and Zecchina]{baldassi2016unreasonable}
Baldassi, C., Borgs, C., Chayes, J.~T., Ingrosso, A., Lucibello, C., Saglietti,
  L., and Zecchina, R.
\newblock Unreasonable effectiveness of learning neural networks: From
  accessible states and robust ensembles to basic algorithmic schemes.
\newblock \emph{Proceedings of the National Academy of Sciences}, 113\penalty0
  (48):\penalty0 E7655--E7662, 2016.
\newblock \doi{10.1073/pnas.1608103113}.
\newblock URL \url{https://doi.org/10.1073/pnas.1608103113}.

\bibitem[Baldassi et~al.(2018)Baldassi, Gerace, Kappen, Lucibello, Saglietti,
  Tartaglione, and Zecchina]{baldassi2018role}
Baldassi, C., Gerace, F., Kappen, H.~J., Lucibello, C., Saglietti, L.,
  Tartaglione, E., and Zecchina, R.
\newblock Role of synaptic stochasticity in training low-precision neural
  networks.
\newblock \emph{Physical review letters}, 120\penalty0 (26):\penalty0 268103,
  Jun 2018.
\newblock \doi{10.1103/PhysRevLett.120.268103}.
\newblock URL \url{https://link.aps.org/doi/10.1103/PhysRevLett.120.268103}.

\bibitem[Baldassi et~al.(2019)Baldassi, Malatesta, and Zecchina]{relu_locent}
Baldassi, C., Malatesta, E.~M., and Zecchina, R.
\newblock Properties of the geometry of solutions and capacity of multilayer
  neural networks with rectified linear unit activations.
\newblock \emph{Phys. Rev. Lett.}, 123:\penalty0 170602, Oct 2019.
\newblock \doi{10.1103/PhysRevLett.123.170602}.
\newblock URL \url{https://link.aps.org/doi/10.1103/PhysRevLett.123.170602}.

\bibitem[Baldassi et~al.(2020)Baldassi, Pittorino, and Zecchina]{Baldassi20}
Baldassi, C., Pittorino, F., and Zecchina, R.
\newblock Shaping the learning landscape in neural networks around wide flat
  minima.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (1):\penalty0 161--170, 2020.
\newblock ISSN 0027-8424.
\newblock URL \url{https://www.pnas.org/content/117/1/161}.

\bibitem[Baldassi et~al.(2021{\natexlab{a}})Baldassi, Lauditi, Malatesta,
  Pacelli, Perugini, and Zecchina]{baldassi2021learning}
Baldassi, C., Lauditi, C., Malatesta, E.~M., Pacelli, R., Perugini, G., and
  Zecchina, R.
\newblock Learning through atypical''phase transitions''in overparameterized
  neural networks.
\newblock \emph{arXiv preprint arXiv:2110.00683}, 2021{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2110.00683}.

\bibitem[Baldassi et~al.(2021{\natexlab{b}})Baldassi, Lauditi, Malatesta,
  Perugini, and Zecchina]{baldassi2021unveiling}
Baldassi, C., Lauditi, C., Malatesta, E.~M., Perugini, G., and Zecchina, R.
\newblock Unveiling the structure of wide flat minima in neural networks.
\newblock \emph{Physical Review Letters}, 127\penalty0 (27):\penalty0 278301,
  Dec 2021{\natexlab{b}}.
\newblock \doi{10.1103/PhysRevLett.127.278301}.
\newblock URL \url{https://link.aps.org/doi/10.1103/PhysRevLett.127.278301}.

\bibitem[Belkin et~al.(2019)Belkin, Hsu, Ma, and Mandal]{belkin2019reconciling}
Belkin, M., Hsu, D., Ma, S., and Mandal, S.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (32):\penalty0 15849--15854, 2019.
\newblock ISSN 0027-8424.
\newblock \doi{10.1073/pnas.1903070116}.
\newblock URL \url{https://www.pnas.org/content/116/32/15849}.

\bibitem[Brea et~al.(2019)Brea, Simsek, Illing, and Gerstner]{brea2019weight}
Brea, J., Simsek, B., Illing, B., and Gerstner, W.
\newblock Weight-space symmetry in deep networks gives rise to permutation
  saddles, connected by equal-loss valleys across the loss landscape.
\newblock \emph{arXiv preprint arXiv:1907.02911}, 2019.
\newblock URL \url{https://arxiv.org/abs/1907.02911}.

\bibitem[Chaudhari \& Soatto(2018)Chaudhari and
  Soatto]{chaudhari2018stochastic}
Chaudhari, P. and Soatto, S.
\newblock Stochastic gradient descent performs variational inference, converges
  to limit cycles for deep networks.
\newblock In \emph{2018 Information Theory and Applications Workshop (ITA)},
  pp.\  1--10. IEEE, 2018.
\newblock \doi{10.1109/ITA.2018.8503224}.
\newblock URL \url{https://doi.org/10.1109/ITA.2018.8503224}.

\bibitem[Chaudhari et~al.(2019)Chaudhari, Choromanska, Soatto, LeCun, Baldassi,
  Borgs, Chayes, Sagun, and Zecchina]{chaudhari2019entropy}
Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y., Baldassi, C., Borgs, C.,
  Chayes, J., Sagun, L., and Zecchina, R.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2019\penalty0 (12):\penalty0 124018, 2019.
\newblock \doi{10.1088/1742-5468/ab39d9}.
\newblock URL \url{https://doi.org/10.1088/1742-5468/ab39d9}.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh2017sharp}
Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1019--1028. PMLR, 2017.

\bibitem[Draxler et~al.(2018)Draxler, Veschgini, Salmhofer, and
  Hamprecht]{draxler2018}
Draxler, F., Veschgini, K., Salmhofer, M., and Hamprecht, F.
\newblock Essentially no barriers in neural network energy landscape.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1309--1318. PMLR, 10--15 Jul 2018.
\newblock URL \url{http://proceedings.mlr.press/v80/draxler18a.html}.

\bibitem[Dziugaite \& Roy(2017)Dziugaite and Roy]{dziugaite2017computing}
Dziugaite, G.~K. and Roy, D.~M.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{arXiv preprint arXiv:1703.11008}, 2017.
\newblock URL \url{https://arxiv.org/abs/1703.11008}.

\bibitem[Entezari et~al.(2021)Entezari, Sedghi, Saukh, and
  Neyshabur]{entezari2021role}
Entezari, R., Sedghi, H., Saukh, O., and Neyshabur, B.
\newblock The role of permutation invariance in linear mode connectivity of
  neural networks.
\newblock \emph{arXiv preprint arXiv:2110.06296}, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.06296}.

\bibitem[Feng \& Tu(2021)Feng and Tu]{feng2021inverse}
Feng, Y. and Tu, Y.
\newblock The inverse variance--flatness relation in stochastic gradient
  descent is critical for finding flat minima.
\newblock \emph{Proceedings of the National Academy of Sciences}, 118\penalty0
  (9), 2021.
\newblock ISSN 0027-8424.
\newblock \doi{10.1073/pnas.2015617118}.
\newblock URL \url{https://www.pnas.org/content/118/9/e2015617118}.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2021sharpnessaware}
Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=6Tm1mposlrM}.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{garipov2018}
Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D.~P., and Wilson, A.~G.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/be3087e74e9100d4bc4c6268cdbe8456-Paper.pdf}.

\bibitem[Gerace et~al.(2020)Gerace, Loureiro, Krzakala, M{\'e}zard, and
  Zdeborov{\'a}]{gerace2020generalisation}
Gerace, F., Loureiro, B., Krzakala, F., M{\'e}zard, M., and Zdeborov{\'a}, L.
\newblock Generalisation error in learning with random features and the hidden
  manifold model.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{International Conference on
  Machine Learning}, volume 119 of \emph{Proceedings of Machine Learning
  Research}, pp.\  3452--3462. PMLR, PMLR, 13--18 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/gerace20a.html}.

\bibitem[Goldt et~al.(2020)Goldt, M{\'e}zard, Krzakala, and
  Zdeborov{\'a}]{goldt2020modeling}
Goldt, S., M{\'e}zard, M., Krzakala, F., and Zdeborov{\'a}, L.
\newblock Modeling the influence of data structure on learning in neural
  networks: The hidden manifold model.
\newblock \emph{Physical Review X}, 10\penalty0 (4):\penalty0 041044, 2020.
\newblock \doi{10.1103/PhysRevX.10.041044}.
\newblock URL \url{https://link.aps.org/doi/10.1103/PhysRevX.10.041044}.

\bibitem[Gotmare et~al.(2018)Gotmare, Keskar, Xiong, and Socher]{gotmare2018}
Gotmare, A., Keskar, N.~S., Xiong, C., and Socher, R.
\newblock Using mode connectivity for loss landscape analysis.
\newblock \emph{arXiv preprint arXiv:1806.06977}, 2018.
\newblock URL \url{https://arxiv.org/abs/1806.06977}.

\bibitem[Hinton \& Van~Camp(1993)Hinton and Van~Camp]{hinton1993keeping}
Hinton, G.~E. and Van~Camp, D.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In \emph{Proceedings of the sixth annual conference on Computational
  learning theory}, pp.\  5--13, 1993.
\newblock URL \url{https://dl.acm.org/doi/pdf/10.1145/168304.168306}.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997flat}
Hochreiter, S. and Schmidhuber, J.
\newblock Flat minima.
\newblock \emph{Neural computation}, 9\penalty0 (1):\penalty0 1--42, 1997.
\newblock \doi{10.1162/neco.1997.9.1.1}.
\newblock URL \url{https://doi.org/10.1162/neco.1997.9.1.1}.

\bibitem[Hubara et~al.(2016)Hubara, Courbariaux, Soudry, El-Yaniv, and
  Bengio]{hubara2016binarized}
Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y.
\newblock Binarized neural networks.
\newblock In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2016/file/d8330f857a17c53d217014ee776bfd50-Paper.pdf}.

\bibitem[Jiang* et~al.(2020)Jiang*, Neyshabur*, Mobahi, Krishnan, and
  Bengio]{Jiang2020Fantastic}
Jiang*, Y., Neyshabur*, B., Mobahi, H., Krishnan, D., and Bengio, S.
\newblock Fantastic generalization measures and where to find them.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=SJgIPJBFvH}.

\bibitem[J{\'o}nsson et~al.(1998)J{\'o}nsson, Mills, and
  Jacobsen]{jonsson1998nudged}
J{\'o}nsson, H., Mills, G., and Jacobsen, K.~W.
\newblock Nudged elastic band method for finding minimum energy paths of
  transitions.
\newblock 1998.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kuditipudi et~al.(2019)Kuditipudi, Wang, Lee, Zhang, Li, Hu, Arora,
  and Ge]{kuditipudi2019explaining}
Kuditipudi, R., Wang, X., Lee, H., Zhang, Y., Li, Z., Hu, W., Arora, S., and
  Ge, R.
\newblock Explaining landscape connectivity of low-cost solutions for
  multilayer nets.
\newblock \emph{arXiv preprint arXiv:1906.06247}, 2019.

\bibitem[Liu et~al.(2020)Liu, Papailiopoulos, and Achlioptas]{bad_minima_sgd}
Liu, S., Papailiopoulos, D., and Achlioptas, D.
\newblock Bad global minima exist and sgd can reach them.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  8543--8552. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/618491e20a9b686b79e158c293ab4f91-Paper.pdf}.

\bibitem[Mei \& Montanari(2019)Mei and Montanari]{mei2019generalization}
Mei, S. and Montanari, A.
\newblock The generalization error of random features regression: Precise
  asymptotics and the double descent curve.
\newblock \emph{Communications on Pure and Applied Mathematics}, 2019.

\bibitem[Pittorino et~al.(2021)Pittorino, Lucibello, Feinauer, Perugini,
  Baldassi, Demyanenko, and Zecchina]{pittorino2021}
Pittorino, F., Lucibello, C., Feinauer, C., Perugini, G., Baldassi, C.,
  Demyanenko, E., and Zecchina, R.
\newblock Entropic gradient descent algorithms and wide flat minima.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=xjXg0bnoDmS}.

\bibitem[Rocks \& Mehta(2020)Rocks and Mehta]{rocks2020memorizing}
Rocks, J.~W. and Mehta, P.
\newblock Memorizing without overfitting: Bias, variance, and interpolation in
  over-parameterized models.
\newblock \emph{arXiv preprint arXiv:2010.13933}, 2020.
\newblock URL \url{https://arxiv.org/abs/2010.13933}.

\bibitem[Shevchenko \& Mondelli(2020)Shevchenko and Mondelli]{shevchenko20a}
Shevchenko, A. and Mondelli, M.
\newblock Landscape connectivity and dropout stability of {SGD} solutions for
  over-parameterized neural networks.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  8773--8784. PMLR,
  13--18 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/shevchenko20a.html}.

\bibitem[Simons \& Lee(2019)Simons and Lee]{simons2019review}
Simons, T. and Lee, D.-J.
\newblock A review of binarized neural networks.
\newblock \emph{Electronics}, 8\penalty0 (6):\penalty0 661, 2019.

\bibitem[Singh \& Jaggi(2020)Singh and Jaggi]{singh2020model}
Singh, S.~P. and Jaggi, M.
\newblock Model fusion via optimal transport.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Tatro et~al.(2020)Tatro, Chen, Das, Melnyk, Sattigeri, and
  Lai]{tatro2020}
Tatro, N., Chen, P.-Y., Das, P., Melnyk, I., Sattigeri, P., and Lai, R.
\newblock Optimizing mode connectivity via neuron alignment.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  15300--15311. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/aecad42329922dfc97eee948606e1f8e-Paper.pdf}.

\bibitem[Virtanen et~al.(2020)Virtanen, Gommers, Oliphant, Haberland, Reddy,
  Cournapeau, Burovski, Peterson, Weckesser, Bright, {van der Walt}, Brett,
  Wilson, Millman, Mayorov, Nelson, Jones, Kern, Larson, Carey, Polat, Feng,
  Moore, {VanderPlas}, Laxalde, Perktold, Cimrman, Henriksen, Quintero, Harris,
  Archibald, Ribeiro, Pedregosa, {van Mulbregt}, and {SciPy 1.0
  Contributors}]{2020SciPy-NMeth}
Virtanen, P., Gommers, R., Oliphant, T.~E., Haberland, M., Reddy, T.,
  Cournapeau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, J., {van
  der Walt}, S.~J., Brett, M., Wilson, J., Millman, K.~J., Mayorov, N., Nelson,
  A. R.~J., Jones, E., Kern, R., Larson, E., Carey, C.~J., Polat, {\.I}., Feng,
  Y., Moore, E.~W., {VanderPlas}, J., Laxalde, D., Perktold, J., Cimrman, R.,
  Henriksen, I., Quintero, E.~A., Harris, C.~R., Archibald, A.~M., Ribeiro,
  A.~H., Pedregosa, F., {van Mulbregt}, P., and {SciPy 1.0 Contributors}.
\newblock {{SciPy} 1.0: Fundamental Algorithms for Scientific Computing in
  Python}.
\newblock \emph{Nature Methods}, 17:\penalty0 261--272, 2020.
\newblock \doi{10.1038/s41592-019-0686-2}.

\bibitem[Yue et~al.(2020)Yue, Nouiehed, and Kontar]{yue2020salr}
Yue, X., Nouiehed, M., and Kontar, R.~A.
\newblock Salr: Sharpness-aware learning rates for improved generalization.
\newblock \emph{arXiv preprint arXiv:2011.05348}, 2020.

\bibitem[Zhang et~al.(2015)Zhang, Zou, He, and Sun]{zhang2015accelerating}
Zhang, X., Zou, J., He, K., and Sun, J.
\newblock Accelerating very deep convolutional networks for classification and
  detection.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 38\penalty0 (10):\penalty0 1943--1955, 2015.
\newblock \doi{10.1109/TPAMI.2015.2502579}.

\bibitem[Zhang \& Strogatz(2021)Zhang and Strogatz]{tentacles}
Zhang, Y. and Strogatz, S.~H.
\newblock Basins with tentacles.
\newblock \emph{Phys. Rev. Lett.}, 127:\penalty0 194101, Nov 2021.
\newblock \doi{10.1103/PhysRevLett.127.194101}.
\newblock URL \url{https://link.aps.org/doi/10.1103/PhysRevLett.127.194101}.

\bibitem[Zhang et~al.(2021)Zhang, Zhou, and Xu]{zhang2021variance}
Zhang, Z., Zhou, H., and Xu, Z.-Q.~J.
\newblock A variance principle explains why dropout finds flatter minima.
\newblock \emph{arXiv preprint arXiv:2111.01022}, 2021.
\newblock URL \url{https://arxiv.org/abs/2111.01022}.

\bibitem[Zhao et~al.(2020)Zhao, Chen, Das, Ramamurthy, and
  Lin]{Zhao2020Bridging}
Zhao, P., Chen, P.-Y., Das, P., Ramamurthy, K.~N., and Lin, X.
\newblock Bridging mode connectivity in loss landscapes and adversarial
  robustness.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=SJgwzCEKwH}.

\bibitem[Zhou et~al.(2018)Zhou, Veitch, Austern, Adams, and
  Orbanz]{zhou2018non}
Zhou, W., Veitch, V., Austern, M., Adams, R.~P., and Orbanz, P.
\newblock Non-vacuous generalization bounds at the imagenet scale: a
  pac-bayesian compression approach.
\newblock \emph{arXiv preprint arXiv:1804.05862}, 2018.

\end{thebibliography}
