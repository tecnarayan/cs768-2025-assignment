\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2019)Arora, Khandeparkar, Khodak, Plevrakis, and
  Saunshi]{arora2019theoretical}
Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and
  Nikunj Saunshi.
\newblock A theoretical analysis of contrastive unsupervised representation
  learning.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Chen et~al.(2021)Chen, Xie, Zhang, Yan, Deng, Tan, Huang, Si, and
  Chen]{chen2021adaprompt}
Xiang Chen, Xin Xie, Ningyu Zhang, Jiahuan Yan, Shumin Deng, Chuanqi Tan, Fei
  Huang, Luo Si, and Huajun Chen.
\newblock Adaprompt: Adaptive prompt-based finetuning for relation extraction.
\newblock \emph{arXiv preprint arXiv:2104.07650}, 2021.

\bibitem[Chiu and Rush(2020)]{chiu2020scaling}
Justin~T Chiu and Alexander~M Rush.
\newblock Scaling hidden markov language models.
\newblock \emph{arXiv preprint arXiv:2011.04640}, 2020.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Ethayarajh(2019)]{ethayarajh2019contextual}
Kawin Ethayarajh.
\newblock How contextual are contextualized word representations? comparing the
  geometry of bert, elmo, and gpt-2 embeddings.
\newblock \emph{arXiv preprint arXiv:1909.00512}, 2019.

\bibitem[Gao et~al.(2020)Gao, Fisch, and Chen]{gao2020making}
Tianyu Gao, Adam Fisch, and Danqi Chen.
\newblock Making pre-trained language models better few-shot learners.
\newblock \emph{arXiv preprint arXiv:2012.15723}, 2020.

\bibitem[Giulianelli et~al.(2018)Giulianelli, Harding, Mohnert, Hupkes, and
  Zuidema]{giulianelli2018under}
Mario Giulianelli, Jack Harding, Florian Mohnert, Dieuwke Hupkes, and Willem
  Zuidema.
\newblock Under the hood: Using diagnostic classifiers to investigate and
  improve how language models track agreement information.
\newblock \emph{arXiv preprint arXiv:1808.08079}, 2018.

\bibitem[Gruber et~al.(2007)Gruber, Weiss, and Rosen-Zvi]{gruber2007hidden}
Amit Gruber, Yair Weiss, and Michal Rosen-Zvi.
\newblock Hidden topic markov models.
\newblock In \emph{Artificial intelligence and statistics}, pages 163--170.
  PMLR, 2007.

\bibitem[Hambardzumyan et~al.(2021)Hambardzumyan, Khachatrian, and
  May]{hambardzumyan2021warp}
Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May.
\newblock Warp: Word-level adversarial reprogramming.
\newblock \emph{arXiv preprint arXiv:2101.00121}, 2021.

\bibitem[HaoChen et~al.(2021)HaoChen, Wei, Gaidon, and Ma]{haochen2021provable}
Jeff~Z. HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma.
\newblock Provable guarantees for self-supervised deep learning with spectral
  contrastive loss, 2021.

\bibitem[Hewitt and Manning(2019)]{hewitt2019structural}
John Hewitt and Christopher~D Manning.
\newblock A structural probe for finding syntax in word representations.
\newblock In \emph{Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4129--4138, 2019.

\bibitem[Jawahar et~al.(2019)Jawahar, Sagot, and Seddah]{jawahar2019does}
Ganesh Jawahar, Beno{\^\i}t Sagot, and Djam{\'e} Seddah.
\newblock What does bert learn about the structure of language?
\newblock In \emph{ACL 2019-57th Annual Meeting of the Association for
  Computational Linguistics}, 2019.

\bibitem[Jiang et~al.(2020)Jiang, Xu, Araki, and Neubig]{jiang2020can}
Zhengbao Jiang, Frank~F Xu, Jun Araki, and Graham Neubig.
\newblock How can we know what language models know?
\newblock \emph{Transactions of the Association for Computational Linguistics},
  8:\penalty0 423--438, 2020.

\bibitem[Joshi et~al.(2020)Joshi, Chen, Liu, Weld, Zettlemoyer, and
  Levy]{joshi2020spanbert}
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel~S Weld, Luke Zettlemoyer, and Omer
  Levy.
\newblock Spanbert: Improving pre-training by representing and predicting
  spans.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  8:\penalty0 64--77, 2020.

\bibitem[Kim et~al.(2020)Kim, Choi, Edmiston, and Lee]{kim2020pre}
Taeuk Kim, Jihun Choi, Daniel Edmiston, and Sang-goo Lee.
\newblock Are pre-trained language models aware of phrases? simple but strong
  baselines for grammar induction.
\newblock \emph{arXiv preprint arXiv:2002.00737}, 2020.

\bibitem[Koller and Friedman(2009)]{koller2009probabilistic}
Daphne Koller and Nir Friedman.
\newblock \emph{Probabilistic graphical models: principles and techniques}.
\newblock MIT press, 2009.

\bibitem[Kumar et~al.(2022)Kumar, Raghunathan, Jones, Ma, and
  Liang]{kumar2022fine}
Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang.
\newblock Fine-tuning can distort pretrained features and underperform
  out-of-distribution.
\newblock \emph{arXiv preprint arXiv:2202.10054}, 2022.

\bibitem[Kupiec(1992)]{kupiec1992robust}
Julian Kupiec.
\newblock Robust part-of-speech tagging using a hidden markov model.
\newblock \emph{Computer speech \& language}, 6\penalty0 (3):\penalty0
  225--242, 1992.

\bibitem[Lee et~al.(2020)Lee, Lei, Saunshi, and Zhuo]{lee2020predicting}
Jason~D Lee, Qi~Lei, Nikunj Saunshi, and Jiacheng Zhuo.
\newblock Predicting what you already know helps: Provable self-supervised
  learning.
\newblock \emph{arXiv preprint arXiv:2008.01064}, 2020.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock \emph{arXiv preprint arXiv:2104.08691}, 2021.

\bibitem[Levine et~al.(2020)Levine, Lenz, Lieber, Abend, Leyton-Brown,
  Tennenholtz, and Shoham]{levine2020pmi}
Yoav Levine, Barak Lenz, Opher Lieber, Omri Abend, Kevin Leyton-Brown, Moshe
  Tennenholtz, and Yoav Shoham.
\newblock Pmi-masking: Principled masking of correlated spans.
\newblock \emph{arXiv preprint arXiv:2010.01825}, 2020.

\bibitem[Li and Liang(2021)]{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock \emph{arXiv}, 2021.

\bibitem[Liu et~al.(2021)Liu, HaoChen, Gaidon, and Ma]{liu2021selfsupervised}
Hong Liu, Jeff~Z. HaoChen, Adrien Gaidon, and Tengyu Ma.
\newblock Self-supervised learning is more robust to dataset imbalance, 2021.

\bibitem[Peters et~al.(2018)Peters, Neumann, Iyyer, Gardner, Clark, Lee, and
  Zettlemoyer]{peters2018deep}
Matthew~E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
  Kenton Lee, and Luke Zettlemoyer.
\newblock Deep contextualized word representations.
\newblock \emph{arXiv preprint arXiv:1802.05365}, 2018.

\bibitem[Qin and Eisner(2021)]{qin2021learning}
Guanghui Qin and Jason Eisner.
\newblock Learning how to ask: Querying lms with mixtures of soft prompts.
\newblock \emph{arXiv preprint arXiv:2104.06599}, 2021.

\bibitem[Rabiner and Juang(1986)]{rabiner1986introduction}
Lawrence Rabiner and Biinghwang Juang.
\newblock An introduction to hidden markov models.
\newblock \emph{ieee assp magazine}, 3\penalty0 (1):\penalty0 4--16, 1986.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}, 2019.

\bibitem[Rogers et~al.(2020)Rogers, Kovaleva, and Rumshisky]{rogers2020primer}
Anna Rogers, Olga Kovaleva, and Anna Rumshisky.
\newblock A primer in bertology: What we know about how bert works.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  8:\penalty0 842--866, 2020.

\bibitem[Saunshi et~al.(2020)Saunshi, Malladi, and
  Arora]{saunshi2020mathematical}
Nikunj Saunshi, Sadhika Malladi, and Sanjeev Arora.
\newblock A mathematical exploration of why language models help solve
  downstream tasks.
\newblock \emph{arXiv preprint arXiv:2010.03648}, 2020.

\bibitem[Schick and Sch{\"u}tze(2020{\natexlab{a}})]{schick2020exploiting}
Timo Schick and Hinrich Sch{\"u}tze.
\newblock Exploiting cloze questions for few shot text classification and
  natural language inference.
\newblock \emph{arXiv preprint arXiv:2001.07676}, 2020{\natexlab{a}}.

\bibitem[Schick and Sch{\"u}tze(2020{\natexlab{b}})]{schick2020s}
Timo Schick and Hinrich Sch{\"u}tze.
\newblock It's not just size that matters: Small language models are also
  few-shot learners.
\newblock \emph{arXiv preprint arXiv:2009.07118}, 2020{\natexlab{b}}.

\bibitem[Shin et~al.(2020)Shin, Razeghi, Logan~IV, Wallace, and
  Singh]{shin2020autoprompt}
Taylor Shin, Yasaman Razeghi, Robert~L Logan~IV, Eric Wallace, and Sameer
  Singh.
\newblock Autoprompt: Eliciting knowledge from language models with
  automatically generated prompts.
\newblock \emph{arXiv preprint arXiv:2010.15980}, 2020.

\bibitem[Sinha et~al.(2021)Sinha, Jia, Hupkes, Pineau, Williams, and
  Kiela]{sinha2021masked}
Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, and
  Douwe Kiela.
\newblock Masked language modeling and the distributional hypothesis: Order
  word matters pre-training for little.
\newblock \emph{arXiv preprint arXiv:2104.06644}, 2021.

\bibitem[Sukhbaatar et~al.(2015)Sukhbaatar, Szlam, Weston, and
  Fergus]{sukhbaatar2015end}
Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.
\newblock End-to-end memory networks.
\newblock \emph{arXiv preprint arXiv:1503.08895}, 2015.

\bibitem[Tenney et~al.(2019{\natexlab{a}})Tenney, Das, and
  Pavlick]{tenney2019bert}
Ian Tenney, Dipanjan Das, and Ellie Pavlick.
\newblock Bert rediscovers the classical nlp pipeline.
\newblock \emph{arXiv preprint arXiv:1905.05950}, 2019{\natexlab{a}}.

\bibitem[Tenney et~al.(2019{\natexlab{b}})Tenney, Xia, Chen, Wang, Poliak,
  McCoy, Kim, Van~Durme, Bowman, Das, et~al.]{tenney2019you}
Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R~Thomas McCoy,
  Najoung Kim, Benjamin Van~Durme, Samuel~R Bowman, Dipanjan Das, et~al.
\newblock What do you learn from context? probing for sentence structure in
  contextualized word representations.
\newblock \emph{arXiv preprint arXiv:1905.06316}, 2019{\natexlab{b}}.

\bibitem[Tosh et~al.(2020)Tosh, Krishnamurthy, and Hsu]{tosh2020contrastive}
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu.
\newblock Contrastive estimation reveals topic posterior information to linear
  models.
\newblock \emph{arXiv:2003.02234}, 2020.

\bibitem[Tosh et~al.(2021)Tosh, Krishnamurthy, and Hsu]{tosh2021contrastive}
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu.
\newblock Contrastive learning, multi-view redundancy, and linear models.
\newblock In \emph{Algorithmic Learning Theory}, pages 1179--1206. PMLR, 2021.

\bibitem[Wei et~al.(2020)Wei, Shen, Chen, and Ma]{wei2020theoretical}
Colin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma.
\newblock Theoretical analysis of self-training with deep networks on unlabeled
  data, 2020.
\newblock URL \url{https://openreview.net/forum?id=rC8sJ4i6kaH}.

\bibitem[Weston et~al.(2014)Weston, Chopra, and Bordes]{weston2014memory}
Jason Weston, Sumit Chopra, and Antoine Bordes.
\newblock Memory networks.
\newblock \emph{arXiv preprint arXiv:1410.3916}, 2014.

\bibitem[Zhang and Hashimoto(2021)]{zhang2021inductive}
Tianyi Zhang and Tatsunori Hashimoto.
\newblock On the inductive bias of masked language modeling: From statistical
  to syntactic dependencies.
\newblock \emph{arXiv preprint arXiv:2104.05694}, 2021.

\bibitem[Zhong et~al.(2021)Zhong, Friedman, and Chen]{zhong2021factual}
Zexuan Zhong, Dan Friedman, and Danqi Chen.
\newblock Factual probing is [mask]: Learning vs. learning to recall.
\newblock \emph{arXiv preprint arXiv:2104.05240}, 2021.

\end{thebibliography}
