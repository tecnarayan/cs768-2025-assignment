@article{kartik2021upper,
  title={Upper and lower values in zero-sum stochastic games with asymmetric information},
  author={Kartik, Dhruva and Nayyar, Ashutosh},
  journal={Dynamic Games and Applications},
  volume={11},
  pages={363--388},
  year={2021},
  publisher={Springer}
}
@inproceedings{timeable,
author = {Jakobsen, Sune K. and S\o{}rensen, Troels B. and Conitzer, Vincent},
title = {Timeability of Extensive-Form Games},
year = {2016},
isbn = {9781450340571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2840728.2840737},
doi = {10.1145/2840728.2840737},
abstract = {Extensive-form games constitute the standard representation scheme for games with a temporal component. But do all extensive-form games correspond to protocols that we can implement in the real world? We often rule out games with imperfect recall, which prescribe that an agent forget something that she knew before. In this paper, we show that even some games with perfect recall can be problematic to implement. Specifically, we show that if the agents have a sense of time passing (say, access to a clock), then some extensive-form games can no longer be implemented; no matter how we attempt to time the game, some information will leak to the agents that they are not supposed to have. We say such a game is not exactly timeable. We provide easy-to-check necessary and sufficient conditions for a game to be exactly timeable. Most of the technical depth of the paper concerns how to approximately time games, which we show can always be done, though it may require large amounts of time. Specifically, we show that some games require time proportional to the power tower of height proportional to the number of players, which in practice would make them untimeable. We hope to convince the reader that timeability should be a standard assumption, just as perfect recall is today. Besides the conceptual contribution to game theory, we show that timeability has implications for onion routing protocols.},
booktitle = {Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
pages = {191–199},
numpages = {9},
keywords = {computational game theory, equilibrium computation},
location = {Cambridge, Massachusetts, USA},
series = {ITCS '16}
}
@INPROCEEDINGS{nayyar-zs,
  author={Nayyar, Ashutosh and Gupta, Abhishek},
  booktitle={2017 American Control Conference (ACC)}, 
  title={Information structures and values in zero-sum stochastic games}, 
  year={2017},
  volume={},
  number={},
  pages={3658-3663},
  doi={10.23919/ACC.2017.7963513}}
@inproceedings{
mmd,
title={A Unified Approach to Reinforcement Learning, Quantal Response Equilibria, and Two-Player Zero-Sum Games},
author={Samuel Sokota and Ryan D'Orazio and J Zico Kolter and Nicolas Loizou and Marc Lanctot and Ioannis Mitliagkas and Noam Brown and Christian Kroer},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=DpE5UYUQzZH}
}
@inproceedings{cfr,
 author = {Zinkevich, Martin and Johanson, Michael and Bowling, Michael and Piccione, Carmelo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Regret Minimization in Games with Incomplete Information},
 url = {https://proceedings.neurips.cc/paper/2007/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf},
 volume = {20},
 year = {2007}
}
@article{BARD2020103216,
title = {The Hanabi challenge: A new frontier for AI research},
journal = {Artificial Intelligence},
volume = {280},
pages = {103216},
year = {2020},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2019.103216},
url = {https://www.sciencedirect.com/science/article/pii/S0004370219300116},
author = {Nolan Bard and Jakob N. Foerster and Sarath Chandar and Neil Burch and Marc Lanctot and H. Francis Song and Emilio Parisotto and Vincent Dumoulin and Subhodeep Moitra and Edward Hughes and Iain Dunning and Shibl Mourad and Hugo Larochelle and Marc G. Bellemare and Michael Bowling},
keywords = {Multi-agent learning, Challenge paper, Reinforcement learning, Games, Theory of mind, Communication, Imperfect information, Cooperative},
abstract = {From the early days of computing, games have been important testbeds for studying how well machines can do sophisticated decision making. In recent years, machine learning has made dramatic advances with artificial agents reaching superhuman performance in challenge domains like Go, Atari, and some variants of poker. As with their predecessors of chess, checkers, and backgammon, these game domains have driven research by providing sophisticated yet well-defined challenges for artificial intelligence practitioners. We continue this tradition by proposing the game of Hanabi as a new challenge domain with novel problems that arise from its combination of purely cooperative gameplay with two to five players and imperfect information. In particular, we argue that Hanabi elevates reasoning about the beliefs and intentions of other agents to the foreground. We believe developing novel techniques for such theory of mind reasoning will not only be crucial for success in Hanabi, but also in broader collaborative efforts, especially those with human partners. To facilitate future research, we introduce the open-source Hanabi Learning Environment, propose an experimental framework for the research community to evaluate algorithmic advances, and assess the performance of current state-of-the-art techniques.}
}
@phdthesis{exit-thesis,
  title={Expert iteration},
  author={Anthony, Thomas William},
  year={2021},
  school={UCL (University College London)}
}
@article{Zarick2020UnlockingTP,
  title={Unlocking the Potential of Deep Counterfactual Value Networks},
  author={Ryan Zarick and Bryan Pellegrino and Noam Brown and Caleb Banister},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.10442}
}
@inproceedings{
zhang2022subgame,
title={Subgame Solving in Adversarial Team Games},
author={Brian Hu Zhang and Luca Carminati and Federico Cacciamani and Gabriele Farina and Pierriccardo Olivieri and Nicola Gatti and Tuomas Sandholm},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=Roiw2Trm-qP}
}
@misc{delage_2022,
  doi = {10.48550/ARXIV.2210.14640},
  
  url = {https://arxiv.org/abs/2210.14640},
  
  author = {Delage, Aurélien and Buffet, Olivier and Dibangoye, Jilles S. and Saffidine, Abdallah},
  
  keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {HSVI can solve zero-sum Partially Observable Stochastic Games},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{sustr,
  author    = {Michal Sustr and
               Martin Schmid and
               Matej Moravc{\'{\i}}k and
               Neil Burch and
               Marc Lanctot and
               Michael Bowling},
  title     = {Sound Algorithms in Imperfect Information Games},
  booktitle = {{AAMAS} '21: 20th International Conference on Autonomous Agents and Multiagent Systems, Virtual Event, United Kingdom, May 3-7, 2021},
  year      = {2021},
}
@inproceedings{Ganzfried,
author = {Ganzfried, Sam and Sandholm, Tuomas},
title = {Endgame Solving in Large Imperfect-Information Games},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {The leading approach for computing strong game-theoretic strategies in large imperfect-information games is to first solve an abstracted version of the game offline, then perform a table lookup during game play. We consider a modification to this approach where we solve the portion of the game that we have actually reached in real time to a greater degree of accuracy than in the initial computation. We call this approach endgame solving. Theoretically, we show that endgame solving can produce highly exploitable strategies in some games; however, we show that it can guarantee a low exploitability in certain games where the opponent is given sufficient exploitative power within the endgame. Furthermore, despite the lack of a general worst-case guarantee, we describe many benefits of endgame solving. We present an efficient algorithm for performing endgame solving in large imperfect-information games, and present a new variance-reduction technique for evaluating the performance of an agent that uses endgame solving. Experiments on no-limit Texas Hold'em show that our algorithm leads to significantly stronger performance against the strongest agents from the 2013 AAAI Annual Computer Poker Competition.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {37–45},
numpages = {9},
keywords = {imperfect information, game theory, game solving},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}
@inproceedings{hsvi,
author = {Smith, Trey and Simmons, Reid},
title = {Heuristic Search Value Iteration for POMDPs},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We present a novel POMDP planning algorithm called heuristic search value iteration (HSVI). HSVI is an anytime algorithm that returns a policy and a provable bound on its regret with respect to the optimal policy. HSVI gets its power by combining two well-known techniques: attention-focusing search heuristics and piecewise linear convex representations of the value function. HSVI's soundness and convergence have been proven. On some bench-mark problems from the literature, HSVI displays speedups of greater than 100 with respect to other state-of-the-art POMDP value iteration algorithms. We also apply HSVI to a new rover exploration problem 10 times larger than most POMDP problems in the literature.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {520–527},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@misc{zeng2021,
  doi = {10.48550/ARXIV.2205.13746},
  
  url = {https://arxiv.org/abs/2205.13746},
  
  author = {Zeng, Sihan and Doan, Thinh T. and Romberg, Justin},
  
  keywords = {Optimization and Control (math.OC), Machine Learning (cs.LG), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Regularized Gradient Descent Ascent for Two-Player Zero-Sum Markov Games},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@inproceedings{Cen2021FastPE,
  title={Fast Policy Extragradient Methods for Competitive Games with Entropy Regularization},
  author={Shicong Cen and Yuting Wei and Yuejie Chi},
  booktitle={NeurIPS},
  year={2021}
}
@book{stackelberg,
  author = {von Stackelberg, H.},
  publisher = { Springer, Vienna},
  title = {Marktform und Gleichgewicht},
  year = 1934
}
@book{Gibbons1992,
  added-at = {2016-12-19T12:09:05.000+0100},
  author = {Gibbons, R.},
  biburl = {https://www.bibsonomy.org/bibtex/2f7d89c222a0a6859a1c64146ecd1fd6a/swarmlab},
  interhash = {bd495f47c2b1b3b742684fe3dd0dbf2d},
  intrahash = {f7d89c222a0a6859a1c64146ecd1fd6a},
  keywords = {imported},
  publisher = {Pearson Education},
  timestamp = {2016-12-19T12:18:59.000+0100},
  title = {{A} {P}rimer in {G}ame {T}heory},
  year = 1992
}
@book{Schelling1960,
  added-at = {2015-03-29T16:24:43.000+0200},
  author = {Schelling, T.C.},
  biburl = {https://www.bibsonomy.org/bibtex/24b4d2b6ef91bcf40a1495bad0feb7670/vitelot},
  interhash = {f39988f21cffca506ac74fac2051d835},
  intrahash = {4b4d2b6ef91bcf40a1495bad0feb7670},
  keywords = {focalpoints gametheory schelling sitc},
  publisher = {Harvard University Press},
  timestamp = {2015-03-29T16:24:43.000+0200},
  title = {{The Strategy of Conflict}},
  year = 1960
}

@TECHREPORT{Stengel04leadershipwith,
    author = {Bernhard von Stengel and Shmuel Zamir},
    title = {Leadership with commitment to mixed strategies},
    institution = {},
    year = {2004}
}
@unpublished{buffet:hal-03080287,
  TITLE = {{On Bellman's Optimality Principle for zs-POSGs}},
  AUTHOR = {Buffet, Olivier and Dibangoye, Jilles and Delage, Aur{\'e}lien and Saffidine, Abdallah and Thomas, Vincent},
  URL = {https://hal.inria.fr/hal-03080287},
  NOTE = {working paper or preprint},
  YEAR = {2020},
  MONTH = Dec,
  KEYWORDS = {POSG ; partially observable stochastic game ; POSG ; Bellman's optimality principle ; Heuristic Search Value Iteration},
  PDF = {https://hal.inria.fr/hal-03080287/file/2006.16395.pdf},
  HAL_ID = {hal-03080287},
  HAL_VERSION = {v1},
}
@inproceedings{Oliehoek2013SufficientPS,
  title={Sufficient Plan-Time Statistics for Decentralized POMDPs},
  author={Frans A. Oliehoek},
  booktitle={IJCAI},
  year={2013}
}
@article{
az,
author = {David Silver  and Thomas Hubert  and Julian Schrittwieser  and Ioannis Antonoglou  and Matthew Lai  and Arthur Guez  and Marc Lanctot  and Laurent Sifre  and Dharshan Kumaran  and Thore Graepel  and Timothy Lillicrap  and Karen Simonyan  and Demis Hassabis },
title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
journal = {Science},
volume = {362},
number = {6419},
pages = {1140-1144},
year = {2018},
doi = {10.1126/science.aar6404},
URL = {https://www.science.org/doi/abs/10.1126/science.aar6404},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aar6404},
abstract = {Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system. Science, this issue p. 1140; see also pp. 1087 and 1118 AlphaZero teaches itself to play three different board games and beats state-of-the-art programs in each. The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.}}

@inproceedings{safenested,
author = {Brown, Noam and Sandholm, Tuomas},
title = {Safe and Nested Subgame Solving for Imperfect-Information Games},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In imperfect-information games, the optimal strategy in a subgame may depend on the strategy in other, unreached subgames. Thus a subgame cannot be solved in isolation and must instead consider the strategy for the entire game as a whole, unlike perfect-information games. Nevertheless, it is possible to first approximate a solution for the whole game and then improve it in individual subgames. This is referred to as subgame solving. We introduce subgame-solving techniques that outperform prior methods both in theory and practice. We also show how to adapt them, and past subgame-solving techniques, to respond to opponent actions that are outside the original action abstraction; this significantly outperforms the prior state-of-the-art approach, action translation. Finally, we show that subgame solving can be repeated as the game progresses down the game tree, leading to far lower exploitability. These techniques were a key component of Libratus, the first AI to defeat top humans in heads-up no-limit Texas hold'em poker.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {689–699},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@unpublished{delage:hal-03523399,
  TITLE = {{HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties}},
  AUTHOR = {Delage, Aur{\'e}lien and Buffet, Olivier and Dibangoye, Jilles},
  URL = {https://hal.inria.fr/hal-03523399},
  NOTE = {37 pages, 4 figures, 4 tables, 3 algorithms},
  YEAR = {2021},
  MONTH = Oct,
  HAL_ID = {hal-03523399},
  HAL_VERSION = {v1},
}
@inproceedings{Wiggers2016StructureIT,
  title={Structure in the Value Function of Two-Player Zero-Sum Games of Incomplete Information},
  author={Auke J. Wiggers and Frans A. Oliehoek and Diederik M. Roijers},
  booktitle={ECAI},
  year={2016}
}
@MISC{Littman96algorithmsfor,
    author = {Michael Lederman Littman},
    title = {Algorithms for Sequential Decision Making},
    year = {1996}
}
@inproceedings{valuedp,
author = {Hor\'{a}k, Karel and Bo\v{s}ansk\'{y}, Branislav},
title = {Solving Partially Observable Stochastic Games with Public Observations},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33012029},
doi = {10.1609/aaai.v33i01.33012029},
abstract = {In many real-world problems, there is a dynamic interaction between competitive agents. Partially observable stochastic games (POSGs) are among the most general formal models that capture such dynamic scenarios. The model captures stochastic events, partial information of players about the environment, and the scenario does not have a fixed horizon. Solving POSGs in the most general setting is intractable. Therefore, the research has been focused on subclasses of POSGs that have a value of the game and admit designing (approximate) optimal algorithms. We propose such a subclass for two-player zero-sum games with discounted-sum objective function—POSGs with public observations (PO-POSGs)—where each player is able to reconstruct beliefs of the other player over the unobserved states. Our results include: (1) theoretical analysis of PO-POSGs and their value functions showing convexity (concavity) in beliefs of maximizing (minimizing) player, (2) a novel algorithm for approximating the value of the game, and (3) a practical demonstration of scalability of our algorithm. Experimental results show that our algorithm can closely approximate the value of non-trivial games with hundreds of states.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {251},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}
@inproceedings{friendorfoe,
  title={Finding Friend and Foe in Multi-Agent Games},
  author={Jack Serrino and Max Kleiman-Weiner and David C. Parkes and Joshua B. Tenenbaum},
  booktitle={NeurIPS},
  year={2019}
}
@article{margin, title={Refining Subgames in Large Imperfect Information Games}, volume={30}, url={https://ojs.aaai.org/index.php/AAAI/article/view/10033}, DOI={10.1609/aaai.v30i1.10033}, abstractNote={ &lt;p&gt; The leading approach to solving large imperfect information games is to pre-calculate an approximate solution using a simplified abstraction of the full game; that solution is then used to play the original, full-scale game. The abstraction step is necessitated by the size of the game tree. However, as the original game progresses, the remaining portion of the tree (the subgame) becomes smaller. An appealing idea is to use the simplified abstraction to play the early parts of the game and then, once the subgame becomes tractable, to calculate a solution using a finer-grained abstraction in real time, creating a combined final strategy. While this approach is straightforward for perfect information games, it is a much more complex problem for imperfect information games. If the subgame is solved locally, the opponent can alter his play in prior to this subgame to exploit our combined strategy. To prevent this, we introduce the notion of subgame margin, a simple value with appealing properties. If any best response reaches the subgame, the improvement of exploitability of the combined strategy is (at least) proportional to the subgame margin. This motivates subgame refinements resulting in large positive margins. Unfortunately, current techniques either neglect subgame margin (potentially leading to a large negative subgame margin and drastically more exploitable strategies), or guarantee only non-negative subgame margin (possibly producing the original, unrefined strategy, even if much stronger strategies are possible). Our technique remedies this problem by maximizing the subgame margin and is guaranteed to find the optimal solution. We evaluate our technique using one of the top participants of the AAAI-14 Computer Poker Competition, the leading playground for agents in imperfect information setting &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Moravcik, Matej and Schmid, Martin and Ha, Karel and Hladik, Milan and Gaukrodger, Stephen}, year={2016}, month={Feb.} }
@inproceedings{cfrd,
author = {Burch, Neil and Johanson, Michael and Bowling, Michael},
title = {Solving Imperfect Information Games Using Decomposition},
year = {2014},
publisher = {AAAI Press},
abstract = {Decomposition, i.e., independently analyzing possible subgames, has proven to be an essential principle for effective decision-making in perfect information games. However, in imperfect information games, decomposition has proven to be problematic. To date, all proposed techniques for decomposition in imperfect information games have abandoned theoretical guarantees. This work presents the first technique for decomposing an imperfect information game into subgames that can be solved independently, while retaining optimality guarantees on the full-game solution. We can use this technique to construct theoretically justified algorithms that make better use of information available at run-time, overcome memory or disk limitations at run-time, or make a time/space trade-off to overcome memory or disk limitations while solving a game. In particular, we present an algorithm for subgame solving which guarantees performance in the whole game, in contrast to existing methods which may have unbounded error. In addition, we present an offline game solving algorithm, CFR-D, which can produce a Nash equilibrium for a game that is larger than available storage.},
booktitle = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
pages = {602–608},
numpages = {7},
location = {Qu\'{e}bec City, Qu\'{e}bec, Canada},
series = {AAAI'14}
}

@inproceedings{multivalued,
  title={Depth-Limited Solving for Imperfect-Information Games},
  author={Noam Brown and Tuomas Sandholm and Brandon Amos},
  booktitle={NeurIPS},
  year={2018}
}
@article{
pluribus,
author = {Noam Brown  and Tuomas Sandholm },
title = {Superhuman AI for multiplayer poker},
journal = {Science},
volume = {365},
number = {6456},
pages = {885-890},
year = {2019},
doi = {10.1126/science.aay2400},
URL = {https://www.science.org/doi/abs/10.1126/science.aay2400},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aay2400},
abstract = {Computer programs have shown superiority over humans in two-player games such as chess, Go, and heads-up, no-limit Texas hold'em poker. However, poker games usually include six players—a much trickier challenge for artificial intelligence than the two-player variant. Brown and Sandholm developed a program, dubbed Pluribus, that learned how to play six-player no-limit Texas hold'em by playing against five copies of itself (see the Perspective by Blair and Saffidine). When pitted against five elite professional poker players, or with five copies of Pluribus playing against one professional, the computer performed significantly better than humans over the course of 10,000 hands of poker. Science, this issue p. 885; see also p. 864 An AI dubbed Pluribus performs significantly better than human professionals in six-player no-limit Texas hold’em poker. In recent years there have been great strides in artificial intelligence (AI), with games often serving as challenge problems, benchmarks, and milestones for progress. Poker has served for decades as such a challenge problem. Past successes in such benchmarks, including poker, have been limited to two-player games. However, poker in particular is traditionally played with more than two players. Multiplayer games present fundamental additional issues beyond those in two-player games, and multiplayer poker is a recognized AI milestone. In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold’em poker, the most popular form of poker played by humans.}}

@article{
deepstack,
author = {Matej Moravčík  and Martin Schmid  and Neil Burch  and Viliam Lisý  and Dustin Morrill  and Nolan Bard  and Trevor Davis  and Kevin Waugh  and Michael Johanson  and Michael Bowling },
title = {DeepStack: Expert-level artificial intelligence in heads-up no-limit poker},
journal = {Science},
volume = {356},
number = {6337},
pages = {508-513},
year = {2017},
doi = {10.1126/science.aam6960},
URL = {https://www.science.org/doi/abs/10.1126/science.aam6960},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aam6960},
abstract = {Computer code based on continual problem re-solving beats human professional poker players at a two-player variant of poker. Computers can beat humans at games as complex as chess or go. In these and similar games, both players have access to the same information, as displayed on the board. Although computers have the ultimate poker face, it has been tricky to teach them to be good at poker, where players cannot see their opponents' cards. Moravčík et al. built a code dubbed DeepStack that managed to beat professional poker players at a two-player poker variant called heads-up no-limit Texas hold'em. Instead of devising its strategy beforehand, DeepStack recalculated it at each step, taking into account the current state of the game. The principles behind DeepStack may enable advances in solving real-world problems that involve information asymmetry. Science, this issue p. 508 Artificial intelligence has seen several breakthroughs in recent years, with games often serving as milestones. A common feature of these games is that players have perfect information. Poker, the quintessential game of imperfect information, is a long-standing challenge problem in artificial intelligence. We introduce DeepStack, an algorithm for imperfect-information settings. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition that is automatically learned from self-play using deep learning. In a study involving 44,000 hands of poker, DeepStack defeated, with statistical significance, professional poker players in heads-up no-limit Texas hold’em. The approach is theoretically sound and is shown to produce strategies that are more difficult to exploit than prior approaches.}}

@inproceedings{libratus,
  added-at = {2019-08-20T00:00:00.000+0200},
  author = {Brown, Noam and Sandholm, Tuomas},
  biburl = {https://www.bibsonomy.org/bibtex/2b01a93e96e535a5290c9bcacd6b10457/dblp},
  booktitle = {IJCAI},
  editor = {Sierra, Carles},
  ee = {https://doi.org/10.24963/ijcai.2017/772},
  interhash = {3cb1d5e9caa6a936ac61711c14ddc06c},
  intrahash = {b01a93e96e535a5290c9bcacd6b10457},
  isbn = {978-0-9992411-0-3},
  keywords = {dblp},
  pages = {5226-5228},
  publisher = {ijcai.org},
  timestamp = {2019-08-21T11:50:23.000+0200},
  title = {Libratus: The Superhuman AI for No-Limit Poker.},
  url = {http://dblp.uni-trier.de/db/conf/ijcai/ijcai2017.html#BrownS17},
  year = 2017
}

@InProceedings{pikl,
  title = 	 {Modeling Strong and Human-Like Gameplay with {KL}-Regularized Search},
  author =       {Jacob, Athul Paul and Wu, David J and Farina, Gabriele and Lerer, Adam and Hu, Hengyuan and Bakhtin, Anton and Andreas, Jacob and Brown, Noam},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {9695--9728},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/jacob22a/jacob22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/jacob22a.html},
  abstract = 	 {We consider the task of accurately modeling strong human policies in multi-agent decision-making problems, given examples of human behavior. Imitation learning is effective at predicting human actions but may not match the strength of expert humans (e.g., by sometimes committing blunders), while self-play learning and search techniques such as AlphaZero lead to strong performance but may produce policies that differ markedly from human behavior. In chess and Go, we show that regularized search algorithms that penalize KL divergence from an imitation-learned policy yield higher prediction accuracy of strong humans and better performance than imitation learning alone. We then introduce a novel regret minimization algorithm that is regularized based on the KL divergence from an imitation-learned policy, and show that using this algorithm for search in no-press Diplomacy yields a policy that matches the human prediction accuracy of imitation learning while being substantially stronger.}
}

@InProceedings{carminati2,
  title = 	 {A Marriage between Adversarial Team Games and 2-player Games: Enabling Abstractions, No-regret Learning, and Subgame Solving},
  author =       {Carminati, Luca and Cacciamani, Federico and Ciccone, Marco and Gatti, Nicola},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {2638--2657},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/carminati22a/carminati22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/carminati22a.html},
  abstract = 	 {<em>Ex ante</em> correlation is becoming the mainstream approach for <em>sequential adversarial team games</em>, where a team of players faces another team in a zero-sum game. It is known that team members’ asymmetric information makes both equilibrium computation \textsf{APX}-hard and team’s strategies not directly representable on the game tree. This latter issue prevents the adoption of successful tools for huge 2-player zero-sum games such as, <em>e.g.</em>, abstractions, no-regret learning, and subgame solving. This work shows that we can recover from this weakness by bridging the gap between sequential adversarial team games and 2-player games. In particular, we propose a new, suitable game representation that we call <em>team-public-information</em>, in which a team is represented as a single coordinator who only knows information common to the whole team and prescribes to each member an action for any possible private state. The resulting representation is highly <em>explainable</em>, being a 2-player tree in which the team’s strategies are behavioral with a direct interpretation and more expressive than the original extensive form when designing abstractions. Furthermore, we prove payoff equivalence of our representation, and we provide techniques that, starting directly from the extensive form, generate dramatically more compact representations without information loss. Finally, we experimentally evaluate our techniques when applied to a standard testbed, comparing their performance with the current state of the art.}
}

@misc{carminati1,
  doi = {10.48550/ARXIV.2201.10377},
  
  url = {https://arxiv.org/abs/2201.10377},
  
  author = {Carminati, Luca and Cacciamani, Federico and Ciccone, Marco and Gatti, Nicola},
  
  keywords = {Computer Science and Game Theory (cs.GT), Artificial Intelligence (cs.AI), Multiagent Systems (cs.MA), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Public Information Representation for Adversarial Team Games},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{Zhang2022TeamBD,
  title={Team Belief DAG Form: A Concise Representation for Team-Correlated Game-Theoretic Decision Making},
  author={B. Zhang and Gabriele Farina and Tuomas Sandholm},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.00789}
}
@inproceedings{
bft,
title={A Fine-Tuning Approach to Belief State Modeling},
author={Samuel Sokota and Hengyuan Hu and David J Wu and J Zico Kolter and Jakob Nicolaus Foerster and Noam Brown},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=ckZY7DGa7FQ}
}
@inproceedings{
fickinger2021scalable,
title={Scalable Online Planning via Reinforcement Learning Fine-Tuning},
author={Arnaud Fickinger and Hengyuan Hu and Brandon Amos and Stuart Russell and Noam Brown},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=D0xGh031I9m}
}
@inproceedings{sparta,
  title={Improving Policies via Search in Cooperative Partially Observable Games},
  author={Adam Lerer and Hengyuan Hu and Jakob N. Foerster and Noam Brown},
  booktitle={AAAI},
  year={2020}
}
@InProceedings{bad,
  title = 	 {{B}ayesian Action Decoder for Deep Multi-Agent Reinforcement Learning},
  author =       {Foerster, Jakob and Song, Francis and Hughes, Edward and Burch, Neil and Dunning, Iain and Whiteson, Shimon and Botvinick, Matthew and Bowling, Michael},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1942--1951},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/foerster19a/foerster19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/foerster19a.html},
  abstract = 	 {When observing the actions of others, humans make inferences about why they acted as they did, and what this implies about the world; humans also use the fact that their actions will be interpreted in this manner, allowing them to act informatively and thereby communicate efficiently with others. Although learning algorithms have recently achieved superhuman performance in a number of two-player, zero-sum games, scalable multi-agent reinforcement learning algorithms that can discover effective strategies and conventions in complex, partially observable settings have proven elusive. We present the <em>Bayesian action decoder</em> (BAD), a new multi-agent learning method that uses an approximate Bayesian update to obtain a public belief that conditions on the actions taken by all agents in the environment. BAD introduces a new Markov decision process, the <em>public belief MDP</em>, in which the action space consists of all deterministic partial policies, and exploits the fact that an agent acting only on this public belief state can still learn to use its private information if the action space is augmented to be over all partial policies mapping private information into environment actions. The Bayesian update is closely related to the <em>theory of mind</em> reasoning that humans carry out when observing others’ actions. We first validate BAD on a proof-of-principle two-step matrix game, where it outperforms policy gradient methods; we then evaluate BAD on the challenging, cooperative partial-information card game Hanabi, where, in the two-player setting, it surpasses all previously published learning and hand-coded approaches, establishing a new state of the art.}
}

@InProceedings{ml2018,
  title = 	 {Learning to Act in Decentralized Partially Observable {MDP}s},
  author =       {Dibangoye, Jilles and Buffet, Olivier},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1233--1242},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/dibangoye18a/dibangoye18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/dibangoye18a.html},
  abstract = 	 {We address a long-standing open problem of reinforcement learning in decentralized partially observable Markov decision processes. Previous attempts focussed on different forms of generalized policy iteration, which at best led to local optima. In this paper, we restrict attention to plans, which are simpler to store and update than policies. We derive, under certain conditions, the first near-optimal cooperative multi-agent reinforcement learning algorithm. To achieve significant scalability gains, we replace the greedy maximization by mixed-integer linear programming. Experiments show our approach can learn to act near-optimally in many finite domains from the literature.}
}

@inproceedings{ml2013b,
author = {MacDermed, Liam and Isbell, Charles L.},
title = {Point Based Value Iteration with Optimal Belief Compression for Dec-POMDPs},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present four major results towards solving decentralized partially observable Markov decision problems (DecPOMDPs) culminating in an algorithm that outperforms all existing algorithms on all but one standard infinite-horizon benchmark problems. (1) We give an integer program that solves collaborative Bayesian games (CBGs). The program is notable because its linear relaxation is very often integral. (2) We show that a DecPOMDP with bounded belief can be converted to a POMDP (albeit with actions exponential in the number of beliefs). These actions correspond to strategies of a CBG. (3) We present a method to transform any DecPOMDP into a DecPOMDP with bounded beliefs (the number of beliefs is a free parameter) using optimal (not lossless) belief compression. (4) We show that the combination of these results opens the door for new classes of DecPOMDP algorithms based on previous POMDP algorithms. We choose one such algorithm, point-based valued iteration, and modify it to produce the first tractable value iteration method for DecPOMDPs that outperforms existing algorithms.},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 1},
pages = {100–108},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}
@inproceedings{ml2015,
author = {Dibangoye, Jilles S. and Buffet, Olivier and Simonin, Olivier},
title = {Structural Results for Cooperative Decentralized Control Models},
year = {2015},
isbn = {9781577357384},
publisher = {AAAI Press},
abstract = {The intractability in cooperative, decentralized control models is mainly due to prohibitive memory requirements in both optimal policies and value functions. The complexity analysis has emerged as the standard method to estimating the memory needed for solving a given computational problem, but complexity results may be somewhat limited. This paper introduces a general methodology-- structural analysis--for the design of optimality-preserving concise policies and value functions, which will eventually lead to the development of efficient theory and algorithms. For the first time, we show that memory requirements for policies and value functions may be asymmetric, resulting in cooperative, decentralized control models with exponential reductions in memory requirements.},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
pages = {46–52},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {IJCAI'15}
}

@inproceedings{ml2014b,
author = {Dibangoye, Jilles S. and Amato, Christopher and Buffet, Olivier and Charpillet, Franc\c{c}ois},
title = {Exploiting Separability in Multiagent Planning with Continuous-State MDPs},
year = {2014},
isbn = {9781450327381},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Recent years have seen significant advances in techniques for optimally solving multiagent problems represented as decentralized partially observable Markov decision processes (Dec-POMDPs). A new method achieves scalability gains by converting Dec-POMDPs into continuous state MDPs. This method relies on the assumption of a centralized planning phase that generates a set of decentralized policies for the agents to execute. However, scalability remains limited when the number of agents or problem variables becomes large. In this paper, we show that, under certain separability conditions of the optimal value function, the scalability of this approach can increase considerably. This separability is present when there is locality of interaction, which --- as other approaches (such as those based on the ND-POMDP subclass) have already shown --- can be exploited to improve performance. Unlike most previous methods, the novel continuous-state MDP algorithm retains optimality and convergence guarantees. Results show that the extension using separability can scale to a large number of agents and domain variables while maintaining optimality.},
booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {1281–1288},
numpages = {8},
keywords = {planning under uncertainty, decentralized pomdps, nd-pomdps, cooperative multiagent systems},
location = {Paris, France},
series = {AAMAS '14}
}
@inproceedings{ml2014a,
author = {Dibangoye, Jilles S. and Buffet, Olivier and Charpillet, Fran\c{c}ois},
title = {Error-Bounded Approximations for Infinite-Horizon Discounted Decentralized POMDPs},
isbn = {978-3-662-44847-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-44848-9_22},
doi = {10.1007/978-3-662-44848-9_22},
abstract = {We address decentralized stochastic control problems represented as decentralized partially observable Markov decision processes (Dec-POMDPs). This formalism provides a general model for decision-making under uncertainty in cooperative, decentralized settings, but the worst-case complexity makes it difficult to solve optimally (NEXP-complete). Recent advances suggest recasting Dec-POMDPs into continuous-state and deterministic MDPs. In this form, however, states and actions are embedded into high-dimensional spaces, making accurate estimate of states and greedy selection of actions intractable for all but trivial-sized problems. The primary contribution of this paper is the first framework for error-monitoring during approximate estimation of states and selection of actions. Such a framework permits us to convert state-of-the-art exact methods into error-bounded algorithms, which results in a scalability increase as demonstrated by experiments over problems of unprecedented sizes.},
booktitle = {Machine Learning and Knowledge Discovery in Databases},
pages = {338–353},
numpages = {16},
keywords = {error-bounded approximations, decentralized stochastic control},
location = {Nancy
France},
year = {2014}
}
@inproceedings{ml2013a,
  title={Producing efficient error-bounded solutions for transition independent decentralized mdps},
  author={Jilles Steeve Dibangoye and Chris Amato and Arnaud Doniec and François Charpillet},
  booktitle={AAMAS},
  year={2013}
}
@article{ml2022,
place = {Country unknown/Code not available}, title = {Common Information based Approximate State Representations in Multi-Agent Reinforcement Learning}, url = {https://par.nsf.gov/biblio/10332971}, abstractNote = {Due to information asymmetry, finding optimal policies for Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) is hard with the complexity growing doubly exponentially in the horizon length. The challenge increases greatly in the multi-agent reinforcement learning (MARL) setting where the transition probabilities, observation kernel, and reward function are unknown. Here, we develop a general compression framework with approximate common and private state representations, based on which decentralized policies can be constructed. We derive the optimality gap of executing dynamic programming (DP) with the approximate states in terms of the approximation error parameters and the remaining time steps. When the compression is exact (no error), the resulting DP is equivalent to the one in existing work. Our general framework generalizes a number of methods proposed in the literature. The results shed light on designing practically useful deep-MARL network structures under the "centralized learning distributed execution" scheme.}, journal = {25th International Conference on Artifi- cial Intelligence and Statistics (AISTATS) 2022, Valencia, Spain. PMLR}, volume = {151}, author = {Kao, Hsu and Subramanian and Vijay}, 
year={2022}}
@ARTICLE{control2021,
  author={Gupta, Abhishek},
  journal={IEEE Transactions on Automatic Control}, 
  title={Existence of Team-Optimal Strategies in Teams With Countable Observation Spaces}, 
  year={2021},
  volume={66},
  number={10},
  pages={4792-4798},
  doi={10.1109/TAC.2020.3037047}}
@ARTICLE{control2014a,
  author={Nayyar, Ashutosh and Gupta, Abhishek and Langbort, Cédric and Başar, Tamer},
  journal={IEEE Transactions on Automatic Control}, 
  title={Common Information Based Markov Perfect Equilibria for Stochastic Games With Asymmetric Information: Finite Games}, 
  year={2014},
  volume={59},
  number={3},
  pages={555-570},
  doi={10.1109/TAC.2013.2283743}}
@article{control2019,
  title={Online Planning for Decentralized Stochastic Control with Partial History Sharing},
  author={K. Zhang and Erik Miehling and Tamer Başar},
  journal={2019 American Control Conference (ACC)},
  year={2019},
  pages={3544-3550}
}
@INPROCEEDINGS{control2015,
  author={Ouyang, Yi and Tavafoghi, Hamidreza and Teneketzis, Demosthenis},
  booktitle={2015 54th IEEE Conference on Decision and Control (CDC)}, 
  title={Dynamic oligopoly games with private Markovian dynamics}, 
  year={2015},
  volume={},
  number={},
  pages={5851-5858},
  doi={10.1109/CDC.2015.7403139}}
@INPROCEEDINGS{control2016b,
  author={Tavafoghi, Hamidreza and Ouyang, Yi and Teneketzis, Demosthenis},
  booktitle={2016 IEEE 55th Conference on Decision and Control (CDC)}, 
  title={On stochastic dynamic games with delayed sharing information structure}, 
  year={2016},
  volume={},
  number={},
  pages={7002-7009},
  doi={10.1109/CDC.2016.7799348}}
@inproceedings{control2018c,
author = {Tavafoghi, Hamidreza and Ouyang, Yi and Teneketzis, Demosthenis},
title = {A Sufficient Information Approach to Decentralized Decision Making},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CDC.2018.8619040},
doi = {10.1109/CDC.2018.8619040},
abstract = {We study a general class of decentralized dynamic decision-making problems with many agents, asymmetric information, and hidden actions. We propose the notion of sufficient information that provides a mutually consistent compression of the agents' private and common information for decision-making purposes. We define a class of strategies, called sufficient information-based (SIB) strategies, that are based on the agents' sufficient information. We show that restriction to SIB strategies is without loss of optimality in decentralized decision problems with non-strategic agents (i.e. teams). Accordingly, we provide a sequential decomposition of dynamic teams over time that specifies an algorithm for determining globally optimal strategies. For decentralized decision problems with strategic agents (i.e. games), we show that the class of SIB strategies is closed under the best response map. Consequently, we propose a notion of sufficient information-based equilibrium and provide a sequential decomposition of dynamic games over time that specifies an algorithm for determining Sufficient Information Based Perfect Bayesian Equilibria (SIB-PBE).},
booktitle = {2018 IEEE Conference on Decision and Control (CDC)},
pages = {5069–5076},
numpages = {8},
location = {FL, USA}
}
@INPROCEEDINGS{control2013,
  author={Lessard, Laurent and Nayyar, Ashutosh},
  booktitle={52nd IEEE Conference on Decision and Control}, 
  title={Structural results and explicit solution for two-player LQG systems on a finite time horizon}, 
  year={2013},
  volume={},
  number={},
  pages={6542-6549},
  doi={10.1109/CDC.2013.6760924}}
@INPROCEEDINGS{control2014b,
  author={Arabneydi, Jalal and Mahajan, Aditya},
  booktitle={53rd IEEE Conference on Decision and Control}, 
  title={Team optimal control of coupled subsystems with mean-field sharing}, 
  year={2014},
  volume={},
  number={},
  pages={1669-1674},
  doi={10.1109/CDC.2014.7039639}}
@INPROCEEDINGS{control2016a,
  author={Vasconcelos, Marcos M. and Martins, Nuno C.},
  booktitle={2016 IEEE 55th Conference on Decision and Control (CDC)}, 
  title={The structure of optimal communication policies for remote estimation over the collision channel with private and common observations}, 
  year={2016},
  volume={},
  number={},
  pages={320-326},
  doi={10.1109/CDC.2016.7798289}}
@INPROCEEDINGS{control2018a,
  author={Afshari, Mohammad and Mahajan, Aditya},
  booktitle={2018 IEEE Conference on Decision and Control (CDC)}, 
  title={Team Optimal Decentralized State Estimation}, 
  year={2018},
  volume={},
  number={},
  pages={5044-5050},
  doi={10.1109/CDC.2018.8619493}}
@INPROCEEDINGS{control2018b,
  author={Gagrani, Mukul and Nayyar, Ashutosh},
  booktitle={2018 IEEE Conference on Decision and Control (CDC)}, 
  title={Thompson Sampling for some decentralized control problems}, 
  year={2018},
  volume={},
  number={},
  pages={1053-1058},
  doi={10.1109/CDC.2018.8619423}}
@inproceedings{Dibangoye,
author = {Dibangoye, Jilles Steeve and Amato, Christopher and Buffet, Olivier and Charpillet, Fran\c{c}ois},
title = {Optimally Solving Dec-POMDPs as Continuous-State MDPs},
year = {2013},
isbn = {9781577356332},
publisher = {AAAI Press},
abstract = {Optimally solving decentralized partially observable Markov decision processes (Dec-POMDPs) is a hard combinatorial problem. Current algorithms search through the space of full histories for each agent. Because of the doubly exponential growth in the number of policies in this space as the planning horizon increases, these methods quickly become intractable. However, in real world problems, computing policies over the full history space is often unnecessary. True histories experienced by the agents often lie near a structured, low-dimensional manifold embedded into the history space. We show that by transforming a Dec-POMDP into a continuous-state MDP, we are able to find and exploit these low-dimensional representations. Using this novel transformation, we can then apply powerful techniques for solving POMDPs and continuous-state MDPs. By combining a general search algorithm and dimension reduction based on feature selection, we introduce a novel approach to optimally solve problems with significantly longer planning horizons than previous methods.},
booktitle = {Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence},
pages = {90–96},
numpages = {7},
location = {Beijing, China},
series = {IJCAI '13}
}


@misc{stratego,
  doi = {10.48550/ARXIV.2206.15378},
  
  url = {https://arxiv.org/abs/2206.15378},
  
  author = {Perolat, Julien and de Vylder, Bart and Hennes, Daniel and Tarassov, Eugene and Strub, Florian and de Boer, Vincent and Muller, Paul and Connor, Jerome T. and Burch, Neil and Anthony, Thomas and McAleer, Stephen and Elie, Romuald and Cen, Sarah H. and Wang, Zhe and Gruslys, Audrunas and Malysheva, Aleksandra and Khan, Mina and Ozair, Sherjil and Timbers, Finbarr and Pohlen, Toby and Eccles, Tom and Rowland, Mark and Lanctot, Marc and Lespiau, Jean-Baptiste and Piot, Bilal and Omidshafiei, Shayegan and Lockhart, Edward and Sifre, Laurent and Beauguerlange, Nathalie and Munos, Remi and Silver, David and Singh, Satinder and Hassabis, Demis and Tuyls, Karl},
  
  keywords = {Artificial Intelligence (cs.AI), Computer Science and Game Theory (cs.GT), Multiagent Systems (cs.MA), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Mastering the Game of Stratego with Model-Free Multiagent Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{maxent,
  author = {Ziebart, Brian D. and Maas, Andrew L. and Bagnell, J. Andrew and Dey, Anind K.},
  editor = {Fox, Dieter and Gomes, Carla P.},
  publisher = {AAAI Press},
  timestamp = {2012-12-11T11:46:42.000+0100},
  title = {Maximum Entropy Inverse Reinforcement Learning.},
  year = 2008
}

@article{aqre,
  title={Quantal Response Equilibria for Extensive Form Games},
  author={Richard D. McKelvey and Thomas R. Palfrey},
  journal={Experimental Economics},
  year={1998},
  volume={1},
  pages={9-41}
}
@inproceedings{horde,
author = {Sutton, Richard S. and Modayil, Joseph and Delp, Michael and Degris, Thomas and Pilarski, Patrick M. and White, Adam and Precup, Doina},
title = {Horde: A Scalable Real-Time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction},
year = {2011},
isbn = {0982657161},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Maintaining accurate world knowledge in a complex and changing environment is a perennial problem for robots and other artificial intelligence systems. Our architecture for addressing this problem, called Horde, consists of a large number of independent reinforcement learning sub-agents, or demons. Each demon is responsible for answering a single predictive or goal-oriented question about the world, thereby contributing in a factored, modular way to the system's overall knowledge. The questions are in the form of a value function, but each demon has its own policy, reward function, termination function, and terminal-reward function unrelated to those of the base problem. Learning proceeds in parallel by all demons simultaneously so as to extract the maximal training information from whatever actions are taken by the system as a whole. Gradient-based temporal-difference learning methods are used to learn efficiently and reliably with function approximation in this off-policy setting. Horde runs in constant time and memory per time step, and is thus suitable for learning online in real-time applications such as robotics. We present results using Horde on a multi-sensored mobile robot to successfully learn goal-oriented behaviors and long-term predictions from off-policy experience. Horde is a significant incremental step towards a real-time architecture for efficient learning of general knowledge from unsupervised sensorimotor interaction.},
booktitle = {The 10th International Conference on Autonomous Agents and Multiagent Systems - Volume 2},
pages = {761–768},
numpages = {8},
keywords = {value function approximation, real-time, temporal-difference learning, robotics, off-policy learning, knowledge representation, reinforcement learning, artificial intelligence},
location = {Taipei, Taiwan},
series = {AAMAS '11}
}

@InProceedings{fforel,
  title = 	 {From Poincar{é} Recurrence to Convergence in Imperfect Information Games: Finding Equilibrium via Regularization},
  author =       {Perolat, Julien and Munos, Remi and Lespiau, Jean-Baptiste and Omidshafiei, Shayegan and Rowland, Mark and Ortega, Pedro and Burch, Neil and Anthony, Thomas and Balduzzi, David and De Vylder, Bart and Piliouras, Georgios and Lanctot, Marc and Tuyls, Karl},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8525--8535},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/perolat21a/perolat21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/perolat21a.html},
  abstract = 	 {In this paper we investigate the Follow the Regularized Leader dynamics in sequential imperfect information games (IIG). We generalize existing results of Poincar{é} recurrence from normal-form games to zero-sum two-player imperfect information games and other sequential game settings. We then investigate how adapting the reward (by adding a regularization term) of the game can give strong convergence guarantees in monotone games. We continue by showing how this reward adaptation technique can be leveraged to build algorithms that converge exactly to the Nash equilibrium. Finally, we show how these insights can be directly used to build state-of-the-art model-free algorithms for zero-sum two-player Imperfect Information Games (IIG).}
}

@inproceedings{capi_paper, 
title       = {Solving Common-Payoff Games with Approximate Policy Iteration}, 
journal     = {Proceedings of the AAAI Conference on Artificial Intelligence}, 
author      = {Sokota, Samuel and Lockhart, Edward and Timbers, Finbarr and Davoodi, Elnaz and D’Orazio, Ryan and Burch, Neil and Schmid, Martin and Bowling, Michael and Lanctot, Marc}, 
year        = {2021}
}

@mastersthesis{capi_thesis,
author       = {Samuel Sokota}, 
title        = {Solving Common-Payoff Games with Approximate Policy Iteration},
school       = {University of Alberta},
year         = {2020},
}

@MISC{littman,
    author = {Michael Lederman Littman},
    title = {Algorithms for Sequential Decision Making},
    year = {1996}
}
@article{fosg,
  author    = {Vojtech Kovar{\'{\i}}k and
               Martin Schmid and
               Neil Burch and
               Michael Bowling and
               Viliam Lis{\'{y}}},
  title     = {Rethinking Formal Models of Partially Observable Multiagent Decision
               Making},
  journal   = {CoRR},
  volume    = {abs/1906.11110},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.11110},
  eprinttype = {arXiv},
  eprint    = {1906.11110},
  timestamp = {Thu, 27 Jun 2019 18:54:51 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-11110.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{exit,
author = {Anthony, Thomas and Tian, Zheng and Barber, David},
title = {Thinking Fast and Slow with Deep Learning and Tree Search},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Sequential decision making problems, such as structured prediction, robotic control, and game playing, require a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration (EXIT), a novel reinforcement learning algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans. In contrast, standard deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that EXIT outperforms REINFORCE for training a neural network to play the board game Hex, and our final tree search agent, trained tabula rasa, defeats MOHEX 1.0, the most recent Olympiad Champion player to be publicly released.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {5366–5376},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}
@article{horak2019, title={Solving Partially Observable Stochastic Games with Public Observations}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4032}, DOI={10.1609/aaai.v33i01.33012029}, abstractNote={&lt;p&gt;In many real-world problems, there is a dynamic interaction between competitive agents. Partially observable stochastic games (POSGs) are among the most general formal models that capture such dynamic scenarios. The model captures stochastic events, partial information of players about the environment, and the scenario does not have a fixed horizon. Solving POSGs in the most general setting is intractable.Therefore, the research has been focused on subclasses of POSGs that have a value of the game and admit designing (approximate) optimal algorithms. We propose such a subclass for two-player zero-sum games with discounted-sum objective function—POSGs with &lt;em&gt;public observations&lt;/em&gt; (POPOSGs)—where each player is able to reconstruct beliefs of the other player over the unobserved states. Our results include: (1) theoretical analysis of PO-POSGs and their value functions showing convexity (concavity) in beliefs of maximizing (minimizing) player, (2) a novel algorithm for approximating the value of the game, and (3) a practical demonstration of scalability of our algorithm. Experimental results show that our algorithm can closely approximate the value of non-trivial games with hundreds of states.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Horák, Karel and Bošanský, Branislav}, year={2019}, month={Jul.}, pages={2029-2036} }

@article{brown2020rebel,
  title={Combining deep reinforcement learning and search for imperfect-information games},
  author={Brown, Noam and Bakhtin, Anton and Lerer, Adam and Gong, Qucheng},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
@article{pog,
  author    = {Martin Schmid and
               Matej Moravcik and
               Neil Burch and
               Rudolf Kadlec and
               Joshua Davidson and
               Kevin Waugh and
               Nolan Bard and
               Finbarr Timbers and
               Marc Lanctot and
               Zach Holland and
               Elnaz Davoodi and
               Alden Christianson and
               Michael Bowling},
  title     = {Player of Games},
  journal   = {CoRR},
  volume    = {abs/2112.03178},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.03178},
  eprinttype = {arXiv},
  eprint    = {2112.03178},
  timestamp = {Wed, 08 Dec 2021 14:48:59 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-03178.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{dp,
author = {Bertsekas, Dimitri P.},
title = {Dynamic Programming and Optimal Control},
year = {2000},
isbn = {1886529094},
publisher = {Athena Scientific},
edition = {2nd}
}

@ARTICLE{nayyar,  
author={Nayyar, Ashutosh and Mahajan, Aditya and Teneketzis, Demosthenis},  journal={IEEE Transactions on Automatic Control},   title={Decentralized Stochastic Control with Partial History Sharing: A Common Information Approach},   year={2013},  volume={58},  number={7},  pages={1644-1658},  doi={10.1109/TAC.2013.2239000}}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}