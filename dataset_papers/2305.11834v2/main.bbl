\begin{thebibliography}{65}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agostinelli et~al.(2023)Agostinelli, Denk, Borsos, Engel, Verzetti, Caillon, Huang, Jansen, Roberts, Tagliasacchi, et~al.]{agostinelli2023musiclm}
A.~Agostinelli, T.~I. Denk, Z.~Borsos, J.~Engel, M.~Verzetti, A.~Caillon, Q.~Huang, A.~Jansen, A.~Roberts, M.~Tagliasacchi, et~al.
\newblock Musiclm: Generating music from text.
\newblock \emph{arXiv preprint arXiv:2301.11325}, 2023.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo}
J.-B. Alayrac, J.~Donahue, P.~Luc, A.~Miech, I.~Barr, Y.~Hasson, K.~Lenc, A.~Mensch, K.~Millican, M.~Reynolds, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 23716--23736, 2022.

\bibitem[Baevski et~al.(2020)Baevski, Zhou, Mohamed, and Auli]{wav2vec2}
A.~Baevski, Y.~Zhou, A.~Mohamed, and M.~Auli.
\newblock wav2vec 2.0: A framework for self-supervised learning of speech representations.
\newblock \emph{Advances in neural information processing systems}, 2020.

\bibitem[Borsos et~al.(2022)Borsos, Marinier, Vincent, Kharitonov, Pietquin, Sharifi, Teboul, Grangier, Tagliasacchi, and Zeghidour]{borsos2022audiolm}
Z.~Borsos, R.~Marinier, D.~Vincent, E.~Kharitonov, O.~Pietquin, M.~Sharifi, O.~Teboul, D.~Grangier, M.~Tagliasacchi, and N.~Zeghidour.
\newblock Audiolm: a language modeling approach to audio generation.
\newblock \emph{arXiv preprint arXiv:2209.03143}, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal, A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, Du, Zhu, Ma, Berg-Kirkpatrick, and Dubnov]{chen2022hts}
K.~Chen, X.~Du, B.~Zhu, Z.~Ma, T.~Berg-Kirkpatrick, and S.~Dubnov.
\newblock Hts-at: A hierarchical token-semantic audio transformer for sound classification and detection.
\newblock In \emph{ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 2022{\natexlab{a}}.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Wu, Wang, Liu, Tompkins, Chen, and Wei]{chen2022beats}
S.~Chen, Y.~Wu, C.~Wang, S.~Liu, D.~Tompkins, Z.~Chen, and F.~Wei.
\newblock Beats: Audio pre-training with acoustic tokenizers.
\newblock \emph{arXiv preprint arXiv:2212.09058}, 2022{\natexlab{b}}.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et~al.]{chung2022scaling}
H.~W. Chung, L.~Hou, S.~Longpre, B.~Zoph, Y.~Tay, W.~Fedus, E.~Li, X.~Wang, M.~Dehghani, S.~Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv:2210.11416}, 2022.

\bibitem[Defferrard et~al.(2017)Defferrard, Benzi, Vandergheynst, and Bresson]{fma_dataset}
M.~Defferrard, K.~Benzi, P.~Vandergheynst, and X.~Bresson.
\newblock {FMA}: A dataset for music analysis.
\newblock In \emph{18th International Society for Music Information Retrieval Conference (ISMIR)}, 2017.

\bibitem[Deshmukh and Rade(2018)]{toxic}
S.~Deshmukh and R.~Rade.
\newblock Tackling toxic online communication with recurrent capsule networks.
\newblock In \emph{2018 Conference on Information and Communication Technology (CICT)}, pages 1--7, 2018.
\newblock \doi{10.1109/INFOCOMTECH.2018.8722433}.

\bibitem[Deshmukh et~al.(2021)Deshmukh, Raj, and Singh]{deshmukh2021improving}
S.~Deshmukh, B.~Raj, and R.~Singh.
\newblock Improving weakly supervised sound event detection with self-supervised auxiliary tasks.
\newblock pages 596--600, 08 2021.
\newblock \doi{10.21437/Interspeech.2021-2079}.

\bibitem[Deshmukh et~al.(2023)Deshmukh, Elizalde, and Wang]{deshmukh2022audio}
S.~Deshmukh, B.~Elizalde, and H.~Wang.
\newblock {Audio Retrieval with WavText5K and CLAP Training}.
\newblock In \emph{Proc. INTERSPEECH 2023}, pages 2948--2952, 2023.
\newblock \doi{10.21437/Interspeech.2023-1136}.

\bibitem[Dhamyal et~al.(2022)Dhamyal, Elizalde, Deshmukh, Wang, Raj, and Singh]{dhamyal2022describing}
H.~Dhamyal, B.~Elizalde, S.~Deshmukh, H.~Wang, B.~Raj, and R.~Singh.
\newblock Describing emotions with acoustic property prompts for speech emotion recognition.
\newblock \emph{arXiv preprint arXiv:2211.07737}, 2022.

\bibitem[Dhamyal et~al.(2023)Dhamyal, Elizalde, Deshmukh, Wang, Raj, and Singh]{dhamyal2023prompting}
H.~Dhamyal, B.~Elizalde, S.~Deshmukh, H.~Wang, B.~Raj, and R.~Singh.
\newblock Prompting audios using acoustic properties for emotion representation.
\newblock \emph{arXiv preprint arXiv:2310.02298}, 2023.

\bibitem[Drossos et~al.(2020)Drossos, Lipping, and Virtanen]{clotho}
K.~Drossos, S.~Lipping, and T.~Virtanen.
\newblock Clotho: an audio captioning dataset.
\newblock In \emph{IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 2020.
\newblock \doi{10.1109/ICASSP40776.2020.9052990}.

\bibitem[Elizalde et~al.(2019)Elizalde, Zarar, and Raj]{crossmodal}
B.~Elizalde, S.~Zarar, and B.~Raj.
\newblock Cross modal audio search and retrieval with joint embeddings based on text and audio.
\newblock In \emph{ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 2019.

\bibitem[Elizalde et~al.(2022)Elizalde, Deshmukh, Ismail, and Wang]{elizalde2022clap}
B.~Elizalde, S.~Deshmukh, M.~A. Ismail, and H.~Wang.
\newblock Clap: Learning audio concepts from natural language supervision.
\newblock \emph{arXiv preprint arXiv:2206.04769}, 2022.

\bibitem[Elizalde(2020)]{elizalde2020never}
B.~M. Elizalde.
\newblock \emph{Never-ending learning of sounds}.
\newblock Carnegie Mellon University, 2020.

\bibitem[Engel et~al.(2017)Engel, Resnick, Roberts, Dieleman, Norouzi, Eck, and Simonyan]{engel2017neural}
J.~Engel, C.~Resnick, A.~Roberts, S.~Dieleman, M.~Norouzi, D.~Eck, and K.~Simonyan.
\newblock Neural audio synthesis of musical notes with wavenet autoencoders.
\newblock In \emph{International Conference on Machine Learning}, pages 1068--1077. PMLR, 2017.

\bibitem[Fonseca et~al.(2017)Fonseca, Pons~Puig, Favory, Font~Corbera, Bogdanov, Ferraro, Oramas, Porter, and Serra]{fonseca2017freesound}
E.~Fonseca, J.~Pons~Puig, X.~Favory, F.~Font~Corbera, D.~Bogdanov, A.~Ferraro, S.~Oramas, A.~Porter, and X.~Serra.
\newblock Freesound datasets: a platform for the creation of open audio datasets.
\newblock In \emph{Hu X, Cunningham SJ, Turnbull D, Duan Z, editors. Proceedings of the 18th ISMIR Conference; 2017 oct 23-27; Suzhou, China.[Canada]: International Society for Music Information Retrieval}. International Society for Music Information Retrieval (ISMIR), 2017.

\bibitem[Fonseca et~al.(2021)Fonseca, Ortego, McGuinness, O’Connor, and Serra]{9415009}
E.~Fonseca, D.~Ortego, K.~McGuinness, N.~E. O’Connor, and X.~Serra.
\newblock Unsupervised contrastive learning of sound event representations.
\newblock In \emph{ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 2021.
\newblock \doi{10.1109/ICASSP39728.2021.9415009}.

\bibitem[Fonseca et~al.(2022)Fonseca, Favory, Pons, Font, and Serra]{fsd50k}
E.~Fonseca, X.~Favory, J.~Pons, F.~Font, and X.~Serra.
\newblock Fsd50k: An open dataset of human-labeled sound events.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 2022.
\newblock \doi{10.1109/TASLP.2021.3133208}.

\bibitem[Gemmeke et~al.(2017)Gemmeke, Ellis, Freedman, Jansen, Lawrence, Moore, Plakal, and Ritter]{audioset}
J.~F. Gemmeke, D.~P.~W. Ellis, D.~Freedman, A.~Jansen, W.~Lawrence, R.~C. Moore, M.~Plakal, and M.~Ritter.
\newblock Audio set: An ontology and human-labeled dataset for audio events.
\newblock In \emph{2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages 776--780, 2017.
\newblock \doi{10.1109/ICASSP.2017.7952261}.

\bibitem[Gong et~al.(2022)Gong, Lai, Chung, and Glass]{gong2021ssast}
Y.~Gong, C.-I. Lai, Y.-A. Chung, and J.~Glass.
\newblock Ssast: Self-supervised audio spectrogram transformer.
\newblock 36:\penalty0 10699--10709, Jun. 2022.
\newblock \doi{10.1609/aaai.v36i10.21315}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/21315}.

\bibitem[Guzhov et~al.(2022)Guzhov, Raue, Hees, and Dengel]{audioclip}
A.~Guzhov, F.~Raue, J.~Hees, and A.~Dengel.
\newblock Audioclip: Extending clip to image, text and audio.
\newblock In \emph{IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 2022.
\newblock \doi{10.1109/ICASSP43922.2022.9747631}.

\bibitem[Hsu et~al.(2021)Hsu, Bolte, Tsai, Lakhotia, Salakhutdinov, and Mohamed]{hubert}
W.-N. Hsu, B.~Bolte, Y.-H.~H. Tsai, K.~Lakhotia, R.~Salakhutdinov, and A.~Mohamed.
\newblock Hubert: Self-supervised speech representation learning by masked prediction of hidden units.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 29:\penalty0 3451--3460, 2021.

\bibitem[Huang et~al.(2022{\natexlab{a}})Huang, Xu, Li, Baevski, Auli, Galuba, Metze, and Feichtenhofer]{huang2022masked}
P.-Y. Huang, H.~Xu, J.~Li, A.~Baevski, M.~Auli, W.~Galuba, F.~Metze, and C.~Feichtenhofer.
\newblock Masked autoencoders that listen.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 28708--28720, 2022{\natexlab{a}}.

\bibitem[Huang et~al.(2022{\natexlab{b}})Huang, Jansen, Lee, Ganti, Li, and Ellis]{huang2022mulan}
Q.~Huang, A.~Jansen, J.~Lee, R.~Ganti, J.~Y. Li, and D.~P.~W. Ellis.
\newblock Mulan: A joint embedding of music audio and natural language.
\newblock In \emph{International Society for Music Information Retrieval Conference}, 2022{\natexlab{b}}.

\bibitem[Jeong and Park(2022)]{jeong2022cochlscene}
I.-Y. Jeong and J.~Park.
\newblock Cochlscene: Acquisition of acoustic scene data using crowdsourcing.
\newblock In \emph{2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)}, pages 17--21. IEEE, 2022.

\bibitem[Ji et~al.(2023)Ji, Lee, Frieske, Yu, Su, Xu, Ishii, Bang, Madotto, and Fung]{ji2023survey}
Z.~Ji, N.~Lee, R.~Frieske, T.~Yu, D.~Su, Y.~Xu, E.~Ishii, Y.~J. Bang, A.~Madotto, and P.~Fung.
\newblock Survey of hallucination in natural language generation.
\newblock \emph{ACM Computing Surveys}, 2023.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and Duerig]{jia2021scaling}
C.~Jia, Y.~Yang, Y.~Xia, Y.-T. Chen, Z.~Parekh, H.~Pham, Q.~Le, Y.-H. Sung, Z.~Li, and T.~Duerig.
\newblock Scaling up visual and vision-language representation learning with noisy text supervision.
\newblock In \emph{International Conference on Machine Learning}, pages 4904--4916. PMLR, 2021.

\bibitem[Kim et~al.(2019)Kim, Kim, Lee, and Kim]{audiocaps}
C.~D. Kim, B.~Kim, H.~Lee, and G.~Kim.
\newblock {AudioCaps: Generating Captions for Audios in The Wild}.
\newblock In \emph{NAACL-HLT}, 2019.

\bibitem[Kim et~al.(2023)Kim, Sung-Bin, and Oh]{kim2023prefix}
M.~Kim, K.~Sung-Bin, and T.-H. Oh.
\newblock Prefix tuning for automated audio captioning.
\newblock \emph{arXiv preprint arXiv:2303.17489}, 2023.

\bibitem[Kingma and Ba(2015)]{adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR (Poster)}, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.6980}.

\bibitem[Koepke et~al.(2022)Koepke, Oncescu, Henriques, Akata, and Albanie]{sounddescs}
A.~S. Koepke, A.-M. Oncescu, J.~Henriques, Z.~Akata, and S.~Albanie.
\newblock Audio retrieval with natural language queries: A benchmark study.
\newblock \emph{IEEE Transactions on Multimedia}, 2022.
\newblock \doi{10.1109/TMM.2022.3149712}.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
B.~Lester, R.~Al-Rfou, and N.~Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 3045--3059, 2021.

\bibitem[Li et~al.(2019)Li, Yatskar, Yin, Hsieh, and Chang]{li2019visualbert}
L.~H. Li, M.~Yatskar, D.~Yin, C.-J. Hsieh, and K.-W. Chang.
\newblock Visualbert: Asimple and performant baseline for vision and language.
\newblock \emph{arXiv preprint arXiv:1908.03557}, 2019.

\bibitem[Li and Liang(2021)]{li2021prefix}
X.~L. Li and P.~Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 4582--4597, 2021.

\bibitem[Lipping et~al.(2022)Lipping, Sudarsanam, Drossos, and Virtanen]{lipping2022clotho}
S.~Lipping, P.~Sudarsanam, K.~Drossos, and T.~Virtanen.
\newblock Clotho-aqa: A crowdsourced dataset for audio question answering.
\newblock In \emph{2022 30th European Signal Processing Conference (EUSIPCO)}, pages 1140--1144. IEEE, 2022.

\bibitem[Lotfian and Busso(2017)]{msp_podcast}
R.~Lotfian and C.~Busso.
\newblock Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings.
\newblock \emph{IEEE Transactions on Affective Computing}, 10\penalty0 (4):\penalty0 471--483, 2017.

\bibitem[Ma{\~n}as et~al.(2023)Ma{\~n}as, Rodriguez~Lopez, Ahmadi, Nematzadeh, Goyal, and Agrawal]{manas2022mapl}
O.~Ma{\~n}as, P.~Rodriguez~Lopez, S.~Ahmadi, A.~Nematzadeh, Y.~Goyal, and A.~Agrawal.
\newblock {MAPL}: Parameter-efficient adaptation of unimodal pre-trained models for vision-language few-shot prompting.
\newblock In \emph{Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics}. Association for Computational Linguistics, 2023.

\bibitem[Mart{\'\i}n-Morat{\'o} and Mesaros(2021)]{macs}
I.~Mart{\'\i}n-Morat{\'o} and A.~Mesaros.
\newblock What is the ground truth? reliability of multi-annotator data for audio tagging.
\newblock In \emph{2021 29th European Signal Processing Conference (EUSIPCO)}, 2021.

\bibitem[Mei et~al.(2023)Mei, Meng, Liu, Kong, Ko, Zhao, Plumbley, Zou, and Wang]{mei2023wavcaps}
X.~Mei, C.~Meng, H.~Liu, Q.~Kong, T.~Ko, C.~Zhao, M.~D. Plumbley, Y.~Zou, and W.~Wang.
\newblock Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research.
\newblock \emph{arXiv preprint arXiv:2303.17395}, 2023.

\bibitem[Mokady et~al.(2021)Mokady, Hertz, and Bermano]{mokady2021clipcap}
R.~Mokady, A.~Hertz, and A.~H. Bermano.
\newblock Clipcap: Clip prefix for image captioning.
\newblock \emph{arXiv preprint arXiv:2111.09734}, 2021.

\bibitem[Niizumi et~al.(2021{\natexlab{a}})Niizumi, Takeuchi, Ohishi, Harada, and Kashino]{niizumi2021byol-a}
D.~Niizumi, D.~Takeuchi, Y.~Ohishi, N.~Harada, and K.~Kashino.
\newblock Byol for audio: Self-supervised learning for general-purpose audio representation.
\newblock In \emph{2021 International Joint Conference on Neural Networks, {IJCNN} 2021}, 2021{\natexlab{a}}.

\bibitem[Niizumi et~al.(2021{\natexlab{b}})Niizumi, Takeuchi, Ohishi, Harada, and Kashino]{niizumi2021byola}
D.~Niizumi, D.~Takeuchi, Y.~Ohishi, N.~Harada, and K.~Kashino.
\newblock Byol for audio: Self-supervised learning for general-purpose audio representation.
\newblock In \emph{2021 International Joint Conference on Neural Networks (IJCNN)}, pages 1--8, 2021{\natexlab{b}}.
\newblock \doi{10.1109/IJCNN52387.2021.9534474}.

\bibitem[Piczak(2015)]{esc50}
K.~J. Piczak.
\newblock {ESC}: {Dataset} for {Environmental Sound Classification}.
\newblock In \emph{Proceedings of the 23rd {Annual ACM Conference} on {Multimedia}}, pages 1015--1018. {ACM Press}, 2015.
\newblock ISBN 978-1-4503-3459-4.
\newblock \doi{10.1145/2733373.2806390}.

\bibitem[Poria et~al.(2019)Poria, Hazarika, Majumder, Naik, Cambria, and Mihalcea]{poria2019meld}
S.~Poria, D.~Hazarika, N.~Majumder, G.~Naik, E.~Cambria, and R.~Mihalcea.
\newblock Meld: A multimodal multi-party dataset for emotion recognition in conversations.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 527--536, 2019.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, et~al.]{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2021.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling}
J.~W. Rae, S.~Borgeaud, T.~Cai, K.~Millican, J.~Hoffmann, F.~Song, J.~Aslanides, S.~Henderson, R.~Ring, S.~Young, et~al.
\newblock Scaling language models: Methods, analysis \& insights from training gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou, W.~Li, and P.~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0 (1):\penalty0 5485--5551, 2020.

\bibitem[Saeed et~al.(2021)Saeed, Grangier, and Zeghidour]{cola}
A.~Saeed, D.~Grangier, and N.~Zeghidour.
\newblock Contrastive learning of general-purpose audio representations.
\newblock In \emph{ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 2021.

\bibitem[Shor et~al.(2020)Shor, Jansen, Maor, Lang, Tuval, de~Chaumont~Quitry, Tagliasacchi, Shavitt, Emanuel, and Haviv]{trill}
J.~Shor, A.~Jansen, R.~Maor, O.~Lang, O.~Tuval, F.~de~Chaumont~Quitry, M.~Tagliasacchi, I.~Shavitt, D.~Emanuel, and Y.~Haviv.
\newblock Towards learning a universal non-semantic representation of speech.
\newblock \emph{Proc. Interspeech 2020}, pages 140--144, 2020.

\bibitem[Tsimpoukelli et~al.(2021)Tsimpoukelli, Menick, Cabi, Eslami, Vinyals, and Hill]{tsimpoukelli2021multimodal}
M.~Tsimpoukelli, J.~L. Menick, S.~Cabi, S.~Eslami, O.~Vinyals, and F.~Hill.
\newblock Multimodal few-shot learning with frozen language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 200--212, 2021.

\bibitem[Turian et~al.(2022)Turian, Shier, et~al.]{turian2022hear}
J.~Turian, J.~Shier, et~al.
\newblock {HEAR: Holistic Evaluation of Audio Representations}.
\newblock In \emph{NeurIPS 2021 Competitions and Demonstrations Track}, 2022.

\bibitem[Wang et~al.()Wang, Yu, Yu, Dai, Tsvetkov, and Cao]{wangsimvlm}
Z.~Wang, J.~Yu, A.~W. Yu, Z.~Dai, Y.~Tsvetkov, and Y.~Cao.
\newblock Simvlm: Simple visual language model pretraining with weak supervision.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{weifinetuned}
J.~Wei, M.~Bosma, V.~Zhao, K.~Guu, A.~W. Yu, B.~Lester, N.~Du, A.~M. Dai, and Q.~V. Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Weidinger et~al.(2021)Weidinger, Mellor, Rauh, Griffin, Uesato, Huang, Cheng, Glaese, Balle, Kasirzadeh, et~al.]{weidinger2021ethical}
L.~Weidinger, J.~Mellor, M.~Rauh, C.~Griffin, J.~Uesato, P.-S. Huang, M.~Cheng, M.~Glaese, B.~Balle, A.~Kasirzadeh, et~al.
\newblock Ethical and social risks of harm from language models.
\newblock \emph{arXiv preprint arXiv:2112.04359}, 2021.

\bibitem[Wu et~al.(2022{\natexlab{a}})Wu, Seetharaman, Kumar, et~al.]{wav2clip}
H.-H. Wu, P.~Seetharaman, K.~Kumar, et~al.
\newblock Wav2clip: Learning robust audio representations from clip.
\newblock In \emph{IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 2022{\natexlab{a}}.
\newblock \doi{10.1109/ICASSP43922.2022.9747669}.

\bibitem[Wu et~al.(2022{\natexlab{b}})Wu, Chen, Zhang, Hui, Berg-Kirkpatrick, and Dubnov]{wu2022large}
Y.~Wu, K.~Chen, T.~Zhang, Y.~Hui, T.~Berg-Kirkpatrick, and S.~Dubnov.
\newblock Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation.
\newblock \emph{arXiv preprint arXiv:2211.06687}, 2022{\natexlab{b}}.

\bibitem[Yuan et~al.(2021)Yuan, Chen, Chen, Codella, et~al.]{yuan2021florence}
L.~Yuan, D.~Chen, Y.-L. Chen, N.~Codella, et~al.
\newblock Florence: A new foundation model for computer vision.
\newblock \emph{arXiv preprint arXiv:2111.11432}, 2021.

\bibitem[Zadeh et~al.(2016)Zadeh, Zellers, Pincus, and Morency]{zadeh2016mosi}
A.~Zadeh, R.~Zellers, E.~Pincus, and L.-P. Morency.
\newblock Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos.
\newblock \emph{arXiv preprint arXiv:1606.06259}, 2016.

\bibitem[Zadeh et~al.(2018)Zadeh, Liang, Poria, Cambria, and Morency]{mosei}
A.~B. Zadeh, P.~P. Liang, S.~Poria, E.~Cambria, and L.-P. Morency.
\newblock Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 2236--2246, 2018.

\bibitem[Zhang and Yang(2022)]{mtl_survey}
Y.~Zhang and Q.~Yang.
\newblock A survey on multi-task learning.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering}, 34\penalty0 (12):\penalty0 5586--5609, 2022.
\newblock \doi{10.1109/TKDE.2021.3070203}.

\bibitem[Zhou et~al.(2021)Zhou, Deshmukh, Greer, and Lee]{zhou2021narle}
R.~Zhou, S.~Deshmukh, J.~Greer, and C.~Lee.
\newblock Narle: Natural language models using reinforcement learning with emotion feedback.
\newblock \emph{arXiv preprintarXiv:2110.02148}, 2021.

\end{thebibliography}
