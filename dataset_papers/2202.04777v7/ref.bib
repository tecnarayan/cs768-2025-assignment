@inproceedings{choromanska2015open,
  title={Open problem: The landscape of the loss surfaces of multilayer networks},
  author={Choromanska, Anna and LeCun, Yann and Arous, G{\'e}rard Ben},
  booktitle={Conference on Learning Theory},
  pages={1756--1760},
  year={2015},
  organization={PMLR}
}

@article{ziyin2022exactb,
  title={Exact Phase Transitions in Deep Learning},
  author={Ziyin, Liu and Ueda, Masahito},
  journal={arXiv preprint arXiv:2205.12510},
  year={2022}
}


@article{rangamani2021dynamics,
  title={Dynamics and Neural Collapse in Deep Classifiers trained with the Square Loss},
  author={Rangamani, Akshay and Xu, Mengjia and Banburski, Andrzej and Liao, Qianli and Poggio, Tomaso},
  journal={Center for Brains, Minds and Machines (CBMM) Memo},
  number={117},
  year={2021}
}
@inproceedings{rangamani2022neural,
  title={Neural Collapse in Deep Homogeneous Classifiers and The Role of Weight Decay},
  author={Rangamani, Akshay and Banburski-Fahey, Andrzej},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4243--4247},
  year={2022},
  organization={IEEE}
}

@inproceedings{
ziyin2023what,
title={What shapes the loss landscape of self supervised learning?},
author={Liu Ziyin and Ekdeep Singh Lubana and Masahito Ueda and Hidenori Tanaka},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=3zSn48RUO8M}
}

@inproceedings{ziyin2021sgd,
  title={Sgd can converge to local maxima},
  author={Ziyin, Liu and Li, Botao and Simon, James B and Ueda, Masahito},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{ziyin2022sparsity,
  title="{spred: Solving L1 Penalty with SGD}",
  author={Ziyin, Liu and Wang, Zihao},
  booktitle={International Conference on Machine Learning},
  year={2023}
}

@article{tanaka2020pruning,
  title={Pruning neural networks without any data by iteratively conserving synaptic flow},
  author={Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel L and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6377--6389},
  year={2020}
}

@inproceedings{wang2022posterior,
  title={Posterior Collapse of a Linear Latent Variable Model},
  author={Wang, Zihao and Ziyin, Liu},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{tian2022deep,
  title={Deep Contrastive Learning is Provably (almost) Principal Component Analysis},
  author={Tian, Yuandong},
  journal={arXiv preprint arXiv:2201.12680},
  year={2022}
}



@inproceedings{alemi2018fixing,
  title="{Fixing a broken ELBO}",
  author={Alemi, Alexander and Poole, Ben and Fischer, Ian and Dillon, Joshua and Saurous, Rif A and Murphy, Kevin},
  booktitle={International Conference on Machine Learning},
  pages={159--168},
  year={2018},
  organization={PMLR}
}

@article{coker2021wide,
  title={Wide Mean-Field Variational Bayesian Neural Networks Ignore the Data},
  author={Coker, Beau and Pan, Weiwei and Doshi-Velez, Finale},
  journal={arXiv preprint arXiv:2106.07052},
  year={2021}
}

@misc{ziyin2022stochastic,
      title={Stochastic Neural Networks with Infinite Width are Deterministic}, 
      author={Liu Ziyin and Hanlin Zhang and Xiangming Meng and Yuting Lu and Eric Xing and Masahito Ueda},
      year={2022},
      eprint={2201.12724},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{yun2018small,
  title={Small nonlinearities in activation functions create bad local minima in neural networks},
  author={Yun, Chulhee and Sra, Suvrit and Jadbabaie, Ali},
  journal={arXiv preprint arXiv:1802.03487},
  year={2018}
}

@article{liu2021spurious,
  title={Spurious Local Minima Are Common for Deep Neural Networks with Piecewise Linear Activations},
  author={Liu, Bo},
  journal={arXiv preprint arXiv:2102.13233},
  year={2021}
}

@article{he2020piecewise,
  title={Piecewise linear activations substantially shape the loss surfaces of neural networks},
  author={He, Fengxiang and Wang, Bohan and Tao, Dacheng},
  journal={arXiv preprint arXiv:2003.12236},
  year={2020}
}

@article{venturi2019spurious,
  title={Spurious valleys in one-hidden-layer neural network optimization landscapes},
  author={Venturi, Luca and Bandeira, Afonso S and Bruna, Joan},
  journal={Journal of Machine Learning Research},
  volume={20},
  pages={133},
  year={2019},
  publisher={MIT Press}
}

@inproceedings{safran2018spurious,
  title={Spurious local minima are common in two-layer relu neural networks},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={International conference on machine learning},
  pages={4433--4441},
  year={2018},
  organization={PMLR}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{gotmare2018closer,
  title={A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation},
  author={Gotmare, Akhilesh and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1810.13243},
  year={2018}
}

@article{lu2017depth,
  title={Depth creates no bad local minima},
  author={Lu, Haihao and Kawaguchi, Kenji},
  journal={arXiv preprint arXiv:1702.08580},
  year={2017}
}

@article{baldi1989neural,
  title={Neural networks and principal component analysis: Learning from examples without local minima},
  author={Baldi, Pierre and Hornik, Kurt},
  journal={Neural networks},
  volume={2},
  number={1},
  pages={53--58},
  year={1989},
  publisher={Elsevier}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}


@inproceedings{mori2022power,
  title={Power-law escape rate of SGD},
  author={Mori, Takashi and Ziyin, Liu and Liu, Kangqiao and Ueda, Masahito},
  booktitle={International Conference on Machine Learning},
  pages={15959--15975},
  year={2022},
  organization={PMLR}
}

@inproceedings{kleinberg2018alternative,
  title={An alternative view: When does SGD escape local minima?},
  author={Kleinberg, Bobby and Li, Yuanzhi and Yuan, Yang},
  booktitle={International Conference on Machine Learning},
  pages={2698--2707},
  year={2018},
  organization={PMLR}
}


@article{ziyin2020neural,
  title={Neural networks fail to learn periodic functions and how to fix it},
  author={Ziyin, Liu and Hartwig, Tilman and Ueda, Masahito},
  journal={arXiv preprint arXiv:2006.08195},
  year={2020}
}

@misc{doersch2021tutorial,
      title={Tutorial on Variational Autoencoders}, 
      author={Carl Doersch},
      year={2021},
      eprint={1606.05908},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{lucas2019dont,
      title="{Don't Blame the ELBO! A Linear VAE Perspective on Posterior Collapse}", 
      author={James Lucas and George Tucker and Roger Grosse and Mohammad Norouzi},
      year={2019},
      eprint={1911.02469},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{koehler2021variational,
  title={Variational autoencoders in the presence of low-dimensional data: landscape and implicit bias},
  author={Koehler, Frederic and Mehta, Viraj and Risteski, Andrej and Zhou, Chenghui},
  journal={arXiv preprint arXiv:2112.06868},
  year={2021}
}

@inproceedings{cavazza2018dropout,
  title={Dropout as a low-rank regularizer for matrix factorization},
  author={Cavazza, Jacopo and Morerio, Pietro and Haeffele, Benjamin and Lane, Connor and Murino, Vittorio and Vidal, Rene},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={435--444},
  year={2018},
  organization={PMLR}
}

@inproceedings{mianjy2019dropout,
  title={On dropout and nuclear norm regularization},
  author={Mianjy, Poorya and Arora, Raman},
  booktitle={International Conference on Machine Learning},
  pages={4575--4584},
  year={2019},
  organization={PMLR}
}

@misc{arora2020dropout,
      title={Dropout: Explicit Forms and Capacity Control}, 
      author={Raman Arora and Peter Bartlett and Poorya Mianjy and Nathan Srebro},
      year={2020},
      eprint={2003.03397},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ramachandran2017searching,
      title={Searching for Activation Functions}, 
      author={Prajit Ramachandran and Barret Zoph and Quoc V. Le},
      year={2017},
      eprint={1710.05941},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}


@article{al2017learning,
  title={Learning scalable deep kernels with recurrent structure},
  author={Al-Shedivat, Maruan and Wilson, Andrew Gordon and Saatchi, Yunus and Hu, Zhiting and Xing, Eric P},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={2850--2886},
  year={2017},
  publisher={JMLR}
}

@inproceedings{wilson2016deep,
  title={Deep kernel learning},
  author={Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P},
  booktitle={Artificial intelligence and statistics},
  pages={370--378},
  year={2016},
  organization={PMLR}
}

@article{kyle1985continuous,
  title={Continuous auctions and insider trading},
  author={Kyle, Albert S},
  journal={Econometrica: Journal of the Econometric Society},
  pages={1315--1335},
  year={1985},
  publisher={JSTOR}
}

@article{higgins2016beta,
  title={beta-vae: Learning basic visual concepts with a constrained variational framework},
  author={Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  year={2016}
}

@article{burgess2018understanding,
  title={Understanding disentangling in $\beta$-VAE},
  author={Burgess, Christopher P and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
  journal={arXiv preprint arXiv:1804.03599},
  year={2018}
}

@article{dai2019diagnosing,
  title={Diagnosing and enhancing VAE models},
  author={Dai, Bin and Wipf, David},
  journal={arXiv preprint arXiv:1903.05789},
  year={2019}
}

@inproceedings{liu2021noise,
  title={Noise and fluctuation of finite learning rate stochastic gradient descent},
  author={Liu, Kangqiao and Ziyin, Liu and Ueda, Masahito},
  booktitle={International Conference on Machine Learning},
  pages={7045--7056},
  year={2021},
  organization={PMLR}
}



@misc{ziyin2021minibatch,
      title={On Minibatch Noise: Discrete-Time SGD, Overparametrization, and Bayes}, 
      author={Liu Ziyin and Kangqiao Liu and Takashi Mori and Masahito Ueda},
      year={2021},
      eprint={2102.05375},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{wasserman2013all,
  title={All of statistics: a concise course in statistical inference},
  author={Wasserman, Larry},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{ziyin2019deep,
  title={Deep gamblers: Learning to abstain with portfolio theory},
  author={Ziyin, Liu and Wang, Zhikang and Liang, Paul Pu and Salakhutdinov, Ruslan and Morency, Louis-Philippe and Ueda, Masahito},
  journal={arXiv preprint arXiv:1907.00208},
  year={2019}
}

@article{may1976simple,
  title={Simple mathematical models with very complicated dynamics},
  author={May, Robert M},
  journal={Nature},
  volume={261},
  number={5560},
  pages={459--467},
  year={1976},
  publisher={Springer}
}

@article{levy1996power,
  title={Power laws are logarithmic Boltzmann laws},
  author={Levy, Moshe and Solomon, Sorin},
  journal={International Journal of Modern Physics C},
  volume={7},
  number={04},
  pages={595--601},
  year={1996},
  publisher={World Scientific}
}

@article{daniels2003quantitative,
  title={Quantitative model of price diffusion and market friction based on trading as a mechanistic random process},
  author={Daniels, Marcus G and Farmer, J Doyne and Gillemot, L{\'a}szl{\'o} and Iori, Giulia and Smith, Eric},
  journal={Physical review letters},
  volume={90},
  number={10},
  pages={108102},
  year={2003},
  publisher={APS}
}

@article{bouchaud2010price,
  title={Price impact},
  author={Bouchaud, Jean-Philippe},
  journal={Encyclopedia of quantitative finance},
  year={2010},
  publisher={Wiley Online Library}
}

@article{solomon2001power,
  title={Power laws of wealth, market order volumes and market returns},
  author={Solomon, Sorin and Richmond, Peter},
  journal={Physica A: Statistical Mechanics and its Applications},
  volume={299},
  number={1-2},
  pages={188--197},
  year={2001},
  publisher={Elsevier}
}

@article{bouchaud2000wealth,
  title={Wealth condensation in a simple model of economy},
  author={Bouchaud, Jean-Philippe and M{\'e}zard, Marc},
  journal={Physica A: Statistical Mechanics and its Applications},
  volume={282},
  number={3-4},
  pages={536--545},
  year={2000},
  publisher={Elsevier}
}


@article{toth2011anomalous,
  title={Anomalous price impact and the critical nature of liquidity in financial markets},
  author={T{\'o}th, Bence and Lemperiere, Yves and Deremble, Cyril and De Lataillade, Joachim and Kockelkoren, Julien and Bouchaud, J-P},
  journal={Physical Review X},
  volume={1},
  number={2},
  pages={021006},
  year={2011},
  publisher={APS}
}

@article{bucci2019crossover,
  title={Crossover from linear to square-root market impact},
  author={Bucci, Fr{\'e}d{\'e}ric and Benzaquen, Michael and Lillo, Fabrizio and Bouchaud, Jean-Philippe},
  journal={Physical review letters},
  volume={122},
  number={10},
  pages={108302},
  year={2019},
  publisher={APS}
}

@article{cont2001empirical,
  title={Empirical properties of asset returns: stylized facts and statistical issues},
  author={Cont, Rama},
  journal={Quantitative Finance},
  year={2001},
  publisher={Taylor \& Francis}
}

@article{levy1994microscopic,
  title={A microscopic model of the stock market: cycles, booms, and crashes},
  author={Levy, Moshe and Levy, Haim and Solomon, Sorin},
  journal={Economics Letters},
  volume={45},
  number={1},
  pages={103--111},
  year={1994},
  publisher={Elsevier}
}

@incollection{dieci2018heterogeneous,
  title={Heterogeneous agent models in finance},
  author={Dieci, Roberto and He, Xue-Zhong},
  booktitle={Handbook of computational economics},
  volume={4},
  pages={257--328},
  year={2018},
  publisher={Elsevier}
}

@article{FINOCCHIARO2011146,
title = "Inattention, wealth inequality and equilibrium asset prices",
journal = "Journal of Monetary Economics",
volume = "58",
number = "2",
pages = "146 - 155",
year = "2011",
issn = "0304-3932",
doi = "https://doi.org/10.1016/j.jmoneco.2011.03.002",
url = "http://www.sciencedirect.com/science/article/pii/S030439321100016X",
author = "Daria Finocchiaro",
abstract = "Heterogeneity in planning propensity affects wealth inequality and asset prices. This paper presents an economy where attentive agents plan their consumption period by period, while inattentive agents plan every other period. Inattentive consumers face more uncertainty and trade at unfavorable prices. If the only source of uncertainty is future income, inattentive consumers accumulate more wealth. In contrast, with uncertain asset returns inattentive investors accumulate less wealth. Asset prices must induce attentive consumers to voluntarily bear the burden of adjusting to aggregate shocks and, as a result, are much more volatile than in a representative agent model with full attention."
}

@article{levine2005finance,
  title={Finance and growth: theory and evidence},
  author={Levine, Ross},
  journal={Handbook of economic growth},
  volume={1},
  pages={865--934},
  year={2005},
  publisher={Elsevier}
}

@article{panait2012stylized,
  title={Stylized facts of the daily and monthly returns for the European stock indices during 2007-2012},
  author={Panait, Iulian and Constantinescu, Alexandru},
  journal={ Journal of Applied Quantitative Methods },
  number={3},
  year={2012}
}

@article{chiarella2006asset,
  title={Asset price and wealth dynamics in a financial market with heterogeneous agents},
  author={Chiarella, Carl and Dieci, Roberto and Gardini, Laura},
  journal={Journal of Economic Dynamics and Control},
  volume={30},
  number={9-10},
  pages={1755--1786},
  year={2006},
  publisher={Elsevier}
}

@article{kanazawa2018kinetic,
  title={Kinetic theory for financial Brownian motion from microscopic dynamics},
  author={Kanazawa, Kiyoshi and Sueshige, Takumi and Takayasu, Hideki and Takayasu, Misako},
  journal={Physical Review E},
  volume={98},
  number={5},
  pages={052317},
  year={2018},
  publisher={APS}
}

@article{laloux1999noise,
  title={Noise dressing of financial correlation matrices},
  author={Laloux, Laurent and Cizeau, Pierre and Bouchaud, Jean-Philippe and Potters, Marc},
  journal={Physical review letters},
  volume={83},
  number={7},
  pages={1467},
  year={1999},
  publisher={APS}
}

@article{mastromatteo2014anomalous,
  title={Anomalous impact in reaction-diffusion financial models},
  author={Mastromatteo, Iacopo and Toth, Bence and Bouchaud, J-P},
  journal={Physical review letters},
  volume={113},
  number={26},
  pages={268701},
  year={2014},
  publisher={APS}
}

@article{bariviera2017some,
  title={Some stylized facts of the Bitcoin market},
  author={Bariviera, Aurelio F and Basgall, Mar{\'\i}a Jos{\'e} and Hasperu{\'e}, Waldo and Naiouf, Marcelo},
  journal={Physica A: Statistical Mechanics and its Applications},
  volume={484},
  pages={82--90},
  year={2017},
  publisher={Elsevier}
}

@article{goncu2018anatomy,
  title={Anatomy of Chinese Futures Markets},
  author={Goncu, Ahmet and Yang, Yurun},
  journal={Available at SSRN 3181266},
  year={2018}
}

@article{takahashi2019modeling,
  title={Modeling financial time-series with generative adversarial networks},
  author={Takahashi, Shuntaro and Chen, Yu and Tanaka-Ishii, Kumiko},
  journal={Physica A: Statistical Mechanics and its Applications},
  volume={527},
  pages={121261},
  year={2019},
  publisher={Elsevier}
}

@article{hopf1948mathematical,
  title={A mathematical example displaying features of turbulence},
  author={Hopf, Eberhard},
  journal={Communications on Pure and Applied Mathematics},
  volume={1},
  number={4},
  pages={303--322},
  year={1948},
  publisher={Wiley Online Library}
}

@article{maldarella2012kinetic,
  title={Kinetic models for socio-economic dynamics of speculative markets},
  author={Maldarella, Dario and Pareschi, Lorenzo},
  journal={Physica A: Statistical Mechanics and its Applications},
  volume={391},
  number={3},
  pages={715--730},
  year={2012},
  publisher={Elsevier}
}

@article{ausloos2000gas,
  title={Gas-kinetic theory and Boltzmann equation of share price within an equilibrium market hypothesis and ad hoc strategy},
  author={Ausloos, Marcel},
  journal={Physica A: Statistical Mechanics and its Applications},
  volume={284},
  number={1-4},
  pages={385--392},
  year={2000},
  publisher={Elsevier}
}

@article{gabaix2009power,
  title={Power laws in economics and finance},
  author={Gabaix, Xavier},
  journal={Annu. Rev. Econ.},
  volume={1},
  number={1},
  pages={255--294},
  year={2009},
  publisher={Annual Reviews}
}

@article{sornette2014physics,
  title={Physics and financial economics (1776--2014): puzzles, Ising and agent-based models},
  author={Sornette, Didier},
  journal={Reports on progress in physics},
  volume={77},
  number={6},
  pages={062001},
  year={2014},
  publisher={IOP Publishing}
}

@article{chakraborti2011econophysics,
  title={Econophysics review: II. Agent-based models},
  author={Chakraborti, Anirban and Toke, Ioane Muni and Patriarca, Marco and Abergel, Fr{\'e}d{\'e}ric},
  journal={Quantitative Finance},
  volume={11},
  number={7},
  pages={1013--1041},
  year={2011},
  publisher={Taylor \& Francis}
}

@article{feigenbaum2003financial,
  title={Financial physics},
  author={Feigenbaum, James},
  journal={Reports on Progress in Physics},
  volume={66},
  number={10},
  pages={1611},
  year={2003},
  publisher={IOP Publishing}
}

@inproceedings{bachelier1900theorie,
  title={Th{\'e}orie de la sp{\'e}culation},
  author={Bachelier, Louis},
  booktitle={Annales scientifiques de l'{\'E}cole normale sup{\'e}rieure},
  volume={17},
  pages={21--86},
  year={1900}
}

@article{einstein1906theory,
  title={On the theory of the Brownian movement},
  author={Einstein, Albert},
  journal={Ann. Phys.},
  volume={17},
  number={549},
  year={1905}
}

@article{sunder2006determinants,
  title={Determinants of economic interaction: Behavior or structure},
  author={Sunder, Shyam},
  journal={Journal of Economic Interaction and Coordination},
  volume={1},
  number={1},
  pages={21--32},
  year={2006},
  publisher={Springer}
}

@article{kanazawa2018derivation,
  title={Derivation of the Boltzmann equation for financial Brownian motion: Direct observation of the collective motion of high-frequency traders},
  author={Kanazawa, Kiyoshi and Sueshige, Takumi and Takayasu, Hideki and Takayasu, Misako},
  journal={Physical review letters},
  volume={120},
  number={13},
  pages={138301},
  year={2018},
  publisher={APS}
}

@article{farmer2009economy,
  title={The economy needs agent-based modelling},
  author={Farmer, J Doyne and Foley, Duncan},
  journal={Nature},
  volume={460},
  number={7256},
  pages={685--686},
  year={2009},
  publisher={Nature Publishing Group}
}

@article{becker1962irrational,
  title={Irrational behavior and economic theory},
  author={Becker, Gary S},
  journal={Journal of political economy},
  volume={70},
  number={1},
  pages={1--13},
  year={1962},
  publisher={The University of Chicago Press}
}

@article{gode1993allocative,
  title={Allocative efficiency of markets with zero-intelligence traders: Market as a partial substitute for individual rationality},
  author={Gode, Dhananjay K and Sunder, Shyam},
  journal={Journal of political economy},
  volume={101},
  number={1},
  pages={119--137},
  year={1993},
  publisher={The University of Chicago Press}
}

@article{ladley2012zero,
  title={Zero intelligence in economics and finance},
  author={Ladley, Dan},
  journal={The Knowledge Engineering Review},
  volume={27},
  year={2012}
}

@article{cont1997herd,
  title={Herd behavior and aggregate fluctuations in financial markets},
  author={Cont, Rama and Bouchaud, Jean-Philippe},
  journal={arXiv preprint cond-mat/9712318},
  year={1997}
}

@article{bornholdt2001expectation,
  title={Expectation bubbles in a spin model of markets: Intermittency from frustration across scales},
  author={Bornholdt, Stefan},
  journal={International Journal of Modern Physics C},
  volume={12},
  number={05},
  pages={667--674},
  year={2001},
  publisher={World Scientific}
}

@article{paluch2015hierarchical,
  title={Hierarchical Cont-Bouchaud Model},
  author={Paluch, Robert and Suchecki, Krzysztof and Holyst, Janusz A},
  journal={arXiv preprint arXiv:1502.02015},
  year={2015}
}

@article{brock1997rational,
  title={A rational route to randomness},
  author={Brock, William A and Hommes, Cars H},
  journal={Econometrica: Journal of the Econometric Society},
  pages={1059--1095},
  year={1997},
  publisher={JSTOR}
}

@article{brock1998heterogeneous,
  title={Heterogeneous beliefs and routes to chaos in a simple asset pricing model},
  author={Brock, William A and Hommes, Cars H},
  journal={Journal of Economic dynamics and Control},
  volume={22},
  number={8-9},
  pages={1235--1274},
  year={1998},
  publisher={Elsevier}
}

@article{kirman1993ants,
  title={Ants, rationality, and recruitment},
  author={Kirman, Alan},
  journal={The Quarterly Journal of Economics},
  volume={108},
  number={1},
  pages={137--156},
  year={1993},
  publisher={MIT Press}
}

@article{lux1999scaling,
  title={Scaling and criticality in a stochastic multi-agent model of a financial market},
  author={Lux, Thomas and Marchesi, Michele},
  journal={Nature},
  volume={397},
  number={6719},
  pages={498--500},
  year={1999},
  publisher={Nature Publishing Group}
}

@article{gilli2003global,
  title={A global optimization heuristic for estimating agent based models},
  author={Gilli, Manfred and Winker, Peter},
  journal={Computational Statistics \& Data Analysis},
  volume={42},
  number={3},
  pages={299--312},
  year={2003},
  publisher={Elsevier}
}

@article{franke2012structural,
  title={Structural stochastic volatility in asset pricing dynamics: Estimation and model contest},
  author={Franke, Reiner and Westerhoff, Frank},
  journal={Journal of Economic Dynamics and Control},
  volume={36},
  number={8},
  pages={1193--1211},
  year={2012},
  publisher={Elsevier}
}

@article{kukacka2020complex,
  title={Do ‘complex’financial models really lead to complex dynamics? Agent-based models and multifractality},
  author={Kukacka, Jiri and Kristoufek, Ladislav},
  journal={Journal of Economic Dynamics and Control},
  volume={113},
  pages={103855},
  year={2020},
  publisher={Elsevier}
}

@article{barde2016direct,
  title={Direct comparison of agent-based models of herding in financial markets},
  author={Barde, Sylvain},
  journal={Journal of Economic Dynamics and Control},
  volume={73},
  pages={329--353},
  year={2016},
  publisher={Elsevier}
}

@article{hommes2006heterogeneous,
  title={Heterogeneous agent models in economics and finance},
  author={Hommes, Cars H},
  journal={Handbook of computational economics},
  volume={2},
  pages={1109--1186},
  year={2006},
  publisher={Elsevier}
}

@article{simon1969effect,
  title={The effect of income on fertility},
  author={Simon, Julian L},
  journal={Population Studies},
  volume={23},
  number={3},
  pages={327--341},
  year={1969},
  publisher={Taylor \& Francis}
}

@article{levy1997new,
  title={New evidence for the power-law distribution of wealth},
  author={Levy, Moshe and Solomon, Sorin},
  journal={Physica A: Statistical Mechanics and its Applications},
  volume={242},
  number={1-2},
  pages={90--94},
  year={1997},
  publisher={Elsevier}
}

@article{mantegna1995scaling,
  title={Scaling behaviour in the dynamics of an economic index},
  author={Mantegna, Rosario N and Stanley, H Eugene},
  journal={Nature},
  volume={376},
  number={6535},
  pages={46--49},
  year={1995},
  publisher={Nature Publishing Group}
}

@article{adamou2016dynamics,
  title={Dynamics of inequality},
  author={Adamou, Alexander and Peters, Ole},
  journal={Significance},
  volume={13},
  number={3},
  pages={32--35},
  year={2016},
  publisher={Wiley Online Library}
}


@article{gabaix2016dynamics,
  title={The dynamics of inequality},
  author={Gabaix, Xavier and Lasry, Jean-Michel and Lions, Pierre-Louis and Moll, Benjamin},
  journal={Econometrica},
  volume={84},
  number={6},
  pages={2071--2111},
  year={2016},
  publisher={Wiley Online Library}
}

@article{voitchovsky2009inequality,
  title={Inequality and economic growth},
  author={Voitchovsky, Sarah},
  journal={The Oxford Handbook of Economic Inequality},
  year={2009}
}

@article{panizza2002income,
  title={Income inequality and economic growth: Evidence from American data},
  author={Panizza, Ugo},
  journal={Journal of Economic Growth},
  volume={7},
  number={1},
  pages={25--41},
  year={2002},
  publisher={Springer}
}

@article{fama1970efficient,
  title={Efficient capital markets: A review of theory and empirical work},
  author={Fama, Eugene F},
  journal={The journal of Finance},
  volume={25},
  number={2},
  pages={383--417},
  year={1970},
  publisher={JSTOR}
}

@article{bouchaud2010endogenous,
  title={The endogenous dynamics of markets: price impact and feedback loops},
  author={Bouchaud, Jean-Philippe},
  journal={arXiv preprint arXiv:1009.2928},
  year={2010}
}

@article{joulin2008stock,
  title={Stock price jumps: news and volume play a minor role},
  author={Joulin, Armand and Lefevre, Augustin and Grunberg, Daniel and Bouchaud, Jean-Philippe},
  journal={arXiv preprint arXiv:0803.1769},
  year={2008}
}

@article{lux2001turbulence,
  title={Turbulence in financial markets: the surprising explanatory power of simple cascade models},
  author={Lux, Thomas and others},
  journal={Quantitative finance},
  volume={1},
  number={6},
  pages={632--640},
  year={2001},
  publisher={Taylor \& Francis}
}

@article{ghashghaie1996turbulent,
  title={Turbulent cascades in foreign exchange markets},
  author={Ghashghaie, Shoaleh and Breymann, Wolfgang and Peinke, Joachim and Talkner, Peter and Dodge, Yadollah},
  journal={Nature},
  volume={381},
  number={6585},
  pages={767--770},
  year={1996},
  publisher={Nature Publishing Group}
}

@incollection{gaunersdorfer2007nonlinear,
  title={A nonlinear structural model for volatility clustering},
  author={Gaunersdorfer, Andrea and Hommes, Cars},
  booktitle={Long memory in economics},
  pages={265--288},
  year={2007},
  publisher={Springer}
}


@article{grossman1988government,
  title={Government and economic growth: A non-linear relationship},
  author={Grossman, Philip J},
  journal={Public Choice},
  volume={56},
  number={2},
  pages={193--200},
  year={1988},
  publisher={Springer}
}

@article{brinca2019nonlinear,
  title={The nonlinear effects of fiscal policy},
  author={Brinca, Pedro and Faria-e-Castro, Miguel and Homem Ferreira, Miguel and Holter, Hans},
  journal={FRB St. Louis Working Paper},
  number={2019-15},
  year={2019}
}

@article{ozbayoglu2020deep,
	title={Deep learning for financial applications: A survey},
	author={Ozbayoglu, Ahmet Murat and Gudelek, Mehmet Ugur and Sezer, Omer Berat},
	journal={Applied Soft Computing},
	pages={106384},
	year={2020},
	publisher={Elsevier}
}

@article{hasegawa2019fluctuation,
	title={Fluctuation theorem uncertainty relation},
	author={Hasegawa, Yoshihiko and Van Vu, Tan},
	journal={Physical Review Letters},
	volume={123},
	number={11},
	pages={110602},
	year={2019},
	publisher={APS}
}

@article{ito2020stochastic,
	title={Stochastic Time Evolution, Information Geometry, and the Cram{\'e}r-Rao Bound},
	author={Ito, Sosuke and Dechant, Andreas},
	journal={Physical Review X},
	volume={10},
	number={2},
	pages={021056},
	year={2020},
	publisher={APS}
}

@article{liu2019thermodynamic,
	title={Thermodynamic Uncertainty Relation for Arbitrary Initial States},
	author={Liu, Kangqiao and Gong, Zongping and Ueda, Masahito},
	journal={arXiv preprint arXiv:1912.11797},
	year={2019}
}

@incollection{LANDAU198795,
title = "CHAPTER III - TURBULENCE",
editor = "L.D. LANDAU and E.M. LIFSHITZ",
booktitle = "Fluid Mechanics (Second Edition)",
publisher = "Pergamon",
edition = "Second Edition",
pages = "95 - 156",
year = "1987",
isbn = "978-0-08-033933-7",
doi = "https://doi.org/10.1016/B978-0-08-033933-7.50011-8",
url = "http://www.sciencedirect.com/science/article/pii/B9780080339337500118",
author = "L.D. LANDAU and E.M. LIFSHITZ"
}

@book{tikhomirov1991selected,
  title={Selected Works of AN Kolmogorov: Volume I: Mathematics and Mechanics},
  author={Tikhomirov, Vladimir M},
  volume={25},
  year={1991},
  publisher={Springer Science \& Business Media}
}

@article{grant1962turbulence,
  title={Turbulence spectra from a tidal channel},
  author={Grant, HL and Stewart, RW and Moilliet, A},
  journal={Journal of Fluid Mechanics},
  volume={12},
  number={2},
  pages={241--268},
  year={1962},
  publisher={Cambridge University Press}
}

@article{kraichnan1967inertial,
  title={Inertial ranges in two-dimensional turbulence},
  author={Kraichnan, Robert H},
  journal={The Physics of Fluids},
  volume={10},
  number={7},
  pages={1417--1423},
  year={1967},
  publisher={American Institute of Physics}
}

@book{wyld1961formulation,
  title={Formulation of the theory of turbulence in an incompressible fluid},
  author={Wyld, Henry William},
  volume={9868},
  year={1961},
  publisher={Physics Department, University of Illinois}
}

@article{zakharov2004one,
  title={One-dimensional wave turbulence},
  author={Zakharov, Vladimir and Dias, Fr{\'e}d{\'e}ric and Pushkarev, Andrei},
  journal={Physics Reports},
  volume={398},
  number={1},
  pages={1--65},
  year={2004},
  publisher={Elsevier}
}

@article{kolmogorov1991local,
  title={The local structure of turbulence in incompressible viscous fluid for very large Reynolds numbers},
  author={Kolmogorov, Andrei Nikolaevich},
  journal={Proceedings of the Royal Society of London. Series A: Mathematical and Physical Sciences},
  volume={434},
  number={1890},
  pages={9--13},
  year={1991},
  publisher={The Royal Society London}
}

@book{ter2013collected,
  title={Collected papers of LD Landau},
  author={Ter Haar, Dirk},
  year={2013},
  publisher={Elsevier}
}

@book{barenblatt2003scaling,
  title={Scaling},
  author={Barenblatt, Grigory Isaakovich},
  volume={34},
  year={2003},
  publisher={Cambridge University Press}
}


@inproceedings{krogh1992simple,
  title={A simple weight decay can improve generalization},
  author={Krogh, Anders and Hertz, John A},
  booktitle={Advances in neural information processing systems},
  pages={950--957},
  year={1992}
}

@inproceedings{domingos2000unified,
  title={A unified bias-variance decomposition},
  author={Domingos, Pedro},
  booktitle={Proceedings of 17th International Conference on Machine Learning},
  pages={231--238},
  year={2000}
}

@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@article{allen2018convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  journal={arXiv preprint arXiv:1811.03962},
  year={2018}
}

@article{geiger2019jamming,
  title={Jamming transition as a paradigm to understand the loss landscape of deep neural networks},
  author={Geiger, Mario and Spigler, Stefano and d'Ascoli, St{\'e}phane and Sagun, Levent and Baity-Jesi, Marco and Biroli, Giulio and Wyart, Matthieu},
  journal={Physical Review E},
  volume={100},
  number={1},
  pages={012115},
  year={2019},
  publisher={APS}
}

@article{franz2019jamming,
  title={Jamming in multilayer supervised learning models},
  author={Franz, Silvio and Hwang, Sungmin and Urbani, Pierfrancesco},
  journal={Physical review letters},
  volume={123},
  number={16},
  pages={160602},
  year={2019},
  publisher={APS}
}

@article{li2018neural,
  title={Neural network renormalization group},
  author={Li, Shuo-Hui and Wang, Lei},
  journal={Physical review letters},
  volume={121},
  number={26},
  pages={260601},
  year={2018},
  publisher={APS}
}

@article{li2018exploring,
  title={Exploring the function space of deep-learning machines},
  author={Li, Bo and Saad, David},
  journal={Physical review letters},
  volume={120},
  number={24},
  pages={248301},
  year={2018},
  publisher={APS}
}

@article{zdeborova2016statistical,
  title={Statistical physics of inference: Thresholds and algorithms},
  author={Zdeborov{\'a}, Lenka and Krzakala, Florent},
  journal={Advances in Physics},
  volume={65},
  number={5},
  pages={453--552},
  year={2016},
  publisher={Taylor \& Francis}
}

@article{goldt2017stochastic,
  title={Stochastic thermodynamics of learning},
  author={Goldt, Sebastian and Seifert, Udo},
  journal={Physical review letters},
  volume={118},
  number={1},
  pages={010601},
  year={2017},
  publisher={APS}
}

@inproceedings{duvenaud2016early,
  title={Early stopping as nonparametric variational inference},
  author={Duvenaud, David and Maclaurin, Dougal and Adams, Ryan},
  booktitle={Artificial Intelligence and Statistics},
  pages={1070--1077},
  year={2016}
}

@article{chaudhari2019entropy,
  title={Entropy-sgd: Biasing gradient descent into wide valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2019},
  number={12},
  pages={124018},
  year={2019},
  publisher={IOP Publishing}
}

@article{li2019gradient,
  title={Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks},
  author={Li, Mingchen and Soltanolkotabi, Mahdi and Oymak, Samet},
  journal={arXiv preprint arXiv:1903.11680},
  year={2019}
}

@article{chizat2018note,
  title={A note on lazy training in supervised differentiable programming},
  author={Chizat, Lenaic and Bach, Francis},
  journal={arXiv preprint arXiv:1812.07956},
  volume={8},
  year={2018}
}

@article{krogh1992generalization,
  title={Generalization in a linear perceptron in the presence of noise},
  author={Krogh, Anders and Hertz, John A},
  journal={Journal of Physics A: Mathematical and General},
  volume={25},
  number={5},
  pages={1135},
  year={1992},
  publisher={IOP Publishing}
}

@article{gardner1989three,
  title={Three unfinished works on the optimal storage capacity of networks},
  author={Gardner, Elizabeth and Derrida, Bernard},
  journal={Journal of Physics A: Mathematical and General},
  volume={22},
  number={12},
  pages={1983},
  year={1989},
  publisher={IOP Publishing}
}

@article{mei2019generalization,
  title={The generalization error of random features regression: Precise asymptotics and double descent curve},
  author={Mei, Song and Montanari, Andrea},
  journal={arXiv preprint arXiv:1908.05355},
  year={2019}
}

@article{hastie2019surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={arXiv preprint arXiv:1903.08560},
  year={2019}
}

@article{mei2019mean,
  title={Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={arXiv preprint arXiv:1902.06015},
  year={2019}
}

@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}

@article{mehta2018loss,
  title={The loss surface of deep linear networks viewed through the algebraic geometry lens},
  author={Mehta, Dhagash and Chen, Tianran and Tang, Tingting and Hauenstein, Jonathan D},
  journal={arXiv preprint arXiv:1810.07716},
  year={2018}
}

@article{arora2018convergence,
  title={A convergence analysis of gradient descent for deep linear neural networks},
  author={Arora, Sanjeev and Cohen, Nadav and Golowich, Noah and Hu, Wei},
  journal={arXiv preprint arXiv:1810.02281},
  year={2018}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}


@Article{Simonyan14c,
    author       = "Simonyan, K. and Zisserman, A.",
    title        = "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    journal      = "CoRR",
    volume       = "abs/1409.1556",
    year         = "2014"
}

@inproceedings{Reddi2018convergence,
title	= {On the convergence of Adam and Beyond},
author	= {Sashank Reddi and Satyen Kale and Sanjiv Kumar},
year	= {2018},
booktitle	= {International Conference on Learning Representations}
}



@book{amari2007methods,
  title={Methods of Information Geometry},
  author={Amari, S. and Nagaoka, H.},
  isbn={9780821843024},
  lccn={00059362},
  series={Translations of mathematical monographs},
  url={https://books.google.co.jp/books?id=vc2FWSo7wLUC},
  year={2007},
  publisher={American Mathematical Society}
}

@inproceedings{hoffer2017train,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1731--1741},
  year={2017}
}

@article{gring2012relaxation,
  title={Relaxation and prethermalization in an isolated quantum system},
  author={Gring, Michael and Kuhnert, Maximilian and Langen, Tim and Kitagawa, Takuya and Rauer, Bernhard and Schreitl, Matthias and Mazets, Igor and Smith, D Adu and Demler, Eugene and Schmiedmayer, J{\"o}rg},
  journal={Science},
  volume={337},
  number={6100},
  pages={1318--1322},
  year={2012},
  publisher={American Association for the Advancement of Science}
}

@book{villani2003topics,
  title={Topics in Optimal Transportation},
  author={Villani, C.},
  isbn={9780821833124},
  lccn={2003040350},
  series={Graduate studies in mathematics},
  url={https://books.google.co.jp/books?id=GqRXYFxe0l0C},
  year={2003},
  publisher={American Mathematical Society}
}

@article{funai2018thermodynamicsRG,
  title={Thermodynamics and Feature Extraction by Machine Learning},
  author={Funai, Shotaro Shiba and Giataganas, Dimitrios},
  journal={arXiv preprint arXiv:1810.08179},
  year={2018}
}

@article{iso2018scaleRG,
  title={Scale-invariant feature extraction of neural network and renormalization group flow},
  author={Iso, Satoshi and Shiba, Shotaro and Yokoo, Sumito},
  journal={Physical Review E},
  volume={97},
  number={5},
  pages={053304},
  year={2018},
  publisher={APS}
}

@inproceedings{cuturi2013sinkhorn,
  title={Sinkhorn distances: Lightspeed computation of optimal transport},
  author={Cuturi, Marco},
  booktitle={Advances in neural information processing systems},
  pages={2292--2300},
  year={2013}
}

@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@article{renormalizationGroup2014,
  title={An exact mapping between the variational renormalization group and deep learning},
  author={Mehta, Pankaj and Schwab, David J},
  journal={arXiv preprint arXiv:1410.3831},
  year={2014}
}

@article{freeman2016topology,
  title={Topology and geometry of half-rectified network optimization},
  author={Freeman, C Daniel and Bruna, Joan},
  journal={arXiv preprint arXiv:1611.01540},
  year={2016}
}

@article{smith2017bayesian,
  title={A bayesian perspective on generalization and stochastic gradient descent},
  author={Smith, Samuel L and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.06451},
  year={2017}
}

@article{mori2018thermalization,
  title={Thermalization and prethermalization in isolated quantum systems: a theoretical overview},
  author={Mori, Takashi and Ikeda, Tatsuhiko N and Kaminishi, Eriko and Ueda, Masahito},
  journal={Journal of Physics B: Atomic, Molecular and Optical Physics},
  volume={51},
  number={11},
  pages={112001},
  year={2018},
  publisher={IOP Publishing}
}

@article{baity2018comparing,
  title={Comparing dynamics: Deep neural networks versus glassy systems},
  author={Baity-Jesi, Marco and Sagun, Levent and Geiger, Mario and Spigler, Stefano and Arous, G Ben and Cammarota, Chiara and LeCun, Yann and Wyart, Matthieu and Biroli, Giulio},
  journal={arXiv preprint arXiv:1803.06969},
  year={2018}
}

@inproceedings{choromanska2015loss,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  pages={192--204},
  year={2015}
}

@article{Cichocki2015TensorDF,
  title={Tensor Decompositions for Signal Processing Applications: From two-way to multiway component analysis},
  author={Andrzej Cichocki and Danilo P. Mandic and Anh Huy Phan and Cesar F. Caiafa and Guoxu Zhou and Qibin Zhao and Lieven De Lathauwer},
  journal={IEEE Signal Processing Magazine},
  year={2015},
  volume={32},
  pages={145-163}
}

@inproceedings{nesterov1983method,
  title={A method of solving a convex programming problem with convergence rate O(k^2)},
  author={Nesterov, Yurii Evgen'evich},
  booktitle={Doklady Akademii Nauk},
  volume={269},
  number={3},
  pages={543--547},
  year={1983},
  organization={Russian Academy of Sciences}
}

@article{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}

@article{arjovsky2017wasserstein,
  title={Wasserstein gan},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1701.07875},
  year={2017}
}

@article{neyshabur2017geometry,
  title={Geometry of optimization and implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Salakhutdinov, Ruslan and Srebro, Nathan},
  journal={arXiv preprint arXiv:1705.03071},
  year={2017}
}

@article{seifert2008stochastic,
  title={Stochastic thermodynamics: principles and perspectives},
  author={Seifert, Udo},
  journal={The European Physical Journal B},
  volume={64},
  number={3-4},
  pages={423--431},
  year={2008},
  publisher={Springer}
}

@inproceedings{DeepLagrangianNetworks,
  title={ICLR 2019 Deep Lagrangian Networks : Using Physics as Model Prior for Deep Learning},
  author={Michael Lutter and Jan Peters},
  year={2019}
}

@article{smith2017don,
  title={Don't decay the learning rate, increase the batch size},
  author={Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V},
  journal={arXiv preprint arXiv:1711.00489},
  year={2017}
}

@article{krizhevsky2014one,
  title={One weird trick for parallelizing convolutional neural networks},
  author={Krizhevsky, Alex},
  journal={arXiv preprint arXiv:1404.5997},
  year={2014}
}

@InProceedings{pmlr-v37-rezende15,
  title = 	 {Variational Inference with Normalizing Flows},
  author = 	 {Danilo Rezende and Shakir Mohamed},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1530--1538},
  year = 	 {2015},
  editor = 	 {Francis Bach and David Blei},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/rezende15.pdf},
  url = 	 {http://proceedings.mlr.press/v37/rezende15.html},
  abstract = 	 {The choice of the approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.}
}

@inproceedings{NNhamiltonian,
author = {W. Howse, James and Abdallah, Chaouki and Heileman, Gregory},
year = {1995},
month = {01},
pages = {274-280},
title = {Gradient and Hamiltonian Dynamics Applied to Learning in Neural Networks}
}

@article{parcollet2018quaternion,
  title={Quaternion recurrent neural networks},
  author={Parcollet, Titouan and Ravanelli, Mirco and Morchid, Mohamed and Linar{\`e}s, Georges and Trabelsi, Chiheb and De Mori, Renato and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1806.04418},
  year={2018}
}


@inproceedings{wisdom2016fullUnitary,
  title={Full-capacity unitary recurrent neural networks},
  author={Wisdom, Scott and Powers, Thomas and Hershey, John and Le Roux, Jonathan and Atlas, Les},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4880--4888},
  year={2016}
}

@inproceedings{arjovsky2016unitary,
  title={Unitary evolution recurrent neural networks},
  author={Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={1120--1128},
  year={2016}
}

@article{1999mft,
author = {Tanaka, Toshiyuki},
year = {1999},
month = {04},
pages = {},
title = {A Theory of Mean Field Approximation},
journal = {Advances in Neural Information Processing Systems (NIPS)}
}

@article{2018mft,
  title={Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks},
  author={Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha and Schoenholz, Samuel S and Pennington, Jeffrey},
  journal={arXiv preprint arXiv:1806.05393},
  year={2018}
}

@inproceedings{Zhang2017langevin,
  title={No . 067 April 4 , 2017 Theory of Deep Learning III : Generalization Properties of SGD by},
  author={Chiyuan Zhang and Qianli Liao and Alexander Rakhlin and Karthik Sridharan and Brando Miranda and Noah Golowich and Tomaso A. Poggio},
  year={2017}
}

@article{quantumAnnealing_langevin,
author = {Ohzeki, Masayuki and Okada, Shuntaro and Terabe, Masayoshi and Taguchi, Shinichiro},
year = {2018},
month = {12},
pages = {},
title = {Optimization of neural networks via finite-value quantum fluctuations},
volume = {8},
journal = {Scientific Reports},
doi = {10.1038/s41598-018-28212-4}
}

@inproceedings{marceau2017langevin,
  title={Natural Langevin dynamics for neural networks},
  author={Marceau-Caron, Ga{\'e}tan and Ollivier, Yann},
  booktitle={International Conference on Geometric Science of Information},
  pages={451--459},
  year={2017},
  organization={Springer}
}

@Article{Neelakanta1991_langevin,
author="Neelakanta, P. S.
and Sudhakar, R.
and DeGroff, D.",
title="Langevin machine: a neural network based on stochastically justifiable sigmoidal function",
journal="Biological Cybernetics",
year="1991",
month="Sep",
day="01",
volume="65",
number="5",
pages="331--338",
abstract="In neural networks the activation process controls the output as a nonlinear function of the input; and, this output remains bounded between limits as decided by a logistic function known as the sigmoid (S-shaped). Presently, by applying the considerations of Maxwell-Boltzmann statistics, the Langevin function is shown as the appropriate and justifiable sigmoid (instead of the conventional hyperbolic tangent function) to depict the bipolar nonlinear logic-operation enunciated by the collective stochastical response of artificial neurons under activation. That is, the graded response of a large network of `neurons' such as Hopfield's can be stochastically justified via the proposed model. The model is consistent with the established link between the Hopfield model and the statistical mechanics. The possible outcomes and implications of using the Langevin function (in lieu of conventional hyperbolic tangent and/or exponential sigmoids) in determining nonlinear decision boundaries, in characterizing the neural networks by the Langevin machine versus the Boltzmann machine, in sharpening and annealing schedules and in the optimization of nonlinear detector performance are discussed.",
issn="1432-0770",
doi="10.1007/BF00216966",
url="https://doi.org/10.1007/BF00216966"
}




@INPROCEEDINGS{whyNaturalGradient, 
author={S. {Amari} and S. C. {Douglas}}, 
booktitle={Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)}, 
title={Why natural gradient?}, 
year={1998}, 
volume={2}, 
number={}, 
pages={1213-1216 vol.2}, 
keywords={differential geometry;maximum likelihood estimation;convergence of numerical methods;adaptive estimation;optimisation;iterative methods;convergence speed;cost function;natural gradient adaptation;differential geometry;Riemannian structure;parameter space;gradient search direction;maximum likelihood estimation tasks;Cost function;Parameter estimation;Newton method;Iterative methods;Convergence;Information systems;Cities and towns;Geometry;Optimization methods;Adaptive filters}, 
doi={10.1109/ICASSP.1998.675489}, 
ISSN={1520-6149}, 
month={May},}



@inproceedings{Bamler2018ImprovingOI,
  title={Improving Optimization in Models With Continuous Symmetry Breaking},
  author={Robert Bamler and Stephan Mandt},
  booktitle={ICML},
  year={2018}
}

@article{SMInference,
author = {Lenka Zdeborová and Florent Krzakala},
title = {Statistical physics of inference: thresholds and algorithms},
journal = {Advances in Physics},
volume = {65},
number = {5},
pages = {453-552},
year  = {2016},
publisher = {Taylor & Francis},
doi = {10.1080/00018732.2016.1211393},

URL = { 
        https://doi.org/10.1080/00018732.2016.1211393
    
},
eprint = { 
        https://doi.org/10.1080/00018732.2016.1211393
    
}

}

@article{rnnPrl,
  title = {Local Dynamics in Trained Recurrent Neural Networks},
  author = {Rivkind, Alexander and Barak, Omri},
  journal = {Phys. Rev. Lett.},
  volume = {118},
  issue = {25},
  pages = {258101},
  numpages = {5},
  year = {2017},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.118.258101},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.118.258101}
}


@article{functionSpaceDL,
  title = {Exploring the Function Space of Deep-Learning Machines},
  author = {Li, Bo and Saad, David},
  journal = {Phys. Rev. Lett.},
  volume = {120},
  issue = {24},
  pages = {248301},
  numpages = {6},
  year = {2018},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.120.248301},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.120.248301}
}



@article{chaosInNN,
  title = {Chaos in Random Neural Networks},
  author = {Sompolinsky, H. and Crisanti, A. and Sommers, H. J.},
  journal = {Phys. Rev. Lett.},
  volume = {61},
  issue = {3},
  pages = {259--262},
  numpages = {0},
  year = {1988},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.61.259},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.61.259}
}


@article{matrixProductState,
  title = {Unsupervised Generative Modeling Using Matrix Product States},
  author = {Han, Zhao-Yu and Wang, Jun and Fan, Heng and Wang, Lei and Zhang, Pan},
  journal = {Phys. Rev. X},
  volume = {8},
  issue = {3},
  pages = {031012},
  numpages = {13},
  year = {2018},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevX.8.031012},
  url = {https://link.aps.org/doi/10.1103/PhysRevX.8.031012}
}



@article {Hopfield2554,
	author = {Hopfield, J J},
	title = {Neural networks and physical systems with emergent collective computational abilities},
	volume = {79},
	number = {8},
	pages = {2554--2558},
	year = {1982},
	doi = {10.1073/pnas.79.8.2554},
	publisher = {National Academy of Sciences},
	abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/79/8/2554},
	eprint = {https://www.pnas.org/content/79/8/2554.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@InProceedings{boltzman,
  title = 	 {Deep Boltzmann Machines},
  author = 	 {Ruslan Salakhutdinov and Geoffrey Hinton},
  booktitle = 	 {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {448--455},
  year = 	 {2009},
  editor = 	 {David van Dyk and Max Welling},
  volume = 	 {5},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
  month = 	 {16--18 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf},
  url = 	 {http://proceedings.mlr.press/v5/salakhutdinov09a.html},
  abstract = 	 {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized by a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.}
}

@article{neutrino,
  title = {Evidence for the Appearance of Atmospheric Tau Neutrinos in Super-Kamiokande},
  author = {Abe, K. and Hayato, Y. and Iida, T. and Iyogi, K. and Kameda, J. and Koshio, Y. and Kozuma, Y. and Marti, Ll. and Miura, M. and Moriyama, S. and Nakahata, M. and Nakayama, S. and Obayashi, Y. and Sekiya, H. and Shiozawa, M. and Suzuki, Y. and Takeda, A. and Takenaga, Y. and Ueno, K. and Ueshima, K. and Yamada, S. and Yokozawa, T. and Ishihara, C. and Kaji, H. and Kajita, T. and Kaneyuki, K. and Lee, K. P. and McLachlan, T. and Okumura, K. and Shimizu, Y. and Tanimoto, N. and Labarga, L. and Kearns, E. and Litos, M. and Raaf, J. L. and Stone, J. L. and Sulak, L. R. and Goldhaber, M. and Bays, K. and Kropp, W. R. and Mine, S. and Regis, C. and Renshaw, A. and Smy, M. B. and Sobel, H. W. and Ganezer, K. S. and Hill, J. and Keig, W. E. and Jang, J. S. and Kim, J. Y. and Lim, I. T. and Albert, J. B. and Scholberg, K. and Walter, C. W. and Wendell, R. and Wongjirad, T. M. and Ishizuka, T. and Tasaka, S. and Learned, J. G. and Matsuno, S. and Smith, S. N. and Hasegawa, T. and Ishida, T. and Ishii, T. and Kobayashi, T. and Nakadaira, T. and Nakamura, K. and Nishikawa, K. and Oyama, Y. and Sakashita, K. and Sekiguchi, T. and Tsukamoto, T. and Suzuki, A. T. and Takeuchi, Y. and Ikeda, M. and Minamino, A. and Nakaya, T. and Fukuda, Y. and Itow, Y. and Mitsuka, G. and Tanaka, T. and Jung, C. K. and Lopez, G. D. and Taylor, I. and Yanagisawa, C. and Ishino, H. and Kibayashi, A. and Mino, S. and Mori, T. and Sakuda, M. and Toyota, H. and Kuno, Y. and Yoshida, M. and Kim, S. B. and Yang, B. S. and Okazawa, H. and Choi, Y. and Nishijima, K. and Koshiba, M. and Yokoyama, M. and Totsuka, Y. and Martens, K. and Schuemann, J. and Vagins, M. R. and Chen, S. and Heng, Y. and Yang, Z. and Zhang, H. and Kielczewska, D. and Mijakowski, P. and Connolly, K. and Dziomba, M. and Thrane, E. and Wilkes, R. J.},
  collaboration = {Super-Kamiokande Collaboration},
  journal = {Phys. Rev. Lett.},
  volume = {110},
  issue = {18},
  pages = {181802},
  numpages = {7},
  year = {2013},
  month = {May},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.110.181802},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.110.181802}
}

@inproceedings{vahdat2017toward,
  title={Toward robustness against label noise in training deep discriminative neural networks},
  author={Vahdat, Arash},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5596--5605},
  year={2017}
}

@inproceedings{ross2018improving,
  title={Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients},
  author={Ross, Andrew Slavin and Doshi-Velez, Finale},
  booktitle={Thirty-second AAAI conference on artificial intelligence},
  year={2018}
}

@ARTICLE{CMB,
       author = {{Caldeira}, J. and {Wu}, W.~L.~K. and {Nord}, B. and {Avestruz}, C. and
         {Trivedi}, S. and {Story}, K.~T.},
        title = "{DeepCMB: Lensing Reconstruction of the Cosmic Microwave Background with Deep Neural Networks}",
      journal = {arXiv e-prints},
     keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Computer Science - Computer Vision and Pattern Recognition},
         year = "2018",
        month = "Oct",
          eid = {arXiv:1810.01483},
        pages = {arXiv:1810.01483},
archivePrefix = {arXiv},
       eprint = {1810.01483},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2018arXiv181001483C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{phaseTransition,
author = {Ohtsuki ,Tomoki and Ohtsuki ,Tomi},
title = {Deep Learning the Quantum Phase Transitions in Random Two-Dimensional Electron Systems},
journal = {Journal of the Physical Society of Japan},
volume = {85},
number = {12},
pages = {123706},
year = {2016},
doi = {10.7566/JPSJ.85.123706},

URL = { 
        https://doi.org/10.7566/JPSJ.85.123706
    
},
eprint = { 
        https://doi.org/10.7566/JPSJ.85.123706
    
}

}



@article{renormalizationGroup,
  title = {Neural Network Renormalization Group},
  author = {Li, Shuo-Hui and Wang, Lei},
  journal = {Phys. Rev. Lett.},
  volume = {121},
  issue = {26},
  pages = {260601},
  numpages = {7},
  year = {2018},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.121.260601},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.121.260601}
}








@article{UniversalPortfolio,
author = {Cover, Thomas M.},
title = {Universal Portfolios},
journal = {Mathematical Finance},
volume = {1},
number = {1},
pages = {1-29},
keywords = {portfolio selection, robust trading strategies, performance weighting, rebalancing},
doi = {10.1111/j.1467-9965.1991.tb00002.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9965.1991.tb00002.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9965.1991.tb00002.x},
year = {1991}
}



@book{CoverInformationTheory,
 author = {Cover, Thomas M. and Thomas, Joy A.},
 title = {Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)},
 year = {2006},
 isbn = {0471241954},
 publisher = {Wiley-Interscience},
 address = {New York, NY, USA},
} 


@article{Portfoliotheory,
 ISSN = {00221082, 15406261},
 URL = {http://www.jstor.org/stable/2975974},
 author = {Harry Markowitz},
 journal = {The Journal of Finance},
 number = {1},
 pages = {77--91},
 publisher = {[American Finance Association, Wiley]},
 title = {Portfolio Selection},
 volume = {7},
 year = {1952}
}



@inproceedings{Graves2011,
 author = {Graves, Alex},
 title = {Practical Variational Inference for Neural Networks},
 booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
 series = {NIPS'11},
 year = {2011},
 isbn = {978-1-61839-599-3},
 location = {Granada, Spain},
 pages = {2348--2356},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2986459.2986721},
 acmid = {2986721},
 publisher = {Curran Associates Inc.},
 address = {USA},
} 



@inproceedings{sgld,
  author = {Welling, Max and Teh, Yee Whye},
  biburl = {https://www.bibsonomy.org/bibtex/2f5ff37ac41677e6453ed602dae088478/dblp},
  booktitle = {ICML},
  editor = {Getoor, Lise and Scheffer, Tobias},
  interhash = {5c8207b828c210fb5283c258720164ee},
  intrahash = {f5ff37ac41677e6453ed602dae088478},
  keywords = {dblp},
  pages = {681-688},
  publisher = {Omnipress},
  timestamp = {2011-12-17T11:33:26.000+0100},
  title = {Bayesian Learning via Stochastic Gradient Langevin Dynamics.},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2011.html#WellingT11},
  year = 2011
}



@ARTICLE{BayesianDropout,
       author = {{Gal}, Yarin and {Ghahramani}, Zoubin},
        title = "{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = 2015,
        month = Jun,
          eid = {arXiv:1506.02142},
        pages = {arXiv:1506.02142},
archivePrefix = {arXiv},
       eprint = {1506.02142},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2015arXiv150602142G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{BayesianRNN,
       author = {{Fortunato}, Meire and {Blundell}, Charles and {Vinyals}, Oriol},
        title = "{Bayesian Recurrent Neural Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2017,
        month = Apr,
          eid = {arXiv:1704.02798},
        pages = {arXiv:1704.02798},
archivePrefix = {arXiv},
       eprint = {1704.02798},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2017arXiv170402798F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@inproceedings{Gal2016UncertaintyID,
  title={Uncertainty in Deep Learning},
  author={Yarin Gal},
  year={2016}
}

@ARTICLE{cutout,
       author = {{DeVries}, Terrance and {Taylor}, Graham W.},
        title = "{Improved Regularization of Convolutional Neural Networks with Cutout}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2017,
        month = Aug,
          eid = {arXiv:1708.04552},
        pages = {arXiv:1708.04552},
archivePrefix = {arXiv},
       eprint = {1708.04552},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2017arXiv170804552D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}




@ARTICLE{WRN,
       author = {{Zagoruyko}, Sergey and {Komodakis}, Nikos},
        title = "{Wide Residual Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
         year = 2016,
        month = May,
          eid = {arXiv:1605.07146},
        pages = {arXiv:1605.07146},
archivePrefix = {arXiv},
       eprint = {1605.07146},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/\#abs/2016arXiv160507146Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}




@inproceedings{Zhang_rethink,
title = {Understanding deep learning requires rethinking generalization},
author  = {Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
year  = {2017},
URL = {https://arxiv.org/abs/1611.03530}
}


@ARTICLE{memorization2017,
   author = {{Arpit}, D. and {Jastrz{\c e}bski}, S. and {Ballas}, N. and 
	{Krueger}, D. and {Bengio}, E. and {Kanwal}, M.~S. and {Maharaj}, T. and 
	{Fischer}, A. and {Courville}, A. and {Bengio}, Y. and {Lacoste-Julien}, S.
	},
    title = "{A Closer Look at Memorization in Deep Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1706.05394},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning},
     year = 2017,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170605394A},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{LeiWu_losslandscape,
   author = {{Wu}, L. and {Zhu}, Z. and {E}, W.},
    title = "{Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1706.10239},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
     year = 2017,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170610239W},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{HaoLi_visualizingLossLandscape,
   author = {{Li}, H. and {Xu}, Z. and {Taylor}, G. and {Studer}, C. and 
	{Goldstein}, T.},
    title = "{Visualizing the Loss Landscape of Neural Nets}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1712.09913},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
     year = 2017,
    month = dec,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171209913L},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{journals/corr/KingmaB14_adam,
  added-at = {2015-01-01T00:00:00.000+0100},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  biburl = {https://www.bibsonomy.org/bibtex/23b0328784dbfce338ba0dd2618a7a059/dblp},
  ee = {http://arxiv.org/abs/1412.6980},
  interhash = {57d2ac873f398f21bb94790081e80394},
  intrahash = {3b0328784dbfce338ba0dd2618a7a059},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2015-06-18T04:22:29.000+0200},
  title = {Adam: A Method for Stochastic Optimization.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1412.html#KingmaB14},
  volume = {abs/1412.6980},
  year = 2014
}

@article{journals/corr/abs-1212-5701_adadelta,
  added-at = {2013-01-02T00:00:00.000+0100},
  author = {Zeiler, Matthew D.},
  biburl = {https://www.bibsonomy.org/bibtex/2593eceee0e364927f3dd9c85e788bba8/dblp},
  ee = {http://arxiv.org/abs/1212.5701},
  interhash = {0485dc964af0cd2296b6868b7f97c90d},
  intrahash = {593eceee0e364927f3dd9c85e788bba8},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2013-01-03T11:32:48.000+0100},
  title = {ADADELTA: An Adaptive Learning Rate Method},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1212.html#abs-1212-5701},
  volume = {abs/1212.5701},
  year = 2012
}


@ARTICLE{Im_empiricalAnalysisoftheOptimization,
   author = {{Jiwoong Im}, D. and {Tao}, M. and {Branson}, K.},
    title = "{An empirical analysis of the optimization of deep network loss surfaces}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1612.04010},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = 2016,
    month = dec,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161204010J},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{xing2018_walk_sgd,
  abstract = {Exploring why stochastic gradient descent (SGD) based optimization methods
train deep neural networks (DNNs) that generalize well has become an active
area of research. Towards this end, we empirically study the dynamics of SGD
when training over-parametrized DNNs. Specifically we study the DNN loss
surface along the trajectory of SGD by interpolating the loss surface between
parameters from consecutive \textit{iterations} and tracking various metrics
during training. We find that the loss interpolation between parameters before
and after a training update is roughly convex with a minimum (\textit{valley
floor}) in between for most of the training. Based on this and other metrics,
we deduce that during most of the training, SGD explores regions in a valley by
bouncing off valley walls at a height above the valley floor. This 'bouncing
off walls at a height' mechanism helps SGD traverse larger distance for small
batch sizes and large learning rates which we find play qualitatively different
roles in the dynamics. While a large learning rate maintains a large height
from the valley floor, a small batch size injects noise facilitating
exploration. We find this mechanism is crucial for generalization because the
valley floor has barriers and this exploration above the valley floor allows
SGD to quickly travel far away from the initialization point (without being
affected by barriers) and find flatter regions, corresponding to better
generalization.},
  added-at = {2018-03-08T07:57:40.000+0100},
  author = {Xing, Chen and Arpit, Devansh and Tsirigotis, Christos and Bengio, Yoshua},
  biburl = {https://www.bibsonomy.org/bibtex/23e7a724ad89813716ce8172f0d9f68ba/jk_itwm},
  description = {A Walk with SGD},
  interhash = {5c2c3bd2326b3b4908374c68b52b9d65},
  intrahash = {3e7a724ad89813716ce8172f0d9f68ba},
  keywords = {SGD to_read},
  note = {cite arxiv:1802.08770Comment: First two authors contributed equally},
  timestamp = {2018-03-08T07:57:40.000+0100},
  title = {A Walk with SGD},
  url = {http://arxiv.org/abs/1802.08770},
  year = 2018
}

@misc{Tieleman2012_rmsprop,
  title={{Lecture 6.5---RmsProp: Divide the gradient by a running average of its recent magnitude}},
  author={Tieleman, T. and Hinton, G.},
  howpublished={COURSERA: Neural Networks for Machine Learning},
  year={2012}
}

@article{robbins1951_sgd,
author = "Robbins, Herbert and Monro, Sutton",
doi = "10.1214/aoms/1177729586",
fjournal = "The Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "09",
number = "3",
pages = "400--407",
publisher = "The Institute of Mathematical Statistics",
title = "A Stochastic Approximation Method",
url = "https://doi.org/10.1214/aoms/1177729586",
volume = "22",
year = "1951"
}

@ARTICLE{Zhang_optimizationProperitesSGD,
   author = {{Zhang}, C. and {Liao}, Q. and {Rakhlin}, A. and {Miranda}, B. and 
	{Golowich}, N. and {Poggio}, T.},
    title = "{Theory of Deep Learning IIb: Optimization Properties of SGD}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1801.02254},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = 2018,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180102254Z},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}








@article{1997flatMinima,
author = { Sepp Hochreiter and  Jürgen Schmidhuber},
title = {Flat Minima},
journal = {Neural Computation},
volume = {9},
number = {1},
pages = {1-42},
year = {1997},
doi = {10.1162/neco.1997.9.1.1},

URL = { 
        https://doi.org/10.1162/neco.1997.9.1.1
    
},
eprint = { 
        https://doi.org/10.1162/neco.1997.9.1.1
    
}
,
    abstract = { We present a new algorithm for finding low-complexity neural networks with high generalization capability. The algorithm searches for a “flat” minimum of the error function. A flat minimum is a large connected region in weight space where the error remains approximately constant. An MDL-based, Bayesian argument suggests that flat minima correspond to “simple” networks and low expected overfitting. The argument is based on a Gibbs algorithm variant and a novel way of splitting generalization error into underfitting and overfitting error. Unlike many previous approaches, ours does not require gaussian assumptions and does not depend on a “good” weight prior. Instead we have a prior over input output functions, thus taking into account net architecture and training set. Although our algorithm requires the computation of second-order derivatives, it has backpropagation's order of complexity. Automatically, it effectively prunes units, weights, and input lines. Various experiments with feedforward and recurrent nets are described. In an application to stock market prediction, flat minimum search outperforms conventional backprop, weight decay, and “optimal brain surgeon/optimal brain damage.” }
}


@ARTICLE{Dinh_SharpMinima,
   author = {{Dinh}, L. and {Pascanu}, R. and {Bengio}, S. and {Bengio}, Y.
	},
    title = "{Sharp Minima Can Generalize For Deep Nets}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1703.04933},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = 2017,
    month = mar,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170304933D},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Keskar2017,
   author = {{Shirish Keskar}, N. and {Mudigere}, D. and {Nocedal}, J. and 
	{Smelyanskiy}, M. and {Tang}, P.~T.~P.},
    title = "{On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1609.04836},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Mathematics - Optimization and Control},
     year = 2016,
    month = sep,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160904836S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Nair_relu,
 author = {Nair, Vinod and Hinton, Geoffrey E.},
 title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
 booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
 series = {ICML'10},
 year = {2010},
 isbn = {978-1-60558-907-7},
 location = {Haifa, Israel},
 pages = {807--814},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=3104322.3104425},
 acmid = {3104425},
 publisher = {Omnipress},
 address = {USA},
} 
@ARTICLE{Chiyuan_SGD,
   author = {{Zhang}, C. and {Liao}, Q. and {Rakhlin}, A. and {Miranda}, B. and 
	{Golowich}, N. and {Poggio}, T.},
    title = "{Theory of Deep Learning IIb: Optimization Properties of SGD}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1801.02254},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = 2018,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180102254Z},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{vc2017,
   author = {{Bartlett}, P.~L. and {Harvey}, N. and {Liaw}, C. and {Mehrabian}, A.
	},
    title = "{Nearly-tight VC-dimension and pseudodimension bounds for piecewise linear neural networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1703.02930},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = 2017,
    month = mar,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170302930B},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{largeBatch,
   author = {{Goyal}, P. and {Doll{\'a}r}, P. and {Girshick}, R. and {Noordhuis}, P. and 
	{Wesolowski}, L. and {Kyrola}, A. and {Tulloch}, A. and {Jia}, Y. and 
	{He}, K.},
    title = "{Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1706.02677},
 primaryClass = "cs.CV",
 keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Learning},
     year = 2017,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170602677G},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Qian:1999:MTG:307343.307376,
 author = {Qian, Ning},
 title = {On the Momentum Term in Gradient Descent Learning Algorithms},
 journal = {Neural Netw.},
 issue_date = {Jan. 1999},
 volume = {12},
 number = {1},
 month = jan,
 year = {1999},
 issn = {0893-6080},
 pages = {145--151},
 numpages = {7},
 url = {http://dx.doi.org/10.1016/S0893-6080(98)00116-6},
 doi = {10.1016/S0893-6080(98)00116-6},
 acmid = {307376},
 publisher = {Elsevier Science Ltd.},
 address = {Oxford, UK, UK},
 keywords = {critical damping, damped harmonic oscillator, gradient descent learning algorithm, learning rate, momentum, speed of convergence},
} 

@article{Duchi:2011:ASM:1953048.2021068,
 author = {Duchi, John and Hazan, Elad and Singer, Yoram},
 title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
 journal = {J. Mach. Learn. Res.},
 issue_date = {2/1/2011},
 volume = {12},
 month = jul,
 year = {2011},
 issn = {1532-4435},
 pages = {2121--2159},
 numpages = {39},
 url = {http://dl.acm.org/citation.cfm?id=1953048.2021068},
 acmid = {2021068},
 publisher = {JMLR.org},
} 

@inproceedings{
j.2018on,
title={On the Convergence of Adam and Beyond},
author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=ryQu7f-RZ},
}

@inproceedings{wilson2017marginal,
  title={The marginal value of adaptive gradient methods in machine learning},
  author={Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4148--4158},
  year={2017}
}


@article{journals/corr/Ruder16,
  added-at = {2018-08-13T00:00:00.000+0200},
  author = {Ruder, Sebastian},
  biburl = {https://www.bibsonomy.org/bibtex/2c57674dd553c513f7121bd6d93ad4b4f/dblp},
  ee = {http://arxiv.org/abs/1609.04747},
  interhash = {6e9f951ec79eba6cb7eb27db1e6d4ad6},
  intrahash = {c57674dd553c513f7121bd6d93ad4b4f},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2018-08-14T13:38:28.000+0200},
  title = {An overview of gradient descent optimization algorithms.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1609.html#Ruder16},
  volume = {abs/1609.04747},
  year = 2016
}

@article{Amari:1998:NGW:287476.287477,
 author = {Amari, Shun-Ichi},
 title = {Natural Gradient Works Efficiently in Learning},
 journal = {Neural Comput.},
 issue_date = {Feb. 15, 1998},
 volume = {10},
 number = {2},
 month = feb,
 year = {1998},
 issn = {0899-7667},
 pages = {251--276},
 numpages = {26},
 url = {http://dx.doi.org/10.1162/089976698300017746},
 doi = {10.1162/089976698300017746},
 acmid = {287477},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 


@misc{martens2014insights,
  abstract = {Natural gradient descent is an optimization method traditionally motivated
from the perspective of information geometry, and works well for many
applications as an alternative to stochastic gradient descent. In this paper we
critically analyze this method and its properties, and show how it can be
viewed as a type of approximate 2nd-order optimization method, where the Fisher
information matrix used to compute the natural gradient direction can be viewed
as an approximation of the Hessian. This perspective turns out to have
significant implications for how to design a practical and robust version of
the method. Among our various other contributions is a thorough analysis of the
convergence speed of natural gradient descent and more general stochastic
methods, a critical examination of the oft-used "empirical" approximation of
the Fisher matrix, and an analysis of the (approximate) parameterization
invariance property possessed by the method, which we show still holds for
certain other choices of the curvature matrix, but notably not the Hessian.},
  added-at = {2015-10-02T17:08:04.000+0200},
  author = {Martens, James},
  biburl = {https://www.bibsonomy.org/bibtex/2ec78b6b6c87d8bcf246f998355669b0b/stdiff},
  description = {New insights and perspectives on the natural gradient method},
  interhash = {5aa53b4d22911c7c08d404fe37a4b2b1},
  intrahash = {ec78b6b6c87d8bcf246f998355669b0b},
  keywords = {gradient-descent neural-network},
  note = {cite arxiv:1412.1193Comment: New title and abstract. Added multiple sections, including a proper  introduction/outline and one on convergence speed. Many other revisions  throughout},
  timestamp = {2015-10-02T17:08:04.000+0200},
  title = {New insights and perspectives on the natural gradient method},
  url = {http://arxiv.org/abs/1412.1193},
  year = 2014
}



% references added by mxm
@article{jospin2020hands,
  title={Hands-on Bayesian Neural Networks--a Tutorial for Deep Learning Users},
  author={Jospin, Laurent Valentin and Buntine, Wray and Boussaid, Farid and Laga, Hamid and Bennamoun, Mohammed},
  journal={arXiv preprint arXiv:2007.06823},
  year={2020}
}

@article{smith2017bayesian,
  title={A bayesian perspective on generalization and stochastic gradient descent},
  author={Smith, Samuel L and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.06451},
  year={2017}
}

@inproceedings{khan2018fast,
  title={Fast and scalable bayesian deep learning by weight-perturbation in adam},
  author={Khan, Mohammad and Nielsen, Didrik and Tangkaratt, Voot and Lin, Wu and Gal, Yarin and Srivastava, Akash},
  booktitle={International Conference on Machine Learning},
  pages={2611--2620},
  year={2018},
  organization={PMLR}
}

@article{maddox2019simple,
  title={A simple baseline for bayesian uncertainty in deep learning},
  author={Maddox, Wesley J and Izmailov, Pavel and Garipov, Timur and Vetrov, Dmitry P and Wilson, Andrew Gordon},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={13153--13164},
  year={2019}
}

@book{murphy2012machine,
  title={Machine learning: a probabilistic perspective},
  author={Murphy, Kevin P},
  year={2012},
  publisher={MIT press}
}


@inproceedings{gal2016dropout,
  title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning},
  pages={1050--1059},
  year={2016},
  organization={PMLR}
}

@article{lee2017deep,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.00165},
  year={2017}
}

@article{lakshminarayanan2017simple,
  title={Simple and scalable predictive uncertainty estimation using deep ensembles},
  author={Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{mackay1995probable,
  title={Probable networks and plausible predictions-a review of practical Bayesian methods for supervised neural networks},
  author={MacKay, David JC},
  journal={Network: computation in neural systems},
  volume={6},
  number={3},
  pages={469},
  year={1995},
  publisher={IOP Publishing}
}

@article{minka2013expectation,
  title={Expectation propagation for approximate Bayesian inference},
  author={Minka, Thomas P},
  journal={arXiv preprint arXiv:1301.2294},
  year={2013}
}



@inproceedings{welling2011bayesian,
  title={Bayesian learning via stochastic gradient Langevin dynamics},
  author={Welling, Max and Teh, Yee W},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={681--688},
  year={2011},
  organization={Citeseer}
}

@article{pearce2018uncertainty,
  title={Uncertainty in neural networks: Bayesian ensembling},
  author={Pearce, Tim and Zaki, Mohamed and Brintrup, Alexandra and Anastassacos, N and Neely, A},
  journal={stat},
  volume={1050},
  pages={12},
  year={2018}
}



@article{hoffman2013stochastic,
  title={Stochastic variational inference.},
  author={Hoffman, Matthew D and Blei, David M and Wang, Chong and Paisley, John},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={5},
  year={2013}
}

@article{graves2011practical,
  title={Practical variational inference for neural networks},
  author={Graves, Alex},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@inproceedings{dusenberry2020efficient,
  title={Efficient and scalable bayesian neural nets with rank-1 factors},
  author={Dusenberry, Michael and Jerfel, Ghassen and Wen, Yeming and Ma, Yian and Snoek, Jasper and Heller, Katherine and Lakshminarayanan, Balaji and Tran, Dustin},
  booktitle={International conference on machine learning},
  pages={2782--2792},
  year={2020},
  organization={PMLR}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}


@phdthesis{mackay1992bayesian,
  title={Bayesian methods for adaptive models},
  author={Mackay, David John Cameron},
  year={1992},
  school={California Institute of Technology}
}

@article{izmailov2021bayesian,
  title={What Are Bayesian Neural Network Posteriors Really Like?},
  author={Izmailov, Pavel and Vikram, Sharad and Hoffman, Matthew D and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2104.14421},
  year={2021}
}

@inproceedings{soudry2014expectation,
  title={Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights.},
  author={Soudry, Daniel and Hubara, Itay and Meir, Ron},
  booktitle={NIPS},
  volume={1},
  pages={2},
  year={2014}
}

@inproceedings{hernandez2015probabilistic,
  title={Probabilistic backpropagation for scalable learning of bayesian neural networks},
  author={Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Adams, Ryan},
  booktitle={International conference on machine learning},
  pages={1861--1869},
  year={2015},
  organization={PMLR}
}

@inproceedings{ritter2018scalable,
  title={A scalable laplace approximation for neural networks},
  author={Ritter, Hippolyt and Botev, Aleksandar and Barber, David},
  booktitle={6th International Conference on Learning Representations, ICLR 2018-Conference Track Proceedings},
  volume={6},
  year={2018},
  organization={International Conference on Representation Learning}
}

@article{osawa2019practical,
  title={Practical deep learning with Bayesian principles},
  author={Osawa, Kazuki and Swaroop, Siddharth and Jain, Anirudh and Eschenhagen, Runa and Turner, Richard E and Yokota, Rio and Khan, Mohammad Emtiyaz},
  journal={arXiv preprint arXiv:1906.02506},
  year={2019}
}



@article{izmailov2018averaging,
  title={Averaging weights leads to wider optima and better generalization},
  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1803.05407},
  year={2018}
}

@article{wilson2020bayesian,
  title={Bayesian deep learning and a probabilistic perspective of generalization},
  author={Wilson, Andrew Gordon and Izmailov, Pavel},
  journal={arXiv preprint arXiv:2002.08791},
  year={2020}
}

@inproceedings{rasmussen2003gaussian,
  title={Gaussian processes in machine learning},
  author={Rasmussen, Carl Edward},
  booktitle={Summer school on machine learning},
  pages={63--71},
  year={2003},
  organization={Springer}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}


@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:1806.07572},
  year={2018}
}



@article{gawlikowski2021survey,
  title={A survey of uncertainty in deep neural networks},
  author={Gawlikowski, Jakob and Tassi, Cedrique Rovile Njieutcheu and Ali, Mohsin and Lee, Jongseok and Humt, Matthias and Feng, Jianxiang and Kruspe, Anna and Triebel, Rudolph and Jung, Peter and Roscher, Ribana and others},
  journal={arXiv preprint arXiv:2107.03342},
  year={2021}
}

@book{neal1993probabilistic,
  title={Probabilistic inference using Markov chain Monte Carlo methods},
  author={Neal, Radford M},
  year={1993},
  publisher={Department of Computer Science, University of Toronto Toronto, ON, Canada}
}





@article{neal1996bayesian,
  title={Bayesian Learning for Neural Networks},
  author={Neal, RM},
  journal={Lecture Notes in Statistics},
  year={1996},
  publisher={Springer}
}

@inproceedings{lee2018deep,
  title={Deep Neural Networks as Gaussian Processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={8572--8583},
  year={2019}
}

@article{matthews2018gaussian,
  title={Gaussian process behaviour in wide deep neural networks},
  author={Matthews, Alexander G de G and Rowland, Mark and Hron, Jiri and Turner, Richard E and Ghahramani, Zoubin},
  journal={arXiv preprint arXiv:1804.11271},
  year={2018}
}

@article{novak2018bayesian,
  title={Bayesian deep convolutional networks with many channels are gaussian processes},
  author={Novak, Roman and Xiao, Lechao and Lee, Jaehoon and Bahri, Yasaman and Yang, Greg and Hron, Jiri and Abolafia, Daniel A and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1810.05148},
  year={2018}
}

@article{garriga2018deep,
  title={Deep convolutional networks as shallow gaussian processes},
  author={Garriga-Alonso, Adri{\`a} and Rasmussen, Carl Edward and Aitchison, Laurence},
  journal={arXiv preprint arXiv:1808.05587},
  year={2018}
}

@article{agrawal2020wide,
  title={Wide neural networks with bottlenecks are deep Gaussian processes},
  author={Agrawal, Devanshu and Papamarkou, Theodore and Hinkle, Jacob},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={175},
  year={2020},
  publisher={Oak Ridge National Lab.(ORNL), Oak Ridge, TN (United States)}
}


@article{khan2019approximate,
  title={Approximate Inference Turns Deep Networks into Gaussian Processes},
  author={Khan, Mohammad Emtiyaz E and Immer, Alexander and Abedi, Ehsan and Korzepa, Maciej},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@article{chizat2018lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal={arXiv preprint arXiv:1812.07956},
  year={2018}
}

@article{allen2018learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  journal={arXiv preprint arXiv:1811.04918},
  year={2018}
}


@inproceedings{laurent2018deep,
  title={Deep linear networks with arbitrary loss: All local minima are global},
  author={Laurent, Thomas and Brecht, James},
  booktitle={International conference on machine learning},
  pages={2902--2907},
  year={2018},
  organization={PMLR}
}

@article{hardt2016identity,
  title={Identity matters in deep learning},
  author={Hardt, Moritz and Ma, Tengyu},
  journal={arXiv preprint arXiv:1611.04231},
  year={2016}
}

@article{kawaguchi2016deep,
  title={Deep Learning without Poor Local Minima},
  author={Kawaguchi, Kenji},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  pages={586--594},
  year={2016}
}

@article{taghvaei2017regularization,
  title={How regularization affects the critical points in linear networks},
  author={Taghvaei, Amirhossein and Kim, Jin W and Mehta, Prashant},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{mehta2021loss,
  title={The loss surface of deep linear networks viewed through the algebraic geometry lens},
  author={Mehta, Dhagash and Chen, Tianran and Tang, Tingting and Hauenstein, Jonathan},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  publisher={IEEE}
}
