\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Baldi \& Hornik(1989)Baldi and Hornik]{baldi1989neural}
Baldi, P. and Hornik, K.
\newblock Neural networks and principal component analysis: Learning from
  examples without local minima.
\newblock \emph{Neural networks}, 2\penalty0 (1):\penalty0 53--58, 1989.

\bibitem[Bao et~al.(2020)Bao, Lucas, Sachdeva, and Grosse]{bao2020regularized}
Bao, X., Lucas, J., Sachdeva, S., and Grosse, R.~B.
\newblock Regularized linear autoencoders recover the principal components,
  eventually.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  6971--6981. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/4dd9cec1c21bc54eecb53786a2c5fa09-Paper.pdf}.

\bibitem[Biehl \& Schlösser(1998)Biehl and Schlösser]{biehl1998dynamics}
Biehl, M. and Schlösser, E.
\newblock The dynamics of on-line principal component analysis.
\newblock \emph{Journal of Physics A: Mathematical and General}, 31\penalty0
  (5):\penalty0 L97--L103, 1998.
\newblock \doi{10.1088/0305-4470/31/5/002}.
\newblock URL \url{https://doi.org/10.1088/0305-4470/31/5/002}.

\bibitem[Bishop(2006)]{bishop2006pattern}
Bishop, C.
\newblock \emph{{Pattern recognition and machine learning}}.
\newblock Springer, New York, 2006.

\bibitem[Bourlard \& Kamp(1988)Bourlard and Kamp]{bourlard1988auto}
Bourlard, H. and Kamp, Y.
\newblock Auto-association by multilayer perceptrons and singular value
  decomposition.
\newblock \emph{Biological cybernetics}, 59\penalty0 (4):\penalty0 291--294,
  1988.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2019lazy}
Chizat, L., Oyallon, E., and Bach, F.
\newblock On lazy training in differentiable programming.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2937--2947, 2019.

\bibitem[Eckart \& Young(1936)Eckart and Young]{eckart1936approximation}
Eckart, C. and Young, G.
\newblock The approximation of one matrix by another of lower rank.
\newblock \emph{Psychometrika}, 1\penalty0 (3):\penalty0 211--218, 1936.

\bibitem[Gidel et~al.(2019)Gidel, Bach, and Lacoste-Julien]{gidel2019implicit}
Gidel, G., Bach, F., and Lacoste-Julien, S.
\newblock Implicit regularization of discrete gradient dynamics in linear
  neural networks.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/f39ae9ff3a81f499230c4126e01f421b-Paper.pdf}.

\bibitem[Goldt et~al.(2019)Goldt, Advani, Saxe, Krzakala, and
  Zdeborov{\'a}]{goldt2019dynamics}
Goldt, S., Advani, M., Saxe, A., Krzakala, F., and Zdeborov{\'a}, L.
\newblock Dynamics of stochastic gradient descent for two-layer neural networks
  in the teacher-student setup.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, 2019.

\bibitem[Goldt et~al.(2020)Goldt, M{\'e}zard, Krzakala, and
  Zdeborov{\'a}]{goldt2020modelling}
Goldt, S., M{\'e}zard, M., Krzakala, F., and Zdeborov{\'a}, L.
\newblock Modeling the influence of data structure on learning in neural
  networks: The hidden manifold model.
\newblock \emph{Phys. Rev. X}, 10\penalty0 (4):\penalty0 041044, 2020.

\bibitem[Goldt et~al.(2022)Goldt, Loureiro, Reeves, Krzakala, Mezard, and
  Zdeborova]{goldt2021gaussian}
Goldt, S., Loureiro, B., Reeves, G., Krzakala, F., Mezard, M., and Zdeborova,
  L.
\newblock The gaussian equivalence of generative models for learning with
  shallow neural networks.
\newblock In Bruna, J., Hesthaven, J., and Zdeborova, L. (eds.),
  \emph{Proceedings of the 2nd Mathematical and Scientific Machine Learning
  Conference}, volume 145 of \emph{Proceedings of Machine Learning Research},
  pp.\  426--471. PMLR, 2022.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{Gunasekar2017}
Gunasekar, S., Woodworth, B., Bhojanapalli, S., Neyshabur, B., and Srebro, N.
\newblock {Implicit Regularization in Matrix Factorization}.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pp.\
  6151--6159, 2017.

\bibitem[Horn \& Johnson(2012)Horn and Johnson]{horn2012matrix}
Horn, R.~A. and Johnson, C.~R.
\newblock \emph{Matrix analysis}.
\newblock Cambridge university press, 2012.

\bibitem[Hu \& Lu(2020)Hu and Lu]{hu2020universality}
Hu, H. and Lu, Y.~M.
\newblock Universality laws for high-dimensional learning with random features.
\newblock \emph{arXiv:2009.07669}, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  8571--8580, 2018.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock arXiV preprint, 2009.
\newblock URL
  \url{https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}.

\bibitem[Kunin et~al.(2019)Kunin, Bloom, Goeva, and Seed]{kunin2019loss}
Kunin, D., Bloom, J., Goeva, A., and Seed, C.
\newblock Loss landscapes of regularized linear autoencoders.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pp.\  3560--3569. PMLR,
  2019.
\newblock URL \url{https://proceedings.mlr.press/v97/kunin19a.html}.

\bibitem[Liao \& Couillet(2018)Liao and Couillet]{liao2018spectrum}
Liao, Z. and Couillet, R.
\newblock On the spectrum of random features maps of high dimensional data.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3063--3071. PMLR, 2018.

\bibitem[Loureiro et~al.(2021)Loureiro, Gerbelot, Cui, Goldt, Krzakala,
  M{\'e}zard, and Zdeborov{\'a}]{loureiro2021capturing}
Loureiro, B., Gerbelot, C., Cui, H., Goldt, S., Krzakala, F., M{\'e}zard, M.,
  and Zdeborov{\'a}, L.
\newblock Learning curves of generic features maps for realistic datasets with
  a teacher-student model.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, 2021.

\bibitem[Marchenko \& Pastur(1967)Marchenko and
  Pastur]{marchenko1967distribution}
Marchenko, V. and Pastur, L.
\newblock Distribution of eigenvalues for some sets of random matrices.
\newblock \emph{Matematicheskii Sbornik}, 114\penalty0 (4):\penalty0 507--536,
  1967.

\bibitem[Mei \& Montanari(2021)Mei and Montanari]{mei2021generalization}
Mei, S. and Montanari, A.
\newblock The generalization error of random features regression: Precise
  asymptotics and the double descent curve.
\newblock \emph{Communications on Pure and Applied Mathematics}, 2021.
\newblock \doi{https://doi.org/10.1002/cpa.22008}.
\newblock URL \url{https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.22008}.

\bibitem[Mianjy et~al.(2018)Mianjy, Arora, and Vidal]{mianjy2018implicit}
Mianjy, P., Arora, R., and Vidal, R.
\newblock On the implicit bias of dropout.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  3540--3548. PMLR, 2018.
\newblock URL \url{https://proceedings.mlr.press/v80/mianjy18b.html}.

\bibitem[Nakkiran et~al.(2021)Nakkiran, Neyshabur, and
  Sedghi]{nakkiran2020bootstrap}
Nakkiran, P., Neyshabur, B., and Sedghi, H.
\newblock The bootstrap framework: Generalization through the lens of online
  optimization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=guetrIHLFGI}.

\bibitem[Nguyen(2021)]{nguyen2021analysis}
Nguyen, P.-M.
\newblock Analysis of feature learning in weight-tied autoencoders via the mean
  field lens.
\newblock \emph{arXiv preprint arXiv:2102.08373}, 2021.

\bibitem[Nguyen et~al.(2019)Nguyen, Wong, and Hegde]{nguyen2019dynamics}
Nguyen, T.~V., Wong, R.~K., and Hegde, C.
\newblock On the dynamics of gradient descent for autoencoders.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  2858--2867. PMLR, 2019.

\bibitem[Oftadeh et~al.(2020)Oftadeh, Shen, Wang, and
  Shell]{oftadeh2020eliminating}
Oftadeh, R., Shen, J., Wang, Z., and Shell, D.
\newblock Eliminating the invariance on the loss landscape of linear
  autoencoders.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  7405--7413. PMLR, 07
  2020.
\newblock URL \url{https://proceedings.mlr.press/v119/oftadeh20a.html}.

\bibitem[Oja(1982)]{oja1982simplified}
Oja, E.
\newblock Simplified neuron model as a principal component analyzer.
\newblock \emph{Journal of mathematical biology}, 15\penalty0 (3):\penalty0
  267--273, 1982.

\bibitem[Plaut(2018)]{plaut2018principal}
Plaut, E.
\newblock From principal subspaces to principal components with linear
  autoencoders.
\newblock \emph{arXiv preprint arXiv:1804.10253}, 2018.

\bibitem[Potters \& Bouchaud(2020)Potters and Bouchaud]{potters2020first}
Potters, M. and Bouchaud, J.-P.
\newblock \emph{A First Course in Random Matrix Theory: For Physicists,
  Engineers and Data Scientists}.
\newblock Cambridge University Press, 2020.

\bibitem[Pretorius et~al.(2018)Pretorius, Kroon, and
  Kamper]{pretorius2018learning}
Pretorius, A., Kroon, S., and Kamper, H.
\newblock Learning dynamics of linear denoising autoencoders.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4141--4150. PMLR, 2018.

\bibitem[Refinetti et~al.(2021)Refinetti, Goldt, Krzakala, and
  Zdeborova]{refinettiClassifying}
Refinetti, M., Goldt, S., Krzakala, F., and Zdeborova, L.
\newblock Classifying high-dimensional gaussian mixtures: Where kernel methods
  fail and neural networks succeed.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  8936--8947. PMLR,
  2021.
\newblock URL \url{https://proceedings.mlr.press/v139/refinetti21b.html}.

\bibitem[Riegler \& Biehl(1995)Riegler and Biehl]{Riegler1995}
Riegler, P. and Biehl, M.
\newblock {On-line backpropagation in two-layered neural networks}.
\newblock \emph{Journal of Physics A: Mathematical and General}, 28\penalty0
  (20), 1995.

\bibitem[Saad(2009)]{saad2009line}
Saad, D.
\newblock \emph{On-line learning in neural networks}, volume~17.
\newblock Cambridge University Press, 2009.

\bibitem[Saad \& Solla(1995{\natexlab{a}})Saad and Solla]{Saad1995a}
Saad, D. and Solla, S.
\newblock {Exact Solution for On-Line Learning in Multilayer Neural Networks}.
\newblock \emph{Phys. Rev. Lett.}, 74\penalty0 (21):\penalty0 4337--4340,
  1995{\natexlab{a}}.

\bibitem[Saad \& Solla(1995{\natexlab{b}})Saad and Solla]{Saad1995b}
Saad, D. and Solla, S.
\newblock {On-line learning in soft committee machines}.
\newblock \emph{Phys. Rev. E}, 52\penalty0 (4):\penalty0 4225--4243,
  1995{\natexlab{b}}.

\bibitem[Sanger(1989)]{sanger1989optimal}
Sanger, T.~D.
\newblock Optimal unsupervised learning in a single-layer linear feedforward
  neural network.
\newblock \emph{Neural networks}, 2\penalty0 (6):\penalty0 459--473, 1989.

\bibitem[Saxe et~al.(2019)Saxe, McClelland, and Ganguli]{saxe2019mathematical}
Saxe, A., McClelland, J., and Ganguli, S.
\newblock {A mathematical theory of semantic development in deep neural
  networks}.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (23):\penalty0 11537--11546, 2019.

\bibitem[Schlösser et~al.(1999)Schlösser, Saad, and
  Biehl]{schloesser1999optimization}
Schlösser, E., Saad, D., and Biehl, M.
\newblock Optimization of on-line principal component analysis.
\newblock \emph{Journal of Physics A: Mathematical and General}, 32\penalty0
  (22):\penalty0 4061--4067, 1999.
\newblock \doi{10.1088/0305-4470/32/22/306}.
\newblock URL \url{https://doi.org/10.1088/0305-4470/32/22/306}.

\bibitem[Seddik et~al.(2019)Seddik, Tamaazousti, and
  Couillet]{seddik2019kernel}
Seddik, M., Tamaazousti, M., and Couillet, R.
\newblock Kernel random matrices of large concentrated data: the example of
  gan-generated images.
\newblock In \emph{ICASSP 2019-2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pp.\  7480--7484. IEEE, 2019.

\bibitem[Veiga et~al.(2022)Veiga, Stephan, Loureiro, Krzakala, and
  Zdeborov{\'a}]{veiga2022phase}
Veiga, R., Stephan, L., Loureiro, B., Krzakala, F., and Zdeborov{\'a}, L.
\newblock Phase diagram of stochastic gradient descent in high-dimensional
  two-layer neural networks.
\newblock \emph{arXiv preprint arXiv:2202.00293}, 2022.

\bibitem[Wishart(1928)]{wishart1928generalised}
Wishart, J.
\newblock The generalised product moment distribution in samples from a normal
  multivariate population.
\newblock \emph{Biometrika}, 20\penalty0 (1/2):\penalty0 32--52, 1928.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017online}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock arXiV preprint, 2017.
\newblock URL \url{https://arxiv.org/pdf/1708.07747.pdf}.

\bibitem[Yoshida \& Okada(2019)Yoshida and Okada]{yoshida2019datadependence}
Yoshida, Y. and Okada, M.
\newblock Data-dependence of plateau phenomenon in learning with neural network
  --- statistical mechanical analysis.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  1720--1728, 2019.

\end{thebibliography}
