\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anil et~al.(2022{\natexlab{a}})Anil, Pokle, Liang, Treutlein, Wu, Bai, Kolter, and Grosse]{anil2022path}
Cem Anil, Ashwini Pokle, Kaiqu Liang, Johannes Treutlein, Yuhuai Wu, Shaojie Bai, J~Zico Kolter, and Roger~B Grosse.
\newblock Path independent equilibrium models can better exploit test-time computation.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 7796--7809, 2022{\natexlab{a}}.

\bibitem[Anil et~al.(2022{\natexlab{b}})Anil, Wu, Andreassen, Lewkowycz, Misra, Ramasesh, Slone, Gur-Ari, Dyer, and Neyshabur]{anil2022exploring}
Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur.
\newblock Exploring length generalization in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 38546--38556, 2022{\natexlab{b}}.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bansal et~al.(2022)Bansal, Schwarzschild, Borgnia, Emam, Huang, Goldblum, and Goldstein]{bansal2022endtoend}
Arpit Bansal, Avi Schwarzschild, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah Goldblum, and Tom Goldstein.
\newblock End-to-end algorithm synthesis with recurrent networks: Logical extrapolation without overthinking.
\newblock \emph{Advances in Neural Information Processing Systems}, 35, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Chi et~al.(2022)Chi, Fan, Ramadge, and Rudnicky]{chi2022kerple}
Ta-Chung Chi, Ting-Han Fan, Peter Ramadge, and Alexander Rudnicky.
\newblock Kerple: Kernelized relative positional embedding for length extrapolation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Chi et~al.(2023)Chi, Fan, Rudnicky, and Ramadge]{chi2023dissecting}
Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge.
\newblock Dissecting transformer length extrapolation via the lens of receptive field analysis.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 13522--13537, 2023.

\bibitem[de~Luca and Fountoulakis(2024)]{de2024simulation}
Artur~Back de~Luca and Kimon Fountoulakis.
\newblock Simulation of graph algorithms with looped transformers.
\newblock \emph{arXiv preprint arXiv:2402.01107}, 2024.

\bibitem[Dehghani et~al.(2018)Dehghani, Gouws, Vinyals, Uszkoreit, and Kaiser]{dehghani2018universal}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser.
\newblock Universal transformers.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Dziri et~al.(2023)Dziri, Lu, Sclar, Li, Jian, Lin, West, Bhagavatula, Bras, Hwang, et~al.]{dziri2023faith}
Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang~Lorraine Li, Liwei Jian, Bill~Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan~Le Bras, Jena~D Hwang, et~al.
\newblock Faith and fate: Limits of transformers on compositionality.
\newblock \emph{arXiv preprint arXiv:2305.18654}, 2023.

\bibitem[Geiping and Goldstein(2023)]{geiping2023cramming}
Jonas Geiping and Tom Goldstein.
\newblock Cramming: Training a language model on a single gpu in one day.
\newblock In \emph{International Conference on Machine Learning}, pages 11117--11143. PMLR, 2023.

\bibitem[Giannou et~al.(2023)Giannou, Rajput, Sohn, Lee, Lee, and Papailiopoulos]{giannou2023looped}
Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason~D Lee, and Dimitris Papailiopoulos.
\newblock Looped transformers as programmable computers.
\newblock In \emph{International Conference on Machine Learning}, pages 11398--11442. PMLR, 2023.

\bibitem[Golkar et~al.(2023)Golkar, Pettee, Eickenberg, Bietti, Cranmer, Krawezik, Lanusse, McCabe, Ohana, Parker, et~al.]{golkar2023xval}
Siavash Golkar, Mariel Pettee, Michael Eickenberg, Alberto Bietti, Miles Cranmer, Geraud Krawezik, Francois Lanusse, Michael McCabe, Ruben Ohana, Liam Parker, et~al.
\newblock xval: A continuous number encoding for large language models.
\newblock \emph{arXiv preprint arXiv:2310.02989}, 2023.

\bibitem[Ibarz et~al.(2022)Ibarz, Kurin, Papamakarios, Nikiforou, Bennani, Csord{\'a}s, Dudzik, Bo{\v{s}}njak, Vitvitskyi, Rubanova, et~al.]{ibarz2022generalist}
Borja Ibarz, Vitaly Kurin, George Papamakarios, Kyriacos Nikiforou, Mehdi Bennani, R{\'o}bert Csord{\'a}s, Andrew~Joseph Dudzik, Matko Bo{\v{s}}njak, Alex Vitvitskyi, Yulia Rubanova, et~al.
\newblock A generalist neural algorithmic learner.
\newblock In \emph{Learning on graphs conference}, pages 2--1. PMLR, 2022.

\bibitem[Jelassi et~al.(2023)Jelassi, d'Ascoli, Domingo-Enrich, Wu, Li, and Charton]{jelassi2023length}
Samy Jelassi, St{\'e}phane d'Ascoli, Carles Domingo-Enrich, Yuhuai Wu, Yuanzhi Li, and Fran{\c{c}}ois Charton.
\newblock Length generalization in arithmetic transformers.
\newblock \emph{arXiv preprint arXiv:2306.15400}, 2023.

\bibitem[Kazemnejad et~al.(2023)Kazemnejad, Padhi, Ramamurthy, Das, and Reddy]{kazemnejad2023impact}
Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan~Natesan Ramamurthy, Payel Das, and Siva Reddy.
\newblock The impact of positional encoding on length generalization in transformers.
\newblock \emph{arXiv preprint arXiv:2305.19466}, 2023.

\bibitem[Lan et~al.(2020)Lan, Chen, Goodman, Gimpel, Sharma, and Soricut]{Lan2020ALBERT}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.
\newblock Albert: A lite bert for self-supervised learning of language representations.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=H1eA7AEtvS}.

\bibitem[Lee et~al.(2023)Lee, Sreenivasan, Lee, Lee, and Papailiopoulos]{lee2023teaching}
Nayoung Lee, Kartik Sreenivasan, Jason~D Lee, Kangwook Lee, and Dimitris Papailiopoulos.
\newblock Teaching arithmetic to small transformers.
\newblock \emph{arXiv preprint arXiv:2307.03381}, 2023.

\bibitem[Li et~al.(2023)Li, You, Guruganesh, Ainslie, Ontanon, Zaheer, Sanghai, Yang, Kumar, and Bhojanapalli]{li2023functional}
Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli.
\newblock Functional interpolation for relative positions improves long context transformers.
\newblock \emph{arXiv preprint arXiv:2310.04418}, 2023.

\bibitem[Loeber(2024)]{loeber_16_2024}
John Loeber.
\newblock \#16: {{Notes}} on {{Arithmetic}} in {{GPT-4}}, February 2024.
\newblock URL \url{https://loeber.substack.com/p/16-notes-on-arithmetic-in-gpt-4}.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Ma et~al.(2022)Ma, Zhou, Kong, He, Gui, Neubig, May, and Zettlemoyer]{ma2022mega}
Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.
\newblock Mega: moving average equipped gated attention.
\newblock \emph{arXiv preprint arXiv:2209.10655}, 2022.

\bibitem[McLeish et~al.(2024)McLeish, Schwarzschild, and Goldstein]{mcleish2024benchmarking}
Sean McLeish, Avi Schwarzschild, and Tom Goldstein.
\newblock Benchmarking chatgpt on algorithmic reasoning.
\newblock \emph{arXiv preprint arXiv:2404.03441}, 2024.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan, Mann, Askell, Bai, Chen, et~al.]{olsson2022context}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et~al.
\newblock In-context learning and induction heads.
\newblock \emph{arXiv preprint arXiv:2209.11895}, 2022.

\bibitem[OpenAI(2023)]{openai2023gpt4tr}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{ArXiv}, abs/2303.08774, 2023.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:257532815}.

\bibitem[Peng et~al.(2024)Peng, Quesnelle, Fan, and Shippole]{peng2024yarn}
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
\newblock Yarn: Efficient context window extension of large language models.
\newblock \emph{International Conference on Learning Representations}, 2024.

\bibitem[Press et~al.(2022)Press, Smith, and Lewis]{press2022train}
Ofir Press, Noah Smith, and Mike Lewis.
\newblock Train short, test long: Attention with linear biases enables input length extrapolation.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=R8sQPpGCv0}.

\bibitem[Qian et~al.(2022)Qian, Wang, Li, Li, and Yan]{qian2022limitations}
Jing Qian, Hong Wang, Zekun Li, Shiyang Li, and Xifeng Yan.
\newblock Limitations of language models in arithmetic and symbolic induction.
\newblock \emph{arXiv preprint arXiv:2208.05051}, 2022.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of machine learning research}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Rodionov and Prokhorenkova(2024)]{rodionov2024discrete}
Gleb Rodionov and Liudmila Prokhorenkova.
\newblock Discrete neural algorithmic reasoning.
\newblock \emph{arXiv preprint arXiv:2402.11628}, 2024.

\bibitem[Ruoss et~al.(2023)Ruoss, Del{\'e}tang, Genewein, Grau-Moya, Csord{\'a}s, Bennani, Legg, and Veness]{ruoss2023randomized}
Anian Ruoss, Gr{\'e}goire Del{\'e}tang, Tim Genewein, Jordi Grau-Moya, R{\'o}bert Csord{\'a}s, Mehdi Bennani, Shane Legg, and Joel Veness.
\newblock Randomized positional encodings boost length generalization of transformers.
\newblock \emph{arXiv preprint arXiv:2305.16843}, 2023.

\bibitem[Saxton et~al.(2019)Saxton, Grefenstette, Hill, and Kohli]{saxton2019analysing}
David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli.
\newblock Analysing mathematical reasoning abilities of neural models.
\newblock \emph{arXiv preprint arXiv:1904.01557}, 2019.

\bibitem[Schwarzschild et~al.(2021)Schwarzschild, Borgnia, Gupta, Huang, Vishkin, Goldblum, and Goldstein]{schwarzschild2021can}
Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein.
\newblock Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Shaw et~al.(2018)Shaw, Uszkoreit, and Vaswani]{shaw2018self}
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
\newblock Self-attention with relative position representations.
\newblock \emph{arXiv preprint arXiv:1803.02155}, 2018.

\bibitem[Shazeer(2020)]{shazeer2020glu}
Noam Shazeer.
\newblock Glu variants improve transformer.
\newblock \emph{arXiv preprint arXiv:2002.05202}, 2020.

\bibitem[Shen et~al.(2023)Shen, Bubeck, Eldan, Lee, Li, and Zhang]{shen2023positional}
Ruoqi Shen, S{\'e}bastien Bubeck, Ronen Eldan, Yin~Tat Lee, Yuanzhi Li, and Yi~Zhang.
\newblock Positional description matters for transformers arithmetic.
\newblock \emph{arXiv preprint arXiv:2311.14737}, 2023.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{su2024roformer}
Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:\penalty0 127063, 2024.

\bibitem[Sukhbaatar et~al.(2019)Sukhbaatar, Grave, Bojanowski, and Joulin]{sukhbaatar-etal-2019-adaptive}
Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin.
\newblock Adaptive attention span in transformers.
\newblock In Anna Korhonen, David Traum, and Llu{\'\i}s M{\`a}rquez, editors, \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 331--335, Florence, Italy, July 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P19-1032}.
\newblock URL \url{https://aclanthology.org/P19-1032}.

\bibitem[Testolin(2024)]{alberto2024can}
Alberto Testolin.
\newblock Can neural networks do arithmetic? a survey on the elementary numerical skills of state-of-the-art deep learning models.
\newblock \emph{Applied Sciences}, 14\penalty0 (2), 2024.
\newblock ISSN 2076-3417.
\newblock \doi{10.3390/app14020744}.
\newblock URL \url{https://www.mdpi.com/2076-3417/14/2/744}.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Veli{\v{c}}kovi{\'c} et~al.(2022)Veli{\v{c}}kovi{\'c}, Badia, Budden, Pascanu, Banino, Dashevskiy, Hadsell, and Blundell]{velivckovic2022clrs}
Petar Veli{\v{c}}kovi{\'c}, Adri{\`a}~Puigdom{\`e}nech Badia, David Budden, Razvan Pascanu, Andrea Banino, Misha Dashevskiy, Raia Hadsell, and Charles Blundell.
\newblock The clrs algorithmic reasoning benchmark.
\newblock In \emph{International Conference on Machine Learning}, pages 22084--22102. PMLR, 2022.

\bibitem[Wang et~al.(2022)Wang, Ma, Dong, Huang, Zhang, and Wei]{wang_deepnet_2022}
Hongyu Wang, Shuming Ma, Li~Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.
\newblock {{DeepNet}}: {{Scaling Transformers}} to 1,000 {{Layers}}.
\newblock \emph{arXiv:2203.00555 [cs]}, March 2022.
\newblock URL \url{http://arxiv.org/abs/2203.00555}.

\bibitem[Yang et~al.(2023{\natexlab{a}})Yang, Lee, Nowak, and Papailiopoulos]{yang2023looped}
Liu Yang, Kangwook Lee, Robert Nowak, and Dimitris Papailiopoulos.
\newblock Looped transformers are better at learning learning algorithms.
\newblock \emph{arXiv preprint arXiv:2311.12424}, 2023{\natexlab{a}}.

\bibitem[Yang et~al.(2023{\natexlab{b}})Yang, Ding, Lv, Jiang, He, Guo, Bai, and Tang]{yang2023gpt}
Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai, and Jie Tang.
\newblock Gpt can solve mathematical problems without a calculator.
\newblock \emph{arXiv preprint arXiv:2309.03241}, 2023{\natexlab{b}}.

\bibitem[Zhai et~al.(2022)Zhai, Kolesnikov, Houlsby, and Beyer]{zhai2022scaling}
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
\newblock Scaling vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 12104--12113, 2022.

\bibitem[Zhou et~al.(2023)Zhou, Bradley, Littwin, Razin, Saremi, Susskind, Bengio, and Nakkiran]{zhou2023algorithms}
Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, and Preetum Nakkiran.
\newblock What algorithms can transformers learn? a study in length generalization.
\newblock \emph{arXiv preprint arXiv:2310.16028}, 2023.

\bibitem[Zhou et~al.(2024)Zhou, Alon, Chen, Wang, Agarwal, and Zhou]{zhou2024transformers}
Yongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal, and Denny Zhou.
\newblock Transformers can achieve length generalization but not robustly.
\newblock \emph{arXiv preprint arXiv:2402.09371}, 2024.

\end{thebibliography}
