\begin{thebibliography}{}

\bibitem[Ahn et~al., 2012]{ahn2012bayesian}
Ahn, S., Korattikara, A., and Welling, M. (2012).
\newblock {B}ayesian posterior sampling via stochastic gradient {F}isher
  scoring.
\newblock {\em arXiv preprint arXiv:1206.6380}.

\bibitem[Ba and Frey, 2013]{ba2013adaptive}
Ba, J. and Frey, B. (2013).
\newblock Adaptive dropout for training deep neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3084--3092.

\bibitem[Bayer et~al., 2015]{bayer2015fast}
Bayer, J., Karol, M., Korhammer, D., and Van~der Smagt, P. (2015).
\newblock Fast adaptive weight noise.
\newblock {\em arXiv preprint arXiv:1507.05331}.

\bibitem[Bengio, 2013]{bengio2013estimating}
Bengio, Y. (2013).
\newblock Estimating or propagating gradients through stochastic neurons.
\newblock {\em arXiv preprint arXiv:1305.2982}.

\bibitem[Bergstra et~al., 2010]{bergstra2010theano}
Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins,
  G., Turian, J., Warde-Farley, D., and Bengio, Y. (2010).
\newblock Theano: a {CPU} and {GPU} math expression compiler.
\newblock In {\em Proceedings of the Python for Scientific Computing Conference
  (SciPy)}, volume~4.

\bibitem[Blundell et~al., 2015]{blundell2015weight}
Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. (2015).
\newblock Weight uncertainty in neural networks.
\newblock {\em arXiv preprint arXiv:1505.05424}.

\bibitem[Gal and Ghahramani, 2015]{gal2015dropout}
Gal, Y. and Ghahramani, Z. (2015).
\newblock Dropout as a {B}ayesian approximation: Representing model uncertainty
  in deep learning.
\newblock {\em arXiv preprint arXiv:1506.02142}.

\bibitem[Graves, 2011]{graves2011practical}
Graves, A. (2011).
\newblock Practical variational inference for neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2348--2356.

\bibitem[Hern{\'a}ndez-Lobato and Adams, 2015]{hernandez2015probabilistic}
Hern{\'a}ndez-Lobato, J.~M. and Adams, R.~P. (2015).
\newblock Probabilistic backpropagation for scalable learning of {B}ayesian
  neural networks.
\newblock {\em arXiv preprint arXiv:1502.05336}.

\bibitem[Hinton et~al., 2012]{hinton2012improving}
Hinton, G.~E., Srivastava, N., Krizhevsky, A., Sutskever, I., and
  Salakhutdinov, R.~R. (2012).
\newblock Improving neural networks by preventing co-adaptation of feature
  detectors.
\newblock {\em arXiv preprint arXiv:1207.0580}.

\bibitem[Hinton and Van~Camp, 1993]{hinton1993keeping}
Hinton, G.~E. and Van~Camp, D. (1993).
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In {\em Proceedings of the sixth annual conference on Computational
  learning theory}, pages 5--13. ACM.

\bibitem[Kingma and Ba, 2015]{kingma2015adam}
Kingma, D. and Ba, J. (2015).
\newblock Adam: A method for stochastic optimization.
\newblock {\em Proceedings of the International Conference on Learning
  Representations 2015}.

\bibitem[Kingma, 2013]{kingma2013fast}
Kingma, D.~P. (2013).
\newblock Fast gradient-based inference with continuous latent variable models
  in auxiliary form.
\newblock {\em arXiv preprint arXiv:1306.0733}.

\bibitem[Kingma and Welling, 2014]{kingma2013auto}
Kingma, D.~P. and Welling, M. (2014).
\newblock Auto-encoding variational {B}ayes.
\newblock {\em Proceedings of the 2nd International Conference on Learning
  Representations}.

\bibitem[Maeda, 2014]{maeda2014bayesian}
Maeda, S.-i. (2014).
\newblock A {B}ayesian encourages dropout.
\newblock {\em arXiv preprint arXiv:1412.7003}.

\bibitem[Neal, 1995]{neal1995bayesian}
Neal, R.~M. (1995).
\newblock {\em {B}ayesian learning for neural networks}.
\newblock PhD thesis, University of Toronto.

\bibitem[Rezende et~al., 2014]{rezende2014stochastic}
Rezende, D.~J., Mohamed, S., and Wierstra, D. (2014).
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In {\em Proceedings of the 31st International Conference on Machine
  Learning (ICML-14)}, pages 1278--1286.

\bibitem[Robbins and Monro, 1951]{robbinsmonro}
Robbins, H. and Monro, S. (1951).
\newblock A stochastic approximation method.
\newblock {\em The Annals of Mathematical Statistics}, 22(3):400--407.

\bibitem[Salimans and Knowles, 2013]{salimans2013fixedform}
Salimans, T. and Knowles, D.~A. (2013).
\newblock Fixed-form variational posterior approximation through stochastic
  linear regression.
\newblock {\em Bayesian Analysis}, 8(4).

\bibitem[Srivastava et~al., 2014]{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R. (2014).
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock {\em The Journal of Machine Learning Research}, 15(1):1929--1958.

\bibitem[Wan et~al., 2013]{wan2013regularization}
Wan, L., Zeiler, M., Zhang, S., Cun, Y.~L., and Fergus, R. (2013).
\newblock Regularization of neural networks using dropconnect.
\newblock In {\em Proceedings of the 30th International Conference on Machine
  Learning (ICML-13)}, pages 1058--1066.

\bibitem[Wang and Manning, 2013]{wang2013fast}
Wang, S. and Manning, C. (2013).
\newblock Fast dropout training.
\newblock In {\em Proceedings of the 30th International Conference on Machine
  Learning (ICML-13)}, pages 118--126.

\bibitem[Welling and Teh, 2011]{welling2011bayesian}
Welling, M. and Teh, Y.~W. (2011).
\newblock {B}ayesian learning via stochastic gradient {L}angevin dynamics.
\newblock In {\em Proceedings of the 28th International Conference on Machine
  Learning (ICML-11)}, pages 681--688.

\end{thebibliography}
