@article{Aghasi2017Net-Trim,
	title={{Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee}},
	author={Alireza Aghasi and Afshin Abdi and Nam Nguyen and Justin Romberg},
	journal={Advances in Neural Information Processing Systems},
	volume={30},
	year={2017}
}

@misc{dosovitskiy2021image,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{geiping2022cramming,
      title={Cramming: Training a Language Model on a Single GPU in One Day}, 
      author={Jonas Geiping and Tom Goldstein},
      year={2022},
      eprint={2212.14034},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{blumensath2009iterative,
  title={Iterative hard thresholding for compressed sensing},
  author={Blumensath, Thomas and Davies, Mike E.},
  journal={Applied and Computational Harmonic Analysis},
  volume={27},
  number={3},
  pages={265--274},
  year={2009},
  publisher={Elsevier}
}

@article{tropp2007signal,
  title={Signal recovery from random measurements via orthogonal matching pursuit},
  author={Tropp, Joel A. and Gilbert, Anna C.},
  journal={IEEE Transactions on Information Theory},
  volume={53},
  number={12},
  pages={4655--4666},
  year={2007},
  publisher={IEEE}
}


@article{Harutyunyan2023KD,
  title={{Supervision Complexity and its Role in Knowledge Distillation}},
  author={Hrayr Harutyunyan and Ankit Singh Rawat and Aditya Krishna Menon and Seungyeon Kim and Sanjiv Kumar},
  journal={International Conference on Learning Representations (ICLR)},
  year={2023}
}

@article{Guo2020OnlineKD,
  title={{Online knowledge distillation via collaborative learning}},
  author={Qiushan Guo and Xinjiang Wang and Yichao Wu and Zhipeng Yu and Ding Liang and Xiaolin Hu and Ping Luo},
  journal={In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition},
  pages={11020â€“11029},
  year={2020}
}


@article{Kim2023SqueezeLLM,
  title={{SqueezeLLM: Dense-and-Sparse Quantization}},
  author={Sehoon Kim and Coleman Hooper and Amir Gholami and Zhen Dong and Xiuyu Li and Sheng Shen and Michael W. Mahoney and Kurt Keutzer},
  journal={arXiv preprint arXiv:2306.07629},
  year={2023}
}

@article{Dettmers2022LLMint8,
  title={{LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale}},
  author={Tim Dettmers and Mike Lewis and Younes Belkada and Luke Zettlemoyer},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}

@article{Yao2022ZeroQuant,
  title={{ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers}},
  author={Zhewei Yao and Reza Yazdani Aminabadi and Minjia Zhang and Xiaoxia Wu and Conglong Li and Yuxiong He},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}


@article{Mishra2018Apprentice,
  title={{Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy}},
  author={Asit Mishra and Debbie Marr},
  journal={International Conference on Learning Representations (ICLR)},
  year={2018}
}


@article{Hinton2014KD,
  title={{Distilling the Knowledge in a Neural Network}},
  author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
  journal={Deep Learning Workshop at NIPS},
  year={2014}
}


@article{Xu2023TensorGPT,
  title={{TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition}},
  author={Mingxue Xu and Yao Lei Xu and Danilo P. Mandic},
  journal={arXiv preprint arXiv:2307.00526},
  year={2023}
}

@article{Frantar2023OPTQ,
  title={{OPTQ: Accurate Quantization for Generative Pre-trained Transformers}},
  author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
  journal={International Conference on Learning Representations (ICLR)},
  year={2023}
}

@article{Frantar2023SparseGPT,
  title={{SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot}},
  author={Elias Frantar and Dan Alistarh},
  journal={International Conference on Machine Learning (ICML)},
  year={2023}
}

@article{Nikdan2024RoSA,
  title={{RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation}},
  author={Mahdi Nikdan and Soroush Tabesh and Dan Alistarh},
  journal={arXiv preprint arXiv:2401.04679},
  year={2024}
}

@article{Lialin2024ReLoRA,
  title={{ReLoRA: High-Rank Training Through Low-Rank Updates}},
  author={Vladislav Lialin and Sherin Muckatira and Namrata Shivagunde and Anna Rumshisky},
  journal={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@article{Hu2022LoRA,
  title={{LoRA: Low-Rank Adaptation of Large Language Models}},
  author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  journal={International Conference on Learning Representations (ICLR)},
  year={2022}
}

@article{Polino2018KDQ,
  title={{Model compression via distillation and quantization}},
  author={Antonio Polino and Razvan Pascanu and Dan Alistarh},
  journal={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@article{Lin2020DPF,
  title={{Dynamic Model Pruning with Feedback}},
  author={Tao Lin and Sebastian U. Stich and Luis Barba and Daniil Dmitriev and Martin Jaggi},
  journal={International Conference on Learning Representations (ICLR)},
  year={2020}
}


@article{Wang2022Adam,
  title={{Provable adaptivity in Adam}},
  author={Bohan Wang and Yushun Zhang and Huishuai Zhang and Qi Meng and Zhi-Ming Ma and Tie-Yan Liu and Wei Chen},
  journal={arXiv preprint arXiv:2208.09900},
  year={2022}
}


@article{Zhang2020clipping,
author       = {Jingzhao Zhang and
                  Tianxing He and
                  Suvrit Sra and
                  Ali Jadbabaie},
  title        = {Why Gradient Clipping Accelerates Training: {A} Theoretical Justification
                  for Adaptivity},
  journal    = {8th International Conference on Learning Representations, {ICLR} 2020,
                  Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher    = {OpenReview.net},
  year         = {2020},
  url          = {https://openreview.net/forum?id=BJgnXpVYwS},
  timestamp    = {Thu, 07 May 2020 17:11:48 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/ZhangHSJ20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{hassibi1992second,
  title={Second order derivatives for network pruning: Optimal brain surgeon},
  author={Hassibi, Babak and Stork, David},
  journal={Advances in neural information processing systems},
  volume={5},
  year={1992}
}

@article{hassibi1993optimal,
  title={Optimal brain surgeon: Extensions and performance comparisons},
  author={Hassibi, Babak and Stork, David and Wolff, Gregory},
  journal={Advances in neural information processing systems},
  volume={6},
  year={1993}
}


@article{Foster2015variable,
author = {Foster, D. and Karloff, H. and Thaler, J.},
title = {Variable selection is hard},
journal = {Conference on Learning Theory},
pages = {696--709},
year = {2015}
}

@article{Chen2014complexity,
author = {Chen, Xiaojun and Ge, Dongdong and Wang, Zizhuo and Ye, Yinyu},
title = {Complexity of unconstrained {$L_2-L_p$} minimization},
journal = {Mathematical Programming},
volume = {143},
number = {1},
pages = {371--383},
year = {2014}
}


@article{Natarajan1995sparse,
author = {Natarajan, B. K.},
title = {Sparse Approximate Solutions to Linear Systems},
journal = {SIAM Journal on Computing},
volume = {24},
number = {2},
pages = {227-234},
year = {1995}
}

@article{peste2021ac,
	title={{AC/DC}: Alternating compressed/decompressed training of deep neural networks},
	author={Peste, Alexandra and Iofinova, Eugenia and Vladu, Adrian and Alistarh, Dan},
	journal={Advances in Neural Information Processing Systems},
	volume={34},
	pages={8557--8570},
	year={2021}
}

@article{garrigos2023handbook,
	title={Handbook of convergence theorems for (stochastic) gradient methods},
	author={Garrigos, Guillaume and Gower, Robert M},
	journal={arXiv preprint arXiv:2301.11235},
	year={2023}
}

@article{singh2020woodfisher,
	title={{WoodFisher: Efficient second-order approximation for neural network compression}},
	author={Singh, Sidak Pal and Alistarh, Dan},
	journal={Advances in Neural Information Processing Systems},
	volume={33},
	pages={18098--18109},
	year={2020}
}

@book{nesterov2018lectures,
	title={Lectures on convex optimization},
	author={Nesterov, Yurii},
	volume={137},
	year={2018},
	publisher={Springer}
}


@inproceedings{
	frantar2022optimal,
	title={Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning},
	author={Elias Frantar and Dan Alistarh},
	booktitle={Advances in Neural Information Processing Systems},
	editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
	year={2022},
	url={https://openreview.net/forum?id=ksVGCOlOEba}
}

@inproceedings{safaryan2022fednl,
	title={FedNL: Making Newton-Type Methods Applicable to Federated Learning},
	author={Safaryan, Mher and Islamov, Rustem and Qian, Xun and Richtarik, Peter},
	booktitle={International Conference on Machine Learning},
	pages={18959--19010},
	year={2022},
	organization={PMLR}
}


@book{horn2012matrix,
  title={Matrix analysis},
  author={Horn, Roger A and Johnson, Charles R},
  year={2012},
  publisher={Cambridge university press}
}


@inproceedings{lecun90brain,
 author = {LeCun, Yann and Denker, John and Solla, Sara},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Optimal Brain Damage},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf},
 volume = {2},
 year = {1989}
}

@article{hoefler2021sparsity,
  title={Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks},
  author={Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={10882--11005},
  year={2021},
  publisher={JMLRORG}
}

@article{frantar2021m,
  title={M-FAC: Efficient matrix-free approximations of second-order information},
  author={Frantar, Elias and Kurtic, Eldar and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={14873--14886},
  year={2021}
}


@book{zhao2018sparse,
  title={Sparse optimization theory and methods},
  author={Zhao, Yun-Bin},
  year={2018},
  publisher={CRC Press}
}

@article{donoho2006compressed,
  title={Compressed sensing},
  author={Donoho, David L},
  journal={IEEE Transactions on information theory},
  volume={52},
  number={4},
  pages={1289--1306},
  year={2006},
  publisher={IEEE}
}



@article{blumensath2012accelerated,
  title={Accelerated iterative hard thresholding},
  author={Blumensath, Thomas},
  journal={Signal Processing},
  volume={92},
  number={3},
  pages={752--756},
  year={2012},
  publisher={Elsevier}
}

@article{meng2022newton,
  title={Newton-type optimal thresholding algorithms for sparse optimization problems},
  author={Meng, Nan and Zhao, Yun-Bin},
  journal={Journal of the Operations Research Society of China},
  volume={10},
  number={3},
  pages={447--469},
  year={2022},
  publisher={Springer}
}

@article{zhou2021global,
  title={Global and quadratic convergence of Newton hard-thresholding pursuit},
  author={Zhou, Shenglong and Xiu, Naihua and Qi, Hou-Duo},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={599--643},
  year={2021},
  publisher={JMLRORG}
}

@inproceedings{chen2017fast,
  title={Fast newton hard thresholding pursuit for sparsity constrained nonconvex optimization},
  author={Chen, Jinghui and Gu, Quanquan},
  booktitle={Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={757--766},
  year={2017}
}

@inproceedings{kyrillidis2011recipes,
  title={Recipes on hard thresholding methods},
  author={Kyrillidis, Anastasios and Cevher, Volkan},
  booktitle={2011 4th IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)},
  pages={353--356},
  year={2011},
  organization={IEEE}
}

@article{bahmani2013greedy,
  title={Greedy sparsity-constrained optimization},
  author={Bahmani, Sohail and Raj, Bhiksha and Boufounos, Petros T},
  journal={The Journal of Machine Learning Research},
  volume={14},
  number={1},
  pages={807--841},
  year={2013},
  publisher={JMLR. org}
}


@article{meng2020newton,
  title={Newton-step-based hard thresholding algorithms for sparse signal recovery},
  author={Meng, Nan and Zhao, Yun-Bin},
  journal={IEEE Transactions on Signal Processing},
  volume={68},
  pages={6594--6606},
  year={2020},
  publisher={IEEE}
}

@article{mocanu2018scalable,
title = "Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science",
keywords = "Complex networks, Evolutionary algorithms, Deep learning, Sparse artificial neural networks, Restricted Boltzmann machine, Sparse training",
author = "Mocanu, {Decebal Constantin} and Elena Mocanu and Peter Stone and Nguyen, {Phuong H.} and Madeleine Gibescu and Antonio Liotta",
year = "2018",
month = jun,
day = "19",
doi = "10.1038/s41467-018-04316-3",
language = "English",
volume = "9",
journal = "Nature communications",
issn = "2041-1723",
publisher = "Nature Publishing Group",
number = "1",
}

@inproceedings{evci2020rigging,
  title={Rigging the lottery: Making all tickets winners},
  author={Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  booktitle={International Conference on Machine Learning},
  pages={2943--2952},
  year={2020},
  organization={PMLR}
}

@article{amari1998natural,
  title={Natural gradient works efficiently in learning},
  author={Amari, Shun-Ichi},
  journal={Neural computation},
  volume={10},
  number={2},
  pages={251--276},
  year={1998},
  publisher={MIT Press}
}

@article{mishchenko2021regularized,
  title={Regularized {Newton} Method with Global {$O(1/k^2)$} Convergence},
  author={Mishchenko, Konstantin},
  journal={SIAM Journal on Optimization},
  volume={33},
  number={3},
  pages={1440-1462},
  year={2023},
  doi={10.1137/22M1488752},
}

@article{dong2017learning,
  title={Learning to prune deep neural networks via layer-wise optimal brain surgeon},
  author={Dong, Xin and Chen, Shangyu and Pan, Sinno},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{yu2022combinatorial,
  title={The combinatorial brain surgeon: pruning weights that cancel one another in neural networks},
  author={Yu, Xin and Serra, Thiago and Ramalingam, Srikumar and Zhe, Shandian},
  booktitle={International Conference on Machine Learning},
  pages={25668--25683},
  year={2022},
  organization={PMLR}
}

@inproceedings{benbaki2023fast,
  title={Fast as chita: Neural network pruning with combinatorial optimization},
  author={Benbaki, Riade and Chen, Wenyu and Meng, Xiang and Hazimeh, Hussein and Ponomareva, Natalia and Zhao, Zhe and Mazumder, Rahul},
  booktitle={International Conference on Machine Learning},
  pages={2031--2049},
  year={2023},
  organization={PMLR}
}