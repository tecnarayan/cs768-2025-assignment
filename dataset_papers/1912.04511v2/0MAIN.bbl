\begin{thebibliography}{50}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

\bibitem[{Allen-Zhu et~al.(2019{\natexlab{a}})Allen-Zhu, Li and
  Liang}]{allenzhu2019learning}
\textsc{Allen-Zhu, Z.}, \textsc{Li, Y.} and \textsc{Liang, Y.}
  (2019{\natexlab{a}}).
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In \textit{Advances in neural information processing systems}.

\bibitem[{Allen-Zhu et~al.(2019{\natexlab{b}})Allen-Zhu, Li and
  Song}]{allen2019convergence}
\textsc{Allen-Zhu, Z.}, \textsc{Li, Y.} and \textsc{Song, Z.}
  (2019{\natexlab{b}}).
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Arora et~al.(2019)Arora, Du, Hu, Li and Wang}]{arora2019fine}
\textsc{Arora, S.}, \textsc{Du, S.}, \textsc{Hu, W.}, \textsc{Li, Z.} and
  \textsc{Wang, R.} (2019).
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Baird(1995)}]{baird1995residual}
\textsc{Baird, L.} (1995).
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In \textit{Machine Learning Proceedings 1995}. Elsevier, 30--37.

\bibitem[{Bertsekas et~al.(1995)Bertsekas, Bertsekas, Bertsekas and
  Bertsekas}]{bertsekas1995dynamic}
\textsc{Bertsekas, D.~P.}, \textsc{Bertsekas, D.~P.}, \textsc{Bertsekas, D.~P.}
  and \textsc{Bertsekas, D.~P.} (1995).
\newblock \textit{Dynamic programming and optimal control}, vol.~1.
\newblock Athena scientific Belmont, MA.

\bibitem[{Bhandari et~al.(2018)Bhandari, Russo and Singal}]{bhandari2018finite}
\textsc{Bhandari, J.}, \textsc{Russo, D.} and \textsc{Singal, R.} (2018).
\newblock A finite time analysis of temporal difference learning with linear
  function approximation.
\newblock In \textit{Conference On Learning Theory}.

\bibitem[{Borkar and Meyn(2000)}]{borkar2000ode}
\textsc{Borkar, V.~S.} and \textsc{Meyn, S.~P.} (2000).
\newblock The ode method for convergence of stochastic approximation and
  reinforcement learning.
\newblock \textit{SIAM Journal on Control and Optimization} \textbf{38}
  447--469.

\bibitem[{Cai et~al.(2019{\natexlab{a}})Cai, Yang, Lee and
  Wang}]{cai2019neural}
\textsc{Cai, Q.}, \textsc{Yang, Z.}, \textsc{Lee, J.~D.} and \textsc{Wang, Z.}
  (2019{\natexlab{a}}).
\newblock Neural temporal-difference learning converges to global optima.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Cai et~al.(2019{\natexlab{b}})Cai, Gao, Hou, Chen, Wang, He, Zhang
  and Wang}]{cai2019gram}
\textsc{Cai, T.}, \textsc{Gao, R.}, \textsc{Hou, J.}, \textsc{Chen, S.},
  \textsc{Wang, D.}, \textsc{He, D.}, \textsc{Zhang, Z.} and \textsc{Wang, L.}
  (2019{\natexlab{b}}).
\newblock A gram-gauss-newton method learning overparameterized deep neural
  networks for regression problems.
\newblock \textit{arXiv preprint arXiv:1905.11675} .

\bibitem[{Cao and Gu(2019{\natexlab{a}})}]{cao2019generalization2}
\textsc{Cao, Y.} and \textsc{Gu, Q.} (2019{\natexlab{a}}).
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Cao and Gu(2019{\natexlab{b}})}]{cao2019generalization}
\textsc{Cao, Y.} and \textsc{Gu, Q.} (2019{\natexlab{b}}).
\newblock A generalization theory of gradient descent for learning
  over-parameterized deep relu networks.
\newblock \textit{arXiv preprint arXiv:1902.01384} .

\bibitem[{Chen et~al.(2019)Chen, Zhang, Doan, Maguluri and
  Clarke}]{chen2019Performance}
\textsc{Chen, Z.}, \textsc{Zhang, S.}, \textsc{Doan, T.~T.}, \textsc{Maguluri,
  S.~T.} and \textsc{Clarke, J.-P.} (2019).
\newblock Performance of q-learning with linear function approximation:
  Stability and finite-time analysis.
\newblock \textit{arXiv preprint arXiv:1905.11425} .

\bibitem[{Chizat and Bach(2018)}]{chizat2018global}
\textsc{Chizat, L.} and \textsc{Bach, F.} (2018).
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock In \textit{Advances in neural information processing systems}.

\bibitem[{Dalal et~al.(2018)Dalal, Sz{\"o}r{\'e}nyi, Thoppe and
  Mannor}]{dalal2018finite}
\textsc{Dalal, G.}, \textsc{Sz{\"o}r{\'e}nyi, B.}, \textsc{Thoppe, G.} and
  \textsc{Mannor, S.} (2018).
\newblock Finite sample analyses for td (0) with function approximation.
\newblock In \textit{Thirty-Second AAAI Conference on Artificial Intelligence}.

\bibitem[{Devraj and Meyn(2017)}]{devraj2017zap}
\textsc{Devraj, A.~M.} and \textsc{Meyn, S.} (2017).
\newblock Zap q-learning.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang and
  Zhai}]{du2019gradient_icml}
\textsc{Du, S.}, \textsc{Lee, J.}, \textsc{Li, H.}, \textsc{Wang, L.} and
  \textsc{Zhai, X.} (2019{\natexlab{a}}).
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Du et~al.(2019{\natexlab{b}})Du, Zhai, Poczos and
  Singh}]{du2019gradient_iclr}
\textsc{Du, S.~S.}, \textsc{Zhai, X.}, \textsc{Poczos, B.} and \textsc{Singh,
  A.} (2019{\natexlab{b}}).
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \textit{International Conference on Learning Representations}.
\newline\urlprefix\url{https://openreview.net/forum?id=S1eK3i09YQ}

\bibitem[{Hu and Syed(2019)}]{hu2019characterizing}
\textsc{Hu, B.} and \textsc{Syed, U.~A.} (2019).
\newblock Characterizing the exact behaviors of temporal difference learning
  algorithms using markov jump linear system theory.
\newblock \textit{arXiv preprint arXiv:1906.06781} .

\bibitem[{Jaakkola et~al.(1994)Jaakkola, Jordan and
  Singh}]{jaakkola1994convergence}
\textsc{Jaakkola, T.}, \textsc{Jordan, M.~I.} and \textsc{Singh, S.~P.} (1994).
\newblock Convergence of stochastic iterative dynamic programming algorithms.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Jacot et~al.(2018)Jacot, Gabriel and Hongler}]{jacot2018neural}
\textsc{Jacot, A.}, \textsc{Gabriel, F.} and \textsc{Hongler, C.} (2018).
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \textit{Advances in neural information processing systems}.

\bibitem[{Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke
  et~al.}]{kalashnikov2018scalable}
\textsc{Kalashnikov, D.}, \textsc{Irpan, A.}, \textsc{Pastor, P.},
  \textsc{Ibarz, J.}, \textsc{Herzog, A.}, \textsc{Jang, E.}, \textsc{Quillen,
  D.}, \textsc{Holly, E.}, \textsc{Kalakrishnan, M.}, \textsc{Vanhoucke, V.}
  \textsc{et~al.} (2018).
\newblock Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock In \textit{Conference on Robot Learning}.

\bibitem[{Konda and Tsitsiklis(2000)}]{konda2000actor}
\textsc{Konda, V.~R.} and \textsc{Tsitsiklis, J.~N.} (2000).
\newblock Actor-critic algorithms.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Lakshminarayanan and Szepesvari(2018)}]{lakshminarayanan2018linear}
\textsc{Lakshminarayanan, C.} and \textsc{Szepesvari, C.} (2018).
\newblock Linear stochastic approximation: How far does constant step-size and
  iterate averaging go?
\newblock In \textit{International Conference on Artificial Intelligence and
  Statistics}.

\bibitem[{Levin and Peres(2017)}]{levin2017markov}
\textsc{Levin, D.~A.} and \textsc{Peres, Y.} (2017).
\newblock \textit{Markov chains and mixing times}, vol. 107.
\newblock American Mathematical Soc.

\bibitem[{Levine et~al.(2015)Levine, Wagener and Abbeel}]{levine2015learning}
\textsc{Levine, S.}, \textsc{Wagener, N.} and \textsc{Abbeel, P.} (2015).
\newblock Learning contact-rich manipulation skills with guided policy search.
\newblock In \textit{2015 IEEE International Conference on Robotics and
  Automation (ICRA)}. IEEE.

\bibitem[{Liu et~al.(2015)Liu, Liu, Ghavamzadeh, Mahadevan and
  Petrik}]{liu2015finite}
\textsc{Liu, B.}, \textsc{Liu, J.}, \textsc{Ghavamzadeh, M.},
  \textsc{Mahadevan, S.} and \textsc{Petrik, M.} (2015).
\newblock Finite-sample analysis of proximal gradient td algorithms.
\newblock In \textit{Proceedings of the Thirty-First Conference on Uncertainty
  in Artificial Intelligence}. AUAI Press.

\bibitem[{Mehta and Meyn(2009)}]{mehta2009q}
\textsc{Mehta, P.} and \textsc{Meyn, S.} (2009).
\newblock Q-learning and pontryagin's minimum principle.
\newblock In \textit{Proceedings of the 48h IEEE Conference on Decision and
  Control (CDC) held jointly with 2009 28th Chinese Control Conference}. IEEE.

\bibitem[{Melo et~al.(2008)Melo, Meyn and Ribeiro}]{melo2008analysis}
\textsc{Melo, F.~S.}, \textsc{Meyn, S.~P.} and \textsc{Ribeiro, M.~I.} (2008).
\newblock An analysis of reinforcement learning with function approximation.
\newblock In \textit{Proceedings of the 25th International Conference on
  Machine Learning}. ACM.

\bibitem[{Meyn and Tweedie(2012)}]{meyn2012markov}
\textsc{Meyn, S.~P.} and \textsc{Tweedie, R.~L.} (2012).
\newblock \textit{Markov chains and stochastic stability}.
\newblock Springer Science \& Business Media.

\bibitem[{Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski et~al.}]{mnih2015human}
\textsc{Mnih, V.}, \textsc{Kavukcuoglu, K.}, \textsc{Silver, D.}, \textsc{Rusu,
  A.~A.}, \textsc{Veness, J.}, \textsc{Bellemare, M.~G.}, \textsc{Graves, A.},
  \textsc{Riedmiller, M.}, \textsc{Fidjeland, A.~K.}, \textsc{Ostrovski, G.}
  \textsc{et~al.} (2015).
\newblock Human-level control through deep reinforcement learning.
\newblock \textit{Nature} \textbf{518} 529.

\bibitem[{Munos and Szepesv{\'a}ri(2008)}]{munos2008finite}
\textsc{Munos, R.} and \textsc{Szepesv{\'a}ri, C.} (2008).
\newblock Finite-time bounds for fitted value iteration.
\newblock \textit{Journal of Machine Learning Research} \textbf{9} 815--857.

\bibitem[{Ormoneit and Sen(2002)}]{ormoneit2002kernel}
\textsc{Ormoneit, D.} and \textsc{Sen, {\'S}.} (2002).
\newblock Kernel-based reinforcement learning.
\newblock \textit{Machine learning} \textbf{49} 161--178.

\bibitem[{Perkins and Pendrith(2002)}]{perkins2002existence}
\textsc{Perkins, T.~J.} and \textsc{Pendrith, M.~D.} (2002).
\newblock On the existence of fixed points for q-learning and sarsa in
  partially observable domains.
\newblock In \textit{Proceedings of the Nineteenth International Conference on
  Machine Learning}. Morgan Kaufmann Publishers Inc.

\bibitem[{Riedmiller(2005)}]{riedmiller2005neural}
\textsc{Riedmiller, M.} (2005).
\newblock Neural fitted q iteration--first experiences with a data efficient
  neural reinforcement learning method.
\newblock In \textit{European Conference on Machine Learning}. Springer.

\bibitem[{Schmidhuber(2015)}]{schmidhuber2015deep}
\textsc{Schmidhuber, J.} (2015).
\newblock Deep learning in neural networks: An overview.
\newblock \textit{Neural Networks} \textbf{61} 85--117.

\bibitem[{Schwarting et~al.(2018)Schwarting, Alonso-Mora and
  Rus}]{schwarting2018planning}
\textsc{Schwarting, W.}, \textsc{Alonso-Mora, J.} and \textsc{Rus, D.} (2018).
\newblock Planning and decision-making for autonomous vehicles.
\newblock \textit{Annual Review of Control, Robotics, and Autonomous Systems} .

\bibitem[{Shalev{-}Shwartz et~al.(2016)Shalev{-}Shwartz, Shammah and
  Shashua}]{shalev2016auto-driving}
\textsc{Shalev{-}Shwartz, S.}, \textsc{Shammah, S.} and \textsc{Shashua, A.}
  (2016).
\newblock Safe, multi-agent, reinforcement learning for autonomous driving.
\newblock \textit{CoRR} \textbf{abs/1610.03295}.
\newline\urlprefix\url{http://arxiv.org/abs/1610.03295}

\bibitem[{Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, van~den
  Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, Dieleman,
  Grewe, Nham, Kalchbrenner, Sutskever, Lillicrap, Leach, Kavukcuoglu, Graepel
  and Hassabis}]{silver2016mastering-go}
\textsc{Silver, D.}, \textsc{Huang, A.}, \textsc{Maddison, C.~J.},
  \textsc{Guez, A.}, \textsc{Sifre, L.}, \textsc{van~den Driessche, G.},
  \textsc{Schrittwieser, J.}, \textsc{Antonoglou, I.}, \textsc{Panneershelvam,
  V.}, \textsc{Lanctot, M.}, \textsc{Dieleman, S.}, \textsc{Grewe, D.},
  \textsc{Nham, J.}, \textsc{Kalchbrenner, N.}, \textsc{Sutskever, I.},
  \textsc{Lillicrap, T.~P.}, \textsc{Leach, M.}, \textsc{Kavukcuoglu, K.},
  \textsc{Graepel, T.} and \textsc{Hassabis, D.} (2016).
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \textit{Nature} \textbf{529} 484--489.

\bibitem[{Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou,
  Huang, Guez, Hubert, Baker, Lai, Bolton et~al.}]{silver2017mastering}
\textsc{Silver, D.}, \textsc{Schrittwieser, J.}, \textsc{Simonyan, K.},
  \textsc{Antonoglou, I.}, \textsc{Huang, A.}, \textsc{Guez, A.},
  \textsc{Hubert, T.}, \textsc{Baker, L.}, \textsc{Lai, M.}, \textsc{Bolton,
  A.} \textsc{et~al.} (2017).
\newblock Mastering the game of go without human knowledge.
\newblock \textit{Nature} \textbf{550} 354.

\bibitem[{Srikant and Ying(2019)}]{srikant2019finite}
\textsc{Srikant, R.} and \textsc{Ying, L.} (2019).
\newblock Finite-time error bounds for linear stochastic approximation and td
  learning.
\newblock \textit{arXiv preprint arXiv:1902.00923} .

\bibitem[{Sutton and Barto(2018)}]{sutton2018reinforcement}
\textsc{Sutton, R.~S.} and \textsc{Barto, A.~G.} (2018).
\newblock \textit{Reinforcement learning: An introduction}.
\newblock MIT press.

\bibitem[{Szepesvari(2010)}]{szepesvari2010algorithms}
\textsc{Szepesvari, C.} (2010).
\newblock Algorithms for reinforcement learning.
\newblock \textit{Synthesis lectures on artificial intelligence and machine
  learning} \textbf{4} 1--103.

\bibitem[{Tsitsiklis and Van~Roy(1997)}]{tsitsiklis1997analysis}
\textsc{Tsitsiklis, J.~N.} and \textsc{Van~Roy, B.} (1997).
\newblock Analysis of temporal-diffference learning with function
  approximation.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Van~Hasselt et~al.(2016)Van~Hasselt, Guez and Silver}]{van2016deep}
\textsc{Van~Hasselt, H.}, \textsc{Guez, A.} and \textsc{Silver, D.} (2016).
\newblock Deep reinforcement learning with double q-learning.
\newblock In \textit{Thirtieth AAAI conference on artificial intelligence}.

\bibitem[{Wang et~al.(2016)Wang, Schaul, Hessel, Hasselt, Lanctot and
  Freitas}]{wang2016dueling}
\textsc{Wang, Z.}, \textsc{Schaul, T.}, \textsc{Hessel, M.}, \textsc{Hasselt,
  H.}, \textsc{Lanctot, M.} and \textsc{Freitas, N.} (2016).
\newblock Dueling network architectures for deep reinforcement learning.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Watkins and Dayan(1992)}]{watkins1992q}
\textsc{Watkins, C.~J.} and \textsc{Dayan, P.} (1992).
\newblock Q-learning.
\newblock \textit{Machine Learning} \textbf{8} 279--292.

\bibitem[{Yang et~al.(2019)Yang, Xie and Wang}]{yang2019theoretical}
\textsc{Yang, Z.}, \textsc{Xie, Y.} and \textsc{Wang, Z.} (2019).
\newblock A theoretical analysis of deep q-learning.
\newblock \textit{arXiv preprint arXiv:1901.00137} .

\bibitem[{Zou et~al.(2019{\natexlab{a}})Zou, Cao, Zhou and
  Gu}]{zou2019stochastic}
\textsc{Zou, D.}, \textsc{Cao, Y.}, \textsc{Zhou, D.} and \textsc{Gu, Q.}
  (2019{\natexlab{a}}).
\newblock Stochastic gradient descent optimizes over-parameterized deep relu
  networks.
\newblock \textit{Machine Learning} .

\bibitem[{Zou and Gu(2019)}]{zou2019improved}
\textsc{Zou, D.} and \textsc{Gu, Q.} (2019).
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Zou et~al.(2019{\natexlab{b}})Zou, Xu and Liang}]{zou2019finite}
\textsc{Zou, S.}, \textsc{Xu, T.} and \textsc{Liang, Y.} (2019{\natexlab{b}}).
\newblock Finite-sample analysis for sarsa and q-learning with linear function
  approximation.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\end{thebibliography}
