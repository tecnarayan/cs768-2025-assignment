\begin{thebibliography}{84}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Schuurmans, and
  Norouzi]{agarwal2020optimistic}
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi.
\newblock An optimistic perspective on offline reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  104--114. PMLR, 2020.

\bibitem[Agarwal et~al.(2021)Agarwal, Schwarzer, Castro, Courville, and
  Bellemare]{agarwal2021deep}
Rishabh Agarwal, Max Schwarzer, Pablo~Samuel Castro, Aaron~C Courville, and
  Marc Bellemare.
\newblock Deep reinforcement learning at the edge of the statistical precipice.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Ahn et~al.(2022)Ahn, Brohan, Brown, Chebotar, Cortes, David, Finn,
  Gopalakrishnan, Hausman, Herzog, et~al.]{ahn2022can}
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron
  David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog,
  et~al.
\newblock Do as i can, not as i say: Grounding language in robotic affordances.
\newblock \emph{arXiv preprint arXiv:2204.01691}, 2022.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc,
  Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
  Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds,
  et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{arXiv preprint arXiv:2204.14198}, 2022.

\bibitem[Alexander and Gershman(2021)]{alexander2021representation}
William~H Alexander and Samuel~J Gershman.
\newblock Representation learning with reward prediction errors.
\newblock \emph{arXiv preprint arXiv:2108.12402}, 2021.

\bibitem[Arnab et~al.(2021)Arnab, Dehghani, Heigold, Sun, Lu{\v{c}}i{\'c}, and
  Schmid]{arnab2021vivit}
Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu{\v{c}}i{\'c},
  and Cordelia Schmid.
\newblock Vivit: A video vision transformer.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 6836--6846, 2021.

\bibitem[Babuschkin et~al.(2020)Babuschkin, Baumli, Bell, Bhupatiraju, Bruce,
  Buchlovsky, Budden, Cai, Clark, Danihelka, Fantacci, Godwin, Jones, Hennigan,
  Hessel, Kapturowski, Keck, Kemaev, King, Martens, Mikulik, Norman, Quan,
  Papamakarios, Ring, Ruiz, Sanchez, Schneider, Sezener, Spencer, Srinivasan,
  Stokowiec, and Viola]{deepmind2020jax}
Igor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter
  Buchlovsky, David Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, Claudio
  Fantacci, Jonathan Godwin, Chris Jones, Tom Hennigan, Matteo Hessel, Steven
  Kapturowski, Thomas Keck, Iurii Kemaev, Michael King, Lena Martens, Vladimir
  Mikulik, Tamara Norman, John Quan, George Papamakarios, Roman Ring, Francisco
  Ruiz, Alvaro Sanchez, Rosalia Schneider, Eren Sezener, Stephen Spencer,
  Srivatsan Srinivasan, Wojciech Stokowiec, and Fabio Viola.
\newblock The {D}eep{M}ind {JAX} {E}cosystem, 2020.
\newblock URL \url{http://github.com/deepmind}.

\bibitem[Bastani et~al.(2018)Bastani, Pu, and
  Solar-Lezama]{bastani2018verifiable}
Osbert Bastani, Yewen Pu, and Armando Solar-Lezama.
\newblock Verifiable reinforcement learning via policy extraction.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[{Bellemare} et~al.(2013){Bellemare}, {Naddaf}, {Veness}, and
  {Bowling}]{bellemare13arcade}
M.~G. {Bellemare}, Y.~{Naddaf}, J.~{Veness}, and M.~{Bowling}.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, jun 2013.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare2013arcade}
Marc~G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and
  Munos]{bellemare2017distributional}
Marc~G Bellemare, Will Dabney, and R{\'e}mi Munos.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  449--458. PMLR, 2017.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Castro et~al.(2018)Castro, Moitra, Gelada, Kumar, and
  Bellemare]{castro2018dopamine}
Pablo~Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc~G
  Bellemare.
\newblock Dopamine: A research framework for deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1812.06110}, 2018.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel,
  Srinivas, and Mordatch]{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha
  Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{Advances in neural information processing systems}, 34, 2021.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Cuccu et~al.(2018)Cuccu, Togelius, and
  Cudr{\'e}-Mauroux]{cuccu2018playing}
Giuseppe Cuccu, Julian Togelius, and Philippe Cudr{\'e}-Mauroux.
\newblock Playing atari with six neurons.
\newblock \emph{arXiv preprint arXiv:1806.01363}, 2018.

\bibitem[Dabral et~al.(2021)Dabral, Ramakrishnan, Jyothi,
  et~al.]{dabral2021rudder}
Rishabh Dabral, Ganesh Ramakrishnan, Preethi Jyothi, et~al.
\newblock Rudder: A cross lingual video and text retrieval dataset.
\newblock \emph{arXiv preprint arXiv:2103.05457}, 2021.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, et~al.]{espeholt2018impala}
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward,
  Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In \emph{International Conference on Machine Learning}, pages
  1407--1416. PMLR, 2018.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International Conference on Machine Learning}, pages
  2052--2062. PMLR, 2019.

\bibitem[Furuta et~al.(2021{\natexlab{a}})Furuta, Kozuno, Matsushima, Matsuo,
  and Gu]{furuta2021co}
Hiroki Furuta, Tadashi Kozuno, Tatsuya Matsushima, Yutaka Matsuo, and
  Shixiang~Shane Gu.
\newblock Co-adaptation of algorithmic and implementational innovations in
  inference-based deep reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 9828--9842, 2021{\natexlab{a}}.

\bibitem[Furuta et~al.(2021{\natexlab{b}})Furuta, Matsuo, and
  Gu]{furuta2021generalized}
Hiroki Furuta, Yutaka Matsuo, and Shixiang~Shane Gu.
\newblock Generalized decision transformer for offline hindsight information
  matching.
\newblock \emph{arXiv preprint arXiv:2111.10364}, 2021{\natexlab{b}}.

\bibitem[Gelada et~al.(2019)Gelada, Kumar, Buckman, Nachum, and
  Bellemare]{gelada2019deepmdp}
Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc~G Bellemare.
\newblock Deepmdp: Learning continuous latent space models for representation
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  2170--2179. PMLR, 2019.

\bibitem[Gulcehre et~al.(2020)Gulcehre, Wang, Novikov, Paine, G{\'o}mez, Zolna,
  Agarwal, Merel, Mankowitz, Paduraru, et~al.]{gulcehre2020rl}
Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Thomas Paine, Sergio G{\'o}mez,
  Konrad Zolna, Rishabh Agarwal, Josh~S Merel, Daniel~J Mankowitz, Cosmin
  Paduraru, et~al.
\newblock Rl unplugged: A suite of benchmarks for offline reinforcement
  learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7248--7259, 2020.

\bibitem[Gupta et~al.(2021)Gupta, Savarese, Ganguli, and
  Fei-Fei]{gupta2021embodied}
Agrim Gupta, Silvio Savarese, Surya Ganguli, and Li~Fei-Fei.
\newblock Embodied intelligence via learning and evolution.
\newblock \emph{Nature communications}, 12\penalty0 (1):\penalty0 1--12, 2021.

\bibitem[Hafner et~al.(2019{\natexlab{a}})Hafner, Lillicrap, Ba, and
  Norouzi]{hafner2019dream}
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi.
\newblock Dream to control: Learning behaviors by latent imagination.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Hafner et~al.(2019{\natexlab{b}})Hafner, Lillicrap, Fischer, Villegas,
  Ha, Lee, and Davidson]{hafner2019learning}
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha,
  Honglak Lee, and James Davidson.
\newblock Learning latent dynamics for planning from pixels.
\newblock In \emph{International conference on machine learning}, pages
  2555--2565. PMLR, 2019{\natexlab{b}}.

\bibitem[Hafner et~al.(2020)Hafner, Lillicrap, Norouzi, and
  Ba]{hafner2020mastering}
Danijar Hafner, Timothy~P Lillicrap, Mohammad Norouzi, and Jimmy Ba.
\newblock Mastering atari with discrete world models.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Hessel et~al.(2018)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{hessel2018rainbow}
Matteo Hessel, Joseph Modayil, Hado Van~Hasselt, Tom Schaul, Georg Ostrovski,
  Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In \emph{Thirty-second AAAI conference on artificial intelligence},
  2018.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and
  Choi]{holtzman2019curious}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock \emph{arXiv preprint arXiv:1904.09751}, 2019.

\bibitem[Huang et~al.(2020)Huang, Mordatch, and Pathak]{huang2020one}
Wenlong Huang, Igor Mordatch, and Deepak Pathak.
\newblock One policy to control them all: Shared modular policies for
  agent-agnostic control.
\newblock In \emph{International Conference on Machine Learning}, pages
  4455--4464. PMLR, 2020.

\bibitem[Jang et~al.(2022)Jang, Irpan, Khansari, Kappler, Ebert, Lynch, Levine,
  and Finn]{jang2022bc}
Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey
  Lynch, Sergey Levine, and Chelsea Finn.
\newblock Bc-z: Zero-shot task generalization with robotic imitation learning.
\newblock In \emph{Conference on Robot Learning}, pages 991--1002. PMLR, 2022.

\bibitem[Janner et~al.(2021)Janner, Li, and Levine]{janner2021offline}
Michael Janner, Qiyang Li, and Sergey Levine.
\newblock Offline reinforcement learning as one big sequence modeling problem.
\newblock \emph{Advances in neural information processing systems}, 34, 2021.

\bibitem[Kaiser et~al.(2017)Kaiser, Gomez, Shazeer, Vaswani, Parmar, Jones, and
  Uszkoreit]{kaiser2017one}
Lukasz Kaiser, Aidan~N Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion
  Jones, and Jakob Uszkoreit.
\newblock One model to learn them all.
\newblock \emph{arXiv preprint arXiv:1706.05137}, 2017.

\bibitem[Kalashnikov et~al.(2021)Kalashnikov, Varley, Chebotar, Swanson,
  Jonschkowski, Finn, Levine, and Hausman]{kalashnikov2021mt}
Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico
  Jonschkowski, Chelsea Finn, Sergey Levine, and Karol Hausman.
\newblock Mt-opt: Continuous multi-task robotic reinforcement learning at
  scale.
\newblock \emph{arXiv preprint arXiv:2104.08212}, 2021.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kappen et~al.(2012)Kappen, G{\'o}mez, and Opper]{kappen2012optimal}
Hilbert~J Kappen, Vicen{\c{c}} G{\'o}mez, and Manfred Opper.
\newblock Optimal control as a graphical model inference problem.
\newblock \emph{Machine learning}, 87\penalty0 (2):\penalty0 159--182, 2012.

\bibitem[Krause et~al.(2020)Krause, Gotmare, McCann, Keskar, Joty, Socher, and
  Rajani]{krause2020gedi}
Ben Krause, Akhilesh~Deepak Gotmare, Bryan McCann, Nitish~Shirish Keskar,
  Shafiq Joty, Richard Socher, and Nazneen~Fatema Rajani.
\newblock Gedi: Generative discriminator guided sequence generation.
\newblock \emph{arXiv preprint arXiv:2009.06367}, 2020.

\bibitem[Kumar et~al.(2019)Kumar, Peng, and Levine]{kumar2019reward}
Aviral Kumar, Xue~Bin Peng, and Sergey Levine.
\newblock Reward-conditioned policies.
\newblock \emph{arXiv preprint arXiv:1912.13465}, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1179--1191, 2020.

\bibitem[Kurin et~al.(2020)Kurin, Igl, Rockt{\"a}schel, Boehmer, and
  Whiteson]{kurin2020my}
Vitaly Kurin, Maximilian Igl, Tim Rockt{\"a}schel, Wendelin Boehmer, and Shimon
  Whiteson.
\newblock My body is a cage: the role of morphology in graph-based incompatible
  control.
\newblock \emph{arXiv preprint arXiv:2010.01856}, 2020.

\bibitem[Lee et~al.(2020)Lee, Fischer, Liu, Guo, Lee, Canny, and
  Guadarrama]{lee2020predictive}
Kuang-Huei Lee, Ian Fischer, Anthony Liu, Yijie Guo, Honglak Lee, John Canny,
  and Sergio Guadarrama.
\newblock Predictive information accelerates learning in rl.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 11890--11901, 2020.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Lu et~al.(2021)Lu, Grover, Abbeel, and Mordatch]{lu2021pretrained}
Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch.
\newblock Pretrained transformers as universal computation engines.
\newblock \emph{arXiv preprint arXiv:2103.05247}, 2021.

\bibitem[Lyle et~al.(2021)Lyle, Rowland, Ostrovski, and Dabney]{lyle2021effect}
Clare Lyle, Mark Rowland, Georg Ostrovski, and Will Dabney.
\newblock On the effect of auxiliary tasks on representation dynamics.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1--9. PMLR, 2021.

\bibitem[Lynch and Sermanet(2020)]{lynch2020language}
Corey Lynch and Pierre Sermanet.
\newblock Language conditioned imitation learning over unstructured data.
\newblock \emph{arXiv preprint arXiv:2005.07648}, 2020.

\bibitem[Mania et~al.(2018)Mania, Guy, and Recht]{mania2018simple}
Horia Mania, Aurelia Guy, and Benjamin Recht.
\newblock Simple random search provides a competitive approach to reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1803.07055}, 2018.

\bibitem[McCarthy et~al.(2006)McCarthy, Minsky, Rochester, and
  Shannon]{mccarthy2006proposal}
John McCarthy, Marvin~L Minsky, Nathaniel Rochester, and Claude~E Shannon.
\newblock A proposal for the dartmouth summer research project on artificial
  intelligence, august 31, 1955.
\newblock \emph{AI magazine}, 27\penalty0 (4):\penalty0 12--12, 2006.

\bibitem[Mendonca et~al.(2021)Mendonca, Rybkin, Daniilidis, Hafner, and
  Pathak]{mendonca2021discovering}
Russell Mendonca, Oleh Rybkin, Kostas Daniilidis, Danijar Hafner, and Deepak
  Pathak.
\newblock Discovering and achieving goals via world models.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages
  1928--1937. PMLR, 2016.

\bibitem[Nachum and Yang(2021)]{nachum2021provable}
Ofir Nachum and Mengjiao Yang.
\newblock Provable representation learning for imitation with contrastive
  fourier features.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Ortega et~al.(2021)Ortega, Kunesch, Del{\'e}tang, Genewein, Grau-Moya,
  Veness, Buchli, Degrave, Piot, Perolat, et~al.]{ortega2021shaking}
Pedro~A Ortega, Markus Kunesch, Gr{\'e}goire Del{\'e}tang, Tim Genewein, Jordi
  Grau-Moya, Joel Veness, Jonas Buchli, Jonas Degrave, Bilal Piot, Julien
  Perolat, et~al.
\newblock Shaking the foundations: delusions in sequence models for interaction
  and control.
\newblock \emph{arXiv preprint arXiv:2110.10819}, 2021.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{arXiv preprint arXiv:2203.02155}, 2022.

\bibitem[Parisotto et~al.(2015)Parisotto, Ba, and
  Salakhutdinov]{parisotto2015actor}
Emilio Parisotto, Jimmy~Lei Ba, and Ruslan Salakhutdinov.
\newblock Actor-mimic: Deep multitask and transfer reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1511.06342}, 2015.

\bibitem[Pomerleau(1991)]{pomerleau1991efficient}
Dean~A Pomerleau.
\newblock Efficient training of artificial neural networks for autonomous
  navigation.
\newblock \emph{Neural computation}, 3\penalty0 (1):\penalty0 88--97, 1991.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning}, pages
  8748--8763. PMLR, 2021.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  et~al.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}, 2021.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}, 2019.

\bibitem[Raghu et~al.(2021)Raghu, Unterthiner, Kornblith, Zhang, and
  Dosovitskiy]{raghu2021vision}
Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey
  Dosovitskiy.
\newblock Do vision transformers see like convolutional neural networks?
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 12116--12128, 2021.

\bibitem[Reed et~al.(2022)Reed, Zolna, Parisotto, Colmenarejo, Novikov,
  Barth-Maron, Gimenez, Sulsky, Kay, Springenberg, Eccles, Bruce, Razavi,
  Edwards, Heess, Chen, Hadsell, Vinyals, Bordbar, and
  de~Freitas]{gato2022deepmind}
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio~Gomez Colmenarejo, Alexander
  Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay,
  Jost~Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards,
  Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and
  Nando de~Freitas.
\newblock A generalist agent, 2022.
\newblock URL \url{https://arxiv.org/abs/2205.06175}.

\bibitem[Reid et~al.(2022)Reid, Yamada, and Gu]{reid2022can}
Machel Reid, Yutaro Yamada, and Shixiang~Shane Gu.
\newblock Can wikipedia help offline reinforcement learning?
\newblock \emph{arXiv preprint arXiv:2201.12122}, 2022.

\bibitem[Rusu et~al.(2015)Rusu, Colmenarejo, Gulcehre, Desjardins, Kirkpatrick,
  Pascanu, Mnih, Kavukcuoglu, and Hadsell]{rusu2015policy}
Andrei~A Rusu, Sergio~Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins,
  James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and
  Raia Hadsell.
\newblock Policy distillation.
\newblock \emph{arXiv preprint arXiv:1511.06295}, 2015.

\bibitem[Schmidhuber(2019)]{schmidhuber2019reinforcement}
Juergen Schmidhuber.
\newblock Reinforcement learning upside down: Don't predict rewards--just map
  them to actions.
\newblock \emph{arXiv preprint arXiv:1912.02875}, 2019.

\bibitem[Schrittwieser et~al.(2020)Schrittwieser, Antonoglou, Hubert, Simonyan,
  Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel,
  et~al.]{schrittwieser2020mastering}
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan,
  Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis,
  Thore Graepel, et~al.
\newblock Mastering atari, go, chess and shogi by planning with a learned
  model.
\newblock \emph{Nature}, 588\penalty0 (7839):\penalty0 604--609, 2020.

\bibitem[Shachter(1988)]{shachter1988probabilistic}
Ross~D Shachter.
\newblock Probabilistic inference and influence diagrams.
\newblock \emph{Operations research}, 36\penalty0 (4):\penalty0 589--604, 1988.

\bibitem[Srivastava et~al.(2019)Srivastava, Shyam, Mutz, Ja{\'s}kowski, and
  Schmidhuber]{srivastava2019training}
Rupesh~Kumar Srivastava, Pranav Shyam, Filipe Mutz, Wojciech Ja{\'s}kowski, and
  J{\"u}rgen Schmidhuber.
\newblock Training agents using upside-down reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.02877}, 2019.

\bibitem[Su et~al.(2021)Su, Lu, Pan, Wen, and Liu]{su2021roformer}
Jianlin Su, Yu~Lu, Shengfeng Pan, Bo~Wen, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{arXiv preprint arXiv:2104.09864}, 2021.

\bibitem[Todorov(2006)]{todorov2006linearly}
Emanuel Todorov.
\newblock Linearly-solvable markov decision problems.
\newblock \emph{Advances in neural information processing systems}, 19, 2006.

\bibitem[Toussaint(2009)]{toussaint2009robot}
Marc Toussaint.
\newblock Robot trajectory optimization using approximate inference.
\newblock In \emph{Proceedings of the 26th annual international conference on
  machine learning}, pages 1049--1056, 2009.

\bibitem[Tsimpoukelli et~al.(2021)Tsimpoukelli, Menick, Cabi, Eslami, Vinyals,
  and Hill]{tsimpoukelli2021multimodal}
Maria Tsimpoukelli, Jacob~L Menick, Serkan Cabi, SM~Eslami, Oriol Vinyals, and
  Felix Hill.
\newblock Multimodal few-shot learning with frozen language models.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 200--212, 2021.

\bibitem[van~den Oord et~al.(2017)van~den Oord, Vinyals, and
  Kavukcuoglu]{van2017neural}
Aaron van~den Oord, Oriol Vinyals, and Koray Kavukcuoglu.
\newblock Neural discrete representation learning.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 6309--6318, 2017.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Xue et~al.(2021)Xue, Constant, Roberts, Kale, Al-Rfou, Siddhant,
  Barua, and Raffel]{xue2021mt5}
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
  Siddhant, Aditya Barua, and Colin Raffel.
\newblock mt5: A massively multilingual pre-trained text-to-text transformer.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 483--498, 2021.

\bibitem[Yang and Klein(2021)]{yang2021fudge}
Kevin Yang and Dan Klein.
\newblock Fudge: Controlled text generation with future discriminators.
\newblock \emph{arXiv preprint arXiv:2104.05218}, 2021.

\bibitem[Yang and Nachum(2021)]{yang2021representation}
Mengjiao Yang and Ofir Nachum.
\newblock Representation matters: Offline pretraining for sequential decision
  making.
\newblock In \emph{International Conference on Machine Learning}, pages
  11784--11794. PMLR, 2021.

\bibitem[You et~al.(2019)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song,
  Demmel, Keutzer, and Hsieh]{you2019large}
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh
  Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
\newblock Large batch optimization for deep learning: Training bert in 76
  minutes.
\newblock \emph{arXiv preprint arXiv:1904.00962}, 2019.

\bibitem[Yu et~al.(2020)Yu, Quillen, He, Julian, Hausman, Finn, and
  Levine]{yu2020meta}
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea
  Finn, and Sergey Levine.
\newblock Meta-world: A benchmark and evaluation for multi-task and meta
  reinforcement learning.
\newblock In \emph{Conference on Robot Learning}, pages 1094--1100. PMLR, 2020.

\bibitem[Zhang et~al.(2020)Zhang, McAllister, Calandra, Gal, and
  Levine]{zhang2020learning}
Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine.
\newblock Learning invariant representations for reinforcement learning without
  reconstruction.
\newblock \emph{arXiv preprint arXiv:2006.10742}, 2020.

\bibitem[Zheng et~al.(2022)Zheng, Zhang, and Grover]{zheng2022online}
Qinqing Zheng, Amy Zhang, and Aditya Grover.
\newblock Online decision transformer.
\newblock \emph{arXiv preprint arXiv:2202.05607}, 2022.

\end{thebibliography}
