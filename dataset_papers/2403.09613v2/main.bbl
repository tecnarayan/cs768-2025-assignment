\begin{thebibliography}{10}

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In {\em Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies}, 2019.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in Neural Information Processing Systems}, 33:1877--1901, 2020.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock {LLaMA}: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{openai2023gpt4}
OpenAI.
\newblock {GPT}-4 technical report, 2023.

\bibitem{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock {\em arXiv preprint arXiv:2203.15556}, 2022.

\bibitem{chowdhery2023palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al.
\newblock {PaLM}: Scaling language modeling with pathways.
\newblock {\em Journal of Machine Learning Research}, 24(240):1--113, 2023.

\bibitem{xue2023repeat}
Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You.
\newblock To repeat or not to repeat: Insights from scaling llm under token-crisis.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{chen2018lifelong}
Zhiyuan Chen and Bing Liu.
\newblock {\em Lifelong Machine Learning}, volume~1.
\newblock Springer, 2018.

\bibitem{mccloskey1989catastrophic}
Michael McCloskey and Neal~J Cohen.
\newblock Catastrophic interference in connectionist networks: The sequential learning problem.
\newblock In {\em Psychology of Learning and Motivation}, volume~24, pages 109--165. Elsevier, 1989.

\bibitem{cai2021online}
Zhipeng Cai, Ozan Sener, and Vladlen Koltun.
\newblock Online continual learning with natural distribution shifts: An empirical study with visual data.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 8281--8290, 2021.

\bibitem{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin~Gregory Anthony, Herbie Bradley, Kyle Oâ€™Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, et~al.
\newblock Pythia: A suite for analyzing large language models across training and scaling.
\newblock In {\em International Conference on Machine Learning}, pages 2397--2430. PMLR, 2023.

\bibitem{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The {P}ile: An 800{GB} dataset of diverse text for language modeling.
\newblock {\em arXiv preprint arXiv:2101.00027}, 2020.

\bibitem{biderman2022datasheet}
Stella Biderman, Kieran Bicheno, and Leo Gao.
\newblock Datasheet for the {P}ile.
\newblock {\em arXiv preprint arXiv:2201.07311}, 2022.

\bibitem{chen2020generative}
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.
\newblock Generative pretraining from pixels.
\newblock In {\em International Conference on Machine Learning}, pages 1691--1703. PMLR, 2020.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In {\em International Conference on Learning Representations}, 2015.

\bibitem{nallapati2016abstractive}
Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, {\c{C}}a{\u{g}}lar Gul{\c{c}}ehre, and Bing Xiang.
\newblock Abstractive text summarization using sequence-to-sequence rnns and beyond.
\newblock In {\em Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning}, pages 280--290, 2016.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock {\em University of Toronto}, 2009.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on Computer Vision and Pattern Recognition}, pages 248--255. IEEE, 2009.

\bibitem{kingma2014adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In {\em International Conference on Learning Representations}, 2015.

\bibitem{ahn2020sgd}
Kwangjun Ahn, Chulhee Yun, and Suvrit Sra.
\newblock {SGD} with shuffling: {O}ptimal rates without component convexity and large epoch requirements.
\newblock {\em {Advances in Neural Information Processing Systems}}, 33:17526--17535, 2020.

\bibitem{gurbuzbalaban2019convergence}
M~Gurbuzbalaban, Asu Ozdaglar, and Pablo~A Parrilo.
\newblock Convergence rate of incremental gradient and incremental newton methods.
\newblock {\em SIAM Journal on Optimization}, 29(4):2542--2565, 2019.

\bibitem{mishchenko2020random}
Konstantin Mishchenko, Ahmed Khaled, and Peter Richt{\'a}rik.
\newblock Random reshuffling: Simple analysis with vast improvements.
\newblock {\em {Advances in Neural Information Processing Systems}}, 33:17309--17320, 2020.

\bibitem{safran2020good}
Itay Safran and Ohad Shamir.
\newblock How good is {SGD} with random shuffling?
\newblock In {\em Conference on Learning Theory}, pages 3250--3284. PMLR, 2020.

\bibitem{xu2022stochastic}
Lijie Xu, Shuang Qiu, Binhang Yuan, Jiawei Jiang, Cedric Renggli, Shaoduo Gan, Kaan Kara, Guoliang Li, Ji~Liu, Wentao Wu, et~al.
\newblock Stochastic gradient descent without full data shuffle.
\newblock {\em arXiv preprint arXiv:2206.05830}, 2022.

\bibitem{hannan1957approximation}
James Hannan.
\newblock Approximation to {B}ayes risk in repeated play.
\newblock {\em Contributions to the Theory of Games}, 3:97--139, 1957.

\bibitem{zinkevich2003online}
Martin Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient ascent.
\newblock In {\em International Conference on Machine Learning}, pages 928--936, 2003.

\bibitem{cesa2006prediction}
Nicolo Cesa-Bianchi and G{\'a}bor Lugosi.
\newblock {\em Prediction, learning, and games}.
\newblock Cambridge University Press, 2006.

\bibitem{shalev2012online}
Shai Shalev-Shwartz et~al.
\newblock Online learning and online convex optimization.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning}, 4(2):107--194, 2012.

\bibitem{denevi2019learning}
Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil.
\newblock Learning-to-learn stochastic gradient descent with biased regularization.
\newblock In {\em International Conference on Machine Learning}, pages 1566--1575. PMLR, 2019.

\bibitem{finn2019online}
Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine.
\newblock Online meta-learning.
\newblock In {\em International Conference on Machine Learning}, pages 1920--1930. PMLR, 2019.

\bibitem{denevi2019online}
Giulia Denevi, Dimitris Stamos, Carlo Ciliberto, and Massimiliano Pontil.
\newblock Online-within-online meta-learning.
\newblock {\em {Advances in Neural Information Processing Systems}}, 32, 2019.

\bibitem{javed2019meta}
Khurram Javed and Martha White.
\newblock Meta-learning representations for continual learning.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{fini2020online}
Enrico Fini, St{\'e}phane Lathuiliere, Enver Sangineto, Moin Nabi, and Elisa Ricci.
\newblock Online continual learning under extreme memory constraints.
\newblock In {\em Proceedings of the European Conference on Computer Vision}, pages 720--735. Springer, 2020.

\bibitem{ren2020wandering}
Mengye Ren, Michael~Louis Iuzzolino, Michael~Curtis Mozer, and Richard~S. Zemel.
\newblock Wandering within a world: Online contextualized few-shot learning.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{wang2021wanderlust}
Jianren Wang, Xin Wang, Yue Shang-Guan, and Abhinav Gupta.
\newblock Wanderlust: Online continual object detection in the real world.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 10829--10838, 2021.

\bibitem{biehl1995learning}
Michael Biehl and Holm Schwarze.
\newblock Learning by on-line gradient descent.
\newblock {\em Journal of Physics A: Mathematical and general}, 28(3):643, 1995.

\bibitem{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock {\em Proceedings of the National Academy of Sciences}, 114(13):3521--3526, 2017.

\bibitem{zenke2017continual}
Friedemann Zenke, Ben Poole, and Surya Ganguli.
\newblock Continual learning through synaptic intelligence.
\newblock In {\em International Conference on Machine Learning}, pages 3987--3995. PMLR, 2017.

\bibitem{aljundi2018memory}
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars.
\newblock Memory aware synapses: Learning what (not) to forget.
\newblock In {\em Proceedings of the European Conference on Computer Vision}, pages 139--154, 2018.

\bibitem{rebuffi2017icarl}
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph~H Lampert.
\newblock {iCaRL}: Incremental classifier and representation learning.
\newblock In {\em Proceedings of the IEEE conference on Computer Vision and Pattern Recognition}, pages 2001--2010, 2017.

\bibitem{rolnick2019experience}
David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne.
\newblock Experience replay for continual learning.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{chaudhry2019tiny}
Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet~K Dokania, Philip~HS Torr, and Marc'Aurelio Ranzato.
\newblock On tiny episodic memories in continual learning.
\newblock {\em arXiv preprint arXiv:1902.10486}, 2019.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{li2017learning}
Zhizhong Li and Derek Hoiem.
\newblock Learning without forgetting.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 40(12):2935--2947, 2017.

\bibitem{buzzega2020dark}
Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara.
\newblock Dark experience for general continual learning: a strong, simple baseline.
\newblock {\em Advances in Neural Information Processing Systems}, 33:15920--15930, 2020.

\bibitem{madaan2023heterogeneous}
Divyam Madaan, Hongxu Yin, Wonmin Byeon, Jan Kautz, and Pavlo Molchanov.
\newblock Heterogeneous continual learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 15985--15995, 2023.

\bibitem{yoon2017lifelong}
Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung~Ju Hwang.
\newblock Lifelong learning with dynamically expandable networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{serra2018overcoming}
Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou.
\newblock Overcoming catastrophic forgetting with hard attention to the task.
\newblock In {\em International Conference on Machine Learning}, pages 4548--4557. PMLR, 2018.

\bibitem{gurbuz2022nispa}
Mustafa~B Gurbuz and Constantine Dovrolis.
\newblock {NISPA}: Neuro-inspired stability-plasticity adaptation for continual learning in sparse networks.
\newblock In {\em International Conference on Machine Learning}, pages 8157--8174. PMLR, 2022.

\bibitem{kang2022forget}
Haeyong Kang, Rusty John~Lloyd Mina, Sultan Rizky~Hikmawan Madjid, Jaehong Yoon, Mark Hasegawa-Johnson, Sung~Ju Hwang, and Chang~D Yoo.
\newblock Forget-free continual learning with winning subnetworks.
\newblock In {\em International Conference on Machine Learning}, pages 10734--10750. PMLR, 2022.

\bibitem{mayo2023multitask}
David Mayo, Tyler Scott, Mengye Ren, Gamaleldin Elsayed, Katherine Hermann, Matt Jones, and Michael Mozer.
\newblock Multitask learning via interleaving: A neural network investigation.
\newblock In M.~Goldwater, F.~K. Anggoro, B.~K. Hayes, and D.~C. Ong, editors, {\em Proceedings of the 45th Annual Conference of the Cognitive Science Society}, volume~45, 2023.

\bibitem{madotto2021continual}
Andrea Madotto, Zhaojiang Lin, Zhenpeng Zhou, Seungwhan Moon, Paul Crook, Bing Liu, Zhou Yu, Eunjoon Cho, Pascale Fung, and Zhiguang Wang.
\newblock Continual learning in task-oriented dialogue systems.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 7452--7467, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.

\bibitem{qin2022LFPT5}
Chengwei Qin and Shafiq~R. Joty.
\newblock {LFPT5:} {A} unified framework for lifelong few-shot language learning based on prompt tuning of {T5}.
\newblock In {\em International Conference on Learning Representations}. OpenReview.net, 2022.

\bibitem{razdaibiedina2023progressive}
Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian Khabsa, Mike Lewis, and Amjad Almahairi.
\newblock Progressive prompts: Continual learning for language models.
\newblock In {\em International Conference on Learning Representations}. OpenReview.net, 2023.

\bibitem{davidson2020sequential}
Guy Davidson and Michael~C Mozer.
\newblock Sequential mastery of multiple visual tasks: Networks naturally learn to learn and forget to forget.
\newblock In {\em Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition}, pages 9282--9293, 2020.

\bibitem{janson2022simple}
Paul Janson, Wenxuan Zhang, Rahaf Aljundi, and Mohamed Elhoseiny.
\newblock A simple baseline that questions the use of pretrained-models in continual learning.
\newblock In {\em NeurIPS 2022 Workshop on Distribution Shifts: Connecting Methods and Applications}, 2022.

\bibitem{lee2023pre}
Kuan-Ying Lee, Yuanyi Zhong, and Yu-Xiong Wang.
\newblock Do pre-trained models benefit equally in continual learning?
\newblock In {\em Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pages 6485--6493, 2023.

\bibitem{fini2022self}
Enrico Fini, Victor G~Turrisi Da~Costa, Xavier Alameda-Pineda, Elisa Ricci, Karteek Alahari, and Julien Mairal.
\newblock Self-supervised models are continual learners.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 9621--9630, 2022.

\bibitem{scialom2022fine}
Thomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan.
\newblock Fine-tuned language models are continual learners.
\newblock In {\em Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 6107--6122, 2022.

\bibitem{ke2022continual}
Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu.
\newblock Continual pre-training of language models.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{jones2023learning}
Matt Jones, Tyler~R. Scott, Mengye Ren, Gamaleldin~Fathy Elsayed, Katherine~L. Hermann, David Mayo, and Michael~Curtis Mozer.
\newblock Learning in temporally structured environments.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pages 38--45, Online, October 2020. Association for Computational Linguistics.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{merity2017pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock In {\em 5th International Conference on Learning Representations, Toulon, France, April 24-26, 2017, Conference Track Proceedings}, 2017.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al.
\newblock Emergent abilities of large language models.
\newblock {\em Transactions on Machine Learning Research}, 2022.

\bibitem{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:24824--24837, 2022.

\bibitem{ganguli2022predictability}
Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, et~al.
\newblock Predictability and surprise in large generative models.
\newblock In {\em Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency}, pages 1747--1764, 2022.

\bibitem{srivastava2022beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock {\em arXiv preprint arXiv:2206.04615}, 2022.

\bibitem{carlini2022quantifying}
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tram{\`{e}}r, and Chiyuan Zhang.
\newblock Quantifying memorization across neural language models.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{orhan2023recognition}
A~Emin Orhan.
\newblock Recognition, recall, and retention of few-shot memories in large language models.
\newblock {\em arXiv preprint arXiv:2303.17557}, 2023.

\end{thebibliography}
