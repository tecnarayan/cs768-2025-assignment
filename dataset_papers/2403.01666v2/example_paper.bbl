\begin{thebibliography}{73}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arbel et~al.(2021)Arbel, Zhou, and Gretton]{arbel2020generalized}
Arbel, M., Zhou, L., and Gretton, A.
\newblock Generalized energy based models.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and Bottou]{arjovsky2017wasserstein}
Arjovsky, M., Chintala, S., and Bottou, L.
\newblock Wasserstein generative adversarial networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  214--223. PMLR, 2017.

\bibitem[Bao et~al.(2022)Bao, Li, Zhu, and Zhang]{bao2022analytic}
Bao, F., Li, C., Zhu, J., and Zhang, B.
\newblock Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models.
\newblock \emph{International Conference on Learning Representations}, 2022.

\bibitem[Brock et~al.(2019)Brock, Donahue, and Simonyan]{brock2018large}
Brock, A., Donahue, J., and Simonyan, K.
\newblock Large scale gan training for high fidelity natural image synthesis.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Chen et~al.(2018)Chen, Dai, Pu, Zhou, Li, Su, Chen, and Carin]{chen2018symmetric}
Chen, L., Dai, S., Pu, Y., Zhou, E., Li, C., Su, Q., Chen, C., and Carin, L.
\newblock Symmetric variational autoencoder and connections to adversarial learning.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  661--669. PMLR, 2018.

\bibitem[Cimpoi et~al.(2014)Cimpoi, Maji, Kokkinos, Mohamed, and Vedaldi]{cimpoi2014describing}
Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., and Vedaldi, A.
\newblock Describing textures in the wild.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  3606--3613, 2014.

\bibitem[Cui \& Han(2023)Cui and Han]{cui2023learning}
Cui, J. and Han, T.
\newblock Learning energy-based model via dual-mcmc teaching.
\newblock \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Dieng et~al.(2019)Dieng, Ruiz, Blei, and Titsias]{dieng2019prescribed}
Dieng, A.~B., Ruiz, F.~J., Blei, D.~M., and Titsias, M.~K.
\newblock Prescribed generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1910.04302}, 2019.

\bibitem[Dinh et~al.(2015)Dinh, Krueger, and Bengio]{dinh2014nice}
Dinh, L., Krueger, D., and Bengio, Y.
\newblock Nice: Non-linear independent components estimation.
\newblock In \emph{International Conference Learning Representations Workshops}, 2015.

\bibitem[Du \& Mordatch(2019)Du and Mordatch]{du2019implicit}
Du, Y. and Mordatch, I.
\newblock Implicit generation and generalization in energy-based models.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Du et~al.(2021)Du, Li, Tenenbaum, and Mordatch]{du2020improved}
Du, Y., Li, S., Tenenbaum, J., and Mordatch, I.
\newblock Improved contrastive divergence training of energy based models.
\newblock \emph{International Conference on Machine Learning}, 2021.

\bibitem[Du et~al.(2022)Du, Li, Tenenbaum, and Mordatch]{du2022learning}
Du, Y., Li, S., Tenenbaum, J., and Mordatch, I.
\newblock Learning iterative reasoning through energy minimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5570--5582. PMLR, 2022.

\bibitem[Du et~al.(2023)Du, Durkan, Strudel, Tenenbaum, Dieleman, Fergus, Sohl-Dickstein, Doucet, and Grathwohl]{du2023reduce}
Du, Y., Durkan, C., Strudel, R., Tenenbaum, J.~B., Dieleman, S., Fergus, R., Sohl-Dickstein, J., Doucet, A., and Grathwohl, W.~S.
\newblock Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc.
\newblock In \emph{International Conference on Machine Learning}, pp.\  8489--8510. PMLR, 2023.

\bibitem[Gao et~al.(2020)Gao, Nijkamp, Kingma, Xu, Dai, and Wu]{gao2020flow}
Gao, R., Nijkamp, E., Kingma, D.~P., Xu, Z., Dai, A.~M., and Wu, Y.~N.
\newblock Flow contrastive estimation of energy-based models.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  7518--7528, 2020.

\bibitem[Gao et~al.(2021)Gao, Song, Poole, Wu, and Kingma]{gao2020learning}
Gao, R., Song, Y., Poole, B., Wu, Y.~N., and Kingma, D.~P.
\newblock Learning energy-based models by diffusion recovery likelihood.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Geng et~al.(2021)Geng, Wang, Gao, Frellsen, and Hauberg]{geng2021bounds}
Geng, C., Wang, J., Gao, Z., Frellsen, J., and Hauberg, S.
\newblock Bounds all around: training energy-based models with bidirectional bounds.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 19808--19821, 2021.

\bibitem[Grathwohl et~al.(2020)Grathwohl, Wang, Jacobsen, Duvenaud, Norouzi, and Swersky]{grathwohl2019your}
Grathwohl, W., Wang, K.-C., Jacobsen, J.-H., Duvenaud, D., Norouzi, M., and Swersky, K.
\newblock Your classifier is secretly an energy based model and you should treat it like one.
\newblock \emph{International Conference on Learning Representations}, 2020.

\bibitem[Grathwohl et~al.(2021)Grathwohl, Kelly, Hashemi, Norouzi, Swersky, and Duvenaud]{nomcmc}
Grathwohl, W.~S., Kelly, J.~J., Hashemi, M., Norouzi, M., Swersky, K., and Duvenaud, D.
\newblock No {MCMC} for me: Amortized sampling for fast and stable training of energy-based models.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Han et~al.(2019)Han, Nijkamp, Fang, Hill, Zhu, and Wu]{han2019divergence}
Han, T., Nijkamp, E., Fang, X., Hill, M., Zhu, S.-C., and Wu, Y.~N.
\newblock Divergence triangle for joint training of generator model, energy-based model, and inferential model.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  8670--8679, 2019.

\bibitem[Han et~al.(2020)Han, Nijkamp, Zhou, Pang, Zhu, and Wu]{han2020joint}
Han, T., Nijkamp, E., Zhou, L., Pang, B., Zhu, S.-C., and Wu, Y.~N.
\newblock Joint training of variational auto-encoder and latent energy-based model.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  7978--7987, 2020.

\bibitem[Hill et~al.(2022)Hill, Nijkamp, Mitchell, Pang, and Zhu]{hill2022learning}
Hill, M., Nijkamp, E., Mitchell, J., Pang, B., and Zhu, S.-C.
\newblock Learning probabilistic models from generator latent spaces with hat ebm.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 928--940, 2022.

\bibitem[Hinton \& Sejnowski(1983)Hinton and Sejnowski]{hinton1983optimal}
Hinton, G.~E. and Sejnowski, T.~J.
\newblock Optimal perceptual inference.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition}, volume 448, 1983.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Ho, J., Jain, A., and Abbeel, P.
\newblock Denoising diffusion probabilistic models.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  6840--6851, 2020.

\bibitem[Hopfield(1982)]{Hopfield2554}
Hopfield, J.~J.
\newblock Neural networks and physical systems with emergent collective computational abilities.
\newblock \emph{Proceedings of the National Academy of Sciences}, 79\penalty0 (8):\penalty0 2554--2558, 1982.
\newblock ISSN 0027-8424.

\bibitem[Jeffreys(1946)]{jeffreys1946invariant}
Jeffreys, H.
\newblock An invariant form for the prior probability in estimation problems.
\newblock \emph{Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences}, 186\penalty0 (1007):\penalty0 453--461, 1946.

\bibitem[Jolicoeur-Martineau et~al.(2021)Jolicoeur-Martineau, Pich{\'e}-Taillefer, Combes, and Mitliagkas]{jolicoeur2020adversarial}
Jolicoeur-Martineau, A., Pich{\'e}-Taillefer, R., Combes, R. T.~d., and Mitliagkas, I.
\newblock Adversarial score matching and improved sampling for image generation.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Kan et~al.(2022)Kan, L{\"u}, Wang, Zhang, Zhu, Huang, Guo, and Snoussi]{kan2022bi}
Kan, G., L{\"u}, J., Wang, T., Zhang, B., Zhu, A., Huang, L., Guo, G., and Snoussi, H.
\newblock Bi-level doubly variational learning for energy-based latent variable models.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  18460--18469, 2022.

\bibitem[Karras et~al.(2020{\natexlab{a}})Karras, Aittala, Hellsten, Laine, Lehtinen, and Aila]{karras2020training}
Karras, T., Aittala, M., Hellsten, J., Laine, S., Lehtinen, J., and Aila, T.
\newblock Training generative adversarial networks with limited data.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 12104--12114, 2020{\natexlab{a}}.

\bibitem[Karras et~al.(2020{\natexlab{b}})Karras, Laine, Aittala, Hellsten, Lehtinen, and Aila]{karras2020analyzing}
Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., and Aila, T.
\newblock Analyzing and improving the image quality of stylegan.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  8110--8119, 2020{\natexlab{b}}.

\bibitem[Kingma et~al.(2021)Kingma, Salimans, Poole, and Ho]{kingma2021variational}
Kingma, D., Salimans, T., Poole, B., and Ho, J.
\newblock Variational diffusion models.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 21696--21707, 2021.

\bibitem[Kingma \& Dhariwal(2018)Kingma and Dhariwal]{kingma2018glow}
Kingma, D.~P. and Dhariwal, P.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Kong et~al.(2023)Kong, Duan, Sun, Cheng, Xu, Shen, Zhu, Shi, and Xu]{kong2023act}
Kong, F., Duan, J., Sun, L., Cheng, H., Xu, R., Shen, H., Zhu, X., Shi, X., and Xu, K.
\newblock Act: Adversarial consistency models.
\newblock \emph{arXiv preprint arXiv:2311.14097}, 2023.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kumar et~al.(2019)Kumar, Ozair, Goyal, Courville, and Bengio]{kumar2019maximum}
Kumar, R., Ozair, S., Goyal, A., Courville, A., and Bengio, Y.
\newblock Maximum entropy generators for energy-based models.
\newblock \emph{arXiv preprint arXiv:1901.08508}, 2019.

\bibitem[Langley(2000)]{langley00}
Langley, P.
\newblock Crafting papers on machine learning.
\newblock In Langley, P. (ed.), \emph{International Conference on Machine Learning}, pp.\  1207--1216, Stanford, CA, 2000. Morgan Kaufmann.

\bibitem[Lee et~al.(2023)Lee, Jeong, Park, and Shin]{lee2023guiding}
Lee, H., Jeong, J., Park, S., and Shin, J.
\newblock Guiding energy-based models via contrastive latent variables.
\newblock \emph{International Conference on Learning Representations}, 2023.

\bibitem[Lin et~al.(2019)Lin, Chang, Chen, Juan, Wei, and Chen]{lin2019coco}
Lin, C.~H., Chang, C.-C., Chen, Y.-S., Juan, D.-C., Wei, W., and Chen, H.-T.
\newblock Coco-gan: Generation by parts via conditional coordinating.
\newblock In \emph{International Conference on Computer Vision}, pp.\  4512--4521, 2019.

\bibitem[Lin et~al.(2020)Lin, Khetan, Fanti, and Oh]{Lin_Khetan_Fanti_Oh_2020}
Lin, Z., Khetan, A., Fanti, G., and Oh, S.
\newblock Pacgan: The power of two samples in generative adversarial networks.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory}, pp.\  324–335, May 2020.

\bibitem[Liu et~al.(2022)Liu, Gao, Zhang, Meng, and Lin]{Liu_Gao_Zhang_Meng_Lin_2022}
Liu, R., Gao, J., Zhang, J., Meng, D., and Lin, Z.
\newblock Investigating bi-level optimization for learning and vision from a unified perspective: A survey and beyond.
\newblock \emph{IEEE Transaction on Pattern Analysis and Machine Intelligence}, pp.\  10045–10067, 2022.

\bibitem[Liu et~al.(2020)Liu, Wang, Owens, and Li]{liu2020energy}
Liu, W., Wang, X., Owens, J., and Li, Y.
\newblock Energy-based out-of-distribution detection.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 21464--21475, 2020.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015deep}
Liu, Z., Luo, P., Wang, X., and Tang, X.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{International Conference on Computer Vision}, pp.\  3730--3738, 2015.

\bibitem[Lu et~al.(2022{\natexlab{a}})Lu, Zheng, Bao, Chen, Li, and Zhu]{lu2022maximum}
Lu, C., Zheng, K., Bao, F., Chen, J., Li, C., and Zhu, J.
\newblock Maximum likelihood training for score-based diffusion odes by high order denoising score matching.
\newblock In \emph{International Conference on Machine Learning}, pp.\  14429--14460. PMLR, 2022{\natexlab{a}}.

\bibitem[Lu et~al.(2022{\natexlab{b}})Lu, Zhou, Bao, Chen, Li, and Zhu]{lu2022dpm}
Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J.
\newblock Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 5775--5787, 2022{\natexlab{b}}.

\bibitem[Mescheder et~al.(2018)Mescheder, Geiger, and Nowozin]{mescheder2018training}
Mescheder, L., Geiger, A., and Nowozin, S.
\newblock Which training methods for gans do actually converge?
\newblock In \emph{International Conference on Machine Learning}, pp.\  3481--3490. PMLR, 2018.

\bibitem[Misra et~al.(2005)Misra, Singh, and Demchuk]{misra2005estimation}
Misra, N., Singh, H., and Demchuk, E.
\newblock Estimation of the entropy of a multivariate normal distribution.
\newblock \emph{Journal of multivariate analysis}, 92\penalty0 (2):\penalty0 324--342, 2005.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and Yoshida]{miyato2018spectral}
Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y.
\newblock Spectral normalization for generative adversarial networks.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and Ng]{netzer2011reading}
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A.~Y.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Nijkamp et~al.(2019)Nijkamp, Hill, Zhu, and Wu]{nijkamp2019learning}
Nijkamp, E., Hill, M., Zhu, S.-C., and Wu, Y.~N.
\newblock Learning non-convergent non-persistent short-run mcmc toward energy-based model.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Orlitsky(2003)]{ORLITSKY2003751}
Orlitsky, A.
\newblock Information theory.
\newblock In Meyers, R.~A. (ed.), \emph{Encyclopedia of Physical Science and Technology (Third Edition)}, pp.\  751--769. Academic Press, New York, third edition edition, 2003.

\bibitem[Pang et~al.(2020)Pang, Han, Nijkamp, Zhu, and Wu]{pang2020learning}
Pang, B., Han, T., Nijkamp, E., Zhu, S.-C., and Wu, Y.~N.
\newblock Learning latent space energy-based prior model.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 21994--22008, 2020.

\bibitem[Smolensky(1986)]{10.5555/104279.104290}
Smolensky, P.
\newblock \emph{Information Processing in Dynamical Systems: Foundations of Harmony Theory}, pp.\  194–281.
\newblock MIT Press, Cambridge, MA, USA, 1986.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and Ganguli]{sohl2015deep}
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In \emph{International Conference on Machine Learning}, pp.\  2256--2265. PMLR, 2015.

\bibitem[Song et~al.(2021{\natexlab{a}})Song, Meng, and Ermon]{song2020denoising}
Song, J., Meng, C., and Ermon, S.
\newblock Denoising diffusion implicit models.
\newblock \emph{International Conference on Learning Representations}, 2021{\natexlab{a}}.

\bibitem[Song \& Ermon(2019)Song and Ermon]{song2019generative}
Song, Y. and Ermon, S.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Song \& Ermon(2020)Song and Ermon]{song2020improved}
Song, Y. and Ermon, S.
\newblock Improved techniques for training score-based generative models.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 12438--12448, 2020.

\bibitem[Song et~al.(2020)Song, Garg, Shi, and Ermon]{song2020sliced}
Song, Y., Garg, S., Shi, J., and Ermon, S.
\newblock Sliced score matching: A scalable approach to density and score estimation.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  574--584. PMLR, 2020.

\bibitem[Song et~al.(2021{\natexlab{b}})Song, Durkan, Murray, and Ermon]{song2021maximum}
Song, Y., Durkan, C., Murray, I., and Ermon, S.
\newblock Maximum likelihood training of score-based diffusion models.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 1415--1428, 2021{\natexlab{b}}.

\bibitem[Song et~al.(2021{\natexlab{c}})Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and Poole]{song2020score}
Song, Y., Sohl-Dickstein, J., Kingma, D.~P., Kumar, A., Ermon, S., and Poole, B.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock \emph{International Conference on Learning Representations}, 2021{\natexlab{c}}.

\bibitem[Srivastava et~al.(2017)Srivastava, Valkov, Russell, Gutmann, and Sutton]{Srivastava_Valkov_Russell_Gutmann_Sutton_2017}
Srivastava, A., Valkov, L., Russell, C., Gutmann, M., and Sutton, C.
\newblock Veegan: Reducing mode collapse in gans using implicit variational learning.
\newblock \emph{Advances in Neural Information Processing Systems}, Dec 2017.

\bibitem[Vahdat \& Kautz(2020)Vahdat and Kautz]{vahdat2020nvae}
Vahdat, A. and Kautz, J.
\newblock Nvae: A deep hierarchical variational autoencoder.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 19667--19679, 2020.

\bibitem[Vincent(2011)]{Vincent_2011}
Vincent, P.
\newblock A connection between score matching and denoising autoencoders.
\newblock \emph{Neural Computation}, pp.\  1661–1674, Jul 2011.

\bibitem[Xiao et~al.(2021)Xiao, Kreis, Kautz, and Vahdat]{xiao2020vaebm}
Xiao, Z., Kreis, K., Kautz, J., and Vahdat, A.
\newblock Vaebm: A symbiosis between variational autoencoders and energy-based models.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Xiao et~al.(2022)Xiao, Kreis, and Vahdat]{xiao2021tackling}
Xiao, Z., Kreis, K., and Vahdat, A.
\newblock Tackling the generative learning trilemma with denoising diffusion gans.
\newblock \emph{International Conference on Learning Representations}, 2022.

\bibitem[Xie et~al.(2020)Xie, Lu, Gao, Zhu, and Wu]{Xie_Lu_Gao_Zhu_Wu_2020}
Xie, J., Lu, Y., Gao, R., Zhu, S.-C., and Wu, Y.~N.
\newblock Cooperative training of descriptor and generator networks.
\newblock \emph{IEEE Transaction on Pattern Analysis and Machine Intelligence}, pp.\  27–45, Jan 2020.

\bibitem[Xie et~al.(2021)Xie, Zheng, and Li]{xie2021learning}
Xie, J., Zheng, Z., and Li, P.
\newblock Learning energy-based model with variational auto-encoder as amortized sampler.
\newblock In \emph{Association for the Advancement of Artificial Intelligence}, volume~35, pp.\  10441--10451, 2021.

\bibitem[Xie et~al.(2022)Xie, Zhu, Li, and Li]{xie2022tale}
Xie, J., Zhu, Y., Li, J., and Li, P.
\newblock A tale of two flows: Cooperative learning of langevin flow and normalizing flow toward energy-based model.
\newblock \emph{International Conference on Learning Representations}, 2022.

\bibitem[Yu et~al.(2015)Yu, Seff, Zhang, Song, Funkhouser, and Xiao]{yu2015lsun}
Yu, F., Seff, A., Zhang, Y., Song, S., Funkhouser, T., and Xiao, J.
\newblock Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop.
\newblock \emph{arXiv preprint arXiv:1506.03365}, 2015.

\bibitem[Yu et~al.(2023)Yu, Zhu, Xie, Ma, Gao, Zhu, and Wu]{yu2023learning}
Yu, P., Zhu, Y., Xie, S., Ma, X., Gao, R., Zhu, S.-C., and Wu, Y.~N.
\newblock Learning energy-based prior model with diffusion-amortized mcmc.
\newblock \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Zhai et~al.(2016)Zhai, Cheng, Feris, and Zhang]{zhai2016generative}
Zhai, S., Cheng, Y., Feris, R., and Zhang, Z.
\newblock Generative adversarial networks as variational training of energy based models.
\newblock \emph{arXiv preprint arXiv:1611.01799}, 2016.

\bibitem[Zhang et~al.(2022)Zhang, Xie, Zheng, and Barnes]{zhang2022energy}
Zhang, J., Xie, J., Zheng, Z., and Barnes, N.
\newblock Energy-based generative cooperative saliency prediction.
\newblock In \emph{Association for the Advancement of Artificial Intelligence}, volume~36, pp.\  3280--3290, 2022.

\bibitem[Zhang et~al.(2021)Zhang, Goldstein, and Ranganath]{zhang2021understanding}
Zhang, L., Goldstein, M., and Ranganath, R.
\newblock Understanding failures in out-of-distribution detection with deep generative models.
\newblock In \emph{International Conference on Machine Learning}, pp.\  12427--12436. PMLR, 2021.

\bibitem[Zhao et~al.(2017)Zhao, Mathieu, and LeCun]{zhao2016energy}
Zhao, J., Mathieu, M., and LeCun, Y.
\newblock Energy-based generative adversarial network.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[Zhu et~al.(2024)Zhu, Xie, Wu, and Gao]{zhu2023learning}
Zhu, Y., Xie, J., Wu, Y., and Gao, R.
\newblock Learning energy-based models by cooperative diffusion recovery likelihood.
\newblock \emph{International Conference on Learning Representations}, 2024.

\end{thebibliography}
