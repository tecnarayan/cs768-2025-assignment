% attention mechanism

@misc{Neural-Machine-Translation-2014,
  doi = {10.48550/ARXIV.1409.0473},
  
  url = {https://arxiv.org/abs/1409.0473},
  
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{Attention-NIPS-2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@misc{neural-turing-machine-2014,
  doi = {10.48550/ARXIV.1410.5401},
  
  url = {https://arxiv.org/abs/1410.5401},
  
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  
  keywords = {Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Neural Turing Machines},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{attention-based-nmt-2015,
  doi = {10.48550/ARXIV.1508.04025},
  
  url = {https://arxiv.org/abs/1508.04025},
  
  author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Effective Approaches to Attention-based Neural Machine Translation},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{seq2seq-NIPS-2014,
 author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sequence to Sequence Learning with Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf},
 volume = {27},
 year = {2014}
}



%==================================================
% neural operator
% ===============================================
@inproceedings{cao2021galerkin,
 author = {Cao, Shuhao},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {24924--24940},
 publisher = {Curran Associates, Inc.},
 title = {Choose a Transformer: Fourier or Galerkin},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/d0921d442ee91b896ad95059d13df618-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{
li2023transformer,
title={Transformer for Partial Differential Equations{\textquoteright} Operator Learning},
author={Zijie Li and Kazem Meidani and Amir Barati Farimani},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=EPPqt3uERT},
note={}
}

@misc{kissas2022learning,
      title={Learning Operators with Coupled Attention}, 
      author={Georgios Kissas and Jacob Seidman and Leonardo Ferreira Guilhoto and Victor M. Preciado and George J. Pappas and Paris Perdikaris},
      year={2022},
      eprint={2201.01032},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{li2020gno,
      title={Neural Operator: Graph Kernel Network for Partial Differential Equations}, 
      author={Zongyi Li and Nikola Kovachki and Kamyar Azizzadenesheli and Burigede Liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
      year={2020},
      eprint={2003.03485},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


% Hybrid approach
@article{um2020solver,
  title={Solver-in-the-loop: Learning from differentiable physics to interact with iterative pde-solvers},
  author={Um, Kiwon and Brand, Robert and Fei, Yun Raymond and Holl, Philipp and Thuerey, Nils},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6111--6122},
  year={2020}
}

@article{bar2019learning,
  title={Learning data-driven discretizations for partial differential equations},
  author={Bar-Sinai, Yohai and Hoyer, Stephan and Hickey, Jason and Brenner, Michael P},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={31},
  pages={15344--15349},
  year={2019},
  publisher={National Acad Sciences}
}

@article{pathak2020using,
  title={Using machine learning to augment coarse-grid computational fluid dynamics simulations},
  author={Pathak, Jaideep and Mustafa, Mustafa and Kashinath, Karthik and Motheau, Emmanuel and Kurth, Thorsten and Day, Marcus},
  journal={arXiv preprint arXiv:2010.00072},
  year={2020}
}

@inproceedings{greenfeld2019learning,
  title={Learning to optimize multigrid PDE solvers},
  author={Greenfeld, Daniel and Galun, Meirav and Basri, Ronen and Yavneh, Irad and Kimmel, Ron},
  booktitle={International Conference on Machine Learning},
  pages={2415--2423},
  year={2019},
  organization={PMLR}
}

@article{hsieh2019learning,
  title={Learning neural PDE solvers with convergence guarantees},
  author={Hsieh, Jun-Ting and Zhao, Shengjia and Eismann, Stephan and Mirabella, Lucia and Ermon, Stefano},
  journal={arXiv preprint arXiv:1906.01200},
  year={2019}
}

@article{chen2022crom,
  title={CROM: Continuous reduced-order modeling of PDEs using implicit neural representations},
  author={Chen, Peter Yichen and Xiang, Jinxu and Cho, Dong Heon and Chang, Yue and Pershing, GA and Maia, Henrique Teles and Chiaramonte, Maurizio and Carlberg, Kevin and Grinspun, Eitan},
  journal={arXiv preprint arXiv:2206.02607},
  year={2022}
}

% ==================
% end-to-end

% --------------
% surrogate

@article{gupta2022towards,
  title={Towards Multi-spatiotemporal-scale Generalized PDE Modeling},
  author={Gupta, Jayesh K and Brandstetter, Johannes},
  journal={arXiv preprint arXiv:2209.15616},
  year={2022}
}

@article{brandstetter2022message,
  title={Message passing neural PDE solvers},
  author={Brandstetter, Johannes and Worrall, Daniel and Welling, Max},
  journal={arXiv preprint arXiv:2202.03376},
  year={2022}
}

@misc{sanchezgonzalez2020learning,
      title={Learning to Simulate Complex Physics with Graph Networks}, 
      author={Alvaro Sanchez-Gonzalez and Jonathan Godwin and Tobias Pfaff and Rex Ying and Jure Leskovec and Peter W. Battaglia},
      year={2020},
      eprint={2002.09405},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{pfaff2021learning,
      title={Learning Mesh-Based Simulation with Graph Networks}, 
      author={Tobias Pfaff and Meire Fortunato and Alvaro Sanchez-Gonzalez and Peter W. Battaglia},
      year={2021},
      eprint={2010.03409},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{shu2023physics,
  title={A physics-informed diffusion model for high-fidelity flow field reconstruction},
  author={Shu, Dule and Li, Zijie and Farimani, Amir Barati},
  journal={Journal of Computational Physics},
  volume={478},
  pages={111972},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{
Ummenhofer2020Lagrangian,
title={Lagrangian Fluid Simulation with Continuous Convolutions},
author={Benjamin Ummenhofer and Lukas Prantl and Nils Thuerey and Vladlen Koltun},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=B1lDoJSYDH}
}

@misc{prantl2022guaranteed,
      title={Guaranteed Conservation of Momentum for Learning Particle-based Fluid Dynamics}, 
      author={Lukas Prantl and Benjamin Ummenhofer and Vladlen Koltun and Nils Thuerey},
      year={2022},
      eprint={2210.06036},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

%----------------------
% transformer for phisycs simulation
@article{cachayclimformer,
  title={ClimFormer--A Spherical Transformer Model for Long-term Climate Projections},
  author={Cachay, Salva R{\"u}hling and Mitra, Peetak and Hirasawa, Haruki and Kim, Sookyung and Hazarika, Subhashis and Hingmire, Dipti and Rasch, Phil and Singh, Hansi and Ramea, Kalai}
}

@article{liu2022ht,
  title={HT-Net: Hierarchical Transformer based Operator Learning Model for Multiscale PDEs},
  author={Liu, Xinliang and Xu, Bo and Zhang, Lei},
  journal={arXiv preprint arXiv:2210.10890},
  year={2022}
}

@article{ovadia2023vito,
  title={ViTO: Vision Transformer-Operator},
  author={Ovadia, Oded and Kahana, Adar and Stinis, Panos and Turkel, Eli and Karniadakis, George Em},
  journal={arXiv preprint arXiv:2303.08891},
  year={2023}
}

@article{nguyen2022fourierformer,
  title={Fourierformer: Transformer meets generalized fourier integral theorem},
  author={Nguyen, Tan and Pham, Minh and Nguyen, Tam and Nguyen, Khai and Osher, Stanley and Ho, Nhat},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={29319--29335},
  year={2022}
}

@article{han2022predicting,
  title={Predicting physics in mesh-reduced space with temporal attention},
  author={Han, Xu and Gao, Han and Pfaff, Tobias and Wang, Jian-Xun and Liu, Li-Ping},
  journal={arXiv preprint arXiv:2201.09113},
  year={2022}
}

@article{fonseca2023continuous,
  title={Continuous Spatiotemporal Transformers},
  author={Fonseca, Antonio H de O and Zappala, Emanuele and Caro, Josue Ortega and van Dijk, David},
  journal={arXiv preprint arXiv:2301.13338},
  year={2023}
}






@article{li2022fgn,
title = {Graph neural network-accelerated Lagrangian fluid simulation},
journal = {Computers \& Graphics},
volume = {103},
pages = {201-211},
year = {2022},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2022.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0097849322000206},
author = {Zijie Li and Amir Barati Farimani},
keywords = {Particle-based fluid dynamics, Graph neural networks, Data-driven modeling},
abstract = {We present a data-driven model for fluid simulation under Lagrangian representation. Our model, Fluid Graph Networks (FGN), uses graphs to represent the fluid field. In FGN, fluid particles are represented as nodes and their interactions are represented as edges. Instead of directly predicting the acceleration or position correction given the current state, FGN decomposes the simulation scheme into separate parts — advection, collision, and pressure projection. For these different predictions tasks, we propose two kinds of graph neural network structures, node-focused networks and edge-focused networks. We show that the learned model can produce accurate results and remain stable in scenarios with different geometries. In addition, FGN is able to retain many important physical properties of incompressible fluids, such as low velocity divergence, and adapt to time step sizes beyond the one used in the training set. FGN is also computationally efficient compared to classical simulation methods as it operates on a smaller neighborhood and does not require iteration at each timestep during the inference.}
}

@inproceedings{
lotzsch2022learning,
title={Learning the Solution Operator of Boundary Value Problems using Graph Neural Networks},
author={Winfried L{\"o}tzsch and Simon Ohler and Johannes Otterbach},
booktitle={ICML 2022 2nd AI for Science Workshop},
year={2022},
url={https://openreview.net/forum?id=4vx9FQA7wiC}
}

@misc{bhattacharya2021pcanet,
      title={Model Reduction and Neural Networks for Parametric PDEs}, 
      author={Kaushik Bhattacharya and Bamdad Hosseini and Nikola B. Kovachki and Andrew M. Stuart},
      year={2021},
      eprint={2005.03180},
      archivePrefix={arXiv},
      primaryClass={math.NA}
}

@article{Pant2021dlrom,
	doi = {10.1063/5.0062546},
  
	url = {https://doi.org/10.1063%2F5.0062546},
  
	year = 2021,
	month = {oct},
  
	publisher = {{AIP} Publishing},
  
	volume = {33},
  
	number = {10},
  
	pages = {107101},
  
	author = {Pranshu Pant and Ruchit Doshi and Pranav Bahl and Amir Barati Farimani},
  
	title = {Deep learning for reduced order modelling and efficient temporal evolution of fluid simulations},
  
	journal = {Physics of Fluids}
}}

@inproceedings{fluidunet-kdd2020,
author = {Wang, Rui and Kashinath, Karthik and Mustafa, Mustafa and Albert, Adrian and Yu, Rose},
title = {Towards Physics-Informed Deep Learning for Turbulent Flow Prediction},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403198},
doi = {10.1145/3394486.3403198},
abstract = {While deep learning has shown tremendous success in a wide range of domains, it remains a grand challenge to incorporate physical principles in a systematic manner to the design, training, and inference of such models. In this paper, we aim to predict turbulent flow by learning its highly nonlinear dynamics from spatiotemporal velocity fields of large-scale fluid flow simulations of relevance to turbulence modeling and climate modeling. We adopt a hybrid approach by marrying two well-established turbulent flow simulation techniques with deep learning. Specifically, we introduce trainable spectral filters in a coupled model of Reynolds-averaged Navier-Stokes (RANS) and Large Eddy Simulation (LES), followed by a specialized U-net for prediction. Our approach, which we call Turbulent-Flow Net, is grounded in a principled physics model, yet offers the flexibility of learned representations. We compare our model with state-of-the-art baselines and observe significant reductions in error for predictions 60 frames ahead. Most importantly, our method predicts physical fields that obey desirable physical characteristics, such as conservation of mass, whilst faithfully emulating the turbulent kinetic energy field and spectrum, which are critical for accurate prediction of turbulent flows.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {1457–1466},
numpages = {10},
keywords = {physics-informed machine learning, deep learning, turbulent flows, spatiotemporal forecasting, video forward prediction},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

% weather
@misc{lam2022graphcast,
      title={GraphCast: Learning skillful medium-range global weather forecasting}, 
      author={Remi Lam and Alvaro Sanchez-Gonzalez and Matthew Willson and Peter Wirnsberger and Meire Fortunato and Alexander Pritzel and Suman Ravuri and Timo Ewalds and Ferran Alet and Zach Eaton-Rosen and Weihua Hu and Alexander Merose and Stephan Hoyer and George Holland and Jacklynn Stott and Oriol Vinyals and Shakir Mohamed and Peter Battaglia},
      year={2022},
      eprint={2212.12794},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{nguyen2023climax,
      title={ClimaX: A foundation model for weather and climate}, 
      author={Tung Nguyen and Johannes Brandstetter and Ashish Kapoor and Jayesh K. Gupta and Aditya Grover},
      year={2023},
      eprint={2301.10343},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{pathak2022fourcastnet,
      title={FourCastNet: A Global Data-driven High-resolution Weather Model using Adaptive Fourier Neural Operators}, 
      author={Jaideep Pathak and Shashank Subramanian and Peter Harrington and Sanjeev Raja and Ashesh Chattopadhyay and Morteza Mardani and Thorsten Kurth and David Hall and Zongyi Li and Kamyar Azizzadenesheli and Pedram Hassanzadeh and Karthik Kashinath and Animashree Anandkumar},
      year={2022},
      eprint={2202.11214},
      archivePrefix={arXiv},
      primaryClass={physics.ao-ph}
}

@article{Rasp2020weatherbench,
	doi = {10.1029/2020ms002203},
  
	url = {https://doi.org/10.1029%2F2020ms002203},
  
	year = 2020,
	month = {nov},
  
	publisher = {American Geophysical Union ({AGU})},
  
	volume = {12},
  
	number = {11},
  
	author = {Stephan Rasp and Peter D. Dueben and Sebastian Scher and Jonathan A. Weyn and Soukayna Mouatadid and Nils Thuerey},
  
	title = {{WeatherBench}: A Benchmark Data Set for Data-Driven Weather Forecasting},
  
	journal = {Journal of Advances in Modeling Earth Systems}
}

@misc{li2019dpi,
      title={Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable Objects, and Fluids}, 
      author={Yunzhu Li and Jiajun Wu and Russ Tedrake and Joshua B. Tenenbaum and Antonio Torralba},
      year={2019},
      eprint={1810.01566},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{thuerey2020deep,
  title={Deep learning methods for Reynolds-averaged Navier--Stokes simulations of airfoil flows},
  author={Thuerey, Nils and Wei{\ss}enow, Konstantin and Prantl, Lukas and Hu, Xiangyu},
  journal={AIAA Journal},
  volume={58},
  number={1},
  pages={25--36},
  year={2020},
  publisher={American Institute of Aeronautics and Astronautics}
}

% --------------
% pinn

@article{karniadakis2021physics,
  title={Physics-informed machine learning},
  author={Karniadakis, George Em and Kevrekidis, Ioannis G and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  journal={Nature Reviews Physics},
  volume={3},
  number={6},
  pages={422--440},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{raissi2019physics,
  title={Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},
  journal={Journal of Computational physics},
  volume={378},
  pages={686--707},
  year={2019},
  publisher={Elsevier}
}

@article{zhu2023reliable,
  title={Reliable extrapolation of deep neural operators informed by physics or sparse observations},
  author={Zhu, Min and Zhang, Handi and Jiao, Anran and Karniadakis, George Em and Lu, Lu},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={412},
  pages={116064},
  year={2023},
  publisher={Elsevier}
}

@article{pang2019fpinns,
  title={fPINNs: Fractional physics-informed neural networks},
  author={Pang, Guofei and Lu, Lu and Karniadakis, George Em},
  journal={SIAM Journal on Scientific Computing},
  volume={41},
  number={4},
  pages={A2603--A2626},
  year={2019},
  publisher={SIAM}
}


@article{cai2021physics,
  title={Physics-informed neural networks (PINNs) for fluid mechanics: A review},
  author={Cai, Shengze and Mao, Zhiping and Wang, Zhicheng and Yin, Minglang and Karniadakis, George Em},
  journal={Acta Mechanica Sinica},
  volume={37},
  number={12},
  pages={1727--1738},
  year={2021},
  publisher={Springer}
}

@article{lu2021deepxde,
  title={DeepXDE: A deep learning library for solving differential equations},
  author={Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George Em},
  journal={SIAM review},
  volume={63},
  number={1},
  pages={208--228},
  year={2021},
  publisher={SIAM}
}

@article{sun2020surrogate,
  title={Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data},
  author={Sun, Luning and Gao, Han and Pan, Shaowu and Wang, Jian-Xun},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={361},
  pages={112732},
  year={2020},
  publisher={Elsevier}
}

@article{han2018solving,
  title={Solving high-dimensional partial differential equations using deep learning},
  author={Han, Jiequn and Jentzen, Arnulf and E, Weinan},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={34},
  pages={8505--8510},
  year={2018},
  publisher={National Acad Sciences}
}

@article{
pi-deeponet,
author = {Sifan Wang  and Hanwen Wang  and Paris Perdikaris },
title = {Learning the solution operator of parametric partial differential equations with physics-informed DeepONets},
journal = {Science Advances},
volume = {7},
number = {40},
pages = {eabi8605},
year = {2021},
doi = {10.1126/sciadv.abi8605},
URL = {https://www.science.org/doi/abs/10.1126/sciadv.abi8605},
eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.abi8605},
abstract = {Enabling the rapid emulation of parametric differential equations with physics-informed deep operator networks. Partial differential equations (PDEs) play a central role in the mathematical analysis and modeling of complex dynamic processes across all corners of science and engineering. Their solution often requires laborious analytical or computational tools, associated with a cost that is markedly amplified when different scenarios need to be investigated, for example, corresponding to different initial or boundary conditions, different inputs, etc. In this work, we introduce physics-informed DeepONets, a deep learning framework for learning the solution operator of arbitrary PDEs, even in the absence of any paired input-output training data. We illustrate the effectiveness of the proposed framework in rapidly predicting the solution of various types of parametric PDEs up to three orders of magnitude faster compared to conventional PDE solvers, setting a previously unexplored paradigm for modeling and simulation of nonlinear and nonequilibrium processes in science and engineering.}}




% --------------
% neural operators
@article{kovachki2021neural,
  title={Neural operator: Learning maps between function spaces},
  author={Kovachki, Nikola and Li, Zongyi and Liu, Burigede and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2108.08481},
  year={2021}
}

@article{brandstetter2022clifford,
  title={Clifford neural layers for PDE modeling},
  author={Brandstetter, Johannes and Berg, Rianne van den and Welling, Max and Gupta, Jayesh K},
  journal={arXiv preprint arXiv:2209.04934},
  year={2022}
}

@InProceedings{brandstetter2022lipoint,
  title = 	 {Lie Point Symmetry Data Augmentation for Neural {PDE} Solvers},
  author =       {Brandstetter, Johannes and Welling, Max and Worrall, Daniel E},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {2241--2256},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/brandstetter22a/brandstetter22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/brandstetter22a.html},
  abstract = 	 {Neural networks are increasingly being used to solve partial differential equations (PDEs), replacing slower numerical solvers. However, a critical issue is that neural PDE solvers require high-quality ground truth data, which usually must come from the very solvers they are designed to replace. Thus, we are presented with a proverbial chicken-and-egg problem. In this paper, we present a method, which can partially alleviate this problem, by improving neural PDE solver sample complexity—Lie point symmetry data augmentation (LPSDA). In the context of PDEs, it turns out we are able to quantitatively derive an exhaustive list of data transformations, based on the Lie point symmetry group of the PDEs in question, something not possible in other application areas. We present this framework and demonstrate how it can easily be deployed to improve neural PDE solver sample complexity by an order of magnitude.}
}


@article{li2020neural,
  title={Neural operator: Graph kernel network for partial differential equations},
  author={Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2003.03485},
  year={2020}
}

@article{lu2019deeponet,
  title={Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators},
  author={Lu, Lu and Jin, Pengzhan and Karniadakis, George Em},
  journal={arXiv preprint arXiv:1910.03193},
  year={2019}
}

@article{jin2022mionet,
  title={MIONet: Learning multiple-input operators via tensor product},
  author={Jin, Pengzhan and Meng, Shuai and Lu, Lu},
  journal={SIAM Journal on Scientific Computing},
  volume={44},
  number={6},
  pages={A3490--A3514},
  year={2022},
  publisher={SIAM}
}

@article{li2020fourier,
  title={Fourier neural operator for parametric partial differential equations},
  author={Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2010.08895},
  year={2020}
}

@misc{li2020multipole,
      title={Multipole Graph Neural Operator for Parametric Partial Differential Equations}, 
      author={Zongyi Li and Nikola Kovachki and Kamyar Azizzadenesheli and Burigede Liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
      year={2020},
      eprint={2006.09535},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{wen2022ufno,
  title={U-FNO—An enhanced Fourier neural operator-based deep-learning model for multiphase flow},
  author={Wen, Gege and Li, Zongyi and Azizzadenesheli, Kamyar and Anandkumar, Anima and Benson, Sally M},
  journal={Advances in Water Resources},
  volume={163},
  pages={104180},
  year={2022},
  publisher={Elsevier}
}

@article{guibas2021adaptive,
  title={Adaptive fourier neural operators: Efficient token mixers for transformers},
  author={Guibas, John and Mardani, Morteza and Li, Zongyi and Tao, Andrew and Anandkumar, Anima and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2111.13587},
  year={2021}
}

@misc{tran2023factfno,
      title={Factorized Fourier Neural Operators}, 
      author={Alasdair Tran and Alexander Mathews and Lexing Xie and Cheng Soon Ong},
      year={2023},
      eprint={2111.13802},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{gupta2021multiwaveletbased,
      title={Multiwavelet-based Operator Learning for Differential Equations}, 
      author={Gaurav Gupta and Xiongye Xiao and Paul Bogdan},
      year={2021},
      eprint={2109.13459},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@ARTICLE{Universal-apprx-operator-IEEE-1995,
  author={Tianping Chen and Hong Chen},
  journal={IEEE Transactions on Neural Networks}, 
  title={Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems}, 
  year={1995},
  volume={6},
  number={4},
  pages={911-917},
  doi={10.1109/72.392253}
  }

  @article{Lu2022fair,
	doi = {10.1016/j.cma.2022.114778},
  
	url = {https://doi.org/10.1016%2Fj.cma.2022.114778},
  
	year = 2022,
	month = {apr},
  
	publisher = {Elsevier {BV}
},
  
	volume = {393},
  
	pages = {114778},
  
	author = {Lu Lu and Xuhui Meng and Shengze Cai and Zhiping Mao and Somdatta Goswami and Zhongqiang Zhang and George Em Karniadakis},
  
	title = {A comprehensive and fair comparison of two neural operators (with practical extensions) based on {FAIR} data},
  
	journal = {Computer Methods in Applied Mechanics and Engineering}
}


% ======================================
% transformer
@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}


@misc{su2022roformer,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
      author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
      year={2022},
      eprint={2104.09864},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{rae2019compressive,
  title={Compressive transformers for long-range sequence modelling},
  author={Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Lillicrap, Timothy P},
  journal={arXiv preprint arXiv:1911.05507},
  year={2019}
}

@article{wu2020transformer,
  title={Da-transformer: Distance-aware transformer},
  author={Wu, Chuhan and Wu, Fangzhao and Huang, Yongfeng},
  journal={arXiv preprint arXiv:2010.06925},
  year={2020}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{arnab2021vivit,
  title={Vivit: A video vision transformer},
  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\v{c}}i{\'c}, Mario and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6836--6846},
  year={2021}
}

@inproceedings{li2019neural,
  title={Neural speech synthesis with transformer network},
  author={Li, Naihan and Liu, Shujie and Liu, Yanqing and Zhao, Sheng and Liu, Ming},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={6706--6713},
  year={2019}
}

@article{bagal2021molgpt,
  title={MolGPT: molecular generation using a transformer-decoder model},
  author={Bagal, Viraj and Aggarwal, Rishal and Vinod, PK and Priyakumar, U Deva},
  journal={Journal of Chemical Information and Modeling},
  volume={62},
  number={9},
  pages={2064--2076},
  year={2021},
  publisher={ACS Publications}
}

@article{chen2021decision,
  title={Decision transformer: Reinforcement learning via sequence modeling},
  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={15084--15097},
  year={2021}
}

@article{wright2021transformers,
  title={Transformers are deep infinite-dimensional non-mercer binary kernel machines},
  author={Wright, Matthew A and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2106.01506},
  year={2021}
}


@misc{choromanski2022rethinking,
      title={Rethinking Attention with Performers}, 
      author={Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy Colwell and Adrian Weller},
      year={2022},
      eprint={2009.14794},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{cao2022understand,
      title={How to Understand Masked Autoencoders}, 
      author={Shuhao Cao and Peng Xu and David A. Clifton},
      year={2022},
      eprint={2202.03670},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{tsai2019transformer,
      title={Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel}, 
      author={Yao-Hung Hubert Tsai and Shaojie Bai and Makoto Yamada and Louis-Philippe Morency and Ruslan Salakhutdinov},
      year={2019},
      eprint={1908.11775},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{guo2022transformer,
  title={Transformer meets boundary value inverse problems},
  author={Guo, Ruchi and Cao, Shuhao and Chen, Long},
  journal={arXiv preprint arXiv:2209.14977},
  year={2022}
}


@article{geneva2022transformers,
  title={Transformers for modeling physical systems},
  author={Geneva, Nicholas and Zabaras, Nicholas},
  journal={Neural Networks},
  volume={146},
  pages={272--289},
  year={2022},
  publisher={Elsevier}
}


@article{gao2022earthformer,
  title={Earthformer: Exploring space-time transformers for earth system forecasting},
  author={Gao, Zhihan and Shi, Xingjian and Wang, Hao and Zhu, Yi and Wang, Yuyang Bernie and Li, Mu and Yeung, Dit-Yan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25390--25403},
  year={2022}
}

@inproceedings{chattopadhyay2020deep,
  title={Deep spatial transformers for autoregressive data-driven forecasting of geophysical turbulence},
  author={Chattopadhyay, Ashesh and Mustafa, Mustafa and Hassanzadeh, Pedram and Kashinath, Karthik},
  booktitle={Proceedings of the 10th international conference on climate informatics},
  pages={106--112},
  year={2020}
}

@article{hao2023gnot,
  title={GNOT: A General Neural Operator Transformer for Operator Learning},
  author={Hao, Zhongkai and Ying, Chengyang and Wang, Zhengyi and Su, Hang and Dong, Yinpeng and Liu, Songming and Cheng, Ze and Zhu, Jun and Song, Jian},
  journal={arXiv preprint arXiv:2302.14376},
  year={2023}
}

@article{boulle2022learning,
  title={Learning Green’s functions associated with time-dependent partial differential equations},
  author={Boull{\'e}, Nicolas and Kim, Seick and Shi, Tianyi and Townsend, Alex},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={218},
  pages={1--34},
  year={2022}
}

@article{tang2022neural,
  title={Neural Green’s function for Laplacian systems},
  author={Tang, Jingwei and Azevedo, Vinicius C and Cordonnier, Guillaume and Solenthaler, Barbara},
  journal={Computers \& Graphics},
  volume={107},
  pages={186--196},
  year={2022},
  publisher={Elsevier}
}



%----------------


@article{tensor-decomp-review,
author = {Kolda, Tamara G. and Bader, Brett W.},
title = {Tensor Decompositions and Applications},
year = {2009},
issue_date = {August 2009},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
volume = {51},
number = {3},
issn = {0036-1445},
url = {https://doi.org/10.1137/07070111X},
doi = {10.1137/07070111X},
abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or $N$-way array. Decompositions of higher-order tensors (i.e., $N$-way arrays with $N geq 3$) have applications in psycho-metrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.},
journal = {SIAM Rev.},
month = {aug},
pages = {455–500},
numpages = {46},
keywords = {parallel factors (PARAFAC), multiway arrays, multilinear algebra, tensor decompositions, higher-order singular value decomposition (HOSVD), canonical decomposition (CANDECOMP), higher-order principal components analysis (Tucker)}
}

@misc{
ho2020axial,
title={Axial Attention in Multidimensional Transformers},
author={Jonathan Ho and Nal Kalchbrenner and Dirk Weissenborn and Tim Salimans},
year={2020},
url={https://openreview.net/forum?id=H1e5GJBtDr}
}

@misc{ulyanov2017instance,
      title={Instance Normalization: The Missing Ingredient for Fast Stylization}, 
      author={Dmitry Ulyanov and Andrea Vedaldi and Victor Lempitsky},
      year={2017},
      eprint={1607.08022},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{brandstetter2023message,
      title={Message Passing Neural PDE Solvers}, 
      author={Johannes Brandstetter and Daniel Worrall and Max Welling},
      year={2023},
      eprint={2202.03376},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{method-lines-1991,
  title={The Numerical Method of Lines: Integration of Partial Differential Equations},
  author={William E. Schiesser},
  year={1991}
}


@misc{wang2021learning,
      title={Learning the solution operator of parametric partial differential equations with physics-informed DeepOnets}, 
      author={Sifan Wang and Hanwen Wang and Paris Perdikaris},
      year={2021},
      eprint={2103.10974},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{li2023physicsinformed,
      title={Physics-Informed Neural Operator for Learning Partial Differential Equations}, 
      author={Zongyi Li and Hongkai Zheng and Nikola Kovachki and David Jin and Haoxuan Chen and Burigede Liu and Kamyar Azizzadenesheli and Anima Anandkumar},
      year={2023},
      eprint={2111.03794},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{wang2022respecting,
      title={Respecting causality is all you need for training physics-informed neural networks}, 
      author={Sifan Wang and Shyam Sankaran and Paris Perdikaris},
      year={2022},
      eprint={2203.07404},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{stachenfeld2022learned,
      title={Learned Coarse Models for Efficient Turbulence Simulation}, 
      author={Kimberly Stachenfeld and Drummond B. Fielding and Dmitrii Kochkov and Miles Cranmer and Tobias Pfaff and Jonathan Godwin and Can Cui and Shirley Ho and Peter Battaglia and Alvaro Sanchez-Gonzalez},
      year={2022},
      eprint={2112.15275},
      archivePrefix={arXiv},
      primaryClass={physics.flu-dyn}
}

@misc{yu2016multiscale,
      title={Multi-Scale Context Aggregation by Dilated Convolutions}, 
      author={Fisher Yu and Vladlen Koltun},
      year={2016},
      eprint={1511.07122},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{He2016Residual,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@misc{gupta2022multispatiotemporalscale,
      title={Towards Multi-spatiotemporal-scale Generalized PDE Modeling}, 
      author={Jayesh K. Gupta and Johannes Brandstetter},
      year={2022},
      eprint={2209.15616},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Kochkov2021mlcfd,
	doi = {10.1073/pnas.2101784118},
  
	url = {https://doi.org/10.1073%2Fpnas.2101784118},
  
	year = 2021,
	month = {may},
  
	publisher = {Proceedings of the National Academy of Sciences},
  
	volume = {118},
  
	number = {21},
  
	author = {Dmitrii Kochkov and Jamie A. Smith and Ayya Alieva and Qing Wang and Michael P. Brenner and Stephan Hoyer},
  
	title = {Machine learning{\textendash}accelerated computational fluid dynamics},
  
	journal = {Proceedings of the National Academy of Sciences}
}

@article{fukami2019superres,
title={Super-resolution reconstruction of turbulent flows with machine learning}, volume={870}, DOI={10.1017/jfm.2019.238}, journal={Journal of Fluid Mechanics}, publisher={Cambridge University Press}, author={Fukami, Kai and Fukagata, Koji and Taira, Kunihiko}, year={2019}, pages={106–120}}

@inproceedings{
li2022tpugan,
title={{TPU}-{GAN}: Learning temporal coherence from dynamic point cloud sequences},
author={Zijie Li and Tianqin Li and Amir Barati Farimani},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=FEBFJ98FKx}
}

@article{Chu2017tempoGAN,
	doi = {10.1145/3072959.3073643},
  
	url = {https://doi.org/10.1145%2F3072959.3073643},
  
	year = 2017,
	month = {jul},
  
	publisher = {Association for Computing Machinery ({ACM})},
  
	volume = {36},
  
	number = {4},
  
	pages = {1--14},
  
	author = {Mengyu Chu and Nils Thuerey},
  
	title = {Data-driven synthesis of smoke flows with {CNN}-based feature descriptors},
  
	journal = {{ACM} Transactions on Graphics}
}

@INPROCEEDINGS{meshfreeflownet,
  author={Jiang, Chiyu “Max” and Esmaeilzadeh, Soheil and Azizzadenesheli, Kamyar and Kashinath, Karthik and Mustafa, Mustafa and Tchelepi, Hamdi A. and Marcus, Philip and Prabhat, Mr and Anandkumar, Anima},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis}, 
  title={MESHFREEFLOWNET: A Physics-Constrained Deep Continuous Space-Time Super-Resolution Framework}, 
  year={2020},
  volume={},
  number={},
  pages={1-15},
  doi={10.1109/SC41405.2020.00013}}

@article{Mortensen2016spectralDNS,
	doi = {10.1016/j.cpc.2016.02.005},
  
	url = {https://doi.org/10.1016%2Fj.cpc.2016.02.005},
  
	year = 2016,
	month = {jun},
  
	publisher = {Elsevier {BV}
},
  
	volume = {203},
  
	pages = {53--65},
  
	author = {Mikael Mortensen and Hans Petter Langtangen},
  
	title = {High performance Python for direct numerical simulations of turbulent flows},
  
	journal = {Computer Physics Communications}
}

@article{3dturb-dns,
    author = {Lamorgese, A. G. and Caughey, D. A. and Pope, S. B.},
    title = "{Direct numerical simulation of homogeneous turbulence with hyperviscosity}",
    journal = {Physics of Fluids},
    volume = {17},
    number = {1},
    year = {2004},
    month = {12},
    abstract = "{We perform direct numerical simulations (DNS) of the hyperviscous Navier–Stokes equations in a periodic box. We consider values of the hyperviscosity index h=1, 2, 8, and vary the hyperviscosity to obtain the largest range of lengthscale ratios possible for well-resolved pseudo-spectral DNS. It is found that the spectral bump, or bottleneck, in the energy spectrum observed at the start of the dissipation range becomes more pronounced as the hyperviscosity index is increased. The calculated energy spectra are used to develop an empirical model for the dissipation range which accurately represents the bottleneck. This model is used to predict the approach of the turbulent kinetic energy k to its asymptotic value, k∞, as the hyperviscosity tends to zero.}",
    issn = {1070-6631},
    doi = {10.1063/1.1833415},
    url = {https://doi.org/10.1063/1.1833415},
    note = {015106},
    eprint = {https://pubs.aip.org/aip/pof/article-pdf/doi/10.1063/1.1833415/14692925/015106\_1\_online.pdf},
}

@misc{brandstetter2023clifford,
      title={Clifford Neural Layers for PDE Modeling}, 
      author={Johannes Brandstetter and Rianne van den Berg and Max Welling and Jayesh K. Gupta},
      year={2023},
      eprint={2209.04934},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{dresdner2022correctspectral,
      title={Learning to correct spectral methods for simulating turbulent flows}, 
      author={Gideon Dresdner and Dmitrii Kochkov and Peter Norgaard and Leonardo Zepeda-Núñez and Jamie A. Smith and Michael P. Brenner and Stephan Hoyer},
      year={2022},
      eprint={2207.00556},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{holl2020phiflow,
  title={phiflow: A differentiable pde solving framework for deep learning via physical simulations},
  author={Holl, Philipp and Koltun, Vladlen and Um, Kiwon}
}


% low rank property of attention matrix

@misc{dong2021attention,
      title={Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth}, 
      author={Yihe Dong and Jean-Baptiste Cordonnier and Andreas Loukas},
      year={2021},
      eprint={2103.03404},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{bhojanapalli2021eigen,
      title={Eigen Analysis of Self-Attention and its Reconstruction from Partial Computation}, 
      author={Srinadh Bhojanapalli and Ayan Chakrabarti and Himanshu Jain and Sanjiv Kumar and Michal Lukasik and Andreas Veit},
      year={2021},
      eprint={2106.08823},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{halko2010finding,
      title={Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions}, 
      author={Nathan Halko and Per-Gunnar Martinsson and Joel A. Tropp},
      year={2010},
      eprint={0909.4061},
      archivePrefix={arXiv},
      primaryClass={math.NA}
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@misc{carion2020detr,
      title={End-to-End Object Detection with Transformers}, 
      author={Nicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko},
      year={2020},
      eprint={2005.12872},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{
janny2023eagle,
title={{EAGLE}: Large-scale Learning of Turbulent Fluid Dynamics with Mesh Transformers},
author={Steeven JANNY and Aur{\'e}lien B{\'e}n{\'e}teau and Madiha Nadri and Julie Digne and Nicolas THOME and Christian Wolf},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=mfIX4QpsARJ}
}

% linear attention

@InProceedings{transformer-rnn,
  title = 	 {Transformers are {RNN}s: Fast Autoregressive Transformers with Linear Attention},
  author =       {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {5156--5165},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/katharopoulos20a/katharopoulos20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/katharopoulos20a.html},
  abstract = 	 {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input’s length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\bigO{N^2}$ to $\bigO{N}$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our \emph{Linear Transformers} achieve similar performance to vanilla Transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.}
}

@misc{shen2020efficient,
      title={Efficient Attention: Attention with Linear Complexities}, 
      author={Zhuoran Shen and Mingyuan Zhang and Haiyu Zhao and Shuai Yi and Hongsheng Li},
      year={2020},
      eprint={1812.01243},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{
Kitaev2020Reformer,
title={Reformer: The Efficient Transformer},
author={Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgNKkHtvB}
}

@misc{xiong2021nystromformer,
      title={Nystr\"omformer: A Nystr\"om-Based Algorithm for Approximating Self-Attention}, 
      author={Yunyang Xiong and Zhanpeng Zeng and Rudrasis Chakraborty and Mingxing Tan and Glenn Fung and Yin Li and Vikas Singh},
      year={2021},
      eprint={2102.03902},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zaheer2021bigbird,
      title={Big Bird: Transformers for Longer Sequences}, 
      author={Manzil Zaheer and Guru Guruganesh and Avinava Dubey and Joshua Ainslie and Chris Alberti and Santiago Ontanon and Philip Pham and Anirudh Ravula and Qifan Wang and Li Yang and Amr Ahmed},
      year={2021},
      eprint={2007.14062},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{child2019generating,
      title={Generating Long Sequences with Sparse Transformers}, 
      author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
      year={2019},
      eprint={1904.10509},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{beltagy2020longformer,
      title={Longformer: The Long-Document Transformer}, 
      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2020},
      eprint={2004.05150},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ma2019tensorized,
      title={A Tensorized Transformer for Language Modeling}, 
      author={Xindian Ma and Peng Zhang and Shuai Zhang and Nan Duan and Yuexian Hou and Dawei Song and Ming Zhou},
      year={2019},
      eprint={1906.09777},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{tensor-train-decomposition,
author = {Oseledets, I. V.},
title = {Tensor-Train Decomposition},
journal = {SIAM Journal on Scientific Computing},
volume = {33},
number = {5},
pages = {2295-2317},
year = {2011},
doi = {10.1137/090752286},

URL = { 
    
        https://doi.org/10.1137/090752286
    
    

},
eprint = { 
    
        https://doi.org/10.1137/090752286
    
    

}
,
    abstract = { A simple nonrecursive form of the tensor decomposition in d dimensions is presented. It does not inherently suffer from the curse of dimensionality, it has asymptotically the same number of parameters as the canonical decomposition, but it is stable and its computation is based on low-rank approximation of auxiliary unfolding matrices. The new form gives a clear and convenient way to implement all basic operations efficiently. A fast rounding procedure is presented, as well as basic linear algebra operations. Examples showing the benefits of the decomposition are given, and the efficiency is demonstrated by the computation of the smallest eigenvalue of a 19-dimensional operator. }
}

@misc{novikov2015tensorizing,
      title={Tensorizing Neural Networks}, 
      author={Alexander Novikov and Dmitry Podoprikhin and Anton Osokin and Dmitry Vetrov},
      year={2015},
      eprint={1509.06569},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{yang2017tensorrnn,
      title={Tensor-Train Recurrent Neural Networks for Video Classification}, 
      author={Yinchong Yang and Denis Krompass and Volker Tresp},
      year={2017},
      eprint={1707.01786},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{lebedev2015speedingup,
      title={Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition}, 
      author={Vadim Lebedev and Yaroslav Ganin and Maksim Rakhuba and Ivan Oseledets and Victor Lempitsky},
      year={2015},
      eprint={1412.6553},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


% for appendix
@inproceedings{rff2007,
 author = {Rahimi, Ali and Recht, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Random Features for Large-Scale Kernel Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
 volume = {20},
 year = {2007}
}

@misc{tancik2020fourier,
      title={Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains}, 
      author={Matthew Tancik and Pratul P. Srinivasan and Ben Mildenhall and Sara Fridovich-Keil and Nithin Raghavan and Utkarsh Singhal and Ravi Ramamoorthi and Jonathan T. Barron and Ren Ng},
      year={2020},
      eprint={2006.10739},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{rombach2022highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2022},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@book{quarteroni1999domain,
  title={Domain decomposition methods for partial differential equations},
  author={Quarteroni, Alfio and Valli, Alberto},
  number={BOOK},
  year={1999},
  publisher={Oxford University Press}
}

@article{jagtap2020extended,
title={Extended physics-informed neural networks (xpinns): A generalized space-time domain decomposition based deep learning framework for nonlinear         partial differential equations},
author={Jagtap, Ameya D and Karniadakis, George Em},
journal={Communications in Computational Physics},
volume={28},
number={5},
pages={2002--2041},
year={2020}
}

@inbook{large-scale-kmflow,
title = {Large-Scale Dynamics and Transition to Turbulence in the Two-Dimensional Kolmogorov Flow},
booktitle = {Current Trends in Turbulence Research},
chapter = {},
pages = {374-396},
doi = {10.2514/5.9781600865831.0374.0396},
URL = {https://arc.aiaa.org/doi/abs/10.2514/5.9781600865831.0374.0396},
eprint = {https://arc.aiaa.org/doi/pdf/10.2514/5.9781600865831.0374.0396}
}

@article{chandler2013kmflow, 
title={Invariant recurrent solutions embedded in a turbulent two-dimensional Kolmogorov flow},
volume={722}, DOI={10.1017/jfm.2013.122},
journal={Journal of Fluid Mechanics},
publisher={Cambridge University Press},
author={Chandler, Gary J. and Kerswell, Rich R.}, year={2013}, 
pages={554–595}}

@inproceedings{Rogallo1981NumericalEI,
  title={Numerical experiments in homogeneous turbulence},
  author={Robert S. Rogallo},
  year={1981}
}

@misc{liu2023mitigating,
      title={Mitigating spectral bias for the multiscale operator learning with hierarchical attention}, 
      author={Xinliang Liu and Bo Xu and Lei Zhang},
      year={2023},
      eprint={2210.10890},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hao2023physicsinformed,
      title={Physics-Informed Machine Learning: A Survey on Problems, Methods and Applications}, 
      author={Zhongkai Hao and Songming Liu and Yichi Zhang and Chengyang Ying and Yao Feng and Hang Su and Jun Zhu},
      year={2023},
      eprint={2211.08064},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{rahman2023uno,
      title={U-NO: U-shaped Neural Operators}, 
      author={Md Ashiqur Rahman and Zachary E. Ross and Kamyar Azizzadenesheli},
      year={2023},
      eprint={2204.11127},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{
kossaifi2023multigrid,
title={Multi-Grid Tensorized Fourier Neural  Operator for High Resolution {PDE}s},
author={Jean Kossaifi and Nikola Borislavov Kovachki and Kamyar Azizzadenesheli and Anima Anandkumar},
year={2023},
url={https://openreview.net/forum?id=po-oqRst4Xm}
}

@misc{kovachki2021universal,
      title={On universal approximation and error bounds for Fourier Neural Operators}, 
      author={Nikola Kovachki and Samuel Lanthaler and Siddhartha Mishra},
      year={2021},
      eprint={2107.07562},
      archivePrefix={arXiv},
      primaryClass={math.NA}
}