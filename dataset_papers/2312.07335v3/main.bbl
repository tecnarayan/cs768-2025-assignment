\begin{thebibliography}{}

\bibitem[Ambrosio et~al., 2005]{ambrosio2005gradient}
Ambrosio, L., Gigli, N., and Savar{\'e}, G. (2005).
\newblock {\em Gradient flows: in metric spaces and in the space of probability
  measures}.
\newblock Springer Science \& Business Media.

\bibitem[Andrieu and Thoms, 2008]{andrieu2008tutorial}
Andrieu, C. and Thoms, J. (2008).
\newblock A tutorial on adaptive {MCMC}.
\newblock {\em Statistics and computing}, 18:343--373.

\bibitem[Bakry and {\'E}mery, 2006]{bakry2006diffusions}
Bakry, D. and {\'E}mery, M. (2006).
\newblock Diffusions hypercontractives.
\newblock In {\em S{\'e}minaire de Probabilit{\'e}s XIX 1983/84: Proceedings},
  pages 177--206. Springer.

\bibitem[Bernton, 2018]{bernton2018langevin}
Bernton, E. (2018).
\newblock Langevin {M}onte {C}arlo and {JKO} splitting.
\newblock In {\em Conference on learning theory}, pages 1777--1798. PMLR.

\bibitem[Caprio et~al., 2024]{caprio2024error}
Caprio, R., Kuntz, J., Power, S., and Johansen, A.~M. (2024).
\newblock Error bounds for particle gradient descent, and extensions of the
  log-{S}obolev and {T}alagrand inequalities.
\newblock {\em arXiv preprint arXiv:2403.02004}.

\bibitem[Carmona, 2016]{carmona2016lectures}
Carmona, R. (2016).
\newblock {\em Lectures on {BSDE}s, stochastic control, and stochastic
  differential games with financial applications}.
\newblock SIAM.

\bibitem[Chen et~al., 2024]{chen2022uniform}
Chen, F., Lin, Y., Ren, Z., and Wang, S. (2024).
\newblock Uniform-in-time propagation of chaos for kinetic mean field
  {L}angevin dynamics.
\newblock {\em Electronic Journal of Probability}, 29:1--43.

\bibitem[Cheng et~al., 2018]{cheng2018underdamped}
Cheng, X., Chatterji, N.~S., Bartlett, P.~L., and Jordan, M.~I. (2018).
\newblock Underdamped {L}angevin {MCMC}: A non-asymptotic analysis.
\newblock In {\em Conference on Learning Theory}, pages 300--323. PMLR.

\bibitem[Chizat, 2022]{chizat2022mean}
Chizat, L. (2022).
\newblock Mean-field langevin dynamics : Exponential convergence and annealing.
\newblock {\em Transactions on Machine Learning Research}.

\bibitem[Chizat and Bach, 2018]{chizat2018global}
Chizat, L. and Bach, F. (2018).
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock {\em Advances in neural information processing systems}, 31.

\bibitem[De~Bortoli et~al., 2021]{de2021efficient}
De~Bortoli, V., Durmus, A., Pereyra, M., and Vidal, A.~F. (2021).
\newblock Efficient stochastic optimisation by unadjusted {L}angevin {M}onte
  {C}arlo: {A}pplication to maximum marginal likelihood and empirical
  {B}ayesian estimation.
\newblock {\em Statistics and Computing}, 31:1--18.

\bibitem[Delyon et~al., 1999]{delyon1999convergence}
Delyon, B., Lavielle, M., and Moulines, E. (1999).
\newblock Convergence of a stochastic approximation version of the {EM}
  algorithm.
\newblock {\em Annals of statistics}, pages 94--128.

\bibitem[Diao et~al., 2023]{diao2023forward}
Diao, M.~Z., Balasubramanian, K., Chewi, S., and Salim, A. (2023).
\newblock Forward-backward {G}aussian variational inference via {JKO} in the
  {B}ures-{W}asserstein space.
\newblock In {\em International Conference on Machine Learning}, pages
  7960--7991. PMLR.

\bibitem[Dockhorn et~al., 2022]{dockhorn2022score}
Dockhorn, T., Vahdat, A., and Kreis, K. (2022).
\newblock Score-based generative modeling with critically-damped {L}angevin
  diffusion.
\newblock In {\em International Conference on Learning Representations (ICLR)}.

\bibitem[Duncan et~al., 2023]{duncan2019geometry}
Duncan, A., N{\"u}sken, N., and Szpruch, L. (2023).
\newblock On the geometry of {S}tein variational gradient descent.
\newblock {\em Journal of Machine Learning Research}, 24(56):1--39.

\bibitem[Garbuno-Inigo et~al., 2020]{garbuno2020affine}
Garbuno-Inigo, A., N\"{u}sken, N., and Reich, S. (2020).
\newblock Affine invariant interacting {L}angevin dynamics for {B}ayesian
  inference.
\newblock {\em SIAM Journal on Applied Dynamical Systems}, 19(3):1633--1658.

\bibitem[Gelfand and Silverman, 2000]{gelfand2000calculus}
Gelfand, I.~M. and Silverman, R.~A. (2000).
\newblock {\em Calculus of Variations}.
\newblock Courier Corporation.

\bibitem[Good, 1983]{good1983}
Good, I.~J. (1983).
\newblock {\em Good thinking: The foundations of probability and its
  applications}.
\newblock University of Minnesota Press.

\bibitem[Han et~al., 2017]{han2017alternating}
Han, T., Lu, Y., Zhu, S.-C., and Wu, Y.~N. (2017).
\newblock Alternating back-propagation for generator network.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~31.

\bibitem[Heusel et~al., 2017]{heusel2017gans}
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S.
  (2017).
\newblock {GAN}s trained by a two time-scale update rule converge to a local
  nash equilibrium.
\newblock {\em Advances in Neural Information Processing Systems}, 30.

\bibitem[Hinton, 2002]{hinton2002training}
Hinton, G.~E. (2002).
\newblock Training products of experts by minimizing contrastive divergence.
\newblock {\em Neural computation}, 14(8):1771--1800.

\bibitem[Hochbruck and Ostermann, 2010]{hochbruck2010exponential}
Hochbruck, M. and Ostermann, A. (2010).
\newblock Exponential integrators.
\newblock {\em Acta Numerica}, 19:209--286.

\bibitem[Hu et~al., 2021]{hu2021mean}
Hu, K., Ren, Z., {\v{S}}i{\v{s}}ka, D., and Szpruch, {\L}. (2021).
\newblock Mean-field {L}angevin dynamics and energy landscape of neural
  networks.
\newblock In {\em Annales de l'Institut Henri Poincare (B) Probabilites et
  statistiques}, volume~57, pages 2043--2065. Institut Henri Poincar{\'e}.

\bibitem[Jordan et~al., 1998]{jordan1998variational}
Jordan, R., Kinderlehrer, D., and Otto, F. (1998).
\newblock The variational formulation of the {F}okker--{P}lanck equation.
\newblock {\em SIAM Journal on Mathematical Analysis}, 29(1):1--17.

\bibitem[Kac, 1956]{kac1956foundations}
Kac, M. (1956).
\newblock Foundations of kinetic theory.
\newblock In {\em Proceedings of The third Berkeley symposium on mathematical
  statistics and probability}, volume~3, pages 171--197.

\bibitem[Kingma and Welling, 2014]{kingma2013auto}
Kingma, D.~P. and Welling, M. (2014).
\newblock Auto-encoding variational {B}ayes.
\newblock In Bengio, Y. and LeCun, Y., editors, {\em 2nd International
  Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April
  14-16, 2014, Conference Track Proceedings}.

\bibitem[Krizhevsky and Hinton, 2009]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G. (2009).
\newblock Learning multiple layers of features from tiny images.

\bibitem[Kruger, 2003]{kruger2003frechet}
Kruger, A.~Y. (2003).
\newblock On {F}r{\'e}chet subdifferentials.
\newblock {\em Journal of Mathematical Sciences}, 116(3):3325--3358.

\bibitem[Kuntz et~al., 2023]{Kuntz2022}
Kuntz, J., Lim, J.~N., and Johansen, A.~M. (2023).
\newblock Particle algorithms for maximum likelihood training of latent
  variable models.
\newblock In {\em Proceedings on 26th International Conference on Artificial
  Intelligence and Statistics (AISTATS)}, volume 206 of {\em Proceedings of
  Machine Learning Research}, pages 5134--5180.

\bibitem[Lambert et~al., 2022]{lambert2022variational}
Lambert, M., Chewi, S., Bach, F., Bonnabel, S., and Rigollet, P. (2022).
\newblock Variational inference via {W}asserstein gradient flows.
\newblock {\em Advances in Neural Information Processing Systems},
  35:14434--14447.

\bibitem[LeCun et~al., 1998]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998).
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324.

\bibitem[Liu, 2017]{liu2017stein}
Liu, Q. (2017).
\newblock Stein variational gradient descent as gradient flow.
\newblock {\em Advances in Neural Information Processing Systems}, 30.

\bibitem[Lo{\`e}ve, 1977]{Loève1977}
Lo{\`e}ve, M. (1977).
\newblock {\em Probability Concepts}, pages 151--176.
\newblock Springer New York, New York, NY.

\bibitem[Ma et~al., 2021]{ma2019there}
Ma, Y.-A., Chatterji, N.~S., Cheng, X., Flammarion, N., Bartlett, P.~L., and
  Jordan, M.~I. (2021).
\newblock {Is there an analog of Nesterov acceleration for gradient-based
  {MCMC}?}
\newblock {\em Bernoulli}, 27(3):1942 -- 1992.

\bibitem[Maddison et~al., 2018]{maddison2018hamiltonian}
Maddison, C.~J., Paulin, D., Teh, Y.~W., O'Donoghue, B., and Doucet, A. (2018).
\newblock Hamiltonian descent methods.
\newblock {\em arXiv preprint arXiv:1809.05042}.

\bibitem[Martens, 2010]{martens2010deep}
Martens, J. (2010).
\newblock Deep learning via {H}essian-free optimization.
\newblock In {\em International Conference on Machine Learning}, volume~27,
  pages 735--742.

\bibitem[McCall, 2010]{mccall2010classical}
McCall, M.~W. (2010).
\newblock {\em Classical Mechanics: From {N}ewton to {E}instein: A Modern
  Introduction}.
\newblock John Wiley \& Sons.

\bibitem[McKean~Jr, 1966]{mckean1966class}
McKean~Jr, H.~P. (1966).
\newblock A class of {M}arkov processes associated with nonlinear parabolic
  equations.
\newblock {\em Proceedings of the National Academy of Sciences},
  56(6):1907--1911.

\bibitem[McLachlan and Perlmutter, 2001]{mclachlan2001conformal}
McLachlan, R. and Perlmutter, M. (2001).
\newblock Conformal {H}amiltonian systems.
\newblock {\em Journal of Geometry and Physics}, 39(4):276--300.

\bibitem[Mei et~al., 2018]{mei2018mean}
Mei, S., Montanari, A., and Nguyen, P.-M. (2018).
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock {\em Proceedings of the National Academy of Sciences},
  115(33):E7665--E7671.

\bibitem[Neal and Hinton, 1998]{neal1998view}
Neal, R.~M. and Hinton, G.~E. (1998).
\newblock A view of the {EM} algorithm that justifies incremental, sparse, and
  other variants.
\newblock {\em Learning in graphical models}, pages 355--368.

\bibitem[Nemirovskij and Yudin, 1983]{nemirovskij1983problem}
Nemirovskij, A.~S. and Yudin, D.~B. (1983).
\newblock {\em Problem complexity and method efficiency in optimization}.
\newblock Wiley-Interscience.

\bibitem[Nesterov, 2003]{nesterov2003introductory}
Nesterov, Y. (2003).
\newblock {\em Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Springer Science \& Business Media.

\bibitem[Nesterov, 1983]{nesterov1983method}
Nesterov, Y.~E. (1983).
\newblock A method of solving a convex programming problem with convergence
  rate $\mathcal{O}\bigl(\frac{1 }{k^2}\bigr)$.
\newblock In {\em Doklady Akademii Nauk}, volume 269, pages 543--547. Russian
  Academy of Sciences.

\bibitem[Nijkamp et~al., 2020]{nijkamp2020learning}
Nijkamp, E., Pang, B., Han, T., Zhou, L., Zhu, S.-C., and Wu, Y.~N. (2020).
\newblock Learning multi-layer latent variable model via variational
  optimization of short run {MCMC} for approximate inference.
\newblock In {\em Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part VI 16}, pages 361--378.
  Springer.

\bibitem[Nitanda and Suzuki, 2017]{nitanda2017stochastic}
Nitanda, A. and Suzuki, T. (2017).
\newblock Stochastic particle gradient descent for infinite ensembles.
\newblock {\em arXiv preprint arXiv:1712.05438}.

\bibitem[Nitanda et~al., 2022]{nitanda2022convex}
Nitanda, A., Wu, D., and Suzuki, T. (2022).
\newblock Convex analysis of the mean field {L}angevin dynamics.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 9741--9757. PMLR.

\bibitem[Otto and Villani, 2000]{otto2000}
Otto, F. and Villani, C. (2000).
\newblock {Generalization of an Inequality by {T}alagrand and Links with the
  Logarithmic {S}obolev Inequality}.
\newblock {\em Journal of Functional Analysis}, 173(2):361--400.

\bibitem[O’Donoghue and Candes, 2015]{o2015adaptive}
O’Donoghue, B. and Candes, E. (2015).
\newblock Adaptive restart for accelerated gradient schemes.
\newblock {\em Foundations of Computational Mathematics}, 15:715--732.

\bibitem[Pang et~al., 2020]{pang2020learning}
Pang, B., Han, T., Nijkamp, E., Zhu, S.-C., and Wu, Y.~N. (2020).
\newblock Learning latent space energy-based prior model.
\newblock {\em Advances in Neural Information Processing Systems},
  33:21994--22008.

\bibitem[Peyr\'e and Cuturi, 2019]{peyre2019}
Peyr\'e, G. and Cuturi, M. (2019).
\newblock Computational optimal transport: With applications to data science.
\newblock {\em Foundations and Trends in Machine Learning}, 11(5-6):355--607.

\bibitem[Platen and Bruti-Liberati, 2010]{platen2010numerical}
Platen, E. and Bruti-Liberati, N. (2010).
\newblock {\em Numerical Solution of Stochastic Differential Equations with
  Jumps in Finance}.
\newblock Springer Science \& Business Media.

\bibitem[Riou-Durand et~al., 2023]{riou2023adaptive}
Riou-Durand, L., Sountsov, P., Vogrinc, J., Margossian, C., and Power, S.
  (2023).
\newblock Adaptive tuning for {M}etropolis adjusted {L}angevin trajectories.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 8102--8116. PMLR.

\bibitem[Roberts and Rosenthal, 2009]{roberts2009examples}
Roberts, G.~O. and Rosenthal, J.~S. (2009).
\newblock Examples of adaptive {MCMC}.
\newblock {\em Journal of computational and graphical statistics},
  18(2):349--367.

\bibitem[Santambrogio, 2017]{santambrogio2017euclidean}
Santambrogio, F. (2017).
\newblock $\{$Euclidean, metric, and Wasserstein$\}$ gradient flows: an
  overview.
\newblock {\em Bulletin of Mathematical Sciences}, 7:87--154.

\bibitem[Sanz-Serna and Zygalakis, 2021]{sanz2021wasserstein}
Sanz-Serna, J.~M. and Zygalakis, K.~C. (2021).
\newblock Wasserstein distance estimates for the distributions of numerical
  approximations to ergodic stochastic differential equations.
\newblock {\em The Journal of Machine Learning Research}, 22(1):11006--11042.

\bibitem[Sharrock and Nemeth, 2023]{sharrock2023coinem}
Sharrock, L. and Nemeth, C. (2023).
\newblock Coin sampling: Gradient-based {B}ayesian inference without learning
  rates.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,
  and Scarlett, J., editors, {\em Proceedings of the 40th International
  Conference on Machine Learning}, volume 202 of {\em Proceedings of Machine
  Learning Research}, pages 30850--30882. PMLR.

\bibitem[Shi et~al., 2021]{shi2021understanding}
Shi, B., Du, S.~S., Jordan, M.~I., and Su, W.~J. (2021).
\newblock Understanding the acceleration phenomenon via high-resolution
  differential equations.
\newblock {\em Mathematical Programming}, 195:79--148.

\bibitem[Silvester, 2000]{silvester2000determinants}
Silvester, J.~R. (2000).
\newblock Determinants of block matrices.
\newblock {\em The Mathematical Gazette}, 84(501):460--467.

\bibitem[Staib et~al., 2019]{staib2019escaping}
Staib, M., Reddi, S., Kale, S., Kumar, S., and Sra, S. (2019).
\newblock Escaping saddle points with adaptive gradient methods.
\newblock In {\em International Conference on Machine Learning}, pages
  5956--5965. PMLR.

\bibitem[Su et~al., 2014]{su2014differential}
Su, W., Boyd, S., and Candes, E. (2014).
\newblock A differential equation for modeling {N}esterov’s accelerated
  gradient method: theory and insights.
\newblock {\em Advances in Neural Information Processing Systems}, 27.

\bibitem[Sutskever et~al., 2013]{sutskever2013importance}
Sutskever, I., Martens, J., Dahl, G., and Hinton, G. (2013).
\newblock On the importance of initialization and momentum in deep learning.
\newblock In {\em International Conference on Machine Learning}, pages
  1139--1147. PMLR.

\bibitem[Suzuki et~al., 2023]{suzuki2023convergence}
Suzuki, T., Wu, D., and Nitanda, A. (2023).
\newblock Mean-field langevin dynamics: Time-space discretization, stochastic
  gradient, and variance reduction.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing
  Systems}.

\bibitem[Tieleman and Hinton, 2012]{tieleman2012lecture}
Tieleman, T. and Hinton, G. (2012).
\newblock Lecture 6.5-rmsprop, coursera: Neural networks for machine learning.
\newblock {\em University of Toronto, Technical Report}, 6.

\bibitem[Tomczak and Welling, 2018]{tomczak2018vae}
Tomczak, J. and Welling, M. (2018).
\newblock {VAE} with a {V}amp{P}rior.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1214--1223. PMLR.

\bibitem[Van~Handel, 2014]{van2014probability}
Van~Handel, R. (2014).
\newblock Probability in high dimension.
\newblock {\em Lecture Notes (Princeton University)}.

\bibitem[Villani, 2009]{villani2009optimal}
Villani, C. (2009).
\newblock {\em Optimal transport: old and new}, volume 338.
\newblock Springer.

\bibitem[Wibisono, 2018]{wibisono2018}
Wibisono, A. (2018).
\newblock Sampling as optimization in the space of measures: The {L}angevin
  dynamics as a composite optimization problem.
\newblock In {\em Conference on Learning Theory}, pages 2093--3027. PMLR.

\bibitem[Wibisono et~al., 2016]{wibisono2016variational}
Wibisono, A., Wilson, A.~C., and Jordan, M.~I. (2016).
\newblock A variational perspective on accelerated methods in optimization.
\newblock {\em Proceedings of the National Academy of Sciences},
  113(47):E7351--E7358.

\bibitem[Wilson et~al., 2021]{wilson2016lyapunov}
Wilson, A.~C., Recht, B., and Jordan, M.~I. (2021).
\newblock A {L}yapunov analysis of accelerated methods in optimization.
\newblock {\em Journal of Machine Learning Research}, 22(113):1--34.

\bibitem[Yao and Yang, 2022]{yao2022mean}
Yao, R. and Yang, Y. (2022).
\newblock Mean field variational inference via {W}asserstein gradient flow.
\newblock {\em arXiv preprint arXiv:2207.08074}.

\end{thebibliography}
