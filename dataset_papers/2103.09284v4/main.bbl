\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agliari et~al.(2016)Agliari, Naimzada, and
  Pecora]{agliari2016nonlinear}
Agliari, A., Naimzada, A.~K., and Pecora, N.
\newblock Nonlinear dynamics of a cournot duopoly game with differentiated
  products.
\newblock \emph{Applied Mathematics and Computation}, 281:\penalty0 1--15,
  2016.

\bibitem[Antos et~al.(2008)Antos, Szepesv{\'a}ri, and Munos]{antos2008fitted}
Antos, A., Szepesv{\'a}ri, C., and Munos, R.
\newblock Fitted q-iteration in continuous action-space mdps.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  9--16, 2008.

\bibitem[Balduzzi et~al.(2018)Balduzzi, Racaniere, Martens, Foerster, Tuyls,
  and Graepel]{balduzzi2018mechanics}
Balduzzi, D., Racaniere, S., Martens, J., Foerster, J., Tuyls, K., and Graepel,
  T.
\newblock The mechanics of n-player differentiable games.
\newblock \emph{arXiv preprint arXiv:1802.05642}, 2018.

\bibitem[Bertsekas(2012)]{bertsekas2012approximate}
Bertsekas, D.~P.
\newblock Approximate dynamic programming.
\newblock 2012.

\bibitem[Bertsekas \& Tsitsiklis(2000)Bertsekas and
  Tsitsiklis]{bertsekas2000gradient}
Bertsekas, D.~P. and Tsitsiklis, J.~N.
\newblock Gradient convergence in gradient methods with errors.
\newblock \emph{SIAM Journal on Optimization}, 10\penalty0 (3):\penalty0
  627--642, 2000.

\bibitem[Bloembergen et~al.(2015)Bloembergen, Tuyls, Hennes, and
  Kaisers]{bloembergen2015evolutionary}
Bloembergen, D., Tuyls, K., Hennes, D., and Kaisers, M.
\newblock Evolutionary dynamics of multi-agent learning: A survey.
\newblock \emph{Journal of Artificial Intelligence Research}, 53:\penalty0
  659--697, 2015.

\bibitem[Candogan et~al.(2011)Candogan, Menache, Ozdaglar, and
  Parrilo]{candogan2011flows}
Candogan, O., Menache, I., Ozdaglar, A., and Parrilo, P.~A.
\newblock Flows and decompositions of games: Harmonic and potential games.
\newblock \emph{Mathematics of Operations Research}, 36\penalty0 (3):\penalty0
  474--503, 2011.

\bibitem[Candogan et~al.(2013)Candogan, Ozdaglar, and
  Parrilo]{candogan2013near}
Candogan, O., Ozdaglar, A., and Parrilo, P.~A.
\newblock Near-potential games: Geometry and dynamics.
\newblock \emph{ACM Transactions on Economics and Computation (TEAC)},
  1\penalty0 (2):\penalty0 1--32, 2013.

\bibitem[Chang(2015)]{CHANG2015325}
Chang, K.-H.
\newblock Chapter 5 - multiobjective optimization and advanced topics.
\newblock In Chang, K.-H. (ed.), \emph{Design Theory and Methods Using
  CAD/CAE}, pp.\  325 -- 406. Academic Press, Boston, 2015.

\bibitem[Chen et~al.(2009)Chen, Deng, and Teng]{chen2009settling}
Chen, X., Deng, X., and Teng, S.-H.
\newblock Settling the complexity of computing two-player nash equilibria.
\newblock \emph{Journal of the ACM (JACM)}, 56\penalty0 (3):\penalty0 1--57,
  2009.

\bibitem[Cheng et~al.(2017)Cheng, Diakonikolas, and Stewart]{cheng2017playing}
Cheng, Y., Diakonikolas, I., and Stewart, A.
\newblock Playing anonymous games using simple strategies.
\newblock In \emph{Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium
  on Discrete Algorithms}, pp.\  616--631. SIAM, 2017.

\bibitem[Daskalakis \& Papadimitriou(2007)Daskalakis and
  Papadimitriou]{daskalakis2007computing}
Daskalakis, C. and Papadimitriou, C.
\newblock Computing equilibria in anonymous games.
\newblock In \emph{48th Annual IEEE Symposium on Foundations of Computer
  Science (FOCS'07)}, pp.\  83--93. IEEE, 2007.

\bibitem[Davis et~al.(2014)Davis, Burch, and Bowling]{davis2014using}
Davis, T., Burch, N., and Bowling, M.
\newblock Using response functions to measure strategy strength.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~28, 2014.

\bibitem[de~Witt et~al.(2020)de~Witt, Peng, Kamienny, Torr, B{\"o}hmer, and
  Whiteson]{de2020deep}
de~Witt, C.~S., Peng, B., Kamienny, P.-A., Torr, P., B{\"o}hmer, W., and
  Whiteson, S.
\newblock Deep multi-agent reinforcement learning for decentralized continuous
  cooperative control.
\newblock \emph{arXiv preprint arXiv:2003.06709}, 2020.

\bibitem[Foerster et~al.(2018)Foerster, Farquhar, Afouras, Nardelli, and
  Whiteson]{foerster2018counterfactual}
Foerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.
\newblock Counterfactual multi-agent policy gradients.
\newblock In \emph{Thirty-second AAAI conference on artificial intelligence},
  2018.

\bibitem[Fudenberg \& Tirole(1991)Fudenberg and Tirole]{fudenberg1991tirole}
Fudenberg, D. and Tirole, J.
\newblock Game theory.
\newblock \emph{MIT Press}, 726:\penalty0 764, 1991.

\bibitem[Gonz{\'a}lez-S{\'a}nchez \&
  Hern{\'a}ndez-Lerma(2013)Gonz{\'a}lez-S{\'a}nchez and
  Hern{\'a}ndez-Lerma]{gonzalez2013discrete}
Gonz{\'a}lez-S{\'a}nchez, D. and Hern{\'a}ndez-Lerma, O.
\newblock \emph{Discrete--time stochastic control and dynamic potential games:
  the Euler--Equation approach}.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Grau-Moya et~al.(2018)Grau-Moya, Leibfried, and
  Bou-Ammar]{grau2018balancing}
Grau-Moya, J., Leibfried, F., and Bou-Ammar, H.
\newblock Balancing two-player stochastic games with soft q-learning.
\newblock \emph{arXiv preprint arXiv:1802.03216}, 2018.

\bibitem[Hernandez-Leal et~al.(2017)Hernandez-Leal, Kaisers, Baarslag, and
  de~Cote]{hernandez2017survey}
Hernandez-Leal, P., Kaisers, M., Baarslag, T., and de~Cote, E.~M.
\newblock A survey of learning in multiagent environments: Dealing with
  non-stationarity.
\newblock \emph{arXiv preprint arXiv:1707.09183}, 2017.

\bibitem[Hu \& Wellman(2003)Hu and Wellman]{hu2003nash}
Hu, J. and Wellman, M.~P.
\newblock Nash q-learning for general-sum stochastic games.
\newblock \emph{Journal of machine learning research}, 4\penalty0
  (Nov):\penalty0 1039--1069, 2003.

\bibitem[Jaderberg et~al.(2019)Jaderberg, Czarnecki, Dunning, Marris, Lever,
  Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, et~al.]{jaderberg2019human}
Jaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda,
  A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., et~al.
\newblock Human-level performance in 3d multiplayer games with population-based
  reinforcement learning.
\newblock \emph{Science}, 364\penalty0 (6443):\penalty0 859--865, 2019.

\bibitem[Ja{\'s}kiewicz \& Nowak(2018)Ja{\'s}kiewicz and
  Nowak]{jaskiewicz2018symmetric}
Ja{\'s}kiewicz, A. and Nowak, A.~S.
\newblock On symmetric stochastic games of resource extraction with weakly
  continuous transitions.
\newblock \emph{Top}, 26\penalty0 (2):\penalty0 239--256, 2018.

\bibitem[Konda \& Tsitsiklis(2000)Konda and Tsitsiklis]{konda2000actor}
Konda, V.~R. and Tsitsiklis, J.~N.
\newblock Actor-critic algorithms.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1008--1014, 2000.

\bibitem[L{\~a} et~al.(2016)L{\~a}, Chew, and Soong]{la2016potential}
L{\~a}, Q.~D., Chew, Y.~H., and Soong, B.-H.
\newblock \emph{Potential Game Theory}.
\newblock Springer, 2016.

\bibitem[Leonardos et~al.(2021)Leonardos, Overman, Panageas, and
  Piliouras]{Leonardos2021GlobalCO}
Leonardos, S., Overman, W., Panageas, I., and Piliouras, G.
\newblock Global convergence of multi-agent policy gradient in markov potential
  games.
\newblock 2021.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Littman(2001)]{littman2001friend}
Littman, M.~L.
\newblock Friend-or-foe q-learning in general-sum games.
\newblock In \emph{ICML}, volume~1, pp.\  322--328, 2001.

\bibitem[Lowe et~al.(2017)Lowe, Wu, Tamar, Harb, Abbeel, and
  Mordatch]{lowe2017multi}
Lowe, R., Wu, Y.~I., Tamar, A., Harb, J., Abbeel, O.~P., and Mordatch, I.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  6379--6390, 2017.

\bibitem[Macua et~al.(2010)Macua, Belanovic, and Zazo]{macua2010consensus}
Macua, S.~V., Belanovic, P., and Zazo, S.
\newblock Consensus-based distributed principal component analysis in wireless
  sensor networks.
\newblock In \emph{2010 IEEE 11th International Workshop on Signal Processing
  Advances in Wireless Communications (SPAWC)}, pp.\  1--5. IEEE, 2010.

\bibitem[Macua et~al.(2018)Macua, Zazo, and Zazo]{macua2018learning}
Macua, S.~V., Zazo, J., and Zazo, S.
\newblock Learning parametric closed-loop policies for markov potential games.
\newblock \emph{arXiv preprint arXiv:1802.00899}, 2018.

\bibitem[Marden et~al.(2009)Marden, Arslan, and Shamma]{marden2009cooperative}
Marden, J.~R., Arslan, G., and Shamma, J.~S.
\newblock Cooperative control and potential games.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics, Part B
  (Cybernetics)}, 39\penalty0 (6):\penalty0 1393--1407, 2009.

\bibitem[Mariano et~al.(2001)Mariano, Pereira, Correia, Ribeiro, Abramov,
  Szirbik, Goossenaerts, Marwala, and De~Wilde]{mariano2001simulation}
Mariano, P., Pereira, A., Correia, L., Ribeiro, R., Abramov, V., Szirbik, N.,
  Goossenaerts, J., Marwala, T., and De~Wilde, P.
\newblock Simulation of a trading multi-agent system.
\newblock In \emph{2001 IEEE International Conference on Systems, Man and
  Cybernetics. e-Systems and e-Man for Cybernetics in Cyberspace (Cat. No.
  01CH37236)}, volume~5, pp.\  3378--3384. IEEE, 2001.

\bibitem[Mguni et~al.(2018)Mguni, Jennings, and
  de~Cote]{mguni2018decentralised}
Mguni, D., Jennings, J., and de~Cote, E.~M.
\newblock Decentralised learning in systems with many, many strategic agents.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Monderer \& Shapley(1996{\natexlab{a}})Monderer and
  Shapley]{monderer1996fictitious}
Monderer, D. and Shapley, L.~S.
\newblock Fictitious play property for games with identical interests.
\newblock \emph{Journal of economic theory}, 68\penalty0 (1):\penalty0
  258--265, 1996{\natexlab{a}}.

\bibitem[Monderer \& Shapley(1996{\natexlab{b}})Monderer and
  Shapley]{monderer1996potential}
Monderer, D. and Shapley, L.~S.
\newblock Potential games.
\newblock \emph{Games and economic behavior}, 14\penalty0 (1):\penalty0
  124--143, 1996{\natexlab{b}}.

\bibitem[Nedic \& Ozdaglar(2009)Nedic and Ozdaglar]{nedic2009distributed}
Nedic, A. and Ozdaglar, A.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 54\penalty0
  (1):\penalty0 48--61, 2009.

\bibitem[Nicolaescu(2011)]{nicolaescu2011coarea}
Nicolaescu, L.~I.
\newblock The coarea formula.
\newblock In \emph{seminar notes}. Citeseer, 2011.

\bibitem[Papadimitriou \& Tsitsiklis(1987)Papadimitriou and
  Tsitsiklis]{papadimitriou1987complexity}
Papadimitriou, C.~H. and Tsitsiklis, J.~N.
\newblock The complexity of markov decision processes.
\newblock \emph{Mathematics of operations research}, 12\penalty0 (3):\penalty0
  441--450, 1987.

\bibitem[Peng et~al.(2017)Peng, Wen, Yang, Yuan, Tang, Long, and
  Wang]{peng2017multiagent}
Peng, P., Wen, Y., Yang, Y., Yuan, Q., Tang, Z., Long, H., and Wang, J.
\newblock Multiagent bidirectionally-coordinated nets: Emergence of human-level
  coordination in learning to play starcraft combat games.
\newblock \emph{arXiv preprint arXiv:1703.10069}, 2017.

\bibitem[Rashid et~al.(2018)Rashid, Samvelyan, De~Witt, Farquhar, Foerster, and
  Whiteson]{rashid2018qmix}
Rashid, T., Samvelyan, M., De~Witt, C.~S., Farquhar, G., Foerster, J., and
  Whiteson, S.
\newblock Qmix: monotonic value function factorisation for deep multi-agent
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1803.11485}, 2018.

\bibitem[Roughgarden(2007)]{roughgarden2007routing}
Roughgarden, T.
\newblock Routing games.
\newblock \emph{Algorithmic game theory}, 18:\penalty0 459--484, 2007.

\bibitem[Samvelyan et~al.(2019)Samvelyan, Rashid, de~Witt, Farquhar, Nardelli,
  Rudner, Hung, Torr, Foerster, and Whiteson]{samvelyan2019starcraft}
Samvelyan, M., Rashid, T., de~Witt, C.~S., Farquhar, G., Nardelli, N., Rudner,
  T.~G., Hung, C.-M., Torr, P.~H., Foerster, J., and Whiteson, S.
\newblock The starcraft multi-agent challenge.
\newblock \emph{arXiv preprint arXiv:1902.04043}, 2019.

\bibitem[Shapley(1953)]{shapley1953stochastic}
Shapley, L.~S.
\newblock Stochastic games.
\newblock \emph{Proceedings of the national academy of sciences}, 39\penalty0
  (10):\penalty0 1095--1100, 1953.

\bibitem[Shoham \& Leyton-Brown(2008)Shoham and
  Leyton-Brown]{shoham2008multiagent}
Shoham, Y. and Leyton-Brown, K.
\newblock \emph{Multiagent systems: Algorithmic, game-theoretic, and logical
  foundations}.
\newblock Cambridge University Press, 2008.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{silver2014deterministic}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.
\newblock Deterministic policy gradient algorithms.
\newblock 2014.

\bibitem[Simon et~al.(1983)]{simon1983lectures}
Simon, L. et~al.
\newblock \emph{Lectures on geometric measure theory}.
\newblock The Australian National University, Mathematical Sciences Institute,
  Centre~…, 1983.

\bibitem[Slade(1994)]{slade1994does}
Slade, M.~E.
\newblock What does an oligopoly maximize?
\newblock \emph{The Journal of Industrial Economics}, pp.\  45--61, 1994.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{sutton2000policy}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1057--1063, 2000.

\bibitem[Szepesv{\'a}ri \& Munos(2005)Szepesv{\'a}ri and
  Munos]{szepesvari2005finite}
Szepesv{\'a}ri, C. and Munos, R.
\newblock Finite time bounds for sampling based fitted value iteration.
\newblock In \emph{Proceedings of the 22nd international conference on Machine
  learning}, pp.\  880--887, 2005.

\bibitem[Tan(1993)]{tan1993multi}
Tan, M.
\newblock Multi-agent reinforcement learning: Independent vs. cooperative
  agents.
\newblock In \emph{Proceedings of the tenth international conference on machine
  learning}, pp.\  330--337, 1993.

\bibitem[Tutunov et~al.(2019)Tutunov, Bou-Ammar, and
  Jadbabaie]{tutunov2019distributed}
Tutunov, R., Bou-Ammar, H., and Jadbabaie, A.
\newblock Distributed newton method for large-scale consensus optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 64\penalty0
  (10):\penalty0 3983--3994, 2019.

\bibitem[Ui(2000)]{ui2000shapley}
Ui, T.
\newblock A shapley value representation of potential games.
\newblock \emph{Games and Economic Behavior}, 31\penalty0 (1):\penalty0
  121--135, 2000.

\bibitem[Wang \& Sandholm(2003)Wang and Sandholm]{wang2003reinforcement}
Wang, X. and Sandholm, T.
\newblock Reinforcement learning to play an optimal nash equilibrium in team
  markov games.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1603--1610, 2003.

\bibitem[Wiering(2000)]{wiering2000multi}
Wiering, M.
\newblock Multi-agent reinforcement learning for traffic light control.
\newblock In \emph{Machine Learning: Proceedings of the Seventeenth
  International Conference (ICML'2000)}, pp.\  1151--1158, 2000.

\bibitem[Yang \& Wang(2020)Yang and Wang]{yang2020overview}
Yang, Y. and Wang, J.
\newblock An overview of multi-agent reinforcement learning from game
  theoretical perspective.
\newblock \emph{arXiv preprint arXiv:2011.00583}, 2020.

\bibitem[Yang et~al.(2018)Yang, Luo, Li, Zhou, Zhang, and Wang]{yang2018mean}
Yang, Y., Luo, R., Li, M., Zhou, M., Zhang, W., and Wang, J.
\newblock Mean field multi-agent reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5571--5580. PMLR, 2018.

\bibitem[Yang et~al.(2019)Yang, Tutunov, Sakulwongtana, Ammar, and
  Wang]{yang2019alpha}
Yang, Y., Tutunov, R., Sakulwongtana, P., Ammar, H.~B., and Wang, J.
\newblock $\alpha$-rank: Scalable multi-agent evaluation through evolution.
\newblock \emph{arXiv preprint arXiv:1909.11628}, 2019.

\bibitem[Yang et~al.(2020)Yang, Wen, Wang, Chen, Shao, Mguni, and
  Zhang]{yang2020multi}
Yang, Y., Wen, Y., Wang, J., Chen, L., Shao, K., Mguni, D., and Zhang, W.
\newblock Multi-agent determinantal q-learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10757--10766. PMLR, 2020.

\bibitem[Ye et~al.(2015)Ye, Zhang, and Yang]{ye2015multi}
Ye, D., Zhang, M., and Yang, Y.
\newblock A multi-agent framework for packet routing in wireless sensor
  networks.
\newblock \emph{sensors}, 15\penalty0 (5):\penalty0 10026--10047, 2015.

\bibitem[Zazo et~al.(2015)Zazo, Macua, S{\'a}nchez-Fern{\'a}ndez, and
  Zazo]{zazo2015dynamic}
Zazo, S., Macua, S.~V., S{\'a}nchez-Fern{\'a}ndez, M., and Zazo, J.
\newblock Dynamic potential games in communications: Fundamentals and
  applications.
\newblock \emph{arXiv preprint arXiv:1509.01313}, 2015.

\bibitem[Zhang et~al.(2020)Zhang, Chen, Huang, Li, Yang, Zhang, and
  Wang]{zhang2020bi}
Zhang, H., Chen, W., Huang, Z., Li, M., Yang, Y., Zhang, W., and Wang, J.
\newblock Bi-level actor-critic for multi-agent coordination.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pp.\  7325--7332, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Ren, and Li]{zhang2021gradient}
Zhang, R., Ren, Z., and Li, N.
\newblock Gradient play in multi-agent markov stochastic games: Stationary
  points and convergence.
\newblock \emph{arXiv preprint arXiv:2106.00198}, 2021.

\bibitem[Zhou et~al.(2020)Zhou, Luo, Villela, Yang, Rusu, Miao, Zhang, Alban,
  Fadakar, Chen, et~al.]{zhou2020smarts}
Zhou, M., Luo, J., Villela, J., Yang, Y., Rusu, D., Miao, J., Zhang, W., Alban,
  M., Fadakar, I., Chen, Z., et~al.
\newblock Smarts: Scalable multi-agent reinforcement learning training school
  for autonomous driving.
\newblock \emph{arXiv preprint arXiv:2010.09776}, 2020.

\end{thebibliography}
