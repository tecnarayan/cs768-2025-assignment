@misc{abutbulDNFNetNeuralArchitecture2020a,
  title = {{{DNF-Net}}: {{A Neural Architecture}} for {{Tabular Data}}},
  shorttitle = {{{DNF-Net}}},
  author = {Abutbul, Ami and Elidan, Gal and Katzir, Liran and {El-Yaniv}, Ran},
  year = {2020},
  month = jun,
  number = {arXiv:2006.06465},
  eprint = {2006.06465},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.06465},
  urldate = {2024-02-01},
  abstract = {A challenging open question in deep learning is how to handle tabular data. Unlike domains such as image and natural language processing, where deep architectures prevail, there is still no widely accepted neural architecture that dominates tabular data. As a step toward bridging this gap, we present DNF-Net a novel generic architecture whose inductive bias elicits models whose structure corresponds to logical Boolean formulas in disjunctive normal form (DNF) over affine soft-threshold decision terms. In addition, DNF-Net promotes localized decisions that are taken over small subsets of the features. We present an extensive empirical study showing that DNF-Nets significantly and consistently outperform FCNs over tabular data. With relatively few hyperparameters, DNF-Nets open the door to practical end-to-end handling of tabular data using neural networks. We present ablation studies, which justify the design choices of DNF-Net including the three inductive bias elements, namely, Boolean formulation, locality, and feature selection.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/leo/Zotero/storage/BB6DIPAB/Abutbul et al. - 2020 - DNF-Net A Neural Architecture for Tabular Data.pdf;/Users/leo/Zotero/storage/KXL4JFTY/2006.html}
}

@misc{alainUnderstandingIntermediateLayers2018,
  title = {Understanding Intermediate Layers Using Linear Classifier Probes},
  author = {Alain, Guillaume and Bengio, Yoshua},
  year = {2018},
  month = nov,
  number = {arXiv:1610.01644},
  eprint = {1610.01644},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1610.01644},
  urldate = {2023-11-24},
  abstract = {Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as "probes", trained entirely independently of the model itself. This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/leo/Zotero/storage/K9K6C2E6/Alain and Bengio - 2018 - Understanding intermediate layers using linear cla.pdf;/Users/leo/Zotero/storage/CI43KS42/1610.html}
}

@misc{ApplyingLargeLanguage,
  title = {Applying {{Large Language Models To Tabular Data}}: {{A New Approach}}},
  shorttitle = {Applying {{Large Language Models To Tabular Data}}},
  journal = {Arize AI},
  urldate = {2023-10-04},
  abstract = {Transform your tabular data predictions with large language models. Explore a revolutionary approach using prompt engineering and latent structure embeddings. Discover the potential of GPT-4 for tabular data modeling.},
  howpublished = {https://arize.com/blog-course/applying-large-language-models-to-tabular-data/},
  langid = {american},
  file = {/Users/leo/Zotero/storage/G4QM57C4/applying-large-language-models-to-tabular-data.html}
}

@misc{arikTabNetAttentiveInterpretable2020,
  title = {{{TabNet}}: {{Attentive Interpretable Tabular Learning}}},
  shorttitle = {{{TabNet}}},
  author = {Arik, Sercan O. and Pfister, Tomas},
  year = {2020},
  month = dec,
  number = {arXiv:1908.07442},
  eprint = {1908.07442},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1908.07442},
  urldate = {2024-02-01},
  abstract = {We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other neural network and decision tree variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into the global model behavior. Finally, for the first time to our knowledge, we demonstrate self-supervised learning for tabular data, significantly improving performance with unsupervised representation learning when unlabeled data is abundant.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/leo/Zotero/storage/7L5V6BDE/Arik and Pfister - 2020 - TabNet Attentive Interpretable Tabular Learning.pdf;/Users/leo/Zotero/storage/59JXEBSG/1908.html}
}

@misc{barrybeckerAdult1996,
  title = {Adult},
  author = {Barry Becker, Ronny Kohavi},
  year = {1996},
  publisher = {{UCI Machine Learning Repository}},
  doi = {10.24432/C5XW20},
  urldate = {2023-11-16}
}

@inproceedings{bayardoScalingAllPairs2007,
  title = {Scaling up All Pairs Similarity Search},
  booktitle = {Proceedings of the 16th International Conference on {{World Wide Web}}},
  author = {Bayardo, Roberto J. and Ma, Yiming and Srikant, Ramakrishnan},
  year = {2007},
  month = may,
  pages = {131--140},
  publisher = {{ACM}},
  address = {{Banff Alberta Canada}},
  doi = {10.1145/1242572.1242591},
  urldate = {2023-12-01},
  abstract = {Given a large collection of sparse vector data in a high dimensional space, we investigate the problem of finding all pairs of vectors whose similarity score (as determined by a function such as cosine distance) is above a given threshold. We propose a simple algorithm based on novel indexing and optimization strategies that solves this problem without relying on approximation methods or extensive parameter tuning. We show the approach efficiently handles a variety of datasets across a wide setting of similarity thresholds, with large speedups over previous state-of-the-art approaches.},
  isbn = {978-1-59593-654-7},
  langid = {english},
  file = {/Users/leo/Zotero/storage/GCT42NCK/Bayardo et al. - 2007 - Scaling up all pairs similarity search.pdf}
}

@misc{bidermanPythiaSuiteAnalyzing2023,
  title = {Pythia: {{A Suite}} for {{Analyzing Large Language Models Across Training}} and {{Scaling}}},
  shorttitle = {Pythia},
  author = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and Skowron, Aviya and Sutawika, Lintang and {van der Wal}, Oskar},
  year = {2023},
  month = may,
  number = {arXiv:2304.01373},
  eprint = {2304.01373},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.01373},
  urldate = {2023-11-24},
  abstract = {How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce {\textbackslash}textit\{Pythia\}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend {\textbackslash}textit\{Pythia\} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at {\textbackslash}url\{https://github.com/EleutherAI/pythia\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/leo/Zotero/storage/K9N4FB57/Biderman et al. - 2023 - Pythia A Suite for Analyzing Large Language Model.pdf;/Users/leo/Zotero/storage/ZQ77EN8M/2304.html}
}

@article{bojanowskiEnrichingWordVectors2017,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  year = {2017},
  month = jun,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {135--146},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00051},
  urldate = {2023-12-01},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
  file = {/Users/leo/Zotero/storage/PIS93F33/Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf;/Users/leo/Zotero/storage/MWQZM5ZZ/Enriching-Word-Vectors-with-Subword-Information.html}
}

@misc{borisovLanguageModelsAre2023,
  title = {Language {{Models}} Are {{Realistic Tabular Data Generators}}},
  author = {Borisov, Vadim and Se{\ss}ler, Kathrin and Leemann, Tobias and Pawelczyk, Martin and Kasneci, Gjergji},
  year = {2023},
  month = apr,
  number = {arXiv:2210.06280},
  eprint = {2210.06280},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-04},
  abstract = {Tabular data is among the oldest and most ubiquitous forms of data. However, the generation of synthetic samples with the original data's characteristics remains a significant challenge for tabular data. While many generative models from the computer vision domain, such as variational autoencoders or generative adversarial networks, have been adapted for tabular data generation, less research has been directed towards recent transformer-based large language models (LLMs), which are also generative in nature. To this end, we propose GReaT (Generation of Realistic Tabular data), which exploits an auto-regressive generative LLM to sample synthetic and yet highly realistic tabular data. Furthermore, GReaT can model tabular data distributions by conditioning on any subset of features; the remaining features are sampled without additional overhead. We demonstrate the effectiveness of the proposed approach in a series of experiments that quantify the validity and quality of the produced data samples from multiple angles. We find that GReaT maintains state-of-the-art performance across numerous real-world and synthetic data sets with heterogeneous feature types coming in various sizes.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/DBK2C4TY/Borisov et al. - 2023 - Language Models are Realistic Tabular Data Generat.pdf;/Users/leo/Zotero/storage/ALLLGLGK/2210.html}
}

@misc{carballoTabTextFlexibleContextual2023,
  title = {{{TabText}}: {{A Flexible}} and {{Contextual Approach}} to {{Tabular Data Representation}}},
  shorttitle = {{{TabText}}},
  author = {Carballo, Kimberly Villalobos and Na, Liangyuan and Ma, Yu and Boussioux, L{\'e}onard and Zeng, Cynthia and Soenksen, Luis R. and Bertsimas, Dimitris},
  year = {2023},
  month = jul,
  number = {arXiv:2206.10381},
  eprint = {2206.10381},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.10381},
  urldate = {2023-10-04},
  abstract = {Tabular data is essential for applying machine learning tasks across various industries. However, traditional data processing methods do not fully utilize all the information available in the tables, ignoring important contextual information such as column header descriptions. In addition, pre-processing data into a tabular format can remain a labor-intensive bottleneck in model development. This work introduces TabText, a processing and feature extraction framework that extracts contextual information from tabular data structures. TabText addresses processing difficulties by converting the content into language and utilizing pre-trained large language models (LLMs). We evaluate our framework on nine healthcare prediction tasks ranging from patient discharge, ICU admission, and mortality. We show that 1) applying our TabText framework enables the generation of high-performing and simple machine learning baseline models with minimal data pre-processing, and 2) augmenting pre-processed tabular data with TabText representations improves the average and worst-case AUC performance of standard machine learning models by as much as 6\%.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/EX2ERPLJ/Carballo et al. - 2023 - TabText A Flexible and Contextual Approach to Tab.pdf;/Users/leo/Zotero/storage/L26AYRQF/2206.html}
}

@misc{carballoTabTextFlexibleContextual2023a,
  title = {{{TabText}}: {{A Flexible}} and {{Contextual Approach}} to {{Tabular Data Representation}}},
  shorttitle = {{{TabText}}},
  author = {Carballo, Kimberly Villalobos and Na, Liangyuan and Ma, Yu and Boussioux, L{\'e}onard and Zeng, Cynthia and Soenksen, Luis R. and Bertsimas, Dimitris},
  year = {2023},
  month = jul,
  number = {arXiv:2206.10381},
  eprint = {2206.10381},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-04},
  abstract = {Tabular data is essential for applying machine learning tasks across various industries. However, traditional data processing methods do not fully utilize all the information available in the tables, ignoring important contextual information such as column header descriptions. In addition, pre-processing data into a tabular format can remain a labor-intensive bottleneck in model development. This work introduces TabText, a processing and feature extraction framework that extracts contextual information from tabular data structures. TabText addresses processing difficulties by converting the content into language and utilizing pre-trained large language models (LLMs). We evaluate our framework on nine healthcare prediction tasks ranging from patient discharge, ICU admission, and mortality. We show that 1) applying our TabText framework enables the generation of high-performing and simple machine learning baseline models with minimal data pre-processing, and 2) augmenting pre-processed tabular data with TabText representations improves the average and worst-case AUC performance of standard machine learning models by as much as 6\%.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/BY37MR4Z/Carballo et al. - 2023 - TabText A Flexible and Contextual Approach to Tab.pdf;/Users/leo/Zotero/storage/2NLVVAEP/2206.html}
}

@article{cerdaEncodingHighcardinalityString2022,
  title = {Encoding High-Cardinality String Categorical Variables},
  author = {Cerda, Patricio and Varoquaux, Ga{\"e}l},
  year = {2022},
  month = mar,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {34},
  number = {3},
  eprint = {1907.01860},
  primaryclass = {cs, stat},
  pages = {1164--1176},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2020.2992529},
  urldate = {2023-10-04},
  abstract = {Statistical models usually require vector representations of categorical variables, using for instance one-hot encoding. This strategy breaks down when the number of categories grows, as it creates high-dimensional feature vectors. Additionally, for string entries, one-hot encoding does not capture information in their representation.Here, we seek low-dimensional encoding of high-cardinality string categorical variables. Ideally, these should be: scalable to many categories; interpretable to end users; and facilitate statistical analysis. We introduce two encoding approaches for string categories: a Gamma-Poisson matrix factorization on substring counts, and the min-hash encoder, for fast approximation of string similarities. We show that min-hash turns set inclusions into inequality relations that are easier to learn. Both approaches are scalable and streamable. Experiments on real and simulated data show that these methods improve supervised learning with high-cardinality categorical variables. We recommend the following: if scalability is central, the min-hash encoder is the best option as it does not require any data fit; if interpretability is important, the Gamma-Poisson factorization is the best alternative, as it can be interpreted as one-hot encoding on inferred categories with informative feature names. Both models enable autoML on the original string entries as they remove the need for feature engineering or data cleaning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/leo/Zotero/storage/UFHIX9ZP/Cerda and Varoquaux - 2022 - Encoding high-cardinality string categorical varia.pdf;/Users/leo/Zotero/storage/QM6W4WY7/1907.html}
}

@article{cerdaSimilarityEncodingLearning2018,
  title = {Similarity Encoding for Learning with Dirty Categorical Variables},
  author = {Cerda, Patricio and Varoquaux, Ga{\"e}l and K{\'e}gl, Bal{\'a}zs},
  year = {2018},
  month = sep,
  journal = {Machine Learning},
  volume = {107},
  number = {8},
  pages = {1477--1494},
  issn = {1573-0565},
  doi = {10.1007/s10994-018-5724-2},
  urldate = {2024-01-29},
  abstract = {For statistical learning, categorical variables in a table are usually considered as discrete entities and encoded separately to feature vectors, e.g., with one-hot encoding. ``Dirty'' non-curated data give rise to categorical variables with a very high cardinality but redundancy: several categories reflect the same entity. In databases, this issue is typically solved with a deduplication step. We show that a simple approach that exposes the redundancy to the learning algorithm brings significant gains. We study a generalization of one-hot encoding, similarity encoding, that builds feature vectors from similarities across categories. We perform a thorough empirical validation on non-curated tables, a problem seldom studied in machine learning. Results on seven real-world datasets show that similarity encoding brings significant gains in predictive performance in comparison with known encoding methods for categories or strings, notably one-hot encoding and bag of character n-grams. We draw practical recommendations for encoding dirty categories: 3-gram similarity appears to be a good choice to capture morphological resemblance. For very high-cardinalities, dimensionality reduction significantly reduces the computational cost with little loss in performance: random projections or choosing a subset of prototype categories still outperform classic encoding approaches.},
  langid = {english},
  keywords = {Categorical variables,Dirty data,Statistical learning,String similarity measures},
  file = {/Users/leo/Zotero/storage/67J3ZRS7/Cerda et al. - 2018 - Similarity encoding for learning with dirty catego.pdf}
}

@misc{chenExcelFormerNeuralNetwork2023,
  title = {{{ExcelFormer}}: {{A Neural Network Surpassing GBDTs}} on {{Tabular Data}}},
  shorttitle = {{{ExcelFormer}}},
  author = {Chen, Jintai and Yan, Jiahuan and Chen, Danny Ziyi and Wu, Jian},
  year = {2023},
  month = jan,
  number = {arXiv:2301.02819},
  eprint = {2301.02819},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.02819},
  urldate = {2024-02-01},
  abstract = {Though deep neural networks have gained enormous successes in various fields (e.g., computer vision) with supervised learning, they have so far been still trailing after the performances of GBDTs on tabular data. Delving into this task, we determine that a judicious handling of feature interactions and feature representation is crucial to the effectiveness of neural networks on tabular data. We develop a novel neural network called ExcelFormer, which alternates in turn between two attention modules that shrewdly manipulate feature interactions and feature representation updates, respectively. A bespoke training methodology is jointly introduced to facilitate model performances. Specifically, by initializing parameters with minuscule values, these attention modules are attenuated when the training begins, and the effects of feature interactions and representation updates grow progressively up to optimum levels under the guidance of our proposed specific regularization schemes Feat-Mix and Hidden-Mix as the training proceeds. Experiments on 28 public tabular datasets show that our ExcelFormer approach is superior to extensively-tuned GBDTs, which is an unprecedented progress of deep neural networks on supervised tabular learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/NK9IBD9R/Chen et al. - 2023 - ExcelFormer A Neural Network Surpassing GBDTs on .pdf;/Users/leo/Zotero/storage/HV5SWVX2/2301.html}
}

@article{chenImputing2022,
  title = {Imputing Out-of-Vocabulary Embeddings with {{LOVE}} Makes Language Models Robust with Little Cost},
  author = {Chen, Lihu and Varoquaux, Ga{\"e}l and Suchanek, Fabian M},
  year = {2022},
  journal = {arXiv preprint arXiv:2203.07860},
  eprint = {2203.07860},
  archiveprefix = {arxiv}
}

@article{crossleyLarge2023,
  title = {A Large-Scaled Corpus for Assessing Text Readability},
  author = {Crossley, Scott and Heintz, Aron and Choi, Joon Suh and Batchelor, Jordan and Karimi, Mehrnoush and Malatinszky, Agnes},
  year = {2023},
  journal = {Behavior Research Methods},
  volume = {55},
  number = {2},
  pages = {491--507},
  publisher = {{Springer}}
}

@article{cvetkov-ilievRelationalDataEmbeddings2022,
  title = {Relational {{Data Embeddings}} for {{Feature Enrichment}} with {{Background Information}}},
  author = {{Cvetkov-Iliev}, Alexis and Allauzen, Alexandre and Varoquaux, Ga{\"e}l},
  year = {2022},
  journal = {Machine Learning},
  volume = {112},
  publisher = {{Springer Verlag}},
  doi = {10.1007/s10994-022-06277-7},
  urldate = {2023-10-04},
  abstract = {For many machine-learning tasks, augmenting the data table at hand with features built from external sources is key to improving performance. For instance, estimating housing prices benefits from background information on the location, such as the population density or the average income. However, this information must often be assembled across many tables, requiring time and expertise from the data scientist. Instead, we propose to replace human-crafted features by vectorial representations of entities (e.g. cities) that capture the corresponding information. We represent the relational data on the entities as a graph and adapt graph-embedding methods to create feature vectors for each entity. We show that two technical ingredients are crucial: modeling well the different relationships between entities, and capturing numerical attributes. We adapt knowledge graph embedding methods that were primarily designed for graph completion. Yet, they model only discrete entities, while creating good feature vectors from relational data also requires capturing numerical attributes. For this, we introduce KEN: Knowledge Embedding with Numbers. We thoroughly evaluate approaches to enrich features with background information on 7 prediction tasks. We show that a good embedding model coupled with KEN can perform better than manually handcrafted features, while requiring much less human effort. It is also competitive with combinatorial feature engineering methods, but much more scalable. Our approach can be applied to huge databases, creating general-purpose feature vectors reusable in various downstream tasks.},
  keywords = {feature engineering,feature enrichment,knowledge graph embedding},
  file = {/Users/leo/Zotero/storage/QT7WAZS5/Cvetkov-Iliev et al. - 2022 - Relational Data Embeddings for Feature Enrichment .pdf}
}

@article{deazambujaXWinesWineDataset2023,
  title = {X-{{Wines}}: {{A Wine Dataset}} for {{Recommender Systems}} and {{Machine Learning}}},
  shorttitle = {X-{{Wines}}},
  author = {{de Azambuja}, Rog{\'e}rio Xavier and Morais, A. Jorge and Filipe, V{\'i}tor},
  year = {2023},
  month = mar,
  journal = {Big Data and Cognitive Computing},
  volume = {7},
  number = {1},
  pages = {20},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2504-2289},
  doi = {10.3390/bdcc7010020},
  urldate = {2023-11-30},
  abstract = {In the current technological scenario of artificial intelligence growth, especially using machine learning, large datasets are necessary. Recommender systems appear with increasing frequency with different techniques for information filtering. Few large wine datasets are available for use with wine recommender systems. This work presents X-Wines, a new and consistent wine dataset containing 100,000 instances and 21 million real evaluations carried out by users. Data were collected on the open Web in 2022 and pre-processed for wider free use. They refer to the scale 1{\textendash}5 ratings carried out over a period of 10 years (2012{\textendash}2021) for wines produced in 62 different countries. A demonstration of some applications using X-Wines in the scope of recommender systems with deep learning algorithms is also presented.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {big data,deep learning,machine learning,recommender systems,wine reviews},
  file = {/Users/leo/Zotero/storage/GW4S2PAW/de Azambuja et al. - 2023 - X-Wines A Wine Dataset for Recommender Systems an.pdf}
}

@misc{dengTURLTableUnderstanding2020,
  title = {{{TURL}}: {{Table Understanding}} through {{Representation Learning}}},
  shorttitle = {{{TURL}}},
  author = {Deng, Xiang and Sun, Huan and Lees, Alyssa and Wu, You and Yu, Cong},
  year = {2020},
  month = dec,
  number = {arXiv:2006.14806},
  eprint = {2006.14806},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-11-24},
  abstract = {Relational tables on the Web store a vast amount of knowledge. Owing to the wealth of such tables, there has been tremendous progress on a variety of tasks in the area of table understanding. However, existing work generally relies on heavily-engineered task-specific features and model architectures. In this paper, we present TURL, a novel framework that introduces the pre-training/fine-tuning paradigm to relational Web tables. During pre-training, our framework learns deep contextualized representations on relational tables in an unsupervised manner. Its universal model design with pre-trained representations can be applied to a wide range of tasks with minimal task-specific fine-tuning. Specifically, we propose a structure-aware Transformer encoder to model the row-column structure of relational tables, and present a new Masked Entity Recovery (MER) objective for pre-training to capture the semantics and knowledge in large-scale unlabeled data. We systematically evaluate TURL with a benchmark consisting of 6 different tasks for table understanding (e.g., relation extraction, cell filling). We show that TURL generalizes well to all tasks and substantially outperforms existing methods in almost all instances.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/Users/leo/Zotero/storage/YJ4BMTQJ/Deng et al. - 2020 - TURL Table Understanding through Representation L.pdf;/Users/leo/Zotero/storage/JB2626FC/2006.html}
}

@misc{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2023-12-03},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/leo/Zotero/storage/ZQ7VWJVW/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/Users/leo/Zotero/storage/ZVQE42PB/1810.html}
}

@article{dinhLIFTLanguageInterfacedFineTuning2022,
  title = {{{LIFT}}: {{Language-Interfaced Fine-Tuning}} for {{Non-language Machine Learning Tasks}}},
  shorttitle = {{{LIFT}}},
  author = {Dinh, Tuan and Zeng, Yuchen and Zhang, Ruisu and Lin, Ziqian and Gira, Michael and Rajput, Shashank and Sohn, Jy-yong and Papailiopoulos, Dimitris and Lee, Kangwook},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {11763--11784},
  urldate = {2023-10-04},
  langid = {english},
  file = {/Users/leo/Zotero/storage/Z8TX57TT/Dinh et al. - 2022 - LIFT Language-Interfaced Fine-Tuning for Non-lang.pdf}
}

@article{dorogushCatboost2018,
  title = {{{CatBoost}}: Gradient Boosting with Categorical Features Support},
  author = {Dorogush, Anna Veronika and Ershov, Vasily and Gulin, Andrey},
  year = {2018},
  journal = {arXiv preprint arXiv:1810.11363},
  eprint = {1810.11363},
  archiveprefix = {arxiv}
}

@misc{eggertTabLibDataset627M2023,
  title = {{{TabLib}}: {{A Dataset}} of {{627M Tables}} with {{Context}}},
  shorttitle = {{{TabLib}}},
  author = {Eggert, Gus and Huo, Kevin and Biven, Mike and Waugh, Justin},
  year = {2023},
  month = oct,
  number = {arXiv:2310.07875},
  eprint = {2310.07875},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.07875},
  urldate = {2024-02-01},
  abstract = {It is well-established that large, diverse datasets play a pivotal role in the performance of modern AI systems for text and image modalities. However, there are no datasets for tabular data of comparable size and diversity to those available for text and images. Thus we present "TabLib'', a compilation of 627 million tables totaling 69 TiB, along with 867B tokens of context. TabLib was extracted from numerous file formats, including CSV, HTML, SQLite, PDF, Excel, and others, sourced from GitHub and Common Crawl. The size and diversity of TabLib offer considerable promise in the table modality, reminiscent of the original promise of foundational datasets for text and images, such as The Pile and LAION.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Databases,Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/ALXWR7XZ/Eggert et al. - 2023 - TabLib A Dataset of 627M Tables with Context.pdf;/Users/leo/Zotero/storage/A346I2QT/2310.html}
}

@misc{gaoSimCSESimpleContrastive2022,
  title = {{{SimCSE}}: {{Simple Contrastive Learning}} of {{Sentence Embeddings}}},
  shorttitle = {{{SimCSE}}},
  author = {Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  year = {2022},
  month = may,
  number = {arXiv:2104.08821},
  eprint = {2104.08821},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.08821},
  urldate = {2023-11-24},
  abstract = {This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using "entailment" pairs as positives and "contradiction" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3\% and 81.6\% Spearman's correlation respectively, a 4.2\% and 2.2\% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/Z37HSIG9/Gao et al. - 2022 - SimCSE Simple Contrastive Learning of Sentence Em.pdf;/Users/leo/Zotero/storage/E4VCNZ5E/2104.html}
}

@article{gardnerSubgroupRobustnessGrows,
  title = {Subgroup {{Robustness Grows On Trees}}: {{An Empirical Baseline Investigation}}},
  author = {Gardner, Josh and Popovic, Zoran and Schmidt, Ludwig},
  langid = {english},
  file = {/Users/leo/Zotero/storage/DE29U8EQ/Gardner et al. - Subgroup Robustness Grows On Trees An Empirical B.pdf}
}

@article{gardnerSubgroupRobustnessGrows2022,
  title = {Subgroup {{Robustness Grows On Trees}}: {{An Empirical Baseline Investigation}}},
  shorttitle = {Subgroup {{Robustness Grows On Trees}}},
  author = {Gardner, Josh and Popovic, Zoran and Schmidt, Ludwig},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {9939--9954},
  urldate = {2024-02-01},
  langid = {english},
  file = {/Users/leo/Zotero/storage/769RSNXX/Gardner et al. - 2022 - Subgroup Robustness Grows On Trees An Empirical B.pdf}
}

@misc{gorishniyRevisitingDeepLearning2023,
  title = {Revisiting {{Deep Learning Models}} for {{Tabular Data}}},
  author = {Gorishniy, Yury and Rubachev, Ivan and Khrulkov, Valentin and Babenko, Artem},
  year = {2023},
  month = oct,
  number = {arXiv:2106.11959},
  eprint = {2106.11959},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.11959},
  urldate = {2024-02-01},
  abstract = {The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets. However, the proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols. As a result, it is unclear for both researchers and practitioners what models perform best. Additionally, the field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems. In this work, we perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures. The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works. The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solutions on most tasks. Both models are compared to many existing architectures on a diverse set of tasks under the same training and tuning protocols. We also compare the best DL models with Gradient Boosted Decision Trees and conclude that there is still no universally superior solution.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/E78F5KBX/Gorishniy et al. - 2023 - Revisiting Deep Learning Models for Tabular Data.pdf;/Users/leo/Zotero/storage/XX8LB4AV/2106.html}
}

@misc{gorishniyTabRTabularDeep2023,
  title = {{{TabR}}: {{Tabular Deep Learning Meets Nearest Neighbors}} in 2023},
  shorttitle = {{{TabR}}},
  author = {Gorishniy, Yury and Rubachev, Ivan and Kartashev, Nikolay and Shlenskii, Daniil and Kotelnikov, Akim and Babenko, Artem},
  year = {2023},
  month = oct,
  number = {arXiv:2307.14338},
  eprint = {2307.14338},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-02-01},
  abstract = {Deep learning (DL) models for tabular data problems (e.g. classification, regression) are currently receiving increasingly more attention from researchers. However, despite the recent efforts, the non-DL algorithms based on gradient-boosted decision trees (GBDT) remain a strong go-to solution for these problems. One of the research directions aimed at improving the position of tabular DL involves designing so-called retrieval-augmented models. For a target object, such models retrieve other objects (e.g. the nearest neighbors) from the available training data and use their features and labels to make a better prediction. In this work, we present TabR -- essentially, a feed-forward network with a custom k-Nearest-Neighbors-like component in the middle. On a set of public benchmarks with datasets up to several million objects, TabR marks a big step forward for tabular DL: it demonstrates the best average performance among tabular DL models, becomes the new state-of-the-art on several datasets, and even outperforms GBDT models on the recently proposed "GBDT-friendly" benchmark (see Figure 1). Among the important findings and technical details powering TabR, the main ones lie in the attention-like mechanism that is responsible for retrieving the nearest neighbors and extracting valuable signal from them. In addition to the much higher performance, TabR is simple and significantly more efficient compared to prior retrieval-based tabular DL models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/DR4D6KQZ/Gorishniy et al. - 2023 - TabR Tabular Deep Learning Meets Nearest Neighbor.pdf;/Users/leo/Zotero/storage/4C9PN5PV/2307.html}
}

@misc{grinsztajnWhyTreebasedModels2022,
  title = {Why Do Tree-Based Models Still Outperform Deep Learning on Tabular Data?},
  author = {Grinsztajn, L{\'e}o and Oyallon, Edouard and Varoquaux, Ga{\"e}l},
  year = {2022},
  month = jul,
  number = {arXiv:2207.08815},
  eprint = {2207.08815},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.08815},
  urldate = {2023-11-24},
  abstract = {While deep learning has enabled tremendous progress on text and image datasets, its superiority on tabular data is not clear. We contribute extensive benchmarks of standard and novel deep learning methods as well as tree-based models such as XGBoost and Random Forests, across a large number of datasets and hyperparameter combinations. We define a standard set of 45 datasets from varied domains with clear characteristics of tabular data and a benchmarking methodology accounting for both fitting models and finding good hyperparameters. Results show that tree-based models remain state-of-the-art on medium-sized data (\${\textbackslash}sim\$10K samples) even without accounting for their superior speed. To understand this gap, we conduct an empirical investigation into the differing inductive biases of tree-based models and Neural Networks (NNs). This leads to a series of challenges which should guide researchers aiming to build tabular-specific NNs: 1. be robust to uninformative features, 2. preserve the orientation of the data, and 3. be able to easily learn irregular functions. To stimulate research on tabular architectures, we contribute a standard benchmark and raw data for baselines: every point of a 20 000 compute hours hyperparameter search for each learner.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/leo/Zotero/storage/Y66S5HNY/Grinsztajn et al. - 2022 - Why do tree-based models still outperform deep lea.pdf;/Users/leo/Zotero/storage/WM8UTN9U/2207.html}
}

@misc{gurneeLanguageModelsRepresent2023,
  title = {Language {{Models Represent Space}} and {{Time}}},
  author = {Gurnee, Wes and Tegmark, Max},
  year = {2023},
  month = oct,
  number = {arXiv:2310.02207},
  eprint = {2310.02207},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.02207},
  urldate = {2023-11-15},
  abstract = {The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a coherent model of the data generating process -- a world model. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual ``space neurons'' and ``time neurons'' that reliably encode spatial and temporal coordinates. Our analysis demonstrates that modern LLMs acquire structured knowledge about fundamental dimensions such as space and time, supporting the view that they learn not merely superficial statistics, but literal world models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/HPLWPRAM/Gurnee and Tegmark - 2023 - Language Models Represent Space and Time.pdf;/Users/leo/Zotero/storage/PBJG4P6N/2310.html}
}

@misc{gurneeLanguageModelsRepresent2023a,
  title = {Language {{Models Represent Space}} and {{Time}}},
  author = {Gurnee, Wes and Tegmark, Max},
  year = {2023},
  month = oct,
  number = {arXiv:2310.02207},
  eprint = {2310.02207},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.02207},
  urldate = {2023-11-24},
  abstract = {The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a coherent model of the data generating process -- a world model. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual ``space neurons'' and ``time neurons'' that reliably encode spatial and temporal coordinates. Our analysis demonstrates that modern LLMs acquire structured knowledge about fundamental dimensions such as space and time, supporting the view that they learn not merely superficial statistics, but literal world models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/QLUMZYHN/Gurnee and Tegmark - 2023 - Language Models Represent Space and Time.pdf;/Users/leo/Zotero/storage/MACSZCTR/2310.html}
}

@misc{hanComprehensiveSurveyVector2023,
  title = {A {{Comprehensive Survey}} on {{Vector Database}}: {{Storage}} and {{Retrieval Technique}}, {{Challenge}}},
  shorttitle = {A {{Comprehensive Survey}} on {{Vector Database}}},
  author = {Han, Yikun and Liu, Chunjiang and Wang, Pengfei},
  year = {2023},
  month = oct,
  number = {arXiv:2310.11703},
  eprint = {2310.11703},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-11-24},
  abstract = {A vector database is used to store high-dimensional data that cannot be characterized by traditional DBMS. Although there are not many articles describing existing or introducing new vector database architectures, the approximate nearest neighbor search problem behind vector databases has been studied for a long time, and considerable related algorithmic articles can be found in the literature. This article attempts to comprehensively review relevant algorithms to provide a general understanding of this booming research area. The basis of our framework categorises these studies by the approach of solving ANNS problem, respectively hash-based, tree-based, graph-based and quantization-based approaches. Then we present an overview of existing challenges for vector databases. Lastly, we sketch how vector databases can be combined with large language models and provide new possibilities.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases},
  file = {/Users/leo/Zotero/storage/YCUQCQ62/Han et al. - 2023 - A Comprehensive Survey on Vector Database Storage.pdf;/Users/leo/Zotero/storage/DLLQ4P4J/2310.html}
}

@inproceedings{harariFewShotTabularData2022,
  title = {Few-{{Shot Tabular Data Enrichment Using Fine-Tuned Transformer Architectures}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Harari, Asaf and Katz, Gilad},
  year = {2022},
  month = may,
  pages = {1577--1591},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.111},
  urldate = {2023-10-04},
  abstract = {The enrichment of tabular datasets using external sources has gained significant attention in recent years. Existing solutions, however, either ignore external unstructured data completely or devise dataset-specific solutions. In this study we proposed Few-Shot Transformer based Enrichment (FeSTE), a generic and robust framework for the enrichment of tabular datasets using unstructured data. By training over multiple datasets, our approach is able to develop generic models that can be applied to additional datasets with minimal training (i.e., few-shot). Our approach is based on an adaptation of BERT, for which we present a novel fine-tuning approach that reformulates the tuples of the datasets as sentences. Our evaluation, conducted on 17 datasets, shows that FeSTE is able to generate high quality features and significantly outperform existing fine-tuning solutions.},
  file = {/Users/leo/Zotero/storage/4BGU4NFY/Harari and Katz - 2022 - Few-Shot Tabular Data Enrichment Using Fine-Tuned .pdf}
}

@misc{hegselmannTabLLMFewshotClassification2023,
  title = {{{TabLLM}}: {{Few-shot Classification}} of {{Tabular Data}} with {{Large Language Models}}},
  shorttitle = {{{TabLLM}}},
  author = {Hegselmann, Stefan and Buendia, Alejandro and Lang, Hunter and Agrawal, Monica and Jiang, Xiaoyi and Sontag, David},
  year = {2023},
  month = mar,
  number = {arXiv:2210.10723},
  eprint = {2210.10723},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.10723},
  urldate = {2023-10-04},
  abstract = {We study the application of large language models to zero-shot and few-shot classification of tabular data. We prompt the large language model with a serialization of the tabular data to a natural-language string, together with a short description of the classification problem. In the few-shot setting, we fine-tune the large language model using some labeled examples. We evaluate several serialization methods including templates, table-to-text models, and large language models. Despite its simplicity, we find that this technique outperforms prior deep-learning-based tabular classification methods on several benchmark datasets. In most cases, even zero-shot classification obtains non-trivial performance, illustrating the method's ability to exploit prior knowledge encoded in large language models. Unlike many deep learning methods for tabular datasets, this approach is also competitive with strong traditional baselines like gradient-boosted trees, especially in the very-few-shot setting.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/leo/Zotero/storage/5FUW3T2Q/Hegselmann et al. - 2023 - TabLLM Few-shot Classification of Tabular Data wi.pdf;/Users/leo/Zotero/storage/AU2A5BHN/2210.html}
}

@misc{hollmannLargeLanguageModels2023,
  title = {Large {{Language Models}} for {{Automated Data Science}}: {{Introducing CAAFE}} for {{Context-Aware Automated Feature Engineering}}},
  shorttitle = {Large {{Language Models}} for {{Automated Data Science}}},
  author = {Hollmann, Noah and M{\"u}ller, Samuel and Hutter, Frank},
  year = {2023},
  month = sep,
  number = {arXiv:2305.03403},
  eprint = {2305.03403},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-04},
  abstract = {As the field of automated machine learning (AutoML) advances, it becomes increasingly important to incorporate domain knowledge into these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to iteratively generate additional semantically meaningful features for tabular datasets based on the description of the dataset. The method produces both Python code for creating new features and explanations for the utility of the generated features. Despite being methodologically simple, CAAFE improves performance on 11 out of 14 datasets -- boosting mean ROC AUC performance from 0.798 to 0.822 across all dataset - similar to the improvement achieved by using a random forest instead of logistic regression on our datasets. Furthermore, CAAFE is interpretable by providing a textual explanation for each generated feature. CAAFE paves the way for more extensive semi-automation in data science tasks and emphasizes the significance of context-aware solutions that can extend the scope of AutoML systems to semantic AutoML. We release our \${\textbackslash}href\{https://github.com/automl/CAAFE\}\{code\}\$, a simple \${\textbackslash}href\{https://colab.research.google.com/drive/1mCA8xOAJZ4MaB\_alZvyARTMjhl6RZf0a\}\{demo\}\$ and a \${\textbackslash}href\{https://pypi.org/project/caafe/\}\{python{\textbackslash} package\}\$.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/EQ5L4VBK/Hollmann et al. - 2023 - Large Language Models for Automated Data Science .pdf;/Users/leo/Zotero/storage/SB7SNRMN/2305.html}
}

@misc{hollmannLargeLanguageModels2023a,
  title = {Large {{Language Models}} for {{Automated Data Science}}: {{Introducing CAAFE}} for {{Context-Aware Automated Feature Engineering}}},
  shorttitle = {Large {{Language Models}} for {{Automated Data Science}}},
  author = {Hollmann, Noah and M{\"u}ller, Samuel and Hutter, Frank},
  year = {2023},
  month = sep,
  number = {arXiv:2305.03403},
  eprint = {2305.03403},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-04},
  abstract = {As the field of automated machine learning (AutoML) advances, it becomes increasingly important to incorporate domain knowledge into these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to iteratively generate additional semantically meaningful features for tabular datasets based on the description of the dataset. The method produces both Python code for creating new features and explanations for the utility of the generated features. Despite being methodologically simple, CAAFE improves performance on 11 out of 14 datasets -- boosting mean ROC AUC performance from 0.798 to 0.822 across all dataset - similar to the improvement achieved by using a random forest instead of logistic regression on our datasets. Furthermore, CAAFE is interpretable by providing a textual explanation for each generated feature. CAAFE paves the way for more extensive semi-automation in data science tasks and emphasizes the significance of context-aware solutions that can extend the scope of AutoML systems to semantic AutoML. We release our \${\textbackslash}href\{https://github.com/automl/CAAFE\}\{code\}\$, a simple \${\textbackslash}href\{https://colab.research.google.com/drive/1mCA8xOAJZ4MaB\_alZvyARTMjhl6RZf0a\}\{demo\}\$ and a \${\textbackslash}href\{https://pypi.org/project/caafe/\}\{python{\textbackslash} package\}\$.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/MPVW7GEU/Hollmann et al. - 2023 - Large Language Models for Automated Data Science .pdf;/Users/leo/Zotero/storage/M43WUZUR/2305.html}
}

@misc{hollmannTabPFNTransformerThat2023,
  title = {{{TabPFN}}: {{A Transformer That Solves Small Tabular Classification Problems}} in a {{Second}}},
  shorttitle = {{{TabPFN}}},
  author = {Hollmann, Noah and M{\"u}ller, Samuel and Eggensperger, Katharina and Hutter, Frank},
  year = {2023},
  month = sep,
  number = {arXiv:2207.01848},
  eprint = {2207.01848},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.01848},
  urldate = {2023-10-05},
  abstract = {We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods. TabPFN performs in-context learning (ICL), it learns to make predictions using sequences of labeled examples (x, f(x)) given in the input, without requiring further parameter updates. TabPFN is fully entailed in the weights of our network, which accepts training and test samples as a set-valued input and yields predictions for the entire test set in a single forward pass. TabPFN is a Prior-Data Fitted Network (PFN) and is trained offline once, to approximate Bayesian inference on synthetic datasets drawn from our prior. This prior incorporates ideas from causal reasoning: It entails a large space of structural causal models with a preference for simple structures. On the 18 datasets in the OpenML-CC18 suite that contain up to 1 000 training data points, up to 100 purely numerical features without missing values, and up to 10 classes, we show that our method clearly outperforms boosted trees and performs on par with complex state-of-the-art AutoML systems with up to 230\${\textbackslash}times\$ speedup. This increases to a 5 700\${\textbackslash}times\$ speedup when using a GPU. We also validate these results on an additional 67 small numerical datasets from OpenML. We provide all our code, the trained TabPFN, an interactive browser demo and a Colab notebook at https://github.com/automl/TabPFN.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/leo/Zotero/storage/T48MJB7V/Hollmann et al. - 2023 - TabPFN A Transformer That Solves Small Tabular Cl.pdf;/Users/leo/Zotero/storage/MH66LXR3/2207.html}
}

@article{hulsebosGitTablesLargeScaleCorpus2023,
  title = {{{GitTables}}: {{A Large-Scale Corpus}} of {{Relational Tables}}},
  shorttitle = {{{GitTables}}},
  author = {Hulsebos, Madelon and Demiralp, {\c C}a{\u g}atay and Groth, Paul},
  year = {2023},
  month = may,
  journal = {Proceedings of the ACM on Management of Data},
  volume = {1},
  number = {1},
  eprint = {2106.07258},
  primaryclass = {cs},
  pages = {1--17},
  issn = {2836-6573},
  doi = {10.1145/3588710},
  urldate = {2024-02-01},
  abstract = {The success of deep learning has sparked interest in improving relational table tasks, like data preparation and search, with table representation models trained on large table corpora. Existing table corpora primarily contain tables extracted from HTML pages, limiting the capability to represent offline database tables. To train and evaluate high-capacity models for applications beyond the Web, we need resources with tables that resemble relational database tables. Here we introduce GitTables, a corpus of 1M relational tables extracted from GitHub. Our continuing curation aims at growing the corpus to at least 10M tables. Analyses of GitTables show that its structure, content, and topical coverage differ significantly from existing table corpora. We annotate table columns in GitTables with semantic types, hierarchical relations and descriptions from Schema.org and DBpedia. The evaluation of our annotation pipeline on the T2Dv2 benchmark illustrates that our approach provides results on par with human annotations. We present three applications of GitTables, demonstrating its value for learned semantic type detection models, schema completion methods, and benchmarks for table-to-KG matching, data search, and preparation. We make the corpus and code available at https://gittables.github.io.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Databases,Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/JZXIHCI8/Hulsebos et al. - 2023 - GitTables A Large-Scale Corpus of Relational Tabl.pdf;/Users/leo/Zotero/storage/ESFUD6NX/2106.html}
}

@misc{jiangMistral7B2023,
  title = {Mistral {{7B}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, L{\'e}lio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2023},
  month = oct,
  number = {arXiv:2310.06825},
  eprint = {2310.06825},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.06825},
  urldate = {2023-11-24},
  abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/MR4UG8ZF/Jiang et al. - 2023 - Mistral 7B.pdf;/Users/leo/Zotero/storage/XBFXZQ7G/2310.html}
}

@inproceedings{joulinBagTricksEfficient2017,
  title = {Bag of {{Tricks}} for {{Efficient Text Classification}}},
  booktitle = {Proceedings of the 15th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Volume}} 2, {{Short Papers}}},
  author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  editor = {Lapata, Mirella and Blunsom, Phil and Koller, Alexander},
  year = {2017},
  month = apr,
  pages = {427--431},
  publisher = {{Association for Computational Linguistics}},
  address = {{Valencia, Spain}},
  urldate = {2023-12-03},
  abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.},
  file = {/Users/leo/Zotero/storage/BUYSNWRV/Joulin et al. - 2017 - Bag of Tricks for Efficient Text Classification.pdf}
}

@inproceedings{kanterDeepFeatureSynthesis2015,
  title = {Deep Feature Synthesis: {{Towards}} Automating Data Science Endeavors},
  shorttitle = {Deep Feature Synthesis},
  booktitle = {2015 {{IEEE International Conference}} on {{Data Science}} and {{Advanced Analytics}} ({{DSAA}})},
  author = {Kanter, James Max and Veeramachaneni, Kalyan},
  year = {2015},
  month = oct,
  pages = {1--10},
  publisher = {{IEEE}},
  address = {{Campus des Cordeliers, Paris, France}},
  doi = {10.1109/DSAA.2015.7344858},
  urldate = {2023-10-04},
  isbn = {978-1-4673-8272-4},
  file = {/Users/leo/Zotero/storage/YP2PK7VC/Kanter and Veeramachaneni - 2015 - Deep feature synthesis Towards automating data sc.pdf}
}

@misc{kaplanScalingLawsNeural2020,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year = {2020},
  month = jan,
  number = {arXiv:2001.08361},
  eprint = {2001.08361},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2001.08361},
  urldate = {2023-12-01},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/leo/Zotero/storage/A2M933N5/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf;/Users/leo/Zotero/storage/G78ZAS2Y/2001.html}
}

@article{kwiatkowskiNaturalQuestionsBenchmark2019,
  title = {Natural {{Questions}}: {{A Benchmark}} for {{Question Answering Research}}},
  shorttitle = {Natural {{Questions}}},
  author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
  editor = {Lee, Lillian and Johnson, Mark and Roark, Brian and Nenkova, Ani},
  year = {2019},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {7},
  pages = {452--466},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA}},
  doi = {10.1162/tacl_a_00276},
  urldate = {2023-11-24},
  abstract = {We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
  file = {/Users/leo/Zotero/storage/DPRIVJWU/Kwiatkowski et al. - 2019 - Natural Questions A Benchmark for Question Answer.pdf}
}

@misc{LargestDatasetAnalyzed,
  title = {Largest {{Dataset Analyzed}} - {{Poll Results}} and {{Trends}}},
  journal = {KDnuggets},
  urldate = {2023-11-24},
  abstract = {The results show that despite the deluge of Big Data, large majority still works in Gigabyte or Megabyte-size datasets. Data Scientists work with the largest-size datasets, followed by Data Engineers, Data Analysts, and Business Analysts. Read more for details.},
  chapter = {2020 Jul Opinions},
  howpublished = {https://www.kdnuggets.com/largest-dataset-analyzed-poll-results-and-trends},
  langid = {american},
  file = {/Users/leo/Zotero/storage/AEENR7DZ/poll-largest-dataset-analyzed-results.html}
}

@misc{levinTransferLearningDeep2023,
  title = {Transfer {{Learning}} with {{Deep Tabular Models}}},
  author = {Levin, Roman and Cherepanova, Valeriia and Schwarzschild, Avi and Bansal, Arpit and Bruss, C. Bayan and Goldstein, Tom and Wilson, Andrew Gordon and Goldblum, Micah},
  year = {2023},
  month = aug,
  number = {arXiv:2206.15306},
  eprint = {2206.15306},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.15306},
  urldate = {2024-02-01},
  abstract = {Recent work on deep learning for tabular data demonstrates the strong performance of deep tabular models, often bridging the gap between gradient boosted decision trees and neural networks. Accuracy aside, a major advantage of neural models is that they learn reusable features and are easily fine-tuned in new domains. This property is often exploited in computer vision and natural language applications, where transfer learning is indispensable when task-specific training data is scarce. In this work, we demonstrate that upstream data gives tabular neural networks a decisive advantage over widely used GBDT models. We propose a realistic medical diagnosis benchmark for tabular transfer learning, and we present a how-to guide for using upstream data to boost performance with a variety of tabular neural network architectures. Finally, we propose a pseudo-feature method for cases where the upstream and downstream feature sets differ, a tabular-specific problem widespread in real-world applications. Our code is available at https://github.com/LevinRoman/tabular-transfer-learning .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/leo/Zotero/storage/AXZ3IV48/Levin et al. - 2023 - Transfer Learning with Deep Tabular Models.pdf;/Users/leo/Zotero/storage/Q9FR4KHY/2206.html}
}

@misc{liAutoFuzzyJoinAutoProgramFuzzy2021,
  title = {Auto-{{FuzzyJoin}}: {{Auto-Program Fuzzy Similarity Joins Without Labeled Examples}}},
  shorttitle = {Auto-{{FuzzyJoin}}},
  author = {Li, Peng and Cheng, Xiang and Chu, Xu and He, Yeye and Chaudhuri, Surajit},
  year = {2021},
  month = mar,
  number = {arXiv:2103.04489},
  eprint = {2103.04489},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-11-24},
  abstract = {Fuzzy similarity join is an important database operator widely used in practice. So far the research community has focused exclusively on optimizing fuzzy join {\textbackslash}textit\{scalability\}. However, practitioners today also struggle to optimize fuzzy-join {\textbackslash}textit\{quality\}, because they face a daunting space of parameters (e.g., distance-functions, distance-thresholds, tokenization-options, etc.), and often have to resort to a manual trial-and-error approach to program these parameters in order to optimize fuzzy-join quality. This key challenge of automatically generating high-quality fuzzy-join programs has received surprisingly little attention thus far. In this work, we study the problem of "auto-program" fuzzy-joins. Leveraging a geometric interpretation of distance-functions, we develop an unsupervised {\textbackslash}textsc\{Auto-FuzzyJoin\} framework that can infer suitable fuzzy-join programs on given input tables, without requiring explicit human input such as labeled training data. Using {\textbackslash}textsc\{Auto-FuzzyJoin\}, users only need to provide two input tables \$L\$ and \$R\$, and a desired precision target \${\textbackslash}tau\$ (say 0.9). {\textbackslash}textsc\{Auto-FuzzyJoin\} leverages the fact that one of the input is a reference table to automatically program fuzzy-joins that meet the precision target \${\textbackslash}tau\$ in expectation, while maximizing fuzzy-join recall (defined as the number of correctly joined records). Experiments on both existing benchmarks and a new benchmark with 50 fuzzy-join tasks created from Wikipedia data suggest that the proposed {\textbackslash}textsc\{Auto-FuzzyJoin\} significantly outperforms existing unsupervised approaches, and is surprisingly competitive even against supervised approaches (e.g., Magellan and DeepMatcher) when 50{\textbackslash}\% of ground-truth labels are used as training data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Databases},
  file = {/Users/leo/Zotero/storage/Z2UUP5HI/Li et al. - 2021 - Auto-FuzzyJoin Auto-Program Fuzzy Similarity Join.pdf;/Users/leo/Zotero/storage/EB4U6NJ2/2103.html}
}

@misc{liSentenceEmbeddingsPretrained2020,
  title = {On the {{Sentence Embeddings}} from {{Pre-trained Language Models}}},
  author = {Li, Bohan and Zhou, Hao and He, Junxian and Wang, Mingxuan and Yang, Yiming and Li, Lei},
  year = {2020},
  month = nov,
  number = {arXiv:2011.05864},
  eprint = {2011.05864},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.05864},
  urldate = {2023-11-24},
  abstract = {Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at https://github.com/bohanli/BERT-flow.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/JI7RJU22/Li et al. - 2020 - On the Sentence Embeddings from Pre-trained Langua.pdf;/Users/leo/Zotero/storage/ZPYTEAKM/2011.html}
}

@misc{liuRoBERTaRobustlyOptimized2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  number = {arXiv:1907.11692},
  eprint = {1907.11692},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-04},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/leo/Zotero/storage/N7FU5DK6/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf;/Users/leo/Zotero/storage/I6YZVM4J/1907.html}
}

@misc{louniciMuddlingLabelRegularization2021b,
  title = {Muddling {{Label Regularization}}: {{Deep Learning}} for {{Tabular Datasets}}},
  shorttitle = {Muddling {{Label Regularization}}},
  author = {Lounici, Karim and Meziani, Katia and Riu, Benjamin},
  year = {2021},
  month = jun,
  number = {arXiv:2106.04462},
  eprint = {2106.04462},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.04462},
  urldate = {2024-02-01},
  abstract = {Deep Learning (DL) is considered the state-of-the-art in computer vision, speech recognition and natural language processing. Until recently, it was also widely accepted that DL is irrelevant for learning tasks on tabular data, especially in the small sample regime where ensemble methods are acknowledged as the gold standard. We present a new end-to-end differentiable method to train a standard FFNN. Our method, {\textbackslash}textbf\{Muddling labels for Regularization\} ({\textbackslash}texttt\{MLR\}), penalizes memorization through the generation of uninformative labels and the application of a differentiable close-form regularization scheme on the last hidden layer during training. {\textbackslash}texttt\{MLR\} outperforms classical NN and the gold standard (GBDT, RF) for regression and classification tasks on several datasets from the UCI database and Kaggle covering a large range of sample sizes and feature to sample ratios. Researchers and practitioners can use {\textbackslash}texttt\{MLR\} on its own as an off-the-shelf {\textbackslash}DL\{\} solution or integrate it into the most advanced ML pipelines.},
  archiveprefix = {arxiv},
  keywords = {68T07,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/RYMPLZD6/Lounici et al. - 2021 - Muddling Label Regularization Deep Learning for T.pdf;/Users/leo/Zotero/storage/P635MZM5/2106.html}
}

@misc{luccioniPowerHungryProcessing2023,
  title = {Power {{Hungry Processing}}: {{Watts Driving}} the {{Cost}} of {{AI Deployment}}?},
  shorttitle = {Power {{Hungry Processing}}},
  author = {Luccioni, Alexandra Sasha and Jernite, Yacine and Strubell, Emma},
  year = {2023},
  month = nov,
  number = {arXiv:2311.16863},
  eprint = {2311.16863},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2311.16863},
  urldate = {2023-12-03},
  abstract = {Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of "generality" comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and `general-purpose' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/JDP7RUP5/Luccioni et al. - 2023 - Power Hungry Processing Watts Driving the Cost of.pdf;/Users/leo/Zotero/storage/52EFNQXK/2311.html}
}



@misc{mcelfreshWhenNeuralNets2023,
  title = {When {{Do Neural Nets Outperform Boosted Trees}} on {{Tabular Data}}?},
  author = {McElfresh, Duncan and Khandagale, Sujay and Valverde, Jonathan and C, Vishak Prasad and Feuer, Benjamin and Hegde, Chinmay and Ramakrishnan, Ganesh and Goldblum, Micah and White, Colin},
  year = {2023},
  month = oct,
  number = {arXiv:2305.02997},
  eprint = {2305.02997},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2024-02-01},
  abstract = {Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and question the importance of this debate. To this end, we conduct the largest tabular data analysis to date, comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than choosing between NNs and GBDTs. A remarkable exception is the recently-proposed prior-data fitted network, TabPFN: although it is effectively limited to training sets of size 3000, we find that it outperforms all other algorithms on average, even when randomly sampling 3000 training datapoints. Next, we analyze dozens of metafeatures to determine what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better than NNs at handling skewed or heavy-tailed feature distributions and other forms of dataset irregularities. Our insights act as a guide for practitioners to determine which techniques may work best on their dataset. Finally, with the goal of accelerating tabular data research, we release the TabZilla Benchmark Suite: a collection of the 36 'hardest' of the datasets we study. Our benchmark suite, codebase, and all raw results are available at https://github.com/naszilla/tabzilla.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/leo/Zotero/storage/RPMGQDXF/McElfresh et al. - 2023 - When Do Neural Nets Outperform Boosted Trees on Ta.pdf;/Users/leo/Zotero/storage/3VRHZZMB/2305.html}
}

@misc{mengLocatingEditingFactual2023,
  title = {Locating and {{Editing Factual Associations}} in {{GPT}}},
  author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  year = {2023},
  month = jan,
  number = {arXiv:2202.05262},
  eprint = {2202.05262},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.05262},
  urldate = {2023-11-24},
  abstract = {We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.7},
  file = {/Users/leo/Zotero/storage/RJLWB67E/Meng et al. - 2023 - Locating and Editing Factual Associations in GPT.pdf;/Users/leo/Zotero/storage/W6Z9855T/2202.html}
}

@article{micci-barrecaPreprocessingSchemeHighCardinality2001,
  title = {A {{Preprocessing Scheme}} for {{High-Cardinality Categorical Attributes}} in {{Classification}} and {{Prediction Problems}}.},
  author = {{Micci-Barreca}, Daniele},
  year = {2001},
  month = jul,
  journal = {SIGKDD Explorations},
  volume = {3},
  pages = {27--32},
  doi = {10.1145/507533.507538},
  abstract = {Categorical data fields characterized by a large number of distinct values represent a serious challenge for many classification and regression algorithms that require numerical inputs. On the other hand, these types of data fields are quite common in real-world data mining applications and often contain potentially relevant information that is difficult to represent for modeling purposes.This paper presents a simple preprocessing scheme for high-cardinality categorical data that allows this class of attributes to be used in predictive models such as neural networks, linear and logistic regression. The proposed method is based on a well-established statistical method (empirical Bayes) that is straightforward to implement as an in-database procedure. Furthermore, for categorical attributes with an inherent hierarchical structure, like ZIP codes, the preprocessing scheme can directly leverage the hierarchy by blending statistics at the various levels of aggregation.While the statistical methods discussed in this paper were first introduced in the mid 1950's, the use of these methods as a preprocessing step for complex models, like neural networks, has not been previously discussed in any literature.}
}

@misc{muennighoffMTEBMassiveText2023,
  title = {{{MTEB}}: {{Massive Text Embedding Benchmark}}},
  shorttitle = {{{MTEB}}},
  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"i}c and Reimers, Nils},
  year = {2023},
  month = mar,
  number = {arXiv:2210.07316},
  eprint = {2210.07316},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.07316},
  urldate = {2023-11-24},
  abstract = {Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/YZZ42E4L/Muennighoff et al. - 2023 - MTEB Massive Text Embedding Benchmark.pdf;/Users/leo/Zotero/storage/E2XXT78U/2210.html}
}

@misc{neelakantanTextCodeEmbeddings2022,
  title = {Text and {{Code Embeddings}} by {{Contrastive Pre-Training}}},
  author = {Neelakantan, Arvind and Xu, Tao and Puri, Raul and Radford, Alec and Han, Jesse Michael and Tworek, Jerry and Yuan, Qiming and Tezak, Nikolas and Kim, Jong Wook and Hallacy, Chris and Heidecke, Johannes and Shyam, Pranav and Power, Boris and Nekoul, Tyna Eloundou and Sastry, Girish and Krueger, Gretchen and Schnurr, David and Such, Felipe Petroski and Hsu, Kenny and Thompson, Madeleine and Khan, Tabarak and Sherbakov, Toki and Jang, Joanne and Welinder, Peter and Weng, Lilian},
  year = {2022},
  month = jan,
  number = {arXiv:2201.10005},
  eprint = {2201.10005},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-04},
  abstract = {Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4\% and 1.8\% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4\%, 14.7\%, and 10.6\% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8\% relative improvement over prior best work on code search.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/NJ9EVQEG/Neelakantan et al. - 2022 - Text and Code Embeddings by Contrastive Pre-Traini.pdf;/Users/leo/Zotero/storage/4UA9KALX/2201.html}
}

@misc{niSentenceT5ScalableSentence2021,
  title = {Sentence-{{T5}}: {{Scalable Sentence Encoders}} from {{Pre-trained Text-to-Text Models}}},
  shorttitle = {Sentence-{{T5}}},
  author = {Ni, Jianmo and {\'A}brego, Gustavo Hern{\'a}ndez and Constant, Noah and Ma, Ji and Hall, Keith B. and Cer, Daniel and Yang, Yinfei},
  year = {2021},
  month = dec,
  number = {arXiv:2108.08877},
  eprint = {2108.08877},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-11-24},
  abstract = {We provide the first exploration of sentence embeddings from text-to-text transformers (T5). Sentence embeddings are broadly useful for language processing tasks. While T5 achieves impressive performance on language tasks cast as sequence-to-sequence mapping problems, it is unclear how to produce sentence embeddings from encoder-decoder models. We investigate three methods for extracting T5 sentence embeddings: two utilize only the T5 encoder and one uses the full T5 encoder-decoder model. To support our investigation, we establish a new sentence representation transfer benchmark, SentGLUE, which extends the SentEval toolkit to nine tasks from the GLUE benchmark. Our encoder-only models outperforms Sentence-BERT and SimCSE sentence embeddings on both SentEval and SentGLUE transfer tasks, including semantic textual similarity (STS). Scaling up T5 from millions to billions of parameters is found to produce consistent further improvements. Finally, our encoder-decoder method achieves a new state-of-the-art on STS when using sentence embeddings. Our models are released at https://tfhub.dev/google/collections/sentence-t5/1.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/leo/Zotero/storage/BYP7EC9V/Ni et al. - 2021 - Sentence-T5 Scalable Sentence Encoders from Pre-t.pdf;/Users/leo/Zotero/storage/W4VXNHT8/2108.html}
}

@misc{nyckelTopScoreKaggle,
  title = {A Top 3\% Score in the {{Kaggle Titanic Challenge}} Using {{Transformers}}},
  author = {Nyckel},
  urldate = {2023-10-04},
  abstract = {In this post we show how to make a top 3\% submission on the Kaggle Titanic Challenge using Transformers},
  howpublished = {https://www.nyckel.com/blog/titanic-vs-transformers/www.nyckel.com/blog/titanic-vs-transformers/},
  langid = {english},
  file = {/Users/leo/Zotero/storage/ZSR75KPT/titanic-vs-transformers.html}
}

@misc{popovNeuralObliviousDecision2019,
  title = {Neural {{Oblivious Decision Ensembles}} for {{Deep Learning}} on {{Tabular Data}}},
  author = {Popov, Sergei and Morozov, Stanislav and Babenko, Artem},
  year = {2019},
  month = sep,
  number = {arXiv:1909.06312},
  eprint = {1909.06312},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1909.06312},
  urldate = {2024-02-01},
  abstract = {Nowadays, deep neural networks (DNNs) have become the main instrument for machine learning tasks within a wide range of domains, including vision, NLP, and speech. Meanwhile, in an important case of heterogenous tabular data, the advantage of DNNs over shallow counterparts remains questionable. In particular, there is no sufficient evidence that deep learning machinery allows constructing methods that outperform gradient boosting decision trees (GBDT), which are often the top choice for tabular problems. In this paper, we introduce Neural Oblivious Decision Ensembles (NODE), a new deep learning architecture, designed to work with any tabular data. In a nutshell, the proposed NODE architecture generalizes ensembles of oblivious decision trees, but benefits from both end-to-end gradient-based optimization and the power of multi-layer hierarchical representation learning. With an extensive experimental comparison to the leading GBDT packages on a large number of tabular datasets, we demonstrate the advantage of the proposed NODE architecture, which outperforms the competitors on most of the tasks. We open-source the PyTorch implementation of NODE and believe that it will become a universal framework for machine learning on tabular data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/leo/Zotero/storage/YDIEBPYG/Popov et al. - 2019 - Neural Oblivious Decision Ensembles for Deep Learn.pdf;/Users/leo/Zotero/storage/WAIWBRZQ/1909.html}
}

@misc{prokhorenkovaCatBoostUnbiasedBoosting2019,
  title = {{{CatBoost}}: Unbiased Boosting with Categorical Features},
  shorttitle = {{{CatBoost}}},
  author = {Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
  year = {2019},
  month = jan,
  number = {arXiv:1706.09516},
  eprint = {1706.09516},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.09516},
  urldate = {2023-10-04},
  abstract = {This paper presents the key algorithmic techniques behind CatBoost, a new gradient boosting toolkit. Their combination leads to CatBoost outperforming other publicly available boosting implementations in terms of quality on a variety of datasets. Two critical algorithmic advances introduced in CatBoost are the implementation of ordered boosting, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for processing categorical features. Both techniques were created to fight a prediction shift caused by a special kind of target leakage present in all currently existing implementations of gradient boosting algorithms. In this paper, we provide a detailed analysis of this problem and demonstrate that proposed algorithms solve it effectively, leading to excellent empirical results.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/4KHN82K3/Prokhorenkova et al. - 2019 - CatBoost unbiased boosting with categorical featu.pdf;/Users/leo/Zotero/storage/KKLLKT2I/1706.html}
}

@misc{reimersSentenceBERTSentenceEmbeddings2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  number = {arXiv:1908.10084},
  eprint = {1908.10084},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-04},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/leo/Zotero/storage/GT699YDX/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf;/Users/leo/Zotero/storage/BCV4ZFKN/1908.html}
}

@misc{reimersSentenceBERTSentenceEmbeddings2019a,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  number = {arXiv:1908.10084},
  eprint = {1908.10084},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1908.10084},
  urldate = {2023-11-24},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/leo/Zotero/storage/KH2XK8HE/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf;/Users/leo/Zotero/storage/JCPYTXJL/1908.html}
}

@misc{sanhDistilBERTDistilledVersion2020,
  title = {{{DistilBERT}}, a Distilled Version of {{BERT}}: Smaller, Faster, Cheaper and Lighter},
  shorttitle = {{{DistilBERT}}, a Distilled Version of {{BERT}}},
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  year = {2020},
  month = feb,
  number = {arXiv:1910.01108},
  eprint = {1910.01108},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.01108},
  urldate = {2023-10-04},
  abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/leo/Zotero/storage/8NQMJWUP/Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf;/Users/leo/Zotero/storage/UF497MX7/1910.html}
}

@misc{ScalingAllPairs,
  title = {Scaling up All Pairs Similarity Search {\textbar} {{Proceedings}} of the 16th International Conference on {{World Wide Web}}},
  urldate = {2023-12-01},
  howpublished = {https://dl.acm.org/doi/10.1145/1242572.1242591},
  file = {/Users/leo/Zotero/storage/WSVYIJS9/1242572.html}
}

@article{scikit-learn,
  title = {Scikit-Learn: {{Machine}} Learning in {{Python}}},
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2825--2830}
}

@misc{shengFlexGenHighThroughputGenerative2023,
  title = {{{FlexGen}}: {{High-Throughput Generative Inference}} of {{Large Language Models}} with a {{Single GPU}}},
  shorttitle = {{{FlexGen}}},
  author = {Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Fu, Daniel Y. and Xie, Zhiqiang and Chen, Beidi and Barrett, Clark and Gonzalez, Joseph E. and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
  year = {2023},
  month = jun,
  number = {arXiv:2303.06865},
  eprint = {2303.06865},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.06865},
  urldate = {2023-11-24},
  abstract = {The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accuracy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a single 16GB GPU, FlexGen achieves significantly higher throughput compared to state-of-the-art offloading systems, reaching a generation throughput of 1 token/s for the first time with an effective batch size of 144. On the HELM benchmark, FlexGen can benchmark a 30B model with a 16GB GPU on 7 representative sub-scenarios in 21 hours. The code is available at https://github.com/FMInference/FlexGen},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Performance},
  file = {/Users/leo/Zotero/storage/7FMELQB3/Sheng et al. - 2023 - FlexGen High-Throughput Generative Inference of L.pdf;/Users/leo/Zotero/storage/PYHX5LTI/2303.html}
}

@misc{shwartz-zivTabularDataDeep2021b,
  title = {Tabular {{Data}}: {{Deep Learning}} Is {{Not All You Need}}},
  shorttitle = {Tabular {{Data}}},
  author = {{Shwartz-Ziv}, Ravid and Armon, Amitai},
  year = {2021},
  month = nov,
  number = {arXiv:2106.03253},
  eprint = {2106.03253},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.03253},
  urldate = {2024-02-01},
  abstract = {A key element in solving real-life data science problems is selecting the types of models to use. Tree ensemble models (such as XGBoost) are usually recommended for classification and regression problems with tabular data. However, several deep learning models for tabular data have recently been proposed, claiming to outperform XGBoost for some use cases. This paper explores whether these deep models should be a recommended option for tabular data by rigorously comparing the new deep models to XGBoost on various datasets. In addition to systematically comparing their performance, we consider the tuning and computation they require. Our study shows that XGBoost outperforms these deep models across the datasets, including the datasets used in the papers that proposed the deep models. We also demonstrate that XGBoost requires much less tuning. On the positive side, we show that an ensemble of deep models and XGBoost performs better on these datasets than XGBoost alone.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/T7K6KIV6/Shwartz-Ziv and Armon - 2021 - Tabular Data Deep Learning is Not All You Need.pdf;/Users/leo/Zotero/storage/VN4SFX9D/2106.html}
}

@misc{SimilarityEncodingLearning,
  title = {Similarity Encoding for Learning with Dirty Categorical Variables {\textbar} {{Machine Learning}}},
  urldate = {2024-01-29},
  howpublished = {https://link.springer.com/article/10.1007/s10994-018-5724-2},
  file = {/Users/leo/Zotero/storage/ISI6JMSC/s10994-018-5724-2.html}
}

@manual{skrub2023,
  type = {Manual},
  title = {Skrub: {{Prepping}} Tables for Machine Learnin},
  author = {Team, Soda},
  year = {2023},
  address = {{Palaiseau, France}},
  organization = {{Inria Saclay}}
}

@misc{somepalliSAINTImprovedNeural2021a,
  title = {{{SAINT}}: {{Improved Neural Networks}} for {{Tabular Data}} via {{Row Attention}} and {{Contrastive Pre-Training}}},
  shorttitle = {{{SAINT}}},
  author = {Somepalli, Gowthami and Goldblum, Micah and Schwarzschild, Avi and Bruss, C. Bayan and Goldstein, Tom},
  year = {2021},
  month = jun,
  number = {arXiv:2106.01342},
  eprint = {2106.01342},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.01342},
  urldate = {2024-02-01},
  abstract = {Tabular data underpins numerous high-impact applications of machine learning from fraud detection to genomics and healthcare. Classical approaches to solving tabular problems, such as gradient boosting and random forests, are widely used by practitioners. However, recent deep learning methods have achieved a degree of performance competitive with popular techniques. We devise a hybrid deep learning approach to solving tabular data problems. Our method, SAINT, performs attention over both rows and columns, and it includes an enhanced embedding method. We also study a new contrastive self-supervised pre-training method for use when labels are scarce. SAINT consistently improves performance over previous deep learning methods, and it even outperforms gradient boosting methods, including XGBoost, CatBoost, and LightGBM, on average over a variety of benchmark tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/leo/Zotero/storage/FVWR559Q/Somepalli et al. - 2021 - SAINT Improved Neural Networks for Tabular Data v.pdf;/Users/leo/Zotero/storage/XFJGHKRB/2106.html}
}

@misc{songMPNetMaskedPermuted2020,
  title = {{{MPNet}}: {{Masked}} and {{Permuted Pre-training}} for {{Language Understanding}}},
  shorttitle = {{{MPNet}}},
  author = {Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  year = {2020},
  month = nov,
  number = {arXiv:2004.09297},
  eprint = {2004.09297},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-04},
  abstract = {BERT adopts masked language modeling (MLM) for pre-training and is one of the most successful pre-training models. Since BERT neglects dependency among predicted tokens, XLNet introduces permuted language modeling (PLM) for pre-training to address this problem. However, XLNet does not leverage the full position information of a sentence and thus suffers from position discrepancy between pre-training and fine-tuning. In this paper, we propose MPNet, a novel pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. MPNet leverages the dependency among predicted tokens through permuted language modeling (vs. MLM in BERT), and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy (vs. PLM in XLNet). We pre-train MPNet on a large-scale dataset (over 160GB text corpora) and fine-tune on a variety of down-streaming tasks (GLUE, SQuAD, etc). Experimental results show that MPNet outperforms MLM and PLM by a large margin, and achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g., BERT, XLNet, RoBERTa) under the same model setting. The code and the pre-trained models are available at: https://github.com/microsoft/MPNet.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/leo/Zotero/storage/6BQZF78V/Song et al. - 2020 - MPNet Masked and Permuted Pre-training for Langua.pdf;/Users/leo/Zotero/storage/Q6Z9BX38/2004.html}
}

@misc{thakurAugmentedSBERTData2021,
  title = {Augmented {{SBERT}}: {{Data Augmentation Method}} for {{Improving Bi-Encoders}} for {{Pairwise Sentence Scoring Tasks}}},
  shorttitle = {Augmented {{SBERT}}},
  author = {Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes and Gurevych, Iryna},
  year = {2021},
  month = apr,
  number = {arXiv:2010.08240},
  eprint = {2010.08240},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2010.08240},
  urldate = {2023-11-24},
  abstract = {There are two approaches for pairwise sentence scoring: Cross-encoders, which perform full-attention over the input pair, and Bi-encoders, which map each input independently to a dense vector space. While cross-encoders often achieve higher performance, they are too slow for many practical use cases. Bi-encoders, on the other hand, require substantial training data and fine-tuning over the target task to achieve competitive performance. We present a simple yet efficient data augmentation strategy called Augmented SBERT, where we use the cross-encoder to label a larger set of input pairs to augment the training data for the bi-encoder. We show that, in this process, selecting the sentence pairs is non-trivial and crucial for the success of the method. We evaluate our approach on multiple tasks (in-domain) as well as on a domain adaptation task. Augmented SBERT achieves an improvement of up to 6 points for in-domain and of up to 37 points for domain adaptation tasks compared to the original bi-encoder performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/leo/Zotero/storage/6ZK32VG5/Thakur et al. - 2021 - Augmented SBERT Data Augmentation Method for Impr.pdf;/Users/leo/Zotero/storage/TFHS8EPQ/2010.html}
}

@misc{timbersTransformerInferenceTricks2023,
  title = {Transformer Inference Tricks},
  author = {Timbers, Finbarr},
  year = {2023},
  month = sep,
  urldate = {2023-11-24},
  abstract = {How to make your model run faster than a greased pig},
  howpublished = {https://www.artfintel.com/p/transformer-inference-tricks},
  langid = {english},
  file = {/Users/leo/Zotero/storage/WNTQVQE9/transformer-inference-tricks.html}
}

@misc{touvronLLaMAOpenEfficient2023,
  title = {{{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}},
  shorttitle = {{{LLaMA}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  year = {2023},
  month = feb,
  number = {arXiv:2302.13971},
  eprint = {2302.13971},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.13971},
  urldate = {2023-11-24},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/leo/Zotero/storage/WWYHN2TK/Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Mode.pdf;/Users/leo/Zotero/storage/GF5CJEWM/2302.html}
}

@misc{touvronLlamaOpenFoundation2023,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09288},
  eprint = {2307.09288},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.09288},
  urldate = {2023-11-24},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/leo/Zotero/storage/MEEU4IBU/Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf;/Users/leo/Zotero/storage/E6MUFM6T/2307.html}
}

@misc{wangTextEmbeddingsWeaklySupervised2022,
  title = {Text {{Embeddings}} by {{Weakly-Supervised Contrastive Pre-training}}},
  author = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
  year = {2022},
  month = dec,
  number = {arXiv:2212.03533},
  eprint = {2212.03533},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-11-24},
  abstract = {This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40x more parameters.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/Users/leo/Zotero/storage/T9MPAEBB/Wang et al. - 2022 - Text Embeddings by Weakly-Supervised Contrastive P.pdf;/Users/leo/Zotero/storage/4R9K5S94/2212.html}
}

@inproceedings{wangTUTATreebasedTransformers2021,
  title = {{{TUTA}}: {{Tree-based Transformers}} for {{Generally Structured Table Pre-training}}},
  shorttitle = {{{TUTA}}},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Wang, Zhiruo and Dong, Haoyu and Jia, Ran and Li, Jia and Fu, Zhiyi and Han, Shi and Zhang, Dongmei},
  year = {2021},
  month = aug,
  eprint = {2010.12537},
  primaryclass = {cs},
  pages = {1780--1790},
  doi = {10.1145/3447548.3467434},
  urldate = {2023-11-24},
  abstract = {Tables are widely used with various structures to organize and present data. Recent attempts on table understanding mainly focus on relational tables, yet overlook to other common table structures. In this paper, we propose TUTA, a unified pre-training architecture for understanding generally structured tables. Noticing that understanding a table requires spatial, hierarchical, and semantic information, we enhance transformers with three novel structure-aware mechanisms. First, we devise a unified tree-based structure, called a bi-dimensional coordinate tree, to describe both the spatial and hierarchical information of generally structured tables. Upon this, we propose tree-based attention and position embedding to better capture the spatial and hierarchical information. Moreover, we devise three progressive pre-training objectives to enable representations at the token, cell, and table levels. We pre-train TUTA on a wide range of unlabeled web and spreadsheet tables and fine-tune it on two critical tasks in the field of table structure understanding: cell type classification and table type classification. Experiments show that TUTA is highly effective, achieving state-of-the-art on five widely-studied datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Information Retrieval},
  file = {/Users/leo/Zotero/storage/DVANHNGY/Wang et al. - 2021 - TUTA Tree-based Transformers for Generally Struct.pdf;/Users/leo/Zotero/storage/JXKKTQ5D/2010.html}
}

@misc{xiaoCPackPackagedResources2023,
  title = {C-{{Pack}}: {{Packaged Resources To Advance General Chinese Embedding}}},
  shorttitle = {C-{{Pack}}},
  author = {Xiao, Shitao and Liu, Zheng and Zhang, Peitian and Muennighof, Niklas},
  year = {2023},
  month = sep,
  number = {arXiv:2309.07597},
  eprint = {2309.07597},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-11-16},
  abstract = {We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10\% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve state-of-the-art performance on MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. All these resources are made publicly available at https://github.com/FlagOpen/FlagEmbedding.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/Users/leo/Zotero/storage/CPWJQC5G/Xiao et al. - 2023 - C-Pack Packaged Resources To Advance General Chin.pdf;/Users/leo/Zotero/storage/6VFX4IS4/2309.html}
}

@misc{zhangTableLlamaOpenLarge2023,
  title = {{{TableLlama}}: {{Towards Open Large Generalist Models}} for {{Tables}}},
  shorttitle = {{{TableLlama}}},
  author = {Zhang, Tianshu and Yue, Xiang and Li, Yifei and Sun, Huan},
  year = {2023},
  month = nov,
  number = {arXiv:2311.09206},
  eprint = {2311.09206},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-11-24},
  abstract = {Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically interpret, augment, and query tables. Current methods often require pretraining on tables or special model architecture design, are restricted to specific table types, or have simplifying assumptions about tables and tasks. This paper makes the first step towards developing open-source large language models (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves comparable or better performance than the SOTA for each task, despite the latter often has task-specific design. On 6 out-of-domain datasets, it achieves 6-48 absolute point gains compared with the base model, showing that training on TableInstruct enhances the model's generalizability. We will open-source our dataset and trained model to boost future work on developing open generalist models for tables.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/leo/Zotero/storage/8Y22QHX6/Zhang et al. - 2023 - TableLlama Towards Open Large Generalist Models f.pdf;/Users/leo/Zotero/storage/XFASTTZH/2311.html}
}

@misc{zotero-482,
  title = {Skrub}
}

@misc{zotero-532,
  type = {Misc}
}

@article{zotero-533,
  type = {Article}
}
