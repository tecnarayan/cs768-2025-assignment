% This file was created with JabRef 2.7b.
% Encoding: UTF-8


@book{copbook,
  added-at = {2009-08-14T15:59:35.000+0200},
  asin = {1886529043},
  author = {Bertsekas, Dimitri P.},
  biburl = {https://www.bibsonomy.org/bibtex/2d48c40d4067d2aee63eed3f91cbd5a9b/ahmedjawwad4u},
  description = {Amazon.com: Constrained Optimization and Lagrange Multiplier Methods (Optimization and Neural Computation Series) (9781886529045): Dimitri P. Bertsekas: Books},
  ean = {9781886529045},
  edition = 1,
  interhash = {1a85768c18441adba5282c5787939af2},
  intrahash = {d48c40d4067d2aee63eed3f91cbd5a9b},
  isbn = {1886529043},
  keywords = {book lagrange-multiplier optimization},
  publisher = {Athena Scientific},
  timestamp = {2009-08-26T14:47:04.000+0200},
  title = {Constrained Optimization and Lagrange Multiplier Methods (Optimization and Neural Computation Series)},
  year = 1996
}

@thesis{Ahn-2019-117213,
author = {Edward Ahn},
title = {Towards Safe Reinforcement Learning in the Real World},
year = {2019},
month = {July},
school = {Carnegie Mellon University},
address = {Pittsburgh, PA},
number = {CMU-RI-TR-19-56},
}

@incollection{bhatnagaretal,
  title={Stochastic Recursive Algorithms for Optimization: Simultaneous Perturbation Methods},
  author={Bhatnagar, Shalabh and Prasad, H.L. and Prashanth, L.A.},
  booktitle={Stochastic Recursive Algorithms for Optimization: Simultaneous Perturbation Methods},
  volume={434},
  pages={320},
  year={2013},
  publisher={Springer}
}



@article{conjgrad,
  title={Methods of conjugate gradients for solving linear systems},
  author={Magnus R. Hestenes and Eduard Stiefel},
  journal={Journal of research of the National Bureau of Standards},
  year={1952},
  volume={49},
  pages={409-435}
}
@article{naturalgradient,
    author = {Amari, Shun-ichi},
    title = "{Natural Gradient Works Efficiently in Learning}",
    journal = {Neural Computation},
    volume = {10},
    number = {2},
    pages = {251-276},
    year = {1998},
    month = {02},
    abstract = "{When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed.}",
    issn = {0899-7667},
    doi = {10.1162/089976698300017746},
    url = {https://doi.org/10.1162/089976698300017746},
    eprint = {https://direct.mit.edu/neco/article-pdf/10/2/251/813415/089976698300017746.pdf},
}








@INPROCEEDINGS{qconvergence,
  author={Melo, Francisco S. and Ribeiro, M. Isabel},
  booktitle={2007 European Control Conference (ECC)}, 
  title={Convergence of Q-learning with linear function approximation}, 
  year={2007},
  volume={},
  number={},
  pages={2671-2678},
  doi={10.23919/ECC.2007.7068926}}




@article{nac,
  TITLE = {{Natural Actor-Critic Algorithms}},
  AUTHOR = {Bhatnagar, Shalabh and Sutton, Richard and Ghavamzadeh, Mohammad and Lee, Mark},
  URL = {https://hal.inria.fr/hal-00840470},
  JOURNAL = {{Automatica}},
  PUBLISHER = {{Elsevier}},
  VOLUME = {45},
  NUMBER = {11},
  YEAR = {2009},
  MONTH = Jul,
  DOI = {10.1016/j.automatica.2009.07.008},
  PDF = {https://hal.inria.fr/hal-00840470/file/tr-final.pdf},
  HAL_ID = {hal-00840470},
  HAL_VERSION = {v1},
}

@inproceedings{cnn1,
 author = {Atlas, Les and Homma, Toshiteru and Marks, Robert},
 booktitle = {Neural Information Processing Systems},
 editor = {D. Anderson},
 pages = {},
 publisher = {American Institute of Physics},
 title = {An Artificial Neural Network for Spatio-Temporal Bipolar Patterns: Application to Phoneme Classification},
 url = {https://proceedings.neurips.cc/paper/1987/file/98f13708210194c475687be6106a3b84-Paper.pdf},
 volume = {0},
 year = {1987}
}


@ARTICLE{cnn2,
  author={LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  journal={Neural Computation}, 
  title={Backpropagation Applied to Handwritten Zip Code Recognition}, 
  year={1989},
  volume={1},
  number={4},
  pages={541-551},
  doi={10.1162/neco.1989.1.4.541}}







@Article{go,
author={Schrittwieser, Julian
and Antonoglou, Ioannis
and Hubert, Thomas
and Simonyan, Karen
and Sifre, Laurent
and Schmitt, Simon
and Guez, Arthur
and Lockhart, Edward
and Hassabis, Demis
and Graepel, Thore
and Lillicrap, Timothy
and Silver, David},
title={Mastering Atari, Go, chess and shogi by planning with a learned model},
journal={Nature},
year={2020},
month={Dec},
day={01},
volume={588},
number={7839},
pages={604-609},
abstract={Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3---the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4---the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi---canonical environments for high-performance planning---the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game.},
issn={1476-4687},
doi={10.1038/s41586-020-03051-4},
url={https://doi.org/10.1038/s41586-020-03051-4}
}
@INPROCEEDINGS{diddigi,
  author={Diddigi, Raghuram Bharadwaj and Jain, Prateek and Prabuchandran, K.J. and Bhatnagar, Shalabh},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Neural Network Compatible Off-Policy Natural Actor-Critic Algorithm}, 
  year={2022},
  volume={},
  number={},
  pages={1-10},
  doi={10.1109/IJCNN55064.2022.9892303}}

﻿@Article{dqnnature,
author={Mnih, Volodymyr
and Kavukcuoglu, Koray
and Silver, David
and Rusu, Andrei A.
and Veness, Joel
and Bellemare, Marc G.
and Graves, Alex
and Riedmiller, Martin
and Fidjeland, Andreas K.
and Ostrovski, Georg
and Petersen, Stig
and Beattie, Charles
and Sadik, Amir
and Antonoglou, Ioannis
and King, Helen
and Kumaran, Dharshan
and Wierstra, Daan
and Legg, Shane
and Hassabis, Demis},
title={Human-level control through deep reinforcement learning},
journal={Nature},
year={2015},
month={Feb},
day={01},
volume={518},
number={7540},
pages={529-533},
abstract={An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
issn={1476-4687},
doi={10.1038/nature14236},
url={https://doi.org/10.1038/nature14236}
}




@misc{dqnatari,
  doi = {10.48550/ARXIV.1312.5602},
  
  url = {https://arxiv.org/abs/1312.5602},
  
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Playing Atari with Deep Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2013},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}




@article{suttonpg,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}


@inproceedings{pg_peterschaal,
  title={Policy gradient methods for robotics},
  author={Peters, Jan and Schaal, Stefan},
  booktitle={2006 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={2219--2225},
  year={2006},
  organization={IEEE}
}






@article{reinforce1992,
author={Williams, Ronald J.},
title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
journal={Machine Learning},
year={1992},
month={May},
day={01},
volume={8},
number={3},
pages={229-256},
abstract={This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
issn={1573-0565},
doi={10.1007/BF00992696},
url={https://doi.org/10.1007/BF00992696}
}



@Article{crossentropy,
author={de Boer, Pieter-Tjerk
and Kroese, Dirk P.
and Mannor, Shie
and Rubinstein, Reuven Y.},
title={A Tutorial on the Cross-Entropy Method},
journal={Annals of Operations Research},
year={2005},
month={Feb},
day={01},
volume={134},
number={1},
pages={19-67},
abstract={The cross-entropy (CE) method is a new generic approach to combinatorial and multi-extremal optimization and rare event simulation. The purpose of this tutorial is to give a gentle introduction to the CE method. We present the CE methodology, the basic algorithm and its modifications, and discuss applications in combinatorial optimization and machine learning.},
issn={1572-9338},
doi={10.1007/s10479-005-5724-z},
url={https://doi.org/10.1007/s10479-005-5724-z}
}


@article{Watkins1992,
author={Watkins, Christopher J. C. H.
and Dayan, Peter},
title={Q-learning},
journal={Machine Learning},
year={1992},
month={May},
day={01},
volume={8},
number={3},
pages={279-292},
abstract={Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
issn={1573-0565},
doi={10.1007/BF00992698},
url={https://doi.org/10.1007/BF00992698}
}









@misc{STABILITY_SAFERL,
  doi = {10.48550/ARXIV.1705.08551},
  
  url = {https://arxiv.org/abs/1705.08551},
  
  author = {Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela P. and Krause, Andreas},
  
  keywords = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Systems and Control (eess.SY), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {Safe Model-based Reinforcement Learning with Stability Guarantees},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{imaginingnips,
  doi = {10.48550/ARXIV.2202.07789},
  
  url = {https://arxiv.org/abs/2202.07789},
  
  author = {Thomas, Garrett and Luo, Yuping and Ma, Tengyu},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Safe Reinforcement Learning by Imagining the Near Future},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{Achiam2019BenchmarkingSE,
  title={Benchmarking safe exploration in deep reinforcement learning},
  author={Ray, Alex and Achiam, Joshua and Amodei, Dario},
  journal={arXiv preprint arXiv:1910.01708},
  volume={7},
  pages={1},
  year={2019}
}



@misc{fasthvp,
      title={Fast Exact Multiplication by the Hessian}, 
      author={Barak A. Pearlmutter},
      year={1994}
}

@article{LOOP,
  author    = {Harshit Sikchi and
               Wenxuan Zhou and
               David Held},
  title     = {Learning Off-Policy with Online Planning},
  journal   = {CoRL},
  year={2021}
  }



@article{MBPO,
  title={When to trust your model: Model-based policy optimization},
  author={Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{dalal2018safe,
  title={Safe exploration in continuous action spaces},
  author={Dalal, Gal and Dvijotham, Krishnamurthy and Vecerik, Matej and Hester, Todd and Paduraru, Cosmin and Tassa, Yuval},
  journal={arXiv preprint arXiv:1801.08757},
  year={2018}
}
@inproceedings{manifoldliu,
  title={Robot Reinforcement Learning on the Constraint Manifold},
  author={Liu, Puze and Tateo, Davide and Ammar, Haitham Bou and Peters, Jan},
  booktitle={Conference on Robot Learning},
  pages={1357--1366},
  year={2022},
  organization={PMLR}
}
@article{chow2018lyapunov,
  title={A {Lyapunov-based} approach to safe reinforcement learning},
  author={Chow, Yinlam and Nachum, Ofir and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{chow2019lyapunov,
  title={Lyapunov-based safe policy optimization for continuous control},
  author={Chow, Yinlam and Nachum, Ofir and Faust, Aleksandra and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad},
  journal={arXiv preprint arXiv:1901.10031},
  year={2019}
}
@article{sikchi2021lyapunov,
  title={Lyapunov barrier policy optimization},
  author={Sikchi, Harshit and Zhou, Wenxuan and Held, David},
  journal={arXiv preprint arXiv:2103.09230},
  year={2021}
}

@article{surveysafe,
  author  = {Javier Garc{{\'i}}a and Fern and o Fern{{\'a}}ndez},
  title   = {A Comprehensive Survey on Safe Reinforcement Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  number  = {42},
  pages   = {1437-1480},
  url     = {http://jmlr.org/papers/v16/garcia15a.html}
}
@article{cmdp2,
  title={An online actor--critic algorithm with function approximation for constrained {Markov} decision processes},
  author={Bhatnagar, Shalabh and Lakshmanan, K},
  journal={Journal of Optimization Theory and Applications},
  volume={153},
  number={3},
  pages={688--708},
  year={2012},
  publisher={Springer}
}



@article{zhang2020order,
  title={First order constrained optimization in policy space},
  author={Zhang, Yiming and Vuong, Quan and Ross, Keith},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15338--15349},
  year={2020}
}




@article{borkar-cmdp,
  title={An actor-critic algorithm for constrained {Markov} decision processes},
  author={Borkar, Vivek S},
  journal={Systems \& control letters},
  volume={54},
  number={3},
  pages={207--213},
  year={2005},
  publisher={Elsevier}
}
@misc{tessler2018reward,
      title={Reward Constrained Policy Optimization}, 
      author={Chen Tessler and Daniel J. Mankowitz and Shie Mannor},
      year={2018},
      eprint={1805.11074},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{articlecmdpbhatnagar,
author = {Bhatnagar, Shalabh},
year = {2010},
month = {12},
pages = {760-766},
title = {An actor-critic algorithm with function approximation for discounted cost constrained Markov decision processes},
volume = {59},
journal = {Systems \& Control Letters}
}
@phdthesis{Jin:EECS-2019-53,
    Author = {Jin, Chi},
    Title = {Machine Learning: Why Do Simple Algorithms Work So Well?},
    School = {EECS Department, University of California, Berkeley},
    Year = {2019},
    Month = {May},
    Number = {UCB/EECS-2019-53},
    Abstract = {While state-of-the-art machine learning models are deep, large-scale, sequential and highly nonconvex, the backbone of modern learning algorithms are simple algorithms such as stochastic gradient descent, gradient descent with momentum or Q-learning (in the case of reinforcement learning tasks). A basic question endures---why do simple algorithms work so well even in these challenging settings?

To answer above question, this thesis focuses on four concrete and fundamental questions:
- In nonconvex optimization, can (stochastic) gradient descent or its variants escape saddle points efficiently?
- Is gradient descent with momentum provably faster than gradient descent in the general nonconvex setting?
- In nonconvex-nonconcave minmax optimization, what is a proper definition of local optima and is gradient descent ascent game-theoretically meaningful?
- In reinforcement learning, is Q-learning sample efficient?

This thesis provides the first line of provably positive answers to all above questions. In particular, this thesis will show that although the standard versions of these classical algorithms do not enjoy good theoretical properties in the worst case, simple modifications are sufficient to grant them desirable behaviors, which explain the underlying mechanisms behind their favorable performance in practice.}
}

@misc{yu2019convergent,
      title={Convergent Policy Optimization for Safe Reinforcement Learning}, 
      author={Ming Yu and Zhuoran Yang and Mladen Kolar and Zhaoran Wang},
      year={2019},
      eprint={1910.12156},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@inproceedings{achiam2017constrained,
  title={Constrained policy optimization},
  author={Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={22--31},
  year={2017},
  organization={PMLR}
}


@misc{ADAM,
  doi = {10.48550/ARXIV.1412.6980},
  
  url = {https://arxiv.org/abs/1412.6980},
  
  author = {Kingma, Diederik P. and Ba, Jimmy},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Adam: A Method for Stochastic Optimization},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{suttonpolicygradeint,
 author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 pages = {},
 publisher = {MIT Press},
 title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
 url = {https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf},
 volume = {12},
 year = {1999}
}


@misc{captum,
  doi = {10.48550/ARXIV.2009.07896},
  
  url = {https://arxiv.org/abs/2009.07896},
  
  author = {Kokhlikyan, Narine and Miglani, Vivek and Martin, Miguel and Wang, Edward and Alsallakh, Bilal and Reynolds, Jonathan and Melnikov, Alexander and Kliushkina, Natalia and Araya, Carlos and Yan, Siqi and Reblitz-Richardson, Orion},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Captum: A unified and generic model interpretability library for PyTorch},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}





@Article{uncertaintyALEpi,
author={H{\"u}llermeier, Eyke
and Waegeman, Willem},
title={Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods},
journal={Machine Learning},
year={2021},
month={Mar},
day={01},
volume={110},
number={3},
pages={457-506},
abstract={The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.},
issn={1573-0565},
doi={10.1007/s10994-021-05946-3},
url={https://doi.org/10.1007/s10994-021-05946-3}
}








@ARTICLE{crossentropysafe,
  author={Wen, Min and Topcu, Ufuk},
  journal={IEEE Transactions on Automatic Control}, 
  title={Constrained Cross-Entropy Method for Safe Reinforcement Learning}, 
  year={2021},
  volume={66},
  number={7},
  pages={3123-3137},
  doi={10.1109/TAC.2020.3015931}}

@article{agarwal2021deep,
  title={Deep reinforcement learning at the edge of the statistical precipice},
  author={Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@misc{liu2020safe,            

            title={Safe Model-based Reinforcement Learning with Robust Cross-Entropy Method},             

            author={Zuxin Liu and Hongyi Zhou and Baiming Chen and Sicheng Zhong and Martial Hebert and Ding Zhao},            

            year={2020},          

            eprint={2010.07968},           

            archivePrefix={arXiv},           

            primaryClass={cs.AI}


}

@misc{samba,
  doi = {10.48550/ARXIV.2006.09436},
  
  url = {https://arxiv.org/abs/2006.09436},
  
  author = {Cowen-Rivers, Alexander I. and Palenicek, Daniel and Moens, Vincent and Abdullah, Mohammed and Sootla, Aivar and Wang, Jun and Ammar, Haitham},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Robotics (cs.RO), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {SAMBA: Safe Model-Based \& Active Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{schulman2015gradient,
  title={Gradient estimation using stochastic computation graphs},
  author={Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  year={2015}
}

@misc{benchmodelbased,
  doi = {10.48550/ARXIV.1907.02057},
  
  url = {https://arxiv.org/abs/1907.02057},
  
  author = {Wang, Tingwu and Bao, Xuchan and Clavera, Ignasi and Hoang, Jerrick and Wen, Yeming and Langlois, Eric and Zhang, Shunshi and Zhang, Guodong and Abbeel, Pieter and Ba, Jimmy},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Robotics (cs.RO), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Benchmarking Model-Based Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}


@inproceedings{PILCO,
  title={PILCO: A model-based and data-efficient approach to policy search},
  author={Deisenroth, Marc and Rasmussen, Carl E},
  booktitle={Proceedings of the 28th International Conference on machine learning (ICML-11)},
  pages={465--472},
  year={2011},
  organization={Citeseer}
}

@misc{yu2020reinforcement,
      title={Reinforcement Learning in Healthcare: A Survey}, 
      author={Chao Yu and Jiming Liu and Shamim Nemati},
      year={2020},
      eprint={1908.08796},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{schulman2017proximal,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{schulman2018highdimensional,
title = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
author = {John Schulman and Philipp Moritz and Sergey Levine and Michael Jordan and Pieter Abbeel},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
year  = 2016
}



@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}


@INPROCEEDINGS{8367110,
  author={Han, Seung-Ho and Choi, Ho-Jin and Benz, Philipp and Loaiciga, Jorge},
  booktitle={2018 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={Sensor-Based Mobile Robot Navigation via Deep Reinforcement Learning}, 
  year={2018},
  volume={},
  number={},
  pages={147-154},
  doi={10.1109/BigComp.2018.00030}}
  
  @misc{kendall2018learning,
      title={Learning to Drive in a Day}, 
      author={Alex Kendall and Jeffrey Hawke and David Janz and Przemyslaw Mazur and Daniele Reda and John-Mark Allen and Vinh-Dieu Lam and Alex Bewley and Amar Shah},
      year={2018},
      eprint={1807.00412},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{stooke2020responsive,
  title={Responsive safety in reinforcement learning by {PID Lagrangian} methods},
  author={Stooke, Adam and Achiam, Joshua and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={9133--9143},
  year={2020},
  organization={PMLR}
}




@misc{adolphs2019local,
      title={Local Saddle Point Optimization: A Curvature Exploitation Approach}, 
      author={Leonard Adolphs and Hadi Daneshmand and Aurelien Lucchi and Thomas Hofmann},
      year={2019},
      eprint={1805.05751},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{curtis2018exploiting,
      title={Exploiting Negative Curvature in Deterministic and Stochastic Optimization}, 
      author={Frank E. Curtis and Daniel P. Robinson},
      year={2018},
      eprint={1703.00412},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{atari,
      title={Playing {Atari} with Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
      year={2013},
      eprint={1312.5602},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@book{cmdpbook,
title={Constrained {Markov} Decision Processes},
author={Eitan Altman},
volume={1},
year={1998},
publisher={Taylor \& Francis}
}


@misc{gym,
  doi = {10.48550/ARXIV.1606.01540},
  
  url = {https://arxiv.org/abs/1606.01540},
  
  author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {{OpenAI Gym}},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{slbo,
  doi = {10.48550/ARXIV.1807.03858},
  
  url = {https://arxiv.org/abs/1807.03858},
  
  author = {Luo, Yuping and Xu, Huazhe and Li, Yuanzhi and Tian, Yuandong and Darrell, Trevor and Ma, Tengyu},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{svg,
  doi = {10.48550/ARXIV.1510.09142},
  
  url = {https://arxiv.org/abs/1510.09142},
  
  author = {Heess, Nicolas and Wayne, Greg and Silver, David and Lillicrap, Timothy and Tassa, Yuval and Erez, Tom},
  
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning Continuous Control Policies by Stochastic Value Gradients},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}





@article{petshandful,
  title={Deep reinforcement learning in a handful of trials using probabilistic dynamics models},
  author={Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}


@article{uncertaintyprediction,
  title={Simple and scalable predictive uncertainty estimation using deep ensembles},
  author={Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{Kakade2001ANP,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}


  
@inproceedings{TRPO,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}
  
  
  @inproceedings{debt,
author = {Abe, Naoki and Melville, Prem and Pendus, Cezar and Reddy, Chandan K. and Jensen, David L. and Thomas, Vince P. and Bennett, James J. and Anderson, Gary F. and Cooley, Brent R. and Kowalczyk, Melissa and Domick, Mark and Gardinier, Timothy},
title = {Optimizing Debt Collections Using Constrained Reinforcement Learning},
year = {2010},
publisher = {Association for Computing Machinery},
abstract = {The problem of optimally managing the collections process by taxation authorities is one of prime importance, not only for the revenue it brings but also as a means to administer a fair taxing system. The analogous problem of debt collections management in the private sector, such as banks and credit card companies, is also increasingly gaining attention. With the recent successes in the applications of data analytics and optimization to various business areas, the question arises to what extent such collections processes can be improved by use of leading edge data modeling and optimization techniques. In this paper, we propose and develop a novel approach to this problem based on the framework of constrained Markov Decision Process (MDP), and report on our experience in an actual deployment of a tax collections optimization system at New York State Department of Taxation and Finance (NYS DTF).},
booktitle = {16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
}



@book{cmdpwireless_alex,
  title={Constrained Markov Decision Processes with Application to Wireless Communications},
  author={Zadorojniy, Alexander},
  year={2004},
  publisher={Technion-Israel Institute of Technology, Faculty of Electrical Engineering}
}

@INPROCEEDINGS{4G,
  author={Sun, C. and Stevens-Navarro, E. and Wong, V. W. S.},
  booktitle={2008 IEEE International Conference on Communications}, 
  title={A Constrained MDP-Based Vertical Handoff Decision Algorithm for 4G Wireless Networks}, 
  year={2008}}
  
  @article{bhatnagarrouter,
author = {Bhatnagar, Shalabh and Lakshmanan, K.},
year = {2012},
month = {06},
pages = {},
title = {An Online Actor-Critic Algorithm with Function Approximation for Constrained Markov Decision Processes},
volume = {153},
journal = {Journal of Optimization Theory and Applications},
}

@article{intgrad,
  author    = {Mukund Sundararajan and
               Ankur Taly and
               Qiqi Yan},
  title     = {Axiomatic Attribution for Deep Networks},
  journal   = {ICML},
  year      = {2017}}
  




@inproceedings{metrpo,
  title={Model-Ensemble Trust-Region Policy Optimization},
  author={Kurutach, Thanard and Clavera, Ignasi and Duan, Yan and Tamar, Aviv and Abbeel, Pieter},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@incollection{cvar,
  title={Conditional value-at-risk: optimization approach},
  author={Uryasev, Stanislav and Rockafellar, R Tyrrell},
  booktitle={Stochastic optimization: algorithms and applications},
  pages={411--435},
  year={2001},
  publisher={Springer}
}

@misc{cvarmdp,
  doi = {10.48550/ARXIV.1406.3339},
  
  url = {https://arxiv.org/abs/1406.3339},
  
  author = {Chow, Yinlam and Ghavamzadeh, Mohammad},
  
  keywords = {Artificial Intelligence (cs.AI), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Algorithms for {CVaR} Optimization in {MDPs}},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



%explainability----------



@misc{deepconvgrad,
  doi = {10.48550/ARXIV.1312.6034},
  
  url = {https://arxiv.org/abs/1312.6034},
  
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
  
  publisher = {arXiv},
  
  year = {2013},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{saliencymapssanity,
  author    = {Julius Adebayo and
               Justin Gilmer and
               Michael Muelly and
               Ian J. Goodfellow and
               Moritz Hardt and
               Been Kim},
  title     = {Sanity Checks for Saliency Maps},
  journal   = {CoRR},
  volume    = {abs/1810.03292},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.03292},
  eprinttype = {arXiv},
  eprint    = {1810.03292},
  timestamp = {Tue, 30 Oct 2018 10:49:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-03292.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{reachabilitymoldovan,
  doi = {10.48550/ARXIV.1205.4810},
  
  url = {https://arxiv.org/abs/1205.4810},
  
  author = {Moldovan, Teodor Mihai and Abbeel, Pieter},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Safe Exploration in Markov Decision Processes},
  
  publisher = {arXiv},
  
  year = {2012},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}




@inproceedings{variancesaferl,
author = {Gehring, Clement and Precup, Doina},
title = {Smart Exploration in Reinforcement Learning Using Absolute Temporal Difference Errors},
year = {2013},
isbn = {9781450319935},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Exploration is still one of the crucial problems in reinforcement learning, especially for agents acting in safety-critical situations. We propose a new directed exploration method, based on a notion of state controlability. Intuitively, if an agent wants to stay safe, it should seek out states where the effects of its actions are easier to predict; we call such states more controllable. Our main contribution is a new notion of controlability, computed directly from temporal-difference errors. Unlike other existing approaches of this type, our method scales linearly with the number of state features, and is directly applicable to function approximation. Our method converges to correct values in the policy evaluation setting. We also demonstrate significantly faster learning when this exploration strategy is used in large control problems.},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {1037–1044},
numpages = {8},
keywords = {exploration, reinforcement learning, temporal-difference error},
location = {St. Paul, MN, USA},
series = {AAMAS '13}
}




@inproceedings{pirottasaferlmonotonic,
author = {Pirotta, Matteo and Restelli, Marcello and Pecorino, Alessio and Calandriello, Daniele},
title = {Safe Policy Iteration},
year = {2013},
publisher = {JMLR.org},
abstract = {This paper presents a study of the policy improvement step that can be usefully exploited by approximate policy-iteration algorithms. When either the policy evaluation step or the policy improvement step returns an approximated result, the sequence of policies produced by policy iteration may not be monotonically increasing, and oscillations may occur. To address this issue, we consider safe policy improvements, i.e., at each iteration we search for a policy that maximizes a lower bound to the policy improvement w.r.t. the current policy. When no improving policy can be found the algorithm stops. We propose two safe policy-iteration algorithms that differ in the way the next policy is chosen w.r.t. the estimated greedy policy. Besides being theoretically derived and discussed, the proposed algorithms are empirically evaluated and compared with state-of-the-art approaches on some chain-walk domains and on the Blackjack card game.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {III–307–III–315},
location = {Atlanta, GA, USA},
series = {ICML'13}
}


@article{Joseph2018ce1,
author={Joseph, Ajin George
and Bhatnagar, Shalabh},
title={An online prediction algorithm for reinforcement learning with linear function approximation using cross entropy method},
journal={Machine Learning},
year={2018},
month={Sep},
day={01},
volume={107},
number={8},
pages={1385-1429},
abstract={In this paper, we provide two new stable online algorithms for the problem of prediction in reinforcement learning, i.e., estimating the value function of a model-free Markov reward process using the linear function approximation architecture and with memory and computation costs scaling quadratically in the size of the feature set. The algorithms employ the multi-timescale stochastic approximation variant of the very popular cross entropy optimization method which is a model based search method to find the global optimum of a real-valued function. A proof of convergence of the algorithms using the ODE method is provided. We supplement our theoretical results with experimental comparisons. The algorithms achieve good performance fairly consistently on many RL benchmark problems with regards to computational efficiency, accuracy and stability.},
issn={1573-0565},
doi={10.1007/s10994-018-5727-z},
url={https://doi.org/10.1007/s10994-018-5727-z}
}

@article{Joseph2018ce2,
author={Joseph, Ajin George
and Bhatnagar, Shalabh},
title={An incremental off-policy search in a model-free Markov decision process using a single sample path},
journal={Machine Learning},
year={2018},
month={Jun},
day={01},
volume={107},
number={6},
pages={969-1011},
abstract={In this paper, we consider a modified version of the control problem in a model free Markov decision process (MDP) setting with large state and action spaces. The control problem most commonly addressed in the contemporary literature is to find an optimal policy which maximizes the value function, i.e., the long run discounted reward of the MDP. The current settings also assume access to a generative model of the MDP with the hidden premise that observations of the system behaviour in the form of sample trajectories can be obtained with ease from the model. In this paper, we consider a modified version, where the cost function is the expectation of a non-convex function of the value function without access to the generative model. Rather, we assume that a sample trajectory generated using a priori chosen behaviour policy is made available. In this restricted setting, we solve the modified control problem in its true sense, i.e., to find the best possible policy given this limited information. We propose a stochastic approximation algorithm based on the well-known cross entropy method which is data (sample trajectory) efficient, stable, robust as well as computationally and storage efficient. We provide a proof of convergence of our algorithm to a policy which is globally optimal relative to the behaviour policy. We also present experimental results to corroborate our claims and we demonstrate the superiority of the solution produced by our algorithm compared to the state-of-the-art algorithms under appropriately chosen behaviour policy.},
issn={1573-0565},
doi={10.1007/s10994-018-5697-1},
url={https://doi.org/10.1007/s10994-018-5697-1}
}










@article{stabilitymbrl,
  title={Safe model-based reinforcement learning with stability guarantees},
  author={Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela and Krause, Andreas},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}









@inproceedings{safexphans,
  title={Safe exploration for reinforcement learning},
  author={Alexander Hans and Daniel Schneega{\ss} and Anton Maximilian Sch{\"a}fer and Steffen Udluft},
  booktitle={ESANN},
  year={2008}
}

@inproceedings{likelihoodratio,
author = {Glynn, Peter W.},
title = {Likelilood Ratio Gradient Estimation: An Overview},
year = {1987},
isbn = {0911801324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/318371.318612},
doi = {10.1145/318371.318612},
abstract = {The likelihood ratio method for gradient estimation is briefly surveyed. Two applications settings are described, namely Monte Carlo optimization and statistical analysis of complex stochastic systems. Steady-state gradient estimation is emphasized, and both regenerative and non-regenerative approaches are given. The paper also indicates how these methods apply to general discrete-event simulations; the idea is to view such systems as general state space Markov chains.},
booktitle = {Proceedings of the 19th Conference on Winter Simulation},
pages = {366–375},
numpages = {10},
location = {Atlanta, Georgia, USA},
series = {WSC '87}
}

@book{suttonbook,
author = {Sutton, Richard S. and Barto, Andrew G.},
title = {Introduction to Reinforcement Learning},
year = {1998},
isbn = {0262193981},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
edition = {1st},
abstract = {From the Publisher:In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.}
}






@book{ndpbook,
author = {Bertsekas, Dimitri and Tsitsiklis, John},
year = {1996},
month = {01},
publisher = {Athena Scientific},
pages = {},
title = {Neuro-Dynamic Programming},
volume = {27},
journal = {Third World Planning Review - THIRD WORLD PLAN REV},
doi = {10.1007/978-0-387-74759-0_440}
}

@article{causalexp,
  author    = {Prashan Madumal and
               Tim Miller and
               Liz Sonenberg and
               Frank Vetere},
  title     = {Explainable Reinforcement Learning Through a Causal Lens},
  journal   = {CoRR},
  volume    = {abs/1905.10958},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.10958},
  eprinttype = {arXiv},
  eprint    = {1905.10958},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-10958.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{rewarddecomp,
  title={Explainable reinforcement learning via reward decomposition},
  author={Juozapaitis, Zoe and Koul, Anurag and Fern, Alan and Erwig, Martin and Doshi-Velez, Finale},
  booktitle={IJCAI/ECAI Workshop on explainable artificial intelligence},
  year={2019}
}

@misc{surveyexprl,
  doi = {10.48550/ARXIV.2008.06693},
  
  url = {https://arxiv.org/abs/2008.06693},
  
  author = {Heuillet, Alexandre and Couthouis, Fabien and Díaz-Rodríguez, Natalia},
  
  keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Explainability in Deep Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

@misc{perturb,
  doi = {10.48550/ARXIV.1311.2901},
  
  url = {https://arxiv.org/abs/1311.2901},
  
  author = {Zeiler, Matthew D and Fergus, Rob},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Visualizing and Understanding Convolutional Networks},
  
  publisher = {arXiv},
  
  year = {2013},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{lrp,
  doi = {10.48550/ARXIV.1604.00825},
  
  url = {https://arxiv.org/abs/1604.00825},
  
  author = {Binder, Alexander and Montavon, Grégoire and Bach, Sebastian and Müller, Klaus-Robert and Samek, Wojciech},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Layer-wise Relevance Propagation for Neural Networks with Local Renormalization Layers},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@inproceedings{deeplift,
  title={Learning important features through propagating activation differences},
  author={Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  booktitle={International conference on machine learning},
  pages={3145--3153},
  year={2017},
  organization={PMLR}
}




@inproceedings{visatari,
  title={Visualizing and understanding atari agents},
  author={Greydanus, Samuel and Koul, Anurag and Dodge, Jonathan and Fern, Alan},
  booktitle={International conference on machine learning},
  pages={1792--1801},
  year={2018},
  organization={PMLR}
}


@misc{axiomaticexp,
  doi = {10.48550/ARXIV.1703.01365},
  
  url = {https://arxiv.org/abs/1703.01365},
  
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Axiomatic Attribution for Deep Networks},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


