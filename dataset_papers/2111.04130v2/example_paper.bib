@inproceedings{structuredpruning,
    title = "Structured Pruning of Large Language Models",
    author = "Wang, Ziheng  and
      Wohlwend, Jeremy  and
      Lei, Tao",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = Nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "6151--6162",
}

@inproceedings{sixtneeneheads,
 author = {Michel, Paul and Levy, Omer and Neubig, Graham},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Are Sixteen Heads Really Better than One?},
 volume = {32},
 year = {2019}
}

@inproceedings{weightpruning,
   title={Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning},
   booktitle={Proceedings of the 5th Workshop on Representation Learning for NLP},
   publisher={Association for Computational Linguistics},
   author={Gordon, Mitchell and Duh, Kevin and Andrews, Nicholas},
   year={2020},
   pages={143--155}
}

@article{QBERT, title={Q-{BERT}: Hessian Based Ultra Low Precision Quantization of {BERT}}, volume={34}, number={05}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W. and Keutzer, Kurt}, year={2020}, month={Apr.}, pages={8815-8821} }

@article{IBERT,
  author    = {Sehoon Kim and
               Amir Gholami and
               Zhewei Yao and
               Michael W. Mahoney and
               Kurt Keutzer},
  title     = {{I-BERT:} Integer-only {BERT} Quantization},
  journal={International Conference on Machine Learning},
  year      = {2021}
}

@article{Q8BERT,
   title={{Q8BERT:} Quantized 8Bit {BERT}},
   journal={2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS)},
   publisher={IEEE},
   author={Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe},
   year={2019},
   month={Dec}
}

@inproceedings{BERT-of-Theseus,
    title = "{BERT}-of-Theseus: Compressing {BERT} by Progressive Module Replacing",
    author = "Xu, Canwen  and
      Zhou, Wangchunshu  and
      Ge, Tao  and
      Wei, Furu  and
      Zhou, Ming",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = Nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "7859--7869"
}

@article{optimalsubarchitecture,
  author    = {Adrian de Wynter and
               Daniel J. Perry},
  title     = {Optimal Subarchitecture Extraction For {BERT}},
  journal   = {CoRR},
  volume    = {abs/2010.10499},
  year      = {2020}
}

@inproceedings{FastBERT,
    title = "{F}ast{BERT}: a Self-distilling {BERT} with Adaptive Inference Time",
    author = "Liu, Weijie  and
      Zhou, Peng  and
      Wang, Zhiruo  and
      Zhao, Zhe  and
      Deng, Haotang  and
      Ju, Qi",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = Jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "6035--6044",
}

@inproceedings{MobileBERT,
    title = "{M}obile{BERT}: a Compact Task-Agnostic {BERT} for Resource-Limited Devices",
    author = "Sun, Zhiqing  and
      Yu, Hongkun  and
      Song, Xiaodan  and
      Liu, Renjie  and
      Yang, Yiming  and
      Zhou, Denny",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = Jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "2158--2170",
}

@article{rose2010automatic,
  title={Automatic keyword extraction from individual documents},
  author={Rose, Stuart and Engel, Dave and Cramer, Nick and Cowley, Wendy},
  journal={Text mining: applications and theory},
  volume={1},
  pages={1--20},
  year={2010},
  publisher={Citeseer}
}

@article{bm25paper,
  author    = {Stephen E. Robertson and
               Hugo Zaragoza},
  title     = {The Probabilistic Relevance Framework: {BM25} and Beyond},
  journal   = {Found. Trends Inf. Retr.},
  volume    = {3},
  number    = {4},
  pages     = {333--389},
  year      = {2009}
}

@article{deduplicatetrainingdata,
  author    = {Katherine Lee and
               Daphne Ippolito and
               Andrew Nystrom and
               Chiyuan Zhang and
               Douglas Eck and
               Chris Callison{-}Burch and
               Nicholas Carlini},
  title     = {Deduplicating Training Data Makes Language Models Better},
  journal   = {CoRR},
  volume    = {abs/2107.06499},
  year      = {2021}
}




@inproceedings{scibert,
  author    = {Iz Beltagy and
               Kyle Lo and
               Arman Cohan},
  title     = {Sci{BERT}: {A} Pretrained Language Model for Scientific Text},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages     = {3613--3618},
  address = {Hongkong, China},
  publisher = {Association for Computational Linguistics},
  year      = {2019}
}

@inproceedings{dontstoppretraining,
  author    = {Suchin Gururangan and
               Ana Marasovic and
               Swabha Swayamdipta and
               Kyle Lo and
               Iz Beltagy and
               Doug Downey and
               Noah A. Smith},
  title     = {Don't Stop Pretraining: Adapt Language Models to Domains and Tasks},
  booktitle = {{ACL}},
  pages     = {8342--8360},
  publisher = {Association for Computational Linguistics},
  year      = {2020}
}


@article{chemprot,
  author    = {Jens Kringelum and
               Sonny Kim Kj{\ae}rulff and
               S{\o}ren Brunak and
               Ole Lund and
               Tudor I. Oprea and
               Olivier Taboureau},
  title     = {ChemProt-3.0: a global chemical biology diseases mapping},
  journal   = {Database J. Biol. Databases Curation},
  volume    = {2016},
  year      = {2016}
}


@inproceedings{RCT,
  author    = {Franck Dernoncourt and
               Ji Young Lee},
  title     = {PubMed 200k {RCT:} a Dataset for Sequential Sentence Classification
               in Medical Abstracts},
  booktitle = {{IJCNLP(2)}},
  pages     = {308--313},
  publisher = {Asian Federation of Natural Language Processing},
  year      = {2017}
}


@article{ACLARC,
  author    = {David Jurgens and
               Srijan Kumar and
               Raine Hoover and
               Daniel A. McFarland and
               Dan Jurafsky},
  title     = {Measuring the Evolution of a Scientific Field through Citation Frames},
  journal   = {Trans. Assoc. Comput. Linguistics},
  volume    = {6},
  pages     = {391--406},
  year      = {2018}
}


@inproceedings{SCIERC,
  author    = {Yi Luan and
               Luheng He and
               Mari Ostendorf and
               Hannaneh Hajishirzi},
  title     = {Multi-Task Identification of Entities, Relations, and Coreference
               for Scientific Knowledge Graph Construction},
  booktitle = {{EMNLP}},
  pages     = {3219--3232},
  publisher = {Association for Computational Linguistics},
  year      = {2018}
}

@inproceedings{hyperpartisan,
  author    = {Johannes Kiesel and
               Maria Mestre and
               Rishabh Shukla and
               Emmanuel Vincent and
               Payam Adineh and
               David P. A. Corney and
               Benno Stein and
               Martin Potthast},
  title     = {SemEval-2019 Task 4: Hyperpartisan News Detection},
  booktitle = {SemEval@NAACL-HLT},
  pages     = {829--839},
  publisher = {Association for Computational Linguistics},
  year      = {2019}
}

@inproceedings{agnews,
 author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {649--657},
 publisher = {Curran Associates, Inc.},
 title = {Character-level Convolutional Networks for Text Classification},
 volume = {28},
 year = {2015}
}

@inproceedings{helpfulness,
  author    = {Julian J. McAuley and
               Christopher Targett and
               Qinfeng Shi and
               Anton van den Hengel},
  title     = {Image-Based Recommendations on Styles and Substitutes},
  booktitle = {{SIGIR}},
  pages     = {43--52},
  publisher = {{ACM}},
  year      = {2015}
}

@inproceedings{imdb,
  author    = {Andrew L. Maas and
               Raymond E. Daly and
               Peter T. Pham and
               Dan Huang and
               Andrew Y. Ng and
               Christopher Potts},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {{ACL}},
  pages     = {142--150},
  publisher = {The Association for Computer Linguistics},
  year      = {2011}
}





@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
   publisher = {American Psychological Association},
   address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}}

@inproceedings{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 volume = {30},
 year = {2017}
}



@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    pages = "4171--4186",
}

@misc{liu2019roberta,
    title={Ro{BERT}a: A Robustly Optimized BERT Pretraining Approach},
    author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
    year={2019},
    eprint={1907.11692},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{radford2018gpt,
  added-at = {2019-02-27T03:35:25.000+0100},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/2b30710316a8cfbae687672ea1f85c193/kirk86},
  description = {Language Models are Unsupervised Multitask Learners},
  interhash = {ce8168300081d74707849ed488e2a458},
  intrahash = {b30710316a8cfbae687672ea1f85c193},
  keywords = {learning multitask},
  timestamp = {2019-02-27T03:35:25.000+0100},
  title = {Language Models are Unsupervised Multitask Learners},
  url = {https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf},
  year = 2018
}

@misc{raffel2019exploring,
    title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
    author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
    year={2019},
    eprint={1910.10683},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{yang2020xlnet,
 author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {{XLN}et: Generalized Autoregressive Pretraining for Language Understanding},
 volume = {32},
 year = {2019}
}



@misc{so2021primer,
    title={Primer: Searching for Efficient Transformers for Language Modeling},
    author={David R. So and Wojciech Mańke and Hanxiao Liu and Zihang Dai and Noam Shazeer and Quoc V. Le},
    year={2021},
    eprint={2109.08668},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{tay2021scale,
    title={Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers},
    author={Yi Tay and Mostafa Dehghani and Jinfeng Rao and William Fedus and Samira Abnar and Hyung Won Chung and Sharan Narang and Dani Yogatama and Ashish Vaswani and Donald Metzler},
    year={2021},
    eprint={2109.10686},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{Chen_2021,
  title={Early{BERT}: Efficient {BERT} Training via Early-bird Lottery Tickets},
  author={Chen, Xiaohan and Cheng, Yu and Wang, Shuohang and Gan, Zhe and Wang,
  Zhangyang and Liu, Jingjing},
  booktitle={Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing},
  year={2021}
}

@inproceedings{
clark2020electra,
title={{ELECTRA}: Pre-training Text Encoders as Discriminators Rather Than Generators},
author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
booktitle={International Conference on Learning Representations},
year={2020},
}

@article{Dai_2019,
   title={Using Similarity Measures to Select Pretraining Data for NER},
   url={http://dx.doi.org/10.18653/v1/N19-1149},
   DOI={10.18653/v1/n19-1149},
   journal={Proceedings of the 2019 Conference of the North},
   publisher={Association for Computational Linguistics},
   author={Dai, Xiang and Karimi, Sarvnaz and Hachey, Ben and Paris, Cecile},
   year={2019}
}

@article{Zhang_2019,
   title={Curriculum Learning for Domain Adaptation in Neural Machine Translation},
   journal={Proceedings of the 2019 Conference of the North},
   publisher={Association for Computational Linguistics},
   author={Zhang, Xuan and Shapiro, Pamela and Kumar, Gaurav and McNamee, Paul and Carpuat, Marine and Duh, Kevin},
   year={2019}
}

@misc{hern2021scaling,
    title={Scaling Laws for Transfer},
    author={Danny Hernandez and Jared Kaplan and Tom Henighan and Sam McCandlish},
    year={2021},
    eprint={2102.01293},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{schaul2015prioritized,
    title={Prioritized Experience Replay},
    author={Tom Schaul and John Quan and Ioannis Antonoglou and David Silver},
    year={2015},
    eprint={1511.05952},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{bengio2009cl,
author = {Bengio, Yoshua and Louradour, J\'{e}r\^{o}me and Collobert, Ronan and Weston, Jason},
title = {Curriculum Learning},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553380},
doi = {10.1145/1553374.1553380},
abstract = {Humans and animals learn much better when the examples are not randomly presented
but organized in a meaningful order which illustrates gradually more concepts, and
gradually more complex ones. Here, we formalize such training strategies in the context
of machine learning, and call them "curriculum learning". In the context of recent
research studying the difficulty of training in the presence of non-convex training
criteria (for deep deterministic and stochastic neural networks), we explore curriculum
learning in various set-ups. The experiments show that significant improvements in
generalization can be achieved. We hypothesize that curriculum learning has both an
effect on the speed of convergence of the training process to a minimum and, in the
case of non-convex criteria, on the quality of the local minima obtained: curriculum
learning can be seen as a particular form of continuation method (a general strategy
for global optimization of non-convex functions).},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {41–48},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@inproceedings{you2019large,
  author    = {Yang You and
               Jing Li and
               Sashank J. Reddi and
               Jonathan Hseu and
               Sanjiv Kumar and
               Srinadh Bhojanapalli and
               Xiaodan Song and
               James Demmel and
               Kurt Keutzer and
               Cho{-}Jui Hsieh},
  title     = {Large Batch Optimization for Deep Learning: Training {BERT} in 76
               minutes},
  booktitle = {8th International Conference on Learning Representations (ICLR 2020)},
  address = {Addis Ababa, Ethiopia},
  publisher = {OpenReview.net},
  year      = {2020}
}

@misc{shoeybi2019megatronlm,
    title={Megatron-{LM:} Training Multi-Billion Parameter Language Models Using Model Parallelism},
    author={Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
    year={2019},
    eprint={1909.08053},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{he2020deberta,
author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Wei},
title = {De{BERT}a: Decoding-Enhanced BERT with Disentangled Attention},
booktitle = {2021 International Conference on Learning Representations},
year = {2021},
month = {May},
}

@inproceedings{jiao2019tinybert,
    title = "{T}iny{BERT}: Distilling {BERT} for Natural Language Understanding",
    author = "Jiao, Xiaoqi  and
      Yin, Yichun  and
      Shang, Lifeng  and
      Jiang, Xin  and
      Chen, Xiao  and
      Li, Linlin  and
      Wang, Fang  and
      Liu, Qun",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = Nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "4163--4174",
}

@article{sanh2019distilbert,
  title={Distil{BERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}


@inproceedings{voita-etal-2019-analyzing,
    title = "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
    author = "Voita, Elena  and
      Talbot, David  and
      Moiseev, Fedor  and
      Sennrich, Rico  and
      Titov, Ivan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = Jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    pages = "5797--5808",
}

@inproceedings{kovaleva-etal-2019-revealing,
    title = "Revealing the Dark Secrets of {BERT}",
    author = "Kovaleva, Olga  and
      Romanov, Alexey  and
      Rogers, Anna  and
      Rumshisky, Anna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    year = "2019",
    address = "Hongkong, China",
    publisher = "Association for Computational Linguistics",
    pages = "4365--4374",
}

@article{li2020task,
  title={Task-specific objectives of pre-trained language models for dialogue adaptation},
  author={Li, Junlong and Zhang, Zhuosheng and Zhao, Hai and Zhou, Xi and Zhou, Xiang},
  journal={arXiv preprint arXiv:2009.04984},
  year={2020}
}



@article{lee2020biobert,
  title={Bio{BERT}: a pre-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020},
  publisher={Oxford University Press}
}

@article{cotraining1,
   title={Deep Co-Training for Semi-Supervised Image Recognition},
   ISBN={9783030012670},
   ISSN={1611-3349},
   url={http://dx.doi.org/10.1007/978-3-030-01267-0_9},
   DOI={10.1007/978-3-030-01267-0_9},
   journal={Lecture Notes in Computer Science},
   publisher={Springer International Publishing},
   author={Qiao, Siyuan and Shen, Wei and Zhang, Zhishuai and Wang, Bo and Yuille, Alan},
   year={2018},
   pages={142–159} }

@article{cotraining2,
   title={Deep Co-Training with Task Decomposition for Semi-Supervised Domain Adaptation},
   url={http://dx.doi.org/10.1109/iccv48922.2021.00878},
   DOI={10.1109/iccv48922.2021.00878},
   journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
   publisher={IEEE},
   author={Yang, Luyu and Wang, Yan and Gao, Mingfei and Shrivastava, Abhinav and Weinberger, Kilian Q. and Chao, Wei-Lun and Lim, Ser-Nam},
   year={2021},
   month={Oct} }

@ARTICLE{al1,
  author={Zhu, Jingbo and Wang, Huizhen and Tsou, Benjamin K. and Ma, Matthew},
  journal={IEEE Transactions on Audio, Speech, and Language Processing}, 
  title={Active Learning With Sampling by Uncertainty and Density for Data Annotations}, 
  year={2010},
  volume={18},
  number={6},
  pages={1323-1331},
  doi={10.1109/TASL.2009.2033421}}

@article{al2,
title = {Active learning through density clustering},
journal = {Expert Systems with Applications},
volume = {85},
pages = {305-317},
year = {2017},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.05.046},
url = {https://www.sciencedirect.com/science/article/pii/S095741741730369X},
author = {Min Wang and Fan Min and Zhi-Heng Zhang and Yan-Xue Wu},
keywords = {Active learning, Classification, Density clustering, Master tree, Tri-partitioning},
abstract = {Active learning is used for classification when labeling data are costly, while the main challenge is to identify the critical instances that should be labeled. Clustering-based approaches take advantage of the structure of the data to select representative instances. In this paper, we developed the active learning through density peak clustering (ALEC) algorithm with three new features. First, a master tree was built to express the relationships among the nodes and assist the growth of the cluster tree. Second, a deterministic instance selection strategy was designed using a new importance measure. Third, tri-partitioning was employed to determine the action to be taken on each instance during iterative clustering, labeling, and classifying. Experiments were performed with 14 datasets to compare against state-of-the-art active learning algorithms. Results demonstrated that the new algorithm had higher classification accuracy using the same number of labeled data.}
}

@inproceedings{karpukhin2020dense,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Min, Sewon  and
      Lewis, Patrick  and
      Wu, Ledell  and
      Edunov, Sergey  and
      Chen, Danqi  and
      Yih, Wen-tau",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = Nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "6769--6781",
}

@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle = {34th Conference on Neural Information Processing Systems (NeurIPS 2020)},
  address = {Vancouver, Canada},
  year={2020}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@inproceedings{leeadapting,
    title = "Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections",
    author = "Zhong, Ruiqi  and
      Lee, Kristy  and
      Zhang, Zheng  and
      Klein, Dan",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = Nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    pages = "2856--2878",
}

@inproceedings{gu2020train,
    title = "Train No Evil: Selective Masking for Task-Guided Pre-Training",
    author = "Gu, Yuxian  and
      Zhang, Zhengyan  and
      Wang, Xiaozhi  and
      Liu, Zhiyuan  and
      Sun, Maosong",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "6966--6974",
}

@inproceedings{wang2018glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = Nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    pages = "353--355",
}