\begin{thebibliography}{19}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and Zheng]{Abadi2015}
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
  G.~S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp,
  A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M.,
  Levenberg, J., Man\'{e}, D., Monga, R., Moore, S., Murray, D., Olah, C.,
  Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.,
  Vanhoucke, V., Vasudevan, V., Vi\'{e}gas, F., Vinyals, O., Warden, P.,
  Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X.
\newblock {TensorFlow}: {Large-Scale Machine Learning on Heterogeneous
  Systems}, 2015.
\newblock URL \url{http://tensorflow.org/}.

\bibitem[Agarwal et~al.(2020)Agarwal, Anil, Hazan, Koren, and
  Zhang]{Agarwal2020}
Agarwal, N., Anil, R., Hazan, E., Koren, T., and Zhang, C.
\newblock {Disentangling Adaptive Gradient Methods from Learning Rates}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2002.11803}{{\ttfamily \emph{2002.11803}}}, 2020.

\bibitem[Bergstra \& Bengio(2012)Bergstra and Bengio]{BergstraB12}
Bergstra, J. and Bengio, Y.
\newblock {Random Search for Hyper-Parameter Optimization}.
\newblock \emph{Journal of Machine Learning Research, JMLR}, 13, 2012.

\bibitem[Choi et~al.(2019)Choi, Shallue, Nado, Lee, Maddison, and
  Dahl]{Choi2019}
Choi, D., Shallue, C.~J., Nado, Z., Lee, J., Maddison, C.~J., and Dahl, G.~E.
\newblock {On Empirical Comparisons of Optimizers for Deep Learning}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1910.05446}{{\ttfamily \emph{1910.05446}}}, 2019.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{Deng2009}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In \emph{Proceedings of the IEEE Computer Society Conference on
  Computer Vision and Pattern Recognition}. IEEE, 2009.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{Garipov2018}
Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D.~P., and Wilson, A.~G.
\newblock {Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs}.
\newblock In \emph{Advances in Neural Information Processing Systems 31,
  NeurIPS}, 2018.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{Goodfellow2016}
Goodfellow, I., Bengio, Y., and Courville, A.
\newblock \emph{{Deep Learning}}.
\newblock MIT Press, 2016.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{Goodfellow2014}
Goodfellow, I.~J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
  Ozair, S., Courville, A., and Bengio, Y.
\newblock {Generative Adversarial Nets}.
\newblock In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.~D., and
  Weinberger, K.~Q. (eds.), \emph{Advances in Neural Information Processing
  Systems 27, NIPS}, 2014.

\bibitem[Goyal et~al.(2017)Goyal, Dollár, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola,
  A., Tulloch, A., Jia, Y., and He, K.
\newblock {Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}.
  \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1706.02677}{{\ttfamily \emph{1706.02677}}}, 2017.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  G]{Izmailov2018}
Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., and G, W.~A.
\newblock {Averaging weights leads to wider optima and better generalization}.
\newblock In \emph{Uncertainty in Artificial Intelligence - Proceedings of the
  34th Conference, UAI 2018}, 2018.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{Loshchilov2017}
Loshchilov, I. and Hutter, F.
\newblock {SGDR: Stochastic Gradient Descent with Warm Restarts}.
\newblock In \emph{5th International Conference on Learning Representations,
  ICLR}, 2017.

\bibitem[Metz et~al.(2020)Metz, Maheswaranathan, Sun, Freeman, Poole, and
  Sohl-Dickstein]{Metz2020}
Metz, L., Maheswaranathan, N., Sun, R., Freeman, C.~D., Poole, B., and
  Sohl-Dickstein, J.
\newblock {Using a thousand optimization tasks to learn hyperparameter search
  strategies}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/2002.11887}{{\ttfamily \emph{2002.11887}}}, 2020.

\bibitem[Schneider et~al.(2019)Schneider, Balles, and Hennig]{Schneider2019}
Schneider, F., Balles, L., and Hennig, P.
\newblock {DeepOBS: A Deep Learning Optimizer Benchmark Suite}.
\newblock In \emph{7th International Conference on Learning Representations,
  ICLR}, 2019.

\bibitem[Sivaprasad et~al.(2020)Sivaprasad, Mai, Vogels, Jaggi, and
  Fleuret]{Sivaprasad2020}
Sivaprasad, P.~T., Mai, F., Vogels, T., Jaggi, M., and Fleuret, F.
\newblock {Optimizer Benchmarking Needs to Account for Hyperparameter Tuning}.
\newblock In \emph{37th International Conference on Machine Learning, ICML},
  2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{Vaswani2017}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock {Attention Is All You Need}.
\newblock In \emph{Advances in Neural Information Processing Systems 30, NIPS},
  2017.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, and
  Recht]{Wilson2017}
Wilson, A.~C., Roelofs, R., Stern, M., Srebro, N., and Recht, B.
\newblock {The Marginal Value of Adaptive Gradient Methods in Machine
  Learning}.
\newblock In \emph{Advances in Neural Information Processing Systems 30, NIPS},
  2017.

\bibitem[Wolpert \& Macready(1997)Wolpert and Macready]{DolpertM97}
Wolpert, D.~H. and Macready, W.~G.
\newblock No free lunch theorems for optimization.
\newblock \emph{{IEEE} Trans. Evol. Comput.}, 1\penalty0 (1):\penalty0 67--82,
  1997.

\bibitem[Xing et~al.(2018)Xing, Arpit, Tsirigotis, and Bengio]{Xing2018}
Xing, C., Arpit, D., Tsirigotis, C., and Bengio, Y.
\newblock {A Walk with SGD}. \emph{arXiv}\emph{ preprint:}
  \href{http://arxiv.org/abs/1802.08770}{{\ttfamily \emph{1802.08770}}}, 2018.

\bibitem[Zhang et~al.(2020)Zhang, Lipton, Li, and Smola]{zhang2020dive}
Zhang, A., Lipton, Z.~C., Li, M., and Smola, A.~J.
\newblock \emph{Dive into Deep Learning}.
\newblock 2020.
\newblock \url{https://d2l.ai}.

\end{thebibliography}
