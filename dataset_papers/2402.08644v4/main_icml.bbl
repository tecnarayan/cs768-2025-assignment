\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anil et~al.(2023)]{palm2}
Anil, R. et~al.
\newblock Palm 2 technical report, 2023.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, and Sutton]{Austin2021ProgramSW}
Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C.~J., Terry, M., Le, Q.~V., and Sutton, C.
\newblock Program synthesis with large language models.
\newblock \emph{ArXiv}, abs/2108.07732, 2021.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:237142385}.

\bibitem[Berant et~al.(2013)Berant, Chou, Frostig, and Liang]{DBLP:conf/emnlp/BerantCFL13}
Berant, J., Chou, A., Frostig, R., and Liang, P.
\newblock Semantic parsing on freebase from question-answer pairs.
\newblock In \emph{Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, {A} meeting of SIGDAT, a Special Interest Group of the {ACL}}, pp.\  1533--1544. {ACL}, 2013.
\newblock URL \url{https://aclanthology.org/D13-1160/}.

\bibitem[Cai et~al.(2024)Cai, Li, Geng, Peng, Lee, Chen, and Dao]{medusa}
Cai, T., Li, Y., Geng, Z., Peng, H., Lee, J.~D., Chen, D., and Dao, T.
\newblock Medusa: Simple {LLM} inference acceleration framework with multiple decoding heads.
\newblock \emph{CoRR}, abs/2401.10774, 2024.
\newblock \doi{10.48550/ARXIV.2401.10774}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2401.10774}.

\bibitem[Chelba et~al.(2014)Chelba, Mikolov, Schuster, Ge, Brants, Koehn, and Robinson]{DBLP:conf/interspeech/ChelbaMSGBKR14}
Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., and Robinson, T.
\newblock One billion word benchmark for measuring progress in statistical language modeling.
\newblock In Li, H., Meng, H.~M., Ma, B., Chng, E., and Xie, L. (eds.), \emph{{INTERSPEECH} 2014, 15th Annual Conference of the International Speech Communication Association, Singapore, September 14-18, 2014}, pp.\  2635--2639. {ISCA}, 2014.
\newblock \doi{10.21437/INTERSPEECH.2014-564}.
\newblock URL \url{https://doi.org/10.21437/Interspeech.2014-564}.

\bibitem[Clark et~al.(2020)Clark, Choi, Collins, Garrette, Kwiatkowski, Nikolaev, and Palomaki]{Clark2020TyDiQA}
Clark, J., Choi, E., Collins, M., Garrette, D., Kwiatkowski, T., Nikolaev, V., and Palomaki, J.
\newblock Tydi qa: A benchmark for information-seeking question answering in typologically diverse languages.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 8:\penalty0 454--470, 2020.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:212657414}.

\bibitem[Cloud()]{tpuv5e}
Cloud, G.
\newblock Cloud tpu v5e inference.
\newblock URL \url{https://cloud.google.com/tpu/docs/v5e-inference}.
\newblock Accessed on Feb 1, 2024.

\bibitem[Du et~al.(2022)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu, Firat, et~al.]{du2022glam}
Du, N., Huang, Y., Dai, A.~M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A.~W., Firat, O., et~al.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5547--5569. PMLR, 2022.

\bibitem[He et~al.(2023)He, Zhong, Cai, Lee, and He]{he2023rest}
He, Z., Zhong, Z., Cai, T., Lee, J.~D., and He, D.
\newblock Rest: Retrieval-based speculative decoding.
\newblock \emph{arXiv preprint arXiv:2311.08252}, 2023.

\bibitem[Hermann et~al.(2015)Hermann, Kocisky, Grefenstette, Espeholt, Kay, Suleyman, and Blunsom]{hermann2015teaching}
Hermann, K.~M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., and Blunsom, P.
\newblock Teaching machines to read and comprehend.
\newblock In \emph{Advances in neural information processing systems}, pp.\  1693--1701, 2015.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and Zettlemoyer]{DBLP:conf/acl/JoshiCWZ17}
Joshi, M., Choi, E., Weld, D.~S., and Zettlemoyer, L.
\newblock Triviaqa: {A} large scale distantly supervised challenge dataset for reading comprehension.
\newblock In Barzilay, R. and Kan, M. (eds.), \emph{Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, {ACL} 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers}, pp.\  1601--1611. Association for Computational Linguistics, 2017.
\newblock \doi{10.18653/V1/P17-1147}.
\newblock URL \url{https://doi.org/10.18653/v1/P17-1147}.

\bibitem[Kim et~al.(2018)Kim, Kim, and Kim]{DBLP:journals/corr/abs-1811-00783}
Kim, B., Kim, H., and Kim, G.
\newblock Abstractive summarization of reddit posts with multi-level memory networks.
\newblock \emph{CoRR}, abs/1811.00783, 2018.
\newblock URL \url{http://arxiv.org/abs/1811.00783}.

\bibitem[Kim et~al.(2023)Kim, Mangalam, Moon, Malik, Mahoney, Gholami, and Keutzer]{kim2023speculative}
Kim, S., Mangalam, K., Moon, S., Malik, J., Mahoney, M.~W., Gholami, A., and Keutzer, K.
\newblock Speculative decoding with big little decoder.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins, Parikh, Alberti, Epstein, Polosukhin, Devlin, Lee, Toutanova, Jones, Kelcey, Chang, Dai, Uszkoreit, Le, and Petrov]{DBLP:journals/tacl/KwiatkowskiPRCP19}
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A.~P., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M., Dai, A.~M., Uszkoreit, J., Le, Q., and Petrov, S.
\newblock Natural questions: a benchmark for question answering research.
\newblock \emph{Trans. Assoc. Comput. Linguistics}, 7:\penalty0 452--466, 2019.
\newblock \doi{10.1162/TACL\_A\_00276}.
\newblock URL \url{https://doi.org/10.1162/tacl\_a\_00276}.

\bibitem[Leviathan et~al.(2023)Leviathan, Kalman, and Matias]{leviathan2023fast}
Leviathan, Y., Kalman, M., and Matias, Y.
\newblock Fast inference from transformers via speculative decoding.
\newblock In \emph{International Conference on Machine Learning}, pp.\  19274--19286. PMLR, 2023.

\bibitem[Li et~al.(2022)Li, You, Bhojanapalli, Li, Rawat, Reddi, Ye, Chern, Yu, Guo, et~al.]{li2022lazy}
Li, Z., You, C., Bhojanapalli, S., Li, D., Rawat, A.~S., Reddi, S.~J., Ye, K., Chern, F., Yu, F., Guo, R., et~al.
\newblock The lazy neuron phenomenon: On emergence of activation sparsity in transformers.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Liu et~al.(2023)Liu, Wang, Dao, Zhou, Yuan, Song, Shrivastava, Zhang, Tian, Re, et~al.]{liu2023deja}
Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z., Shrivastava, A., Zhang, C., Tian, Y., Re, C., et~al.
\newblock Deja vu: Contextual sparsity for efficient llms at inference time.
\newblock In \emph{International Conference on Machine Learning}, pp.\  22137--22176. PMLR, 2023.

\bibitem[Mudgal et~al.(2023)Mudgal, Lee, Ganapathy, Li, Wang, Huang, Chen, Cheng, Collins, Strohman, Chen, Beutel, and Beirami]{DBLP:journals/corr/abs-2310-17022}
Mudgal, S., Lee, J., Ganapathy, H., Li, Y., Wang, T., Huang, Y., Chen, Z., Cheng, H., Collins, M., Strohman, T., Chen, J., Beutel, A., and Beirami, A.
\newblock Controlled decoding from language models.
\newblock \emph{CoRR}, abs/2310.17022, 2023.
\newblock \doi{10.48550/ARXIV.2310.17022}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2310.17022}.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi, Pezzelle, Baroni, Boleda, and Fern{\'{a}}ndez]{DBLP:conf/acl/PapernoKLPBPBBF16}
Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q.~N., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fern{\'{a}}ndez, R.
\newblock The {LAMBADA} dataset: Word prediction requiring a broad discourse context.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, {ACL} 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers}. The Association for Computer Linguistics, 2016.
\newblock \doi{10.18653/V1/P16-1144}.
\newblock URL \url{https://doi.org/10.18653/v1/p16-1144}.

\bibitem[Rajpurkar et~al.(2018)Rajpurkar, Jia, and Liang]{DBLP:conf/acl/RajpurkarJL18}
Rajpurkar, P., Jia, R., and Liang, P.
\newblock Know what you don't know: Unanswerable questions for squad.
\newblock In Gurevych, I. and Miyao, Y. (eds.), \emph{Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, {ACL} 2018, Melbourne, Australia, July 15-20, 2018, Volume 2: Short Papers}, pp.\  784--789. Association for Computational Linguistics, 2018.
\newblock \doi{10.18653/V1/P18-2124}.
\newblock URL \url{https://aclanthology.org/P18-2124/}.

\bibitem[Stern et~al.(2018)Stern, Shazeer, and Uszkoreit]{stern2018blockwise}
Stern, M., Shazeer, N., and Uszkoreit, J.
\newblock Blockwise parallel decoding for deep autoregressive models.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2019)Wang, Pruksachatkun, Nangia, Singh, Michael, Hill, Levy, and Bowman]{wang2019superglue}
Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.
\newblock Superglue: A stickier benchmark for general-purpose language understanding systems.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Zerva et~al.(2022)Zerva, Blain, Rei, Lertvittayakumjorn, De~Souza, Eger, Kanojia, Alves, Orǎsan, Fomicheva, et~al.]{zerva2022findings}
Zerva, C., Blain, F., Rei, R., Lertvittayakumjorn, P., De~Souza, J.~G., Eger, S., Kanojia, D., Alves, D., Orǎsan, C., Fomicheva, M., et~al.
\newblock Findings of the wmt 2022 shared task on quality estimation.
\newblock In \emph{Proceedings of the Seventh Conference on Machine Translation (WMT)}, pp.\  69--99, 2022.

\bibitem[Zhou et~al.(2023)Zhou, Lyu, Rawat, Menon, Rostamizadeh, Kumar, Kagy, and Agarwal]{zhou2023distillspec}
Zhou, Y., Lyu, K., Rawat, A.~S., Menon, A.~K., Rostamizadeh, A., Kumar, S., Kagy, J.-F., and Agarwal, R.
\newblock Distillspec: Improving speculative decoding via knowledge distillation.
\newblock \emph{arXiv preprint arXiv:2310.08461}, 2023.

\end{thebibliography}
