\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andreas(2018)]{andreas2018measuring}
Jacob Andreas.
\newblock Measuring compositionality in representation learning.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Auersperger and Pecina(2022)]{auersperger2022defending}
Michal Auersperger and Pavel Pecina.
\newblock Defending compositionality in emergent languages.
\newblock \emph{NAACL 2022}, page 285, 2022.

\bibitem[Bouchacourt and Baroni(2018)]{bouchacourt2018agents}
Diane Bouchacourt and Marco Baroni.
\newblock How agents see things: On visual representations in an emergent language game.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pages 981--985, 2018.

\bibitem[Brandizzi(2023)]{brandizzi2023toward}
Nicolo’ Brandizzi.
\newblock Toward more human-like ai communication: A review of emergent communication research.
\newblock \emph{IEEE Access}, 11:\penalty0 142317--142340, 2023.

\bibitem[Brighton and Kirby(2006)]{brighton2006understanding}
Henry Brighton and Simon Kirby.
\newblock Understanding linguistic evolution by visualizing the emergence of topographic mappings.
\newblock \emph{Artificial life}, 12\penalty0 (2):\penalty0 229--242, 2006.

\bibitem[Carmeli et~al.(2024)Carmeli, Belinkov, and Meir]{carmeli2024concept}
Boaz Carmeli, Yonatan Belinkov, and Ron Meir.
\newblock Concept-best-matching: Evaluating compositionality in emergent communication.
\newblock \emph{arXiv preprint arXiv:2403.14705}, 2024.

\bibitem[Chaabouni et~al.(2019)Chaabouni, Kharitonov, Dupoux, and Baroni]{chaabouni2019anti}
Rahma Chaabouni, Eugene Kharitonov, Emmanuel Dupoux, and Marco Baroni.
\newblock Anti-efficient encoding in emergent communication.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Chaabouni et~al.(2020)Chaabouni, Kharitonov, Bouchacourt, Dupoux, and Baroni]{chaabouni2020compositionality}
Rahma Chaabouni, Eugene Kharitonov, Diane Bouchacourt, Emmanuel Dupoux, and Marco Baroni.
\newblock Compositionality and generalization in emergent languages.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pages 4427--4442, 2020.

\bibitem[Chen et~al.(2018)Chen, Li, Grosse, and Duvenaud]{chen2018isolating}
Ricky~TQ Chen, Xuechen Li, Roger~B Grosse, and David~K Duvenaud.
\newblock Isolating sources of disentanglement in variational autoencoders.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Choi et~al.(2018)Choi, Lazaridou, and De~Freitas]{choi2018compositional}
Edward Choi, Angeliki Lazaridou, and Nando De~Freitas.
\newblock Compositional obverter communication learning from raw visual input.
\newblock In \emph{6th International Conference on Learning Representations, ICLR 2018}. International Conference on Learning Representations, ICLR, 2018.

\bibitem[Denamgana{\"\i} and Walker(2020{\natexlab{a}})]{denamganai2020emergent}
Kevin Denamgana{\"\i} and James~Alfred Walker.
\newblock On (emergent) systematic generalisation and compositionality in visual referential games with straight-through gumbel-softmax estimator.
\newblock \emph{arXiv preprint arXiv:2012.10776}, 2020{\natexlab{a}}.

\bibitem[Denamgana{\"\i} and Walker(2020{\natexlab{b}})]{denamganai2020referentialgym}
Kevin Denamgana{\"\i} and James~Alfred Walker.
\newblock Referentialgym: A nomenclature and framework for language emergence \& grounding in (visual) referential games.
\newblock \emph{arXiv preprint arXiv:2012.09486}, 2020{\natexlab{b}}.

\bibitem[Denamgana{\"\i} et~al.(2023)Denamgana{\"\i}, Missaoui, and Walker]{denamganai2023visual}
Kevin Denamgana{\"\i}, Sondess Missaoui, and James~Alfred Walker.
\newblock Visual referential games further the emergence of disentangled representations.
\newblock \emph{arXiv preprint arXiv:2304.14511}, 2023.

\bibitem[Evtimova et~al.(2018)Evtimova, Drozdov, Kiela, and Cho]{evtimova2018emergent}
Katrina Evtimova, Andrew Drozdov, Douwe Kiela, and Kyunghyun Cho.
\newblock Emergent communication in a multi-modal, multi-step referential game.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Guo et~al.(2021)Guo, Ren, Mathewson, Kirby, Albrecht, and Smith]{guo2021expressivity}
Shangmin Guo, Yi~Ren, Kory~Wallace Mathewson, Simon Kirby, Stefano~V Albrecht, and Kenny Smith.
\newblock Expressivity of emergent languages is a trade-off between contextual complexity and unpredictability.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Gupta et~al.(2021)Gupta, Somepalli, Anubhav, Magalle~Hewa, Zwicker, and Shrivastava]{gupta2021patchgame}
Kamal Gupta, Gowthami Somepalli, Anubhav Anubhav, Vinoj Yasanga~Jayasundara Magalle~Hewa, Matthias Zwicker, and Abhinav Shrivastava.
\newblock Patchgame: Learning to signal mid-level patches in referential games.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 26015--26027, 2021.

\bibitem[Havrylov and Titov(2017)]{havrylov2017emergence}
Serhii Havrylov and Ivan Titov.
\newblock Emergence of language with multi-agent games: Learning to communicate with sequences of symbols.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Hinton and Salakhutdinov(2006)]{hinton2006autoencoder}
Geoffrey~E Hinton and Ruslan~R Salakhutdinov.
\newblock Reducing the dimensionality of data with neural networks.
\newblock \emph{science}, 313\penalty0 (5786):\penalty0 504--507, 2006.

\bibitem[Hjelm et~al.(2018)Hjelm, Fedorov, Lavoie-Marchildon, Grewal, Bachman, Trischler, and Bengio]{infomax}
R~Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio.
\newblock Learning deep representations by mutual information estimation and maximization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Jang et~al.(2017)Jang, Gu, and Poole]{jang2017categorical}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparametrization with gumble-softmax.
\newblock In \emph{International Conference on Learning Representations (ICLR 2017)}. OpenReview. net, 2017.

\bibitem[Jaques et~al.(2019)Jaques, Lazaridou, Hughes, Gulcehre, Ortega, Strouse, Leibo, and De~Freitas]{jaques2019social}
Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, DJ~Strouse, Joel~Z Leibo, and Nando De~Freitas.
\newblock Social influence as intrinsic motivation for multi-agent deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages 3040--3049. PMLR, 2019.

\bibitem[Kharitonov et~al.(2019)Kharitonov, Chaabouni, Bouchacourt, and Baroni]{kharitonov2019egg}
Eugene Kharitonov, Rahma Chaabouni, Diane Bouchacourt, and Marco Baroni.
\newblock Egg: a toolkit for research on emergence of language in games.
\newblock \emph{EMNLP-IJCNLP 2019}, page~55, 2019.

\bibitem[Kharitonov et~al.(2020)Kharitonov, Chaabouni, Bouchacourt, and Baroni]{kharitonov2020entropy}
Eugene Kharitonov, Rahma Chaabouni, Diane Bouchacourt, and Marco Baroni.
\newblock Entropy minimization in emergent languages.
\newblock In \emph{International Conference on Machine Learning}, pages 5220--5230. PMLR, 2020.

\bibitem[Korbak et~al.(2020)Korbak, Zubek, and R{\k{a}}czaszek-Leonardi]{korbak2020measuring}
Tomasz Korbak, Julian Zubek, and Joanna R{\k{a}}czaszek-Leonardi.
\newblock Measuring non-trivial compositionality in emergent communication.
\newblock \emph{arXiv preprint arXiv:2010.15058}, 2020.

\bibitem[Kottur et~al.(2017)Kottur, Moura, Lee, and Batra]{kottur2017natural}
Satwik Kottur, Jos{\'e} Moura, Stefan Lee, and Dhruv Batra.
\newblock Natural language does not emerge ‘naturally’in multi-agent dialog.
\newblock In \emph{Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, pages 2962--2967, 2017.

\bibitem[Kuhn and Tucker(2013)]{kuhn2013nonlinear}
Harold~W Kuhn and Albert~W Tucker.
\newblock Nonlinear programming.
\newblock In \emph{Traces and emergence of nonlinear programming}, pages 247--258. Springer, 2013.

\bibitem[Kuhnle and Copestake(2017)]{kuhnle2017shapeworld}
Alexander Kuhnle and Ann Copestake.
\newblock Shapeworld-a new test methodology for multimodal language understanding.
\newblock \emph{arXiv preprint arXiv:1704.04517}, 2017.

\bibitem[Lazaridou et~al.(2016)Lazaridou, Peysakhovich, and Baroni]{lazaridou2016multi}
Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni.
\newblock Multi-agent cooperation and the emergence of (natural) language.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Lazaridou et~al.(2018)Lazaridou, Hermann, Tuyls, and Clark]{lazaridou2018emergence}
Angeliki Lazaridou, Karl~Moritz Hermann, Karl Tuyls, and Stephen Clark.
\newblock Emergence of linguistic communication from referential games with symbolic and pixel input.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lecun1998mnist}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0 2278--2324, 1998.

\bibitem[Lee et~al.(2019)Lee, Cho, and Kiela]{lee2019countering}
Jason Lee, Kyunghyun Cho, and Douwe Kiela.
\newblock Countering language drift via visual grounding.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 4385--4395, 2019.

\bibitem[Lewis(2008)]{lewis2008convention}
David Lewis.
\newblock \emph{Convention: A philosophical study}.
\newblock John Wiley \& Sons, 2008.

\bibitem[Li and Bowling(2019)]{li2019ease}
Fushan Li and Michael Bowling.
\newblock Ease-of-teaching and language structure from emergent communication.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Lin et~al.(2021)Lin, Huh, Stauffer, Lim, and Isola]{lin2021learning}
Toru Lin, Jacob Huh, Christopher Stauffer, Ser~Nam Lim, and Phillip Isola.
\newblock Learning to ground multi-agent communication with autoencoders.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 15230--15242, 2021.

\bibitem[Lowe et~al.(2019)Lowe, Foerster, Boureau, Pineau, and Dauphin]{lowe2019pitfalls}
Ryan Lowe, Jakob Foerster, Y-Lan Boureau, Joelle Pineau, and Yann Dauphin.
\newblock On the pitfalls of measuring emergent communication.
\newblock In \emph{Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems}, pages 693--701, 2019.

\bibitem[MACQUEEN(1967)]{macqueen1967some}
JB~MACQUEEN.
\newblock Some methods for classification and analysis of multivariate observations.
\newblock In \emph{Proceedings of the 5th Berkeley Symposium on Mathematical Statistics \& Probability}, volume~1, pages 281--297. University of California Press, 1967.

\bibitem[Mu and Goodman(2021)]{mu2021emergent}
Jesse Mu and Noah Goodman.
\newblock Emergent communication of generalizations.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 17994--18007, 2021.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Pihlgren et~al.(2020)Pihlgren, Sandin, and Liwicki]{pihlgren2020improving}
Gustav~Grund Pihlgren, Fredrik Sandin, and Marcus Liwicki.
\newblock Improving image autoencoder embeddings with perceptual loss.
\newblock In \emph{2020 International Joint Conference on Neural Networks (IJCNN)}, pages 1--7. IEEE, 2020.

\bibitem[Ren et~al.(2020)Ren, Guo, Labeau, Cohen, and Kirby]{ren2020compositional}
Yi~Ren, Shangmin Guo, Matthieu Labeau, Shay~B Cohen, and Simon Kirby.
\newblock Compositional languages emerge in a neural iterated learning model.
\newblock In \emph{8th International Conference on Learning Representations}, 2020.

\bibitem[Rita et~al.(2020)Rita, Chaabouni, and Dupoux]{rita2020lazimpa}
Mathieu Rita, Rahma Chaabouni, and Emmanuel Dupoux.
\newblock ``lazimpa'': Lazy and impatient neural agents learn to communicate efficiently.
\newblock In \emph{CONLL 2020-The SIGNLL Conference on Computational Natural Language Learning}, 2020.

\bibitem[Rita et~al.(2022)Rita, Tallec, Michel, Grill, Pietquin, Dupoux, and Strub]{rita2022emergent}
Mathieu Rita, Corentin Tallec, Paul Michel, Jean-Bastien Grill, Olivier Pietquin, Emmanuel Dupoux, and Florian Strub.
\newblock Emergent communication: Generalization and overfitting in lewis games.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 1389--1404, 2022.

\bibitem[Steinert-Threlkeld et~al.(2022)Steinert-Threlkeld, Zhou, Liu, and Downey]{steinert2022emergent}
Shane Steinert-Threlkeld, Xuhui Zhou, Zeyu Liu, and CM~Downey.
\newblock Emergent communication fine-tuning (ec-ft) for pretrained language models.
\newblock In \emph{Emergent Communication Workshop at ICLR 2022}, 2022.

\bibitem[Tucker et~al.(2022{\natexlab{a}})Tucker, Levy, Shah, and Zaslavsky]{tucker2022trading}
Mycal Tucker, Roger Levy, Julie~A Shah, and Noga Zaslavsky.
\newblock Trading off utility, informativeness, and complexity in emergent communication.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 22214--22228, 2022{\natexlab{a}}.

\bibitem[Tucker et~al.(2022{\natexlab{b}})Tucker, Levy, Shah, and Zaslavsky]{tucker2022generalization}
Mycal Tucker, Roger~P Levy, Julie Shah, and Noga Zaslavsky.
\newblock Generalization and translatability in emergent communication via informational constraints.
\newblock In \emph{NeurIPS 2022 Workshop on Information-Theoretic Principles in Cognitive Systems}, 2022{\natexlab{b}}.

\bibitem[Yao et~al.(2022)Yao, Yu, Zhang, Narasimhan, Tenenbaum, and Gan]{yao2022linking}
Shunyu Yao, Mo~Yu, Yang Zhang, Karthik Narasimhan, Joshua Tenenbaum, and Chuang Gan.
\newblock Linking emergent and natural languages via corpus transfer.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\end{thebibliography}
