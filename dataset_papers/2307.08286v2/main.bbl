\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ainsworth et~al.(2023)Ainsworth, Hayase, and Srinivasa]{ainsworth2023git}
Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa.
\newblock Git re-basin: Merging models modulo permutation symmetries.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=CQsmMYmlP5T}.

\bibitem[Ashmore and Gashler(2015)]{ashmore2015method}
Stephen Ashmore and Michael Gashler.
\newblock A method for finding similarity between multi-layer perceptrons by forward bipartite alignment.
\newblock In \emph{2015 International Joint Conference on Neural Networks (IJCNN)}, pages 1--7. IEEE, 2015.

\bibitem[Bansal et~al.(2021)Bansal, Nakkiran, and Barak]{bansal2021revisiting}
Yamini Bansal, Preetum Nakkiran, and Boaz Barak.
\newblock Revisiting model stitching to compare neural representations.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan, editors, \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=ak06J5jNR4}.

\bibitem[Cao et~al.(2022)Cao, Chen, Belkin, and Gu]{cao2022benign}
Yuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu.
\newblock Benign overfitting in two-layer convolutional neural networks.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 25237--25250, 2022.

\bibitem[Draxler et~al.(2018)Draxler, Veschgini, Salmhofer, and Hamprecht]{draxler2018essentially}
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht.
\newblock Essentially no barriers in neural network energy landscape.
\newblock In \emph{International conference on machine learning}, pages 1309--1318. PMLR, 2018.

\bibitem[Entezari et~al.(2022)Entezari, Sedghi, Saukh, and Neyshabur]{entezari2022the}
Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur.
\newblock The role of permutation invariance in linear mode connectivity of neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=dNigytemkL}.

\bibitem[Fort et~al.(2020)Fort, Dziugaite, Paul, Kharaghani, Roy, and Ganguli]{fort2020deep}
Stanislav Fort, Gintare~Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel~M Roy, and Surya Ganguli.
\newblock Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 5850--5861, 2020.

\bibitem[Frankle and Carbin(2019)]{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJl-b3RcF7}.

\bibitem[Frankle et~al.(2020)Frankle, Dziugaite, Roy, and Carbin]{frankle2020linear}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel Roy, and Michael Carbin.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{International Conference on Machine Learning}, pages 3259--3269. PMLR, 2020.

\bibitem[Freeman and Bruna(2017)]{freeman2017topology}
C.~Daniel Freeman and Joan Bruna.
\newblock Topology and geometry of half-rectified network optimization.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=Bk0FWVcgx}.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and Wilson]{garipov2018loss}
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry~P Vetrov, and Andrew~G Wilson.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{kaiming2016residual}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 770--778, 2016.
\newblock \doi{10.1109/CVPR.2016.90}.

\bibitem[Hecht-Nielsen(1990)]{HechtNielsen1990ONTA}
Robert Hecht-Nielsen.
\newblock On the algebraic structure of feedforward network weight spaces.
\newblock 1990.

\bibitem[Koopmans and Beckmann(1957)]{koopmans1957assignment}
Tjalling~C Koopmans and Martin Beckmann.
\newblock Assignment problems and the location of economic activities.
\newblock \emph{Econometrica: journal of the Econometric Society}, pages 53--76, 1957.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kuditipudi et~al.(2019)Kuditipudi, Wang, Lee, Zhang, Li, Hu, Ge, and Arora]{NEURIPS2019_46a4378f}
Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi~Zhang, Zhiyuan Li, Wei Hu, Rong Ge, and Sanjeev Arora.
\newblock Explaining landscape connectivity of low-cost solutions for multilayer nets.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/46a4378f835dc8040c8057beb6a2da52-Paper.pdf}.

\bibitem[Le and Yang(2015)]{le2015tiny}
Ya~Le and Xuan Yang.
\newblock Tiny imagenet visual recognition challenge.
\newblock \emph{CS 231N}, 7\penalty0 (7):\penalty0 3, 2015.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0 2278--2324, 1998.

\bibitem[Lenc and Vedaldi(2015)]{lenc2015understanding}
Karel Lenc and Andrea Vedaldi.
\newblock Understanding image representations by measuring their equivariance and equivalence.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 991--999, 2015.

\bibitem[Li et~al.(2015)Li, Yosinski, Clune, Lipson, and Hopcroft]{li2015convergent}
Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft.
\newblock Convergent learning: Do different neural networks learn the same representations?
\newblock In Dmitry Storcheus, Afshin Rostamizadeh, and Sanjiv Kumar, editors, \emph{Proceedings of the 1st International Workshop on Feature Extraction: Modern Questions and Challenges at NIPS 2015}, volume~44 of \emph{Proceedings of Machine Learning Research}, pages 196--212, Montreal, Canada, 11 Dec 2015. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v44/li15convergent.html}.

\bibitem[Liang et~al.(2018)Liang, Sun, Li, and Srikant]{liang2018understanding}
Shiyu Liang, Ruoyu Sun, Yixuan Li, and Rayadurgam Srikant.
\newblock Understanding the loss surface of neural networks for binary classification.
\newblock In \emph{International Conference on Machine Learning}, pages 2835--2843. PMLR, 2018.

\bibitem[Liu et~al.(2022)Liu, Lou, Wang, Xi, Shen, and Yan]{DBLP:conf/icml/LiuLWXSY22}
Chang Liu, Chenfei Lou, Runzhong Wang, Alan~Yuhan Xi, Li~Shen, and Junchi Yan.
\newblock Deep neural network fusion via graph matching with applications to model ensemble and federated learning.
\newblock In \emph{ICML}, pages 13857--13869, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/liu22k.html}.

\bibitem[Lubana et~al.(2023)Lubana, Bigelow, Dick, Krueger, and Tanaka]{lubana2023mechanistic}
Ekdeep~Singh Lubana, Eric~J Bigelow, Robert~P. Dick, David Krueger, and Hidenori Tanaka.
\newblock Mechanistic mode connectivity, 2023.
\newblock URL \url{https://openreview.net/forum?id=NZZoABNZECq}.

\bibitem[Nagarajan and Kolter(2019)]{nagarajan2019uniform}
Vaishnavh Nagarajan and J~Zico Kolter.
\newblock Uniform convergence may be unable to explain generalization in deep learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Nguyen(2019)]{nguyen2019connected}
Quynh Nguyen.
\newblock On connected sublevel sets in deep learning.
\newblock In \emph{International conference on machine learning}, pages 4790--4799. PMLR, 2019.

\bibitem[Nguyen et~al.(2019)Nguyen, Mukkamala, and Hein]{nguyen2018loss}
Quynh Nguyen, Mahesh~Chandra Mukkamala, and Matthias Hein.
\newblock On the loss landscape of a class of deep neural networks with no bad local valleys.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HJgXsjA5tQ}.

\bibitem[Rumelhart et~al.(1985)Rumelhart, Hinton, and Williams]{rumelhart1985learning}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning internal representations by error propagation.
\newblock Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985.

\bibitem[Serra et~al.(2021)Serra, Yu, Kumar, and Ramalingam]{serra2021scaling}
Thiago Serra, Xin Yu, Abhinav Kumar, and Srikumar Ramalingam.
\newblock Scaling up exact neural network compression by re{LU} stability.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan, editors, \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=tqQ-8MuSqm}.

\bibitem[Simonyan and Zisserman(2015)]{simonyan2015vgg}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In Yoshua Bengio and Yann LeCun, editors, \emph{3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings}, 2015.
\newblock URL \url{http://arxiv.org/abs/1409.1556}.

\bibitem[Singh and Jaggi(2020)]{singh2020model}
Sidak~Pal Singh and Martin Jaggi.
\newblock Model fusion via optimal transport.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 22045--22055, 2020.

\bibitem[Tatro et~al.(2020)Tatro, Chen, Das, Melnyk, Sattigeri, and Lai]{tatro2020optimizing}
Norman Tatro, Pin-Yu Chen, Payel Das, Igor Melnyk, Prasanna Sattigeri, and Rongjie Lai.
\newblock Optimizing mode connectivity via neuron alignment.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 15300--15311, 2020.

\bibitem[Venturi et~al.(2019)Venturi, Bandeira, and Bruna]{venturi2018spurious}
Luca Venturi, Afonso~S. Bandeira, and Joan Bruna.
\newblock Spurious valleys in one-hidden-layer neural network optimization landscapes.
\newblock \emph{J. Mach. Learn. Res.}, 20:\penalty0 133:1--133:34, 2019.
\newblock URL \url{http://jmlr.org/papers/v20/18-674.html}.

\bibitem[Wang et~al.(2020)Wang, Yurochkin, Sun, Papailiopoulos, and Khazaeni]{Wang2020Federated}
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni.
\newblock Federated learning with matched averaging.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=BkluqlSFDS}.

\bibitem[Wang et~al.(2022)Wang, Yan, and Yang]{WangTPAMI22}
Runzhong Wang, Junchi Yan, and Xiaokang Yang.
\newblock Neural graph matching network: Learning lawler's quadratic assignment problem with extension to hypergraph and multiple-graph matching.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 2022.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Gadre, Roelofs, Gontijo-Lopes, Morcos, Namkoong, Farhadi, Carmon, Kornblith, et~al.]{wortsman2022model}
Mitchell Wortsman, Gabriel Ilharco, Samir~Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari~S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et~al.
\newblock Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time.
\newblock In \emph{International Conference on Machine Learning}, pages 23965--23998. PMLR, 2022.

\bibitem[Xu et~al.(2023)Xu, Wang, Frei, Vardi, and Hu]{xu2023benign}
Zhiwei Xu, Yutong Wang, Spencer Frei, Gal Vardi, and Wei Hu.
\newblock Benign overfitting and grokking in relu networks for xor cluster data.
\newblock \emph{arXiv preprint arXiv:2310.02541}, 2023.

\bibitem[Yu et~al.(2018)Yu, Yan, Wang, Liu, and Li]{NEURIPS2018_51d92be1}
Tianshu Yu, Junchi Yan, Yilin Wang, Wei Liu, and baoxin Li.
\newblock Generalizing graph matching beyond quadratic assignment model.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi, and R.~Garnett, editors, \emph{Advances in Neural Information Processing Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2018/file/51d92be1c60d1db1d2e5e7a07da55b26-Paper.pdf}.

\bibitem[Yunis et~al.(2022)Yunis, Patel, Savarese, Vardi, Frankle, Walter, Livescu, and Maire]{yunis2022on}
David Yunis, Kumar~Kshitij Patel, Pedro Henrique~Pamplona Savarese, Gal Vardi, Jonathan Frankle, Matthew Walter, Karen Livescu, and Michael Maire.
\newblock On convexity and linear mode connectivity in neural networks.
\newblock In \emph{OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)}, 2022.
\newblock URL \url{https://openreview.net/forum?id=TZQ3PKL3fPr}.

\bibitem[Zhu et~al.(2023)Zhu, Liu, Chrysos, Locatello, and Cevher]{pmlr-v202-zhu23h}
Zhenyu Zhu, Fanghui Liu, Grigorios Chrysos, Francesco Locatello, and Volkan Cevher.
\newblock Benign overfitting in deep neural networks under lazy training.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, \emph{Proceedings of the 40th International Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 43105--43128. PMLR, 23--29 Jul 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/zhu23h.html}.

\end{thebibliography}
