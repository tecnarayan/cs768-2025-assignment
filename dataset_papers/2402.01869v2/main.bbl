\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{openai2023gpt4}
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.~L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Agrawal et~al.(2023)Agrawal, Panwar, Mohan, Kwatra, Gulavani, and Ramjee]{chunk_prefill_sarathi}
Agrawal, A., Panwar, A., Mohan, J., Kwatra, N., Gulavani, B.~S., and Ramjee, R.
\newblock Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills.
\newblock \emph{arXiv preprint arXiv:2308.16369}, August 2023.

\bibitem[AI(2023)]{tts_sunoai2023bark}
AI, S.
\newblock Bark: Text-to-speech model.
\newblock \url{https://github.com/suno-ai/bark}, 2023.

\bibitem[Ainslie et~al.(2023)Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy, Lebron, and Sanghai]{ainslie-etal-2023-gqa}
Ainslie, J., Lee-Thorp, J., de~Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S.
\newblock {GQA}: Training generalized multi-query transformer models from multi-head checkpoints.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)}, Singapore, December 2023.

\bibitem[Aminabadi et~al.(2022)Aminabadi, Rajbhandari, Awan, Li, Li, Zheng, Ruwase, Smith, Zhang, Rasley, et~al.]{aminabadi2022deepspeed}
Aminabadi, R.~Y., Rajbhandari, S., Awan, A.~A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., et~al.
\newblock Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale.
\newblock In \emph{SC22: International Conference for High Performance Computing, Networking, Storage and Analysis}, Dallas, Texas, November 2022. IEEE.

\bibitem[Baeza-Yates et~al.(1999)Baeza-Yates, Ribeiro-Neto, et~al.]{baeza1999modern}
Baeza-Yates, R., Ribeiro-Neto, B., et~al.
\newblock \emph{Modern Information Retrieval}, volume 463.
\newblock ACM Press, New York, 1999.

\bibitem[Betker et~al.(2024)Betker, Goh, Jing, Brooks, Wang, Li, Ouyang, Zhuang, Lee, Guo, Manassra, Dhariwal, Chu, Jiao, and Ramesh]{betker2024improving}
Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., Manassra, W., Dhariwal, P., Chu, C., Jiao, Y., and Ramesh, A.
\newblock Improving image generation with better captions.
\newblock \url{https://cdn.openai.com/papers/dall-e-3.pdf}, 2024.

\bibitem[Brockman et~al.(2023)Brockman, Eleti, Georges, Jang, Kilpatrick, Lim, Miller, and Pokrass]{openai-chatgpt-whisper-apis}
Brockman, G., Eleti, A., Georges, E., Jang, J., Kilpatrick, L., Lim, R., Miller, L., and Pokrass, M.
\newblock Introducing chatgpt and whisper apis.
\newblock \url{https://openai.com/blog/introducing-chatgpt-and-whisper-apis}, March 1 2023.

\bibitem[Brysbaert(2019)]{brysbaert_2019}
Brysbaert, M.
\newblock How many words do we read per minute? a review and meta-analysis of reading rate.
\newblock \emph{Journal of memory and language}, 109:\penalty0 104047, 2019.

\bibitem[Chase(2022)]{Chase_LangChain_2022}
Chase, H.
\newblock {LangChain}.
\newblock \url{https://github.com/langchain-ai/langchain}, October 2022.

\bibitem[Chen et~al.(2019)Chen, Chen, Tan, Long, Gasic, and Yu]{chen2019agentgraph}
Chen, L., Chen, Z., Tan, B., Long, S., Gasic, M., and Yu, K.
\newblock Agentgraph: Towards universal dialogue management with structured deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1905.11259}, May 2019.

\bibitem[Costa-jussà et~al.(2022)Costa-jussà, Cross, Çelebi, Elbayad, Heafield, Heffernan, Kalbassi, Lam, Licht, Maillard, Sun, Wang, Wenzek, Youngblood, Akula, Barrault, Gonzalez, Hansanti, Hoffman, Jarrett, Sadagopan, Rowe, Spruit, Tran, Andrews, Ayan, Bhosale, Edunov, Fan, Gao, Goswami, Guzmán, Koehn, Mourachko, Ropers, Saleem, Schwenk, and Wang]{nllbteam2022language}
Costa-jussà, M.~R., Cross, J., Çelebi, O., Elbayad, M., Heafield, K., Heffernan, K., Kalbassi, E., Lam, J., Licht, D., Maillard, J., Sun, A., Wang, S., Wenzek, G., Youngblood, A., Akula, B., Barrault, L., Gonzalez, G.~M., Hansanti, P., Hoffman, J., Jarrett, S., Sadagopan, K.~R., Rowe, D., Spruit, S., Tran, C., Andrews, P., Ayan, N.~F., Bhosale, S., Edunov, S., Fan, A., Gao, C., Goswami, V., Guzmán, F., Koehn, P., Mourachko, A., Ropers, C., Saleem, S., Schwenk, H., and Wang, J.
\newblock No language left behind: Scaling human-centered machine translation.
\newblock \emph{arXiv preprint arXiv:2207.04672}, July 2022.

\bibitem[Greyling(2023)]{greyling-chatgpt-api}
Greyling, C.
\newblock When using the chatgpt api, users will have to manage the context.
\newblock \url{https://cobusgreyling.medium.com/when-using-the-chatgpt-api-users-will-have-to-manage-the-context-ba5869238913}, March 6 2023.

\bibitem[Gu et~al.(2022)Gu, Stefani, Wu, Thomason, and Wang]{Gu_2022}
Gu, J., Stefani, E., Wu, Q., Thomason, J., and Wang, X.
\newblock Vision-and-language navigation: A survey of tasks, methods, and future directions.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, Dublin, Ireland, 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.524}.

\bibitem[Hao et~al.(2023)Hao, Liu, Wang, and Hu]{hao2023toolkengpt}
Hao, S., Liu, T., Wang, Z., and Hu, Z.
\newblock Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings.
\newblock In \emph{Advances in Neural Information Processing Systems 36}, New Orleans, Louisiana, December 2023.

\bibitem[Hu et~al.(2024)Hu, Huang, Xu, Chen, Xu, Chen, Feng, Wang, Wang, Bao, Sun, and Shan]{Hu2024InferenceWI}
Hu, C., Huang, H., Xu, L., Chen, X., Xu, J., Chen, S., Feng, H., Wang, C., Wang, S., Bao, Y., Sun, N., and Shan, Y.
\newblock Inference without interference: Disaggregate llm inference for mixed downstream workloads.
\newblock \emph{arXiv preprint arXiv:2401.11181}, January 2024.

\bibitem[Hu et~al.(2023)Hu, Lin, Zhang, Yi, and Gao]{hu2023look}
Hu, Y., Lin, F., Zhang, T., Yi, L., and Gao, Y.
\newblock Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning.
\newblock \emph{arXiv preprint arXiv:2311.17842}, November 2023.

\bibitem[Huang et~al.(2022)Huang, Abbeel, Pathak, and Mordatch]{huang2022language}
Huang, W., Abbeel, P., Pathak, D., and Mordatch, I.
\newblock Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.
\newblock In \emph{Proceedings of 39th International Conference on Machine Learning}, Honolulu, Hawai'i, 2022.

\bibitem[Izacard et~al.(2022)Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, and Grave]{izacard2022atlas}
Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E.
\newblock Atlas: Few-shot learning with retrieval augmented language models, 2022.

\bibitem[Khattab et~al.(2024)Khattab, Singhvi, Maheshwari, Zhang, Santhanam, A, Haq, Sharma, Joshi, Moazam, Miller, Zaharia, and Potts]{khattab2024dspy}
Khattab, O., Singhvi, A., Maheshwari, P., Zhang, Z., Santhanam, K., A, S.~V., Haq, S., Sharma, A., Joshi, T.~T., Moazam, H., Miller, H., Zaharia, M., and Potts, C.
\newblock {DSP}y: Compiling declarative language model calls into state-of-the-art pipelines.
\newblock In \emph{The Twelfth International Conference on Learning Representations (ICLR '24)}, Vienna, Austria, 2024.
\newblock URL \url{https://openreview.net/forum?id=sY5N0zY5Od}.

\bibitem[Kwon et~al.(2023)Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang, and Stoica]{vLLM}
Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C.~H., Gonzalez, J., Zhang, H., and Stoica, I.
\newblock Efficient memory management for large language model serving with pagedattention.
\newblock In \emph{Proceedings of the 29th Symposium on Operating Systems Principles}, Koblenz, Germany, October 2023.

\bibitem[Li et~al.(2023)Li, Zheng, Zhong, Liu, Sheng, Jin, Huang, Chen, Zhang, Gonzalez, and Stoica]{li2023alpaserve}
Li, Z., Zheng, L., Zhong, Y., Liu, V., Sheng, Y., Jin, X., Huang, Y., Chen, Z., Zhang, H., Gonzalez, J.~E., and Stoica, I.
\newblock {AlpaServe}: Statistical multiplexing with model parallelism for deep learning serving.
\newblock In \emph{17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)}, Boston, MA, July 2023.

\bibitem[Lu et~al.(2023)Lu, Peng, Cheng, Galley, Chang, Wu, Zhu, and Gao]{lu2023chameleon}
Lu, P., Peng, B., Cheng, H., Galley, M., Chang, K.-W., Wu, Y.~N., Zhu, S.-C., and Gao, J.
\newblock Chameleon: Plug-and-play compositional reasoning with large language models.
\newblock In \emph{Proceedings of the 37th International Conference on Neural Information Processing Systems (NeurIPS '23)}, New Orleans, Louisiana, December 2023.

\bibitem[Ma et~al.(2015)Ma, Edge, Findlater, and Tan]{ma2015haptic}
Ma, Z., Edge, D., Findlater, L., and Tan, H.~Z.
\newblock Haptic keyclick feedback improves typing speed and reduces typing errors on a flat keyboard.
\newblock In \emph{2015 IEEE World Haptics Conference (WHC)}, Evanston, Illinois, 2015. IEEE.

\bibitem[Meta(2024)]{llama3}
Meta.
\newblock Meta llama 3.
\newblock \url{https://llama.meta.com/llama3/}, 2024.

\bibitem[Mialon et~al.(2023)Mialon, Dessi, Lomeli, Nalmpantis, Pasunuru, Raileanu, Roziere, Schick, Dwivedi-Yu, Celikyilmaz, Grave, LeCun, and Scialom]{mialon2023augmented}
Mialon, G., Dessi, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., Roziere, B., Schick, T., Dwivedi-Yu, J., Celikyilmaz, A., Grave, E., LeCun, Y., and Scialom, T.
\newblock Augmented language models: a survey.
\newblock \emph{Transactions on Machine Learning Research (TMLR)}, 2023.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=jh7wH2AzKK}.
\newblock Survey Certification.

\bibitem[Ng(2024)]{ng-blog}
Ng, A.
\newblock The batch weekly issues 241.
\newblock \url{https://www.deeplearning.ai/the-batch/issue-241/}, March 2024.

\bibitem[NVIDIA(2023)]{nvidia2023fastertransformer}
NVIDIA.
\newblock Fastertransformer.
\newblock \url{https://github.com/NVIDIA/FasterTransformer}, 2023.

\bibitem[{OpenAI}(2023)]{chatgpt-plugin}
{OpenAI}.
\newblock {ChatGPT plugins}.
\newblock \url{https://openai.com/blog/chatgpt-plugins}, March 2023.

\bibitem[OpenTable(2023)]{opentable-chatgpt}
OpenTable.
\newblock New: Chatgpt restaurant recs, powered by opentable.
\newblock \url{https://www.opentable.com/blog/chatgpt/}, March 23 2023.

\bibitem[Parisi et~al.(2022)Parisi, Zhao, and Fiedel]{parisi2022talm}
Parisi, A., Zhao, Y., and Fiedel, N.
\newblock Talm: Tool augmented language models.
\newblock \emph{arXiv preprint arXiv:2205.12255}, May 2022.

\bibitem[Patel et~al.(2024)Patel, Choukse, Zhang, Shah, Goiri, Maleki, and Bianchini]{patel2024splitwise}
Patel, P., Choukse, E., Zhang, C., Shah, A., Goiri, {\'I}., Maleki, S., and Bianchini, R.
\newblock Splitwise: Efficient generative llm inference using phase splitting.
\newblock In \emph{The 53th International Symposium on Computer Architecture (ISCA 2024)}, Buenos Aires, Argentina, June 2024.

\bibitem[Patil et~al.(2023)Patil, Zhang, Wang, and Gonzalez]{patil2023gorilla}
Patil, S.~G., Zhang, T., Wang, X., and Gonzalez, J.~E.
\newblock Gorilla: Large language model connected with massive apis.
\newblock \emph{arXiv preprint arXiv:2305.15334}, 2023.

\bibitem[Qian et~al.(2023)Qian, Han, Fung, Qin, Liu, and Ji]{qian-etal-2023-creator}
Qian, C., Han, C., Fung, Y., Qin, Y., Liu, Z., and Ji, H.
\newblock {CREATOR}: Tool creation for disentangling abstract and concrete reasoning of large language models.
\newblock In Bouamor, H., Pino, J., and Bali, K. (eds.), \emph{Findings of the Association for Computational Linguistics: (EMNLP '23)}, Singapore, December 2023.
\newblock URL \url{https://aclanthology.org/2023.findings-emnlp.462}.

\bibitem[Qin et~al.(2023)Qin, Hu, Lin, Chen, Ding, Cui, Zeng, Huang, Xiao, Han, Fung, Su, Wang, Qian, Tian, Zhu, Liang, Shen, Xu, Zhang, Ye, Li, Tang, Yi, Zhu, Dai, Yan, Cong, Lu, Zhao, Huang, Yan, Han, Sun, Li, Phang, Yang, Wu, Ji, Liu, and Sun]{qin2023tool}
Qin, Y., Hu, S., Lin, Y., Chen, W., Ding, N., Cui, G., Zeng, Z., Huang, Y., Xiao, C., Han, C., Fung, Y.~R., Su, Y., Wang, H., Qian, C., Tian, R., Zhu, K., Liang, S., Shen, X., Xu, B., Zhang, Z., Ye, Y., Li, B., Tang, Z., Yi, J., Zhu, Y., Dai, Z., Yan, L., Cong, X., Lu, Y., Zhao, W., Huang, Y., Yan, J., Han, X., Sun, X., Li, D., Phang, J., Yang, C., Wu, T., Ji, H., Liu, Z., and Sun, M.
\newblock Tool learning with foundation models.
\newblock \emph{arXiv preprint arXiv:2304.08354}, June 2023.

\bibitem[Rombach et~al.(2021)Rombach, Blattmann, Lorenz, Esser, and Ommer]{stable_diffusion}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock \emph{arXiv preprint arXiv:2112.10752}, 2021.

\bibitem[Schick et~al.(2023)Schick, Dwivedi-Yu, Dess{\`\i}, Raileanu, Lomeli, Hambro, Zettlemoyer, Cancedda, and Scialom]{schick2023toolformer}
Schick, T., Dwivedi-Yu, J., Dess{\`\i}, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N., and Scialom, T.
\newblock Toolformer: Language models can teach themselves to use tools.
\newblock \emph{37th Conference on Neural Information Processing Systems}, 2023.

\bibitem[Shen et~al.(2023)Shen, Song, Tan, Li, Lu, and Zhuang]{shen2023hugginggpt}
Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y.
\newblock Hugging{GPT}: Solving {AI} tasks with chat{GPT} and its friends in hugging face.
\newblock In \emph{Proceedings of the 37th International Conference on Neural Information Processing Systems (NeurIPS '23)}, New Orleans, Louisiana, December 2023.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro]{shoeybi2020megatronlm}
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B.
\newblock Megatron-lm: Training multi-billion parameter language models using model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Shridhar et~al.(2021)Shridhar, Yuan, C\^ot\'e, Bisk, Trischler, and Hausknecht]{shridhar2020alfworld}
Shridhar, M., Yuan, X., C\^ot\'e, M.-A., Bisk, Y., Trischler, A., and Hausknecht, M.
\newblock Alfworld: Aligning text and embodied environments for interactive learning.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, Virtual, 2021.

\bibitem[Srivatsa et~al.(2024)Srivatsa, He, Abhyankar, Li, and Zhang]{srivatsa2024preble}
Srivatsa, V., He, Z., Abhyankar, R., Li, D., and Zhang, Y.
\newblock Preble: Efficient distributed prompt scheduling for llm serving.
\newblock \emph{UCSD CSE Technical Reports}, May 2024.
\newblock URL \url{https://escholarship.org/uc/item/1bm0k1w0}.

\bibitem[Suris et~al.(2023)Suris, Menon, and Vondrick]{surís2023vipergpt}
Suris, D., Menon, S., and Vondrick, C.
\newblock Vipergpt: Visual inference via python execution for reasoning.
\newblock In \emph{2023 IEEE/CVF International Conference on Computer Vision (ICCV '23)}, pp.\  11854--11864, Los Alamitos, CA, USA, October 2023.
\newblock URL \url{https://doi.ieeecomputersociety.org/10.1109/ICCV51070.2023.01092}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi{\`e}re, B., Goyal, N., Hambro, E., Azhar, F., et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Vaidya et~al.(2023)Vaidya, Comly, DeLaere, Patel, and Oh]{tensorRTllm}
Vaidya, N., Comly, N., DeLaere, J., Patel, A., and Oh, F.
\newblock Nvidia tensorrt-llm supercharges large language model inference on nvidia h100 gpus.
\newblock \url{https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/}, 2023.

\bibitem[Wang \& Komatsuzaki(2021)Wang and Komatsuzaki]{gpt-j-6b}
Wang, B. and Komatsuzaki, A.
\newblock {GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}.
\newblock \url{https://github.com/kingoflolz/mesh-transformer-jax}, May 2021.

\bibitem[Wang et~al.(2023)Wang, Ma, Feng, Zhang, ran Yang, Zhang, Chen, Tang, Chen, Lin, Zhao, Wei, and rong Wen]{Wang2023ASO}
Wang, L., Ma, C., Feng, X., Zhang, Z., ran Yang, H., Zhang, J., Chen, Z.-Y., Tang, J., Chen, X., Lin, Y., Zhao, W.~X., Wei, Z., and rong Wen, J.
\newblock A survey on large language model based autonomous agents.
\newblock \emph{arXiv preprint arXiv:2308.11432}, August 2023.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:261064713}.

\bibitem[Wei et~al.(2023)Wei, Wang, Schuurmans, Bosma, Ichter, Xia, Chi, Le, and Zhou]{wei2022chain}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E.~H., Le, Q.~V., and Zhou, D.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock In \emph{Proceedings of the 36th International Conference on Neural Information Processing Systems}, New Orleans, Louisiana, 2023.

\bibitem[Wolfram(2023)]{wolfram-chatgpt}
Wolfram, S.
\newblock Chatgpt gets its `wolfram superpowers'!
\newblock \url{{https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/}}, March 2023.

\bibitem[Wu et~al.(2023)Wu, Zhong, Zhang, Huang, Liu, and Jin]{wu2023fast}
Wu, B., Zhong, Y., Zhang, Z., Huang, G., Liu, X., and Jin, X.
\newblock Fast distributed inference serving for large language models.
\newblock \emph{arXiv preprint arXiv:2305.05920}, May 2023.

\bibitem[Yang et~al.(2018)Yang, Qi, Zhang, Bengio, Cohen, Salakhutdinov, and Manning]{yang2018hotpotqa}
Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W.~W., Salakhutdinov, R., and Manning, C.~D.
\newblock Hotpotqa: A dataset for diverse, explainable multi-hop question answering.
\newblock \emph{arXiv preprint arXiv:1809.09600}, 2018.

\bibitem[Yao et~al.(2023)Yao, Zhao, Yu, Du, Shafran, Narasimhan, and Cao]{yao2022react}
Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K.~R., and Cao, Y.
\newblock React: Synergizing reasoning and acting in language models.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, Kigali, Rwanda, May 2023.

\bibitem[Yu et~al.(2022)Yu, Jeong, Kim, Kim, and Chun]{Orca}
Yu, G.-I., Jeong, J.~S., Kim, G.-W., Kim, S., and Chun, B.-G.
\newblock {Orca: A Distributed Serving System for {Transformer-Based} Generative Models}.
\newblock In \emph{16th USENIX Symposium on Operating Systems Design and Implementation (OSDI '22)}, Carlsbad, CA, July 2022.

\bibitem[Zhang et~al.(2023)Zhang, Zhang, Li, and Smola]{zhang2023automatic}
Zhang, Z., Zhang, A., Li, M., and Smola, A.
\newblock Automatic chain of thought prompting in large language models.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, Kigali, Rwanda, May 2023.

\bibitem[Zheng et~al.(2023{\natexlab{a}})Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, Zhang, Gonzalez, and Stoica]{vicuna_share_gpt}
Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., Zhang, H., Gonzalez, J.~E., and Stoica, I.
\newblock Judging {LLM}-as-a-judge with {MT}-bench and chatbot arena.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, New Orleans, Louisiana, December 2023{\natexlab{a}}.

\bibitem[Zheng et~al.(2023{\natexlab{b}})Zheng, Yin, Xie, Huang, Sun, Yu, Cao, Kozyrakis, Stoica, Gonzalez, Barrett, and Sheng]{zheng2023efficiently}
Zheng, L., Yin, L., Xie, Z., Huang, J., Sun, C., Yu, C.~H., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J.~E., Barrett, C., and Sheng, Y.
\newblock Efficiently programming large language models using sglang.
\newblock \emph{arXiv preprint arXiv:2312.07104}, December 2023{\natexlab{b}}.

\bibitem[Zhong et~al.(2024)Zhong, Liu, Chen, Hu, Zhu, Liu, Jin, and Zhang]{zhong2024distserve}
Zhong, Y., Liu, S., Chen, J., Hu, J., Zhu, Y., Liu, X., Jin, X., and Zhang, H.
\newblock Distllm: Disaggregating prefill and decoding for goodput-optimized large language model serving.
\newblock In \emph{Proceedings of the 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI '24)}, Santa Clara, CA, July 2024.

\end{thebibliography}
