\begin{thebibliography}{10}

\bibitem{woodruff2014sketching}
David~P Woodruff et~al.
\newblock Sketching as a tool for numerical linear algebra.
\newblock {\em Foundations and Trends{\textregistered} in Theoretical Computer
  Science}, 10(1--2):1--157, 2014.

\bibitem{dasgupta2009sampling}
Anirban Dasgupta, Petros Drineas, Boulos Harb, Ravi Kumar, and Michael~W
  Mahoney.
\newblock Sampling algorithms and coresets for $\ell_{p}$ regression.
\newblock {\em SIAM Journal on Computing}, 38(5):2060--2078, 2009.

\bibitem{cohen2015p}
Michael~B Cohen and Richard Peng.
\newblock L p row sampling by lewis weights.
\newblock In {\em Proceedings of the forty-seventh annual ACM symposium on
  Theory of computing}, pages 183--192. ACM, 2015.

\bibitem{clarkson2016fast}
Kenneth~L Clarkson, Petros Drineas, Malik Magdon-Ismail, Michael~W Mahoney,
  Xiangrui Meng, and David~P Woodruff.
\newblock The fast cauchy transform and faster robust linear regression.
\newblock {\em SIAM Journal on Computing}, 45(3):763--810, 2016.

\bibitem{kruskal1977three}
Joseph~B Kruskal.
\newblock Three-way arrays: rank and uniqueness of trilinear decompositions,
  with application to arithmetic complexity and statistics.
\newblock {\em Linear algebra and its applications}, 18(2):95--138, 1977.

\bibitem{anandkumar2014tensor}
Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham~M Kakade, and Matus Telgarsky.
\newblock Tensor decompositions for learning latent variable models.
\newblock {\em The Journal of Machine Learning Research}, 15(1):2773--2832,
  2014.

\bibitem{anandkumar2012method}
Animashree Anandkumar, Daniel Hsu, and Sham~M Kakade.
\newblock A method of moments for mixture models and hidden markov models.
\newblock In {\em Conference on Learning Theory}, pages 33--1, 2012.

\bibitem{hsu2012spectral}
Daniel Hsu, Sham~M Kakade, and Tong Zhang.
\newblock A spectral algorithm for learning hidden markov models.
\newblock {\em Journal of Computer and System Sciences}, 78(5):1460--1480,
  2012.

\bibitem{jenatton2012latent}
Rodolphe Jenatton, Nicolas~L Roux, Antoine Bordes, and Guillaume~R Obozinski.
\newblock A latent factor model for highly multi-relational data.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3167--3175, 2012.

\bibitem{janzamin2015beating}
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar.
\newblock Beating the perils of non-convexity: Guaranteed training of neural
  networks using tensor methods.
\newblock {\em arXiv preprint arXiv:1506.08473}, 2015.

\bibitem{song2016sublinear}
Zhao Song, David Woodruff, and Huan Zhang.
\newblock Sublinear time orthogonal tensor decomposition.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  793--801, 2016.

\bibitem{dickens2018leveraging}
Charlie Dickens, Graham Cormode, and David Woodruff.
\newblock Leveraging well-conditioned bases: Streaming and distributed
  summaries in minkowski $ p $-norms.
\newblock In {\em International Conference on Machine Learning}, pages
  1243--1251, 2018.

\bibitem{cohen2016online}
Michael~B Cohen, Cameron Musco, and Jakub Pachocki.
\newblock Online row sampling.
\newblock In {\em Approximation, Randomization, and Combinatorial Optimization.
  Algorithms and Techniques (APPROX/RANDOM 2016)}. Schloss
  Dagstuhl-Leibniz-Zentrum fuer Informatik, 2016.

\bibitem{kolda2009tensor}
Tamara~G Kolda and Brett~W Bader.
\newblock Tensor decompositions and applications.
\newblock {\em SIAM review}, 51(3):455--500, 2009.

\bibitem{oseledets2011tensor}
Ivan~V Oseledets.
\newblock Tensor-train decomposition.
\newblock {\em SIAM Journal on Scientific Computing}, 33(5):2295--2317, 2011.

\bibitem{langberg2010universal}
Michael Langberg and Leonard~J Schulman.
\newblock Universal $\varepsilon$-approximators for integrals.
\newblock In {\em Proceedings of the twenty-first annual ACM-SIAM symposium on
  Discrete Algorithms}, pages 598--607. SIAM, 2010.

\bibitem{dubhashi2009concentration}
Devdatt~P Dubhashi and Alessandro Panconesi.
\newblock {\em Concentration of measure for the analysis of randomized
  algorithms}.
\newblock Cambridge University Press, 2009.

\bibitem{tropp2015introduction}
Joel~A Tropp et~al.
\newblock An introduction to matrix concentration inequalities.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  8(1-2):1--230, 2015.

\bibitem{haussler1987e}
David Haussler and Emo Welzl.
\newblock $\epsilon$-nets and simplex range queries.
\newblock {\em Discrete \& Computational Geometry}, 2(2):127--151, 1987.

\bibitem{agarwal2004approximating}
Pankaj~K Agarwal, Sariel Har-Peled, and Kasturi~R Varadarajan.
\newblock Approximating extent measures of points.
\newblock {\em Journal of the ACM (JACM)}, 51(4):606--635, 2004.

\bibitem{har2004coresets}
Sariel Har-Peled and Soham Mazumdar.
\newblock On coresets for k-means and k-median clustering.
\newblock In {\em Proceedings of the thirty-sixth annual ACM symposium on
  Theory of computing}, pages 291--300. ACM, 2004.

\bibitem{feldman2011unified}
Dan Feldman and Michael Langberg.
\newblock A unified framework for approximating and clustering data.
\newblock In {\em Proceedings of the forty-third annual ACM symposium on Theory
  of computing}, pages 569--578. ACM, 2011.

\bibitem{braverman2016new}
Vladimir Braverman, Dan Feldman, and Harry Lang.
\newblock New frameworks for offline and streaming coreset constructions.
\newblock {\em arXiv preprint arXiv:1612.00889}, 2016.

\bibitem{bachem2017practical}
Olivier Bachem, Mario Lucic, and Andreas Krause.
\newblock Practical coreset constructions for machine learning.
\newblock {\em stat}, 1050:4, 2017.

\bibitem{cohen2017input}
Michael~B Cohen, Cameron Musco, and Christopher Musco.
\newblock Input sparsity time low-rank approximation via ridge leverage score
  sampling.
\newblock In {\em Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 1758--1777. SIAM, 2017.

\bibitem{hillar2013most}
Christopher~J Hillar and Lek-Heng Lim.
\newblock Most tensor problems are np-hard.
\newblock {\em Journal of the ACM (JACM)}, 60(6):45, 2013.

\bibitem{wang2015fast}
Yining Wang, Hsiao-Yu Tung, Alexander~J Smola, and Anima Anandkumar.
\newblock Fast and guaranteed tensor decomposition via sketching.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  991--999, 2015.

\bibitem{bhojanapalli2015new}
Srinadh Bhojanapalli and Sujay Sanghavi.
\newblock A new sampling technique for tensors.
\newblock {\em stat}, 1050:19, 2015.

\bibitem{huang2015online}
Furong Huang, UN~Niranjan, Mohammad~Umar Hakeem, and Animashree Anandkumar.
\newblock Online tensor methods for learning latent variable models.
\newblock {\em The Journal of Machine Learning Research}, 16(1):2797--2835,
  2015.

\bibitem{wang2016online}
Yining Wang and Anima Anandkumar.
\newblock Online and differentially-private tensor decomposition.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3531--3539, 2016.

\bibitem{song2019relative}
Zhao Song, David~P Woodruff, and Peilin Zhong.
\newblock Relative error tensor low rank approximation.
\newblock In {\em Proceedings of the Thirtieth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 2772--2789. Society for Industrial and Applied
  Mathematics, 2019.

\bibitem{battaglino2018practical}
Casey Battaglino, Grey Ballard, and Tamara~G Kolda.
\newblock A practical randomized cp tensor decomposition.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  39(2):876--901, 2018.

\bibitem{erichson2020randomized}
N~Benjamin Erichson, Krithika Manohar, Steven~L Brunton, and J~Nathan Kutz.
\newblock Randomized cp tensor decomposition.
\newblock {\em Machine Learning: Science and Technology}, 1(2):025012, 2020.

\bibitem{woodruff2013subspace}
David Woodruff and Qin Zhang.
\newblock Subspace embeddings and $\ell_{p}$-regression using exponential
  random variables.
\newblock In {\em Conference on Learning Theory}, pages 546--567, 2013.

\bibitem{schechtman2011tight}
Gideon Schechtman.
\newblock Tight embedding of subspaces of $\ell_p$ in $\ell_{p}^{n}$ for even.
\newblock {\em Proceedings of the American Mathematical Society},
  139(12):4419--4421, 2011.

\bibitem{sherman1950adjustment}
Jack Sherman and Winifred~J Morrison.
\newblock Adjustment of an inverse matrix corresponding to a change in one
  element of a given matrix.
\newblock {\em The Annals of Mathematical Statistics}, 21(1):124--127, 1950.

\bibitem{clarkson2017low}
Kenneth~L Clarkson and David~P Woodruff.
\newblock Low-rank approximation and regression in input sparsity time.
\newblock {\em Journal of the ACM (JACM)}, 63(6):54, 2017.

\bibitem{tropp2011freedman}
Joel Tropp et~al.
\newblock Freedman's inequality for matrix martingales.
\newblock {\em Electronic Communications in Probability}, 16:262--270, 2011.

\bibitem{harville1998matrix}
DA~Harville.
\newblock Matrix algebra from a statistician's perspective.
\newblock Technical report, Springer-Verlag, 1997.

\bibitem{cohen2015uniform}
Michael~B Cohen, Yin~Tat Lee, Cameron Musco, Christopher Musco, Richard Peng,
  and Aaron Sidford.
\newblock Uniform sampling for matrix approximation.
\newblock In {\em Proceedings of the 2015 Conference on Innovations in
  Theoretical Computer Science}, pages 181--190. ACM, 2015.

\bibitem{vervliet2016tensorlab}
N~Vervliet, O~Debals, L~Sorber, M~Van~Barel, and L~De~Lathauwer.
\newblock Tensorlab 3.0. available online.
\newblock {\em URL: http://www. tensorlab. net}, 2016.

\end{thebibliography}
