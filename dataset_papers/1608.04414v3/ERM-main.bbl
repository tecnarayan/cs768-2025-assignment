\begin{thebibliography}{10}

\bibitem{BachM13}
F.~R. Bach and E.~Moulines.
\newblock Non-strongly-convex smooth stochastic approximation with convergence
  rate o(1/n).
\newblock In {\em NIPS}, pages 773--781, 2013.

\bibitem{BassilyNSSSU16}
R.~Bassily, K.~Nissim, A.~D. Smith, T.~Steinke, U.~Stemmer, and J.~Ullman.
\newblock Algorithmic stability for adaptive data analysis.
\newblock In {\em STOC}, pages 1046--1059, 2016.

\bibitem{BelloniLNR15}
A.~Belloni, T.~Liang, H.~Narayanan, and A.~Rakhlin.
\newblock Escaping the local minima via simulated annealing: Optimization of
  approximately convex functions.
\newblock In {\em COLT}, pages 240--265, 2015.

\bibitem{BousquettE02}
O.~Bousquet and A.~Elisseeff.
\newblock Stability and generalization.
\newblock {\em JMLR}, 2:499--526, 2002.

\bibitem{Bubeck15}
S.~Bubeck.
\newblock Convex optimization: Algorithms and complexity.
\newblock {\em Foundations and Trends in Machine Learning}, 8(3-4):231--357,
  2015.

\bibitem{Cesa-BianchiCG04}
N.~Cesa{-}Bianchi, A.~Conconi, and C.~Gentile.
\newblock On the generalization ability of on-line learning algorithms.
\newblock {\em {IEEE} Transactions on Information Theory}, 50(9):2050--2057,
  2004.

\bibitem{DworkFHPRR14:arxiv}
C.~Dwork, V.~Feldman, M.~Hardt, T.~Pitassi, O.~Reingold, and A.~Roth.
\newblock Preserving statistical validity in adaptive data analysis.
\newblock {\em CoRR}, abs/1411.2664, 2014.
\newblock Extended abstract in STOC 2015.

\bibitem{DworkFHPRR15:arxiv}
C.~Dwork, V.~Feldman, M.~Hardt, T.~Pitassi, O.~Reingold, and A.~Roth.
\newblock Generalization in adaptive data analysis and holdout reuse.
\newblock {\em CoRR}, abs/1506, 2015.
\newblock Extended abstract in NIPS 2015.

\bibitem{Feldman:16erm}
V.~Feldman.
\newblock Generalization of {ERM} in stochastic convex optimization: The
  dimension strikes back.
\newblock {\em CoRR}, abs/1608.04414, 2016.
\newblock Extended abstract in NIPS 2016.

\bibitem{FeldmanGV:15}
V.~Feldman, C.~Guzman, and S.~Vempala.
\newblock Statistical query algorithms for mean vector estimation and
  stochastic convex optimization.
\newblock {\em CoRR}, abs/1512.09170, 2015.
\newblock Extended abstract in SODA 2017.

\bibitem{HardtRS16}
M.~Hardt, B.~Recht, and Y.~Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In {\em {ICML}}, pages 1225--1234, 2016.

\bibitem{Justesen:72}
J.~Justesen.
\newblock Class of constructive asymptotically good algebraic codes.
\newblock {\em IEEE Trans. Inf. Theor.}, 18(5):652 -- 656, 1972.

\bibitem{KakadeST:08}
S.~Kakade, K.~Sridharan, and A.~Tewari.
\newblock On the complexity of linear prediction: Risk bounds, margin bounds,
  and regularization.
\newblock In {\em NIPS}, pages 793--800, 2008.

\bibitem{Nemirovski:2009}
A.~{Nemirovski}, A.~{Juditsky}, G.~{Lan}, and A.~{Shapiro}.
\newblock {Robust stochastic approximation approach to stochastic programming.}
\newblock {\em {SIAM J. Optim.}}, 19(4):1574--1609, 2009.

\bibitem{RakhlinSS12}
A.~Rakhlin, O.~Shamir, and K.~Sridharan.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock In {\em ICML}, 2012.

\bibitem{RakhlinS15}
A.~Rakhlin and K.~Sridharan.
\newblock Sequential probability assignment with binary alphabets and large
  classes of experts.
\newblock {\em CoRR}, abs/1501.07340, 2015.

\bibitem{Shalev-ShwartzBen-David:2014}
S.~Shalev-Shwartz and S.~Ben-David.
\newblock {\em Understanding Machine Learning: From Theory to Algorithms}.
\newblock Cambridge University Press, 2014.

\bibitem{SSSSS:2009}
S.~Shalev{-}Shwartz, O.~Shamir, N.~Srebro, and K.~Sridharan.
\newblock Stochastic convex optimization.
\newblock In {\em {COLT}}, 2009.

\bibitem{ShwartzSSS10}
S.~Shalev-Shwartz, O.~Shamir, N.~Srebro, and K.~Sridharan.
\newblock Learnability, stability and uniform convergence.
\newblock {\em The Journal of Machine Learning Research}, 11:2635--2670, 2010.

\bibitem{ShamirZ13}
O.~Shamir and T.~Zhang.
\newblock Stochastic gradient descent for non-smooth optimization: Convergence
  results and optimal averaging schemes.
\newblock In {\em ICML}, pages 71--79, 2013.

\bibitem{ShapiroNemirovsky:05}
A.~Shapiro and A.~Nemirovski.
\newblock On complexity of stochastic programming problems.
\newblock In V.~Jeyakumar and A.~M. Rubinov, editors, {\em Continuous
  Optimization: Current Trends and Applications 144}. Springer, 2005.

\bibitem{Spielman:96}
D.~Spielman.
\newblock Linear-time encodable and decodable error-correcting codes.
\newblock {\em IEEE Transactions on Information Theory}, 42(6):1723--1731,
  1996.

\bibitem{Vapnik:98}
V.~Vapnik.
\newblock {\em Statistical Learning Theory}.
\newblock Wiley-Interscience, New York, 1998.

\end{thebibliography}
