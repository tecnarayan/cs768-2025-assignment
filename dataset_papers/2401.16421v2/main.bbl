\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ainslie et~al.(2023)Ainslie, Lei, de~Jong, Ontanon, Brahma, Zemlyanskiy, Uthus, Guo, Lee-Thorp, Tay, Sung, and Sanghai]{ainslie2023colt}
Ainslie, J., Lei, T., de~Jong, M., Ontanon, S., Brahma, S., Zemlyanskiy, Y., Uthus, D., Guo, M., Lee-Thorp, J., Tay, Y., Sung, Y.-H., and Sanghai, S.
\newblock Co{LT}5: Faster long-range transformers with conditional computation.
\newblock In \emph{The 2023 Conference on Empirical Methods in Natural Language Processing}, 2023.

\bibitem[Alur et~al.(1999)Alur, Kannan, and Yannakakis]{alur1999communicating}
Alur, R., Kannan, S., and Yannakakis, M.
\newblock Communicating hierarchical state machines.
\newblock In \emph{Automata, Languages and Programming: 26th International Colloquium, ICALPâ€™99 Prague, Czech Republic, July 11--15, 1999 Proceedings 26}, pp.\  169--178. Springer, 1999.

\bibitem[Anil et~al.(2022)Anil, Wu, Andreassen, Lewkowycz, Misra, Ramasesh, Slone, Gur-Ari, Dyer, and Neyshabur]{anil2022exploring}
Anil, C., Wu, Y., Andreassen, A., Lewkowycz, A., Misra, V., Ramasesh, V., Slone, A., Gur-Ari, G., Dyer, E., and Neyshabur, B.
\newblock Exploring length generalization in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 38546--38556, 2022.

\bibitem[Bai et~al.(2021)Bai, Shi, Lin, Xie, Tan, Xiong, Gao, and Li]{bai2021segatron}
Bai, H., Shi, P., Lin, J., Xie, Y., Tan, L., Xiong, K., Gao, W., and Li, M.
\newblock Segatron: Segment-aware transformer for language modeling and understanding.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, pp.\  12526--12534, 2021.

\bibitem[Blei et~al.(2003)Blei, Ng, and Jordan]{blei2003latent}
Blei, D.~M., Ng, A.~Y., and Jordan, M.~I.
\newblock Latent dirichlet allocation.
\newblock \emph{Journal of machine Learning research}, 3\penalty0 (Jan):\penalty0 993--1022, 2003.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Li, Meng, Liang, and Bing]{chen2023clex}
Chen, G., Li, X., Meng, Z., Liang, S., and Bing, L.
\newblock Clex: Continuous length extrapolation for large language models.
\newblock \emph{arXiv preprint arXiv:2310.16450}, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2022)Chen, Chu, Wiseman, and Gimpel]{chen-etal-2022-summscreen}
Chen, M., Chu, Z., Wiseman, S., and Gimpel, K.
\newblock {S}umm{S}creen: A dataset for abstractive screenplay summarization.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  8602--8615, May 2022.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Wong, Chen, and Tian]{chen2023extending}
Chen, S., Wong, S., Chen, L., and Tian, Y.
\newblock Extending context window of large language models via positional interpolation.
\newblock \emph{arXiv preprint arXiv:2306.15595}, 2023{\natexlab{b}}.

\bibitem[Chi et~al.(2022)Chi, Fan, Ramadge, and Rudnicky]{chi2022kerple}
Chi, T.-C., Fan, T.-H., Ramadge, P.~J., and Rudnicky, A.
\newblock Kerple: Kernelized relative positional embedding for length extrapolation.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 8386--8399, 2022.

\bibitem[Chi et~al.(2023)Chi, Fan, Rudnicky, and Ramadge]{chi2023dissecting}
Chi, T.-C., Fan, T.-H., Rudnicky, A., and Ramadge, P.
\newblock Dissecting transformer length extrapolation via the lens of receptive field analysis.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  13522--13537, 2023.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{chowdhery2022palm}
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.~W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A.~M., Pillai, T.~S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N.
\newblock Palm: Scaling language modeling with pathways, 2022.

\bibitem[Chowdhury \& Caragea(2023)Chowdhury and Caragea]{chowdhury2023monotonic}
Chowdhury, J.~R. and Caragea, C.
\newblock Monotonic location attention for length generalization.
\newblock \emph{arXiv preprint arXiv:2305.20019}, 2023.

\bibitem[Chung et~al.(2016)Chung, Ahn, and Bengio]{chung2016hierarchical}
Chung, J., Ahn, S., and Bengio, Y.
\newblock Hierarchical multiscale recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1609.01704}, 2016.

\bibitem[Dasigi et~al.(2021)Dasigi, Lo, Beltagy, Cohan, Smith, and Gardner]{dasigi2021qasper}
Dasigi, P., Lo, K., Beltagy, I., Cohan, A., Smith, N.~A., and Gardner, M.
\newblock A dataset of information-seeking questions and answers anchored in research papers.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, June 2021.

\bibitem[Devlin et~al.(2019{\natexlab{a}})Devlin, Chang, Lee, and Toutanova]{bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, June 2019{\natexlab{a}}.

\bibitem[Devlin et~al.(2019{\natexlab{b}})Devlin, Chang, Lee, and Toutanova]{devlin-etal-2019-bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pp.\  4171--4186, June 2019{\natexlab{b}}.

\bibitem[Eilenberg(1974)]{eilenberg1974automata}
Eilenberg, S.
\newblock \emph{Automata, languages, and machines}.
\newblock Academic press, 1974.

\bibitem[Feng et~al.(2023)Feng, Zhang, Gu, Ye, He, and Wang]{feng2023towards}
Feng, G., Zhang, B., Gu, Y., Ye, H., He, D., and Wang, L.
\newblock Towards revealing the mystery behind chain of thought: A theoretical perspective.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Fine et~al.(1998)Fine, Singer, and Tishby]{fine1998hierarchical}
Fine, S., Singer, Y., and Tishby, N.
\newblock The hierarchical hidden markov model: Analysis and applications.
\newblock \emph{Machine learning}, 32:\penalty0 41--62, 1998.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, et~al.]{gao2020pile}
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et~al.
\newblock The {P}ile: An 800{GB} dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[Gao et~al.(2023)Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster, Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds, Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and Zou]{eval-harness}
Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le~Noac'h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A.
\newblock A framework for few-shot language model evaluation, 12 2023.
\newblock URL \url{https://zenodo.org/records/10256836}.

\bibitem[Griffiths et~al.(2003)Griffiths, Jordan, Tenenbaum, and Blei]{griffiths2003hierarchical}
Griffiths, T., Jordan, M., Tenenbaum, J., and Blei, D.
\newblock Hierarchical topic models and the nested chinese restaurant process.
\newblock \emph{Advances in neural information processing systems}, 16, 2003.

\bibitem[Halliday \& Matthiessen(2013)Halliday and Matthiessen]{halliday2013halliday}
Halliday, M. A.~K. and Matthiessen, C.~M.
\newblock \emph{Halliday's introduction to functional grammar}.
\newblock Routledge, 2013.

\bibitem[Han et~al.(2023)Han, Wang, Xiong, Chen, Ji, and Wang]{han2023lminfinite}
Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang, S.
\newblock Lm-infinite: Simple on-the-fly length generalization for large language models, 2023.

\bibitem[Haviv et~al.(2022)Haviv, Ram, Press, Izsak, and Levy]{haviv-etal-2022-transformer}
Haviv, A., Ram, O., Press, O., Izsak, P., and Levy, O.
\newblock Transformer language models without positional encodings still learn positional information.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2022}, pp.\  1382--1390, December 2022.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{mmlu}
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J.
\newblock Measuring massive multitask language understanding.
\newblock \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2021.

\bibitem[Huang et~al.(2021)Huang, Cao, Parulian, Ji, and Wang]{huang2021govreport}
Huang, L., Cao, S., Parulian, N., Ji, H., and Wang, L.
\newblock Efficient attentions for long document summarization.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  1419--1436, June 2021.

\bibitem[Jin et~al.(2024)Jin, Han, Yang, Jiang, Liu, Chang, Chen, and Hu]{jin2024llm}
Jin, H., Han, X., Yang, J., Jiang, Z., Liu, Z., Chang, C.-Y., Chen, H., and Hu, X.
\newblock Llm maybe longlm: Self-extend llm context window without tuning.
\newblock \emph{arXiv preprint arXiv:2401.01325}, 2024.

\bibitem[Kazemnejad et~al.(2023)Kazemnejad, Padhi, Ramamurthy, Das, and Reddy]{kazemnejad2023impact}
Kazemnejad, A., Padhi, I., Ramamurthy, K.~N., Das, P., and Reddy, S.
\newblock The impact of positional encoding on length generalization in transformers.
\newblock \emph{arXiv preprint arXiv:2305.19466}, 2023.

\bibitem[Ko{\v{c}}isk{\'y} et~al.(2018)Ko{\v{c}}isk{\'y}, Schwarz, Blunsom, Dyer, Hermann, Melis, and Grefenstette]{kocisky2018narrativeqa}
Ko{\v{c}}isk{\'y}, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K.~M., Melis, G., and Grefenstette, E.
\newblock The {N}arrative{QA} reading comprehension challenge.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 6, 2018.

\bibitem[Koreeda \& Manning(2021)Koreeda and Manning]{koreeda-manning-2021-contractnli-dataset}
Koreeda, Y. and Manning, C.
\newblock {C}ontract{NLI}: A dataset for document-level natural language inference for contracts.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2021}, pp.\  1907--1919, November 2021.

\bibitem[Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy]{lai-etal-2017-race}
Lai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E.
\newblock {RACE}: Large-scale {R}e{A}ding comprehension dataset from examinations.
\newblock In \emph{Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, pp.\  785--794, September 2017.

\bibitem[Li et~al.(2023)Li, You, Guruganesh, Ainslie, Ontanon, Zaheer, Sanghai, Yang, Kumar, and Bhojanapalli]{li2023functional}
Li, S., You, C., Guruganesh, G., Ainslie, J., Ontanon, S., Zaheer, M., Sanghai, S., Yang, Y., Kumar, S., and Bhojanapalli, S.
\newblock Functional interpolation for relative positions improves long context transformers.
\newblock \emph{arXiv preprint arXiv:2310.04418}, 2023.

\bibitem[Lightman et~al.(2023)Lightman, Kosaraju, Burda, Edwards, Baker, Lee, Leike, Schulman, Sutskever, and Cobbe]{lightman2023lets}
Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K.
\newblock Let's verify step by step.
\newblock \emph{arXiv preprint arXiv:2305.20050}, 2023.

\bibitem[Lin et~al.(2022)Lin, Hilton, and Evans]{lin-etal-2022-truthfulqa}
Lin, S., Hilton, J., and Evans, O.
\newblock {T}ruthful{QA}: Measuring how models mimic human falsehoods.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  3214--3252, May 2022.

\bibitem[Liu et~al.(2022)Liu, Ash, Goel, Krishnamurthy, and Zhang]{liu2022transformers}
Liu, B., Ash, J.~T., Goel, S., Krishnamurthy, A., and Zhang, C.
\newblock Transformers learn shortcuts to automata.
\newblock \emph{arXiv preprint arXiv:2210.10749}, 2022.

\bibitem[Liu et~al.(2023)Liu, Yan, Zhang, An, Qiu, and Lin]{liu2023scaling}
Liu, X., Yan, H., Zhang, S., An, C., Qiu, X., and Lin, D.
\newblock Scaling laws of rope-based extrapolation.
\newblock \emph{arXiv preprint arXiv:2310.05209}, 2023.

\bibitem[Liu(2019)]{liu2019fine}
Liu, Y.
\newblock Fine-tune bert for extractive summarization.
\newblock \emph{arXiv preprint arXiv:1903.10318}, 2019.

\bibitem[Pang et~al.(2022)Pang, Parrish, Joshi, Nangia, Phang, Chen, Padmakumar, Ma, Thompson, He, and Bowman]{pang-etal-2022-quality}
Pang, R.~Y., Parrish, A., Joshi, N., Nangia, N., Phang, J., Chen, A., Padmakumar, V., Ma, J., Thompson, J., He, H., and Bowman, S.
\newblock {Q}u{ALITY}: Question answering with long input texts, yes!
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  5336--5358, July 2022.

\bibitem[Peng et~al.(2023)Peng, Quesnelle, Fan, and Shippole]{peng2023yarn}
Peng, B., Quesnelle, J., Fan, H., and Shippole, E.
\newblock Yarn: Efficient context window extension of large language models, 2023.

\bibitem[Press et~al.(2022)Press, Smith, and Lewis]{alibi}
Press, O., Smith, N., and Lewis, M.
\newblock Train short, test long: Attention with linear biases enables input length extrapolation.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2022.

\bibitem[Rae et~al.(2020{\natexlab{a}})Rae, Potapenko, Jayakumar, Hillier, and Lillicrap]{Rae2020Compressive}
Rae, J.~W., Potapenko, A., Jayakumar, S.~M., Hillier, C., and Lillicrap, T.~P.
\newblock Compressive transformers for long-range sequence modelling.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2020{\natexlab{a}}.

\bibitem[Rae et~al.(2020{\natexlab{b}})Rae, Potapenko, Jayakumar, Hillier, and Lillicrap]{pg19}
Rae, J.~W., Potapenko, A., Jayakumar, S.~M., Hillier, C., and Lillicrap, T.~P.
\newblock Compressive transformers for long-range sequence modelling.
\newblock In \emph{International Conference on Learning Representations}, 2020{\natexlab{b}}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0 (1):\penalty0 5485--5551, 2020.

\bibitem[Ratner et~al.(2023)Ratner, Levine, Belinkov, Ram, Magar, Abend, Karpas, Shashua, Leyton-Brown, and Shoham]{ratner2023parallel}
Ratner, N., Levine, Y., Belinkov, Y., Ram, O., Magar, I., Abend, O., Karpas, E., Shashua, A., Leyton-Brown, K., and Shoham, Y.
\newblock Parallel context windows for large language models.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  6383--6402, 2023.

\bibitem[Roziere et~al.(2023)Roziere, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Remez, Rapin, et~al.]{roziere2023code}
Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X.~E., Adi, Y., Liu, J., Remez, T., Rapin, J., et~al.
\newblock Code llama: Open foundation models for code.
\newblock \emph{arXiv preprint arXiv:2308.12950}, 2023.

\bibitem[Ruoss et~al.(2023{\natexlab{a}})Ruoss, Del{\'e}tang, Genewein, Grau-Moya, Csord{\'a}s, Bennani, Legg, and Veness]{randompos}
Ruoss, A., Del{\'e}tang, G., Genewein, T., Grau-Moya, J., Csord{\'a}s, R., Bennani, M., Legg, S., and Veness, J.
\newblock Randomized positional encodings boost length generalization of transformers.
\newblock In \emph{Association for Computational Linguistics (ACL)}, July 2023{\natexlab{a}}.

\bibitem[Ruoss et~al.(2023{\natexlab{b}})Ruoss, Del{\'e}tang, Genewein, Grau-Moya, Csord{\'a}s, Bennani, Legg, and Veness]{ruoss-etal-2023-randomized}
Ruoss, A., Del{\'e}tang, G., Genewein, T., Grau-Moya, J., Csord{\'a}s, R., Bennani, M., Legg, S., and Veness, J.
\newblock Randomized positional encodings boost length generalization of transformers.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pp.\  1889--1903, July 2023{\natexlab{b}}.

\bibitem[Sakaguchi et~al.(2020)Sakaguchi, Le~Bras, Bhagavatula, and Choi]{sakaguchi2020winogrande}
Sakaguchi, K., Le~Bras, R., Bhagavatula, C., and Choi, Y.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, pp.\  8732--8740, 2020.

\bibitem[Shaham et~al.(2022)Shaham, Segal, Ivgi, Efrat, Yoran, Haviv, Gupta, Xiong, Geva, Berant, et~al.]{shaham2022scrolls}
Shaham, U., Segal, E., Ivgi, M., Efrat, A., Yoran, O., Haviv, A., Gupta, A., Xiong, W., Geva, M., Berant, J., et~al.
\newblock Scrolls: Standardized comparison over long language sequences.
\newblock \emph{arXiv preprint arXiv:2201.03533}, 2022.

\bibitem[Shaw et~al.(2018)Shaw, Uszkoreit, and Vaswani]{shaw2018self}
Shaw, P., Uszkoreit, J., and Vaswani, A.
\newblock Self-attention with relative position representations.
\newblock \emph{arXiv preprint arXiv:1803.02155}, 2018.

\bibitem[Su et~al.(2021)Su, Lu, Pan, Murtadha, Wen, and Liu]{rope}
Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y.
\newblock Roformer: Enhanced transformer with rotary position embedding, 2021.

\bibitem[Sun et~al.(2023)Sun, Dong, Patra, Ma, Huang, Benhaim, Chaudhary, Song, and Wei]{sun2022length}
Sun, Y., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim, A., Chaudhary, V., Song, X., and Wei, F.
\newblock A length-extrapolatable transformer.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}. Association for Computational Linguistics, July 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C.~C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P.~S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E.~M., Subramanian, R., Tan, X.~E., Tang, B., Taylor, R., Williams, A., Kuan, J.~X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T.
\newblock Llama 2: Open foundation and fine-tuned chat models, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 30, 2017.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, brian ichter, Xia, Chi, Le, and Zhou]{wei2022chain}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E.~H., Le, Q.~V., and Zhou, D.
\newblock Chain of thought prompting elicits reasoning in large language models.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Xiao et~al.(2023)Xiao, Tian, Chen, Han, and Lewis]{xiao2023efficient}
Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M.
\newblock Efficient streaming language models with attention sinks, 2023.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag}
Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, 2019.

\bibitem[Zhong et~al.(2021)Zhong, Yin, Yu, Zaidi, Mutuma, Jha, Awadallah, Celikyilmaz, Liu, Qiu, and Radev]{zhong2021qmsum}
Zhong, M., Yin, D., Yu, T., Zaidi, A., Mutuma, M., Jha, R., Awadallah, A.~H., Celikyilmaz, A., Liu, Y., Qiu, X., and Radev, D.
\newblock {QMS}um: A new benchmark for query-based multi-domain meeting summarization.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  5905--5921, June 2021.

\bibitem[Zhu et~al.(2023)Zhu, Yang, Wang, Song, Wu, Wei, and Li]{zhu2023pose}
Zhu, D., Yang, N., Wang, L., Song, Y., Wu, W., Wei, F., and Li, S.
\newblock Pose: Efficient context window extension of llms via positional skip-wise training.
\newblock \emph{arXiv preprint arXiv:2309.10400}, 2023.

\end{thebibliography}
