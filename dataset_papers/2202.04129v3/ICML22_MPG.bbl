\begin{thebibliography}{93}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2019)Abbasi-Yadkori, Bartlett, Bhatia, Lazic,
  Szepesvari, and Weisz]{abbasi2019politex}
Abbasi-Yadkori, Y., Bartlett, P., Bhatia, K., Lazic, N., Szepesvari, C., and
  Weisz, G.
\newblock Politex: {R}egret bounds for policy iteration using expert
  prediction.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3692--3702, 2019.

\bibitem[Agarwal et~al.(2021)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2021theory}
Agarwal, A., Kakade, S.~M., Lee, J.~D., and Mahajan, G.
\newblock On the theory of policy gradient methods: {O}ptimality,
  approximation, and distribution shift.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (98):\penalty0 1--76, 2021.

\bibitem[Altman et~al.(1997)Altman, Hordijk, and
  Spieksma]{altman1997contraction}
Altman, E., Hordijk, A., and Spieksma, F.
\newblock Contraction conditions for average and $\alpha$-discount optimality
  in countable state {M}arkov games with unbounded rewards.
\newblock \emph{Mathematics of Operations Research}, 22\penalty0 (3):\penalty0
  588--618, 1997.

\bibitem[Arora(2008)]{Arora08}
Arora, S.
\newblock Lecture 6 in toward theoretical understanding of deep learning,
  October 2008.
\newblock
  \url{https://www.cs.princeton.edu/courses/archive/fall18/cos597G/lecnotes/lecture6.pdf}.

\bibitem[Audibert \& Catoni(2009)Audibert and Catoni]{audibert09}
Audibert, J.-Y. and Catoni, O.
\newblock Risk bounds for linear regression, 2009.
\newblock
  \url{http://imagine.enpc.fr/~audibert/Mes%20articles/Chevaleret09.pdf}.

\bibitem[Bailey \& Piliouras(2019)Bailey and Piliouras]{bailey2019fast}
Bailey, J. and Piliouras, G.
\newblock Fast and furious learning in zero-sum games: {V}anishing regret with
  non-vanishing step sizes.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Bailey \& Piliouras(2018)Bailey and
  Piliouras]{bailey2018multiplicative}
Bailey, J.~P. and Piliouras, G.
\newblock Multiplicative weights update in zero-sum games.
\newblock In \emph{Proceedings of the 2018 ACM Conference on Economics and
  Computation}, pp.\  321--338, 2018.

\bibitem[Bhandari \& Russo(2019)Bhandari and Russo]{bhandari2019global}
Bhandari, J. and Russo, D.
\newblock Global optimality guarantees for policy gradient methods.
\newblock \emph{arXiv preprint arXiv:1906.01786}, 2019.

\bibitem[Bistritz \& Bambos(2020)Bistritz and Bambos]{bistritz2020cooperative}
Bistritz, I. and Bambos, N.
\newblock Cooperative multi-player bandit optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Bu et~al.(2019)Bu, Ratliff, and Mesbahi]{bu2019global}
Bu, J., Ratliff, L.~J., and Mesbahi, M.
\newblock Global convergence of policy gradient for sequential zero-sum linear
  quadratic dynamic games.
\newblock \emph{arXiv preprint arXiv:1911.04672}, 2019.

\bibitem[Busoniu et~al.(2008)Busoniu, Babuska, and
  De~Schutter]{busoniu2008comprehensive}
Busoniu, L., Babuska, R., and De~Schutter, B.
\newblock A comprehensive survey of multiagent reinforcement learning.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics, Part C
  (Applications and Reviews)}, 38\penalty0 (2):\penalty0 156--172, 2008.

\bibitem[Cai et~al.(2020)Cai, Yang, Jin, and Wang]{cai2020provably}
Cai, Q., Yang, Z., Jin, C., and Wang, Z.
\newblock Provably efficient exploration in policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1283--1294. PMLR, 2020.

\bibitem[Cen et~al.(2021)Cen, Wei, and Chi]{cen2021fast}
Cen, S., Wei, Y., and Chi, Y.
\newblock Fast policy extragradient methods for competitive games with entropy
  regularization.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Cesa-Bianchi \& Lugosi(2006)Cesa-Bianchi and
  Lugosi]{cesa2006prediction}
Cesa-Bianchi, N. and Lugosi, G.
\newblock \emph{Prediction, Learning, and Games}.
\newblock Cambridge University Press, 2006.

\bibitem[Cheung \& Piliouras(2020)Cheung and Piliouras]{cheung2020chaosvideo}
Cheung, Y.~K. and Piliouras, G.
\newblock Chaos, extremism and optimism: {V}olume analysis of learning in
  games.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 9039--9049, 2020.

\bibitem[Cohen et~al.(2017{\natexlab{a}})Cohen, H{\'e}liou, and
  Mertikopoulos]{cohen2017learning}
Cohen, J., H{\'e}liou, A., and Mertikopoulos, P.
\newblock Learning with bandit feedback in potential games.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pp.\  6372--6381, 2017{\natexlab{a}}.

\bibitem[Cohen et~al.(2017{\natexlab{b}})Cohen, Nedi{\'c}, and
  Srikant]{cohen2017projected}
Cohen, K., Nedi{\'c}, A., and Srikant, R.
\newblock On projected stochastic gradient descent algorithm with weighted
  averaging for least squares regression.
\newblock \emph{IEEE Transactions on Automatic Control}, 62\penalty0
  (11):\penalty0 5974--5981, 2017{\natexlab{b}}.

\bibitem[Dafoe et~al.(2020)Dafoe, Hughes, Bachrach, Collins, McKee, Leibo,
  Larson, and Graepel]{dafoe2020open}
Dafoe, A., Hughes, E., Bachrach, Y., Collins, T., McKee, K.~R., Leibo, J.~Z.,
  Larson, K., and Graepel, T.
\newblock Open problems in cooperative {AI}.
\newblock \emph{arXiv preprint arXiv:2012.08630}, 2020.

\bibitem[Dafoe et~al.(2021)Dafoe, Bachrach, Hadfield, Horvitz, Larson, and
  Graepel]{dafoe2021cooperative}
Dafoe, A., Bachrach, Y., Hadfield, G., Horvitz, E., Larson, K., and Graepel, T.
\newblock Cooperative {AI}: machines must learn to find common ground, 2021.

\bibitem[Daskalakis \& Panageas(2018)Daskalakis and
  Panageas]{daskalakis2018limit}
Daskalakis, C. and Panageas, I.
\newblock The limit points of (optimistic) gradient descent in min-max
  optimization.
\newblock \emph{arXiv preprint arXiv:1807.03907}, 2018.

\bibitem[Daskalakis et~al.(2020)Daskalakis, Foster, and
  Golowich]{daskalakis2020independent}
Daskalakis, C., Foster, D.~J., and Golowich, N.
\newblock Independent policy gradient methods for competitive reinforcement
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Dechert \& O’Donnell(2006)Dechert and
  O’Donnell]{dechert2006stochastic}
Dechert, W.~D. and O’Donnell, S.
\newblock The stochastic lake game: {A} numerical solution.
\newblock \emph{Journal of Economic Dynamics and Control}, 30\penalty0
  (9-10):\penalty0 1569--1587, 2006.

\bibitem[Dubey \& Pentland(2021)Dubey and Pentland]{dubey2021provably}
Dubey, A. and Pentland, A.
\newblock Provably efficient cooperative multi-agent reinforcement learning
  with function approximation.
\newblock \emph{arXiv preprint arXiv:2103.04972}, 2021.

\bibitem[Fink(1964)]{fink1964equilibrium}
Fink, A.~M.
\newblock Equilibrium in a stochastic $n$-person game.
\newblock \emph{Journal of science of the hiroshima university, series ai
  (mathematics)}, 28\penalty0 (1):\penalty0 89--93, 1964.

\bibitem[Fox et~al.(2022)Fox, Mcaleer, Overman, and
  Panageas]{fox2021independent}
Fox, R., Mcaleer, S.~M., Overman, W., and Panageas, I.
\newblock Independent natural policy gradient always converges in {M}arkov
  potential games.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  4414--4425, 2022.

\bibitem[Gonz{\'a}lez-S{\'a}nchez \&
  Hern{\'a}ndez-Lerma(2013)Gonz{\'a}lez-S{\'a}nchez and
  Hern{\'a}ndez-Lerma]{gonzalez2013discrete}
Gonz{\'a}lez-S{\'a}nchez, D. and Hern{\'a}ndez-Lerma, O.
\newblock \emph{Discrete--time stochastic control and dynamic potential games:
  the {E}uler--Equation approach}.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Hambly et~al.(2021)Hambly, Xu, and Yang]{hambly2021policy}
Hambly, B.~M., Xu, R., and Yang, H.
\newblock Policy gradient methods find the {N}ash equilibrium in {N}-player
  general-sum linear-quadratic games.
\newblock \emph{arXiv preprint arXiv:2107.13090}, 2021.

\bibitem[Hofbauer \& Sandholm(2002)Hofbauer and Sandholm]{hofbauer2002global}
Hofbauer, J. and Sandholm, W.~H.
\newblock On the global convergence of stochastic fictitious play.
\newblock \emph{Econometrica}, 70\penalty0 (6):\penalty0 2265--2294, 2002.

\bibitem[Hsu et~al.(2012)Hsu, Kakade, and Zhang]{hsu2012random}
Hsu, D., Kakade, S.~M., and Zhang, T.
\newblock Random design analysis of ridge regression.
\newblock In \emph{Conference on learning theory}, pp.\  9--1, 2012.

\bibitem[Huang et~al.(2022)Huang, Lee, Wang, and Yang]{huang2022towards}
Huang, B., Lee, J.~D., Wang, Z., and Yang, Z.
\newblock Towards general function approximation in zero-sum {M}arkov games.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Jin et~al.(2021{\natexlab{a}})Jin, Liu, Wang, and Yu]{jin2021v}
Jin, C., Liu, Q., Wang, Y., and Yu, T.
\newblock {V}-learning -- {A} simple, efficient, decentralized algorithm for
  multiagent {RL}.
\newblock \emph{arXiv preprint arXiv:2110.14555}, 2021{\natexlab{a}}.

\bibitem[Jin et~al.(2021{\natexlab{b}})Jin, Liu, and Yu]{jin2021power}
Jin, C., Liu, Q., and Yu, T.
\newblock The power of exploiter: {P}rovable multi-agent {RL} in large state
  spaces.
\newblock \emph{arXiv preprint arXiv:2106.03352}, 2021{\natexlab{b}}.

\bibitem[Kakade(2001)]{kakade2001natural}
Kakade, S.~M.
\newblock A natural policy gradient.
\newblock \emph{Advances in Neural Information Processing Systems}, 14, 2001.

\bibitem[Kao et~al.(2022)Kao, Wei, and Subramanian]{kao2021decentralized}
Kao, H., Wei, C.-Y., and Subramanian, V.
\newblock Decentralized cooperative reinforcement learning with hierarchical
  information structure.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pp.\  573--605, 2022.

\bibitem[Kleinberg et~al.(2009)Kleinberg, Piliouras, and
  Tardos]{kleinberg2009multiplicative}
Kleinberg, R., Piliouras, G., and Tardos, {\'E}.
\newblock Multiplicative updates outperform generic no-regret learning in
  congestion games.
\newblock In \emph{Proceedings of the forty-first annual ACM symposium on
  Theory of computing}, pp.\  533--542, 2009.

\bibitem[Lan(2022)]{lan2021policy}
Lan, G.
\newblock Policy mirror descent for reinforcement learning: {L}inear
  convergence, new sampling complexity, and generalized problem classes.
\newblock \emph{Mathematical Programming}, 2022.

\bibitem[Leonardos \& Piliouras(2022)Leonardos and
  Piliouras]{leonardos2022exploration}
Leonardos, S. and Piliouras, G.
\newblock Exploration-exploitation in multi-agent learning: Catastrophe theory
  meets game theory.
\newblock \emph{Artificial Intelligence}, 304:\penalty0 103653, 2022.

\bibitem[Leonardos et~al.(2021)Leonardos, Piliouras, and
  Spendlove]{leonardos2021exploration}
Leonardos, S., Piliouras, G., and Spendlove, K.
\newblock Exploration-exploitation in multi-agent competition: Convergence with
  bounded rationality.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Leonardos et~al.(2022)Leonardos, Overman, Panageas, and
  Piliouras]{leonardos2021global}
Leonardos, S., Overman, W., Panageas, I., and Piliouras, G.
\newblock Global convergence of multi-agent policy gradient in {M}arkov
  potential games.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{levine2016end}
Levine, S., Finn, C., Darrell, T., and Abbeel, P.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1334--1373, 2016.

\bibitem[Liu et~al.(2019)Liu, Cai, Yang, and Wang]{liu2019neural}
Liu, B., Cai, Q., Yang, Z., and Wang, Z.
\newblock Neural trust region/proximal policy optimization attains globally
  optimal policy.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Liu et~al.(2021)Liu, Yu, Bai, and Jin]{liu2021sharp}
Liu, Q., Yu, T., Bai, Y., and Jin, C.
\newblock A sharp analysis of model-based reinforcement learning with
  self-play.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7001--7010, 2021.

\bibitem[Lowe et~al.(2017)Lowe, WU, Tamar, Harb, Pieter~Abbeel, and
  Mordatch]{lowe2017multi}
Lowe, R., WU, Y., Tamar, A., Harb, J., Pieter~Abbeel, O., and Mordatch, I.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments.
\newblock \emph{Advances in Neural Information Processing Systems},
  30:\penalty0 6379--6390, 2017.

\bibitem[Luo et~al.(2021)Luo, Wei, and Lee]{luo2021policy}
Luo, H., Wei, C.-Y., and Lee, C.-W.
\newblock Policy optimization in adversarial {MDP}s: {I}mproved exploration via
  dilated bonuses.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Macua et~al.(2018)Macua, Zazo, and Zazo]{macua2018learning}
Macua, S.~V., Zazo, J., and Zazo, S.
\newblock Learning parametric closed-loop policies for {M}arkov potential
  games.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Maitra \& Parthasarathy(1970)Maitra and
  Parthasarathy]{maitra1970stochastic}
Maitra, A. and Parthasarathy, T.
\newblock On stochastic games.
\newblock \emph{Journal of Optimization Theory and Applications}, 5\penalty0
  (4):\penalty0 289--300, 1970.

\bibitem[Maitra \& Parthasarathy(1971)Maitra and
  Parthasarathy]{maitra1971stochastic}
Maitra, A. and Parthasarathy, T.
\newblock On stochastic games, {II}.
\newblock \emph{Journal of Optimization Theory and Applications}, 8\penalty0
  (2):\penalty0 154--160, 1971.

\bibitem[Mao et~al.(2022)Mao, Basar, Yang, and Zhang]{mao2022on}
Mao, W., Basar, T., Yang, L.~F., and Zhang, K.
\newblock On improving model-free algorithms for decentralized multi-agent
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Marden(2012)]{marden2012state}
Marden, J.~R.
\newblock State based potential games.
\newblock \emph{Automatica}, 48\penalty0 (12):\penalty0 3075--3088, 2012.

\bibitem[Matignon et~al.(2012)Matignon, Laurent, and
  Le~Fort-Piat]{matignon2012independent}
Matignon, L., Laurent, G.~J., and Le~Fort-Piat, N.
\newblock Independent reinforcement learners in cooperative {M}arkov games: {A}
  survey regarding coordination problems.
\newblock \emph{The Knowledge Engineering Review}, 27\penalty0 (1):\penalty0
  1--31, 2012.

\bibitem[Mazalov et~al.(2017)Mazalov, Rettieva, and
  Avrachenkov]{mazalov2017linear}
Mazalov, V.~V., Rettieva, A.~N., and Avrachenkov, K.~E.
\newblock Linear-quadratic discrete-time dynamic potential games.
\newblock \emph{Automation and Remote Control}, 78\penalty0 (8):\penalty0
  1537--1544, 2017.

\bibitem[Mazumdar et~al.(2020)Mazumdar, Ratliff, Jordan, and
  Sastry]{mazumdar2019policy}
Mazumdar, E., Ratliff, L.~J., Jordan, M.~I., and Sastry, S.~S.
\newblock Policy-gradient algorithms have no guarantees of convergence in
  linear quadratic games.
\newblock In \emph{AAMAS}, 2020.

\bibitem[Mguni et~al.(2018)Mguni, Jennings, and
  de~Cote]{mguni2018decentralised}
Mguni, D., Jennings, J., and de~Cote, E.~M.
\newblock Decentralised learning in systems with many, many strategic agents.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Mguni et~al.(2021)Mguni, Wu, Du, Yang, Wang, Li, Wen, Jennings, and
  Wang]{mguni2021learning}
Mguni, D.~H., Wu, Y., Du, Y., Yang, Y., Wang, Z., Li, M., Wen, Y., Jennings,
  J., and Wang, J.
\newblock Learning in nonzero-sum stochastic games with potentials.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7688--7699, 2021.

\bibitem[Mokhtari et~al.(2020)Mokhtari, Ozdaglar, and
  Pattathil]{mokhtari2020unified}
Mokhtari, A., Ozdaglar, A., and Pattathil, S.
\newblock A unified analysis of extra-gradient and optimistic gradient methods
  for saddle point problems: {P}roximal point approach.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1497--1507. PMLR, 2020.

\bibitem[Monderer \& Shapley(1996)Monderer and Shapley]{ref:Monderer96b}
Monderer, D. and Shapley, L.
\newblock Fictitious play property for games with identical interests.
\newblock \emph{Journal of Economic Theory}, 68:\penalty0 258--265, 1996.

\bibitem[Ozdaglar et~al.(2021)Ozdaglar, Sayin, and
  Zhang]{ozdaglar2021independent}
Ozdaglar, A., Sayin, M.~O., and Zhang, K.
\newblock Independent learning in stochastic games.
\newblock \emph{arXiv preprint arXiv:2111.11743}, 2021.

\bibitem[Palaiopanos et~al.(2017)Palaiopanos, Panageas, and
  Piliouras]{palaiopanos2017multiplicative}
Palaiopanos, G., Panageas, I., and Piliouras, G.
\newblock Multiplicative weights update with constant step-size in congestion
  games: {C}onvergence, limit cycles and chaos.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Peng et~al.(2021)Peng, Rashid, Schroeder~de Witt, Kamienny, Torr,
  B{\"o}hmer, and Whiteson]{peng2021facmac}
Peng, B., Rashid, T., Schroeder~de Witt, C., Kamienny, P.-A., Torr, P.,
  B{\"o}hmer, W., and Whiteson, S.
\newblock {FACMAC}: {F}actored multi-agent centralised policy gradients.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 12208--12221, 2021.

\bibitem[Robinson(1951)]{robinson1951iterative}
Robinson, J.
\newblock An iterative method of solving a game.
\newblock \emph{Annals of Mathematics}, pp.\  296--301, 1951.

\bibitem[Sayin et~al.(2021)Sayin, Zhang, Leslie, Basar, and
  Ozdaglar]{sayin2021decentralized}
Sayin, M.~O., Zhang, K., Leslie, D.~S., Basar, T., and Ozdaglar, A.~E.
\newblock Decentralized {Q}-learning in zero-sum {M}arkov games.
\newblock In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W.
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Shalev-Shwartz \& Ben-David(2014)Shalev-Shwartz and
  Ben-David]{shalev2014understanding}
Shalev-Shwartz, S. and Ben-David, S.
\newblock \emph{Understanding machine learning: {F}rom theory to algorithms}.
\newblock Cambridge University Press, 2014.

\bibitem[Shalev-Shwartz et~al.(2016)Shalev-Shwartz, Shammah, and
  Shashua]{shalev2016safe}
Shalev-Shwartz, S., Shammah, S., and Shashua, A.
\newblock Safe, multi-agent, reinforcement learning for autonomous driving.
\newblock \emph{arXiv preprint arXiv:1610.03295}, 2016.

\bibitem[Shapley(1953)]{shapley1953stochastic}
Shapley, L.~S.
\newblock Stochastic games.
\newblock \emph{Proceedings of the national academy of sciences}, 39\penalty0
  (10):\penalty0 1095--1100, 1953.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al.
\newblock Mastering the game of {G}o with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Silver et~al.(2018)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2018general}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
  Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al.
\newblock A general reinforcement learning algorithm that masters chess, shogi,
  and {G}o through self-play.
\newblock \emph{Science}, 362\penalty0 (6419):\penalty0 1140--1144, 2018.

\bibitem[Song et~al.(2022)Song, Mei, and Bai]{song2021can}
Song, Z., Mei, S., and Bai, Y.
\newblock When can we learn general-sum {M}arkov games with a large number of
  players sample-efficiently?
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Stastny et~al.(2021)Stastny, Rich{\'e}, Lyzhov, Treutlein, Dafoe, and
  Clifton]{stastny2021normative}
Stastny, J., Rich{\'e}, M., Lyzhov, A., Treutlein, J., Dafoe, A., and Clifton,
  J.
\newblock Normative disagreement as a challenge for cooperative {AI}.
\newblock \emph{arXiv preprint arXiv:2111.13872}, 2021.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{sutton2000policy}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1057--1063, 2000.

\bibitem[Takahashi(1962)]{takahashi1962stochastic}
Takahashi, M.
\newblock Stochastic games with infinitely many strategies.
\newblock \emph{Journal of Science of the Hiroshima University, Series AI
  (Mathematics)}, 26\penalty0 (2):\penalty0 123--134, 1962.

\bibitem[Trott et~al.(2021)Trott, Srinivasa, van~der Wal, Haneuse, and
  Zheng]{trott2021building}
Trott, A., Srinivasa, S., van~der Wal, D., Haneuse, S., and Zheng, S.
\newblock Building a foundation for data-driven, interpretable, and robust
  policy design using the {AI} economist.
\newblock \emph{arXiv preprint arXiv:2108.02904}, 2021.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al.
\newblock Grandmaster level in {S}tar{C}raft {II} using multi-agent
  reinforcement learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Wang \& Sandholm(2002)Wang and Sandholm]{wang2002reinforcement}
Wang, X. and Sandholm, T.
\newblock Reinforcement learning to play an optimal {N}ash equilibrium in team
  {M}arkov games.
\newblock \emph{Advances in neural information processing systems},
  15:\penalty0 1603--1610, 2002.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Han, Wang, Dong, and
  Zhang]{wang2020off}
Wang, Y., Han, B., Wang, T., Dong, H., and Zhang, C.
\newblock {DOP}: {O}ff-policy multi-agent decomposed policy gradients.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Wang, and
  Kakade]{wang2021exponential}
Wang, Y., Wang, R., and Kakade, S.
\newblock An exponential lower bound for linearly realizable {MDP} with
  constant suboptimality gap.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{b}}.

\bibitem[Wei et~al.(2020)Wei, Lee, Zhang, and Luo]{wei2020linear}
Wei, C.-Y., Lee, C.-W., Zhang, M., and Luo, H.
\newblock Linear last-iterate convergence in constrained saddle-point
  optimization.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Wei et~al.(2021{\natexlab{a}})Wei, Jahromi, Luo, and
  Jain]{wei2021learning}
Wei, C.-Y., Jahromi, M.~J., Luo, H., and Jain, R.
\newblock Learning infinite-horizon average-reward {MDP}s with linear function
  approximation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3007--3015. PMLR, 2021{\natexlab{a}}.

\bibitem[Wei et~al.(2021{\natexlab{b}})Wei, Lee, Zhang, and
  Luo]{Wei2021LastiterateCO}
Wei, C.-Y., Lee, C.-W., Zhang, M., and Luo, H.
\newblock Last-iterate convergence of decentralized optimistic gradient
  descent/ascent in infinite-horizon competitive {M}arkov games.
\newblock In \emph{Conference on learning theory}, 2021{\natexlab{b}}.

\bibitem[Weisz et~al.(2021)Weisz, Amortila, and
  Szepesv{\'a}ri]{weisz2021exponential}
Weisz, G., Amortila, P., and Szepesv{\'a}ri, C.
\newblock Exponential lower bounds for planning in {MDP}s with
  linearly-realizable optimal action-value functions.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  1237--1264, 2021.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3):\penalty0 229--256, 1992.

\bibitem[Xiao(2022)]{xiao2022convergence}
Xiao, L.
\newblock On the convergence rates of policy gradient methods.
\newblock \emph{arXiv preprint arXiv:2201.07443}, 2022.

\bibitem[Xie \& Zhong(2020)Xie and Zhong]{xie2020semicentralized}
Xie, D. and Zhong, X.
\newblock Semicentralized deep deterministic policy gradient in cooperative
  {S}tar{C}raft games.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  2020.

\bibitem[Xie et~al.(2020)Xie, Chen, Wang, and Yang]{xie2020learning}
Xie, Q., Chen, Y., Wang, Z., and Yang, Z.
\newblock Learning zero-sum simultaneous-move {M}arkov games using function
  approximation and correlated equilibrium.
\newblock In \emph{Conference on learning theory}, pp.\  3674--3682, 2020.

\bibitem[Yu et~al.(2021)Yu, Velu, Vinitsky, Wang, Bayen, and
  Wu]{yu2021surprising}
Yu, C., Velu, A., Vinitsky, E., Wang, Y., Bayen, A., and Wu, Y.
\newblock The surprising effectiveness of {PPO} in cooperative, multi-agent
  games.
\newblock \emph{arXiv preprint arXiv:2103.01955}, 2021.

\bibitem[Zazo et~al.(2016)Zazo, Macua, S{\'a}nchez-Fern{\'a}ndez, and
  Zazo]{zazo2016dynamic}
Zazo, S., Macua, S.~V., S{\'a}nchez-Fern{\'a}ndez, M., and Zazo, J.
\newblock Dynamic potential games with constraints: Fundamentals and
  applications in communications.
\newblock \emph{IEEE Transactions on Signal Processing}, 64\penalty0
  (14):\penalty0 3806--3821, 2016.

\bibitem[Zhan et~al.(2021)Zhan, Cen, Huang, Chen, Lee, and Chi]{zhan2021policy}
Zhan, W., Cen, S., Huang, B., Chen, Y., Lee, J.~D., and Chi, Y.
\newblock Policy mirror descent for regularized reinforcement learning: {A}
  generalized framework with linear convergence.
\newblock \emph{arXiv preprint arXiv:2105.11066}, 2021.

\bibitem[Zhang et~al.(2019)Zhang, Yang, and Basar]{zhang2019policy}
Zhang, K., Yang, Z., and Basar, T.
\newblock Policy optimization provably converges to {N}ash equilibria in
  zero-sum linear quadratic games.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 11602--11614, 2019.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Yang, and
  Ba{\c{s}}ar]{zhang2021multi}
Zhang, K., Yang, Z., and Ba{\c{s}}ar, T.
\newblock Multi-agent reinforcement learning: {A} selective overview of
  theories and algorithms.
\newblock \emph{Handbook of Reinforcement Learning and Control}, pp.\
  321--384, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Ren, and Li]{zhang2021gradient}
Zhang, R., Ren, Z., and Li, N.
\newblock Gradient play in multi-agent {M}arkov stochastic games: {S}tationary
  points and convergence.
\newblock \emph{arXiv preprint arXiv:2106.00198}, 2021{\natexlab{b}}.

\bibitem[Zhang et~al.(2022)Zhang, Mei, Dai, Schuurmans, and
  Li]{zhang2022effect}
Zhang, R., Mei, J., Dai, B., Schuurmans, D., and Li, N.
\newblock On the effect of log-barrier regularization in decentralized softmax
  gradient play in multiagent systems.
\newblock \emph{arXiv preprint arXiv:2202.00872}, 2022.

\bibitem[Zhao et~al.(2021)Zhao, Tian, Lee, and Du]{zhao2021provably}
Zhao, Y., Tian, Y., Lee, J.~D., and Du, S.~S.
\newblock Provably efficient policy gradient methods for two-player zero-sum
  {M}arkov games.
\newblock \emph{arXiv preprint arXiv:2102.08903}, 2021.

\bibitem[Zheng et~al.(2020)Zheng, Trott, Srinivasa, Naik, Gruesbeck, Parkes,
  and Socher]{zheng2020ai}
Zheng, S., Trott, A., Srinivasa, S., Naik, N., Gruesbeck, M., Parkes, D.~C.,
  and Socher, R.
\newblock The {AI} economist: {I}mproving equality and productivity with
  {AI}-driven tax policies.
\newblock \emph{arXiv preprint arXiv:2004.13332}, 2020.

\bibitem[Zinkevich(2003)]{zinkevich2003online}
Zinkevich, M.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{Proceedings of the 20th international conference on machine
  learning}, pp.\  928--936, 2003.

\end{thebibliography}
