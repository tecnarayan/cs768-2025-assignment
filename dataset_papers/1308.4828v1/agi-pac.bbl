\begin{thebibliography}{17}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Auer et~al.(2010)Auer, Jaksch, and Ortner]{AJO10}
P.~Auer, T.~Jaksch, and R.~Ortner.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{J. Mach. Learn. Res.}, 99:\penalty0 1563--1600, August 2010.
\newblock ISSN 1532-4435.

\bibitem[Azar et~al.(2012)Azar, Munos, and Kappen]{AMK12}
M.~Azar, R.~Munos, and B.~Kappen.
\newblock On the sample complexity of reinforcement learning with a generative
  model.
\newblock In \emph{Proceedings of the 29th international conference on machine
  learning}, New York, NY, USA, 2012. ACM.

\bibitem[Chakraborty and Stone(2011)]{CS11}
D.~Chakraborty and P.~Stone.
\newblock Structure learning in ergodic factored mdps without knowledge of the
  transition function's in-degree.
\newblock In \emph{Proceedings of the Twenty Eighth International Conference on
  Machine Learning (ICML'11)}, 2011.

\bibitem[Diuk et~al.(2009)Diuk, Li, and Leffler]{DLL09}
C.~Diuk, L.~Li, and B.~Leffler.
\newblock The adaptive $k$-meteorologists problem and its application to
  structure learning and feature selection in reinforcement learning.
\newblock In Andrea~Pohoreckyj Danyluk, L{\'e}on Bottou, and Michael~L.
  Littman, editors, \emph{Proceedings of the 26th Annual International
  Conference on Machine Learning (ICML 2009)}, pages 249--256. ACM, 2009.

\bibitem[Even-dar et~al.(2005)Even-dar, Kakade, and Mansour]{EKM05}
E.~Even-dar, S.~Kakade, and Y.~Mansour.
\newblock Reinforcement learning in {POMDP}s without resets.
\newblock In \emph{In IJCAI}, pages 690--695, 2005.

\bibitem[Hutter(2002)]{Hut02}
M.~Hutter.
\newblock Self-optimizing and {P}areto-optimal policies in general environments
  based on {B}ayes-mixtures.
\newblock In \emph{Proc. 15th Annual Conf. on Computational Learning Theory
  ({COLT'02})}, volume 2375 of \emph{LNAI}, pages 364--379, Sydney, 2002.
  Springer, Berlin.
\newblock URL \url{http://arxiv.org/abs/cs.AI/0204040}.

\bibitem[Hutter(2005)]{Hut05}
M.~Hutter.
\newblock \emph{Universal Artificial Intelligence: Sequential Decisions based
  on Algorithmic Probability}.
\newblock Springer, Berlin, 2005.
\newblock URL \url{http://www.hutter1.net/ai/uaibook.htm}.

\bibitem[Lattimore and Hutter(2011{\natexlab{a}})]{HL11}
T.~Lattimore and M.~Hutter.
\newblock Time consistent discounting.
\newblock In Jyrki Kivinen, Csaba Szepesv{\'a}ri, Esko Ukkonen, and Thomas
  Zeugmann, editors, \emph{Algorithmic Learning Theory}, volume 6925 of
  \emph{Lecture Notes in Computer Science}. Springer Berlin / Heidelberg,
  2011{\natexlab{a}}.

\bibitem[Lattimore and Hutter(2011{\natexlab{b}})]{HL11b}
T.~Lattimore and M.~Hutter.
\newblock Asymptotically optimal agents.
\newblock In Jyrki Kivinen, Csaba Szepesv{\'a}ri, Esko Ukkonen, and Thomas
  Zeugmann, editors, \emph{Algorithmic Learning Theory}, volume 6925 of
  \emph{Lecture Notes in Computer Science}. Springer Berlin / Heidelberg,
  2011{\natexlab{b}}.

\bibitem[Lattimore and Hutter(2012)]{LH12}
T.~Lattimore and M.~Hutter.
\newblock {PAC} bounds for discounted {MDP}s.
\newblock Technical report, 2012.
\newblock http://tor-lattimore.com/pubs/pac-tech.pdf.

\bibitem[Mannor and Tsitsiklis(2004)]{MT04}
S.~Mannor and J.~Tsitsiklis.
\newblock The sample complexity of exploration in the multi-armed bandit
  problem.
\newblock \emph{J. Mach. Learn. Res.}, 5:\penalty0 623--648, December 2004.
\newblock ISSN 1532-4435.

\bibitem[Odalric-Ambrym et~al.(2013)Odalric-Ambrym, Nguyen, Ortner, and
  Ryabko]{MNOR13}
M.~Odalric-Ambrym, P.~Nguyen, R.~Ortner, and D.~Ryabko.
\newblock Optimal regret bounds for selecting the state representation in
  reinforcement learning.
\newblock In \emph{Proceedings of the Thirtieth International Conference on
  Machine Learning (ICML'13)}, 2013.

\bibitem[Ryabko and Hutter(2008)]{RH08}
D.~Ryabko and M.~Hutter.
\newblock On the possibility of learning in reactive environments with
  arbitrary dependence.
\newblock \emph{Theoretical Computer Science}, 405\penalty0 (3):\penalty0
  274--284, 2008.

\bibitem[Strehl and Littman(2005)]{LS05}
A.~Strehl and M.~Littman.
\newblock A theoretical analysis of model-based interval estimation.
\newblock In \emph{Proceedings of the 22nd international conference on Machine
  learning}, ICML '05, pages 856--863, 2005.

\bibitem[Strehl et~al.(2009)Strehl, Li, and Littman]{Str09}
A.~Strehl, L.~Li, and M.~Littman.
\newblock Reinforcement learning in finite {MDP}s: {PAC} analysis.
\newblock \emph{J. Mach. Learn. Res.}, 10:\penalty0 2413--2444, December 2009.

\bibitem[Sunehag and Hutter(2012)]{HS12}
P.~Sunehag and M.~Hutter.
\newblock Optimistic agents are asymptotically optimal.
\newblock In \emph{Proceedings of the 25th Australasian AI conference}, 2012.

\bibitem[Szita and Szepesv{\'a}ri(2010)]{SS10}
I.~Szita and C.~Szepesv{\'a}ri.
\newblock Model-based reinforcement learning with nearly tight exploration
  complexity bounds.
\newblock In \emph{Proceedings of the 27th international conference on Machine
  learning}, pages 1031--1038, New York, NY, USA, 2010. ACM.

\end{thebibliography}
